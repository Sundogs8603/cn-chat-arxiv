<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25972;&#21512;&#24378;&#21270;&#23398;&#20064;&#21644;Stack Overflow&#35780;&#20998;&#65292;&#25552;&#39640;&#20102;GPT Neo&#22312;&#32534;&#31243;&#38382;&#31572;&#20013;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#25351;&#20986;&#20256;&#32479;&#35821;&#35328;&#24230;&#37327;&#26041;&#27861;&#22312;&#32534;&#31243;&#39046;&#22495;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10882</link><description>&lt;p&gt;
&#20351;&#29992;&#20844;&#20849;&#31038;&#21306;&#35780;&#20998;&#20316;&#20026;&#20154;&#31867;&#21453;&#39304;&#65292;&#22312;&#32534;&#31243;&#39046;&#22495;&#20013;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#38382;&#31572;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning for question answering in programming domain using public community scoring as a human feedback. (arXiv:2401.10882v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10882
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25972;&#21512;&#24378;&#21270;&#23398;&#20064;&#21644;Stack Overflow&#35780;&#20998;&#65292;&#25552;&#39640;&#20102;GPT Neo&#22312;&#32534;&#31243;&#38382;&#31572;&#20013;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#25351;&#20986;&#20256;&#32479;&#35821;&#35328;&#24230;&#37327;&#26041;&#27861;&#22312;&#32534;&#31243;&#39046;&#22495;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25972;&#21512;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#21644;&#26469;&#33258;Stack Overflow&#30340;&#35780;&#20998;&#65292;&#25506;&#35752;&#20102;&#22312;&#32534;&#31243;&#39046;&#22495;&#20013;&#25552;&#39640;GPT Neo 125M&#22312;&#31038;&#21306;&#38382;&#31572;&#20013;&#30340;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#37319;&#29992;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#22870;&#21169;&#27169;&#22411;&#35757;&#32451;&#31574;&#30053;&#36827;&#34892;&#24494;&#35843;&#65292;&#36890;&#36807;Proximal Policy Optimization (PPO)&#23454;&#29616;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#24615;&#33021;&#25913;&#36827;&#26041;&#38754;&#19982;GPT Neo 2.7B&#21442;&#25968;&#21464;&#20307;&#30456;&#24403;&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#36741;&#21161;&#35780;&#20998;&#26426;&#21046;&#65292;&#26174;&#31034;&#20102;&#20256;&#32479;&#35821;&#35328;&#24230;&#37327;&#22312;&#32534;&#31243;&#39046;&#22495;&#20013;&#35780;&#20272;&#21709;&#24212;&#30340;&#23616;&#38480;&#24615;&#12290;&#36890;&#36807;&#20934;&#30830;&#30340;&#20998;&#26512;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#24212;&#29992;&#20110;&#32534;&#31243;&#31038;&#21306;&#38382;&#31572;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#38656;&#35201;&#39046;&#22495;&#29305;&#23450;&#30340;&#35780;&#20272;&#26041;&#27861;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we investigate the enhancement of the GPT Neo 125M performance in Community Question Answering (CQA) with a focus on programming, through the integration of Reinforcement Learning from Human Feedback (RLHF) and the utilization of scores from Stack Overflow. Two distinct reward model training strategies are employed for fine-tuning with Proximal Policy Optimization (PPO). Notably, the improvements in performance achieved through this method are comparable to those of GPT Neo 2.7B parameter variant. Additionally, an auxiliary scoring mechanism is introduced, which demonstrates the limitations of conventional linguistic metrics in evaluating responses in the programming domain. Through accurate analysis, this paper looks at the divergence between traditional linguistic metrics and our human-preferences-based reward model, underscoring the imperative for domain-specific evaluation methods. By elucidating the complexities involved in applying RLHF to programming CQA and accen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21098;&#26525;&#23545;&#40784;&#30340;LLMs&#30340;&#20445;&#25252;&#25514;&#26045;&#65292;&#21457;&#29616;&#21098;&#26525;LLM&#21442;&#25968;&#21487;&#20197;&#26174;&#33879;&#22686;&#24378;&#20854;&#25269;&#25239;&#8220;&#36234;&#29425;&#8221;&#25552;&#31034;&#25915;&#20987;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#23545;&#20854;&#20182;LLM&#34892;&#20026;&#20063;&#21487;&#33021;&#26377;&#26356;&#26222;&#36941;&#30340;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26377;&#23475;&#20219;&#21153;&#25968;&#25454;&#38598;&#65292;&#35777;&#26126;&#21098;&#26525;&#26377;&#21161;&#20110;&#38598;&#20013;&#27880;&#24847;&#21147;&#22312;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#26631;&#35760;&#19978;&#12290;&#31361;&#20986;&#30340;&#32842;&#22825;&#27169;&#22411;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#26131;&#24863;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10862</link><description>&lt;p&gt;
&#22522;&#20110;&#21098;&#26525;&#30340;&#20445;&#25252;: &#22312;&#19981;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#22686;&#21152;&#23545;&#40784;&#30340;LLMs&#30340;&#36234;&#29425;&#25269;&#25239;&#21147;
&lt;/p&gt;
&lt;p&gt;
Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning. (arXiv:2401.10862v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21098;&#26525;&#23545;&#40784;&#30340;LLMs&#30340;&#20445;&#25252;&#25514;&#26045;&#65292;&#21457;&#29616;&#21098;&#26525;LLM&#21442;&#25968;&#21487;&#20197;&#26174;&#33879;&#22686;&#24378;&#20854;&#25269;&#25239;&#8220;&#36234;&#29425;&#8221;&#25552;&#31034;&#25915;&#20987;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#23545;&#20854;&#20182;LLM&#34892;&#20026;&#20063;&#21487;&#33021;&#26377;&#26356;&#26222;&#36941;&#30340;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26377;&#23475;&#20219;&#21153;&#25968;&#25454;&#38598;&#65292;&#35777;&#26126;&#21098;&#26525;&#26377;&#21161;&#20110;&#38598;&#20013;&#27880;&#24847;&#21147;&#22312;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#26631;&#35760;&#19978;&#12290;&#31361;&#20986;&#30340;&#32842;&#22825;&#27169;&#22411;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#26131;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23481;&#26131;&#21463;&#21040;&#8220;&#36234;&#29425;&#8221;&#25552;&#31034;&#30340;&#25915;&#20987;&#65292;&#36825;&#31181;&#25915;&#20987;&#21487;&#20197;&#35825;&#20351;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#26377;&#23475;&#21644;&#36829;&#27861;&#20869;&#23481;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#21098;&#26525;LLM&#21442;&#25968;&#22810;&#36798;20&#65285;&#21487;&#20197;&#26174;&#33879;&#22686;&#21152;&#23427;&#20204;&#23545;&#27492;&#31867;&#25915;&#20987;&#30340;&#25269;&#25239;&#21147;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#24182;&#19988;&#19981;&#25439;&#23475;&#20854;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#21098;&#26525;&#21518;&#35266;&#23519;&#21040;&#30340;&#22686;&#24378;&#23433;&#20840;&#24615;&#19982;&#27169;&#22411;&#30340;&#21021;&#22987;&#23433;&#20840;&#35757;&#32451;&#27700;&#24179;&#30456;&#20851;&#65292;&#36825;&#26263;&#31034;&#21098;&#26525;&#30340;&#25928;&#26524;&#21487;&#33021;&#26356;&#26222;&#36941;&#65292;&#20063;&#21487;&#33021;&#36866;&#29992;&#20110;&#36229;&#20986;&#23433;&#20840;&#24615;&#33539;&#30068;&#30340;&#20854;&#20182;LLM&#34892;&#20026;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;&#20116;&#20010;&#31867;&#21035;&#12289;&#25554;&#20837;&#21040;&#21313;&#20010;&#19981;&#21516;&#36234;&#29425;&#25552;&#31034;&#20013;&#30340;225&#20010;&#26377;&#23475;&#20219;&#21153;&#30340;&#31934;&#36873;&#25968;&#25454;&#38598;&#65292;&#34920;&#26126;&#21098;&#26525;&#26377;&#21161;&#20110;LLMs&#38598;&#20013;&#27880;&#24847;&#21147;&#22312;&#36234;&#29425;&#25552;&#31034;&#20013;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#26631;&#35760;&#19978;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#31361;&#20986;&#30340;&#32842;&#22825;&#27169;&#22411;&#65288;&#22914;LLaMA-2 Chat&#65292;Vicuna&#21644;Mistral Instruct&#65289;&#20855;&#26377;&#24456;&#39640;&#30340;&#26131;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are vulnerable to `Jailbreaking' prompts, a type of attack that can coax these models into generating harmful and illegal content. In this paper, we show that pruning up to 20% of LLM parameters markedly increases their resistance to such attacks without additional training and without sacrificing their performance in standard benchmarks. Intriguingly, we discovered that the enhanced safety observed post-pruning correlates to the initial safety training level of the model, hinting that the effect of pruning could be more general and may hold for other LLM behaviors beyond safety. Additionally, we introduce a curated dataset of 225 harmful tasks across five categories, inserted into ten different Jailbreaking prompts, showing that pruning aids LLMs in concentrating attention on task-relevant tokens in jailbreaking prompts. Lastly, our experiments reveal that the prominent chat models, such as LLaMA-2 Chat, Vicuna, and Mistral Instruct exhibit high susceptibi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#22312;eHealth&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#36827;&#23637;&#65292;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#21644;&#35299;&#37322;&#21307;&#30103;&#39046;&#22495;&#30340;&#22823;&#37327;&#25991;&#26412;&#25968;&#25454;&#65292;&#20174;&#32780;&#25552;&#39640;&#21307;&#30103;&#26381;&#21153;&#21644;&#25972;&#20010;&#21307;&#30103;&#39046;&#22495;&#30340;&#25928;&#29575;&#21644;&#30693;&#35782;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2401.10850</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#22312;eHealth&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Advancements in eHealth Data Analytics through Natural Language Processing and Deep Learning. (arXiv:2401.10850v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10850
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#22312;eHealth&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#36827;&#23637;&#65292;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#21644;&#35299;&#37322;&#21307;&#30103;&#39046;&#22495;&#30340;&#22823;&#37327;&#25991;&#26412;&#25968;&#25454;&#65292;&#20174;&#32780;&#25552;&#39640;&#21307;&#30103;&#26381;&#21153;&#21644;&#25972;&#20010;&#21307;&#30103;&#39046;&#22495;&#30340;&#25928;&#29575;&#21644;&#30693;&#35782;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#29615;&#22659;&#36890;&#24120;&#34987;&#31216;&#20026;&#8220;&#20449;&#24687;&#20016;&#23500;&#8221;&#20294;&#20063;&#8220;&#30693;&#35782;&#21294;&#20047;&#8221;&#12290;&#21307;&#30103;&#31995;&#32479;&#20174;&#21508;&#31181;&#26469;&#28304;&#25910;&#38598;&#22823;&#37327;&#25968;&#25454;&#65292;&#21253;&#25324;&#23454;&#39564;&#23460;&#25253;&#21578;&#12289;&#21307;&#30103;&#20449;&#20989;&#12289;&#21307;&#30103;&#24037;&#20855;&#25110;&#31243;&#24207;&#30340;&#26085;&#24535;&#12289;&#21307;&#30103;&#22788;&#26041;&#31561;&#12290;&#36825;&#20123;&#24222;&#22823;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#25552;&#20379;&#26377;&#20851;&#21307;&#30103;&#26381;&#21153;&#21644;&#25972;&#20010;&#21307;&#30103;&#39046;&#22495;&#30340;&#23453;&#36149;&#30693;&#35782;&#21644;&#20449;&#24687;&#65292;&#20363;&#22914;&#36890;&#36807;&#20998;&#26512;&#24739;&#32773;&#30151;&#29366;&#36827;&#34892;&#30142;&#30149;&#39044;&#27979;&#25110;&#36890;&#36807;&#21457;&#29616;&#30142;&#30149;&#30340;&#34892;&#20026;&#22240;&#32032;&#36827;&#34892;&#30142;&#30149;&#39044;&#38450;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#21482;&#26377;&#30456;&#23545;&#36739;&#23569;&#30340;&#25991;&#26412;eHealth&#25968;&#25454;&#34987;&#22788;&#29702;&#21644;&#35299;&#37322;&#65292;&#20854;&#20013;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#26159;&#22312;&#25191;&#34892;&#22823;&#25968;&#25454;&#25805;&#20316;&#26102;&#30340;&#22256;&#38590;&#12290;&#22312;&#21307;&#30103;&#39046;&#22495;&#65292;&#26816;&#27979;&#29305;&#23450;&#39046;&#22495;&#30340;&#22810;&#35789;&#26415;&#35821;&#26159;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#29992;&#20960;&#20010;&#35789;&#26469;&#23450;&#20041;&#19968;&#20010;&#25972;&#20010;&#27010;&#24565;&#12290;&#26415;&#35821;&#21487;&#20197;&#23450;&#20041;&#20026;&#19968;&#20010;&#35821;&#35328;&#32467;&#26500;&#25110;&#27010;&#24565;&#65292;&#23427;&#30001;&#19968;&#20010;&#25110;&#22810;&#20010;&#35789;&#32452;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
The healthcare environment is commonly referred to as "information-rich" but also "knowledge poor". Healthcare systems collect huge amounts of data from various sources: lab reports, medical letters, logs of medical tools or programs, medical prescriptions, etc. These massive sets of data can provide great knowledge and information that can improve the medical services, and overall the healthcare domain, such as disease prediction by analyzing the patient's symptoms or disease prevention, by facilitating the discovery of behavioral factors for diseases. Unfortunately, only a relatively small volume of the textual eHealth data is processed and interpreted, an important factor being the difficulty in efficiently performing Big Data operations. In the medical field, detecting domain-specific multi-word terms is a crucial task as they can define an entire concept with a few words. A term can be defined as a linguistic structure or a concept, and it is composed of one or more words with a s
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#26816;&#27979;&#26032;&#20986;&#29616;&#30340;&#32534;&#30721;&#24694;&#24847;&#26415;&#35821;&#65292;&#20026;&#26497;&#31471;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#21453;&#29369;&#22826;&#24694;&#24847;&#35328;&#35770;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.10841</link><description>&lt;p&gt;
&#20351;&#29992;LLMs&#21457;&#29616;&#26497;&#31471;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#32534;&#30721;&#21453;&#29369;&#22826;&#24694;&#24847;&#35328;&#35770;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
Using LLMs to discover emerging coded antisemitic hate-speech emergence in extremist social media. (arXiv:2401.10841v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10841
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#26816;&#27979;&#26032;&#20986;&#29616;&#30340;&#32534;&#30721;&#24694;&#24847;&#26415;&#35821;&#65292;&#20026;&#26497;&#31471;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#21453;&#29369;&#22826;&#24694;&#24847;&#35328;&#35770;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#20167;&#24680;&#35328;&#35770;&#30340;&#34067;&#24310;&#32473;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#24102;&#26469;&#20102;&#19968;&#20010;&#38590;&#39064;&#12290;&#19968;&#20010;&#29305;&#27530;&#30340;&#25361;&#25112;&#19982;&#20351;&#29992;&#32534;&#30721;&#35821;&#35328;&#30340;&#32676;&#20307;&#26377;&#20851;&#65292;&#36825;&#20123;&#32676;&#20307;&#26082;&#24819;&#20026;&#20854;&#29992;&#25143;&#21019;&#36896;&#24402;&#23646;&#24863;&#65292;&#21448;&#24819;&#22238;&#36991;&#26816;&#27979;&#12290;&#32534;&#30721;&#35821;&#35328;&#21457;&#23637;&#36805;&#36895;&#65292;&#24182;&#19988;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#20351;&#29992;&#26041;&#24335;&#19981;&#21516;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#27979;&#26032;&#20986;&#29616;&#30340;&#32534;&#30721;&#24694;&#24847;&#26415;&#35821;&#30340;&#26041;&#27861;&#35770;&#12290;&#35813;&#26041;&#27861;&#22312;&#22312;&#32447;&#21453;&#29369;&#22826;&#35328;&#35770;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#20174;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#25235;&#21462;&#30340;&#24086;&#23376;&#65292;&#36890;&#24120;&#26159;&#26497;&#31471;&#20027;&#20041;&#29992;&#25143;&#20351;&#29992;&#30340;&#12290;&#24086;&#23376;&#26159;&#20351;&#29992;&#19982;&#20197;&#21069;&#24050;&#30693;&#30340;&#38024;&#23545;&#29369;&#22826;&#20154;&#30340;&#20167;&#24680;&#35328;&#35770;&#30456;&#20851;&#30340;&#31181;&#23376;&#34920;&#36798;&#24335;&#36827;&#34892;&#25235;&#21462;&#30340;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#36890;&#36807;&#35782;&#21035;&#27599;&#20010;&#24086;&#23376;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#34920;&#36798;&#24335;&#65292;&#24182;&#35745;&#31639;&#23427;&#20204;&#22312;&#25972;&#20010;&#35821;&#26009;&#24211;&#20013;&#30340;&#39057;&#29575;&#12290;&#36807;&#28388;&#25481;&#35821;&#27861;&#19981;&#19968;&#33268;&#30340;&#34920;&#36798;&#24335;&#21644;&#20043;&#21069;&#36935;&#21040;&#36807;&#30340;&#34920;&#36798;&#24335;&#65292;&#20197;&#20415;&#20851;&#27880;&#26032;&#20986;&#29616;&#30340;&#33391;&#22909;&#24418;&#24335;&#30340;&#26415;&#35821;&#12290;&#28982;&#21518;&#36827;&#34892;&#20102;&#35821;&#20041;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online hate speech proliferation has created a difficult problem for social media platforms. A particular challenge relates to the use of coded language by groups interested in both creating a sense of belonging for its users and evading detection. Coded language evolves quickly and its use varies over time. This paper proposes a methodology for detecting emerging coded hate-laden terminology. The methodology is tested in the context of online antisemitic discourse. The approach considers posts scraped from social media platforms, often used by extremist users. The posts are scraped using seed expressions related to previously known discourse of hatred towards Jews. The method begins by identifying the expressions most representative of each post and calculating their frequency in the whole corpus. It filters out grammatically incoherent expressions as well as previously encountered ones so as to focus on emergent well-formed terminology. This is followed by an assessment of semantic s
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35843;&#26597;&#20102;&#26368;&#36817;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30740;&#31350;&#36827;&#23637;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#19981;&#21516;&#31639;&#27861;&#24615;&#33021;&#30340;&#28145;&#24230;&#27604;&#36739;&#65292;&#36824;&#25506;&#35752;&#20102;&#25968;&#25454;&#38598;&#29305;&#24449;&#23545;&#26041;&#27861;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.10825</link><description>&lt;p&gt;
&#26368;&#26032;&#36827;&#23637;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A survey on recent advances in named entity recognition. (arXiv:2401.10825v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10825
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35843;&#26597;&#20102;&#26368;&#36817;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30740;&#31350;&#36827;&#23637;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#19981;&#21516;&#31639;&#27861;&#24615;&#33021;&#30340;&#28145;&#24230;&#27604;&#36739;&#65292;&#36824;&#25506;&#35752;&#20102;&#25968;&#25454;&#38598;&#29305;&#24449;&#23545;&#26041;&#27861;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26088;&#22312;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#20986;&#21629;&#21517;&#30495;&#23454;&#19990;&#30028;&#23545;&#35937;&#30340;&#23376;&#23383;&#31526;&#20018;&#65292;&#24182;&#30830;&#23450;&#20854;&#31867;&#22411;&#65288;&#20363;&#22914;&#65292;&#26159;&#21542;&#25351;&#20154;&#29289;&#25110;&#32452;&#32455;&#65289;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#27010;&#36848;&#20102;&#26368;&#36817;&#27969;&#34892;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#36824;&#20851;&#27880;&#20102;&#22522;&#20110;&#22270;&#21644;&#21464;&#25442;&#22120;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#24456;&#23569;&#22312;&#20854;&#20182;&#32508;&#36848;&#20013;&#28041;&#21450;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;&#38024;&#23545;&#31232;&#32570;&#27880;&#37322;&#25968;&#25454;&#38598;&#35774;&#35745;&#30340;&#26041;&#27861;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20027;&#35201;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#23454;&#29616;&#22312;&#21508;&#31181;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#65288;&#39046;&#22495;&#12289;&#35268;&#27169;&#21644;&#31867;&#21035;&#25968;&#65289;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#20174;&#26410;&#21516;&#26102;&#32771;&#34385;&#30340;&#31639;&#27861;&#30340;&#28145;&#24230;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#25968;&#25454;&#38598;&#29305;&#24449;&#22914;&#20309;&#24433;&#21709;&#25105;&#20204;&#27604;&#36739;&#30340;&#26041;&#27861;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Named Entity Recognition seeks to extract substrings within a text that name real-world objects and to determine their type (for example, whether they refer to persons or organizations). In this survey, we first present an overview of recent popular approaches, but we also look at graph- and transformer- based methods including Large Language Models (LLMs) that have not had much coverage in other surveys. Second, we focus on methods designed for datasets with scarce annotations. Third, we evaluate the performance of the main NER implementations on a variety of datasets with differing characteristics (as regards their domain, their size, and their number of classes). We thus provide a deep comparison of algorithms that are never considered together. Our experiments shed some light on how the characteristics of datasets affect the behavior of the methods that we compare.
&lt;/p&gt;</description></item><item><title>Medusa&#26159;&#19968;&#20010;&#33021;&#22815;&#25552;&#21319;LLM&#25512;&#29702;&#24615;&#33021;&#30340;&#31616;&#27905;&#26694;&#26550;&#65292;&#36890;&#36807;&#22686;&#21152;&#22810;&#20010;&#35299;&#30721;&#22836;&#20197;&#23454;&#29616;&#24182;&#34892;&#39044;&#27979;&#22810;&#20010;&#21518;&#32493;&#26631;&#35760;&#65292;&#24182;&#36890;&#36807;&#26641;&#29366;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#24182;&#34892;&#22788;&#29702;&#26469;&#20943;&#23569;&#35299;&#30721;&#27493;&#39588;&#12290;</title><link>http://arxiv.org/abs/2401.10774</link><description>&lt;p&gt;
Medusa: &#22810;&#35299;&#30721;&#22836;&#30340;&#31616;&#27905;LLM&#25512;&#29702;&#21152;&#36895;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads. (arXiv:2401.10774v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10774
&lt;/p&gt;
&lt;p&gt;
Medusa&#26159;&#19968;&#20010;&#33021;&#22815;&#25552;&#21319;LLM&#25512;&#29702;&#24615;&#33021;&#30340;&#31616;&#27905;&#26694;&#26550;&#65292;&#36890;&#36807;&#22686;&#21152;&#22810;&#20010;&#35299;&#30721;&#22836;&#20197;&#23454;&#29616;&#24182;&#34892;&#39044;&#27979;&#22810;&#20010;&#21518;&#32493;&#26631;&#35760;&#65292;&#24182;&#36890;&#36807;&#26641;&#29366;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#24182;&#34892;&#22788;&#29702;&#26469;&#20943;&#23569;&#35299;&#30721;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#25512;&#29702;&#36807;&#31243;&#36890;&#24120;&#21463;&#38480;&#20110;&#33258;&#22238;&#24402;&#35299;&#30721;&#36807;&#31243;&#20013;&#30340;&#24182;&#34892;&#24615;&#32570;&#22833;&#65292;&#20351;&#24471;&#22823;&#22810;&#25968;&#25805;&#20316;&#21463;&#38480;&#20110;&#21152;&#36895;&#22120;&#30340;&#20869;&#23384;&#24102;&#23485;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#31867;&#20284;&#20110;&#25512;&#27979;&#35299;&#30721;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#30001;&#20110;&#33719;&#24471;&#21644;&#32500;&#25252;&#29420;&#31435;&#30340;&#33609;&#31295;&#27169;&#22411;&#25152;&#28041;&#21450;&#30340;&#25361;&#25112;&#65292;&#23427;&#20204;&#30340;&#23454;&#26045;&#21463;&#21040;&#20102;&#38459;&#30861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28155;&#21152;&#39069;&#22806;&#30340;&#35299;&#30721;&#22836;&#26469;&#22686;&#24378;LLM&#25512;&#29702;&#65292;&#20197;&#24182;&#34892;&#39044;&#27979;&#22810;&#20010;&#21518;&#32493;&#26631;&#35760;&#12290;Medusa&#21033;&#29992;&#22522;&#20110;&#26641;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#22312;&#27599;&#20010;&#35299;&#30721;&#27493;&#39588;&#20013;&#21516;&#26102;&#26500;&#36896;&#22810;&#20010;&#20505;&#36873;&#24310;&#32493;&#24182;&#36827;&#34892;&#39564;&#35777;&#12290;&#36890;&#36807;&#21033;&#29992;&#24182;&#34892;&#22788;&#29702;&#65292;Medusa&#22312;&#21333;&#27493;&#24310;&#36831;&#26041;&#38754;&#20165;&#24341;&#20837;&#20102;&#26368;&#23567;&#30340;&#24320;&#38144;&#65292;&#21516;&#26102;&#22823;&#22823;&#38477;&#20302;&#20102;&#25152;&#38656;&#30340;&#35299;&#30721;&#27493;&#39588;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The inference process in Large Language Models (LLMs) is often limited due to the absence of parallelism in the auto-regressive decoding process, resulting in most operations being restricted by the memory bandwidth of accelerators. While methods such as speculative decoding have been suggested to address this issue, their implementation is impeded by the challenges associated with acquiring and maintaining a separate draft model. In this paper, we present Medusa, an efficient method that augments LLM inference by adding extra decoding heads to predict multiple subsequent tokens in parallel. Using a tree-based attention mechanism, Medusa constructs multiple candidate continuations and verifies them simultaneously in each decoding step. By leveraging parallel processing, Medusa introduces only minimal overhead in terms of single-step latency while substantially reducing the number of decoding steps required.  We present two levels of fine-tuning procedures for Medusa to meet the needs o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#30693;&#35782;&#19968;&#33268;&#24615;&#23545;&#40784;&#65288;KCA&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20943;&#23569;&#35757;&#32451;&#25968;&#25454;&#20013;&#22806;&#37096;&#30693;&#35782;&#21644;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#20869;&#22312;&#30693;&#35782;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;KCA&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.10768</link><description>&lt;p&gt;
&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;&#38382;&#39064;&#65306;&#36890;&#36807;&#30693;&#35782;&#19968;&#33268;&#24615;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Mitigating Hallucinations of Large Language Models via Knowledge Consistent Alignment. (arXiv:2401.10768v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#30693;&#35782;&#19968;&#33268;&#24615;&#23545;&#40784;&#65288;KCA&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20943;&#23569;&#35757;&#32451;&#25968;&#25454;&#20013;&#22806;&#37096;&#30693;&#35782;&#21644;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#20869;&#22312;&#30693;&#35782;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;KCA&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23545;&#40784;&#21518;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#20173;&#21487;&#33021;&#20135;&#29983;&#19982;&#19978;&#19979;&#25991;&#25110;&#19990;&#30028;&#30693;&#35782;&#33258;&#20449;&#30683;&#30462;&#30340;&#21709;&#24212;&#65292;&#36825;&#34987;&#31216;&#20026;&#8220;&#24187;&#35273;&#8221;&#29616;&#35937;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#36890;&#36807;&#20943;&#23569;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#22806;&#37096;&#30693;&#35782;&#19982;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#32487;&#25215;&#30340;&#20869;&#22312;&#30693;&#35782;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#21487;&#20197;&#32531;&#35299;&#23545;&#40784;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#19968;&#33268;&#24615;&#23545;&#40784;&#65288;KCA&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26681;&#25454;&#22806;&#37096;&#30693;&#35782;&#33258;&#21160;&#21046;&#23450;&#32771;&#35797;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#23545;&#20110;&#21253;&#21547;&#30693;&#35782;&#19981;&#19968;&#33268;&#24615;&#30340;&#25968;&#25454;&#65292;KCA&#23454;&#26045;&#20102;&#20960;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#22788;&#29702;&#31574;&#30053;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#32972;&#26223;&#21644;&#35268;&#27169;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20845;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;KCA&#26041;&#27861;&#22312;&#32531;&#35299;&#24187;&#35273;&#26041;&#38754;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Large Language Models (LLMs) have proven to be exceptional on a variety of tasks after alignment, they may still produce responses that contradict the context or world knowledge confidently, a phenomenon known as ``hallucination''. In this paper, we demonstrate that reducing the inconsistency between the external knowledge encapsulated in the training data and the intrinsic knowledge inherited in the pretraining corpus could mitigate hallucination in alignment. Specifically, we introduce a novel knowledge consistent alignment (KCA) approach, which involves automatically formulating examinations based on external knowledge for accessing the comprehension of LLMs. For data encompassing knowledge inconsistency, KCA implements several simple yet efficient strategies for processing. We illustrate the superior performance of the proposed KCA approach in mitigating hallucinations across six benchmarks using LLMs of different backbones and scales. Furthermore, we confirm the correlation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#36801;&#31227;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32570;&#22833;&#27169;&#24577;&#19979;&#36827;&#34892;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#12290;&#36890;&#36807;&#32763;&#35793;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#20869;&#23481;&#20197;&#37325;&#26500;&#32570;&#22833;&#30340;&#38899;&#39057;&#27169;&#24577;&#65292;&#24182;&#21033;&#29992;&#36328;&#27169;&#24577;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#24773;&#24863;&#39044;&#27979;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#21644;&#19982;&#23436;&#25972;&#22810;&#27169;&#24577;&#30417;&#30563;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.10747</link><description>&lt;p&gt;
&#32570;&#22833;&#27169;&#24577;&#19979;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;:&#19968;&#31181;&#30693;&#35782;&#36801;&#31227;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multimodal Sentiment Analysis with Missing Modality: A Knowledge-Transfer Approach. (arXiv:2401.10747v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#36801;&#31227;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32570;&#22833;&#27169;&#24577;&#19979;&#36827;&#34892;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#12290;&#36890;&#36807;&#32763;&#35793;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#20869;&#23481;&#20197;&#37325;&#26500;&#32570;&#22833;&#30340;&#38899;&#39057;&#27169;&#24577;&#65292;&#24182;&#21033;&#29992;&#36328;&#27169;&#24577;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#24773;&#24863;&#39044;&#27979;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#21644;&#19982;&#23436;&#25972;&#22810;&#27169;&#24577;&#30417;&#30563;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#26088;&#22312;&#36890;&#36807;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#22768;&#38899;&#32447;&#32034;&#26469;&#35782;&#21035;&#20010;&#20307;&#34920;&#36798;&#30340;&#24773;&#32490;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#22823;&#22810;&#20551;&#35774;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#36807;&#31243;&#20013;&#25152;&#26377;&#27169;&#24577;&#37117;&#26159;&#21487;&#29992;&#30340;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#30340;&#31639;&#27861;&#23481;&#26131;&#21463;&#21040;&#32570;&#22833;&#27169;&#24577;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#36801;&#31227;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#65292;&#20197;&#37325;&#26500;&#32570;&#22833;&#30340;&#38899;&#39057;&#27169;&#24577;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#36328;&#27169;&#24577;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#20445;&#30041;&#37325;&#26500;&#21644;&#35266;&#23519;&#21040;&#30340;&#27169;&#24577;&#30340;&#26368;&#22823;&#20449;&#24687;&#65292;&#29992;&#20110;&#24773;&#24863;&#39044;&#27979;&#12290;&#22312;&#19977;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#30456;&#23545;&#20110;&#22522;&#32447;&#31639;&#27861;&#30340;&#26174;&#33879;&#25913;&#36827;&#65292;&#24182;&#23454;&#29616;&#20102;&#19982;&#20855;&#26377;&#23436;&#25972;&#22810;&#27169;&#24577;&#30417;&#30563;&#30340;&#20808;&#21069;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal sentiment analysis aims to identify the emotions expressed by individuals through visual, language, and acoustic cues. However, most of the existing research efforts assume that all modalities are available during both training and testing, making their algorithms susceptible to the missing modality scenario. In this paper, we propose a novel knowledge-transfer network to translate between different modalities to reconstruct the missing audio modalities. Moreover, we develop a cross-modality attention mechanism to retain the maximal information of the reconstructed and observed modalities for sentiment prediction. Extensive experiments on three publicly available datasets demonstrate significant improvements over baselines and achieve comparable results to the previous methods with complete multi-modality supervision.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#32467;&#26500;&#21270;&#30340;&#20195;&#30721;&#34920;&#31034;&#26469;&#25552;&#39640;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#25928;&#29575;&#36866;&#24212;&#24615;&#65292;&#36890;&#36807;&#20351;&#29992;&#35299;&#26512;&#26641;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#21363;&#20351;&#21482;&#23545;&#34920;&#38754;&#24418;&#24335;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20063;&#33021;&#22312;&#21508;&#31181;&#20195;&#30721;&#20219;&#21153;&#19978;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.10716</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#20195;&#30721;&#34920;&#31034;&#20351;&#24471;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#25928;&#29575;&#36866;&#24212;&#21464;&#24471;&#21487;&#33021;
&lt;/p&gt;
&lt;p&gt;
Structured Code Representations Enable Data-Efficient Adaptation of Code Language Models. (arXiv:2401.10716v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#32467;&#26500;&#21270;&#30340;&#20195;&#30721;&#34920;&#31034;&#26469;&#25552;&#39640;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#25928;&#29575;&#36866;&#24212;&#24615;&#65292;&#36890;&#36807;&#20351;&#29992;&#35299;&#26512;&#26641;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#21363;&#20351;&#21482;&#23545;&#34920;&#38754;&#24418;&#24335;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20063;&#33021;&#22312;&#21508;&#31181;&#20195;&#30721;&#20219;&#21153;&#19978;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#20026;&#20195;&#30721;&#20219;&#21153;&#23450;&#21046;&#30340;&#35821;&#35328;&#27169;&#22411;&#24120;&#24120;&#37319;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#65292;&#23558;&#28304;&#20195;&#30721;&#24314;&#27169;&#20026;&#32431;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24573;&#35270;&#20102;&#32534;&#31243;&#35821;&#35328;&#20013;&#22266;&#26377;&#30340;&#26126;&#30830;&#32467;&#26500;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#36827;&#19968;&#27493;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#25506;&#32034;&#20102;&#23545;&#39044;&#35757;&#32451;&#30340;&#20195;&#30721;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#25928;&#29575;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#23558;&#31243;&#24207;&#34920;&#31034;&#20026;&#35299;&#26512;&#26641;&#65288;&#20063;&#31216;&#20026;&#20855;&#20307;&#35821;&#27861;&#26641;&#65289;&#65292;&#24182;&#22312;&#24207;&#21015;&#21270;&#30340;&#35299;&#26512;&#26641;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#12290;&#23613;&#31649;&#25105;&#20204;&#36866;&#24212;&#30340;&#27169;&#22411;&#20165;&#22312;&#31243;&#24207;&#30340;&#34920;&#38754;&#24418;&#24335;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#22312;&#19981;&#25913;&#21464;&#27169;&#22411;&#26550;&#26500;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#35299;&#26512;&#26641;&#19978;&#36827;&#34892;&#23569;&#37327;&#30340;&#36830;&#32493;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#21487;&#20197;&#22312;&#21508;&#31181;&#20195;&#30721;&#20219;&#21153;&#19978;&#25913;&#36827;&#22522;&#32447;&#26041;&#27861;&#12290;&#24403;&#35757;&#32451;&#26679;&#26412;&#26377;&#38480;&#26102;&#65292;&#36825;&#31181;&#25913;&#36827;&#23588;&#20026;&#26174;&#33879;&#65292;&#35777;&#26126;&#20102;&#25972;&#21512;&#31243;&#24207;&#32467;&#26500;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current language models tailored for code tasks often adopt the pre-training-then-fine-tuning paradigm from natural language processing, modeling source code as plain text. This approach, however, overlooks the unambiguous structures inherent in programming languages. In this work, we explore data-efficient adaptation of pre-trained code models by further pre-training and fine-tuning them with program structures. Specifically, we represent programs as parse trees -- also known as concrete syntax trees (CSTs) -- and adapt pre-trained models on serialized CSTs. Although the models that we adapt have been pre-trained only on the surface form of programs, we find that a small amount of continual pre-training and fine-tuning on CSTs without changing the model architecture yields improvements over the baseline approach across various code tasks. The improvements are found to be particularly significant when there are limited training examples, demonstrating the effectiveness of integrating p
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;Q&amp;A&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25366;&#25496;&#22270;&#20687;&#20013;&#30340;&#38382;&#39064;-&#22238;&#31572;&#23545;&#26469;&#21457;&#29616;&#20016;&#23500;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#20197;&#24110;&#21161;AI&#27169;&#22411;&#26356;&#22909;&#22320;&#29702;&#35299;&#22797;&#26434;&#35270;&#35273;&#38382;&#39064;&#65292;&#25552;&#39640;&#36328;&#27169;&#24577;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.10712</link><description>&lt;p&gt;
Q&amp;A&#25552;&#31034;&#65306;&#36890;&#36807;&#25366;&#25496;&#38382;&#39064;-&#22238;&#31572;&#25552;&#31034;&#26469;&#21457;&#29616;&#20016;&#23500;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#20197;&#28385;&#36275;&#23545;&#22810;&#26679;&#19990;&#30028;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;&#30340;&#38656;&#27714;
&lt;/p&gt;
&lt;p&gt;
Q&amp;A Prompts: Discovering Rich Visual Clues through Mining Question-Answer Prompts for VQA requiring Diverse World Knowledge. (arXiv:2401.10712v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;Q&amp;A&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25366;&#25496;&#22270;&#20687;&#20013;&#30340;&#38382;&#39064;-&#22238;&#31572;&#23545;&#26469;&#21457;&#29616;&#20016;&#23500;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#20197;&#24110;&#21161;AI&#27169;&#22411;&#26356;&#22909;&#22320;&#29702;&#35299;&#22797;&#26434;&#35270;&#35273;&#38382;&#39064;&#65292;&#25552;&#39640;&#36328;&#27169;&#24577;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#30772;&#65292;&#22238;&#31572;&#38656;&#35201;&#39640;&#32423;&#25512;&#29702;&#33021;&#21147;&#21644;&#19990;&#30028;&#30693;&#35782;&#30340;&#22797;&#26434;&#35270;&#35273;&#38382;&#39064;&#27604;&#20197;&#24448;&#20219;&#20309;&#26102;&#20505;&#37117;&#26356;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20026;AI&#27169;&#22411;&#37197;&#22791;&#24378;&#22823;&#30340;&#36328;&#27169;&#24577;&#25512;&#29702;&#33021;&#21147;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#20154;&#31867;&#30340;&#35748;&#30693;&#26041;&#26696;&#23578;&#26410;&#31995;&#32479;&#22320;&#34987;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30456;&#20449;&#65292;&#22914;&#26524;&#25105;&#20204;&#33021;&#23613;&#21487;&#33021;&#25910;&#38598;&#32473;&#23450;&#22270;&#20687;&#20013;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#25105;&#20204;&#23558;&#33021;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;&#22270;&#20687;&#65292;&#26356;&#22909;&#22320;&#29702;&#35299;&#38382;&#39064;&#65292;&#26356;&#23481;&#26131;&#22238;&#24518;&#30456;&#20851;&#30693;&#35782;&#65292;&#24182;&#26368;&#32456;&#25512;&#29702;&#20986;&#31572;&#26696;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#22270;&#20687;&#20013;&#25366;&#25496;&#38382;&#39064;-&#22238;&#31572;&#23545;&#26469;&#21457;&#29616;&#36825;&#20123;&#20016;&#23500;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#24182;&#23558;&#23427;&#20204;&#20316;&#20026;&#25552;&#31034;&#21457;&#36865;&#21040;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;Q&amp;A&#25552;&#31034;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#35757;&#32451;&#38598;&#20013;&#30340;&#22270;&#20687;-&#31572;&#26696;&#23545;&#21644;&#30456;&#24212;&#30340;&#38382;&#39064;&#20316;&#20026;&#36755;&#20837;&#21644;&#36755;&#20986;&#26469;&#35757;&#32451;&#19968;&#20010;&#35270;&#35273;&#38382;&#39064;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the breakthrough of multi-modal large language models, answering complex visual questions that demand advanced reasoning abilities and world knowledge has become a much more important testbed for developing AI models than ever. However, equipping AI models with robust cross-modality reasoning ability remains challenging since the cognition scheme of humans has not been understood systematically. In this paper, we believe that if we can collect visual clues in the given image as much as possible, we will recognize the image more accurately, understand the question better, recall relevant knowledge more easily, and finally reason out the answer. We discover these rich visual clues by mining question-answer pairs in images and sending them into multi-modal large language models as prompts. We call the proposed method Q&amp;A Prompts. Specifically, we first use the image-answer pairs and the corresponding questions in the training set as inputs and outputs to train a visual question gener
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#22810;&#27169;&#22411;&#30340;&#24369;&#30417;&#30563;&#39640;&#26031;&#23545;&#27604;&#22522;&#30784;&#27169;&#22411;&#26469;&#22788;&#29702;&#35270;&#39057;&#38382;&#31572;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#21644;&#31572;&#26696;&#23545;&#20316;&#20026;&#20107;&#20214;&#25551;&#36848;&#65292;&#25214;&#21040;&#22810;&#20010;&#20851;&#38190;&#24103;&#20316;&#20026;&#30446;&#26631;&#26102;&#21051;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#26102;&#21051;&#20316;&#20026;&#20266;&#26631;&#31614;&#26469;&#24378;&#21046;LMMs&#36827;&#34892;&#25512;&#29702;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;&#39640;&#26031;&#30340;&#23545;&#27604;&#22522;&#30784;&#27169;&#22359;&#65288;GCG&#65289;&#26469;&#23398;&#20064;&#26102;&#25928;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2401.10711</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#22810;&#27169;&#22411;&#30340;&#24369;&#30417;&#30563;&#39640;&#26031;&#23545;&#27604;&#22522;&#30784;&#27169;&#22411;&#26469;&#22788;&#29702;&#35270;&#39057;&#38382;&#31572;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Weakly Supervised Gaussian Contrastive Grounding with Large Multimodal Models for Video Question Answering. (arXiv:2401.10711v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#22810;&#27169;&#22411;&#30340;&#24369;&#30417;&#30563;&#39640;&#26031;&#23545;&#27604;&#22522;&#30784;&#27169;&#22411;&#26469;&#22788;&#29702;&#35270;&#39057;&#38382;&#31572;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#21644;&#31572;&#26696;&#23545;&#20316;&#20026;&#20107;&#20214;&#25551;&#36848;&#65292;&#25214;&#21040;&#22810;&#20010;&#20851;&#38190;&#24103;&#20316;&#20026;&#30446;&#26631;&#26102;&#21051;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#26102;&#21051;&#20316;&#20026;&#20266;&#26631;&#31614;&#26469;&#24378;&#21046;LMMs&#36827;&#34892;&#25512;&#29702;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;&#39640;&#26031;&#30340;&#23545;&#27604;&#22522;&#30784;&#27169;&#22359;&#65288;GCG&#65289;&#26469;&#23398;&#20064;&#26102;&#25928;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#38382;&#31572;&#65288;VideoQA&#65289;&#26088;&#22312;&#22522;&#20110;&#35266;&#23519;&#21040;&#30340;&#35270;&#39057;&#20449;&#24687;&#22238;&#31572;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#12290;&#23613;&#31649;&#22823;&#22411;&#22810;&#27169;&#22411;&#65288;LMMs&#65289;&#22312;&#22270;&#20687;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#36817;&#26399;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#22312;&#22788;&#29702;&#35270;&#39057;&#38382;&#31572;&#26041;&#38754;&#36824;&#19981;&#36275;&#22815;&#65292;&#20165;&#20165;&#26159;&#23558;&#22343;&#21248;&#37319;&#26679;&#30340;&#24103;&#20316;&#20026;&#35270;&#35273;&#36755;&#20837;&#65292;&#24573;&#30053;&#20102;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#35270;&#35273;&#32447;&#32034;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#35270;&#39057;&#38382;&#31572;&#25968;&#25454;&#38598;&#20013;&#27809;&#26377;&#38024;&#23545;&#38382;&#39064;&#20851;&#38190;&#26102;&#38388;&#25139;&#30340;&#20154;&#24037;&#27880;&#37322;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24369;&#30417;&#30563;&#26694;&#26550;&#65292;&#24378;&#21046;LMMs&#20351;&#29992;&#38382;&#39064;&#20851;&#38190;&#26102;&#21051;&#20316;&#20026;&#35270;&#35273;&#36755;&#20837;&#25512;&#29702;&#20986;&#31572;&#26696;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#21644;&#31572;&#26696;&#23545;&#21512;&#24182;&#20026;&#20107;&#20214;&#25551;&#36848;&#65292;&#20197;&#25214;&#21040;&#22810;&#20010;&#20851;&#38190;&#24103;&#20316;&#20026;&#30446;&#26631;&#26102;&#21051;&#65292;&#36825;&#20123;&#26102;&#21051;&#23558;&#20316;&#20026;&#20266;&#26631;&#31614;&#12290;&#36890;&#36807;&#23558;&#36825;&#20123;&#20266;&#26631;&#31614;&#20316;&#20026;&#39069;&#22806;&#30340;&#24369;&#30417;&#30563;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;&#39640;&#26031;&#30340;&#23545;&#27604;&#22522;&#30784;&#27169;&#22359;&#65288;GCG&#65289;&#12290;GCG&#23398;&#20064;&#22810;&#20010;&#39640;&#26031;&#20989;&#25968;&#26469;&#25551;&#36848;&#26102;&#25928;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video Question Answering (VideoQA) aims to answer natural language questions based on the information observed in videos. Despite the recent success of Large Multimodal Models (LMMs) in image-language understanding and reasoning, they deal with VideoQA insufficiently by simply taking uniformly sampled frames as visual inputs, which ignores question-relevant visual clues. Moreover, there are no human annotations for question-critical timestamps in existing VideoQA datasets. In light of this, we propose a novel weakly supervised framework to enforce the LMMs to reason out the answers with question-critical moments as visual inputs. Specifically, we fuse the question and answer pairs as event descriptions to find multiple keyframes as target moments, which will be pseudo-labels. With these pseudo-labels as additionally weak supervision, we devise a lightweight Gaussian-based Contrastive Grounding (GCG) module. GCG learns multiple Gaussian functions to characterize the temporal structure o
&lt;/p&gt;</description></item><item><title>LangBridge&#26159;&#19968;&#31181;&#26080;&#38656;&#22810;&#35821;&#35328;&#30417;&#30563;&#30340;&#22810;&#35821;&#35328;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#36830;&#25509;&#20004;&#20010;&#27169;&#22411;&#26469;&#36866;&#24212;&#22810;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#65292;&#23613;&#31649;&#21482;&#20351;&#29992;&#33521;&#25991;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#23427;&#26174;&#33879;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.10695</link><description>&lt;p&gt;
LangBridge: &#26080;&#22810;&#35821;&#35328;&#30417;&#30563;&#30340;&#22810;&#35821;&#35328;&#25512;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LangBridge: Multilingual Reasoning Without Multilingual Supervision. (arXiv:2401.10695v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10695
&lt;/p&gt;
&lt;p&gt;
LangBridge&#26159;&#19968;&#31181;&#26080;&#38656;&#22810;&#35821;&#35328;&#30417;&#30563;&#30340;&#22810;&#35821;&#35328;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#36830;&#25509;&#20004;&#20010;&#27169;&#22411;&#26469;&#36866;&#24212;&#22810;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#65292;&#23613;&#31649;&#21482;&#20351;&#29992;&#33521;&#25991;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#23427;&#26174;&#33879;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;LangBridge&#65292;&#36825;&#26159;&#19968;&#31181;&#38646;&#30417;&#30563;&#26041;&#24335;&#65292;&#29992;&#20110;&#36866;&#24212;&#22810;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#65292;&#26080;&#38656;&#22810;&#35821;&#35328;&#30417;&#30563;&#12290;LangBridge&#36890;&#36807;&#36830;&#25509;&#20004;&#20010;&#27169;&#22411;&#26469;&#36816;&#20316;&#65292;&#27599;&#20010;&#27169;&#22411;&#19987;&#38376;&#22788;&#29702;&#19981;&#21516;&#26041;&#38754;&#65306;(1)&#19968;&#20010;&#19987;&#38376;&#22788;&#29702;&#22810;&#31181;&#35821;&#35328;&#30340;&#27169;&#22411;&#65288;&#20363;&#22914;mT5&#32534;&#30721;&#22120;&#65289;&#65292;&#21644;(2)&#19968;&#20010;&#19987;&#38376;&#22788;&#29702;&#25512;&#29702;&#30340;&#27169;&#22411;&#65288;&#20363;&#22914;Orca 2&#65289;&#12290;LangBridge&#36890;&#36807;&#22312;&#20004;&#20010;&#27169;&#22411;&#20043;&#38388;&#24341;&#20837;&#26368;&#23569;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#26469;&#36830;&#25509;&#23427;&#20204;&#12290;&#23613;&#31649;&#21482;&#21033;&#29992;&#33521;&#25991;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;LangBridge&#26174;&#33879;&#25552;&#21319;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#25512;&#29702;&#12289;&#32534;&#30721;&#21644;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;LangBridge&#30340;&#26377;&#25928;&#24615;&#28304;&#20110;&#22810;&#35821;&#35328;&#34920;&#31034;&#30340;&#19981;&#21463;&#35821;&#35328;&#38480;&#21046;&#30340;&#29305;&#24615;&#12290;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce LangBridge, a zero-shot approach to adapt language models for multilingual reasoning tasks without multilingual supervision. LangBridge operates by bridging two models, each specialized in different aspects: (1) one specialized in understanding multiple languages (e.g., mT5 encoder) and (2) one specialized in reasoning (e.g., Orca 2). LangBridge connects the two models by introducing minimal trainable parameters between them. Despite utilizing only English data for training, LangBridge considerably enhances the performance of language models on low-resource languages across mathematical reasoning, coding, and logical reasoning. Our analysis suggests that the efficacy of LangBridge stems from the language-agnostic characteristics of multilingual representations. We publicly release our code and models.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#21152;&#36895;&#38750;&#33521;&#35821;&#35821;&#35328;&#30340;&#25991;&#26412;&#29983;&#25104;&#12290;&#36890;&#36807;&#39044;&#27979;&#26356;&#22823;&#30340;&#35821;&#35328;&#21333;&#20803;&#24182;&#38024;&#23545;&#30446;&#26631;&#35821;&#35328;&#36827;&#34892;&#35843;&#25972;&#65292;&#35813;&#26694;&#26550;&#38477;&#20302;&#20102;&#35299;&#30721;&#27493;&#39588;&#30340;&#25968;&#37327;&#65292;&#24182;&#23558;&#29983;&#25104;&#36895;&#24230;&#25552;&#39640;&#20102;1.9&#20493;&#12290;</title><link>http://arxiv.org/abs/2401.10660</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#21333;&#35821;&#25991;&#26412;&#29983;&#25104;&#30340;&#21152;&#36895;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Simple Framework to Accelerate Multilingual Language Model for Monolingual Text Generation. (arXiv:2401.10660v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10660
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#21152;&#36895;&#38750;&#33521;&#35821;&#35821;&#35328;&#30340;&#25991;&#26412;&#29983;&#25104;&#12290;&#36890;&#36807;&#39044;&#27979;&#26356;&#22823;&#30340;&#35821;&#35328;&#21333;&#20803;&#24182;&#38024;&#23545;&#30446;&#26631;&#35821;&#35328;&#36827;&#34892;&#35843;&#25972;&#65292;&#35813;&#26694;&#26550;&#38477;&#20302;&#20102;&#35299;&#30721;&#27493;&#39588;&#30340;&#25968;&#37327;&#65292;&#24182;&#23558;&#29983;&#25104;&#36895;&#24230;&#25552;&#39640;&#20102;1.9&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#19981;&#20165;&#22312;&#33521;&#35821;&#32780;&#19988;&#22312;&#38750;&#33521;&#35821;&#35821;&#35328;&#20013;&#37117;&#20419;&#36827;&#20102;&#22797;&#26434;&#30340;&#35821;&#35328;&#20219;&#21153;&#30340;&#25191;&#34892;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#35821;&#35328;&#27169;&#22411;&#30340;&#26631;&#35760;&#22120;&#65288;&#22914;Llama&#65289;&#22312;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#65292;&#20542;&#21521;&#20110;&#22312;&#38750;&#33521;&#35821;&#35821;&#35328;&#20013;&#36807;&#20998;&#20998;&#21106;&#26631;&#35760;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#38750;&#32599;&#39532;&#23383;&#27597;&#35821;&#35328;&#20013;&#23588;&#20026;&#26126;&#26174;&#65292;&#36825;&#20123;&#35821;&#35328;&#36890;&#24120;&#22312;&#23383;&#31526;&#25110;Unicode&#32423;&#21035;&#19978;&#34987;&#21010;&#20998;&#65292;&#23548;&#33268;&#25991;&#26412;&#29983;&#25104;&#36895;&#24230;&#36739;&#24930;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#21152;&#36895;&#36825;&#20123;&#35821;&#35328;&#30340;&#25991;&#26412;&#29983;&#25104;&#12290;&#35813;&#26694;&#26550;&#39044;&#27979;&#27604;&#20256;&#32479;&#30340;&#22810;&#35821;&#35328;&#26631;&#35760;&#22120;&#26356;&#22823;&#30340;&#35821;&#35328;&#21333;&#20803;&#65292;&#24182;&#19988;&#19987;&#38376;&#38024;&#23545;&#30446;&#26631;&#35821;&#35328;&#36827;&#34892;&#20102;&#35843;&#25972;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#35299;&#30721;&#25152;&#38656;&#30340;&#27493;&#39588;&#25968;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26631;&#20934;&#35299;&#30721;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#23558;&#29983;&#25104;&#36895;&#24230;&#25552;&#39640;&#20102;1.9&#20493;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39044;&#20808;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large language models have facilitated the execution of complex language tasks, not only in English but also in non-English languages. However, the tokenizers of most language models, such as Llama, trained on English-centric corpora, tend to excessively fragment tokens in non-English languages. This issue is especially pronounced in non-roman alphabetic languages, which are often divided at a character or even Unicode level, leading to slower text generation. To address this, our study introduces a novel framework designed to expedite text generation in these languages. This framework predicts larger linguistic units than those of conventional multilingual tokenizers and is specifically tailored to the target language, thereby reducing the number of decoding steps required. Our empirical results demonstrate that the proposed framework increases the generation speed by a factor of 1.9 compared to standard decoding while maintaining the performance of a pre-traine
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#22522;&#20110;Transformer&#26694;&#26550;&#21644;"&#20851;&#27880;&#34701;&#21512;"&#23618;&#65292;&#37319;&#29992;&#22810;&#27169;&#24577;&#26041;&#27861;&#35782;&#21035;&#20167;&#24680;&#35328;&#35770;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;&#30340;&#25991;&#26412;&#20998;&#26512;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.10653</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22810;&#27169;&#24577;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26041;&#27861;&#65306;&#20851;&#27880;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Attentive Fusion: A Transformer-based Approach to Multimodal Hate Speech Detection. (arXiv:2401.10653v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10653
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#22522;&#20110;Transformer&#26694;&#26550;&#21644;"&#20851;&#27880;&#34701;&#21512;"&#23618;&#65292;&#37319;&#29992;&#22810;&#27169;&#24577;&#26041;&#27861;&#35782;&#21035;&#20167;&#24680;&#35328;&#35770;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;&#30340;&#25991;&#26412;&#20998;&#26512;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#20351;&#29992;&#37327;&#30340;&#28608;&#22686;&#21644;&#25351;&#25968;&#22686;&#38271;&#65292;&#23457;&#26597;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#20013;&#26159;&#21542;&#23384;&#22312;&#20219;&#20309;&#20167;&#24680;&#20869;&#23481;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#12290;&#30740;&#31350;&#32773;&#20204;&#33258;&#36807;&#21435;&#21313;&#24180;&#20197;&#26469;&#19968;&#30452;&#33268;&#21147;&#20110;&#21306;&#20998;&#23459;&#20256;&#20167;&#24680;&#21644;&#38750;&#23459;&#20256;&#20167;&#24680;&#30340;&#20869;&#23481;&#12290;&#20256;&#32479;&#19978;&#65292;&#20027;&#35201;&#20851;&#27880;&#30340;&#26159;&#23545;&#25991;&#26412;&#20869;&#23481;&#30340;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#20063;&#24320;&#22987;&#28041;&#21450;&#23545;&#22522;&#20110;&#38899;&#39057;&#30340;&#20869;&#23481;&#30340;&#35782;&#21035;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#34920;&#26126;&#20165;&#20165;&#20381;&#36182;&#38899;&#39057;&#25110;&#22522;&#20110;&#25991;&#26412;&#30340;&#20869;&#23481;&#21487;&#33021;&#26159;&#26080;&#25928;&#30340;&#65292;&#22240;&#20026;&#36817;&#26399;&#30340;&#28608;&#22686;&#34920;&#26126;&#20010;&#20307;&#22312;&#35328;&#36766;&#21644;&#20889;&#20316;&#20013;&#32463;&#24120;&#20351;&#29992;&#35773;&#21050;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#38899;&#39057;&#21644;&#25991;&#26412;&#34920;&#31034;&#26469;&#21028;&#26029;&#19968;&#27573;&#35328;&#36766;&#26159;&#21542;&#23459;&#20256;&#20102;&#20167;&#24680;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#25105;&#20204;&#33258;&#24049;&#30340;&#23618;&#31216;&#20026;"&#20851;&#27880;&#34701;&#21512;"&#12290;
&lt;/p&gt;
&lt;p&gt;
With the recent surge and exponential growth of social media usage, scrutinizing social media content for the presence of any hateful content is of utmost importance. Researchers have been diligently working since the past decade on distinguishing between content that promotes hatred and content that does not. Traditionally, the main focus has been on analyzing textual content. However, recent research attempts have also commenced into the identification of audio-based content. Nevertheless, studies have shown that relying solely on audio or text-based content may be ineffective, as recent upsurge indicates that individuals often employ sarcasm in their speech and writing. To overcome these challenges, we present an approach to identify whether a speech promotes hate or not utilizing both audio and textual representations. Our methodology is based on the Transformer framework that incorporates both audio and text sampling, accompanied by our very own layer called "Attentive Fusion". Th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#32534;&#36753;&#35821;&#35328;&#27169;&#22411;&#30340;&#22797;&#26434;&#21518;&#26524;&#65292;&#21457;&#29616;&#22312;&#22686;&#24378;&#27169;&#22411;&#20934;&#30830;&#24615;&#19982;&#20445;&#25345;&#36947;&#24503;&#23436;&#25972;&#24615;&#20043;&#38388;&#23384;&#22312;&#24726;&#35770;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#27880;&#20837;&#20934;&#30830;&#20449;&#24687;&#23545;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#24456;&#37325;&#35201;&#65292;&#20294;&#23427;&#21487;&#33021;&#30772;&#22351;&#27169;&#22411;&#30340;&#22522;&#26412;&#26694;&#26550;&#65292;&#23548;&#33268;&#19981;&#21487;&#39044;&#27979;&#21644;&#28508;&#22312;&#30340;&#19981;&#23433;&#20840;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2401.10647</link><description>&lt;p&gt;
&#25773;&#39118;&#25769;&#36215;&#39118;&#26292;&#65306;&#32534;&#36753;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Sowing the Wind, Reaping the Whirlwind: The Impact of Editing Language Models. (arXiv:2401.10647v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#32534;&#36753;&#35821;&#35328;&#27169;&#22411;&#30340;&#22797;&#26434;&#21518;&#26524;&#65292;&#21457;&#29616;&#22312;&#22686;&#24378;&#27169;&#22411;&#20934;&#30830;&#24615;&#19982;&#20445;&#25345;&#36947;&#24503;&#23436;&#25972;&#24615;&#20043;&#38388;&#23384;&#22312;&#24726;&#35770;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#27880;&#20837;&#20934;&#30830;&#20449;&#24687;&#23545;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#24456;&#37325;&#35201;&#65292;&#20294;&#23427;&#21487;&#33021;&#30772;&#22351;&#27169;&#22411;&#30340;&#22522;&#26412;&#26694;&#26550;&#65292;&#23548;&#33268;&#19981;&#21487;&#39044;&#27979;&#21644;&#28508;&#22312;&#30340;&#19981;&#23433;&#20840;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#65292;&#32418;&#38431;&#27979;&#35797;&#25110;&#36234;&#29425;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#27010;&#24565;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#36890;&#36807;&#23545;&#27169;&#22411;&#36827;&#34892;&#32534;&#36753;&#65292;&#25581;&#31034;&#20102;&#36825;&#31181;&#20462;&#25913;&#30340;&#22797;&#26434;&#21518;&#26524;&#65292;&#21457;&#29616;&#20102;&#22686;&#24378;&#27169;&#22411;&#20934;&#30830;&#24615;&#19982;&#20445;&#25345;&#20854;&#36947;&#24503;&#23436;&#25972;&#24615;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#28145;&#20837;&#20998;&#26512;&#25581;&#31034;&#20102;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#24726;&#35770;&#65306;&#34429;&#28982;&#27880;&#20837;&#20934;&#30830;&#20449;&#24687;&#23545;&#20110;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#23427;&#21364;&#21487;&#33021;&#30772;&#22351;&#27169;&#22411;&#30340;&#22522;&#26412;&#26694;&#26550;&#65292;&#23548;&#33268;&#19981;&#21487;&#39044;&#27979;&#21644;&#28508;&#22312;&#30340;&#19981;&#23433;&#20840;&#34892;&#20026;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;NicheHazardQA&#65292;&#29992;&#20110;&#30740;&#31350;&#27169;&#22411;&#22312;&#30456;&#21516;&#21644;&#36328;&#39046;&#22495;&#20013;&#30340;&#19981;&#23433;&#20840;&#34892;&#20026;&#12290;&#36825;&#19968;&#26041;&#38754;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#32534;&#36753;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#30340;&#23433;&#20840;&#24230;&#37327;&#21644;&#20445;&#25252;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly advancing field of artificial intelligence, the concept of Red-Teaming or Jailbreaking large language models (LLMs) has emerged as a crucial area of study. This approach is especially significant in terms of assessing and enhancing the safety and robustness of these models. This paper investigates the intricate consequences of such modifications through model editing, uncovering a complex relationship between enhancing model accuracy and preserving its ethical integrity. Our in-depth analysis reveals a striking paradox: while injecting accurate information is crucial for model reliability, it can paradoxically destabilize the model's foundational framework, resulting in unpredictable and potentially unsafe behaviors. Additionally, we propose a benchmark dataset NicheHazardQA to investigate this unsafe behavior both within the same and cross topical domain. This aspect of our research sheds light on how the edits, impact the model's safety metrics and guardrails. Our find
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24320;&#28304;&#30340;&#35821;&#35328;&#36866;&#24212;&#26041;&#27861;PHOENIX&#65292;&#29992;&#20110;&#30452;&#25509;&#20248;&#21270;&#20559;&#22909;&#12290;&#30740;&#31350;&#26500;&#24314;&#22312;&#26368;&#26032;&#30340;&#25913;&#36827;&#22522;&#30784;&#19978;&#65292;&#24182;&#23558;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#26041;&#27861;&#24212;&#29992;&#20110;&#24503;&#35821;&#12290;</title><link>http://arxiv.org/abs/2401.10580</link><description>&lt;p&gt;
PHOENIX: &#24320;&#28304;&#35821;&#35328;&#36866;&#24212;&#65292;&#29992;&#20110;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
PHOENIX: Open-Source Language Adaption for Direct Preference Optimization. (arXiv:2401.10580v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10580
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24320;&#28304;&#30340;&#35821;&#35328;&#36866;&#24212;&#26041;&#27861;PHOENIX&#65292;&#29992;&#20110;&#30452;&#25509;&#20248;&#21270;&#20559;&#22909;&#12290;&#30740;&#31350;&#26500;&#24314;&#22312;&#26368;&#26032;&#30340;&#25913;&#36827;&#22522;&#30784;&#19978;&#65292;&#24182;&#23558;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#26041;&#27861;&#24212;&#29992;&#20110;&#24503;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#20960;&#24180;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#65292;&#24182;&#22312;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#25104;&#23601;&#65292;&#20294;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#20013;&#36824;&#26377;&#35768;&#22810;&#38382;&#39064;&#23578;&#26410;&#35299;&#31572;&#12290;&#38500;&#20102;&#22312;&#25512;&#29702;&#20013;&#26368;&#20339;&#20351;&#29992;&#27169;&#22411;&#21644;&#23558;&#32467;&#26524;&#19982;&#26399;&#26395;&#35268;&#33539;&#23545;&#40784;&#20043;&#22806;&#65292;&#23558;&#27169;&#22411;&#36716;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#20173;&#28982;&#26159;&#19968;&#20010;&#23578;&#26410;&#24320;&#21457;&#23436;&#21892;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#26368;&#36817;&#21457;&#24067;&#30340;Llama-2&#21644;Zephyr&#31561;&#27169;&#22411;&#25552;&#20379;&#20102;&#20851;&#20110;&#26550;&#26500;&#25913;&#36827;&#21644;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#30340;&#26032;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#22914;&#20309;&#23558;&#36825;&#20123;&#25216;&#26415;&#36866;&#37197;&#21040;&#20854;&#20182;&#35821;&#35328;&#30340;&#27934;&#35265;&#20173;&#28982;&#24456;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#26368;&#26032;&#30340;&#25913;&#36827;&#65292;&#23558;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;(DPO)&#26041;&#27861;&#24212;&#29992;&#21040;&#24503;&#35821;&#20013;&#12290;&#35813;&#27169;&#22411;&#21487;&#22312;https://huggingface.co/DRXD1000/Phoenix&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have gained immense importance in recent years and have demonstrated outstanding results in solving various tasks. However, despite these achievements, many questions remain unanswered in the context of large language models. Besides the optimal use of the models for inference and the alignment of the results to the desired specifications, the transfer of models to other languages is still an underdeveloped area of research. The recent publication of models such as Llama-2 and Zephyr has provided new insights into architectural improvements and the use of human feedback. However, insights into adapting these techniques to other languages remain scarce. In this paper, we build on latest improvements and apply the Direct Preference Optimization(DPO) approach to the German language. The model is available at https://huggingface.co/DRXD1000/Phoenix.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#33258;&#25105;&#35760;&#24518;&#33258;&#25105;&#35757;&#32451;&#65288;STSM&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#33258;&#25105;&#35760;&#24518;&#20316;&#20026;&#35757;&#32451;&#23376;&#38598;&#65292;&#24182;&#20351;&#29992;&#39044;&#23450;&#20041;&#26465;&#20214;&#39564;&#35777;&#20854;&#36136;&#37327;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20197;&#36739;&#39640;&#30340;&#24615;&#33021;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2401.10567</link><description>&lt;p&gt;
&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#33258;&#25105;&#35760;&#24518;&#33258;&#25105;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Self-training from Self-memory in Data-to-text Generation. (arXiv:2401.10567v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10567
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#33258;&#25105;&#35760;&#24518;&#33258;&#25105;&#35757;&#32451;&#65288;STSM&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#33258;&#25105;&#35760;&#24518;&#20316;&#20026;&#35757;&#32451;&#23376;&#38598;&#65292;&#24182;&#20351;&#29992;&#39044;&#23450;&#20041;&#26465;&#20214;&#39564;&#35777;&#20854;&#36136;&#37327;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20197;&#36739;&#39640;&#30340;&#24615;&#33021;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#25105;&#35760;&#24518;&#33258;&#25105;&#35757;&#32451;&#27169;&#22411;&#65288;STSM&#65289;&#22312;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#24212;&#29992;&#65292;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#22312;&#23376;&#38598;&#19978;&#36827;&#34892;&#33258;&#25105;&#35757;&#32451;&#65292;&#20854;&#20013;&#21253;&#25324;&#20174;&#35757;&#32451;&#27169;&#22411;&#21644;/&#25110;&#26032;&#25968;&#25454;&#20013;&#30452;&#25509;&#25512;&#26029;&#20986;&#30340;&#33258;&#25105;&#35760;&#24518;&#12290;&#36890;&#36807;&#20004;&#20010;&#39044;&#23450;&#20041;&#26465;&#20214;&#26469;&#39564;&#35777;&#33258;&#25105;&#35760;&#24518;&#30340;&#36136;&#37327;&#65306;&#65288;1&#65289;D2T&#27169;&#22411;&#30340;&#36755;&#20986;&#20013;&#21253;&#21547;&#25152;&#26377;&#28304;&#25968;&#20540;&#65292;&#65288;2&#65289;T2D&#27169;&#22411;&#30340;&#36755;&#20986;&#33021;&#22815;&#36716;&#25442;&#22238;&#28304;&#25968;&#25454;&#12290;&#22914;&#26524;D2T&#30340;&#36755;&#20986;&#21253;&#21547;&#25152;&#26377;&#28304;&#25968;&#20540;&#65292;&#25105;&#20204;&#20351;&#29992;&#36138;&#23146;&#31639;&#27861;&#29983;&#25104;&#36739;&#30701;&#30340;&#36755;&#20986;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;T2D&#27169;&#22411;&#26469;&#30830;&#35748;&#36825;&#20123;&#36755;&#20986;&#33021;&#22815;&#25429;&#25417;&#36755;&#20837;&#20851;&#31995;&#65292;&#36890;&#36807;&#28436;&#31034;&#23427;&#20204;&#23558;&#25991;&#26412;&#36716;&#25442;&#22238;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#22312;30%&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;D2T&#27169;&#22411;&#21487;&#20197;&#22312;&#30456;&#21516;&#30340;&#35774;&#32622;&#20013;&#20197;&#19982;&#23436;&#20840;&#35757;&#32451;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#65288;E2E NLG&#21644;DART&#65289;&#19978;&#23545;&#25105;&#20204;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel training model, self-training from self-memory (STSM) in data-to-text generation (DTG), allowing the model to self-train on subsets, including self-memory as outputs inferred directly from the trained models and/or the new data. The quality of self-memory is validated by two models, data-to-text (D2T) and text-to-data (T2D), by two pre-defined conditions: (1) the appearance of all source values in the outputs of the D2T model and (2) the ability to convert back to source data in the outputs in the T2D model. We utilize a greedy algorithm to generate shorter D2T outputs if they contain all source values. Subsequently, we use the T2D model to confirm that these outputs can capture input relationships by demonstrating their capacity to convert text back into data. With 30% of the dataset, we can train the D2T model with a competitive performance compared to full training in the same setup. We experiment with our model on two datasets, E2E NLG and DART. STSM o
&lt;/p&gt;</description></item><item><title>OrchMoE&#36890;&#36807;&#21033;&#29992;&#27169;&#22359;&#21270;&#25216;&#33021;&#26550;&#26500;&#21644;&#33258;&#21160;&#20219;&#21153;&#35782;&#21035;&#65292;&#25552;&#21319;&#20102;&#21442;&#25968;&#25928;&#29575;&#24494;&#35843;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#37325;&#22823;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2401.10559</link><description>&lt;p&gt;
OrchMoE&#65306;&#20855;&#26377;&#20219;&#21153;-&#25216;&#33021;&#21327;&#21516;&#25928;&#24212;&#30340;&#39640;&#25928;&#22810;&#36866;&#37197;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
OrchMoE: Efficient Multi-Adapter Learning with Task-Skill Synergy. (arXiv:2401.10559v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10559
&lt;/p&gt;
&lt;p&gt;
OrchMoE&#36890;&#36807;&#21033;&#29992;&#27169;&#22359;&#21270;&#25216;&#33021;&#26550;&#26500;&#21644;&#33258;&#21160;&#20219;&#21153;&#35782;&#21035;&#65292;&#25552;&#21319;&#20102;&#21442;&#25968;&#25928;&#29575;&#24494;&#35843;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#37325;&#22823;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#21019;&#26032;&#30340;&#22810;&#36866;&#37197;&#22120;&#26041;&#27861;OrchMoE&#25512;&#36827;&#20102;&#21442;&#25968;&#25928;&#29575;&#24494;&#35843;&#65288;PEFT&#65289;&#39046;&#22495;&#65292;&#21033;&#29992;&#27169;&#22359;&#21270;&#25216;&#33021;&#26550;&#26500;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#30340;&#21069;&#21521;&#20256;&#36882;&#12290;&#19982;&#20381;&#36182;&#26174;&#24335;&#20219;&#21153;&#35782;&#21035;&#36755;&#20837;&#30340;&#20808;&#21069;&#27169;&#22411;&#19981;&#21516;&#65292;OrchMoE&#33258;&#21160;&#35782;&#21035;&#20219;&#21153;&#31867;&#21035;&#65292;&#31616;&#21270;&#23398;&#20064;&#36807;&#31243;&#12290;&#36825;&#26159;&#36890;&#36807;&#19968;&#20010;&#25972;&#21512;&#26426;&#21046;&#23454;&#29616;&#30340;&#65292;&#21253;&#25324;&#33258;&#21160;&#20219;&#21153;&#20998;&#31867;&#27169;&#22359;&#21644;&#20219;&#21153;-&#25216;&#33021;&#20998;&#37197;&#27169;&#22359;&#65292;&#20849;&#21516;&#25512;&#26029;&#20219;&#21153;&#29305;&#23450;&#30340;&#20998;&#31867;&#24182;&#35843;&#25972;&#25216;&#33021;&#20998;&#37197;&#30697;&#38453;&#12290;&#25105;&#20204;&#22312;&#8220;&#36229;&#33258;&#28982;&#25351;&#20196;&#8221;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;1,600&#20010;&#22810;&#26679;&#30340;&#25351;&#20196;&#20219;&#21153;&#65292;&#32467;&#26524;&#34920;&#26126;OrchMoE&#22312;&#24615;&#33021;&#21644;&#26679;&#26412;&#21033;&#29992;&#25928;&#29575;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#21487;&#27604;&#30340;&#22810;&#36866;&#37197;&#22120;&#22522;&#32447;&#65292;&#24182;&#19988;&#22312;&#30456;&#21516;&#21442;&#25968;&#38480;&#21046;&#19979;&#36816;&#34892;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#65292;OrchMoE&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
We advance the field of Parameter-Efficient Fine-Tuning (PEFT) with our novel multi-adapter method, OrchMoE, which capitalizes on modular skill architecture for enhanced forward transfer in neural networks. Unlike prior models that depend on explicit task identification inputs, OrchMoE automatically discerns task categories, streamlining the learning process. This is achieved through an integrated mechanism comprising an Automatic Task Classification module and a Task-Skill Allocation module, which collectively deduce task-specific classifications and tailor skill allocation matrices. Our extensive evaluations on the 'Super Natural Instructions' dataset, featuring 1,600 diverse instructional tasks, indicate that OrchMoE substantially outperforms comparable multi-adapter baselines in terms of both performance and sample utilization efficiency, all while operating within the same parameter constraints. These findings suggest that OrchMoE offers a significant leap forward in multi-task le
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21457;&#23637;&#20102;&#19968;&#31181;&#22810;&#35821;&#35328;&#22768;&#23398;&#35789;&#23884;&#20837;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32570;&#20047;&#26631;&#27880;&#25968;&#25454;&#30340;&#38646;&#36164;&#28304;&#35821;&#35328;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#21644;&#22810;&#35821;&#35328;&#36716;&#31227;&#65292;&#35813;&#26041;&#27861;&#22312;&#38646;&#36164;&#28304;&#35821;&#35328;&#19978;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#36824;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#21644;&#35821;&#20041;&#26597;&#35810;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.10543</link><description>&lt;p&gt;
&#38646;&#36164;&#28304;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#22768;&#23398;&#35789;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Multilingual acoustic word embeddings for zero-resource languages. (arXiv:2401.10543v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10543
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21457;&#23637;&#20102;&#19968;&#31181;&#22810;&#35821;&#35328;&#22768;&#23398;&#35789;&#23884;&#20837;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32570;&#20047;&#26631;&#27880;&#25968;&#25454;&#30340;&#38646;&#36164;&#28304;&#35821;&#35328;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#21644;&#22810;&#35821;&#35328;&#36716;&#31227;&#65292;&#35813;&#26041;&#27861;&#22312;&#38646;&#36164;&#28304;&#35821;&#35328;&#19978;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#36824;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#21644;&#35821;&#20041;&#26597;&#35810;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35299;&#20915;&#20102;&#22312;&#32570;&#20047;&#26631;&#27880;&#25968;&#25454;&#30340;&#38646;&#36164;&#28304;&#35821;&#35328;&#20013;&#24320;&#21457;&#35821;&#38899;&#24212;&#29992;&#30340;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#20351;&#29992;&#22768;&#23398;&#35789;&#23884;&#20837;&#65288;AWE&#65289;-&#23558;&#21487;&#21464;&#26102;&#38271;&#30340;&#35821;&#38899;&#29255;&#27573;&#36716;&#25442;&#20026;&#22266;&#23450;&#32500;&#24230;&#30340;&#34920;&#31034;-&#24182;&#20351;&#29992;&#22810;&#35821;&#35328;&#36716;&#31227;&#65292;&#22312;&#22810;&#20010;&#36164;&#28304;&#20016;&#23500;&#30340;&#35821;&#35328;&#30340;&#26631;&#27880;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#38646;&#36164;&#28304;&#35821;&#35328;&#19978;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;AWE&#27169;&#22411;&#12290;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#36164;&#28304;&#20016;&#23500;&#35821;&#35328;&#30340;&#36873;&#25321;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;AWE&#24212;&#29992;&#20110;&#26031;&#29926;&#24076;&#37324;&#35821;&#24191;&#25773;&#20013;&#30340;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#20851;&#38190;&#35789;&#35782;&#21035;&#31995;&#32479;&#20013;&#65292;&#23637;&#31034;&#20102;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#26032;&#39062;&#30340;&#35821;&#20041;AWE&#27169;&#22411;&#25913;&#36827;&#20102;&#35821;&#20041;&#26597;&#35810;&#31034;&#20363;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research addresses the challenge of developing speech applications for zero-resource languages that lack labelled data. It specifically uses acoustic word embedding (AWE) -- fixed-dimensional representations of variable-duration speech segments -- employing multilingual transfer, where labelled data from several well-resourced languages are used for pertaining. The study introduces a new neural network that outperforms existing AWE models on zero-resource languages. It explores the impact of the choice of well-resourced languages. AWEs are applied to a keyword-spotting system for hate speech detection in Swahili radio broadcasts, demonstrating robustness in real-world scenarios. Additionally, novel semantic AWE models improve semantic query-by-example search.
&lt;/p&gt;</description></item><item><title>Speech Swin-Transformer&#26159;&#19968;&#31181;&#23618;&#27425;&#21270;&#30340;&#35821;&#38899;Transformer&#65292;&#21033;&#29992;&#24179;&#31227;&#31383;&#21475;&#32858;&#21512;&#22810;&#23610;&#24230;&#24773;&#24863;&#29305;&#24449;&#65292;&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20998;&#21106;&#35821;&#38899;&#39057;&#35889;&#22270;&#20026;&#27573;&#32423;&#34917;&#19969;&#65292;&#21033;&#29992;&#26412;&#22320;&#31383;&#21475;Transformer&#21644;&#24179;&#31227;&#31383;&#21475;Transformer&#25506;&#32034;&#34917;&#19969;&#20869;&#37096;&#30340;&#24773;&#24863;&#20449;&#24687;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;</title><link>http://arxiv.org/abs/2401.10536</link><description>&lt;p&gt;
Speech Swin-Transformer: &#25506;&#32034;&#20855;&#26377;&#24179;&#31227;&#31383;&#21475;&#30340;&#23618;&#27425;&#36716;&#25442;&#22120;&#22312;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Speech Swin-Transformer: Exploring a Hierarchical Transformer with Shifted Windows for Speech Emotion Recognition. (arXiv:2401.10536v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10536
&lt;/p&gt;
&lt;p&gt;
Speech Swin-Transformer&#26159;&#19968;&#31181;&#23618;&#27425;&#21270;&#30340;&#35821;&#38899;Transformer&#65292;&#21033;&#29992;&#24179;&#31227;&#31383;&#21475;&#32858;&#21512;&#22810;&#23610;&#24230;&#24773;&#24863;&#29305;&#24449;&#65292;&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20998;&#21106;&#35821;&#38899;&#39057;&#35889;&#22270;&#20026;&#27573;&#32423;&#34917;&#19969;&#65292;&#21033;&#29992;&#26412;&#22320;&#31383;&#21475;Transformer&#21644;&#24179;&#31227;&#31383;&#21475;Transformer&#25506;&#32034;&#34917;&#19969;&#20869;&#37096;&#30340;&#24773;&#24863;&#20449;&#24687;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Swin-Transformer&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#21033;&#29992;&#20854;&#22522;&#20110;Transformer&#30340;&#23618;&#27425;&#21270;&#29305;&#24449;&#34920;&#31034;&#12290;&#22312;&#35821;&#38899;&#20449;&#21495;&#20013;&#65292;&#24773;&#24863;&#20449;&#24687;&#22312;&#19981;&#21516;&#23610;&#24230;&#30340;&#35821;&#38899;&#29305;&#24449;&#19978;&#20998;&#24067;&#65292;&#20363;&#22914;&#35789;&#35821;&#12289;&#30701;&#35821;&#21644;&#35805;&#35821;&#12290;&#22312;&#27492;&#21551;&#21457;&#19979;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24179;&#31227;&#31383;&#21475;&#23545;&#22810;&#23610;&#24230;&#24773;&#24863;&#29305;&#24449;&#36827;&#34892;&#32858;&#21512;&#30340;&#23618;&#27425;&#21270;&#35821;&#38899;Transformer&#65292;&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65288;SER&#65289;&#65292;&#31216;&#20026;Speech Swin-Transformer&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#35821;&#38899;&#39057;&#35889;&#22270;&#22312;&#26102;&#22495;&#20998;&#21106;&#20026;&#27573;&#32423;&#34917;&#19969;&#65292;&#30001;&#22810;&#20010;&#24103;&#34917;&#19969;&#32452;&#25104;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#19968;&#31995;&#21015;Swin&#22359;&#23545;&#36825;&#20123;&#27573;&#32423;&#34917;&#19969;&#36827;&#34892;&#32534;&#30721;&#65292;&#20854;&#20013;&#21033;&#29992;&#26412;&#22320;&#31383;&#21475;Transformer&#26469;&#25506;&#32034;&#27599;&#20010;&#27573;&#32423;&#34917;&#19969;&#20013;&#24103;&#34917;&#19969;&#20043;&#38388;&#30340;&#26412;&#22320;&#24103;&#38388;&#24773;&#24863;&#20449;&#24687;&#12290;&#20043;&#21518;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#24179;&#31227;&#31383;&#21475;Transformer&#65292;&#20197;&#34917;&#20607;&#38752;&#36817;&#27573;&#32423;&#34917;&#19969;&#36793;&#30028;&#30340;&#34917;&#19969;&#30456;&#20851;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;Speech Swin-Transformer&#22312;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Swin-Transformer has demonstrated remarkable success in computer vision by leveraging its hierarchical feature representation based on Transformer. In speech signals, emotional information is distributed across different scales of speech features, e.\,g., word, phrase, and utterance. Drawing above inspiration, this paper presents a hierarchical speech Transformer with shifted windows to aggregate multi-scale emotion features for speech emotion recognition (SER), called Speech Swin-Transformer. Specifically, we first divide the speech spectrogram into segment-level patches in the time domain, composed of multiple frame patches. These segment-level patches are then encoded using a stack of Swin blocks, in which a local window Transformer is utilized to explore local inter-frame emotional information across frame patches of each segment patch. After that, we also design a shifted window Transformer to compensate for patch correlations near the boundaries of segment patches. Finally, we em
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23457;&#26597;&#20102;&#22312;&#23391;&#21152;&#25289;&#31038;&#32676;&#20013;&#32463;&#21382;&#27542;&#27665;&#20027;&#20041;&#24433;&#21709;&#30340;&#24773;&#24863;&#20998;&#26512;&#24037;&#20855;&#65292;&#21457;&#29616;&#23427;&#20204;&#21487;&#33021;&#23384;&#22312;&#22522;&#20110;&#36523;&#20221;&#30340;&#20559;&#35265;&#65292;&#24182;&#25552;&#20986;&#20102;&#35686;&#31034;&#12290;</title><link>http://arxiv.org/abs/2401.10535</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#8220;&#27542;&#27665;&#20914;&#21160;&#8221;: &#23391;&#21152;&#25289;&#24773;&#24863;&#20998;&#26512;&#24037;&#20855;&#21450;&#20854;&#22522;&#20110;&#36523;&#20221;&#30340;&#20559;&#35265;&#30340;&#23457;&#26597;
&lt;/p&gt;
&lt;p&gt;
The "Colonial Impulse" of Natural Language Processing: An Audit of Bengali Sentiment Analysis Tools and Their Identity-based Biases. (arXiv:2401.10535v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23457;&#26597;&#20102;&#22312;&#23391;&#21152;&#25289;&#31038;&#32676;&#20013;&#32463;&#21382;&#27542;&#27665;&#20027;&#20041;&#24433;&#21709;&#30340;&#24773;&#24863;&#20998;&#26512;&#24037;&#20855;&#65292;&#21457;&#29616;&#23427;&#20204;&#21487;&#33021;&#23384;&#22312;&#22522;&#20110;&#36523;&#20221;&#30340;&#20559;&#35265;&#65292;&#24182;&#25552;&#20986;&#20102;&#35686;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#27542;&#27665;&#20027;&#20041;&#22312;&#31038;&#20250;&#21382;&#21490;&#19978;&#23545;&#20154;&#20204;&#30340;&#36523;&#20221;&#20135;&#29983;&#20102;&#21508;&#31181;&#24433;&#21709;&#65292;&#20294;&#36825;&#20123;&#27542;&#27665;&#30340;&#20215;&#20540;&#35266;&#21644;&#20559;&#35265;&#20173;&#36890;&#36807;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#24471;&#21040;&#20102;&#24310;&#32493;&#12290;&#19968;&#31867;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#65292;&#24773;&#24863;&#20998;&#26512;&#24037;&#20855;&#65292;&#20063;&#21487;&#33021;&#24310;&#32493;&#27542;&#27665;&#30340;&#20215;&#20540;&#35266;&#21644;&#20559;&#35265;&#65292;&#28982;&#32780;&#65292;&#23545;&#36825;&#20123;&#24037;&#20855;&#21487;&#33021;&#22914;&#20309;&#19982;&#27542;&#27665;&#20027;&#20041;&#30340;&#24310;&#32493;&#30456;&#20851;&#32852;&#30340;&#20851;&#27880;&#21364;&#36739;&#23569;&#65292;&#23613;&#31649;&#23427;&#20204;&#32463;&#24120;&#34987;&#29992;&#26469;&#25351;&#23548;&#21508;&#31181;&#23454;&#36341;&#65288;&#20363;&#22914;&#65292;&#20869;&#23481;&#31649;&#29702;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#32463;&#21382;&#21644;&#32487;&#32493;&#32463;&#21382;&#27542;&#27665;&#20027;&#20041;&#24433;&#21709;&#30340;&#23391;&#21152;&#25289;&#31038;&#32676;&#32972;&#26223;&#19979;&#65292;&#24773;&#24863;&#20998;&#26512;&#24037;&#20855;&#21487;&#33021;&#23384;&#22312;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#26681;&#25454;&#24403;&#22320;&#23391;&#21152;&#25289;&#31038;&#32676;&#20013;&#21463;&#27542;&#27665;&#20027;&#20041;&#24433;&#21709;&#26368;&#22823;&#30340;&#36523;&#20221;&#31867;&#21035;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#24615;&#21035;&#12289;&#23447;&#25945;&#21644;&#22269;&#31821;&#12290;&#25105;&#20204;&#23545;&#22312;Python&#21253;&#32034;&#24341;(PyPI)&#21644;GitHub&#19978;&#25552;&#20379;&#30340;&#25152;&#26377;&#23391;&#21152;&#25289;&#24773;&#24863;&#20998;&#26512;&#24037;&#20855;&#36827;&#34892;&#20102;&#31639;&#27861;&#23457;&#26597;&#12290;&#23613;&#31649;&#35821;&#20041;&#20869;&#23481;&#30456;&#20284;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#24037;&#20855;&#22312;&#22788;&#29702;&#19982;&#23391;&#21152;&#25289;&#31038;&#32676;&#26377;&#20851;&#30340;&#24773;&#24863;&#26102;&#23384;&#22312;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
While colonization has sociohistorically impacted people's identities across various dimensions, those colonial values and biases continue to be perpetuated by sociotechnical systems. One category of sociotechnical systems--sentiment analysis tools--can also perpetuate colonial values and bias, yet less attention has been paid to how such tools may be complicit in perpetuating coloniality, although they are often used to guide various practices (e.g., content moderation). In this paper, we explore potential bias in sentiment analysis tools in the context of Bengali communities that have experienced and continue to experience the impacts of colonialism. Drawing on identity categories most impacted by colonialism amongst local Bengali communities, we focused our analytic attention on gender, religion, and nationality. We conducted an algorithmic audit of all sentiment analysis tools for Bengali, available on the Python package index (PyPI) and GitHub. Despite similar semantic content and
&lt;/p&gt;</description></item><item><title>Mementos&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#20687;&#24207;&#21015;&#25512;&#29702;&#20013;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;MLLM&#22312;&#20934;&#30830;&#25551;&#36848;&#22270;&#20687;&#24207;&#21015;&#30340;&#21160;&#24577;&#20449;&#24687;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#23481;&#26131;&#23548;&#33268;&#29289;&#20307;&#21450;&#20854;&#34892;&#20026;&#30340;&#38169;&#35823;&#25551;&#36848;&#25110;&#38169;&#35273;&#12290;</title><link>http://arxiv.org/abs/2401.10529</link><description>&lt;p&gt;
Mementos: &#19968;&#31181;&#38024;&#23545;&#22270;&#20687;&#24207;&#21015;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences. (arXiv:2401.10529v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10529
&lt;/p&gt;
&lt;p&gt;
Mementos&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#20687;&#24207;&#21015;&#25512;&#29702;&#20013;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;MLLM&#22312;&#20934;&#30830;&#25551;&#36848;&#22270;&#20687;&#24207;&#21015;&#30340;&#21160;&#24577;&#20449;&#24687;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#23481;&#26131;&#23548;&#33268;&#29289;&#20307;&#21450;&#20854;&#34892;&#20026;&#30340;&#38169;&#35823;&#25551;&#36848;&#25110;&#38169;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22312;&#22788;&#29702;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#26041;&#38754;&#23637;&#31034;&#20102;&#39640;&#36229;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;MLLM&#22522;&#20934;&#27979;&#35797;&#20027;&#35201;&#29992;&#20110;&#35780;&#20272;&#22522;&#20110;&#21333;&#20010;&#22270;&#20687;&#30340;&#38745;&#24577;&#20449;&#24687;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#32780;&#29616;&#20195;MLLM&#22312;&#20174;&#22270;&#20687;&#24207;&#21015;&#20013;&#36827;&#34892;&#25512;&#26029;&#30340;&#33021;&#21147;&#65292;&#22312;&#29702;&#35299;&#19981;&#26029;&#21464;&#21270;&#30340;&#19990;&#30028;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#21364;&#34987;&#36739;&#23569;&#30740;&#31350;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;Mementos&#65292;&#29992;&#20110;&#35780;&#20272;MLLM&#30340;&#24207;&#21015;&#22270;&#20687;&#25512;&#29702;&#33021;&#21147;&#12290;Mementos&#21253;&#25324;4761&#20010;&#20855;&#26377;&#19981;&#21516;&#38271;&#24230;&#30340;&#22810;&#26679;&#30340;&#22270;&#20687;&#24207;&#21015;&#12290;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;GPT-4&#36741;&#21161;&#26041;&#27861;&#26469;&#35780;&#20272;MLLM&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;&#36890;&#36807;&#23545;Mementos&#20013;&#21253;&#25324;GPT-4V&#21644;Gemini&#22312;&#20869;&#30340;&#20061;&#20010;&#26368;&#26032;MLLM&#36827;&#34892;&#20180;&#32454;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#22312;&#20934;&#30830;&#25551;&#36848;&#25152;&#32473;&#22270;&#20687;&#24207;&#21015;&#30340;&#21160;&#24577;&#20449;&#24687;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#24448;&#24448;&#23548;&#33268;&#23545;&#35937;&#21450;&#20854;&#23545;&#24212;&#34892;&#20026;&#30340;&#38169;&#35823;&#25551;&#36848;&#25110;&#38169;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Large Language Models (MLLMs) have demonstrated proficiency in handling a variety of visual-language tasks. However, current MLLM benchmarks are predominantly designed to evaluate reasoning based on static information about a single image, and the ability of modern MLLMs to extrapolate from image sequences, which is essential for understanding our ever-changing world, has been less investigated. To address this challenge, this paper introduces Mementos, a new benchmark designed to assess MLLMs' sequential image reasoning abilities. Mementos features 4,761 diverse image sequences with varying lengths. We also employ a GPT-4 assisted method to evaluate MLLM reasoning performance. Through a careful evaluation of nine recent MLLMs on Mementos, including GPT-4V and Gemini, we find that they struggle to accurately describe dynamic information about given image sequences, often leading to hallucinations/misrepresentations of objects and their corresponding behaviors. Our quantitati
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#36328;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#65288;XME&#65289;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#19968;&#31181;&#35821;&#35328;&#20013;&#32534;&#36753;&#20107;&#23454;&#24182;&#35266;&#23519;&#20854;&#23545;&#20854;&#20182;&#35821;&#35328;&#30340;&#26356;&#26032;&#20256;&#25773;&#65292;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#65288;MET&#65289;&#30340;&#24615;&#33021;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.10521</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36328;&#35821;&#35328;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Cross-lingual Editing in Multilingual Language Models. (arXiv:2401.10521v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#36328;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#65288;XME&#65289;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#19968;&#31181;&#35821;&#35328;&#20013;&#32534;&#36753;&#20107;&#23454;&#24182;&#35266;&#23519;&#20854;&#23545;&#20854;&#20182;&#35821;&#35328;&#30340;&#26356;&#26032;&#20256;&#25773;&#65292;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#65288;MET&#65289;&#30340;&#24615;&#33021;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35757;&#32451;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#65292;&#32780;&#26356;&#26032;&#36807;&#26102;&#30340;LLM&#38656;&#35201;&#22823;&#37327;&#30340;&#24037;&#20316;&#21644;&#36164;&#28304;&#12290;&#34429;&#28982;&#20986;&#29616;&#20102;&#35768;&#22810;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#65288;MET&#65289;&#20197;&#20415;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#26356;&#26032;&#27169;&#22411;&#36755;&#20986;&#65292;&#20294;&#22312;&#22810;&#35821;&#35328;LLM&#20013;&#65292;&#20854;&#20013;&#30340;&#30693;&#35782;&#20197;&#22810;&#31181;&#35821;&#35328;&#23384;&#20648;&#65292;&#36825;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#28145;&#20837;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#36328;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#65288;XME&#65289;&#33539;&#24335;&#65292;&#22312;&#35813;&#33539;&#24335;&#20013;&#65292;&#19968;&#20010;&#20107;&#23454;&#22312;&#19968;&#31181;&#35821;&#35328;&#20013;&#34987;&#32534;&#36753;&#65292;&#35266;&#23519;&#20854;&#22312;&#20854;&#20182;&#35821;&#35328;&#20013;&#30340;&#26356;&#26032;&#20256;&#25773;&#12290;&#20026;&#20102;&#30740;&#31350;XME&#33539;&#24335;&#65292;&#25105;&#20204;&#20351;&#29992;BLOOM&#12289;mBERT&#21644;XLM-RoBERTa&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20351;&#29992;&#20102;&#20004;&#31181;&#20889;&#20316;&#33050;&#26412;&#65292;&#21363;&#25289;&#19969;&#35821;&#65288;&#33521;&#35821;&#12289;&#27861;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#65289;&#21644;&#21360;&#22320;&#35821;&#65288;&#21360;&#22320;&#35821;&#12289;&#21476;&#21513;&#25289;&#29305;&#35821;&#21644;&#23391;&#21152;&#25289;&#35821;&#65289;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;XME&#35774;&#32622;&#19979;&#65292;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;MET&#23384;&#22312;&#26126;&#26174;&#30340;&#24615;&#33021;&#38480;&#21046;&#65292;&#29305;&#21035;&#26159;&#24403;&#28041;&#21450;&#30340;&#35821;&#35328;&#23646;&#20110;&#20004;&#20010;&#19981;&#21516;&#30340;&#35821;&#26063;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
The training of large language models (LLMs) necessitates substantial data and computational resources, and updating outdated LLMs entails significant efforts and resources. While numerous model editing techniques (METs) have emerged to efficiently update model outputs without retraining, their effectiveness in multilingual LLMs, where knowledge is stored in diverse languages, remains an underexplored research area. This research paper introduces the cross-lingual model editing (\textbf{XME}) paradigm, wherein a fact is edited in one language, and the subsequent update propagation is observed across other languages. To investigate the XME paradigm, we conducted experiments using BLOOM, mBERT, and XLM-RoBERTa using the two writing scripts: \textit{Latin} (English, French, and Spanish) and \textit{Indic} (Hindi, Gujarati, and Bengali). The results reveal notable performance limitations of state-of-the-art METs under the XME setting, mainly when the languages involved belong to two distin
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#36827;&#21270;&#31639;&#27861;&#30340;&#32467;&#21512;&#20855;&#26377;&#24378;&#22823;&#30340;&#19968;&#33268;&#24615;&#65292;&#21253;&#25324;&#26631;&#35760;&#23884;&#20837;&#21644;&#22522;&#22240;&#22411;-&#34920;&#29616;&#22411;&#26144;&#23556;&#12289;&#20301;&#32622;&#32534;&#30721;&#21644;&#36866;&#24212;&#24615;&#22609;&#36896;&#12289;&#20301;&#32622;&#23884;&#20837;&#21644;&#36873;&#25321;&#12289;&#27880;&#24847;&#21147;&#21644;&#20132;&#21449;&#12289;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#21644;&#31361;&#21464;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#21442;&#25968;&#26356;&#26032;&#20197;&#21450;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#31561;&#22810;&#20010;&#26680;&#24515;&#29305;&#24449;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#32806;&#21512;&#30740;&#31350;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#26412;&#36335;&#32447;&#21644;&#20851;&#38190;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.10510</link><description>&lt;p&gt;
&#22825;&#20316;&#20043;&#21512;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#36827;&#21270;&#31639;&#27861;&#30340;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
A match made in consistency heaven: when large language models meet evolutionary algorithms. (arXiv:2401.10510v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10510
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#36827;&#21270;&#31639;&#27861;&#30340;&#32467;&#21512;&#20855;&#26377;&#24378;&#22823;&#30340;&#19968;&#33268;&#24615;&#65292;&#21253;&#25324;&#26631;&#35760;&#23884;&#20837;&#21644;&#22522;&#22240;&#22411;-&#34920;&#29616;&#22411;&#26144;&#23556;&#12289;&#20301;&#32622;&#32534;&#30721;&#21644;&#36866;&#24212;&#24615;&#22609;&#36896;&#12289;&#20301;&#32622;&#23884;&#20837;&#21644;&#36873;&#25321;&#12289;&#27880;&#24847;&#21147;&#21644;&#20132;&#21449;&#12289;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#21644;&#31361;&#21464;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#21442;&#25968;&#26356;&#26032;&#20197;&#21450;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#31561;&#22810;&#20010;&#26680;&#24515;&#29305;&#24449;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#32806;&#21512;&#30740;&#31350;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#26412;&#36335;&#32447;&#21644;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#25104;&#21019;&#36896;&#24615;&#30340;&#33258;&#28982;&#25991;&#26412;&#26041;&#38754;&#20855;&#26377;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#36827;&#21270;&#31639;&#27861;&#65288;EAs&#65289;&#21487;&#20197;&#21457;&#29616;&#22797;&#26434;&#23454;&#38469;&#38382;&#39064;&#30340;&#22810;&#26679;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#25991;&#26412;&#24207;&#21015;&#29983;&#25104;&#21644;&#36827;&#21270;&#30340;&#20849;&#21516;&#29305;&#28857;&#21644;&#26041;&#21521;&#24615;&#65292;&#38416;&#36848;&#20102;LLMs&#19982;EAs&#20043;&#38388;&#30340;&#24378;&#22823;&#19968;&#33268;&#24615;&#65292;&#21253;&#25324;&#22810;&#20010;&#19968;&#23545;&#19968;&#30340;&#26680;&#24515;&#29305;&#24449;&#65306;&#26631;&#35760;&#23884;&#20837;&#21644;&#22522;&#22240;&#22411;-&#34920;&#29616;&#22411;&#26144;&#23556;&#12289;&#20301;&#32622;&#32534;&#30721;&#21644;&#36866;&#24212;&#24615;&#22609;&#36896;&#12289;&#20301;&#32622;&#23884;&#20837;&#21644;&#36873;&#25321;&#12289;&#27880;&#24847;&#21147;&#21644;&#20132;&#21449;&#12289;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#21644;&#31361;&#21464;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#21442;&#25968;&#26356;&#26032;&#20197;&#21450;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#12290;&#22312;&#36825;&#31181;&#19968;&#33268;&#24615;&#35270;&#35282;&#19979;&#65292;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#32806;&#21512;&#30740;&#31350;&#65292;&#21253;&#25324;&#36827;&#21270;&#24494;&#35843;&#21644;LLM&#22686;&#24378;&#22411;EAs&#12290;&#20511;&#21161;&#36825;&#20123;&#27934;&#35265;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#26410;&#26469;&#22312;LLMs&#21644;EAs&#32806;&#21512;&#26041;&#38754;&#30340;&#22522;&#26412;&#30740;&#31350;&#36335;&#32447;&#65292;&#24182;&#31361;&#20986;&#20102;&#20854;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained large language models (LLMs) have powerful capabilities for generating creative natural text. Evolutionary algorithms (EAs) can discover diverse solutions to complex real-world problems. Motivated by the common collective and directionality of text sequence generation and evolution, this paper illustrates the strong consistency of LLMs and EAs, which includes multiple one-to-one key characteristics: token embedding and genotype-phenotype mapping, position encoding and fitness shaping, position embedding and selection, attention and crossover, feed-forward neural network and mutation, model training and parameter update, and multi-task learning and multi-objective optimization. Based on this consistency perspective, existing coupling studies are analyzed, including evolutionary fine-tuning and LLM-enhanced EAs. Leveraging these insights, we outline a fundamental roadmap for future research in coupling LLMs and EAs, while highlighting key challenges along the way. The consist
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#37329;&#34701;&#20998;&#26512;&#30340;&#22522;&#20110;LLMs&#30340;&#27169;&#22411;&#26080;&#20851;&#30340;&#25991;&#26412;&#21040;SQL&#26694;&#26550;FinSQL&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#37329;&#34701;&#20998;&#26512;&#25991;&#26412;&#21040;SQL&#22522;&#20934;&#25968;&#25454;&#38598;BULL&#65292;&#20174;&#25552;&#31034;&#26500;&#36896;&#21644;&#21442;&#25968;&#21270;&#30340;&#35282;&#24230;&#20026;&#37329;&#34701;&#25991;&#26412;&#21040;SQL&#25552;&#20379;&#20102;&#31995;&#32479;&#21270;&#30340;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2401.10506</link><description>&lt;p&gt;
FinSQL&#65306;&#38754;&#21521;&#37329;&#34701;&#20998;&#26512;&#30340;&#22522;&#20110;&#27169;&#22411;&#26080;&#20851;&#30340;&#22522;&#20110;LLMs&#30340;&#25991;&#26412;&#21040;SQL&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FinSQL: Model-Agnostic LLMs-based Text-to-SQL Framework for Financial Analysis. (arXiv:2401.10506v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#37329;&#34701;&#20998;&#26512;&#30340;&#22522;&#20110;LLMs&#30340;&#27169;&#22411;&#26080;&#20851;&#30340;&#25991;&#26412;&#21040;SQL&#26694;&#26550;FinSQL&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#37329;&#34701;&#20998;&#26512;&#25991;&#26412;&#21040;SQL&#22522;&#20934;&#25968;&#25454;&#38598;BULL&#65292;&#20174;&#25552;&#31034;&#26500;&#36896;&#21644;&#21442;&#25968;&#21270;&#30340;&#35282;&#24230;&#20026;&#37329;&#34701;&#25991;&#26412;&#21040;SQL&#25552;&#20379;&#20102;&#31995;&#32479;&#21270;&#30340;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20851;&#27880;&#30340;&#26159;&#25991;&#26412;&#21040;SQL&#30340;&#39046;&#22495;&#65292;&#35813;&#39046;&#22495;&#20026;&#25805;&#20316;&#20851;&#31995;&#25968;&#25454;&#24211;&#25552;&#20379;&#20102;&#26080;&#20195;&#30721;&#25509;&#21475;&#65292;&#22312;&#37329;&#34701;&#20998;&#26512;&#20013;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65307;&#22240;&#20026;&#37329;&#34701;&#19987;&#19994;&#20154;&#21592;&#21487;&#33021;&#19981;&#25797;&#38271;SQL&#32534;&#31243;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#36824;&#27809;&#26377;&#20026;&#37329;&#34701;&#20998;&#26512;&#25552;&#20379;&#23454;&#29992;&#30340;&#25991;&#26412;&#21040;SQL&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#29616;&#26377;&#30340;&#25991;&#26412;&#21040;SQL&#26041;&#27861;&#27809;&#26377;&#32771;&#34385;&#21040;&#37329;&#34701;&#24212;&#29992;&#20013;&#25968;&#25454;&#24211;&#30340;&#29420;&#29305;&#29305;&#28857;&#65292;&#22914;&#36890;&#24120;&#23384;&#22312;&#30340;&#23485;&#34920;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#37329;&#34701;&#20998;&#26512;&#25991;&#26412;&#21040;SQL&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#26080;&#20851;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#37329;&#34701;&#20998;&#26512;&#25991;&#26412;&#21040;SQL&#26694;&#26550;&#12290;&#36825;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;BULL&#26159;&#20174;&#24658;&#29983;&#31185;&#25216;&#20844;&#21496;&#30340;&#23454;&#38469;&#37329;&#34701;&#20998;&#26512;&#19994;&#21153;&#20013;&#25910;&#38598;&#30340;&#65292;&#21253;&#25324;&#22522;&#37329;&#12289;&#32929;&#31080;&#21644;&#23439;&#35266;&#32463;&#27982;&#30340;&#25968;&#25454;&#24211;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#25552;&#20986;&#30340;&#22522;&#20110;LLMs&#30340;&#25991;&#26412;&#21040;SQL&#26694;&#26550;FinSQL&#20174;&#25552;&#31034;&#26500;&#36896;&#21644;&#21442;&#25968;&#21270;&#30340;&#35282;&#24230;&#20026;&#37329;&#34701;&#25991;&#26412;&#21040;SQL&#25552;&#20379;&#20102;&#31995;&#32479;&#21270;&#30340;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-SQL, which provides zero-code interface for operating relational databases, has gained much attention in financial analysis; because, financial professionals may not well-skilled in SQL programming. However, until now, there is no practical Text-to-SQL benchmark dataset for financial analysis, and existing Text-to-SQL methods have not considered the unique characteristics of databases in financial applications, such as commonly existing wide tables. To address these issues, we collect a practical Text-to-SQL benchmark dataset and propose a model-agnostic Large Language Model (LLMs)-based Text-to-SQL framework for financial analysis. The benchmark dataset, BULL, is collected from the practical financial analysis business of Hundsun Technologies Inc., including databases for fund, stock, and macro economy. Besides, the proposed LLMs-based Text-to-SQL framework, FinSQL, provides a systematic treatment for financial Text-to-SQL from the perspectives of prompt construction, paramete
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#34701;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#29616;&#26377;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#21512;&#24182;&#20026;&#19968;&#20010;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#30446;&#26631;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#39564;&#35777;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10491</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Knowledge Fusion of Large Language Models. (arXiv:2401.10491v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#34701;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#29616;&#26377;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#21512;&#24182;&#20026;&#19968;&#20010;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#30446;&#26631;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#39564;&#35777;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#29420;&#29305;&#21151;&#33021;&#21644;&#20248;&#21183;&#30340;&#27169;&#22411;&#65292;&#20294;&#36825;&#23558;&#24102;&#26469;&#24040;&#22823;&#30340;&#25104;&#26412;&#65292;&#24182;&#21487;&#33021;&#23548;&#33268;&#20887;&#20313;&#30340;&#33021;&#21147;&#12290;&#30456;&#21453;&#65292;&#19968;&#31181;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#21644;&#24378;&#22823;&#21151;&#33021;&#30340;&#26041;&#27861;&#26159;&#23558;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;LLMs&#21512;&#24182;&#20026;&#19968;&#20010;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20123;LLMs&#30340;&#19981;&#21516;&#26550;&#26500;&#65292;&#30452;&#25509;&#28151;&#21512;&#23427;&#20204;&#30340;&#26435;&#37325;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LLMs&#30340;&#30693;&#35782;&#34701;&#21512;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#23558;&#29616;&#26377;LLMs&#30340;&#33021;&#21147;&#32467;&#21512;&#36215;&#26469;&#24182;&#36716;&#31227;&#21040;&#21333;&#20010;LLM&#20013;&#12290;&#36890;&#36807;&#21033;&#29992;&#28304;LLMs&#30340;&#29983;&#25104;&#20998;&#24067;&#65292;&#25105;&#20204;&#22806;&#37096;&#21270;&#23427;&#20204;&#30340;&#38598;&#20307;&#30693;&#35782;&#21644;&#29420;&#29305;&#20248;&#21183;&#65292;&#20174;&#32780;&#21487;&#33021;&#25552;&#39640;&#30446;&#26631;&#27169;&#22411;&#30340;&#33021;&#21147;&#36229;&#36807;&#20219;&#20309;&#21333;&#20010;&#28304;LLM&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#20855;&#26377;&#19981;&#21516;&#26550;&#26500;&#30340;&#19977;&#20010;&#27969;&#34892;LLMs &#8212;&#8212; Llama-2&#12289;MPT&#21644;OpenLLaMA&#22312;&#21508;&#31181;&#22522;&#20934;&#21644;&#20219;&#21153;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#35777;&#23454;&#20102;&#34701;&#21512;&#36825;&#19977;&#20010;LLMs&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
While training large language models (LLMs) from scratch can generate models with distinct functionalities and strengths, it comes at significant costs and may result in redundant capabilities. Alternatively, a cost-effective and compelling approach is to merge existing pre-trained LLMs into a more potent model. However, due to the varying architectures of these LLMs, directly blending their weights is impractical. In this paper, we introduce the notion of knowledge fusion for LLMs, aimed at combining the capabilities of existing LLMs and transferring them into a single LLM. By leveraging the generative distributions of source LLMs, we externalize their collective knowledge and unique strengths, thereby potentially elevating the capabilities of the target model beyond those of any individual source LLM. We validate our approach using three popular LLMs with different architectures--Llama-2, MPT, and OpenLLaMA--across various benchmarks and tasks. Our findings confirm that the fusion of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#29983;&#25104;&#24335;&#23494;&#38598;&#26816;&#32034;&#65288;GDR&#65289;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#26597;&#35810;&#21644;&#25991;&#26723;&#20043;&#38388;&#23454;&#29616;&#31751;&#38388;&#21305;&#37197;&#21644;&#32454;&#31890;&#24230;&#30340;&#31751;&#20869;&#21305;&#37197;&#65292;&#32531;&#35299;&#20102;&#29983;&#25104;&#24335;&#26816;&#32034;&#38754;&#20020;&#30340;&#35760;&#24518;&#20934;&#30830;&#24615;&#24046;&#12289;&#35760;&#24518;&#28151;&#28102;&#21644;&#35760;&#24518;&#26356;&#26032;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.10487</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#23494;&#38598;&#26816;&#32034;&#65306;&#35760;&#24518;&#21487;&#20197;&#26159;&#19968;&#20010;&#36127;&#25285;
&lt;/p&gt;
&lt;p&gt;
Generative Dense Retrieval: Memory Can Be a Burden. (arXiv:2401.10487v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#29983;&#25104;&#24335;&#23494;&#38598;&#26816;&#32034;&#65288;GDR&#65289;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#26597;&#35810;&#21644;&#25991;&#26723;&#20043;&#38388;&#23454;&#29616;&#31751;&#38388;&#21305;&#37197;&#21644;&#32454;&#31890;&#24230;&#30340;&#31751;&#20869;&#21305;&#37197;&#65292;&#32531;&#35299;&#20102;&#29983;&#25104;&#24335;&#26816;&#32034;&#38754;&#20020;&#30340;&#35760;&#24518;&#20934;&#30830;&#24615;&#24046;&#12289;&#35760;&#24518;&#28151;&#28102;&#21644;&#35760;&#24518;&#26356;&#26032;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#26816;&#32034; (GR) &#22312;&#23567;&#35268;&#27169;&#35821;&#26009;&#24211;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#33394;&#65292;&#36890;&#36807;&#35760;&#24518;&#27169;&#22411;&#21442;&#25968;&#26469;&#38544;&#24335;&#22320;&#23454;&#29616;&#26597;&#35810;&#21644;&#25991;&#26723;&#30340;&#28145;&#24230;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35760;&#24518;&#26426;&#21046;&#38754;&#20020;&#19977;&#20010;&#38382;&#39064;&#65306;(1) &#23545;&#25991;&#26723;&#30340;&#32454;&#31890;&#24230;&#29305;&#24449;&#30340;&#35760;&#24518;&#20934;&#30830;&#24615;&#36739;&#24046;&#65307;(2) &#38543;&#30528;&#35821;&#26009;&#24211;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#35760;&#24518;&#28151;&#28102;&#31243;&#24230;&#36234;&#26469;&#36234;&#20005;&#37325;&#65307;(3) &#23545;&#26032;&#25991;&#26723;&#30340;&#35760;&#24518;&#26356;&#26032;&#25104;&#26412;&#24040;&#22823;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29983;&#25104;&#24335;&#23494;&#38598;&#26816;&#32034;&#65288;GDR&#65289;&#30340;&#33539;&#24335;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;GDR&#39318;&#20808;&#20351;&#29992;&#26377;&#38480;&#30340;&#20869;&#23384;&#23481;&#37327;&#65292;&#20174;&#26597;&#35810;&#21040;&#30456;&#20851;&#25991;&#26723;&#31751;&#23454;&#29616;&#31751;&#38388;&#21305;&#37197;&#12290;&#28982;&#21518;&#65292;&#24341;&#20837;&#26080;&#35760;&#24518;&#30340;&#23494;&#38598;&#26816;&#32034;&#65288;DR&#65289;&#21305;&#37197;&#26426;&#21046;&#65292;&#20174;&#31751;&#21040;&#30456;&#20851;&#25991;&#26723;&#36827;&#34892;&#32454;&#31890;&#24230;&#30340;&#31751;&#20869;&#21305;&#37197;&#12290;&#36825;&#31181;&#20174;&#31895;&#21040;&#32454;&#30340;&#36807;&#31243;&#26368;&#22823;&#21270;&#20102;GR&#28145;&#24230;&#20132;&#20114;&#21644;DR&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Retrieval (GR), autoregressively decoding relevant document identifiers given a query, has been shown to perform well under the setting of small-scale corpora. By memorizing the document corpus with model parameters, GR implicitly achieves deep interaction between query and document. However, such a memorizing mechanism faces three drawbacks: (1) Poor memory accuracy for fine-grained features of documents; (2) Memory confusion gets worse as the corpus size increases; (3) Huge memory update costs for new documents. To alleviate these problems, we propose the Generative Dense Retrieval (GDR) paradigm. Specifically, GDR first uses the limited memory volume to achieve inter-cluster matching from query to relevant document clusters. Memorizing-free matching mechanism from Dense Retrieval (DR) is then introduced to conduct fine-grained intra-cluster matching from clusters to relevant documents. The coarse-to-fine process maximizes the advantages of GR's deep interaction and DR's s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#25552;&#21069;&#20572;&#27490;&#33258;&#19968;&#33268;&#24615;&#65288;ESC&#65289;&#30340;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;&#37319;&#26679;&#31574;&#30053;&#65292;&#29992;&#20110;&#38477;&#20302;&#22810;&#27493;&#25512;&#29702;&#20219;&#21153;&#20013;&#33258;&#19968;&#33268;&#24615;&#30340;&#25104;&#26412;&#65292;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10480</link><description>&lt;p&gt;
&#36867;&#31163;&#39640;&#26114;&#25104;&#26412;&#65306;&#22810;&#27493;&#25512;&#29702;&#30340;&#25552;&#21069;&#20572;&#27490;&#33258;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Escape Sky-high Cost: Early-stopping Self-Consistency for Multi-step Reasoning. (arXiv:2401.10480v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10480
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#25552;&#21069;&#20572;&#27490;&#33258;&#19968;&#33268;&#24615;&#65288;ESC&#65289;&#30340;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;&#37319;&#26679;&#31574;&#30053;&#65292;&#29992;&#20110;&#38477;&#20302;&#22810;&#27493;&#25512;&#29702;&#20219;&#21153;&#20013;&#33258;&#19968;&#33268;&#24615;&#30340;&#25104;&#26412;&#65292;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#19968;&#33268;&#24615;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#20110;&#24605;&#32500;&#38142;&#25512;&#29702;&#30340;&#35299;&#30721;&#31574;&#30053;&#12290;&#23613;&#31649;&#22312;&#21508;&#31181;&#22810;&#27493;&#25512;&#29702;&#20219;&#21153;&#20013;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#20294;&#33258;&#19968;&#33268;&#24615;&#26159;&#19968;&#31181;&#39640;&#25104;&#26412;&#30340;&#26041;&#27861;&#65292;&#38656;&#35201;&#36827;&#34892;&#22810;&#27425;&#39044;&#35774;&#22823;&#23567;&#30340;&#25277;&#26679;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;&#37319;&#26679;&#36807;&#31243;&#8212;&#8212;&#25552;&#21069;&#20572;&#27490;&#33258;&#19968;&#33268;&#24615;&#65288;ESC&#65289;&#65292;&#20197;&#26497;&#22823;&#38477;&#20302;&#33258;&#19968;&#33268;&#24615;&#30340;&#25104;&#26412;&#65292;&#21516;&#26102;&#19981;&#25439;&#23475;&#24615;&#33021;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#36824;&#36827;&#19968;&#27493;&#25512;&#23548;&#20102;&#19968;&#31181;&#25511;&#21046;&#26041;&#26696;&#65292;&#21487;&#20197;&#21160;&#24577;&#36873;&#25321;&#19981;&#21516;&#20219;&#21153;&#21644;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#25104;&#26412;&#24179;&#34913;&#12290;&#20026;&#20102;&#35777;&#26126;ESC&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#19977;&#20010;&#24120;&#35265;&#30340;&#25512;&#29702;&#20219;&#21153;&#31867;&#21035;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65306;&#31639;&#26415;&#25512;&#29702;&#12289;&#24120;&#35782;&#25512;&#29702;&#21644;&#31526;&#21495;&#25512;&#29702;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21253;&#25324;MATH&#22312;&#20869;&#30340;&#20845;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;ESC&#23558;&#24605;&#32500;&#38142;&#25512;&#29702;&#30340;&#24179;&#22343;&#25277;&#26679;&#27425;&#25968;&#26174;&#33879;&#20943;&#23569;&#20102;&#65288;-33.8%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-consistency (SC) has been a widely used decoding strategy for chain-of-thought reasoning. Despite bringing significant performance improvements across a variety of multi-step reasoning tasks, it is a high-cost method that requires multiple sampling with the preset size. In this paper, we propose a simple and scalable sampling process, \textbf{E}arly-Stopping \textbf{S}elf-\textbf{C}onsistency (ESC), to greatly reduce the cost of SC without sacrificing performance. On this basis, one control scheme for ESC is further derivated to dynamically choose the performance-cost balance for different tasks and models. To demonstrate ESC's effectiveness, we conducted extensive experiments on three popular categories of reasoning tasks: arithmetic, commonsense and symbolic reasoning over language models with varying scales. The empirical results show that ESC reduces the average number of sampling of chain-of-thought reasoning by a significant margin on six benchmarks, including MATH (-33.8%),
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#24230;&#37327;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#29983;&#21629;&#31185;&#23398;&#39046;&#22495;&#19979;&#39046;&#22495;&#28418;&#31227;&#30340;&#21629;&#21517;&#26631;&#31614;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#28304;&#23454;&#20307;&#21644;&#30446;&#26631;&#23454;&#20307;&#25237;&#24433;&#21040;&#29305;&#24449;&#31354;&#38388;&#30340;&#19981;&#21516;&#21306;&#22495;&#26469;&#20943;&#36731;&#28304;&#23454;&#20307;&#38169;&#35823;&#26631;&#35760;&#20026;&#30446;&#26631;&#23454;&#20307;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.10472</link><description>&lt;p&gt;
&#29983;&#21629;&#31185;&#23398;&#39046;&#22495;&#36890;&#36807;&#24230;&#37327;&#23398;&#20064;&#23545;&#39046;&#22495;&#28418;&#31227;&#19979;&#30340;&#21629;&#21517;&#26631;&#31614;&#36827;&#34892;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Name Tagging Under Domain Shift via Metric Learning for Life Sciences. (arXiv:2401.10472v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#24230;&#37327;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#29983;&#21629;&#31185;&#23398;&#39046;&#22495;&#19979;&#39046;&#22495;&#28418;&#31227;&#30340;&#21629;&#21517;&#26631;&#31614;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#28304;&#23454;&#20307;&#21644;&#30446;&#26631;&#23454;&#20307;&#25237;&#24433;&#21040;&#29305;&#24449;&#31354;&#38388;&#30340;&#19981;&#21516;&#21306;&#22495;&#26469;&#20943;&#36731;&#28304;&#23454;&#20307;&#38169;&#35823;&#26631;&#35760;&#20026;&#30446;&#26631;&#23454;&#20307;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#26631;&#31614;&#26159;&#20449;&#24687;&#25277;&#21462;&#65288;IE&#65289;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#23588;&#20854;&#22312;&#29983;&#29289;&#21307;&#23398;&#21644;&#21270;&#23398;&#31561;&#31185;&#23398;&#39046;&#22495;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#20363;&#22914;ChatGPT&#65292;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#36801;&#31227;&#23398;&#20064;&#24212;&#29992;&#20110;&#22686;&#24378;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#65288;&#28304;&#39046;&#22495;&#65289;&#35757;&#32451;&#30340;&#21629;&#21517;&#26631;&#31614;&#27169;&#22411;&#22312;&#21270;&#23398;&#39046;&#22495;&#65288;&#30446;&#26631;&#39046;&#22495;&#65289;&#20013;&#30340;&#20351;&#29992;&#12290;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#35774;&#32622;&#20013;&#35757;&#32451;&#36825;&#26679;&#30340;&#27169;&#22411;&#30340;&#19968;&#31181;&#24120;&#35265;&#20570;&#27861;&#26159;&#22312;&#26631;&#35760;&#30340;&#28304;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#28982;&#21518;&#22312;&#23569;&#37327;&#26631;&#35760;&#30340;&#30446;&#26631;&#31034;&#20363;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#26679;&#30340;&#27169;&#22411;&#23481;&#26131;&#23558;&#28304;&#23454;&#20307;&#38169;&#35823;&#26631;&#35760;&#20026;&#30446;&#26631;&#23454;&#20307;&#65292;&#22240;&#20026;&#28304;&#23454;&#20307;&#32463;&#24120;&#20986;&#29616;&#22312;&#25991;&#26412;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#65292;&#23558;&#30693;&#35782;&#20174;&#28304;&#39046;&#22495;&#36716;&#31227;&#21040;&#30446;&#26631;&#39046;&#22495;&#65292;&#20294;&#21516;&#26102;&#23558;&#28304;&#23454;&#20307;&#21644;&#30446;&#26631;&#23454;&#20307;&#25237;&#24433;&#21040;&#29305;&#24449;&#31354;&#38388;&#30340;&#19981;&#21516;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Name tagging is a key component of Information Extraction (IE), particularly in scientific domains such as biomedicine and chemistry, where large language models (LLMs), e.g., ChatGPT, fall short. We investigate the applicability of transfer learning for enhancing a name tagging model trained in the biomedical domain (the source domain) to be used in the chemical domain (the target domain). A common practice for training such a model in a few-shot learning setting is to pretrain the model on the labeled source data, and then, to finetune it on a hand-full of labeled target examples. In our experiments we observed that such a model is prone to mis-labeling the source entities, which can often appear in the text, as the target entities. To alleviate this problem, we propose a model to transfer the knowledge from the source domain to the target domain, however, at the same time, to project the source entities and target entities into separate regions of the feature space. This diminishes 
&lt;/p&gt;</description></item><item><title>DeepEdit&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#22909;&#30340;&#25512;&#29702;&#19968;&#33268;&#24615;&#21644;&#23545;&#26356;&#26032;&#30693;&#35782;&#30340;&#24847;&#35782;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#33021;&#21147;&#65292;&#23545;&#22810;&#36339;&#38382;&#39064;&#25968;&#25454;&#38598;MQuaKE&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2401.10471</link><description>&lt;p&gt;
DeepEdit: &#24102;&#26377;&#32422;&#26463;&#30340;&#35299;&#30721;&#24335;&#30693;&#35782;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
DeepEdit: Knowledge Editing as Decoding with Constraints. (arXiv:2401.10471v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10471
&lt;/p&gt;
&lt;p&gt;
DeepEdit&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#22909;&#30340;&#25512;&#29702;&#19968;&#33268;&#24615;&#21644;&#23545;&#26356;&#26032;&#30693;&#35782;&#30340;&#24847;&#35782;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#33021;&#21147;&#65292;&#23545;&#22810;&#36339;&#38382;&#39064;&#25968;&#25454;&#38598;MQuaKE&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30693;&#35782;&#32534;&#36753;&#35270;&#20026;&#24102;&#26377;&#32422;&#26463;&#30340;&#35299;&#30721;&#36807;&#31243;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DeepEdit&#65288;&#22522;&#20110;&#28145;&#24230;&#20248;&#20808;&#25628;&#32034;&#30340;&#28176;&#36827;&#24335;&#35299;&#30721;&#30693;&#35782;&#32534;&#36753;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#22909;&#30340;&#25512;&#29702;&#19968;&#33268;&#24615;&#12289;&#38382;&#39064;&#30456;&#20851;&#24615;&#21644;&#23545;&#26356;&#26032;&#30693;&#35782;&#30340;&#24847;&#35782;&#26469;&#25913;&#36827;&#30693;&#35782;&#32534;&#36753;&#12290;DeepEdit&#21487;&#28789;&#27963;&#24212;&#29992;&#20110;&#25152;&#26377;&#40657;&#30418;LLMs&#65306;&#19981;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#12289;&#34920;&#31034;&#25110;&#36755;&#20986;&#35789;&#27719;&#20998;&#24067;&#12290;DeepEdit&#36880;&#27493;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#25512;&#29702;&#27493;&#39588;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#30693;&#35782;&#32534;&#36753;&#12290;&#23427;&#21033;&#29992;&#28145;&#24230;&#20248;&#20808;&#25628;&#32034;&#26469;&#20462;&#25913;LLMs&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#25552;&#39640;&#36755;&#20986;&#23545;&#38382;&#39064;&#30340;&#30456;&#20851;&#24615;&#21644;&#23545;&#26356;&#26032;&#30693;&#35782;&#30340;&#24847;&#35782;&#12290;&#22312;&#30693;&#35782;&#32534;&#36753;&#26041;&#38754;&#65292;DeepEdit&#22312;&#25511;&#21046;LLMs&#20135;&#29983;&#26356;&#31616;&#27905;&#30340;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#22312;MQuaKE&#19978;&#65292;DeepEdit&#22312;&#23450;&#37327;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#36339;&#38382;&#39064;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a new perspective of knowledge editing for large language models (LLMs) as decoding with constraints. We propose DeepEdit (Depth-first Search based Progressive Decoding for Knowledge Editing), a neuro-symbolic method that improves knowledge editing with better coherence of reasoning, relevance to the question, and awareness of updated knowledge. DeepEdit can be flexibly applied to all black-box LLMs: it does not require any access to the model parameters, representations, or output vocabulary distributions. DeepEdit progressively produces the high-quality reasoning steps towards effective knowledge editing. It utilizes a depth-first search to revise the LLMs' output, which improves the output's informativeness to the input question and awareness of the updated knowledge. Qualitatively, DeepEdit effectively controls LLMs to produce more succinct reasoning in accord with knowledge editing. Quantitatively, DeepEdit yields significant gains on MQuaKE, a challenging multi-hop que
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#28040;&#38500;&#20102;&#20256;&#32479;G2P&#31995;&#32479;&#25152;&#38754;&#20020;&#30340;&#23383;&#20856;&#29983;&#25104;&#38382;&#39064;&#21644;&#22266;&#23450;&#38899;&#32032;&#34920;&#31034;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#23383;&#20856;&#30340;&#25968;&#25454;&#39537;&#21160;&#38899;&#32032;&#34920;&#31034;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#30456;&#24403;&#25110;&#31245;&#24494;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.10465</link><description>&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#23383;&#32032;&#21040;&#38899;&#32032;&#34920;&#31034;&#27861;&#29992;&#20110;&#26080;&#23383;&#20856;&#25991;&#26412;&#36716;&#35821;&#38899;
&lt;/p&gt;
&lt;p&gt;
Data-driven grapheme-to-phoneme representations for a lexicon-free text-to-speech. (arXiv:2401.10465v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10465
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#28040;&#38500;&#20102;&#20256;&#32479;G2P&#31995;&#32479;&#25152;&#38754;&#20020;&#30340;&#23383;&#20856;&#29983;&#25104;&#38382;&#39064;&#21644;&#22266;&#23450;&#38899;&#32032;&#34920;&#31034;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#23383;&#20856;&#30340;&#25968;&#25454;&#39537;&#21160;&#38899;&#32032;&#34920;&#31034;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#30456;&#24403;&#25110;&#31245;&#24494;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23383;&#32032;&#21040;&#38899;&#32032;&#65288;G2P&#65289;&#26159;&#20219;&#20309;&#29616;&#20195;&#39640;&#36136;&#37327;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#31995;&#32479;&#20013;&#30340;&#37325;&#35201;&#31532;&#19968;&#27493;&#12290;&#24403;&#21069;&#22823;&#22810;&#25968;G2P&#31995;&#32479;&#20381;&#36182;&#20110;&#19987;&#23478;&#32534;&#21046;&#30340;&#31934;&#24515;&#25163;&#24037;&#32534;&#20889;&#30340;&#23383;&#20856;&#12290;&#36825;&#24102;&#26469;&#20102;&#19968;&#20010;&#21452;&#37325;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#23383;&#20856;&#26159;&#20351;&#29992;&#22266;&#23450;&#30340;&#38899;&#32032;&#38598;&#29983;&#25104;&#30340;&#65292;&#36890;&#24120;&#26159;ARPABET&#25110;IPA&#65292;&#36825;&#21487;&#33021;&#19981;&#26159;&#26368;&#20248;&#21270;&#22320;&#34920;&#31034;&#25152;&#26377;&#35821;&#35328;&#30340;&#38899;&#32032;&#30340;&#26041;&#24335;&#12290;&#20854;&#27425;&#65292;&#29983;&#20135;&#36825;&#26679;&#19968;&#20010;&#19987;&#23478;&#23383;&#20856;&#25152;&#38656;&#30340;&#24037;&#26102;&#38750;&#24120;&#39640;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#32780;&#19981;&#26159;&#22266;&#23450;&#34920;&#31034;&#27861;&#65292;&#28040;&#38500;&#20102;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#20174;&#32780;&#33719;&#24471;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#38899;&#32032;&#34920;&#31034;&#27861;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26080;&#23383;&#20856;&#26041;&#27861;&#19982;&#20351;&#29992;&#31934;&#24515;&#32534;&#20889;&#30340;&#23383;&#20856;&#30340;&#24378;&#22522;&#32447;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#39537;&#21160;&#26080;&#23383;&#20856;&#26041;&#27861;&#22312;&#24179;&#22343;&#24847;&#35265;&#20998;&#25968;&#65288;MOS&#65289;&#26041;&#38754;&#19982;&#24120;&#35268;&#22522;&#20110;&#35268;&#21017;&#25110;&#22522;&#20110;&#23383;&#20856;&#30340;&#31070;&#32463;G2P&#30456;&#24403;&#25110;&#31245;&#24494;&#26356;&#22909;&#65292;&#21516;&#26102;&#19981;&#20351;&#29992;&#20219;&#20309;&#20808;&#39564;&#35821;&#35328;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Grapheme-to-Phoneme (G2P) is an essential first step in any modern, high-quality Text-to-Speech (TTS) system. Most of the current G2P systems rely on carefully hand-crafted lexicons developed by experts. This poses a two-fold problem. Firstly, the lexicons are generated using a fixed phoneme set, usually, ARPABET or IPA, which might not be the most optimal way to represent phonemes for all languages. Secondly, the man-hours required to produce such an expert lexicon are very high. In this paper, we eliminate both of these issues by using recent advances in self-supervised learning to obtain data-driven phoneme representations instead of fixed representations. We compare our lexicon-free approach against strong baselines that utilize a well-crafted lexicon. Furthermore, we show that our data-driven lexicon-free method performs as good or even marginally better than the conventional rule-based or lexicon-based neural G2Ps in terms of Mean Opinion Score (MOS) while using no prior language
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#29702;&#35299;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20851;&#38190;&#25968;&#25454;&#35268;&#27169;&#65292;&#35777;&#26126;&#20102;&#21482;&#26377;&#24403;&#35821;&#35328;&#27169;&#22411;&#36798;&#21040;&#20851;&#38190;&#22823;&#23567;&#26102;&#25165;&#20250;&#21457;&#29983;&#27867;&#21270;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#26356;&#22823;&#30340;&#27169;&#22411;&#38656;&#35201;&#26356;&#22810;&#25968;&#25454;&#30340;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.10463</link><description>&lt;p&gt;
&#20174;&#29702;&#35299;&#30340;&#35282;&#24230;&#25506;&#35752;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#25968;&#25454;&#35268;&#27169;
&lt;/p&gt;
&lt;p&gt;
Critical Data Size of Language Models from a Grokking Perspective. (arXiv:2401.10463v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#29702;&#35299;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20851;&#38190;&#25968;&#25454;&#35268;&#27169;&#65292;&#35777;&#26126;&#20102;&#21482;&#26377;&#24403;&#35821;&#35328;&#27169;&#22411;&#36798;&#21040;&#20851;&#38190;&#22823;&#23567;&#26102;&#25165;&#20250;&#21457;&#29983;&#27867;&#21270;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#26356;&#22823;&#30340;&#27169;&#22411;&#38656;&#35201;&#26356;&#22810;&#25968;&#25454;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20851;&#38190;&#25968;&#25454;&#35268;&#27169;&#65292;&#36825;&#26159;&#20174;&#24555;&#36895;&#35760;&#24518;&#21040;&#32531;&#24930;&#27867;&#21270;&#30340;&#19968;&#20010;&#22522;&#26412;&#36716;&#21464;&#38408;&#20540;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#30456;&#21464;&#24418;&#24335;&#21270;&#20026;Grokking&#37197;&#32622;&#19979;&#30340;&#25968;&#25454;&#25928;&#29575;&#20551;&#35828;&#65292;&#24182;&#30830;&#23450;&#20102;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#21160;&#21147;&#23398;&#20013;&#30340;&#25968;&#25454;&#19981;&#36275;&#12289;&#20805;&#36275;&#21644;&#36807;&#21097;&#38454;&#27573;&#12290;&#25105;&#20204;&#36890;&#36807;&#37325;&#26032;&#35843;&#25972;&#21021;&#22987;&#21270;&#21644;&#26435;&#37325;&#34928;&#20943;&#65292;&#24320;&#21457;&#20986;&#20102;&#19968;&#31181;Grokking&#37197;&#32622;&#65292;&#31283;&#23450;&#22320;&#22312;&#31616;&#21270;&#30340;&#35821;&#35328;&#27169;&#22411;&#19978;&#37325;&#29616;&#20102;Grokking&#12290;&#25105;&#20204;&#34920;&#26126;&#21482;&#26377;&#24403;&#35821;&#35328;&#27169;&#22411;&#36798;&#21040;&#20851;&#38190;&#22823;&#23567;&#26102;&#25165;&#20250;&#21457;&#29983;&#27867;&#21270;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#26679;&#26412;&#32423;&#21644;&#27169;&#22411;&#32423;&#30340;Grokking&#65292;&#39564;&#35777;&#20102;&#25552;&#20986;&#30340;&#25968;&#25454;&#25928;&#29575;&#20551;&#35828;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#35821;&#35328;&#25968;&#25454;&#38598;&#30340;&#20851;&#38190;&#25968;&#25454;&#38598;&#22823;&#23567;&#22788;&#21457;&#29983;&#30340;&#26356;&#24179;&#28369;&#30340;&#30456;&#21464;&#12290;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#65292;&#36825;&#20010;&#20020;&#30028;&#28857;&#20063;&#21464;&#24471;&#26356;&#22823;&#65292;&#36825;&#34920;&#26126;&#26356;&#22823;&#30340;&#27169;&#22411;&#38656;&#35201;&#26356;&#22810;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#21152;&#28145;&#20102;&#23545;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#29702;&#35299;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the critical data size in language models, a threshold that marks a fundamental shift from quick memorization to slow generalization. We formalize the phase transition under the grokking configuration into the Data Efficiency Hypothesis and identify data insufficiency, sufficiency, and surplus regimes in language models training dynamics. We develop a grokking configuration to reproduce grokking on simplistic language models stably by rescaling initialization and weight decay. We show that generalization occurs only when language models reach a critical size. We analyze grokking across sample-wise and model-wise, verifying the proposed data efficiency hypothesis. Our experiments reveal smoother phase transitions occurring at the critical dataset size for language datasets. As the model size increases, this critical point also becomes larger, indicating that larger models require more data. Our results deepen the understanding of language model training, offering a novel pers
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#19978;&#19979;&#25991;&#20559;&#24046;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#32534;&#36753;&#30701;&#35821;&#21015;&#34920;&#36827;&#34892;&#23450;&#21046;&#65292;&#24182;&#32467;&#21512;&#29305;&#27530;&#26631;&#35760;&#26469;&#26377;&#25928;&#22320;&#35757;&#32451;&#65292;&#20197;&#20415;&#22312;&#36755;&#20837;&#35821;&#38899;&#25968;&#25454;&#20013;&#26816;&#27979;&#20559;&#24046;&#30701;&#35821;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20559;&#24046;&#30701;&#35821;&#32034;&#24341;&#27010;&#29575;&#30340;&#20559;&#24046;&#30701;&#35821;&#22686;&#24378;&#26463;&#25628;&#32034;&#31639;&#27861;&#65292;&#33021;&#36827;&#19968;&#27493;&#25552;&#39640;&#19978;&#19979;&#25991;&#21270;&#24615;&#33021;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#25345;&#32493;&#25913;&#21892;&#23383;&#38169;&#35823;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.10449</link><description>&lt;p&gt;
&#24102;&#26377;&#27880;&#24847;&#21147;&#20559;&#24046;&#30701;&#35821;&#22686;&#24378;&#26463;&#25628;&#32034;&#30340;&#19978;&#19979;&#25991;&#21270;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Contextualized Automatic Speech Recognition with Attention-Based Bias Phrase Boosted Beam Search. (arXiv:2401.10449v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10449
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#19978;&#19979;&#25991;&#20559;&#24046;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#32534;&#36753;&#30701;&#35821;&#21015;&#34920;&#36827;&#34892;&#23450;&#21046;&#65292;&#24182;&#32467;&#21512;&#29305;&#27530;&#26631;&#35760;&#26469;&#26377;&#25928;&#22320;&#35757;&#32451;&#65292;&#20197;&#20415;&#22312;&#36755;&#20837;&#35821;&#38899;&#25968;&#25454;&#20013;&#26816;&#27979;&#20559;&#24046;&#30701;&#35821;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20559;&#24046;&#30701;&#35821;&#32034;&#24341;&#27010;&#29575;&#30340;&#20559;&#24046;&#30701;&#35821;&#22686;&#24378;&#26463;&#25628;&#32034;&#31639;&#27861;&#65292;&#33021;&#36827;&#19968;&#27493;&#25552;&#39640;&#19978;&#19979;&#25991;&#21270;&#24615;&#33021;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#25345;&#32493;&#25913;&#21892;&#23383;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31471;&#21040;&#31471;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#26041;&#27861;&#34920;&#29616;&#20986;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#19982;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#19978;&#19979;&#25991;&#23494;&#20999;&#30456;&#20851;&#65292;&#23545;&#20110;&#26410;&#30693;&#29992;&#25143;&#19978;&#19979;&#25991;&#65288;&#20363;&#22914;&#25216;&#26415;&#26415;&#35821;&#12289;&#20010;&#20154;&#22995;&#21517;&#21644;&#25773;&#25918;&#21015;&#34920;&#65289;&#65292;&#31471;&#21040;&#31471;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#26041;&#27861;&#26080;&#27861;&#23454;&#29616;&#39044;&#26399;&#30340;&#25928;&#26524;&#12290;&#22240;&#27492;&#65292;&#31471;&#21040;&#31471;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#26041;&#27861;&#24517;&#39035;&#33021;&#22815;&#36731;&#26494;&#36827;&#34892;&#19978;&#19979;&#25991;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#19978;&#19979;&#25991;&#20559;&#24046;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#21487;&#32534;&#36753;&#30340;&#30701;&#35821;&#21015;&#34920;&#65288;&#31216;&#20026;&#20559;&#24046;&#21015;&#34920;&#65289;&#36827;&#34892;&#23450;&#21046;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#32467;&#21512;&#20559;&#24046;&#30701;&#35821;&#32034;&#24341;&#25439;&#22833;&#21644;&#29305;&#27530;&#26631;&#35760;&#26469;&#26377;&#25928;&#22320;&#35757;&#32451;&#65292;&#20197;&#20415;&#22312;&#36755;&#20837;&#35821;&#38899;&#25968;&#25454;&#20013;&#26816;&#27979;&#20559;&#24046;&#30701;&#35821;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#36827;&#19968;&#27493;&#25913;&#21892;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#19978;&#19979;&#25991;&#21270;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20559;&#24046;&#30701;&#35821;&#32034;&#24341;&#27010;&#29575;&#30340;&#20559;&#24046;&#30701;&#35821;&#22686;&#24378;&#26463;&#25628;&#32034;&#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22987;&#32456;&#21487;&#20197;&#25913;&#21892;&#23383;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-end (E2E) automatic speech recognition (ASR) methods exhibit remarkable performance. However, since the performance of such methods is intrinsically linked to the context present in the training data, E2E-ASR methods do not perform as desired for unseen user contexts (e.g., technical terms, personal names, and playlists). Thus, E2E-ASR methods must be easily contextualized by the user or developer. This paper proposes an attention-based contextual biasing method that can be customized using an editable phrase list (referred to as a bias list). The proposed method can be trained effectively by combining a bias phrase index loss and special tokens to detect the bias phrases in the input speech data. In addition, to improve the contextualization performance during inference further, we propose a bias phrase boosted (BPB) beam search algorithm based on the bias phrase index probability. Experimental results demonstrate that the proposed method consistently improves the word error ra
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#35821;&#38899;&#35782;&#21035;&#20013;&#20302;&#31209;&#36866;&#24212;&#30340;&#35757;&#32451;&#31574;&#30053;&#21644;&#27169;&#22411;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#24341;&#20837;&#19981;&#21516;&#30340;LoRA&#35757;&#32451;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#30456;&#23545;&#35789;&#38169;&#35823;&#29575;&#30340;&#38477;&#20302;&#65292;&#24182;&#30740;&#31350;&#20102;&#27169;&#22411;&#23545;&#36755;&#20837;&#25200;&#21160;&#30340;&#31283;&#23450;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#39640;&#32423;LoRA&#21464;&#20307;&#23548;&#33268;&#20102;&#26576;&#20123;&#25200;&#21160;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2401.10447</link><description>&lt;p&gt;
&#35821;&#38899;&#35782;&#21035;&#20013;&#20302;&#31209;&#36866;&#24212;&#30340;&#35757;&#32451;&#31574;&#30053;&#21644;&#27169;&#22411;&#40065;&#26834;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Investigating Training Strategies and Model Robustness of Low-Rank Adaptation for Language Modeling in Speech Recognition. (arXiv:2401.10447v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#35821;&#38899;&#35782;&#21035;&#20013;&#20302;&#31209;&#36866;&#24212;&#30340;&#35757;&#32451;&#31574;&#30053;&#21644;&#27169;&#22411;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#24341;&#20837;&#19981;&#21516;&#30340;LoRA&#35757;&#32451;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#30456;&#23545;&#35789;&#38169;&#35823;&#29575;&#30340;&#38477;&#20302;&#65292;&#24182;&#30740;&#31350;&#20102;&#27169;&#22411;&#23545;&#36755;&#20837;&#25200;&#21160;&#30340;&#31283;&#23450;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#39640;&#32423;LoRA&#21464;&#20307;&#23548;&#33268;&#20102;&#26576;&#20123;&#25200;&#21160;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36164;&#28304;&#26377;&#38480;&#30340;&#30828;&#20214;&#35774;&#22791;&#30340;&#26222;&#21450;&#65292;&#20351;&#29992;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#19982;&#20923;&#32467;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#20027;&#27969;&#12289;&#36164;&#28304;&#39640;&#25928;&#30340;&#24314;&#27169;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#39318;&#20808;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#24341;&#20837;&#21508;&#31181;LoRA&#35757;&#32451;&#31574;&#30053;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#22312;&#20844;&#24320;&#30340;Librispeech&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#30456;&#23545;&#35789;&#38169;&#35823;&#29575;&#38477;&#20302;3.50&#65285;&#65292;&#22312;&#28040;&#24687;&#39046;&#22495;&#30340;&#20869;&#37096;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;3.67&#65285;&#30340;&#38477;&#20302;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#35780;&#20272;&#22522;&#20110;LoRA&#30340;&#20108;&#27425;&#20256;&#36882;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23545;&#36755;&#20837;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#12290;&#36825;&#20123;&#25200;&#21160;&#28304;&#20110;&#21516;&#38899;&#23383;&#26367;&#20195;&#21644;&#19968;&#31181;&#21517;&#20026;N-best Perturbation-based Rescoring Robustness&#65288;NPRR&#65289;&#30340;&#26032;&#24230;&#37327;&#26631;&#20934;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#29992;&#20110;&#34913;&#37327;&#37325;&#35780;&#20998;&#27169;&#22411;&#24615;&#33021;&#30340;&#30456;&#23545;&#38477;&#35299;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;LoRA&#30340;&#39640;&#32423;&#21464;&#20307;&#65288;&#20363;&#22914;&#21160;&#24577;&#31209;&#20998;&#37197;&#30340;LoRA&#65289;&#23548;&#33268;&#20102;$1$-best&#25200;&#21160;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of low-rank adaptation (LoRA) with frozen pretrained language models (PLMs) has become increasing popular as a mainstream, resource-efficient modeling approach for memory-constrained hardware. In this study, we first explore how to enhance model performance by introducing various LoRA training strategies, achieving relative word error rate reductions of 3.50\% on the public Librispeech dataset and of 3.67\% on an internal dataset in the messaging domain. To further characterize the stability of LoRA-based second-pass speech recognition models, we examine robustness against input perturbations. These perturbations are rooted in homophone replacements and a novel metric called N-best Perturbation-based Rescoring Robustness (NPRR), both designed to measure the relative degradation in the performance of rescoring models. Our experimental results indicate that while advanced variants of LoRA, such as dynamic rank-allocated LoRA, lead to performance degradation in $1$-best perturbati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#22122;&#22768;&#20449;&#24687;&#20316;&#20026;&#26465;&#20214;&#22120;&#65292;&#24182;&#20174;N-best&#21015;&#34920;&#20013;&#25552;&#21462;&#35821;&#35328;&#31354;&#38388;&#22122;&#22768;&#23884;&#20837;&#65292;&#25945;&#20250;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#22122;&#22768;&#21435;&#38500;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22122;&#22768;&#40065;&#26834;&#35821;&#38899;&#35782;&#21035;&#30340;&#29983;&#25104;&#24335;&#38169;&#35823;&#32416;&#27491;&#65288;GER&#65289;&#12290;</title><link>http://arxiv.org/abs/2401.10446</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#22122;&#22768;&#40065;&#26834;&#35821;&#38899;&#35782;&#21035;&#30340;&#39640;&#25928;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Efficient Learners of Noise-Robust Speech Recognition. (arXiv:2401.10446v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#22122;&#22768;&#20449;&#24687;&#20316;&#20026;&#26465;&#20214;&#22120;&#65292;&#24182;&#20174;N-best&#21015;&#34920;&#20013;&#25552;&#21462;&#35821;&#35328;&#31354;&#38388;&#22122;&#22768;&#23884;&#20837;&#65292;&#25945;&#20250;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#22122;&#22768;&#21435;&#38500;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22122;&#22768;&#40065;&#26834;&#35821;&#38899;&#35782;&#21035;&#30340;&#29983;&#25104;&#24335;&#38169;&#35823;&#32416;&#27491;&#65288;GER&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#20419;&#36827;&#20102;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#30340;&#29983;&#25104;&#24335;&#38169;&#35823;&#32416;&#27491;&#65288;GER&#65289;&#65292;&#21033;&#29992;LLMs&#30340;&#20016;&#23500;&#35821;&#35328;&#30693;&#35782;&#21644;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#26469;&#25913;&#21892;&#35782;&#21035;&#32467;&#26524;&#12290;&#26368;&#26032;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;GER&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#20351;&#29992;HyPoradise&#25968;&#25454;&#38598;&#36890;&#36807;&#39640;&#25928;&#30340;LLM&#24494;&#35843;&#20174;ASR N-best&#20551;&#35774;&#21040;&#22320;&#38754;&#30495;&#23454;&#36716;&#24405;&#30340;&#26144;&#23556;&#65292;&#36825;&#26174;&#31034;&#20986;&#26497;&#22823;&#30340;&#25928;&#26524;&#65292;&#20294;&#22312;&#22122;&#22768;&#40065;&#26834;ASR&#26041;&#38754;&#32570;&#20047;&#20855;&#20307;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#22522;&#20934;&#27979;&#35797;&#25193;&#23637;&#21040;&#22122;&#22768;&#26465;&#20214;&#19979;&#65292;&#24182;&#30740;&#31350;&#26159;&#21542;&#21487;&#20197;&#25945;&#20250;LLMs&#20687;&#22122;&#22768;&#40065;&#26834;ASR&#19968;&#26679;&#25191;&#34892;&#21435;&#22122;&#12290;&#20854;&#20013;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#26159;&#23558;&#22122;&#22768;&#20449;&#24687;&#20316;&#20026;&#26465;&#20214;&#22120;&#24341;&#20837;LLM&#20013;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#20174;&#38899;&#39057;&#32534;&#30721;&#22120;&#20013;&#24341;&#20837;&#22122;&#22768;&#23884;&#20837;&#21487;&#33021;&#20250;&#23545;LLM&#24494;&#35843;&#36896;&#25104;&#25439;&#23475;&#65292;&#22240;&#20026;&#23384;&#22312;&#36328;&#27169;&#24577;&#24046;&#36317;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20174;N-best&#21015;&#34920;&#20013;&#25552;&#21462;&#35821;&#35328;&#31354;&#38388;&#22122;&#22768;&#23884;&#20837;&#26469;&#34920;&#31034;&#28304;&#35821;&#38899;&#30340;&#22122;&#22768;&#26465;&#20214;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in large language models (LLMs) have promoted generative error correction (GER) for automatic speech recognition (ASR), which leverages the rich linguistic knowledge and powerful reasoning ability of LLMs to improve recognition results. The latest work proposes a GER benchmark with HyPoradise dataset to learn the mapping from ASR N-best hypotheses to ground-truth transcription by efficient LLM finetuning, which shows great effectiveness but lacks specificity on noise-robust ASR. In this work, we extend the benchmark to noisy conditions and investigate if we can teach LLMs to perform denoising for GER just like what robust ASR do}, where one solution is introducing noise information as a conditioner into LLM. However, directly incorporating noise embeddings from audio encoder could harm the LLM tuning due to cross-modality gap. To this end, we propose to extract a language-space noise embedding from the N-best list to represent the noise conditions of source speech, whic
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;X-ELM&#30340;&#36328;&#35821;&#35328;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#29420;&#31435;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#23376;&#38598;&#26469;&#20943;&#36731;&#22810;&#35821;&#35328;&#31454;&#20105;&#65292;&#20026;&#22810;&#35821;&#35328;&#22788;&#29702;&#24102;&#26469;&#25552;&#21319;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;X-ELM&#22312;&#21508;&#31181;&#35821;&#35328;&#19978;&#20248;&#20110;&#32852;&#21512;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#19988;&#21487;&#20197;&#36866;&#24212;&#26032;&#35821;&#35328;&#30340;&#36845;&#20195;&#28155;&#21152;&#12290;</title><link>http://arxiv.org/abs/2401.10440</link><description>&lt;p&gt;
&#29992;&#36328;&#35821;&#35328;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#31361;&#30772;&#22810;&#35821;&#35328;&#29615;&#22659;&#30340;&#38590;&#39064;
&lt;/p&gt;
&lt;p&gt;
Breaking the Curse of Multilinguality with Cross-lingual Expert Language Models. (arXiv:2401.10440v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;X-ELM&#30340;&#36328;&#35821;&#35328;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#29420;&#31435;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#23376;&#38598;&#26469;&#20943;&#36731;&#22810;&#35821;&#35328;&#31454;&#20105;&#65292;&#20026;&#22810;&#35821;&#35328;&#22788;&#29702;&#24102;&#26469;&#25552;&#21319;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;X-ELM&#22312;&#21508;&#31181;&#35821;&#35328;&#19978;&#20248;&#20110;&#32852;&#21512;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#19988;&#21487;&#20197;&#36866;&#24212;&#26032;&#35821;&#35328;&#30340;&#36845;&#20195;&#28155;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#33521;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#30001;&#20110;&#27169;&#22411;&#21442;&#25968;&#20043;&#38388;&#30340;&#36328;&#35821;&#35328;&#31454;&#20105;&#65292;&#23427;&#20204;&#24448;&#24448;&#34920;&#29616;&#19981;&#21450;&#21333;&#35821;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36328;&#35821;&#35328;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#65288;X-ELM&#65289;&#65292;&#36890;&#36807;&#23545;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#30340;&#23376;&#38598;&#36827;&#34892;&#29420;&#31435;&#35757;&#32451;&#65292;&#26469;&#20943;&#36731;&#36825;&#31181;&#31454;&#20105;&#12290;&#36825;&#20010;&#36807;&#31243;&#20351;X-ELM&#38024;&#23545;&#19981;&#21516;&#35821;&#35328;&#36827;&#34892;&#19987;&#38376;&#35757;&#32451;&#65292;&#21516;&#26102;&#20316;&#20026;&#19968;&#20010;&#22810;&#35821;&#35328;&#38598;&#21512;&#20445;&#25345;&#26377;&#25928;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#32473;&#23450;&#30456;&#21516;&#35745;&#31639;&#39044;&#31639;&#30340;&#24773;&#20917;&#19979;&#65292;X-ELM&#22312;&#25152;&#26377;&#32771;&#34385;&#30340;&#35821;&#35328;&#19978;&#20248;&#20110;&#32852;&#21512;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#19988;&#36825;&#20123;&#25910;&#30410;&#21487;&#20197;&#36716;&#31227;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#12290;X-ELM&#22312;&#24615;&#33021;&#25913;&#36827;&#26041;&#38754;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#22909;&#22788;&#65306;&#21487;&#20197;&#36845;&#20195;&#22320;&#28155;&#21152;&#26032;&#30340;&#19987;&#23478;&#65292;&#36866;&#24212;&#26032;&#35821;&#35328;&#32780;&#19981;&#20250;&#20135;&#29983;&#28798;&#38590;&#24615;&#30340;&#36951;&#24536;&#12290;&#27492;&#22806;&#65292;&#35757;&#32451;&#26159;&#24322;&#27493;&#36827;&#34892;&#30340;&#65292;&#20943;&#23569;&#20102;&#22810;&#35821;&#35328;&#35757;&#32451;&#30340;&#30828;&#20214;&#35201;&#27714;&#65292;&#23454;&#29616;&#22810;&#35821;&#35328;&#24314;&#27169;&#30340;&#27665;&#20027;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their popularity in non-English NLP, multilingual language models often underperform monolingual ones due to inter-language competition for model parameters. We propose Cross-lingual Expert Language Models (X-ELM), which mitigate this competition by independently training language models on subsets of the multilingual corpus. This process specializes X-ELMs to different languages while remaining effective as a multilingual ensemble. Our experiments show that when given the same compute budget, X-ELM outperforms jointly trained multilingual models across all considered languages and that these gains transfer to downstream tasks. X-ELM provides additional benefits over performance improvements: new experts can be iteratively added, adapting X-ELM to new languages without catastrophic forgetting. Furthermore, training is asynchronous, reducing the hardware requirements for multilingual training and democratizing multilingual modeling.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31185;&#23398;&#25688;&#35201;&#20219;&#21153;&#20013;&#30340;&#21487;&#25511;&#24615;&#12290;&#36890;&#36807;&#25511;&#21046;&#39118;&#26684;&#29305;&#24449;&#65292;&#38750;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#35780;&#35770;&#29983;&#25104;&#20219;&#21153;&#20013;&#20248;&#20110;&#20154;&#31867;&#65292;&#21516;&#26102;&#22522;&#20110;&#20851;&#38190;&#35789;&#30340;&#24341;&#23548;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#30340;&#21487;&#25511;&#24615;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#22312;&#29983;&#25104;&#38271;&#25688;&#35201;&#21644;&#39640;&#24230;&#25277;&#35937;&#30340;&#31616;&#21270;&#25688;&#35201;&#26041;&#38754;&#26377;&#38480;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25688;&#35201;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#36890;&#29992;&#33021;&#21147;&#65292;&#20294;&#22312;&#22797;&#26434;&#25511;&#21046;&#26041;&#38754;&#26377;&#38480;&#12290;</title><link>http://arxiv.org/abs/2401.10415</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25688;&#35201;&#26426;&#33021;&#21542;&#36866;&#24212;&#19981;&#21516;&#31185;&#23398;&#20256;&#25773;&#30446;&#26631;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Model Summarizers Adapt to Diverse Scientific Communication Goals?. (arXiv:2401.10415v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31185;&#23398;&#25688;&#35201;&#20219;&#21153;&#20013;&#30340;&#21487;&#25511;&#24615;&#12290;&#36890;&#36807;&#25511;&#21046;&#39118;&#26684;&#29305;&#24449;&#65292;&#38750;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#35780;&#35770;&#29983;&#25104;&#20219;&#21153;&#20013;&#20248;&#20110;&#20154;&#31867;&#65292;&#21516;&#26102;&#22522;&#20110;&#20851;&#38190;&#35789;&#30340;&#24341;&#23548;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#30340;&#21487;&#25511;&#24615;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#22312;&#29983;&#25104;&#38271;&#25688;&#35201;&#21644;&#39640;&#24230;&#25277;&#35937;&#30340;&#31616;&#21270;&#25688;&#35201;&#26041;&#38754;&#26377;&#38480;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25688;&#35201;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#36890;&#29992;&#33021;&#21147;&#65292;&#20294;&#22312;&#22797;&#26434;&#25511;&#21046;&#26041;&#38754;&#26377;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#22312;&#31185;&#23398;&#25688;&#35201;&#20219;&#21153;&#20013;&#30340;&#21487;&#25511;&#24615;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#34920;&#24449;&#35770;&#25991;&#35780;&#35770;&#12289;&#25688;&#35201;&#21644;&#31616;&#21270;&#25688;&#35201;&#31561;&#19981;&#21516;&#31867;&#22411;&#25688;&#35201;&#30340;&#20851;&#38190;&#39118;&#26684;&#21644;&#20869;&#23481;&#35206;&#30422;&#22240;&#32032;&#12290;&#36890;&#36807;&#25511;&#21046;&#39118;&#26684;&#29305;&#24449;&#65292;&#25105;&#20204;&#21457;&#29616;&#38750;&#24494;&#35843;&#30340;LLMs&#22312;MuP&#35780;&#35770;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#20154;&#31867;&#65292;&#26080;&#35770;&#26159;&#22312;&#19982;&#21442;&#32771;&#25688;&#35201;&#30340;&#30456;&#20284;&#24230;&#36824;&#26159;&#22312;&#20154;&#31867;&#20559;&#22909;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#22522;&#20110;&#20851;&#38190;&#35789;&#30340;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548; (CFG) &#26469;&#25913;&#21892;LLMs&#30340;&#21487;&#25511;&#24615;&#65292;&#22312;arXiv&#21644;PubMed&#19978;&#23454;&#29616;&#19982;&#24378;&#24494;&#35843;&#22522;&#32447;&#30456;&#24403;&#30340;&#35789;&#27719;&#37325;&#21472;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;LLMs&#26080;&#27861;&#19968;&#33268;&#22320;&#29983;&#25104;&#36229;&#36807;8&#20010;&#21477;&#23376;&#30340;&#38271;&#25688;&#35201;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#29983;&#25104;&#39640;&#24230;&#25277;&#35937;&#30340;&#31616;&#21270;&#25688;&#35201;&#26041;&#38754;&#33021;&#21147;&#26377;&#38480;&#12290;&#34429;&#28982;LLMs&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#36890;&#29992;&#25688;&#35201;&#33021;&#21147;&#65292;&#20294;&#22312;&#19981;&#26114;&#36149;&#30340;&#24494;&#35843;&#25514;&#26045;&#19979;&#65292;&#23545;&#20869;&#23481;&#30340;&#22797;&#26434;&#25511;&#21046;&#33021;&#21147;&#26377;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we investigate the controllability of large language models (LLMs) on scientific summarization tasks. We identify key stylistic and content coverage factors that characterize different types of summaries such as paper reviews, abstracts, and lay summaries. By controlling stylistic features, we find that non-fine-tuned LLMs outperform humans in the MuP review generation task, both in terms of similarity to reference summaries and human preferences. Also, we show that we can improve the controllability of LLMs with keyword-based classifier-free guidance (CFG) while achieving lexical overlap comparable to strong fine-tuned baselines on arXiv and PubMed. However, our results also indicate that LLMs cannot consistently generate long summaries with more than 8 sentences. Furthermore, these models exhibit limited capacity to produce highly abstractive lay summaries. Although LLMs demonstrate strong generic summarization competency, sophisticated content control without costly fi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#26694;&#26550;&#26469;&#23398;&#20064;&#39640;&#36136;&#37327;&#21644;&#36890;&#29992;&#24615;&#30340;&#30701;&#35821;&#34920;&#31034;&#12290;&#35813;&#26694;&#26550;&#22312;&#26080;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#30701;&#35821;&#34920;&#31034;&#65292;&#36890;&#36807;&#30701;&#35821;&#31867;&#22411;&#20998;&#31867;&#21644;&#26377;&#25928;&#22320;&#34701;&#21512;&#23383;&#31526;&#32423;&#20449;&#24687;&#26469;&#25552;&#39640;&#34920;&#31034;&#30340;&#31934;&#30830;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;&#27492;&#22806;&#65292;&#36824;&#37319;&#29992;&#20102;&#19977;&#31181;&#31890;&#24230;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#20197;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10407</link><description>&lt;p&gt;
&#23398;&#20064;&#39640;&#36136;&#37327;&#21644;&#36890;&#29992;&#24615;&#30340;&#30701;&#35821;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning High-Quality and General-Purpose Phrase Representations. (arXiv:2401.10407v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#26694;&#26550;&#26469;&#23398;&#20064;&#39640;&#36136;&#37327;&#21644;&#36890;&#29992;&#24615;&#30340;&#30701;&#35821;&#34920;&#31034;&#12290;&#35813;&#26694;&#26550;&#22312;&#26080;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#30701;&#35821;&#34920;&#31034;&#65292;&#36890;&#36807;&#30701;&#35821;&#31867;&#22411;&#20998;&#31867;&#21644;&#26377;&#25928;&#22320;&#34701;&#21512;&#23383;&#31526;&#32423;&#20449;&#24687;&#26469;&#25552;&#39640;&#34920;&#31034;&#30340;&#31934;&#30830;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;&#27492;&#22806;&#65292;&#36824;&#37319;&#29992;&#20102;&#19977;&#31181;&#31890;&#24230;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#20197;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30701;&#35821;&#34920;&#31034;&#22312;&#25968;&#25454;&#31185;&#23398;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#26377;&#21033;&#20110;&#23454;&#20307;&#23545;&#40784;&#12289;&#35760;&#24405;&#38142;&#25509;&#12289;&#27169;&#31946;&#36830;&#25509;&#21644;&#37322;&#20041;&#20998;&#31867;&#31561;&#21508;&#31181;&#20219;&#21153;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20197;&#33719;&#21462;&#30701;&#35821;&#23884;&#20837;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#24050;&#32463;&#21457;&#29616;&#20102;&#38656;&#35201;&#25913;&#36827;&#30340;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#24448;&#24448;&#36807;&#20110;&#22797;&#26434;&#65292;&#24182;&#38656;&#35201;&#22312;&#20855;&#26377;&#19978;&#19979;&#25991;&#21477;&#23376;&#30340;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#20854;&#27425;&#65292;&#21033;&#29992;&#30701;&#35821;&#31867;&#22411;&#21644;&#24418;&#24577;&#32473;&#20986;&#26356;&#31934;&#30830;&#21644;&#26356;&#28789;&#27963;&#30340;&#30701;&#35821;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#26694;&#26550;&#20197;&#20197;&#26080;&#19978;&#19979;&#25991;&#30340;&#26041;&#24335;&#23398;&#20064;&#30701;&#35821;&#34920;&#31034;&#12290;&#35813;&#26694;&#26550;&#23558;&#30701;&#35821;&#31867;&#22411;&#20998;&#31867;&#20316;&#20026;&#36741;&#21161;&#20219;&#21153;&#65292;&#24182;&#26356;&#26377;&#25928;&#22320;&#23558;&#23383;&#31526;&#32423;&#20449;&#24687;&#34701;&#20837;&#30701;&#35821;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#31181;&#31890;&#24230;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Phrase representations play an important role in data science and natural language processing, benefiting various tasks like Entity Alignment, Record Linkage, Fuzzy Joins, and Paraphrase Classification. The current state-of-the-art method involves fine-tuning pre-trained language models for phrasal embeddings using contrastive learning. However, we have identified areas for improvement. First, these pre-trained models tend to be unnecessarily complex and require to be pre-trained on a corpus with context sentences. Second, leveraging the phrase type and morphology gives phrase representations that are both more precise and more flexible. We propose an improved framework to learn phrase representations in a context-free fashion. The framework employs phrase type classification as an auxiliary task and incorporates character-level information more effectively into the phrase representation. Furthermore, we design three granularities of data augmentation to increase the diversity of train
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23545;&#35805;&#31995;&#32479;&#20013;&#19968;&#33268;&#24615;&#38382;&#39064;&#30340;&#35780;&#20272;&#21644;&#22686;&#24378;&#26041;&#27861;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#26469;&#30740;&#31350;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#32452;&#20219;&#21153;&#65292;&#19987;&#27880;&#20110;&#23545;&#35805;&#19968;&#33268;&#24615;&#30340;&#26816;&#27979;&#21644;&#35299;&#20915;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#26174;&#33879;&#22320;&#20419;&#36827;&#20102;&#23545;&#35805;&#31995;&#32479;&#20013;&#19981;&#19968;&#33268;&#24615;&#30340;&#35782;&#21035;&#21644;&#35299;&#20915;&#65292;&#28982;&#32780;&#30446;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#19981;&#19968;&#33268;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#29305;&#28857;&#65292;&#20294;&#22312;&#26816;&#27979;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2401.10353</link><description>&lt;p&gt;
&#19981;&#19968;&#33268;&#30340;&#23545;&#35805;&#22238;&#24212;&#21450;&#20854;&#24674;&#22797;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Inconsistent dialogue responses and how to recover from them. (arXiv:2401.10353v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23545;&#35805;&#31995;&#32479;&#20013;&#19968;&#33268;&#24615;&#38382;&#39064;&#30340;&#35780;&#20272;&#21644;&#22686;&#24378;&#26041;&#27861;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#26469;&#30740;&#31350;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#32452;&#20219;&#21153;&#65292;&#19987;&#27880;&#20110;&#23545;&#35805;&#19968;&#33268;&#24615;&#30340;&#26816;&#27979;&#21644;&#35299;&#20915;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#26174;&#33879;&#22320;&#20419;&#36827;&#20102;&#23545;&#35805;&#31995;&#32479;&#20013;&#19981;&#19968;&#33268;&#24615;&#30340;&#35782;&#21035;&#21644;&#35299;&#20915;&#65292;&#28982;&#32780;&#30446;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#19981;&#19968;&#33268;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#29305;&#28857;&#65292;&#20294;&#22312;&#26816;&#27979;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#31995;&#32479;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#20445;&#25345;&#20854;&#33258;&#36523;&#30340;&#20559;&#22909;&#12289;&#35266;&#28857;&#12289;&#20449;&#24565;&#21644;&#20107;&#23454;&#30340;&#19968;&#33268;&#24615;&#65292;&#36825;&#34987;&#35777;&#26126;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35780;&#20272;&#21644;&#22686;&#24378;&#23545;&#35805;&#31995;&#32479;&#35805;&#35821;&#19968;&#33268;&#24615;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#26469;&#30740;&#31350;&#19981;&#19968;&#33268;&#24615;&#65292;&#20854;&#20013;&#21253;&#25324;&#30001;&#27880;&#37322;&#32773;&#32534;&#20889;&#30340;&#19981;&#19968;&#33268;&#30340;&#23545;&#35805;&#22238;&#24212;&#12289;&#19981;&#19968;&#33268;&#24615;&#30340;&#35299;&#37322;&#21644;&#24674;&#22797;&#35805;&#35821;&#12290;&#36825;&#28085;&#30422;&#20102;&#19981;&#19968;&#33268;&#24615;&#30340;&#29983;&#21629;&#21608;&#26399;&#65292;&#21363;&#24341;&#20837;&#12289;&#29702;&#35299;&#21644;&#35299;&#20915;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#32452;&#20197;&#23545;&#35805;&#19968;&#33268;&#24615;&#20026;&#20013;&#24515;&#30340;&#20219;&#21153;&#65292;&#20855;&#20307;&#20851;&#27880;&#20854;&#26816;&#27979;&#21644;&#35299;&#20915;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#26174;&#33879;&#24110;&#21161;&#20102;&#37492;&#21035;&#21644;&#35299;&#20915;&#23545;&#35805;&#19981;&#19968;&#33268;&#24615;&#30340;&#36827;&#23637;&#65292;&#24182;&#19988;&#30446;&#21069;&#27969;&#34892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#22312;&#35299;&#20915;&#19981;&#19968;&#33268;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#29305;&#28857;&#65292;&#20294;&#22312;&#26816;&#27979;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
One critical issue for chat systems is to stay consistent about preferences, opinions, beliefs and facts of itself, which has been shown a difficult problem. In this work, we study methods to assess and bolster utterance consistency of chat systems. A dataset is first developed for studying the inconsistencies, where inconsistent dialogue responses, explanations of the inconsistencies, and recovery utterances are authored by annotators. This covers the life span of inconsistencies, namely introduction, understanding, and resolution. Building on this, we introduce a set of tasks centered on dialogue consistency, specifically focused on its detection and resolution. Our experimental findings indicate that our dataset significantly helps the progress in identifying and resolving conversational inconsistencies, and current popular large language models like ChatGPT which are good at resolving inconsistencies however still struggle with detection.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;cuDialog&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#25991;&#21270;&#20026;&#35270;&#35282;&#30340;&#23545;&#35805;&#29983;&#25104;&#22522;&#20934;&#65292;&#24182;&#24320;&#21457;&#20102;&#33021;&#22815;&#20174;&#23545;&#35805;&#20013;&#25552;&#21462;&#25991;&#21270;&#23646;&#24615;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#32467;&#21512;&#25991;&#21270;&#20215;&#20540;&#35843;&#26597;&#21487;&#20197;&#25552;&#39640;&#23545;&#35805;&#20195;&#29702;&#30340;&#23545;&#20010;&#24615;&#21270;&#21644;&#23545;&#35805;&#36136;&#37327;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10352</link><description>&lt;p&gt;
&#36890;&#36807;&#25991;&#21270;&#20215;&#20540;&#35843;&#26597;&#65292;&#24357;&#21512;&#23545;&#35805;&#20195;&#29702;&#20013;&#30340;&#25991;&#21270;&#32454;&#24494;&#24046;&#21035;
&lt;/p&gt;
&lt;p&gt;
Bridging Cultural Nuances in Dialogue Agents through Cultural Value Surveys. (arXiv:2401.10352v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10352
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;cuDialog&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#25991;&#21270;&#20026;&#35270;&#35282;&#30340;&#23545;&#35805;&#29983;&#25104;&#22522;&#20934;&#65292;&#24182;&#24320;&#21457;&#20102;&#33021;&#22815;&#20174;&#23545;&#35805;&#20013;&#25552;&#21462;&#25991;&#21270;&#23646;&#24615;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#32467;&#21512;&#25991;&#21270;&#20215;&#20540;&#35843;&#26597;&#21487;&#20197;&#25552;&#39640;&#23545;&#35805;&#20195;&#29702;&#30340;&#23545;&#20010;&#24615;&#21270;&#21644;&#23545;&#35805;&#36136;&#37327;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#20195;&#29702;&#19982;&#25991;&#21270;&#30340;&#20132;&#20114;&#39046;&#22495;&#26159;&#19968;&#20010;&#24341;&#20154;&#27880;&#30446;&#20294;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#30340;&#39046;&#22495;&#12290;&#21508;&#31181;&#31038;&#20250;&#25991;&#21270;&#26041;&#38754;&#65292;&#20174;&#27807;&#36890;&#39118;&#26684;&#21644;&#20449;&#24565;&#21040;&#20849;&#20139;&#30340;&#38544;&#21947;&#21644;&#30693;&#35782;&#65292;&#37117;&#28145;&#21051;&#22320;&#24433;&#21709;&#30528;&#36825;&#20123;&#20132;&#20114;&#12290;&#20026;&#20102;&#26356;&#28145;&#20837;&#22320;&#30740;&#31350;&#36825;&#19968;&#21160;&#24577;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;cuDialog&#65292;&#36825;&#26159;&#19968;&#20010;&#20197;&#25991;&#21270;&#20026;&#35270;&#35282;&#30340;&#23545;&#35805;&#29983;&#25104;&#22522;&#20934;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#33021;&#22815;&#20174;&#23545;&#35805;&#20132;&#27969;&#20013;&#25552;&#21462;&#25991;&#21270;&#23646;&#24615;&#30340;&#22522;&#20934;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#39640;&#23545;&#35805;&#20195;&#29702;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#36136;&#37327;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#20849;&#21516;&#23398;&#20064;&#25991;&#21270;&#29702;&#35299;&#21644;&#22810;&#36718;&#23545;&#35805;&#39044;&#27979;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#25991;&#21270;&#32500;&#24230;&#19982;&#23545;&#35805;&#32534;&#30721;&#29305;&#24449;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21152;&#20837;&#25991;&#21270;&#20215;&#20540;&#35843;&#26597;&#33021;&#22815;&#22686;&#24378;&#19982;&#21442;&#32771;&#25991;&#29486;&#21644;&#25991;&#21270;&#26631;&#35760;&#30340;&#19968;&#33268;&#24615;&#65292;&#26174;&#31034;&#20986;&#23427;&#23545;&#20010;&#24615;&#21270;&#21644;&#23545;&#35805;&#36136;&#37327;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#20419;&#36827;&#23545;&#35805;&#20195;&#29702;&#19982;&#25991;&#21270;&#30340;&#20132;&#20114;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The cultural landscape of interactions with dialogue agents is a compelling yet relatively unexplored territory. It's clear that various sociocultural aspects -- from communication styles and beliefs to shared metaphors and knowledge -- profoundly impact these interactions. To delve deeper into this dynamic, we introduce cuDialog, a first-of-its-kind benchmark for dialogue generation with a cultural lens. We also develop baseline models capable of extracting cultural attributes from dialogue exchanges, with the goal of enhancing the predictive accuracy and quality of dialogue agents. To effectively co-learn cultural understanding and multi-turn dialogue predictions, we propose to incorporate cultural dimensions with dialogue encoding features. Our experimental findings highlight that incorporating cultural value surveys boosts alignment with references and cultural markers, demonstrating its considerable influence on personalization and dialogue quality. To facilitate further explorati
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#30340;&#20302;&#36164;&#28304;&#23433;&#20840;&#25915;&#20987;&#27169;&#24335;&#35782;&#21035;&#21305;&#37197;&#26694;&#26550;&#65292;&#36890;&#36807;&#30452;&#25509;&#35821;&#20041;&#30456;&#20284;&#24230;&#20915;&#23450;&#25991;&#26412;&#19982;&#25915;&#20987;&#27169;&#24335;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#20197;&#38477;&#20302;&#22823;&#37327;&#31867;&#21035;&#12289;&#26631;&#31614;&#20998;&#24067;&#19981;&#22343;&#21644;&#26631;&#31614;&#31354;&#38388;&#22797;&#26434;&#24615;&#24102;&#26469;&#30340;&#23398;&#20064;&#38590;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.10337</link><description>&lt;p&gt;
&#22522;&#20110;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#30340;&#20302;&#36164;&#28304;&#23433;&#20840;&#25915;&#20987;&#27169;&#24335;&#35782;&#21035;&#21305;&#37197;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Noise Contrastive Estimation-based Matching Framework for Low-resource Security Attack Pattern Recognition. (arXiv:2401.10337v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10337
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#30340;&#20302;&#36164;&#28304;&#23433;&#20840;&#25915;&#20987;&#27169;&#24335;&#35782;&#21035;&#21305;&#37197;&#26694;&#26550;&#65292;&#36890;&#36807;&#30452;&#25509;&#35821;&#20041;&#30456;&#20284;&#24230;&#20915;&#23450;&#25991;&#26412;&#19982;&#25915;&#20987;&#27169;&#24335;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#20197;&#38477;&#20302;&#22823;&#37327;&#31867;&#21035;&#12289;&#26631;&#31614;&#20998;&#24067;&#19981;&#22343;&#21644;&#26631;&#31614;&#31354;&#38388;&#22797;&#26434;&#24615;&#24102;&#26469;&#30340;&#23398;&#20064;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25112;&#26415;&#12289;&#25216;&#26415;&#21644;&#31243;&#24207;&#65288;TTPs&#65289;&#26159;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#22797;&#26434;&#30340;&#25915;&#20987;&#27169;&#24335;&#65292;&#22312;&#25991;&#26412;&#30693;&#35782;&#24211;&#20013;&#26377;&#35814;&#32454;&#30340;&#25551;&#36848;&#12290;&#22312;&#32593;&#32476;&#23433;&#20840;&#20889;&#20316;&#20013;&#35782;&#21035;TTPs&#65292;&#36890;&#24120;&#31216;&#20026;TTP&#26144;&#23556;&#65292;&#26159;&#19968;&#20010;&#37325;&#35201;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20256;&#32479;&#30340;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#20197;&#32463;&#20856;&#30340;&#22810;&#31867;&#25110;&#22810;&#26631;&#31614;&#20998;&#31867;&#35774;&#32622;&#20026;&#30446;&#26631;&#12290;&#30001;&#20110;&#23384;&#22312;&#22823;&#37327;&#30340;&#31867;&#21035;&#65288;&#21363;TTPs&#65289;&#65292;&#26631;&#31614;&#20998;&#24067;&#30340;&#19981;&#22343;&#34913;&#21644;&#26631;&#31614;&#31354;&#38388;&#30340;&#22797;&#26434;&#23618;&#27425;&#32467;&#26500;&#65292;&#36825;&#31181;&#35774;&#32622;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#23398;&#20064;&#33539;&#24335;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#23558;&#25991;&#26412;&#19982;TTP&#26631;&#31614;&#20043;&#38388;&#30340;&#30452;&#25509;&#35821;&#20041;&#30456;&#20284;&#24230;&#20915;&#23450;&#20026;&#25991;&#26412;&#20998;&#37197;&#32473;TTP&#26631;&#31614;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#20165;&#20165;&#22312;&#22823;&#22411;&#26631;&#31614;&#31354;&#38388;&#19978;&#31454;&#20105;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#26377;&#25928;&#30340;&#22522;&#20110;&#37319;&#26679;&#30340;&#23398;&#20064;&#27604;&#36739;&#26426;&#21046;&#30340;&#31070;&#32463;&#21305;&#37197;&#26550;&#26500;&#65292;&#20419;&#36827;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tactics, Techniques and Procedures (TTPs) represent sophisticated attack patterns in the cybersecurity domain, described encyclopedically in textual knowledge bases. Identifying TTPs in cybersecurity writing, often called TTP mapping, is an important and challenging task. Conventional learning approaches often target the problem in the classical multi-class or multilabel classification setting. This setting hinders the learning ability of the model due to a large number of classes (i.e., TTPs), the inevitable skewness of the label distribution and the complex hierarchical structure of the label space. We formulate the problem in a different learning paradigm, where the assignment of a text to a TTP label is decided by the direct semantic similarity between the two, thus reducing the complexity of competing solely over the large labeling space. To that end, we propose a neural matching architecture with an effective sampling-based learn-to-compare mechanism, facilitating the learning pr
&lt;/p&gt;</description></item><item><title>DrugAssist&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#20998;&#23376;&#20248;&#21270;&#27169;&#22411;&#65292;&#36890;&#36807;&#20154;&#26426;&#23545;&#35805;&#23454;&#29616;&#20248;&#21270;&#65292;&#21033;&#29992;LLM&#30340;&#24378;&#20132;&#20114;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#21462;&#24471;&#20102;&#39046;&#20808;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.10334</link><description>&lt;p&gt;
DrugAssist&#65306;&#19968;&#20010;&#29992;&#20110;&#20998;&#23376;&#20248;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DrugAssist: A Large Language Model for Molecule Optimization. (arXiv:2401.10334v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10334
&lt;/p&gt;
&lt;p&gt;
DrugAssist&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#20998;&#23376;&#20248;&#21270;&#27169;&#22411;&#65292;&#36890;&#36807;&#20154;&#26426;&#23545;&#35805;&#23454;&#29616;&#20248;&#21270;&#65292;&#21033;&#29992;LLM&#30340;&#24378;&#20132;&#20114;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#21462;&#24471;&#20102;&#39046;&#20808;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#21560;&#24341;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#23581;&#35797;&#23558;LLMs&#24212;&#29992;&#20110;&#33647;&#29289;&#21457;&#29616;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22312;&#33647;&#29289;&#21457;&#29616;&#27969;&#31243;&#20013;&#65292;&#20998;&#23376;&#20248;&#21270;&#26159;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#20294;&#30446;&#21069;LLMs&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#21442;&#19982;&#24456;&#23569;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20165;&#20851;&#27880;&#25429;&#25417;&#25968;&#25454;&#20013;&#25552;&#20379;&#30340;&#21270;&#23398;&#32467;&#26500;&#30340;&#28508;&#22312;&#27169;&#24335;&#65292;&#32780;&#27809;&#26377;&#21033;&#29992;&#19987;&#23478;&#21453;&#39304;&#12290;&#36825;&#20123;&#38750;&#20132;&#20114;&#24335;&#26041;&#27861;&#24573;&#35270;&#20102;&#33647;&#29289;&#21457;&#29616;&#36807;&#31243;&#23454;&#38469;&#19978;&#38656;&#35201;&#19987;&#23478;&#32463;&#39564;&#21644;&#36845;&#20195;&#25913;&#36827;&#30340;&#20107;&#23454;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DrugAssist&#65292;&#19968;&#20010;&#36890;&#36807;&#20154;&#26426;&#23545;&#35805;&#21033;&#29992;LLM&#30340;&#24378;&#20132;&#20114;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#36827;&#34892;&#20998;&#23376;&#20248;&#21270;&#30340;&#20132;&#20114;&#24335;&#27169;&#22411;&#12290;DrugAssist&#22312;&#21333;&#19968;&#21644;&#22810;&#20010;&#24615;&#36136;&#20248;&#21270;&#26041;&#38754;&#21462;&#24471;&#20102;&#39046;&#20808;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the impressive performance of large language models (LLMs) on a wide range of tasks has attracted an increasing number of attempts to apply LLMs in drug discovery. However, molecule optimization, a critical task in the drug discovery pipeline, is currently an area that has seen little involvement from LLMs. Most of existing approaches focus solely on capturing the underlying patterns in chemical structures provided by the data, without taking advantage of expert feedback. These non-interactive approaches overlook the fact that the drug discovery process is actually one that requires the integration of expert experience and iterative refinement. To address this gap, we propose DrugAssist, an interactive molecule optimization model which performs optimization through human-machine dialogue by leveraging LLM's strong interactivity and generalizability. DrugAssist has achieved leading results in both single and multiple property optimization, simultaneously showcasing immense pot
&lt;/p&gt;</description></item><item><title>&#22312;&#20013;&#25991;&#25968;&#25454;&#22788;&#29702;&#20013;&#65292;&#22522;&#20110;&#20195;&#30721;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#32534;&#31243;&#20013;&#25991;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#23588;&#20854;&#26159;&#22312;&#23545;&#20013;&#25991;&#24187;&#35273;&#25935;&#24863;&#30340;&#20219;&#21153;&#20013;&#12290;&#27492;&#30740;&#31350;&#20026;&#35752;&#35770;&#8220;&#20013;&#25991;&#25151;&#38388;&#8221;&#24605;&#24819;&#23454;&#39564;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2401.10286</link><description>&lt;p&gt;
&#20013;&#25991;&#25968;&#25454;&#22788;&#29702;&#20013;&#30340;&#20348;&#20348;&#32773;&#65306;&#33521;&#25991;&#20195;&#30721;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Top in Chinese Data Processing: English Code Models. (arXiv:2401.10286v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10286
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20013;&#25991;&#25968;&#25454;&#22788;&#29702;&#20013;&#65292;&#22522;&#20110;&#20195;&#30721;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#32534;&#31243;&#20013;&#25991;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#23588;&#20854;&#26159;&#22312;&#23545;&#20013;&#25991;&#24187;&#35273;&#25935;&#24863;&#30340;&#20219;&#21153;&#20013;&#12290;&#27492;&#30740;&#31350;&#20026;&#35752;&#35770;&#8220;&#20013;&#25991;&#25151;&#38388;&#8221;&#24605;&#24819;&#23454;&#39564;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#24212;&#29992;&#20013;&#65292;&#20219;&#21153;&#19982;&#35757;&#32451;&#35821;&#26009;&#20043;&#38388;&#30340;&#23545;&#40784;&#26159;&#19968;&#20010;&#22522;&#26412;&#30340;&#20849;&#35782;&#65292;&#20294;&#25105;&#20204;&#30340;&#19968;&#31995;&#21015;&#23454;&#39564;&#21644;&#25105;&#20204;&#35774;&#35745;&#30340;&#35780;&#20272;&#25351;&#26631;&#34920;&#26126;&#65292;&#22522;&#20110;&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#38750;&#32534;&#31243;&#20013;&#25991;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#19982;&#20219;&#21153;&#32039;&#23494;&#21305;&#37197;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#22312;&#23545;&#20013;&#25991;&#24187;&#35273;&#25935;&#24863;&#31243;&#24230;&#36739;&#39640;&#30340;&#20219;&#21153;&#20013;&#65292;&#23637;&#31034;&#36739;&#23569;&#20013;&#25991;&#35821;&#35328;&#29305;&#24449;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#22320;&#29992;&#20195;&#30721;&#27169;&#22411;&#26367;&#25442;&#22522;&#30784;&#27169;&#22411;&#65292;&#22312;&#20013;&#25991;&#25968;&#25454;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;&#22914;&#20026;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;(RAG)&#20934;&#22791;&#25968;&#25454;&#65292;&#24456;&#23481;&#26131;&#24471;&#21040;&#22797;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#35752;&#35770;&#8220;&#20013;&#25991;&#25151;&#38388;&#8221;&#24605;&#24819;&#23454;&#39564;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the alignment between tasks and training corpora is a fundamental consensus in the application of language models, our series of experiments and the metrics we designed reveal that code-based Large Language Models (LLMs) significantly outperform models trained on data that is closely matched to the tasks in non-coding Chinese tasks. Moreover, in tasks high sensitivity to Chinese hallucinations, models exhibiting fewer linguistic features of the Chinese language achieve better performance. Our experimental results can be easily replicated in Chinese data processing tasks, such as preparing data for Retrieval-Augmented Generation (RAG), by simply replacing the base model with a code-based model. Additionally, our research offers a distinct perspective for discussion on the philosophical "Chinese Room" thought experiment.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#31995;&#32479;&#32508;&#36848;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22320;&#29702;&#20301;&#32622;&#23884;&#20837;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#22235;&#31181;&#20027;&#35201;&#30340;&#23884;&#20837;&#20027;&#39064;&#65292;&#24182;&#24378;&#35843;&#20102;&#22312;&#31354;&#38388;&#24418;&#24577;&#21644;&#29983;&#25104;&#27169;&#24577;&#26041;&#38754;&#36827;&#19968;&#27493;&#21457;&#23637;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2401.10279</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#22320;&#29702;&#20301;&#32622;&#23884;&#20837;&#26041;&#27861;&#30340;&#31995;&#32479;&#32508;&#36848;&#65306;&#36808;&#21521;&#31354;&#38388;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
A systematic review of geospatial location embedding approaches in large language models: A path to spatial AI systems. (arXiv:2401.10279v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10279
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#31995;&#32479;&#32508;&#36848;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22320;&#29702;&#20301;&#32622;&#23884;&#20837;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#22235;&#31181;&#20027;&#35201;&#30340;&#23884;&#20837;&#20027;&#39064;&#65292;&#24182;&#24378;&#35843;&#20102;&#22312;&#31354;&#38388;&#24418;&#24577;&#21644;&#29983;&#25104;&#27169;&#24577;&#26041;&#38754;&#36827;&#19968;&#27493;&#21457;&#23637;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#29702;&#20301;&#32622;&#23884;&#20837;&#65288;GLE&#65289;&#24110;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21560;&#25910;&#21644;&#20998;&#26512;&#31354;&#38388;&#25968;&#25454;&#12290;GLE&#22312;&#22320;&#29702;&#20154;&#24037;&#26234;&#33021;&#65288;GeoAI&#65289;&#20013;&#30340;&#20986;&#29616;&#26159;&#30001;&#20110;&#25105;&#20204;&#22797;&#26434;&#24403;&#20195;&#31354;&#38388;&#20013;&#23545;&#26356;&#28145;&#20837;&#30340;&#22320;&#29702;&#35748;&#30693;&#30340;&#38656;&#27714;&#20197;&#21450;LLM&#22312;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#20013;&#25552;&#21462;&#28145;&#23618;&#21547;&#20041;&#30340;&#25104;&#21151;&#12290;&#25105;&#20204;&#22312;Google Scholar&#12289;Science Direct&#21644;arXiv&#19978;&#25628;&#32034;&#20102;&#20851;&#20110;&#22320;&#29702;&#20301;&#32622;&#23884;&#20837;&#21644;LLM&#30340;&#35770;&#25991;&#65292;&#24182;&#23457;&#26597;&#20102;&#30528;&#37325;&#20110;&#36890;&#36807;LLM&#23454;&#29616;&#26356;&#28145;&#20837;&#31354;&#38388;&#8220;&#30693;&#35782;&#8221;&#30340;&#25991;&#31456;&#12290;&#25105;&#20204;&#31579;&#36873;&#20102;304&#20010;&#26631;&#39064;&#12289;30&#20010;&#25688;&#35201;&#21644;18&#31687;&#20840;&#25991;&#35770;&#25991;&#65292;&#25581;&#31034;&#20102;&#22235;&#20010;GLE&#20027;&#39064; - &#23454;&#20307;&#20301;&#32622;&#23884;&#20837;&#65288;ELE&#65289;&#12289;&#25991;&#26723;&#20301;&#32622;&#23884;&#20837;&#65288;DLE&#65289;&#12289;&#24207;&#21015;&#20301;&#32622;&#23884;&#20837;&#65288;SLE&#65289;&#21644;&#20196;&#29260;&#20301;&#32622;&#23884;&#20837;&#65288;TLE&#65289;&#12290;&#32508;&#36848;&#20197;&#34920;&#26684;&#21644;&#21465;&#36848;&#30340;&#24418;&#24335;&#21576;&#29616;&#65292;&#21253;&#25324;&#8220;&#31354;&#38388;&#8221;&#21644;&#8220;LLM&#8221;&#20043;&#38388;&#30340;&#23545;&#35805;&#12290;&#23613;&#31649;GLE&#36890;&#36807;&#21472;&#21152;&#31354;&#38388;&#25968;&#25454;&#26377;&#21161;&#20110;&#29702;&#35299;&#31354;&#38388;&#65292;&#20294;&#24378;&#35843;&#20102;&#22312;&#31354;&#38388;&#24418;&#24577;&#21644;&#29983;&#25104;&#27169;&#24577;&#26041;&#38754;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Geospatial Location Embedding (GLE) helps a Large Language Model (LLM) assimilate and analyze spatial data. GLE emergence in Geospatial Artificial Intelligence (GeoAI) is precipitated by the need for deeper geospatial awareness in our complex contemporary spaces and the success of LLMs in extracting deep meaning in Generative AI. We searched Google Scholar, Science Direct, and arXiv for papers on geospatial location embedding and LLM and reviewed articles focused on gaining deeper spatial "knowing" through LLMs. We screened 304 titles, 30 abstracts, and 18 full-text papers that reveal four GLE themes - Entity Location Embedding (ELE), Document Location Embedding (DLE), Sequence Location Embedding (SLE), and Token Location Embedding (TLE). Synthesis is tabular and narrative, including a dialogic conversation between "Space" and "LLM." Though GLEs aid spatial understanding by superimposing spatial data, they emphasize the need to advance in the intricacies of spatial modalities and gener
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#25512;&#33616;&#27169;&#22411;KGLN&#65292;&#36890;&#36807;&#21512;&#24182;&#33410;&#28857;&#29305;&#24449;&#12289;&#35843;&#25972;&#32858;&#21512;&#26435;&#37325;&#21644;&#36845;&#20195;&#28436;&#21270;&#65292;&#25552;&#39640;&#20102;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;&#12290;&#22312;&#23454;&#39564;&#20013;&#30456;&#23545;&#20110;&#24050;&#26377;&#22522;&#20934;&#26041;&#27861;&#65292;KGLN&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;AUC&#25552;&#39640;&#20102;0.3%&#33267;5.9%&#21644;1.1%&#33267;8.2%&#12290;</title><link>http://arxiv.org/abs/2401.10244</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#39537;&#21160;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#25512;&#33616;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph driven recommendation model of graph neural network. (arXiv:2401.10244v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10244
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#25512;&#33616;&#27169;&#22411;KGLN&#65292;&#36890;&#36807;&#21512;&#24182;&#33410;&#28857;&#29305;&#24449;&#12289;&#35843;&#25972;&#32858;&#21512;&#26435;&#37325;&#21644;&#36845;&#20195;&#28436;&#21270;&#65292;&#25552;&#39640;&#20102;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;&#12290;&#22312;&#23454;&#39564;&#20013;&#30456;&#23545;&#20110;&#24050;&#26377;&#22522;&#20934;&#26041;&#27861;&#65292;KGLN&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;AUC&#25552;&#39640;&#20102;0.3%&#33267;5.9%&#21644;1.1%&#33267;8.2%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25512;&#33616;&#27169;&#22411;KGLN&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;&#12290;&#35813;&#27169;&#22411;&#39318;&#20808;&#21033;&#29992;&#21333;&#23618;&#31070;&#32463;&#32593;&#32476;&#23558;&#22270;&#20013;&#30340;&#20010;&#20307;&#33410;&#28857;&#29305;&#24449;&#21512;&#24182;&#65292;&#28982;&#21518;&#36890;&#36807;&#32467;&#21512;&#24433;&#21709;&#22240;&#32032;&#35843;&#25972;&#30456;&#37051;&#23454;&#20307;&#30340;&#32858;&#21512;&#26435;&#37325;&#12290;&#36890;&#36807;&#36845;&#20195;&#65292;&#27169;&#22411;&#20174;&#21333;&#23618;&#36880;&#28176;&#28436;&#21464;&#20026;&#22810;&#23618;&#65292;&#20351;&#23454;&#20307;&#33021;&#22815;&#33719;&#21462;&#20016;&#23500;&#30340;&#22810;&#38454;&#20851;&#32852;&#23454;&#20307;&#20449;&#24687;&#12290;&#26368;&#21518;&#65292;&#23558;&#23454;&#20307;&#21644;&#29992;&#25143;&#30340;&#29305;&#24449;&#32467;&#21512;&#36215;&#26469;&#20135;&#29983;&#25512;&#33616;&#20998;&#25968;&#12290;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#32858;&#21512;&#26041;&#27861;&#21644;&#24433;&#21709;&#22240;&#32032;&#30340;&#25928;&#26524;&#65292;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22312;&#20351;&#29992;MovieLen-1M&#21644;Book-Crossing&#25968;&#25454;&#38598;&#36827;&#34892;&#27979;&#35797;&#26102;&#65292;KGLN&#30456;&#23545;&#20110;LibFM&#21644;D&#31561;&#24050;&#26377;&#22522;&#20934;&#26041;&#27861;&#65292;AUC&#65288;ROC&#26354;&#32447;&#19979;&#30340;&#38754;&#31215;&#65289;&#25552;&#39640;&#20102;0.3%&#33267;5.9%&#21644;1.1%&#33267;8.2%&#12290;
&lt;/p&gt;
&lt;p&gt;
A new graph neural network-based recommendation model called KGLN, which leverages Knowledge Graph (KG) information, was developed to enhance the accuracy and effectiveness of personalized recommendations. This model begins by using a single-layer neural network to merge individual node features in the graph. It then adjusts the aggregation weights of neighboring entities by incorporating influence factors. The model evolves from a single layer to multiple layers through iteration, enabling entities to access extensive multi-order associated entity information. The final step involves integrating features of entities and users to produce a recommendation score. The model's performance was evaluated by comparing its effects on various aggregation methods and influence factors. In tests using the MovieLen-1M and Book-Crossing datasets, KGLN showed an AUC (Area Under the ROC curve) improvement of 0.3% to 5.9% and 1.1% to 8.2%, respectively, over established benchmark methods like LibFM, D
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#23618;&#38388;&#30456;&#20851;&#20256;&#25773;&#26041;&#27861;&#20043;&#19978;&#20351;&#29992;&#31934;&#32454;&#21270;&#30340;&#20449;&#24687;&#27969;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;Transformer&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#31361;&#20986;&#37325;&#35201;&#20449;&#24687;&#24182;&#28040;&#38500;&#26080;&#20851;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22788;&#29702;&#20998;&#31867;&#21644;&#38382;&#31572;&#20219;&#21153;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#30456;&#27604;&#20854;&#20182;&#20843;&#31181;&#22522;&#32447;&#27169;&#22411;&#26356;&#21152;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2401.09972</link><description>&lt;p&gt;
&#36890;&#36807;&#31361;&#20986;&#37325;&#35201;&#20449;&#24687;&#26469;&#26356;&#22909;&#22320;&#35299;&#37322;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Better Explain Transformers by Illuminating Important Information. (arXiv:2401.09972v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09972
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#23618;&#38388;&#30456;&#20851;&#20256;&#25773;&#26041;&#27861;&#20043;&#19978;&#20351;&#29992;&#31934;&#32454;&#21270;&#30340;&#20449;&#24687;&#27969;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;Transformer&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#31361;&#20986;&#37325;&#35201;&#20449;&#24687;&#24182;&#28040;&#38500;&#26080;&#20851;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22788;&#29702;&#20998;&#31867;&#21644;&#38382;&#31572;&#20219;&#21153;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#30456;&#27604;&#20854;&#20182;&#20843;&#31181;&#22522;&#32447;&#27169;&#22411;&#26356;&#21152;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#21560;&#24341;&#20102;&#26080;&#25968;&#21162;&#21147;&#26469;&#35299;&#37322;&#20854;&#20869;&#37096;&#24037;&#20316;&#21407;&#29702;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#20851;&#27880;&#21407;&#22987;&#26799;&#24230;&#21644;&#27880;&#24847;&#21147;&#26469;&#35299;&#37322;Transformer&#65292;&#23558;&#38750;&#30456;&#20851;&#20449;&#24687;&#36890;&#24120;&#35270;&#20026;&#35299;&#37322;&#35745;&#31639;&#30340;&#19968;&#37096;&#20998;&#65292;&#23548;&#33268;&#32467;&#26524;&#28151;&#20081;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23618;&#38388;&#30456;&#20851;&#20256;&#25773;&#65288;LRP&#65289;&#26041;&#27861;&#20043;&#19978;&#36890;&#36807;&#31934;&#32454;&#21270;&#20449;&#24687;&#27969;&#26469;&#31361;&#20986;&#37325;&#35201;&#20449;&#24687;&#24182;&#28040;&#38500;&#26080;&#20851;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#23558;&#21477;&#27861;&#21644;&#20301;&#32622;&#22836;&#35782;&#21035;&#20026;&#37325;&#35201;&#27880;&#24847;&#21147;&#22836;&#65292;&#24182;&#19987;&#27880;&#20110;&#20174;&#36825;&#20123;&#37325;&#35201;&#22836;&#37096;&#33719;&#24471;&#30340;&#30456;&#20851;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#26080;&#20851;&#20449;&#24687;&#30830;&#23454;&#20250;&#25197;&#26354;&#36755;&#20986;&#30340;&#24402;&#22240;&#20998;&#25968;&#65292;&#22240;&#27492;&#22312;&#35299;&#37322;&#35745;&#31639;&#36807;&#31243;&#20013;&#24212;&#35813;&#23545;&#20854;&#36827;&#34892;&#23631;&#34109;&#12290;&#19982;&#20843;&#31181;&#22522;&#32447;&#27169;&#22411;&#22312;&#20998;&#31867;&#21644;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#27604;&#36739;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#32467;&#26524;&#19978;&#19981;&#26029;&#22320;&#34920;&#29616;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based models excel in various natural language processing (NLP) tasks, attracting countless efforts to explain their inner workings. Prior methods explain Transformers by focusing on the raw gradient and attention as token attribution scores, where non-relevant information is often considered during explanation computation, resulting in confusing results. In this work, we propose highlighting the important information and eliminating irrelevant information by a refined information flow on top of the layer-wise relevance propagation (LRP) method. Specifically, we consider identifying syntactic and positional heads as important attention heads and focus on the relevance obtained from these important heads. Experimental results demonstrate that irrelevant information does distort output attribution scores and then should be masked during explanation computation. Compared to eight baselines on both classification and question-answering datasets, our method consistently outperfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#21453;&#20107;&#23454;&#23545;&#25239;&#20248;&#21270;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#39118;&#26684;&#23545;&#40784;&#65292;&#36991;&#20813;&#20154;&#31867;&#24178;&#39044;&#65292;&#24182;&#25104;&#21151;&#22521;&#20859;&#20986;&#21487;&#21462;&#34892;&#20026;&#21644;&#20943;&#36731;&#19981;&#21487;&#21462;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2401.09566</link><description>&lt;p&gt;
&#20351;&#29992;&#21453;&#20107;&#23454;&#23545;&#25239;&#20248;&#21270;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Aligning Large Language Models with Counterfactual DPO. (arXiv:2401.09566v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#21453;&#20107;&#23454;&#23545;&#25239;&#20248;&#21270;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#39118;&#26684;&#23545;&#40784;&#65292;&#36991;&#20813;&#20154;&#31867;&#24178;&#39044;&#65292;&#24182;&#25104;&#21151;&#22521;&#20859;&#20986;&#21487;&#21462;&#34892;&#20026;&#21644;&#20943;&#36731;&#19981;&#21487;&#21462;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#36827;&#27493;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#29983;&#25104;&#19978;&#19979;&#25991;&#36830;&#36143;&#19988;&#28085;&#30422;&#24191;&#27867;&#20027;&#39064;&#30340;&#25991;&#26412;&#34917;&#20840;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#35757;&#32451;&#25152;&#38656;&#30340;&#22823;&#37327;&#25968;&#25454;&#20351;&#24471;&#22312;&#39044;&#35757;&#32451;&#21644;&#25351;&#20196;&#35843;&#25972;&#38454;&#27573;&#23545;&#40784;&#21709;&#24212;&#39118;&#26684;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#36890;&#24120;&#20250;&#37319;&#29992;&#39069;&#22806;&#30340;&#23545;&#40784;&#38454;&#27573;&#65292;&#36827;&#19968;&#27493;&#20351;&#29992;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#23545;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#26356;&#22909;&#22320;&#23558;&#20854;&#36755;&#20986;&#19982;&#20154;&#31867;&#26399;&#26395;&#23545;&#40784;&#12290;&#34429;&#28982;&#36825;&#20010;&#36807;&#31243;&#26412;&#36523;&#24182;&#27809;&#26377;&#24341;&#20837;&#26032;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#31361;&#20986;&#20102;&#27169;&#22411;&#22266;&#26377;&#30340;&#29983;&#25104;&#39118;&#26684;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;(DPO)&#26694;&#26550;&#20869;&#21033;&#29992;&#21453;&#20107;&#23454;&#25552;&#31034;&#26469;&#23545;&#40784;&#27169;&#22411;&#30340;&#39118;&#26684;&#65292;&#32780;&#19981;&#20381;&#36182;&#20154;&#31867;&#24178;&#39044;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#22320;&#22521;&#20859;&#20102;&#21487;&#21462;&#30340;&#34892;&#20026;&#65292;&#20943;&#36731;&#20102;&#19981;&#21487;&#21462;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advancements in large language models (LLMs) have demonstrated remarkable capabilities across a diverse range of applications. These models excel in generating text completions that are contextually coherent and cover an extensive array of subjects. However, the vast datasets required for their training make aligning response styles during the pretraining and instruction tuning phases challenging. Consequently, an additional alignment phase is typically employed, wherein the model is further trained with human preference data to better align its outputs with human expectations. While this process doesn't introduce new capabilities per se, it does accentuate generation styles innate to the model. This paper explores the utilization of counterfactual prompting within the framework of Direct Preference Optimization (DPO) to align the model's style without relying on human intervention. We demonstrate that this method effectively instils desirable behaviour, mitigates undesirable ones, and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#27133;&#20301;&#26631;&#27880;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#22522;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#38656;&#27714;&#21644;&#35757;&#32451;&#21442;&#25968;&#37327;&#65292;&#24182;&#22312;&#23454;&#38469;&#24037;&#19994;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2401.09343</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#27133;&#20301;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
Efficient slot labelling. (arXiv:2401.09343v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#27133;&#20301;&#26631;&#27880;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#22522;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#38656;&#27714;&#21644;&#35757;&#32451;&#21442;&#25968;&#37327;&#65292;&#24182;&#22312;&#23454;&#38469;&#24037;&#19994;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27133;&#20301;&#26631;&#27880;&#26159;&#23545;&#35805;&#31995;&#32479;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#26088;&#22312;&#22312;&#27599;&#20010;&#29992;&#25143;&#22238;&#21512;&#20013;&#25214;&#21040;&#37325;&#35201;&#30340;&#21442;&#25968;&#12290;&#24120;&#35265;&#30340;&#26041;&#27861;&#21253;&#25324;&#20351;&#29992;BERT&#25110;RoBERTa&#31561;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#20294;&#23427;&#20204;&#38754;&#20020;&#39640;&#35745;&#31639;&#38656;&#27714;&#21644;&#23545;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#20381;&#36182;&#31561;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#20854;&#24615;&#33021;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;PLM&#30340;&#26041;&#27861;&#30456;&#24403;&#25110;&#26356;&#22909;&#65292;&#21516;&#26102;&#21487;&#35757;&#32451;&#21442;&#25968;&#20960;&#20046;&#23569;&#20102;10&#20493;&#12290;&#36825;&#20351;&#24471;&#23427;&#29305;&#21035;&#36866;&#29992;&#20110;&#23454;&#38469;&#24037;&#19994;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Slot labelling is an essential component of any dialogue system, aiming to find important arguments in every user turn. Common approaches involve large pre-trained language models (PLMs) like BERT or RoBERTa, but they face challenges such as high computational requirements and dependence on pre-training data. In this work, we propose a lightweight method which performs on par or better than the state-of-the-art PLM-based methods, while having almost 10x less trainable parameters. This makes it especially applicable for real-life industry scenarios.
&lt;/p&gt;</description></item><item><title>RoTBench&#26159;&#19968;&#20010;&#22810;&#32423;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;LLMs&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#22122;&#22768;&#19979;&#34920;&#29616;&#20986;&#30340;&#31283;&#23450;&#24615;&#38656;&#24471;&#21040;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2401.08326</link><description>&lt;p&gt;
RoTBench: &#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#30340;&#40065;&#26834;&#24615;&#30340;&#22810;&#32423;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
RoTBench: A Multi-Level Benchmark for Evaluating the Robustness of Large Language Models in Tool Learning. (arXiv:2401.08326v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08326
&lt;/p&gt;
&lt;p&gt;
RoTBench&#26159;&#19968;&#20010;&#22810;&#32423;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;LLMs&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#22122;&#22768;&#19979;&#34920;&#29616;&#20986;&#30340;&#31283;&#23450;&#24615;&#38656;&#24471;&#21040;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#20855;&#23398;&#20064;&#20316;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#29289;&#29702;&#19990;&#30028;&#20043;&#38388;&#20114;&#21160;&#30340;&#37325;&#35201;&#25163;&#27573;&#65292;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20852;&#36259;&#12290;&#24403;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#24378;&#35843;LLMs&#22312;&#32467;&#26500;&#33391;&#22909;&#30340;&#29615;&#22659;&#20013;&#21033;&#29992;&#24037;&#20855;&#30340;&#33021;&#21147;&#65292;&#20294;&#24573;&#35270;&#20102;&#23427;&#20204;&#22312;&#38754;&#23545;&#30495;&#23454;&#19990;&#30028;&#20013;&#19981;&#21487;&#36991;&#20813;&#30340;&#22122;&#22768;&#26102;&#30340;&#31283;&#23450;&#24615;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;RoTBench&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#40065;&#26834;&#24615;&#30340;&#22810;&#32423;&#22522;&#20934;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20116;&#20010;&#22806;&#37096;&#29615;&#22659;&#65292;&#27599;&#20010;&#29615;&#22659;&#37117;&#20855;&#26377;&#19981;&#21516;&#32423;&#21035;&#30340;&#22122;&#22768;&#65288;&#21363;&#28165;&#27905;&#12289;&#36731;&#24494;&#12289;&#20013;&#31561;&#12289;&#37325;&#24230;&#21644;&#32852;&#21512;&#65289;&#65292;&#23545;&#27169;&#22411;&#22312;&#24037;&#20855;&#36873;&#25321;&#12289;&#21442;&#25968;&#35782;&#21035;&#21644;&#20869;&#23481;&#22635;&#20805;&#19977;&#20010;&#20851;&#38190;&#38454;&#27573;&#30340;&#25239;&#24178;&#25200;&#33021;&#21147;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#20845;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#27169;&#22411;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25552;&#39640;LLMs&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#30340;&#40065;&#26834;&#24615;&#36843;&#22312;&#30473;&#30571;&#12290;&#20363;&#22914;&#65292;&#24403;&#27809;&#26377;&#23454;&#36136;&#24615;&#30340;&#22122;&#22768;&#23384;&#22312;&#26102;&#65292;GPT-4&#30340;&#24615;&#33021;&#29978;&#33267;&#20174;80.00&#19979;&#38477;&#21040;58.10&#12290;
&lt;/p&gt;
&lt;p&gt;
Tool learning has generated widespread interest as a vital means of interaction between Large Language Models (LLMs) and the physical world. Current research predominantly emphasizes LLMs' capacity to utilize tools in well-structured environments while overlooking their stability when confronted with the inevitable noise of the real world. To bridge this gap, we introduce RoTBench, a multi-level benchmark for evaluating the robustness of LLMs in tool learning. Specifically, we establish five external environments, each featuring varying levels of noise (i.e., Clean, Slight, Medium, Heavy, and Union), providing an in-depth analysis of the model's resilience across three critical phases: tool selection, parameter identification, and content filling. Experiments involving six widely-used models underscore the urgent necessity for enhancing the robustness of LLMs in tool learning. For instance, the performance of GPT-4 even drops significantly from 80.00 to 58.10 when there is no substanti
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20915;&#23450;&#32473;&#23450;&#35821;&#35328;&#26159;&#21542;&#26159;&#23450;&#21521;&#35821;&#35328;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#23450;&#21521;&#24615;&#38382;&#39064;&#22312;&#27491;&#21017;&#35821;&#35328;&#21644;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#35328;&#20013;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.07106</link><description>&lt;p&gt;
&#23450;&#21521;&#30340;&#27491;&#21017;&#21644;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Directed Regular and Context-Free Languages. (arXiv:2401.07106v2 [cs.FL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07106
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20915;&#23450;&#32473;&#23450;&#35821;&#35328;&#26159;&#21542;&#26159;&#23450;&#21521;&#35821;&#35328;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#23450;&#21521;&#24615;&#38382;&#39064;&#22312;&#27491;&#21017;&#35821;&#35328;&#21644;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#35328;&#20013;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20915;&#23450;&#32473;&#23450;&#35821;&#35328;&#26159;&#21542;&#26159;&#23450;&#21521;&#35821;&#35328;&#30340;&#38382;&#39064;&#12290;&#19968;&#20010;&#35821;&#35328;L&#26159;\emph{&#23450;&#21521;}&#30340;&#65292;&#22914;&#26524;L&#20013;&#30340;&#27599;&#23545;&#21333;&#35789;&#37117;&#26377;&#19968;&#20010;&#20849;&#21516;&#30340;&#65288;&#25955;&#20081;&#30340;&#65289;&#36229;&#32423;&#35789;&#22312;L&#20013;&#12290;&#20915;&#23450;&#23450;&#21521;&#24615;&#26159;&#19982;&#21521;&#19979;&#23553;&#38381;&#38598;&#21512;&#30340;&#29702;&#24819;&#20998;&#35299;&#30456;&#20851;&#30340;&#22522;&#26412;&#38382;&#39064;&#12290;&#21478;&#19968;&#20010;&#21160;&#26426;&#26159;&#20915;&#23450;&#20004;&#20010;\emph{&#23450;&#21521;&#30340;}&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#35328;&#26159;&#21542;&#20855;&#26377;&#30456;&#21516;&#30340;&#21521;&#19979;&#38381;&#21253;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#20915;&#23450;&#65292;&#32780;&#23545;&#20110;&#19968;&#33324;&#30340;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#35328;&#65292;&#36825;&#20010;&#38382;&#39064;&#34987;&#31216;&#20026;coNEXP-complete&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20316;&#20026;NFAs&#32473;&#20986;&#30340;&#27491;&#21017;&#35821;&#35328;&#30340;&#23450;&#21521;&#24615;&#38382;&#39064;&#23646;&#20110;$AC^1$&#65292;&#22240;&#27492;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#22266;&#23450;&#23383;&#27597;&#34920;&#22823;&#23567;&#65292;&#23427;&#26159;NL-complete&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#23545;&#20110;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#35328;&#65292;&#23450;&#21521;&#24615;&#38382;&#39064;&#26159;PSPACE-complete&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of deciding whether a given language is directed. A language $L$ is \emph{directed} if every pair of words in $L$ have a common (scattered) superword in $L$. Deciding directedness is a fundamental problem in connection with ideal decompositions of downward closed sets. Another motivation is that deciding whether two \emph{directed} context-free languages have the same downward closures can be decided in polynomial time, whereas for general context-free languages, this problem is known to be coNEXP-complete.  We show that the directedness problem for regular languages, given as NFAs, belongs to $AC^1$, and thus polynomial time. Moreover, it is NL-complete for fixed alphabet sizes. Furthermore, we show that for context-free languages, the directedness problem is PSPACE-complete.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;INACIA&#31995;&#32479;&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#21040;&#24052;&#35199;&#23457;&#35745;&#27861;&#38498;&#20013;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#33258;&#21160;&#21270;&#26696;&#20214;&#20998;&#26512;&#30340;&#21508;&#20010;&#38454;&#27573;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20174;&#26696;&#20214;&#25991;&#20214;&#20013;&#25552;&#21462;&#20449;&#24687;&#12289;&#35780;&#20272;&#21512;&#27861;&#24615;&#21644;&#29983;&#25104;&#21496;&#27861;&#24314;&#35758;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.05273</link><description>&lt;p&gt;
INACIA&#65306;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#21040;&#24052;&#35199;&#23457;&#35745;&#27861;&#38498;&#20013;&#30340;&#26426;&#20250;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
INACIA: Integrating Large Language Models in Brazilian Audit Courts: Opportunities and Challenges. (arXiv:2401.05273v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;INACIA&#31995;&#32479;&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#21040;&#24052;&#35199;&#23457;&#35745;&#27861;&#38498;&#20013;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#33258;&#21160;&#21270;&#26696;&#20214;&#20998;&#26512;&#30340;&#21508;&#20010;&#38454;&#27573;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20174;&#26696;&#20214;&#25991;&#20214;&#20013;&#25552;&#21462;&#20449;&#24687;&#12289;&#35780;&#20272;&#21512;&#27861;&#24615;&#21644;&#29983;&#25104;&#21496;&#27861;&#24314;&#35758;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;INACIA&#65288;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#36741;&#21161;&#25351;&#20196;&#31995;&#32479;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#21019;&#24615;&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25972;&#21512;&#21040;&#24052;&#35199;&#32852;&#37030;&#23457;&#35745;&#27861;&#38498;&#65288;TCU&#65289;&#30340;&#36816;&#33829;&#26694;&#26550;&#20013;&#12290;&#35813;&#31995;&#32479;&#33258;&#21160;&#21270;&#20102;&#26696;&#20214;&#20998;&#26512;&#30340;&#21508;&#20010;&#38454;&#27573;&#65292;&#21253;&#25324;&#22522;&#26412;&#20449;&#24687;&#25552;&#21462;&#12289;&#21487;&#21463;&#29702;&#24615;&#23457;&#26597;&#12289;Periculum in mora&#21644;Fumus boni iuris&#20998;&#26512;&#20197;&#21450;&#24314;&#35758;&#29983;&#25104;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;INACIA&#20174;&#26696;&#20214;&#25991;&#20214;&#20013;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#12289;&#35780;&#20272;&#20854;&#21512;&#27861;&#24615;&#24182;&#29983;&#25104;&#21496;&#27861;&#24314;&#35758;&#30340;&#28508;&#21147;&#12290;&#21033;&#29992;&#39564;&#35777;&#25968;&#25454;&#38598;&#21644;LLMs&#65292;&#25105;&#20204;&#30340;&#35780;&#20272;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#31995;&#32479;&#24615;&#33021;&#65292;&#19982;&#20154;&#31867;&#21028;&#26029;&#39640;&#24230;&#30456;&#20851;&#12290;&#32467;&#26524;&#31361;&#26174;&#20102;INACIA&#22788;&#29702;&#22797;&#26434;&#27861;&#24459;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#34920;&#26126;&#20854;&#36866;&#29992;&#20110;&#22686;&#21152;&#27861;&#24459;&#31995;&#32479;&#30340;&#25928;&#29575;&#21644;&#21496;&#27861;&#20844;&#27491;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces INACIA (Instru\c{c}\~ao Assistida com Intelig\^encia Artificial), a groundbreaking system designed to integrate Large Language Models (LLMs) into the operational framework of Brazilian Federal Court of Accounts (TCU). The system automates various stages of case analysis, including basic information extraction, admissibility examination, Periculum in mora and Fumus boni iuris analyses, and recommendations generation. Through a series of experiments, we demonstrate INACIA's potential in extracting relevant information from case documents, evaluating its legal plausibility, and generating judicial recommendations. Utilizing a validation dataset alongside LLMs, our evaluation methodology presents an innovative approach to assessing system performance, correlating highly with human judgment. The results highlight INACIA's proficiency in handling complex legal tasks, indicating its suitability for augmenting efficiency and judicial fairness within legal systems. The pap
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;Chain-of-Table&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#38142;&#20013;&#20351;&#29992;&#34920;&#26684;&#25968;&#25454;&#20316;&#20026;&#20013;&#38388;&#24605;&#32500;&#30340;&#20195;&#29702;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#26684;&#29702;&#35299;&#20219;&#21153;&#20013;&#36827;&#34892;&#25512;&#29702;&#65292;&#23454;&#29616;&#20102;&#21160;&#24577;&#28436;&#21270;&#30340;&#34920;&#26684;&#25512;&#29702;&#38142;&#12290;</title><link>http://arxiv.org/abs/2401.04398</link><description>&lt;p&gt;
Chain-of-Table: &#22312;&#25512;&#29702;&#38142;&#20013;&#28436;&#21270;&#34920;&#26684;&#29992;&#20110;&#34920;&#26684;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding. (arXiv:2401.04398v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04398
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;Chain-of-Table&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#38142;&#20013;&#20351;&#29992;&#34920;&#26684;&#25968;&#25454;&#20316;&#20026;&#20013;&#38388;&#24605;&#32500;&#30340;&#20195;&#29702;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#26684;&#29702;&#35299;&#20219;&#21153;&#20013;&#36827;&#34892;&#25512;&#29702;&#65292;&#23454;&#29616;&#20102;&#21160;&#24577;&#28436;&#21270;&#30340;&#34920;&#26684;&#25512;&#29702;&#38142;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34920;&#26684;&#25512;&#29702;&#26159;&#35299;&#20915;&#35768;&#22810;&#34920;&#26684;&#29702;&#35299;&#20219;&#21153;&#65288;&#22914;&#22522;&#20110;&#34920;&#26684;&#30340;&#38382;&#31572;&#21644;&#20107;&#23454;&#39564;&#35777;&#65289;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;&#19982;&#36890;&#24120;&#30340;&#25512;&#29702;&#30456;&#27604;&#65292;&#22522;&#20110;&#34920;&#26684;&#30340;&#25512;&#29702;&#38656;&#35201;&#20174;&#33258;&#30001;&#24418;&#24335;&#38382;&#39064;&#21644;&#21322;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#20013;&#25552;&#21462;&#28508;&#22312;&#35821;&#20041;&#12290;Chain-of-Thought&#21450;&#20854;&#31867;&#20284;&#26041;&#27861;&#23558;&#25512;&#29702;&#38142;&#20197;&#25991;&#26412;&#19978;&#19979;&#25991;&#30340;&#24418;&#24335;&#32435;&#20837;&#20854;&#20013;&#65292;&#20294;&#22914;&#20309;&#26377;&#25928;&#21033;&#29992;&#34920;&#26684;&#25968;&#25454;&#22312;&#25512;&#29702;&#38142;&#20013;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Chain-of-Table&#26694;&#26550;&#65292;&#20854;&#20013;&#34920;&#26684;&#25968;&#25454;&#20197;&#20316;&#20026;&#20013;&#38388;&#24605;&#32500;&#30340;&#20195;&#29702;&#26126;&#30830;&#22320;&#29992;&#20110;&#25512;&#29702;&#38142;&#20013;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#25351;&#23548;LLMs&#26469;&#36845;&#20195;&#29983;&#25104;&#25805;&#20316;&#24182;&#26356;&#26032;&#34920;&#26684;&#65292;&#20197;&#20195;&#34920;&#19968;&#20010;&#34920;&#26684;&#25512;&#29702;&#38142;&#12290;&#22240;&#27492;&#65292;LLMs&#21487;&#20197;&#26681;&#25454;&#20043;&#21069;&#25805;&#20316;&#30340;&#32467;&#26524;&#21160;&#24577;&#22320;&#35268;&#21010;&#19979;&#19968;&#20010;&#25805;&#20316;&#12290;&#36825;&#31181;&#34920;&#26684;&#30340;&#25345;&#32493;&#28436;&#21270;&#24418;&#25104;&#20102;&#19968;&#20010;&#38142;&#12290;
&lt;/p&gt;
&lt;p&gt;
Table-based reasoning with large language models (LLMs) is a promising direction to tackle many table understanding tasks, such as table-based question answering and fact verification. Compared with generic reasoning, table-based reasoning requires the extraction of underlying semantics from both free-form questions and semi-structured tabular data. Chain-of-Thought and its similar approaches incorporate the reasoning chain in the form of textual context, but it is still an open question how to effectively leverage tabular data in the reasoning chain. We propose the Chain-of-Table framework, where tabular data is explicitly used in the reasoning chain as a proxy for intermediate thoughts. Specifically, we guide LLMs using in-context learning to iteratively generate operations and update the table to represent a tabular reasoning chain. LLMs can therefore dynamically plan the next operation based on the results of the previous ones. This continuous evolution of the table forms a chain, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21482;&#29992;&#21512;&#25104;&#25968;&#25454;&#21644;&#23569;&#37327;&#35757;&#32451;&#27493;&#39588;&#33719;&#21462;&#39640;&#36136;&#37327;&#25991;&#26412;&#23884;&#20837;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#27809;&#26377;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#31454;&#20105;&#28608;&#28872;&#30340;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.00368</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25913;&#21892;&#25991;&#26412;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Improving Text Embeddings with Large Language Models. (arXiv:2401.00368v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21482;&#29992;&#21512;&#25104;&#25968;&#25454;&#21644;&#23569;&#37327;&#35757;&#32451;&#27493;&#39588;&#33719;&#21462;&#39640;&#36136;&#37327;&#25991;&#26412;&#23884;&#20837;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#27809;&#26377;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#31454;&#20105;&#28608;&#28872;&#30340;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#20165;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#21644;&#23569;&#20110;1k&#20010;&#35757;&#32451;&#27493;&#39588;&#21363;&#21487;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#20381;&#36182;&#22810;&#38454;&#27573;&#20013;&#38388;&#39044;&#35757;&#32451;&#65292;&#20351;&#29992;&#25968;&#21313;&#20159;&#20010;&#24369;&#30417;&#30563;&#25991;&#26412;&#23545;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#20877;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#26500;&#24314;&#22797;&#26434;&#30340;&#35757;&#32451;&#27969;&#31243;&#65292;&#20063;&#19981;&#20381;&#36182;&#20110;&#36890;&#24120;&#21463;&#20219;&#21153;&#22810;&#26679;&#24615;&#21644;&#35821;&#35328;&#35206;&#30422;&#33539;&#22260;&#38480;&#21046;&#30340;&#25163;&#21160;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#21033;&#29992;&#19987;&#26377;&#30340;LLM&#26469;&#20026;&#36817;100&#31181;&#35821;&#35328;&#30340;&#25968;&#21313;&#19975;&#20010;&#25991;&#26412;&#23884;&#20837;&#20219;&#21153;&#29983;&#25104;&#22810;&#26679;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#26631;&#20934;&#30340;&#23545;&#27604;&#25439;&#22833;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#24494;&#35843;&#24320;&#28304;&#30340;&#21482;&#26377;&#35299;&#30721;&#22120;&#30340;LLM&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#31454;&#20105;&#28608;&#28872;&#30340;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#27809;&#26377;&#20351;&#29992;&#20219;&#20309;&#26631;&#35760;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#24403;&#19982;&#21512;&#25104;&#25968;&#25454;&#21644;&#26631;&#35760;&#25968;&#25454;&#30340;&#28151;&#21512;&#36827;&#34892;&#24494;&#35843;&#26102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21019;&#36896;&#20102;&#26032;&#30340;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a novel and simple method for obtaining high-quality text embeddings using only synthetic data and less than 1k training steps. Unlike existing methods that often depend on multi-stage intermediate pre-training with billions of weakly-supervised text pairs, followed by fine-tuning with a few labeled datasets, our method does not require building complex training pipelines or relying on manually collected datasets that are often constrained by task diversity and language coverage. We leverage proprietary LLMs to generate diverse synthetic data for hundreds of thousands of text embedding tasks across nearly 100 languages. We then fine-tune open-source decoder-only LLMs on the synthetic data using standard contrastive loss. Experiments demonstrate that our method achieves strong performance on highly competitive text embedding benchmarks without using any labeled data. Furthermore, when fine-tuned with a mixture of synthetic and labeled data, our model sets new
&lt;/p&gt;</description></item><item><title>KnowledgeNavigator&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#26816;&#32034;&#21644;&#36807;&#28388;&#22806;&#37096;&#30693;&#35782;&#26469;&#35299;&#20915;LLM&#22312;&#38271;&#36923;&#36753;&#38142;&#21644;&#22797;&#26434;&#25512;&#29702;&#22330;&#26223;&#20013;&#30340;&#30693;&#35782;&#38480;&#21046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2312.15880</link><description>&lt;p&gt;
KnowledgeNavigator: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
KnowledgeNavigator: Leveraging Large Language Models for Enhanced Reasoning over Knowledge Graph. (arXiv:2312.15880v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15880
&lt;/p&gt;
&lt;p&gt;
KnowledgeNavigator&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#26816;&#32034;&#21644;&#36807;&#28388;&#22806;&#37096;&#30693;&#35782;&#26469;&#35299;&#20915;LLM&#22312;&#38271;&#36923;&#36753;&#38142;&#21644;&#22797;&#26434;&#25512;&#29702;&#22330;&#26223;&#20013;&#30340;&#30693;&#35782;&#38480;&#21046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20197;&#20854;&#24378;&#22823;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#38646;-shot&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#34920;&#29616;&#65292;&#20294;LLM&#20173;&#28982;&#38754;&#20020;&#30693;&#35782;&#38480;&#21046;&#30340;&#38382;&#39064;&#12290;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#38271;&#36923;&#36753;&#38142;&#25110;&#22797;&#26434;&#25512;&#29702;&#30340;&#22330;&#26223;&#20013;&#65292;LLM&#30340;&#24187;&#24819;&#21644;&#30693;&#35782;&#38480;&#21046;&#38480;&#21046;&#20102;&#23427;&#22312;&#38382;&#31572;&#20013;&#30340;&#34920;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;KnowledgeNavigator&#65292;&#36890;&#36807;&#39640;&#25928;&#20934;&#30830;&#22320;&#26816;&#32034;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22806;&#37096;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#22686;&#24378;LLM&#25512;&#29702;&#30340;&#20851;&#38190;&#22240;&#32032;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;KnowledgeNavigator&#39318;&#20808;&#25366;&#25496;&#21644;&#22686;&#24378;&#32473;&#23450;&#38382;&#39064;&#30340;&#28508;&#22312;&#32422;&#26463;&#20197;&#24341;&#23548;&#25512;&#29702;&#12290;&#28982;&#21518;&#65292;&#22312;LLM&#21644;&#38382;&#39064;&#30340;&#25351;&#23548;&#19979;&#65292;&#36890;&#36807;&#23545;&#30693;&#35782;&#22270;&#35889;&#30340;&#36845;&#20195;&#25512;&#29702;&#26816;&#32034;&#21644;&#36807;&#28388;&#25903;&#25345;&#22238;&#31572;&#30340;&#22806;&#37096;&#30693;&#35782;&#12290;&#26368;&#21518;&#65292;KnowledgeNavigator&#23558;&#32467;&#26500;&#21270;&#30693;&#35782;&#26500;&#24314;&#25104;&#26377;&#25928;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language model (LLM) has achieved outstanding performance on various downstream tasks with its powerful natural language understanding and zero-shot capability, but LLM still suffers from knowledge limitation. Especially in scenarios that require long logical chains or complex reasoning, the hallucination and knowledge limitation of LLM limit its performance in question answering (QA). In this paper, we propose a novel framework KnowledgeNavigator to address these challenges by efficiently and accurately retrieving external knowledge from knowledge graph and using it as a key factor to enhance LLM reasoning. Specifically, KnowledgeNavigator first mines and enhances the potential constraints of the given question to guide the reasoning. Then it retrieves and filters external knowledge that supports answering through iterative reasoning on knowledge graph with the guidance of LLM and the question. Finally, KnowledgeNavigator constructs the structured knowledge into effective prompt
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#21521;&#37327;&#23884;&#20837;&#21644;&#38750;&#32447;&#24615;&#38477;&#32500;&#26041;&#27861;&#65292;&#21457;&#29616;GPT-2&#19982;UMAP&#30340;&#32467;&#21512;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#31163;&#21644;&#32858;&#31867;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;DistilBERT&#27169;&#22411;&#21487;&#29992;&#20110;&#35782;&#21035;&#24635;&#32479;&#21644;&#28436;&#35762;&#30340;&#24180;&#20221;&#12290;</title><link>http://arxiv.org/abs/2312.01185</link><description>&lt;p&gt;
&#26102;&#38388;&#20013;&#30340;&#28063;&#28458;&#65306;&#32654;&#22269;&#21382;&#21490;&#20013;&#30340;&#19981;&#36830;&#32493;&#24615;
&lt;/p&gt;
&lt;p&gt;
A ripple in time: a discontinuity in American history. (arXiv:2312.01185v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.01185
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#21521;&#37327;&#23884;&#20837;&#21644;&#38750;&#32447;&#24615;&#38477;&#32500;&#26041;&#27861;&#65292;&#21457;&#29616;GPT-2&#19982;UMAP&#30340;&#32467;&#21512;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#31163;&#21644;&#32858;&#31867;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;DistilBERT&#27169;&#22411;&#21487;&#29992;&#20110;&#35782;&#21035;&#24635;&#32479;&#21644;&#28436;&#35762;&#30340;&#24180;&#20221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;Kaggle&#30340;&#22269;&#24773;&#21672;&#25991;&#25968;&#25454;&#38598;&#23545;&#32654;&#22269;&#21382;&#21490;&#30340;&#24635;&#20307;&#26102;&#38388;&#32447;&#21450;&#21672;&#25991;&#26412;&#36523;&#30340;&#29305;&#28857;&#21644;&#24615;&#36136;&#36827;&#34892;&#20102;&#19968;&#20123;&#20196;&#20154;&#24778;&#35766;&#65288;&#20063;&#26377;&#20123;&#19981;&#37027;&#20040;&#20196;&#20154;&#24778;&#35766;&#65289;&#30340;&#35266;&#23519;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#26041;&#27861;&#26159;&#20351;&#29992;&#21521;&#37327;&#23884;&#20837;&#65292;&#22914;BERT&#65288;DistilBERT&#65289;&#21644;GPT-2&#12290;&#34429;&#28982;&#24191;&#27867;&#35748;&#20026;BERT&#65288;&#21450;&#20854;&#21464;&#20307;&#65289;&#26368;&#36866;&#21512;NLP&#20998;&#31867;&#20219;&#21153;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;GPT-2&#32467;&#21512;UMAP&#31561;&#38750;&#32447;&#24615;&#38477;&#32500;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#31163;&#21644;&#26356;&#24378;&#30340;&#32858;&#31867;&#25928;&#26524;&#12290;&#36825;&#20351;&#24471;GPT-2 + UMAP&#25104;&#20026;&#19968;&#20010;&#26377;&#36259;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#22312;&#25105;&#20204;&#30340;&#24773;&#20917;&#19979;&#65292;&#19981;&#38656;&#35201;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#39044;&#35757;&#32451;&#30340;GPT-2&#27169;&#22411;&#23601;&#36275;&#22815;&#22909;&#29992;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#20102;&#32463;&#36807;&#24494;&#35843;&#30340;DistilBERT&#27169;&#22411;&#26469;&#26816;&#27979;&#21738;&#20301;&#24635;&#32479;&#21457;&#34920;&#20102;&#21738;&#31687;&#28436;&#35762;&#65292;&#24182;&#21462;&#24471;&#20102;&#38750;&#24120;&#22909;&#30340;&#32467;&#26524;&#65288;&#20934;&#30830;&#29575;&#20026;93\% - 95\%&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#36816;&#34892;&#24773;&#20917;&#65289;&#12290;&#20026;&#20102;&#30830;&#23450;&#20889;&#20316;&#24180;&#20221;&#65292;&#25105;&#20204;&#36824;&#25191;&#34892;&#20102;&#19968;&#20010;&#31867;&#20284;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this note we use the State of the Union Address (SOTU) dataset from Kaggle to make some surprising (and some not so surprising) observations pertaining to the general timeline of American history, and the character and nature of the addresses themselves. Our main approach is using vector embeddings, such as BERT (DistilBERT) and GPT-2.  While it is widely believed that BERT (and its variations) is most suitable for NLP classification tasks, we find out that GPT-2 in conjunction with nonlinear dimension reduction methods such as UMAP provide better separation and stronger clustering. This makes GPT-2 + UMAP an interesting alternative. In our case, no model fine-tuning is required, and the pre-trained out-of-the-box GPT-2 model is enough.  We also used a fine-tuned DistilBERT model for classification detecting which President delivered which address, with very good results (accuracy 93\% - 95\% depending on the run). An analogous task was performed to determine the year of writing, an
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#20004;&#31181;&#19981;&#21516;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;shot prompting&#21644;pattern prompting&#65292;&#35813;&#30740;&#31350;&#25552;&#39640;&#20102;&#33258;&#21160;&#21307;&#23398;&#25253;&#21578;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#20351;&#29992;ROUGE&#20998;&#25968;&#21644;&#20154;&#24037;&#35780;&#20272;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.13274</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;Transformer&#30340;&#25552;&#31034;&#24037;&#31243;&#25552;&#39640;&#33258;&#21160;&#21307;&#30103;&#25253;&#21578;&#20013;&#30340;&#25688;&#35201;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Enhancing Summarization Performance through Transformer-Based Prompt Engineering in Automated Medical Reporting. (arXiv:2311.13274v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.13274
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#20004;&#31181;&#19981;&#21516;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;shot prompting&#21644;pattern prompting&#65292;&#35813;&#30740;&#31350;&#25552;&#39640;&#20102;&#33258;&#21160;&#21307;&#23398;&#25253;&#21578;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#20351;&#29992;ROUGE&#20998;&#25968;&#21644;&#20154;&#24037;&#35780;&#20272;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23450;&#21046;&#21307;&#30103;&#25552;&#31034;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#21307;&#23398;&#23545;&#35805;&#25688;&#35201;&#12290;&#21307;&#23398;&#25253;&#21578;&#30340;&#36807;&#31243;&#24120;&#24120;&#32791;&#26102;&#38271;&#65292;&#23545;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#26469;&#35828;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#36890;&#36807;&#29983;&#25104;&#33258;&#21160;&#21270;&#30340;&#21307;&#23398;&#25253;&#21578;&#65292;&#23454;&#26045;&#21307;&#23398;&#23545;&#35805;&#25688;&#35201;&#25216;&#26415;&#21487;&#20197;&#26377;&#25928;&#32531;&#35299;&#36825;&#20010;&#26102;&#38388;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#21363;shot prompting&#21644;pattern prompting&#65292;&#20197;&#25552;&#39640;&#33258;&#21160;&#21307;&#23398;&#25253;&#21578;&#30340;&#24615;&#33021;&#12290;&#20351;&#29992;ROUGE&#20998;&#25968;&#21644;&#19987;&#23478;&#23567;&#32452;&#30340;&#20154;&#24037;&#35780;&#20272;&#26469;&#35780;&#20272;&#33258;&#21160;&#21307;&#23398;&#25253;&#21578;&#30340;&#25928;&#26524;&#12290;&#20004;&#27425;shot prompting&#26041;&#27861;&#19982;&#33539;&#22260;&#21644;&#39046;&#22495;&#19978;&#19979;&#25991;&#30456;&#32467;&#21512;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#21462;&#24471;&#20102;
&lt;/p&gt;
&lt;p&gt;
Customized medical prompts enable Large Language Models (LLM) to effectively address medical dialogue summarization. The process of medical reporting is often time-consuming for healthcare professionals. Implementing medical dialogue summarization techniques presents a viable solution to alleviate this time constraint by generating automated medical reports. The effectiveness of LLMs in this process is significantly influenced by the formulation of the prompt, which plays a crucial role in determining the quality and relevance of the generated reports. In this research, we used a combination of two distinct prompting strategies, known as shot prompting and pattern prompting to enhance the performance of automated medical reporting. The evaluation of the automated medical reports is carried out using the ROUGE score and a human evaluation with the help of an expert panel. The two-shot prompting approach in combination with scope and domain context outperforms other methods and achieves 
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#23545;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#19982;&#22270;&#32467;&#21512;&#30340;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#21644;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2311.12399</link><description>&lt;p&gt;
&#22270;&#36935;&#19978;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#36827;&#23637;&#19982;&#26410;&#26469;&#26041;&#21521;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Graph Meets Large Language Model: Progress and Future Directions. (arXiv:2311.12399v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.12399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#23545;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#19982;&#22270;&#32467;&#21512;&#30340;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#21644;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#22312;&#34920;&#31034;&#21644;&#20998;&#26512;&#35832;&#22914;&#24341;&#29992;&#32593;&#32476;&#12289;&#31038;&#20132;&#32593;&#32476;&#21644;&#29983;&#29289;&#25968;&#25454;&#31561;&#23454;&#38469;&#24212;&#29992;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#24182;&#19988;&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#22270;&#30456;&#20851;&#20219;&#21153;&#20013;&#65292;&#36229;&#36234;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#30340;&#20256;&#32479;&#26041;&#27861;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#23558;LLMs&#19982;&#22270;&#32467;&#21512;&#30340;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#30340;&#22238;&#39038;&#21644;&#20998;&#26512;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#26681;&#25454;LLMs&#22312;&#22270;&#30456;&#20851;&#20219;&#21153;&#20013;&#25198;&#28436;&#30340;&#35282;&#33394;(&#21363;&#22686;&#24378;&#22120;&#12289;&#39044;&#27979;&#22120;&#21644;&#23545;&#40784;&#32452;&#20214;)&#65292;&#23558;&#29616;&#26377;&#26041;&#27861;&#32452;&#32455;&#20026;&#19977;&#20010;&#31867;&#21035;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#35843;&#26597;&#20102;&#20998;&#31867;&#27861;&#19977;&#20010;&#31867;&#21035;&#20013;&#30340;&#20195;&#34920;&#24615;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#29616;&#26377;&#30740;&#31350;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#31361;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph plays a significant role in representing and analyzing complex relationships in real-world applications such as citation networks, social networks, and biological data. Recently, Large Language Models (LLMs), which have achieved tremendous success in various domains, have also been leveraged in graph-related tasks to surpass traditional Graph Neural Networks (GNNs) based methods and yield state-of-the-art performance. In this survey, we first present a comprehensive review and analysis of existing methods that integrate LLMs with graphs. First of all, we propose a new taxonomy, which organizes existing methods into three categories based on the role (i.e., enhancer, predictor, and alignment component) played by LLMs in graph-related tasks. Then we systematically survey the representative methods along the three categories of the taxonomy. Finally, we discuss the remaining limitations of existing studies and highlight promising avenues for future research. The relevant papers are 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#30417;&#30563;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#29305;&#21035;&#26159;&#25968;&#23398;&#25512;&#29702;&#21644;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#26041;&#38754;&#65292;&#25968;&#25454;&#32452;&#21512;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36739;&#22823;&#27169;&#22411;&#22312;&#30456;&#21516;&#25968;&#25454;&#37327;&#19979;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#22686;&#21152;&#24494;&#35843;&#25968;&#25454;&#21644;&#27169;&#22411;&#21442;&#25968;&#65292;&#25968;&#23398;&#25512;&#29702;&#21644;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#24471;&#21040;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.05492</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23545;&#30417;&#30563;&#24494;&#35843;&#25968;&#25454;&#32452;&#21512;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition. (arXiv:2310.05492v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#30417;&#30563;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#29305;&#21035;&#26159;&#25968;&#23398;&#25512;&#29702;&#21644;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#26041;&#38754;&#65292;&#25968;&#25454;&#32452;&#21512;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36739;&#22823;&#27169;&#22411;&#22312;&#30456;&#21516;&#25968;&#25454;&#37327;&#19979;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#22686;&#21152;&#24494;&#35843;&#25968;&#25454;&#21644;&#27169;&#22411;&#21442;&#25968;&#65292;&#25968;&#23398;&#25512;&#29702;&#21644;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#24471;&#21040;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#22791;&#22823;&#37327;&#30340;&#39044;&#35757;&#32451;&#26631;&#35760;&#21644;&#21442;&#25968;&#65292;&#23637;&#29616;&#20986;&#25968;&#23398;&#25512;&#29702;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#25351;&#20196;&#36319;&#38543;&#31561;&#33021;&#21147;&#12290;&#36825;&#20123;&#33021;&#21147;&#36890;&#36807;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#36827;&#19968;&#27493;&#22686;&#24378;&#12290;&#24320;&#28304;&#31038;&#21306;&#24050;&#32463;&#30740;&#31350;&#20102;&#38024;&#23545;&#27599;&#31181;&#33021;&#21147;&#30340;&#20020;&#26102;SFT&#65292;&#32780;&#19987;&#26377;LLMs&#21487;&#20197;&#36866;&#29992;&#20110;&#25152;&#26377;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#22914;&#20309;&#36890;&#36807;SFT&#35299;&#38145;&#22810;&#37325;&#33021;&#21147;&#21464;&#24471;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;SFT&#36807;&#31243;&#20013;&#25968;&#23398;&#25512;&#29702;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#20154;&#31867;&#23545;&#40784;&#33021;&#21147;&#20043;&#38388;&#30340;&#25968;&#25454;&#32452;&#21512;&#12290;&#20174;&#35268;&#27169;&#30340;&#35282;&#24230;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27169;&#22411;&#33021;&#21147;&#19982;&#21508;&#31181;&#22240;&#32032;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21253;&#25324;&#25968;&#25454;&#37327;&#12289;&#25968;&#25454;&#32452;&#21512;&#27604;&#20363;&#12289;&#27169;&#22411;&#21442;&#25968;&#21644;SFT&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#21457;&#29616;&#19981;&#21516;&#30340;&#33021;&#21147;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#25193;&#23637;&#27169;&#24335;&#65292;&#36739;&#22823;&#30340;&#27169;&#22411;&#36890;&#24120;&#22312;&#30456;&#21516;&#30340;&#25968;&#25454;&#37327;&#19979;&#34920;&#29616;&#20986;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;&#25968;&#23398;&#25512;&#29702;&#21644;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#36890;&#36807;&#24494;&#35843;&#25968;&#25454;&#21644;&#27169;&#22411;&#21442;&#25968;&#30340;&#22686;&#21152;&#32780;&#33719;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) with enormous pre-training tokens and parameter amounts emerge abilities, including math reasoning, code generation, and instruction following. These abilities are further enhanced by supervised fine-tuning (SFT). The open-source community has studied on ad-hoc SFT for each ability, while proprietary LLMs are versatile for all abilities. It is important to investigate how to unlock them with multiple abilities via SFT. In this study, we specifically focus on the data composition between mathematical reasoning, code generation, and general human-aligning abilities during SFT. From a scaling perspective, we investigate the relationship between model abilities and various factors including data amounts, data composition ratio, model parameters, and SFT strategies. Our experiments reveal that different abilities exhibit different scaling patterns, and larger models generally show superior performance with the same amount of data. Mathematical reasoning and code
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25361;&#25112;MultiScript&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#33050;&#26412;&#23398;&#20064;&#26041;&#27861;&#23545;&#20110;&#24320;&#25918;&#39046;&#22495;&#26085;&#24120;&#20219;&#21153;&#30340;&#38480;&#21046;&#12290;&#35770;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#22810;&#27169;&#24335;&#33050;&#26412;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#24212;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.04965</link><description>&lt;p&gt;
MULTISCRIPT: &#22810;&#27169;&#24335;&#33050;&#26412;&#23398;&#20064;&#29992;&#20110;&#25903;&#25345;&#24320;&#25918;&#39046;&#22495;&#30340;&#26085;&#24120;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
MULTISCRIPT: Multimodal Script Learning for Supporting Open Domain Everyday Tasks. (arXiv:2310.04965v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04965
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25361;&#25112;MultiScript&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#33050;&#26412;&#23398;&#20064;&#26041;&#27861;&#23545;&#20110;&#24320;&#25918;&#39046;&#22495;&#26085;&#24120;&#20219;&#21153;&#30340;&#38480;&#21046;&#12290;&#35770;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#22810;&#27169;&#24335;&#33050;&#26412;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#24212;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35270;&#39057;&#28436;&#31034;&#20013;&#33258;&#21160;&#29983;&#25104;&#33050;&#26412;&#65288;&#21363;&#25991;&#26412;&#25551;&#36848;&#30340;&#20851;&#38190;&#27493;&#39588;&#24207;&#21015;&#65289;&#24182;&#25512;&#29702;&#21518;&#32493;&#27493;&#39588;&#23545;&#20110;&#29616;&#20195;AI&#34394;&#25311;&#21161;&#25163;&#26469;&#24341;&#23548;&#20154;&#20204;&#23436;&#25104;&#26085;&#24120;&#20219;&#21153;&#65292;&#23588;&#20854;&#26159;&#38476;&#29983;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#29983;&#25104;&#24335;&#33050;&#26412;&#23398;&#20064;&#26041;&#27861;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#32467;&#26500;&#33391;&#22909;&#30340;&#21069;&#32622;&#27493;&#39588;&#30340;&#25991;&#26412;&#21644;/&#25110;&#22270;&#20687;&#25551;&#36848;&#65292;&#25110;&#32773;&#38480;&#20110;&#29305;&#23450;&#39046;&#22495;&#65292;&#23548;&#33268;&#19982;&#30495;&#23454;&#19990;&#30028;&#20013;&#29992;&#25143;&#22330;&#26223;&#23384;&#22312;&#24046;&#36317;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25361;&#25112;&#8212;&#8212;MultiScript&#65292;&#20854;&#20013;&#21253;&#21547;&#20004;&#20010;&#20851;&#20110;&#38754;&#21521;&#20219;&#21153;&#30340;&#22810;&#27169;&#24335;&#33050;&#26412;&#23398;&#20064;&#30340;&#26032;&#20219;&#21153;&#65306;&#65288;1&#65289;&#22810;&#27169;&#24335;&#33050;&#26412;&#29983;&#25104;&#65292;&#21644;&#65288;2&#65289;&#21518;&#32493;&#27493;&#39588;&#39044;&#27979;&#12290;&#23545;&#20110;&#36825;&#20004;&#20010;&#20219;&#21153;&#65292;&#36755;&#20837;&#21253;&#25324;&#30446;&#26631;&#20219;&#21153;&#21517;&#31216;&#21644;&#28436;&#31034;&#35270;&#39057;&#65292;&#39044;&#26399;&#36755;&#20986;&#20026;&#65288;1&#65289;&#22522;&#20110;&#28436;&#31034;&#35270;&#39057;&#30340;&#32467;&#26500;&#21270;&#27493;&#39588;&#25551;&#36848;&#30340;&#24207;&#21015;&#65292;&#21644;&#65288;2&#65289;&#38024;&#23545;&#27599;&#20010;&#27493;&#39588;&#30340;&#21333;&#19968;&#25991;&#26412;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatically generating scripts (i.e. sequences of key steps described in text) from video demonstrations and reasoning about the subsequent steps are crucial to the modern AI virtual assistants to guide humans to complete everyday tasks, especially unfamiliar ones. However, current methods for generative script learning rely heavily on well-structured preceding steps described in text and/or images or are limited to a certain domain, resulting in a disparity with real-world user scenarios. To address these limitations, we present a new benchmark challenge -- MultiScript, with two new tasks on task-oriented multimodal script learning: (1) multimodal script generation, and (2) subsequent step prediction. For both tasks, the input consists of a target task name and a video illustrating what has been done to complete the target task, and the expected output is (1) a sequence of structured step descriptions in text based on the demonstration video, and (2) a single text description for th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;LLMCarbon&#65292;&#19968;&#20010;&#38024;&#23545;&#23494;&#38598;&#22411;&#21644;MoE LLMs&#35774;&#35745;&#30340;&#31471;&#21040;&#31471;&#30899;&#36275;&#36857;&#39044;&#27979;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#24037;&#20855;&#30340;&#38480;&#21046;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14393</link><description>&lt;p&gt;
LLMCarbon: &#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#30899;&#36275;&#36857;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
LLMCarbon: Modeling the end-to-end Carbon Footprint of Large Language Models. (arXiv:2309.14393v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;LLMCarbon&#65292;&#19968;&#20010;&#38024;&#23545;&#23494;&#38598;&#22411;&#21644;MoE LLMs&#35774;&#35745;&#30340;&#31471;&#21040;&#31471;&#30899;&#36275;&#36857;&#39044;&#27979;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#24037;&#20855;&#30340;&#38480;&#21046;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30899;&#36275;&#36857;&#26159;&#19968;&#20010;&#37325;&#35201;&#20851;&#27880;&#28857;&#65292;&#21253;&#25324;&#23427;&#20204;&#30340;&#35757;&#32451;&#12289;&#25512;&#29702;&#12289;&#23454;&#39564;&#21644;&#23384;&#20648;&#36807;&#31243;&#20013;&#30340;&#25490;&#25918;&#65292;&#21253;&#25324;&#36816;&#33829;&#21644;&#22266;&#23450;&#30899;&#25490;&#25918;&#12290;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#26159;&#22312;LLMs&#35757;&#32451;&#20043;&#21069;&#20934;&#30830;&#20272;&#35745;&#20854;&#30899;&#24433;&#21709;&#65292;&#36825;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;GPU&#30340;&#20351;&#29992;&#12290;&#29616;&#26377;&#30740;&#31350;&#24050;&#25253;&#21578;&#20102;LLMs&#35757;&#32451;&#30340;&#30899;&#36275;&#36857;&#65292;&#20294;&#21482;&#26377;&#19968;&#20010;&#24037;&#20855;mlco2&#33021;&#22815;&#22312;&#23454;&#38469;&#35757;&#32451;&#20043;&#21069;&#39044;&#27979;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#30899;&#36275;&#36857;&#12290;&#28982;&#32780;&#65292;mlco2&#23384;&#22312;&#19968;&#20123;&#20005;&#37325;&#30340;&#38480;&#21046;&#12290;&#23427;&#19981;&#33021;&#25193;&#23637;&#20854;&#23545;&#23494;&#38598;&#25110;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;LLMs&#30340;&#20272;&#35745;&#65292;&#24573;&#35270;&#20102;&#20851;&#38190;&#30340;&#26550;&#26500;&#21442;&#25968;&#65292;&#20165;&#20851;&#27880;GPU&#65292;&#24182;&#19981;&#33021;&#24314;&#27169;&#22266;&#21270;&#30340;&#30899;&#36275;&#36857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LLMCarbon&#65292;&#19968;&#20010;&#20026;&#23494;&#38598;&#22411;&#21644;MoE LLMs&#35774;&#35745;&#30340;&#31471;&#21040;&#31471;&#30899;&#36275;&#36857;&#39044;&#27979;&#27169;&#22411;&#12290;&#19982;mlco2&#30456;&#27604;&#65292;LLMCarbon&#26174;&#33879;&#22686;&#24378;&#20102;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The carbon footprint associated with large language models (LLMs) is a significant concern, encompassing emissions from their training, inference, experimentation, and storage processes, including operational and embodied carbon emissions. An essential aspect is accurately estimating the carbon impact of emerging LLMs even before their training, which heavily relies on GPU usage. Existing studies have reported the carbon footprint of LLM training, but only one tool, mlco2, can predict the carbon footprint of new neural networks prior to physical training. However, mlco2 has several serious limitations. It cannot extend its estimation to dense or mixture-of-experts (MoE) LLMs, disregards critical architectural parameters, focuses solely on GPUs, and cannot model embodied carbon footprints. Addressing these gaps, we introduce \textit{LLMCarbon}, an end-to-end carbon footprint projection model designed for both dense and MoE LLMs. Compared to mlco2, LLMCarbon significantly enhances the ac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#25105;&#24378;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#33258;&#21160;&#29983;&#25104;&#21644;&#35780;&#20272;&#23398;&#29983;&#29983;&#25104;&#30340;&#35299;&#37322;&#65292;&#29992;&#20110;&#25913;&#36827;&#23398;&#29983;&#36164;&#28304;&#20849;&#20139;&#20013;&#23398;&#29983;&#29983;&#25104;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#35299;&#37322;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.10444</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#32034;&#33258;&#25105;&#24378;&#21270;&#20197;&#25913;&#36827;&#23398;&#29983;&#29983;&#25104;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Exploring Self-Reinforcement for Improving Learnersourced Multiple-Choice Question Explanations with Large Language Models. (arXiv:2309.10444v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#25105;&#24378;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#33258;&#21160;&#29983;&#25104;&#21644;&#35780;&#20272;&#23398;&#29983;&#29983;&#25104;&#30340;&#35299;&#37322;&#65292;&#29992;&#20110;&#25913;&#36827;&#23398;&#29983;&#36164;&#28304;&#20849;&#20139;&#20013;&#23398;&#29983;&#29983;&#25104;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#35299;&#37322;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#29983;&#36164;&#28304;&#20849;&#20139;&#28041;&#21450;&#23398;&#29983;&#29983;&#25104;&#21644;&#20998;&#20139;&#23398;&#20064;&#36164;&#28304;&#12290;&#22312;&#23398;&#29983;&#29983;&#25104;&#22810;&#39033;&#36873;&#25321;&#39064;&#26102;&#65292;&#21019;&#24314;&#35299;&#37322;&#26159;&#19968;&#20010;&#20851;&#38190;&#27493;&#39588;&#65292;&#22240;&#20026;&#23427;&#26377;&#21161;&#20110;&#23545;&#30456;&#20851;&#27010;&#24565;&#30340;&#28145;&#20837;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#23398;&#29983;&#24448;&#24448;&#30001;&#20110;&#20027;&#39064;&#29702;&#35299;&#26377;&#38480;&#21644;&#20165;&#20165;&#37325;&#30003;&#38382;&#39064;&#12289;&#24178;&#25200;&#22240;&#32032;&#21644;&#27491;&#30830;&#31572;&#26696;&#30340;&#20542;&#21521;&#32780;&#38590;&#20197;&#32534;&#20889;&#26377;&#25928;&#30340;&#35299;&#37322;&#12290;&#20026;&#20102;&#24110;&#21161;&#25903;&#25745;&#36825;&#20010;&#20219;&#21153;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#25105;&#24378;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#26088;&#22312;&#33258;&#21160;&#29983;&#25104;&#21644;&#35780;&#20272;&#35299;&#37322;&#12290;&#35813;&#26694;&#26550;&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65292;&#29983;&#25104;&#19982;&#23398;&#29983;&#23545;&#40784;&#30340;&#35299;&#37322;&#65292;&#35780;&#20272;&#36825;&#20123;&#35299;&#37322;&#20197;&#30830;&#20445;&#20854;&#36136;&#37327;&#65292;&#24182;&#36845;&#20195;&#22686;&#24378;&#35299;&#37322;&#12290;&#22914;&#26524;&#19968;&#20010;&#35299;&#37322;&#30340;&#35780;&#20272;&#20998;&#25968;&#20302;&#20110;&#23450;&#20041;&#30340;&#38408;&#20540;&#65292;&#26694;&#26550;&#20250;&#36845;&#20195;&#22320;&#20248;&#21270;&#21644;&#37325;&#26032;&#35780;&#20272;&#35299;&#37322;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#27169;&#25311;&#20102;&#19968;&#20010;&#23398;&#29983;&#23398;&#20064;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learnersourcing involves students generating and sharing learning resources with their peers. When learnersourcing multiple-choice questions, creating explanations for the generated questions is a crucial step as it facilitates a deeper understanding of the related concepts. However, it is often difficult for students to craft effective explanations due to limited subject understanding and a tendency to merely restate the question stem, distractors, and correct answer. To help scaffold this task, in this work we propose a self-reinforcement large-language-model framework, with the goal of generating and evaluating explanations automatically. Comprising three modules, the framework generates student-aligned explanations, evaluates these explanations to ensure their quality and iteratively enhances the explanations. If an explanation's evaluation score falls below a defined threshold, the framework iteratively refines and reassesses the explanation. Importantly, our framework emulates th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#32763;&#35793;&#27169;&#22411;&#20013;&#30340;&#23646;&#24615;&#25511;&#21046;&#22120;&#36801;&#31227;&#21040;&#27809;&#26377;&#30417;&#30563;&#25968;&#25454;&#30340;&#35821;&#35328;&#12290;&#36890;&#36807;&#20840;&#38754;&#20998;&#26512;&#19981;&#21516;&#25968;&#25454;&#22330;&#26223;&#19979;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#26102;&#25511;&#21046;&#25216;&#26415;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#38646;&#26679;&#26412;&#24615;&#33021;&#21644;&#39046;&#22495;&#40065;&#26834;&#24615;&#19978;&#30340;&#30456;&#23545;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.08565</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#22810;&#35821;&#35328;&#32763;&#35793;&#27169;&#22411;&#19978;&#30340;&#23646;&#24615;&#25511;&#21046;&#22120;&#33021;&#21542;&#36801;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Transferable are Attribute Controllers on Pretrained Multilingual Translation Models?. (arXiv:2309.08565v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08565
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#32763;&#35793;&#27169;&#22411;&#20013;&#30340;&#23646;&#24615;&#25511;&#21046;&#22120;&#36801;&#31227;&#21040;&#27809;&#26377;&#30417;&#30563;&#25968;&#25454;&#30340;&#35821;&#35328;&#12290;&#36890;&#36807;&#20840;&#38754;&#20998;&#26512;&#19981;&#21516;&#25968;&#25454;&#22330;&#26223;&#19979;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#26102;&#25511;&#21046;&#25216;&#26415;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#38646;&#26679;&#26412;&#24615;&#33021;&#21644;&#39046;&#22495;&#40065;&#26834;&#24615;&#19978;&#30340;&#30456;&#23545;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23558;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#23450;&#21046;&#20026;&#31526;&#21512;&#32454;&#31890;&#24230;&#23646;&#24615;&#65288;&#22914;&#24418;&#24335;&#65289;&#24050;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26041;&#27861;&#22823;&#22810;&#20381;&#36182;&#20110;&#33267;&#23569;&#19968;&#20123;&#24102;&#26377;&#23646;&#24615;&#27880;&#37322;&#30340;&#30417;&#30563;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#25968;&#25454;&#31232;&#32570;&#20173;&#28982;&#26159;&#23558;&#27492;&#23450;&#21046;&#33021;&#21147;&#26222;&#21450;&#21040;&#26356;&#24191;&#27867;&#35821;&#35328;&#33539;&#22260;&#65292;&#23588;&#20854;&#26159;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#19968;&#20010;&#29942;&#39048;&#12290;&#37492;&#20110;&#26368;&#36817;&#22312;&#39044;&#35757;&#32451;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#32763;&#35793;&#27169;&#22411;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#23558;&#23427;&#20204;&#20316;&#20026;&#23545;&#27809;&#26377;&#30417;&#30563;&#25968;&#25454;&#30340;&#35821;&#35328;&#36827;&#34892;&#23646;&#24615;&#25511;&#21046;&#33021;&#21147;&#36801;&#31227;&#30340;&#22522;&#30784;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;NLLB-200&#27169;&#22411;&#23545;&#23646;&#24615;&#25511;&#21046;&#22120;&#30340;&#36801;&#31227;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#21508;&#31181;&#25968;&#25454;&#22330;&#26223;&#19979;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#26102;&#25511;&#21046;&#25216;&#26415;&#65292;&#24182;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#38646;&#26679;&#26412;&#24615;&#33021;&#21644;&#39046;&#22495;&#40065;&#26834;&#24615;&#19978;&#30340;&#30456;&#23545;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;&#25105;&#20204;&#26174;&#31034;&#20986;&#20004;&#31181;&#33539;&#24335;&#26159;&#20114;&#34917;&#30340;&#65292;&#36890;&#36807;&#19968;&#33268;&#30340;&#25913;&#36827;&#26469;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Customizing machine translation models to comply with fine-grained attributes such as formality has seen tremendous progress recently. However, current approaches mostly rely on at least some supervised data with attribute annotation. Data scarcity therefore remains a bottleneck to democratizing such customization possibilities to a wider range of languages, lower-resource ones in particular. Given recent progress in pretrained massively multilingual translation models, we use them as a foundation to transfer the attribute controlling capabilities to languages without supervised data. In this work, we present a comprehensive analysis of transferring attribute controllers based on a pretrained NLLB-200 model. We investigate both training- and inference-time control techniques under various data scenarios, and uncover their relative strengths and weaknesses in zero-shot performance and domain robustness. We show that both paradigms are complementary, as shown by consistent improvements o
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#21457;&#23637;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#25506;&#35752;&#20102;&#20854;&#22312;&#25429;&#25417;&#19978;&#19979;&#25991;&#20449;&#21495;&#21644;&#35821;&#20041;&#32454;&#24494;&#20043;&#22788;&#26041;&#38754;&#30340;&#20248;&#21183;&#21644;&#25361;&#25112;&#65292;&#20197;&#21450;&#19982;&#20256;&#32479;&#26816;&#32034;&#26041;&#27861;&#30340;&#32467;&#21512;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.07107</link><description>&lt;p&gt;
&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Information Retrieval: A Survey. (arXiv:2308.07107v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#21457;&#23637;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#25506;&#35752;&#20102;&#20854;&#22312;&#25429;&#25417;&#19978;&#19979;&#25991;&#20449;&#21495;&#21644;&#35821;&#20041;&#32454;&#24494;&#20043;&#22788;&#26041;&#38754;&#30340;&#20248;&#21183;&#21644;&#25361;&#25112;&#65292;&#20197;&#21450;&#19982;&#20256;&#32479;&#26816;&#32034;&#26041;&#27861;&#30340;&#32467;&#21512;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#20449;&#24687;&#33719;&#21462;&#30340;&#20027;&#35201;&#25163;&#27573;&#65292;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#31995;&#32479;&#65292;&#22914;&#25628;&#32034;&#24341;&#25806;&#65292;&#24050;&#32463;&#34701;&#20837;&#21040;&#25105;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#20013;&#12290;&#36825;&#20123;&#31995;&#32479;&#36824;&#20316;&#20026;&#23545;&#35805;&#12289;&#38382;&#31572;&#21644;&#25512;&#33616;&#31995;&#32479;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;IR&#30340;&#21457;&#23637;&#36712;&#36857;&#20174;&#22522;&#20110;&#35789;&#39033;&#30340;&#26041;&#27861;&#36215;&#27493;&#65292;&#36880;&#28176;&#21457;&#23637;&#25104;&#19982;&#20808;&#36827;&#30340;&#31070;&#32463;&#27169;&#22411;&#30456;&#34701;&#21512;&#12290;&#23613;&#31649;&#31070;&#32463;&#27169;&#22411;&#25797;&#38271;&#25429;&#25417;&#22797;&#26434;&#30340;&#19978;&#19979;&#25991;&#20449;&#21495;&#21644;&#35821;&#20041;&#32454;&#24494;&#20043;&#22788;&#65292;&#20174;&#32780;&#25913;&#21464;&#20102;IR&#30340;&#26684;&#23616;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#38754;&#20020;&#30528;&#25968;&#25454;&#31232;&#32570;&#12289;&#21487;&#35299;&#37322;&#24615;&#20197;&#21450;&#29983;&#25104;&#19978;&#19979;&#25991;&#21512;&#29702;&#20294;&#28508;&#22312;&#19981;&#20934;&#30830;&#21709;&#24212;&#30340;&#25361;&#25112;&#12290;&#36825;&#31181;&#28436;&#21464;&#38656;&#35201;&#20256;&#32479;&#26041;&#27861;&#65288;&#22914;&#22522;&#20110;&#35789;&#39033;&#30340;&#31232;&#30095;&#26816;&#32034;&#26041;&#27861;&#19982;&#24555;&#36895;&#21709;&#24212;&#65289;&#21644;&#29616;&#20195;&#31070;&#32463;&#26550;&#26500;&#65288;&#22914;&#20855;&#26377;&#24378;&#22823;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#65289;&#30340;&#32467;&#21512;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#21644;GPT-4&#30340;&#20986;&#29616;&#65292;&#24341;&#36215;&#20102;&#19968;&#22330;&#38761;&#21629;
&lt;/p&gt;
&lt;p&gt;
As a primary means of information acquisition, information retrieval (IR) systems, such as search engines, have integrated themselves into our daily lives. These systems also serve as components of dialogue, question-answering, and recommender systems. The trajectory of IR has evolved dynamically from its origins in term-based methods to its integration with advanced neural models. While the neural models excel at capturing complex contextual signals and semantic nuances, thereby reshaping the IR landscape, they still face challenges such as data scarcity, interpretability, and the generation of contextually plausible yet potentially inaccurate responses. This evolution requires a combination of both traditional methods (such as term-based sparse retrieval methods with rapid response) and modern neural architectures (such as language models with powerful language understanding capacity). Meanwhile, the emergence of large language models (LLMs), typified by ChatGPT and GPT-4, has revolu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#30446;&#26631;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#33021;&#22815;&#22312;&#24320;&#25918;&#24335;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#34920;&#29616;&#20986;&#33394;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#25351;&#23548;&#35843;&#25972;&#21644;&#38754;&#21521;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#23558;ChatGPT&#33976;&#39311;&#25104;&#26356;&#23567;&#30340;UniversalNER&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UniversalNER&#22312;&#21508;&#31181;&#39046;&#22495;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#20934;&#30830;&#24615;&#65292;&#36229;&#36807;&#20102;&#21407;&#22987;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.03279</link><description>&lt;p&gt;
UniversalNER&#65306;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#30446;&#26631;&#33976;&#39311;&#65292;&#29992;&#20110;&#24320;&#25918;&#24335;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition. (arXiv:2308.03279v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03279
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#30446;&#26631;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#33021;&#22815;&#22312;&#24320;&#25918;&#24335;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#34920;&#29616;&#20986;&#33394;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#25351;&#23548;&#35843;&#25972;&#21644;&#38754;&#21521;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#23558;ChatGPT&#33976;&#39311;&#25104;&#26356;&#23567;&#30340;UniversalNER&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UniversalNER&#22312;&#21508;&#31181;&#39046;&#22495;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#20934;&#30830;&#24615;&#65292;&#36229;&#36807;&#20102;&#21407;&#22987;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20363;&#22914;&#29702;&#35299;&#20219;&#24847;&#23454;&#20307;&#21644;&#20851;&#31995;&#12290;&#25351;&#23548;&#35843;&#25972;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#26377;&#25928;&#22320;&#23558;LLMs&#33976;&#39311;&#25104;&#26356;&#39640;&#25928;&#30340;&#27169;&#22411;&#65292;&#22914;Alpaca&#21644;Vicuna&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#23398;&#29983;&#27169;&#22411;&#22312;&#19979;&#28216;&#24212;&#29992;&#20013;&#20173;&#28982;&#36828;&#36828;&#33853;&#21518;&#20110;&#21407;&#22987;LLMs&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#23450;&#21521;&#33976;&#39311;&#19982;&#38754;&#21521;&#20219;&#21153;&#30340;&#25351;&#23548;&#35843;&#25972;&#65292;&#20197;&#35757;&#32451;&#33021;&#22815;&#22312;&#24320;&#25918;&#24335;&#20449;&#24687;&#25552;&#21462;&#31561;&#24191;&#27867;&#24212;&#29992;&#31867;&#20013;&#34920;&#29616;&#20986;&#33394;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;&#36890;&#36807;&#20197;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;ChatGPT&#33976;&#39311;&#25104;&#26356;&#23567;&#30340;UniversalNER&#27169;&#22411;&#29992;&#20110;&#24320;&#25918;&#24335;NER&#12290;&#20026;&#20102;&#35780;&#20272;&#65292;&#25105;&#20204;&#32452;&#21512;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;NER&#22522;&#20934;&#65292;&#21253;&#25324;9&#20010;&#22810;&#26679;&#39046;&#22495;&#30340;43&#20010;&#25968;&#25454;&#38598;&#65292;&#22914;&#29983;&#29289;&#21307;&#23398;&#12289;&#32534;&#31243;&#12289;&#31038;&#20132;&#23186;&#20307;&#12289;&#27861;&#24459;&#12289;&#37329;&#34701;&#12290;&#22312;&#19981;&#20351;&#29992;&#20219;&#20309;&#30452;&#25509;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#65292;UniversalNER&#22312;&#25968;&#19975;&#31181;&#23454;&#20307;&#31867;&#22411;&#19978;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;NER&#20934;&#30830;&#24615;&#65292;&#36229;&#36807;&#20102;gen&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable generalizability, such as understanding arbitrary entities and relations. Instruction tuning has proven effective for distilling LLMs into more cost-efficient models such as Alpaca and Vicuna. Yet such student models still trail the original LLMs by large margins in downstream applications. In this paper, we explore targeted distillation with mission-focused instruction tuning to train student models that can excel in a broad application class such as open information extraction. Using named entity recognition (NER) for case study, we show how ChatGPT can be distilled into much smaller UniversalNER models for open NER. For evaluation, we assemble the largest NER benchmark to date, comprising 43 datasets across 9 diverse domains such as biomedicine, programming, social media, law, finance. Without using any direct supervision, UniversalNER attains remarkable NER accuracy across tens of thousands of entity types, outperforming gen
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;TransNormerLLM&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#32447;&#24615;&#27880;&#24847;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#22522;&#20110;softmax&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#12290;&#36890;&#36807;&#24341;&#20837;&#20301;&#32622;&#23884;&#20837;&#12289;&#32447;&#24615;&#27880;&#24847;&#21147;&#21152;&#36895;&#12289;&#38376;&#25511;&#26426;&#21046;&#31561;&#20808;&#36827;&#25913;&#36827;&#65292;&#24182;&#21033;&#29992;Lightning Attention&#25216;&#26415;&#21152;&#36895;&#32447;&#24615;&#27880;&#24847;&#21147;&#65292;&#20197;&#21450;&#37319;&#29992;&#24352;&#37327;&#24402;&#19968;&#21270;&#26041;&#26696;&#21152;&#36895;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;TransNormer&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.14995</link><description>&lt;p&gt;
&#23558;TransNormer&#25193;&#23637;&#21040;1750&#20159;&#21442;&#25968;
&lt;/p&gt;
&lt;p&gt;
Scaling TransNormer to 175 Billion Parameters. (arXiv:2307.14995v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14995
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;TransNormerLLM&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#32447;&#24615;&#27880;&#24847;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#22522;&#20110;softmax&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#12290;&#36890;&#36807;&#24341;&#20837;&#20301;&#32622;&#23884;&#20837;&#12289;&#32447;&#24615;&#27880;&#24847;&#21147;&#21152;&#36895;&#12289;&#38376;&#25511;&#26426;&#21046;&#31561;&#20808;&#36827;&#25913;&#36827;&#65292;&#24182;&#21033;&#29992;Lightning Attention&#25216;&#26415;&#21152;&#36895;&#32447;&#24615;&#27880;&#24847;&#21147;&#65292;&#20197;&#21450;&#37319;&#29992;&#24352;&#37327;&#24402;&#19968;&#21270;&#26041;&#26696;&#21152;&#36895;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;TransNormer&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;TransNormerLLM&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#32447;&#24615;&#27880;&#24847;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#20004;&#26041;&#38754;&#37117;&#20248;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;softmax&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#12290;TransNormerLLM&#20174;&#20043;&#21069;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#26550;&#26500;TransNormer&#21457;&#23637;&#32780;&#26469;&#65292;&#36890;&#36807;&#24341;&#20837;&#20301;&#32622;&#23884;&#20837;&#12289;&#32447;&#24615;&#27880;&#24847;&#21147;&#21152;&#36895;&#12289;&#38376;&#25511;&#26426;&#21046;&#12289;&#24352;&#37327;&#24402;&#19968;&#21270;&#12289;&#25512;&#29702;&#21152;&#36895;&#21644;&#31283;&#23450;&#21270;&#31561;&#20808;&#36827;&#25913;&#36827;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;LRPE&#19982;&#25351;&#25968;&#34928;&#20943;&#32467;&#21512;&#65292;&#26082;&#36991;&#20813;&#20102;&#27880;&#24847;&#21147;&#31232;&#37322;&#38382;&#39064;&#65292;&#21448;&#20351;&#27169;&#22411;&#20445;&#30041;&#20102;&#26631;&#35760;&#20043;&#38388;&#30340;&#20840;&#23616;&#20132;&#20114;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Lightning Attention&#65292;&#19968;&#31181;&#20808;&#36827;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#23558;&#32447;&#24615;&#27880;&#24847;&#21147;&#21152;&#36895;&#36229;&#36807;&#20004;&#20493;&#65292;&#24182;&#23558;&#20869;&#23384;&#20351;&#29992;&#37327;&#20943;&#23569;&#20102;&#22235;&#20493;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;TransNormer&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#21033;&#29992;&#38376;&#25511;&#26426;&#21046;&#24179;&#28369;&#35757;&#32451;&#65292;&#24182;&#37319;&#29992;&#26032;&#30340;&#24352;&#37327;&#24402;&#19968;&#21270;&#26041;&#26696;&#21152;&#36895;&#27169;&#22411;&#65292;&#20174;&#32780;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present TransNormerLLM, the first linear attention-based Large Language Model (LLM) that outperforms conventional softmax attention-based models in terms of both accuracy and efficiency. TransNormerLLM evolves from the previous linear attention architecture TransNormer by making advanced modifications that include positional embedding, linear attention acceleration, gating mechanism, tensor normalization, inference acceleration and stabilization. Specifically, we use LRPE together with an exponential decay to avoid attention dilution issues while allowing the model to retain global interactions between tokens. Additionally, we propose Lightning Attention, a cutting-edge technique that accelerates linear attention by more than twice in runtime and reduces memory usage by a remarkable four times. To further enhance the performance of TransNormer, we leverage a gating mechanism to smooth training and a new tensor normalization scheme to accelerate the model, resulting in an impressive 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24320;&#21457;&#39046;&#22495;&#29305;&#23450;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20013;&#25972;&#21512;&#29983;&#25104;&#24335;&#29992;&#25143;&#20307;&#39564;&#30740;&#31350;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#39046;&#22495;&#29992;&#25143;&#32435;&#20837;&#21407;&#22411;&#24320;&#21457;&#30340;&#19981;&#21516;&#38454;&#27573;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#29992;&#25143;&#38656;&#27714;&#21644;&#35780;&#20272;&#29992;&#25143;&#20215;&#20540;&#30340;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.16143</link><description>&lt;p&gt;
&#20026;&#24320;&#21457;&#39046;&#22495;&#29305;&#23450;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#32780;&#36827;&#34892;&#30340;&#29983;&#25104;&#24335;&#29992;&#25143;&#20307;&#39564;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Generative User-Experience Research for Developing Domain-specific Natural Language Processing Applications. (arXiv:2306.16143v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24320;&#21457;&#39046;&#22495;&#29305;&#23450;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20013;&#25972;&#21512;&#29983;&#25104;&#24335;&#29992;&#25143;&#20307;&#39564;&#30740;&#31350;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#39046;&#22495;&#29992;&#25143;&#32435;&#20837;&#21407;&#22411;&#24320;&#21457;&#30340;&#19981;&#21516;&#38454;&#27573;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#29992;&#25143;&#38656;&#27714;&#21644;&#35780;&#20272;&#29992;&#25143;&#20215;&#20540;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#20307;&#39564;&#65288;UX&#65289;&#26159;&#20154;&#26426;&#20132;&#20114;&#65288;HCI&#65289;&#30740;&#31350;&#30340;&#19968;&#37096;&#20998;&#65292;&#19987;&#27880;&#20110;&#25552;&#39640;&#31995;&#32479;&#29992;&#25143;&#30340;&#30452;&#35266;&#24615;&#12289;&#36879;&#26126;&#24230;&#12289;&#31616;&#27905;&#24615;&#21644;&#20449;&#20219;&#24230;&#12290;&#22823;&#22810;&#25968;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;UX&#30740;&#31350;&#37117;&#37319;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#21363;&#27809;&#26377;&#20851;&#27880;&#29992;&#25143;&#38656;&#27714;&#65292;&#24182;&#20165;&#20165;&#23558;&#39046;&#22495;&#29992;&#25143;&#29992;&#20110;&#21487;&#29992;&#24615;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#26356;&#20856;&#22411;&#30340;UX&#26041;&#27861;&#26159;&#20808;&#38024;&#23545;&#29992;&#25143;&#30340;&#21487;&#29992;&#24615;&#36827;&#34892;&#23450;&#21046;&#65292;&#32780;&#19981;&#26159;&#39318;&#20808;&#20102;&#35299;&#29992;&#25143;&#38656;&#27714;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#29983;&#25104;&#24335;UX&#30740;&#31350;&#25972;&#21512;&#21040;&#24320;&#21457;&#39046;&#22495;NLP&#24212;&#29992;&#20013;&#30340;&#26041;&#27861;&#12290;&#29983;&#25104;&#24335;UX&#30740;&#31350;&#23558;&#39046;&#22495;&#29992;&#25143;&#32435;&#20837;&#21407;&#22411;&#24320;&#21457;&#30340;&#21021;&#22987;&#38454;&#27573;&#65292;&#21363;&#26500;&#24605;&#21644;&#27010;&#24565;&#35780;&#20272;&#38454;&#27573;&#65292;&#20197;&#21450;&#26368;&#21518;&#19968;&#38454;&#27573;&#35780;&#20272;&#29992;&#25143;&#20215;&#20540;&#30340;&#21464;&#21270;&#12290;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25253;&#36947;&#20102;&#19968;&#20010;&#38024;&#23545;&#36807;&#31243;&#24037;&#19994;&#20013;&#26085;&#24120;&#25805;&#20316;&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#20041;&#25628;&#32034;&#30340;&#23436;&#25972;&#21407;&#22411;&#24320;&#21457;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
User experience (UX) is a part of human-computer interaction (HCI) research and focuses on increasing intuitiveness, transparency, simplicity, and trust for system users. Most of the UX research for machine learning (ML) or natural language processing (NLP) focuses on a data-driven methodology, i.e., it fails to focus on users' requirements, and engages domain users mainly for usability evaluation. Moreover, more typical UX methods tailor the systems towards user usability, unlike learning about the user needs first. The paper proposes a methodology for integrating generative UX research into developing domain NLP applications. Generative UX research employs domain users at the initial stages of prototype development, i.e., ideation and concept evaluation, and the last stage for evaluating the change in user value. In the case study, we report the full-cycle prototype development of a domain-specific semantic search for daily operations in the process industry. Our case study shows tha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#33258;&#28982;&#39046;&#22495;&#36716;&#31227;&#35774;&#32622;&#19979;&#24494;&#35843;&#21644;&#23567;&#26679;&#26412;&#23398;&#20064;&#27169;&#22411;&#30340;DR&#25361;&#25112;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;DR&#22522;&#20934;&#65292;&#25552;&#20986;&#20102;DR&#25361;&#25112;&#30340;&#20004;&#20010;&#35270;&#35282;&#65306;&#28304;&#22495;&#38477;&#20302;&#65288;SD&#65289;&#21644;&#30446;&#26631;&#22495;&#38477;&#20302;&#65288;TD&#65289;&#65292;&#24182;&#21457;&#29616;&#20004;&#32773;&#20043;&#19968;&#36890;&#24120;&#26159;&#27491;&#20540;&#65292;&#24378;&#35843;&#20102;&#35780;&#20272;DR&#25361;&#25112;&#30340;&#20004;&#20010;&#35270;&#35282;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.00168</link><description>&lt;p&gt;
&#34913;&#37327;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#38754;&#23545;&#39046;&#22495;&#36716;&#31227;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Measuring the Robustness of Natural Language Processing Models to Domain Shifts. (arXiv:2306.00168v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#33258;&#28982;&#39046;&#22495;&#36716;&#31227;&#35774;&#32622;&#19979;&#24494;&#35843;&#21644;&#23567;&#26679;&#26412;&#23398;&#20064;&#27169;&#22411;&#30340;DR&#25361;&#25112;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;DR&#22522;&#20934;&#65292;&#25552;&#20986;&#20102;DR&#25361;&#25112;&#30340;&#20004;&#20010;&#35270;&#35282;&#65306;&#28304;&#22495;&#38477;&#20302;&#65288;SD&#65289;&#21644;&#30446;&#26631;&#22495;&#38477;&#20302;&#65288;TD&#65289;&#65292;&#24182;&#21457;&#29616;&#20004;&#32773;&#20043;&#19968;&#36890;&#24120;&#26159;&#27491;&#20540;&#65292;&#24378;&#35843;&#20102;&#35780;&#20272;DR&#25361;&#25112;&#30340;&#20004;&#20010;&#35270;&#35282;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#24494;&#35843;&#12289;&#23567;&#26679;&#26412;&#23398;&#20064;&#21644;&#38646;&#26679;&#26412;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#27809;&#26377;&#26631;&#35760;&#25968;&#25454;&#30340;&#39046;&#22495;&#20013;&#30340;&#24615;&#33021;&#20173;&#28982;&#33853;&#21518;&#20110;&#26377;&#26631;&#35760;&#25968;&#25454;&#30340;&#39046;&#22495;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#39046;&#22495;&#40065;&#26834;&#24615;&#65288;DR&#65289;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;DR&#30740;&#31350;&#23384;&#22312;&#19981;&#19968;&#33268;&#30340;&#35774;&#32622;&#12289;&#32570;&#20047;&#35780;&#20272;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#21644;&#36807;&#22810;&#20381;&#38752;&#25361;&#25112;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#33258;&#28982;&#39046;&#22495;&#36716;&#31227;&#35774;&#32622;&#19979;&#24494;&#35843;&#21644;&#23567;&#26679;&#26412;&#23398;&#20064;&#27169;&#22411;&#30340;DR&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;DR&#22522;&#20934;&#65292;&#21253;&#25324;&#22810;&#26679;&#21270;&#30340;NLP&#20219;&#21153;&#65292;&#21253;&#25324;&#21477;&#23376;&#21644;&#26631;&#35760;&#32423;&#20998;&#31867;&#12289;&#38382;&#31572;&#21644;&#29983;&#25104;&#65292;&#27599;&#20010;&#20219;&#21153;&#37117;&#30001;&#20960;&#20010;&#39046;&#22495;&#32452;&#25104;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DR&#25361;&#25112;&#30340;&#20004;&#20010;&#35270;&#35282;&#65306;&#28304;&#22495;&#38477;&#20302;&#65288;SD&#65289;&#21644;&#30446;&#26631;&#22495;&#38477;&#20302;&#65288;TD&#65289;&#65292;&#23427;&#20204;&#20132;&#26367;&#20316;&#20026;&#21442;&#32771;&#28857;&#26469;&#27604;&#36739;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#37325;&#22823;&#27604;&#20363;&#30340;&#39046;&#22495;&#36716;&#31227;&#20013;&#65292;SD&#25110;TD&#20043;&#19968;&#26159;&#27491;&#30340;&#65292;&#20294;&#19981;&#26159;&#20004;&#32773;&#37117;&#27491;&#65292;&#24378;&#35843;&#20102;&#35780;&#20272;DR&#25361;&#25112;&#30340;&#20004;&#20010;&#35270;&#35282;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#20801;&#35768;&#22312;&#27169;&#22411;&#12289;&#20219;&#21153;&#21644;&#35774;&#32622;&#19978;&#20844;&#24179;&#27604;&#36739;DR&#65292;&#24182;&#25552;&#20379;&#26377;&#20851;NLP&#27169;&#22411;DR&#24615;&#36136;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models have shown promising performance on various tasks, including fine-tuning, few-shot learning, and zero-shot learning. However, their performance on domains without labeled data still lags behind those with labeled data, which we refer as the Domain Robustness (DR) challenge. Existing research on DR suffers from disparate setups, lack of evaluation task variety, and reliance on challenge sets. In this paper, we explore the DR challenge of both fine-tuned and few-shot learning models in natural domain shift settings. We introduce a DR benchmark comprising diverse NLP tasks, including sentence and token-level classification, QA, and generation, each task consists of several domains. We propose two views of the DR challenge: Source Drop (SD) and Target Drop (TD), which alternate between the source and target in-domain performance as reference points. We find that in significant proportions of domain shifts, either SD or TD is positive, but not both, emphasizing the imp
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31038;&#20132;&#23186;&#20307;&#23454;&#26102;&#34394;&#20551;&#26032;&#38395;&#32531;&#35299;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#26469;&#26816;&#27979;&#20551;&#26032;&#38395;&#24182;&#37319;&#21462;&#23454;&#26102;&#30340;&#32593;&#32476;&#24863;&#30693;&#31574;&#30053;&#26469;&#20943;&#23569;&#20854;&#20256;&#25773;&#12290;</title><link>http://arxiv.org/abs/2302.12190</link><description>&lt;p&gt;
MCWDST: &#19968;&#31181;&#29992;&#20110;&#31038;&#20132;&#23186;&#20307;&#23454;&#26102;&#34394;&#20551;&#26032;&#38395;&#32531;&#35299;&#30340;&#26368;&#23567;&#25104;&#26412;&#21152;&#26435;&#26377;&#21521;&#29983;&#25104;&#26641;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
MCWDST: a Minimum-Cost Weighted Directed Spanning Tree Algorithm for Real-Time Fake News Mitigation in Social Media. (arXiv:2302.12190v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31038;&#20132;&#23186;&#20307;&#23454;&#26102;&#34394;&#20551;&#26032;&#38395;&#32531;&#35299;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#26469;&#26816;&#27979;&#20551;&#26032;&#38395;&#24182;&#37319;&#21462;&#23454;&#26102;&#30340;&#32593;&#32476;&#24863;&#30693;&#31574;&#30053;&#26469;&#20943;&#23569;&#20854;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#30340;&#24191;&#27867;&#26222;&#21450;&#21644;&#25163;&#25345;&#35774;&#22791;&#30340;&#21487;&#29992;&#24615;&#20351;&#24471;&#31038;&#20132;&#23186;&#20307;&#20855;&#26377;&#31867;&#20284;&#25253;&#32440;&#30340;&#24433;&#21709;&#21147;&#12290;&#20154;&#20204;&#21487;&#20197;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#21363;&#26102;&#33719;&#21462;&#24265;&#20215;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20415;&#21033;&#24615;&#20063;&#24102;&#26469;&#20102;&#21361;&#38505;&#65292;&#20219;&#20309;&#29992;&#25143;&#37117;&#21487;&#20197;&#33258;&#30001;&#21457;&#24067;&#20219;&#20309;&#20869;&#23481;&#65292;&#32780;&#19981;&#35770;&#20854;&#30495;&#23454;&#24615;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#26816;&#27979;&#34394;&#20551;&#20449;&#24687;&#65292;&#20063;&#31216;&#20026;&#20551;&#26032;&#38395;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#26816;&#27979;&#20551;&#26032;&#38395;&#65292;&#24182;&#22312;&#23454;&#26102;&#20013;&#20813;&#20110;&#20256;&#25773;&#23427;&#20204;&#30340;&#32593;&#32476;&#33410;&#28857;&#19978;&#36827;&#34892;&#20813;&#30123;&#12290;&#20026;&#20102;&#26816;&#27979;&#20551;&#26032;&#38395;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#22534;&#26632;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#21033;&#29992;&#21367;&#31215;&#21644;&#21452;&#21521;LSTM&#23618;&#12290;&#20026;&#20102;&#32531;&#35299;&#20551;&#26032;&#38395;&#30340;&#20256;&#25773;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#26102;&#32593;&#32476;&#24863;&#30693;&#31574;&#30053;&#65292;&#23427;&#65288;1&#65289;&#20026;&#26816;&#27979;&#21040;&#30340;&#33410;&#28857;&#26500;&#24314;&#20102;&#19968;&#20010;&#26368;&#23567;&#25104;&#26412;&#21152;&#26435;&#30340;&#26377;&#21521;&#29983;&#25104;&#26641;&#65292;&#24182;&#19988;&#65288;2&#65289;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#20998;&#25968;&#21270;&#21361;&#38505;&#24615;&#26041;&#27861;&#26469;&#20813;&#30123;&#35813;&#26641;&#20013;&#30340;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The widespread availability of internet access and handheld devices confers to social media a power similar to the one newspapers used to have. People seek affordable information on social media and can reach it within seconds. Yet this convenience comes with dangers; any user may freely post whatever they please and the content can stay online for a long period, regardless of its truthfulness. A need to detect untruthful information, also known as fake news, arises. In this paper, we present an end-to-end solution that accurately detects fake news and immunizes network nodes that spread them in real-time. To detect fake news, we propose two new stack deep learning architectures that utilize convolutional and bidirectional LSTM layers. To mitigate the spread of fake news, we propose a real-time network-aware strategy that (1) constructs a minimum-cost weighted directed spanning tree for a detected node, and (2) immunizes nodes in that tree by scoring their harmfulness using a novel ran
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25511;&#21046;&#26080;&#20851;&#21477;&#23376;&#30340;&#28151;&#28102;&#25928;&#24212;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#25277;&#35937;&#25688;&#35201;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;AnswerSumm&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;20\%&#30340;&#20934;&#30830;&#24615;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2212.09726</link><description>&lt;p&gt;
&#36890;&#36807;&#25511;&#21046;&#26080;&#20851;&#21477;&#23376;&#30340;&#28151;&#28102;&#25928;&#24212;&#65292;&#25552;&#39640;&#25277;&#35937;&#25688;&#35201;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Faithfulness of Abstractive Summarization by Controlling Confounding Effect of Irrelevant Sentences. (arXiv:2212.09726v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09726
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25511;&#21046;&#26080;&#20851;&#21477;&#23376;&#30340;&#28151;&#28102;&#25928;&#24212;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#25277;&#35937;&#25688;&#35201;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;AnswerSumm&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;20\%&#30340;&#20934;&#30830;&#24615;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#20808;&#36827;&#30340;&#25688;&#35201;&#31995;&#32479;&#22312;&#29983;&#25104;&#30475;&#20284;&#27969;&#21033;&#30340;&#25688;&#35201;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36827;&#23637;&#65292;&#20294;&#32570;&#20047;&#20107;&#23454;&#27491;&#30830;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#26080;&#20851;&#30340;&#36755;&#20837;&#25991;&#26412;&#37096;&#20998;&#21487;&#33021;&#23548;&#33268;&#20107;&#23454;&#19981;&#19968;&#33268;&#65292;&#20316;&#20026;&#28151;&#28102;&#22240;&#32032;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#22240;&#26524;&#25928;&#24212;&#30340;&#20449;&#24687;&#29702;&#35770;&#24230;&#37327;&#26469;&#37327;&#21270;&#28151;&#28102;&#30340;&#31243;&#24230;&#65292;&#24182;&#20934;&#30830;&#34913;&#37327;&#20854;&#23545;&#25688;&#35201;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#22522;&#20110;&#20174;&#29702;&#35770;&#32467;&#26524;&#20013;&#24471;&#20986;&#30340;&#35265;&#35299;&#65292;&#24403;&#26377;&#20154;&#24037;&#27880;&#37322;&#30340;&#30456;&#20851;&#21477;&#23376;&#21487;&#29992;&#26102;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22810;&#20219;&#21153;&#27169;&#22411;&#26469;&#25511;&#21046;&#36825;&#31181;&#28151;&#28102;&#12290;&#20851;&#38190;&#26159;&#65292;&#25105;&#20204;&#23545;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#20102;&#21407;&#21017;&#24615;&#30340;&#21051;&#30011;&#65292;&#20174;&#32780;&#30830;&#20445;&#20102;&#20351;&#29992;&#20154;&#24037;&#27880;&#37322;&#30340;&#30456;&#20851;&#21477;&#23376;&#26469;&#29983;&#25104;&#20934;&#30830;&#30340;&#25688;&#35201;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;AnswerSumm&#19978;&#65288;&#21442;&#32771;&#25991;&#29486;&#65306;fabbri2021answersumm&#65289;&#19978;&#24378;&#22522;&#20934;&#30340;&#22522;&#30784;&#19978;&#23558;&#20934;&#30830;&#24615;&#24471;&#20998;&#25552;&#39640;&#20102;20\%&#12290;
&lt;/p&gt;
&lt;p&gt;
Lack of factual correctness is an issue that still plagues state-of-the-art summarization systems despite their impressive progress on generating seemingly fluent summaries. In this paper, we show that factual inconsistency can be caused by irrelevant parts of the input text, which act as confounders. To that end, we leverage information-theoretic measures of causal effects to quantify the amount of confounding and precisely quantify how they affect the summarization performance. Based on insights derived from our theoretical results, we design a simple multi-task model to control such confounding by leveraging human-annotated relevant sentences when available. Crucially, we give a principled characterization of data distributions where such confounding can be large thereby necessitating the use of human annotated relevant sentences to generate factual summaries. Our approach improves faithfulness scores by 20\% over strong baselines on AnswerSumm \citep{fabbri2021answersumm}, a conver
&lt;/p&gt;</description></item></channel></rss>