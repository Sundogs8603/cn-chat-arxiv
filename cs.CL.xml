<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#21033;&#29992;&#21345;&#36890;&#22270;&#20687;&#26500;&#24314;&#30340;CausalChaos!&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#26356;&#38271;&#22240;&#26524;&#38142;&#30340;&#22240;&#26524;&#38382;&#31572;&#65292;&#36890;&#36807;&#21160;&#24577;&#20114;&#21160;&#21644;&#35270;&#35273;&#23637;&#31034;&#25361;&#25112;&#24615;&#22240;&#26524;&#20851;&#31995;&#65292;&#20026;&#27169;&#22411;&#25552;&#20379;&#20102;&#26356;&#22810;&#20855;&#25361;&#25112;&#24615;&#19988;&#26126;&#30830;&#23450;&#20041;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2404.01299</link><description>&lt;p&gt;
CausalChaos!&#25968;&#25454;&#38598;&#65306;&#22522;&#20110;&#21160;&#24577;&#35270;&#35273;&#22330;&#26223;&#20013;&#26356;&#38271;&#22240;&#26524;&#38142;&#30340;&#20840;&#38754;&#22240;&#26524;&#34892;&#21160;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
CausalChaos! Dataset for Comprehensive Causal Action Question Answering Over Longer Causal Chains Grounded in Dynamic Visual Scenes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01299
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#21345;&#36890;&#22270;&#20687;&#26500;&#24314;&#30340;CausalChaos!&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#26356;&#38271;&#22240;&#26524;&#38142;&#30340;&#22240;&#26524;&#38382;&#31572;&#65292;&#36890;&#36807;&#21160;&#24577;&#20114;&#21160;&#21644;&#35270;&#35273;&#23637;&#31034;&#25361;&#25112;&#24615;&#22240;&#26524;&#20851;&#31995;&#65292;&#20026;&#27169;&#22411;&#25552;&#20379;&#20102;&#26356;&#22810;&#20855;&#25361;&#25112;&#24615;&#19988;&#26126;&#30830;&#23450;&#20041;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#35270;&#39057;&#38382;&#31572;&#65288;QA&#65289;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#28982;&#32780;&#29616;&#26377;&#25968;&#25454;&#38598;&#22312;&#22240;&#26524;&#25512;&#29702;&#20998;&#26512;&#26041;&#38754;&#24448;&#24448;&#32570;&#20047;&#28145;&#24230;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#21033;&#29992;&#21345;&#36890;&#30340;&#29420;&#29305;&#23646;&#24615;&#26500;&#24314;&#20102;CausalChaos!&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22240;&#26524;&#38382;&#31572;&#65288;Why-QA&#65289;&#25968;&#25454;&#38598;&#65292;&#22522;&#20110;&#26631;&#24535;&#24615;&#30340;&#8220;&#29483;&#21644;&#32769;&#40736;&#8221;&#21345;&#36890;&#31995;&#21015;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#36890;&#36807;&#21608;&#21040;&#30340;&#38382;&#39064;&#21644;&#22810;&#23618;&#27425;&#31572;&#26696;&#65292;&#21253;&#21547;&#30528;&#23884;&#20837;&#21160;&#24577;&#20114;&#21160;&#21644;&#35270;&#35273;&#20013;&#30340;&#26356;&#38271;&#22240;&#26524;&#38142;&#65292;&#21516;&#26102;&#21160;&#30011;&#21407;&#29702;&#20801;&#35768;&#21160;&#30011;&#24072;&#21019;&#36896;&#23450;&#20041;&#26126;&#30830;&#12289;&#26126;&#20102;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#36825;&#20123;&#22240;&#32032;&#20351;&#27169;&#22411;&#33021;&#22815;&#35299;&#20915;&#26356;&#20855;&#25361;&#25112;&#24615;&#20294;&#26126;&#30830;&#23450;&#20041;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#30828;&#36127;&#37319;&#26679;&#65292;&#21253;&#25324;CausalConfusion&#29256;&#26412;&#12290;&#34429;&#28982;&#27169;&#22411;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#20173;&#26377;&#24456;&#22823;&#25913;&#36827;&#31354;&#38388;&#65292;&#29305;&#21035;&#26159;&#22312;&#24320;&#25918;&#24335;&#31572;&#26696;&#26041;&#38754;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#26356;&#20026;&#20808;&#36827;/&#26126;&#30830;&#30340;&#22240;&#26524;&#20851;&#31995;&#24314;&#27169;&#21644;&#32852;&#21512;&#24314;&#27169;&#31561;&#25913;&#36827;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01299v1 Announce Type: cross  Abstract: Causal video question answering (QA) has garnered increasing interest, yet existing datasets often lack depth in causal reasoning analysis. To address this gap, we capitalize on the unique properties of cartoons and construct CausalChaos!, a novel, challenging causal Why-QA dataset built upon the iconic "Tom and Jerry" cartoon series. With thoughtful questions and multi-level answers, our dataset contains much longer causal chains embedded in dynamic interactions and visuals, at the same time principles of animation allows animators to create well-defined, unambiguous causal relationships. These factors allow models to solve more challenging, yet well-defined causal relationships. We also introduce hard negative mining, including CausalConfusion version. While models perform well, there is much room for improvement, especially, on open-ended answers. We identify more advanced/explicit causal relationship modeling and joint modeling of 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25511;&#21046;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23646;&#24615;&#65292;&#23454;&#29616;&#23433;&#20840;&#21644;&#24110;&#21161;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#24212;&#29992;&#22330;&#26223;&#20013;&#23454;&#29616;&#36825;&#31181;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2404.01295</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#25511;&#22823;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#23433;&#20840;&#21644;&#24110;&#21161;&#24179;&#34913;&#30340;&#22238;&#24212;
&lt;/p&gt;
&lt;p&gt;
Towards Safety and Helpfulness Balanced Responses via Controllable Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01295
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25511;&#21046;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23646;&#24615;&#65292;&#23454;&#29616;&#23433;&#20840;&#21644;&#24110;&#21161;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#24212;&#29992;&#22330;&#26223;&#20013;&#23454;&#29616;&#36825;&#31181;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22914;&#20170;&#21464;&#24471;&#23481;&#26131;&#33719;&#24471;&#65292;&#23433;&#20840;&#21644;&#24110;&#21161;&#20043;&#38388;&#30340;&#26435;&#34913;&#20250;&#26174;&#33879;&#24433;&#21709;&#29992;&#25143;&#20307;&#39564;&#12290;&#19968;&#20010;&#20248;&#20808;&#32771;&#34385;&#23433;&#20840;&#30340;&#27169;&#22411;&#20250;&#23548;&#33268;&#29992;&#25143;&#24863;&#21040;&#36739;&#23569;&#21442;&#19982;&#21644;&#34987;&#21327;&#21161;&#65292;&#32780;&#20248;&#20808;&#32771;&#34385;&#24110;&#21161;&#24615;&#21017;&#21487;&#33021;&#23548;&#33268;&#20260;&#23475;&#12290;&#21487;&#33021;&#30340;&#21361;&#23475;&#21253;&#25324;&#25945;&#25480;&#20154;&#20204;&#22914;&#20309;&#21046;&#36896;&#28856;&#24377;&#65292;&#21521;&#38738;&#23569;&#24180;&#26292;&#38706;&#19981;&#24403;&#20869;&#23481;&#20197;&#21450;&#20260;&#23475;&#29992;&#25143;&#30340;&#24515;&#29702;&#20581;&#24247;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#25511;&#21046;LLMs&#20013;&#30340;&#20004;&#20010;&#23646;&#24615;&#26469;&#22312;&#19981;&#21516;&#30340;&#29992;&#20363;&#20013;&#24179;&#34913;&#23433;&#20840;&#24615;&#21644;&#24110;&#21161;&#24615;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#19981;&#38656;&#35201;&#39069;&#22806;&#20154;&#21592;&#27880;&#37322;&#30340;&#35757;&#32451;&#26080;&#20851;&#21644;&#24494;&#35843;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#25511;&#21046;LLMs&#20013;&#23433;&#20840;&#24615;&#21644;&#24110;&#21161;&#24615;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#37325;&#32622;&#24050;&#23398;&#20064;&#30340;&#27169;&#22411;&#24182;&#35299;&#38145;&#20854;&#21487;&#25511;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01295v1 Announce Type: cross  Abstract: As large language models (LLMs) become easily accessible nowadays, the trade-off between safety and helpfulness can significantly impact user experience. A model that prioritizes safety will cause users to feel less engaged and assisted while prioritizing helpfulness will potentially cause harm. Possible harms include teaching people how to build a bomb, exposing youth to inappropriate content, and hurting users' mental health. In this work, we propose to balance safety and helpfulness in diverse use cases by controlling both attributes in LLM. We explore training-free and fine-tuning methods that do not require extra human annotations and analyze the challenges of controlling safety and helpfulness in LLMs. Our experiments demonstrate that our method can rewind a learned model and unlock its controllability.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;VQAScore&#65292;&#21033;&#29992;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#29983;&#25104;&#23545;&#40784;&#20998;&#25968;&#65292;&#21462;&#24471;&#20102;&#22312;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#22522;&#20934;&#19978;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;</title><link>https://arxiv.org/abs/2404.01291</link><description>&lt;p&gt;
&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#19982;&#22270;&#20687;&#21040;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Evaluating Text-to-Visual Generation with Image-to-Text Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01291
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;VQAScore&#65292;&#21033;&#29992;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#29983;&#25104;&#23545;&#40784;&#20998;&#25968;&#65292;&#21462;&#24471;&#20102;&#22312;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#22522;&#20934;&#19978;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#26377;&#25928;&#30340;&#24230;&#37327;&#26631;&#20934;&#21644;&#26631;&#20934;&#21270;&#22522;&#20934;&#65292;&#32508;&#21512;&#35780;&#20272;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;VQAScore&#65292;&#20351;&#29992;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#27169;&#22411;&#36890;&#36807;&#35745;&#31639;&#23545;&#31616;&#21333;&#30340;&#8220;&#36825;&#24133;&#22270;&#34920;&#29616;&#20986;&#20102;'{&#25991;&#26412;}'&#21527;&#65311;&#8221;&#38382;&#39064;&#30340;&#8220;&#26159;&#8221;&#31572;&#26696;&#30340;&#27010;&#29575;&#26469;&#29983;&#25104;&#23545;&#40784;&#20998;&#25968;&#12290;&#23613;&#31649;&#27604;&#20808;&#21069;&#30340;&#26041;&#27861;&#26356;&#31616;&#21333;&#65292;&#20294;&#20351;&#29992;&#29616;&#25104;&#27169;&#22411;&#35745;&#31639;&#30340;VQAScore&#22312;&#35768;&#22810;&#65288;8&#20010;&#65289;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#22522;&#20934;&#19978;&#20135;&#29983;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01291v1 Announce Type: cross  Abstract: Despite significant progress in generative AI, comprehensive evaluation remains challenging because of the lack of effective metrics and standardized benchmarks. For instance, the widely-used CLIPScore measures the alignment between a (generated) image and text prompt, but it fails to produce reliable scores for complex prompts involving compositions of objects, attributes, and relations. One reason is that text encoders of CLIP can notoriously act as a "bag of words", conflating prompts such as "the horse is eating the grass" with "the grass is eating the horse". To address this, we introduce the VQAScore, which uses a visual-question-answering (VQA) model to produce an alignment score by computing the probability of a "Yes" answer to a simple "Does this figure show '{text}'?" question. Though simpler than prior art, VQAScore computed with off-the-shelf models produces state-of-the-art results across many (8) image-text alignment benc
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#35748;&#30693;&#37325;&#35780;&#65292;&#36825;&#39033;&#24037;&#20316;&#23558;&#24515;&#29702;&#23398;&#21407;&#21017;&#34701;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20026;&#20854;&#25552;&#20379;&#20808;&#36827;&#30340;&#24515;&#29702;&#23398;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.01288</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#25552;&#20379;&#35748;&#30693;&#37325;&#35780;&#65292;&#22914;&#24471;&#21040;&#25351;&#23548;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Capable of Offering Cognitive Reappraisal, if Guided
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01288
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#35748;&#30693;&#37325;&#35780;&#65292;&#36825;&#39033;&#24037;&#20316;&#23558;&#24515;&#29702;&#23398;&#21407;&#21017;&#34701;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20026;&#20854;&#25552;&#20379;&#20808;&#36827;&#30340;&#24515;&#29702;&#23398;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#24773;&#24863;&#25903;&#25345;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23427;&#20204;&#21487;&#20197;&#23545;&#22788;&#20110;&#22256;&#22659;&#20013;&#30340;&#20154;&#20135;&#29983;&#20849;&#24773;&#22238;&#24212;&#12290;&#28982;&#32780;&#65292;&#38271;&#26399;&#30340;&#24515;&#29702;&#20581;&#24247;&#38656;&#35201;&#24773;&#32490;&#33258;&#25105;&#35843;&#33410;&#65292;&#32780;&#19968;&#27425;&#24615;&#30340;&#20849;&#24773;&#22238;&#24212;&#21017;&#26174;&#24471;&#21147;&#19981;&#20174;&#24515;&#12290;&#26412;&#25991;&#36890;&#36807;&#21442;&#19982;&#35748;&#30693;&#37325;&#35780;&#36808;&#20986;&#20102;&#31532;&#19968;&#27493;&#65292;&#35748;&#30693;&#37325;&#35780;&#26159;&#24515;&#29702;&#23398;&#20174;&#19994;&#32773;&#20351;&#29992;&#35821;&#35328;&#26377;&#38024;&#23545;&#24615;&#22320;&#25913;&#21464;&#20010;&#20307;&#23545;&#24773;&#22659;&#30340;&#36127;&#38754;&#35780;&#20215;&#30340;&#31574;&#30053;&#65307;&#36825;&#31181;&#35780;&#20215;&#34987;&#35748;&#20026;&#26159;&#20154;&#31867;&#24773;&#24863;&#20307;&#39564;&#30340;&#26681;&#28304;&#12290;&#25105;&#20204;&#20551;&#35774;&#24515;&#29702;&#23398;&#19978;&#30340;&#21407;&#21017;&#21487;&#20197;&#20351;LLMs&#20855;&#22791;&#36825;&#31181;&#20808;&#36827;&#30340;&#24515;&#29702;&#23398;&#33021;&#21147;&#65292;&#24182;&#35774;&#35745;&#20102;RESORT&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31995;&#21015;&#36328;&#22810;&#20010;&#32500;&#24230;&#30340;&#37325;&#35780;&#26500;&#25104;&#65292;&#21487;&#29992;&#20316;LLM&#30340;&#25351;&#23548;&#12290;&#25105;&#20204;&#23545;LLM&#36827;&#34892;&#20102;&#39318;&#27425;&#19987;&#23478;&#35780;&#20272;&#65288;&#30001;&#25345;&#26377;&#30805;&#22763;&#25110;&#21338;&#22763;&#23398;&#20301;&#30340;&#20020;&#24202;&#24515;&#29702;&#23398;&#23478;&#36827;&#34892;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01288v1 Announce Type: new  Abstract: Large language models (LLMs) have offered new opportunities for emotional support, and recent work has shown that they can produce empathic responses to people in distress. However, long-term mental well-being requires emotional self-regulation, where a one-time empathic response falls short. This work takes a first step by engaging with cognitive reappraisals, a strategy from psychology practitioners that uses language to targetedly change negative appraisals that an individual makes of the situation; such appraisals is known to sit at the root of human emotional experience. We hypothesize that psychologically grounded principles could enable such advanced psychology capabilities in LLMs, and design RESORT which consists of a series of reappraisal constitutions across multiple dimensions that can be used as LLM instructions. We conduct a first-of-its-kind expert evaluation (by clinical psychologists with M.S. or Ph.D. degrees) of an LLM
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23383;&#23402;&#29983;&#20307;TWIN-GPT&#65292;&#29992;&#20110;&#25903;&#25345;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2404.01273</link><description>&lt;p&gt;
TWIN-GPT: &#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20020;&#24202;&#35797;&#39564;&#25968;&#23383;&#23402;&#29983;&#20307;
&lt;/p&gt;
&lt;p&gt;
TWIN-GPT: Digital Twins for Clinical Trials via Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01273
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23383;&#23402;&#29983;&#20307;TWIN-GPT&#65292;&#29992;&#20110;&#25903;&#25345;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#34394;&#25311;&#20020;&#24202;&#35797;&#39564;&#20135;&#29983;&#20102;&#26085;&#30410;&#22686;&#38271;&#30340;&#20852;&#36259;&#65292;&#36825;&#20123;&#35797;&#39564;&#27169;&#25311;&#20102;&#29616;&#23454;&#19990;&#30028;&#24773;&#22659;&#65292;&#26377;&#26395;&#26174;&#33879;&#22686;&#24378;&#24739;&#32773;&#23433;&#20840;&#24615;&#65292;&#21152;&#24555;&#24320;&#21457;&#36895;&#24230;&#65292;&#38477;&#20302;&#25104;&#26412;&#65292;&#24182;&#20026;&#21307;&#30103;&#39046;&#22495;&#30340;&#26356;&#24191;&#27867;&#31185;&#23398;&#30693;&#35782;&#36129;&#29486;&#21147;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23383;&#23402;&#29983;&#20307;TWIN-GPT&#65292;&#29992;&#20110;&#25903;&#25345;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01273v1 Announce Type: cross  Abstract: Recently, there has been a burgeoning interest in virtual clinical trials, which simulate real-world scenarios and hold the potential to significantly enhance patient safety, expedite development, reduce costs, and contribute to the broader scientific knowledge in healthcare. Existing research often focuses on leveraging electronic health records (EHRs) to support clinical trial outcome prediction. Yet, trained with limited clinical trial outcome data, existing approaches frequently struggle to perform accurate predictions. Some research has attempted to generate EHRs to augment model development but has fallen short in personalizing the generation for individual patient profiles. Recently, the emergence of large language models has illuminated new possibilities, as their embedded comprehensive clinical knowledge has proven beneficial in addressing medical issues. In this paper, we propose a large language model-based digital twin crea
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;&#31185;&#23398;&#35770;&#25991;&#20013;LLM&#20351;&#29992;&#22686;&#21152;&#30340;&#26144;&#23556;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#20998;&#26512;&#65292;&#22635;&#34917;&#20102;&#23398;&#26415;&#20889;&#20316;&#20013;&#30495;&#23454;LLM&#20462;&#25913;&#20869;&#23481;&#27604;&#20363;&#32570;&#22833;&#30340;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2404.01268</link><description>&lt;p&gt;
&#31185;&#23398;&#35770;&#25991;&#20013;LLM&#20351;&#29992;&#22686;&#21152;&#30340;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Mapping the Increasing Use of LLMs in Scientific Papers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01268
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;&#31185;&#23398;&#35770;&#25991;&#20013;LLM&#20351;&#29992;&#22686;&#21152;&#30340;&#26144;&#23556;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#20998;&#26512;&#65292;&#22635;&#34917;&#20102;&#23398;&#26415;&#20889;&#20316;&#20013;&#30495;&#23454;LLM&#20462;&#25913;&#20869;&#23481;&#27604;&#20363;&#32570;&#22833;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#20986;&#29256;&#36890;&#36807;&#20256;&#25773;&#30740;&#31350;&#25104;&#26524;&#12289;&#20419;&#36827;&#21512;&#20316;&#12289;&#40723;&#21169;&#21487;&#37325;&#22797;&#24615;&#65292;&#24182;&#30830;&#20445;&#31185;&#23398;&#30693;&#35782;&#38543;&#26102;&#38388;&#21487;&#35775;&#38382;&#12289;&#21487;&#39564;&#35777;&#24182;&#19981;&#26029;&#24314;&#31435;&#65292;&#20026;&#31185;&#23398;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#23545;&#22810;&#23569;&#20154;&#22312;&#23398;&#26415;&#20889;&#20316;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#21450;&#36825;&#31181;&#24037;&#20855;&#23545;&#20840;&#29699;&#31185;&#23398;&#23454;&#36341;&#21487;&#33021;&#20135;&#29983;&#20309;&#31181;&#24433;&#21709;&#36827;&#34892;&#20102;&#22823;&#37327;&#29468;&#27979;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#32570;&#20047;&#23545;&#23398;&#26415;&#20889;&#20316;&#20013;&#23454;&#36136;&#24615;&#20462;&#25913;&#25110;&#29983;&#25104;&#20869;&#23481;&#30340;&#20934;&#30830;&#24230;&#37327;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#23545;&#22312;arXiv&#12289;bioRxiv&#21644;&#33258;&#28982;&#23398;&#25253;&#31995;&#21015;&#26399;&#21002;&#19978;&#30340;950,965&#31687;&#35770;&#25991;&#65288;&#26102;&#38388;&#36328;&#24230;&#20174;2020&#24180;1&#26376;&#33267;2024&#24180;2&#26376;&#65289;&#36827;&#34892;&#20102;&#39318;&#27425;&#31995;&#32479;&#24615;&#12289;&#22823;&#35268;&#27169;&#30340;&#20998;&#26512;&#65292;&#21033;&#29992;&#20154;&#32676;&#32423;&#21035;&#30340;&#32479;&#35745;&#26694;&#26550;&#26469;&#27979;&#37327;LLM&#20462;&#25913;&#20869;&#23481;&#38543;&#26102;&#38388;&#30340;&#30427;&#34892;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#32479;&#35745;&#20272;&#35745;&#26159;&#22312;&#35821;&#26009;&#24211;&#32423;&#21035;&#21450;i
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01268v1 Announce Type: cross  Abstract: Scientific publishing lays the foundation of science by disseminating research findings, fostering collaboration, encouraging reproducibility, and ensuring that scientific knowledge is accessible, verifiable, and built upon over time. Recently, there has been immense speculation about how many people are using large language models (LLMs) like ChatGPT in their academic writing, and to what extent this tool might have an effect on global scientific practices. However, we lack a precise measure of the proportion of academic writing substantially modified or produced by LLMs. To address this gap, we conduct the first systematic, large-scale analysis across 950,965 papers published between January 2020 and February 2024 on the arXiv, bioRxiv, and Nature portfolio journals, using a population-level statistical framework to measure the prevalence of LLM-modified content over time. Our statistical estimation operates on the corpus level and i
&lt;/p&gt;</description></item><item><title>IsoBench&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#22522;&#20110;&#19981;&#21516;&#21516;&#26500;&#34920;&#31034;&#30340;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#26356;&#20559;&#22909;&#25991;&#26412;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2404.01266</link><description>&lt;p&gt;
IsoBench&#65306;&#22522;&#20110;&#21516;&#26500;&#34920;&#31034;&#23545;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
IsoBench: Benchmarking Multimodal Foundation Models on Isomorphic Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01266
&lt;/p&gt;
&lt;p&gt;
IsoBench&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#22522;&#20110;&#19981;&#21516;&#21516;&#26500;&#34920;&#31034;&#30340;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#26356;&#20559;&#22909;&#25991;&#26412;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22522;&#30784;&#27169;&#22411;&#22312;&#20165;&#25991;&#26412;&#25110;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#21516;&#26102;&#25552;&#31034;&#26102;&#34920;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#20294;&#23427;&#20204;&#30340;&#33021;&#21147;&#26159;&#21542;&#20250;&#26681;&#25454;&#36755;&#20837;&#26041;&#24335;&#32780;&#25913;&#21464;&#21602;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;$\textbf{IsoBench}$&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#26469;&#33258;&#22235;&#20010;&#20027;&#35201;&#39046;&#22495;&#30340;&#38382;&#39064;: &#25968;&#23398;&#12289;&#31185;&#23398;&#12289;&#31639;&#27861;&#21644;&#28216;&#25103;&#12290;&#27599;&#20010;&#31034;&#20363;&#21576;&#29616;&#20102;&#22810;&#20010;&#36755;&#20837;&#30340;&#21516;&#26500;&#34920;&#31034;&#65292;&#22914;&#35270;&#35273;&#12289;&#25991;&#26412;&#21644;&#25968;&#23398;&#23637;&#31034;&#12290;IsoBench&#25552;&#20379;&#20102;&#32454;&#31890;&#24230;&#30340;&#21453;&#39304;&#65292;&#20197;&#35786;&#26029;&#30001;&#34920;&#31034;&#24418;&#24335;&#36896;&#25104;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#22312;&#21508;&#31181;&#22522;&#30784;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#30456;&#21516;&#38382;&#39064;&#19978;&#65292;&#27169;&#22411;&#19968;&#36143;&#20559;&#22909;&#25991;&#26412;&#34920;&#31034;&#12290;&#26368;&#31361;&#20986;&#30340;&#26159;&#65292;&#22312;&#25152;&#26377;IsoBench&#38382;&#39064;&#19978;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;Claude-3 Opus&#22312;&#25552;&#20379;&#22270;&#20687;&#32780;&#19981;&#26159;&#25991;&#26412;&#26102;&#24615;&#33021;&#19979;&#38477;28.7&#20998;&#65307;&#21516;&#26679;&#65292;GPT-4 Turbo&#24615;&#33021;&#19979;&#38477;18.7&#20998;&#65292;Gemini Pro&#19979;&#38477;14.9&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01266v1 Announce Type: new  Abstract: Current foundation models exhibit impressive capabilities when prompted either with text only or with both image and text inputs. But do their capabilities change depending on the input modality? In this work, we propose $\textbf{IsoBench}$, a benchmark dataset containing problems from four major areas: math, science, algorithms, and games. Each example is presented with multiple $\textbf{isomorphic representations}$ of inputs, such as visual, textual, and mathematical presentations. IsoBench provides fine-grained feedback to diagnose performance gaps caused by the form of the representation. Across various foundation models, we observe that on the same problem, models have a consistent preference towards textual representations. Most prominently, when evaluated on all IsoBench problems, Claude-3 Opus performs 28.7 points worse when provided with images instead of text; similarly, GPT-4 Turbo is 18.7 points worse and Gemini Pro is 14.9 p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;AI&#27169;&#22411;&#22312;&#35821;&#35328;&#25991;&#29486;&#35760;&#24405;&#20013;&#21019;&#24314;&#35821;&#35328;&#22320;&#22270;&#30340;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;AI&#27169;&#22411;&#20419;&#36827;&#35821;&#35328;&#30340;&#31354;&#38388;&#25991;&#29486;&#35760;&#24405;&#12290;</title><link>https://arxiv.org/abs/2404.01263</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#19982;&#35821;&#35328;&#31354;&#38388;&#25991;&#26723;&#21270;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence and the Spatial Documentation of Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;AI&#27169;&#22411;&#22312;&#35821;&#35328;&#25991;&#29486;&#35760;&#24405;&#20013;&#21019;&#24314;&#35821;&#35328;&#22320;&#22270;&#30340;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;AI&#27169;&#22411;&#20419;&#36827;&#35821;&#35328;&#30340;&#31354;&#38388;&#25991;&#29486;&#35760;&#24405;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25216;&#26415;&#30340;&#21457;&#23637;&#20351;&#36328;&#23398;&#31185;&#30740;&#31350;&#21464;&#24471;&#26356;&#21152;&#23481;&#26131;&#12290;&#23588;&#20854;&#26159;&#20154;&#24037;&#26234;&#33021;AI&#30340;&#31361;&#30772;&#20026;&#22312;&#36328;&#23398;&#31185;&#21644;&#22810;&#23398;&#31185;&#39046;&#22495;&#24037;&#20316;&#30340;&#30740;&#31350;&#20154;&#21592;&#24102;&#26469;&#20102;&#24040;&#22823;&#20248;&#21183;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;AI&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;GPT4&#21644;GPT&#25968;&#25454;&#20998;&#26512;&#21592;&#65292;&#22312;&#35821;&#35328;&#25991;&#29486;&#35760;&#24405;&#20013;&#21019;&#24314;&#35821;&#35328;&#22320;&#22270;&#30340;&#33021;&#21147;&#12290;&#35813;&#30740;&#31350;&#23558;&#25991;&#29486;&#23398;&#12289;&#35821;&#35328;&#22320;&#29702;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#30456;&#32467;&#21512;&#65292;&#23637;&#31034;&#20102;AI&#27169;&#22411;&#22914;&#20309;&#36890;&#36807;&#21019;&#24314;&#20855;&#26377;&#26368;&#23569;&#21046;&#22270;&#19987;&#19994;&#30693;&#35782;&#30340;&#35821;&#35328;&#22320;&#22270;&#20419;&#36827;&#35821;&#35328;&#30340;&#31354;&#38388;&#25991;&#29486;&#35760;&#24405;&#12290;&#35813;&#30740;&#31350;&#20351;&#29992;&#20102;&#20174;HDX&#21644;&#30740;&#31350;&#20154;&#21592;&#30340;&#30000;&#37326;&#24037;&#20316;&#20013;&#33719;&#21462;&#30340;CSV&#25991;&#20214;&#21644;GeoJSON&#25991;&#20214;&#12290;&#28982;&#21518;&#23558;&#30740;&#31350;&#25968;&#25454;&#24212;&#29992;&#20110;&#19982;AI&#27169;&#22411;&#30340;&#23454;&#26102;&#23545;&#35805;&#20013;&#65292;&#20197;&#29983;&#25104;&#35821;&#35328;&#20998;&#24067;&#22320;&#22270;&#12290;&#30740;&#31350;&#31361;&#20986;&#20102;&#36825;&#20004;&#20010;AI&#27169;&#22411;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#38745;&#24577;&#21644;int
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01263v1 Announce Type: new  Abstract: The advancement in technology has made interdisciplinary research more accessible. Particularly the breakthrough in Artificial Intelligence AI has given huge advantages to researchers working in interdisciplinary and multidisciplinary fields. This study investigates the ability of AI models, particularly GPT4 and GPT Data Analyst in creating language maps for language documentation. The study Integrates documentary linguistics linguistic geography and AI by showcasing how AI models facilitate the spatial documentation of languages through the creation of language maps with minimal cartographic expertise. The study is conducted using a CSV file and a GeoJSON file both obtained from HDX and from the researchers fieldwork. The study data is then applied in realtime conversations with the AI models in order to generate the language distribution maps. The study highlights the two AI models capabilities in generating highquality static and int
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#23545;LLM&#29983;&#25104;&#30340;&#34394;&#26500;&#20070;&#31821;&#25688;&#35201;&#36827;&#34892;&#20102;&#24544;&#23454;&#24615;&#21644;&#20869;&#23481;&#36873;&#25321;&#30340;&#22823;&#35268;&#27169;&#20154;&#31867;&#35780;&#20272;&#65292;&#24314;&#31435;&#20102;FABLES&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#23545;26&#26412;&#20070;&#30340;3158&#20010;&#22768;&#26126;&#36827;&#34892;&#20102;&#27880;&#37322;&#65292;&#25104;&#21151;&#23545;LLM&#25688;&#35201;&#36827;&#34892;&#20102;&#22522;&#20110;&#24544;&#23454;&#24615;&#30340;&#25490;&#21517;</title><link>https://arxiv.org/abs/2404.01261</link><description>&lt;p&gt;
FABLES&#65306;&#35780;&#20272;&#20070;&#31821;&#25688;&#35201;&#20013;&#30340;&#24544;&#23454;&#24615;&#21644;&#20869;&#23481;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
FABLES: Evaluating faithfulness and content selection in book-length summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#23545;LLM&#29983;&#25104;&#30340;&#34394;&#26500;&#20070;&#31821;&#25688;&#35201;&#36827;&#34892;&#20102;&#24544;&#23454;&#24615;&#21644;&#20869;&#23481;&#36873;&#25321;&#30340;&#22823;&#35268;&#27169;&#20154;&#31867;&#35780;&#20272;&#65292;&#24314;&#31435;&#20102;FABLES&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#23545;26&#26412;&#20070;&#30340;3158&#20010;&#22768;&#26126;&#36827;&#34892;&#20102;&#27880;&#37322;&#65292;&#25104;&#21151;&#23545;LLM&#25688;&#35201;&#36827;&#34892;&#20102;&#22522;&#20110;&#24544;&#23454;&#24615;&#30340;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#38271;&#25991;&#26412;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25216;&#26415;&#19978;&#21487;&#20197;&#24635;&#32467;&#38271;&#36798;100K&#20010;&#26631;&#35760;&#30340;&#20070;&#31821;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#65292;&#25991;&#26723;&#30340;&#38271;&#24230;&#21644;&#22797;&#26434;&#24615;&#38459;&#30861;&#20102;&#23545;&#24544;&#23454;&#24615;&#31561;&#36755;&#20837;&#30456;&#20851;&#26041;&#38754;&#30340;&#35780;&#20272;&#12290;&#26412;&#25991;&#22312;&#34394;&#26500;&#20070;&#31821;&#30340;LLM&#29983;&#25104;&#25688;&#35201;&#19978;&#36827;&#34892;&#20102;&#39318;&#27425;&#22823;&#35268;&#27169;&#20154;&#31867;&#35780;&#20272;&#65292;&#36890;&#36807;&#19987;&#27880;&#20110;2023&#25110;2024&#24180;&#20986;&#29256;&#30340;&#20070;&#31821;&#25688;&#35201;&#65292;&#38599;&#20323;&#22312;&#36827;&#34892;&#27880;&#37322;&#20219;&#21153;&#20043;&#21069;&#24050;&#23436;&#20840;&#38405;&#35835;&#27599;&#26412;&#20070;&#30340;&#27880;&#37322;&#32773;&#26469;&#20943;&#23569;&#25104;&#26412;&#21644;&#35748;&#30693;&#36127;&#25285;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;FABLES&#25968;&#25454;&#38598;&#65292;&#23545;26&#26412;&#20070;&#30340;LLM&#29983;&#25104;&#25688;&#35201;&#20013;&#30340;3158&#20010;&#22768;&#26126;&#36827;&#34892;&#20102;&#27880;&#37322;&#65292;&#33457;&#36153;&#20102;5200&#32654;&#20803;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#22522;&#20110;&#24544;&#23454;&#24615;&#23545;LLM&#25688;&#35201;&#36827;&#34892;&#25490;&#21517;&#65306;Claude-3-Opus&#22312;&#24544;&#23454;&#24615;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#25152;&#26377;&#38381;&#28304;LLMs&#65292;&#32780;&#24320;&#28304;&#30340;Mixtral&#19982;GPT-3.5-Turbo&#25345;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01261v1 Announce Type: cross  Abstract: While long-context large language models (LLMs) can technically summarize book-length documents (&gt;100K tokens), the length and complexity of the documents have so far prohibited evaluations of input-dependent aspects like faithfulness. In this paper, we conduct the first large-scale human evaluation of faithfulness and content selection on LLM-generated summaries of fictional books. Our study mitigates the issue of data contamination by focusing on summaries of books published in 2023 or 2024, and we hire annotators who have fully read each book prior to the annotation task to minimize cost and cognitive burden. We collect FABLES, a dataset of annotations on 3,158 claims made in LLM-generated summaries of 26 books, at a cost of $5.2K USD, which allows us to rank LLM summarizers based on faithfulness: Claude-3-Opus significantly outperforms all closed-source LLMs, while the open-source Mixtral is on par with GPT-3.5-Turbo. An analysis o
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;UniArk&#26694;&#26550;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36866;&#37197;&#22120;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21435;&#20559;&#24046;&#30340;&#26041;&#24335;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#26410;&#35265;&#25552;&#31034;&#19979;&#30340;&#27867;&#21270;&#24615;&#21644;&#19968;&#33268;&#24615;</title><link>https://arxiv.org/abs/2404.01253</link><description>&lt;p&gt;
UniArk: &#36890;&#36807;&#21435;&#20559;&#24046;&#25552;&#39640;&#20107;&#23454;&#30693;&#35782;&#25552;&#21462;&#30340;&#27867;&#21270;&#24615;&#21644;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
UniArk: Improving Generalisation and Consistency for Factual Knowledge Extraction through Debiasing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01253
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;UniArk&#26694;&#26550;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36866;&#37197;&#22120;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21435;&#20559;&#24046;&#30340;&#26041;&#24335;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#26410;&#35265;&#25552;&#31034;&#19979;&#30340;&#27867;&#21270;&#24615;&#21644;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#19968;&#20123;&#35770;&#25991;&#25506;&#35752;&#20102;&#23558;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#30693;&#35782;&#24211;&#30340;&#28508;&#21147;&#65292;&#20197;&#21450;&#22312;&#25552;&#21462;&#20107;&#23454;&#30693;&#35782;&#26102;&#23384;&#22312;&#20005;&#37325;&#20559;&#35265;&#30340;&#23384;&#22312;&#12290;&#26412;&#25991;&#32858;&#28966;&#20110;&#20174;&#24494;&#35843;&#20013;&#35266;&#23519;&#20107;&#23454;&#25506;&#27979;&#24615;&#33021;&#65292;&#24182;&#20351;&#29992;&#27010;&#29575;&#35270;&#35282;&#23637;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#24494;&#35843;&#30446;&#26631;&#20043;&#38388;&#22266;&#26377;&#30340;&#38169;&#20301;&#12290;&#25105;&#20204;&#20551;&#35774;&#21516;&#26102;&#21435;&#20559;&#24046;&#36825;&#20123;&#30446;&#26631;&#21487;&#33021;&#26159;&#27867;&#21270;&#21040;&#26410;&#35265;&#25552;&#31034;&#30340;&#20851;&#38190;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36866;&#37197;&#22120;&#30340;&#26694;&#26550;UniArk&#65292;&#36890;&#36807;&#31616;&#21333;&#26041;&#27861;&#23454;&#29616;&#27867;&#21270;&#21644;&#19968;&#33268;&#30340;&#20107;&#23454;&#30693;&#35782;&#25552;&#21462;&#65292;&#32780;&#19981;&#24341;&#20837;&#39069;&#22806;&#21442;&#25968;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;UniArk&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#22312;&#39046;&#22495;&#22806;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#20197;&#21450;&#22312;&#21508;&#31181;&#25552;&#31034;&#19979;&#30340;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;ParaTrex&#65292;&#19968;&#20010;&#29992;&#20110;&#27979;&#37327;&#30683;&#30462;&#30340;&#22823;&#35268;&#27169;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01253v1 Announce Type: new  Abstract: Several recent papers have investigated the potential of language models as knowledge bases as well as the existence of severe biases when extracting factual knowledge. In this work, we focus on the factual probing performance over unseen prompts from tuning, and using a probabilistic view we show the inherent misalignment between pre-training and downstream tuning objectives in language models for probing knowledge. We hypothesize that simultaneously debiasing these objectives can be the key to generalisation over unseen prompts. We propose an adapter-based framework, UniArk, for generalised and consistent factual knowledge extraction through simple methods without introducing extra parameters. Extensive experiments show that UniArk can significantly improve the model's out-of-domain generalisation as well as consistency under various prompts. Additionally, we construct ParaTrex, a large-scale and diverse dataset for measuring the incon
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#39318;&#27425;&#23581;&#35797;&#32763;&#35793;&#22270;&#20687;&#20197;&#20351;&#20854;&#20855;&#26377;&#25991;&#21270;&#30456;&#20851;&#24615;&#65292;&#26500;&#24314;&#20102;&#35780;&#20272;&#25968;&#25454;&#38598;&#24182;&#36827;&#34892;&#20102;&#22810;&#26041;&#38754;&#30340;&#20154;&#31867;&#35780;&#20272;&#65292;&#21457;&#29616;&#30446;&#21069;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2404.01247</link><description>&lt;p&gt;
&#22270;&#29255;&#34429;&#28982;&#20195;&#34920;&#21315;&#35328;&#19975;&#35821;&#65292;&#20294;&#27599;&#20010;&#20154;&#37117;&#33021;&#21548;&#25026;&#21527;&#65311;&#20851;&#20110;&#32763;&#35793;&#20855;&#26377;&#25991;&#21270;&#30456;&#20851;&#24615;&#30340;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
An image speaks a thousand words, but can everyone listen? On translating images for cultural relevance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01247
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#39318;&#27425;&#23581;&#35797;&#32763;&#35793;&#22270;&#20687;&#20197;&#20351;&#20854;&#20855;&#26377;&#25991;&#21270;&#30456;&#20851;&#24615;&#65292;&#26500;&#24314;&#20102;&#35780;&#20272;&#25968;&#25454;&#38598;&#24182;&#36827;&#34892;&#20102;&#22810;&#26041;&#38754;&#30340;&#20154;&#31867;&#35780;&#20272;&#65292;&#21457;&#29616;&#30446;&#21069;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22810;&#23186;&#20307;&#20869;&#23481;&#30340;&#20852;&#36215;&#65292;&#20154;&#31867;&#32763;&#35793;&#36234;&#26469;&#36234;&#22810;&#22320;&#20851;&#27880;&#20110;&#25991;&#21270;&#36866;&#24212;&#65292;&#19981;&#20165;&#38480;&#20110;&#25991;&#23383;&#65292;&#36824;&#21253;&#25324;&#22270;&#29255;&#31561;&#20854;&#20182;&#24418;&#24335;&#65292;&#20197;&#20256;&#36798;&#30456;&#21516;&#30340;&#21547;&#20041;&#12290;&#34429;&#28982;&#26377;&#20960;&#31181;&#24212;&#29992;&#23558;&#21463;&#30410;&#20110;&#36825;&#19968;&#28857;&#65292;&#20294;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#20173;&#28982;&#23616;&#38480;&#20110;&#22788;&#29702;&#35821;&#35328;&#30340;&#21475;&#22836;&#21644;&#25991;&#23383;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36808;&#20986;&#20102;&#32763;&#35793;&#22270;&#20687;&#20197;&#20351;&#20854;&#20855;&#26377;&#25991;&#21270;&#30456;&#20851;&#24615;&#30340;&#31532;&#19968;&#27493;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19977;&#20010;&#21253;&#21547;&#26368;&#20808;&#36827;&#29983;&#25104;&#27169;&#22411;&#30340;&#27969;&#27700;&#32447;&#26469;&#23436;&#25104;&#36825;&#39033;&#20219;&#21153;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#30001;&#20004;&#37096;&#20998;&#32452;&#25104;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#65306;i&#65289;&#27010;&#24565;&#65306;&#21253;&#25324;600&#24352;&#36328;&#25991;&#21270;&#36830;&#36143;&#30340;&#22270;&#20687;&#65292;&#27599;&#24352;&#22270;&#20687;&#19987;&#27880;&#20110;&#21333;&#20010;&#27010;&#24565;&#65292;ii&#65289;&#24212;&#29992;&#65306;&#21253;&#25324;&#20174;&#23454;&#38469;&#24212;&#29992;&#20013;&#31579;&#36873;&#20986;&#30340;100&#24352;&#22270;&#20687;&#12290;&#25105;&#20204;&#23545;&#32763;&#35793;&#21518;&#30340;&#22270;&#20687;&#36827;&#34892;&#20102;&#22810;&#26041;&#38754;&#30340;&#20154;&#31867;&#35780;&#20272;&#65292;&#20197;&#35780;&#20272;&#20854;&#25991;&#21270;&#30456;&#20851;&#24615;&#21644;&#21547;&#20041;&#20445;&#30041;&#12290;&#25105;&#20204;&#21457;&#29616;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#22312;&#36825;&#39033;&#20219;&#21153;&#19978;&#22833;&#36133;&#20102;&#65292;&#20294;&#21487;&#20197;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01247v1 Announce Type: new  Abstract: Given the rise of multimedia content, human translators increasingly focus on culturally adapting not only words but also other modalities such as images to convey the same meaning. While several applications stand to benefit from this, machine translation systems remain confined to dealing with language in speech and text. In this work, we take a first step towards translating images to make them culturally relevant. First, we build three pipelines comprising state-of-the-art generative models to do the task. Next, we build a two-part evaluation dataset: i) concept: comprising 600 images that are cross-culturally coherent, focusing on a single concept per image, and ii) application: comprising 100 images curated from real-world applications. We conduct a multi-faceted human evaluation of translated images to assess for cultural relevance and meaning preservation. We find that as of today, image-editing models fail at this task, but can 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#30340;&#32479;&#35745;&#25928;&#29575;&#21644;&#26816;&#27979;&#35268;&#21017;&#65292;&#36890;&#36807;&#20851;&#38190;&#32479;&#35745;&#37327;&#21644;&#31192;&#23494;&#23494;&#38053;&#25511;&#21046;&#35823;&#25253;&#29575;&#65292;&#21516;&#26102;&#35780;&#20272;&#27700;&#21360;&#26816;&#27979;&#35268;&#21017;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.01245</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#30340;&#32479;&#35745;&#26694;&#26550;: &#26530;&#36724;&#12289;&#26816;&#27979;&#25928;&#29575;&#21644;&#26368;&#20248;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
A Statistical Framework of Watermarks for Large Language Models: Pivot, Detection Efficiency and Optimal Rules
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01245
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#30340;&#32479;&#35745;&#25928;&#29575;&#21644;&#26816;&#27979;&#35268;&#21017;&#65292;&#36890;&#36807;&#20851;&#38190;&#32479;&#35745;&#37327;&#21644;&#31192;&#23494;&#23494;&#38053;&#25511;&#21046;&#35823;&#25253;&#29575;&#65292;&#21516;&#26102;&#35780;&#20272;&#27700;&#21360;&#26816;&#27979;&#35268;&#21017;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;ChatGPT&#20110;2022&#24180;11&#26376;&#25512;&#20986;&#20197;&#26469;&#65292;&#23558;&#20960;&#20046;&#19981;&#21487;&#23519;&#35273;&#30340;&#32479;&#35745;&#20449;&#21495;&#23884;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#65292;&#20063;&#34987;&#31216;&#20026;&#27700;&#21360;&#65292;&#24050;&#34987;&#29992;&#20316;&#20174;&#20854;&#20154;&#31867;&#25776;&#20889;&#23545;&#24212;&#29289;&#19978;&#21487;&#35777;&#26816;&#27979;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#21407;&#21017;&#24615;&#26041;&#27861;&#12290; &#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#29992;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25512;&#29702;&#27700;&#21360;&#30340;&#32479;&#35745;&#25928;&#29575;&#24182;&#35774;&#35745;&#24378;&#22823;&#30340;&#26816;&#27979;&#35268;&#21017;&#12290;&#21463;&#27700;&#21360;&#26816;&#27979;&#30340;&#20551;&#35774;&#26816;&#39564;&#20844;&#24335;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#39318;&#20808;&#36873;&#25321;&#25991;&#26412;&#30340;&#26530;&#36724;&#32479;&#35745;&#37327;&#21644;&#30001;LLM&#25552;&#20379;&#32473;&#39564;&#35777;&#22120;&#30340;&#31192;&#23494;&#23494;&#38053;&#65292;&#20197;&#23454;&#29616;&#25511;&#21046;&#35823;&#25253;&#29575;&#65288;&#23558;&#20154;&#31867;&#25776;&#20889;&#30340;&#25991;&#26412;&#38169;&#35823;&#22320;&#26816;&#27979;&#20026;LLM&#29983;&#25104;&#30340;&#38169;&#35823;&#65289;&#12290; &#25509;&#19979;&#26469;&#65292;&#35813;&#26694;&#26550;&#20801;&#35768;&#36890;&#36807;&#33719;&#21462;&#28176;&#36817;&#38169;&#35823;&#36127;&#29575;&#65288;&#23558;LLM&#29983;&#25104;&#25991;&#26412;&#38169;&#35823;&#22320;&#26816;&#27979;&#20026;&#20154;&#31867;&#25776;&#20889;&#30340;&#38169;&#35823;&#65289;&#30340;&#23553;&#38381;&#24418;&#24335;&#34920;&#36798;&#24335;&#26469;&#35780;&#20272;&#27700;&#21360;&#26816;&#27979;&#35268;&#21017;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01245v1 Announce Type: cross  Abstract: Since ChatGPT was introduced in November 2022, embedding (nearly) unnoticeable statistical signals into text generated by large language models (LLMs), also known as watermarking, has been used as a principled approach to provable detection of LLM-generated text from its human-written counterpart. In this paper, we introduce a general and flexible framework for reasoning about the statistical efficiency of watermarks and designing powerful detection rules. Inspired by the hypothesis testing formulation of watermark detection, our framework starts by selecting a pivotal statistic of the text and a secret key -- provided by the LLM to the verifier -- to enable controlling the false positive rate (the error of mistakenly detecting human-written text as LLM-generated). Next, this framework allows one to evaluate the power of watermark detection rules by obtaining a closed-form expression of the asymptotic false negative rate (the error of 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;Lottery Ticket Prompt-learning&#65288;LTP&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20013;&#22870;&#31080;&#19982;&#36719;&#25552;&#31034;&#30456;&#32467;&#21512;&#65292;&#20026;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#31616;&#21333;&#12289;&#21482;&#38656;&#19968;&#27425;&#25191;&#34892;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#36827;&#34892;&#36328;&#35821;&#35328;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2404.01242</link><description>&lt;p&gt;
&#36890;&#36807;&#32988;&#20986;&#30340;&#31080;&#26041;&#27861;&#26377;&#25928;&#22320;&#24341;&#23548;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36328;&#35821;&#35328;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Effectively Prompting Small-sized Language Models for Cross-lingual Tasks via Winning Tickets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01242
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;Lottery Ticket Prompt-learning&#65288;LTP&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20013;&#22870;&#31080;&#19982;&#36719;&#25552;&#31034;&#30456;&#32467;&#21512;&#65292;&#20026;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#31616;&#21333;&#12289;&#21482;&#38656;&#19968;&#27425;&#25191;&#34892;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#36827;&#34892;&#36328;&#35821;&#35328;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#36719;&#25552;&#31034;&#26041;&#27861;&#22312;&#24212;&#29992;&#20110;&#23567;&#22411;&#27169;&#22411;&#65288;&#23569;&#20110;&#21313;&#20159;&#20010;&#21442;&#25968;&#65289;&#26102;&#24615;&#33021;&#26377;&#38480;&#12290;&#28145;&#24230;&#25552;&#31034;&#35843;&#25972;&#28041;&#21450;&#22312;&#27599;&#23618;&#20013;&#28155;&#21152;&#21442;&#25968;&#20197;&#22686;&#24378;&#25928;&#21147;&#65292;&#20026;&#23567;&#22411;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#25552;&#31034;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23613;&#31649;&#38656;&#35201;&#31934;&#24515;&#35774;&#35745;&#30340;&#23454;&#26045;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#23558;&#20013;&#22870;&#31080;&#19982;&#36719;&#25552;&#31034;&#30456;&#32467;&#21512;&#30340;Lottery Ticket Prompt-learning&#65288;LTP&#65289;&#26694;&#26550;&#12290;LTP&#25552;&#20379;&#20102;&#26356;&#31616;&#21333;&#30340;&#23454;&#26045;&#65292;&#24182;&#19988;&#21482;&#38656;&#35201;&#25191;&#34892;&#19968;&#27425;&#12290;&#25105;&#20204;&#22312;&#36328;&#35821;&#35328;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;LTP&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#20381;&#36182;&#20110;&#22806;&#37096;&#24037;&#20855;&#65292;&#22914;&#20154;&#24037;&#35774;&#35745;&#30340;&#22810;&#35821;&#35328;&#27169;&#26495;&#21644;&#21452;&#35821;&#35789;&#20856;&#65292;&#36825;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#21487;&#33021;&#19981;&#21487;&#34892;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#22312;&#20351;&#29992;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#30446;&#26631;&#36827;&#34892;&#24494;&#35843;&#26102;&#21464;&#21270;&#26368;&#22823;&#30340;&#19968;&#37096;&#20998;&#21442;&#25968;&#65292;&#28982;&#21518;&#22312;&#21407;&#22987;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20043;&#21069;&#28155;&#21152;&#36719;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01242v1 Announce Type: new  Abstract: Current soft prompt methods yield limited performance when applied to small-sized models (fewer than a billion parameters). Deep prompt-tuning, which entails prepending parameters in each layer for enhanced efficacy, presents a solution for prompting small-sized models, albeit requiring carefully designed implementation. In this paper, we introduce the Lottery Ticket Prompt-learning (LTP) framework that integrates winning tickets with soft prompts. The LTP offers a simpler implementation and requires only a one-time execution. We demonstrate LTP on cross-lingual tasks, where prior works rely on external tools like human-designed multilingual templates and bilingual dictionaries, which may not be feasible in a low-resource regime. Specifically, we select a subset of parameters that have been changed the most during the fine-tuning with the Masked Language Modeling objective. Then, we prepend soft prompts to the original pre-trained langua
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26032;&#24418;&#24335;&#30340;&#33258;&#21160;&#21270;&#35821;&#20041;&#23631;&#24149;&#29702;&#35299;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AURORA&#30340;&#25216;&#26415;&#65292;&#26088;&#22312;&#24110;&#21161;AIG&#24037;&#20855;&#26377;&#25928;&#22320;&#23548;&#33322;&#24212;&#29992;&#31243;&#24207;&#25506;&#32034;&#36807;&#31243;&#20013;&#30340;&#8220;tarpits&#8221;&#12290;</title><link>https://arxiv.org/abs/2404.01240</link><description>&lt;p&gt;
AURORA: &#36890;&#36807;&#33258;&#21160;&#21270;&#31070;&#32463;&#23631;&#24149;&#29702;&#35299;&#26469;&#23548;&#33322;UI&#8220;TarPits&#8221;
&lt;/p&gt;
&lt;p&gt;
AURORA: Navigating UI Tarpits via Automated Neural Screen Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01240
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26032;&#24418;&#24335;&#30340;&#33258;&#21160;&#21270;&#35821;&#20041;&#23631;&#24149;&#29702;&#35299;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AURORA&#30340;&#25216;&#26415;&#65292;&#26088;&#22312;&#24110;&#21161;AIG&#24037;&#20855;&#26377;&#25928;&#22320;&#23548;&#33322;&#24212;&#29992;&#31243;&#24207;&#25506;&#32034;&#36807;&#31243;&#20013;&#30340;&#8220;tarpits&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36719;&#20214;&#24037;&#31243;&#39046;&#22495;&#65292;&#36817;&#21313;&#24180;&#26469;&#30340;&#30740;&#31350;&#20391;&#37325;&#20110;&#33258;&#21160;&#21270;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#27979;&#35797;&#65292;&#20197;&#24110;&#21161;&#24037;&#31243;&#24072;&#20811;&#26381;&#36719;&#20214;&#24179;&#21488;&#25152;&#38754;&#20020;&#30340;&#29420;&#29305;&#25361;&#25112;&#12290;&#36825;&#39033;&#24037;&#20316;&#20027;&#35201;&#20197;&#33258;&#21160;&#21270;&#36755;&#20837;&#29983;&#25104;&#24037;&#20855;(AIG&#24037;&#20855;)&#30340;&#24418;&#24335;&#20986;&#29616;&#65292;&#21160;&#24577;&#25506;&#32034;&#24212;&#29992;&#31243;&#24207;&#23631;&#24149;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20855;&#21453;&#22797;&#35777;&#26126;&#36798;&#21040;&#30340;&#20195;&#30721;&#35206;&#30422;&#29575;&#20302;&#20110;&#39044;&#26399; - &#23588;&#20854;&#26159;&#22312;&#22797;&#26434;&#30340;&#19987;&#26377;&#24212;&#29992;&#31243;&#24207;&#19978;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23548;&#33268;&#36825;&#20123;&#35206;&#30422;&#29575;&#19981;&#36275;&#30340;&#19968;&#20010;&#20027;&#35201;&#21407;&#22240;&#19982;&#25152;&#35859;&#30340;&#8220;tarpits&#8221;&#26377;&#20851;&#65292;&#21363;&#22797;&#26434;&#19988;&#38590;&#20197;&#23548;&#33322;&#30340;&#23631;&#24149;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01240v1 Announce Type: cross  Abstract: Nearly a decade of research in software engineering has focused on automating mobile app testing to help engineers in overcoming the unique challenges associated with the software platform. Much of this work has come in the form of Automated Input Generation tools (AIG tools) that dynamically explore app screens. However, such tools have repeatedly been demonstrated to achieve lower-than-expected code coverage - particularly on sophisticated proprietary apps. Prior work has illustrated that a primary cause of these coverage deficiencies is related to so-called tarpits, or complex screens that are difficult to navigate.   In this paper, we take a critical step toward enabling AIG tools to effectively navigate tarpits during app exploration through a new form of automated semantic screen understanding. We introduce AURORA, a technique that learns from the visual and textual patterns that exist in mobile app UIs to automatically detect co
&lt;/p&gt;</description></item><item><title>GFLean&#26159;&#19968;&#20010;&#33258;&#21160;&#24418;&#24335;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;GF&#36827;&#34892;&#35299;&#26512;&#21644;&#32447;&#24615;&#21270;&#65292;&#21516;&#26102;&#32467;&#21512;&#31070;&#32463;&#32593;&#32476;&#21644;&#35268;&#21017;&#32763;&#35793;&#31243;&#24207;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;autoformalisation&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2404.01234</link><description>&lt;p&gt;
GFLean: &#22522;&#20110;GF&#30340;Lean&#33258;&#21160;&#24418;&#24335;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
GFLean: An Autoformalisation Framework for Lean via GF
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01234
&lt;/p&gt;
&lt;p&gt;
GFLean&#26159;&#19968;&#20010;&#33258;&#21160;&#24418;&#24335;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;GF&#36827;&#34892;&#35299;&#26512;&#21644;&#32447;&#24615;&#21270;&#65292;&#21516;&#26102;&#32467;&#21512;&#31070;&#32463;&#32593;&#32476;&#21644;&#35268;&#21017;&#32763;&#35793;&#31243;&#24207;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;autoformalisation&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GFLean&#30340;&#33258;&#21160;&#24418;&#24335;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;Lean&#23450;&#29702;&#35777;&#26126;&#22120;&#12290;GFLean&#20351;&#29992;&#19968;&#31181;&#31216;&#20026;Grammatical Framework&#65288;GF&#65289;&#30340;&#39640;&#32423;&#35821;&#27861;&#32534;&#20889;&#24037;&#20855;&#36827;&#34892;&#35299;&#26512;&#21644;&#32447;&#24615;&#21270;&#12290;GFLean&#26159;&#29992;Haskell&#23454;&#29616;&#30340;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;GFLean&#30340;&#21151;&#33021;&#12289;&#20869;&#37096;&#24037;&#20316;&#21407;&#29702;&#24182;&#35752;&#35770;&#20854;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#22914;&#20309;&#23558;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#32763;&#35793;&#31243;&#24207;&#21644;&#22522;&#20110;&#35268;&#21017;&#30340;&#32763;&#35793;&#31243;&#24207;&#32467;&#21512;&#36215;&#26469;&#65292;&#30456;&#20114;&#34917;&#20805;&#65292;&#26500;&#24314;&#24378;&#22823;&#30340;&#33258;&#21160;&#24418;&#24335;&#21270;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01234v1 Announce Type: new  Abstract: We present an autoformalisation framework for the Lean theorem prover, called GFLean. GFLean uses a high-level grammar writing tool called Grammatical Framework (GF) for parsing and linearisation. GFLean is implemented in Haskell. We explain the functionalities of GFLean, its inner working and discuss its limitations. We also discuss how we can use neural network based translation programs and rule based translation programs together complimenting each other to build robust autoformalisation frameworks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24320;&#25918;&#35789;&#27719;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;&#32852;&#37030;&#22810;&#27169;&#24335;&#21407;&#22411;&#65288;Fed-MP&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#26032;&#29992;&#25143;&#25552;&#20986;&#30340;&#28041;&#21450;&#20219;&#24847;&#26410;&#30693;&#31867;&#21035;&#30340;&#26597;&#35810;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.01232</link><description>&lt;p&gt;
&#20855;&#26377;&#22810;&#27169;&#24335;&#21407;&#22411;&#30340;&#24320;&#25918;&#35789;&#27719;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Open-Vocabulary Federated Learning with Multimodal Prototyping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24320;&#25918;&#35789;&#27719;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;&#32852;&#37030;&#22810;&#27169;&#24335;&#21407;&#22411;&#65288;Fed-MP&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#26032;&#29992;&#25143;&#25552;&#20986;&#30340;&#28041;&#21450;&#20219;&#24847;&#26410;&#30693;&#31867;&#21035;&#30340;&#26597;&#35810;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30740;&#31350;&#36890;&#24120;&#20551;&#35774;&#35757;&#32451;&#26631;&#31614;&#31354;&#38388;&#21644;&#27979;&#35797;&#26631;&#31614;&#31354;&#38388;&#26159;&#30456;&#21516;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#24212;&#29992;&#20013;&#65292;&#36825;&#20010;&#20551;&#35774;&#22826;&#29702;&#24819;&#21270;&#20102;&#12290;&#26032;&#29992;&#25143;&#21487;&#33021;&#25552;&#20986;&#28041;&#21450;&#26469;&#33258;&#26410;&#35265;&#31867;&#21035;&#25968;&#25454;&#30340;&#26597;&#35810;&#65292;&#36825;&#20123;&#24320;&#25918;&#35789;&#27719;&#26597;&#35810;&#23558;&#30452;&#25509;&#23548;&#33268;&#36825;&#31181;FL&#31995;&#32479;&#30340;&#32570;&#38519;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26126;&#30830;&#20851;&#27880;FL&#20013;&#23578;&#26410;&#24320;&#21457;&#30340;&#24320;&#25918;&#35789;&#27719;&#25361;&#25112;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#23545;&#20110;&#19968;&#20010;&#26032;&#29992;&#25143;&#65292;&#20840;&#23616;&#26381;&#21153;&#22120;&#24212;&#35813;&#29702;&#35299;&#22905;/&#20182;&#30340;&#26597;&#35810;&#28041;&#21450;&#20219;&#24847;&#26410;&#30693;&#31867;&#21035;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;FL&#29615;&#22659;&#20013;VLMs&#30340;&#26032;&#22411;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;&#32852;&#37030;&#22810;&#27169;&#24335;&#21407;&#22411;&#65288;Fed-MP&#65289;&#12290;Fed-MP&#26681;&#25454;&#36731;&#37327;&#32423;&#23458;&#25143;&#31471;&#27531;&#24046;&#33258;&#36866;&#24212;&#32858;&#21512;&#26412;&#22320;&#27169;&#22411;&#26435;&#37325;&#65292;&#24182;&#26681;&#25454;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#27169;&#24335;&#21407;&#22411;&#26426;&#21046;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01232v1 Announce Type: new  Abstract: Existing federated learning (FL) studies usually assume the training label space and test label space are identical. However, in real-world applications, this assumption is too ideal to be true. A new user could come up with queries that involve data from unseen classes, and such open-vocabulary queries would directly defect such FL systems. Therefore, in this work, we explicitly focus on the under-explored open-vocabulary challenge in FL. That is, for a new user, the global server shall understand her/his query that involves arbitrary unknown classes. To address this problem, we leverage the pre-trained vision-language models (VLMs). In particular, we present a novel adaptation framework tailored for VLMs in the context of FL, named as Federated Multimodal Prototyping (Fed-MP). Fed-MP adaptively aggregates the local model weights based on light-weight client residuals, and makes predictions based on a novel multimodal prototyping mechan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25112;&#30053;&#25512;&#29702;&#39046;&#22495;&#30340;&#29616;&#29366;&#21644;&#26426;&#36935;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#26088;&#22312;&#31995;&#32479;&#26803;&#29702;&#28548;&#28165;&#35813;&#20027;&#39064;&#30340;&#38646;&#25955;&#25991;&#29486;&#65292;&#31361;&#20986;&#20102;&#20854;&#20915;&#31574;&#24615;&#33021;&#30340;&#36328;&#23398;&#31185;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.01230</link><description>&lt;p&gt;
LLM&#20316;&#20026;&#26234;&#22218;&#22242;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25112;&#30053;&#25512;&#29702;&#20013;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
LLM as a Mastermind: A Survey of Strategic Reasoning with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25112;&#30053;&#25512;&#29702;&#39046;&#22495;&#30340;&#29616;&#29366;&#21644;&#26426;&#36935;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#26088;&#22312;&#31995;&#32479;&#26803;&#29702;&#28548;&#28165;&#35813;&#20027;&#39064;&#30340;&#38646;&#25955;&#25991;&#29486;&#65292;&#31361;&#20986;&#20102;&#20854;&#20915;&#31574;&#24615;&#33021;&#30340;&#36328;&#23398;&#31185;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25112;&#30053;&#25512;&#29702;&#39046;&#22495;&#30340;&#29616;&#29366;&#21644;&#26426;&#36935;&#65292;&#25112;&#30053;&#25512;&#29702;&#26159;&#19968;&#31181;&#22797;&#26434;&#24418;&#24335;&#30340;&#25512;&#29702;&#65292;&#38656;&#35201;&#29702;&#35299;&#21644;&#39044;&#27979;&#22810;&#26234;&#20307;&#29615;&#22659;&#20013;&#23545;&#25163;&#34892;&#20026;&#65292;&#24182;&#30456;&#24212;&#35843;&#25972;&#31574;&#30053;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#19982;LLMs&#36827;&#34892;&#25112;&#30053;&#25512;&#29702;&#30456;&#20851;&#30340;&#33539;&#22260;&#12289;&#24212;&#29992;&#12289;&#26041;&#27861;&#35770;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#31361;&#20986;&#20102;&#35813;&#39046;&#22495;&#30340;&#26085;&#30410;&#21457;&#23637;&#20197;&#21450;&#22686;&#24378;&#20854;&#20915;&#31574;&#24615;&#33021;&#30340;&#36328;&#23398;&#31185;&#26041;&#27861;&#12290;&#26412;&#25991;&#26088;&#22312;&#31995;&#32479;&#26803;&#29702;&#24182;&#28548;&#28165;&#35813;&#20027;&#39064;&#30340;&#38646;&#25955;&#25991;&#29486;&#65292;&#25552;&#20379;&#19968;&#20010;&#24378;&#35843;&#25112;&#30053;&#25512;&#29702;&#20316;&#20026;&#37325;&#35201;&#35748;&#30693;&#33021;&#21147;&#30340;&#31995;&#32479;&#24615;&#22238;&#39038;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01230v1 Announce Type: new  Abstract: This paper presents a comprehensive survey of the current status and opportunities for Large Language Models (LLMs) in strategic reasoning, a sophisticated form of reasoning that necessitates understanding and predicting adversary actions in multi-agent settings while adjusting strategies accordingly. Strategic reasoning is distinguished by its focus on the dynamic and uncertain nature of interactions among multi-agents, where comprehending the environment and anticipating the behavior of others is crucial. We explore the scopes, applications, methodologies, and evaluation metrics related to strategic reasoning with LLMs, highlighting the burgeoning development in this area and the interdisciplinary approaches enhancing their decision-making performance. It aims to systematize and clarify the scattered literature on this subject, providing a systematic review that underscores the importance of strategic reasoning as a critical cognitive 
&lt;/p&gt;</description></item><item><title>Stable Code&#26159;&#19968;&#20010;&#26032;&#19968;&#20195;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#36890;&#29992;&#22522;&#30784;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#25903;&#25345;&#20195;&#30721;&#23436;&#25104;&#12289;&#25512;&#29702;&#12289;&#25968;&#23398;&#31561;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#65292;&#21516;&#26102;&#36824;&#24341;&#20837;&#20102;&#20855;&#26377;&#33258;&#28982;&#23545;&#35805;&#30028;&#38754;&#30340;&#25351;&#20196;&#21464;&#20307;Stable Code Instruct&#65292;&#21487;&#25191;&#34892;&#38382;&#31572;&#21644;&#22522;&#20110;&#25351;&#20196;&#30340;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2404.01226</link><description>&lt;p&gt;
&#31283;&#23450;&#20195;&#30721;&#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Stable Code Technical Report
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01226
&lt;/p&gt;
&lt;p&gt;
Stable Code&#26159;&#19968;&#20010;&#26032;&#19968;&#20195;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#36890;&#29992;&#22522;&#30784;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#25903;&#25345;&#20195;&#30721;&#23436;&#25104;&#12289;&#25512;&#29702;&#12289;&#25968;&#23398;&#31561;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#65292;&#21516;&#26102;&#36824;&#24341;&#20837;&#20102;&#20855;&#26377;&#33258;&#28982;&#23545;&#35805;&#30028;&#38754;&#30340;&#25351;&#20196;&#21464;&#20307;Stable Code Instruct&#65292;&#21487;&#25191;&#34892;&#38382;&#31572;&#21644;&#22522;&#20110;&#25351;&#20196;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Stable Code&#65292;&#20316;&#20026;&#25105;&#20204;&#26032;&#19968;&#20195;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#20013;&#30340;&#31532;&#19968;&#20010;&#65292;&#23427;&#20316;&#20026;&#19968;&#20010;&#36890;&#29992;&#22522;&#30784;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#30446;&#26631;&#26159;&#23436;&#25104;&#20195;&#30721;&#12289;&#25512;&#29702;&#12289;&#25968;&#23398;&#21644;&#20854;&#20182;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Stable Code Instruct&#30340;&#25351;&#20196;&#21464;&#20307;&#65292;&#20801;&#35768;&#19982;&#27169;&#22411;&#36827;&#34892;&#33258;&#28982;&#23545;&#35805;&#30028;&#38754;&#65292;&#25191;&#34892;&#38382;&#31572;&#21644;&#22522;&#20110;&#25351;&#20196;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#20010;&#25216;&#26415;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#23548;&#33268;&#36825;&#20004;&#20010;&#27169;&#22411;&#30340;&#25968;&#25454;&#21644;&#35757;&#32451;&#36807;&#31243;&#12290;&#23427;&#20204;&#30340;&#26435;&#37325;&#21487;&#20197;&#36890;&#36807;Hugging Face&#33719;&#24471;&#65292;&#20219;&#20309;&#20154;&#37117;&#21487;&#20197;&#22312;https://huggingface.co/stabilityai/stable-code-3b&#21644;https://huggingface.co/stabilityai/stable-code-instruct-3b&#19978;&#19979;&#36733;&#21644;&#20351;&#29992;&#12290;&#35813;&#25253;&#21578;&#21253;&#21547;&#23545;&#27169;&#22411;&#30340;&#24443;&#24213;&#35780;&#20272;&#65292;&#21253;&#25324;&#22810;&#35821;&#35328;&#32534;&#31243;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#21450;&#37325;&#28857;&#25918;&#22312;&#22810;&#36718;&#23545;&#35805;&#30340;MT&#22522;&#20934;&#27979;&#35797;&#19978;&#12290;&#22312;&#21457;&#24067;&#26102;&#65292;Stable Code&#26159;&#26368;&#20808;&#36827;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01226v1 Announce Type: new  Abstract: We introduce Stable Code, the first in our new-generation of code language models series, which serves as a general-purpose base code language model targeting code completion, reasoning, math, and other software engineering-based tasks. Additionally, we introduce an instruction variant named Stable Code Instruct that allows conversing with the model in a natural chat interface for performing question-answering and instruction-based tasks. In this technical report, we detail the data and training procedure leading to both models. Their weights are available via Hugging Face for anyone to download and use at https://huggingface.co/stabilityai/stable-code-3b and https://huggingface.co/stabilityai/stable-code-instruct-3b. This report contains thorough evaluations of the models, including multilingual programming benchmarks, and the MT benchmark focusing on multi-turn dialogues. At the time of its release, Stable Code is the state-of-the-art 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#27169;&#22411;&#35843;&#20248;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#27169;&#22411;&#36827;&#34892;&#38598;&#25104;&#65292;&#25104;&#21151;&#22312;&#24187;&#35273;&#26816;&#27979;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;77.8%&#21644;79.9%&#30340;&#20934;&#30830;&#29575;&#65292;&#20248;&#20110;&#32452;&#32455;&#32773;&#30340;&#22522;&#32447;&#21644;&#31454;&#36187;&#20013;&#20854;&#20182;&#21442;&#36187;&#32773;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2404.01210</link><description>&lt;p&gt;
AILS-NTUA&#21442;&#21152;SemEval-2024&#20219;&#21153;6&#65306;&#29992;&#20110;&#24187;&#35273;&#26816;&#27979;&#21644;&#20998;&#26512;&#30340;&#39640;&#25928;&#27169;&#22411;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
AILS-NTUA at SemEval-2024 Task 6: Efficient model tuning for hallucination detection and analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01210
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#27169;&#22411;&#35843;&#20248;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#27169;&#22411;&#36827;&#34892;&#38598;&#25104;&#65292;&#25104;&#21151;&#22312;&#24187;&#35273;&#26816;&#27979;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;77.8%&#21644;79.9%&#30340;&#20934;&#30830;&#29575;&#65292;&#20248;&#20110;&#32452;&#32455;&#32773;&#30340;&#22522;&#32447;&#21644;&#31454;&#36187;&#20013;&#20854;&#20182;&#21442;&#36187;&#32773;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#25105;&#20204;&#22242;&#38431;&#38024;&#23545;SemEval-2024&#20219;&#21153;6 - SHROOM&#30340;&#25552;&#20132;&#20869;&#23481;&#65292;&#36825;&#26159;&#19968;&#20010;&#20851;&#20110;&#24187;&#35273;&#21644;&#30456;&#20851;&#21487;&#35266;&#27979;&#36807;&#24230;&#29983;&#25104;&#38169;&#35823;&#30340;&#20849;&#20139;&#20219;&#21153;&#12290;&#21442;&#19982;&#32773;&#34987;&#35201;&#27714;&#36827;&#34892;&#20108;&#20803;&#20998;&#31867;&#65292;&#20197;&#35782;&#21035;&#27969;&#21033;&#36807;&#24230;&#29983;&#25104;&#30340;&#24187;&#35273;&#26696;&#20363;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#21253;&#25324;&#23545;&#24187;&#35273;&#26816;&#27979;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#26368;&#25104;&#21151;&#30340;&#31574;&#30053;&#28041;&#21450;&#21019;&#24314;&#36825;&#20123;&#27169;&#22411;&#30340;&#38598;&#25104;&#65292;&#32467;&#26524;&#22312;&#27169;&#22411;&#19981;&#21487;&#30693;&#21644;&#27169;&#22411;&#24863;&#30693;&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#20998;&#21035;&#20026;77.8&#65285;&#21644;79.9&#65285;&#65292;&#36229;&#36807;&#20102;&#32452;&#32455;&#32773;&#30340;&#22522;&#32447;&#65292;&#24182;&#22312;&#19982;&#31454;&#36187;&#20013;&#34920;&#29616;&#26368;&#20339;&#30340;&#32467;&#26524;&#36827;&#34892;&#23545;&#27604;&#26102;&#21462;&#24471;&#26174;&#30528;&#25104;&#26524;&#65292;&#35813;&#31454;&#36187;&#25253;&#21578;&#30340;&#20934;&#30830;&#29575;&#20998;&#21035;&#20026;84.7&#65285;&#21644;81.3&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01210v1 Announce Type: new  Abstract: In this paper, we present our team's submissions for SemEval-2024 Task-6 - SHROOM, a Shared-task on Hallucinations and Related Observable Overgeneration Mistakes. The participants were asked to perform binary classification to identify cases of fluent overgeneration hallucinations. Our experimentation included fine-tuning a pre-trained model on hallucination detection and a Natural Language Inference (NLI) model. The most successful strategy involved creating an ensemble of these models, resulting in accuracy rates of 77.8% and 79.9% on model-agnostic and model-aware datasets respectively, outperforming the organizers' baseline and achieving notable results when contrasted with the top-performing results in the competition, which reported accuracies of 84.7% and 81.3% correspondingly.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#19981;&#21516;&#33021;&#21147;&#30340;&#32508;&#21512;&#20998;&#26512;&#25581;&#31034;&#20102;&#20854;&#21160;&#24577;&#24046;&#24322;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#26631;&#24230;&#23450;&#24459;&#20013;&#30340;&#32570;&#22833;&#12290;</title><link>https://arxiv.org/abs/2404.01204</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#19982;&#19979;&#28216;&#33021;&#21147;&#20998;&#26512;&#30340;&#24494;&#22937;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
The Fine Line: Navigating Large Language Model Pretraining with Down-streaming Capability Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#19981;&#21516;&#33021;&#21147;&#30340;&#32508;&#21512;&#20998;&#26512;&#25581;&#31034;&#20102;&#20854;&#21160;&#24577;&#24046;&#24322;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#26631;&#24230;&#23450;&#24459;&#20013;&#30340;&#32570;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25581;&#31034;&#21453;&#26144;&#26368;&#32456;&#27169;&#22411;&#24615;&#33021;&#30340;&#26089;&#26399;&#25351;&#26631;&#26159;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#19968;&#20010;&#26680;&#24515;&#21407;&#21017;&#12290;&#29616;&#26377;&#30340;&#26631;&#24230;&#23450;&#24459;&#23637;&#31034;&#20102;&#39044;&#35757;&#32451;&#25439;&#22833;&#19982;&#35757;&#32451;&#28014;&#28857;&#25968;&#20043;&#38388;&#30340;&#24130;&#24459;&#30456;&#20851;&#24615;&#65292;&#36825;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24403;&#21069;&#35757;&#32451;&#29366;&#24577;&#30340;&#37325;&#35201;&#25351;&#26631;&#21313;&#20998;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#21407;&#21017;&#21482;&#20851;&#27880;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#30340;&#21387;&#32553;&#29305;&#24615;&#65292;&#23548;&#33268;&#19982;&#19979;&#28216;&#20219;&#21153;&#33021;&#21147;&#30340;&#25552;&#21319;&#20043;&#38388;&#23384;&#22312;&#19981;&#19968;&#33268;&#24615;&#12290;&#19968;&#20123;&#21518;&#32493;&#30740;&#31350;&#35797;&#22270;&#23558;&#26631;&#24230;&#23450;&#24459;&#25193;&#23637;&#21040;&#26356;&#22797;&#26434;&#30340;&#25351;&#26631;&#65288;&#22914;&#36229;&#21442;&#25968;&#65289;&#65292;&#20294;&#20173;&#28982;&#32570;&#20047;&#23545;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#21508;&#31181;&#33021;&#21147;&#20043;&#38388;&#21160;&#24577;&#24046;&#24322;&#30340;&#20840;&#38754;&#20998;&#26512;&#12290;&#20026;&#35299;&#20915;&#19978;&#36848;&#38480;&#21046;&#65292;&#26412;&#25991;&#23545;&#21508;&#31181;&#39044;&#35757;&#32451;&#20013;&#38388;&#26816;&#26597;&#28857;&#19979;&#27169;&#22411;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01204v1 Announce Type: new  Abstract: Uncovering early-stage metrics that reflect final model performance is one core principle for large-scale pretraining. The existing scaling law demonstrates the power-law correlation between pretraining loss and training flops, which serves as an important indicator of the current training state for large language models. However, this principle only focuses on the model's compression properties on the training data, resulting in an inconsistency with the ability improvements on the downstream tasks. Some follow-up works attempted to extend the scaling-law to more complex metrics (such as hyperparameters), but still lacked a comprehensive analysis of the dynamic differences among various capabilities during pretraining. To address the aforementioned limitations, this paper undertakes a comprehensive comparison of model capabilities at various pretraining intermediate checkpoints. Through this analysis, we confirm that specific downstream
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#25991;&#26723;&#32423;&#20998;&#24067;&#20013;&#20272;&#35745;&#35789;&#27719;&#22797;&#26434;&#24230;&#30340;&#20004;&#27493;&#26041;&#27861;&#65292;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#39044;&#27880;&#37322;&#25968;&#25454;&#65292;&#29992;&#20110;&#25903;&#25345;&#20581;&#24247;&#20174;&#19994;&#32773;&#21019;&#24314;&#26356;&#22909;&#30340;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2404.01196</link><description>&lt;p&gt;
&#20174;&#25991;&#26723;&#32423;&#20998;&#24067;&#20013;&#20272;&#35745;&#35789;&#27719;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Estimating Lexical Complexity from Document-Level Distributions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#25991;&#26723;&#32423;&#20998;&#24067;&#20013;&#20272;&#35745;&#35789;&#27719;&#22797;&#26434;&#24230;&#30340;&#20004;&#27493;&#26041;&#27861;&#65292;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#39044;&#27880;&#37322;&#25968;&#25454;&#65292;&#29992;&#20110;&#25903;&#25345;&#20581;&#24247;&#20174;&#19994;&#32773;&#21019;&#24314;&#26356;&#22909;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22797;&#26434;&#24615;&#20272;&#35745;&#26041;&#27861;&#36890;&#24120;&#26159;&#38024;&#23545;&#25972;&#20010;&#25991;&#26723;&#24320;&#21457;&#30340;&#12290;&#36825;&#31181;&#33539;&#22260;&#30340;&#38480;&#21046;&#20351;&#24471;&#23427;&#20204;&#26080;&#27861;&#24212;&#29992;&#20110;&#36739;&#30701;&#30340;&#25991;&#26412;&#29255;&#27573;&#65292;&#22914;&#20581;&#24247;&#35780;&#20272;&#24037;&#20855;&#12290;&#36825;&#20123;&#24037;&#20855;&#36890;&#24120;&#30001;&#29420;&#31435;&#21477;&#23376;&#30340;&#21015;&#34920;&#32452;&#25104;&#65292;&#36825;&#20123;&#21477;&#23376;&#37117;&#22826;&#30701;&#65292;&#20197;&#33268;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#26080;&#27861;&#24212;&#29992;&#12290;&#22312;&#36825;&#20123;&#35780;&#20272;&#24037;&#20855;&#20013;&#25152;&#36873;&#25321;&#30340;&#25514;&#36766;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#21463;&#35775;&#24739;&#32773;&#32676;&#20307;&#30340;&#35748;&#30693;&#33021;&#21147;&#21644;&#35821;&#35328;&#33021;&#21147;&#21487;&#33021;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#20026;&#20102;&#20026;&#25903;&#25345;&#21355;&#29983;&#20174;&#19994;&#32773;&#21019;&#24314;&#26356;&#22909;&#30340;&#24037;&#20855;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#39044;&#27880;&#37322;&#25968;&#25454;&#30340;&#20272;&#35745;&#35789;&#27719;&#22797;&#26434;&#24230;&#30340;&#20004;&#27493;&#26041;&#27861;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#36825;&#20010;&#26041;&#27861;&#29992;&#20110;&#25386;&#23041;&#35821;&#65292;&#24182;&#20351;&#29992;&#32479;&#35745;&#27979;&#35797;&#21644;&#23545;&#23454;&#38469;&#35780;&#20272;&#24037;&#20855;&#26679;&#26412;&#30340;&#23450;&#24615;&#35780;&#20272;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#25105;&#20204;&#30340;&#22797;&#26434;&#24230;&#27979;&#37327;&#21644;&#26576;&#20123;&#29305;&#24449;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01196v1 Announce Type: new  Abstract: Existing methods for complexity estimation are typically developed for entire documents. This limitation in scope makes them inapplicable for shorter pieces of text, such as health assessment tools. These typically consist of lists of independent sentences, all of which are too short for existing methods to apply. The choice of wording in these assessment tools is crucial, as both the cognitive capacity and the linguistic competency of the intended patient groups could vary substantially. As a first step towards creating better tools for supporting health practitioners, we develop a two-step approach for estimating lexical complexity that does not rely on any pre-annotated data. We implement our approach for the Norwegian language and verify its effectiveness using statistical testing and a qualitative evaluation of samples from real assessment tools. We also investigate the relationship between our complexity measure and certain feature
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20174;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#29983;&#25104;&#24544;&#23454;&#23436;&#25972;&#30340;&#21307;&#38498;&#35786;&#30103;&#25688;&#35201;&#65292;&#24182;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#33258;&#21160;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2404.01189</link><description>&lt;p&gt;
&#20174;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#29983;&#25104;&#24544;&#23454;&#23436;&#25972;&#30340;&#21307;&#38498;&#35786;&#30103;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Generating Faithful and Complete Hospital-Course Summaries from the Electronic Health Record
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20174;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#29983;&#25104;&#24544;&#23454;&#23436;&#25972;&#30340;&#21307;&#38498;&#35786;&#30103;&#25688;&#35201;&#65292;&#24182;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#33258;&#21160;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#24555;&#36895;&#26222;&#21450;&#22312;&#31616;&#21270;&#34892;&#25919;&#20219;&#21153;&#12289;&#22686;&#21152;&#36879;&#26126;&#24230;&#20197;&#21450;&#20419;&#36827;&#36328;&#21307;&#25252;&#20154;&#21592;&#36830;&#32493;&#24615;&#25252;&#29702;&#26041;&#38754;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#22686;&#21152;&#20102;&#25991;&#26723;&#35760;&#24405;&#36127;&#25285;&#30340;&#19968;&#20010;&#24847;&#22806;&#21518;&#26524;&#26159;&#20943;&#23569;&#20102;&#19982;&#24739;&#32773;&#38754;&#23545;&#38754;&#30340;&#26102;&#38388;&#65292;&#21516;&#26102;&#20063;&#22823;&#24133;&#22686;&#21152;&#20102;&#20020;&#24202;&#21307;&#29983;&#30340;&#30130;&#21171;&#12290;&#26412;&#35770;&#25991;&#30528;&#37325;&#30740;&#31350;&#19968;&#20010;&#23588;&#20026;&#32791;&#26102;&#20294;&#20851;&#38190;&#30340;&#25991;&#26723;&#35760;&#24405;&#20219;&#21153;&#65306;&#29983;&#25104;&#30149;&#20154;&#20303;&#38498;&#30340;&#25688;&#35201;&#65292;&#24182;&#25552;&#20986;&#24182;&#35780;&#20272;&#33258;&#21160;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#31532;2&#31456;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;10.9&#19975;&#27425;&#20303;&#38498;&#65288;2&#30334;&#19975;&#28304;&#27880;&#37322;&#65289;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#36827;&#34892;&#25506;&#32034;&#24615;&#20998;&#26512;&#20197;&#28608;&#21169;&#26410;&#26469;&#20851;&#20110;&#24314;&#27169;&#21644;&#35780;&#20272;&#30340;&#24037;&#20316;[NAACL 2021]&#12290;&#22312;&#31532;3&#31456;&#20013;&#65292;&#25105;&#20204;&#20174;&#24314;&#27169;&#35282;&#24230;&#35299;&#20915;&#20102;&#24544;&#23454;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#20462;&#35746;&#26377;&#22122;&#21442;&#32771;[EMNLP 2022]&#65292;&#20026;&#20102;&#20943;&#23569;&#23545;&#21442;&#32771;&#30340;&#20381;&#36182;&#65292;&#30452;&#25509;&#26657;&#20934;&#27169;&#22411;&#36755;&#20986;&#21040;&#25351;&#26631;[ACL 202]&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01189v1 Announce Type: new  Abstract: The rapid adoption of Electronic Health Records (EHRs) has been instrumental in streamlining administrative tasks, increasing transparency, and enabling continuity of care across providers. An unintended consequence of the increased documentation burden, however, has been reduced face-time with patients and, concomitantly, a dramatic rise in clinician burnout. In this thesis, we pinpoint a particularly time-intensive, yet critical, documentation task: generating a summary of a patient's hospital admissions, and propose and evaluate automated solutions. In Chapter 2, we construct a dataset based on 109,000 hospitalizations (2M source notes) and perform exploratory analyses to motivate future work on modeling and evaluation [NAACL 2021]. In Chapter 3, we address faithfulness from a modeling perspective by revising noisy references [EMNLP 2022] and, to reduce the reliance on references, directly calibrating model outputs to metrics [ACL 202
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25972;&#21512;&#31070;&#32463;&#31526;&#21495;&#35268;&#21017;&#65292;&#35813;&#23545;&#35805;&#31995;&#32479;&#22312;&#30417;&#27979;&#39135;&#29289;&#20013;&#30340;&#30416;&#21547;&#37327;&#26041;&#38754;&#21462;&#24471;&#20102;&#36229;&#36807;20%&#30340;&#32852;&#21512;&#30446;&#26631;&#20934;&#30830;&#29575;&#30340;&#25913;&#21892;&#12290;</title><link>https://arxiv.org/abs/2404.01182</link><description>&lt;p&gt;
&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#26469;&#30417;&#27979;&#39135;&#29289;&#20013;&#30340;&#30416;&#21547;&#37327;
&lt;/p&gt;
&lt;p&gt;
A Neuro-Symbolic Approach to Monitoring Salt Content in Food
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01182
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25972;&#21512;&#31070;&#32463;&#31526;&#21495;&#35268;&#21017;&#65292;&#35813;&#23545;&#35805;&#31995;&#32479;&#22312;&#30417;&#27979;&#39135;&#29289;&#20013;&#30340;&#30416;&#21547;&#37327;&#26041;&#38754;&#21462;&#24471;&#20102;&#36229;&#36807;20%&#30340;&#32852;&#21512;&#30446;&#26631;&#20934;&#30830;&#29575;&#30340;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#35805;&#31995;&#32479;&#65292;&#20351;&#24515;&#21147;&#34928;&#31469;&#24739;&#32773;&#33021;&#22815;&#26597;&#35810;&#39135;&#29289;&#20013;&#30340;&#30416;&#21547;&#37327;&#65292;&#24182;&#24110;&#21161;&#20182;&#20204;&#30417;&#27979;&#21644;&#20943;&#23569;&#30416;&#30340;&#25668;&#20837;&#37327;&#12290;&#38024;&#23545;&#39135;&#29289;&#30416;&#21547;&#37327;&#26597;&#35810;&#32570;&#20047;&#29305;&#23450;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#22522;&#20110;&#27169;&#26495;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#32467;&#26500;&#21270;&#22320;&#25552;&#20986;&#28548;&#28165;&#38382;&#39064;&#65292;&#20197;&#35782;&#21035;&#39135;&#29289;&#21450;&#20854;&#30416;&#21547;&#37327;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#26377;&#38480;&#65292;&#20294;&#26159;&#25972;&#21512;&#31070;&#32463;&#31526;&#21495;&#35268;&#21017;&#26174;&#33879;&#25552;&#39640;&#20102;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36890;&#36807;&#25972;&#21512;&#31070;&#32463;&#31526;&#21495;&#35268;&#21017;&#65292;&#30456;&#36739;&#20110;&#22825;&#30495;&#22320;&#24494;&#35843;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#19981;&#21516;&#25968;&#25454;&#35268;&#27169;&#19979;&#23454;&#29616;&#20102;&#36229;&#36807;20%&#30340;&#32852;&#21512;&#30446;&#26631;&#20934;&#30830;&#29575;&#30340;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01182v1 Announce Type: new  Abstract: We propose a dialogue system that enables heart failure patients to inquire about salt content in foods and help them monitor and reduce salt intake. Addressing the lack of specific datasets for food-based salt content inquiries, we develop a template-based conversational dataset. The dataset is structured to ask clarification questions to identify food items and their salt content. Our findings indicate that while fine-tuning transformer-based models on the dataset yields limited performance, the integration of Neuro-Symbolic Rules significantly enhances the system's performance. Our experiments show that by integrating neuro-symbolic rules, our system achieves an improvement in joint goal accuracy of over 20% across different data sizes compared to naively fine-tuning transformer-based models.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;LITE&#27169;&#22411;&#65292;&#21487;&#20197;&#23558;&#19981;&#21516;&#30340;&#29615;&#22659;&#21464;&#37327;&#36716;&#25442;&#25104;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#21644;&#25240;&#32447;&#22270;&#20687;&#65292;&#24182;&#21033;&#29992;&#32479;&#19968;&#32534;&#30721;&#22120;&#26469;&#25429;&#25417;&#31354;&#38388;-&#26102;&#38388;&#20851;&#31995;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#39044;&#27979;&#29615;&#22659;&#21464;&#37327;&#12290;</title><link>https://arxiv.org/abs/2404.01165</link><description>&lt;p&gt;
&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#29615;&#22659;&#29983;&#24577;&#31995;&#32479;&#36827;&#34892;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
LITE: Modeling Environmental Ecosystems with Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01165
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;LITE&#27169;&#22411;&#65292;&#21487;&#20197;&#23558;&#19981;&#21516;&#30340;&#29615;&#22659;&#21464;&#37327;&#36716;&#25442;&#25104;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#21644;&#25240;&#32447;&#22270;&#20687;&#65292;&#24182;&#21033;&#29992;&#32479;&#19968;&#32534;&#30721;&#22120;&#26469;&#25429;&#25417;&#31354;&#38388;-&#26102;&#38388;&#20851;&#31995;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#39044;&#27979;&#29615;&#22659;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#29615;&#22659;&#29983;&#24577;&#31995;&#32479;&#36827;&#34892;&#24314;&#27169;&#22312;&#21487;&#25345;&#32493;&#31649;&#29702;&#22320;&#29699;&#30340;&#36807;&#31243;&#20013;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#12290;&#31934;&#30830;&#39044;&#27979;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#30340;&#20851;&#38190;&#29615;&#22659;&#21464;&#37327;&#21487;&#20197;&#24110;&#21161;&#21046;&#23450;&#26126;&#26234;&#30340;&#25919;&#31574;&#21644;&#20915;&#31574;&#65292;&#20174;&#32780;&#25913;&#21892;&#20154;&#20204;&#30340;&#29983;&#27963;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;&#24314;&#27169;&#31354;&#38388;-&#26102;&#38388;&#20851;&#31995;&#20197;&#39044;&#27979;&#29615;&#22659;&#21464;&#37327;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#22312;&#22788;&#29702;&#19981;&#23436;&#25972;&#29305;&#24449;&#21644;&#20998;&#24067;&#21464;&#21270;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#29615;&#22659;&#25968;&#25454;&#20013;&#24120;&#35265;&#36825;&#20123;&#38382;&#39064;&#26159;&#30001;&#20110;&#25968;&#25454;&#25910;&#38598;&#25104;&#26412;&#39640;&#26114;&#21644;&#27979;&#37327;&#20202;&#22120;&#22833;&#28789;&#36896;&#25104;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LITE&#8212;&#8212;&#29992;&#20110;&#29615;&#22659;&#29983;&#24577;&#31995;&#32479;&#24314;&#27169;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;LITE&#36890;&#36807;&#23558;&#19981;&#21516;&#29615;&#22659;&#21464;&#37327;&#36716;&#25442;&#25104;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#21644;&#25240;&#32447;&#22270;&#20687;&#26469;&#32479;&#19968;&#23427;&#20204;&#12290;&#28982;&#21518;&#65292;LITE&#21033;&#29992;&#32479;&#19968;&#32534;&#30721;&#22120;&#26469;&#25429;&#25417;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01165v1 Announce Type: new  Abstract: The modeling of environmental ecosystems plays a pivotal role in the sustainable management of our planet. Accurate prediction of key environmental variables over space and time can aid in informed policy and decision-making, thus improving people's livelihood. Recently, deep learning-based methods have shown promise in modeling the spatial-temporal relationships for predicting environmental variables. However, these approaches often fall short in handling incomplete features and distribution shifts, which are commonly observed in environmental data due to the substantial cost of data collection and malfunctions in measuring instruments. To address these issues, we propose LITE -- a multimodal large language model for environmental ecosystems modeling. Specifically, LITE unifies different environmental variables by transforming them into natural language descriptions and line graph images. Then, LITE utilizes unified encoders to capture 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#39033;&#25552;&#26696;&#65306;&#25945;&#32946;&#12289;&#22522;&#20934;&#27979;&#35797;&#20197;&#21450;&#28041;&#21450;&#19982;&#26426;&#22120;&#20154;&#21475;&#22836;&#20132;&#20114;&#26102;&#23545;&#35821;&#35328;&#24314;&#27169;&#30340;&#30740;&#31350;&#65292;&#20197;&#20419;&#36827;&#25193;&#22823;SLIVAR&#31038;&#21306;&#21442;&#19982;&#21644;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2404.01158</link><description>&lt;p&gt;
&#19982;&#26426;&#22120;&#20154;&#23545;&#35805;&#65306;&#25193;&#22823;SLIVAR&#31038;&#21306;&#21442;&#19982;&#21644;&#30740;&#31350;&#30340;&#25552;&#26696;
&lt;/p&gt;
&lt;p&gt;
Dialogue with Robots: Proposals for Broadening Participation and Research in the SLIVAR Community
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#39033;&#25552;&#26696;&#65306;&#25945;&#32946;&#12289;&#22522;&#20934;&#27979;&#35797;&#20197;&#21450;&#28041;&#21450;&#19982;&#26426;&#22120;&#20154;&#21475;&#22836;&#20132;&#20114;&#26102;&#23545;&#35821;&#35328;&#24314;&#27169;&#30340;&#30740;&#31350;&#65292;&#20197;&#20419;&#36827;&#25193;&#22823;SLIVAR&#31038;&#21306;&#21442;&#19982;&#21644;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#33258;&#28982;&#20154;&#31867;&#35821;&#35328;&#19982;&#26426;&#22120;&#36827;&#34892;&#20132;&#20114;&#30340;&#33021;&#21147;&#19981;&#20165;&#21464;&#24471;&#21496;&#31354;&#35265;&#24815;&#65292;&#32780;&#19988;&#36234;&#26469;&#36234;&#21463;&#21040;&#26399;&#24453;&#12290;&#19979;&#19968;&#20010;&#27493;&#39588;&#19981;&#20165;&#26159;&#25991;&#26412;&#30028;&#38754;&#65292;&#32780;&#26159;&#35821;&#38899;&#30028;&#38754;&#65292;&#19981;&#20165;&#26159;&#19982;&#35745;&#31639;&#26426;&#65292;&#32780;&#26159;&#19982;&#21253;&#25324;&#26426;&#22120;&#20154;&#22312;&#20869;&#30340;&#25152;&#26377;&#26426;&#22120;&#30340;&#23545;&#35805;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35760;&#24405;&#20102;&#26368;&#36817;&#36825;&#19968;&#26085;&#30410;&#22686;&#38271;&#30340;&#19982;&#26426;&#22120;&#20154;&#23545;&#35805;&#39046;&#22495;&#30340;&#21382;&#21490;&#65292;&#24182;&#21521;&#31038;&#21306;&#25552;&#20379;&#20102;&#19977;&#20010;&#25552;&#26696;&#65292;&#31532;&#19968;&#20010;&#20391;&#37325;&#20110;&#25945;&#32946;&#65292;&#31532;&#20108;&#20010;&#20391;&#37325;&#20110;&#22522;&#20934;&#27979;&#35797;&#65292;&#31532;&#19977;&#20010;&#20391;&#37325;&#20110;&#22312;&#28041;&#21450;&#19982;&#26426;&#22120;&#20154;&#30340;&#21475;&#22836;&#20132;&#20114;&#26102;&#23545;&#35821;&#35328;&#30340;&#24314;&#27169;&#12290;&#36825;&#19977;&#20010;&#25552;&#26696;&#24212;&#35813;&#20316;&#20026;&#20219;&#20309;&#30740;&#31350;&#20154;&#21592;&#21487;&#20511;&#37492;&#21644;&#24314;&#31435;&#30340;&#30333;&#30382;&#20070;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01158v1 Announce Type: new  Abstract: The ability to interact with machines using natural human language is becoming not just commonplace, but expected. The next step is not just text interfaces, but speech interfaces and not just with computers, but with all machines including robots. In this paper, we chronicle the recent history of this growing field of spoken dialogue with robots and offer the community three proposals, the first focused on education, the second on benchmarks, and the third on the modeling of language when it comes to spoken interaction with robots. The three proposals should act as white papers for any researcher to take and build upon.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;CO2&#25490;&#25918;&#37327;&#65292;&#24378;&#35843;&#20102;&#36825;&#20123;&#27169;&#22411;&#30001;&#20110;&#20854;&#22823;&#37327;&#30340;&#27169;&#22411;&#21442;&#25968;&#23548;&#33268;&#20854;&#30899;&#36275;&#36857;&#29305;&#21035;&#39640;&#12290;</title><link>https://arxiv.org/abs/2404.01157</link><description>&lt;p&gt;
&#32511;&#33394;AI&#65306;&#25506;&#35752;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#30899;&#36275;&#36857;&#12289;&#32531;&#35299;&#31574;&#30053;&#21644;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Green AI: Exploring Carbon Footprints, Mitigation Strategies, and Trade Offs in Large Language Model Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01157
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;CO2&#25490;&#25918;&#37327;&#65292;&#24378;&#35843;&#20102;&#36825;&#20123;&#27169;&#22411;&#30001;&#20110;&#20854;&#22823;&#37327;&#30340;&#27169;&#22411;&#21442;&#25968;&#23548;&#33268;&#20854;&#30899;&#36275;&#36857;&#29305;&#21035;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#37325;&#35201;&#30740;&#31350;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#19981;&#26029;&#23581;&#35797;&#36890;&#36807;&#25913;&#36827;&#20808;&#21069;&#30340;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#12289;&#25913;&#21464;&#27169;&#22411;&#26550;&#26500;&#21644;&#24320;&#21457;&#26356;&#28145;&#20837;&#30340;&#25968;&#25454;&#38598;&#26469;&#21019;&#36896;&#26032;&#30340;&#21019;&#26032;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;NLP&#39046;&#22495;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#28201;&#23460;&#27668;&#20307;&#25490;&#25918;&#37327;&#19981;&#26029;&#22686;&#21152;&#65292;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;LLMs&#35757;&#32451;&#36896;&#25104;&#30340;&#29615;&#22659;&#25439;&#23475;&#30340;&#25285;&#24551;&#12290;&#23545;&#20154;&#24037;&#26234;&#33021;&#30456;&#20851;&#25104;&#26412;&#30340;&#20840;&#38754;&#20102;&#35299;&#65292;&#23588;&#20854;&#26159;&#19982;&#29615;&#22659;&#30456;&#20851;&#30340;&#25104;&#26412;&#65292;&#26159;&#30830;&#20445;&#23433;&#20840;AI&#27169;&#22411;&#30340;&#22522;&#30784;&#12290;&#30446;&#21069;&#65292;&#23545;AI&#27169;&#22411;&#30340;CO2&#25490;&#25918;&#30340;&#35843;&#26597;&#20173;&#28982;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#30693;&#21517;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;CO2&#25490;&#25918;&#37327;&#65292;&#36825;&#20123;&#27169;&#22411;&#30001;&#20110;&#20854;&#22823;&#37327;&#30340;&#27169;&#22411;&#21442;&#25968;&#32780;&#23548;&#33268;&#20854;&#30899;&#36275;&#36857;&#29305;&#21035;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01157v1 Announce Type: new  Abstract: Prominent works in the field of Natural Language Processing have long attempted to create new innovative models by improving upon previous model training approaches, altering model architecture, and developing more in-depth datasets to better their performance. However, with the quickly advancing field of NLP comes increased greenhouse gas emissions, posing concerns over the environmental damage caused by training LLMs. Gaining a comprehensive understanding of the various costs, particularly those pertaining to environmental aspects, that are associated with artificial intelligence serves as the foundational basis for ensuring safe AI models. Currently, investigations into the CO2 emissions of AI models remain an emerging area of research, and as such, in this paper, we evaluate the CO2 emissions of well-known large language models, which have an especially high carbon footprint due to their significant amount of model parameters. We arg
&lt;/p&gt;</description></item><item><title>LLMs&#22312;&#27169;&#25311;&#22522;&#20110;&#20107;&#23454;&#38382;&#39064;&#30340;&#31038;&#20132;&#23186;&#20307;&#38382;&#39064;&#20013;&#20154;&#31867;&#31572;&#26696;&#26102;&#34920;&#29616;&#36739;&#22909;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2404.01147</link><description>&lt;p&gt;
LLM&#26159;&#21542;&#20250;&#23545;&#22522;&#20110;&#20107;&#23454;&#30340;&#38382;&#39064;&#30340;&#20154;&#31867;&#31572;&#26696;&#24863;&#21040;&#22256;&#24785;&#65311;&#20197;Reddit&#20026;&#20010;&#26696;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Do LLMs Find Human Answers To Fact-Driven Questions Perplexing? A Case Study on Reddit
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01147
&lt;/p&gt;
&lt;p&gt;
LLMs&#22312;&#27169;&#25311;&#22522;&#20110;&#20107;&#23454;&#38382;&#39064;&#30340;&#31038;&#20132;&#23186;&#20307;&#38382;&#39064;&#20013;&#20154;&#31867;&#31572;&#26696;&#26102;&#34920;&#29616;&#36739;&#22909;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#29087;&#32451;&#22320;&#27491;&#30830;&#22238;&#31572;&#22312;&#32447;&#35805;&#35821;&#29615;&#22659;&#19979;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;LLMs&#26469;&#27169;&#25311;&#20154;&#31867;&#23545;&#22522;&#20110;&#20107;&#23454;&#30340;&#31038;&#20132;&#23186;&#20307;&#38382;&#39064;&#30340;&#22238;&#31572;&#20173;&#26410;&#34987;&#20805;&#20998;&#25506;&#35752;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;LLMs&#22914;&#20309;&#27169;&#25311;&#22312;&#20960;&#20010;&#19987;&#39064;&#24615;Reddit&#31038;&#21306;&#65288;&#25110;&#23376;&#31038;&#21306;&#65289;&#20013;&#25552;&#20986;&#30340;&#22522;&#20110;&#20107;&#23454;&#38382;&#39064;&#30340;&#21508;&#31181;&#20154;&#31867;&#31572;&#26696;&#12290;&#25105;&#20204;&#25910;&#38598;&#24182;&#20844;&#24320;&#20102;409&#20010;&#22522;&#20110;&#20107;&#23454;&#38382;&#39064;&#21644;&#26469;&#33258;15&#20010;r/Ask{Topic}&#31038;&#21306;&#30340;7,534&#20010;&#22810;&#26679;&#21270;&#30340;&#12289;&#32463;&#20154;&#31867;&#35780;&#20998;&#30340;&#31572;&#26696;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#31038;&#21306;&#35206;&#30422;&#20102;3&#20010;&#31867;&#21035;&#65306;&#32844;&#19994;&#12289;&#31038;&#20250;&#36523;&#20221;&#21644;&#22320;&#29702;&#20301;&#32622;&#12290;&#25105;&#20204;&#21457;&#29616;LLMs&#22312;&#27169;&#25311;&#39640;&#35780;&#20998;&#30340;&#20154;&#31867;&#31572;&#26696;&#26041;&#38754;&#35201;&#27604;&#27169;&#25311;&#20302;&#35780;&#20998;&#30340;&#20154;&#31867;&#31572;&#26696;&#25928;&#26524;&#26174;&#33879;&#12290;&#25105;&#20204;&#22522;&#20110;&#25105;&#20204;&#30340;&#21021;&#27493;&#21457;&#29616;&#25552;&#20986;&#20102;&#20960;&#20010;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01147v1 Announce Type: new  Abstract: Large language models (LLMs) have been shown to be proficient in correctly answering questions in the context of online discourse. However, the study of using LLMs to model human-like answers to fact-driven social media questions is still under-explored. In this work, we investigate how LLMs model the wide variety of human answers to fact-driven questions posed on several topic-specific Reddit communities, or subreddits. We collect and release a dataset of 409 fact-driven questions and 7,534 diverse, human-rated answers from 15 r/Ask{Topic} communities across 3 categories: profession, social identity, and geographic location. We find that LLMs are considerably better at modeling highly-rated human answers to such questions, as opposed to poorly-rated human answers. We present several directions for future research based on our initial findings.
&lt;/p&gt;</description></item><item><title>KoCoNovel&#26159;&#19968;&#20010;&#20174;&#38889;&#22269;&#25991;&#23398;&#25991;&#26412;&#20013;&#34893;&#29983;&#20986;&#30340;&#23567;&#35828;&#20154;&#29289;&#20849;&#25351;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#27880;&#37322;&#25351;&#21335;&#65292;&#26159;&#38889;&#35821;&#20844;&#20849;&#20849;&#25351;&#35299;&#20915;&#35821;&#26009;&#24211;&#20013;&#31532;&#20108;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#65292;&#20063;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#25991;&#23398;&#25991;&#26412;&#30340;&#25968;&#25454;&#38598;</title><link>https://arxiv.org/abs/2404.01140</link><description>&lt;p&gt;
KoCoNovel&#65306;&#38889;&#22269;&#23567;&#35828;&#20013;&#20154;&#29289;&#20849;&#25351;&#30340;&#27880;&#37322;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
KoCoNovel: Annotated Dataset of Character Coreference in Korean Novels
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01140
&lt;/p&gt;
&lt;p&gt;
KoCoNovel&#26159;&#19968;&#20010;&#20174;&#38889;&#22269;&#25991;&#23398;&#25991;&#26412;&#20013;&#34893;&#29983;&#20986;&#30340;&#23567;&#35828;&#20154;&#29289;&#20849;&#25351;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#27880;&#37322;&#25351;&#21335;&#65292;&#26159;&#38889;&#35821;&#20844;&#20849;&#20849;&#25351;&#35299;&#20915;&#35821;&#26009;&#24211;&#20013;&#31532;&#20108;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#65292;&#20063;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#25991;&#23398;&#25991;&#26412;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;KoCoNovel&#65292;&#36825;&#26159;&#19968;&#20010;&#20174;&#38889;&#22269;&#25991;&#23398;&#25991;&#26412;&#20013;&#34893;&#29983;&#20986;&#30340;&#23567;&#35828;&#20154;&#29289;&#20849;&#25351;&#25968;&#25454;&#38598;&#65292;&#37197;&#22791;&#20102;&#35814;&#32454;&#30340;&#27880;&#37322;&#25351;&#21335;&#12290;KoCoNovel&#30001;50&#37096;&#29616;&#20195;&#21644;&#24403;&#20195;&#38889;&#22269;&#23567;&#35828;&#20013;&#30340;178,000&#20010;&#26631;&#35760;&#32452;&#25104;&#65292;&#26159;&#32487;NIKL&#35821;&#26009;&#24211;&#20043;&#21518;&#38889;&#35821;&#20844;&#20849;&#20849;&#25351;&#35299;&#20915;&#35821;&#26009;&#24211;&#20013;&#31532;&#20108;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#65292;&#20063;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#25991;&#23398;&#25991;&#26412;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#25193;&#22823;&#20854;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22235;&#20010;&#19981;&#21516;&#29256;&#26412;&#30340;KoCoNovel&#65292;&#20026;&#20840;&#30693;&#20316;&#32773;&#21644;&#35835;&#32773;&#30340;&#35266;&#28857;&#20197;&#21450;&#22788;&#29702;&#22810;&#23454;&#20307;&#30340;&#20998;&#31163;&#25110;&#37325;&#21472;&#25552;&#20379;&#36873;&#25321;&#12290;&#36825;&#31181;&#26041;&#27861;&#25972;&#21512;&#20102;&#22260;&#32469;&#25991;&#23398;&#25991;&#26412;&#20849;&#25351;&#35299;&#26512;&#30340;&#29616;&#26377;&#35805;&#35821;&#65292;&#20026;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#12290;KoCoNovel&#30340;&#19968;&#20010;&#26174;&#33879;&#29305;&#28857;&#26159;24%&#30340;&#20154;&#29289;&#25552;&#21450;&#26159;&#21333;&#19968;&#26222;&#36890;&#21517;&#35789;&#65292;&#32570;&#23569;&#25152;&#26377;&#26684;&#26631;&#35760;&#25110;&#20896;&#35789;&#12290;&#36825;&#19968;&#29305;&#24449;&#29305;&#21035;&#21463;&#21040;&#38889;&#35821;&#31216;&#35859;&#30340;&#32454;&#24494;&#24046;&#21035;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01140v1 Announce Type: new  Abstract: We present KoCoNovel, an novel character coreference dataset derived from Korean literary texts, complete with detailed annotation guidelines. Comprising 178K tokens from 50 modern and contemporary Korean novels, KoCoNovel stands as the second-largest public coreference resolution corpus in Korean, after the NIKL corpus, and the first to be based on literary texts. To broaden its utility, we provide four distinct versions of KoCoNovel, offering options for the perspectives of the omniscient author and readers, and for handling multiple entities as either separate or overlapping. This approach integrates existing discourse surrounding coreference resolution in literary texts, providing a comprehensive dataset for exploration. One of KoCoNovel's distinctive features is that 24% of all character mentions are single common nouns, lacking possessive markers or articles. This feature is particularly influenced by the nuances of Korean address 
&lt;/p&gt;</description></item><item><title>&#23558;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#32467;&#21512;&#21040;LLMs&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#26377;&#25928;&#30340;&#26694;&#26550;&#29992;&#20110;&#25913;&#21892;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#35780;&#20272;</title><link>https://arxiv.org/abs/2404.01129</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#20449;&#24687;&#24456;&#37325;&#35201;&#65306;&#23558;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#24341;&#20837;LLMs&#20197;&#25913;&#21892;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Structured Information Matters: Incorporating Abstract Meaning Representation into LLMs for Improved Open-Domain Dialogue Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01129
&lt;/p&gt;
&lt;p&gt;
&#23558;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#32467;&#21512;&#21040;LLMs&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#26377;&#25928;&#30340;&#26694;&#26550;&#29992;&#20110;&#25913;&#21892;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01129v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#33258;&#21160;&#30340;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#35780;&#20272;&#24050;&#32463;&#24341;&#36215;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#21487;&#35757;&#32451;&#30340;&#35780;&#20272;&#25351;&#26631;&#36890;&#24120;&#26159;&#36890;&#36807;&#35757;&#32451;&#20855;&#26377;&#30495;&#27491;&#27491;&#20363;&#21644;&#38543;&#26426;&#36873;&#25321;&#30340;&#36127;&#20363;&#22238;&#22797;&#26469;&#35757;&#32451;&#30340;&#65292;&#23548;&#33268;&#23427;&#20204;&#20542;&#21521;&#20110;&#23558;&#26356;&#39640;&#20869;&#23481;&#30456;&#20284;&#24615;&#30340;&#22238;&#22797;&#20998;&#37197;&#26356;&#39640;&#30340;&#24471;&#20998;&#32473;&#23450;&#19968;&#20010;&#19978;&#19979;&#25991;&#12290;&#28982;&#32780;&#65292;&#23545;&#25239;&#24615;&#30340;&#36127;&#38754;&#22238;&#22797;&#20855;&#26377;&#19982;&#19978;&#19979;&#25991;&#39640;&#20869;&#23481;&#30456;&#20284;&#24615;&#65292;&#21516;&#26102;&#22312;&#35821;&#20041;&#19978;&#19981;&#21516;&#12290;&#22240;&#27492;&#65292;&#29616;&#26377;&#30340;&#35780;&#20272;&#25351;&#26631;&#19981;&#36275;&#20197;&#35780;&#20272;&#36825;&#31867;&#22238;&#22797;&#65292;&#23548;&#33268;&#19982;&#20154;&#31867;&#21028;&#26029;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#36739;&#20302;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#35780;&#20272;&#26041;&#38754;&#26377;&#19968;&#23450;&#25928;&#26524;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#22312;&#26377;&#25928;&#22788;&#29702;&#23545;&#25239;&#24615;&#36127;&#38754;&#31034;&#20363;&#26041;&#38754;&#36935;&#21040;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#29992;&#20110;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#35780;&#20272;&#65292;&#23427;&#32467;&#21512;&#20102;&#39046;&#22495;&#29305;&#23450;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01129v1 Announce Type: new  Abstract: Automatic open-domain dialogue evaluation has attracted increasing attention. Trainable evaluation metrics are commonly trained with true positive and randomly selected negative responses, resulting in a tendency for them to assign a higher score to the responses that share higher content similarity with a given context. However, adversarial negative responses possess high content similarity with the contexts whilst being semantically different. Therefore, existing evaluation metrics are not robust enough to evaluate such responses, resulting in low correlations with human judgments. While recent studies have shown some efficacy in utilizing Large Language Models (LLMs) for open-domain dialogue evaluation, they still encounter challenges in effectively handling adversarial negative examples. In this paper, we propose a simple yet effective framework for open-domain dialogue evaluation, which combines domain-specific language models (SLMs
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#24773;&#24863;&#34920;&#31034;&#36136;&#37327;&#30340;&#26032;&#25351;&#26631;SgTS&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#20855;&#26377;&#24773;&#24863;&#24863;&#30693;&#23545;&#27604;&#30340;&#21477;&#23376;&#23884;&#20837;&#26694;&#26550;SentiCSE&#12290;</title><link>https://arxiv.org/abs/2404.01104</link><description>&lt;p&gt;
SentiCSE: &#19968;&#31181;&#20855;&#26377;&#24773;&#24863;&#24863;&#30693;&#23545;&#27604;&#21477;&#23376;&#23884;&#20837;&#26694;&#26550;&#65292;&#24102;&#26377;&#24773;&#24863;&#24341;&#23548;&#25991;&#26412;&#30456;&#20284;&#24615;
&lt;/p&gt;
&lt;p&gt;
SentiCSE: A Sentiment-aware Contrastive Sentence Embedding Framework with Sentiment-guided Textual Similarity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01104
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#24773;&#24863;&#34920;&#31034;&#36136;&#37327;&#30340;&#26032;&#25351;&#26631;SgTS&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#20855;&#26377;&#24773;&#24863;&#24863;&#30693;&#23545;&#27604;&#30340;&#21477;&#23376;&#23884;&#20837;&#26694;&#26550;SentiCSE&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#24773;&#24863;&#24863;&#30693;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#19979;&#28216;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24573;&#35270;&#20102;&#35780;&#20272;&#26500;&#24314;&#30340;&#24773;&#24863;&#34920;&#31034;&#30340;&#36136;&#37327;&#65307;&#23427;&#20204;&#21482;&#19987;&#27880;&#20110;&#25552;&#39640;&#24494;&#35843;&#24615;&#33021;&#65292;&#36825;&#36974;&#34109;&#20102;&#34920;&#31034;&#36136;&#37327;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22914;&#26524;&#19981;&#33021;&#20445;&#35777;&#34920;&#31034;&#36136;&#37327;&#65292;&#23427;&#20204;&#30340;&#19979;&#28216;&#24615;&#33021;&#21487;&#33021;&#39640;&#24230;&#20381;&#36182;&#20110;&#24494;&#35843;&#25968;&#25454;&#30340;&#30417;&#30563;&#65292;&#32780;&#19981;&#26159;&#34920;&#31034;&#36136;&#37327;&#12290;&#36825;&#20010;&#38382;&#39064;&#20250;&#20351;&#23427;&#20204;&#24456;&#38590;&#36827;&#20837;&#20854;&#20182;&#24773;&#24863;&#30456;&#20851;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#22312;&#26631;&#27880;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#24773;&#24863;&#24341;&#23548;&#25991;&#26412;&#30456;&#20284;&#24615;&#65288;SgTS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#24773;&#24863;&#34920;&#31034;&#36136;&#37327;&#30340;&#26032;&#39062;&#25351;&#26631;&#65292;&#23427;&#22522;&#20110;&#20004;&#20010;&#21477;&#23376;&#20043;&#38388;&#24773;&#24863;&#26497;&#24615;&#31561;&#20215;&#31243;&#24230;&#36827;&#34892;&#35774;&#35745;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;SentiCSE&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#20855;&#26377;&#24773;&#24863;&#24863;&#30693;&#23545;&#27604;&#30340;&#21477;&#23376;&#23884;&#20837;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01104v1 Announce Type: new  Abstract: Recently, sentiment-aware pre-trained language models (PLMs) demonstrate impressive results in downstream sentiment analysis tasks. However, they neglect to evaluate the quality of their constructed sentiment representations; they just focus on improving the fine-tuning performance, which overshadows the representation quality. We argue that without guaranteeing the representation quality, their downstream performance can be highly dependent on the supervision of the fine-tuning data rather than representation quality. This problem would make them difficult to foray into other sentiment-related domains, especially where labeled data is scarce. We first propose Sentiment-guided Textual Similarity (SgTS), a novel metric for evaluating the quality of sentiment representations, which is designed based on the degree of equivalence in sentiment polarity between two sentences. We then propose SentiCSE, a novel Sentiment-aware Contrastive Senten
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21452;&#21521;&#38170;&#23450;&#26041;&#27861;&#65292;&#35782;&#21035;&#37027;&#20123;&#22312;&#24494;&#35843;&#21518;&#26356;&#21487;&#33021;&#38477;&#20302;&#27169;&#22411;&#23433;&#20840;&#24615;&#30340;&#33391;&#24615;&#25968;&#25454;&#23376;&#38598;&#65292;&#25552;&#39640;&#27169;&#22411;&#23545;&#26377;&#23475;&#35831;&#27714;&#30340;&#21709;&#24212;&#29575;&#12290;</title><link>https://arxiv.org/abs/2404.01099</link><description>&lt;p&gt;
&#20320;&#30340;&#8220;&#23433;&#20840;&#8221;&#25968;&#25454;&#20013;&#26377;&#20160;&#20040;&#65311;&#65306;&#35782;&#21035;&#30772;&#22351;&#23433;&#20840;&#24615;&#30340;&#33391;&#24615;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
What's in Your "Safe" Data?: Identifying Benign Data that Breaks Safety
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01099
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21452;&#21521;&#38170;&#23450;&#26041;&#27861;&#65292;&#35782;&#21035;&#37027;&#20123;&#22312;&#24494;&#35843;&#21518;&#26356;&#21487;&#33021;&#38477;&#20302;&#27169;&#22411;&#23433;&#20840;&#24615;&#30340;&#33391;&#24615;&#25968;&#25454;&#23376;&#38598;&#65292;&#25552;&#39640;&#27169;&#22411;&#23545;&#26377;&#23475;&#35831;&#27714;&#30340;&#21709;&#24212;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21363;&#20351;&#32463;&#36807;&#35843;&#25972;&#20197;&#30830;&#20445;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#65292;&#20063;&#23481;&#26131;&#34987;&#36234;&#29425;&#12290;&#19968;&#20123;&#30740;&#31350;&#34920;&#26126;&#65292;&#21482;&#26159;&#36827;&#19968;&#27493;&#20351;&#29992;&#33391;&#24615;&#25968;&#25454;&#65288;&#21363;&#27809;&#26377;&#26377;&#23475;&#20869;&#23481;&#30340;&#25968;&#25454;&#65289;&#23545;&#19968;&#20010;&#23545;&#40784;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20250;&#23548;&#33268;&#23433;&#20840;&#24615;&#22823;&#24133;&#19979;&#38477;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#33391;&#24615;&#24494;&#35843;&#19981;&#32463;&#24847;&#38388;&#23548;&#33268;&#36234;&#29425;&#30340;&#25968;&#25454;&#20013;&#24515;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#20004;&#31181;&#35270;&#35282;&#34920;&#24449;&#24494;&#35843;&#25968;&#25454;&#65306;&#34920;&#31034;&#21644;&#26799;&#24230;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#21521;&#38170;&#23450;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20248;&#20808;&#32771;&#34385;&#38752;&#36817;&#26377;&#23475;&#31034;&#20363;&#24182;&#36828;&#31163;&#33391;&#24615;&#31034;&#20363;&#30340;&#25968;&#25454;&#28857;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#35782;&#21035;&#20986;&#26356;&#26377;&#21487;&#33021;&#22312;&#24494;&#35843;&#21518;&#38477;&#20302;&#27169;&#22411;&#23433;&#20840;&#24615;&#30340;&#33391;&#24615;&#25968;&#25454;&#23376;&#38598;&#12290;&#20165;&#20165;&#35757;&#32451;100&#20010;&#36825;&#20123;&#30475;&#20284;&#33391;&#24615;&#30340;&#25968;&#25454;&#28857;&#65292;&#23601;&#21487;&#20197;&#20351;&#24494;&#35843;&#27169;&#22411;&#32943;&#23450;&#22320;&#22238;&#24212;&#36229;&#36807;70&#65285;&#30340;&#34987;&#27979;&#35797;&#30340;&#26377;&#23475;&#35831;&#27714;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01099v1 Announce Type: cross  Abstract: Current Large Language Models (LLMs), even those tuned for safety and alignment, are susceptible to jailbreaking. Some have found that just further fine-tuning an aligned model with benign data (i.e., data without harmful content) surprisingly leads to substantial degradation in safety. We delve into the data-centric aspects of why benign fine-tuning inadvertently contributes to jailbreaking. First, we represent fine-tuning data through two lenses: representation and gradient spaces. Furthermore, we propose a bi-directional anchoring method that prioritizes data points that are close to harmful examples and distant from benign ones. By doing so, our approach effectively identifies subsets of benign data that are more likely to degrade the model's safety after fine-tuning. Training on just 100 of these seemingly benign datapoints can lead to the fine-tuned model affirmatively responding to &gt; 70% of tested harmful requests, compared to &lt;
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35843;&#25972;&#22810;&#31181;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#22312;SemEval-2024&#20219;&#21153;9&#30340;&#33041;&#31563;&#24613;&#36716;&#24367;&#31454;&#36187;&#20013;&#21462;&#24471;&#20102;&#31454;&#20105;&#21147;&#21313;&#36275;&#30340;&#34920;&#29616;&#65292;&#22312;&#21477;&#23376;&#35868;&#39064;&#21644;&#21333;&#35789;&#35868;&#39064;&#30340;&#35780;&#20272;&#20013;&#65292;&#26368;&#20339;&#25552;&#20132;&#30340;&#20934;&#30830;&#29575;&#20998;&#21035;&#36229;&#36807;&#20102;&#26368;&#20339;&#31070;&#32463;&#22522;&#32447;ChatGPT&#36229;&#36807;20%&#21644;30%&#12290;</title><link>https://arxiv.org/abs/2404.01084</link><description>&lt;p&gt;
AILS-NTUA&#21442;&#21152;SemEval-2024&#20219;&#21153;9&#65306;&#30772;&#35299;&#33041;&#31563;&#24613;&#36716;&#24367;&#65306;&#21464;&#21387;&#22120;&#27169;&#22411;&#29992;&#20110;&#27178;&#21521;&#24605;&#32500;&#35868;&#39064;
&lt;/p&gt;
&lt;p&gt;
AILS-NTUA at SemEval-2024 Task 9: Cracking Brain Teasers: Transformer Models for Lateral Thinking Puzzles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01084
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35843;&#25972;&#22810;&#31181;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#22312;SemEval-2024&#20219;&#21153;9&#30340;&#33041;&#31563;&#24613;&#36716;&#24367;&#31454;&#36187;&#20013;&#21462;&#24471;&#20102;&#31454;&#20105;&#21147;&#21313;&#36275;&#30340;&#34920;&#29616;&#65292;&#22312;&#21477;&#23376;&#35868;&#39064;&#21644;&#21333;&#35789;&#35868;&#39064;&#30340;&#35780;&#20272;&#20013;&#65292;&#26368;&#20339;&#25552;&#20132;&#30340;&#20934;&#30830;&#29575;&#20998;&#21035;&#36229;&#36807;&#20102;&#26368;&#20339;&#31070;&#32463;&#22522;&#32447;ChatGPT&#36229;&#36807;20%&#21644;30%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#25105;&#20204;&#22312;SemEval-2024&#20219;&#21153;9&#31454;&#36187;&#20013;&#30340;&#25552;&#20132;:&#8220;BRAINTEASER:&#19968;&#20010;&#25361;&#25112;&#24120;&#35782;&#30340;&#26032;&#20219;&#21153;&#8221;&#12290;&#25105;&#20204;&#21442;&#19982;&#20102;&#23376;&#20219;&#21153;A-&#21477;&#23376;&#35868;&#39064;&#21644;&#23376;&#20219;&#21153;B-&#21333;&#35789;&#35868;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#24494;&#35843;&#35780;&#20272;&#20102;&#22823;&#37327;&#19981;&#21516;&#23610;&#23544;&#30340;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#27169;&#22411;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23545;&#23427;&#20204;&#30340;&#24471;&#20998;&#21644;&#21709;&#24212;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#20197;&#24110;&#21161;&#26410;&#26469;&#30340;&#30740;&#31350;&#20154;&#21592;&#26377;&#25928;&#22320;&#29702;&#35299;&#21644;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#12290;&#25105;&#20204;&#25490;&#21517;&#38752;&#21069;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;&#23376;&#20219;&#21153;&#30340;&#31454;&#36187;&#25490;&#34892;&#27036;&#19978;&#33719;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#20301;&#32622;&#12290;&#22312;&#35780;&#20272;&#38454;&#27573;&#65292;&#25105;&#20204;&#26368;&#22909;&#30340;&#25552;&#20132;&#22312;&#21477;&#23376;&#35868;&#39064;&#20013;&#33719;&#24471;&#20102;81.7%&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#65292;&#22312;&#21333;&#35789;&#35868;&#39064;&#20013;&#33719;&#24471;&#20102;85.4%&#30340;&#20934;&#30830;&#29575;&#65292;&#20998;&#21035;&#27604;&#26368;&#20339;&#31070;&#32463;&#22522;&#32447;&#65288;ChatGPT&#65289;&#39640;&#20986;&#36229;&#36807;20%&#21644;30%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01084v1 Announce Type: cross  Abstract: In this paper, we outline our submission for the SemEval-2024 Task 9 competition: 'BRAINTEASER: A Novel Task Defying Common Sense'. We engage in both sub-tasks: Sub-task A-Sentence Puzzle and Sub-task B-Word Puzzle. We evaluate a plethora of pre-trained transformer-based language models of different sizes through fine-tuning. Subsequently, we undertake an analysis of their scores and responses to aid future researchers in understanding and utilizing these models effectively. Our top-performing approaches secured competitive positions on the competition leaderboard across both sub-tasks. In the evaluation phase, our best submission attained an average accuracy score of 81.7% in the Sentence Puzzle, and 85.4% in the Word Puzzle, significantly outperforming the best neural baseline (ChatGPT) by more than 20% and 30% respectively.
&lt;/p&gt;</description></item><item><title>&#25552;&#31034;&#24050;&#25104;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#29305;&#23450;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#20027;&#27969;&#33539;&#24335;&#65292;&#32780;&#39640;&#25928;&#25552;&#31034;&#26041;&#27861;&#22312;&#21387;&#32553;&#25552;&#31034;&#21644;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2404.01077</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#25552;&#31034;&#26041;&#27861;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Efficient Prompting Methods for Large Language Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01077
&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24050;&#25104;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#29305;&#23450;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#20027;&#27969;&#33539;&#24335;&#65292;&#32780;&#39640;&#25928;&#25552;&#31034;&#26041;&#27861;&#22312;&#21387;&#32553;&#25552;&#31034;&#21644;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24050;&#25104;&#20026;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20197;&#36866;&#24212;&#29305;&#23450;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#20027;&#27969;&#33539;&#24335;&#12290;&#23613;&#31649;&#36825;&#31181;&#26041;&#27861;&#20026;LLMs&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#25171;&#24320;&#20102;&#22823;&#38376;&#65292;&#20294;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#35745;&#31639;&#36127;&#25285;&#65292;&#21363;&#27169;&#22411;&#25512;&#29702;&#30340;&#35745;&#31639;&#36127;&#25285;&#21644;&#25163;&#21160;&#35774;&#35745;&#25552;&#31034;&#30340;&#20154;&#21147;&#21171;&#21160;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#20887;&#38271;&#21644;&#22797;&#26434;&#30340;&#25552;&#31034;&#26469;&#24341;&#23548;&#21644;&#25511;&#21046;LLMs&#30340;&#34892;&#20026;&#26102;&#12290;&#22240;&#27492;&#65292;LLM&#39046;&#22495;&#35265;&#35777;&#20102;&#39640;&#25928;&#25552;&#31034;&#26041;&#27861;&#30340;&#26174;&#33879;&#28608;&#22686;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#27010;&#36848;&#12290;&#22312;&#36739;&#39640;&#30340;&#23618;&#38754;&#19978;&#65292;&#39640;&#25928;&#25552;&#31034;&#26041;&#27861;&#21487;&#20197;&#24191;&#27867;&#20998;&#31867;&#20026;&#20004;&#31181;&#26041;&#24335;&#65306;&#20855;&#26377;&#39640;&#25928;&#35745;&#31639;&#30340;&#25552;&#31034;&#21644;&#20855;&#26377;&#39640;&#25928;&#35774;&#35745;&#30340;&#25552;&#31034;&#12290;&#21069;&#32773;&#28041;&#21450;&#21508;&#31181;&#21387;&#32553;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#21518;&#32773;&#37319;&#29992;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#25216;&#26415;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#25552;&#31034;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#22238;&#39038;&#20102;&#39640;&#25928;&#25552;&#31034;&#30340;&#36827;&#23637;&#65292;&#24182;&#31361;&#20986;&#26174;&#31034;&#20986;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01077v1 Announce Type: new  Abstract: Prompting has become a mainstream paradigm for adapting large language models (LLMs) to specific natural language processing tasks. While this approach opens the door to in-context learning of LLMs, it brings the additional computational burden of model inference and human effort of manual-designed prompts, particularly when using lengthy and complex prompts to guide and control the behavior of LLMs. As a result, the LLM field has seen a remarkable surge in efficient prompting methods. In this paper, we present a comprehensive overview of these methods. At a high level, efficient prompting methods can broadly be categorized into two approaches: prompting with efficient computation and prompting with efficient design. The former involves various ways of compressing prompts, and the latter employs techniques for automatic prompt optimization. We present the basic concepts of prompting, review the advances for efficient prompting, and highl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#20262;&#29702;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21644;&#25991;&#29486;&#32508;&#36848;&#30830;&#23450;&#21644;&#35299;&#20915;&#20102;&#25968;&#25454;&#22788;&#29702;&#12289;&#38544;&#31169;&#12289;&#25968;&#25454;&#25152;&#26377;&#26435;&#31561;&#26041;&#38754;&#30340;&#20262;&#29702;&#38382;&#39064;&#65292;&#20026;&#30830;&#20445;AI&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#21644;&#25991;&#21270;&#25935;&#24863;&#24615;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2404.01070</link><description>&lt;p&gt;
&#20197;&#35802;&#20449;&#25512;&#21160;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#65306;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#20262;&#29702;&#25361;&#25112;&#19982;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Advancing AI with Integrity: Ethical Challenges and Solutions in Neural Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#20262;&#29702;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21644;&#25991;&#29486;&#32508;&#36848;&#30830;&#23450;&#21644;&#35299;&#20915;&#20102;&#25968;&#25454;&#22788;&#29702;&#12289;&#38544;&#31169;&#12289;&#25968;&#25454;&#25152;&#26377;&#26435;&#31561;&#26041;&#38754;&#30340;&#20262;&#29702;&#38382;&#39064;&#65292;&#20026;&#30830;&#20445;AI&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#21644;&#25991;&#21270;&#25935;&#24863;&#24615;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#31995;&#32479;&#20013;&#30340;&#20262;&#29702;&#25361;&#25112;&#65292;&#24378;&#35843;&#24320;&#21457;&#32773;&#30830;&#20445;&#20844;&#24179;&#24615;&#21644;&#25991;&#21270;&#25935;&#24863;&#24615;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;NMT&#20013;AI&#27169;&#22411;&#30340;&#20262;&#29702;&#32032;&#20859;&#65292;&#23457;&#35270;&#20102;NMT&#24320;&#21457;&#30340;&#27599;&#20010;&#38454;&#27573;&#30340;&#20262;&#29702;&#32771;&#34385;&#65292;&#21253;&#25324;&#25968;&#25454;&#22788;&#29702;&#12289;&#38544;&#31169;&#12289;&#25968;&#25454;&#25152;&#26377;&#26435;&#21644;&#21516;&#24847;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#30830;&#23450;&#24182;&#35299;&#20915;&#20102;&#20262;&#29702;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#21253;&#25324;&#21033;&#29992;Transformer&#27169;&#22411;&#36827;&#34892;&#21346;&#24178;&#36798;&#35821;-&#33521;&#35821;&#32763;&#35793;&#65292;&#20197;&#21450;&#21033;&#29992;&#21477;&#23376;&#36855;&#20320;&#25209;&#22788;&#29702;&#25552;&#39640;&#25928;&#29575;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#32454;&#21270;&#25968;&#25454;&#26631;&#35760;&#25216;&#26415;&#21644;&#23545;&#21346;&#24178;&#36798;&#35821;&#21644;&#33521;&#35821;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#36827;&#34892;BERT&#21644;Longformer&#27169;&#22411;&#30340;&#24494;&#35843;&#31561;&#34917;&#20805;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#31532;&#20108;&#31181;&#26041;&#27861;&#26159;&#26469;&#33258;Google Scholar&#31561;&#25968;&#25454;&#24211;&#21644;GitHub&#31561;&#24179;&#21488;&#30340;&#25991;&#29486;&#32508;&#36848;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#25506;&#35752;&#20102;AI&#31995;&#32479;&#19982;&#30456;&#20851;&#36131;&#20219;&#20043;&#38388;&#30340;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01070v1 Announce Type: cross  Abstract: This paper addresses the ethical challenges of Artificial Intelligence in Neural Machine Translation (NMT) systems, emphasizing the imperative for developers to ensure fairness and cultural sensitivity. We investigate the ethical competence of AI models in NMT, examining the Ethical considerations at each stage of NMT development, including data handling, privacy, data ownership, and consent. We identify and address ethical issues through empirical studies. These include employing Transformer models for Luganda-English translations and enhancing efficiency with sentence mini-batching. And complementary studies that refine data labeling techniques and fine-tune BERT and Longformer models for analyzing Luganda and English social media content. Our second approach is a literature review from databases such as Google Scholar and platforms like GitHub. Additionally, the paper probes the distribution of responsibility between AI systems and 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#36136;&#37327;&#24863;&#30693;&#22810;&#26679;&#36873;&#25321;&#65288;QaDS&#65289;&#31574;&#30053;&#65292;&#24182;&#39564;&#35777;&#20854;&#22312;&#36873;&#25321;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;&#19978;&#30340;&#20248;&#36234;&#24615;&#65307;&#25193;&#22823;&#20102;&#25968;&#25454;&#35268;&#27169;&#12289;&#29992;QaDS&#36873;&#25321;&#30340;&#36890;&#29992;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#23545;&#25968;&#23398;&#25512;&#29702;&#26377;&#24110;&#21161;&#65292;&#26368;&#21518;&#23450;&#20041;&#20102;&#26368;&#20339;&#28151;&#21512;OpenMathMix&#12290;</title><link>https://arxiv.org/abs/2404.01067</link><description>&lt;p&gt;
&#25506;&#32034;&#25968;&#23398;&#25512;&#29702;&#20013;&#25968;&#25454;&#23545;&#25512;&#29702;&#30340;&#24433;&#21709;&#20043;&#35868;
&lt;/p&gt;
&lt;p&gt;
Exploring the Mystery of Influential Data for Mathematical Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01067
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#36136;&#37327;&#24863;&#30693;&#22810;&#26679;&#36873;&#25321;&#65288;QaDS&#65289;&#31574;&#30053;&#65292;&#24182;&#39564;&#35777;&#20854;&#22312;&#36873;&#25321;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;&#19978;&#30340;&#20248;&#36234;&#24615;&#65307;&#25193;&#22823;&#20102;&#25968;&#25454;&#35268;&#27169;&#12289;&#29992;QaDS&#36873;&#25321;&#30340;&#36890;&#29992;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#23545;&#25968;&#23398;&#25512;&#29702;&#26377;&#24110;&#21161;&#65292;&#26368;&#21518;&#23450;&#20041;&#20102;&#26368;&#20339;&#28151;&#21512;OpenMathMix&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36873;&#25321;&#23545;&#19979;&#28216;&#20219;&#21153;&#24494;&#35843;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;&#26159;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20165;&#20351;&#29992;&#26377;&#38480;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#22312;&#36890;&#29992;&#20219;&#21153;&#19978;&#21487;&#20197;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#65292;&#36825;&#31181;&#21487;&#34892;&#24615;&#23578;&#26410;&#24471;&#21040;&#39564;&#35777;&#12290;&#20026;&#27492;&#65292;&#38024;&#23545;&#25968;&#23398;&#25512;&#29702;&#23384;&#22312;&#20004;&#20010;&#24320;&#25918;&#38382;&#39064;&#65306;&#22914;&#20309;&#36873;&#25321;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;&#20197;&#21450;&#20160;&#20040;&#26159;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;&#32452;&#25104;&#12290;&#23545;&#20110;&#21069;&#32773;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#36136;&#37327;&#24863;&#30693;&#22810;&#26679;&#36873;&#25321;&#65288;QaDS&#65289;&#31574;&#30053;&#12290;&#19982;&#20854;&#20182;&#36873;&#25321;&#31574;&#30053;&#36827;&#34892;&#27604;&#36739;&#39564;&#35777;&#20102;QaDS&#30340;&#20248;&#36234;&#24615;&#12290;&#23545;&#20110;&#21518;&#32773;&#65292;&#25105;&#20204;&#39318;&#20808;&#25193;&#22823;&#20102;&#25105;&#20204;&#30340;&#35774;&#32622;&#24182;&#25506;&#32034;&#20102;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;&#32452;&#25104;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#24182;&#24378;&#35843;&#65306;&#25193;&#22823;&#25512;&#29702;&#25968;&#25454;&#35268;&#27169;&#65292;&#24182;&#35757;&#32451;&#36873;&#29992;QaDS&#36873;&#25321;&#30340;&#36890;&#29992;&#25968;&#25454;&#26159;&#26377;&#30410;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26368;&#20339;&#28151;&#21512;&#23450;&#20041;&#20026;OpenMathMix&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01067v1 Announce Type: new  Abstract: Selecting influential data for fine-tuning on downstream tasks is a key factor for both performance and computation efficiency. Recent works have shown that training with only limited data can show a superior performance on general tasks. However, the feasibility on mathematical reasoning tasks has not been validated. To go further, there exist two open questions for mathematical reasoning: how to select influential data and what is an influential data composition. For the former one, we propose a Quality-aware Diverse Selection (QaDS) strategy adaptable for mathematical reasoning. A comparison with other selection strategies validates the superiority of QaDS. For the latter one, we first enlarge our setting and explore the influential data composition. We conduct a series of experiments and highlight: scaling up reasoning data, and training with general data selected by QaDS is helpful. Then, we define our optimal mixture as OpenMathMix
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Regularized Best-of-N (RBoN)&#65292;&#36890;&#36807;&#24341;&#20837;&#25509;&#36817;&#24615;&#39033;&#26469;&#20943;&#36731;&#22870;&#21169;&#27450;&#39575;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#22312;&#35299;&#30721;&#26102;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2404.01054</link><description>&lt;p&gt;
&#27491;&#21017;&#21270;&#30340;&#26368;&#20339;-N&#37319;&#26679;&#20197;&#20943;&#36731;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#20013;&#30340;&#22870;&#21169;&#27450;&#39575;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Regularized Best-of-N Sampling to Mitigate Reward Hacking for Language Model Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01054
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Regularized Best-of-N (RBoN)&#65292;&#36890;&#36807;&#24341;&#20837;&#25509;&#36817;&#24615;&#39033;&#26469;&#20943;&#36731;&#22870;&#21169;&#27450;&#39575;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#22312;&#35299;&#30721;&#26102;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Best-of-N (BoN)&#37319;&#26679;&#19982;&#22870;&#21169;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#29992;&#20110;&#22312;&#35299;&#30721;&#26102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;BoN&#37319;&#26679;&#23481;&#26131;&#21463;&#21040;&#22870;&#21169;&#27450;&#39575;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#38450;&#27490;&#22870;&#21169;&#27450;&#39575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Regularized Best-of-N (RBoN)&#30340;&#21464;&#20307;&#65292;&#36890;&#36807;&#22312;&#21709;&#24212;&#36873;&#25321;&#20013;&#32467;&#21512;&#25509;&#36817;&#24615;&#39033;&#26469;&#20943;&#36731;&#22870;&#21169;&#27450;&#39575;&#65292;&#31867;&#20284;&#20110;&#20559;&#22909;&#23398;&#20064;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01054v1 Announce Type: cross  Abstract: Best-of-N (BoN) sampling with a reward model has been shown to be an effective strategy for aligning Large Language Models (LLMs) to human preferences at the time of decoding. BoN sampling is susceptible to a problem known as reward hacking. Because the reward model is an imperfect proxy for the true objective, over-optimizing its value can compromise its performance on the true objective. A common solution to prevent reward hacking in preference learning techniques is to optimize a reward using proximity regularization (e.g., KL regularization), which ensures that the language model remains close to the reference model. In this research, we propose Regularized Best-of-N (RBoN), a variant of BoN that aims to mitigate reward hacking by incorporating a proximity term in response selection, similar to preference learning techniques. We evaluate two variants of RBoN on the AlpacaFarm dataset and find that they outperform BoN, especially wh
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340; RAG &#26041;&#27861;&#23545;&#26816;&#32034;&#31934;&#24230;&#21644;&#31572;&#26696;&#30456;&#20284;&#24615;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20551;&#35774;&#25991;&#26723;&#23884;&#20837;&#65288;HyDE&#65289;&#21644;LLM &#37325;&#26032;&#25490;&#24207;&#26174;&#33879;&#25552;&#39640;&#20102;&#26816;&#32034;&#31934;&#24230;&#65292;&#32780;&#26368;&#22823;&#36793;&#38469;&#30456;&#20851;&#24615;&#65288;MMR&#65289;&#21644; Cohere &#37325;&#26032;&#25490;&#24207;&#21017;&#27809;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2404.01037</link><description>&lt;p&gt;
ARAGOG: &#39640;&#32423; RAG &#36755;&#20986;&#35780;&#20998;
&lt;/p&gt;
&lt;p&gt;
ARAGOG: Advanced RAG Output Grading
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340; RAG &#26041;&#27861;&#23545;&#26816;&#32034;&#31934;&#24230;&#21644;&#31572;&#26696;&#30456;&#20284;&#24615;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20551;&#35774;&#25991;&#26723;&#23884;&#20837;&#65288;HyDE&#65289;&#21644;LLM &#37325;&#26032;&#25490;&#24207;&#26174;&#33879;&#25552;&#39640;&#20102;&#26816;&#32034;&#31934;&#24230;&#65292;&#32780;&#26368;&#22823;&#36793;&#38469;&#30456;&#20851;&#24615;&#65288;MMR&#65289;&#21644; Cohere &#37325;&#26032;&#25490;&#24207;&#21017;&#27809;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01037v1 &#36890;&#30693;&#31867;&#22411;: &#26032; &#25552;&#35201;: &#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#23545;&#20110;&#23558;&#22806;&#37096;&#30693;&#35782;&#25972;&#21512;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36755;&#20986;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#26377;&#20851;RAG&#30340;&#25991;&#29486;&#36234;&#26469;&#36234;&#22810;&#65292;&#20294;&#20027;&#35201;&#38598;&#20013;&#22312;&#23545;&#26032;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#19982;&#20854;&#21069;&#36523;&#36827;&#34892;&#31995;&#32479;&#24615;&#23457;&#26597;&#21644;&#27604;&#36739;&#65292;&#23384;&#22312;&#22823;&#37327;&#23454;&#39564;&#24615;&#27604;&#36739;&#30740;&#31350;&#30340;&#31354;&#30333;&#12290;&#26412;&#30740;&#31350;&#24320;&#22987;&#30528;&#25163;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#35780;&#20272;&#20102;&#21508;&#31181;RAG&#26041;&#27861;&#23545;&#26816;&#32034;&#31934;&#24230;&#21644;&#31572;&#26696;&#30456;&#20284;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20551;&#35774;&#25991;&#26723;&#23884;&#20837;&#65288;HyDE&#65289;&#21644;LLM &#37325;&#26032;&#25490;&#24207;&#26174;&#33879;&#25552;&#39640;&#20102;&#26816;&#32034;&#31934;&#24230;&#12290;&#28982;&#32780;&#65292;&#26368;&#22823;&#36793;&#38469;&#30456;&#20851;&#24615;&#65288;MMR&#65289;&#21644;Cohere &#37325;&#26032;&#25490;&#24207;&#24182;&#26410;&#34920;&#29616;&#20986;&#26126;&#26174;&#20248;&#21183;&#65292;&#22810;&#26597;&#35810;&#26041;&#27861;&#34920;&#29616;&#19981;&#20339;&#12290;&#21477;&#31383;&#26816;&#32034;&#22312;&#26816;&#32034;&#31934;&#24230;&#26041;&#38754;&#34920;&#29616;&#26368;&#20026;&#26377;&#25928;&#65292;&#23613;&#31649;&#23427;&#22312;&#31572;&#26696;&#30456;&#20284;&#24615;&#19978;&#30340;&#34920;&#29616;&#19981;&#31283;&#23450;&#12290;&#35813;&#30740;&#31350;&#30830;&#35748;&#20102;&#25991;&#26723;&#25688;&#35201;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01037v1 Announce Type: new  Abstract: Retrieval-Augmented Generation (RAG) is essential for integrating external knowledge into Large Language Model (LLM) outputs. While the literature on RAG is growing, it primarily focuses on systematic reviews and comparisons of new state-of-the-art (SoTA) techniques against their predecessors, with a gap in extensive experimental comparisons. This study begins to address this gap by assessing various RAG methods' impacts on retrieval precision and answer similarity. We found that Hypothetical Document Embedding (HyDE) and LLM reranking significantly enhance retrieval precision. However, Maximal Marginal Relevance (MMR) and Cohere rerank did not exhibit notable advantages over a baseline Naive RAG system, and Multi-query approaches underperformed. Sentence Window Retrieval emerged as the most effective for retrieval precision, despite its variable performance on answer similarity. The study confirms the potential of the Document Summary I
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#20998;&#26512;&#39564;&#35777;&#20102;&#20851;&#20110;&#21160;&#35789;&#38544;&#21947;&#30340;&#20027;&#24352;&#65292;&#21457;&#29616;&#38544;&#21947;&#20013;&#21160;&#35789;&#30340;&#30452;&#25509;&#23486;&#35821;&#24448;&#24448;&#20855;&#26377;&#36739;&#20302;&#30340;&#20855;&#20307;&#24615;&#12289;&#21487;&#24418;&#35937;&#21270;&#31243;&#24230;&#21644;&#29087;&#24713;&#24230;&#65292;&#38544;&#21947;&#26356;&#21487;&#33021;&#22312;&#24773;&#24863;&#21644;&#20027;&#35266;&#21477;&#23376;&#20013;&#20351;&#29992;&#12290;</title><link>https://arxiv.org/abs/2404.01029</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#35268;&#27169;&#33258;&#21160;&#38544;&#21947;&#35782;&#21035;&#39564;&#35777;&#20851;&#20110;&#38544;&#21947;&#30340;&#20027;&#24352;
&lt;/p&gt;
&lt;p&gt;
Verifying Claims About Metaphors with Large-Scale Automatic Metaphor Identification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01029
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#20998;&#26512;&#39564;&#35777;&#20102;&#20851;&#20110;&#21160;&#35789;&#38544;&#21947;&#30340;&#20027;&#24352;&#65292;&#21457;&#29616;&#38544;&#21947;&#20013;&#21160;&#35789;&#30340;&#30452;&#25509;&#23486;&#35821;&#24448;&#24448;&#20855;&#26377;&#36739;&#20302;&#30340;&#20855;&#20307;&#24615;&#12289;&#21487;&#24418;&#35937;&#21270;&#31243;&#24230;&#21644;&#29087;&#24713;&#24230;&#65292;&#38544;&#21947;&#26356;&#21487;&#33021;&#22312;&#24773;&#24863;&#21644;&#20027;&#35266;&#21477;&#23376;&#20013;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#19968;&#20123;&#20851;&#20110;&#35789;&#35821;&#26356;&#21487;&#33021;&#34987;&#29992;&#20316;&#38544;&#21947;&#30340;&#24773;&#20917;&#30340;&#35821;&#35328;&#20027;&#24352;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#35797;&#22270;&#29992;&#22823;&#22411;&#35821;&#26009;&#24211;&#39564;&#35777;&#36825;&#20123;&#20027;&#24352;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#38544;&#21947;&#26816;&#27979;&#24212;&#29992;&#20110;&#20174;Common Crawl&#20013;&#25552;&#21462;&#30340;&#21477;&#23376;&#36827;&#34892;&#22823;&#35268;&#27169;&#22522;&#20110;&#35821;&#26009;&#24211;&#30340;&#20998;&#26512;&#65292;&#39564;&#35777;&#20102;&#20851;&#20110;&#21160;&#35789;&#38544;&#21947;&#30340;&#26576;&#20123;&#29616;&#26377;&#20027;&#24352;&#65292;&#24182;&#20351;&#29992;&#32467;&#26524;&#33719;&#24471;&#30340;&#32479;&#35745;&#25968;&#25454;&#12290;&#39564;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#20316;&#20026;&#38544;&#21947;&#20351;&#29992;&#30340;&#21160;&#35789;&#30340;&#30452;&#25509;&#23486;&#35821;&#24448;&#24448;&#20855;&#26377;&#36739;&#20302;&#30340;&#20855;&#20307;&#24615;&#12289;&#21487;&#24418;&#35937;&#21270;&#31243;&#24230;&#21644;&#29087;&#24713;&#24230;&#65292;&#24182;&#19988;&#38544;&#21947;&#26356;&#21487;&#33021;&#22312;&#24773;&#24863;&#21644;&#20027;&#35266;&#21477;&#23376;&#20013;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01029v1 Announce Type: new  Abstract: There are several linguistic claims about situations where words are more likely to be used as metaphors. However, few studies have sought to verify such claims with large corpora. This study entails a large-scale, corpus-based analysis of certain existing claims about verb metaphors, by applying metaphor detection to sentences extracted from Common Crawl and using the statistics obtained from the results. The verification results indicate that the direct objects of verbs used as metaphors tend to have lower degrees of concreteness, imageability, and familiarity, and that metaphors are more likely to be used in emotional and subjective sentences.
&lt;/p&gt;</description></item><item><title>&#28304;&#24863;&#30693;&#35757;&#32451;&#20351;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#30693;&#35782;&#24402;&#22240;&#33021;&#21147;&#65292;&#36827;&#32780;&#22686;&#24378;&#20102;&#20854;&#36879;&#26126;&#24230;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#39564;&#35777;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.01019</link><description>&lt;p&gt;
&#28304;&#24863;&#30693;&#35757;&#32451;&#20351;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#30693;&#35782;&#24402;&#22240;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Source-Aware Training Enables Knowledge Attribution in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01019
&lt;/p&gt;
&lt;p&gt;
&#28304;&#24863;&#30693;&#35757;&#32451;&#20351;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#30693;&#35782;&#24402;&#22240;&#33021;&#21147;&#65292;&#36827;&#32780;&#22686;&#24378;&#20102;&#20854;&#36879;&#26126;&#24230;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#39564;&#35777;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#23398;&#21040;&#20102;&#22823;&#37327;&#30693;&#35782;&#65292;&#20294;&#24448;&#24448;&#23545;&#27492;&#31867;&#30693;&#35782;&#30340;&#26469;&#28304;&#27627;&#19981;&#22312;&#24847;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20869;&#22312;&#28304;&#24341;&#29992;&#38382;&#39064;&#65292;&#35201;&#27714;LLMs&#24341;&#29992;&#25903;&#25345;&#29983;&#25104;&#21709;&#24212;&#30340;&#39044;&#35757;&#32451;&#26469;&#28304;&#12290;&#20869;&#22312;&#28304;&#24341;&#29992;&#21487;&#20197;&#22686;&#24378;LLMs&#30340;&#36879;&#26126;&#24230;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#39564;&#35777;&#24615;&#12290;&#20026;&#36171;&#20104;LLMs&#36825;&#31181;&#33021;&#21147;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#28304;&#24863;&#30693;&#35757;&#32451;&#8212;&#8212;&#19968;&#20010;&#21518;&#39044;&#35757;&#32451;&#37197;&#26041;&#65292;&#21253;&#25324;&#65288;i&#65289;&#35757;&#32451;LLMs&#23558;&#21807;&#19968;&#28304;&#25991;&#26723;&#26631;&#35782;&#31526;&#19982;&#27599;&#20010;&#25991;&#26723;&#20013;&#30340;&#30693;&#35782;&#20851;&#32852;&#36215;&#26469;&#65292;&#28982;&#21518;&#65288;ii&#65289;&#36827;&#34892;&#25351;&#31034;&#35843;&#25972;&#65292;&#25945;&#23548;LLMs&#22312;&#34987;&#25552;&#31034;&#26102;&#24341;&#29992;&#25903;&#25345;&#30340;&#39044;&#35757;&#32451;&#26469;&#28304;&#12290;&#28304;&#24863;&#30693;&#35757;&#32451;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#21363;&#25554;&#21363;&#29992;&#30340;&#39044;&#35757;&#32451;LLMs&#65292;&#24182;&#19982;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;/&#24494;&#35843;&#26694;&#26550;&#30340;&#24046;&#24322;&#26368;&#23567;&#12290;&#36890;&#36807;&#23545;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#35757;&#32451;&#37197;&#26041;&#21487;&#20197;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01019v1 Announce Type: cross  Abstract: Large language models (LLMs) learn a vast amount of knowledge during pretraining, but they are often oblivious to the source(s) of such knowledge. We investigate the problem of intrinsic source citation, where LLMs are required to cite the pretraining source supporting a generated response. Intrinsic source citation can enhance LLM transparency, interpretability, and verifiability. To give LLMs such ability, we explore source-aware training -- a post pretraining recipe that involves (i) training the LLM to associate unique source document identifiers with the knowledge in each document, followed by (ii) an instruction-tuning to teach the LLM to cite a supporting pretraining source when prompted. Source-aware training can easily be applied to pretrained LLMs off the shelf, and diverges minimally from existing pretraining/fine-tuning frameworks. Through experiments on carefully curated data, we demonstrate that our training recipe can en
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;PairEval&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#35805;&#35780;&#20272;&#25351;&#26631;&#65292;&#36890;&#36807;&#23558;&#22238;&#22797;&#30340;&#36136;&#37327;&#19982;&#19981;&#21516;&#23545;&#35805;&#20013;&#30340;&#22238;&#22797;&#36827;&#34892;&#27604;&#36739;&#26469;&#35780;&#20272;&#65292;&#19982;&#20154;&#31867;&#21028;&#26029;&#20855;&#26377;&#26356;&#39640;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.01015</link><description>&lt;p&gt;
PairEval&#65306;&#20351;&#29992;&#20004;&#20004;&#27604;&#36739;&#36827;&#34892;&#24320;&#25918;&#22495;&#23545;&#35805;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
PairEval: Open-domain Dialogue Evaluation with Pairwise Comparison
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01015
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;PairEval&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#35805;&#35780;&#20272;&#25351;&#26631;&#65292;&#36890;&#36807;&#23558;&#22238;&#22797;&#30340;&#36136;&#37327;&#19982;&#19981;&#21516;&#23545;&#35805;&#20013;&#30340;&#22238;&#22797;&#36827;&#34892;&#27604;&#36739;&#26469;&#35780;&#20272;&#65292;&#19982;&#20154;&#31867;&#21028;&#26029;&#20855;&#26377;&#26356;&#39640;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01015v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; &#24314;&#31435;&#21487;&#38752;&#19988;&#33258;&#21160;&#21270;&#30340;&#35780;&#20272;&#25351;&#26631;&#26159;&#24320;&#25918;&#22495;&#23545;&#35805;&#31995;&#32479;&#20013;&#24517;&#19981;&#21487;&#23569;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#35780;&#20272;&#25351;&#26631;&#65292;&#36890;&#36807;&#32771;&#34385;&#29983;&#25104;&#30340;&#22238;&#22797;&#19982;&#20043;&#21069;&#30340;&#23545;&#35805;&#21382;&#21490;&#30340;&#30456;&#20851;&#24615;&#26469;&#35780;&#20272;&#36825;&#20123;&#22238;&#22797;&#12290;&#23613;&#31649;&#26377;&#25928;&#65292;&#20294;&#36825;&#20123;&#25351;&#26631;&#30452;&#25509;&#35780;&#20272;&#21333;&#20010;&#22238;&#22797;&#65292;&#32780;&#26410;&#32771;&#34385;&#20854;&#30456;&#23545;&#36136;&#37327;&#19982;&#20854;&#20182;&#22238;&#22797;&#30456;&#27604;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PairEval&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#35805;&#35780;&#20272;&#25351;&#26631;&#65292;&#36890;&#36807;&#23558;&#22238;&#22797;&#30340;&#36136;&#37327;&#19982;&#19981;&#21516;&#23545;&#35805;&#20013;&#30340;&#22238;&#22797;&#36827;&#34892;&#27604;&#36739;&#26469;&#35780;&#20272;&#12290;PairEval&#24314;&#31435;&#22312;&#24320;&#28304;&#21644;&#20013;&#31561;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#20043;&#19978;&#65292;&#24182;&#20351;&#20854;&#19987;&#38376;&#21270;&#20110;&#23545;&#35805;&#22238;&#22797;&#20043;&#38388;&#30340;&#20004;&#20004;&#27604;&#36739;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#25351;&#26631;&#19982;&#20154;&#31867;&#21028;&#26029;&#21576;&#29616;&#20986;&#26356;&#39640;&#30340;&#30456;&#20851;&#24615;&#36229;&#36807;&#22522;&#32447;&#25351;&#26631;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#25152;&#25552;&#20986;&#30340;&#27604;&#36739;&#24615;&#25351;&#26631;&#22312;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01015v1 Announce Type: new  Abstract: Building a reliable and automated evaluation metric is a necessary but challenging problem for open-domain dialogue systems. Recent studies proposed evaluation metrics that assess generated responses by considering their relevance to previous dialogue histories. Although effective, these metrics evaluate individual responses directly rather than considering their relative quality compared to other responses. To handle this, we propose PairEval, a novel dialogue evaluation metric for assessing responses by comparing their quality against responses in different conversations. PairEval is built on top of open-sourced and moderate-size language models, and we make them specialized in pairwise comparison between dialogue responses. Extensive experiments on multiple benchmarks demonstrate that our metric exhibits a higher correlation with human judgments than baseline metrics. We also find that the proposed comparative metric is more robust in
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#30340;&#26597;&#35810;&#24615;&#33021;&#39044;&#27979;&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#20808;&#21069;&#26041;&#27861;&#20013;&#23545;&#19981;&#21516;IR&#35780;&#20272;&#25351;&#26631;&#20934;&#30830;&#24615;&#21644;&#35299;&#37322;&#24615;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2404.01012</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#26469;&#39044;&#27979;&#26597;&#35810;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Query Performance Prediction using Relevance Judgments Generated by Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01012
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#30340;&#26597;&#35810;&#24615;&#33021;&#39044;&#27979;&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#20808;&#21069;&#26041;&#27861;&#20013;&#23545;&#19981;&#21516;IR&#35780;&#20272;&#25351;&#26631;&#20934;&#30830;&#24615;&#21644;&#35299;&#37322;&#24615;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26597;&#35810;&#24615;&#33021;&#39044;&#27979;&#65288;QPP&#65289;&#26088;&#22312;&#20272;&#35745;&#25628;&#32034;&#31995;&#32479;&#23545;&#26597;&#35810;&#30340;&#26816;&#32034;&#36136;&#37327;&#65292;&#32780;&#26080;&#38656;&#20154;&#24037;&#30456;&#20851;&#24615;&#21028;&#26029;&#12290;&#20808;&#21069;&#30340;QPP&#26041;&#27861;&#36890;&#24120;&#36820;&#22238;&#21333;&#20010;&#26631;&#37327;&#20540;&#65292;&#24182;&#19981;&#35201;&#27714;&#39044;&#27979;&#20540;&#25509;&#36817;&#29305;&#23450;&#30340;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#35780;&#20272;&#25351;&#26631;&#65292;&#20174;&#32780;&#23548;&#33268;&#20197;&#19979;&#26576;&#20123;&#32570;&#28857;&#65306;&#65288;i&#65289;&#21333;&#20010;&#26631;&#37327;&#26080;&#27861;&#20934;&#30830;&#34920;&#31034;&#19981;&#21516;&#30340;IR&#35780;&#20272;&#25351;&#26631;&#65292;&#29305;&#21035;&#26159;&#24403;&#24230;&#37327;&#19981;&#39640;&#24230;&#30456;&#20851;&#26102;&#65292;&#65288;ii&#65289;&#21333;&#20010;&#26631;&#37327;&#38480;&#21046;&#20102;QPP&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#22240;&#20026;&#20165;&#20351;&#29992;&#26631;&#37327;&#26080;&#27861;&#35299;&#37322;QPP&#32467;&#26524;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#30340;QPP&#26694;&#26550;&#65288;QPP-GenRE&#65289;&#65292;&#23558;QPP&#20998;&#35299;&#20026;&#29420;&#31435;&#30340;&#23376;&#20219;&#21153;&#65292;&#21363;&#23545;&#25490;&#21517;&#21015;&#34920;&#20013;&#27599;&#20010;&#39033;&#30446;&#23545;&#32473;&#23450;&#26597;&#35810;&#30340;&#30456;&#20851;&#24615;&#36827;&#34892;&#21028;&#26029;&#12290;&#36825;&#26679;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#29983;&#25104;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#26469;&#39044;&#27979;&#20219;&#20309;IR&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01012v1 Announce Type: cross  Abstract: Query performance prediction (QPP) aims to estimate the retrieval quality of a search system for a query without human relevance judgments. Previous QPP methods typically return a single scalar value and do not require the predicted values to approximate a specific information retrieval (IR) evaluation measure, leading to certain drawbacks: (i) a single scalar is insufficient to accurately represent different IR evaluation measures, especially when metrics do not highly correlate, and (ii) a single scalar limits the interpretability of QPP methods because solely using a scalar is insufficient to explain QPP results. To address these issues, we propose a QPP framework using automatically generated relevance judgments (QPP-GenRE), which decomposes QPP into independent subtasks of judging the relevance of each item in a ranked list to a given query. This allows us to predict any IR evaluation measure using the generated relevance judgment
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;Bhinneka Korpus&#65292;&#19968;&#20010;&#21253;&#21547;&#20116;&#31181;&#21360;&#23612;&#26412;&#22320;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#24179;&#34892;&#35821;&#26009;&#24211;&#65292;&#26088;&#22312;&#22686;&#24378;&#36164;&#28304;&#30340;&#35775;&#38382;&#21644;&#21033;&#29992;&#65292;&#25193;&#23637;&#20854;&#22312;&#22269;&#20869;&#30340;&#24433;&#21709;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.01009</link><description>&lt;p&gt;
&#26500;&#24314;&#21644;&#25193;&#23637;&#21360;&#23612;&#26412;&#22320;&#35821;&#35328;&#30340;&#20302;&#36164;&#28304;&#21644;&#20195;&#34920;&#24615;&#24179;&#34892;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Constructing and Expanding Low-Resource and Underrepresented Parallel Datasets for Indonesian Local Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01009
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;Bhinneka Korpus&#65292;&#19968;&#20010;&#21253;&#21547;&#20116;&#31181;&#21360;&#23612;&#26412;&#22320;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#24179;&#34892;&#35821;&#26009;&#24211;&#65292;&#26088;&#22312;&#22686;&#24378;&#36164;&#28304;&#30340;&#35775;&#38382;&#21644;&#21033;&#29992;&#65292;&#25193;&#23637;&#20854;&#22312;&#22269;&#20869;&#30340;&#24433;&#21709;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21360;&#24230;&#23612;&#35199;&#20122;&#65292;&#26412;&#22320;&#35821;&#35328;&#22312;&#25991;&#21270;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#21360;&#23612;&#35821;&#35328;&#36164;&#28304;&#20173;&#28982;&#23646;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#26377;&#38480;&#25968;&#25454;&#12290;&#24403;&#20026;&#36825;&#20123;&#35821;&#35328;&#26500;&#24314;NLP&#27169;&#22411;&#26102;&#65292;&#36825;&#23601;&#25104;&#20026;&#19968;&#20010;&#38382;&#39064;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Bhinneka Korpus&#65292;&#19968;&#20010;&#21253;&#21547;&#20116;&#31181;&#21360;&#23612;&#26412;&#22320;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#24179;&#34892;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22686;&#24378;&#36825;&#20123;&#36164;&#28304;&#30340;&#35775;&#38382;&#21644;&#21033;&#29992;&#65292;&#25193;&#23637;&#23427;&#20204;&#22312;&#22269;&#20869;&#30340;&#24433;&#21709;&#21147;&#12290;&#25105;&#20204;&#35814;&#32454;&#35828;&#26126;&#20102;&#25968;&#25454;&#38598;&#25910;&#38598;&#36807;&#31243;&#21450;&#30456;&#20851;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#25968;&#25454;&#38480;&#21046;&#65292;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;IBM Model 1&#36827;&#34892;&#32763;&#35793;&#20219;&#21153;&#23454;&#39564;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#27599;&#31181;&#35821;&#35328;&#30340;&#24615;&#33021;&#24050;&#32463;&#26174;&#31034;&#20986;&#36827;&#19968;&#27493;&#21457;&#23637;&#30340;&#33391;&#22909;&#36857;&#35937;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#35832;&#22914;&#35789;&#27719;&#21464;&#24322;&#12289;&#24179;&#28369;&#25928;&#24212;&#21644;&#36328;&#35821;&#35328;&#21464;&#24322;&#31561;&#25361;&#25112;&#12290;&#25105;&#20204;&#25171;&#31639;&#35780;&#20272;&#35813;&#35821;&#26009;&#24211;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01009v1 Announce Type: new  Abstract: In Indonesia, local languages play an integral role in the culture. However, the available Indonesian language resources still fall into the category of limited data in the Natural Language Processing (NLP) field. This is become problematic when build NLP model for these languages. To address this gap, we introduce Bhinneka Korpus, a multilingual parallel corpus featuring five Indonesian local languages. Our goal is to enhance access and utilization of these resources, extending their reach within the country. We explained in a detail the dataset collection process and associated challenges. Additionally, we experimented with translation task using the IBM Model 1 due to data constraints. The result showed that the performance of each language already shows good indications for further development. Challenges such as lexical variation, smoothing effects, and cross-linguistic variability are discussed. We intend to evaluate the corpus usi
&lt;/p&gt;</description></item><item><title>&#26174;&#24335;&#31034;&#20363;&#65288;&#21435;&#38500;&#36830;&#25509;&#35789;&#65289;&#19978;&#35757;&#32451;&#30340;&#20851;&#31995;&#20998;&#31867;&#22120;&#22312;&#30495;&#23454;&#30340;&#38544;&#24335;&#24773;&#26223;&#20013;&#34920;&#29616;&#19981;&#20339;&#30340;&#21407;&#22240;&#20043;&#19968;&#26159;&#22312;&#21435;&#38500;&#36830;&#25509;&#35789;&#21518;&#26631;&#31614;&#21457;&#29983;&#21464;&#21270;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;&#35821;&#26009;&#24211;&#27700;&#24179;&#19978;&#30340;&#23454;&#35777;&#35777;&#25454;&#24182;&#25506;&#35752;&#20102;&#32531;&#35299;&#26631;&#31614;&#21464;&#21270;&#30340;&#31574;&#30053;</title><link>https://arxiv.org/abs/2404.00999</link><description>&lt;p&gt;
&#26174;&#24335;&#21040;&#38544;&#24335;&#31687;&#31456;&#20851;&#31995;&#35782;&#21035;&#22833;&#36133;&#30340;&#21407;&#22240;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What Causes the Failure of Explicit to Implicit Discourse Relation Recognition?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00999
&lt;/p&gt;
&lt;p&gt;
&#26174;&#24335;&#31034;&#20363;&#65288;&#21435;&#38500;&#36830;&#25509;&#35789;&#65289;&#19978;&#35757;&#32451;&#30340;&#20851;&#31995;&#20998;&#31867;&#22120;&#22312;&#30495;&#23454;&#30340;&#38544;&#24335;&#24773;&#26223;&#20013;&#34920;&#29616;&#19981;&#20339;&#30340;&#21407;&#22240;&#20043;&#19968;&#26159;&#22312;&#21435;&#38500;&#36830;&#25509;&#35789;&#21518;&#26631;&#31614;&#21457;&#29983;&#21464;&#21270;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;&#35821;&#26009;&#24211;&#27700;&#24179;&#19978;&#30340;&#23454;&#35777;&#35777;&#25454;&#24182;&#25506;&#35752;&#20102;&#32531;&#35299;&#26631;&#31614;&#21464;&#21270;&#30340;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#31687;&#31456;&#22788;&#29702;&#39046;&#22495;&#20013;&#19968;&#20010;&#26410;&#35299;&#20043;&#35868;&#65306;&#20026;&#20160;&#20040;&#22312;&#30495;&#23454;&#30340;&#38544;&#24335;&#24773;&#26223;&#20013;&#65292;&#22312;&#26174;&#24335;&#31034;&#20363;&#65288;&#21435;&#38500;&#36830;&#25509;&#35789;&#65289;&#19978;&#35757;&#32451;&#30340;&#20851;&#31995;&#20998;&#31867;&#22120;&#34920;&#29616;&#19981;&#20339;&#65311;&#20197;&#21069;&#30340;&#24037;&#20316;&#22768;&#31216;&#36825;&#26159;&#22240;&#20026;&#26174;&#24335;&#21644;&#38544;&#24335;&#31034;&#20363;&#20043;&#38388;&#30340;&#35821;&#35328;&#24046;&#24322;&#65292;&#20294;&#26410;&#25552;&#20379;&#23454;&#35777;&#35777;&#25454;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#22833;&#36133;&#30340;&#19968;&#20010;&#21407;&#22240;&#26159;&#22312;&#21435;&#38500;&#36830;&#25509;&#35789;&#21518;&#26631;&#31614;&#21457;&#29983;&#20102;&#21464;&#21270;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#21457;&#29616;&#19968;&#20123;&#26174;&#24335;&#23454;&#20363;&#34920;&#36798;&#30340;&#31687;&#31456;&#20851;&#31995;&#22312;&#36830;&#25509;&#35789;&#28040;&#22833;&#21518;&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;&#19982;&#20197;&#21069;&#25163;&#21160;&#20998;&#26512;&#23569;&#37327;&#31034;&#20363;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;&#35821;&#26009;&#24211;&#27700;&#24179;&#19978;&#30340;&#23454;&#35777;&#35777;&#25454;&#26469;&#35777;&#26126;&#36825;&#31181;&#21464;&#21270;&#30340;&#23384;&#22312;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#32771;&#34385;&#36830;&#25509;&#35789;&#22312;&#21477;&#27861;&#20013;&#30340;&#20316;&#29992;&#12289;&#36830;&#25509;&#35789;&#30340;&#27495;&#20041;&#24615;&#31561;&#22240;&#32032;&#65292;&#20998;&#26512;&#26631;&#31614;&#21464;&#21270;&#21457;&#29983;&#30340;&#21407;&#22240;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#32531;&#35299;&#26631;&#31614;&#21464;&#21270;&#30340;&#31574;&#30053;&#65306;&#36807;&#28388;&#25481;&#22122;&#22768;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00999v1 Announce Type: new  Abstract: We consider an unanswered question in the discourse processing community: why do relation classifiers trained on explicit examples (with connectives removed) perform poorly in real implicit scenarios? Prior work claimed this is due to linguistic dissimilarity between explicit and implicit examples but provided no empirical evidence. In this study, we show that one cause for such failure is a label shift after connectives are eliminated. Specifically, we find that the discourse relations expressed by some explicit instances will change when connectives disappear. Unlike previous work manually analyzing a few examples, we present empirical evidence at the corpus level to prove the existence of such shift. Then, we analyze why label shift occurs by considering factors such as the syntactic role played by connectives, ambiguity of connectives, and more. Finally, we investigate two strategies to mitigate the label shift: filtering out noisy d
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25918;&#23556;&#23398;&#25253;&#21578;&#35780;&#20272;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;GPT-4&#23454;&#29616;&#20102;&#19982;&#25918;&#23556;&#31185;&#21307;&#24072;&#35780;&#20272;&#19968;&#33268;&#24615;&#25509;&#36817;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#35757;&#32451;&#30340;&#36739;&#23567;&#27169;&#22411;&#20063;&#36798;&#21040;&#20102;&#31867;&#20284;&#30340;&#35780;&#20272;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.00998</link><description>&lt;p&gt;
LLM-RadJudge: &#23454;&#29616;X&#23556;&#32447;&#25253;&#21578;&#29983;&#25104;&#30340;&#25918;&#23556;&#31185;&#21307;&#24072;&#32423;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
LLM-RadJudge: Achieving Radiologist-Level Evaluation for X-Ray Report Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00998
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25918;&#23556;&#23398;&#25253;&#21578;&#35780;&#20272;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;GPT-4&#23454;&#29616;&#20102;&#19982;&#25918;&#23556;&#31185;&#21307;&#24072;&#35780;&#20272;&#19968;&#33268;&#24615;&#25509;&#36817;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#35757;&#32451;&#30340;&#36739;&#23567;&#27169;&#22411;&#20063;&#36798;&#21040;&#20102;&#31867;&#20284;&#30340;&#35780;&#20272;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25918;&#23556;&#23398;AI&#30340;&#21457;&#23637;&#20013;&#65292;&#35780;&#20272;&#29983;&#25104;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#29616;&#26377;&#25351;&#26631;&#26080;&#27861;&#21453;&#26144;&#20219;&#21153;&#30340;&#20020;&#24202;&#35201;&#27714;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#27604;&#36739;&#25918;&#23556;&#23398;&#25253;&#21578;&#20197;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#21508;&#31181;LLMs&#30340;&#24615;&#33021;&#65292;&#24182;&#35777;&#26126;&#65292;&#24403;&#20351;&#29992;GPT-4&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#25351;&#26631;&#23454;&#29616;&#20102;&#25509;&#36817;&#25918;&#23556;&#31185;&#21307;&#24072;&#35780;&#20272;&#19968;&#33268;&#24615;&#30340;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#38477;&#20302;&#25104;&#26412;&#24182;&#25552;&#39640;&#21487;&#35775;&#38382;&#24615;&#65292;&#20351;&#35813;&#26041;&#27861;&#23454;&#29992;&#21270;&#65292;&#25105;&#20204;&#21033;&#29992;LLM&#35780;&#20272;&#32467;&#26524;&#26500;&#24314;&#25968;&#25454;&#38598;&#65292;&#24182;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#20197;&#35757;&#32451;&#19968;&#20010;&#36739;&#23567;&#30340;&#27169;&#22411;&#12290;&#33976;&#39311;&#27169;&#22411;&#23454;&#29616;&#20102;&#19982;GPT-4&#30456;&#24403;&#30340;&#35780;&#20272;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21644;&#33976;&#39311;&#27169;&#22411;&#20026;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#35775;&#38382;&#19988;&#39640;&#25928;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20419;&#36827;&#20102;&#24320;&#21457;&#26356;&#20855;&#20020;&#24202;&#30456;&#20851;&#24615;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#23558;&#36827;&#19968;&#27493;&#24320;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00998v1 Announce Type: cross  Abstract: Evaluating generated radiology reports is crucial for the development of radiology AI, but existing metrics fail to reflect the task's clinical requirements. This study proposes a novel evaluation framework using large language models (LLMs) to compare radiology reports for assessment. We compare the performance of various LLMs and demonstrate that, when using GPT-4, our proposed metric achieves evaluation consistency close to that of radiologists. Furthermore, to reduce costs and improve accessibility, making this method practical, we construct a dataset using LLM evaluation results and perform knowledge distillation to train a smaller model. The distilled model achieves evaluation capabilities comparable to GPT-4. Our framework and distilled model offer an accessible and efficient evaluation method for radiology report generation, facilitating the development of more clinically relevant models. The model will be further open-sourced 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27861;&#24459;&#39046;&#22495;&#23637;&#31034;&#20986;&#29420;&#29305;&#20316;&#29992;&#65292;&#24102;&#26469;&#22909;&#22788;&#21644;&#25361;&#25112;&#65292;&#35843;&#30740;&#31361;&#20986;&#20102;&#20854;&#22312;&#27861;&#24459;&#25991;&#26412;&#29702;&#35299;&#21644;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#38754;&#20020;&#30340;&#20559;&#35265;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#20262;&#29702;&#25361;&#25112;&#65292;&#23637;&#31034;&#20102;&#38024;&#23545;&#19981;&#21516;&#27861;&#24459;&#31995;&#32479;&#30340;&#31934;&#32454;&#35843;&#25972;&#27861;&#24459;LLMs&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2404.00990</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#27861;&#24459;&#31995;&#32479;&#30340;&#20851;&#31995;&#65306;&#31616;&#35201;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Exploring the Nexus of Large Language Models and Legal Systems: A Short Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00990
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27861;&#24459;&#39046;&#22495;&#23637;&#31034;&#20986;&#29420;&#29305;&#20316;&#29992;&#65292;&#24102;&#26469;&#22909;&#22788;&#21644;&#25361;&#25112;&#65292;&#35843;&#30740;&#31361;&#20986;&#20102;&#20854;&#22312;&#27861;&#24459;&#25991;&#26412;&#29702;&#35299;&#21644;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#38754;&#20020;&#30340;&#20559;&#35265;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#20262;&#29702;&#25361;&#25112;&#65292;&#23637;&#31034;&#20102;&#38024;&#23545;&#19981;&#21516;&#27861;&#24459;&#31995;&#32479;&#30340;&#31934;&#32454;&#35843;&#25972;&#27861;&#24459;LLMs&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#27493;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21457;&#29983;&#20102;&#28145;&#21051;&#21464;&#38761;&#65292;&#23588;&#20854;&#26159;&#22312;&#27861;&#24459;&#39046;&#22495;&#20869;&#12290;LLMs&#30340;&#33021;&#21147;&#36234;&#26469;&#36234;&#22810;&#22320;&#23637;&#31034;&#20986;&#22312;&#27861;&#24459;&#39046;&#22495;&#20013;&#25198;&#28436;&#29420;&#29305;&#35282;&#33394;&#65292;&#24102;&#26469;&#29420;&#29305;&#22909;&#22788;&#21644;&#21508;&#31181;&#25361;&#25112;&#12290;&#26412;&#35843;&#30740;&#25506;&#35752;&#20102;LLMs&#19982;&#27861;&#24459;&#31995;&#32479;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#27604;&#22914;&#23427;&#20204;&#22312;&#27861;&#24459;&#25991;&#26412;&#29702;&#35299;&#12289;&#26696;&#20363;&#26816;&#32034;&#21644;&#20998;&#26512;&#31561;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#26412;&#35843;&#30740;&#37325;&#28857;&#20171;&#32461;&#20102;LLMs&#22312;&#27861;&#24459;&#39046;&#22495;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#21253;&#25324;&#20559;&#35265;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#20262;&#29702;&#32771;&#34385;&#65292;&#20197;&#21450;&#30740;&#31350;&#20154;&#21592;&#22914;&#20309;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#35843;&#30740;&#23637;&#31034;&#20102;&#38024;&#23545;&#21508;&#31181;&#27861;&#24459;&#31995;&#32479;&#23450;&#21046;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#31934;&#32454;&#35843;&#25972;&#30340;&#27861;&#24459;LLMs&#65292;&#20197;&#21450;&#21487;&#29992;&#20110;&#21508;&#31181;&#35821;&#35328;LLMs&#31934;&#32454;&#35843;&#25972;&#30340;&#27861;&#24459;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#23427;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00990v1 Announce Type: new  Abstract: With the advancement of Artificial Intelligence (AI) and Large Language Models (LLMs), there is a profound transformation occurring in the realm of natural language processing tasks within the legal domain. The capabilities of LLMs are increasingly demonstrating unique roles in the legal sector, bringing both distinctive benefits and various challenges. This survey delves into the synergy between LLMs and the legal system, such as their applications in tasks like legal text comprehension, case retrieval, and analysis. Furthermore, this survey highlights key challenges faced by LLMs in the legal domain, including bias, interpretability, and ethical considerations, as well as how researchers are addressing these issues. The survey showcases the latest advancements in fine-tuned legal LLMs tailored for various legal systems, along with legal datasets available for fine-tuning LLMs in various languages. Additionally, it proposes directions f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20808;&#39564;&#32422;&#26463;&#30340;&#22870;&#21169;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#26377;&#25928;&#25913;&#21892;&#20102;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.00978</link><description>&lt;p&gt;
&#22522;&#20110;&#20808;&#39564;&#32422;&#26463;&#30340;&#22870;&#21169;&#27169;&#22411;&#35757;&#32451;&#20197;&#23545;&#40784;&#22823;&#23610;&#23544;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Prior Constraints-based Reward Model Training for Aligning Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20808;&#39564;&#32422;&#26463;&#30340;&#22870;&#21169;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#26377;&#25928;&#25913;&#21892;&#20102;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#35757;&#32451;&#19968;&#20010;&#22870;&#21169;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#27604;&#36739;&#23545;&#26469;&#35745;&#31639;&#25490;&#21517;&#25439;&#22833;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#36807;&#31243;&#23384;&#22312;&#19968;&#20010;&#22266;&#26377;&#38382;&#39064;&#65306;&#30001;&#20110;&#32570;&#20047;&#32422;&#26463;&#65292;&#22870;&#21169;&#20998;&#25968;&#22312;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#20013;&#21576;&#29616;&#19981;&#21463;&#25511;&#21046;&#30340;&#25193;&#23637;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20808;&#39564;&#32422;&#26463;&#30340;&#22870;&#21169;&#27169;&#22411;&#65288;PCRM&#65289;&#35757;&#32451;&#26041;&#27861;&#26469;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#12290;PCRM&#22312;&#22870;&#21169;&#27169;&#22411;&#35757;&#32451;&#20013;&#34701;&#21512;&#20102;&#20808;&#39564;&#32422;&#26463;&#65292;&#20855;&#20307;&#26469;&#35828;&#26159;&#27599;&#20010;&#27604;&#36739;&#23545;&#36755;&#20986;&#20043;&#38388;&#30340;&#38271;&#24230;&#27604;&#21644;&#20313;&#24358;&#30456;&#20284;&#24615;&#65292;&#20197;&#35843;&#33410;&#20248;&#21270;&#24133;&#24230;&#24182;&#25511;&#21046;&#24471;&#20998;&#24046;&#36317;&#12290;&#25105;&#20204;&#36890;&#36807;&#26816;&#26597;PCRM&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#25490;&#21517;&#30456;&#20851;&#24615;&#20197;&#21450;&#36890;&#36807;RL&#23545;LLMs&#23545;&#40784;&#30340;&#26377;&#25928;&#24615;&#26469;&#20840;&#38754;&#35780;&#20272;PCRM&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PCRM&#36890;&#36807;&#26377;&#25928;&#22320;&#32422;&#26463;&#22870;&#21169;&#26174;&#33879;&#25552;&#21319;&#20102;&#23545;&#40784;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00978v1 Announce Type: new  Abstract: Reinforcement learning with human feedback for aligning large language models (LLMs) trains a reward model typically using ranking loss with comparison pairs.However, the training procedure suffers from an inherent problem: the uncontrolled scaling of reward scores during reinforcement learning due to the lack of constraints while training the reward model.This paper proposes a Prior Constraints-based Reward Model (namely PCRM) training method to mitigate this problem. PCRM incorporates prior constraints, specifically, length ratio and cosine similarity between outputs of each comparison pair, during reward model training to regulate optimization magnitude and control score margins. We comprehensively evaluate PCRM by examining its rank correlation with human preferences and its effectiveness in aligning LLMs via RL. Experimental results demonstrate that PCRM significantly improves alignment performance by effectively constraining reward
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31867;&#24179;&#34913;Soft-voting&#31995;&#32479;&#65292;&#26088;&#22312;&#26816;&#27979;&#20154;&#31867;&#25776;&#20889;&#25110;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#36890;&#36807;&#20248;&#21270;&#32534;&#30721;&#22120;&#27169;&#22411;&#34920;&#29616;&#65292;&#24182;&#37319;&#29992;&#21152;&#26435;&#20132;&#21449;&#29109;&#25439;&#22833;&#21644;&#36719;&#25237;&#31080;&#31574;&#30053;&#65292;&#22312;SemEval-2024&#20219;&#21153;8&#30340;Subtask B&#20013;&#21462;&#24471;&#20102;&#26368;&#20339;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2404.00950</link><description>&lt;p&gt;
AISPACE&#22312;SemEval-2024&#20219;&#21153;8&#20013;&#30340;&#34920;&#29616;&#65306;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#22810;&#29983;&#25104;&#22120;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#31867;&#24179;&#34913;Soft-voting&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
AISPACE at SemEval-2024 task 8: A Class-balanced Soft-voting System for Detecting Multi-generator Machine-generated Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31867;&#24179;&#34913;Soft-voting&#31995;&#32479;&#65292;&#26088;&#22312;&#26816;&#27979;&#20154;&#31867;&#25776;&#20889;&#25110;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#36890;&#36807;&#20248;&#21270;&#32534;&#30721;&#22120;&#27169;&#22411;&#34920;&#29616;&#65292;&#24182;&#37319;&#29992;&#21152;&#26435;&#20132;&#21449;&#29109;&#25439;&#22833;&#21644;&#36719;&#25237;&#31080;&#31574;&#30053;&#65292;&#22312;SemEval-2024&#20219;&#21153;8&#30340;Subtask B&#20013;&#21462;&#24471;&#20102;&#26368;&#20339;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00950v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; &#25552;&#35201;&#65306;SemEval-2024&#20219;&#21153;8&#25552;&#20986;&#20102;&#19968;&#20010;&#25361;&#25112;&#65292;&#21363;&#26816;&#27979;&#20154;&#31867;&#25776;&#20889;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#38024;&#23545;&#19981;&#21516;&#30340;&#26816;&#27979;&#22330;&#26223;&#26377;3&#20010;&#23376;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#30340;&#31995;&#32479;&#20027;&#35201;&#22788;&#29702;Subtask B&#65292;&#26088;&#22312;&#26816;&#27979;&#32473;&#23450;&#30340;&#20840;&#25991;&#26159;&#30001;&#20154;&#31867;&#25776;&#20889;&#36824;&#26159;&#30001;&#29305;&#23450;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#30340;&#65292;&#23454;&#38469;&#19978;&#26159;&#19968;&#20010;&#22810;&#31867;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#22242;&#38431;AISPACE&#23545;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#21253;&#25324;&#20165;&#32534;&#30721;&#22120;&#12289;&#20165;&#35299;&#30721;&#22120;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#23427;&#20204;&#22312;&#35813;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#20165;&#32534;&#30721;&#22120;&#27169;&#22411;&#34920;&#29616;&#24322;&#24120;&#20986;&#33394;&#12290;&#25105;&#20204;&#36824;&#24212;&#29992;&#20102;&#21152;&#26435;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#26469;&#35299;&#20915;&#19981;&#21516;&#31867;&#21035;&#26679;&#26412;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#36719;&#25237;&#31080;&#31574;&#30053;&#23545;&#22810;&#27169;&#22411;&#38598;&#25104;&#36827;&#34892;&#22686;&#24378;&#65292;&#20197;&#25552;&#39640;&#25105;&#20204;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;Subtask B&#20013;&#25490;&#21517;&#31532;&#19968;&#65292;&#21019;&#36896;&#20102;&#19968;&#20010;&#26368;&#20808;&#36827;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00950v1 Announce Type: new  Abstract: SemEval-2024 Task 8 provides a challenge to detect human-written and machine-generated text. There are 3 subtasks for different detection scenarios. This paper proposes a system that mainly deals with Subtask B. It aims to detect if given full text is written by human or is generated by a specific Large Language Model (LLM), which is actually a multi-class text classification task. Our team AISPACE conducted a systematic study of fine-tuning transformer-based models, including encoderonly, decoder-only and encoder-decoder models. We compared their performance on this task and identified that encoder-only models performed exceptionally well. We also applied a weighted Cross Entropy loss function to address the issue of data imbalance of different class samples. Additionally, we employed softvoting strategy over multi-models ensemble to enhance the reliability of our predictions. Our system ranked top 1 in Subtask B, which sets a state-of-
&lt;/p&gt;</description></item><item><title>Evalverse&#26159;&#19968;&#20010;&#32479;&#19968;&#21644;&#26131;&#29992;&#30340;&#24211;&#65292;&#31616;&#21270;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#25552;&#20379;&#20102;&#38598;&#20013;&#19988;&#26131;&#20110;&#35775;&#38382;&#30340;&#35780;&#20272;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2404.00943</link><description>&lt;p&gt;
Evalverse: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#32479;&#19968;&#21644;&#26131;&#29992;&#24211;
&lt;/p&gt;
&lt;p&gt;
Evalverse: Unified and Accessible Library for Large Language Model Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00943
&lt;/p&gt;
&lt;p&gt;
Evalverse&#26159;&#19968;&#20010;&#32479;&#19968;&#21644;&#26131;&#29992;&#30340;&#24211;&#65292;&#31616;&#21270;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#25552;&#20379;&#20102;&#38598;&#20013;&#19988;&#26131;&#20110;&#35775;&#38382;&#30340;&#35780;&#20272;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Evalverse&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#24211;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#30340;&#35780;&#20272;&#24037;&#20855;&#32479;&#19968;&#21040;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#26694;&#26550;&#20013;&#65292;&#31616;&#21270;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#12290;Evalverse&#20351;&#24471;&#23545;&#20154;&#24037;&#26234;&#33021;&#20102;&#35299;&#26377;&#38480;&#30340;&#20010;&#20154;&#21487;&#20197;&#36731;&#26494;&#35831;&#27714;LLMs&#35780;&#20272;&#24182;&#25910;&#21040;&#35814;&#32454;&#25253;&#21578;&#65292;&#21033;&#29992;&#19982;Slack&#31561;&#36890;&#20449;&#24179;&#21488;&#30340;&#38598;&#25104;&#12290;&#22240;&#27492;&#65292;Evalverse&#20316;&#20026;LLMs&#30340;&#20840;&#38754;&#35780;&#20272;&#24378;&#22823;&#24037;&#20855;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#25552;&#20379;&#20102;&#38598;&#20013;&#19988;&#26131;&#20110;&#35775;&#38382;&#30340;&#35780;&#20272;&#26694;&#26550;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;Evalverse&#30340;&#28436;&#31034;&#35270;&#39057;&#65292;&#23637;&#31034;&#20102;&#23427;&#30340;&#21151;&#33021;&#21644;&#23454;&#29616;&#26041;&#24335;&#65292;&#20197;&#20004;&#20998;&#38047;&#30340;&#26684;&#24335;&#23637;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00943v1 Announce Type: cross  Abstract: This paper introduces Evalverse, a novel library that streamlines the evaluation of Large Language Models (LLMs) by unifying disparate evaluation tools into a single, user-friendly framework. Evalverse enables individuals with limited knowledge of artificial intelligence to easily request LLM evaluations and receive detailed reports, facilitated by an integration with communication platforms like Slack. Thus, Evalverse serves as a powerful tool for the comprehensive assessment of LLMs, offering both researchers and practitioners a centralized and easily accessible evaluation framework. Finally, we also provide a demo video for Evalverse, showcasing its capabilities and implementation in a two-minute format.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22823;&#22411;&#30693;&#35782;&#22270;&#35889;&#21019;&#24314;&#35780;&#20272;&#27169;&#22411;&#20197;&#26816;&#26597;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#24615;&#19978;&#30340;&#34920;&#29616;&#65292;&#26377;&#25928;&#38477;&#20302;&#35780;&#20272;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2404.00942</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#35268;&#27169;&#30693;&#35782;&#22270;&#35889;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20107;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Factuality of Large Language Models using Large-Scale Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00942
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#30693;&#35782;&#22270;&#35889;&#21019;&#24314;&#35780;&#20272;&#27169;&#22411;&#20197;&#26816;&#26597;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#24615;&#19978;&#30340;&#34920;&#29616;&#65292;&#26377;&#25928;&#38477;&#20302;&#35780;&#20272;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#26174;&#33879;&#25913;&#21464;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#22686;&#24378;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#33021;&#21147;&#12290;&#20107;&#23454;&#24615;&#38382;&#39064;&#23545;LLMs&#26469;&#35828;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#33021;&#29983;&#25104;&#20107;&#23454;&#19981;&#20934;&#30830;&#30340;&#21709;&#24212;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;GraphEval&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#27979;&#35797;&#25968;&#25454;&#38598;&#23545;LLM&#30340;&#24615;&#33021;&#36827;&#34892;&#35780;&#20272;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#27979;&#35797;&#25968;&#25454;&#38598;&#26159;&#20174;&#25317;&#26377;&#36229;&#36807;1000&#19975;&#20010;&#20107;&#23454;&#30340;&#22823;&#22411;&#30693;&#35782;&#22270;&#35889;&#20013;&#26816;&#32034;&#32780;&#26469;&#65292;&#26080;&#38656;&#26114;&#36149;&#30340;&#20154;&#21147;&#25104;&#26412;&#12290;&#19982;&#22522;&#20110;&#29983;&#25104;&#21709;&#24212;&#35780;&#20272;LLMs&#30340;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;GraphEval&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#35780;&#20272;&#27169;&#22411;&#31616;&#21270;&#20102;&#35780;&#20272;&#36807;&#31243;&#65292;&#29992;&#20110;&#20272;&#35745;LLM&#32473;&#20986;&#30340;&#31572;&#26696;&#30340;&#27491;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35780;&#20272;&#27169;&#22411;&#30340;&#20107;&#23454;&#24615;&#35780;&#20272;&#19982;LLM&#29983;&#25104;&#30340;&#36755;&#20986;&#30340;&#27491;&#30830;&#24615;&#23494;&#20999;&#30456;&#20851;&#65292;&#21516;&#26102;&#26174;&#33879;&#38477;&#20302;&#20102;&#35780;&#20272;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#20026;LLM&#30340;&#24615;&#33021;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00942v1 Announce Type: cross  Abstract: The advent of Large Language Models (LLMs) has significantly transformed the AI landscape, enhancing machine learning and AI capabilities. Factuality issue is a critical concern for LLMs, as they may generate factually incorrect responses. In this paper, we propose GraphEval to evaluate an LLM's performance using a substantially large test dataset. Specifically, the test dataset is retrieved from a large knowledge graph with more than 10 million facts without expensive human efforts. Unlike conventional methods that evaluate LLMs based on generated responses, GraphEval streamlines the evaluation process by creating a judge model to estimate the correctness of the answers given by the LLM. Our experiments demonstrate that the judge model's factuality assessment aligns closely with the correctness of the LLM's generated outputs, while also substantially reducing evaluation costs. Besides, our findings offer valuable insights into LLM per
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#20026;&#31038;&#20250;&#36741;&#21161;&#26426;&#22120;&#20154;&#39046;&#22495;&#24102;&#26469;&#20102;&#28508;&#22312;&#30340;&#26032;&#24212;&#29992;&#65292;&#33021;&#22815;&#26174;&#33879;&#25193;&#23637;&#20854;&#33021;&#21147;&#65292;&#20294;&#20063;&#24102;&#26469;&#26032;&#30340;&#39118;&#38505;&#21644;&#36947;&#24503;&#20851;&#20999;&#12290;</title><link>https://arxiv.org/abs/2404.00938</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#20419;&#36827;&#26356;&#22909;&#30340;&#31038;&#20250;&#36741;&#21161;&#20154;&#26426;&#20132;&#20114;&#65306;&#31616;&#35201;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
How Can Large Language Models Enable Better Socially Assistive Human-Robot Interaction: A Brief Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00938
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#20026;&#31038;&#20250;&#36741;&#21161;&#26426;&#22120;&#20154;&#39046;&#22495;&#24102;&#26469;&#20102;&#28508;&#22312;&#30340;&#26032;&#24212;&#29992;&#65292;&#33021;&#22815;&#26174;&#33879;&#25193;&#23637;&#20854;&#33021;&#21147;&#65292;&#20294;&#20063;&#24102;&#26469;&#26032;&#30340;&#39118;&#38505;&#21644;&#36947;&#24503;&#20851;&#20999;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#36741;&#21161;&#26426;&#22120;&#20154;&#65288;SARs&#65289;&#22312;&#20026;&#32769;&#24180;&#20154;&#12289;&#24739;&#26377;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#65288;ASD&#65289;&#30340;&#20799;&#31461;&#20197;&#21450;&#31934;&#31070;&#20581;&#24247;&#25361;&#25112;&#32773;&#31561;&#29305;&#27530;&#32676;&#20307;&#25552;&#20379;&#20010;&#24615;&#21270;&#35748;&#30693;&#24773;&#24863;&#25903;&#25345;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290; SAR&#30340;&#22823;&#37327;&#30740;&#31350;&#20316;&#21697;&#23637;&#31034;&#20102;&#20854;&#22312;&#20026;&#22312;&#23478;&#25552;&#20379;&#25903;&#25345;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#36825;&#31181;&#25903;&#25345;&#21487;&#20197;&#34917;&#20805;&#30001;&#19987;&#19994;&#24515;&#29702;&#20581;&#24247;&#19987;&#19994;&#20154;&#21592;&#25552;&#20379;&#30340;&#35786;&#25152;&#27835;&#30103;&#65292;&#20351;&#36825;&#20123;&#24178;&#39044;&#25514;&#26045;&#26356;&#21152;&#26377;&#25928;&#21644;&#21487;&#35775;&#38382;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#20851;&#38190;&#25216;&#26415;&#25361;&#25112;&#65292;&#38459;&#30861;&#20102;SAR&#20171;&#23548;&#30340;&#20132;&#20114;&#21644;&#24178;&#39044;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;&#30340;&#31038;&#20250;&#26234;&#33021;&#21644;&#21151;&#25928;&#12290; &#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;SAR&#39046;&#22495;&#20869;&#30340;&#26032;&#24212;&#29992;&#28508;&#21147;&#26377;&#25152;&#22686;&#21152;&#65292;&#21487;&#20197;&#26174;&#33879;&#25193;&#23637;SAR&#30340;&#24403;&#21069;&#33021;&#21147;&#12290; &#28982;&#32780;&#65292;&#25972;&#21512;LLM&#20250;&#24341;&#20837;&#26032;&#30340;&#39118;&#38505;&#21644;&#36947;&#24503;&#20851;&#20999;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00938v1 Announce Type: cross  Abstract: Socially assistive robots (SARs) have shown great success in providing personalized cognitive-affective support for user populations with special needs such as older adults, children with autism spectrum disorder (ASD), and individuals with mental health challenges. The large body of work on SAR demonstrates its potential to provide at-home support that complements clinic-based interventions delivered by mental health professionals, making these interventions more effective and accessible. However, there are still several major technical challenges that hinder SAR-mediated interactions and interventions from reaching human-level social intelligence and efficacy. With the recent advances in large language models (LLMs), there is an increased potential for novel applications within the field of SAR that can significantly expand the current capabilities of SARs. However, incorporating LLMs introduces new risks and ethical concerns that ha
&lt;/p&gt;</description></item><item><title>ChatGLM-RLHF&#26159;&#19968;&#31181;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#25910;&#38598;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#12289;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#21644;&#20248;&#21270;&#31574;&#30053;&#31561;&#26041;&#27861;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGLM&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#23545;&#40784;&#24615;&#65292;&#22312;&#23454;&#39564;&#20013;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2404.00934</link><description>&lt;p&gt;
ChatGLM-RLHF&#65306;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#21453;&#39304;&#23545;&#40784;&#30340;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
ChatGLM-RLHF: Practices of Aligning Large Language Models with Human Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00934
&lt;/p&gt;
&lt;p&gt;
ChatGLM-RLHF&#26159;&#19968;&#31181;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#25910;&#38598;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#12289;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#21644;&#20248;&#21270;&#31574;&#30053;&#31561;&#26041;&#27861;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGLM&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#23545;&#40784;&#24615;&#65292;&#22312;&#23454;&#39564;&#20013;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00934v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;ChatGLM&#26159;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23478;&#26063;&#25552;&#20379;&#25903;&#25345;&#30340;&#20813;&#36153;&#20154;&#24037;&#26234;&#33021;&#26381;&#21153;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ChatGLM-RLHF&#27969;&#27700;&#32447;--&#19968;&#31181;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#31995;&#32479;--&#26088;&#22312;&#22686;&#24378;ChatGLM&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#23545;&#40784;&#24615;&#12290;ChatGLM-RLHF&#21253;&#21547;&#19977;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65306;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#30340;&#25910;&#38598;&#65292;&#22870;&#21169;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#20197;&#21450;&#31574;&#30053;&#30340;&#20248;&#21270;&#12290;&#22312;&#23558;ChatGLM-RLHF&#25972;&#21512;&#21040;&#29983;&#20135;&#29615;&#22659;&#30340;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#36935;&#21040;&#24182;&#35299;&#20915;&#20102;&#19968;&#20123;&#21069;&#25152;&#26410;&#26377;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20943;&#23569;&#22870;&#21169;&#26041;&#24046;&#20197;&#23454;&#29616;&#31283;&#23450;&#30340;&#22823;&#35268;&#27169;&#35757;&#32451;&#30340;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#24102;&#26377;&#34701;&#21512;&#26799;&#24230;&#19979;&#38477;&#30340;&#27169;&#22411;&#24182;&#34892;&#24615;&#65292;&#24182;&#35774;&#35745;&#20102;&#27491;&#21017;&#21270;&#32422;&#26463;&#20197;&#36991;&#20813;LLMs&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;ChatGLM&#30340;&#21463;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#29256;&#26412;&#30456;&#27604;&#65292;ChatGLM-RLHF&#22312;&#23545;&#40784;&#20219;&#21153;&#20013;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00934v1 Announce Type: new  Abstract: ChatGLM is a free-to-use AI service powered by the ChatGLM family of large language models (LLMs). In this paper, we present the ChatGLM-RLHF pipeline -- a reinforcement learning from human feedback (RLHF) system -- designed to enhance ChatGLM's alignment with human preferences. ChatGLM-RLHF encompasses three major components: the collection of human preference data, the training of the reward model, and the optimization of policies. Throughout the process of integrating ChatGLM-RLHF into production, we encountered and addressed several unprecedented challenges. We introduce the strategies to mitigate reward variance for stabilized large-scale training, implement model parallelism with fused gradient-descent, and design regularization constraints to avoid catastrophic forgetting in LLMs. Experiments show that ChatGLM-RLHF brings significant improvements in alignment tasks compared to the supervised fine-tuned (SFT) version of ChatGLM. Fo
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20010;&#24615;&#21270;&#30340;&#21512;&#25104;&#23545;&#35805;&#25968;&#25454;&#29983;&#25104;&#27969;&#27700;&#32447;&#65292;&#24341;&#20837;&#20102;PSYDIAL&#38889;&#35821;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26356;&#31526;&#21512;&#30495;&#23454;&#22330;&#26223;&#30340;&#20154;&#31867;&#21270;&#23545;&#35805;&#65292;&#23454;&#39564;&#35777;&#26126;&#30456;&#36739;&#20110;&#39044;&#35757;&#32451;&#25110;&#24494;&#35843;&#27169;&#22411;&#65292;&#20351;&#29992;PSYDIAL&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#29983;&#25104;&#31526;&#21512;&#20010;&#24615;&#30340;&#22238;&#24212;&#19978;&#26377;&#26174;&#33879;&#25552;&#21319;&#65292;&#19988;&#35813;&#27969;&#27700;&#32447;&#20855;&#26377;&#38750;&#23545;&#35805;&#30456;&#20851;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.00930</link><description>&lt;p&gt;
PSYDIAL&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22522;&#20110;&#20010;&#24615;&#21270;&#30340;&#21512;&#25104;&#23545;&#35805;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
PSYDIAL: Personality-based Synthetic Dialogue Generation using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00930
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20010;&#24615;&#21270;&#30340;&#21512;&#25104;&#23545;&#35805;&#25968;&#25454;&#29983;&#25104;&#27969;&#27700;&#32447;&#65292;&#24341;&#20837;&#20102;PSYDIAL&#38889;&#35821;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26356;&#31526;&#21512;&#30495;&#23454;&#22330;&#26223;&#30340;&#20154;&#31867;&#21270;&#23545;&#35805;&#65292;&#23454;&#39564;&#35777;&#26126;&#30456;&#36739;&#20110;&#39044;&#35757;&#32451;&#25110;&#24494;&#35843;&#27169;&#22411;&#65292;&#20351;&#29992;PSYDIAL&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#29983;&#25104;&#31526;&#21512;&#20010;&#24615;&#30340;&#22238;&#24212;&#19978;&#26377;&#26174;&#33879;&#25552;&#21319;&#65292;&#19988;&#35813;&#27969;&#27700;&#32447;&#20855;&#26377;&#38750;&#23545;&#35805;&#30456;&#20851;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;&#22522;&#20110;&#20010;&#24615;&#21270;&#30340;&#21512;&#25104;&#23545;&#35805;&#25968;&#25454;&#29983;&#25104;&#27969;&#27700;&#32447;&#65292;&#19987;&#20026;&#36890;&#36807;&#25552;&#31034;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20986;&#22238;&#24212;&#32780;&#35774;&#35745;&#12290;&#25105;&#20204;&#35774;&#35745;&#25552;&#31034;&#20197;&#29983;&#25104;&#26356;&#31526;&#21512;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#30340;&#23545;&#35805;&#65292;&#24403;&#29992;&#25143;&#19982;&#32842;&#22825;&#26426;&#22120;&#20154;&#20114;&#21160;&#26102;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;PSYDIAL&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#27880;&#20110;&#22522;&#20110;&#20010;&#24615;&#21270;&#23545;&#35805;&#30340;&#38889;&#35821;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#27969;&#27700;&#32447;&#31574;&#21010;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#22823;&#20116;&#20154;&#26684;&#27169;&#22411;&#30340;&#22806;&#21521;&#24615;&#32500;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#37027;&#20123;&#29992;&#32842;&#22825;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#24456;&#38590;&#29983;&#25104;&#21453;&#26144;&#20010;&#24615;&#30340;&#22238;&#24212;&#65292;&#20294;&#36890;&#36807;PSYDIAL&#35757;&#32451;&#30340;&#27169;&#22411;&#26174;&#31034;&#20986;&#26174;&#33879;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#27969;&#27700;&#32447;&#30340;&#22810;&#21151;&#33021;&#24615;&#19981;&#20165;&#23616;&#38480;&#20110;&#23545;&#35805;&#20219;&#21153;&#65292;&#36824;&#20026;&#20854;&#20182;&#38750;&#23545;&#35805;&#30456;&#20851;&#24212;&#29992;&#25552;&#20379;&#28508;&#21147;&#12290;&#36825;&#39033;&#30740;&#31350;&#25171;&#24320;&#20102;&#26356;&#20026;&#24494;&#22937;&#12289;&#20010;&#24615;&#39537;&#21160;&#30340;&#20132;&#35848;&#30340;&#22823;&#38376;.
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00930v1 Announce Type: new  Abstract: We present a novel end-to-end personality-based synthetic dialogue data generation pipeline, specifically designed to elicit responses from large language models via prompting. We design the prompts to generate more human-like dialogues considering real-world scenarios when users engage with chatbots. We introduce PSYDIAL, the first Korean dialogue dataset focused on personality-based dialogues, curated using our proposed pipeline. Notably, we focus on the Extraversion dimension of the Big Five personality model in our research. Experimental results indicate that while pre-trained models and those fine-tuned with a chit-chat dataset struggle to generate responses reflecting personality, models trained with PSYDIAL show significant improvements. The versatility of our pipeline extends beyond dialogue tasks, offering potential for other non-dialogue related applications. This research opens doors for more nuanced, personality-driven conver
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23545;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#28145;&#20837;&#35752;&#35770;&#20102;&#20851;&#38190;&#38382;&#39064;&#65292;&#21253;&#25324;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#12289;&#23545;&#40784;&#21644;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2404.00929</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#35821;&#26009;&#24211;&#12289;&#23545;&#40784;&#21644;&#20559;&#35265;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Multilingual Large Language Models: Corpora, Alignment, and Bias
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00929
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23545;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#28145;&#20837;&#35752;&#35770;&#20102;&#20851;&#38190;&#38382;&#39064;&#65292;&#21253;&#25324;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#12289;&#23545;&#40784;&#21644;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22522;&#30784;&#19978;&#65292;&#21457;&#23637;&#20102;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#26469;&#35299;&#20915;&#22810;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#25361;&#25112;&#65292;&#24076;&#26395;&#23454;&#29616;&#20174;&#39640;&#36164;&#28304;&#21040;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#37325;&#35201;&#38480;&#21046;&#21644;&#25361;&#25112;&#65292;&#27604;&#22914;&#35821;&#35328;&#19981;&#24179;&#34913;&#12289;&#22810;&#35821;&#35328;&#23545;&#40784;&#21644;&#22266;&#26377;&#20559;&#35265;&#12290;&#26412;&#25991;&#26088;&#22312;&#23545;MLLMs&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#28145;&#20837;&#35752;&#35770;&#22260;&#32469;&#36825;&#20123;&#20851;&#38190;&#38382;&#39064;&#30340;&#35758;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00929v1 Announce Type: cross  Abstract: Based on the foundation of Large Language Models (LLMs), Multilingual Large Language Models (MLLMs) have been developed to address the challenges of multilingual natural language processing tasks, hoping to achieve knowledge transfer from high-resource to low-resource languages. However, significant limitations and challenges still exist, such as language imbalance, multilingual alignment, and inherent bias. In this paper, we aim to provide a comprehensive analysis of MLLMs, delving deeply into discussions surrounding these critical issues. First of all, we start by presenting an overview of MLLMs, covering their evolution, key techniques, and multilingual capacities. Secondly, we explore widely utilized multilingual corpora for MLLMs' training and multilingual datasets oriented for downstream tasks that are crucial for enhancing the cross-lingual capability of MLLMs. Thirdly, we survey the existing studies on multilingual representati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;SignLLM&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#22788;&#29702;&#25163;&#35821;&#32763;&#35793;&#65292;&#36890;&#36807;&#23545;&#25163;&#35821;&#35270;&#39057;&#36827;&#34892;&#27491;&#21017;&#21270;&#21644;&#36716;&#25442;&#65292;&#23454;&#29616;&#20102;&#25163;&#35821;&#35270;&#39057;&#21521;&#31867;&#20284;&#35821;&#35328;&#30340;&#34920;&#31034;&#36716;&#25442;&#65292;&#20197;&#25552;&#39640;&#29616;&#25104;LLMs&#30340;&#21487;&#35835;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.00925</link><description>&lt;p&gt;
LLMs &#26159;&#20248;&#31168;&#30340;&#25163;&#35821;&#32763;&#35793;&#22120;
&lt;/p&gt;
&lt;p&gt;
LLMs are Good Sign Language Translators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;SignLLM&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#22788;&#29702;&#25163;&#35821;&#32763;&#35793;&#65292;&#36890;&#36807;&#23545;&#25163;&#35821;&#35270;&#39057;&#36827;&#34892;&#27491;&#21017;&#21270;&#21644;&#36716;&#25442;&#65292;&#23454;&#29616;&#20102;&#25163;&#35821;&#35270;&#39057;&#21521;&#31867;&#20284;&#35821;&#35328;&#30340;&#34920;&#31034;&#36716;&#25442;&#65292;&#20197;&#25552;&#39640;&#29616;&#25104;LLMs&#30340;&#21487;&#35835;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Sign Language Translation (SLT)&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#23558;&#25163;&#35821;&#35270;&#39057;&#32763;&#35793;&#20026;&#21475;&#22836;&#35821;&#35328;&#12290;&#21463;&#35757;&#32451;&#20110;&#24191;&#27867;&#22810;&#35821;&#35328;&#25991;&#26412;&#35821;&#26009;&#24211;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24378;&#22823;&#30340;&#32763;&#35793;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#26088;&#22312;&#21033;&#29992;&#29616;&#25104;&#30340;LLMs&#26469;&#22788;&#29702;SLT&#12290;&#26412;&#25991;&#23545;&#25163;&#35821;&#35270;&#39057;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#20351;&#20854;&#20855;&#26377;&#21475;&#22836;&#35821;&#35328;&#30340;&#35821;&#35328;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;SignLLM&#26694;&#26550;&#65292;&#23558;&#25163;&#35821;&#35270;&#39057;&#36716;&#25442;&#20026;&#31867;&#20284;&#35821;&#35328;&#30340;&#34920;&#31034;&#65292;&#20197;&#20415;&#29616;&#25104;&#30340;LLMs&#26356;&#26131;&#35835;&#22320;&#22788;&#29702;&#12290;SignLLM&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#27169;&#22359;&#65306;&#65288;1&#65289;&#30690;&#37327;&#37327;&#21270;&#35270;&#35273;&#25163;&#35821;&#27169;&#22359;&#23558;&#25163;&#35821;&#35270;&#39057;&#36716;&#25442;&#20026;&#19968;&#31995;&#21015;&#31163;&#25955;&#30340;&#23383;&#31526;&#32423;&#25163;&#35821;&#26631;&#35760;&#65292;&#65288;2&#65289;&#30721;&#26412;&#37325;&#24314;&#21644;&#23545;&#40784;&#27169;&#22359;&#20351;&#29992;&#26368;&#20248;&#20256;&#36755;&#20844;&#24335;&#23558;&#36825;&#20123;&#23383;&#31526;&#32423;&#26631;&#35760;&#36716;&#25442;&#20026;&#21333;&#35789;&#32423;&#25163;&#35821;&#34920;&#31034;&#12290;&#25163;&#35821;&#25991;&#26412;&#23545;&#40784;&#25439;&#22833;&#36827;&#19968;&#27493;&#24357;&#21512;&#20102;&#25163;&#35821;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00925v1 Announce Type: cross  Abstract: Sign Language Translation (SLT) is a challenging task that aims to translate sign videos into spoken language. Inspired by the strong translation capabilities of large language models (LLMs) that are trained on extensive multilingual text corpora, we aim to harness off-the-shelf LLMs to handle SLT. In this paper, we regularize the sign videos to embody linguistic characteristics of spoken language, and propose a novel SignLLM framework to transform sign videos into a language-like representation for improved readability by off-the-shelf LLMs. SignLLM comprises two key modules: (1) The Vector-Quantized Visual Sign module converts sign videos into a sequence of discrete character-level sign tokens, and (2) the Codebook Reconstruction and Alignment module converts these character-level tokens into word-level sign representations using an optimal transport formulation. A sign-text alignment loss further bridges the gap between sign and tex
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Token-Efficient Leverage Learning&#65288;TELL&#65289;&#30340;&#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23637;&#31034;&#20102;&#20854;&#38477;&#20302;&#20219;&#21153;&#25968;&#25454;&#38656;&#27714;&#12289;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#30340;&#28508;&#21147;&#65292;&#20026;&#20302;&#36164;&#28304;&#20219;&#21153;&#24102;&#26469;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.00914</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#39640;&#25928;&#20196;&#29260;&#21033;&#29992;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Token-Efficient Leverage Learning in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00914
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Token-Efficient Leverage Learning&#65288;TELL&#65289;&#30340;&#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23637;&#31034;&#20102;&#20854;&#38477;&#20302;&#20219;&#21153;&#25968;&#25454;&#38656;&#27714;&#12289;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#30340;&#28508;&#21147;&#65292;&#20026;&#20302;&#36164;&#28304;&#20219;&#21153;&#24102;&#26469;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#39640;&#36164;&#28304;&#22330;&#26223;&#20013;&#34920;&#29616;&#26356;&#20339;&#65292;&#36825;&#22312;&#20302;&#36164;&#28304;&#22330;&#26223;&#20013;&#23384;&#22312;&#25361;&#25112;&#12290;&#25968;&#25454;&#31232;&#32570;&#21644;LLMs&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#22266;&#26377;&#30340;&#22256;&#38590;&#21152;&#21095;&#20102;&#36825;&#19968;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#22823;&#38590;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;\textbf{Leverage Learning}&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36825;&#19968;&#26041;&#27861;&#30340;&#31616;&#21270;&#23454;&#29616;&#65292;&#31216;&#20026;Token-Efficient Leverage Learning (TELL)&#12290;TELL&#23637;&#31034;&#20102;Leverage Learning&#30340;&#28508;&#21147;&#65292;&#35777;&#26126;&#23427;&#22312;&#21508;&#31181;LLMs&#21644;&#20302;&#36164;&#28304;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#20174;$10^4$&#21040;$10^6$&#20010;&#20196;&#29260;&#19981;&#31561;&#12290;&#19982;&#20256;&#32479;&#30340;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#30456;&#27604;&#65292;&#23427;&#23558;&#20219;&#21153;&#25968;&#25454;&#38656;&#27714;&#38477;&#20302;&#20102;&#36817;&#19968;&#20010;&#25968;&#37327;&#32423;&#65292;&#21516;&#26102;&#25552;&#20379;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#22312;&#30456;&#21516;&#37327;&#30340;&#20219;&#21153;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;TELL&#22312;&#25913;&#21892;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#39046;&#20808;&#20110;SFT&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;Leverage Learning&#30340;&#26426;&#21046;&#65292;&#26263;&#31034;&#20854;&#31526;&#21512;&#37327;&#21270;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00914v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have excelled in various tasks but perform better in high-resource scenarios, which presents challenges in low-resource scenarios. Data scarcity and the inherent difficulty of adapting LLMs to specific tasks compound the challenge. To address the twin hurdles, we introduce \textbf{Leverage Learning}. We present a streamlined implement of this methodology called Token-Efficient Leverage Learning (TELL). TELL showcases the potential of Leverage Learning, demonstrating effectiveness across various LLMs and low-resource tasks, ranging from $10^4$ to $10^6$ tokens. It reduces task data requirements by up to nearly an order of magnitude compared to conventional Supervised Fine-Tuning (SFT) while delivering competitive performance. With the same amount of task data, TELL leads in improving task performance compared to SFT. We discuss the mechanism of Leverage Learning, suggesting it aligns with quantization hypoth
&lt;/p&gt;</description></item><item><title>LLaMA-Excitor&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#28176;&#26356;&#22810;&#22320;&#20851;&#27880;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#26469;&#28608;&#21457;LLM&#26356;&#22909;&#22320;&#36981;&#24490;&#25351;&#20196;&#65292;&#32780;&#19981;&#30452;&#25509;&#25913;&#21464;&#20013;&#38388;&#38544;&#34255;&#29366;&#24577;&#65292;&#26377;&#25928;&#20445;&#30041;&#39044;&#35757;&#32451;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2404.00913</link><description>&lt;p&gt;
LLaMA-Excitor: &#36890;&#36807;&#38388;&#25509;&#29305;&#24449;&#20132;&#20114;&#36827;&#34892;&#36890;&#29992;&#25351;&#20196;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
LLaMA-Excitor: General Instruction Tuning via Indirect Feature Interaction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00913
&lt;/p&gt;
&lt;p&gt;
LLaMA-Excitor&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#28176;&#26356;&#22810;&#22320;&#20851;&#27880;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#26469;&#28608;&#21457;LLM&#26356;&#22909;&#22320;&#36981;&#24490;&#25351;&#20196;&#65292;&#32780;&#19981;&#30452;&#25509;&#25913;&#21464;&#20013;&#38388;&#38544;&#34255;&#29366;&#24577;&#65292;&#26377;&#25928;&#20445;&#30041;&#39044;&#35757;&#32451;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#29992;&#20110;&#24494;&#35843;LLM&#65288;&#22914;Adapter&#12289;Prefix-tuning&#21644;LoRA&#65289;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#27169;&#22359;&#25110;&#38468;&#21152;&#36755;&#20837;&#24207;&#21015;&#65292;&#20197;&#27880;&#20837;&#26032;&#25216;&#33021;&#25110;&#30693;&#35782;&#65292;&#20294;&#21487;&#33021;&#20250;&#25439;&#23475;LLM&#30340;&#22266;&#26377;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;LLaMA-Excitor&#65292;&#36825;&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#28176;&#26356;&#22810;&#22320;&#20851;&#27880;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#26469;&#28608;&#21457;LLM&#26356;&#22909;&#22320;&#36981;&#24490;&#25351;&#20196;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;LLaMA-Excitor&#22312;transformer&#32467;&#26500;&#30340;&#33258;&#27880;&#24847;&#21147;&#35745;&#31639;&#36807;&#31243;&#20013;&#19981;&#30452;&#25509;&#25913;&#21464;&#20013;&#38388;&#38544;&#34255;&#29366;&#24577;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;Excitor&#22359;&#20316;&#20026;&#29992;&#20110;LLM&#33258;&#27880;&#24847;&#21147;&#20013;&#30456;&#20284;&#24230;&#35745;&#31639;&#30340;&#26049;&#36335;&#27169;&#22359;&#65292;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#25552;&#31034;&#37325;&#26032;&#26500;&#24314;keys&#24182;&#25913;&#21464;values&#30340;&#37325;&#35201;&#24615;&#12290;LLaMA-Excitor&#30830;&#20445;&#33258;&#36866;&#24212;&#22320;&#23558;&#39069;&#22806;&#30340;&#27880;&#24847;&#21147;&#20998;&#37197;&#32473;&#36755;&#20837;&#25351;&#20196;&#65292;&#20174;&#32780;&#22312;&#20302;&#36136;&#37327;&#25351;&#20196;&#19978;&#24494;&#35843;LLM&#26102;&#26377;&#25928;&#22320;&#20445;&#30041;LLM&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00913v1 Announce Type: cross  Abstract: Existing methods to fine-tune LLMs, like Adapter, Prefix-tuning, and LoRA, which introduce extra modules or additional input sequences to inject new skills or knowledge, may compromise the innate abilities of LLMs. In this paper, we propose LLaMA-Excitor, a lightweight method that stimulates the LLMs' potential to better follow instructions by gradually paying more attention to worthwhile information. Specifically, the LLaMA-Excitor does not directly change the intermediate hidden state during the self-attention calculation of the transformer structure. We designed the Excitor block as a bypass module for the similarity score computation in LLMs' self-attention to reconstruct keys and change the importance of values by learnable prompts. LLaMA-Excitor ensures a self-adaptive allocation of additional attention to input instructions, thus effectively preserving LLMs' pre-trained knowledge when fine-tuning LLMs on low-quality instruction-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22522;&#20110;LLM&#30340;&#33258;&#21160;&#36793;&#30028;&#26816;&#27979;&#65292;&#22312;SemEval-2024&#27604;&#36187;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#31532;&#19968;&#21517;&#65292;&#24182;&#28145;&#20837;&#30740;&#31350;&#20102;&#24433;&#21709;LLMs&#26816;&#27979;&#33021;&#21147;&#30340;&#22240;&#32032;&#12290;</title><link>https://arxiv.org/abs/2404.00899</link><description>&lt;p&gt;
TM-TREK&#21442;&#21152;SemEval-2024&#20219;&#21153;8&#65306;&#22522;&#20110;LLM&#30340;&#20154;&#26426;&#28151;&#21512;&#25991;&#26412;&#33258;&#21160;&#36793;&#30028;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
TM-TREK at SemEval-2024 Task 8: Towards LLM-Based Automatic Boundary Detection for Human-Machine Mixed Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00899
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22522;&#20110;LLM&#30340;&#33258;&#21160;&#36793;&#30028;&#26816;&#27979;&#65292;&#22312;SemEval-2024&#27604;&#36187;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#31532;&#19968;&#21517;&#65292;&#24182;&#28145;&#20837;&#30740;&#31350;&#20102;&#24433;&#21709;LLMs&#26816;&#27979;&#33021;&#21147;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#25991;&#26412;&#26085;&#30410;&#26222;&#21450;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;&#21306;&#20998;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#21644;&#20154;&#24037;&#25776;&#20889;&#30340;&#25991;&#26412;&#65292;&#20197;&#38450;&#27490;LLM&#30340;&#28389;&#29992;&#65292;&#22914;&#35823;&#23548;&#24615;&#20449;&#24687;&#30340;&#20256;&#25773;&#21644;&#23398;&#26415;&#19981;&#35802;&#23454;&#34892;&#20026;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#20110;&#23558;&#25991;&#26412;&#20998;&#31867;&#20026;&#23436;&#20840;&#30001;&#20154;&#31867;&#25776;&#20889;&#25110;&#30001;LLM&#29983;&#25104;&#65292;&#24573;&#30053;&#20102;&#21516;&#26102;&#21253;&#21547;&#20004;&#31181;&#20869;&#23481;&#31867;&#22411;&#30340;&#28151;&#21512;&#25991;&#26412;&#30340;&#26816;&#27979;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;LLM&#35782;&#21035;&#20154;&#31867;&#25776;&#20889;&#21644;&#26426;&#22120;&#29983;&#25104;&#28151;&#21512;&#25991;&#26412;&#20013;&#36793;&#30028;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#36825;&#19968;&#20219;&#21153;&#36716;&#21270;&#20026;&#19968;&#20010;&#26631;&#35760;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#23558;&#26631;&#35760;&#36716;&#25240;&#28857;&#35270;&#20026;&#36793;&#30028;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;LLM&#38598;&#25104;&#27169;&#22411;&#22312;SemEval'24&#31454;&#36187;&#20219;&#21153;8&#20013;&#30340;&#8216;&#20154;&#26426;&#28151;&#21512;&#25991;&#26412;&#26816;&#27979;&#8217;&#23376;&#20219;&#21153;&#20013;&#33719;&#24471;&#31532;&#19968;&#21517;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24433;&#21709;LLMs&#22312;&#26816;&#27979;&#20013;&#30340;&#33021;&#21147;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00899v1 Announce Type: new  Abstract: With the increasing prevalence of text generated by large language models (LLMs), there is a growing concern about distinguishing between LLM-generated and human-written texts in order to prevent the misuse of LLMs, such as the dissemination of misleading information and academic dishonesty. Previous research has primarily focused on classifying text as either entirely human-written or LLM-generated, neglecting the detection of mixed texts that contain both types of content. This paper explores LLMs' ability to identify boundaries in human-written and machine-generated mixed texts. We approach this task by transforming it into a token classification problem and regard the label turning point as the boundary. Notably, our ensemble model of LLMs achieved first place in the 'Human-Machine Mixed Text Detection' sub-task of the SemEval'24 Competition Task 8. Additionally, we investigate factors that influence the capability of LLMs in detecti
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#33258;&#25105;&#28436;&#31034;(Self-Demos)&#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36890;&#36807;&#29983;&#25104;&#26597;&#35810;&#24863;&#30693;&#30340;&#31034;&#33539;&#65292;&#24341;&#20986;&#22266;&#26377;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#26500;&#24314;&#20102;&#29992;&#20110;&#35780;&#20272;&#35813;&#26041;&#27861;&#26377;&#25928;&#24615;&#30340;&#25968;&#25454;&#38598;OOD-Toolset&#12290;</title><link>https://arxiv.org/abs/2404.00884</link><description>&lt;p&gt;
&#33258;&#25105;&#28436;&#31034;: &#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20986;&#31034;&#33539;&#20043;&#22806;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Self-Demos: Eliciting Out-of-Demonstration Generalizability in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00884
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#33258;&#25105;&#28436;&#31034;(Self-Demos)&#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36890;&#36807;&#29983;&#25104;&#26597;&#35810;&#24863;&#30693;&#30340;&#31034;&#33539;&#65292;&#24341;&#20986;&#22266;&#26377;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#26500;&#24314;&#20102;&#29992;&#20110;&#35780;&#20272;&#35813;&#26041;&#27861;&#26377;&#25928;&#24615;&#30340;&#25968;&#25454;&#38598;OOD-Toolset&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#26041;&#38754;&#30340;&#33391;&#22909;&#33021;&#21147;&#65292;&#20165;&#20973;&#20511;&#23569;&#37327;&#31034;&#33539;&#23601;&#33021;&#36805;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#23569;&#26679;&#26412;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#39640;&#36136;&#37327;&#12289;&#29305;&#23450;&#26597;&#35810;&#30340;&#31034;&#33539;&#65292;&#32780;&#36825;&#31181;&#31034;&#33539;&#36890;&#24120;&#32570;&#20047;&#12290;&#38754;&#23545;&#31034;&#33539;&#20043;&#22806;&#30340;&#26597;&#35810;&#65292;&#20381;&#36182;&#25163;&#24037;&#21046;&#23450;&#31034;&#33539;&#25110;&#22806;&#37096;&#26816;&#32034;&#22120;&#30340;&#26041;&#27861;&#21487;&#33021;&#20250;&#22833;&#36133;&#12290;&#20026;&#20102;&#24357;&#21512;&#26377;&#38480;&#31034;&#33539;&#21644;&#31034;&#33539;&#20043;&#22806;&#26597;&#35810;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#25105;&#28436;&#31034;(Self-Demos)&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#38754;&#21521;&#26597;&#35810;&#30340;&#31034;&#33539;&#29983;&#25104;&#26469;&#24341;&#20986;LLMs&#20013;&#22266;&#26377;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#26032;&#22411;&#25552;&#31034;&#26041;&#27861;&#12290;&#29983;&#25104;&#30340;&#31034;&#33539;&#31574;&#30053;&#24615;&#22320;&#25554;&#20540;&#20102;&#29616;&#26377;&#31034;&#33539;&#21644;&#32473;&#23450;&#30340;&#26597;&#35810;&#65292;&#23558;&#26597;&#35810;&#20174;&#31034;&#33539;&#20043;&#22806;&#21464;&#20026;&#31034;&#33539;&#20869;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#20154;&#24037;&#26500;&#24314;&#20102;OOD-Toolset&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;300&#20010;&#30495;&#23454;API&#21644;1000&#20010;&#23454;&#20363;&#65292;&#27599;&#20010;&#23454;&#20363;&#21253;&#25324;&#19977;&#20010;&#24037;&#20855;&#20351;&#29992;&#31034;&#20363;&#20316;&#20026;&#31034;&#33539;&#21644;&#19968;&#20010;&#31034;&#33539;&#20043;&#22806;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00884v1 Announce Type: cross  Abstract: Large language models (LLMs) have shown promising abilities of in-context learning (ICL), adapting swiftly to new tasks with only few-shot demonstrations. However, current few-shot methods heavily depend on high-quality, query-specific demos, which are often lacking. When faced with out-of-demonstration (OOD) queries, methods that rely on hand-crafted demos or external retrievers might fail. To bridge the gap between limited demos and OOD queries, we propose Self-Demos, a novel prompting method that elicits the inherent generalizability in LLMs by query-aware demo generation. The generated demos strategically interpolate between existing demos and the given query, transforming the query from OOD to ID. To evaluate the effectiveness of our approach, we manually constructed OOD-Toolset, a dataset in the tool-using scenario with over 300 real-world APIs and 1000 instances, each consisting of three tool-use cases as demos and an OOD query.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#21512;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#21644;&#20808;&#36827;&#30340;&#23884;&#20837;&#21021;&#22987;&#21270;&#25216;&#26415;&#65292;&#26412;&#30740;&#31350;&#22312;&#20197;&#33521;&#35821;&#20026;&#20027;&#23548;&#30340;&#24320;&#28304;LLMs&#19978;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2404.00862</link><description>&lt;p&gt;
&#22522;&#20110;QLoRA&#21644;Zip-tie&#23884;&#20837;&#30340;&#21452;&#35821;&#36801;&#31227;&#23398;&#20064;Bailong
&lt;/p&gt;
&lt;p&gt;
Bailong: Bilingual Transfer Learning based on QLoRA and Zip-tie Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00862
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#21644;&#20808;&#36827;&#30340;&#23884;&#20837;&#21021;&#22987;&#21270;&#25216;&#26415;&#65292;&#26412;&#30740;&#31350;&#22312;&#20197;&#33521;&#35821;&#20026;&#20027;&#23548;&#30340;&#24320;&#28304;LLMs&#19978;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#22823;&#22810;&#25968;&#24320;&#28304;LLMs&#20027;&#35201;&#22312;&#33521;&#35821;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#23545;&#20854;&#20182;&#35821;&#35328;&#30340;&#35206;&#30422;&#36739;&#23569;&#12290;&#22810;&#35821;&#35328;&#35757;&#32451;&#25968;&#25454;&#30340;&#19981;&#36275;&#23548;&#33268;&#22312;&#24212;&#29992;&#21040;&#36164;&#28304;&#36739;&#23569;&#30340;&#35821;&#35328;&#26102;&#24615;&#33021;&#20122;&#20248;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#39069;&#22806;&#25968;&#25454;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#36827;&#34892;&#20840;&#21442;&#25968;&#24494;&#35843;&#20197;&#25552;&#39640;LLMs&#24615;&#33021;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#65292;&#36825;&#32473;&#30740;&#31350;&#26426;&#26500;&#21644;&#20010;&#20154;&#30740;&#31350;&#20154;&#21592;&#24102;&#26469;&#20102;&#35745;&#31639;&#38556;&#30861;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#20123;&#25216;&#26415;&#65292;&#22914;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#21644;&#20808;&#36827;&#30340;&#23884;&#20837;&#21021;&#22987;&#21270;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#26412;&#25991;&#23558;&#23427;&#20204;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#20419;&#36827;&#23545;&#20197;&#33521;&#35821;&#20026;&#20027;&#23548;&#30340;&#24320;&#28304;LLMs&#36827;&#34892;&#36328;&#35821;&#35328;&#36801;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00862v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated exceptional performance in various NLP applications. However, the majority of existing open-source LLMs are pre-trained primarily on English data and little part of other languages. This deficiency in multilingual training data results in suboptimal performance when applied to languages with fewer available resources. Furthermore, enhancing the performance of LLMs on low-resource languages by full-parameter fine-tuning with additional data requires substantial computational resources, posing computational barriers for research organizations and individual researchers. Consequently, several techniques such as parameter-efficient tuning and advanced embedding initialization have been proposed to address these challenges. In this work, we combine them to facilitate cross-lingual transfer on English-dominated open-source LLM. To effectively enhance the model's proficiency in Traditional Chines
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20250;&#25552;&#21069;&#20934;&#22791;&#26410;&#26469;&#26631;&#35760;&#25152;&#38656;&#30340;&#20449;&#24687;&#65292;&#21487;&#33021;&#26159;&#36890;&#36807;&#39044;&#32531;&#23384;&#25110;&#38754;&#21253;&#23633;&#30340;&#26041;&#24335;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2404.00859</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#25552;&#21069;&#20026;&#26410;&#26469;&#26631;&#35760;&#36827;&#34892;&#35268;&#21010;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do language models plan ahead for future tokens?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00859
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20250;&#25552;&#21069;&#20934;&#22791;&#26410;&#26469;&#26631;&#35760;&#25152;&#38656;&#30340;&#20449;&#24687;&#65292;&#21487;&#33021;&#26159;&#36890;&#36807;&#39044;&#32531;&#23384;&#25110;&#38754;&#21253;&#23633;&#30340;&#26041;&#24335;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00859v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#22312;&#32473;&#23450;&#20301;&#32622;&#30340;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#21464;&#21387;&#22120;&#26159;&#21542;&#20250;&#8220;&#25552;&#21069;&#24605;&#32771;&#8221;&#65311;&#24050;&#30693;&#21464;&#21387;&#22120;&#22312;$t$&#30340;&#21069;&#21521;&#20256;&#36882;&#30340;&#38544;&#34255;&#29366;&#24577;&#20013;&#20934;&#22791;&#20449;&#24687;&#65292;&#28982;&#21518;&#22312;&#26410;&#26469;&#30340;&#21069;&#21521;&#20256;&#36882;$t+\tau$&#20013;&#20351;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#35299;&#37322;&#36825;&#31181;&#29616;&#35937;&#30340;&#21487;&#33021;&#24615;&#65306;&#39044;&#32531;&#23384;&#65292;&#21363;&#35757;&#32451;&#20013;&#23384;&#22312;&#30340;&#38750;&#23545;&#35282;&#26799;&#24230;&#39033;&#23548;&#33268;&#27169;&#22411;&#22312;$t$&#35745;&#31639;&#19982;&#24403;&#21069;&#25512;&#29702;&#20219;&#21153;&#26080;&#20851;&#20294;&#23545;&#26410;&#26469;&#26377;&#29992;&#30340;&#29305;&#24449;&#65292;&#20197;&#21450;&#38754;&#21253;&#23633;&#65292;&#21363;&#19982;&#26102;&#38388;&#27493;&#38271;$t$&#26368;&#30456;&#20851;&#30340;&#29305;&#24449;&#24050;&#32463;&#19982;&#37027;&#20123;&#23558;&#26368;&#26377;&#21033;&#20110;&#26102;&#38388;&#27493;&#38271;$t+\tau$&#30340;&#29305;&#24449;&#30456;&#21516;&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#19981;&#23558;&#26799;&#24230;&#20256;&#25773;&#21040;&#36807;&#21435;&#26102;&#38388;&#27493;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#27979;&#35797;&#36825;&#20123;&#20551;&#35774;&#65292;&#36825;&#31181;&#26041;&#26696;&#25105;&#20204;&#27491;&#24335;&#31216;&#20026;&#30701;&#35270;&#35757;&#32451;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#39044;&#32531;&#23384;&#30340;&#26126;&#30830;&#35777;&#25454;&#12290;&#22312;&#33258;&#22238;&#24402;&#35821;&#35328;&#24314;&#27169;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#26356;&#22810;&#22320;&#25903;&#25345;&#20102;&#38754;&#21253;&#23633;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00859v1 Announce Type: cross  Abstract: Do transformers "think ahead" during inference at a given position? It is known transformers prepare information in the hidden states of the forward pass at $t$ that is then used in future forward passes $t+\tau$. We posit two explanations for this phenomenon: pre-caching, in which off-diagonal gradient terms present in training result in the model computing features at $t$ irrelevant to the present inference task but useful for the future, and breadcrumbs, in which features most relevant to time step $t$ are already the same as those that would most benefit inference at time $t+\tau$. We test these hypotheses by training language models without propagating gradients to past timesteps, a scheme we formalize as myopic training. In a synthetic data setting, we find clear evidence for pre-caching. In the autoregressive language modeling setting, our experiments are more suggestive of the breadcrumbs hypothesis.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30830;&#20445;&#25925;&#20107;&#30340;&#31532;&#19968;&#21477;&#21644;&#26368;&#21518;&#19968;&#21477;&#30456;&#20851;&#24615;&#26469;&#29983;&#25104;&#21465;&#20107;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#19981;&#21516;&#21465;&#20107;&#23398;&#26041;&#27861;&#22914;&#20309;&#24433;&#21709;&#25925;&#20107;&#30340;&#35821;&#35328;&#24314;&#27169;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#25925;&#20107;&#21465;&#20107;&#23553;&#38381;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.00829</link><description>&lt;p&gt;
&#36820;&#22238;&#21040;&#24320;&#22987;&#65306;&#29983;&#25104;&#20855;&#26377;&#30456;&#20851;&#32456;&#28857;&#30340;&#21465;&#20107;
&lt;/p&gt;
&lt;p&gt;
Returning to the Start: Generating Narratives with Related Endpoints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00829
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30830;&#20445;&#25925;&#20107;&#30340;&#31532;&#19968;&#21477;&#21644;&#26368;&#21518;&#19968;&#21477;&#30456;&#20851;&#24615;&#26469;&#29983;&#25104;&#21465;&#20107;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#19981;&#21516;&#21465;&#20107;&#23398;&#26041;&#27861;&#22914;&#20309;&#24433;&#21709;&#25925;&#20107;&#30340;&#35821;&#35328;&#24314;&#27169;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#25925;&#20107;&#21465;&#20107;&#23553;&#38381;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20316;&#23478;&#32463;&#24120;&#22312;&#20889;&#20316;&#20013;&#20197;&#19982;&#24320;&#22836;&#21477;&#30456;&#20851;&#30340;&#32467;&#26463;&#21477;&#26469;&#20316;&#20026;&#32467;&#23614;&#65292;&#20197;&#26500;&#25104;&#19968;&#20010;&#8220;&#38381;&#29615;&#8221;&#30340;&#20196;&#20154;&#28385;&#24847;&#30340;&#21465;&#20107;&#12290;&#21463;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RENarGen&#65292;&#19968;&#31181;&#21487;&#25511;&#30340;&#25925;&#20107;&#29983;&#25104;&#33539;&#24335;&#65292;&#36890;&#36807;&#30830;&#20445;&#31532;&#19968;&#21477;&#21644;&#26368;&#21518;&#19968;&#21477;&#30456;&#20851;&#65292;&#28982;&#21518;&#22635;&#20805;&#20013;&#38388;&#21477;&#26469;&#29983;&#25104;&#21465;&#20107;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#21253;&#25324;&#21021;&#27493;&#25506;&#35752;&#20102;&#21465;&#20107;&#23398;&#20013;&#21508;&#31181;&#32467;&#23614;&#26041;&#27861;&#22914;&#20309;&#24433;&#21709;&#25925;&#20107;&#30340;&#35821;&#35328;&#24314;&#27169;&#12290;&#33258;&#21160;&#21270;&#21644;&#20154;&#24037;&#35780;&#20272;&#34920;&#26126;RENarGen&#29983;&#25104;&#30340;&#25925;&#20107;&#27604;&#24403;&#21069;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#21465;&#20107;&#23553;&#38381;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00829v1 Announce Type: new  Abstract: Human writers often bookend their writing with ending sentences that relate back to the beginning sentences in order to compose a satisfying narrative that "closes the loop." Motivated by this observation, we propose RENarGen, a controllable story-generation paradigm that generates narratives by ensuring the first and last sentences are related and then infilling the middle sentences. Our contributions include an initial exploration of how various methods of bookending from Narratology affect language modeling for stories. Automatic and human evaluations indicate RENarGen produces better stories with more narrative closure than current autoregressive models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;PID&#25511;&#21046;&#30340;&#33258;&#24840;&#26426;&#21046;&#65292;&#36890;&#36807;&#36712;&#36857;&#20248;&#21270;&#38382;&#39064;&#21644;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#29366;&#24577;&#30340;&#33258;&#21160;&#20462;&#27491;&#65292;&#26469;&#25552;&#39640;&#22312;&#22312;&#32447;&#25512;&#26029;&#20013;&#23545;&#36755;&#20837;&#25968;&#25454;&#25200;&#21160;&#30340;&#23481;&#24525;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.00828</link><description>&lt;p&gt;
&#22522;&#20110;PID&#25511;&#21046;&#30340;&#33258;&#24840;&#26426;&#21046;&#20197;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
PID Control-Based Self-Healing to Improve the Robustness of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;PID&#25511;&#21046;&#30340;&#33258;&#24840;&#26426;&#21046;&#65292;&#36890;&#36807;&#36712;&#36857;&#20248;&#21270;&#38382;&#39064;&#21644;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#29366;&#24577;&#30340;&#33258;&#21160;&#20462;&#27491;&#65292;&#26469;&#25552;&#39640;&#22312;&#22312;&#32447;&#25512;&#26029;&#20013;&#23545;&#36755;&#20837;&#25968;&#25454;&#25200;&#21160;&#30340;&#23481;&#24525;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20013;&#30340;&#26377;&#25928;&#24615;&#24050;&#34987;&#35777;&#26126;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#26292;&#38706;&#20102;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#22312;&#24341;&#20837;&#24494;&#23567;&#25200;&#21160;&#26102;&#30340;&#33030;&#24369;&#24615;&#12290;&#34429;&#28982;&#23545;&#20154;&#31867;&#26469;&#35828;&#35821;&#20041;&#26080;&#27861;&#21306;&#20998;&#65292;&#20294;&#36825;&#20123;&#25200;&#21160;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#32463;&#36807;&#20805;&#20998;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24341;&#21457;&#20102;&#22312;&#23433;&#20840;&#20851;&#38190;&#29615;&#22659;&#20013;&#37096;&#32626;&#23427;&#20204;&#21487;&#38752;&#24615;&#30340;&#25285;&#24551;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#35745;&#31639;&#39640;&#25928;&#30340;&#33258;&#24840;&#36807;&#31243;&#65292;&#29992;&#20110;&#22312;&#32447;&#25512;&#26029;&#26102;&#32416;&#27491;&#36755;&#20837;&#25968;&#25454;&#34987;&#26045;&#21152;&#25200;&#21160;&#26102;&#19981;&#33391;&#27169;&#22411;&#34892;&#20026;&#12290;&#36825;&#34987;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#36712;&#36857;&#20248;&#21270;&#38382;&#39064;&#65292;&#20869;&#37096;&#31070;&#32463;&#32593;&#32476;&#23618;&#30340;&#29366;&#24577;&#36890;&#36807;PID&#65288;&#27604;&#20363;-&#31215;&#20998;-&#24494;&#20998;&#65289;&#25511;&#21046;&#26426;&#21046;&#33258;&#21160;&#26657;&#27491;&#12290;P&#25511;&#21046;&#22120;&#38024;&#23545;&#21363;&#26102;&#29366;&#24577;&#35843;&#25972;&#65292;&#32780;I&#21644;D&#25511;&#21046;&#22120;&#32771;&#34385;&#36807;&#21435;&#30340;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00828v1 Announce Type: new  Abstract: Despite the effectiveness of deep neural networks in numerous natural language processing applications, recent findings have exposed the vulnerability of these language models when minor perturbations are introduced. While appearing semantically indistinguishable to humans, these perturbations can significantly reduce the performance of well-trained language models, raising concerns about the reliability of deploying them in safe-critical situations. In this work, we construct a computationally efficient self-healing process to correct undesired model behavior during online inference when perturbations are applied to input data. This is formulated as a trajectory optimization problem in which the internal states of the neural network layers are automatically corrected using a PID (Proportional-Integral-Derivative) control mechanism. The P controller targets immediate state adjustments, while the I and D controllers consider past states a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;Fine-tuned&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#24102;&#27880;&#37322;&#35821;&#26009;&#24211;PedSHAC&#65292;&#24182;&#33258;&#21160;&#25552;&#21462;&#20799;&#31185;&#24739;&#32773;&#30149;&#21382;&#20013;&#30340;&#35814;&#32454;&#31038;&#20250;&#20581;&#24247;&#20915;&#23450;&#22240;&#32032;&#12290;</title><link>https://arxiv.org/abs/2404.00826</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#20799;&#31185;&#24739;&#32773;&#30149;&#21382;&#20013;&#25552;&#21462;&#20581;&#24247;&#30340;&#31038;&#20250;&#20915;&#23450;&#22240;&#32032;&#65306;&#26032;&#39062;&#30340;&#35821;&#26009;&#24211;&#21644;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Extracting Social Determinants of Health from Pediatric Patient Notes Using Large Language Models: Novel Corpus and Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00826
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;Fine-tuned&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#24102;&#27880;&#37322;&#35821;&#26009;&#24211;PedSHAC&#65292;&#24182;&#33258;&#21160;&#25552;&#21462;&#20799;&#31185;&#24739;&#32773;&#30149;&#21382;&#20013;&#30340;&#35814;&#32454;&#31038;&#20250;&#20581;&#24247;&#20915;&#23450;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20581;&#24247;&#30340;&#31038;&#20250;&#20915;&#23450;&#22240;&#32032;(SDoH)&#22312;&#22609;&#36896;&#20581;&#24247;&#32467;&#26524;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#20799;&#31185;&#20154;&#32676;&#20013;&#65292;&#24178;&#39044;&#25514;&#26045;&#21487;&#33021;&#20855;&#26377;&#38271;&#26399;&#24433;&#21709;&#12290;SDoH&#32463;&#24120;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;(EHR)&#20013;&#36827;&#34892;&#30740;&#31350;&#65292;EHR&#20026;&#22810;&#26679;&#21270;&#30340;&#24739;&#32773;&#25968;&#25454;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#24211;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#24102;&#27880;&#37322;&#35821;&#26009;&#24211;&#65292;&#20799;&#31185;&#31038;&#20250;&#21490;&#27880;&#37322;&#35821;&#26009;&#24211;(PedSHAC)&#65292;&#24182;&#35780;&#20272;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#24494;&#35843;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#33258;&#21160;&#25552;&#21462;&#35814;&#32454;&#30340;SDoH&#34920;&#24449;&#12290;PedSHAC&#21253;&#25324;&#26469;&#33258;&#21326;&#30427;&#39039;&#22823;&#23398;(UW)&#21307;&#38498;&#31995;&#32479;&#20869;&#30340;1,260&#20221;&#20020;&#24202;&#35760;&#24405;&#20013;&#27880;&#37322;&#30340;&#31038;&#20250;&#21490;&#33410;&#12290;&#37319;&#29992;&#20107;&#20214;&#20026;&#22522;&#30784;&#30340;&#27880;&#37322;&#26041;&#26696;&#65292;PedSHAC&#25429;&#25417;&#20102;&#21313;&#20010;&#19981;&#21516;&#30340;&#20581;&#24247;&#20915;&#23450;&#22240;&#32032;&#65292;&#28085;&#30422;&#20102;&#29983;&#27963;&#21644;&#32463;&#27982;&#31283;&#23450;&#24615;&#65292;&#20197;&#21069;&#30340;&#21019;&#20260;&#65292;&#25945;&#32946;&#33719;&#21462;&#65292;&#29289;&#36136;&#20351;&#29992;&#21490;&#20197;&#21450;&#24515;&#29702;&#20581;&#24247;&#31561;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00826v1 Announce Type: new  Abstract: Social determinants of health (SDoH) play a critical role in shaping health outcomes, particularly in pediatric populations where interventions can have long-term implications. SDoH are frequently studied in the Electronic Health Record (EHR), which provides a rich repository for diverse patient data. In this work, we present a novel annotated corpus, the Pediatric Social History Annotation Corpus (PedSHAC), and evaluate the automatic extraction of detailed SDoH representations using fine-tuned and in-context learning methods with Large Language Models (LLMs). PedSHAC comprises annotated social history sections from 1,260 clinical notes obtained from pediatric patients within the University of Washington (UW) hospital system. Employing an event-based annotation scheme, PedSHAC captures ten distinct health determinants to encompass living and economic stability, prior trauma, education access, substance use history, and mental health with
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#25490;&#32451;&#30340;&#27169;&#22359;&#21270;&#21644;&#32452;&#21512;&#24335;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#25345;&#32493;&#21521;&#35821;&#35328;&#27169;&#22411;&#28155;&#21152;&#26032;&#27169;&#22359;&#24182;&#23558;&#20854;&#19982;&#29616;&#26377;&#27169;&#22359;&#32452;&#21512;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26694;&#26550;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#24182;&#26377;&#25928;&#25512;&#21160;&#30693;&#35782;&#36716;&#31227;&#12290;</title><link>https://arxiv.org/abs/2404.00790</link><description>&lt;p&gt;
&#26080;&#38656;&#25490;&#32451;&#30340;&#27169;&#22359;&#21270;&#21644;&#32452;&#21512;&#24335;&#25345;&#32493;&#23398;&#20064;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Rehearsal-Free Modular and Compositional Continual Learning for Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00790
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#25490;&#32451;&#30340;&#27169;&#22359;&#21270;&#21644;&#32452;&#21512;&#24335;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#25345;&#32493;&#21521;&#35821;&#35328;&#27169;&#22411;&#28155;&#21152;&#26032;&#27169;&#22359;&#24182;&#23558;&#20854;&#19982;&#29616;&#26377;&#27169;&#22359;&#32452;&#21512;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26694;&#26550;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#24182;&#26377;&#25928;&#25512;&#21160;&#30693;&#35782;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26088;&#22312;&#22312;&#19981;&#36951;&#24536;&#29616;&#26377;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#36880;&#27493;&#33719;&#21462;&#26032;&#30340;&#30693;&#35782;&#12290;&#20026;&#20102;&#20811;&#26381;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#26041;&#27861;&#35201;&#20040;&#22522;&#20110;&#25490;&#32451;&#65292;&#21363;&#23384;&#20648;&#26469;&#33258;&#20808;&#21069;&#20219;&#21153;&#30340;&#25968;&#25454;&#31034;&#20363;&#20197;&#36827;&#34892;&#25968;&#25454;&#37325;&#25773;&#65292;&#35201;&#20040;&#23558;&#21442;&#25968;&#38548;&#31163;&#20998;&#37197;&#32473;&#27599;&#20010;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#25490;&#32451;&#30340;&#26041;&#27861;&#20250;&#24341;&#21457;&#38544;&#31169;&#21644;&#20869;&#23384;&#38382;&#39064;&#65292;&#21442;&#25968;&#38548;&#31163;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#19981;&#32771;&#34385;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#38459;&#30861;&#30693;&#35782;&#36716;&#31227;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MoCL&#65292;&#19968;&#20010;&#26080;&#38656;&#25490;&#32451;&#30340;&#27169;&#22359;&#21270;&#21644;&#32452;&#21512;&#24335;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#19981;&#26029;&#21521;&#35821;&#35328;&#27169;&#22411;&#28155;&#21152;&#26032;&#30340;&#27169;&#22359;&#65292;&#24182;&#23558;&#20854;&#19982;&#29616;&#26377;&#27169;&#22359;&#32452;&#21512;&#22312;&#19968;&#36215;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;MoCL&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#24182;&#26377;&#25928;&#20419;&#36827;&#20102;&#30693;&#35782;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00790v1 Announce Type: cross  Abstract: Continual learning aims at incrementally acquiring new knowledge while not forgetting existing knowledge. To overcome catastrophic forgetting, methods are either rehearsal-based, i.e., store data examples from previous tasks for data replay, or isolate parameters dedicated to each task. However, rehearsal-based methods raise privacy and memory issues, and parameter-isolation continual learning does not consider interaction between tasks, thus hindering knowledge transfer. In this work, we propose MoCL, a rehearsal-free Modular and Compositional Continual Learning framework which continually adds new modules to language models and composes them with existing modules. Experiments on various benchmarks show that MoCL outperforms state of the art and effectively facilitates knowledge transfer.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;JacHess&#26041;&#27861;&#65292;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#20102;&#22312;&#39046;&#22495;&#20869;&#27867;&#21270;&#21644;&#26657;&#20934;&#26041;&#38754;&#30340;&#26174;&#33879;&#25913;&#36827;</title><link>https://arxiv.org/abs/2404.00758</link><description>&lt;p&gt;
&#20174;&#40065;&#26834;&#24615;&#21040;&#25913;&#36827;&#30340;&#27867;&#21270;&#21644;&#26657;&#20934;&#65306;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
From Robustness to Improved Generalization and Calibration in Pre-trained Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00758
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;JacHess&#26041;&#27861;&#65292;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#20102;&#22312;&#39046;&#22495;&#20869;&#27867;&#21270;&#21644;&#26657;&#20934;&#26041;&#38754;&#30340;&#26174;&#33879;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#24378;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#23545;&#20110;&#20854;&#26377;&#25928;&#24615;&#21644;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#22880;&#23450;&#20102;&#40065;&#26834;&#24615;&#23545;&#20110;&#25913;&#21892;&#27867;&#21270;&#30340;&#37325;&#35201;&#24615;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#36890;&#36807;&#38597;&#21487;&#27604;&#21644;&#40657;&#22622;&#27491;&#21017;&#21270;&#23454;&#29616;&#30340;&#34920;&#24449;&#24179;&#28369;&#24230;&#22312;&#25552;&#39640;PLM&#24615;&#33021;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#23613;&#31649;&#27492;&#31867;&#27491;&#21017;&#21270;&#26041;&#27861;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#35777;&#26126;&#26377;&#25928;&#65292;&#20294;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#30340;&#24212;&#29992;&#65292;&#20854;&#20013;PLM&#36755;&#20837;&#28304;&#33258;&#31163;&#25955;&#22495;&#65292;&#25552;&#20986;&#20102;&#29420;&#29305;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#27491;&#21017;&#21270;&#26041;&#27861;JacHess&#65292;&#35813;&#26041;&#27861;&#30456;&#23545;&#20110;PLM&#20013;&#38388;&#34920;&#31034;&#30340;&#36755;&#20837;&#26368;&#23567;&#21270;&#38597;&#21487;&#27604;&#21644;&#40657;&#22622;&#30697;&#38453;&#30340;&#33539;&#25968;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#20351;&#29992;GLUE&#22522;&#20934;&#34920;&#26126;&#65292;JacHess&#26174;&#33879;&#25913;&#21892;&#20102;PLMs&#30340;&#39046;&#22495;&#20869;&#27867;&#21270;&#21644;&#26657;&#20934;&#65292;&#34920;&#29616;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00758v1 Announce Type: new  Abstract: Enhancing generalization and uncertainty quantification in pre-trained language models (PLMs) is crucial for their effectiveness and reliability. Building on machine learning research that established the importance of robustness for improving generalization, we investigate the role of representation smoothness, achieved via Jacobian and Hessian regularization, in enhancing PLM performance. Although such regularization methods have proven effective in computer vision, their application in natural language processing (NLP), where PLM inputs are derived from a discrete domain, poses unique challenges. We introduce a novel two-phase regularization approach, JacHess, which minimizes the norms of the Jacobian and Hessian matrices within PLM intermediate representations relative to their inputs. Our evaluation using the GLUE benchmark demonstrates that JacHess significantly improves in-domain generalization and calibration in PLMs, outperformi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#24322;&#24120;&#26816;&#27979;&#26469;&#34913;&#37327;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#20013;&#26679;&#26412;&#25509;&#36817;&#30495;&#23454;&#20998;&#24067;&#30340;&#31243;&#24230;&#65292;&#24182;&#39564;&#35777;&#23454;&#39564;&#32467;&#26524;&#39318;&#27425;&#25903;&#25345;&#20102;&#24615;&#33021;&#19982;&#37319;&#26679;&#36817;&#20284;&#24230;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;</title><link>https://arxiv.org/abs/2404.00752</link><description>&lt;p&gt;
&#20851;&#20110;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#30340;&#30495;&#23454;&#20998;&#24067;&#36817;&#20284;
&lt;/p&gt;
&lt;p&gt;
On the True Distribution Approximation of Minimum Bayes-Risk Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#24322;&#24120;&#26816;&#27979;&#26469;&#34913;&#37327;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#20013;&#26679;&#26412;&#25509;&#36817;&#30495;&#23454;&#20998;&#24067;&#30340;&#31243;&#24230;&#65292;&#24182;&#39564;&#35777;&#23454;&#39564;&#32467;&#26524;&#39318;&#27425;&#25903;&#25345;&#20102;&#24615;&#33021;&#19982;&#37319;&#26679;&#36817;&#20284;&#24230;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#65288;MBR&#65289;&#35299;&#30721;&#26368;&#36817;&#22312;&#25991;&#26412;&#29983;&#25104;&#20013;&#37325;&#26032;&#24341;&#36215;&#20851;&#27880;&#12290;MBR&#35299;&#30721;&#23558;&#20174;&#27169;&#22411;&#20013;&#37319;&#26679;&#30340;&#25991;&#26412;&#35270;&#20026;&#20266;&#21442;&#32771;&#65292;&#24182;&#36873;&#25321;&#19982;&#20854;&#20182;&#25991;&#26412;&#26368;&#30456;&#20284;&#30340;&#25991;&#26412;&#12290;&#22240;&#27492;&#65292;&#37319;&#26679;&#26159;MBR&#35299;&#30721;&#30340;&#20851;&#38190;&#20803;&#32032;&#20043;&#19968;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#25253;&#21578;&#31216;&#24615;&#33021;&#22240;&#37319;&#26679;&#26041;&#27861;&#32780;&#24322;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#36825;&#31181;&#24615;&#33021;&#21464;&#21270;&#24456;&#21487;&#33021;&#19982;&#26679;&#26412;&#22914;&#20309;&#25509;&#36817;&#21442;&#32771;&#30495;&#23454;&#20998;&#24067;&#26377;&#20851;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#36817;&#20284;&#23578;&#26410;&#28145;&#20837;&#30740;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#24322;&#24120;&#26816;&#27979;&#26469;&#34913;&#37327;&#36924;&#36817;&#31243;&#24230;&#12290;&#25105;&#20204;&#39318;&#20808;&#20180;&#32454;&#30740;&#31350;&#24615;&#33021;&#30340;&#21464;&#21270;&#65292;&#28982;&#21518;&#23637;&#31034;&#20197;&#24448;&#20851;&#20110;&#26679;&#26412;&#30340;&#20551;&#35774;&#19982;&#21464;&#21270;&#30340;&#30456;&#20851;&#24615;&#19981;&#20339;&#65292;&#20294;&#25105;&#20204;&#24341;&#20837;&#30340;&#24322;&#24120;&#20998;&#25968;&#21017;&#30456;&#20851;&#12290;&#36825;&#20123;&#32467;&#26524;&#26159;&#39318;&#27425;&#20973;&#32463;&#39564;&#35777;&#26126;&#20102;&#24615;&#33021;&#21644;&#37319;&#26679;&#36817;&#20284;&#24230;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00752v1 Announce Type: cross  Abstract: Minimum Bayes-risk (MBR) decoding has recently gained renewed attention in text generation. MBR decoding considers texts sampled from a model as pseudo-references and selects the text with the highest similarity to the others. Therefore, sampling is one of the key elements of MBR decoding, and previous studies reported that the performance varies by sampling methods. From a theoretical standpoint, this performance variation is likely tied to how closely the samples approximate the true distribution of references. However, this approximation has not been the subject of in-depth study. In this study, we propose using anomaly detection to measure the degree of approximation. We first closely examine the performance variation and then show that previous hypotheses about samples do not correlate well with the variation, but our introduced anomaly scores do. The results are the first to empirically support the link between the performance an
&lt;/p&gt;</description></item><item><title>&#22823;&#35821;&#35328;&#27169;&#22411;&#19981;&#20165;&#33021;&#22815;&#22312;&#35782;&#21035;&#21644;&#21306;&#20998;&#24378;&#21183;&#21644;&#24369;&#21183;&#35770;&#28857;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#36824;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#30340;&#20449;&#24565;&#21644;&#20154;&#21475;&#29305;&#24449;&#39044;&#27979;&#20854;&#31435;&#22330;&#65292;&#24182;&#30830;&#23450;&#35770;&#28857;&#23545;&#20010;&#20154;&#30340;&#21560;&#24341;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.00750</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#33021;&#35782;&#21035;&#20196;&#20154;&#20449;&#26381;&#30340;&#35770;&#28857;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Language Models Recognize Convincing Arguments?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00750
&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#19981;&#20165;&#33021;&#22815;&#22312;&#35782;&#21035;&#21644;&#21306;&#20998;&#24378;&#21183;&#21644;&#24369;&#21183;&#35770;&#28857;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#36824;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#30340;&#20449;&#24565;&#21644;&#20154;&#21475;&#29305;&#24449;&#39044;&#27979;&#20854;&#31435;&#22330;&#65292;&#24182;&#30830;&#23450;&#35770;&#28857;&#23545;&#20010;&#20154;&#30340;&#21560;&#24341;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26174;&#33879;&#19988;&#19981;&#26029;&#22686;&#24378;&#30340;&#33021;&#21147;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#23427;&#20204;&#21487;&#33021;&#34987;&#28389;&#29992;&#29992;&#26469;&#21019;&#36896;&#20010;&#24615;&#21270;&#12289;&#20196;&#20154;&#20449;&#26381;&#30340;&#34394;&#20551;&#20449;&#24687;&#21644;&#23459;&#20256;&#30340;&#25285;&#24551;&#12290;&#20026;&#20102;&#28145;&#20837;&#20102;&#35299;LLMs&#30340;&#35828;&#26381;&#33021;&#21147;&#65292;&#32780;&#21448;&#19981;&#30452;&#25509;&#19982;&#20154;&#31867;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#25552;&#20986;&#30740;&#31350;&#23427;&#20204;&#22312;&#26816;&#27979;&#20196;&#20154;&#20449;&#26381;&#30340;&#35770;&#28857;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#36890;&#36807;&#28155;&#21152;&#36777;&#35770;&#12289;&#25237;&#31080;&#21644;&#29992;&#25143;&#29305;&#24449;&#26469;&#25193;&#23637;&#20102;Durmus&#21644;Cardie&#65288;2018&#65289;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#34913;&#37327;LLMs&#33021;&#21147;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#65288;1&#65289;&#21306;&#20998;&#24378;&#21183;&#21644;&#24369;&#21183;&#35770;&#28857;&#65292;&#65288;2&#65289;&#22522;&#20110;&#20449;&#24565;&#21644;&#20154;&#21475;&#29305;&#24449;&#39044;&#27979;&#31435;&#22330;&#65292;&#20197;&#21450;&#65288;3&#65289;&#26681;&#25454;&#20010;&#20154;&#29305;&#24449;&#30830;&#23450;&#23545;&#19968;&#20010;&#35770;&#28857;&#30340;&#21560;&#24341;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;LLMs&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#34920;&#29616;&#19982;&#20154;&#31867;&#19981;&#30456;&#19978;&#19979;&#65292;&#24182;&#19988;&#32467;&#21512;&#19981;&#21516;LLMs&#30340;&#39044;&#27979;&#21487;&#20197;&#33719;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#29978;&#33267;&#36229;&#36807;&#20154;&#31867;&#30340;&#34920;&#29616;&#12290;&#38543;&#25991;&#38468;&#24102;&#21457;&#24067;&#30340;&#25968;&#25454;&#21644;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00750v1 Announce Type: new  Abstract: The remarkable and ever-increasing capabilities of Large Language Models (LLMs) have raised concerns about their potential misuse for creating personalized, convincing misinformation and propaganda. To gain insights into LLMs' persuasive capabilities without directly engaging in experimentation with humans, we propose studying their performance on the related task of detecting convincing arguments. We extend a dataset by Durmus &amp; Cardie (2018) with debates, votes, and user traits and propose tasks measuring LLMs' ability to (1) distinguish between strong and weak arguments, (2) predict stances based on beliefs and demographic characteristics, and (3) determine the appeal of an argument to an individual based on their traits. We show that LLMs perform on par with humans in these tasks and that combining predictions from different LLMs yields significant performance gains, even surpassing human performance. The data and code released with 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#34913;&#37327;&#25968;&#25454;&#20998;&#24067;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#24615;&#33021;&#21644;&#35780;&#20272;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#25968;&#25454;&#20998;&#24067;&#30340;&#37325;&#35201;&#24615;&#21644;&#23545;&#35780;&#20272;&#26694;&#26550;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2404.00748</link><description>&lt;p&gt;
&#22522;&#20934;&#36879;&#26126;&#24230;&#65306;&#34913;&#37327;&#25968;&#25454;&#23545;&#35780;&#20272;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Benchmark Transparency: Measuring the Impact of Data on Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#34913;&#37327;&#25968;&#25454;&#20998;&#24067;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#24615;&#33021;&#21644;&#35780;&#20272;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#25968;&#25454;&#20998;&#24067;&#30340;&#37325;&#35201;&#24615;&#21644;&#23545;&#35780;&#20272;&#26694;&#26550;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#39033;&#20851;&#20110;&#37327;&#21270;&#25968;&#25454;&#20998;&#24067;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#24615;&#33021;&#21644;&#35780;&#20272;&#24433;&#21709;&#30340;&#21021;&#27493;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#34913;&#37327;&#25968;&#25454;&#28857;&#22312;6&#20010;&#19981;&#21516;&#32500;&#24230;&#19978;&#30340;&#20998;&#24067;&#65306;&#27169;&#31946;&#24230;&#12289;&#22256;&#38590;&#24230;&#12289;&#21487;&#21306;&#20998;&#24615;&#12289;&#38271;&#24230;&#12289;&#22122;&#22768;&#21644;&#22256;&#24785;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#22343;&#34913;&#20998;&#23618;&#25277;&#26679;&#26469;&#34913;&#37327;&#25968;&#25454;&#20998;&#24067;&#23545;&#32477;&#23545;&#65288;&#20934;&#30830;&#29575;/F1&#65289;&#21644;&#30456;&#23545;&#65288;&#25490;&#21517;&#65289;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65288;SQUAD&#21644;MNLI&#65289;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#27979;&#35797;&#20102;&#24635;&#20849;135&#31181;&#19981;&#21516;&#30340;&#27169;&#22411;&#65288;125&#31181;&#22312;SQUAD&#19978;&#65292;10&#31181;&#22312;MNLI&#19978;&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#27809;&#26377;&#26126;&#30830;&#25511;&#21046;&#25968;&#25454;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#65292;&#26631;&#20934;&#35780;&#20272;&#26694;&#26550;&#26159;&#19981;&#19968;&#33268;&#21644;&#19981;&#21487;&#38752;&#30340;&#12290;&#25105;&#20204;&#21457;&#29616;&#25968;&#25454;&#30340;&#24433;&#21709;&#22312;&#32479;&#35745;&#19978;&#26159;&#26174;&#33879;&#30340;&#65292;&#24182;&#19988;&#36890;&#24120;&#27604;&#26356;&#25913;&#24230;&#37327;&#30340;&#24433;&#21709;&#26356;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00748v1 Announce Type: cross  Abstract: In this paper we present an exploratory research on quantifying the impact that data distribution has on the performance and evaluation of NLP models. We propose an automated framework that measures the data point distribution across 6 different dimensions: ambiguity, difficulty, discriminability, length, noise, and perplexity.   We use disproportional stratified sampling to measure how much the data distribution affects absolute (Acc/F1) and relative (Rank) model performance. We experiment on 2 different datasets (SQUAD and MNLI) and test a total of 135 different models (125 on SQUAD and 10 on MNLI). We demonstrate that without explicit control of the data distribution, standard evaluation frameworks are inconsistent and unreliable. We find that the impact of the data is statistically significant and is often larger than the impact of changing the metric.   In a second set of experiments, we demonstrate that the impact of data on eval
&lt;/p&gt;</description></item><item><title>Opera Graeca Adnotata&#65288;OGA&#65289;&#26159;&#21476;&#24076;&#33098;&#26368;&#22823;&#30340;&#24320;&#25918;&#33719;&#21462;&#22810;&#23618;&#35821;&#26009;&#24211;&#65292;&#21253;&#25324;&#26469;&#33258;PerseusDL&#21644;OpenGreekAndLatin GitHub&#20179;&#24211;&#30340;1,687&#37096;&#25991;&#23398;&#20316;&#21697;&#21644;&#36229;&#36807;3400&#19975;&#20010;&#35789;&#20803;&#65292;&#20016;&#23500;&#26631;&#27880;&#20102;&#19971;&#20010;&#27880;&#37322;&#23618;&#12290;</title><link>https://arxiv.org/abs/2404.00739</link><description>&lt;p&gt;
Opera Graeca Adnotata: &#20026;&#21476;&#24076;&#33098;&#24314;&#31435;&#20102;&#19968;&#20010;&#36229;&#36807;3400&#19975;&#35789;&#30340;&#22810;&#23618;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
Opera Graeca Adnotata: Building a 34M+ Token Multilayer Corpus for Ancient Greek
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00739
&lt;/p&gt;
&lt;p&gt;
Opera Graeca Adnotata&#65288;OGA&#65289;&#26159;&#21476;&#24076;&#33098;&#26368;&#22823;&#30340;&#24320;&#25918;&#33719;&#21462;&#22810;&#23618;&#35821;&#26009;&#24211;&#65292;&#21253;&#25324;&#26469;&#33258;PerseusDL&#21644;OpenGreekAndLatin GitHub&#20179;&#24211;&#30340;1,687&#37096;&#25991;&#23398;&#20316;&#21697;&#21644;&#36229;&#36807;3400&#19975;&#20010;&#35789;&#20803;&#65292;&#20016;&#23500;&#26631;&#27880;&#20102;&#19971;&#20010;&#27880;&#37322;&#23618;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Opera Graeca Adnotata&#65288;OGA&#65289;&#30340;beta&#29256;&#26412;0.1.0&#65292;&#36825;&#26159;&#21476;&#24076;&#33098;&#26368;&#22823;&#30340;&#24320;&#25918;&#33719;&#21462;&#22810;&#23618;&#35821;&#26009;&#24211;&#12290;OGA&#21253;&#25324;&#26469;&#33258;PerseusDL&#21644;OpenGreekAndLatin GitHub&#20179;&#24211;&#30340;1,687&#37096;&#25991;&#23398;&#20316;&#21697;&#21644;&#36229;&#36807;3400&#19975;&#20010;&#35789;&#20803;&#65292;&#36825;&#20123;&#20179;&#24211;&#21253;&#21547;&#20174;&#20844;&#20803;&#21069;&#32422;800&#24180;&#21040;&#20844;&#20803;&#32422;250&#24180;&#30340;&#21476;&#24076;&#33098;&#25991;&#26412;&#12290;&#25991;&#26412;&#34987;&#20016;&#23500;&#22320;&#26631;&#27880;&#20102;&#19971;&#20010;&#27880;&#37322;&#23618;&#65306;(i) &#20998;&#35789;&#23618;&#65307;(ii) &#21477;&#23376;&#20998;&#21106;&#23618;&#65307;(iii) &#35789;&#24418;&#26631;&#27880;&#23618;&#65307;(iv) &#35821;&#27861;&#24418;&#24577;&#23618;&#65307;(v) &#20381;&#23384;&#20851;&#31995;&#23618;&#65307;(vi) &#20381;&#23384;&#20851;&#31995;&#20989;&#25968;&#23618;&#65307;(vii) &#35268;&#33539;&#25991;&#26412;&#26381;&#21153;&#65288;CTS&#65289;&#24341;&#25991;&#23618;&#12290;&#25991;&#31456;&#25551;&#36848;&#20102;&#27599;&#20010;&#23618;&#30340;&#21019;&#24314;&#36807;&#31243;&#65292;&#31361;&#20986;&#20102;&#36935;&#21040;&#30340;&#20027;&#35201;&#25216;&#26415;&#21644;&#27880;&#37322;&#30456;&#20851;&#38382;&#39064;&#12290;&#20998;&#35789;&#12289;&#21477;&#23376;&#20998;&#21106;&#21644;CTS&#24341;&#25991;&#26159;&#30001;&#22522;&#20110;&#35268;&#21017;&#30340;&#31639;&#27861;&#25191;&#34892;&#30340;&#65292;&#32780;&#24418;&#24577;&#21477;&#27861;&#26631;&#27880;&#26159;Ancient Greek Dependency Tree&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;COMBO&#35299;&#26512;&#22120;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00739v1 Announce Type: new  Abstract: In this article, the beta version 0.1.0 of Opera Graeca Adnotata (OGA), the largest open-access multilayer corpus for Ancient Greek (AG) is presented. OGA consists of 1,687 literary works and 34M+ tokens coming from the PerseusDL and OpenGreekAndLatin GitHub repositories, which host AG texts ranging from about 800 BCE to about 250 CE. The texts have been enriched with seven annotation layers: (i) tokenization layer; (ii) sentence segmentation layer; (iii) lemmatization layer; (iv) morphological layer; (v) dependency layer; (vi) dependency function layer; (vii) Canonical Text Services (CTS) citation layer. The creation of each layer is described by highlighting the main technical and annotation-related issues encountered. Tokenization, sentence segmentation, and CTS citation are performed by rule-based algorithms, while morphosyntactic annotation is the output of the COMBO parser trained on the data of the Ancient Greek Dependency Treeban
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22823;&#23567;&#30340;&#25511;&#21046;&#65292;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#26680;&#25351;&#20195;&#35299;&#26512;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#25512;&#29702;&#36895;&#24230;&#26041;&#38754;&#20248;&#20110;&#26356;&#36817;&#26399;&#30340;&#22522;&#20110;&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;&#65292;&#32780;&#22312;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#27169;&#22411;&#20013;&#65292;&#26368;&#32769;&#30340;&#27169;&#22411;&#22312;&#36328;&#22495;&#25991;&#26412;&#20307;&#35009;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>https://arxiv.org/abs/2404.00727</link><description>&lt;p&gt;
&#26680;&#25351;&#20195;&#35299;&#26512;&#27169;&#22411;&#30340;&#21463;&#25511;&#37325;&#26032;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Controlled Reevaluation of Coreference Resolution Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00727
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22823;&#23567;&#30340;&#25511;&#21046;&#65292;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#26680;&#25351;&#20195;&#35299;&#26512;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#25512;&#29702;&#36895;&#24230;&#26041;&#38754;&#20248;&#20110;&#26356;&#36817;&#26399;&#30340;&#22522;&#20110;&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;&#65292;&#32780;&#22312;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#27169;&#22411;&#20013;&#65292;&#26368;&#32769;&#30340;&#27169;&#22411;&#22312;&#36328;&#22495;&#25991;&#26412;&#20307;&#35009;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25152;&#26377;&#26368;&#20808;&#36827;&#30340;&#26680;&#25351;&#20195;&#35299;&#26512;&#65288;CR&#65289;&#27169;&#22411;&#37117;&#28041;&#21450;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#19968;&#20010;CR&#27169;&#22411;&#20248;&#20110;&#21478;&#19968;&#20010;&#30340;&#20986;&#33394;&#24615;&#33021;&#26159;&#30001;&#35821;&#35328;&#27169;&#22411;&#36873;&#25321;&#36824;&#26159;&#20854;&#20182;&#22240;&#32032;&#65288;&#22914;&#29305;&#23450;&#20110;&#20219;&#21153;&#30340;&#26550;&#26500;&#65289;&#36896;&#25104;&#30340;&#65292;&#30001;&#20110;&#32570;&#20047;&#26631;&#20934;&#21270;&#30340;&#23454;&#39564;&#35774;&#32622;&#65292;&#36825;&#26159;&#24456;&#38590;&#25110;&#19981;&#21487;&#33021;&#30830;&#23450;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#27169;&#31946;&#24615;&#65292;&#25105;&#20204;&#31995;&#32479;&#35780;&#20272;&#20102;&#20116;&#20010;CR&#27169;&#22411;&#65292;&#24182;&#25511;&#21046;&#20102;&#19968;&#20123;&#35774;&#35745;&#20915;&#31574;&#65292;&#21253;&#25324;&#27599;&#20010;&#27169;&#22411;&#20351;&#29992;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#24403;&#25511;&#21046;&#35821;&#35328;&#27169;&#22411;&#22823;&#23567;&#26102;&#65292;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;CR&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#25512;&#29702;&#36895;&#24230;&#26041;&#38754;&#20248;&#20110;&#26356;&#36817;&#26399;&#30340;&#22522;&#20110;&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;CR&#27169;&#22411;&#20013;&#65292;&#36739;&#36817;&#26399;&#30340;&#27169;&#22411;&#24182;&#19981;&#24635;&#26159;&#26356;&#20934;&#30830;&#65292;&#32780;&#25105;&#20204;&#27979;&#35797;&#30340;&#26368;&#32769;&#30340;CR&#27169;&#22411;&#22312;&#36328;&#22495;&#25991;&#26412;&#20307;&#35009;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#25511;&#21046;&#35821;&#35328;&#27169;&#22411;&#30340;&#36873;&#25321;&#21487;&#20197;&#20943;&#23569;&#22823;&#37096;&#20998;&#65292;&#20294;&#24182;&#38750;&#20840;&#37096;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00727v1 Announce Type: new  Abstract: All state-of-the-art coreference resolution (CR) models involve finetuning a pretrained language model. Whether the superior performance of one CR model over another is due to the choice of language model or other factors, such as the task-specific architecture, is difficult or impossible to determine due to lack of a standardized experimental setup. To resolve this ambiguity, we systematically evaluate five CR models and control for certain design decisions including the pretrained language model used by each. When controlling for language model size, encoder-based CR models outperform more recent decoder-based models in terms of both accuracy and inference speed. Surprisingly, among encoder-based CR models, more recent models are not always more accurate, and the oldest CR model that we test generalizes the best to out-of-domain textual genres. We conclude that controlling for the choice of language model reduces most, but not all, of 
&lt;/p&gt;</description></item><item><title>&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#30456;&#21516;&#39044;&#31639;&#19979;&#20135;&#29983;&#21487;&#38752;&#30340;&#25913;&#36827;&#65292;&#20294;&#22312;&#26080;&#27861;&#36827;&#34892;&#21333;&#20803;&#27979;&#35797;&#30340;&#24773;&#20917;&#19979;&#65292;&#36739;&#23567;&#30340;&#27169;&#22411;&#36873;&#25321;&#25490;&#21517;&#27425;&#20110;&#36739;&#22823;&#27169;&#22411;&#30340;&#21333;&#20010;&#36755;&#20986;&#12290;</title><link>https://arxiv.org/abs/2404.00725</link><description>&lt;p&gt;
&#36234;&#22823;&#36234;&#22909;&#21527;&#65311;&#36890;&#36807;&#39044;&#31639;&#37325;&#26032;&#20998;&#37197;&#25913;&#36827;LLM&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
The Larger the Better? Improved LLM Code-Generation via Budget Reallocation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00725
&lt;/p&gt;
&lt;p&gt;
&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#30456;&#21516;&#39044;&#31639;&#19979;&#20135;&#29983;&#21487;&#38752;&#30340;&#25913;&#36827;&#65292;&#20294;&#22312;&#26080;&#27861;&#36827;&#34892;&#21333;&#20803;&#27979;&#35797;&#30340;&#24773;&#20917;&#19979;&#65292;&#36739;&#23567;&#30340;&#27169;&#22411;&#36873;&#25321;&#25490;&#21517;&#27425;&#20110;&#36739;&#22823;&#27169;&#22411;&#30340;&#21333;&#20010;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#26222;&#36941;&#35748;&#20026;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#27604;&#36739;&#23567;&#30340;&#27169;&#22411;&#26356;&#22909;&#12290;&#28982;&#32780;&#65292;&#26356;&#22823;&#30340;&#27169;&#22411;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#20063;&#38656;&#35201;&#26356;&#22810;&#30340;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#36825;&#23601;&#24341;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#24403;&#20004;&#20010;&#27169;&#22411;&#22312;&#30456;&#21516;&#30340;&#39044;&#31639;&#19979;&#36816;&#34892;&#26102;&#20250;&#21457;&#29983;&#20160;&#20040;&#65311;&#65288;&#20363;&#22914;&#65292;&#35745;&#31639;&#36164;&#28304;&#65292;&#36816;&#34892;&#26102;&#38388;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#21508;&#31181;&#22823;&#23567;&#30340;&#20195;&#30721;&#29983;&#25104;LLMs&#65292;&#24182;&#36827;&#34892;&#27604;&#36739;&#65292;&#20363;&#22914;&#36816;&#34892;&#19968;&#20010;70B&#27169;&#22411;&#19968;&#27425;&#19982;&#20174;13B&#27169;&#22411;&#29983;&#25104;&#20116;&#20010;&#36755;&#20986;&#24182;&#36873;&#25321;&#19968;&#20010;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26631;&#20934;&#21333;&#20803;&#27979;&#35797;&#35774;&#32622;&#20013;&#65292;&#21453;&#22797;&#20351;&#29992;&#36739;&#23567;&#30340;&#27169;&#22411;&#21487;&#20197;&#20135;&#29983;&#19968;&#33268;&#30340;&#25913;&#36827;&#65292;&#22312;&#20116;&#20010;&#20219;&#21153;&#20013;&#26368;&#39640;&#21487;&#36798;15%&#30340;&#22686;&#30410;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22312;&#26080;&#27861;&#36827;&#34892;&#21333;&#20803;&#27979;&#35797;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#36739;&#23567;&#27169;&#22411;&#20013;&#22522;&#20110;&#25490;&#21517;&#30340;&#20505;&#36873;&#36873;&#25321;&#34920;&#29616;&#19981;&#21450;&#26469;&#33258;&#36739;&#22823;&#27169;&#22411;&#30340;&#21333;&#20010;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#26174;&#20102;&#20351;&#29992;&#36739;&#23567;&#27169;&#22411;&#32780;&#38750;&#36739;&#22823;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00725v1 Announce Type: cross  Abstract: It is a common belief that large language models (LLMs) are better than smaller-sized ones. However, larger models also require significantly more time and compute during inference. This begs the question: what happens when both models operate under the same budget? (e.g., compute, run-time). To address this question, we analyze code generation LLMs of various sizes and make comparisons such as running a 70B model once vs. generating five outputs from a 13B model and selecting one. Our findings reveal that, in a standard unit-test setup, the repeated use of smaller models can yield consistent improvements, with gains of up to 15% across five tasks. On the other hand, in scenarios where unit-tests are unavailable, a ranking-based selection of candidates from the smaller model falls short of the performance of a single output from larger ones. Our results highlight the potential of using smaller models instead of larger ones, and the imp
&lt;/p&gt;</description></item><item><title>LLM&#21463;&#21040;&#27745;&#26579;&#21487;&#33021;&#23548;&#33268;&#20854;&#24615;&#33021;&#19981;&#21487;&#38752;&#65292;&#25361;&#25112;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#25972;&#20307;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2404.00699</link><description>&lt;p&gt;
LLM&#21463;&#21040;&#22810;&#23569;&#27745;&#26579;&#65311;&#19968;&#39033;&#20840;&#38754;&#35843;&#26597;&#21644;LLMSanitize&#24211;
&lt;/p&gt;
&lt;p&gt;
How Much are LLMs Contaminated? A Comprehensive Survey and the LLMSanitize Library
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00699
&lt;/p&gt;
&lt;p&gt;
LLM&#21463;&#21040;&#27745;&#26579;&#21487;&#33021;&#23548;&#33268;&#20854;&#24615;&#33021;&#19981;&#21487;&#38752;&#65292;&#25361;&#25112;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#25972;&#20307;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36817;&#24180;&#26469;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23835;&#36215;&#65292;&#26032;&#30340;&#26426;&#20250;&#27491;&#22312;&#20986;&#29616;&#65292;&#20294;&#20063;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#27745;&#26579;&#38382;&#39064;&#36805;&#36895;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20225;&#19994;&#24212;&#29992;&#21644;&#20154;&#24037;&#26234;&#33021;&#31609;&#27454;&#24050;&#32463;&#36798;&#21040;&#19968;&#23450;&#35268;&#27169;&#65292;&#27969;&#34892;&#30340;&#38382;&#31572;&#22522;&#20934;&#25552;&#39640;&#20960;&#20010;&#30334;&#20998;&#28857;&#21487;&#33021;&#24847;&#21619;&#30528;&#25968;&#30334;&#19975;&#32654;&#20803;&#65292;&#23545;&#27169;&#22411;&#30340;&#23436;&#25972;&#24615;&#26045;&#21152;&#20102;&#24040;&#22823;&#21387;&#21147;&#12290;&#21516;&#26102;&#65292;&#36861;&#36394;LLMs&#35265;&#36807;&#30340;&#25968;&#25454;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#65307;&#23545;&#20110;&#20687;GPT-4&#21644;Claude-3&#36825;&#26679;&#30340;&#38381;&#28304;&#27169;&#22411;&#65292;&#20182;&#20204;&#19981;&#36879;&#38706;&#20219;&#20309;&#26377;&#20851;&#35757;&#32451;&#38598;&#30340;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#27745;&#26579;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;LLMs&#30340;&#24615;&#33021;&#21487;&#33021;&#19981;&#20877;&#21487;&#38752;&#65292;&#22240;&#20026;&#20854;&#39640;&#24615;&#33021;&#33267;&#23569;&#37096;&#20998;&#24402;&#22240;&#20110;&#20854;&#20808;&#21069;&#25509;&#35302;&#21040;&#30340;&#25968;&#25454;&#12290;&#36825;&#31181;&#23616;&#38480;&#24615;&#21361;&#21450;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#25972;&#20307;&#36827;&#23637;&#65292;&#28982;&#32780;&#65292;&#22914;&#20309;&#26377;&#25928;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#20173;&#28982;&#32570;&#20047;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00699v1 Announce Type: new  Abstract: With the rise of Large Language Models (LLMs) in recent years, new opportunities are emerging, but also new challenges, and contamination is quickly becoming critical. Business applications and fundraising in AI have reached a scale at which a few percentage points gained on popular question-answering benchmarks could translate into dozens of millions of dollars, placing high pressure on model integrity. At the same time, it is becoming harder and harder to keep track of the data that LLMs have seen; if not impossible with closed-source models like GPT-4 and Claude-3 not divulging any information on the training set. As a result, contamination becomes a critical issue: LLMs' performance may not be reliable anymore, as the high performance may be at least partly due to their previous exposure to the data. This limitation jeopardizes the entire progress in the field of NLP, yet, there remains a lack of methods on how to efficiently address
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#23610;&#24230;&#29305;&#24615;&#65292;&#21487;&#20197;&#39044;&#27979;&#20854;&#35821;&#35328;&#24615;&#33021;&#30340;&#25193;&#23637;&#36895;&#24230;&#65292;&#19982;&#25991;&#26412;-based&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#27604;&#65292;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#24615;&#33021;&#25193;&#23637;&#36895;&#24230;&#24930;&#19977;&#20010;&#25968;&#37327;&#32423;&#12290;</title><link>https://arxiv.org/abs/2404.00685</link><description>&lt;p&gt;
&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#23610;&#24230;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
Scaling Properties of Speech Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00685
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#23610;&#24230;&#29305;&#24615;&#65292;&#21487;&#20197;&#39044;&#27979;&#20854;&#35821;&#35328;&#24615;&#33021;&#30340;&#25193;&#23637;&#36895;&#24230;&#65292;&#19982;&#25991;&#26412;-based&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#27604;&#65292;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#24615;&#33021;&#25193;&#23637;&#36895;&#24230;&#24930;&#19977;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#65288;SLM&#65289;&#26088;&#22312;&#20174;&#21407;&#22987;&#38899;&#39057;&#20013;&#23398;&#20064;&#35821;&#35328;&#65292;&#32780;&#26080;&#38656;&#25991;&#26412;&#36164;&#28304;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#25105;&#20204;&#24403;&#21069;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#24369;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#30340;&#23610;&#24230;&#29305;&#24615;&#23545;&#35821;&#38899;&#27169;&#24577;&#25104;&#31435;&#65292;&#36825;&#20123;&#33021;&#21147;&#23558;&#38543;&#30528;&#35757;&#32451;&#25152;&#20351;&#29992;&#30340;&#35745;&#31639;&#37327;&#22686;&#21152;&#32780;&#25552;&#39640;&#12290;&#26412;&#25991;&#21033;&#29992;&#27169;&#22411;&#30340;&#23610;&#24230;&#34892;&#20026;&#26469;&#20272;&#35745;&#25105;&#20204;&#24403;&#21069;&#26041;&#27861;&#23558;&#20135;&#29983;&#20855;&#26377;&#25991;&#26412;-based&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33521;&#35821;&#29087;&#32451;&#24230;&#30340;SLM&#30340;&#23610;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00685v1 Announce Type: cross  Abstract: Speech Language Models (SLMs) aim to learn language from raw audio, without textual resources. Despite significant advances, our current models exhibit weak syntax and semantic abilities. However, if the scaling properties of neural language models hold for the speech modality, these abilities will improve as the amount of compute used for training increases. In this paper, we use models of this scaling behavior to estimate the scale at which our current methods will yield a SLM with the English proficiency of text-based Large Language Models (LLMs). We establish a strong correlation between pre-training loss and downstream syntactic and semantic performance in SLMs and LLMs, which results in predictable scaling of linguistic performance. We show that the linguistic performance of SLMs scales up to three orders of magnitude more slowly than that of text-based LLMs. Additionally, we study the benefits of synthetic data designed to boost
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;CoUDA&#26694;&#26550;&#65292;&#23558;&#35805;&#35821;&#36830;&#36143;&#24615;&#20998;&#35299;&#20026;&#20840;&#23616;&#21644;&#23616;&#37096;&#20004;&#20010;&#26041;&#38754;&#65292;&#24182;&#25552;&#20986;&#29992;&#20110;&#23616;&#37096;&#36830;&#36143;&#24615;&#30340;&#26032;&#39062;&#29983;&#25104;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2404.00681</link><description>&lt;p&gt;
CoUDA: &#36890;&#36807;&#32479;&#19968;&#25968;&#25454;&#22686;&#24378;&#30340;&#30456;&#24178;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
CoUDA: Coherence Evaluation via Unified Data Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00681
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;CoUDA&#26694;&#26550;&#65292;&#23558;&#35805;&#35821;&#36830;&#36143;&#24615;&#20998;&#35299;&#20026;&#20840;&#23616;&#21644;&#23616;&#37096;&#20004;&#20010;&#26041;&#38754;&#65292;&#24182;&#25552;&#20986;&#29992;&#20110;&#23616;&#37096;&#36830;&#36143;&#24615;&#30340;&#26032;&#39062;&#29983;&#25104;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#24178;&#24615;&#35780;&#20272;&#26088;&#22312;&#35780;&#20272;&#35805;&#35821;&#30340;&#32452;&#32455;&#21644;&#32467;&#26500;&#65292;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#30001;&#20110;&#26631;&#27880;&#25968;&#25454;&#30340;&#31232;&#32570;&#65292;&#25968;&#25454;&#22686;&#24378;&#36890;&#24120;&#29992;&#20110;&#35757;&#32451;&#30456;&#24178;&#24615;&#35780;&#20272;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#38024;&#23545;&#27492;&#20219;&#21153;&#30340;&#22686;&#24378;&#20027;&#35201;&#20381;&#36182;&#20110;&#21551;&#21457;&#24335;&#35268;&#21017;&#65292;&#32570;&#20047;&#35774;&#35745;&#20934;&#21017;&#20316;&#20026;&#25351;&#23548;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#35805;&#35821;&#32467;&#26500;&#30340;&#35821;&#35328;&#23398;&#29702;&#35770;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CoUDA&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#12290;CoUDA&#23558;&#35805;&#35821;&#36830;&#36143;&#24615;&#20998;&#35299;&#20026;&#20840;&#23616;&#21644;&#23616;&#37096;&#20004;&#20010;&#26041;&#38754;&#65292;&#24182;&#20998;&#21035;&#20026;&#20004;&#20010;&#26041;&#38754;&#35774;&#35745;&#22686;&#24378;&#31574;&#30053;&#12290;&#29305;&#21035;&#26159;&#38024;&#23545;&#23616;&#37096;&#36830;&#36143;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#31574;&#30053;&#29992;&#20110;&#26500;&#24314;&#22686;&#24378;&#26679;&#26412;&#65292;&#20854;&#20013;&#28041;&#21450;&#23545;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#20107;&#21518;&#39044;&#35757;&#32451;&#24182;&#24212;&#29992;&#20004;&#20010;&#25511;&#21046;&#26426;&#21046;&#26469;&#25511;&#21046;&#29983;&#25104;&#26679;&#26412;&#30340;&#38590;&#24230;&#12290;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;CoUDA&#36824;&#32852;&#21512;eva
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00681v1 Announce Type: new  Abstract: Coherence evaluation aims to assess the organization and structure of a discourse, which remains challenging even in the era of large language models. Due to the scarcity of annotated data, data augmentation is commonly used for training coherence evaluation models. However, previous augmentations for this task primarily rely on heuristic rules, lacking designing criteria as guidance. In this paper, we take inspiration from linguistic theory of discourse structure, and propose a data augmentation framework named CoUDA. CoUDA breaks down discourse coherence into global and local aspects, and designs augmentation strategies for both aspects, respectively. Especially for local coherence, we propose a novel generative strategy for constructing augmentation samples, which involves post-pretraining a generative model and applying two controlling mechanisms to control the difficulty of generated samples. During inference, CoUDA also jointly eva
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ToE&#30340;&#20196;&#29260;&#29983;&#38271;&#26041;&#26696;&#65292;&#26088;&#22312;&#36890;&#36807;&#21152;&#36895;&#19968;&#33268;&#24615;&#35757;&#32451;&#26469;&#25913;&#21892;ViT&#30340;&#35757;&#32451;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2404.00672</link><description>&lt;p&gt;
&#36890;&#36807;&#20196;&#29260;&#25193;&#23637;&#23454;&#29616;Transformer&#30340;&#19968;&#33324;&#39640;&#25928;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
A General and Efficient Training for Transformer via Token Expansion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ToE&#30340;&#20196;&#29260;&#29983;&#38271;&#26041;&#26696;&#65292;&#26088;&#22312;&#36890;&#36807;&#21152;&#36895;&#19968;&#33268;&#24615;&#35757;&#32451;&#26469;&#25913;&#21892;ViT&#30340;&#35757;&#32451;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Vision Transformer&#65288;ViT&#65289;&#36890;&#24120;&#38656;&#35201;&#26497;&#22823;&#30340;&#35757;&#32451;&#25104;&#26412;&#25165;&#33021;&#21462;&#24471;&#26174;&#33879;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#20196;&#29260;&#29983;&#38271;&#26041;&#26696;Token Expansion (ToE)&#65292;&#20197;&#23454;&#29616;ViT&#30340;&#19968;&#33268;&#24615;&#35757;&#32451;&#21152;&#36895;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#8220;&#21021;&#22987;&#21270;-&#25193;&#23637;-&#21512;&#24182;&#8221;&#31649;&#36947;&#65292;&#20197;&#20445;&#25345;&#21407;&#22987;Transformer&#30340;&#20013;&#38388;&#29305;&#24449;&#20998;&#24067;&#30340;&#23436;&#25972;&#24615;&#65292;&#38450;&#27490;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20002;&#22833;&#20851;&#38190;&#30340;&#21487;&#23398;&#20064;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00672v1 Announce Type: cross  Abstract: The remarkable performance of Vision Transformers (ViTs) typically requires an extremely large training cost. Existing methods have attempted to accelerate the training of ViTs, yet typically disregard method universality with accuracy dropping. Meanwhile, they break the training consistency of the original transformers, including the consistency of hyper-parameters, architecture, and strategy, which prevents them from being widely applied to different Transformer networks. In this paper, we propose a novel token growth scheme Token Expansion (termed ToE) to achieve consistent training acceleration for ViTs. We introduce an "initialization-expansion-merging" pipeline to maintain the integrity of the intermediate feature distribution of original transformers, preventing the loss of crucial learnable information in the training process. ToE can not only be seamlessly integrated into the training and fine-tuning process of transformers (e
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#22238;&#39038;&#20102;&#23545;&#25216;&#26415;&#25991;&#26723;RAG&#31995;&#32479;&#26500;&#24314;&#30340;&#37325;&#35201;&#22240;&#32032;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#26368;&#20339;&#23454;&#36341;&#21644;&#28508;&#22312;&#25361;&#25112;</title><link>https://arxiv.org/abs/2404.00657</link><description>&lt;p&gt;
&#20851;&#20110;&#20026;&#25216;&#26415;&#25991;&#26723;&#26500;&#24314;RAG&#31995;&#32479;&#30340;&#35266;&#23519;
&lt;/p&gt;
&lt;p&gt;
Observations on Building RAG Systems for Technical Documents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00657
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#22238;&#39038;&#20102;&#23545;&#25216;&#26415;&#25991;&#26723;RAG&#31995;&#32479;&#26500;&#24314;&#30340;&#37325;&#35201;&#22240;&#32032;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#26368;&#20339;&#23454;&#36341;&#21644;&#28508;&#22312;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
RAG&#65288;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65289;&#29992;&#20110;&#25216;&#26415;&#25991;&#26723;&#26102;&#23384;&#22312;&#25361;&#25112;&#65292;&#22240;&#20026;&#23884;&#20837;&#36890;&#24120;&#26080;&#27861;&#25429;&#25417;&#39046;&#22495;&#20449;&#24687;&#12290;&#25105;&#20204;&#22238;&#39038;&#20102;&#24433;&#21709;RAG&#30340;&#37325;&#35201;&#22240;&#32032;&#30340;&#20808;&#21069;&#30740;&#31350;&#65292;&#24182;&#36827;&#34892;&#23454;&#39564;&#20197;&#31361;&#20986;&#26500;&#24314;&#25216;&#26415;&#25991;&#26723;RAG&#31995;&#32479;&#30340;&#26368;&#20339;&#23454;&#36341;&#21644;&#28508;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00657v1 Announce Type: cross  Abstract: Retrieval augmented generation (RAG) for technical documents creates challenges as embeddings do not often capture domain information. We review prior art for important factors affecting RAG and perform experiments to highlight best practices and potential challenges to build RAG systems for technical documents.
&lt;/p&gt;</description></item><item><title>WavLLM&#26159;&#19968;&#20010;&#31283;&#20581;&#21644;&#33258;&#36866;&#24212;&#35821;&#38899;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#21452;&#32534;&#30721;&#22120;&#21644;Prompt-aware LoRA&#26435;&#37325;&#36866;&#37197;&#22120;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#20248;&#21270;&#65292;&#35299;&#32806;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#38899;&#20449;&#24687;&#65292;&#20026;&#22788;&#29702;&#35821;&#20041;&#20869;&#23481;&#21644;&#35828;&#35805;&#32773;&#36523;&#20221;&#30340;&#29420;&#29305;&#29305;&#24449;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;</title><link>https://arxiv.org/abs/2404.00656</link><description>&lt;p&gt;
WavLLM&#65306;&#38754;&#21521;&#31283;&#20581;&#21644;&#33258;&#36866;&#24212;&#35821;&#38899;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
WavLLM: Towards Robust and Adaptive Speech Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00656
&lt;/p&gt;
&lt;p&gt;
WavLLM&#26159;&#19968;&#20010;&#31283;&#20581;&#21644;&#33258;&#36866;&#24212;&#35821;&#38899;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#21452;&#32534;&#30721;&#22120;&#21644;Prompt-aware LoRA&#26435;&#37325;&#36866;&#37197;&#22120;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#20248;&#21270;&#65292;&#35299;&#32806;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#38899;&#20449;&#24687;&#65292;&#20026;&#22788;&#29702;&#35821;&#20041;&#20869;&#23481;&#21644;&#35828;&#35805;&#32773;&#36523;&#20221;&#30340;&#29420;&#29305;&#29305;&#24449;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#26368;&#26032;&#36827;&#23637;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#36880;&#28176;&#25299;&#23485;&#20102;&#23427;&#20204;&#30340;&#33539;&#22260;&#21040;&#22810;&#27169;&#24577;&#24863;&#30693;&#21644;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#26377;&#25928;&#22320;&#23558;&#21548;&#35273;&#33021;&#21147;&#25972;&#21512;&#21040;LLMs&#20013;&#20250;&#24102;&#26469;&#26174;&#33879;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#27867;&#21270;&#36328;&#19981;&#21516;&#35821;&#22659;&#21644;&#25191;&#34892;&#22797;&#26434;&#21548;&#35273;&#20219;&#21153;&#26041;&#38754;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;WavLLM&#65292;&#19968;&#20010;&#20855;&#26377;&#21452;&#32534;&#30721;&#22120;&#21644;Prompt-aware LoRA&#26435;&#37325;&#36866;&#37197;&#22120;&#30340;&#31283;&#20581;&#21644;&#33258;&#36866;&#24212;&#35821;&#38899;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;&#21033;&#29992;&#21452;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#35299;&#32806;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#38899;&#20449;&#24687;&#65292;&#21033;&#29992;Whisper&#32534;&#30721;&#22120;&#22788;&#29702;&#35821;&#38899;&#30340;&#35821;&#20041;&#20869;&#23481;&#65292;&#21033;&#29992;WavLM&#32534;&#30721;&#22120;&#25429;&#25417;&#35828;&#35805;&#32773;&#36523;&#20221;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#22312;&#35838;&#31243;&#23398;&#20064;&#26694;&#26550;&#20869;&#65292;WavLLM&#39318;&#20808;&#36890;&#36807;&#28151;&#21512;&#35201;&#32032;&#36827;&#34892;&#20248;&#21270;&#26469;&#24314;&#31435;&#20854;&#22522;&#30784;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00656v1 Announce Type: cross  Abstract: The recent advancements in large language models (LLMs) have revolutionized the field of natural language processing, progressively broadening their scope to multimodal perception and generation. However, effectively integrating listening capabilities into LLMs poses significant challenges, particularly with respect to generalizing across varied contexts and executing complex auditory tasks. In this work, we introduce WavLLM, a robust and adaptive speech large language model with dual encoders, and a prompt-aware LoRA weight adapter, optimized by a two-stage curriculum learning approach. Leveraging dual encoders, we decouple different types of speech information, utilizing a Whisper encoder to process the semantic content of speech, and a WavLM encoder to capture the unique characteristics of the speaker's identity. Within the curriculum learning framework, WavLLM first builds its foundational capabilities by optimizing on mixed elemen
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#29983;&#25104;&#27169;&#22411;&#30340;&#32418;&#38431;&#27979;&#35797;&#36827;&#34892;&#20102;&#24191;&#27867;&#35843;&#26597;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;&#32454;&#31890;&#24230;&#25915;&#20987;&#31574;&#30053;&#20998;&#31867;&#20307;&#31995;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#21508;&#31181;&#33258;&#21160;&#32418;&#38431;&#27979;&#35797;&#26041;&#27861;&#30340;&#25628;&#32034;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2404.00629</link><description>&lt;p&gt;
&#38024;&#23545;&#38463;&#21888;&#29705;&#26031;&#20043;&#36405;&#65306;&#29983;&#25104;&#27169;&#22411;&#32418;&#38431;&#27979;&#35797;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Against The Achilles' Heel: A Survey on Red Teaming for Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00629
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#29983;&#25104;&#27169;&#22411;&#30340;&#32418;&#38431;&#27979;&#35797;&#36827;&#34892;&#20102;&#24191;&#27867;&#35843;&#26597;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;&#32454;&#31890;&#24230;&#25915;&#20987;&#31574;&#30053;&#20998;&#31867;&#20307;&#31995;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#21508;&#31181;&#33258;&#21160;&#32418;&#38431;&#27979;&#35797;&#26041;&#27861;&#30340;&#25628;&#32034;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#27491;&#36805;&#36895;&#26222;&#21450;&#24182;&#34987;&#25972;&#21512;&#21040;&#26085;&#24120;&#24212;&#29992;&#20013;&#65292;&#20294;&#30456;&#20851;&#30340;&#23433;&#20840;&#38382;&#39064;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#25285;&#24551;&#65292;&#22240;&#20026;&#21508;&#31181;&#28431;&#27934;&#19981;&#26029;&#26292;&#38706;&#12290;&#38754;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#32418;&#38431;&#27979;&#35797;&#39046;&#22495;&#27491;&#22312;&#24555;&#36895;&#22686;&#38271;&#65292;&#24378;&#35843;&#20102;&#23545;&#25972;&#20010;&#27969;&#31243;&#36827;&#34892;&#20840;&#38754;&#32452;&#32455;&#24182;&#35299;&#20915;&#31038;&#21306;&#26032;&#20852;&#20027;&#39064;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#35843;&#26597;&#28085;&#30422;&#20102;120&#22810;&#31687;&#35770;&#25991;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#22266;&#26377;&#33021;&#21147;&#30340;&#32454;&#31890;&#24230;&#25915;&#20987;&#31574;&#30053;&#20998;&#31867;&#20307;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#21508;&#31181;&#33258;&#21160;&#32418;&#38431;&#27979;&#35797;&#26041;&#27861;&#30340;&#25628;&#32034;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#35843;&#26597;&#28085;&#30422;&#20102;&#26032;&#39046;&#22495;&#65292;&#21253;&#25324;&#22810;&#27169;&#24335;&#25915;&#20987;&#21644;&#38450;&#24481;&#12289;&#22810;&#35821;&#35328;&#27169;&#22411;&#39118;&#38505;&#12289;&#26080;&#23475;&#26597;&#35810;&#30340;&#36807;&#24230;&#20351;&#29992;&#20197;&#21450;&#19979;&#28216;&#24212;&#29992;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00629v1 Announce Type: new  Abstract: Generative models are rapidly gaining popularity and being integrated into everyday applications, raising concerns over their safety issues as various vulnerabilities are exposed. Faced with the problem, the field of red teaming is experiencing fast-paced growth, which highlights the need for a comprehensive organization covering the entire pipeline and addressing emerging topics for the community. Our extensive survey, which examines over 120 papers, introduces a taxonomy of fine-grained attack strategies grounded in the inherent capabilities of language models. Additionally, we have developed the searcher framework that unifies various automatic red teaming approaches. Moreover, our survey covers novel areas including multimodal attacks and defenses, risks around multilingual models, overkill of harmless queries, and safety of downstream applications. We hope this survey can provide a systematic perspective on the field and unlock new 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30524;&#21160;&#25968;&#25454;&#20849;&#20139;&#26041;&#27861;&#65292;&#20027;&#24352;&#20844;&#24067;&#30524;&#21160;&#25968;&#25454;&#30340;&#25152;&#26377;&#39044;&#22788;&#29702;&#38454;&#27573;&#20197;&#21450;&#38468;&#24102;&#25968;&#25454;&#36136;&#37327;&#25253;&#21578;&#65292;&#20174;&#32780;&#22686;&#21152;&#29616;&#26377;&#30524;&#21160;&#25968;&#25454;&#38598;&#30340;&#21487;&#37325;&#22797;&#20351;&#29992;&#24615;&#21644;&#23454;&#29616;&#36328;&#25968;&#25454;&#38598;&#27604;&#36739;&#12290;</title><link>https://arxiv.org/abs/2404.00620</link><description>&lt;p&gt;
&#27719;&#25253;&#30524;&#21160;&#25968;&#25454;&#36136;&#37327;&#65306;&#36208;&#21521;&#26032;&#30340;&#26631;&#20934;
&lt;/p&gt;
&lt;p&gt;
Reporting Eye-Tracking Data Quality: Towards a New Standard
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00620
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30524;&#21160;&#25968;&#25454;&#20849;&#20139;&#26041;&#27861;&#65292;&#20027;&#24352;&#20844;&#24067;&#30524;&#21160;&#25968;&#25454;&#30340;&#25152;&#26377;&#39044;&#22788;&#29702;&#38454;&#27573;&#20197;&#21450;&#38468;&#24102;&#25968;&#25454;&#36136;&#37327;&#25253;&#21578;&#65292;&#20174;&#32780;&#22686;&#21152;&#29616;&#26377;&#30524;&#21160;&#25968;&#25454;&#38598;&#30340;&#21487;&#37325;&#22797;&#20351;&#29992;&#24615;&#21644;&#23454;&#29616;&#36328;&#25968;&#25454;&#38598;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Eye-tracking datasets&#36890;&#24120;&#20197;&#23427;&#20204;&#30340;&#21019;&#24314;&#32773;&#29992;&#20110;&#21407;&#22987;&#20998;&#26512;&#30340;&#26684;&#24335;&#20849;&#20139;&#65292;&#36890;&#24120;&#20250;&#23548;&#33268;&#25490;&#38500;&#34987;&#35748;&#20026;&#19982;&#20027;&#35201;&#30446;&#30340;&#26080;&#20851;&#30340;&#25968;&#25454;&#12290;&#20026;&#20102;&#22686;&#21152;&#29616;&#26377;&#30524;&#21160;&#25968;&#25454;&#38598;&#30340;&#21487;&#37325;&#22797;&#20351;&#29992;&#24615;&#65292;&#20197;&#28385;&#36275;&#26356;&#22810;&#26679;&#21270;&#21644;&#26368;&#21021;&#19981;&#34987;&#32771;&#34385;&#30340;&#29992;&#20363;&#65292;&#26412;&#25991;&#20027;&#24352;&#19968;&#31181;&#26032;&#30340;&#30524;&#21160;&#25968;&#25454;&#20849;&#20139;&#26041;&#27861;&#12290;&#19982;&#20854;&#21457;&#24067;&#32463;&#36807;&#28388;&#21644;&#39044;&#22788;&#29702;&#30340;&#25968;&#25454;&#38598;&#65292;&#24212;&#35813;&#20844;&#24067;&#25152;&#26377;&#39044;&#22788;&#29702;&#38454;&#27573;&#30340;&#30524;&#21160;&#25968;&#25454;&#65292;&#24182;&#38468;&#24102;&#25968;&#25454;&#36136;&#37327;&#25253;&#21578;&#12290;&#20026;&#20102;&#36879;&#26126;&#22320;&#25253;&#21578;&#25968;&#25454;&#36136;&#37327;&#24182;&#23454;&#29616;&#36328;&#25968;&#25454;&#38598;&#27604;&#36739;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#25968;&#25454;&#36136;&#37327;&#25253;&#21578;&#26631;&#20934;&#21644;&#24230;&#37327;&#26631;&#20934;&#65292;&#21487;&#20197;&#33258;&#21160;&#24212;&#29992;&#20110;&#25968;&#25454;&#38598;&#65292;&#24182;&#23558;&#20854;&#38598;&#25104;&#21040;&#24320;&#28304;Python&#36719;&#20214;&#21253;pymovements&#20013;&#65288;https://github.com/aeye-lab/pymovements&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00620v1 Announce Type: new  Abstract: Eye-tracking datasets are often shared in the format used by their creators for their original analyses, usually resulting in the exclusion of data considered irrelevant to the primary purpose. In order to increase re-usability of existing eye-tracking datasets for more diverse and initially not considered use cases, this work advocates a new approach of sharing eye-tracking data. Instead of publishing filtered and pre-processed datasets, the eye-tracking data at all pre-processing stages should be published together with data quality reports. In order to transparently report data quality and enable cross-dataset comparisons, we develop data quality reporting standards and metrics that can be automatically applied to a dataset, and integrate them into the open-source Python package pymovements (https://github.com/aeye-lab/pymovements).
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#35757;&#32451;&#19968;&#20010;&#29992;&#20110;&#35268;&#21010;&#26410;&#26469;&#20889;&#20316;&#36807;&#31243;&#30340;&#27169;&#22359;&#65292;&#25193;&#23637;&#20102;&#25104;&#21151;&#30340;&#35821;&#35328;&#27169;&#22411;&#20844;&#24335;&#21040;&#26356;&#25277;&#35937;&#30340;&#35268;&#21010;&#20013;&#65292;&#25913;&#21892;&#20102;&#35821;&#35328;&#24314;&#27169;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#25991;&#26412;&#32467;&#26500;&#26041;&#38754;&#65292;&#21516;&#26102;&#26032;&#30340;&#35268;&#21010;&#27169;&#22359;&#21487;&#20197;&#22823;&#35268;&#27169;&#35757;&#32451;&#24182;&#36731;&#26494;&#19982;&#31038;&#21306;&#20849;&#20139;&#12290;</title><link>https://arxiv.org/abs/2404.00614</link><description>&lt;p&gt;
&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#23398;&#20064;&#35821;&#35328;&#24314;&#27169;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Learning to Plan for Language Modeling from Unlabeled Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00614
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#35757;&#32451;&#19968;&#20010;&#29992;&#20110;&#35268;&#21010;&#26410;&#26469;&#20889;&#20316;&#36807;&#31243;&#30340;&#27169;&#22359;&#65292;&#25193;&#23637;&#20102;&#25104;&#21151;&#30340;&#35821;&#35328;&#27169;&#22411;&#20844;&#24335;&#21040;&#26356;&#25277;&#35937;&#30340;&#35268;&#21010;&#20013;&#65292;&#25913;&#21892;&#20102;&#35821;&#35328;&#24314;&#27169;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#25991;&#26412;&#32467;&#26500;&#26041;&#38754;&#65292;&#21516;&#26102;&#26032;&#30340;&#35268;&#21010;&#27169;&#22359;&#21487;&#20197;&#22823;&#35268;&#27169;&#35757;&#32451;&#24182;&#36731;&#26494;&#19982;&#31038;&#21306;&#20849;&#20139;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35757;&#32451;&#26469;&#39044;&#27979;&#26410;&#26631;&#35760;&#35821;&#26009;&#24211;&#20013;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23398;&#20250;&#25191;&#34892;&#35768;&#22810;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#26631;&#35760;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30446;&#26631;&#21487;&#20197;&#35828;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#38656;&#35201;&#35268;&#21010;&#30340;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#65292;&#27604;&#22914;&#20889;&#20316;&#19968;&#31687;&#36830;&#36143;&#30340;&#25991;&#31456;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#35757;&#32451;&#19968;&#20010;&#29992;&#20110;&#35268;&#21010;&#26410;&#26469;&#20889;&#20316;&#36807;&#31243;&#30340;&#27169;&#22359;&#12290;&#36890;&#36807;&#26681;&#25454;&#29983;&#25104;&#30340;&#28508;&#22312;&#35745;&#21010;&#36827;&#34892;&#26465;&#20214;&#21270;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#23558;&#25104;&#21151;&#30340;&#35821;&#35328;&#27169;&#22411;&#20844;&#24335;&#25193;&#23637;&#21040;&#26356;&#25277;&#35937;&#30340;&#35268;&#21010;&#20013;&#12290;&#23454;&#39564;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#25913;&#21892;&#20102;&#35821;&#35328;&#24314;&#27169;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#25991;&#26412;&#32467;&#26500;&#26041;&#38754;&#12290;&#30001;&#20110;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#29992;&#30340;&#26159;&#26080;&#30417;&#30563;&#19988;&#22806;&#37096;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#35268;&#21010;&#27169;&#22359;&#65292;&#22240;&#27492;&#26032;&#30340;&#35268;&#21010;&#27169;&#22359;&#21487;&#20197;&#22823;&#35268;&#27169;&#35757;&#32451;&#65292;&#24182;&#19988;&#33021;&#22815;&#36731;&#26494;&#22320;&#19982;&#31038;&#21306;&#20849;&#20139;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00614v1 Announce Type: cross  Abstract: By training to predict the next token in an unlabeled corpus, large language models learn to perform many tasks without any labeled data. However, their next-token-prediction objective arguably limits their performance in scenarios that require planning, such as writing a coherent article. In this paper, we train a module for planning the future writing process via a self-supervised learning objective. By conditioning on generated latent plans, our model extends the successful language model formula to more abstract planning in an unsupervised way. Empirically, we demonstrate that our method improves language modeling performance in general, particularly with respect to the text structure. Because our framework uses a planner module that is unsupervised and external to the language model, new planner modules can be trained at large scale and easily be shared with the community.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;RQ-RAG&#65292;&#26088;&#22312;&#20026;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#27169;&#22411;&#22686;&#21152;&#32454;&#21270;&#26597;&#35810;&#30340;&#33021;&#21147;&#65292;&#20197;&#20415;&#38024;&#23545;&#27169;&#31946;&#25110;&#22797;&#26434;&#26597;&#35810;&#36827;&#19968;&#27493;&#28548;&#28165;&#25110;&#20998;&#35299;&#65292;&#20174;&#32780;&#25552;&#39640;&#29983;&#25104;&#20934;&#30830;&#24230;&#12290;</title><link>https://arxiv.org/abs/2404.00610</link><description>&lt;p&gt;
RQ-RAG: &#23398;&#20064;&#20026;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#32454;&#21270;&#26597;&#35810;
&lt;/p&gt;
&lt;p&gt;
RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;RQ-RAG&#65292;&#26088;&#22312;&#20026;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#27169;&#22411;&#22686;&#21152;&#32454;&#21270;&#26597;&#35810;&#30340;&#33021;&#21147;&#65292;&#20197;&#20415;&#38024;&#23545;&#27169;&#31946;&#25110;&#22797;&#26434;&#26597;&#35810;&#36827;&#19968;&#27493;&#28548;&#28165;&#25110;&#20998;&#35299;&#65292;&#20174;&#32780;&#25552;&#39640;&#29983;&#25104;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#20294;&#23481;&#26131;&#29983;&#25104;&#19981;&#20934;&#30830;&#25110;&#24187;&#35273;&#24615;&#30340;&#21709;&#24212;&#12290;Retrieval-Augmented Generation (RAG)&#36890;&#36807;&#23558;&#22806;&#37096;&#30456;&#20851;&#25991;&#26723;&#32435;&#20837;&#21709;&#24212;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#20174;&#32780;&#21033;&#29992;&#38750;&#21442;&#25968;&#21270;&#30693;&#35782;&#21644;LLMs&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;RAG&#23454;&#29616;&#20027;&#35201;&#20851;&#27880;&#20110;&#19978;&#19979;&#25991;&#26816;&#32034;&#30340;&#21021;&#22987;&#36755;&#20837;&#65292;&#24573;&#35270;&#20102;&#38656;&#35201;&#36827;&#19968;&#27493;&#28548;&#28165;&#25110;&#20998;&#35299;&#20197;&#33719;&#24471;&#20934;&#30830;&#21709;&#24212;&#30340;&#27169;&#31946;&#25110;&#22797;&#26434;&#26597;&#35810;&#30340;&#32454;&#24494;&#24046;&#21035;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#23398;&#20064;&#20026;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#32454;&#21270;&#26597;&#35810;(RQ-RAG)&#65292;&#33268;&#21147;&#20110;&#36890;&#36807;&#36171;&#20104;&#27169;&#22411;&#26174;&#24335;&#37325;&#20889;&#12289;&#20998;&#35299;&#33021;&#21147;&#26469;&#22686;&#24378;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00610v1 Announce Type: new  Abstract: Large Language Models (LLMs) exhibit remarkable capabilities but are prone to generating inaccurate or hallucinatory responses. This limitation stems from their reliance on vast pretraining datasets, making them susceptible to errors in unseen scenarios. To tackle these challenges, Retrieval-Augmented Generation (RAG) addresses this by incorporating external, relevant documents into the response generation process, thus leveraging non-parametric knowledge alongside LLMs' in-context learning abilities. However, existing RAG implementations primarily focus on initial input for context retrieval, overlooking the nuances of ambiguous or complex queries that necessitate further clarification or decomposition for accurate responses. To this end, we propose learning to Refine Query for Retrieval Augmented Generation (RQ-RAG) in this paper, endeavoring to enhance the model by equipping it with capabilities for explicit rewriting, decomposition, 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#24191;&#27867;&#33258;&#23545;&#27604;&#29983;&#25104;&#36127;&#20363;&#30340;&#26080;&#38656;&#21453;&#39304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.00604</link><description>&lt;p&gt;
&#24191;&#27867;&#30340;&#33258;&#23545;&#27604;&#20351;&#24471;&#26080;&#38656;&#21453;&#39304;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#25104;&#20026;&#21487;&#33021;
&lt;/p&gt;
&lt;p&gt;
Extensive Self-Contrast Enables Feedback-Free Language Model Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#24191;&#27867;&#33258;&#23545;&#27604;&#29983;&#25104;&#36127;&#20363;&#30340;&#26080;&#38656;&#21453;&#39304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#19968;&#30452;&#26159;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23545;&#40784;&#30340;&#26680;&#24515;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#20854;&#20005;&#37325;&#20381;&#36182;&#26114;&#36149;&#30340;&#20154;&#31867;&#25110;LLM&#20316;&#20026;&#35780;&#21028;&#32773;&#30340;&#20559;&#22909;&#21453;&#39304;&#21487;&#33021;&#20250;&#38459;&#30861;&#20854;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Self-Contrast&#65292;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#24191;&#27867;&#33258;&#21160;&#29983;&#25104;&#30340;&#36127;&#20363;&#26469;&#36827;&#34892;&#26080;&#38656;&#21453;&#39304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#26041;&#27861;&#12290;&#20165;&#36890;&#36807;&#30417;&#30563;&#30340;&#24494;&#35843;&#65288;SFT&#65289;&#30446;&#26631;&#65292;Self-Contrast&#21033;&#29992;LLM&#26412;&#36523;&#29983;&#25104;&#22823;&#37327;&#22810;&#26679;&#30340;&#20505;&#36873;&#39033;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#23884;&#20837;&#27169;&#22411;&#26681;&#25454;&#25991;&#26412;&#30456;&#20284;&#24615;&#36807;&#28388;&#22810;&#20010;&#36127;&#20363;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#65292;&#20165;&#20165;&#25193;&#22823;&#36127;&#38754;&#22238;&#24212;&#20173;&#28982;&#21487;&#20197;&#26377;&#25928;&#22320;&#36817;&#20284;&#20855;&#26377;&#26356;&#24179;&#34913;&#30340;&#27491;&#38754;&#21644;&#36127;&#38754;&#20559;&#22909;&#27880;&#37322;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#23545;&#19977;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;Self-Contrast&#33021;&#22815;&#22987;&#32456;&#20248;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00604v1 Announce Type: cross  Abstract: Reinforcement learning from human feedback (RLHF) has been a central technique for recent large language model (LLM) alignment. However, its heavy dependence on costly human or LLM-as-Judge preference feedback could stymie its wider applications. In this work, we introduce Self-Contrast, a feedback-free large language model alignment method via exploiting extensive self-generated negatives. With only supervised fine-tuning (SFT) targets, Self-Contrast leverages the LLM itself to generate massive diverse candidates, and harnesses a pre-trained embedding model to filter multiple negatives according to text similarity. Theoretically, we illustrate that in this setting, merely scaling negative responses can still effectively approximate situations with more balanced positive and negative preference annotations. Our experiments with direct preference optimization (DPO) on three datasets show that, Self-Contrast could consistently outperform
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#35752;&#35770;&#20102;&#20154;&#31867;&#30417;&#30563;&#12289;&#36947;&#24503;&#30417;&#30563;&#21644;&#38544;&#31169;&#24433;&#21709;&#35780;&#20272;&#22312;&#38754;&#23545;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21457;&#23637;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.00600</link><description>&lt;p&gt;
AI&#27861;&#24459;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65306;&#24403;&#20851;&#38190;&#38382;&#39064;&#21644;&#38544;&#31169;&#24433;&#21709;&#38656;&#35201;&#20154;&#31867;&#21644;&#36947;&#24503;&#30417;&#30563;&#26102;
&lt;/p&gt;
&lt;p&gt;
AI Act and Large Language Models (LLMs): When critical issues and privacy impact require human and ethical oversight
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00600
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#35752;&#35770;&#20102;&#20154;&#31867;&#30417;&#30563;&#12289;&#36947;&#24503;&#30417;&#30563;&#21644;&#38544;&#31169;&#24433;&#21709;&#35780;&#20272;&#22312;&#38754;&#23545;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21457;&#23637;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#26085;&#30410;&#21457;&#23637;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#65292;&#20351;&#24471;&#26377;&#24517;&#35201;&#23545;&#23427;&#20204;&#22312;&#38544;&#31169;&#12289;&#20010;&#20154;&#25968;&#25454;&#20445;&#25252;&#20197;&#21450;&#36947;&#24503;&#23618;&#38754;&#65292;&#23588;&#20854;&#26159;&#23545;&#26368;&#33030;&#24369;&#21644;&#26368;&#24369;&#21183;&#32676;&#20307;&#21487;&#33021;&#20135;&#29983;&#30340;&#39118;&#38505;&#21644;&#24433;&#21709;&#36827;&#34892;&#35780;&#20272;&#12290;&#26412;&#25991;&#23545;&#20154;&#31867;&#30417;&#30563;&#12289;&#36947;&#24503;&#30417;&#30563;&#21644;&#38544;&#31169;&#24433;&#21709;&#35780;&#20272;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00600v1 Announce Type: cross  Abstract: The imposing evolution of artificial intelligence systems and, specifically, of Large Language Models (LLM) makes it necessary to carry out assessments of their level of risk and the impact they may have in the area of privacy, personal data protection and at an ethical level, especially on the weakest and most vulnerable. This contribution addresses human oversight, ethical oversight, and privacy impact assessment.
&lt;/p&gt;</description></item><item><title>EvoCodeBench &#26159;&#19968;&#20010;&#19982;&#29616;&#23454;&#19990;&#30028;&#20195;&#30721;&#24211;&#23545;&#40784;&#30340;&#36827;&#21270;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#65292;&#20855;&#26377;&#23436;&#21892;&#30340;&#27880;&#37322;&#21644;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#65292;&#21516;&#26102;&#36991;&#20813;&#25968;&#25454;&#27844;&#38706;&#12290;</title><link>https://arxiv.org/abs/2404.00599</link><description>&lt;p&gt;
EvoCodeBench: &#19968;&#20010;&#19982;&#29616;&#23454;&#19990;&#30028;&#20195;&#30721;&#24211;&#23545;&#40784;&#30340;&#36827;&#21270;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
EvoCodeBench: An Evolving Code Generation Benchmark Aligned with Real-World Code Repositories
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00599
&lt;/p&gt;
&lt;p&gt;
EvoCodeBench &#26159;&#19968;&#20010;&#19982;&#29616;&#23454;&#19990;&#30028;&#20195;&#30721;&#24211;&#23545;&#40784;&#30340;&#36827;&#21270;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#65292;&#20855;&#26377;&#23436;&#21892;&#30340;&#27880;&#37322;&#21644;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#65292;&#21516;&#26102;&#36991;&#20813;&#25968;&#25454;&#27844;&#38706;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00599v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#30028; &#25688;&#35201;: &#22914;&#20309;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#34920;&#29616;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#34920;&#29616;&#20986;&#19982;&#29616;&#23454;&#19990;&#30028;&#20195;&#30721;&#24211;&#30340;&#24046;&#36317;&#65292;&#19981;&#36275;&#20197;&#35780;&#20272;LLMs&#30340;&#32534;&#30721;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797; - EvoCodeBench&#26469;&#35299;&#20915;&#21069;&#36848;&#38382;&#39064;&#65292;&#20854;&#20855;&#26377;&#19977;&#20010;&#20027;&#35201;&#36827;&#23637;&#12290;(1) EvoCodeBench&#22312;&#22810;&#20010;&#32500;&#24230;&#19978;&#19982;&#29616;&#23454;&#19990;&#30028;&#24211;&#23545;&#40784;&#65292;&#20363;&#22914;&#20195;&#30721;&#20998;&#24067;&#21644;&#20381;&#36182;&#20998;&#24067;&#12290;(2) EvoCodeBench&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#27880;&#37322;(&#20363;&#22914;&#38656;&#27714;&#12289;&#21442;&#32771;&#20195;&#30721;&#21644;&#21442;&#32771;&#20381;&#36182;)&#65292;&#20197;&#21450;&#24378;&#22823;&#30340;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;(&#20363;&#22914;Pass@k&#21644;Recall@k)&#12290;(3) EvoCodeBench&#26159;&#19968;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#36991;&#20813;&#25968;&#25454;&#27844;&#28431;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#27969;&#27700;&#32447;&#65292;&#20174;&#26368;&#26032;&#30340;&#24211;&#20013;&#26356;&#26032;EvoCodeBench&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#31532;&#19968;&#20010;&#29256;&#26412; - EvoCodeBench-2403&#65292;&#20854;&#20013;&#21253;&#21547;&#26469;&#33258;25&#20010;&#29616;&#23454;&#19990;&#30028;&#24211;&#30340;275&#20010;&#26679;&#26412;&#12290;&#22522;&#20110;EvoCodeBench&#65292;&#25105;&#20204;&#25552;&#20986;repo
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00599v1 Announce Type: cross  Abstract: How to evaluate Large Language Models (LLMs) in code generation is an open question. Existing benchmarks demonstrate poor alignment with real-world code repositories and are insufficient to evaluate the coding abilities of LLMs. This paper proposes a new benchmark - EvoCodeBench to address the preceding problems, which has three primary advances. (1) EvoCodeBench aligns with real-world repositories in multiple dimensions, e.g., code distributions and dependency distributions. (2) EvoCodeBench offers comprehensive annotations (e.g., requirements, reference code, and reference dependencies), and robust evaluation metrics (e.g., Pass@k and Recall@k). (3) EvoCodeBench is an evolving benchmark to avoid data leakage. We build an automatic pipeline to update EvoCodeBench from the latest repositories. We release the first version - EvoCodeBench-2403, containing 275 samples from 25 real-world repositories. Based on EvoCodeBench, we propose repo
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#27431;&#27954;&#20154;&#26435;&#27861;&#38498;&#35009;&#20915;&#30340;&#20808;&#21069;&#26696;&#20363;&#26816;&#32034;&#25968;&#25454;&#38598;&#65292;&#26377;&#25928;&#22320;&#20998;&#31163;&#20107;&#23454;&#21644;&#35770;&#28857;&#65292;&#23637;&#31034;&#20102;&#21028;&#20363;&#23454;&#36341;&#65292;&#26377;&#21161;&#20110;&#24320;&#21457;&#20808;&#21069;&#26696;&#20363;&#26816;&#32034;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2404.00596</link><description>&lt;p&gt;
ECtHR-PCR&#65306;&#27431;&#27954;&#20154;&#26435;&#27861;&#38498;&#30340;&#21028;&#20363;&#29702;&#35299;&#21644;&#20808;&#21069;&#26696;&#20363;&#26816;&#32034;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ECtHR-PCR: A Dataset for Precedent Understanding and Prior Case Retrieval in the European Court of Human Rights
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00596
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#27431;&#27954;&#20154;&#26435;&#27861;&#38498;&#35009;&#20915;&#30340;&#20808;&#21069;&#26696;&#20363;&#26816;&#32034;&#25968;&#25454;&#38598;&#65292;&#26377;&#25928;&#22320;&#20998;&#31163;&#20107;&#23454;&#21644;&#35770;&#28857;&#65292;&#23637;&#31034;&#20102;&#21028;&#20363;&#23454;&#36341;&#65292;&#26377;&#21161;&#20110;&#24320;&#21457;&#20808;&#21069;&#26696;&#20363;&#26816;&#32034;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26222;&#36890;&#27861;&#31649;&#36758;&#21306;&#65292;&#27861;&#24459;&#23454;&#36341;&#32773;&#20381;&#36182;&#20110;&#21028;&#20363;&#26469;&#26500;&#24314;&#35770;&#28857;&#65292;&#31526;&#21512;\emph{stare decisis}&#21407;&#21017;&#12290;&#38543;&#30528;&#22810;&#24180;&#26469;&#26696;&#20214;&#25968;&#37327;&#30340;&#22686;&#38271;&#65292;&#20808;&#21069;&#26696;&#20363;&#26816;&#32034;&#65288;PCR&#65289;&#21463;&#21040;&#20102;&#37325;&#35270;&#12290;&#29616;&#26377;&#30340;PCR&#25968;&#25454;&#38598;&#38500;&#20102;&#32570;&#20047;&#30495;&#23454;&#19990;&#30028;&#35268;&#27169;&#22806;&#65292;&#24182;&#26410;&#27169;&#25311;&#29616;&#23454;&#24773;&#22659;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#26597;&#35810;&#20351;&#29992;&#23436;&#25972;&#26696;&#20214;&#25991;&#20214;&#65292;&#21482;&#25513;&#30422;&#20808;&#21069;&#26696;&#20363;&#30340;&#24341;&#29992;&#12290;&#22240;&#27492;&#65292;&#26597;&#35810;&#23601;&#20250;&#26292;&#38706;&#20110;&#23578;&#26410;&#24418;&#25104;&#23545;&#26410;&#20915;&#26696;&#36827;&#34892;&#35770;&#35777;&#26102;&#25152;&#38656;&#30340;&#27861;&#24459;&#25512;&#29702;&#65292;&#20197;&#21450;&#21487;&#33021;&#20250;&#24178;&#25200;&#26696;&#20214;&#20107;&#23454;&#21644;&#27861;&#24459;&#21407;&#21017;&#30340;&#20840;&#38754;&#29702;&#35299;&#30340;&#24341;&#29992;&#25513;&#30721;&#25152;&#30041;&#19979;&#30340;&#34394;&#20551;&#27169;&#24335;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#27431;&#27954;&#20154;&#26435;&#27861;&#38498;&#65288;ECtHR&#65289;&#35009;&#20915;&#30340;PCR&#25968;&#25454;&#38598;&#65292;&#26126;&#30830;&#21306;&#20998;&#20107;&#23454;&#21644;&#35770;&#28857;&#65292;&#24182;&#23637;&#31034;&#20102;&#21028;&#20363;&#23454;&#36341;&#65292;&#24110;&#21161;&#25105;&#20204;&#21457;&#23637;&#36825;&#19968;PCR&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00596v1 Announce Type: new  Abstract: In common law jurisdictions, legal practitioners rely on precedents to construct arguments, in line with the doctrine of \emph{stare decisis}. As the number of cases grow over the years, prior case retrieval (PCR) has garnered significant attention. Besides lacking real-world scale, existing PCR datasets do not simulate a realistic setting, because their queries use complete case documents while only masking references to prior cases. The query is thereby exposed to legal reasoning not yet available when constructing an argument for an undecided case as well as spurious patterns left behind by citation masks, potentially short-circuiting a comprehensive understanding of case facts and legal principles. To address these limitations, we introduce a PCR dataset based on judgements from the European Court of Human Rights (ECtHR), which explicitly separate facts from arguments and exhibit precedential practices, aiding us to develop this PCR 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;&#20174;&#27861;&#24459;&#35009;&#20915;&#20013;&#22522;&#20110;&#26597;&#35810;&#25552;&#21462;&#30456;&#20851;&#27573;&#33853;&#30340;&#20219;&#21153;&#65292;&#32467;&#26524;&#34920;&#26126;&#24494;&#35843;&#21644;&#38646;-shot&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#65292;&#24378;&#35843;&#20102;&#22312;&#27861;&#24459;&#39046;&#22495;&#22788;&#29702;&#20998;&#24067;&#21464;&#21270;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2404.00595</link><description>&lt;p&gt;
&#20174;&#27861;&#24459;&#35009;&#20915;&#20013;&#22522;&#20110;&#26597;&#35810;&#25552;&#21462;&#30456;&#20851;&#27573;&#33853;
&lt;/p&gt;
&lt;p&gt;
Query-driven Relevant Paragraph Extraction from Legal Judgments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;&#20174;&#27861;&#24459;&#35009;&#20915;&#20013;&#22522;&#20110;&#26597;&#35810;&#25552;&#21462;&#30456;&#20851;&#27573;&#33853;&#30340;&#20219;&#21153;&#65292;&#32467;&#26524;&#34920;&#26126;&#24494;&#35843;&#21644;&#38646;-shot&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#65292;&#24378;&#35843;&#20102;&#22312;&#27861;&#24459;&#39046;&#22495;&#22788;&#29702;&#20998;&#24067;&#21464;&#21270;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27861;&#24459;&#19987;&#19994;&#20154;&#22763;&#32463;&#24120;&#33510;&#20110;&#22312;&#28459;&#38271;&#30340;&#27861;&#24459;&#35009;&#20915;&#20013;&#23547;&#25214;&#30452;&#25509;&#35299;&#31572;&#20182;&#20204;&#38382;&#39064;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#20851;&#27880;&#20174;&#27861;&#24459;&#35009;&#20915;&#20013;&#22522;&#20110;&#26597;&#35810;&#25552;&#21462;&#30456;&#20851;&#27573;&#33853;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#21033;&#29992;&#27431;&#27954;&#20154;&#26435;&#27861;&#38498;&#65288;ECtHR&#65289;&#30340;&#26696;&#20363;&#27861;&#25351;&#21335;&#26500;&#24314;&#20102;&#19968;&#20010;&#19987;&#38376;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20197;&#38646;-shot&#26041;&#24335;&#35780;&#20272;&#24403;&#21069;&#26816;&#32034;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#24314;&#31435;&#20351;&#29992;&#21508;&#31181;&#27169;&#22411;&#30340;&#24494;&#35843;&#22522;&#20934;&#12290;&#32467;&#26524;&#31361;&#20986;&#20102;&#24494;&#35843;&#21644;&#38646;-shot&#24615;&#33021;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#36317;&#65292;&#24378;&#35843;&#20102;&#22788;&#29702;&#27861;&#24459;&#39046;&#22495;&#20013;&#20998;&#24067;&#21464;&#21270;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#27861;&#24459;&#39044;&#35757;&#32451;&#22312;&#35821;&#26009;&#24211;&#20391;&#22788;&#29702;&#20998;&#24067;&#21464;&#21270;&#65292;&#20294;&#22312;&#26597;&#35810;&#20391;&#20998;&#24067;&#21464;&#21270;&#65288;&#30475;&#19981;&#35265;&#30340;&#27861;&#24459;&#26597;&#35810;&#65289;&#26041;&#38754;&#20173;&#28982;&#26377;&#22256;&#38590;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#21508;&#31181;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;&#23427;&#20204;&#22312;&#23454;&#36341;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00595v1 Announce Type: new  Abstract: Legal professionals often grapple with navigating lengthy legal judgements to pinpoint information that directly address their queries. This paper focus on this task of extracting relevant paragraphs from legal judgements based on the query. We construct a specialized dataset for this task from the European Court of Human Rights (ECtHR) using the case law guides. We assess the performance of current retrieval models in a zero-shot way and also establish fine-tuning benchmarks using various models. The results highlight the significant gap between fine-tuned and zero-shot performance, emphasizing the challenge of handling distribution shift in the legal domain. We notice that the legal pre-training handles distribution shift on the corpus side but still struggles on query side distribution shift, with unseen legal queries. We also explore various Parameter Efficient Fine-Tuning (PEFT) methods to evaluate their practicality within the cont
&lt;/p&gt;</description></item><item><title>LexAbSumm&#26159;&#19968;&#20010;&#38024;&#23545;&#27861;&#24459;&#26696;&#20363;&#20915;&#31574;&#30340;&#22522;&#20110;&#26041;&#38754;&#30340;&#25688;&#35201;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#20102;&#22810;&#20010;&#36866;&#29992;&#20110;&#36739;&#38271;&#25991;&#26723;&#30340;&#25277;&#35937;&#25688;&#35201;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#22312;&#20135;&#29983;&#29305;&#23450;&#26041;&#38754;&#25688;&#35201;&#26102;&#36825;&#20123;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2404.00594</link><description>&lt;p&gt;
LexAbSumm&#65306;&#22522;&#20110;&#26041;&#38754;&#30340;&#27861;&#24459;&#21028;&#20915;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
LexAbSumm: Aspect-based Summarization of Legal Decisions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00594
&lt;/p&gt;
&lt;p&gt;
LexAbSumm&#26159;&#19968;&#20010;&#38024;&#23545;&#27861;&#24459;&#26696;&#20363;&#20915;&#31574;&#30340;&#22522;&#20110;&#26041;&#38754;&#30340;&#25688;&#35201;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#20102;&#22810;&#20010;&#36866;&#29992;&#20110;&#36739;&#38271;&#25991;&#26723;&#30340;&#25277;&#35937;&#25688;&#35201;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#22312;&#20135;&#29983;&#29305;&#23450;&#26041;&#38754;&#25688;&#35201;&#26102;&#36825;&#20123;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27861;&#24459;&#19987;&#19994;&#20154;&#22763;&#32463;&#24120;&#36935;&#21040;&#21253;&#21547;&#23545;&#20182;&#20204;&#24037;&#20316;&#33267;&#20851;&#37325;&#35201;&#35265;&#35299;&#30340;&#38271;&#31687;&#27861;&#24459;&#21028;&#20915;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#36827;&#23637;&#24050;&#32463;&#23548;&#33268;&#20102;&#38024;&#23545;&#27861;&#24459;&#25991;&#20214;&#30340;&#33258;&#21160;&#25688;&#35201;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#25552;&#20379;&#36890;&#29992;&#25688;&#35201;&#65292;&#21487;&#33021;&#26080;&#27861;&#28385;&#36275;&#29992;&#25143;&#22810;&#26679;&#21270;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LexAbSumm&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;&#28304;&#33258;&#27431;&#27954;&#20154;&#26435;&#27861;&#38498;&#31649;&#36758;&#21306;&#30340;&#27861;&#24459;&#26696;&#20363;&#20915;&#31574;&#36827;&#34892;&#22522;&#20110;&#26041;&#38754;&#30340;&#25688;&#35201;&#35774;&#35745;&#30340;&#26032;&#39062;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20960;&#31181;&#38024;&#23545;&#38271;&#31687;&#25991;&#26723;&#30340;&#25277;&#35937;&#25688;&#35201;&#27169;&#22411;&#22312;LexAbSumm&#19978;&#30340;&#24615;&#33021;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#26465;&#20214;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#29305;&#23450;&#26041;&#38754;&#25688;&#35201;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#21457;&#24067;LexAbSumm&#20197;&#20419;&#36827;&#27861;&#24459;&#39046;&#22495;&#22522;&#20110;&#26041;&#38754;&#30340;&#25688;&#35201;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00594v1 Announce Type: new  Abstract: Legal professionals frequently encounter long legal judgments that hold critical insights for their work. While recent advances have led to automated summarization solutions for legal documents, they typically provide generic summaries, which may not meet the diverse information needs of users. To address this gap, we introduce LexAbSumm, a novel dataset designed for aspect-based summarization of legal case decisions, sourced from the European Court of Human Rights jurisdiction. We evaluate several abstractive summarization models tailored for longer documents on LexAbSumm, revealing a challenge in conditioning these models to produce aspect-specific summaries. We release LexAbSum to facilitate research in aspect-based summarization for legal domain.
&lt;/p&gt;</description></item><item><title>CuSINeS&#36890;&#36807;&#35838;&#31243;&#39537;&#21160;&#30340;&#36127;&#37319;&#26679;&#31574;&#30053;&#12289;&#32467;&#26500;&#21270;&#27861;&#35268;&#20449;&#24687;&#21644;&#21160;&#24577;&#35821;&#20041;&#38590;&#24230;&#35780;&#20272;&#19977;&#26041;&#38754;&#36129;&#29486;&#65292;&#25552;&#21319;&#20102;&#27861;&#23450;&#25991;&#31456;&#26816;&#32034;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2404.00590</link><description>&lt;p&gt;
CuSINeS&#65306;&#22522;&#20110;&#35838;&#31243;&#39537;&#21160;&#30340;&#32467;&#26500;&#35825;&#23548;&#36127;&#37319;&#26679;&#29992;&#20110;&#27861;&#23450;&#25991;&#31456;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
CuSINeS: Curriculum-driven Structure Induced Negative Sampling for Statutory Article Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00590
&lt;/p&gt;
&lt;p&gt;
CuSINeS&#36890;&#36807;&#35838;&#31243;&#39537;&#21160;&#30340;&#36127;&#37319;&#26679;&#31574;&#30053;&#12289;&#32467;&#26500;&#21270;&#27861;&#35268;&#20449;&#24687;&#21644;&#21160;&#24577;&#35821;&#20041;&#38590;&#24230;&#35780;&#20272;&#19977;&#26041;&#38754;&#36129;&#29486;&#65292;&#25552;&#21319;&#20102;&#27861;&#23450;&#25991;&#31456;&#26816;&#32034;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CuSINeS&#65292;&#36825;&#26159;&#19968;&#31181;&#36127;&#37319;&#26679;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#27861;&#23450;&#25991;&#31456;&#26816;&#32034;&#65288;SAR&#65289;&#30340;&#24615;&#33021;&#12290;CuSINeS&#25552;&#20379;&#20102;&#19977;&#20010;&#20851;&#38190;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#23427;&#37319;&#29992;&#20102;&#22522;&#20110;&#35838;&#31243;&#30340;&#36127;&#37319;&#26679;&#31574;&#30053;&#65292;&#24341;&#23548;&#27169;&#22411;&#39318;&#20808;&#20851;&#27880;&#26356;&#23481;&#26131;&#30340;&#36127;&#26679;&#26412;&#65292;&#36880;&#28176;&#22788;&#29702;&#26356;&#38590;&#30340;&#26679;&#26412;&#12290;&#20854;&#27425;&#65292;&#23427;&#21033;&#29992;&#20174;&#27861;&#35268;&#32467;&#26500;&#32452;&#32455;&#20013;&#33719;&#24471;&#30340;&#20998;&#23618;&#21644;&#24207;&#21015;&#20449;&#24687;&#26469;&#35780;&#20272;&#26679;&#26412;&#30340;&#38590;&#24230;&#12290;&#26368;&#21518;&#65292;&#23427;&#24341;&#20837;&#20102;&#21160;&#24577;&#35821;&#20041;&#38590;&#24230;&#35780;&#20272;&#65292;&#21033;&#29992;&#27491;&#22312;&#35757;&#32451;&#30340;&#27169;&#22411;&#26412;&#36523;&#65292;&#36229;&#36234;&#20256;&#32479;&#30340;&#38745;&#24577;&#26041;&#27861;&#22914;BM25&#65292;&#20351;&#36127;&#26679;&#26412;&#36866;&#24212;&#27169;&#22411;&#19981;&#26029;&#36827;&#21270;&#30340;&#33021;&#21147;&#12290;&#22312;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#19987;&#23478;&#27880;&#37322;&#30340;SAR&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;CuSINeS&#22312;&#22235;&#31181;&#19981;&#21516;&#22522;&#32447;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#20854;&#22810;&#21151;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00590v1 Announce Type: cross  Abstract: In this paper, we introduce CuSINeS, a negative sampling approach to enhance the performance of Statutory Article Retrieval (SAR). CuSINeS offers three key contributions. Firstly, it employs a curriculum-based negative sampling strategy guiding the model to focus on easier negatives initially and progressively tackle more difficult ones. Secondly, it leverages the hierarchical and sequential information derived from the structural organization of statutes to evaluate the difficulty of samples. Lastly, it introduces a dynamic semantic difficulty assessment using the being-trained model itself, surpassing conventional static methods like BM25, adapting the negatives to the model's evolving competence. Experimental results on a real-world expert-annotated SAR dataset validate the effectiveness of CuSINeS across four different baselines, demonstrating its versatility.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#22270;&#25968;&#25454;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#27169;&#22359;&#22686;&#24378;&#65292;&#25552;&#20379;&#32622;&#20449;&#24230;&#35780;&#20998;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#30693;&#35782;&#22270;&#23436;&#25104;&#21644;&#22270;&#20998;&#31867;&#20219;&#21153;&#19978;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.00589</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#22270;&#25968;&#25454;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Harnessing the Power of Large Language Model for Uncertainty Aware Graph Processing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00589
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#22270;&#25968;&#25454;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#27169;&#22359;&#22686;&#24378;&#65292;&#25552;&#20379;&#32622;&#20449;&#24230;&#35780;&#20998;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#30693;&#35782;&#22270;&#23436;&#25104;&#21644;&#22270;&#20998;&#31867;&#20219;&#21153;&#19978;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#22270;&#25968;&#25454;&#26159;&#19968;&#39033;&#38750;&#24120;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#20256;&#32479;&#25216;&#26415;&#65292;&#20363;&#22914;&#22522;&#20110;&#20960;&#20309;&#21644;&#30697;&#38453;&#20998;&#35299;&#30340;&#25216;&#26415;&#65292;&#20381;&#36182;&#20110;&#23545;&#25968;&#25454;&#20851;&#31995;&#30340;&#20551;&#35774;&#65292;&#22312;&#22788;&#29702;&#22823;&#22411;&#21644;&#22797;&#26434;&#30340;&#22270;&#25968;&#25454;&#26102;&#21464;&#24471;&#19981;&#36275;&#22815;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23637;&#31034;&#20102;&#22788;&#29702;&#22823;&#22411;&#22270;&#25968;&#25454;&#30340;&#33391;&#22909;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#26080;&#27861;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#35299;&#37322;&#12290;&#20026;&#20102;&#20351;&#22270;&#22788;&#29702;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#22686;&#24378;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#27169;&#22359;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#21147;&#37327;&#65292;&#20197;&#25552;&#20379;&#29983;&#25104;&#31572;&#26696;&#30340;&#32622;&#20449;&#24230;&#20998;&#25968;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22270;&#22788;&#29702;&#20219;&#21153;&#19978;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#39564;&#65306;&#23569;&#26679;&#26412;&#30693;&#35782;&#22270;&#23436;&#25104;&#21644;&#22270;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65292;LLM&#22312;&#21508;&#20010;&#26041;&#38754;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00589v1 Announce Type: cross  Abstract: Handling graph data is one of the most difficult tasks. Traditional techniques, such as those based on geometry and matrix factorization, rely on assumptions about the data relations that become inadequate when handling large and complex graph data. On the other hand, deep learning approaches demonstrate promising results in handling large graph data, but they often fall short of providing interpretable explanations. To equip the graph processing with both high accuracy and explainability, we introduce a novel approach that harnesses the power of a large language model (LLM), enhanced by an uncertainty-aware module to provide a confidence score on the generated answer. We experiment with our approach on two graph processing tasks: few-shot knowledge graph completion and graph classification. Our results demonstrate that through parameter efficient fine-tuning, the LLM surpasses state-of-the-art algorithms by a substantial margin across
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#38382;&#39064;&#37325;&#20889;&#27169;&#22411;&#65292;&#36890;&#36807;&#39034;&#24207;&#37325;&#20889;&#22686;&#21152;&#38382;&#39064;&#22797;&#26434;&#24615;&#65292;&#33021;&#22815;&#26080;&#38656;&#20013;&#38388;&#38382;&#39064;&#26631;&#35760;&#35757;&#32451;&#29983;&#25104;&#35299;&#37322;&#24615;&#24378;&#30340;&#22810;&#36339;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.00571</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#22810;&#36339;&#38382;&#39064;&#29983;&#25104;&#65306;&#19968;&#31181;&#26080;&#38656;&#20013;&#38388;&#38382;&#39064;&#26631;&#35760;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Explainable Multi-hop Question Generation: An End-to-End Approach without Intermediate Question Labeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00571
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#38382;&#39064;&#37325;&#20889;&#27169;&#22411;&#65292;&#36890;&#36807;&#39034;&#24207;&#37325;&#20889;&#22686;&#21152;&#38382;&#39064;&#22797;&#26434;&#24615;&#65292;&#33021;&#22815;&#26080;&#38656;&#20013;&#38388;&#38382;&#39064;&#26631;&#35760;&#35757;&#32451;&#29983;&#25104;&#35299;&#37322;&#24615;&#24378;&#30340;&#22810;&#36339;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20132;&#20114;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#22686;&#21152;&#20351;&#29992;&#65292;&#22788;&#29702;&#22797;&#26434;&#38382;&#39064;&#30340;&#38656;&#27714;&#20063;&#22312;&#22686;&#21152;&#12290;&#22810;&#36339;&#38382;&#39064;&#29983;&#25104;&#26088;&#22312;&#29983;&#25104;&#38656;&#35201;&#22312;&#22810;&#20010;&#25991;&#26723;&#19978;&#36827;&#34892;&#22810;&#27493;&#25512;&#29702;&#30340;&#22797;&#26434;&#38382;&#39064;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#37319;&#29992;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#20854;&#20013;&#38382;&#39064;&#26159;&#22522;&#20110;&#19978;&#19979;&#25991;&#25991;&#26723;&#30340;&#34920;&#31034;&#36827;&#34892;&#35299;&#30721;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#32570;&#20047;&#35299;&#37322;&#29983;&#25104;&#30340;&#22810;&#36339;&#38382;&#39064;&#32972;&#21518;&#30340;&#25512;&#29702;&#36807;&#31243;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#38382;&#39064;&#37325;&#20889;&#26041;&#27861;&#65292;&#36880;&#27493;&#22686;&#21152;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65292;&#20063;&#30001;&#20110;&#38656;&#35201;&#20026;&#20013;&#38388;&#38454;&#27573;&#38382;&#39064;&#26631;&#35760;&#25968;&#25454;&#32780;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#31471;&#21040;&#31471;&#38382;&#39064;&#37325;&#20889;&#27169;&#22411;&#65292;&#36890;&#36807;&#39034;&#24207;&#37325;&#20889;&#22686;&#21152;&#38382;&#39064;&#22797;&#26434;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#20855;&#26377;&#20165;&#35757;&#32451;&#26368;&#32456;&#30340;&#22810;&#36339;&#38382;&#39064;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00571v1 Announce Type: new  Abstract: In response to the increasing use of interactive artificial intelligence, the demand for the capacity to handle complex questions has increased. Multi-hop question generation aims to generate complex questions that requires multi-step reasoning over several documents. Previous studies have predominantly utilized end-to-end models, wherein questions are decoded based on the representation of context documents. However, these approaches lack the ability to explain the reasoning process behind the generated multi-hop questions. Additionally, the question rewriting approach, which incrementally increases the question complexity, also has limitations due to the requirement of labeling data for intermediate-stage questions. In this paper, we introduce an end-to-end question rewriting model that increases question complexity through sequential rewriting. The proposed model has the advantage of training with only the final multi-hop questions, w
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ParaICL&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24182;&#34892;&#25209;&#22788;&#29702;&#26469;&#26377;&#25928;&#21033;&#29992;&#25152;&#26377;&#31034;&#20363;&#65292;&#22312;&#19981;&#36229;&#36807;&#21487;&#31649;&#29702;&#30340;&#36755;&#20837;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#21319;&#20102;&#19981;&#21516;&#27979;&#35797;&#26679;&#26412;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.00570</link><description>&lt;p&gt;
ParaICL&#65306;&#38754;&#21521;&#40065;&#26834;&#24615;&#30340;&#24182;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ParaICL: Towards Robust Parallel In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00570
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ParaICL&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24182;&#34892;&#25209;&#22788;&#29702;&#26469;&#26377;&#25928;&#21033;&#29992;&#25152;&#26377;&#31034;&#20363;&#65292;&#22312;&#19981;&#36229;&#36807;&#21487;&#31649;&#29702;&#30340;&#36755;&#20837;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#21319;&#20102;&#19981;&#21516;&#27979;&#35797;&#26679;&#26412;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#39046;&#22495;&#30340;&#24120;&#24577;&#65292;&#22312;&#23569;&#26679;&#26412;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;ICL&#30340;&#25104;&#21151;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#23569;&#26679;&#26412;&#28436;&#31034;&#31034;&#20363;&#30340;&#36873;&#25321;&#65292;&#20351;&#24471;&#36873;&#25321;&#36807;&#31243;&#21464;&#24471;&#24840;&#21457;&#20851;&#38190;&#12290;&#29616;&#26377;&#26041;&#27861;&#24050;&#32463;&#24320;&#22987;&#20248;&#21270;&#36825;&#20123;&#31034;&#20363;&#30340;&#25968;&#37327;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#20197;&#25552;&#39640;ICL&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;ICL&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#36755;&#20837;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#19981;&#21516;&#30340;&#23569;&#26679;&#26412;&#28436;&#31034;&#31034;&#20363;&#32452;&#21512;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#23545;&#19981;&#21516;&#27979;&#35797;&#26679;&#26412;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;parallel in-context learning (ParaICL)&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#21033;&#29992;&#25152;&#26377;&#31034;&#20363;&#32780;&#19981;&#20250;&#36229;&#20986;&#21487;&#31649;&#29702;&#30340;&#36755;&#20837;&#19978;&#19979;&#25991;&#38271;&#24230;&#12290;ParaICL&#37319;&#29992;&#24182;&#34892;&#25209;&#22788;&#29702;&#26469;&#20998;&#21457;&#28436;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00570v1 Announce Type: new  Abstract: Large language models (LLMs) have become the norm in natural language processing (NLP), excelling in few-shot in-context learning (ICL) with their remarkable abilities. Nonetheless, the success of ICL largely hinges on the choice of few-shot demonstration examples, making the selection process increasingly crucial. Existing methods have delved into optimizing the quantity and semantic similarity of these examples to improve ICL performances. However, our preliminary experiments indicate that the effectiveness of ICL is limited by the length of the input context. Moreover, varying combinations of few-shot demonstration examples can significantly boost accuracy across different test samples. To address this, we propose a novel method named parallel in-context learning (ParaICL) that effectively utilizes all demonstration examples without exceeding the manageable input context length. ParaICL employs parallel batching to distribute demonstr
&lt;/p&gt;</description></item><item><title>CM-TTS&#36890;&#36807;&#19968;&#33268;&#24615;&#27169;&#22411;&#21644;&#21152;&#26435;&#37319;&#26679;&#22120;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#23454;&#26102;&#25991;&#26412;&#36716;&#35821;&#38899;&#21512;&#25104;&#65292;&#36991;&#20813;&#20102;&#23545;&#25239;&#35757;&#32451;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20381;&#36182;&#12290;</title><link>https://arxiv.org/abs/2404.00569</link><description>&lt;p&gt;
CM-TTS: &#36890;&#36807;&#21152;&#26435;&#37319;&#26679;&#22120;&#21644;&#19968;&#33268;&#24615;&#27169;&#22411;&#25552;&#39640;&#23454;&#26102;&#25991;&#26412;&#36716;&#35821;&#38899;&#21512;&#25104;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
CM-TTS: Enhancing Real Time Text-to-Speech Synthesis Efficiency through Weighted Samplers and Consistency Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00569
&lt;/p&gt;
&lt;p&gt;
CM-TTS&#36890;&#36807;&#19968;&#33268;&#24615;&#27169;&#22411;&#21644;&#21152;&#26435;&#37319;&#26679;&#22120;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#23454;&#26102;&#25991;&#26412;&#36716;&#35821;&#38899;&#21512;&#25104;&#65292;&#36991;&#20813;&#20102;&#23545;&#25239;&#35757;&#32451;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#31995;&#32479;&#22312;&#35821;&#38899;&#21161;&#25163;&#12289;&#30005;&#23376;&#23398;&#20064;&#21644;&#26377;&#22768;&#35835;&#29289;&#21046;&#20316;&#31561;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#29616;&#20195;&#27169;&#22411;&#30340;&#36861;&#27714;&#65292;&#22914;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#65292;&#26377;&#26395;&#23454;&#29616;&#39640;&#20445;&#30495;&#12289;&#23454;&#26102;&#30340;&#35821;&#38899;&#21512;&#25104;&#12290;&#28982;&#32780;&#65292;&#25193;&#25955;&#27169;&#22411;&#20013;&#22810;&#27493;&#37319;&#26679;&#30340;&#25928;&#29575;&#23384;&#22312;&#25361;&#25112;&#12290;&#24050;&#32463;&#20570;&#20986;&#21162;&#21147;&#23558;GANs&#19982;DMs&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#36890;&#36807;&#36924;&#36817;&#21435;&#22122;&#20998;&#24067;&#21152;&#24555;&#25512;&#26029;&#36895;&#24230;&#65292;&#20294;&#36825;&#30001;&#20110;&#23545;&#25239;&#35757;&#32451;&#23548;&#33268;&#27169;&#22411;&#25910;&#25947;&#20986;&#29616;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CM-TTS&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#19968;&#33268;&#24615;&#27169;&#22411;&#65288;CMs&#65289;&#30340;&#26032;&#22411;&#20307;&#31995;&#32467;&#26500;&#12290;CM-TTS&#27762;&#21462;&#20102;&#36830;&#32493;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#30340;&#28789;&#24863;&#65292;&#22312;&#36739;&#23569;&#27493;&#39588;&#20013;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;&#21512;&#25104;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#23545;&#25239;&#35757;&#32451;&#25110;&#20381;&#36182;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#21152;&#26435;&#37319;&#26679;&#22120;&#65292;&#20197;&#21160;&#24577;&#27010;&#29575;&#23558;&#19981;&#21516;&#37319;&#26679;&#20301;&#32622;&#32435;&#20837;&#27169;&#22411;&#35757;&#32451;&#65292;&#30830;&#20445;&#19981;&#20559;&#20506;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00569v1 Announce Type: cross  Abstract: Neural Text-to-Speech (TTS) systems find broad applications in voice assistants, e-learning, and audiobook creation. The pursuit of modern models, like Diffusion Models (DMs), holds promise for achieving high-fidelity, real-time speech synthesis. Yet, the efficiency of multi-step sampling in Diffusion Models presents challenges. Efforts have been made to integrate GANs with DMs, speeding up inference by approximating denoising distributions, but this introduces issues with model convergence due to adversarial training. To overcome this, we introduce CM-TTS, a novel architecture grounded in consistency models (CMs). Drawing inspiration from continuous-time diffusion models, CM-TTS achieves top-quality speech synthesis in fewer steps without adversarial training or pre-trained model dependencies. We further design weighted samplers to incorporate different sampling positions into model training with dynamic probabilities, ensuring unbias
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;CodeBenchGen&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#20219;&#24847;&#20195;&#30721;&#36716;&#21270;&#20026;&#35780;&#20272;&#31034;&#20363;&#65292;&#21019;&#36896;&#20102;&#19968;&#20010;&#21253;&#21547;&#22823;&#37327;&#20195;&#30721;&#31034;&#20363;&#30340;&#25968;&#25454;&#38598;Exec-CSN&#65292;&#23637;&#31034;&#20102;&#20854;&#21487;&#25193;&#23637;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.00566</link><description>&lt;p&gt;
CodeBenchGen: &#21019;&#24314;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#25191;&#34892;&#30340;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
CodeBenchGen: Creating Scalable Execution-based Code Generation Benchmarks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00566
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;CodeBenchGen&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#20219;&#24847;&#20195;&#30721;&#36716;&#21270;&#20026;&#35780;&#20272;&#31034;&#20363;&#65292;&#21019;&#36896;&#20102;&#19968;&#20010;&#21253;&#21547;&#22823;&#37327;&#20195;&#30721;&#31034;&#20363;&#30340;&#25968;&#25454;&#38598;Exec-CSN&#65292;&#23637;&#31034;&#20102;&#20854;&#21487;&#25193;&#23637;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20419;&#36827;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#35780;&#20272;&#20195;&#30721;&#29983;&#25104;&#31995;&#32479;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CodeBenchGen&#65292;&#36825;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#21019;&#24314;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#25191;&#34892;&#30340;&#22522;&#20934;&#65292;&#20165;&#38656;&#35201;&#36731;&#24494;&#30340;&#20154;&#31867;&#25351;&#23548;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23558;&#20219;&#24847;&#20195;&#30721;&#29255;&#27573;&#36716;&#21270;&#20026;&#35780;&#20272;&#31034;&#20363;&#65292;&#21253;&#25324;&#29992;&#20110;&#25191;&#34892;&#35780;&#20272;&#30340;&#27979;&#35797;&#29992;&#20363;&#12290;&#25105;&#20204;&#36890;&#36807;&#21019;&#24314;&#21253;&#21547;&#26469;&#33258;CodeSearchNet&#25968;&#25454;&#38598;&#30340;367&#20010;GitHub&#23384;&#20648;&#24211;&#20013;&#30340;&#20195;&#30721;&#20462;&#25913;&#30340;293&#20010;&#24211;&#30340;1,931&#20010;&#20363;&#23376;&#30340;&#25968;&#25454;&#38598;Exec-CSN&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#23454;&#29992;&#24615;&#12290;&#20026;&#20102;&#23637;&#31034;Exec-CSN&#20013;&#31034;&#20363;&#30340;&#22797;&#26434;&#24615;&#21644;&#21487;&#35299;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#20154;&#31867;&#30740;&#31350;&#65292;&#32467;&#26524;&#26174;&#31034;81.3%&#30340;&#20363;&#23376;&#21487;&#20197;&#34987;&#20154;&#31867;&#35299;&#20915;&#65292;61%&#34987;&#35780;&#20026;&#8220;&#38656;&#35201;&#21162;&#21147;&#35299;&#20915;&#8221;&#12290;&#25105;&#20204;&#23545;&#24320;&#28304;&#21644;&#19987;&#26377;&#27169;&#22411;&#36827;&#34892;&#20102;&#20195;&#30721;&#29983;&#25104;&#23454;&#39564;&#65292;&#24182;&#20998;&#26512;&#20102;&#20154;&#31867;&#21644;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00566v1 Announce Type: cross  Abstract: To facilitate evaluation of code generation systems across diverse scenarios, we present CodeBenchGen, a framework to create scalable execution-based benchmarks that only requires light guidance from humans. Specifically, we leverage a large language model (LLM) to convert an arbitrary piece of code into an evaluation example, including test cases for execution-based evaluation. We illustrate the usefulness of our framework by creating a dataset, Exec-CSN, which includes 1,931 examples involving 293 libraries revised from code in 367 GitHub repositories taken from the CodeSearchNet dataset. To demonstrate the complexity and solvability of examples in Exec-CSN, we present a human study demonstrating that 81.3% of the examples can be solved by humans and 61% are rated as ``requires effort to solve''. We conduct code generation experiments on open-source and proprietary models and analyze the performance of both humans and models. We will
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25506;&#32034;&#24615;&#30740;&#31350;&#35782;&#21035;&#22467;&#21450;&#38463;&#25289;&#20271;&#32500;&#22522;&#30334;&#31185;&#20013;&#22522;&#20110;&#27169;&#26495;&#32763;&#35793;&#30340;&#25991;&#31456;&#21450;&#20854;&#29305;&#24449;&#65292;&#20197;&#35299;&#20915;&#22240;&#27492;&#23548;&#33268;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.00565</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#26009;&#24211;&#20803;&#25968;&#25454;&#26816;&#27979;&#22522;&#20110;&#27169;&#26495;&#30340;&#32763;&#35793;&#65306;&#22467;&#21450;&#38463;&#25289;&#20271;&#32500;&#22522;&#30334;&#31185;&#29256;&#26412;&#30340;&#25506;&#32034;&#24615;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Leveraging Corpus Metadata to Detect Template-based Translation: An Exploratory Case Study of the Egyptian Arabic Wikipedia Edition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00565
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25506;&#32034;&#24615;&#30740;&#31350;&#35782;&#21035;&#22467;&#21450;&#38463;&#25289;&#20271;&#32500;&#22522;&#30334;&#31185;&#20013;&#22522;&#20110;&#27169;&#26495;&#32763;&#35793;&#30340;&#25991;&#31456;&#21450;&#20854;&#29305;&#24449;&#65292;&#20197;&#35299;&#20915;&#22240;&#27492;&#23548;&#33268;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;&#65288;&#20869;&#23481;&#39029;&#38754;&#65289;&#36890;&#24120;&#34987;&#29992;&#20316;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30740;&#31350;&#20013;&#30340;&#35821;&#26009;&#24211;&#65292;&#29305;&#21035;&#26159;&#23545;&#33521;&#35821;&#20197;&#22806;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#30740;&#31350;&#36807;&#19977;&#20010;&#38463;&#25289;&#20271;&#32500;&#22522;&#30334;&#31185;&#29256;&#26412;&#65292;&#38463;&#25289;&#20271;&#32500;&#22522;&#30334;&#31185;&#65288;AR&#65289;&#12289;&#22467;&#21450;&#38463;&#25289;&#20271;&#32500;&#22522;&#30334;&#31185;&#65288;ARZ&#65289;&#21644;&#25705;&#27931;&#21733;&#38463;&#25289;&#20271;&#32500;&#22522;&#30334;&#31185;&#65288;ARY&#65289;&#65292;&#24182;&#35760;&#24405;&#20102;&#22467;&#21450;&#38463;&#25289;&#20271;&#32500;&#22522;&#30334;&#31185;&#29256;&#26412;&#20013;&#20986;&#29616;&#30340;&#38382;&#39064;&#65292;&#21363;&#36890;&#36807;&#20174;&#33521;&#35821;&#32763;&#35793;&#20026;&#38463;&#25289;&#20271;&#35821;&#30340;&#22522;&#20110;&#27169;&#26495;&#30340;&#33258;&#21160;&#21019;&#24314;&#20854;&#25991;&#31456;&#65292;&#27809;&#26377;&#20154;&#30340;&#21442;&#19982;&#65292;&#23548;&#33268;&#22467;&#21450;&#38463;&#25289;&#20271;&#32500;&#22522;&#30334;&#31185;&#20805;&#26021;&#30528;&#19981;&#20165;&#20869;&#23481;&#36136;&#37327;&#20302;&#19979;&#30340;&#25991;&#31456;&#65292;&#36824;&#26377;&#19981;&#20195;&#34920;&#22467;&#21450;&#20154;&#27665;&#12289;&#20182;&#20204;&#30340;&#25991;&#21270;&#21644;&#26041;&#35328;&#30340;&#25991;&#31456;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25506;&#32034;&#24615;&#22320;&#35782;&#21035;&#36825;&#20123;&#22522;&#20110;&#27169;&#26495;&#32763;&#35793;&#30340;&#25991;&#31456;&#21450;&#20854;&#29305;&#24449;&#65292;&#32531;&#35299;&#22467;&#21450;&#38463;&#25289;&#20271;&#32500;&#22522;&#30334;&#31185;&#20013;&#20986;&#29616;&#30340;&#27169;&#26495;&#32763;&#35793;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00565v1 Announce Type: new  Abstract: Wikipedia articles (content pages) are commonly used corpora in Natural Language Processing (NLP) research, especially in low-resource languages other than English. Yet, a few research studies have studied the three Arabic Wikipedia editions, Arabic Wikipedia (AR), Egyptian Arabic Wikipedia (ARZ), and Moroccan Arabic Wikipedia (ARY), and documented issues in the Egyptian Arabic Wikipedia edition regarding the massive automatic creation of its articles using template-based translation from English to Arabic without human involvement, overwhelming the Egyptian Arabic Wikipedia with articles that do not only have low-quality content but also with articles that do not represent the Egyptian people, their culture, and their dialect. In this paper, we aim to mitigate the problem of template translation that occurred in the Egyptian Arabic Wikipedia by identifying these template-translated articles and their characteristics through exploratory 
&lt;/p&gt;</description></item><item><title>DivTOD &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#35805;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#19982;LLMs&#21512;&#20316;&#23398;&#20064;&#22810;&#26679;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#34920;&#31034;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#27169;&#22411;&#22312;&#21508;&#31181;&#19979;&#28216;&#23545;&#35805;&#20219;&#21153;&#19978;&#20248;&#20110;&#24378;&#22522;&#32447;&#65292;&#24182;&#23398;&#20064;&#20102;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#30340;&#20869;&#22312;&#22810;&#26679;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.00557</link><description>&lt;p&gt;
DivTOD: &#21457;&#25381;LLM&#30340;&#21147;&#37327;&#65292;&#20026;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#34920;&#31034;&#22810;&#26679;&#21270;&#37322;&#25918;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
DivTOD: Unleashing the Power of LLMs for Diversifying Task-Oriented Dialogue Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00557
&lt;/p&gt;
&lt;p&gt;
DivTOD &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#35805;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#19982;LLMs&#21512;&#20316;&#23398;&#20064;&#22810;&#26679;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#34920;&#31034;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#27169;&#22411;&#22312;&#21508;&#31181;&#19979;&#28216;&#23545;&#35805;&#20219;&#21153;&#19978;&#20248;&#20110;&#24378;&#22522;&#32447;&#65292;&#24182;&#23398;&#20064;&#20102;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#30340;&#20869;&#22312;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#20010;&#39046;&#22495;&#26222;&#36941;&#37319;&#29992;&#30340;&#22312;&#36890;&#29992;&#25991;&#26412;&#19978;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#19982;&#36890;&#29992;&#25991;&#26412;&#30456;&#27604;&#65292;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#65288;TOD&#65289;&#20855;&#26377;&#26126;&#26174;&#30340;&#35821;&#35328;&#29305;&#24449;&#65292;&#36825;&#38480;&#21046;&#20102;&#29616;&#26377;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#38469;&#25928;&#29992;&#12290;&#24403;&#21069;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#39044;&#35757;&#32451;&#26041;&#27861;&#24573;&#35270;&#20102;&#23545;&#35805;&#30340;&#19968;&#23545;&#22810;&#23646;&#24615;&#65292;&#21363;&#22312;&#30456;&#21516;&#23545;&#35805;&#19978;&#19979;&#25991;&#19979;&#65292;&#21487;&#20197;&#26377;&#22810;&#20010;&#36866;&#24403;&#30340;&#22238;&#22797;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DivTOD&#30340;&#26032;&#22411;&#23545;&#35805;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#23427;&#19982;LLM&#20849;&#21516;&#23398;&#20064;&#22810;&#26679;&#21270;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#34920;&#31034;&#12290;DivTOD&#25351;&#23548;LLM&#22312;&#21521;&#36739;&#23567;&#27169;&#22411;&#20256;&#36882;&#22810;&#26679;&#21270;&#30693;&#35782;&#30340;&#21516;&#26102;&#65292;&#28040;&#38500;&#19982;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#30456;&#30683;&#30462;&#30340;&#39046;&#22495;&#30693;&#35782;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21508;&#31181;&#19979;&#28216;&#23545;&#35805;&#20219;&#21153;&#19978;&#32988;&#36807;&#24378;&#22823;&#30340;TOD&#22522;&#32447;&#65292;&#24182;&#23398;&#20064;&#20102;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#30340;&#20869;&#22312;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00557v1 Announce Type: new  Abstract: Language models pre-trained on general text have achieved impressive results in diverse fields. Yet, the distinct linguistic characteristics of task-oriented dialogues (TOD) compared to general text limit the practical utility of existing language models. Current task-oriented dialogue pre-training methods overlook the one-to-many property of conversations, where multiple responses can be appropriate given the same conversation context. In this paper, we propose a novel dialogue pre-training model called DivTOD, which collaborates with LLMs to learn diverse task-oriented dialogue representations. DivTOD guides LLMs in transferring diverse knowledge to smaller models while removing domain knowledge that contradicts task-oriented dialogues. Experiments show that our model outperforms strong TOD baselines on various downstream dialogue tasks and learns the intrinsic diversity of task-oriented dialogues.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20559;&#22909;&#33719;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;DOVE&#21327;&#35758;&#23545;&#25351;&#20196;-&#21709;&#24212;&#23545;&#30340;&#32852;&#21512;&#27010;&#29575;&#36827;&#34892;&#20248;&#21270;&#65292;&#20197;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2404.00530</link><description>&lt;p&gt;
&#23558;&#22351;&#33529;&#26524;&#19982;&#22909;&#27224;&#23376;&#36827;&#34892;&#27604;&#36739;&#65306;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#20559;&#22909;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Comparing Bad Apples to Good Oranges: Aligning Large Language Models via Joint Preference Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20559;&#22909;&#33719;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;DOVE&#21327;&#35758;&#23545;&#25351;&#20196;-&#21709;&#24212;&#23545;&#30340;&#32852;&#21512;&#27010;&#29575;&#36827;&#34892;&#20248;&#21270;&#65292;&#20197;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#24120;&#35265;&#30340;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25216;&#26415;&#20381;&#36182;&#20110;&#36890;&#36807;&#27604;&#36739;&#22312;&#22266;&#23450;&#19978;&#19979;&#25991;&#20013;&#26465;&#20214;&#29983;&#25104;&#30340;&#22810;&#20010;&#29983;&#25104;&#30340;&#20154;&#31867;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#24403;&#36825;&#20123;&#29983;&#25104;&#25918;&#32622;&#22312;&#30456;&#21516;&#30340;&#19978;&#19979;&#25991;&#20013;&#26102;&#65292;&#36825;&#20165;&#21033;&#29992;&#20102;&#25104;&#23545;&#27604;&#36739;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26465;&#20214;&#25490;&#21517;&#36890;&#24120;&#26080;&#27861;&#25429;&#33719;&#20154;&#31867;&#20559;&#22909;&#30340;&#22797;&#26434;&#21644;&#22810;&#32500;&#26041;&#38754;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20559;&#22909;&#33719;&#21462;&#30340;&#20256;&#32479;&#33539;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22312;&#25351;&#20196;-&#21709;&#24212;&#23545;&#19978;&#32852;&#21512;&#24341;&#21457;&#20559;&#22909;&#30340;&#26032;&#36724;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#20559;&#22909;&#20248;&#21270;&#26159;&#38024;&#23545;&#26465;&#20214;&#25490;&#21517;&#21327;&#35758;&#65288;&#20363;&#22914;&#65292;DPO&#65289;&#35774;&#35745;&#30340;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#30340;&#20559;&#22909;&#33719;&#21462;&#21327;&#35758;&#24341;&#20837;&#20102;DOVE&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#20559;&#22909;&#20248;&#21270;&#30446;&#26631;&#65292;&#36890;&#36807;&#25552;&#21319;&#25152;&#36873;&#25351;&#20196;-&#21709;&#24212;&#23545;&#30340;&#32852;&#21512;&#27010;&#29575;&#26469;&#38477;&#20302;&#25152;&#25298;&#32477;&#25351;&#20196;-&#21709;&#24212;&#23545;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00530v1 Announce Type: cross  Abstract: A common technique for aligning large language models (LLMs) relies on acquiring human preferences by comparing multiple generations conditioned on a fixed context. This only leverages the pairwise comparisons when the generations are placed in an identical context. However, such conditional rankings often fail to capture the complex and multidimensional aspects of human preferences. In this work, we revisit the traditional paradigm of preference acquisition and propose a new axis that is based on eliciting preferences jointly over the instruction-response pairs. While prior preference optimizations are designed for conditional ranking protocols (e.g., DPO), our proposed preference acquisition protocol introduces DOVE, a new preference optimization objective that upweights the joint probability of the chosen instruction-response pair over the rejected instruction-response pair. Interestingly, we find that the LLM trained with joint ins
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#25991;&#26412;&#12289;&#38899;&#39057;&#21644;&#35270;&#35273;&#27169;&#24577;&#30340;&#22810;&#27169;&#24577;&#24773;&#32490;&#35782;&#21035;&#21644;&#22810;&#27169;&#24577;&#24773;&#32490;&#21407;&#22240;&#25552;&#21462;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#19987;&#38376;&#30340;&#24773;&#32490;&#32534;&#30721;&#22120;&#21644;&#27169;&#24577;&#29305;&#23450;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#25552;&#21319;&#24773;&#32490;&#29702;&#35299;&#21644;&#22240;&#26524;&#25512;&#29702;&#30340;&#31454;&#20105;&#24615;&#25104;&#26524;&#12290;</title><link>https://arxiv.org/abs/2404.00511</link><description>&lt;p&gt;
MIPS&#22312;SemEval-2024&#20219;&#21153;3&#20013;&#30340;&#34920;&#29616;&#65306;&#20351;&#29992;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#22312;&#23545;&#35805;&#20013;&#36827;&#34892;&#22810;&#27169;&#24577;&#24773;&#32490;-&#21407;&#22240;&#23545;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
MIPS at SemEval-2024 Task 3: Multimodal Emotion-Cause Pair Extraction in Conversations with Multimodal Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#25991;&#26412;&#12289;&#38899;&#39057;&#21644;&#35270;&#35273;&#27169;&#24577;&#30340;&#22810;&#27169;&#24577;&#24773;&#32490;&#35782;&#21035;&#21644;&#22810;&#27169;&#24577;&#24773;&#32490;&#21407;&#22240;&#25552;&#21462;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#19987;&#38376;&#30340;&#24773;&#32490;&#32534;&#30721;&#22120;&#21644;&#27169;&#24577;&#29305;&#23450;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#25552;&#21319;&#24773;&#32490;&#29702;&#35299;&#21644;&#22240;&#26524;&#25512;&#29702;&#30340;&#31454;&#20105;&#24615;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;SemEval 2024&#20219;&#21153;3&#30340;&#23376;&#20219;&#21153;2&#20013;&#20851;&#20110;&#23545;&#35805;&#20013;&#22810;&#27169;&#24577;&#24773;&#32490;&#21407;&#22240;&#20998;&#26512;&#30340;&#33719;&#22870;&#25552;&#20132;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#24773;&#32490;&#35782;&#21035;&#21644;&#22810;&#27169;&#24577;&#24773;&#32490;&#21407;&#22240;&#25552;&#21462;&#65288;MER-MCE&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#19987;&#38376;&#30340;&#24773;&#32490;&#32534;&#30721;&#22120;&#25972;&#21512;&#25991;&#26412;&#12289;&#38899;&#39057;&#21644;&#35270;&#35273;&#19977;&#31181;&#27169;&#24577;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#27169;&#24577;&#29305;&#23450;&#29305;&#24449;&#25552;&#21319;&#24773;&#32490;&#29702;&#35299;&#21644;&#22240;&#26524;&#25512;&#29702;&#65292;&#20351;&#33258;&#24049;&#33073;&#39062;&#32780;&#20986;&#12290;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#20102;&#25105;&#20204;&#22810;&#27169;&#24577;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#30340;&#25552;&#20132;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#21152;&#26435;F1&#20998;&#25968;&#20026;0.3435&#65292;&#22312;0.0339&#20043;&#21518;&#25490;&#21517;&#31532;&#19968;&#30340;&#22242;&#38431;&#65292;&#20165;&#22312;0.0025&#20043;&#21518;&#25490;&#21517;&#31532;&#20108;&#12290;&#39033;&#30446;&#38142;&#25509;&#65306;https://github.com/MIPS-COLT/MER-MCE.git
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00511v1 Announce Type: new  Abstract: This paper presents our winning submission to Subtask 2 of SemEval 2024 Task 3 on multimodal emotion cause analysis in conversations. We propose a novel Multimodal Emotion Recognition and Multimodal Emotion Cause Extraction (MER-MCE) framework that integrates text, audio, and visual modalities using specialized emotion encoders. Our approach sets itself apart from top-performing teams by leveraging modality-specific features for enhanced emotion understanding and causality inference. Experimental evaluation demonstrates the advantages of our multimodal approach, with our submission achieving a competitive weighted F1 score of 0.3435, ranking third with a margin of only 0.0339 behind the 1st team and 0.0025 behind the 2nd team. Project: https://github.com/MIPS-COLT/MER-MCE.git
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#35782;&#21035;&#35821;&#35328;&#35889;&#31995;&#65292;&#30740;&#31350;&#20102;&#21333;&#35789;&#23884;&#20837;&#30340;&#24418;&#29366;&#22914;&#20309;&#20256;&#36882;&#20449;&#24687;&#65292;&#37325;&#24314;&#30340;&#35821;&#35328;&#35889;&#31995;&#26641;&#19982;&#21442;&#32771;&#26641;&#23637;&#29616;&#20986;&#24378;&#28872;&#30340;&#30456;&#20284;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.00500</link><description>&lt;p&gt;
&#21333;&#35789;&#23884;&#20837;&#30340;&#24418;&#29366;&#65306;&#36890;&#36807;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#35782;&#21035;&#35821;&#35328;&#35889;&#31995;
&lt;/p&gt;
&lt;p&gt;
The Shape of Word Embeddings: Recognizing Language Phylogenies through Topological Data Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00500
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#35782;&#21035;&#35821;&#35328;&#35889;&#31995;&#65292;&#30740;&#31350;&#20102;&#21333;&#35789;&#23884;&#20837;&#30340;&#24418;&#29366;&#22914;&#20309;&#20256;&#36882;&#20449;&#24687;&#65292;&#37325;&#24314;&#30340;&#35821;&#35328;&#35889;&#31995;&#26641;&#19982;&#21442;&#32771;&#26641;&#23637;&#29616;&#20986;&#24378;&#28872;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00500v1 &#31867;&#22411;&#65306;&#26032; &#21407;&#25991;&#25688;&#35201;&#65306;&#21333;&#35789;&#23884;&#20837;&#23558;&#35821;&#35328;&#35789;&#27719;&#34920;&#31034;&#20026;$d$&#32500;&#31354;&#38388;&#30340;&#28857;&#20113;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20123;&#28857;&#20113;&#30340;&#19968;&#33324;&#24418;&#29366;&#22312;&#38500;&#20102;&#34920;&#31034;&#27599;&#20010;&#20196;&#29260;&#30340;&#35821;&#20041;&#24847;&#20041;&#20043;&#22806;&#20256;&#36882;&#20449;&#24687;&#30340;&#26041;&#24335;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;(TDA)&#20013;&#30340;&#25345;&#20037;&#21516;&#35843;&#27010;&#24565;&#26469;&#27979;&#37327;&#20174;&#23427;&#20204;&#26410;&#26631;&#35760;&#30340;&#23884;&#20837;&#24418;&#29366;&#35745;&#31639;&#30340;&#35821;&#35328;&#23545;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#36317;&#31163;&#30697;&#38453;&#22312;81&#31181;&#21360;&#27431;&#35821;&#35328;&#20043;&#38388;&#26500;&#24314;&#35821;&#35328;&#35889;&#31995;&#26641;&#12290;&#20180;&#32454;&#35780;&#20272;&#34920;&#26126;&#25105;&#20204;&#37325;&#24314;&#30340;&#35889;&#31995;&#26641;&#19982;&#21442;&#32771;&#26641;&#21576;&#29616;&#20986;&#24378;&#28872;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00500v1 Announce Type: new  Abstract: Word embeddings represent language vocabularies as clouds of $d$-dimensional points. We investigate how information is conveyed by the general shape of these clouds, outside of representing the semantic meaning of each token. Specifically, we use the notion of persistent homology from topological data analysis (TDA) to measure the distances between language pairs from the shape of their unlabeled embeddings. We use these distance matrices to construct language phylogenetic trees over 81 Indo-European languages. Careful evaluation shows that our reconstructed trees exhibit strong similarities to the reference tree.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Configurable Safety Tuning&#65288;CST&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#20559;&#22909;&#25968;&#25454;&#65292;&#22312;&#25512;&#26029;&#26102;&#23454;&#29616;&#23545;LLMs&#30340;&#28789;&#27963;&#23433;&#20840;&#37197;&#32622;&#65292;&#20801;&#35768;&#29992;&#25143;&#26681;&#25454;&#38656;&#35201;&#31105;&#29992;/&#21551;&#29992;&#23433;&#20840;&#20559;&#22909;&#65292;&#19988;&#23454;&#39564;&#34920;&#26126;CST&#25104;&#21151;&#31649;&#29702;&#19981;&#21516;&#30340;&#23433;&#20840;&#37197;&#32622;&#24182;&#20445;&#30041;&#20102;&#21407;&#22987;&#21151;&#33021;&#65292;&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#21487;&#37197;&#32622;&#37096;&#32626;&#30340;&#24378;&#22823;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.00495</link><description>&lt;p&gt;
&#20351;&#29992;&#21512;&#25104;&#20559;&#22909;&#25968;&#25454;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21487;&#37197;&#32622;&#23433;&#20840;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Configurable Safety Tuning of Language Models with Synthetic Preference Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00495
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Configurable Safety Tuning&#65288;CST&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#20559;&#22909;&#25968;&#25454;&#65292;&#22312;&#25512;&#26029;&#26102;&#23454;&#29616;&#23545;LLMs&#30340;&#28789;&#27963;&#23433;&#20840;&#37197;&#32622;&#65292;&#20801;&#35768;&#29992;&#25143;&#26681;&#25454;&#38656;&#35201;&#31105;&#29992;/&#21551;&#29992;&#23433;&#20840;&#20559;&#22909;&#65292;&#19988;&#23454;&#39564;&#34920;&#26126;CST&#25104;&#21151;&#31649;&#29702;&#19981;&#21516;&#30340;&#23433;&#20840;&#37197;&#32622;&#24182;&#20445;&#30041;&#20102;&#21407;&#22987;&#21151;&#33021;&#65292;&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#21487;&#37197;&#32622;&#37096;&#32626;&#30340;&#24378;&#22823;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#25216;&#26415;&#65292;&#22914;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#65292;&#36890;&#36807;&#23558;&#39044;&#23450;&#20041;&#34892;&#20026;&#30828;&#32534;&#30721;&#21040;&#27169;&#22411;&#20013;&#65292;&#38480;&#21046;&#20102;&#29992;&#25143;&#30340;&#25511;&#21046;&#26435;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;Configurable Safety Tuning&#65288;CST&#65289;&#65292;&#23427;&#21033;&#29992;&#21512;&#25104;&#20559;&#22909;&#25968;&#25454;&#26469;&#22686;&#24378;DPO&#65292;&#20197;&#20419;&#36827;&#22312;&#25512;&#26029;&#26102;&#23545;LLMs&#36827;&#34892;&#28789;&#27963;&#30340;&#23433;&#20840;&#37197;&#32622;&#12290;CST&#36890;&#36807;&#24341;&#20837;&#25351;&#23450;&#23433;&#20840;&#37197;&#32622;&#30340;&#31995;&#32479;&#25552;&#31034;&#65292;&#20801;&#35768;LLM&#37096;&#32626;&#32773;&#26681;&#25454;&#38656;&#35201;&#31105;&#29992;/&#21551;&#29992;&#23433;&#20840;&#20559;&#22909;&#65292;&#20165;&#38656;&#26356;&#25913;&#31995;&#32479;&#25552;&#31034;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;CST&#25104;&#21151;&#31649;&#29702;&#19981;&#21516;&#30340;&#23433;&#20840;&#37197;&#32622;&#65292;&#24182;&#20445;&#30041;&#20102;LLMs&#30340;&#21407;&#22987;&#21151;&#33021;&#65292;&#26174;&#31034;&#20986;&#23427;&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#21487;&#37197;&#32622;&#37096;&#32626;&#30340;&#24378;&#22823;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00495v1 Announce Type: cross  Abstract: State-of-the-art language model fine-tuning techniques, such as Direct Preference Optimization (DPO), restrict user control by hard-coding predefined behaviors into the model. To address this, we propose a novel method, Configurable Safety Tuning (CST), that augments DPO using synthetic preference data to facilitate flexible safety configuration of LLMs at inference time. CST overcomes the constraints of vanilla DPO by introducing a system prompt specifying safety configurations, enabling LLM deployers to disable/enable safety preferences based on their need, just changing the system prompt. Our experimental evaluations indicate that CST successfully manages different safety configurations and retains the original functionality of LLMs, showing it is a robust method for configurable deployment. Data and models available at https://github.com/vicgalle/configurable-safety-tuning
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;TEMPLE-MQA&#65292;&#36890;&#36807;&#26500;&#24314;&#26102;&#38388;&#24863;&#30693;&#22270;&#21644;&#25512;&#29702;&#36335;&#24452;&#12289;&#32467;&#26500;&#26816;&#32034;&#21644;&#32852;&#21512;&#25512;&#29702;&#38454;&#27573;&#65292;&#26377;&#25928;&#22320;&#35782;&#21035;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#26102;&#38388;&#32972;&#26223;&#65292;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#36129;&#29486;&#20102;&#19968;&#20010;&#26032;&#25968;&#25454;&#38598;TKEMQA&#12290;</title><link>https://arxiv.org/abs/2404.00492</link><description>&lt;p&gt;
&#26102;&#24577;&#30693;&#35782;&#32534;&#36753;&#19979;&#30340;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
Multi-hop Question Answering under Temporal Knowledge Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00492
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;TEMPLE-MQA&#65292;&#36890;&#36807;&#26500;&#24314;&#26102;&#38388;&#24863;&#30693;&#22270;&#21644;&#25512;&#29702;&#36335;&#24452;&#12289;&#32467;&#26500;&#26816;&#32034;&#21644;&#32852;&#21512;&#25512;&#29702;&#38454;&#27573;&#65292;&#26377;&#25928;&#22320;&#35782;&#21035;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#26102;&#38388;&#32972;&#26223;&#65292;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#36129;&#29486;&#20102;&#19968;&#20010;&#26032;&#25968;&#25454;&#38598;TKEMQA&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#65292;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572; (MQA) &#22312;&#30693;&#35782;&#32534;&#36753; (KE) &#19979;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MQA&#22312;&#22788;&#29702;&#21253;&#21547;&#26174;&#24335;&#26102;&#38388;&#32972;&#26223;&#30340;&#38382;&#39064;&#26102;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21363;&#26102;&#24577;&#30693;&#35782;&#22686;&#24378;&#30340;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572; (TEMPLE-MQA)&#12290;&#19981;&#21516;&#20110;&#20197;&#24448;&#30340;&#26041;&#27861;&#65292;TEMPLE-MQA&#39318;&#20808;&#26500;&#24314;&#19968;&#20010;&#26102;&#38388;&#24863;&#30693;&#22270; (TAG)&#65292;&#20197;&#32467;&#26500;&#21270;&#26041;&#24335;&#23384;&#20648;&#32534;&#36753;&#30693;&#35782;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#25512;&#29702;&#36335;&#24452;&#12289;&#32467;&#26500;&#26816;&#32034;&#21644;&#32852;&#21512;&#25512;&#29702;&#38454;&#27573;&#65292;TEMPLE-MQA&#26377;&#25928;&#22320;&#35782;&#21035;&#38382;&#39064;&#26597;&#35810;&#20013;&#30340;&#26102;&#38388;&#32972;&#26223;&#12290;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;TEMPLE-MQA&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36129;&#29486;&#20102;&#19968;&#20010;&#26032;&#25968;&#25454;&#38598;&#65292;&#21517;&#20026;TKEMQA&#65292;&#19987;&#38376;&#20026;&#24102;&#26377;&#26102;&#38388;&#32422;&#26463;&#30340;MQA&#37327;&#36523;&#23450;&#21046;&#65292;&#20316;&#20026;&#39318;&#20010;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00492v1 Announce Type: cross  Abstract: Multi-hop question answering (MQA) under knowledge editing (KE) has garnered significant attention in the era of large language models. However, existing models for MQA under KE exhibit poor performance when dealing with questions containing explicit temporal contexts. To address this limitation, we propose a novel framework, namely TEMPoral knowLEdge augmented Multi-hop Question Answering (TEMPLE-MQA). Unlike previous methods, TEMPLE-MQA first constructs a time-aware graph (TAG) to store edit knowledge in a structured manner. Then, through our proposed inference path, structural retrieval, and joint reasoning stages, TEMPLE-MQA effectively discerns temporal contexts within the question query. Experiments on benchmark datasets demonstrate that TEMPLE-MQA significantly outperforms baseline models. Additionally, we contribute a new dataset, namely TKEMQA, which serves as the inaugural benchmark tailored specifically for MQA with temporal
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;PROMPT-SAW&#27169;&#22411;&#65292;&#21033;&#29992;&#20851;&#31995;&#24863;&#30693;&#22270;&#26469;&#23454;&#29616;&#25991;&#26412;&#25552;&#31034;&#30340;&#21387;&#32553;&#65292;&#25552;&#39640;&#20102;&#25552;&#31034;&#30340;&#21487;&#35835;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.00489</link><description>&lt;p&gt;
PROMPT-SAW&#65306;&#21033;&#29992;&#20851;&#31995;&#24863;&#30693;&#22270;&#36827;&#34892;&#25991;&#26412;&#25552;&#31034;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
PROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00489
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;PROMPT-SAW&#27169;&#22411;&#65292;&#21033;&#29992;&#20851;&#31995;&#24863;&#30693;&#22270;&#26469;&#23454;&#29616;&#25991;&#26412;&#25552;&#31034;&#30340;&#21387;&#32553;&#65292;&#25552;&#39640;&#20102;&#25552;&#31034;&#30340;&#21487;&#35835;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#22810;&#31181;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#25552;&#31034;&#26159;LLM&#25512;&#29702;&#20013;&#30340;&#22522;&#26412;&#24037;&#20855;&#65292;&#20294;&#25105;&#20204;&#35266;&#23519;&#21040;&#36229;&#38271;&#25552;&#31034;&#20250;&#24102;&#26469;&#26174;&#33879;&#30340;&#25104;&#26412;&#12290;&#29616;&#26377;&#30340;&#21387;&#32553;&#38271;&#25552;&#31034;&#30340;&#23581;&#35797;&#23548;&#33268;&#21387;&#32553;&#25552;&#31034;&#22312;&#21487;&#35835;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#23545;&#25552;&#31034;&#25928;&#29992;&#20135;&#29983;&#26377;&#23475;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PROMPT-SAW&#65306;&#36890;&#36807;&#20851;&#31995;&#24863;&#30693;&#22270;&#36827;&#34892;&#25552;&#31034;&#21387;&#32553;&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#20219;&#21153;&#19981;&#21487;&#30693;&#21644;&#20219;&#21153;&#24863;&#30693;&#25552;&#31034;&#30340;&#26377;&#25928;&#31574;&#30053;&#12290;PROMPT-SAW&#20351;&#29992;&#25552;&#31034;&#30340;&#25991;&#26412;&#20449;&#24687;&#26500;&#24314;&#22270;&#24418;&#65292;&#22312;&#22270;&#24418;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#20803;&#32032;&#65292;&#20174;&#32780;&#24471;&#20986;&#21387;&#32553;&#25552;&#31034;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;GSM8K-AUG&#65292;&#21363;&#29616;&#26377;GSM8k&#22522;&#20934;&#30340;&#25193;&#23637;&#29256;&#26412;&#65292;&#29992;&#20110;&#20219;&#21153;&#19981;&#21487;&#30693;&#25552;&#31034;&#65292;&#20197;&#25552;&#20379;&#20840;&#38754;&#30340;&#35780;&#20272;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00489v1 Announce Type: cross  Abstract: Large language models (LLMs) have shown exceptional abilities for multiple different natural language processing tasks. While prompting is a crucial tool for LLM inference, we observe that there is a significant cost associated with exceedingly lengthy prompts. Existing attempts to compress lengthy prompts lead to sub-standard results in terms of readability and interpretability of the compressed prompt, with a detrimental impact on prompt utility. To address this, we propose PROMPT-SAW: Prompt compresSion via Relation AWare graphs, an effective strategy for prompt compression over task-agnostic and task-aware prompts. PROMPT-SAW uses the prompt's textual information to build a graph, later extracts key information elements in the graph to come up with the compressed prompt. We also propose GSM8K-AUG, i.e., an extended version of the existing GSM8k benchmark for task-agnostic prompts in order to provide a comprehensive evaluation platf
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22122;&#22768;&#24863;&#30693;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#20197;&#21487;&#25193;&#23637;&#30340;&#26041;&#24335;&#20351;&#29992;&#24369;&#26631;&#35760;&#25991;&#26723;&#35757;&#32451;&#25552;&#21462;&#22120;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#22312;&#20225;&#19994;&#22330;&#26223;&#20013;&#35757;&#32451;&#22823;&#37327;&#19981;&#21516;&#25991;&#26723;&#31867;&#22411;&#30340;&#33258;&#23450;&#20041;&#25552;&#21462;&#22120;&#25152;&#38656;&#30340;&#26114;&#36149;&#20154;&#24037;&#26631;&#35760;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2404.00488</link><description>&lt;p&gt;
&#22122;&#22768;&#24863;&#30693;&#30340;&#24067;&#23616;&#24863;&#30693;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Noise-Aware Training of Layout-Aware Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22122;&#22768;&#24863;&#30693;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#20197;&#21487;&#25193;&#23637;&#30340;&#26041;&#24335;&#20351;&#29992;&#24369;&#26631;&#35760;&#25991;&#26723;&#35757;&#32451;&#25552;&#21462;&#22120;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#22312;&#20225;&#19994;&#22330;&#26223;&#20013;&#35757;&#32451;&#22823;&#37327;&#19981;&#21516;&#25991;&#26723;&#31867;&#22411;&#30340;&#33258;&#23450;&#20041;&#25552;&#21462;&#22120;&#25152;&#38656;&#30340;&#26114;&#36149;&#20154;&#24037;&#26631;&#35760;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31687;&#35270;&#35273;&#20016;&#23500;&#30340;&#25991;&#26723;&#65288;VRD&#65289;&#21033;&#29992;&#35270;&#35273;&#29305;&#24449;&#21644;&#35821;&#35328;&#32447;&#32034;&#20256;&#25773;&#20449;&#24687;&#12290;&#35757;&#32451;&#19968;&#20010;&#33021;&#22815;&#20174;&#25991;&#26723;&#20013;&#35782;&#21035;&#21629;&#21517;&#23454;&#20307;&#30340;&#33258;&#23450;&#20041;&#25552;&#21462;&#22120;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#20026;&#25991;&#26412;&#21644;&#35270;&#35273;&#27169;&#24577;&#30340;&#30446;&#26631;&#25991;&#26723;&#23454;&#20363;&#12290;&#22312;&#20225;&#19994;&#22330;&#26223;&#20013;&#65292;&#25105;&#20204;&#24076;&#26395;&#20197;&#21487;&#25193;&#23637;&#30340;&#26041;&#24335;&#20026;&#25104;&#21315;&#19978;&#19975;&#31181;&#19981;&#21516;&#25991;&#26723;&#31867;&#22411;&#35757;&#32451;&#33258;&#23450;&#20041;&#25552;&#21462;&#22120;&#65292;&#36825;&#26159;&#19968;&#20010;&#26114;&#36149;&#30340;&#29942;&#39048;&#12290;&#22312;&#26410;&#26631;&#35760;&#30446;&#26631;&#25991;&#26723;&#23454;&#20363;&#19978;&#39044;&#35757;&#32451;&#25552;&#21462;&#22120;&#27169;&#22411;&#65292;&#28982;&#21518;&#22312;&#20154;&#24037;&#26631;&#35760;&#23454;&#20363;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#26159;&#34892;&#19981;&#36890;&#30340;&#65292;&#22240;&#20026;&#23427;&#36229;&#20986;&#20102;&#20026;&#25552;&#21462;&#22120;&#20998;&#37197;&#30340;&#26368;&#22823;&#20801;&#35768;&#35757;&#32451;&#26102;&#38388;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22122;&#22768;&#24863;&#30693;&#35757;&#32451;&#26041;&#27861;&#65288;NAT&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#22330;&#26223;&#12290;NAT&#21033;&#29992;&#24369;&#26631;&#35760;&#25991;&#26723;&#20197;&#21487;&#25193;&#23637;&#30340;&#26041;&#24335;&#35757;&#32451;&#25552;&#21462;&#22120;&#65292;&#32780;&#19981;&#26159;&#33719;&#21462;&#26114;&#36149;&#30340;&#20154;&#24037;&#26631;&#35760;&#25991;&#26723;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00488v1 Announce Type: cross  Abstract: A visually rich document (VRD) utilizes visual features along with linguistic cues to disseminate information. Training a custom extractor that identifies named entities from a document requires a large number of instances of the target document type annotated at textual and visual modalities. This is an expensive bottleneck in enterprise scenarios, where we want to train custom extractors for thousands of different document types in a scalable way. Pre-training an extractor model on unlabeled instances of the target document type, followed by a fine-tuning step on human-labeled instances does not work in these scenarios, as it surpasses the maximum allowable training time allocated for the extractor. We address this scenario by proposing a Noise-Aware Training method or NAT in this paper. Instead of acquiring expensive human-labeled documents, NAT utilizes weakly labeled documents to train an extractor in a scalable way. To avoid degr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36777;&#35777;&#23545;&#40784;&#65288;DA&#65289;&#26694;&#26550;&#26469;&#35299;&#20915;LLM&#22312;&#38754;&#20020;&#22806;&#37096;&#26377;&#27602;&#25968;&#25454;&#26102;&#30340;&#36866;&#24212;&#24615;&#21464;&#33394;&#40857;&#38382;&#39064;&#65292;&#22686;&#24378;&#20102;&#20854;&#23545;&#25239;&#22806;&#37096;&#25915;&#20987;&#30340;&#23433;&#20840;&#24615;</title><link>https://arxiv.org/abs/2404.00486</link><description>&lt;p&gt;
&#36777;&#35777;&#23545;&#40784;&#65306;&#35299;&#20915;3H&#32039;&#24352;&#19982;LLM&#23433;&#20840;&#23041;&#32961;
&lt;/p&gt;
&lt;p&gt;
Dialectical Alignment: Resolving the Tension of 3H and Security Threats of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00486
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36777;&#35777;&#23545;&#40784;&#65288;DA&#65289;&#26694;&#26550;&#26469;&#35299;&#20915;LLM&#22312;&#38754;&#20020;&#22806;&#37096;&#26377;&#27602;&#25968;&#25454;&#26102;&#30340;&#36866;&#24212;&#24615;&#21464;&#33394;&#40857;&#38382;&#39064;&#65292;&#22686;&#24378;&#20102;&#20854;&#23545;&#25239;&#22806;&#37096;&#25915;&#20987;&#30340;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20852;&#36215;&#65292;&#30830;&#20445;&#23427;&#20204;&#20307;&#29616;&#26377;&#30410;&#12289;&#35802;&#23454;&#21644;&#26080;&#23475;&#65288;3H&#65289;&#21407;&#21017;&#65292;&#21363;&#20154;&#31867;&#23545;&#40784;&#65292;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#23545;&#40784;&#26041;&#27861;&#22914;RLHF&#12289;DPO&#31561;&#26377;&#25928;&#22320;&#24494;&#35843;LLM&#20197;&#21305;&#37197;&#20559;&#22909;&#25968;&#25454;&#38598;&#20013;&#30340;&#20559;&#22909;&#65292;&#20294;&#24448;&#24448;&#20250;&#20351;LLM&#23545;&#39640;&#24230;&#25509;&#21463;&#20154;&#31867;&#36755;&#20837;&#21644;&#22806;&#37096;&#35777;&#25454;&#65292;&#21363;&#20351;&#36825;&#20123;&#20449;&#24687;&#26159;&#26377;&#27602;&#30340;&#12290;&#36825;&#23548;&#33268;LLM&#20542;&#21521;&#20110;&#25104;&#20026;&#36866;&#24212;&#21464;&#33394;&#40857;&#65292;&#24403;&#22806;&#37096;&#35777;&#25454;&#19982;&#20854;&#21442;&#25968;&#24615;&#35760;&#24518;&#20914;&#31361;&#26102;&#12290;&#36825;&#21152;&#21095;&#20102;LLM&#36973;&#21463;&#22806;&#37096;&#26377;&#27602;&#25968;&#25454;&#25915;&#20987;&#30340;&#39118;&#38505;&#65292;&#23545;LLM&#31995;&#32479;&#24212;&#29992;&#65288;&#22914;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65289;&#26500;&#25104;&#20102;&#37325;&#22823;&#23433;&#20840;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65306;&#36777;&#35777;&#23545;&#40784;&#65288;DA&#65289;&#65292;&#23427;&#65288;1&#65289;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21453;&#39304;&#26469;&#30830;&#23450;LLM&#23548;&#33322;&#30456;&#20114;&#25991;&#26412;&#20914;&#31361;&#21644;&#19978;&#19979;&#25991;&#35760;&#24518;&#20914;&#31361;&#30340;&#26368;&#20339;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00486v1 Announce Type: cross  Abstract: With the rise of large language models (LLMs), ensuring they embody the principles of being helpful, honest, and harmless (3H), known as Human Alignment, becomes crucial. While existing alignment methods like RLHF, DPO, etc., effectively fine-tune LLMs to match preferences in the preference dataset, they often lead LLMs to highly receptive human input and external evidence, even when this information is poisoned. This leads to a tendency for LLMs to be Adaptive Chameleons when external evidence conflicts with their parametric memory. This exacerbates the risk of LLM being attacked by external poisoned data, which poses a significant security risk to LLM system applications such as Retrieval-augmented generation (RAG). To address the challenge, we propose a novel framework: Dialectical Alignment (DA), which (1) utilizes AI feedback to identify optimal strategies for LLMs to navigate inter-context conflicts and context-memory conflicts w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#24182;&#20998;&#21035;&#20351;&#29992;&#19977;&#20803;&#32452;&#21644;&#35821;&#35328;&#24314;&#27169;&#30446;&#26631;&#36827;&#34892;&#24494;&#35843;&#30340;&#36866;&#37197;&#22120;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#19968;&#33268;&#24615;&#65292;&#20294;&#26410;&#33021;&#36229;&#36234;GPT-4&#22312;&#24544;&#23454;&#24230;&#21644;&#19968;&#33268;&#24615;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.00484</link><description>&lt;p&gt;
SemEval-2024&#20219;&#21153;2&#20013;&#30340;&#29233;&#19969;&#22561;&#20020;&#24202;NLP&#65306;Fine-tune your model unless you have access to GPT-4
&lt;/p&gt;
&lt;p&gt;
Edinburgh Clinical NLP at SemEval-2024 Task 2: Fine-tune your model unless you have access to GPT-4
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#24182;&#20998;&#21035;&#20351;&#29992;&#19977;&#20803;&#32452;&#21644;&#35821;&#35328;&#24314;&#27169;&#30446;&#26631;&#36827;&#34892;&#24494;&#35843;&#30340;&#36866;&#37197;&#22120;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#19968;&#33268;&#24615;&#65292;&#20294;&#26410;&#33021;&#36229;&#36234;GPT-4&#22312;&#24544;&#23454;&#24230;&#21644;&#19968;&#33268;&#24615;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
NLI4CT&#20219;&#21153;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#31995;&#32479;&#22312;&#39044;&#27979;&#20551;&#35774;&#26159;&#21542;&#21253;&#21547;&#25110;&#19982;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#20013;&#30340;&#35777;&#25454;&#30456;&#30683;&#30462;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24182;&#37319;&#29992;&#22810;&#31181;&#31574;&#30053;&#65292;&#21253;&#25324;&#24605;&#32500;&#38142;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;PEFT&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#24182;&#20998;&#21035;&#20351;&#29992;&#19977;&#20803;&#32452;&#21644;&#35821;&#35328;&#24314;&#27169;&#30446;&#26631;&#36827;&#34892;&#24494;&#35843;&#30340;&#36866;&#37197;&#22120;&#26469;&#25552;&#39640;LLMs&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#21512;&#24182;&#20004;&#20010;PEFT&#36866;&#37197;&#22120;&#21487;&#20197;&#25552;&#39640;LLMs&#30340;F1&#20998;&#25968;&#65288;+0.0346&#65289;&#21644;&#19968;&#33268;&#24615;&#65288;+0.152&#65289;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#26410;&#33021;&#20135;&#29983;&#27604;GPT-4&#26356;&#20934;&#30830;&#30340;&#32467;&#26524;&#65292;&#26080;&#35770;&#26159;&#22312;&#24544;&#23454;&#24230;&#36824;&#26159;&#19968;&#33268;&#24615;&#26041;&#38754;&#12290;&#32508;&#21512;&#19977;&#20010;&#25351;&#26631;&#65292;GPT-4&#22312;&#31454;&#36187;&#20013;&#20197;0.8328&#30340;&#24471;&#20998;&#24182;&#21015;&#31532;&#19968;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#19982;GPT-4&#30340;&#27745;&#26579;&#20998;&#26512;&#26174;&#31034;&#27809;&#26377;&#27979;&#35797;&#25968;&#25454;&#27844;&#38706;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00484v1 Announce Type: new  Abstract: The NLI4CT task assesses Natural Language Inference systems in predicting whether hypotheses entail or contradict evidence from Clinical Trial Reports. In this study, we evaluate various Large Language Models (LLMs) with multiple strategies, including Chain-of-Thought, In-Context Learning, and Parameter-Efficient Fine-Tuning (PEFT). We propose a PEFT method to improve the consistency of LLMs by merging adapters that were fine-tuned separately using triplet and language modelling objectives. We found that merging the two PEFT adapters improves the F1 score (+0.0346) and consistency (+0.152) of the LLMs. However, our novel methods did not produce more accurate results than GPT-4 in terms of faithfulness and consistency. Averaging the three metrics, GPT-4 ranks joint-first in the competition with 0.8328. Finally, our contamination analysis with GPT-4 indicates that there was no test data leakage.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#25163;&#21160;&#26631;&#27880;&#30340;&#29992;&#20110;&#20845;&#31181;&#26031;&#25289;&#22827;&#35821;&#35328;&#30340;&#21629;&#21517;&#23454;&#20307;&#35821;&#26009;&#24211;&#65292;&#25552;&#20379;&#20102;&#20004;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#21010;&#20998;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#30340;&#35782;&#21035;&#12289;&#20998;&#31867;&#12289;&#24341;&#29992;&#35789;&#21270;&#21644;&#38142;&#25509;&#12290;</title><link>https://arxiv.org/abs/2404.00482</link><description>&lt;p&gt;
&#29992;&#20110;&#26031;&#25289;&#22827;&#35821;&#30340;&#36328;&#35821;&#35328;&#21629;&#21517;&#23454;&#20307;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
Cross-lingual Named Entity Corpus for Slavic Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00482
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#25163;&#21160;&#26631;&#27880;&#30340;&#29992;&#20110;&#20845;&#31181;&#26031;&#25289;&#22827;&#35821;&#35328;&#30340;&#21629;&#21517;&#23454;&#20307;&#35821;&#26009;&#24211;&#65292;&#25552;&#20379;&#20102;&#20004;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#21010;&#20998;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#30340;&#35782;&#21035;&#12289;&#20998;&#31867;&#12289;&#24341;&#29992;&#35789;&#21270;&#21644;&#38142;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25163;&#21160;&#27880;&#37322;&#30340;&#21253;&#21547;&#20845;&#31181;&#26031;&#25289;&#22827;&#35821;&#35328;&#65288;&#20445;&#21152;&#21033;&#20122;&#35821;&#12289;&#25463;&#20811;&#35821;&#12289;&#27874;&#20848;&#35821;&#12289;&#26031;&#27931;&#25991;&#23612;&#20122;&#35821;&#12289;&#20420;&#35821;&#21644;&#20044;&#20811;&#20848;&#35821;&#65289;&#21629;&#21517;&#23454;&#20307;&#30340;&#35821;&#26009;&#24211;&#12290;&#36825;&#39033;&#24037;&#20316;&#26159;2017-2023&#24180;&#38388;&#26031;&#25289;&#22827;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#35752;&#20250;&#30340;&#19968;&#31995;&#21015;&#20849;&#20139;&#20219;&#21153;&#30340;&#32467;&#26524;&#12290;&#35813;&#35821;&#26009;&#24211;&#21253;&#21547;&#20102;5017&#20221;&#28085;&#30422;&#19971;&#20010;&#20027;&#39064;&#30340;&#25991;&#26723;&#65292;&#25991;&#26723;&#26631;&#26377;&#20116;&#31867;&#21629;&#21517;&#23454;&#20307;&#65292;&#27599;&#20010;&#23454;&#20307;&#30001;&#31867;&#21035;&#12289;&#24341;&#29992;&#35789;&#21644;&#21807;&#19968;&#36328;&#35821;&#35328;&#26631;&#35782;&#31526;&#25551;&#36848;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#20010;&#35757;&#32451;&#35843;&#25972;&#30340;&#25968;&#25454;&#38598;&#21010;&#20998; - &#21333;&#20010;&#20027;&#39064;&#21010;&#20998;&#21644;&#36328;&#20027;&#39064;&#21010;&#20998;&#12290;&#23545;&#20110;&#27599;&#20010;&#21010;&#20998;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#35774;&#32622;&#20102;&#22522;&#20934;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;XLM-RoBERTa-large&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#25552;&#21450;&#35782;&#21035;&#21644;&#20998;&#31867;&#65292;&#20197;&#21450;mT5-large&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#24341;&#29992;&#35789;&#21270;&#21644;&#38142;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00482v1 Announce Type: cross  Abstract: This paper presents a corpus manually annotated with named entities for six Slavic languages - Bulgarian, Czech, Polish, Slovenian, Russian, and Ukrainian. This work is the result of a series of shared tasks, conducted in 2017-2023 as a part of the Workshops on Slavic Natural Language Processing. The corpus consists of 5 017 documents on seven topics. The documents are annotated with five classes of named entities. Each entity is described by a category, a lemma, and a unique cross-lingual identifier. We provide two train-tune dataset splits - single topic out and cross topics. For each split, we set benchmarks using a transformer-based neural network architecture with the pre-trained multilingual models - XLM-RoBERTa-large for named entity mention recognition and categorization, and mT5-large for named entity lemmatization and linking.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#29983;&#25104;&#26469;&#23454;&#29616;&#35821;&#35328;&#26657;&#20934;&#65292;&#21487;&#20197;&#20351;&#29992;&#25143;&#20570;&#20986;&#26657;&#20934;&#27010;&#29575;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.00474</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Linguistic Calibration of Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00474
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#29983;&#25104;&#26469;&#23454;&#29616;&#35821;&#35328;&#26657;&#20934;&#65292;&#21487;&#20197;&#20351;&#29992;&#25143;&#20570;&#20986;&#26657;&#20934;&#27010;&#29575;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20250;&#22312;&#33258;&#20449;&#24187;&#35273;&#26102;&#23548;&#33268;&#29992;&#25143;&#20570;&#20986;&#27425;&#20248;&#21270;&#30340;&#19979;&#28216;&#20915;&#31574;&#12290;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#21475;&#22836;&#20256;&#36798;&#20854;&#20027;&#24352;&#27491;&#30830;&#27010;&#29575;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#29616;&#26377;&#27169;&#22411;&#26080;&#27861;&#29983;&#25104;&#20855;&#26377;&#26657;&#20934;&#32622;&#20449;&#24230;&#22768;&#26126;&#30340;&#25991;&#26412;&#12290;&#25105;&#20204;&#36890;&#36807;&#20915;&#31574;&#35282;&#24230;&#65292;&#20026;&#38271;&#31687;&#29983;&#25104;&#24418;&#24335;&#30340;&#35821;&#35328;&#26657;&#20934;&#24418;&#24335;&#21270;&#23450;&#20041;&#65306;&#22914;&#26524;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#20351;&#20854;&#29992;&#25143;&#33021;&#22815;&#20570;&#20986;&#26657;&#20934;&#27010;&#29575;&#39044;&#27979;&#65292;&#21017;&#35813;&#27169;&#22411;&#26159;&#35821;&#35328;&#19978;&#26657;&#20934;&#30340;&#12290;&#36825;&#20010;&#23450;&#20041;&#20351;&#24471;&#19968;&#20010;&#35757;&#32451;&#26694;&#26550;&#25104;&#20026;&#21487;&#33021;&#65292;&#20854;&#20013;&#19968;&#20010;&#30417;&#30563;&#24494;&#35843;&#27493;&#39588;&#24341;&#23548;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#21457;&#20986;&#24102;&#26377;&#32622;&#20449;&#24230;&#22768;&#26126;&#30340;&#38271;&#31687;&#29983;&#25104;&#65292;&#35832;&#22914;&#8220;&#25105;&#20272;&#35745;&#26377;30%&#30340;&#26426;&#20250;&#8230;&#8221;&#25110;&#8220;&#25105;&#30830;&#20449;&#8230;&#8221;&#65292;&#28982;&#21518;&#26159;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#27493;&#39588;&#65292;&#22870;&#21169;&#20351;&#29992;&#25143;&#33021;&#22815;&#23545;&#30456;&#20851;&#38382;&#39064;&#25552;&#20379;&#26657;&#20934;&#31572;&#26696;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#23545;Llama 2 7B &#36827;&#34892;&#35821;&#35328;&#26657;&#20934;&#65292;&#24182;&#21457;&#29616;&#22312;&#33258;&#21160;&#21270;&#21644;&#20154;&#31867;&#27979;&#35797;&#20013;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00474v1 Announce Type: cross  Abstract: Language models (LMs) may lead their users to make suboptimal downstream decisions when they confidently hallucinate. This issue can be mitigated by having the LM verbally convey the probability that its claims are correct, but existing models cannot produce text with calibrated confidence statements. Through the lens of decision-making, we formalize linguistic calibration for long-form generations: an LM is linguistically calibrated if its generations enable its users to make calibrated probabilistic predictions. This definition enables a training framework where a supervised finetuning step bootstraps an LM to emit long-form generations with confidence statements such as "I estimate a 30% chance of..." or "I am certain that...", followed by a reinforcement learning step which rewards generations that enable a user to provide calibrated answers to related questions. We linguistically calibrate Llama 2 7B and find in automated and huma
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;NLP&#27169;&#22411;&#20013;&#21516;&#26102;&#22788;&#29702;&#32479;&#35745;&#21644;&#22240;&#26524;&#24615;&#21035;&#20844;&#24179;&#24615;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#32467;&#21512;&#32479;&#35745;&#21644;&#22240;&#26524;&#24615;&#21435;&#20559;&#32622;&#25216;&#26415;&#33021;&#22815;&#26377;&#25928;&#20943;&#23569;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2404.00463</link><description>&lt;p&gt;
&#22312;NLP&#27169;&#22411;&#20013;&#35299;&#20915;&#32479;&#35745;&#21644;&#22240;&#26524;&#24615;&#21035;&#20844;&#24179;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Addressing Both Statistical and Causal Gender Fairness in NLP Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;NLP&#27169;&#22411;&#20013;&#21516;&#26102;&#22788;&#29702;&#32479;&#35745;&#21644;&#22240;&#26524;&#24615;&#21035;&#20844;&#24179;&#24615;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#32467;&#21512;&#32479;&#35745;&#21644;&#22240;&#26524;&#24615;&#21435;&#20559;&#32622;&#25216;&#26415;&#33021;&#22815;&#26377;&#25928;&#20943;&#23569;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#20844;&#24179;&#24615;&#35268;&#23450;&#23545;&#27599;&#20010;&#21463;&#20445;&#25252;&#32676;&#20307;&#26377;&#30456;&#21516;&#30340;&#32467;&#26524;&#65292;&#32780;&#22240;&#26524;&#20844;&#24179;&#24615;&#35201;&#27714;&#27169;&#22411;&#23545;&#20010;&#20307;&#30340;&#39044;&#27979;&#19981;&#21463;&#20854;&#21463;&#20445;&#25252;&#29305;&#24449;&#30340;&#24433;&#21709;&#12290;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#65288;CDA&#65289;&#23545;&#20110;&#20943;&#23569;NLP&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#26159;&#26377;&#25928;&#30340;&#65292;&#28982;&#32780;&#20351;&#29992;CDA&#35757;&#32451;&#30340;&#27169;&#22411;&#36890;&#24120;&#21482;&#22522;&#20110;&#19982;&#22240;&#26524;&#20844;&#24179;&#24615;&#27010;&#24565;&#23494;&#20999;&#30456;&#20851;&#30340;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#65307;&#21516;&#26679;&#65292;&#20026;&#20419;&#36827;&#32479;&#35745;&#20844;&#24179;&#24615;&#32780;&#35774;&#35745;&#30340;&#22522;&#20110;&#25277;&#26679;&#30340;&#26041;&#27861;&#24456;&#23569;&#21463;&#21040;&#22240;&#26524;&#20844;&#24179;&#24615;&#30340;&#35780;&#20272;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22312;NLP&#27169;&#22411;&#20013;&#22788;&#29702;&#24615;&#21035;&#20559;&#35265;&#30340;&#32479;&#35745;&#24615;&#21644;&#22240;&#26524;&#24615;&#21435;&#20559;&#32622;&#26041;&#27861;&#65292;&#21457;&#29616;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#38477;&#20302;&#20559;&#35265;&#65292;&#20294;&#24182;&#19981;&#19968;&#23450;&#20250;&#25913;&#21892;&#20854;&#20182;&#20559;&#35265;&#25351;&#26631;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#32479;&#35745;&#24615;&#21644;&#22240;&#26524;&#24615;&#21435;&#20559;&#32622;&#25216;&#26415;&#30340;&#32452;&#21512;&#33021;&#22815;&#20943;&#23569;&#36890;&#36807;&#36825;&#20004;&#31181;&#31867;&#22411;&#25351;&#26631;&#34913;&#37327;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00463v1 Announce Type: new  Abstract: Statistical fairness stipulates equivalent outcomes for every protected group, whereas causal fairness prescribes that a model makes the same prediction for an individual regardless of their protected characteristics. Counterfactual data augmentation (CDA) is effective for reducing bias in NLP models, yet models trained with CDA are often evaluated only on metrics that are closely tied to the causal fairness notion; similarly, sampling-based methods designed to promote statistical fairness are rarely evaluated for causal fairness. In this work, we evaluate both statistical and causal debiasing methods for gender bias in NLP models, and find that while such methods are effective at reducing bias as measured by the targeted metric, they do not necessarily improve results on other bias metrics. We demonstrate that combinations of statistical and causal debiasing techniques are able to reduce bias measured through both types of metrics.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#20013;&#30340;&#24178;&#20928;&#26631;&#31614;&#25915;&#20987;&#36890;&#36807;&#29305;&#23450;&#25552;&#31034;&#20316;&#20026;&#35302;&#21457;&#22120;&#65292;&#23454;&#29616;&#25104;&#21151;&#30340;&#21516;&#26102;&#30830;&#20445;&#27491;&#30830;&#26631;&#35760;&#26377;&#27602;&#26679;&#26412;&#65292;&#20294;&#20173;&#38754;&#20020;&#35823;&#28608;&#27963;&#38382;&#39064;&#21644;&#25361;&#25112;&#65292;&#38656;&#35201;&#26356;&#39640;&#27604;&#20363;&#30340;&#27602;&#23475;&#12290;</title><link>https://arxiv.org/abs/2404.00461</link><description>&lt;p&gt;
&#20174;&#23545;&#27604;&#20013;&#20986;&#29616;&#30340;&#24555;&#36895;&#26041;&#27861;&#65306;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#20013;&#30340;&#26377;&#25928;&#21644;&#38544;&#34109;&#24178;&#20928;&#26631;&#31614;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Shortcuts Arising from Contrast: Effective and Covert Clean-Label Attacks in Prompt-Based Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00461
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#20013;&#30340;&#24178;&#20928;&#26631;&#31614;&#25915;&#20987;&#36890;&#36807;&#29305;&#23450;&#25552;&#31034;&#20316;&#20026;&#35302;&#21457;&#22120;&#65292;&#23454;&#29616;&#25104;&#21151;&#30340;&#21516;&#26102;&#30830;&#20445;&#27491;&#30830;&#26631;&#35760;&#26377;&#27602;&#26679;&#26412;&#65292;&#20294;&#20173;&#38754;&#20020;&#35823;&#28608;&#27963;&#38382;&#39064;&#21644;&#25361;&#25112;&#65292;&#38656;&#35201;&#26356;&#39640;&#27604;&#20363;&#30340;&#27602;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt-based learning&#33539;&#24335;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#25928;&#21147;&#65292;&#21487;&#20197;&#25552;&#21319;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#23398;&#20064;&#33539;&#24335;&#24050;&#34987;&#35777;&#26126;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#24403;&#21069;&#30340;&#24178;&#20928;&#26631;&#31614;&#25915;&#20987;&#65292;&#21033;&#29992;&#29305;&#23450;&#25552;&#31034;&#20316;&#20026;&#35302;&#21457;&#22120;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#22806;&#37096;&#35302;&#21457;&#22120;&#30340;&#24773;&#20917;&#19979;&#25104;&#21151;&#65292;&#24182;&#30830;&#20445;&#23545;&#26377;&#27602;&#26679;&#26412;&#30340;&#27491;&#30830;&#26631;&#35760;&#65292;&#30456;&#27604;&#26377;&#27602;&#26631;&#31614;&#25915;&#20987;&#26356;&#20855;&#38544;&#34109;&#24615;&#65292;&#20294;&#21478;&#19968;&#26041;&#38754;&#65292;&#23427;&#38754;&#20020;&#30528;&#20005;&#37325;&#30340;&#35823;&#28608;&#27963;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26356;&#22823;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#26356;&#39640;&#27604;&#20363;&#30340;&#27602;&#23475;&#12290;&#36890;&#36807;&#20256;&#32479;&#30340;&#36127;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#24178;&#20928;&#26631;&#31614;&#35774;&#32622;&#20013;&#22312;&#25928;&#21147;&#21644;&#38544;&#34109;&#24615;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#26102;&#65292;&#25105;&#20204;&#21463;&#21040;&#21518;&#38376;&#20805;&#24403;&#24555;&#25463;&#26041;&#24335;&#30340;&#35266;&#24565;&#30340;&#21551;&#21457;&#65292;&#24182;&#20551;&#35774;&#36825;&#19968;&#24555;&#25463;&#26041;&#24335;&#28304;&#20110;t&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00461v1 Announce Type: cross  Abstract: Prompt-based learning paradigm has demonstrated remarkable efficacy in enhancing the adaptability of pretrained language models (PLMs), particularly in few-shot scenarios. However, this learning paradigm has been shown to be vulnerable to backdoor attacks. The current clean-label attack, employing a specific prompt as a trigger, can achieve success without the need for external triggers and ensure correct labeling of poisoned samples, which is more stealthy compared to the poisoned-label attack, but on the other hand, it faces significant issues with false activations and poses greater challenges, necessitating a higher rate of poisoning. Using conventional negative data augmentation methods, we discovered that it is challenging to trade off between effectiveness and stealthiness in a clean-label setting. In addressing this issue, we are inspired by the notion that a backdoor acts as a shortcut and posit that this shortcut stems from t
&lt;/p&gt;</description></item><item><title>NumeroLogic&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#23383;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#25968;&#23383;&#21069;&#21253;&#21547;&#25968;&#23383;&#30340;&#20301;&#25968;&#35745;&#25968;&#65292;&#20026;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23383;&#25512;&#29702;&#33021;&#21147;&#25552;&#20379;&#20102;&#22686;&#24378;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#29983;&#25104;&#23454;&#38469;&#25968;&#23383;&#20043;&#21069;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2404.00459</link><description>&lt;p&gt;
NumeroLogic&#65306;&#22686;&#24378;LLMs&#25968;&#23383;&#25512;&#29702;&#30340;&#25968;&#23383;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00459
&lt;/p&gt;
&lt;p&gt;
NumeroLogic&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#23383;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#25968;&#23383;&#21069;&#21253;&#21547;&#25968;&#23383;&#30340;&#20301;&#25968;&#35745;&#25968;&#65292;&#20026;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23383;&#25512;&#29702;&#33021;&#21147;&#25552;&#20379;&#20102;&#22686;&#24378;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#29983;&#25104;&#23454;&#38469;&#25968;&#23383;&#20043;&#21069;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25351;&#20986;&#65292;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#25968;&#20540;&#25968;&#25454;&#21644;&#25191;&#34892;&#31639;&#26415;&#36816;&#31639;&#26102;&#38754;&#20020;&#22256;&#38590;&#12290;&#25105;&#20204;&#20551;&#35774;&#36825;&#31181;&#38480;&#21046;&#37096;&#20998;&#24402;&#22240;&#20110;&#38750;&#30452;&#35266;&#30340;&#25991;&#26412;&#25968;&#23383;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#35843;&#25972;&#26041;&#27861;&#65292;&#21363;&#22312;&#27599;&#20010;&#25968;&#23383;&#21069;&#21253;&#21547;&#25968;&#23383;&#30340;&#20301;&#25968;&#35745;&#25968;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;"{2:42}"&#20195;&#26367;"42"&#20316;&#20026;&#26032;&#30340;&#26684;&#24335;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#31216;&#20026;NumeroLogic&#65292;&#23427;&#22312;&#25968;&#23383;&#29983;&#25104;&#20013;&#20316;&#20026;&#8220;&#24605;&#32500;&#38142;&#8221;&#25552;&#20379;&#20102;&#39069;&#22806;&#20248;&#21183;&#12290;&#36890;&#36807;&#35201;&#27714;&#27169;&#22411;&#39318;&#20808;&#32771;&#34385;&#25968;&#23383;&#30340;&#20301;&#25968;&#65292;&#23427;&#22686;&#24378;&#20102;&#29983;&#25104;&#23454;&#38469;&#25968;&#23383;&#20043;&#21069;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#25105;&#20204;&#20351;&#29992;&#31639;&#26415;&#20219;&#21153;&#26469;&#23637;&#31034;NumeroLogic&#26684;&#24335;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00459v1 Announce Type: new  Abstract: Language models struggle with handling numerical data and performing arithmetic operations. We hypothesize that this limitation can be partially attributed to non-intuitive textual numbers representation. When a digit is read or generated by a causal language model it does not know its place value (e.g. thousands vs. hundreds) until the entire number is processed. To address this issue, we propose a simple adjustment to how numbers are represented by including the count of digits before each number. For instance, instead of "42", we suggest using "{2:42}" as the new format. This approach, which we term NumeroLogic, offers an added advantage in number generation by serving as a Chain of Thought (CoT). By requiring the model to consider the number of digits first, it enhances the reasoning process before generating the actual number. We use arithmetic tasks to demonstrate the effectiveness of the NumeroLogic formatting. We further demonstr
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#20010;&#22810;&#39046;&#22495;&#12289;&#22810;&#20219;&#21153;&#26694;&#26550;&#65292;&#24110;&#21161;&#36873;&#25321;&#26368;&#26377;&#25928;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#23884;&#20837;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2404.00458</link><description>&lt;p&gt;
&#36229;&#36234;&#19968;&#20992;&#20999;&#65306;&#22810;&#39046;&#22495;&#12289;&#22810;&#20219;&#21153;&#26694;&#26550;&#29992;&#20110;&#23884;&#20837;&#27169;&#22411;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Beyond One-Size-Fits-All: Multi-Domain, Multi-Task Framework for Embedding Model Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00458
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#20010;&#22810;&#39046;&#22495;&#12289;&#22810;&#20219;&#21153;&#26694;&#26550;&#65292;&#24110;&#21161;&#36873;&#25321;&#26368;&#26377;&#25928;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#23884;&#20837;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#26041;&#27861;&#65292;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#26694;&#26550;&#65292;&#24110;&#21161;&#36873;&#25321;&#36866;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#30340;&#26368;&#26377;&#25928;&#30340;&#23884;&#20837;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#19987;&#26377;&#21644;&#24320;&#28304;&#32534;&#30721;&#22120;&#27169;&#22411;&#22823;&#37327;&#22686;&#21152;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00458v1 Announce Type: new  Abstract: This position paper proposes a systematic approach towards developing a framework to help select the most effective embedding models for natural language processing (NLP) tasks, addressing the challenge posed by the proliferation of both proprietary and open-source encoder models.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MetaIE&#26694;&#26550;&#65292;&#36890;&#36807;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#31526;&#21495;&#33976;&#39311;&#65292;&#26500;&#24314;&#19968;&#20010;&#23567;&#22411;LM&#20316;&#20026;&#20803;&#27169;&#22411;&#65292;&#23454;&#29616;&#23545;&#25152;&#26377;&#31867;&#22411;&#30340;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#30340;&#39640;&#25928;&#36866;&#24212;&#12290;</title><link>https://arxiv.org/abs/2404.00457</link><description>&lt;p&gt;
MetaIE: &#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#28860;&#20803;&#27169;&#22411;&#65292;&#38024;&#23545;&#25152;&#26377;&#31867;&#22411;&#30340;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
MetaIE: Distilling a Meta Model from LLM for All Kinds of Information Extraction Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00457
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MetaIE&#26694;&#26550;&#65292;&#36890;&#36807;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#31526;&#21495;&#33976;&#39311;&#65292;&#26500;&#24314;&#19968;&#20010;&#23567;&#22411;LM&#20316;&#20026;&#20803;&#27169;&#22411;&#65292;&#23454;&#29616;&#23545;&#25152;&#26377;&#31867;&#22411;&#30340;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#30340;&#39640;&#25928;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#25277;&#21462;&#65288;IE&#65289;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#19968;&#20010;&#22522;&#30784;&#39046;&#22495;&#65292;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#65292;&#21363;&#20351;&#20351;&#29992;&#20102;&#20855;&#26377;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20063;&#26080;&#27861;&#20987;&#36133;&#22312;&#38750;&#24120;&#23567;&#30340;IE&#25968;&#25454;&#38598;&#19978;&#35843;&#25972;&#20102;&#30340;&#23567;&#22411;LM&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#35832;&#22914;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25277;&#21462;&#31561;IE&#20219;&#21153;&#37117;&#38598;&#20013;&#22312;&#25552;&#21462;&#37325;&#35201;&#20449;&#24687;&#65292;&#36825;&#21487;&#20197;&#34987;&#24418;&#24335;&#21270;&#20026;&#26631;&#31614;&#21040;&#36328;&#24230;&#30340;&#21305;&#37197;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;MetaIE&#65292;&#36890;&#36807;&#23398;&#20064;&#25552;&#21462;&#8220;&#37325;&#35201;&#20449;&#24687;&#8221;&#65288;&#21363;IE&#30340;&#20803;&#29702;&#35299;&#65289;&#65292;&#26469;&#26500;&#24314;&#19968;&#20010;&#23567;&#22411;LM&#20316;&#20026;&#20803;&#27169;&#22411;&#65292;&#20174;&#32780;&#20351;&#24471;&#36825;&#20010;&#20803;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#19988;&#39640;&#25928;&#22320;&#36866;&#24212;&#21508;&#31181;IE&#20219;&#21153;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;MetaIE&#36890;&#36807;&#31526;&#21495;&#33976;&#39311;&#20174;LLM&#20013;&#33719;&#21462;&#23567;&#22411;LM&#65292;&#36981;&#24490;&#26631;&#31614;&#21040;&#36328;&#24230;&#30340;&#26041;&#26696;&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;&#65292;&#25105;&#20204;&#30340;&#23454;&#29616;&#20013;&#30340;OpenWebText&#65289;&#20013;&#23545;&#21477;&#23376;&#36827;&#34892;&#37319;&#26679;&#26500;&#24314;&#33976;&#39311;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#31034;&#19968;&#20010;LLM&#26469;&#35782;&#21035;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00457v1 Announce Type: new  Abstract: Information extraction (IE) is a fundamental area in natural language processing where prompting large language models (LLMs), even with in-context examples, cannot defeat small LMs tuned on very small IE datasets. We observe that IE tasks, such as named entity recognition and relation extraction, all focus on extracting important information, which can be formalized as a label-to-span matching. In this paper, we propose a novel framework MetaIE to build a small LM as meta-model by learning to extract "important information", i.e., the meta-understanding of IE, so that this meta-model can be adapted to all kind of IE tasks effectively and efficiently. Specifically, MetaIE obtains the small LM via a symbolic distillation from an LLM following the label-to-span scheme. We construct the distillation dataset via sampling sentences from language model pre-training datasets (e.g., OpenWebText in our implementation) and prompting an LLM to iden
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#8220;&#35268;&#21010;&#19982;&#26816;&#32034;&#8221;&#21644;&#8220;&#32534;&#36753;&#19982;&#30830;&#35748;&#8221;&#33539;&#24335;&#65292;&#36890;&#36807;&#31070;&#32463;&#26816;&#32034;&#27169;&#22359;&#21644;LLM-based&#26597;&#35810;&#35268;&#21010;&#22120;&#25552;&#39640;&#20102;&#24037;&#20855;&#21033;&#29992;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2404.00450</link><description>&lt;p&gt;
&#35268;&#21010;&#21644;&#32534;&#36753;&#26816;&#32034;&#20197;&#22686;&#24378;&#24037;&#20855;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Planning and Editing What You Retrieve for Enhanced Tool Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00450
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#8220;&#35268;&#21010;&#19982;&#26816;&#32034;&#8221;&#21644;&#8220;&#32534;&#36753;&#19982;&#30830;&#35748;&#8221;&#33539;&#24335;&#65292;&#36890;&#36807;&#31070;&#32463;&#26816;&#32034;&#27169;&#22359;&#21644;LLM-based&#26597;&#35810;&#35268;&#21010;&#22120;&#25552;&#39640;&#20102;&#24037;&#20855;&#21033;&#29992;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#23558;&#22806;&#37096;&#24037;&#20855;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38598;&#25104;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#25171;&#24320;&#20102;&#26032;&#30340;&#39046;&#22495;&#65292;&#24212;&#29992;&#33539;&#22260;&#28085;&#30422;&#25968;&#23398;&#25512;&#29702;&#12289;&#20195;&#30721;&#29983;&#25104;&#22120;&#21644;&#26234;&#33021;&#21161;&#25163;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#31616;&#21333;&#30340;&#19968;&#27425;&#24615;&#26816;&#32034;&#31574;&#30053;&#65292;&#26080;&#27861;&#26377;&#25928;&#20934;&#30830;&#22320;&#31579;&#36873;&#30456;&#20851;&#24037;&#20855;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#35268;&#21010;&#19982;&#26816;&#32034;&#65288;P&amp;R&#65289;&#8221;&#21644;&#8220;&#32534;&#36753;&#19982;&#30830;&#35748;&#65288;E&amp;G&#65289;&#8221;&#33539;&#24335;&#30340;&#27169;&#22411;&#65292;&#21253;&#25324;&#20102;&#31070;&#32463;&#26816;&#32034;&#27169;&#22359;&#21644;&#22522;&#20110;LLM&#30340;&#26597;&#35810;&#35268;&#21010;&#22120;&#65292;&#20197;&#22686;&#24378;&#24037;&#20855;&#21033;&#29992;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00450v1 Announce Type: new  Abstract: Recent advancements in integrating external tools with Large Language Models (LLMs) have opened new frontiers, with applications in mathematical reasoning, code generators, and smart assistants. However, existing methods, relying on simple one-time retrieval strategies, fall short on effectively and accurately shortlisting relevant tools. This paper introduces a novel \modelname (\modelmeaning) approach, encompassing ``Plan-and-Retrieve (P\&amp;R)'' and ``Edit-and-Ground (E\&amp;G)'' paradigms. The P\&amp;R paradigm consists of a neural retrieval module for shortlisting relevant tools and an LLM-based query planner that decomposes complex queries into actionable tasks, enhancing the effectiveness of tool utilization. The E\&amp;G paradigm utilizes LLMs to enrich tool descriptions based on user scenarios, bridging the gap between user queries and tool functionalities. Experiment results demonstrate that these paradigms significantly improve the recall an
&lt;/p&gt;</description></item><item><title>DOCMASTER&#26159;&#19968;&#20010;&#32479;&#19968;&#24179;&#21488;&#65292;&#26088;&#22312;&#20026;&#25991;&#26723;&#38382;&#31572;&#25552;&#20379;&#27880;&#37322;&#12289;&#35757;&#32451;&#21644;&#25512;&#26029;&#26381;&#21153;&#65292;&#25903;&#25345;&#29992;&#25143;&#22312;PDF&#25991;&#26723;&#20013;&#36755;&#20837;&#38382;&#39064;&#24182;&#31361;&#20986;&#26174;&#31034;&#25991;&#26412;&#27573;&#20316;&#20026;&#31572;&#26696;&#65292;&#21516;&#26102;&#20445;&#25252;&#38544;&#31169;&#12290;</title><link>https://arxiv.org/abs/2404.00439</link><description>&lt;p&gt;
DOCMASTER: &#19968;&#20010;&#32479;&#19968;&#30340;&#24179;&#21488;&#65292;&#29992;&#20110;&#25991;&#26723;&#38382;&#31572;&#20013;&#30340;&#27880;&#37322;&#12289;&#35757;&#32451;&#21644;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
DOCMASTER: A Unified Platform for Annotation, Training, &amp; Inference in Document Question-Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00439
&lt;/p&gt;
&lt;p&gt;
DOCMASTER&#26159;&#19968;&#20010;&#32479;&#19968;&#24179;&#21488;&#65292;&#26088;&#22312;&#20026;&#25991;&#26723;&#38382;&#31572;&#25552;&#20379;&#27880;&#37322;&#12289;&#35757;&#32451;&#21644;&#25512;&#26029;&#26381;&#21153;&#65292;&#25903;&#25345;&#29992;&#25143;&#22312;PDF&#25991;&#26723;&#20013;&#36755;&#20837;&#38382;&#39064;&#24182;&#31361;&#20986;&#26174;&#31034;&#25991;&#26412;&#27573;&#20316;&#20026;&#31572;&#26696;&#65292;&#21516;&#26102;&#20445;&#25252;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#24212;&#29992;&#20110;PDF&#25991;&#26723;&#23545;&#20110;&#21508;&#31181;&#19994;&#21153;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#22312;&#20225;&#19994;&#20013;&#35757;&#32451;&#27492;&#31867;&#27169;&#22411;&#30340;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#12290;&#36825;&#20123;&#25361;&#25112;&#21253;&#25324;&#20351;&#29992;PDF&#26684;&#24335;&#30340;&#22797;&#26434;&#24615;&#65292;&#38656;&#35201;&#35299;&#26512;&#25991;&#26412;&#21644;&#24067;&#23616;&#20449;&#24687;&#20197;&#25972;&#29702;&#35757;&#32451;&#25968;&#25454;&#65292;&#20197;&#21450;&#32570;&#20047;&#20445;&#25252;&#38544;&#31169;&#30340;&#27880;&#37322;&#24037;&#20855;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;DOCMASTER&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#24179;&#21488;&#65292;&#29992;&#20110;&#27880;&#37322;PDF&#25991;&#26723;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#25512;&#26029;&#65292;&#19987;&#20026;&#25991;&#26723;&#38382;&#31572;&#32780;&#35774;&#35745;&#12290;&#27880;&#37322;&#30028;&#38754;&#20351;&#29992;&#25143;&#33021;&#22815;&#36755;&#20837;&#38382;&#39064;&#24182;&#22312;PDF&#25991;&#20214;&#20013;&#31361;&#20986;&#26174;&#31034;&#25991;&#26412;&#27573;&#20316;&#20026;&#31572;&#26696;&#65292;&#21516;&#26102;&#20445;&#23384;&#24067;&#23616;&#20449;&#24687;&#21644;&#25991;&#26412;&#27573;&#12290;&#27492;&#22806;&#65292;DOCMASTER&#25903;&#25345;&#26368;&#20808;&#36827;&#30340;&#24067;&#23616;&#24863;&#30693;&#21644;&#25991;&#26412;&#27169;&#22411;&#65292;&#29992;&#20110;&#20840;&#38754;&#30340;&#35757;&#32451;&#30446;&#30340;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#30001;&#20110;&#27880;&#37322;&#12289;&#35757;&#32451;&#21644;&#25512;&#26029;&#22312;&#35774;&#22791;&#19978;&#36827;&#34892;&#65292;&#22240;&#27492;&#36824;&#20445;&#25252;&#20102;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00439v1 Announce Type: new  Abstract: The application of natural language processing models to PDF documents is pivotal for various business applications yet the challenge of training models for this purpose persists in businesses due to specific hurdles. These include the complexity of working with PDF formats that necessitate parsing text and layout information for curating training data and the lack of privacy-preserving annotation tools. This paper introduces DOCMASTER, a unified platform designed for annotating PDF documents, model training, and inference, tailored to document question-answering. The annotation interface enables users to input questions and highlight text spans within the PDF file as answers, saving layout information and text spans accordingly. Furthermore, DOCMASTER supports both state-of-the-art layout-aware and text models for comprehensive training purposes. Importantly, as annotations, training, and inference occur on-device, it also safeguards pr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#27861;&#24459;&#25991;&#26412;&#20998;&#31867;&#31995;&#32479;&#65292;&#20351;&#27169;&#22411;&#30340;&#20915;&#31574;&#21464;&#24471;&#21487;&#29702;&#35299;&#32473;&#26368;&#32456;&#29992;&#25143;</title><link>https://arxiv.org/abs/2404.00437</link><description>&lt;p&gt;
&#21033;&#29992;&#26641;&#20272;&#35745;&#22120;&#33258;&#21160;&#35299;&#37322;&#35199;&#29677;&#29273;&#27861;&#24459;&#21028;&#20915;&#22312;&#20381;&#36182;&#21496;&#27861;&#31649;&#36758;&#30340;&#27861;&#24459;&#31867;&#21035;&#20013;&#30340;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Automatic explanation of the classification of Spanish legal judgments in jurisdiction-dependent law categories with tree estimators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#27861;&#24459;&#25991;&#26412;&#20998;&#31867;&#31995;&#32479;&#65292;&#20351;&#27169;&#22411;&#30340;&#20915;&#31574;&#21464;&#24471;&#21487;&#29702;&#35299;&#32473;&#26368;&#32456;&#29992;&#25143;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#27861;&#24459;&#25991;&#26412;&#20998;&#31867;&#31995;&#32479;&#24050;&#32463;&#34987;&#25552;&#20986;&#22312;&#25991;&#29486;&#20013;&#65292;&#20197;&#35299;&#20915;&#30693;&#35782;&#20174;&#21028;&#20915;&#20013;&#25552;&#21462;&#24182;&#26816;&#27979;&#20854;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#23427;&#20204;&#30340;&#27169;&#22411;&#26159;&#21487;&#35299;&#37322;&#30340;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#31995;&#32479;&#37117;&#26159;&#40657;&#30418;&#30340;&#12290;&#36825;&#21487;&#33021;&#24341;&#21457;&#23545;&#23427;&#20204;&#21487;&#20449;&#24230;&#30340;&#25285;&#24551;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#65292;&#20197;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#23545;&#27861;&#24459;&#25991;&#26412;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20915;&#31574;&#20013;&#28041;&#21450;&#30340;&#29305;&#24449;&#21644;&#26641;&#32467;&#26500;&#30340;&#20915;&#31574;&#36335;&#24452;&#30340;&#38408;&#20540;&#20998;&#21449;&#20540;&#65292;&#24182;&#20197;&#33258;&#28982;&#35821;&#35328;&#30340;&#26041;&#24335;&#21521;&#29992;&#25143;&#21576;&#29616;&#36825;&#20123;&#20449;&#24687;&#12290;&#36825;&#26159;&#31532;&#19968;&#39033;&#20851;&#20110;&#33258;&#21160;&#20998;&#26512;&#27861;&#24459;&#25991;&#26412;&#30340;&#24037;&#20316;&#65292;&#32467;&#21512;NLP&#21644;ML&#20197;&#21450;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#20197;&#20351;&#27169;&#22411;&#30340;&#20915;&#31574;&#33258;&#21160;&#21464;&#24471;&#21487;&#29702;&#35299;&#32473;&#26368;&#32456;&#29992;&#25143;&#12290;&#27492;&#22806;&#65292;&#27861;&#24459;&#19987;&#23478;&#24050;&#32463;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#20123;&#30693;&#35782;&#20063;&#24050;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00437v1 Announce Type: cross  Abstract: Automatic legal text classification systems have been proposed in the literature to address knowledge extraction from judgments and detect their aspects. However, most of these systems are black boxes even when their models are interpretable. This may raise concerns about their trustworthiness. Accordingly, this work contributes with a system combining Natural Language Processing (NLP) with Machine Learning (ML) to classify legal texts in an explainable manner. We analyze the features involved in the decision and the threshold bifurcation values of the decision paths of tree structures and present this information to the users in natural language. This is the first work on automatic analysis of legal texts combining NLP and ML along with Explainable Artificial Intelligence techniques to automatically make the models' decisions understandable to end users. Furthermore, legal experts have validated our solution, and this knowledge has al
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#31574;&#21010;&#20102;&#19968;&#20010;&#21517;&#20026;Compun&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#36890;&#36807;&#25991;&#26412;&#25552;&#31034;&#21644;&#22270;&#20687;&#36873;&#25321;&#20219;&#21153;&#35780;&#20272;VLMs&#22312;&#29702;&#35299;&#22797;&#21512;&#21517;&#35789;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#24182;&#23637;&#29616;&#20102;CLIP&#22312;&#26576;&#20123;&#31867;&#22411;&#30340;&#22797;&#21512;&#21517;&#35789;&#29702;&#35299;&#19978;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.00419</link><description>&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#29702;&#35299;&#22797;&#21512;&#21517;&#35789;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Vision-Language Models Understand Compound Nouns?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00419
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#31574;&#21010;&#20102;&#19968;&#20010;&#21517;&#20026;Compun&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#36890;&#36807;&#25991;&#26412;&#25552;&#31034;&#21644;&#22270;&#20687;&#36873;&#25321;&#20219;&#21153;&#35780;&#20272;VLMs&#22312;&#29702;&#35299;&#22797;&#21512;&#21517;&#35789;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#24182;&#23637;&#29616;&#20102;CLIP&#22312;&#26576;&#20123;&#31867;&#22411;&#30340;&#22797;&#21512;&#21517;&#35789;&#29702;&#35299;&#19978;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#35789;&#27719;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22914;CLIP&#65292;&#20351;&#29992;&#23545;&#27604;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#65292;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26032;&#25991;&#26412;&#21040;&#22270;&#20687;&#26816;&#32034;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;VLMs&#26159;&#21542;&#20687;&#29702;&#35299;&#21517;&#35789;&#65288;&#22914;&#23454;&#39564;&#23460;&#65289;&#19968;&#26679;&#29702;&#35299;&#22797;&#21512;&#21517;&#35789;&#65288;CNs&#65289;&#65288;&#22914;&#23454;&#39564;&#23460;&#22806;&#22871;&#65289;&#65311;&#25105;&#20204;&#31574;&#21010;&#20102;&#19968;&#20010;&#21517;&#20026;Compun&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#21253;&#21547;400&#20010;&#29420;&#29305;&#19988;&#24120;&#29992;&#30340;CNs&#65292;&#20197;&#35780;&#20272;VLMs&#22312;&#35299;&#37322;CNs&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;Compun&#22522;&#20934;&#27979;&#35797;&#25361;&#25112;VLM&#36827;&#34892;&#25991;&#26412;&#21040;&#22270;&#20687;&#26816;&#32034;&#65292;&#32473;&#23450;&#19968;&#27573;&#21253;&#21547;CN&#30340;&#25991;&#26412;&#25552;&#31034;&#65292;&#20219;&#21153;&#26159;&#20174;&#19968;&#23545;&#26174;&#31034;&#26500;&#25104;CN&#30340;&#21517;&#35789;&#30340;&#24178;&#25200;&#22270;&#20687;&#20013;&#36873;&#25321;&#27491;&#30830;&#26174;&#31034;&#35813;CN&#30340;&#22270;&#20687;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#65292;&#31361;&#20986;CLIP&#23545;&#26576;&#20123;&#31867;&#22411;&#30340;CN&#30340;&#29702;&#35299;&#33021;&#21147;&#26377;&#38480;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36229;&#36234;CLIP&#31867;&#27169;&#22411;&#24191;&#27867;&#20351;&#29992;&#30340;&#25163;&#20889;&#27169;&#26495;&#30340;&#26367;&#20195;&#26694;&#26550;&#12290;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22810;&#20010;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00419v1 Announce Type: cross  Abstract: Open-vocabulary vision-language models (VLMs) like CLIP, trained using contrastive loss, have emerged as a promising new paradigm for text-to-image retrieval. However, do VLMs understand compound nouns (CNs) (e.g., lab coat) as well as they understand nouns (e.g., lab)? We curate Compun, a novel benchmark with 400 unique and commonly used CNs, to evaluate the effectiveness of VLMs in interpreting CNs. The Compun benchmark challenges a VLM for text-to-image retrieval where, given a text prompt with a CN, the task is to select the correct image that shows the CN among a pair of distractor images that show the constituent nouns that make up the CN. Next, we perform an in-depth analysis to highlight CLIPs' limited understanding of certain types of CNs. Finally, we present an alternative framework that moves beyond hand-written templates for text prompts widely used by CLIP-like models. We employ a Large Language Model to generate multiple 
&lt;/p&gt;</description></item><item><title>CoDa&#26159;&#19968;&#31181;&#38024;&#23545;&#20302;&#36164;&#28304;NLP&#25552;&#20986;&#30340;&#22522;&#20110;&#32422;&#26463;&#29983;&#25104;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#36890;&#36807;&#25552;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#31526;&#21512;&#19968;&#32452;&#32422;&#26463;&#30340;&#25991;&#26412;&#65292;&#26377;&#25928;&#25552;&#21319;&#25968;&#25454;&#22686;&#24378;&#25928;&#26524;&#65292;&#36991;&#20813;&#27169;&#22411;&#20559;&#21521;&#23569;&#37327;&#35757;&#32451;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2404.00415</link><description>&lt;p&gt;
CoDa:&#22522;&#20110;&#32422;&#26463;&#29983;&#25104;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#29992;&#20110;&#20302;&#36164;&#28304;NLP
&lt;/p&gt;
&lt;p&gt;
CoDa: Constrained Generation based Data Augmentation for Low-Resource NLP
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00415
&lt;/p&gt;
&lt;p&gt;
CoDa&#26159;&#19968;&#31181;&#38024;&#23545;&#20302;&#36164;&#28304;NLP&#25552;&#20986;&#30340;&#22522;&#20110;&#32422;&#26463;&#29983;&#25104;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#36890;&#36807;&#25552;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#31526;&#21512;&#19968;&#32452;&#32422;&#26463;&#30340;&#25991;&#26412;&#65292;&#26377;&#25928;&#25552;&#21319;&#25968;&#25454;&#22686;&#24378;&#25928;&#26524;&#65292;&#36991;&#20813;&#27169;&#22411;&#20559;&#21521;&#23569;&#37327;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;CoDa&#65288;&#22522;&#20110;&#32422;&#26463;&#29983;&#25104;&#30340;&#25968;&#25454;&#22686;&#24378;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#25511;&#12289;&#26377;&#25928;&#19988;&#26080;&#38656;&#35757;&#32451;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#36866;&#29992;&#20110;&#20302;&#36164;&#28304;&#65288;&#25968;&#25454;&#31232;&#32570;&#65289;NLP&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#25552;&#31034;&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36981;&#24490;&#25351;&#20196;&#26469;&#29983;&#25104;&#28385;&#36275;&#19968;&#32452;&#32422;&#26463;&#30340;&#25991;&#26412;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#20302;&#36164;&#28304;&#25968;&#25454;&#38598;&#20013;&#30340;&#27599;&#20010;&#23454;&#20363;&#20013;&#25552;&#21462;&#19968;&#32452;&#31616;&#21333;&#30340;&#32422;&#26463;&#65292;&#23558;&#20854;&#34920;&#36798;&#20026;&#25552;&#31034;&#35821;&#65292;&#20197;&#20419;&#20351;LLM&#29983;&#25104;&#26032;&#39062;&#19988;&#22810;&#26679;&#21270;&#30340;&#35757;&#32451;&#23454;&#20363;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#31526;&#21512;&#19979;&#28216;&#25968;&#25454;&#38598;&#20013;&#31616;&#21333;&#32422;&#26463;&#30340;&#21512;&#25104;&#25968;&#25454;&#21487;&#20197;&#20316;&#20026;&#39640;&#25928;&#30340;&#22686;&#24378;&#65292;&#32780;CoDa&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#22797;&#26434;&#35299;&#30721;&#26102;&#32422;&#26463;&#29983;&#25104;&#25216;&#26415;&#25110;&#20351;&#29992;&#22797;&#26434;&#31639;&#27861;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#36825;&#26679;&#21487;&#20197;&#36991;&#20813;&#27169;&#22411;&#20559;&#21521;&#20110;&#23569;&#37327;&#35757;&#32451;&#23454;&#20363;&#12290;&#27492;&#22806;&#65292;CoDa &#26159;&#39318;&#20010;&#25552;&#20379;&#29992;&#25143;&#26126;&#30830;&#25511;&#21046;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00415v1 Announce Type: new  Abstract: We present CoDa (Constrained Generation based Data Augmentation), a controllable, effective, and training-free data augmentation technique for low-resource (data-scarce) NLP. Our approach is based on prompting off-the-shelf instruction-following Large Language Models (LLMs) for generating text that satisfies a set of constraints. Precisely, we extract a set of simple constraints from every instance in the low-resource dataset and verbalize them to prompt an LLM to generate novel and diverse training instances. Our findings reveal that synthetic data that follows simple constraints in the downstream dataset act as highly effective augmentations, and CoDa can achieve this without intricate decoding-time constrained generation techniques or fine-tuning with complex algorithms that eventually make the model biased toward the small number of training instances. Additionally, CoDa is the first framework that provides users explicit control ove
&lt;/p&gt;</description></item><item><title>TACO&#26159;&#31532;&#19968;&#20010;&#21033;&#29992;1,814&#26465;&#25512;&#25991;&#26500;&#24314;&#30340;Twitter Arguments&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;200&#22330;&#23436;&#25972;&#23545;&#35805;&#65292;&#20845;&#20010;&#20027;&#39064;&#65292;&#20855;&#26377;0.718&#30340;Krippendorff's alpha&#19968;&#33268;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#27880;&#37322;&#26694;&#26550;&#26469;&#23450;&#20041;&#21644;&#35782;&#21035;&#35770;&#28857;&#32467;&#26500;&#35201;&#32032;&#12290;</title><link>https://arxiv.org/abs/2404.00406</link><description>&lt;p&gt;
TACO -- Twitter Arguments from COnversations
&lt;/p&gt;
&lt;p&gt;
TACO -- Twitter Arguments from COnversations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00406
&lt;/p&gt;
&lt;p&gt;
TACO&#26159;&#31532;&#19968;&#20010;&#21033;&#29992;1,814&#26465;&#25512;&#25991;&#26500;&#24314;&#30340;Twitter Arguments&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;200&#22330;&#23436;&#25972;&#23545;&#35805;&#65292;&#20845;&#20010;&#20027;&#39064;&#65292;&#20855;&#26377;0.718&#30340;Krippendorff's alpha&#19968;&#33268;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#27880;&#37322;&#26694;&#26550;&#26469;&#23450;&#20041;&#21644;&#35782;&#21035;&#35770;&#28857;&#32467;&#26500;&#35201;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Twitter&#24050;&#32463;&#25104;&#20026;&#20840;&#29699;&#21442;&#19982;&#22312;&#32447;&#23545;&#35805;&#30340;&#20013;&#24515;&#65292;&#20063;&#25104;&#20026;&#21508;&#20010;&#23398;&#31185;&#30740;&#31350;&#35821;&#26009;&#24211;&#30340;&#37325;&#35201;&#26469;&#28304;&#65292;&#36825;&#20123;&#23398;&#31185;&#24050;&#32463;&#24847;&#35782;&#21040;&#20854;&#29992;&#25143;&#29983;&#25104;&#30340;&#20869;&#23481;&#30340;&#37325;&#35201;&#24615;&#12290;&#35770;&#28857;&#25366;&#25496;&#26159;&#22788;&#29702;&#21644;&#29702;&#35299;&#22312;&#32447;&#35805;&#35821;&#30340;&#37325;&#35201;&#20998;&#26512;&#20219;&#21153;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23427;&#26088;&#22312;&#35782;&#21035;&#35770;&#28857;&#30340;&#32467;&#26500;&#35201;&#32032;&#65292;&#34920;&#31034;&#20026;&#20449;&#24687;&#21644;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35201;&#32032;&#24182;&#19981;&#26159;&#38745;&#24577;&#30340;&#65292;&#21487;&#33021;&#38656;&#35201;&#22312;&#25152;&#22312;&#23545;&#35805;&#20013;&#35774;&#32622;&#19978;&#19979;&#25991;&#65292;&#28982;&#32780;&#32570;&#20047;&#35299;&#20915;Twitter&#19978;&#36825;&#19968;&#21160;&#24577;&#26041;&#38754;&#30340;&#25968;&#25454;&#21644;&#27880;&#37322;&#26694;&#26550;&#12290;&#25105;&#20204;&#36129;&#29486;&#20102;TACO&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21033;&#29992;1,814&#26465;&#28085;&#30422;200&#22330;&#23436;&#25972;&#23545;&#35805;&#12289;&#28085;&#30422;&#20845;&#20010;&#24322;&#36136;&#20027;&#39064;&#30340;&#25512;&#25991;&#30340;Twitter Arguments&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#20845;&#21517;&#19987;&#23478;&#20043;&#38388;&#20197;0.718&#30340;Krippendorff's alpha&#36798;&#25104;&#19968;&#33268;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25105;&#20204;&#30340;&#27880;&#37322;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#26469;&#33258;&#21073;&#26725;&#35789;&#20856;&#30340;&#23450;&#20041;&#65292;&#20197;&#23450;&#20041;&#21644;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00406v1 Announce Type: cross  Abstract: Twitter has emerged as a global hub for engaging in online conversations and as a research corpus for various disciplines that have recognized the significance of its user-generated content. Argument mining is an important analytical task for processing and understanding online discourse. Specifically, it aims to identify the structural elements of arguments, denoted as information and inference. These elements, however, are not static and may require context within the conversation they are in, yet there is a lack of data and annotation frameworks addressing this dynamic aspect on Twitter. We contribute TACO, the first dataset of Twitter Arguments utilizing 1,814 tweets covering 200 entire conversations spanning six heterogeneous topics annotated with an agreement of 0.718 Krippendorff's alpha among six experts. Second, we provide our annotation framework, incorporating definitions from the Cambridge Dictionary, to define and identify
&lt;/p&gt;</description></item><item><title>UniMEEC&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24773;&#32490;&#35782;&#21035;&#21644;&#24773;&#32490;-&#21407;&#22240;&#20998;&#26512;&#26694;&#26550;&#65292;&#23558;MERC&#21644;MECPE&#37325;&#26032;&#23450;&#20041;&#20026;&#20004;&#20010;&#25513;&#30721;&#39044;&#27979;&#38382;&#39064;&#65292;&#20197;&#22686;&#24378;&#24773;&#32490;&#21644;&#21407;&#22240;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2404.00403</link><description>&lt;p&gt;
UniMEEC:&#36208;&#21521;&#32479;&#19968;&#30340;&#22810;&#27169;&#24773;&#32490;&#35782;&#21035;&#19982;&#24773;&#32490;&#22240;&#26524;
&lt;/p&gt;
&lt;p&gt;
UniMEEC: Towards Unified Multimodal Emotion Recognition and Emotion Cause
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00403
&lt;/p&gt;
&lt;p&gt;
UniMEEC&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24773;&#32490;&#35782;&#21035;&#21644;&#24773;&#32490;-&#21407;&#22240;&#20998;&#26512;&#26694;&#26550;&#65292;&#23558;MERC&#21644;MECPE&#37325;&#26032;&#23450;&#20041;&#20026;&#20004;&#20010;&#25513;&#30721;&#39044;&#27979;&#38382;&#39064;&#65292;&#20197;&#22686;&#24378;&#24773;&#32490;&#21644;&#21407;&#22240;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#35805;&#20013;&#30340;&#22810;&#27169;&#24773;&#32490;&#35782;&#21035;&#65288;MERC&#65289;&#21644;&#22810;&#27169;&#24773;&#32490;-&#21407;&#22240;&#23545;&#25552;&#21462;&#65288;MECPE&#65289;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#24773;&#32490;&#26159;&#24773;&#24863;&#25110;&#24863;&#21463;&#30340;&#34920;&#36798;&#65307;&#23545;&#29305;&#23450;&#20107;&#20214;&#12289;&#24819;&#27861;&#25110;&#24773;&#20917;&#30340;&#21709;&#24212;&#34987;&#31216;&#20026;&#24773;&#32490;&#21407;&#22240;&#12290;&#23427;&#20204;&#22914;&#21516;&#19968;&#26522;&#30828;&#24065;&#30340;&#20004;&#38754;&#65292;&#20849;&#21516;&#25551;&#36848;&#20102;&#20154;&#31867;&#34892;&#20026;&#21644;&#24847;&#22270;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#20316;&#21697;&#23558;MERC&#21644;MECPE&#35270;&#20026;&#29420;&#31435;&#20219;&#21153;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#22312;&#25972;&#21512;&#24773;&#32490;&#21644;&#21407;&#22240;&#21040;&#29616;&#23454;&#24212;&#29992;&#20013;&#23384;&#22312;&#28508;&#22312;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24773;&#32490;&#35782;&#21035;&#21644;&#24773;&#32490;-&#21407;&#22240;&#20998;&#26512;&#26694;&#26550;&#65288;UniMEEC&#65289;&#65292;&#20197;&#25506;&#32034;&#24773;&#32490;&#21644;&#24773;&#32490;&#21407;&#22240;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#21644;&#20114;&#34917;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;UniMEEC&#23558;MERC&#21644;MECPE&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#20004;&#20010;&#25513;&#30721;&#39044;&#27979;&#38382;&#39064;&#65292;&#22686;&#24378;&#20102;&#24773;&#32490;&#21644;&#21407;&#22240;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;UniMEEC&#22312;&#21508;&#27169;&#24577;&#20043;&#38388;&#20849;&#20139;&#36805;&#36895;&#23398;&#20064;&#20197;&#20419;&#36827;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00403v1 Announce Type: new  Abstract: Multimodal emotion recognition in conversation (MERC) and multimodal emotion-cause pair extraction (MECPE) has recently garnered significant attention. Emotions are the expression of affect or feelings; responses to specific events, thoughts, or situations are known as emotion causes. Both are like two sides of a coin, collectively describing human behaviors and intents. However, most existing works treat MERC and MECPE as separate tasks, which may result in potential challenges in integrating emotion and cause in real-world applications. In this paper, we propose a Unified Multimodal Emotion recognition and Emotion-Cause analysis framework (UniMEEC) to explore the causality and complementarity between emotion and emotion cause. Concretely, UniMEEC reformulates the MERC and MECPE tasks as two mask prediction problems, enhancing the interaction between emotion and cause. Meanwhile, UniMEEC shares the prompt learning among modalities for p
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#25968;&#25454;&#38598;"SciTabQA"&#65292;&#30740;&#31350;&#20102;&#29616;&#26377;&#31185;&#23398;&#28151;&#21512;&#34920;&#26684;&#25968;&#25454;&#19978;&#26368;&#20808;&#36827;Tabular QA&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#35780;&#20272;&#20102;&#20854;&#22312;&#35299;&#37322;&#31185;&#23398;&#34920;&#26684;&#21644;&#25991;&#26412;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.00401</link><description>&lt;p&gt;
&#31185;&#23398;&#34920;&#26684;&#38382;&#31572;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#26377;&#22810;&#24378;&#65311;&#19968;&#39033;&#20351;&#29992;&#23450;&#21046;&#25968;&#25454;&#38598;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
How Robust are the Tabular QA Models for Scientific Tables? A Study using Customized Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00401
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#25968;&#25454;&#38598;"SciTabQA"&#65292;&#30740;&#31350;&#20102;&#29616;&#26377;&#31185;&#23398;&#28151;&#21512;&#34920;&#26684;&#25968;&#25454;&#19978;&#26368;&#20808;&#36827;Tabular QA&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#35780;&#20272;&#20102;&#20854;&#22312;&#35299;&#37322;&#31185;&#23398;&#34920;&#26684;&#21644;&#25991;&#26412;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28151;&#21512;&#31185;&#23398;&#34920;&#26684;&#21644;&#25991;&#26412;&#25968;&#25454;&#19978;&#36827;&#34892;&#38382;&#31572;(QA)&#28041;&#21450;&#31185;&#23398;&#20449;&#24687;&#65292;&#24182;&#20381;&#36182;&#20110;&#22797;&#26434;&#30340;&#25968;&#20540;&#25512;&#29702;&#12290;&#36817;&#24180;&#26469;&#65292;&#34429;&#28982;&#34920;&#26684;QA&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#20219;&#20309;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#23545;&#23427;&#20204;&#22312;&#31185;&#23398;&#20449;&#24687;&#19978;&#30340;&#40065;&#26834;&#24615;&#32570;&#20047;&#29702;&#35299;&#12290;&#20026;&#20102;&#30740;&#31350;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;QA&#27169;&#22411;&#22312;&#31185;&#23398;&#28151;&#21512;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#25968;&#25454;&#38598;&#8220;SciTabQA&#8221;&#65292;&#20854;&#20013;&#21253;&#21547;&#26469;&#33258;&#31185;&#23398;&#34920;&#26684;&#21450;&#20854;&#25551;&#36848;&#30340;822&#20010;&#38382;&#31572;&#23545;&#12290;&#20511;&#21161;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22522;&#20110;&#26368;&#20808;&#36827;Tabular QA&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#21363;(i)&#21033;&#29992;&#38656;&#35201;&#32467;&#26500;&#21270;&#25968;&#25454;(&#34920;&#26684;)&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;(&#25991;&#26412;)&#30340;&#24322;&#26500;&#20449;&#24687;&#20197;&#21450;(ii)&#25191;&#34892;&#22797;&#26434;&#30340;&#31185;&#23398;&#25512;&#29702;&#20219;&#21153;&#12290;&#26412;&#36136;&#19978;&#65292;&#25105;&#20204;&#26816;&#26597;&#27169;&#22411;&#35299;&#37322;&#31185;&#23398;&#34920;&#26684;&#21644;&#25991;&#26412;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#8220;SciTabQA&#8221;&#26159;&#19968;&#39033;&#21019;&#26032;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00401v1 Announce Type: new  Abstract: Question-answering (QA) on hybrid scientific tabular and textual data deals with scientific information, and relies on complex numerical reasoning. In recent years, while tabular QA has seen rapid progress, understanding their robustness on scientific information is lacking due to absence of any benchmark dataset. To investigate the robustness of the existing state-of-the-art QA models on scientific hybrid tabular data, we propose a new dataset, "SciTabQA", consisting of 822 question-answer pairs from scientific tables and their descriptions. With the help of this dataset, we assess the state-of-the-art Tabular QA models based on their ability (i) to use heterogeneous information requiring both structured data (table) and unstructured data (text) and (ii) to perform complex scientific reasoning tasks. In essence, we check the capability of the models to interpret scientific tables and text. Our experiments show that "SciTabQA" is an inno
&lt;/p&gt;</description></item><item><title>Aurora-M &#26159;&#31532;&#19968;&#20010;&#26681;&#25454;&#32654;&#22269;&#34892;&#25919;&#21629;&#20196;&#36827;&#34892;&#32418;&#38431;&#27979;&#35797;&#30340;&#24320;&#28304;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#33521;&#35821;&#12289;&#33452;&#20848;&#35821;&#12289;&#21360;&#22320;&#35821;&#12289;&#26085;&#35821;&#12289;&#36234;&#21335;&#35821;&#21644;&#20195;&#30721;&#19978;&#35757;&#32451;&#65292;&#19981;&#26029;&#39044;&#35757;&#32451;&#65292;&#21253;&#25324;&#20102;&#20154;&#24037;&#23457;&#26680;&#30340;&#23433;&#20840;&#35828;&#26126;&#65292;&#24635;&#35757;&#32451; token &#25968;&#36229;&#36807; 2 &#19975;&#20159;&#20010;</title><link>https://arxiv.org/abs/2404.00399</link><description>&lt;p&gt;
Aurora-M: &#26681;&#25454;&#32654;&#22269;&#34892;&#25919;&#21629;&#20196;&#65292;&#31532;&#19968;&#20010;&#24320;&#28304;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#32418;&#38431;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Aurora-M: The First Open Source Multilingual Language Model Red-teamed according to the U.S. Executive Order
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00399
&lt;/p&gt;
&lt;p&gt;
Aurora-M &#26159;&#31532;&#19968;&#20010;&#26681;&#25454;&#32654;&#22269;&#34892;&#25919;&#21629;&#20196;&#36827;&#34892;&#32418;&#38431;&#27979;&#35797;&#30340;&#24320;&#28304;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#33521;&#35821;&#12289;&#33452;&#20848;&#35821;&#12289;&#21360;&#22320;&#35821;&#12289;&#26085;&#35821;&#12289;&#36234;&#21335;&#35821;&#21644;&#20195;&#30721;&#19978;&#35757;&#32451;&#65292;&#19981;&#26029;&#39044;&#35757;&#32451;&#65292;&#21253;&#25324;&#20102;&#20154;&#24037;&#23457;&#26680;&#30340;&#23433;&#20840;&#35828;&#26126;&#65292;&#24635;&#35757;&#32451; token &#25968;&#36229;&#36807; 2 &#19975;&#20159;&#20010;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25903;&#25345;&#22810;&#31181;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#65292;&#20294;&#26159;&#23427;&#20204;&#22312;&#35757;&#32451;&#26102;&#39640;&#26114;&#30340;&#35745;&#31639;&#25104;&#26412;&#38480;&#21046;&#20102;&#21487;&#35775;&#38382;&#24615;&#12290;BLOOM &#21644; StarCoder &#31561;&#20513;&#35758;&#26088;&#22312;&#20351;&#39044;&#35757;&#32451;&#27169;&#22411;&#23545;&#20110;&#21327;&#20316;&#31038;&#21306;&#24320;&#21457;&#26356;&#20855;&#27665;&#20027;&#24615;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23384;&#22312;&#30340;&#27169;&#22411;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#65306;&#22810;&#35821;&#35328;&#33021;&#21147;&#26377;&#38480;&#65292;&#25345;&#32493;&#30340;&#39044;&#35757;&#32451;&#20250;&#23548;&#33268;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#32780;&#20174;&#22836;&#24320;&#22987;&#39044;&#35757;&#32451;&#21448;&#20855;&#26377;&#39640;&#26114;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#19988;&#38656;&#35201;&#36981;&#23432;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#21644;&#21457;&#23637;&#27861;&#24459;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102; Aurora-M&#65292;&#19968;&#20010;&#21253;&#21547; 15B &#21442;&#25968;&#30340;&#22810;&#35821;&#35328;&#24320;&#28304;&#27169;&#22411;&#65292;&#35757;&#32451;&#35821;&#35328;&#21253;&#25324;&#33521;&#35821;&#12289;&#33452;&#20848;&#35821;&#12289;&#21360;&#22320;&#35821;&#12289;&#26085;&#35821;&#12289;&#36234;&#21335;&#35821;&#21644;&#20195;&#30721;&#12290;Aurora-M &#19981;&#26029;&#20174; StarCoderPlus &#19978;&#39044;&#35757;&#32451;&#65292;&#39069;&#22806;&#35757;&#32451;&#20102; 4350 &#20159;&#20010; token&#65292;&#24635;&#35757;&#32451; token &#25968;&#36229;&#36807;&#20102; 2 &#19975;&#20159;&#20010;&#12290;&#23427;&#26159;&#31532;&#19968;&#20010;&#22312;&#20154;&#24037;&#23457;&#26680;&#30340;&#23433;&#20840;&#35828;&#26126;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#24320;&#28304;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#20854;&#24320;&#21457;&#19982;&#20256;&#32479;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00399v1 Announce Type: cross  Abstract: Pretrained language models underpin several AI applications, but their high computational cost for training limits accessibility. Initiatives such as BLOOM and StarCoder aim to democratize access to pretrained models for collaborative community development. However, such existing models face challenges: limited multilingual capabilities, continual pretraining causing catastrophic forgetting, whereas pretraining from scratch is computationally expensive, and compliance with AI safety and development laws. This paper presents Aurora-M, a 15B parameter multilingual open-source model trained on English, Finnish, Hindi, Japanese, Vietnamese, and code. Continually pretrained from StarCoderPlus on 435 billion additional tokens, Aurora-M surpasses 2 trillion tokens in total training token count. It is the first open-source multilingual model fine-tuned on human-reviewed safety instructions, thus aligning its development not only with conventio
&lt;/p&gt;</description></item><item><title>&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#65292;&#23545;BPE&#35789;&#27719;&#36827;&#34892;&#20462;&#21098;&#19981;&#33021;&#25552;&#39640;&#24615;&#33021;&#65292;&#29978;&#33267;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2404.00397</link><description>&lt;p&gt;
&#23545;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;BPE&#35789;&#27719;&#20462;&#21098;&#36827;&#34892;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An Analysis of BPE Vocabulary Trimming in Neural Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00397
&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#65292;&#23545;BPE&#35789;&#27719;&#36827;&#34892;&#20462;&#21098;&#19981;&#33021;&#25552;&#39640;&#24615;&#33021;&#65292;&#29978;&#33267;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#23383;&#33410;&#23545;&#32534;&#30721;&#23376;&#35789;&#26631;&#35760;&#21270;&#20013;&#30340;&#38408;&#20540;&#35789;&#27719;&#20462;&#21098;&#65292;&#36825;&#26159;&#19968;&#31181;&#21518;&#22788;&#29702;&#27493;&#39588;&#65292;&#29992;&#20854;&#32452;&#25104;&#23376;&#35789;&#26367;&#25442;&#31232;&#26377;&#23376;&#35789;&#30340;&#25216;&#26415;&#12290;&#36825;&#31181;&#25216;&#26415;&#22312;&#27969;&#34892;&#30340;&#26631;&#35760;&#21270;&#24211;&#20013;&#21487;&#29992;&#65292;&#20294;&#23578;&#26410;&#32463;&#36807;&#20005;&#26684;&#30340;&#31185;&#23398;&#26816;&#39564;&#12290;&#34429;&#28982;&#22312;&#26426;&#22120;&#32763;&#35793;&#23454;&#29616;&#20013;&#24314;&#35758;&#21024;&#38500;&#31232;&#26377;&#23376;&#35789;&#20316;&#20026;&#26368;&#20339;&#23454;&#36341;&#65292;&#26082;&#21487;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#65292;&#21448;&#21487;&#36890;&#36807;&#25552;&#39640;&#40065;&#26834;&#24615;&#26469;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#22823;&#37327;&#30340;&#36229;&#21442;&#25968;&#35774;&#32622;&#31354;&#38388;&#20013;&#65292;&#35789;&#27719;&#20462;&#21098;&#26410;&#33021;&#25552;&#39640;&#24615;&#33021;&#65292;&#29978;&#33267;&#24448;&#24448;&#23548;&#33268;&#20005;&#37325;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00397v1 Announce Type: new  Abstract: We explore threshold vocabulary trimming in Byte-Pair Encoding subword tokenization, a postprocessing step that replaces rare subwords with their component subwords. The technique is available in popular tokenization libraries but has not been subjected to rigorous scientific scrutiny. While the removal of rare subwords is suggested as best practice in machine translation implementations, both as a means to reduce model size and for improving model performance through robustness, our experiments indicate that, across a large space of hyperparameter settings, vocabulary trimming fails to improve performance, and is even prone to incurring heavy degradation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;Jetsons&#22242;&#38431;&#22312;&#22810;&#35821;&#35328;ESG&#24433;&#21709;&#26102;&#38271;&#25512;&#26029;&#20849;&#20139;&#20219;&#21153;&#20013;&#25506;&#32034;&#30340;&#19981;&#21516;&#26041;&#27861;&#65292;&#21253;&#25324;&#20351;&#29992;XLM-RoBERTa&#21644;DeBERTa-v3&#23545;&#24433;&#21709;&#26102;&#38271;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#22312;&#33521;&#35821;&#35821;&#35328;&#20013;&#21462;&#24471;&#39046;&#20808;&#22320;&#20301;&#12290;</title><link>https://arxiv.org/abs/2404.00386</link><description>&lt;p&gt;
&#12298;&#26480;&#26862;&#26031;&#38431;&#22312;FinNLP 2024:&#22522;&#20110;Transformer&#27169;&#22411;&#65292;&#25506;&#32034;&#26032;&#38395;&#25991;&#31456;&#30340;ESG&#24433;&#21709;&#29702;&#35299;&#12299;
&lt;/p&gt;
&lt;p&gt;
Jetsons at FinNLP 2024: Towards Understanding the ESG Impact of a News Article using Transformer-based Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;Jetsons&#22242;&#38431;&#22312;&#22810;&#35821;&#35328;ESG&#24433;&#21709;&#26102;&#38271;&#25512;&#26029;&#20849;&#20139;&#20219;&#21153;&#20013;&#25506;&#32034;&#30340;&#19981;&#21516;&#26041;&#27861;&#65292;&#21253;&#25324;&#20351;&#29992;XLM-RoBERTa&#21644;DeBERTa-v3&#23545;&#24433;&#21709;&#26102;&#38271;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#22312;&#33521;&#35821;&#35821;&#35328;&#20013;&#21462;&#24471;&#39046;&#20808;&#22320;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#26480;&#26862;&#26031;&#38431;&#38024;&#23545;&#22810;&#35821;&#35328;ESG&#24433;&#21709;&#26102;&#38271;&#25512;&#26029;&#65288;ML-ESG-3&#65289;&#20849;&#20139;&#20219;&#21153;&#25152;&#25506;&#32034;&#30340;&#19981;&#21516;&#26041;&#27861;&#12290;&#35813;&#20849;&#20139;&#20219;&#21153;&#19987;&#27880;&#20110;&#39044;&#27979;&#26032;&#38395;&#25991;&#31456;&#30340;ESG&#24433;&#21709;&#26102;&#38271;&#21644;&#31867;&#22411;&#12290;&#20849;&#20139;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;2,059&#31687;&#33521;&#35821;&#12289;&#27861;&#35821;&#12289;&#38889;&#35821;&#21644;&#26085;&#35821;&#26032;&#38395;&#26631;&#39064;&#21644;&#25991;&#31456;&#12290;&#38024;&#23545;&#24433;&#21709;&#26102;&#38271;&#20998;&#31867;&#20219;&#21153;&#65292;&#25105;&#20204;&#37319;&#29992;&#33258;&#23450;&#20041;&#24494;&#35843;&#31574;&#30053;&#23545;XLM-RoBERTa&#21644;DeBERTa-v3&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#20998;&#21035;&#20351;&#29992;&#33258;&#35757;&#32451;&#21644;&#20165;&#20351;&#29992;&#33521;&#35821;&#32763;&#35793;&#12290;&#36825;&#20123;&#27169;&#22411;&#20998;&#21035;&#22312;&#38889;&#35821;&#21644;&#26085;&#35821;&#30340;&#25490;&#34892;&#27036;&#19978;&#25490;&#21517;&#31532;&#19968;&#65292;&#24182;&#22312;&#33521;&#35821;&#35821;&#35328;&#30340;&#38598;&#25104;&#20013;&#25490;&#21517;&#31532;&#19968;&#12290;&#38024;&#23545;&#24433;&#21709;&#31867;&#22411;&#20998;&#31867;&#20219;&#21153;&#65292;&#25105;&#20204;&#30340;XLM-RoBERTa&#27169;&#22411;&#36890;&#36807;&#33258;&#23450;&#20041;&#24494;&#35843;&#31574;&#30053;&#22312;&#33521;&#35821;&#35821;&#35328;&#20013;&#25490;&#21517;&#31532;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00386v1 Announce Type: new  Abstract: In this paper, we describe the different approaches explored by the Jetsons team for the Multi-Lingual ESG Impact Duration Inference (ML-ESG-3) shared task. The shared task focuses on predicting the duration and type of the ESG impact of a news article. The shared task dataset consists of 2,059 news titles and articles in English, French, Korean, and Japanese languages. For the impact duration classification task, we fine-tuned XLM-RoBERTa with a custom fine-tuning strategy and using self-training and DeBERTa-v3 using only English translations. These models individually ranked first on the leaderboard for Korean and Japanese and in an ensemble for the English language, respectively. For the impact type classification task, our XLM-RoBERTa model fine-tuned using a custom fine-tuning strategy ranked first for the English language.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20174;&#21307;&#23398;&#25945;&#26448;&#25552;&#21462;&#30340;&#25512;&#29702;&#36335;&#24452;&#21644;&#22810;&#26679;&#21270;&#30340;&#36981;&#24490;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20855;&#26377;70&#20159;&#21442;&#25968;&#30340;Meerkat-7B&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#21830;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#20219;&#21153;&#19978;&#38544;&#31169;&#21644;&#25512;&#29702;&#33021;&#21147;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#20808;&#21069;7B&#27169;&#22411;&#30340;&#26174;&#33879;&#25104;&#26524;&#12290;</title><link>https://arxiv.org/abs/2404.00376</link><description>&lt;p&gt;
&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#21307;&#23398;&#25945;&#26448;&#20013;&#23398;&#20064;&#22686;&#24378;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Small Language Models Learn Enhanced Reasoning Skills from Medical Textbooks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00376
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20174;&#21307;&#23398;&#25945;&#26448;&#25552;&#21462;&#30340;&#25512;&#29702;&#36335;&#24452;&#21644;&#22810;&#26679;&#21270;&#30340;&#36981;&#24490;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20855;&#26377;70&#20159;&#21442;&#25968;&#30340;Meerkat-7B&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#21830;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#20219;&#21153;&#19978;&#38544;&#31169;&#21644;&#25512;&#29702;&#33021;&#21147;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#20808;&#21069;7B&#27169;&#22411;&#30340;&#26174;&#33879;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#21830;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#22312;&#21307;&#23398;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#25104;&#26524;&#65292;&#20294;&#20854;&#38381;&#28304;&#24615;&#36136;&#24341;&#21457;&#20102;&#37325;&#35201;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Meerkat-7B&#65292;&#19968;&#20010;&#21253;&#21547;70&#20159;&#21442;&#25968;&#30340;&#26032;&#22411;&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;Meerkat-7B&#20351;&#29992;&#25105;&#20204;&#26032;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20174;18&#26412;&#21307;&#23398;&#25945;&#26448;&#20013;&#33719;&#21462;&#30340;&#39640;&#36136;&#37327;&#24605;&#32500;&#38142;&#25512;&#29702;&#36335;&#24452;&#65292;&#20197;&#21450;&#22810;&#26679;&#30340;&#36981;&#24490;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#19971;&#20010;&#21307;&#23398;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#65292;&#36229;&#36807;&#20102;GPT-3.5 13.1%&#65292;&#21516;&#26102;&#20063;&#20248;&#20110;&#20197;&#24448;&#26368;&#22909;&#30340;7B&#27169;&#22411;MediTron-7B&#21644;BioMistral-7B&#20998;&#21035;&#36798;&#21040;&#20102;13.4%&#21644;9.8%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00376v1 Announce Type: new  Abstract: While recent advancements in commercial large language models (LM) have shown promising results in medical tasks, their closed-source nature poses significant privacy and security concerns, hindering their widespread use in the medical field. Despite efforts to create open-source models, their limited parameters often result in insufficient multi-step reasoning capabilities required for solving complex medical problems. To address this, we introduce Meerkat-7B, a novel medical AI system with 7 billion parameters. Meerkat-7B was trained using our new synthetic dataset consisting of high-quality chain-of-thought reasoning paths sourced from 18 medical textbooks, along with diverse instruction-following datasets. Our system achieved remarkable accuracy across seven medical benchmarks, surpassing GPT-3.5 by 13.1%, as well as outperforming the previous best 7B models such as MediTron-7B and BioMistral-7B by 13.4% and 9.8%, respectively. Notab
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24635;&#32467;&#30340;&#23545;&#35805;&#22686;&#24378;&#26041;&#27861;SDA&#65292;&#36890;&#36807;&#20351;&#29992;&#23545;&#35805;&#24635;&#32467;&#22686;&#24378;&#20102;LLM&#30340;&#21487;&#25511;&#24615;&#65292;&#23454;&#29616;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#23545;&#35805;&#25968;&#25454;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2404.00361</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#24320;&#25918;&#22495;&#23545;&#35805;&#29983;&#25104;&#30340;&#21487;&#25511;&#22810;&#26679;&#21270;&#25968;&#25454;&#22686;&#24378;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Controllable and Diverse Data Augmentation with Large Language Model for Low-Resource Open-Domain Dialogue Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00361
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24635;&#32467;&#30340;&#23545;&#35805;&#22686;&#24378;&#26041;&#27861;SDA&#65292;&#36890;&#36807;&#20351;&#29992;&#23545;&#35805;&#24635;&#32467;&#22686;&#24378;&#20102;LLM&#30340;&#21487;&#25511;&#24615;&#65292;&#23454;&#29616;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#23545;&#35805;&#25968;&#25454;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#23545;&#20110;&#20943;&#36731;&#20302;&#36164;&#28304;&#24320;&#25918;&#22495;&#23545;&#35805;&#29983;&#25104;&#20013;&#27169;&#22411;&#35757;&#32451;&#19981;&#31283;&#23450;&#21644;&#36807;&#25311;&#21512;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;DA&#26041;&#27861;&#36890;&#24120;&#24573;&#30053;&#20102;&#35821;&#20041;&#25968;&#25454;&#22810;&#26679;&#24615;&#65292;&#38480;&#21046;&#20102;&#25972;&#20307;&#36136;&#37327;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#34987;&#29992;&#20110;DA&#20197;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#23545;&#35805;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#21463;&#21040;&#38480;&#21046;&#30340;&#21487;&#25511;&#24615;&#65292;&#24182;&#19988;&#20542;&#21521;&#20110;&#29983;&#25104;&#19982;&#31181;&#23376;&#23545;&#35805;&#30456;&#27604;&#20855;&#26377;&#20998;&#24067;&#20559;&#31227;&#30340;&#23545;&#35805;&#12290;&#20026;&#20102;&#26368;&#22823;&#21270;&#22686;&#24378;&#22810;&#26679;&#24615;&#24182;&#35299;&#20915;&#21487;&#25511;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#24635;&#32467;&#30340;LLM&#65288;SDA&#65289;&#30340;&#23545;&#35805;&#22686;&#24378;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#23545;&#35805;&#24635;&#32467;&#20316;&#20026;&#35268;&#21010;&#24037;&#20855;&#22686;&#24378;&#20102;LLM&#30340;&#21487;&#25511;&#24615;&#12290;&#22522;&#20110;&#24635;&#32467;&#65292;SDA&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#19988;&#22810;&#26679;&#21270;&#30340;&#23545;&#35805;&#25968;&#25454;&#65292;&#21363;&#20351;&#21482;&#26377;&#19968;&#20010;&#23567;&#30340;&#31181;&#23376;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#35780;&#20272;&#24320;&#25918;&#22495;&#23545;&#35805;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#32858;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00361v1 Announce Type: new  Abstract: Data augmentation (DA) is crucial to mitigate model training instability and over-fitting problems in low-resource open-domain dialogue generation. However, traditional DA methods often neglect semantic data diversity, restricting the overall quality. Recently, large language models (LLM) have been used for DA to generate diversified dialogues. However, they have limited controllability and tend to generate dialogues with a distribution shift compared to the seed dialogues. To maximize the augmentation diversity and address the controllability problem, we propose \textbf{S}ummary-based \textbf{D}ialogue \textbf{A}ugmentation with LLM (SDA). Our approach enhances the controllability of LLM by using dialogue summaries as a planning tool. Based on summaries, SDA can generate high-quality and diverse dialogue data even with a small seed dataset. To evaluate the efficacy of data augmentation methods for open-domain dialogue, we designed a clu
&lt;/p&gt;</description></item><item><title>LLMs&#22312;&#25968;&#23398;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#65292;&#20854;&#20013;GPT-4&#22312;Math Stack Exchange&#19978;&#22238;&#31572;&#25968;&#23398;&#38382;&#39064;&#30340;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>https://arxiv.org/abs/2404.00344</link><description>&lt;p&gt;
LLMs&#33021;&#22815;&#25484;&#25569;&#25968;&#23398;&#21527;&#65311;&#22312;&#25968;&#23398;&#22534;&#26632;&#20132;&#25442;&#19978;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Master Math? Investigating Large Language Models on Math Stack Exchange
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00344
&lt;/p&gt;
&lt;p&gt;
LLMs&#22312;&#25968;&#23398;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#65292;&#20854;&#20013;GPT-4&#22312;Math Stack Exchange&#19978;&#22238;&#31572;&#25968;&#23398;&#38382;&#39064;&#30340;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#24322;&#24120;&#33021;&#21147;&#65292;&#36890;&#24120;&#34920;&#29616;&#20986;&#36229;&#36234;&#20154;&#31867;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#25968;&#23398;&#39046;&#22495;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#20854;&#19987;&#38376;&#30340;&#32467;&#26500;&#21644;&#25152;&#38656;&#30340;&#31934;&#24230;&#12290;&#26412;&#30740;&#31350;&#37319;&#29992;&#20102;&#20004;&#27493;&#26041;&#27861;&#26469;&#35843;&#26597;LLMs&#22312;&#22238;&#31572;&#25968;&#23398;&#38382;&#39064;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#37319;&#29992;&#22312;&#25968;&#23398;&#38382;&#39064;-&#31572;&#26696;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#26368;&#20339;&#30340;LLMs&#26469;&#22238;&#31572;Math Stack Exchange&#65288;MSE&#65289;&#20013;&#30340;78&#20010;&#38382;&#39064;&#12290;&#20854;&#27425;&#65292;&#23545;&#34920;&#29616;&#26368;&#20339;&#30340;LLM&#36827;&#34892;&#26696;&#20363;&#20998;&#26512;&#65292;&#37325;&#28857;&#20851;&#27880;&#20854;&#31572;&#26696;&#30340;&#36136;&#37327;&#21644;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#20026;&#22238;&#31572;&#25968;&#23398;&#38382;&#39064;&#36827;&#34892;&#24494;&#35843;&#30340;&#29616;&#26377;LLMs&#20013;&#65292;GPT-4&#34920;&#29616;&#26368;&#20339;&#65288;nDCG&#20026;0.48&#65292;P@10&#20026;0.37&#65289;&#65292;&#22312;&#25968;&#23398;&#38382;&#39064;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00344v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated exceptional capabilities in various natural language tasks, often achieving performances that surpass those of humans. Despite these advancements, the domain of mathematics presents a distinctive challenge, primarily due to its specialized structure and the precision it demands. In this study, we adopted a two-step approach for investigating the proficiency of LLMs in answering mathematical questions. First, we employ the most effective LLMs, as identified by their performance on math question-answer benchmarks, to generate answers to 78 questions from the Math Stack Exchange (MSE). Second, a case analysis is conducted on the LLM that showed the highest performance, focusing on the quality and accuracy of its answers through manual evaluation. We found that GPT-4 performs best (nDCG of 0.48 and P@10 of 0.37) amongst existing LLMs fine-tuned for answering mathematics questions and outperfor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#20256;&#32479;&#26041;&#27861;&#21040;&#24403;&#20195;&#23454;&#36341;&#65292;&#25506;&#35752;&#20102;&#36866;&#29992;&#20110;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#20248;&#21270;&#21033;&#29992;BERT-based&#32534;&#30721;&#22120;&#27169;&#22411;&#30340;&#24605;&#36335;&#65292;&#24182;&#25581;&#31034;&#20102;&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#21516;&#20041;&#35789;&#26367;&#25442;&#26041;&#27861;&#23384;&#22312;&#30340;&#37325;&#35201;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2404.00303</link><description>&lt;p&gt;
&#38024;&#23545;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25968;&#25454;&#22686;&#24378;&#30340;&#32508;&#21512;&#30740;&#31350;&#65306;&#20256;&#32479;&#26041;&#27861;&#12289;BERT&#21644;LLM
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Study on NLP Data Augmentation for Hate Speech Detection: Legacy Methods, BERT, and LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#20256;&#32479;&#26041;&#27861;&#21040;&#24403;&#20195;&#23454;&#36341;&#65292;&#25506;&#35752;&#20102;&#36866;&#29992;&#20110;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#20248;&#21270;&#21033;&#29992;BERT-based&#32534;&#30721;&#22120;&#27169;&#22411;&#30340;&#24605;&#36335;&#65292;&#24182;&#25581;&#31034;&#20102;&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#21516;&#20041;&#35789;&#26367;&#25442;&#26041;&#27861;&#23384;&#22312;&#30340;&#37325;&#35201;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#25968;&#25454;&#22686;&#24378;&#24341;&#36215;&#30340;&#20852;&#36259;&#28608;&#22686;&#65292;&#39537;&#21160;&#21147;&#26469;&#33258;&#20110;&#38656;&#35201;&#35299;&#20915;&#20167;&#24680;&#35328;&#35770;&#39046;&#22495;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12289;&#31038;&#20132;&#23186;&#20307;&#35789;&#27719;&#30340;&#21160;&#24577;&#24615;&#65292;&#20197;&#21450;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#23545;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#22686;&#24378;&#20013;&#26222;&#36941;&#20351;&#29992;&#30340;&#35789;&#27719;&#26367;&#25442;&#24341;&#21457;&#20102;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#21487;&#33021;&#26080;&#24847;&#20013;&#25913;&#21464;&#39044;&#26399;&#21547;&#20041;&#65292;&#20174;&#32780;&#24433;&#21709;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#23547;&#27714;&#21512;&#36866;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20256;&#32479;&#30340;&#20256;&#32479;&#26041;&#27861;&#21644;&#24403;&#20195;&#23454;&#36341;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#21253;&#25324;GPT&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#21033;&#29992;&#22522;&#20110;BERT&#30340;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#25645;&#37197;&#19978;&#19978;&#25991;&#20313;&#24358;&#30456;&#20284;&#24615;&#36807;&#28388;&#65292;&#25581;&#31034;&#20102;&#20043;&#21069;&#21516;&#20041;&#35789;&#26367;&#25442;&#26041;&#27861;&#30340;&#37325;&#22823;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#30340;&#27604;&#36739;&#20998;&#26512;&#28085;&#30422;&#20102;&#20116;&#31181;&#27969;&#34892;&#30340;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00303v1 Announce Type: new  Abstract: The surge of interest in data augmentation within the realm of NLP has been driven by the need to address challenges posed by hate speech domains, the dynamic nature of social media vocabulary, and the demands for large-scale neural networks requiring extensive training data. However, the prevalent use of lexical substitution in data augmentation has raised concerns, as it may inadvertently alter the intended meaning, thereby impacting the efficacy of supervised machine learning models. In pursuit of suitable data augmentation methods, this study explores both established legacy approaches and contemporary practices such as Large Language Models (LLM), including GPT in Hate Speech detection. Additionally, we propose an optimized utilization of BERT-based encoder models with contextual cosine similarity filtration, exposing significant limitations in prior synonym substitution methods. Our comparative analysis encompasses five popular aug
&lt;/p&gt;</description></item><item><title>TRABSA&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;transformer&#26550;&#26500;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;BiLSTM&#32593;&#32476;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#21033;&#29992;RoBERTa&#22312;&#22823;&#37327;&#25512;&#29305;&#19978;&#35757;&#32451;&#65292;&#22635;&#34917;&#20102;&#24773;&#24863;&#20998;&#26512;&#39046;&#22495;&#30340;&#24046;&#36317;&#65292;&#23454;&#29616;&#20102;94%&#30340;&#20934;&#30830;&#24615;&#21644;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2404.00297</link><description>&lt;p&gt;
TRABSA&#65306;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;BiLSTM&#21644;Twitter-RoBERTa&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#25512;&#25991;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
TRABSA: Interpretable Sentiment Analysis of Tweets using Attention-based BiLSTM and Twitter-RoBERTa
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00297
&lt;/p&gt;
&lt;p&gt;
TRABSA&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;transformer&#26550;&#26500;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;BiLSTM&#32593;&#32476;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#21033;&#29992;RoBERTa&#22312;&#22823;&#37327;&#25512;&#29305;&#19978;&#35757;&#32451;&#65292;&#22635;&#34917;&#20102;&#24773;&#24863;&#20998;&#26512;&#39046;&#22495;&#30340;&#24046;&#36317;&#65292;&#23454;&#29616;&#20102;94%&#30340;&#20934;&#30830;&#24615;&#21644;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#20998;&#26512;&#23545;&#20110;&#29702;&#35299;&#20844;&#20247;&#33286;&#35770;&#21644;&#28040;&#36153;&#32773;&#34892;&#20026;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#27169;&#22411;&#38754;&#20020;&#30528;&#35821;&#35328;&#22810;&#26679;&#24615;&#12289;&#27867;&#21270;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;TRABSA&#65292;&#36825;&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;&#22522;&#20110;transformer&#30340;&#26550;&#26500;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;BiLSTM&#32593;&#32476;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#21033;&#29992;&#22312;124M&#26465;&#25512;&#25991;&#19978;&#35757;&#32451;&#30340;RoBERTa&#65292;&#25105;&#20204;&#22635;&#34917;&#20102;&#24773;&#24863;&#20998;&#26512;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24046;&#36317;&#65292;&#30830;&#20445;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#23558;&#26469;&#33258;32&#20010;&#22269;&#23478;&#21644;&#32654;&#22269;&#21508;&#24030;&#30340;&#25512;&#25991;&#19982;&#25968;&#25454;&#38598;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20845;&#31181;&#35789;&#23884;&#20837;&#25216;&#26415;&#21644;&#19977;&#31181;&#22522;&#20110;&#35789;&#20856;&#30340;&#26631;&#27880;&#25216;&#26415;&#65292;&#24182;&#36873;&#25321;&#20102;&#26368;&#20339;&#25216;&#26415;&#20197;&#23454;&#29616;&#26368;&#20339;&#24773;&#24863;&#20998;&#26512;&#25928;&#26524;&#12290;TRABSA&#20197;94%&#30340;&#20934;&#30830;&#24615;&#21644;&#26174;&#33879;&#30340;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#24471;&#20998;&#22686;&#30410;&#65292;&#32988;&#36807;&#20102;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#26174;&#31034;&#20102;&#19968;&#33268;&#30340;&#20248;&#36234;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;SHAP&#21644;LIME&#20998;&#26512;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#22686;&#24378;&#20102;&#20449;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00297v1 Announce Type: new  Abstract: Sentiment analysis is crucial for understanding public opinion and consumer behavior. Existing models face challenges with linguistic diversity, generalizability, and explainability. We propose TRABSA, a hybrid framework integrating transformer-based architectures, attention mechanisms, and BiLSTM networks to address this. Leveraging RoBERTa-trained on 124M tweets, we bridge gaps in sentiment analysis benchmarks, ensuring state-of-the-art accuracy. Augmenting datasets with tweets from 32 countries and US states, we compare six word-embedding techniques and three lexicon-based labeling techniques, selecting the best for optimal sentiment analysis. TRABSA outperforms traditional ML and deep learning models with 94% accuracy and significant precision, recall, and F1-score gains. Evaluation across diverse datasets demonstrates consistent superiority and generalizability. SHAP and LIME analyses enhance interpretability, improving confidence i
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21463;&#20998;&#23376;&#31995;&#32479;&#21457;&#29983;&#23398;&#21551;&#21457;&#30340;&#20284;&#28982;&#27604;&#26816;&#39564;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;&#35821;&#35328;&#38388;&#26159;&#21542;&#23384;&#22312;&#36951;&#20256;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2404.00284</link><description>&lt;p&gt;
&#19968;&#31181;&#35821;&#35328;&#38388;&#36951;&#20256;&#20851;&#31995;&#30340;&#20284;&#28982;&#27604;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
A Likelihood Ratio Test of Genetic Relationship among Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00284
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21463;&#20998;&#23376;&#31995;&#32479;&#21457;&#29983;&#23398;&#21551;&#21457;&#30340;&#20284;&#28982;&#27604;&#26816;&#39564;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;&#35821;&#35328;&#38388;&#26159;&#21542;&#23384;&#22312;&#36951;&#20256;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#32452;&#35821;&#35328;&#20043;&#38388;&#30340;&#35789;&#27719;&#30456;&#20284;&#24615;&#34920;&#26126;&#36825;&#20123;&#35821;&#35328;&#21487;&#33021;&#23384;&#22312;&#36951;&#20256;&#20851;&#31995;&#65292;&#21363;&#23427;&#20204;&#21487;&#33021;&#28304;&#33258;&#20849;&#21516;&#30340;&#31062;&#35821;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#30456;&#20284;&#24615;&#21487;&#33021;&#26159;&#20598;&#28982;&#20135;&#29983;&#30340;&#65292;&#24182;&#19988;&#19981;&#24635;&#26159;&#24847;&#21619;&#30528;&#23384;&#22312;&#28508;&#22312;&#30340;&#36951;&#20256;&#20851;&#31995;&#12290;&#36807;&#21435;&#20986;&#29616;&#36807;&#35768;&#22810;&#22522;&#20110;&#21333;&#35789;&#21015;&#34920;&#21644;&#21333;&#35789;&#30456;&#20284;&#24615;&#24230;&#37327;&#30340;&#32622;&#25442;&#26816;&#39564;&#20197;&#30830;&#23450;&#27492;&#31867;&#20851;&#31995;&#30340;&#32479;&#35745;&#26174;&#33879;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#23613;&#31649;&#29616;&#26377;&#30340;&#27979;&#35797;&#21487;&#33021;&#23545;&#20110;&#21452;&#36793;&#27604;&#36739;&#65288;&#21363;&#35821;&#35328;&#23545;&#65289;&#25928;&#26524;&#33391;&#22909;&#65292;&#20294;&#24403;&#24212;&#29992;&#20110;&#35821;&#35328;&#32452;&#25110;&#35821;&#35328;&#23478;&#26063;&#26102;&#65292;&#23427;&#20204;&#35201;&#20040;&#30001;&#20110;&#35774;&#35745;&#32780;&#19981;&#21487;&#34892;&#65292;&#35201;&#20040;&#26131;&#20110;&#20135;&#29983;&#20551;&#38451;&#24615;&#12290;&#22240;&#27492;&#65292;&#21463;&#21040;&#20998;&#23376;&#31995;&#32479;&#21457;&#29983;&#23398;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#40784;&#21333;&#35789;&#21015;&#34920;&#20013;&#19981;&#21464;&#23383;&#31526;&#20301;&#28857;&#27604;&#20363;&#30340;&#20284;&#28982;&#27604;&#26816;&#39564;&#65292;&#20197;&#30830;&#23450;&#32473;&#23450;&#35821;&#35328;&#26159;&#21542;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00284v1 Announce Type: new  Abstract: Lexical resemblances among a group of languages indicate that the languages could be genetically related, i.e., they could have descended from a common ancestral language. However, such resemblances can arise by chance and, hence, need not always imply an underlying genetic relationship. Many tests of significance based on permutation of wordlists and word similarity measures appeared in the past to determine the statistical significance of such relationships. We demonstrate that although existing tests may work well for bilateral comparisons, i.e., on pairs of languages, they are either infeasible by design or are prone to yield false positives when applied to groups of languages or language families. To this end, inspired by molecular phylogenetics, we propose a likelihood ratio test to determine if given languages are related based on the proportion of invariant character sites in the aligned wordlists applied during tree inference. F
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#20998;&#31867;&#21644;&#35282;&#33394;&#20998;&#26512;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#25351;&#23548;&#12290;</title><link>https://arxiv.org/abs/2404.00282</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#30340;&#35843;&#26597;:&#27010;&#24565;&#12289;&#20998;&#31867;&#21644;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00282
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#20998;&#31867;&#21644;&#35282;&#33394;&#20998;&#26512;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25317;&#26377;&#24191;&#27867;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#21644;&#39640;&#32423;&#36890;&#29992;&#33021;&#21147;&#65292;&#23427;&#20204;&#22312;&#22686;&#24378;&#23398;&#20064;&#26041;&#38754;&#22914;&#22810;&#20219;&#21153;&#23398;&#20064;&#12289;&#26679;&#26412;&#25928;&#29575;&#21644;&#20219;&#21153;&#35268;&#21010;&#31561;&#26041;&#38754;&#23637;&#29616;&#20986;&#28508;&#21147;&#12290;&#26412;&#35843;&#26597;&#32508;&#36848;&#20102;&#29616;&#26377;$\textit{LLM&#22686;&#24378;RL}$&#25991;&#29486;&#65292;&#24635;&#32467;&#20102;&#20854;&#19982;&#20256;&#32479;RL&#26041;&#27861;&#30340;&#29305;&#24449;&#65292;&#26088;&#22312;&#28548;&#28165;&#30740;&#31350;&#33539;&#22260;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#21033;&#29992;&#32463;&#20856;&#30340;Agent-&#29615;&#22659;&#20132;&#20114;&#33539;&#20363;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#20998;&#31867;&#27861;&#65292;&#31995;&#32479;&#22320;&#23558;LLMs&#22312;RL&#20013;&#30340;&#21151;&#33021;&#20998;&#31867;&#65292;&#21253;&#25324;&#22235;&#31181;&#35282;&#33394;&#65306;&#20449;&#24687;&#22788;&#29702;&#22120;&#12289;&#22870;&#21169;&#35774;&#35745;&#32773;&#12289;&#20915;&#31574;&#32773;&#21644;&#29983;&#25104;&#22120;&#12290;&#27492;&#22806;&#65292;&#38024;&#23545;&#27599;&#20010;&#35282;&#33394;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#26041;&#27861;&#35770;&#65292;&#20998;&#26512;&#20102;&#32531;&#35299;&#30340;&#29305;&#23450;RL&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#26041;&#21521;&#30340;&#35265;&#35299;&#12290;&#26368;&#21518;&#65292;&#28508;&#22312;&#24212;&#29992;&#12289;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00282v1 Announce Type: cross  Abstract: With extensive pre-trained knowledge and high-level general capabilities, large language models (LLMs) emerge as a promising avenue to augment reinforcement learning (RL) in aspects such as multi-task learning, sample efficiency, and task planning. In this survey, we provide a comprehensive review of the existing literature in $\textit{LLM-enhanced RL}$ and summarize its characteristics compared to conventional RL methods, aiming to clarify the research scope and directions for future studies. Utilizing the classical agent-environment interaction paradigm, we propose a structured taxonomy to systematically categorize LLMs' functionalities in RL, including four roles: information processor, reward designer, decision-maker, and generator. Additionally, for each role, we summarize the methodologies, analyze the specific RL challenges that are mitigated, and provide insights into future directions. Lastly, potential applications, prospecti
&lt;/p&gt;</description></item><item><title>LLMs&#23545;&#20316;&#32773;&#30340;&#35821;&#35328;&#27169;&#24335;&#30340;&#24433;&#21709;&#30053;&#24494;&#38477;&#20302;&#20854;&#20010;&#20154;&#29305;&#24449;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#20294;&#26174;&#33879;&#21464;&#21270;&#19981;&#22826;&#39057;&#32321;&#12290;</title><link>https://arxiv.org/abs/2404.00267</link><description>&lt;p&gt;
&#20445;&#23494;&#32773;&#65306;LLM&#23545;&#20010;&#20154;&#29305;&#24449;&#35821;&#35328;&#26631;&#35760;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Secret Keepers: The Impact of LLMs on Linguistic Markers of Personal Traits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00267
&lt;/p&gt;
&lt;p&gt;
LLMs&#23545;&#20316;&#32773;&#30340;&#35821;&#35328;&#27169;&#24335;&#30340;&#24433;&#21709;&#30053;&#24494;&#38477;&#20302;&#20854;&#20010;&#20154;&#29305;&#24449;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#20294;&#26174;&#33879;&#21464;&#21270;&#19981;&#22826;&#39057;&#32321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#30830;&#23450;&#20102;&#20010;&#20307;&#35821;&#35328;&#20351;&#29992;&#19982;&#20854;&#20010;&#20154;&#29305;&#24449;&#20043;&#38388;&#30340;&#20851;&#32852;&#65307;&#25105;&#20204;&#30340;&#35821;&#35328;&#27169;&#24335;&#25581;&#31034;&#20102;&#20851;&#20110;&#25105;&#20204;&#20010;&#24615;&#12289;&#24773;&#32490;&#29366;&#24577;&#21644;&#20449;&#24565;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#26085;&#24120;&#20889;&#20316;&#20013;&#20316;&#20026;&#20889;&#20316;&#21161;&#25163;&#34987;&#36234;&#26469;&#36234;&#24191;&#27867;&#22320;&#37319;&#29992;&#65292;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#20986;&#29616;&#20102;&#65306;&#24403;LLMs&#21442;&#19982;&#20889;&#20316;&#36807;&#31243;&#26102;&#65292;&#20316;&#32773;&#30340;&#35821;&#35328;&#27169;&#24335;&#26159;&#21542;&#20173;&#28982;&#33021;&#39044;&#27979;&#20854;&#20010;&#20154;&#29305;&#24449;&#65311;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#23545;&#20154;&#21475;&#32479;&#35745;&#29305;&#24449;&#21644;&#24515;&#29702;&#29305;&#24449;&#30340;&#35821;&#35328;&#26631;&#35760;&#30340;&#24433;&#21709;&#65292;&#20855;&#20307;&#26816;&#26597;&#20102;&#19977;&#20010;LLMs - GPT3.5&#12289;Llama 2&#21644;Gemini - &#22312;&#20845;&#31181;&#19981;&#21516;&#29305;&#24449;&#19978;&#65306;&#24615;&#21035;&#12289;&#24180;&#40836;&#12289;&#25919;&#27835;&#31435;&#22330;&#12289;&#20010;&#24615;&#12289;&#31227;&#24773;&#33021;&#21147;&#21644;&#36947;&#24503;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#34429;&#28982;&#20351;&#29992;LLMs&#30053;&#24494;&#38477;&#20302;&#20102;&#35821;&#35328;&#27169;&#24335;&#23545;&#20316;&#32773;&#20010;&#20154;&#29305;&#24449;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#20294;&#26174;&#33879;&#21464;&#21270;&#19981;&#22826;&#39057;&#32321;&#65292;LLMs&#30340;&#20351;&#29992;&#24182;&#19981;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00267v1 Announce Type: new  Abstract: Prior research has established associations between individuals' language usage and their personal traits; our linguistic patterns reveal information about our personalities, emotional states, and beliefs. However, with the increasing adoption of Large Language Models (LLMs) as writing assistants in everyday writing, a critical question emerges: are authors' linguistic patterns still predictive of their personal traits when LLMs are involved in the writing process? We investigate the impact of LLMs on the linguistic markers of demographic and psychological traits, specifically examining three LLMs - GPT3.5, Llama 2, and Gemini - across six different traits: gender, age, political affiliation, personality, empathy, and morality. Our findings indicate that although the use of LLMs slightly reduces the predictive power of linguistic patterns over authors' personal traits, the significant changes are infrequent, and the use of LLMs does not 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiLM&#30340;&#25991;&#26412;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#25968;&#25454;&#20316;&#20026;&#21512;&#25104;&#35757;&#32451;&#26679;&#26412;&#65292;&#35299;&#20915;&#20102;&#23884;&#20837;&#32423;&#21035;&#33976;&#39311;&#25968;&#25454;&#38598;&#26080;&#27861;&#29992;&#20110;&#35757;&#32451;&#20854;&#20182;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.00264</link><description>&lt;p&gt;
&#23558;&#25968;&#25454;&#38598;&#25552;&#28860;&#20026;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#25991;&#26412;&#32423;&#25968;&#25454;&#38598;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
DiLM: Distilling Dataset into Language Model for Text-level Dataset Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00264
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiLM&#30340;&#25991;&#26412;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#25968;&#25454;&#20316;&#20026;&#21512;&#25104;&#35757;&#32451;&#26679;&#26412;&#65292;&#35299;&#20915;&#20102;&#23884;&#20837;&#32423;&#21035;&#33976;&#39311;&#25968;&#25454;&#38598;&#26080;&#27861;&#29992;&#20110;&#35757;&#32451;&#20854;&#20182;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#33976;&#39311;&#26088;&#22312;&#36890;&#36807;&#21019;&#24314;&#23569;&#37327;&#20449;&#24687;&#20016;&#23500;&#30340;&#21512;&#25104;&#26679;&#26412;&#26469;&#21387;&#32553;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#20351;&#24471;&#22312;&#20854;&#19978;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#33021;&#22815;&#19982;&#22312;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#32593;&#32476;&#19968;&#26679;&#22909;&#12290;&#24403;&#21069;&#30340;&#25991;&#26412;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#23558;&#27599;&#20010;&#21512;&#25104;&#26679;&#26412;&#21019;&#24314;&#20026;&#35789;&#23884;&#20837;&#24207;&#21015;&#32780;&#19981;&#26159;&#25991;&#26412;&#65292;&#20197;&#24212;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#65307;&#28982;&#32780;&#65292;&#36825;&#31181;&#23884;&#20837;&#32423;&#21035;&#30340;&#33976;&#39311;&#25968;&#25454;&#38598;&#26080;&#27861;&#29992;&#20110;&#35757;&#32451;&#20854;&#20182;&#27169;&#22411;&#65292;&#20854;&#35789;&#23884;&#20837;&#26435;&#37325;&#19981;&#21516;&#20110;&#29992;&#20110;&#33976;&#39311;&#30340;&#27169;&#22411;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25991;&#26412;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#65292;&#31216;&#20026;Distilling dataset into Language Model&#65288;DiLM&#65289;&#65292;&#35813;&#26041;&#27861;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20197;&#29983;&#25104;&#20449;&#24687;&#20016;&#23500;&#30340;&#25991;&#26412;&#25968;&#25454;&#20316;&#20026;&#21512;&#25104;&#35757;&#32451;&#26679;&#26412;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#20248;&#21270;&#21512;&#25104;&#26679;&#26412;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;DiLM&#65292;&#24182;&#23637;&#31034;&#20102;&#20174;DiLM &#20013;&#33976;&#39311;&#24471;&#21040;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#20248;&#31168;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00264v1 Announce Type: new  Abstract: Dataset distillation aims to compress a training dataset by creating a small number of informative synthetic samples such that neural networks trained on them perform as well as those trained on the original training dataset. Current text dataset distillation methods create each synthetic sample as a sequence of word embeddings instead of a text to apply gradient-based optimization; however, such embedding-level distilled datasets cannot be used for training other models whose word embedding weights are different from the model used for distillation. To address this issue, we propose a novel text dataset distillation approach, called Distilling dataset into Language Model (DiLM), which trains a language model to generate informative synthetic training samples as text data, instead of directly optimizing synthetic samples. We evaluated DiLM on various text classification datasets and showed that distilled synthetic datasets from DiLM outp
&lt;/p&gt;</description></item><item><title>&#22312;&#19968;&#20010;&#26041;&#22359;&#19990;&#30028;&#29615;&#22659;&#20013;&#65292;&#35770;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21327;&#20316;&#33021;&#21147;&#65292;&#36890;&#36807;&#35774;&#35745;&#19981;&#26029;&#22686;&#21152;&#25361;&#25112;&#24615;&#30340;&#35774;&#32622;&#26469;&#35780;&#20272;&#19981;&#21516;&#30340;&#21327;&#20316;&#35270;&#35282;&#65292;&#20174;&#29420;&#31435;&#21040;&#26356;&#22797;&#26434;&#30340;&#20381;&#36182;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2404.00246</link><description>&lt;p&gt;
&#20320;&#30340;&#21516;&#20107;&#24456;&#37325;&#35201;&#65306;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#26041;&#22359;&#19990;&#30028;&#20013;&#30340;&#21327;&#20316;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Your Co-Workers Matter: Evaluating Collaborative Capabilities of Language Models in Blocks World
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00246
&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20010;&#26041;&#22359;&#19990;&#30028;&#29615;&#22659;&#20013;&#65292;&#35770;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21327;&#20316;&#33021;&#21147;&#65292;&#36890;&#36807;&#35774;&#35745;&#19981;&#26029;&#22686;&#21152;&#25361;&#25112;&#24615;&#30340;&#35774;&#32622;&#26469;&#35780;&#20272;&#19981;&#21516;&#30340;&#21327;&#20316;&#35270;&#35282;&#65292;&#20174;&#29420;&#31435;&#21040;&#26356;&#22797;&#26434;&#30340;&#20381;&#36182;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#19990;&#30028;&#33258;&#34892;&#20132;&#20114;&#30340;&#35821;&#35328;&#20195;&#29702;&#22312;&#33258;&#21160;&#21270;&#25968;&#23383;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#22312;&#29702;&#35299;&#21644;&#25191;&#34892;&#25991;&#26412;&#28216;&#25103;&#21644;&#32593;&#39029;&#25511;&#21046;&#31561;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#35768;&#22810;&#29616;&#23454;&#20219;&#21153;&#20063;&#38656;&#35201;&#19982;&#20154;&#31867;&#25110;&#20854;&#20182;&#21516;&#31561;&#35282;&#33394;&#30340;LLM&#21327;&#20316;&#65292;&#36825;&#28041;&#21450;&#24847;&#22270;&#29702;&#35299;&#12289;&#20219;&#21153;&#21327;&#35843;&#21644;&#27807;&#36890;&#12290;&#20026;&#27979;&#35797;LLM&#21327;&#20316;&#33021;&#21147;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26041;&#22359;&#19990;&#30028;&#29615;&#22659;&#65292;&#22312;&#36825;&#20010;&#29615;&#22659;&#20013;&#65292;&#20004;&#20010;&#20195;&#29702;&#65292;&#27599;&#20010;&#20195;&#29702;&#37117;&#26377;&#29420;&#29305;&#30340;&#30446;&#26631;&#21644;&#25216;&#33021;&#65292;&#19968;&#36215;&#24314;&#36896;&#19968;&#20010;&#30446;&#26631;&#32467;&#26500;&#12290;&#20026;&#23454;&#29616;&#30446;&#26631;&#65292;&#20182;&#20204;&#21487;&#20197;&#22312;&#19990;&#30028;&#20013;&#34892;&#21160;&#24182;&#29992;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#27807;&#36890;&#12290;&#22312;&#36825;&#20010;&#29615;&#22659;&#19979;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35774;&#32622;&#65292;&#20197;&#35780;&#20272;&#19981;&#21516;&#21327;&#20316;&#35270;&#35282;&#65292;&#20174;&#29420;&#31435;&#30340;&#21040;&#26356;&#22797;&#26434;&#30340;&#20381;&#36182;&#20219;&#21153;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#37319;&#29992;&#20102;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#26469;&#24314;&#27169;&#21512;&#20316;&#20249;&#20276;&#30340;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00246v1 Announce Type: cross  Abstract: Language agents that interact with the world on their own have great potential for automating digital tasks. While large language model (LLM) agents have made progress in understanding and executing tasks such as textual games and webpage control, many real-world tasks also require collaboration with humans or other LLMs in equal roles, which involves intent understanding, task coordination, and communication. To test LLM's ability to collaborate, we design a blocks-world environment, where two agents, each having unique goals and skills, build a target structure together. To complete the goals, they can act in the world and communicate in natural language. Under this environment, we design increasingly challenging settings to evaluate different collaboration perspectives, from independent to more complex, dependent tasks. We further adopt chain-of-thought prompts that include intermediate reasoning steps to model the partner's state a
&lt;/p&gt;</description></item><item><title>DeFT&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;IO&#24847;&#35782;&#30340;&#26641;&#27880;&#24847;&#21147;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;QKV&#20934;&#22791;&#21644;&#27880;&#24847;&#21147;&#35745;&#31639;&#38454;&#27573;&#23454;&#29616;&#20869;&#23384;&#39640;&#25928;&#30340;&#35745;&#31639;&#65292;&#38477;&#20302;&#20869;&#23384;&#21360;&#35760;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#26641;&#35299;&#30721;&#31574;&#30053;&#21644;&#25512;&#26029;&#31995;&#32479;&#19981;&#36866;&#37197;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.00242</link><description>&lt;p&gt;
DeFT&#65306;&#24102;IO&#24847;&#35782;&#30340;Flash Tree-attention&#29992;&#20110;&#39640;&#25928;&#30340;&#22522;&#20110;&#26641;&#25628;&#32034;&#30340;LLM&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
DeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based LLM Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00242
&lt;/p&gt;
&lt;p&gt;
DeFT&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;IO&#24847;&#35782;&#30340;&#26641;&#27880;&#24847;&#21147;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;QKV&#20934;&#22791;&#21644;&#27880;&#24847;&#21147;&#35745;&#31639;&#38454;&#27573;&#23454;&#29616;&#20869;&#23384;&#39640;&#25928;&#30340;&#35745;&#31639;&#65292;&#38477;&#20302;&#20869;&#23384;&#21360;&#35760;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#26641;&#35299;&#30721;&#31574;&#30053;&#21644;&#25512;&#26029;&#31995;&#32479;&#19981;&#36866;&#37197;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26641;&#25628;&#32034;&#36827;&#34892;&#35299;&#30721;&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#39640;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#26029;&#36136;&#37327;&#12290;&#26681;&#25454;&#24341;&#23548;&#20449;&#21495;&#65292;&#23427;&#36890;&#36807;&#24418;&#25104;LLM&#36755;&#20986;&#20174;&#26681;&#21040;&#21494;&#23376;&#30340;&#26368;&#20339;&#36335;&#24452;&#26469;&#25552;&#39640;&#21487;&#25511;&#24615;&#12289;&#25512;&#29702;&#33021;&#21147;&#12289;&#23545;&#40784;&#31561;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35745;&#31639;&#20887;&#20313;&#12289;&#20869;&#23384;&#21344;&#29992;&#21644;&#20869;&#23384;&#35775;&#38382;&#65292;&#24403;&#21069;&#30340;&#26641;&#35299;&#30721;&#31574;&#30053;&#21450;&#20854;&#25512;&#26029;&#31995;&#32479;&#20114;&#30456;&#19981;&#36866;&#37197;&#65292;&#23548;&#33268;&#25512;&#26029;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DeFT&#65292;&#19968;&#31181;IO&#24863;&#30693;&#26641;&#27880;&#24847;&#21147;&#31639;&#27861;&#65292;&#23427;&#22312;&#20004;&#20010;&#38454;&#27573;&#20013;&#20445;&#25345;&#20869;&#23384;&#39640;&#25928;&#30340;&#27880;&#24847;&#21147;&#35745;&#31639;&#65292;&#38477;&#20302;&#20869;&#23384;&#21360;&#35760;&#65306;&#65288;1&#65289;QKV&#20934;&#22791;&#65306;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;KV&#24341;&#23548;&#26641;&#20998;&#35010;&#31574;&#30053;&#65292;&#20026;GPU&#30340;&#39640;&#21033;&#29992;&#29575;&#21644;&#23613;&#21487;&#33021;&#20943;&#23569;GPU&#20840;&#23616;&#20869;&#23384;&#21644;&#33455;&#29255;&#19978;&#20849;&#20139;&#20869;&#23384;&#20043;&#38388;&#30340;KV&#32531;&#23384;&#30340;&#20869;&#23384;&#35835;/&#20889;; &#65288;2&#65289;&#27880;&#24847;&#21147;&#35745;&#31639;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00242v1 Announce Type: cross  Abstract: Decoding using tree search can greatly enhance the inference quality for transformer-based Large Language Models (LLMs). Depending on the guidance signal, it searches for the best path from root to leaf in the tree by forming LLM outputs to improve controllability, reasoning ability, alignment, et cetera. However, current tree decoding strategies and their inference systems do not suit each other well due to redundancy in computation, memory footprints, and memory access, resulting in inefficient inference. To address this issue, we propose DeFT, an IO-aware tree attention algorithm that maintains memory-efficient attention calculation with low memory footprints in two stages: (1) QKV Preparation: we propose a KV-Guided Tree Split strategy to group QKV wisely for high utilization of GPUs and reduction of memory reads/writes for the KV cache between GPU global memory and on-chip shared memory as much as possible; (2) Attention Calculati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LoID&#30340;&#35821;&#20041;&#30693;&#35782;&#20256;&#36882;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#21462;&#22810;&#26041;&#38754;&#30340;&#35821;&#20041;&#20449;&#24687;&#20197;&#22686;&#24378;&#19981;&#21516;&#39046;&#22495;&#65292;&#24182;&#23545;&#40784;&#29992;&#25143;/&#39033;&#30446;ID&#21644;&#20869;&#23481;&#35821;&#20041;&#29305;&#24449;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2404.00236</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#22522;&#20110;&#20869;&#23481;&#30340;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Enhancing Content-based Recommendation via Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LoID&#30340;&#35821;&#20041;&#30693;&#35782;&#20256;&#36882;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#21462;&#22810;&#26041;&#38754;&#30340;&#35821;&#20041;&#20449;&#24687;&#20197;&#22686;&#24378;&#19981;&#21516;&#39046;&#22495;&#65292;&#24182;&#23545;&#40784;&#29992;&#25143;/&#39033;&#30446;ID&#21644;&#20869;&#23481;&#35821;&#20041;&#29305;&#24449;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#29992;&#25143;&#22312;&#19982;&#19981;&#21516;&#39033;&#30446;&#20114;&#21160;&#26102;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#34892;&#20026;&#65292;&#21253;&#25324;&#38544;&#24335;&#30340;&#28857;&#20987;/&#28857;&#36190;&#20114;&#21160;&#20197;&#21450;&#26174;&#24335;&#30340;&#35780;&#35770;/&#35780;&#20215;&#20114;&#21160;&#12290;&#28982;&#32780;&#65292;&#20960;&#20046;&#25152;&#26377;&#30340;&#25512;&#33616;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#22914;&#20309;&#36890;&#36807;&#38544;&#24335;&#30340;&#28857;&#20987;/&#28857;&#36190;&#20114;&#21160;&#26469;&#25551;&#36848;&#29992;&#25143;&#20559;&#22909;&#65292;&#20197;&#25214;&#21040;&#20154;&#20204;&#20043;&#38388;&#30340;&#21327;&#21516;&#12290;&#23545;&#20110;&#22522;&#20110;&#20869;&#23481;&#30340;&#26174;&#24335;&#35780;&#35770;/&#35780;&#20215;&#20114;&#21160;&#65292;&#19968;&#20123;&#24037;&#20316;&#23581;&#35797;&#21033;&#29992;&#23427;&#20204;&#26469;&#25366;&#25496;&#35821;&#20041;&#30693;&#35782;&#20197;&#22686;&#24378;&#25512;&#33616;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20173;&#28982;&#24573;&#35270;&#20102;&#20197;&#19979;&#20004;&#28857;&#65306;&#65288;1&#65289;&#20869;&#23481;&#35821;&#20041;&#26159;&#26222;&#36866;&#30340;&#19990;&#30028;&#30693;&#35782;&#65307;&#25105;&#20204;&#22914;&#20309;&#25552;&#21462;&#22810;&#26041;&#38754;&#30340;&#35821;&#20041;&#20449;&#24687;&#20197;&#22686;&#24378;&#19981;&#21516;&#39046;&#22495;&#65311;&#65288;2&#65289;&#29992;&#25143;/&#39033;&#30446;ID&#29305;&#24449;&#26159;&#25512;&#33616;&#27169;&#22411;&#30340;&#22522;&#30784;&#35201;&#32032;&#65307;&#25105;&#20204;&#22914;&#20309;&#23545;&#40784;ID&#21644;&#20869;&#23481;&#35821;&#20041;&#29305;&#24449;&#31354;&#38388;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#25554;&#20214;&#8221;&#35821;&#20041;&#30693;&#35782;&#20256;&#36882;&#26041;&#27861;LoID&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00236v1 Announce Type: cross  Abstract: In real-world applications, users express different behaviors when they interact with different items, including implicit click/like interactions, and explicit comments/reviews interactions. Nevertheless, almost all recommender works are focused on how to describe user preferences by the implicit click/like interactions, to find the synergy of people. For the content-based explicit comments/reviews interactions, some works attempt to utilize them to mine the semantic knowledge to enhance recommender models. However, they still neglect the following two points: (1) The content semantic is a universal world knowledge; how do we extract the multi-aspect semantic information to empower different domains? (2) The user/item ID feature is a fundamental element for recommender models; how do we align the ID and content semantic feature space? In this paper, we propose a `plugin' semantic knowledge transferring method \textbf{LoID}, which inclu
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#33258;&#21160;&#21270;&#22522;&#30784;&#35774;&#26045;&#21363;&#20195;&#30721;&#32534;&#25490;&#24037;&#20316;&#30340;&#21487;&#34892;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;</title><link>https://arxiv.org/abs/2404.00227</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22522;&#30784;&#35774;&#26045;&#21363;&#20195;&#30721;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey of using Large Language Models for Generating Infrastructure as Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00227
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#33258;&#21160;&#21270;&#22522;&#30784;&#35774;&#26045;&#21363;&#20195;&#30721;&#32534;&#25490;&#24037;&#20316;&#30340;&#21487;&#34892;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#35774;&#26045;&#21363;&#20195;&#30721;&#65288;IaC&#65289;&#26159;&#19968;&#31181;&#38761;&#21629;&#24615;&#30340;&#26041;&#27861;&#65292;&#22312;&#19994;&#20869;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#31361;&#20986;&#22320;&#20301;&#12290; IaC&#36890;&#36807;&#20351;&#29992;&#21487;&#26426;&#22120;&#35835;&#21462;&#30340;&#20195;&#30721;&#26469;&#31649;&#29702;&#21644;&#25552;&#20379;IT&#22522;&#30784;&#35774;&#26045;&#65292;&#23454;&#29616;&#33258;&#21160;&#21270;&#12289;&#29615;&#22659;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12289;&#21487;&#20877;&#29616;&#24615;&#12289;&#29256;&#26412;&#25511;&#21046;&#12289;&#20943;&#23569;&#38169;&#35823;&#21644;&#22686;&#24378;&#21487;&#25193;&#23637;&#24615;&#12290;&#28982;&#32780;&#65292;IaC&#32534;&#25490;&#36890;&#24120;&#26159;&#19968;&#39033;&#36153;&#26102;&#36153;&#21147;&#30340;&#24037;&#20316;&#65292;&#38656;&#35201;&#19987;&#19994;&#25216;&#33021;&#21644;&#22823;&#37327;&#25163;&#21160;&#24037;&#20316;&#12290;&#33258;&#21160;&#21270;IaC&#22312;&#24403;&#21069;&#34892;&#19994;&#26465;&#20214;&#19979;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#65292;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#21487;&#34892;&#24615;&#12290;LLM&#26159;&#22522;&#20110;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#65292;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#65292;&#24182;&#19988;&#26174;&#31034;&#20986;&#33021;&#22815;&#22312;&#24191;&#27867;&#33539;&#22260;&#20869;&#36981;&#24490;&#19968;&#31995;&#21015;&#25351;&#20196;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;&#23427;&#20204;&#20063;&#24050;&#25104;&#21151;&#22320;&#34987;&#36716;&#21270;&#20026;&#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#65292;&#36825;&#20351;&#23427;&#20204;&#25104;&#20026;&#21487;&#33021;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00227v1 Announce Type: cross  Abstract: Infrastructure as Code (IaC) is a revolutionary approach which has gained significant prominence in the Industry. IaC manages and provisions IT infrastructure using machine-readable code by enabling automation, consistency across the environments, reproducibility, version control, error reduction and enhancement in scalability. However, IaC orchestration is often a painstaking effort which requires specialised skills as well as a lot of manual effort. Automation of IaC is a necessity in the present conditions of the Industry and in this survey, we study the feasibility of applying Large Language Models (LLM) to address this problem. LLMs are large neural network-based models which have demonstrated significant language processing abilities and shown to be capable of following a range of instructions within a broad scope. Recently, they have also been adapted for code understanding and generation tasks successfully, which makes them a p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26159;&#39318;&#27425;&#21033;&#29992;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#36827;&#34892;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#65292;&#19987;&#27880;&#20110;&#24341;&#23548;&#27169;&#22411;&#23398;&#20064;&#25152;&#38656;&#30149;&#29702;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#39069;&#22806;&#19987;&#23478;&#27880;&#37322;&#30340;&#38382;&#39064;-&#31572;&#26696;&#23545;&#35774;&#35745;&#26041;&#27861;&#65292;&#20197;&#21450;&#19968;&#31181;&#20934;&#25991;&#26412;&#29305;&#24449;&#36716;&#25442;&#22120;&#27169;&#22359;&#12290;</title><link>https://arxiv.org/abs/2404.00226</link><description>&lt;p&gt;
&#24819;&#35201;&#30340;&#35774;&#35745;&#65306;&#21033;&#29992;&#35270;&#35273;&#38382;&#31572;&#36827;&#34892;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Design as Desired: Utilizing Visual Question Answering for Multimodal Pre-training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00226
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#39318;&#27425;&#21033;&#29992;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#36827;&#34892;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#65292;&#19987;&#27880;&#20110;&#24341;&#23548;&#27169;&#22411;&#23398;&#20064;&#25152;&#38656;&#30149;&#29702;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#39069;&#22806;&#19987;&#23478;&#27880;&#37322;&#30340;&#38382;&#39064;-&#31572;&#26696;&#23545;&#35774;&#35745;&#26041;&#27861;&#65292;&#20197;&#21450;&#19968;&#31181;&#20934;&#25991;&#26412;&#29305;&#24449;&#36716;&#25442;&#22120;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#22312;&#21307;&#30103;&#39046;&#22495;&#23637;&#31034;&#20102;&#20854;&#28508;&#21147;&#65292;&#20174;&#25104;&#23545;&#30340;&#21307;&#30103;&#25253;&#21578;&#20013;&#23398;&#20064;&#21307;&#23398;&#35270;&#35273;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#39044;&#35757;&#32451;&#20219;&#21153;&#38656;&#35201;&#20020;&#24202;&#21307;&#29983;&#39069;&#22806;&#30340;&#27880;&#37322;&#65292;&#22823;&#22810;&#25968;&#20219;&#21153;&#26410;&#33021;&#26126;&#30830;&#24341;&#23548;&#27169;&#22411;&#23398;&#20064;&#19981;&#21516;&#30149;&#29702;&#29305;&#24449;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#21033;&#29992;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#36827;&#34892;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#30340;&#22242;&#38431;&#65292;&#20197;&#24341;&#23548;&#26694;&#26550;&#19987;&#27880;&#20110;&#30446;&#26631;&#30149;&#29702;&#29305;&#24449;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#21307;&#30103;&#25253;&#21578;&#20013;&#30340;&#25551;&#36848;&#35774;&#35745;&#20102;&#19982;&#19981;&#21516;&#30142;&#30149;&#30456;&#20851;&#30340;&#22810;&#31890;&#24230;&#38382;&#39064;-&#31572;&#26696;&#23545;&#65292;&#36825;&#26377;&#21161;&#20110;&#26694;&#26550;&#22312;&#39044;&#35757;&#32451;&#20013;&#26080;&#38656;&#19987;&#23478;&#39069;&#22806;&#30340;&#27880;&#37322;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#20934;&#25991;&#26412;&#29305;&#24449;&#36716;&#25442;&#22120;&#27169;&#22359;&#65292;&#26088;&#22312;&#36890;&#36807;&#23558;&#35270;&#35273;&#29305;&#24449;&#36716;&#25442;&#21040;&#25509;&#36817;&#25991;&#26412;&#39046;&#22495;&#30340;&#20934;&#25991;&#26412;&#31354;&#38388;&#26469;&#36741;&#21161;&#39044;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00226v1 Announce Type: cross  Abstract: Multimodal pre-training demonstrates its potential in the medical domain, which learns medical visual representations from paired medical reports. However, many pre-training tasks require extra annotations from clinicians, and most of them fail to explicitly guide the model to learn the desired features of different pathologies. To the best of our knowledge, we are the first to utilize Visual Question Answering (VQA) for multimodal pre-training to guide the framework focusing on targeted pathological features. In this work, we leverage descriptions in medical reports to design multi-granular question-answer pairs associated with different diseases, which assist the framework in pre-training without requiring extra annotations from experts. We also propose a novel pre-training framework with a quasi-textual feature transformer, a module designed to transform visual features into a quasi-textual space closer to the textual domain via a c
&lt;/p&gt;</description></item><item><title>&#23545;&#27604;&#23398;&#20064;&#29983;&#25104;&#30340;&#21477;&#23376;&#32423;&#31185;&#23398;&#25991;&#31456;&#23884;&#20837;&#36827;&#34892;&#20998;&#31867;&#21644;&#32858;&#31867;&#65292;&#30456;&#27604;&#22522;&#32447;&#27169;&#22411;&#65292;&#22312;&#32858;&#31867;&#21327;&#35758;&#27700;&#24179;&#19978;&#26377;&#20116;&#20493;&#22686;&#38271;</title><link>https://arxiv.org/abs/2404.00224</link><description>&lt;p&gt;
&#30001;&#23545;&#27604;&#23398;&#20064;&#29983;&#25104;&#30340;&#31185;&#23398;&#25991;&#31456;&#21477;&#23376;&#32423;&#23884;&#20837;&#30340;&#20998;&#31867;&#21644;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classification and Clustering of Sentence-Level Embeddings of Scientific Articles Generated by Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00224
&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#29983;&#25104;&#30340;&#21477;&#23376;&#32423;&#31185;&#23398;&#25991;&#31456;&#23884;&#20837;&#36827;&#34892;&#20998;&#31867;&#21644;&#32858;&#31867;&#65292;&#30456;&#27604;&#22522;&#32447;&#27169;&#22411;&#65292;&#22312;&#32858;&#31867;&#21327;&#35758;&#27700;&#24179;&#19978;&#26377;&#20116;&#20493;&#22686;&#38271;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00224v1 &#20844;&#21578;&#31867;&#22411;: &#26032; &#25277;&#35937;: &#31185;&#23398;&#25991;&#31456;&#26159;&#38271;&#25991;&#26723;&#65292;&#36890;&#24120;&#20998;&#20026;&#33509;&#24178;&#37096;&#20998;&#65292;&#27599;&#19968;&#37096;&#20998;&#25551;&#36848;&#30740;&#31350;&#30340;&#19981;&#21516;&#26041;&#38754;&#12290;&#20998;&#26512;&#31185;&#23398;&#20316;&#21697;&#21464;&#24471;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#21487;&#29992;&#25991;&#31456;&#25968;&#37327;&#30340;&#22686;&#21152;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#23545;&#21464;&#21387;&#22120;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#20174;&#31185;&#23398;&#25991;&#31456;&#20013;&#29983;&#25104;&#21477;&#23376;&#32423;&#23884;&#20837;&#65292;&#32771;&#34385;&#20197;&#19979;&#26631;&#31614;: &#32972;&#26223;&#12289;&#30446;&#26631;&#12289;&#26041;&#27861;&#12289;&#32467;&#26524;&#21644;&#32467;&#35770;&#12290;&#25105;&#20204;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#20854;&#20013;&#20004;&#20010;&#25968;&#25454;&#38598;&#26469;&#33258;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#21307;&#23398;&#39046;&#22495;&#30340;&#25991;&#31456;&#25688;&#35201;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;PMC-Sents-FULL&#65292;&#20854;&#20013;&#30340;&#21477;&#23376;&#26159;&#20174;&#21307;&#23398;&#25991;&#31456;&#30340;&#20840;&#25991;&#20013;&#25552;&#21462;&#30340;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#24494;&#35843;&#27169;&#22411;&#21644;&#22522;&#20934;&#27169;&#22411;&#22312;&#32858;&#31867;&#21644;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#20197;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#22312;&#24179;&#22343;&#24773;&#20917;&#19979;&#65292;&#32858;&#31867;&#19968;&#33268;&#24615;&#24230;&#37327;&#20540;&#39640;&#20986;&#20116;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00224v1 Announce Type: new  Abstract: Scientific articles are long text documents organized into sections, each describing aspects of the research. Analyzing scientific production has become progressively challenging due to the increase in the number of available articles. Within this scenario, our approach consisted of fine-tuning transformer language models to generate sentence-level embeddings from scientific articles, considering the following labels: background, objective, methods, results, and conclusion. We trained our models on three datasets with contrastive learning. Two datasets are from the article's abstracts in the computer science and medical domains. Also, we introduce PMC-Sents-FULL, a novel dataset of sentences extracted from the full texts of medical articles. We compare the fine-tuned and baseline models in clustering and classification tasks to evaluate our approach. On average, clustering agreement measures values were five times higher. For the classif
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#29702;&#30340;&#24847;&#35265;&#24635;&#32467;&#33539;&#24335;&#21450;&#20854;&#25552;&#21462;&#26041;&#27861; RATION&#65292;&#21487;&#29983;&#25104;&#20195;&#34920;&#24615;&#24847;&#35265;&#21644;&#30456;&#24212;&#21407;&#29702;&#65292;&#32463;&#36807;&#35780;&#20272;&#26174;&#31034;&#25552;&#21462;&#30340;&#21407;&#29702;&#20855;&#26377;&#30456;&#20851;&#24615;&#12289;&#20855;&#20307;&#24615;&#12289;&#27969;&#34892;&#24230;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.00217</link><description>&lt;p&gt;
&#22522;&#20110;&#21407;&#29702;&#30340;&#24847;&#35265;&#24635;&#32467;
&lt;/p&gt;
&lt;p&gt;
Rationale-based Opinion Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00217
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#29702;&#30340;&#24847;&#35265;&#24635;&#32467;&#33539;&#24335;&#21450;&#20854;&#25552;&#21462;&#26041;&#27861; RATION&#65292;&#21487;&#29983;&#25104;&#20195;&#34920;&#24615;&#24847;&#35265;&#21644;&#30456;&#24212;&#21407;&#29702;&#65292;&#32463;&#36807;&#35780;&#20272;&#26174;&#31034;&#25552;&#21462;&#30340;&#21407;&#29702;&#20855;&#26377;&#30456;&#20851;&#24615;&#12289;&#20855;&#20307;&#24615;&#12289;&#27969;&#34892;&#24230;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24847;&#35265;&#24635;&#32467;&#26088;&#22312;&#29983;&#25104;&#31616;&#26126;&#30340;&#25688;&#35201;&#65292;&#23637;&#31034;&#22823;&#37327;&#35780;&#35770;&#30340;&#28909;&#38376;&#24847;&#35265;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25688;&#35201;&#21487;&#33021;&#36807;&#20110;&#26222;&#36941;&#21270;&#19988;&#32570;&#20047;&#25903;&#25345;&#32454;&#33410;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24635;&#32467;&#35780;&#35770;&#30340;&#33539;&#24335;&#65292;&#21363;&#22522;&#20110;&#21407;&#29702;&#30340;&#24847;&#35265;&#24635;&#32467;&#12290;&#22522;&#20110;&#21407;&#29702;&#30340;&#24847;&#35265;&#25688;&#35201;&#36755;&#20986;&#20195;&#34920;&#24615;&#24847;&#35265;&#20197;&#21450;&#19968;&#20010;&#25110;&#22810;&#20010;&#30456;&#24212;&#30340;&#21407;&#29702;&#12290;&#20026;&#20102;&#25552;&#21462;&#22909;&#30340;&#21407;&#29702;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#22235;&#31181;&#29702;&#24819;&#30340;&#23646;&#24615;&#65306;&#30456;&#20851;&#24615;&#12289;&#20855;&#20307;&#24615;&#12289;&#27969;&#34892;&#24230;&#21644;&#22810;&#26679;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21513;&#24067;&#26031;&#21462;&#26679;&#30340;&#26041;&#27861;&#26469;&#25552;&#21462;&#21407;&#29702;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RATION&#65292;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#25688;&#21462;&#31995;&#32479;&#65292;&#23427;&#26377;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#19968;&#31181;&#24847;&#35265;&#25552;&#21462;&#22120;&#65288;&#25552;&#21462;&#20195;&#34920;&#24615;&#24847;&#35265;&#65289;&#21644;&#19968;&#31181;&#21407;&#29702;&#25552;&#21462;&#22120;&#65288;&#25552;&#21462;&#23545;&#24212;&#30340;&#21407;&#29702;&#65289;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#33258;&#21160;&#21270;&#21644;&#20154;&#24037;&#35780;&#20272;&#65292;&#20197;&#23637;&#31034;&#36890;&#36807;RATION&#25552;&#21462;&#30340;&#21407;&#29702;&#20855;&#26377;&#25152;&#25552;&#20986;&#30340;&#23646;&#24615;&#65292;&#24182;&#19988;&#20854;&#25688;&#35201;&#26159;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00217v1 Announce Type: new  Abstract: Opinion summarization aims to generate concise summaries that present popular opinions of a large group of reviews. However, these summaries can be too generic and lack supporting details. To address these issues, we propose a new paradigm for summarizing reviews, rationale-based opinion summarization. Rationale-based opinion summaries output the representative opinions as well as one or more corresponding rationales. To extract good rationales, we define four desirable properties: relatedness, specificity, popularity, and diversity and present a Gibbs-sampling-based method to extract rationales. Overall, we propose RATION, an unsupervised extractive system that has two components: an Opinion Extractor (to extract representative opinions) and Rationales Extractor (to extract corresponding rationales). We conduct automatic and human evaluations to show that rationales extracted by RATION have the proposed properties and its summaries are 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#20107;&#23454;&#35299;&#30721;&#26041;&#27861;&#25552;&#39640;&#20102;&#20107;&#23454;&#20934;&#30830;&#24615;&#65292;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20351;&#27169;&#22411;&#23545;&#24050;&#30693;&#20107;&#23454;&#36807;&#20110;&#33258;&#20449;&#65292;&#36827;&#19968;&#27493;&#35780;&#20272;&#26174;&#31034;&#22312;&#30693;&#35782;&#32534;&#36753;&#22522;&#20934;&#19978;&#25152;&#26377;&#35299;&#30721;&#26041;&#27861;&#22343;&#26174;&#33879;&#38477;&#20302;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.00216</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20107;&#23454;&#35299;&#30721;&#65306;&#22312;&#30693;&#35782;&#32534;&#36753;&#22522;&#20934;&#19978;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Is Factuality Decoding a Free Lunch for LLMs? Evaluation on Knowledge Editing Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00216
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#20107;&#23454;&#35299;&#30721;&#26041;&#27861;&#25552;&#39640;&#20102;&#20107;&#23454;&#20934;&#30830;&#24615;&#65292;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20351;&#27169;&#22411;&#23545;&#24050;&#30693;&#20107;&#23454;&#36807;&#20110;&#33258;&#20449;&#65292;&#36827;&#19968;&#27493;&#35780;&#20272;&#26174;&#31034;&#22312;&#30693;&#35782;&#32534;&#36753;&#22522;&#20934;&#19978;&#25152;&#26377;&#35299;&#30721;&#26041;&#27861;&#22343;&#26174;&#33879;&#38477;&#20302;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#20351;&#23427;&#20204;&#33021;&#22815;&#20197;&#26356;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#26041;&#24335;&#20256;&#36798;&#20107;&#23454;&#30693;&#35782;&#12290;&#20154;&#20204;&#24050;&#32463;&#20570;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#26469;&#36890;&#36807;&#20462;&#25913;LLMs&#24182;&#38477;&#20302;&#20107;&#23454;&#24187;&#35273;&#26469;&#25552;&#39640;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20462;&#25913;&#20063;&#23384;&#22312;&#38459;&#30861;&#30693;&#35782;&#26356;&#26032;&#30340;&#39118;&#38505;&#65292;&#22240;&#20026;&#23427;&#20204;&#20351;&#27169;&#22411;&#23545;&#24050;&#30693;&#20107;&#23454;&#36807;&#20110;&#33258;&#20449;&#12290;&#26412;&#25991;&#39318;&#20808;&#37325;&#26032;&#23457;&#35270;&#24403;&#21069;&#30340;&#20107;&#23454;&#35299;&#30721;&#26041;&#27861;&#65292;&#24182;&#39564;&#35777;&#20102;&#23427;&#20204;&#22312;&#25552;&#39640;&#20107;&#23454;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23545;&#20960;&#31181;&#24378;&#22823;&#30340;&#20107;&#23454;&#35299;&#30721;&#26041;&#27861;&#22312;&#30693;&#35782;&#32534;&#36753;&#22522;&#20934;&#19978;&#36827;&#34892;&#36827;&#19968;&#27493;&#35780;&#20272;&#12290;&#25152;&#26377;&#36825;&#20123;&#35299;&#30721;&#26041;&#27861;&#19982;&#20854;&#21407;&#22987;&#35299;&#30721;&#30456;&#27604;&#22343;&#26174;&#30528;&#38477;&#20302;&#20102;llama2&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20854;&#20013;&#26368;&#22823;&#30340;&#38477;&#20302;&#24133;&#24230;&#36798;&#21040;&#24778;&#20154;&#30340;81.3\%&#12290;&#36825;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#24403;&#21069;&#30340;&#35299;&#30721;&#26041;&#27861;&#20173;&#26080;&#27861;&#23436;&#20840;&#35299;&#20915;&#20107;&#23454;&#24187;&#35273;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#24573;&#35270;&#20102;&#20808;&#39564;&#30693;&#35782;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00216v1 Announce Type: cross  Abstract: The rapid development of large language models (LLMs) enables them to convey factual knowledge in a more human-like fashion. Extensive efforts have been made to reduce factual hallucinations by modifying LLMs with factuality decoding. However, they also pose risks of hindering knowledge updates, as they make models overly confident in known facts. In this work, we first revisite the current factuality decoding methods and verified their effectiveness in enhancing factual accuracy. Subsequently, we conduct further evaluation of several strong factuality decoding methods on the knowledge editing benchmark. All these decoding methods significantly diminish the performance of llama2 models compared to their original decoding, with the largest decrease being a staggering 81.3\%. This further indicates that the current existing decoding methods still cannot perfectly address the factual hallucinations, as they overlook the importance of pres
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36890;&#36807;&#30417;&#30563;&#24494;&#35843;&#26041;&#27861;&#27880;&#20837;&#26032;&#30693;&#35782;&#30340;&#25928;&#26524;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#26368;&#36817;&#20307;&#32946;&#20107;&#20214;&#39046;&#22495;&#12290;</title><link>https://arxiv.org/abs/2404.00213</link><description>&lt;p&gt;
&#36890;&#36807;&#30417;&#30563;&#24494;&#35843;&#23558;&#26032;&#30693;&#35782;&#27880;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Injecting New Knowledge into Large Language Models via Supervised Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36890;&#36807;&#30417;&#30563;&#24494;&#35843;&#26041;&#27861;&#27880;&#20837;&#26032;&#30693;&#35782;&#30340;&#25928;&#26524;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#26368;&#36817;&#20307;&#32946;&#20107;&#20214;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#34987;&#35777;&#26126;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#26159;&#19968;&#39033;&#23453;&#36149;&#30340;&#36164;&#20135;&#12290;&#28982;&#32780;&#65292;&#20351;&#36825;&#20123;&#27169;&#22411;&#36866;&#24212;&#24182;&#25972;&#21512;&#26032;&#30340;&#39046;&#22495;&#30693;&#35782;&#20173;&#28982;&#26159;&#19968;&#39033;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#27169;&#22411;&#30693;&#35782;&#25130;&#27490;&#26085;&#26399;&#20043;&#21518;&#21457;&#29983;&#30340;&#20107;&#23454;&#21644;&#20107;&#20214;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#20316;&#20026;LLMs&#20013;&#27880;&#20837;&#30693;&#35782;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#26368;&#36817;&#20307;&#32946;&#20107;&#20214;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00213v1 Announce Type: new  Abstract: In recent years, Large Language Models (LLMs) have shown remarkable performance in generating human-like text, proving to be a valuable asset across various applications. However, adapting these models to incorporate new, out-of-domain knowledge remains a challenge, particularly for facts and events that occur after the model's knowledge cutoff date. This paper investigates the effectiveness of Supervised Fine-Tuning (SFT) as a method for knowledge injection in LLMs, specifically focusing on the domain of recent sporting events. We compare different dataset generation strategies -- token-based and fact-based scaling -- to create training data that helps the model learn new information. Our experiments on GPT-4 demonstrate that while token-based scaling can lead to improvements in Q&amp;A accuracy, it may not provide uniform coverage of new knowledge. Fact-based scaling, on the other hand, offers a more systematic approach to ensure even cove
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#35299;&#25512;&#29702;&#26041;&#27861;(MCRank)&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#26465;&#20214;&#25490;&#24207;&#20219;&#21153;&#20013;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.00211</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19979;&#30340;&#22810;&#26465;&#20214;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Multi-Conditional Ranking with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00211
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#35299;&#25512;&#29702;&#26041;&#27861;(MCRank)&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#26465;&#20214;&#25490;&#24207;&#20219;&#21153;&#20013;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23545;&#19968;&#32452;&#39033;&#30446;&#36827;&#34892;&#25490;&#24207;&#24050;&#25104;&#20026;&#25512;&#33616;&#21644;&#26816;&#32034;&#31995;&#32479;&#20013;&#30340;&#24120;&#35265;&#26041;&#27861;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#24182;&#25506;&#35752;&#20102;&#22810;&#26465;&#20214;&#25490;&#24207;&#30340;&#20219;&#21153;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;MCRank&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#36328;&#19981;&#21516;&#39033;&#30446;&#31867;&#22411;&#21644;&#26465;&#20214;&#36827;&#34892;&#22810;&#26465;&#20214;&#25490;&#24207;&#12290;&#25105;&#20204;&#20351;&#29992;MCRank&#23545;LLMs&#36827;&#34892;&#20998;&#26512;&#34920;&#26126;&#65292;&#38543;&#30528;&#39033;&#30446;&#21644;&#26465;&#20214;&#25968;&#37327;&#20197;&#21450;&#22797;&#26434;&#24615;&#30340;&#22686;&#38271;&#65292;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#35299;&#25512;&#29702;&#26041;&#27861;&#65292;&#21253;&#25324;&#25552;&#21462;&#21644;&#25490;&#24207;&#26465;&#20214;&#65292;&#28982;&#21518;&#36845;&#20195;&#22320;&#23545;&#26465;&#20214;&#36827;&#34892;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00211v1 Announce Type: new  Abstract: Utilizing large language models (LLMs) to rank a set of items has become a common approach in recommendation and retrieval systems. Typically, these systems focus on ordering a substantial number of documents in a monotonic order based on a given query. However, real-world scenarios often present a different challenge: ranking a comparatively smaller set of items, but according to a variety of diverse and occasionally conflicting conditions. In this paper, we define and explore the task of multi-conditional ranking by introducing MCRank, a benchmark tailored for assessing multi-conditional ranking across various item types and conditions. Our analysis of LLMs using MCRank indicates a significant decrease in performance as the number and complexity of items and conditions grow. To overcome this limitation, we propose a novel decomposed reasoning method, consisting of EXtracting and Sorting the conditions, and then Iterativly Ranking the i
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;EventGround&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#23558;&#33258;&#30001;&#25991;&#26412;&#19982;&#20197;&#20107;&#20214;&#20026;&#20013;&#24515;&#30340;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#20851;&#32852;&#30340;&#38382;&#39064;&#65292;&#20197;&#36827;&#34892;&#24773;&#22659;&#21270;&#21465;&#20107;&#25512;&#29702;</title><link>https://arxiv.org/abs/2404.00209</link><description>&lt;p&gt;
EventGround&#65306;&#36890;&#36807;&#22522;&#20110;&#20107;&#20214;&#20026;&#20013;&#24515;&#30340;&#30693;&#35782;&#22270;&#35889;&#23454;&#29616;&#21465;&#20107;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
EventGround: Narrative Reasoning by Grounding to Eventuality-centric Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00209
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;EventGround&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#23558;&#33258;&#30001;&#25991;&#26412;&#19982;&#20197;&#20107;&#20214;&#20026;&#20013;&#24515;&#30340;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#20851;&#32852;&#30340;&#38382;&#39064;&#65292;&#20197;&#36827;&#34892;&#24773;&#22659;&#21270;&#21465;&#20107;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21465;&#20107;&#25512;&#29702;&#20381;&#36182;&#20110;&#23545;&#25925;&#20107;&#24773;&#22659;&#20013;&#20107;&#20214;&#30340;&#29702;&#35299;&#65292;&#36825;&#38656;&#35201;&#20016;&#23500;&#30340;&#32972;&#26223;&#19990;&#30028;&#30693;&#35782;&#12290;&#20026;&#20102;&#24110;&#21161;&#26426;&#22120;&#21033;&#29992;&#36825;&#31181;&#30693;&#35782;&#65292;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#21487;&#20998;&#20026;&#20004;&#31867;&#12290;&#19968;&#20123;&#20391;&#37325;&#20110;&#36890;&#36807;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#20107;&#20214;&#24863;&#30693;&#30446;&#26631;&#26469;&#38544;&#24335;&#24314;&#27169;&#20107;&#20214;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#20250;&#30772;&#22351;&#30693;&#35782;&#32467;&#26500;&#24182;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#21478;&#19968;&#20123;&#21017;&#23558;&#20107;&#20214;&#30340;&#19990;&#30028;&#30693;&#35782;&#26126;&#30830;&#22320;&#25910;&#38598;&#21040;&#32467;&#26500;&#21270;&#30340;&#20197;&#20107;&#20214;&#20026;&#20013;&#24515;&#30340;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#20013;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20851;&#20110;&#21033;&#29992;&#36825;&#20123;&#30693;&#35782;&#28304;&#36827;&#34892;&#33258;&#30001;&#25991;&#26412;&#22788;&#29702;&#30340;&#30740;&#31350;&#26377;&#38480;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;EventGround&#30340;&#21021;&#27493;&#32508;&#21512;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#23558;&#33258;&#30001;&#25991;&#26412;&#19982;&#20197;&#20107;&#20214;&#20026;&#20013;&#24515;&#30340;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#20851;&#32852;&#30340;&#38382;&#39064;&#65292;&#20197;&#36827;&#34892;&#24773;&#22659;&#21270;&#21465;&#20107;&#25512;&#29702;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#30340;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#20107;&#20214;&#34920;&#24449;&#21644;&#31232;&#30095;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00209v1 Announce Type: new  Abstract: Narrative reasoning relies on the understanding of eventualities in story contexts, which requires a wealth of background world knowledge. To help machines leverage such knowledge, existing solutions can be categorized into two groups. Some focus on implicitly modeling eventuality knowledge by pretraining language models (LMs) with eventuality-aware objectives. However, this approach breaks down knowledge structures and lacks interpretability. Others explicitly collect world knowledge of eventualities into structured eventuality-centric knowledge graphs (KGs). However, existing research on leveraging these knowledge sources for free-texts is limited. In this work, we propose an initial comprehensive framework called EventGround, which aims to tackle the problem of grounding free-texts to eventuality-centric KGs for contextualized narrative reasoning. We identify two critical problems in this direction: the event representation and sparsi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#20316;&#21160;&#24577;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22240;&#26524;&#20272;&#35745;Incremental Stylistic Effect&#26469;&#35299;&#37322;&#22914;&#20309;&#25913;&#21464;&#21512;&#20316;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2404.00207</link><description>&lt;p&gt;
&#20154;&#31867;-&#35821;&#35328;&#27169;&#22411;&#21327;&#20316;&#30340;&#22240;&#26524;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Causal Inference for Human-Language Model Collaboration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#20316;&#21160;&#24577;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22240;&#26524;&#20272;&#35745;Incremental Stylistic Effect&#26469;&#35299;&#37322;&#22914;&#20309;&#25913;&#21464;&#21512;&#20316;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20154;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#20043;&#38388;&#30340;&#21327;&#20316;&#21160;&#24577;&#65292;&#36825;&#20123;&#20114;&#21160;&#36890;&#24120;&#28041;&#21450;LMs&#25552;&#20986;&#25991;&#26412;&#27573;&#33853;&#65292;&#32780;&#20154;&#31867;&#32534;&#36753;&#25110;&#22238;&#24212;&#36825;&#20123;&#24314;&#35758;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#19982;LMs&#36827;&#34892;&#26377;&#25928;&#30340;&#20114;&#21160;&#35201;&#27714;&#20154;&#31867;&#36776;&#21035;&#20986;&#26377;&#25928;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#20114;&#21160;&#31574;&#30053;&#65292;&#20363;&#22914;&#32534;&#36753;&#21644;&#22238;&#24212;&#26679;&#24335;&#65292;&#20174;&#21382;&#21490;&#20154;&#31867;-LM&#20114;&#21160;&#20013;&#12290;&#36825;&#20010;&#30446;&#26631;&#26412;&#36136;&#19978;&#26159;&#22240;&#26524;&#20851;&#31995;&#65292;&#21463;&#21040;&#21453;&#20107;&#23454;&#8220;&#22914;&#26524;&#8221;&#38382;&#39064;&#30340;&#39537;&#21160;:&#22914;&#26524;&#20154;&#31867;&#37319;&#29992;&#19981;&#21516;&#30340;&#25991;&#26412;&#32534;&#36753;/&#31934;&#28860;&#31574;&#30053;&#65292;&#21327;&#20316;&#30340;&#32467;&#26524;&#20250;&#22914;&#20309;&#25913;&#21464;&#65311;&#22238;&#31572;&#36825;&#20010;&#22240;&#26524;&#25512;&#26029;&#38382;&#39064;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#21046;&#23450;&#19968;&#20010;&#36866;&#24403;&#30340;&#22240;&#26524;&#20272;&#35745;:&#20256;&#32479;&#30340;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;ATE&#65289;&#20272;&#35745;&#30001;&#20110;&#25991;&#26412;&#30340;&#39640;&#32500;&#24230;&#32780;&#19981;&#36866;&#29992;&#20110;&#22522;&#20110;&#25991;&#26412;&#30340;&#22788;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22240;&#26524;&#20272;&#35745; - &#22686;&#37327;&#39118;&#26684;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00207v1 Announce Type: cross  Abstract: In this paper, we examine the collaborative dynamics between humans and language models (LMs), where the interactions typically involve LMs proposing text segments and humans editing or responding to these proposals. Productive engagement with LMs in such scenarios necessitates that humans discern effective text-based interaction strategies, such as editing and response styles, from historical human-LM interactions. This objective is inherently causal, driven by the counterfactual `what-if' question: how would the outcome of collaboration change if humans employed a different text editing/refinement strategy? A key challenge in answering this causal inference question is formulating an appropriate causal estimand: the conventional average treatment effect (ATE) estimand is inapplicable to text-based treatments due to their high dimensionality. To address this concern, we introduce a new causal estimand -- Incremental Stylistic Effect (
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27010;&#24565;&#21270;&#26694;&#26550;&#65292;&#24378;&#21046;&#35821;&#35328;&#27169;&#22411;&#22312;&#25277;&#35937;&#38382;&#39064;&#19978;&#36827;&#34892;&#27010;&#24565;&#25512;&#29702;&#65292;&#25581;&#31034;&#29616;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27010;&#24565;&#25512;&#29702;&#26041;&#38754;&#30340;&#19981;&#36275;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#25913;&#36827;&#27169;&#22411;&#26469;&#23454;&#29616;&#39640;&#32423;&#25277;&#35937;&#25512;&#29702;&#65292;&#20174;&#32780;&#20419;&#36827;&#26080;&#20559;&#21644;&#27867;&#21270;&#20915;&#31574;&#12290;</title><link>https://arxiv.org/abs/2404.00205</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#27010;&#24565;&#24615;&#21644;&#26080;&#20559;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Conceptual and Unbiased Reasoning in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00205
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27010;&#24565;&#21270;&#26694;&#26550;&#65292;&#24378;&#21046;&#35821;&#35328;&#27169;&#22411;&#22312;&#25277;&#35937;&#38382;&#39064;&#19978;&#36827;&#34892;&#27010;&#24565;&#25512;&#29702;&#65292;&#25581;&#31034;&#29616;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27010;&#24565;&#25512;&#29702;&#26041;&#38754;&#30340;&#19981;&#36275;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#25913;&#36827;&#27169;&#22411;&#26469;&#23454;&#29616;&#39640;&#32423;&#25277;&#35937;&#25512;&#29702;&#65292;&#20174;&#32780;&#20419;&#36827;&#26080;&#20559;&#21644;&#27867;&#21270;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#25512;&#29702;&#65292;&#21363;&#22312;&#25277;&#35937;&#21644;&#39640;&#23618;&#27425;&#35270;&#35282;&#36827;&#34892;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#26159;&#20154;&#31867;&#35748;&#30693;&#20013;&#27867;&#21270;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25191;&#34892;&#27010;&#24565;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#26377;&#38480;&#30340;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24357;&#21512;&#20102;&#36825;&#19968;&#24046;&#36317;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27010;&#24565;&#21270;&#26694;&#26550;&#65292;&#24378;&#21046;&#27169;&#22411;&#22312;&#25277;&#35937;&#38382;&#39064;&#19978;&#36827;&#34892;&#27010;&#24565;&#25512;&#29702;&#65292;&#24182;&#22312;&#21487;&#39564;&#35777;&#30340;&#31526;&#21495;&#31354;&#38388;&#20013;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#12290;&#21033;&#29992;&#36825;&#20010;&#26694;&#26550;&#20316;&#20026;&#20998;&#26512;&#24037;&#20855;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27010;&#24565;&#25512;&#29702;&#26041;&#38754;&#30340;&#19981;&#36275;&#65292;&#19982;&#30452;&#25509;&#25512;&#29702;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#19979;&#38477;&#20102;9%&#33267;28%&#12290;&#28982;&#21518;&#25105;&#20204;&#35752;&#35770;&#20102;&#27169;&#22411;&#22914;&#20309;&#25913;&#36827;&#65292;&#22240;&#20026;&#39640;&#32423;&#25277;&#35937;&#25512;&#29702;&#26159;&#26080;&#20559;&#21644;&#27867;&#21270;&#20915;&#31574;&#30340;&#20851;&#38190;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#25216;&#26415;&#65292;&#36890;&#36807;&#29983;&#25104;&#20855;&#26377;&#31867;&#20284;&#28508;&#22312;&#25512;&#29702;&#36335;&#24452;&#30340;&#29087;&#24713;&#38382;&#39064;&#24182;&#35201;&#27714;&#27169;&#22411;&#25191;&#34892;&#33258;&#25105;&#21442;&#29031;&#65292;&#26469;&#28155;&#21152;&#21487;&#20449;&#30340;&#24402;&#32435;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00205v1 Announce Type: new  Abstract: Conceptual reasoning, the ability to reason in abstract and high-level perspectives, is key to generalization in human cognition. However, limited study has been done on large language models' capability to perform conceptual reasoning. In this work, we bridge this gap and propose a novel conceptualization framework that forces models to perform conceptual reasoning on abstract questions and generate solutions in a verifiable symbolic space. Using this framework as an analytical tool, we show that existing large language models fall short on conceptual reasoning, dropping 9% to 28% on various benchmarks compared to direct inference methods. We then discuss how models can improve since high-level abstract reasoning is key to unbiased and generalizable decision-making. We propose two techniques to add trustworthy induction signals by generating familiar questions with similar underlying reasoning paths and asking models to perform self-ref
&lt;/p&gt;</description></item><item><title>GPTA&#24341;&#20837;&#20102;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36741;&#21161;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#21069;&#32512;&#25552;&#31034;&#22686;&#24378;&#20102;&#19979;&#28216;&#20219;&#21153;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#19981;&#20165;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#36824;&#26377;&#25928;&#20943;&#23569;&#20102;&#20302;&#36164;&#28304;&#22330;&#26223;&#19979;&#30340;&#36807;&#25311;&#21512;&#12290;</title><link>https://arxiv.org/abs/2404.00189</link><description>&lt;p&gt;
GPTA&#65306;&#29983;&#25104;&#25552;&#31034;&#35843;&#25972;&#21161;&#25163;&#29992;&#20110;LLM&#30340;&#21327;&#21516;&#19979;&#28216;&#31070;&#32463;&#32593;&#32476;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
GPTA: Generative Prompt Tuning Assistant for Synergistic Downstream Neural Network Enhancement with LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00189
&lt;/p&gt;
&lt;p&gt;
GPTA&#24341;&#20837;&#20102;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36741;&#21161;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#21069;&#32512;&#25552;&#31034;&#22686;&#24378;&#20102;&#19979;&#28216;&#20219;&#21153;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#19981;&#20165;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#36824;&#26377;&#25928;&#20943;&#23569;&#20102;&#20302;&#36164;&#28304;&#22330;&#26223;&#19979;&#30340;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;GPTA&#65292;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36741;&#21161;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#21069;&#32512;&#25552;&#31034;&#22686;&#24378;&#20102;&#19979;&#28216;&#20219;&#21153;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#36890;&#36807;&#26368;&#23567;&#21270;LLM&#23545;&#25968;&#25454;&#30340;&#26292;&#38706;&#65292;&#35813;&#26694;&#26550;&#35299;&#20915;&#20102;&#22312;&#19979;&#28216;&#20219;&#21153;&#27169;&#22411;&#35757;&#32451;&#20013;&#24212;&#29992;LLM&#25152;&#38754;&#20020;&#30340;&#23433;&#20840;&#21644;&#27861;&#24459;&#25361;&#25112;&#12290;GPTA&#21033;&#29992;&#19968;&#31181;&#26032;&#30340;&#21327;&#21516;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#21442;&#25968;&#26799;&#24230;&#20248;&#21270;&#19979;&#28216;&#27169;&#22411;&#21644;LLM&#65292;&#26032;&#30340;&#8220;&#23545;&#35805;&#26799;&#24230;&#8221;&#12290;&#35813;&#26694;&#26550;&#19981;&#20165;&#22312;&#20845;&#20010;NLP&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20986;&#27169;&#22411;&#24615;&#33021;&#30340;&#26174;&#33879;&#25913;&#36827;&#65292;&#32780;&#19988;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#20302;&#36164;&#28304;&#22330;&#26223;&#19979;&#30340;&#36807;&#25311;&#21512;&#12290;&#35814;&#32454;&#20998;&#26512;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#39318;&#21019;&#24615;&#26694;&#26550;&#20026;&#20855;&#26377;LLM&#25903;&#25345;&#30340;&#19979;&#28216;&#20219;&#21153;&#27169;&#22411;&#35757;&#32451;&#25552;&#20379;&#20102;&#19968;&#31181;&#25104;&#26412;&#39640;&#25928;&#19988;&#36866;&#24212;&#24615;&#24378;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00189v1 Announce Type: new  Abstract: This study introduces GPTA, a Large Language Model assistance training framework, that enhances the training of downstream task models via prefix prompt. By minimizing data exposure to LLM, the framework addresses the security and legal challenges of applying LLM in downstream task model training. GPTA utilizes a new synergistic training approach, optimizing the downstream models with parameter gradients and LLMs with the novel ``dialogue gradient''. The framework not only demonstrates significant improvements in model performance across six NLP benchmark datasets, but also reduces overfitting in low-resource scenarios effectively. The detailed analyses further validate that our pioneer framework provides a cost-efficient and adaptive method for downstream task model training with LLM support.
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#20102; OpenAI &#30340; GPT-3.5 &#27169;&#22411;&#20316;&#20026;&#8220;&#35821;&#35328;&#25968;&#25454;&#31185;&#23398;&#23478;&#8221;&#65292;&#25104;&#21151;&#22238;&#31572;&#20102;&#19982;&#22522;&#20934;&#25968;&#25454;&#38598;&#30456;&#20851;&#30340;&#25968;&#25454;&#31185;&#23398;&#26597;&#35810;&#12290;</title><link>https://arxiv.org/abs/2404.00188</link><description>&lt;p&gt;
DataAgent: &#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22238;&#31572;&#38646;&#26679;&#26412;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
DataAgent: Evaluating Large Language Models' Ability to Answer Zero-Shot, Natural Language Queries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00188
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20102; OpenAI &#30340; GPT-3.5 &#27169;&#22411;&#20316;&#20026;&#8220;&#35821;&#35328;&#25968;&#25454;&#31185;&#23398;&#23478;&#8221;&#65292;&#25104;&#21151;&#22238;&#31572;&#20102;&#19982;&#22522;&#20934;&#25968;&#25454;&#38598;&#30456;&#20851;&#30340;&#25968;&#25454;&#31185;&#23398;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#25968;&#25454;&#38598;&#20998;&#26512;&#21644;&#25552;&#21462;&#26377;&#24847;&#20041;&#20449;&#24687;&#30340;&#36807;&#31243;&#24448;&#24448;&#32791;&#26102;&#19988;&#32321;&#29712;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#30830;&#23450;&#25163;&#21160;&#12289;&#37325;&#22797;&#24615;&#32534;&#30721;&#21644;&#25968;&#25454;&#25910;&#38598;&#26159;&#38459;&#30861;&#25968;&#25454;&#31185;&#23398;&#23478;&#20174;&#20107;&#26356;&#24494;&#22937;&#30340;&#24037;&#20316;&#21644;&#39640;&#27700;&#24179;&#39033;&#30446;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102; OpenAI &#30340; GPT-3.5 &#20316;&#20026;&#8220;&#35821;&#35328;&#25968;&#25454;&#31185;&#23398;&#23478;&#8221;&#65288;LDS&#65289;&#65292;&#21487;&#20197;&#20174;&#32473;&#23450;&#25968;&#25454;&#38598;&#20013;&#25512;&#23548;&#20986;&#20851;&#38190;&#21457;&#29616;&#65292;&#21253;&#25324;&#30456;&#20851;&#24615;&#21644;&#22522;&#26412;&#20449;&#24687;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#26631;&#20934;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#21253;&#25324;&#22522;&#20110;&#25968;&#25454;&#31185;&#23398;&#20195;&#30721;&#29983;&#25104;&#30340;&#20219;&#21153;&#65292;&#28041;&#21450;NumPy&#12289;Pandas&#12289;Scikit-Learn &#21644; TensorFlow &#31561;&#24211;&#65292;&#24182;&#22312;&#27491;&#30830;&#22238;&#31572;&#19982;&#22522;&#20934;&#25968;&#25454;&#38598;&#30456;&#20851;&#30340;&#32473;&#23450;&#25968;&#25454;&#31185;&#23398;&#26597;&#35810;&#26041;&#38754;&#21462;&#24471;&#20102;&#24191;&#27867;&#25104;&#21151;&#12290;LDS &#20351;&#29992;&#20102;&#21508;&#31181;&#26032;&#39062;&#30340;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#26469;&#26377;&#25928;&#22238;&#31572;&#32473;&#23450;&#38382;&#39064;&#65292;&#21253;&#25324;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00188v1 Announce Type: cross  Abstract: Conventional processes for analyzing datasets and extracting meaningful information are often time-consuming and laborious. Previous work has identified manual, repetitive coding and data collection as major obstacles that hinder data scientists from undertaking more nuanced labor and high-level projects. To combat this, we evaluated OpenAI's GPT-3.5 as a "Language Data Scientist" (LDS) that can extrapolate key findings, including correlations and basic information, from a given dataset. The model was tested on a diverse set of benchmark datasets to evaluate its performance across multiple standards, including data science code-generation based tasks involving libraries such as NumPy, Pandas, Scikit-Learn, and TensorFlow, and was broadly successful in correctly answering a given data science query related to the benchmark dataset. The LDS used various novel prompt engineering techniques to effectively answer a given question, including
&lt;/p&gt;</description></item><item><title>Word Ladders&#26159;&#19968;&#20010;&#29992;&#20110;&#35821;&#20041;&#25968;&#25454;&#25910;&#38598;&#30340;&#31227;&#21160;&#24212;&#29992;&#65292;&#21487;&#36890;&#36807;&#20998;&#31867;&#21253;&#21547;&#30340;&#35821;&#20041;&#20851;&#31995;&#24314;&#31435;&#21333;&#35789;&#21015;&#34920;&#65292;&#20855;&#26377;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20197;&#21450;&#35748;&#30693;&#31185;&#23398;&#38382;&#39064;&#35843;&#26597;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.00184</link><description>&lt;p&gt;
&#21333;&#35789;&#38454;&#26799;&#65306;&#35821;&#20041;&#25968;&#25454;&#25910;&#38598;&#30340;&#31227;&#21160;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Word Ladders: A Mobile Application for Semantic Data Collection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00184
&lt;/p&gt;
&lt;p&gt;
Word Ladders&#26159;&#19968;&#20010;&#29992;&#20110;&#35821;&#20041;&#25968;&#25454;&#25910;&#38598;&#30340;&#31227;&#21160;&#24212;&#29992;&#65292;&#21487;&#36890;&#36807;&#20998;&#31867;&#21253;&#21547;&#30340;&#35821;&#20041;&#20851;&#31995;&#24314;&#31435;&#21333;&#35789;&#21015;&#34920;&#65292;&#20855;&#26377;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20197;&#21450;&#35748;&#30693;&#31185;&#23398;&#38382;&#39064;&#35843;&#26597;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Word Ladders&#26159;&#19968;&#20010;&#20813;&#36153;&#30340;&#31227;&#21160;&#24212;&#29992;&#65292;&#21487;&#29992;&#20110;Android&#21644;iOS&#65292;&#19987;&#20026;&#25910;&#38598;&#35821;&#35328;&#25968;&#25454;&#32780;&#24320;&#21457;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#20998;&#31867;&#21253;&#21547;&#35821;&#20041;&#20851;&#31995;&#30456;&#20851;&#30340;&#21333;&#35789;&#21015;&#34920;&#65292;&#21253;&#25324;&#22312;&#25277;&#35937;&#39033;&#30446;&#65288;ERC-2021-STG-101039777&#65289;&#20013;&#12290;&#25105;&#20204;&#22312;&#27492;&#25552;&#20379;Word Ladders&#30340;&#27010;&#36848;&#65292;&#35299;&#37322;&#20854;&#28216;&#25103;&#36923;&#36753;&#12289;&#21160;&#26426;&#20197;&#21450;&#39044;&#26399;&#32467;&#26524;&#21644;&#24212;&#29992;&#21040;nlp&#20219;&#21153;&#20197;&#21450;&#23545;&#35748;&#30693;&#31185;&#23398;&#24320;&#25918;&#38382;&#39064;&#30340;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00184v1 Announce Type: new  Abstract: Word Ladders is a free mobile application for Android and iOS, developed for collecting linguistic data, specifically lists of words related to each other through semantic relations of categorical inclusion, within the Abstraction project (ERC-2021-STG-101039777). We hereby provide an overview of Word Ladders, explaining its game logic, motivation and expected results and applications to nlp tasks as well as to the investigation of cognitive scientific open questions
&lt;/p&gt;</description></item><item><title>LSCD&#22522;&#20934;&#27979;&#35797;&#25552;&#20379;&#20102;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;LSCD&#35780;&#20272;&#24179;&#21488;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#35780;&#20272;&#21644;&#32467;&#26524;&#22797;&#29616;&#20013;&#23384;&#22312;&#30340;&#24322;&#36136;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.00176</link><description>&lt;p&gt;
LSCD&#22522;&#20934;&#27979;&#35797;&#65306;&#21382;&#26102;&#35789;&#20041;&#20219;&#21153;&#30340;&#27979;&#35797;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
The LSCD Benchmark: a Testbed for Diachronic Word Meaning Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00176
&lt;/p&gt;
&lt;p&gt;
LSCD&#22522;&#20934;&#27979;&#35797;&#25552;&#20379;&#20102;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;LSCD&#35780;&#20272;&#24179;&#21488;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#35780;&#20272;&#21644;&#32467;&#26524;&#22797;&#29616;&#20013;&#23384;&#22312;&#30340;&#24322;&#36136;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#27719;&#35821;&#20041;&#21464;&#21270;&#26816;&#27979;&#65288;LSCD&#65289;&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#35789;&#20803;&#32423;&#20219;&#21153;&#65292;&#36890;&#24120;&#22522;&#20110;&#20004;&#20010;&#36830;&#32493;&#24212;&#29992;&#30340;&#20351;&#29992;&#32423;&#20219;&#21153;&#26469;&#25805;&#20316;&#65306;&#39318;&#20808;&#65292;&#20026;&#20351;&#29992;&#23545;&#24471;&#21040;Word-in-Context (WiC)&#26631;&#31614;&#12290;&#28982;&#21518;&#65292;&#22312;&#22270;&#19978;&#34920;&#31034;&#36825;&#20123;&#26631;&#31614;&#65292;&#23545;&#20854;&#24212;&#29992;Word Sense Induction (WSI)&#26469;&#25512;&#23548;&#20986;&#21547;&#20041;&#32858;&#31867;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#27604;&#36739;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#21547;&#20041;&#32858;&#31867;&#26469;&#25512;&#23548;&#20986;LSCD&#26631;&#31614;&#12290;&#36825;&#31181;&#27169;&#22359;&#21270;&#21453;&#26144;&#22312;&#22823;&#22810;&#25968;LSCD&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#20013;&#12290;&#36825;&#20063;&#23548;&#33268;&#20102;&#24314;&#27169;&#36873;&#25321;&#21644;&#20219;&#21153;&#23450;&#20041;&#19978;&#30340;&#22823;&#37327;&#24322;&#36136;&#24615;&#65292;&#36825;&#19968;&#28857;&#21448;&#22240;&#21508;&#31181;&#25968;&#25454;&#38598;&#29256;&#26412;&#12289;&#39044;&#22788;&#29702;&#36873;&#39033;&#21644;&#35780;&#20272;&#25351;&#26631;&#32780;&#21152;&#21095;&#12290;&#36825;&#31181;&#24322;&#36136;&#24615;&#20351;&#24471;&#22312;&#21487;&#27604;&#26465;&#20214;&#19979;&#35780;&#20272;&#27169;&#22411;&#12289;&#36873;&#25321;&#26368;&#20339;&#27169;&#22411;&#32452;&#21512;&#25110;&#22797;&#29616;&#32467;&#26524;&#21464;&#24471;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#24211;&#65292;&#20197;&#35268;&#33539;LSCD&#35780;&#20272;&#12290;&#36890;&#36807;&#36879;&#26126;&#30340;&#23454;&#29616;&#65292;&#32467;&#26524;&#21464;&#24471;&#26131;&#20110;&#22797;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00176v1 Announce Type: new  Abstract: Lexical Semantic Change Detection (LSCD) is a complex, lemma-level task, which is usually operationalized based on two subsequently applied usage-level tasks: First, Word-in-Context (WiC) labels are derived for pairs of usages. Then, these labels are represented in a graph on which Word Sense Induction (WSI) is applied to derive sense clusters. Finally, LSCD labels are derived by comparing sense clusters over time. This modularity is reflected in most LSCD datasets and models. It also leads to a large heterogeneity in modeling options and task definitions, which is exacerbated by a variety of dataset versions, preprocessing options and evaluation metrics. This heterogeneity makes it difficult to evaluate models under comparable conditions, to choose optimal model combinations or to reproduce results. Hence, we provide a benchmark repository standardizing LSCD evaluation. Through transparent implementation results become easily reproducib
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20010;&#20154;&#23545;&#20307;&#39564;&#30340;&#24320;&#25918;&#24615;&#36825;&#19968;&#20154;&#26684;&#32500;&#24230;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#20010;&#20307;&#30340;&#35895;&#27468;&#25628;&#32034;&#21382;&#21490;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#30456;&#20284;&#24615;&#29305;&#24449;&#22522;&#20110;&#20010;&#20154;&#25991;&#26412;&#35821;&#26009;&#24211;&#26469;&#35299;&#37322;35%&#30340;&#24320;&#25918;&#24615;&#26041;&#24046;&#12290;</title><link>https://arxiv.org/abs/2404.00165</link><description>&lt;p&gt;
&#20010;&#20154;&#25991;&#26412;&#35821;&#26009;&#24211;&#39044;&#27979;&#24320;&#25918;&#24615;&#12289;&#20852;&#36259;&#12289;&#30693;&#35782;&#21644;&#25945;&#32946;&#27700;&#24179;
&lt;/p&gt;
&lt;p&gt;
Individual Text Corpora Predict Openness, Interests, Knowledge and Level of Education
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20010;&#20154;&#23545;&#20307;&#39564;&#30340;&#24320;&#25918;&#24615;&#36825;&#19968;&#20154;&#26684;&#32500;&#24230;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#20010;&#20307;&#30340;&#35895;&#27468;&#25628;&#32034;&#21382;&#21490;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#30456;&#20284;&#24615;&#29305;&#24449;&#22522;&#20110;&#20010;&#20154;&#25991;&#26412;&#35821;&#26009;&#24211;&#26469;&#35299;&#37322;35%&#30340;&#24320;&#25918;&#24615;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20010;&#20307;&#23545;&#20307;&#39564;&#30340;&#24320;&#25918;&#24615;&#36825;&#19968;&#20154;&#26684;&#32500;&#24230;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#20010;&#20307;&#30340;&#35895;&#27468;&#25628;&#32034;&#21382;&#21490;&#36827;&#34892;&#39044;&#27979;&#12290;&#36890;&#36807;&#32593;&#32476;&#25235;&#21462;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#26469;&#33258;214&#21517;&#21442;&#19982;&#32773;&#30340;&#20010;&#20154;&#25991;&#26412;&#35821;&#26009;&#24211;&#65292;&#24179;&#22343;&#35789;&#27719;&#37327;&#20026;500&#19975;&#20010;&#35789;&#20803;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;word2vec&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#27599;&#20010;&#20010;&#20154;&#25991;&#26412;&#35821;&#26009;&#24211;&#19982;&#26631;&#35760;&#21333;&#35789;&#30340;&#30456;&#20284;&#24615;&#65292;&#36825;&#20123;&#26631;&#35760;&#21333;&#35789;&#26469;&#33258;&#20110;&#20154;&#26684;&#30340;&#35789;&#27719;&#26041;&#27861;&#12290;&#36825;&#20123;&#20010;&#20154;&#25991;&#26412;&#35821;&#26009;&#24211;-&#26631;&#35760;&#21333;&#35789;&#30340;&#30456;&#20284;&#24615;&#34987;&#29992;&#20316;&#31070;&#32463;&#27169;&#22411;&#20013;&#30340;&#39044;&#27979;&#29305;&#24449;&#12290;&#20026;&#20102;&#35757;&#32451;&#21644;&#39564;&#35777;&#65292;&#25105;&#20204;&#20381;&#36182;179&#21517;&#21442;&#19982;&#32773;&#65292;&#24182;&#20445;&#30041;&#20102;35&#21517;&#21442;&#19982;&#32773;&#30340;&#27979;&#35797;&#26679;&#26412;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#24102;&#26377;&#19981;&#21516;&#25968;&#37327;&#39044;&#27979;&#29305;&#24449;&#12289;&#38544;&#34255;&#21333;&#20803;&#21644;&#22686;&#37327;&#22240;&#23376;&#30340;&#32593;&#26684;&#25628;&#32034;&#12290;&#20316;&#20026;&#27169;&#22411;&#36873;&#25321;&#26631;&#20934;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#22312;&#39564;&#35777;&#26679;&#26412;&#20013;&#30001;&#32477;&#23545;R2&#24046;&#24322;&#24809;&#32602;&#30340;R2&#12290;&#36873;&#25321;&#30340;&#31070;&#32463;&#27169;&#22411;&#22312;&#27979;&#35797;&#26679;&#26412;&#20013;&#35299;&#37322;&#20102;35%&#30340;&#24320;&#25918;&#24615;&#26041;&#24046;&#65292;&#32780;&#38598;&#25104;&#27169;&#22411;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00165v1 Announce Type: new  Abstract: Here we examine whether the personality dimension of openness to experience can be predicted from the individual google search history. By web scraping, individual text corpora (ICs) were generated from 214 participants with a mean number of 5 million word tokens. We trained word2vec models and used the similarities of each IC to label words, which were derived from a lexical approach of personality. These IC-label-word similarities were utilized as predictive features in neural models. For training and validation, we relied on 179 participants and held out a test sample of 35 participants. A grid search with varying number of predictive features, hidden units and boost factor was performed. As model selection criterion, we used R2 in the validation samples penalized by the absolute R2 difference between training and validation. The selected neural model explained 35% of the openness variance in the test sample, while an ensemble model w
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23454;&#26102;&#21512;&#24182;&#30456;&#20851;&#27010;&#24565;&#30340;&#23450;&#20041;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30693;&#35782;&#22686;&#24378;&#26041;&#27861;&#20197;&#25913;&#21892;LLMs&#22312;&#29983;&#29289;&#21307;&#23398;NER&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#22312;&#27979;&#35797;&#25968;&#25454;&#35774;&#32622;&#19979;&#24179;&#22343;&#25552;&#39640;&#20102;15\%&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.00152</link><description>&lt;p&gt;
&#22312;&#32447;&#23450;&#20041;&#22686;&#24378;LLMs&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;NER
&lt;/p&gt;
&lt;p&gt;
On-the-fly Definition Augmentation of LLMs for Biomedical NER
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00152
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23454;&#26102;&#21512;&#24182;&#30456;&#20851;&#27010;&#24565;&#30340;&#23450;&#20041;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30693;&#35782;&#22686;&#24378;&#26041;&#27861;&#20197;&#25913;&#21892;LLMs&#22312;&#29983;&#29289;&#21307;&#23398;NER&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#22312;&#27979;&#35797;&#25968;&#25454;&#35774;&#32622;&#19979;&#24179;&#22343;&#25552;&#39640;&#20102;15\%&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;LLMs&#20855;&#26377;&#19968;&#33324;&#30340;&#33021;&#21147;&#65292;&#20294;&#20173;&#28982;&#22312;&#29983;&#29289;&#21307;&#23398;NER&#20219;&#21153;&#20013;&#36935;&#21040;&#22256;&#38590;&#65292;&#36825;&#26159;&#30001;&#20110;&#23384;&#22312;&#19987;&#19994;&#26415;&#35821;&#21644;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#25152;&#33268;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#30693;&#35782;&#22686;&#24378;&#26041;&#27861;&#65292;&#22312;&#26377;&#38480;&#25968;&#25454;&#35774;&#32622;&#19979;&#25913;&#21892;LLMs&#22312;&#29983;&#29289;&#21307;&#23398;NER&#19978;&#30340;&#24615;&#33021;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23454;&#26102;&#21512;&#24182;&#30456;&#20851;&#27010;&#24565;&#30340;&#23450;&#20041;&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;&#20026;&#20102;&#25552;&#20379;&#30693;&#35782;&#22686;&#24378;&#30340;&#27979;&#35797;&#22330;&#26223;&#65292;&#25105;&#20204;&#23545;&#25552;&#31034;&#31574;&#30053;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#25506;&#32034;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#23450;&#20041;&#22686;&#24378;&#23545;&#20110;&#24320;&#28304;&#21644;&#23553;&#38381;&#30340;LLMs&#37117;&#26159;&#26377;&#29992;&#30340;&#12290;&#20363;&#22914;&#65292;&#22312;&#25105;&#20204;&#25152;&#26377;&#65288;&#20845;&#20010;&#65289;&#27979;&#35797;&#25968;&#25454;&#38598;&#20013;&#65292;&#23427;&#23548;&#33268;&#20102;GPT-4&#24615;&#33021;&#65288;F1&#65289;&#24179;&#22343;&#30456;&#23545;&#25552;&#21319;&#20102;15\%&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#28040;&#34701;&#21644;&#20998;&#26512;&#65292;&#20197;&#35777;&#26126;&#25105;&#20204;&#30340;&#24615;&#33021;&#25913;&#36827;&#26469;&#28304;&#20110;&#28155;&#21152;&#30456;&#20851;&#30340;&#23450;&#20041;&#30693;&#35782;&#12290;&#25105;&#20204;&#21457;&#29616;&#35880;&#24910;&#30340;&#25552;&#31034;&#31574;&#30053;&#20063;&#25552;&#39640;&#20102;LLMs&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00152v1 Announce Type: new  Abstract: Despite their general capabilities, LLMs still struggle on biomedical NER tasks, which are difficult due to the presence of specialized terminology and lack of training data. In this work we set out to improve LLM performance on biomedical NER in limited data settings via a new knowledge augmentation approach which incorporates definitions of relevant concepts on-the-fly. During this process, to provide a test bed for knowledge augmentation, we perform a comprehensive exploration of prompting strategies. Our experiments show that definition augmentation is useful for both open source and closed LLMs. For example, it leads to a relative improvement of 15\% (on average) in GPT-4 performance (F1) across all (six) of our test datasets. We conduct extensive ablations and analyses to demonstrate that our performance improvements stem from adding relevant definitional knowledge. We find that careful prompting strategies also improve LLM perform
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20316;&#32773;&#23545;&#38452;&#35851;&#20449;&#20208;&#30340;&#35270;&#35282;&#26469;&#24314;&#31435;&#19968;&#20010;&#36890;&#29992;&#26041;&#26696;&#65292;&#29992;&#20110;&#20998;&#31867;&#19981;&#21516;&#35805;&#39064;&#21644;&#32447;&#19978;&#31038;&#21306;&#20013;&#30340;&#38452;&#35851;&#35752;&#35770;&#65292;&#21033;&#29992;BERT&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#36739;&#20043;&#29983;&#25104;&#24335;&#26041;&#27861;&#25928;&#26524;&#26356;&#22909;&#12290;</title><link>https://arxiv.org/abs/2404.00141</link><description>&lt;p&gt;
&#20197;&#35268;&#27169;&#23545;&#38452;&#35851;&#35770;&#36848;&#36827;&#34892;&#20998;&#31867;&#65306;&#34394;&#20551;&#35686;&#25253;&#21644;&#38169;&#35823;&#32852;&#31995;
&lt;/p&gt;
&lt;p&gt;
Classifying Conspiratorial Narratives At Scale: False Alarms and Erroneous Connections
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00141
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20316;&#32773;&#23545;&#38452;&#35851;&#20449;&#20208;&#30340;&#35270;&#35282;&#26469;&#24314;&#31435;&#19968;&#20010;&#36890;&#29992;&#26041;&#26696;&#65292;&#29992;&#20110;&#20998;&#31867;&#19981;&#21516;&#35805;&#39064;&#21644;&#32447;&#19978;&#31038;&#21306;&#20013;&#30340;&#38452;&#35851;&#35752;&#35770;&#65292;&#21033;&#29992;BERT&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#36739;&#20043;&#29983;&#25104;&#24335;&#26041;&#27861;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#19978;&#35752;&#35770;&#32463;&#24120;&#28041;&#21450;&#38452;&#35851;&#35770;&#65292;&#36825;&#21487;&#33021;&#20250;&#20419;&#20351;&#20154;&#20204;&#23545;&#20854;&#20135;&#29983;&#26356;&#22810;&#30340;&#20449;&#20208;&#12290;&#28982;&#32780;&#65292;&#24182;&#38750;&#25152;&#26377;&#22260;&#32469;&#38452;&#35851;&#35770;&#30340;&#35752;&#35770;&#37117;&#26159;&#25512;&#23815;&#23427;&#20204;&#65292;&#26377;&#20123;&#26159;&#20026;&#20102;&#25581;&#31359;&#23427;&#20204;&#12290;&#29616;&#26377;&#30740;&#31350;&#36890;&#24120;&#20381;&#36182;&#31616;&#21333;&#30340;&#20195;&#29702;&#25110;&#19987;&#27880;&#20110;&#19968;&#32452;&#21463;&#38480;&#20449;&#21495;&#26469;&#35782;&#21035;&#38452;&#35851;&#29702;&#35770;&#65292;&#36825;&#38480;&#21046;&#20102;&#25105;&#20204;&#23545;&#19981;&#21516;&#20027;&#39064;&#21644;&#32447;&#19978;&#31038;&#21306;&#20013;&#30340;&#38452;&#35851;&#35752;&#35770;&#30340;&#29702;&#35299;&#12290;&#26412;&#30740;&#31350;&#24314;&#31435;&#20102;&#19968;&#20010;&#36890;&#29992;&#26041;&#26696;&#65292;&#26681;&#25454;&#20316;&#32773;&#23545;&#38452;&#35851;&#20449;&#20208;&#30340;&#35270;&#35282;&#26469;&#23545;&#19982;&#38452;&#35851;&#29702;&#35770;&#30456;&#20851;&#30340;&#35752;&#35770;&#36827;&#34892;&#20998;&#31867;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#21465;&#20107;&#35201;&#32032;&#26126;&#30830;&#34920;&#36798;&#65292;&#20363;&#22914;&#34892;&#21160;&#32773;&#12289;&#34892;&#21160;&#25110;&#30446;&#26631;&#65292;&#25110;&#32773;&#36890;&#36807;&#38544;&#21547;&#22320;&#25351;&#21521;&#24050;&#30693;&#29702;&#35770;&#65292;&#27604;&#22914;&#21270;&#23398;&#27668;&#28342;&#33014;&#25110;&#26032;&#19990;&#30028;&#31209;&#24207;&#12290;&#25105;&#20204;&#21033;&#29992;&#20154;&#24037;&#26631;&#35760;&#30340;&#22320;&#38754;&#23454;&#20917;&#26469;&#35757;&#32451;&#22522;&#20110;BERT&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#31867;&#22312;&#32447;&#38452;&#35851;&#35770;&#36848;&#65292;&#28982;&#21518;&#23558;&#20854;&#19982;&#29983;&#25104;&#24335;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00141v1 Announce Type: new  Abstract: Online discussions frequently involve conspiracy theories, which can contribute to the proliferation of belief in them. However, not all discussions surrounding conspiracy theories promote them, as some are intended to debunk them. Existing research has relied on simple proxies or focused on a constrained set of signals to identify conspiracy theories, which limits our understanding of conspiratorial discussions across different topics and online communities. This work establishes a general scheme for classifying discussions related to conspiracy theories based on authors' perspectives on the conspiracy belief, which can be expressed explicitly through narrative elements, such as the agent, action, or objective, or implicitly through references to known theories, such as chemtrails or the New World Order. We leverage human-labeled ground truth to train a BERT-based model for classifying online CTs, which we then compared to the Generativ
&lt;/p&gt;</description></item><item><title>&#25361;&#25112;&#26159;&#32570;&#20047;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#25110;&#21487;&#38752;&#36164;&#28304;&#65292;&#38024;&#23545;&#32034;&#25289;&#23612;&#24211;&#23572;&#24503;&#35821;&#26041;&#35328;&#30340;&#20845;&#20010;&#26041;&#35328;&#24314;&#31435;&#20102;&#19968;&#20010;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#24182;&#21033;&#29992;&#19977;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2404.00124</link><description>&lt;p&gt;
&#20320;&#26469;&#33258;&#21738;&#37324;&#65311;&#35753;&#25105;&#29468;&#29468;&#65281;&#32034;&#25289;&#23612;&#24211;&#23572;&#24503;&#35821;&#26041;&#35328;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in Sorani Kurdish
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00124
&lt;/p&gt;
&lt;p&gt;
&#25361;&#25112;&#26159;&#32570;&#20047;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#25110;&#21487;&#38752;&#36164;&#28304;&#65292;&#38024;&#23545;&#32034;&#25289;&#23612;&#24211;&#23572;&#24503;&#35821;&#26041;&#35328;&#30340;&#20845;&#20010;&#26041;&#35328;&#24314;&#31435;&#20102;&#19968;&#20010;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#24182;&#21033;&#29992;&#19977;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#25110;&#21487;&#38752;&#36164;&#28304;&#65288;&#22914;&#31038;&#20132;&#23186;&#20307;&#25110;&#32593;&#31449;&#65289;&#30340;&#38656;&#27714;&#65292;&#23545;&#32034;&#25289;&#23612;&#24211;&#23572;&#24503;&#35821;&#26041;&#35328;&#36827;&#34892;&#20998;&#31867;&#26500;&#25104;&#19968;&#39033;&#25361;&#25112;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#20010;&#22478;&#24066;&#21644;&#26449;&#24196;&#30340;&#23454;&#22320;&#35775;&#38382;&#65292;&#19982;&#19981;&#21516;&#24180;&#40836;&#32452;&#12289;&#24615;&#21035;&#12289;&#23398;&#26415;&#32972;&#26223;&#21644;&#32844;&#19994;&#30340;&#27597;&#35821;&#32773;&#36827;&#34892;&#20102;&#32852;&#31995;&#12290;&#25105;&#20204;&#22312;&#36827;&#34892;&#28085;&#30422;&#21508;&#31181;&#35805;&#39064;&#22914;&#29983;&#27963;&#26041;&#24335;&#12289;&#32972;&#26223;&#21382;&#21490;&#12289;&#29233;&#22909;&#12289;&#20852;&#36259;&#12289;&#20551;&#26399;&#21644;&#29983;&#27963;&#32463;&#39564;&#30340;&#23545;&#35805;&#36807;&#31243;&#20013;&#24405;&#21046;&#20102;&#20182;&#20204;&#30340;&#22768;&#38899;&#12290;&#30740;&#31350;&#30340;&#30446;&#26631;&#22320;&#21306;&#26159;&#20234;&#25289;&#20811;&#24211;&#23572;&#24503;&#26031;&#22374;&#22320;&#21306;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20174;107&#20010;&#35775;&#35848;&#20013;&#31215;&#32047;&#20102;29&#23567;&#26102;16&#20998;&#38047;40&#31186;&#30340;&#38899;&#39057;&#35760;&#24405;&#65292;&#21253;&#25324;&#20845;&#31181;&#26041;&#35328;&#30340;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19977;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;: &#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#12289;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;-&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;RNN-LSTM&#65289;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#37197;&#32622;&#65292;&#21253;&#25324;&#19981;&#21516;&#30340;&#36319;&#36394;&#25345;&#32493;&#26102;&#38388;&#12289;&#25968;&#25454;&#38598;&#25286;&#20998;&#21644;&#19981;&#24179;&#34913;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00124v1 Announce Type: new  Abstract: Classifying Sorani Kurdish subdialects poses a challenge due to the need for publicly available datasets or reliable resources like social media or websites for data collection. We conducted field visits to various cities and villages to address this issue, connecting with native speakers from different age groups, genders, academic backgrounds, and professions. We recorded their voices while engaging in conversations covering diverse topics such as lifestyle, background history, hobbies, interests, vacations, and life lessons. The target area of the research was the Kurdistan Region of Iraq. As a result, we accumulated 29 hours, 16 minutes, and 40 seconds of audio recordings from 107 interviews, constituting an unbalanced dataset encompassing six subdialects. Subsequently, we adapted three deep learning models: ANN, CNN, and RNN-LSTM. We explored various configurations, including different track durations, dataset splitting, and imbalan
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21517;&#20026;&#8220;DirtyFlipping&#8221;&#65292;&#21033;&#29992;&#33039;&#26631;&#31614;&#25216;&#26415;&#22312;&#36873;&#23450;&#30340;&#25968;&#25454;&#27169;&#24335;&#20013;&#36755;&#20837;&#35302;&#21457;&#22120;&#65292;&#20174;&#32780;&#23454;&#29616;&#38544;&#34109;&#30340;&#21518;&#38376;&#12290;</title><link>https://arxiv.org/abs/2404.00076</link><description>&lt;p&gt;
&#20351;&#29992;&#20498;&#32622;&#26631;&#31614;&#30340;&#21518;&#38376;&#26041;&#27861;&#65306;&#33039;&#26631;&#31614;&#32763;&#36716;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
A Backdoor Approach with Inverted Labels Using Dirty Label-Flipping Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00076
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21517;&#20026;&#8220;DirtyFlipping&#8221;&#65292;&#21033;&#29992;&#33039;&#26631;&#31614;&#25216;&#26415;&#22312;&#36873;&#23450;&#30340;&#25968;&#25454;&#27169;&#24335;&#20013;&#36755;&#20837;&#35302;&#21457;&#22120;&#65292;&#20174;&#32780;&#23454;&#29616;&#38544;&#34109;&#30340;&#21518;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22768;&#38899;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#32463;&#24120;&#20351;&#29992;&#20844;&#20849;&#25110;&#31532;&#19977;&#26041;&#25968;&#25454;&#65292;&#36825;&#21487;&#33021;&#26159;&#19981;&#20934;&#30830;&#30340;&#12290;&#36825;&#20351;&#24471;&#35757;&#32451;&#22312;&#36825;&#20123;&#25968;&#25454;&#19978;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#28508;&#22312;&#30340;&#25968;&#25454;&#27602;&#21270;&#25915;&#20987;&#12290;&#22312;&#36825;&#31181;&#25915;&#20987;&#31867;&#22411;&#20013;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#20351;&#29992;&#27602;&#21270;&#25968;&#25454;&#26469;&#35757;&#32451;DNN&#27169;&#22411;&#65292;&#21487;&#33021;&#20250;&#38477;&#20302;&#20854;&#24615;&#33021;&#12290;&#21478;&#19968;&#31181;&#23545;&#25105;&#20204;&#30340;&#30740;&#31350;&#38750;&#24120;&#30456;&#20851;&#30340;&#25968;&#25454;&#27602;&#21270;&#25915;&#20987;&#31867;&#22411;&#26159;&#26631;&#31614;&#32763;&#36716;&#65292;&#25915;&#20987;&#32773;&#22312;&#20854;&#20013;&#25805;&#32437;&#25968;&#25454;&#23376;&#38598;&#30340;&#26631;&#31614;&#12290;&#24050;&#32463;&#35777;&#26126;&#65292;&#21363;&#20351;&#26159;&#33021;&#21147;&#26377;&#38480;&#30340;&#25915;&#20987;&#32773;&#65292;&#36825;&#20123;&#25915;&#20987;&#20063;&#21487;&#33021;&#26497;&#22823;&#22320;&#38477;&#20302;&#31995;&#32479;&#24615;&#33021;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;DirtyFlipping&#8221;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#20351;&#29992;&#33039;&#26631;&#31614;&#25216;&#26415;&#65292;&#8220;&#26631;&#31614;&#23545;&#26631;&#31614;&#8221;&#65292;&#22312;&#19982;&#30446;&#26631;&#31867;&#21035;&#30456;&#20851;&#30340;&#36873;&#23450;&#25968;&#25454;&#27169;&#24335;&#20013;&#36755;&#20837;&#35302;&#21457;&#22120;&#65288;&#25293;&#25163;&#65289;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#38544;&#34109;&#30340;&#21518;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00076v1 Announce Type: cross  Abstract: Audio-based machine learning systems frequently use public or third-party data, which might be inaccurate. This exposes deep neural network (DNN) models trained on such data to potential data poisoning attacks. In this type of assault, attackers can train the DNN model using poisoned data, potentially degrading its performance. Another type of data poisoning attack that is extremely relevant to our investigation is label flipping, in which the attacker manipulates the labels for a subset of data. It has been demonstrated that these assaults may drastically reduce system performance, even for attackers with minimal abilities. In this study, we propose a backdoor attack named 'DirtyFlipping', which uses dirty label techniques, "label-on-label", to input triggers (clapping) in the selected data patterns associated with the target class, thereby enabling a stealthy backdoor.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;ChapTER&#65292;&#19968;&#20010;&#23545;&#27604;&#21382;&#21490;&#24314;&#27169;&#26694;&#26550;&#65292;&#36890;&#36807;&#21069;&#32512;&#35843;&#25972;&#23454;&#29616;&#25991;&#26412;&#19982;&#26102;&#38388;&#30340;&#24179;&#34913;&#65292;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2404.00051</link><description>&lt;p&gt;
Deja vu: &#20351;&#29992;&#21069;&#32512;&#35843;&#25972;&#36827;&#34892;&#23545;&#27604;&#21382;&#21490;&#24314;&#27169;&#65292;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Deja vu: Contrastive Historical Modeling with Prefix-tuning for Temporal Knowledge Graph Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00051
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;ChapTER&#65292;&#19968;&#20010;&#23545;&#27604;&#21382;&#21490;&#24314;&#27169;&#26694;&#26550;&#65292;&#36890;&#36807;&#21069;&#32512;&#35843;&#25972;&#23454;&#29616;&#25991;&#26412;&#19982;&#26102;&#38388;&#30340;&#24179;&#34913;&#65292;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#30693;&#35782;&#22270;&#25512;&#29702;&#65288;TKGR&#65289;&#26159;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#20026;&#19981;&#23436;&#25972;&#30340;TKG&#25512;&#26029;&#32570;&#22833;&#20107;&#23454;&#30340;&#20219;&#21153;&#65288;&#20363;&#22914;&#65292;&#20256;&#23548;&#21644;&#24402;&#32435;&#35774;&#32622;&#65289;&#65292;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#26368;&#36817;&#65292;&#20026;&#20102;&#20943;&#23569;TKG&#20013;&#32467;&#26500;&#36830;&#25509;&#30340;&#20381;&#36182;&#24615;&#65292;&#24050;&#24320;&#21457;&#20102;&#22522;&#20110;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#23454;&#20307;&#25551;&#36848;&#20013;&#20016;&#23500;&#30340;&#35821;&#35328;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24040;&#22823;&#21442;&#25968;&#21644;&#19981;&#28789;&#27963;&#24615;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#26041;&#27861;&#22312;&#35745;&#31639;&#26114;&#36149;&#19988;&#30446;&#30340;&#24314;&#31435;&#30340;&#35757;&#32451;&#31574;&#30053;&#19978;&#24456;&#38590;&#24179;&#34913;&#25991;&#26412;&#30693;&#35782;&#21644;&#26102;&#38388;&#20449;&#24687;&#12290;&#20026;&#20102;&#21457;&#25496;&#25991;&#26412;&#27169;&#22411;&#22312;&#21508;&#31181;&#22797;&#26434;&#22330;&#26223;&#20013;&#29992;&#20110;TKGR&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ChapTER&#65292;&#19968;&#20010;&#20855;&#26377;&#21069;&#32512;&#35843;&#25972;&#23545;&#27604;&#21382;&#21490;&#24314;&#27169;&#26694;&#26550;&#65292;&#29992;&#20110;&#26102;&#38388;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00051v1 Announce Type: new  Abstract: Temporal Knowledge Graph Reasoning (TKGR) is the task of inferring missing facts for incomplete TKGs in complex scenarios (e.g., transductive and inductive settings), which has been gaining increasing attention. Recently, to mitigate dependence on structured connections in TKGs, text-based methods have been developed to utilize rich linguistic information from entity descriptions. However, suffering from the enormous parameters and inflexibility of pre-trained language models, existing text-based methods struggle to balance the textual knowledge and temporal information with computationally expensive purpose-built training strategies. To tap the potential of text-based models for TKGR in various complex scenarios, we propose ChapTER, a Contrastive historical modeling framework with prefix-tuning for TEmporal Reasoning. ChapTER feeds history-contextualized text into the pseudo-Siamese encoders to strike a textual-temporal balance via cont
&lt;/p&gt;</description></item><item><title>&#25506;&#35752;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20889;&#20316;&#21161;&#25163;&#24341;&#21457;&#30340;&#20889;&#20316;&#25152;&#26377;&#26435;&#24863;&#21644;&#20316;&#32773;&#36523;&#20221;&#35748;&#30693;&#20043;&#38388;&#30340;&#24515;&#29702;&#22256;&#22659;&#12290;</title><link>https://arxiv.org/abs/2404.00027</link><description>&lt;p&gt;
LLM&#20316;&#20026;&#20889;&#20316;&#21161;&#25163;&#65306;&#25506;&#35752;&#25152;&#26377;&#26435;&#24863;&#21644;&#25512;&#29702;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
LLMs as Writing Assistants: Exploring Perspectives on Sense of Ownership and Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00027
&lt;/p&gt;
&lt;p&gt;
&#25506;&#35752;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20889;&#20316;&#21161;&#25163;&#24341;&#21457;&#30340;&#20889;&#20316;&#25152;&#26377;&#26435;&#24863;&#21644;&#20316;&#32773;&#36523;&#20221;&#35748;&#30693;&#20043;&#38388;&#30340;&#24515;&#29702;&#22256;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20889;&#20316;&#20013;&#30340;&#25152;&#26377;&#26435;&#24863;&#38480;&#21046;&#20102;&#25105;&#20204;&#23545;&#24605;&#24819;&#12289;&#26102;&#38388;&#21644;&#36129;&#29486;&#30340;&#25237;&#20837;&#65292;&#23548;&#33268;&#23545;&#20135;&#20986;&#29289;&#30340;&#20381;&#24651;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#20889;&#20316;&#21161;&#25163;&#24341;&#20837;&#20102;&#19968;&#31181;&#24515;&#29702;&#22256;&#22659;&#65292;&#22240;&#20026;&#19968;&#20123;&#20869;&#23481;&#24182;&#38750;&#30452;&#25509;&#25105;&#20204;&#30340;&#21019;&#20316;&#12290;&#25105;&#20204;&#24448;&#24448;&#26356;&#20542;&#21521;&#20110;&#22312;&#21019;&#36896;&#24615;&#20219;&#21153;&#20013;&#26356;&#22810;&#22320;&#24402;&#21151;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#23613;&#31649;&#23427;&#20204;&#23545;&#25152;&#26377;&#20219;&#21153;&#37117;&#26159;&#24179;&#31561;&#30340;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#25105;&#20204;&#21487;&#33021;&#19981;&#20250;&#23436;&#20840;&#22768;&#31216;&#23545;&#30001;LLM&#29983;&#25104;&#30340;&#20869;&#23481;&#25317;&#26377;&#25152;&#26377;&#26435;&#65292;&#20294;&#21364;&#33258;&#30001;&#22320;&#22768;&#31216;&#20316;&#32773;&#36523;&#20221;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#31616;&#30701;&#35843;&#26597;&#26469;&#30740;&#31350;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#20102;&#35299;&#28508;&#22312;&#30340;&#35748;&#30693;&#36807;&#31243;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#20154;&#26426;&#20132;&#20114;&#22312;&#20889;&#20316;&#20013;&#30340;&#24212;&#29992;&#24182;&#25913;&#36827;&#20889;&#20316;&#36741;&#21161;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00027v1 Announce Type: cross  Abstract: Sense of ownership in writing confines our investment of thoughts, time, and contribution, leading to attachment to the output. However, using writing assistants introduces a mental dilemma, as some content isn't directly our creation. For instance, we tend to credit Large Language Models (LLMs) more in creative tasks, even though all tasks are equal for them. Additionally, while we may not claim complete ownership of LLM-generated content, we freely claim authorship. We conduct a short survey to examine these issues and understand underlying cognitive processes in order to gain a better knowledge of human-computer interaction in writing and improve writing aid systems.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#20204;&#26085;&#30410;&#20381;&#36182;&#30340;&#22522;&#20110;LLM&#30340;&#20889;&#20316;&#21161;&#25163;&#23545;&#21019;&#36896;&#21147;&#21644;&#20010;&#24615;&#21487;&#33021;&#36896;&#25104;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#26088;&#22312;&#25913;&#36827;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#21644;&#25552;&#21319;&#20889;&#20316;&#21161;&#25163;&#30340;&#20010;&#24615;&#21270;&#21644;&#20010;&#24615;&#21270;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.00026</link><description>&lt;p&gt;
&#22696;&#27700;&#19982;&#20010;&#24615;&#65306;&#22312;LLMs&#26102;&#20195;&#22609;&#36896;&#20010;&#24615;&#21270;&#21465;&#20107;
&lt;/p&gt;
&lt;p&gt;
Ink and Individuality: Crafting a Personalised Narrative in the Age of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00026
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#20204;&#26085;&#30410;&#20381;&#36182;&#30340;&#22522;&#20110;LLM&#30340;&#20889;&#20316;&#21161;&#25163;&#23545;&#21019;&#36896;&#21147;&#21644;&#20010;&#24615;&#21487;&#33021;&#36896;&#25104;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#26088;&#22312;&#25913;&#36827;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#21644;&#25552;&#21319;&#20889;&#20316;&#21161;&#25163;&#30340;&#20010;&#24615;&#21270;&#21644;&#20010;&#24615;&#21270;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21644;&#20010;&#24615;&#21270;&#26500;&#25104;&#20102;&#20351;&#27599;&#20010;&#20316;&#23478;&#29420;&#29305;&#24182;&#24433;&#21709;&#20854;&#25991;&#23383;&#20197;&#26377;&#25928;&#21560;&#24341;&#35835;&#32773;&#21516;&#26102;&#20256;&#36798;&#30495;&#23454;&#24615;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#26085;&#30410;&#20381;&#36182;&#22522;&#20110;LLM&#30340;&#20889;&#20316;&#21161;&#25163;&#21487;&#33021;&#20250;&#21361;&#21450;&#25105;&#20204;&#30340;&#21019;&#36896;&#21147;&#21644;&#20010;&#24615;&#12290;&#25105;&#20204;&#32463;&#24120;&#24573;&#35270;&#36825;&#19968;&#36235;&#21183;&#23545;&#25105;&#20204;&#30340;&#21019;&#36896;&#21147;&#21644;&#29420;&#29305;&#24615;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#23613;&#31649;&#21487;&#33021;&#20250;&#36896;&#25104;&#21518;&#26524;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#36827;&#34892;&#31616;&#35201;&#35843;&#26597;&#25506;&#32034;&#19981;&#21516;&#30340;&#35266;&#28857;&#21644;&#27010;&#24565;&#65292;&#20197;&#21450;&#23581;&#35797;&#29702;&#35299;&#20154;&#20204;&#30340;&#35266;&#28857;&#65292;&#32467;&#21512;&#20197;&#24448;&#22312;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#26469;&#30740;&#31350;&#36825;&#20123;&#38382;&#39064;&#12290;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#23545;&#20110;&#25913;&#36827;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#21644;&#22686;&#24378;&#20010;&#24615;&#21270;&#21644;&#20010;&#24615;&#21270;&#20889;&#20316;&#21161;&#25163;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00026v1 Announce Type: cross  Abstract: Individuality and personalization comprise the distinctive characteristics that make each writer unique and influence their words in order to effectively engage readers while conveying authenticity. However, our growing reliance on LLM-based writing assistants risks compromising our creativity and individuality over time. We often overlook the negative impacts of this trend on our creativity and uniqueness, despite the possible consequences. This study investigates these concerns by performing a brief survey to explore different perspectives and concepts, as well as trying to understand people's viewpoints, in conjunction with past studies in the area. Addressing these issues is essential for improving human-computer interaction systems and enhancing writing assistants for personalization and individuality.
&lt;/p&gt;</description></item><item><title>AI&#31995;&#32479;&#22312;&#29983;&#25104;&#39033;&#30446;&#26631;&#39064;&#26041;&#38754;&#23637;&#29616;&#20986;&#29420;&#29305;&#30340;&#21019;&#26032;&#33021;&#21147;&#65292;&#21363;&#20351;&#22312;&#20219;&#21153;&#22797;&#26434;&#24615;&#22686;&#21152;&#21644;&#35745;&#31639;&#33021;&#21147;&#26497;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#29983;&#25104;&#30340;&#20869;&#23481;&#20855;&#26377;&#34920;&#38754;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.00017</link><description>&lt;p&gt;
&#21019;&#26032;&#30340;&#40550;&#40521;&#65311;&#35780;&#20272;AI&#21019;&#20316;&#30340;&#30495;&#27491;&#26032;&#39062;&#24615;
&lt;/p&gt;
&lt;p&gt;
Psittacines of Innovation? Assessing the True Novelty of AI Creations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00017
&lt;/p&gt;
&lt;p&gt;
AI&#31995;&#32479;&#22312;&#29983;&#25104;&#39033;&#30446;&#26631;&#39064;&#26041;&#38754;&#23637;&#29616;&#20986;&#29420;&#29305;&#30340;&#21019;&#26032;&#33021;&#21147;&#65292;&#21363;&#20351;&#22312;&#20219;&#21153;&#22797;&#26434;&#24615;&#22686;&#21152;&#21644;&#35745;&#31639;&#33021;&#21147;&#26497;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#29983;&#25104;&#30340;&#20869;&#23481;&#20855;&#26377;&#34920;&#38754;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26816;&#39564;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31995;&#32479;&#26159;&#21542;&#29983;&#25104;&#30495;&#27491;&#26032;&#39062;&#30340;&#24819;&#27861;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#22312;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#21040;&#30340;&#27169;&#24335;&#12290;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#23454;&#39564;&#35774;&#35745;&#65292;&#25105;&#20204;&#35753;&#19968;&#20010;AI&#29983;&#25104;&#34394;&#26500;&#30340;&#20247;&#31609;&#27963;&#21160;&#39033;&#30446;&#26631;&#39064;&#12290;&#25105;&#20204;&#27604;&#36739;AI&#29983;&#25104;&#30340;&#39033;&#30446;&#26631;&#39064;&#20869;&#37096;&#65292;&#34913;&#37327;&#37325;&#22797;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#19968;&#31181;&#25193;&#23637;&#30340;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#26041;&#27861;&#65292;&#22312;AI&#29983;&#25104;&#30340;&#26631;&#39064;&#21644;&#23454;&#38469;&#35266;&#27979;&#30340;&#29616;&#22330;&#25968;&#25454;&#20043;&#38388;&#36827;&#34892;&#27604;&#36739;&#65292;&#35813;&#26041;&#27861;&#26159;&#20174;&#23558;&#32479;&#35745;&#20998;&#24067;&#30340;&#26680;&#22343;&#20540;&#23884;&#20837;&#24212;&#29992;&#21040;&#39640;&#32500;&#26426;&#22120;&#23398;&#20064;&#65288;&#22823;&#35821;&#35328;&#65289;&#23884;&#20837;&#21521;&#37327;&#20013;&#25512;&#23548;&#20986;&#26469;&#30340;--&#20174;&#32780;&#24471;&#21040;&#23545;AI&#36755;&#20986;&#26032;&#39062;&#24615;&#30340;&#32467;&#26500;&#21270;&#20998;&#26512;&#12290;&#32467;&#26524;&#34920;&#26126;&#65306;&#65288;1&#65289;&#21363;&#20351;&#22312;&#20219;&#21153;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#24182;&#19988;&#22312;&#35745;&#31639;&#33021;&#21147;&#30340;&#26497;&#38480;&#19979;&#65292;AI&#20063;&#20250;&#29983;&#25104;&#29420;&#29305;&#20869;&#23481;&#65292;&#65288;2&#65289;&#29983;&#25104;&#30340;&#20869;&#23481;&#20855;&#26377;&#34920;&#38754;&#26377;&#25928;&#24615;&#65292;&#19982;&#36865;&#20837;&#20854;&#20182;&#29983;&#25104;AI&#30340;&#36755;&#20837;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00017v1 Announce Type: new  Abstract: We examine whether Artificial Intelligence (AI) systems generate truly novel ideas rather than merely regurgitating patterns learned during training. Utilizing a novel experimental design, we task an AI with generating project titles for hypothetical crowdfunding campaigns. We compare within AI-generated project titles, measuring repetition and complexity. We compare between the AI-generated titles and actual observed field data using an extension of maximum mean discrepancy--a metric derived from the application of kernel mean embeddings of statistical distributions to high-dimensional machine learning (large language) embedding vectors--yielding a structured analysis of AI output novelty. Results suggest that (1) the AI generates unique content even under increasing task complexity, and at the limits of its computational capabilities, (2) the generated content has face validity, being consistent with both inputs to other generative AI 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#37329;&#34701;&#21387;&#21147;&#25351;&#26631;&#19982;&#36130;&#32463;&#26032;&#38395;&#24773;&#24863;&#20998;&#26512;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#32929;&#31080;&#24066;&#22330;&#30340;&#39118;&#38505;&#25511;&#21046;&#31574;&#30053;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2404.00012</link><description>&lt;p&gt;
&#22522;&#20110;&#36130;&#32463;&#26032;&#38395;&#24773;&#24863;&#20998;&#26512;&#30340;&#32929;&#31080;&#24066;&#22330;&#21387;&#21147;&#25351;&#25968;&#31574;&#30053;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Stress index strategy enhanced with financial news sentiment analysis for the equity markets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00012
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#37329;&#34701;&#21387;&#21147;&#25351;&#26631;&#19982;&#36130;&#32463;&#26032;&#38395;&#24773;&#24863;&#20998;&#26512;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#32929;&#31080;&#24066;&#22330;&#30340;&#39118;&#38505;&#25511;&#21046;&#31574;&#30053;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#32929;&#24066;&#39118;&#38505;-&#39118;&#38505;&#31574;&#30053;&#65292;&#23558;&#37329;&#34701;&#21387;&#21147;&#25351;&#26631;&#19982;&#36890;&#36807;ChatGPT&#35835;&#21462;&#21644;&#35299;&#37322;Bloomberg&#27599;&#26085;&#24066;&#22330;&#25688;&#35201;&#36827;&#34892;&#30340;&#24773;&#24863;&#20998;&#26512;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#23558;&#20174;&#27874;&#21160;&#29575;&#21644;&#20449;&#36151;&#21033;&#24046;&#25512;&#23548;&#20986;&#30340;&#24066;&#22330;&#21387;&#21147;&#39044;&#27979;&#19982;GPT-4&#25512;&#23548;&#20986;&#30340;&#36130;&#32463;&#26032;&#38395;&#24773;&#24863;&#30456;&#32467;&#21512;&#65292;&#25913;&#36827;&#20102;&#31574;&#30053;&#30340;&#34920;&#29616;&#65292;&#34920;&#29616;&#20026;&#26356;&#39640;&#30340;&#22799;&#26222;&#27604;&#29575;&#21644;&#38477;&#20302;&#30340;&#26368;&#22823;&#22238;&#25764;&#12290;&#25913;&#36827;&#30340;&#34920;&#29616;&#22312;&#32435;&#26031;&#36798;&#20811;&#12289;&#26631;&#26222;500&#25351;&#25968;&#21644;&#20845;&#20010;&#20027;&#35201;&#32929;&#31080;&#24066;&#22330;&#20013;&#37117;&#20445;&#25345;&#19968;&#33268;&#65292;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#32929;&#31080;&#24066;&#22330;&#20013;&#20855;&#26377;&#26222;&#36941;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00012v1 Announce Type: cross  Abstract: This paper introduces a new risk-on risk-off strategy for the stock market, which combines a financial stress indicator with a sentiment analysis done by ChatGPT reading and interpreting Bloomberg daily market summaries. Forecasts of market stress derived from volatility and credit spreads are enhanced when combined with the financial news sentiment derived from GPT-4. As a result, the strategy shows improved performance, evidenced by higher Sharpe ratio and reduced maximum drawdowns. The improved performance is consistent across the NASDAQ, the S&amp;P 500 and the six major equity markets, indicating that the method generalises across equities markets.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#30028;&#38754;&#65292;&#36890;&#36807;&#20154;&#31867;&#29983;&#25104;&#30340;&#23545;&#25239;&#35868;&#39064;&#25968;&#25454;&#26469;&#24110;&#21161;&#35757;&#32451;&#38382;&#31572;&#20154;&#24037;&#26234;&#33021;&#65292;&#24182;&#25552;&#20379;&#19968;&#22871;&#24037;&#20855;&#21327;&#21161;&#32534;&#20889;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.00011</link><description>&lt;p&gt;
&#19968;&#20010;&#26032;&#39062;&#30340;&#23545;&#25239;&#24335;&#35868;&#39064;&#20889;&#20316;&#30028;&#38754;
&lt;/p&gt;
&lt;p&gt;
A novel interface for adversarial trivia question-writing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00011
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#30028;&#38754;&#65292;&#36890;&#36807;&#20154;&#31867;&#29983;&#25104;&#30340;&#23545;&#25239;&#35868;&#39064;&#25968;&#25454;&#26469;&#24110;&#21161;&#35757;&#32451;&#38382;&#31572;&#20154;&#24037;&#26234;&#33021;&#65292;&#24182;&#25552;&#20379;&#19968;&#22871;&#24037;&#20855;&#21327;&#21161;&#32534;&#20889;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#24320;&#21457;&#38382;&#31572;&#20154;&#24037;&#26234;&#33021;&#26102;&#65292;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#19968;&#20010;&#23545;&#25239;&#25968;&#25454;&#38598;&#65292;&#25361;&#25112;&#27169;&#22411;&#36866;&#24212;&#33258;&#28982;&#35821;&#35328;&#20013;&#22797;&#26434;&#35821;&#27861;&#21644;&#25512;&#29702;&#12290;&#30446;&#21069;&#30340;&#31243;&#24207;&#29983;&#25104;&#23545;&#25239;&#25991;&#26412;&#25216;&#26415;&#23545;&#20110;&#35757;&#32451;&#22238;&#31572;&#22810;&#21477;&#35868;&#39064;&#31561;&#22797;&#26434;&#20219;&#21153;&#30340;&#27169;&#22411;&#24182;&#19981;&#22815;&#20581;&#22766;&#12290;&#25105;&#20204;&#36716;&#21521;&#20154;&#31867;&#29983;&#25104;&#30340;&#25968;&#25454;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#30028;&#38754;&#26469;&#25910;&#38598;&#23545;&#25239;&#30340;&#20154;&#31867;&#32534;&#20889;&#30340;&#35868;&#39064;&#12290;&#25105;&#20204;&#30340;&#30028;&#38754;&#38754;&#21521;&#38382;&#31572;&#20889;&#20316;&#32773;&#21644;Quiz Bowl&#30340;&#21442;&#19982;&#32773;&#65292;Quiz Bowl&#26159;&#19968;&#31181;&#22522;&#20110;&#34562;&#40483;&#22120;&#30340;&#35868;&#39064;&#31454;&#36187;&#65292;&#20854;&#20013;&#30001;&#19968;&#31995;&#21015;&#38590;&#24230;&#36882;&#20943;&#30340;&#25552;&#31034;&#32452;&#25104;&#30340;&#27573;&#33853;&#38271;&#38382;&#39064;&#12290;&#20026;&#20102;&#28608;&#21169;&#20351;&#29992;&#65292;&#25105;&#20204;&#30028;&#38754;&#20013;&#30340;&#19968;&#22871;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24037;&#20855;&#24110;&#21161;&#20154;&#31867;&#32534;&#20889;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#31572;&#38382;&#39064;&#65292;&#23545;&#20110;Quiz Bowl&#21442;&#19982;&#32773;&#21644;&#35745;&#31639;&#26426;&#37117;&#26356;&#38590;&#31572;&#39064;&#12290;&#25105;&#20204;&#30340;&#30028;&#38754;&#19981;&#20165;&#25910;&#38598;&#20102;&#24320;&#21019;&#24615;&#30340;Quiz B&#30340;&#35757;&#32451;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00011v1 Announce Type: cross  Abstract: A critical component when developing question-answering AIs is an adversarial dataset that challenges models to adapt to the complex syntax and reasoning underlying our natural language. Present techniques for procedurally generating adversarial texts are not robust enough for training on complex tasks such as answering multi-sentence trivia questions. We instead turn to human-generated data by introducing an interface for collecting adversarial human-written trivia questions. Our interface is aimed towards question writers and players of Quiz Bowl, a buzzer-based trivia competition where paragraph-long questions consist of a sequence of clues of decreasing difficulty. To incentivize usage, a suite of machine learning-based tools in our interface assist humans in writing questions that are more challenging to answer for Quiz Bowl players and computers alike. Not only does our interface gather training data for the groundbreaking Quiz B
&lt;/p&gt;</description></item><item><title>DiJiang&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39057;&#22495;&#26680;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#39044;&#35757;&#32451;&#30340;&#22522;&#26412;Transformer&#27169;&#22411;&#36716;&#21270;&#20026;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#27169;&#22411;&#65292;&#22823;&#22823;&#20943;&#23569;&#35757;&#32451;&#25104;&#26412;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#25552;&#20379;&#26356;&#22909;&#30340;&#36924;&#36817;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.19928</link><description>&lt;p&gt;
DiJiang&#65306;&#36890;&#36807;&#32039;&#20945;&#30340;&#26680;&#26041;&#27861;&#23454;&#29616;&#39640;&#25928;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DiJiang: Efficient Large Language Models through Compact Kernelization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19928
&lt;/p&gt;
&lt;p&gt;
DiJiang&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39057;&#22495;&#26680;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#39044;&#35757;&#32451;&#30340;&#22522;&#26412;Transformer&#27169;&#22411;&#36716;&#21270;&#20026;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#27169;&#22411;&#65292;&#22823;&#22823;&#20943;&#23569;&#35757;&#32451;&#25104;&#26412;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#25552;&#20379;&#26356;&#22909;&#30340;&#36924;&#36817;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20943;&#23569;Transformers&#30340;&#35745;&#31639;&#36127;&#33655;&#65292;&#32447;&#24615;&#27880;&#24847;&#21147;&#30340;&#30740;&#31350;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#27880;&#24847;&#26426;&#21046;&#30340;&#25913;&#36827;&#31574;&#30053;&#36890;&#24120;&#38656;&#35201;&#32463;&#36807;&#22823;&#37327;&#30340;&#37325;&#26032;&#35757;&#32451;&#65292;&#22312;&#20855;&#26377;&#22823;&#37327;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;DiJiang&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#39057;&#22495;&#26680;&#26041;&#27861;&#65292;&#21487;&#23558;&#39044;&#35757;&#32451;&#30340;&#22522;&#26412;Transformer&#36716;&#21270;&#20026;&#20855;&#26377;&#36739;&#23567;&#35757;&#32451;&#25104;&#26412;&#30340;&#32447;&#24615;&#22797;&#26434;&#24230;&#27169;&#22411;&#12290;&#36890;&#36807;&#37319;&#29992;&#21152;&#26435;&#25311;&#38543;&#26426;&#37319;&#26679;&#27861;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#29702;&#35770;&#19978;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#36924;&#36817;&#25928;&#29575;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#38477;&#20302;&#35757;&#32451;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#25105;&#20204;&#30340;&#26680;&#26041;&#27861;&#22522;&#20110;&#31163;&#25955;&#20313;&#24358;&#21464;&#25442;&#65288;DCT&#65289;&#25805;&#20316;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36798;&#21040;&#20102;&#19982;&#21407;&#22987;Transformer&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#20294;&#35757;&#32451;&#26102;&#38388;&#22823;&#22823;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19928v1 Announce Type: new  Abstract: In an effort to reduce the computational load of Transformers, research on linear attention has gained significant momentum. However, the improvement strategies for attention mechanisms typically necessitate extensive retraining, which is impractical for large language models with a vast array of parameters. In this paper, we present DiJiang, a novel Frequency Domain Kernelization approach that enables the transformation of a pre-trained vanilla Transformer into a linear complexity model with little training costs. By employing a weighted Quasi-Monte Carlo method for sampling, the proposed approach theoretically offers superior approximation efficiency. To further reduce the training computational complexity, our kernelization is based on Discrete Cosine Transform (DCT) operations. Extensive experiments demonstrate that the proposed method achieves comparable performance to the original Transformer, but with significantly reduced trainin
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;TRAC-2024&#31163;&#32447;&#21361;&#23475;&#28508;&#22312;&#24615;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#19987;&#23478;&#26631;&#27880;&#30340;&#31038;&#20132;&#23186;&#20307;&#35780;&#35770;&#25968;&#25454;&#38598;&#65292;&#25104;&#21151;&#35774;&#35745;&#20102;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#21361;&#23475;&#21487;&#33021;&#24615;&#24182;&#35782;&#21035;&#30446;&#26631;&#30340;&#31639;&#27861;&#65292;&#22312;&#20004;&#20010;&#36187;&#36947;&#20013;&#21462;&#24471;&#31532;&#20108;&#21517;&#30340;&#25104;&#32489;&#12290;</title><link>https://arxiv.org/abs/2403.19713</link><description>&lt;p&gt;
NJUST-KMG&#21442;&#21152;TRAC-2024&#20219;&#21153;1&#21644;&#20219;&#21153;2&#65306;&#31163;&#32447;&#21361;&#23475;&#28508;&#22312;&#24615;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
NJUST-KMG at TRAC-2024 Tasks 1 and 2: Offline Harm Potential Identification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19713
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;TRAC-2024&#31163;&#32447;&#21361;&#23475;&#28508;&#22312;&#24615;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#19987;&#23478;&#26631;&#27880;&#30340;&#31038;&#20132;&#23186;&#20307;&#35780;&#35770;&#25968;&#25454;&#38598;&#65292;&#25104;&#21151;&#35774;&#35745;&#20102;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#21361;&#23475;&#21487;&#33021;&#24615;&#24182;&#35782;&#21035;&#30446;&#26631;&#30340;&#31639;&#27861;&#65292;&#22312;&#20004;&#20010;&#36187;&#36947;&#20013;&#21462;&#24471;&#31532;&#20108;&#21517;&#30340;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#25253;&#21578;&#35814;&#32454;&#25551;&#36848;&#20102;&#25105;&#20204;&#22312;TRAC-2024&#31163;&#32447;&#21361;&#23475;&#28508;&#22312;&#24615;&#35782;&#21035;&#20013;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#35813;&#27604;&#36187;&#21253;&#21547;&#20004;&#20010;&#23376;&#20219;&#21153;&#12290;&#30740;&#31350;&#21033;&#29992;&#20102;&#19968;&#20010;&#21253;&#21547;&#22810;&#31181;&#21360;&#24230;&#35821;&#35328;&#31038;&#20132;&#23186;&#20307;&#35780;&#35770;&#30340;&#20016;&#23500;&#25968;&#25454;&#38598;&#65292;&#30001;&#19987;&#23478;&#35780;&#20998;&#26631;&#27880;&#65292;&#20197;&#25429;&#25417;&#31163;&#32447;&#29615;&#22659;&#21361;&#23475;&#30340;&#24494;&#22937;&#21547;&#20041;&#12290;&#21442;&#19982;&#32773;&#30340;&#20219;&#21153;&#26159;&#35774;&#35745;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#29305;&#23450;&#24773;&#20917;&#19979;&#21361;&#23475;&#21487;&#33021;&#24615;&#24182;&#35782;&#21035;&#31163;&#32447;&#21361;&#23475;&#26368;&#21487;&#33021;&#30340;&#30446;&#26631;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#36187;&#36947;&#20013;&#25490;&#21517;&#31532;&#20108;&#65292;F1&#20540;&#20998;&#21035;&#20026;0.73&#21644;0.96&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20027;&#35201;&#28041;&#21450;&#36873;&#25321;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#25972;&#21512;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;&#26041;&#27861;&#24212;&#29992;&#20110;&#27979;&#35797;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19713v1 Announce Type: new  Abstract: This report provide a detailed description of the method that we proposed in the TRAC-2024 Offline Harm Potential dentification which encloses two sub-tasks. The investigation utilized a rich dataset comprised of social media comments in several Indian languages, annotated with precision by expert judges to capture the nuanced implications for offline context harm. The objective assigned to the participants was to design algorithms capable of accurately assessing the likelihood of harm in given situations and identifying the most likely target(s) of offline harm. Our approach ranked second in two separate tracks, with F1 values of 0.73 and 0.96 respectively. Our method principally involved selecting pretrained models for finetuning, incorporating contrastive learning techniques, and culminating in an ensemble approach for the test set.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#65292;&#21487;&#20197;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#21644;&#32534;&#36753;&#21487;&#35299;&#37322;&#30340;&#22240;&#26524;&#22270;&#65292;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#26410;&#39044;&#26009;&#26426;&#21046;&#30340;&#35814;&#32454;&#29702;&#35299;&#21644;&#21253;&#21547;&#20102;&#29992;&#20110;&#25552;&#39640;&#20998;&#31867;&#22120;&#27867;&#21270;&#33021;&#21147;&#30340;SHIFT&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.19647</link><description>&lt;p&gt;
&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#65306;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#21644;&#32534;&#36753;&#21487;&#35299;&#37322;&#30340;&#22240;&#26524;&#22270;
&lt;/p&gt;
&lt;p&gt;
Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19647
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#65292;&#21487;&#20197;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#21644;&#32534;&#36753;&#21487;&#35299;&#37322;&#30340;&#22240;&#26524;&#22270;&#65292;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#26410;&#39044;&#26009;&#26426;&#21046;&#30340;&#35814;&#32454;&#29702;&#35299;&#21644;&#21253;&#21547;&#20102;&#29992;&#20110;&#25552;&#39640;&#20998;&#31867;&#22120;&#27867;&#21270;&#33021;&#21147;&#30340;SHIFT&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#29992;&#20110;&#21457;&#29616;&#21644;&#24212;&#29992;&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#30005;&#36335;&#26159;&#20154;&#31867;&#21487;&#35299;&#37322;&#29305;&#24449;&#30340;&#22240;&#26524;&#30456;&#20851;&#23376;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#34892;&#20026;&#12290; &#22312;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#30830;&#23450;&#30340;&#30005;&#36335;&#30001;&#22810;&#20041;&#19988;&#38590;&#20197;&#35299;&#37322;&#30340;&#21333;&#20803;&#32452;&#25104;&#65292;&#20363;&#22914;&#27880;&#24847;&#21147;&#22836;&#25110;&#31070;&#32463;&#20803;&#65292;&#20351;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#12290; &#30456;&#27604;&#20043;&#19979;&#65292;&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#23454;&#29616;&#20102;&#23545;&#26410;&#39044;&#26009;&#26426;&#21046;&#30340;&#35814;&#32454;&#29702;&#35299;&#12290; &#30001;&#20110;&#23427;&#20204;&#22522;&#20110;&#32454;&#31890;&#24230;&#21333;&#20803;&#65292;&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#23545;&#19979;&#28216;&#20219;&#21153;&#38750;&#24120;&#26377;&#29992;&#65306;&#25105;&#20204; introduc&#20102;SHIFT&#65292;&#36890;&#36807;&#20999;&#38500;&#20154;&#31867;&#21028;&#26029;&#20026;&#20219;&#21153;&#19981;&#30456;&#20851;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290; &#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#21457;&#29616;&#25104;&#21315;&#19978;&#19975;&#20010;&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#26469;&#23637;&#31034;&#19968;&#20010;&#23436;&#20840;&#26080;&#30417;&#30563;&#19988;&#21487;&#25193;&#23637;&#30340;&#21487;&#35299;&#37322;&#24615;&#31649;&#32447;&#65292;&#29992;&#20110;&#33258;&#21160;&#21457;&#29616;&#30340;&#27169;&#22411;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19647v1 Announce Type: cross  Abstract: We introduce methods for discovering and applying sparse feature circuits. These are causally implicated subnetworks of human-interpretable features for explaining language model behaviors. Circuits identified in prior work consist of polysemantic and difficult-to-interpret units like attention heads or neurons, rendering them unsuitable for many downstream applications. In contrast, sparse feature circuits enable detailed understanding of unanticipated mechanisms. Because they are based on fine-grained units, sparse feature circuits are useful for downstream tasks: We introduce SHIFT, where we improve the generalization of a classifier by ablating features that a human judges to be task-irrelevant. Finally, we demonstrate an entirely unsupervised and scalable interpretability pipeline by discovering thousands of sparse feature circuits for automatically discovered model behaviors.
&lt;/p&gt;</description></item><item><title>TableLLM&#26159;&#19968;&#20010;&#25317;&#26377;130&#20159;&#21442;&#25968;&#30340;&#24378;&#22823;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#19987;&#38376;&#29992;&#20110;&#29087;&#32451;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#25805;&#20316;&#20219;&#21153;&#65292;&#36890;&#36807;&#36828;&#31243;&#30417;&#30563;&#26041;&#27861;&#21644;&#20132;&#21449;&#39564;&#35777;&#31574;&#30053;&#65292;TableLLM&#30456;&#23545;&#20110;&#20854;&#20182;&#29616;&#26377;&#30340;&#36890;&#29992;&#21644;&#34920;&#26684;&#25968;&#25454;&#19987;&#27880;&#30340;LLMs&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.19318</link><description>&lt;p&gt;
TableLLM&#65306;&#22312;&#23454;&#38469;&#21150;&#20844;&#20351;&#29992;&#22330;&#26223;&#20013;&#23454;&#29616;LLMs&#23545;&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#22788;&#29702;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
TableLLM: Enabling Tabular Data Manipulation by LLMs in Real Office Usage Scenarios
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19318
&lt;/p&gt;
&lt;p&gt;
TableLLM&#26159;&#19968;&#20010;&#25317;&#26377;130&#20159;&#21442;&#25968;&#30340;&#24378;&#22823;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#19987;&#38376;&#29992;&#20110;&#29087;&#32451;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#25805;&#20316;&#20219;&#21153;&#65292;&#36890;&#36807;&#36828;&#31243;&#30417;&#30563;&#26041;&#27861;&#21644;&#20132;&#21449;&#39564;&#35777;&#31574;&#30053;&#65292;TableLLM&#30456;&#23545;&#20110;&#20854;&#20182;&#29616;&#26377;&#30340;&#36890;&#29992;&#21644;&#34920;&#26684;&#25968;&#25454;&#19987;&#27880;&#30340;LLMs&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;TableLLM&#65292;&#36825;&#26159;&#19968;&#20010;&#25317;&#26377;130&#20159;&#21442;&#25968;&#30340;&#24378;&#22823;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#19987;&#38376;&#29992;&#20110;&#29087;&#32451;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#25805;&#20316;&#20219;&#21153;&#65292;&#26080;&#35770;&#20854;&#23884;&#20837;&#22312;&#25991;&#26723;&#36824;&#26159;&#30005;&#23376;&#34920;&#26684;&#20013;&#65292;&#20197;&#28385;&#36275;&#30495;&#23454;&#21150;&#20844;&#22330;&#26223;&#38656;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36828;&#31243;&#30417;&#30563;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#25512;&#29702;&#36807;&#31243;&#25193;&#23637;&#31574;&#30053;&#65292;&#26377;&#21161;&#20110;&#35757;&#32451;LLMs&#26356;&#26377;&#25928;&#22320;&#29702;&#35299;&#25512;&#29702;&#27169;&#24335;&#65292;&#20197;&#21450;&#19968;&#31181;&#20132;&#21449;&#39564;&#35777;&#31574;&#30053;&#65292;&#30830;&#20445;&#33258;&#21160;&#29983;&#25104;&#25968;&#25454;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#35780;&#20272;TableLLM&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26088;&#22312;&#35299;&#20915;&#25991;&#26723;&#21644;&#30005;&#23376;&#34920;&#26684;&#26684;&#24335;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#20004;&#31181;&#22330;&#26223;&#30340;&#32452;&#32455;&#33391;&#22909;&#30340;&#35780;&#20272;&#31649;&#32447;&#12290;&#24443;&#24213;&#30340;&#35780;&#20272;&#20984;&#26174;&#20102;TableLLM&#30456;&#23545;&#20110;&#21508;&#31181;&#29616;&#26377;&#36890;&#29992;&#21644;&#19987;&#27880;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;LLMs&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#24050;&#20844;&#24320;&#21457;&#24067;&#20102;&#35813;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19318v1 Announce Type: new  Abstract: We introduce TableLLM, a robust large language model (LLM) with 13 billion parameters, purpose-built for proficiently handling tabular data manipulation tasks, whether they are embedded within documents or spreadsheets, catering to real-world office scenarios. We propose a distant supervision method for training, which comprises a reasoning process extension strategy, aiding in training LLMs to understand reasoning patterns more effectively as well as a cross-way validation strategy, ensuring the quality of the automatically generated data. To evaluate the performance of TableLLM, we have crafted a benchmark tailored to address both document and spreadsheet formats as well as constructed a well-organized evaluation pipeline capable of handling both scenarios. Thorough evaluations underscore the advantages of TableLLM when compared to various existing general-purpose and tabular data-focused LLMs. We have publicly released the model check
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35266;&#23519;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#21516;&#23618;&#23545;&#38544;&#34255;&#29366;&#24577;&#30340;&#24433;&#21709;&#31243;&#24230;&#65292;&#25552;&#20986;&#20102;LLM-Streamline&#26041;&#27861;&#65292;&#21253;&#25324;&#23618;&#21098;&#26525;&#21644;&#23618;&#26367;&#25442;&#65292;&#29992;&#20110;&#21387;&#32553;&#27169;&#22411;&#24182;&#20445;&#25345;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.19135</link><description>&lt;p&gt;
&#36890;&#36807;&#31616;&#21270;&#19981;&#37325;&#35201;&#30340;&#23618;&#21387;&#32553;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Compressing Large Language Models by Streamlining the Unimportant Layer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19135
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35266;&#23519;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#21516;&#23618;&#23545;&#38544;&#34255;&#29366;&#24577;&#30340;&#24433;&#21709;&#31243;&#24230;&#65292;&#25552;&#20986;&#20102;LLM-Streamline&#26041;&#27861;&#65292;&#21253;&#25324;&#23618;&#21098;&#26525;&#21644;&#23618;&#26367;&#25442;&#65292;&#29992;&#20110;&#21387;&#32553;&#27169;&#22411;&#24182;&#20445;&#25345;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#21644;&#39046;&#22495;&#65292;&#20294;&#20854;&#36866;&#29992;&#24615;&#21463;&#21040;&#27169;&#22411;&#21442;&#25968;&#30340;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#20851;&#27880;&#34920;&#29616;&#20986;&#39640;&#24615;&#33021;&#30340;&#32039;&#20945;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;LLM&#30340;&#19981;&#21516;&#23618;&#23545;&#38544;&#34255;&#29366;&#24577;&#26377;&#19981;&#21516;&#31243;&#24230;&#30340;&#25200;&#21160;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#35782;&#21035;&#20986;&#19981;&#37027;&#20040;&#37325;&#35201;&#30340;&#23618;&#12290;&#22522;&#20110;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLM-Streamline&#65292;&#21253;&#25324;&#20004;&#37096;&#20998;&#65306;&#23618;&#21098;&#26525;&#65292;&#26681;&#25454;&#30446;&#26631;&#31232;&#30095;&#24230;&#31227;&#38500;&#27169;&#22411;&#20013;&#19968;&#32452;&#36830;&#32493;&#30340;&#26368;&#19981;&#37325;&#35201;&#30340;&#23618;&#65307;&#23618;&#26367;&#25442;&#65292;&#35757;&#32451;&#19968;&#20010;&#36731;&#37327;&#32423;&#27169;&#22411;&#26469;&#26367;&#25442;&#34987;&#21098;&#26525;&#30340;&#23618;&#65292;&#20174;&#32780;&#32531;&#35299;&#30001;&#21098;&#26525;&#36896;&#25104;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;(MLP)&#21644;&#19968;&#20010;transformer&#23618;&#31561;&#32467;&#26500;&#20316;&#20026;&#36731;&#37327;&#32423;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19135v1 Announce Type: cross  Abstract: Large language models (LLM) have been extensively applied in various natural language tasks and domains, but their applicability is constrained by the large number of parameters of the models. Consequently, there is an increasing emphasis on compact models that exhibit high performance. In this study, we observe that different layers in LLM have varying degrees of perturbation on the hidden states, which allows us to identify less important layers. Based on this phenomenon, we propose LLM-Streamline, which consists of two parts: layer pruning, where we remove a set of consecutive layers with the lowest importance in the model according to the target sparsity; and layer replacement, where we train a lightweight model to substitute the pruned layers, thereby mitigating the performance degradation caused by pruning. In our experiments, we utilize structures such as a multi-layer perceptron (MLP) and a transformer layer as lightweight mode
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#20219;&#21153;&#28041;&#21450;14&#31181;&#38750;&#27954;&#21644;&#20122;&#27954;&#35821;&#35328;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#65292;&#26088;&#22312;&#32771;&#23519;&#36328;&#35821;&#35328;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2403.18933</link><description>&lt;p&gt;
SemEval&#20219;&#21153;1&#65306;&#38750;&#27954;&#21644;&#20122;&#27954;&#35821;&#35328;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
SemEval Task 1: Semantic Textual Relatedness for African and Asian Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18933
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#20219;&#21153;&#28041;&#21450;14&#31181;&#38750;&#27954;&#21644;&#20122;&#27954;&#35821;&#35328;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#65292;&#26088;&#22312;&#32771;&#23519;&#36328;&#35821;&#35328;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#20851;&#20110;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#65288;STR&#65289;&#30340;&#20849;&#20139;&#20219;&#21153;&#12290;&#32780;&#20808;&#21069;&#30340;&#20849;&#20139;&#20219;&#21153;&#20027;&#35201;&#20851;&#27880;&#35821;&#20041;&#30456;&#20284;&#24615;&#65292;&#25105;&#20204;&#21017;&#35843;&#26597;&#20102;&#36328;&#36234;14&#31181;&#35821;&#35328;&#65288;&#21253;&#25324;&#21335;&#38750;&#33655;&#20848;&#35821;&#12289;&#38463;&#23572;&#21450;&#21033;&#20122;&#38463;&#25289;&#20271;&#35821;&#12289;&#38463;&#22982;&#21704;&#25289;&#35821;&#12289;&#33521;&#35821;&#12289;&#35946;&#33832;&#35821;&#12289;&#21360;&#22320;&#35821;&#12289;&#21360;&#23612;&#35821;&#12289;&#22522;&#23612;&#20122;&#40065;&#23433;&#36798;&#35821;&#12289;&#39532;&#25289;&#22320;&#35821;&#12289;&#25705;&#27931;&#21733;&#38463;&#25289;&#20271;&#35821;&#12289;&#29616;&#20195;&#26631;&#20934;&#38463;&#25289;&#20271;&#35821;&#12289;&#26049;&#36974;&#26222;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#21644;&#27888;&#21346;&#22266;&#35821;&#65289;&#30340;&#26356;&#24191;&#27867;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#29616;&#35937;&#12290;&#36825;&#20123;&#35821;&#35328;&#26469;&#33258;&#20116;&#20010;&#19981;&#21516;&#30340;&#35821;&#31995;&#65292;&#24182;&#20027;&#35201;&#22312;&#38750;&#27954;&#21644;&#20122;&#27954;&#22320;&#21306;&#20351;&#29992;&#65292;&#36825;&#20123;&#22320;&#21306;&#30340;&#29305;&#28857;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36164;&#28304;&#30340;&#30456;&#23545;&#26377;&#38480;&#12290;&#25968;&#25454;&#38598;&#20013;&#30340;&#27599;&#20010;&#23454;&#20363;&#37117;&#26159;&#19968;&#20010;&#19982;&#20998;&#25968;&#30456;&#20851;&#32852;&#30340;&#21477;&#23545;&#65292;&#35813;&#20998;&#25968;&#34920;&#31034;&#20004;&#20010;&#21477;&#23376;&#20043;&#38388;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#31243;&#24230;&#12290;&#21442;&#19982;&#31995;&#32479;&#34987;&#35201;&#27714;&#22312;&#19977;&#20010;&#20027;&#35201;&#36712;&#36947;&#20013;&#30340;14&#31181;&#35821;&#35328;&#20013;&#25353;&#23427;&#20204;&#22312;&#24847;&#20041;&#19978;&#30340;&#25509;&#36817;&#31243;&#24230;&#65288;&#21363;&#23427;&#20204;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#31243;&#24230;&#65289;&#23545;&#21477;&#23545;&#36827;&#34892;&#25490;&#21517;&#65306;(a) &#30417;&#30563;&#65292;(b) &#26080;&#30417;&#30563;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18933v1 Announce Type: new  Abstract: We present the first shared task on Semantic Textual Relatedness (STR). While earlier shared tasks primarily focused on semantic similarity, we instead investigate the broader phenomenon of semantic relatedness across 14 languages: Afrikaans, Algerian Arabic, Amharic, English, Hausa, Hindi, Indonesian, Kinyarwanda, Marathi, Moroccan Arabic, Modern Standard Arabic, Punjabi, Spanish, and Telugu. These languages originate from five distinct language families and are predominantly spoken in Africa and Asia -- regions characterised by the relatively limited availability of NLP resources. Each instance in the datasets is a sentence pair associated with a score that represents the degree of semantic textual relatedness between the two sentences. Participating systems were asked to rank sentence pairs by their closeness in meaning (i.e., their degree of semantic relatedness) in the 14 languages in three main tracks: (a) supervised, (b) unsupervi
&lt;/p&gt;</description></item><item><title>SDSAT&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#28789;&#27963;&#35299;&#30721;&#33021;&#21147;&#30340;&#35821;&#20041;&#33258;&#36866;&#24212;&#20196;&#29260;&#65292;&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#33609;&#31295;&#20196;&#29260;&#30340;&#33021;&#21147;&#65292;&#24182;&#23454;&#29616;&#36229;&#36807;3.5&#20493;&#21644;3.0&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.18647</link><description>&lt;p&gt;
SDSAT&#65306;&#36890;&#36807;&#20855;&#26377;&#35821;&#20041;&#33258;&#36866;&#24212;&#20196;&#29260;&#30340;&#25512;&#27979;&#35299;&#30721;&#21152;&#36895;LLM&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
SDSAT: Accelerating LLM Inference through Speculative Decoding with Semantic Adaptive Tokens
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18647
&lt;/p&gt;
&lt;p&gt;
SDSAT&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#28789;&#27963;&#35299;&#30721;&#33021;&#21147;&#30340;&#35821;&#20041;&#33258;&#36866;&#24212;&#20196;&#29260;&#65292;&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#33609;&#31295;&#20196;&#29260;&#30340;&#33021;&#21147;&#65292;&#24182;&#23454;&#29616;&#36229;&#36807;3.5&#20493;&#21644;3.0&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21152;&#36895;&#26041;&#26696;&#65292;&#36890;&#36807;&#20855;&#26377;&#35821;&#20041;&#33258;&#36866;&#24212;&#20196;&#29260;&#65288;SDSAT&#65289;&#36827;&#34892;&#25512;&#27979;&#35299;&#30721;&#12290;&#35813;&#35774;&#35745;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#22686;&#24378;LLM&#27169;&#22411;&#29983;&#25104;&#33609;&#31295;&#26631;&#35760;&#30340;&#33021;&#21147;&#65292;&#32780;&#19981;&#24433;&#21709;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#26680;&#24515;&#31574;&#30053;&#21253;&#25324;&#65306;1&#65289;&#36890;&#36807;&#21512;&#24182;&#20855;&#26377;&#28789;&#27963;&#35299;&#30721;&#33021;&#21147;&#30340;&#35821;&#20041;&#33258;&#36866;&#24212;&#20196;&#29260;&#26469;&#24494;&#35843;&#27169;&#22411;&#65292;&#32780;&#19981;&#25913;&#21464;&#20854;&#32467;&#26500;&#65292;&#20351;&#20854;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#33609;&#31295;&#26631;&#35760;&#12290;2&#65289;&#36890;&#36807;&#20351;&#29992;&#19981;&#24433;&#21709;&#26631;&#20934;&#20196;&#29260;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#27169;&#22411;&#21487;&#20197;&#22312;&#20854;&#21407;&#22987;&#26694;&#26550;&#20043;&#19978;&#33719;&#24471;&#24182;&#34892;&#35299;&#30721;&#33021;&#21147;&#65292;&#32780;&#35757;&#32451;&#24320;&#38144;&#26368;&#23567;&#12290;3&#65289;&#25105;&#20204;&#35774;&#35745;&#20102;&#20351;&#29992;&#36138;&#23146;&#25628;&#32034;&#21644;&#26680;&#37319;&#26679;&#30340;&#8220;&#20004;&#27493;&#36215;&#33609;&#28982;&#21518;&#39564;&#35777;&#8221;&#29983;&#25104;&#31574;&#30053;&#12290;&#22312;CodeLlama-13B&#21644;7B&#27169;&#22411;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36895;&#24230;&#20998;&#21035;&#25552;&#39640;&#20102;3.5&#20493;&#21644;3.0&#20493;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18647v1 Announce Type: new  Abstract: We propose an acceleration scheme for large language models (LLMs) through Speculative Decoding with Semantic Adaptive Tokens (SDSAT). The primary objective of this design is to enhance the LLM model's ability to generate draft tokens more accurately without compromising the model's accuracy. The core strategies involve: 1) Fine-tune the model by incorporating semantic adaptive tokens that possess flexible decoding capabilities without changing its structure, allowing them to generate high-quality draft tokens. 2) By employing a training method that does not affect the standard tokens, the model can acquire parallel decoding abilities atop its original framework with minimal training overhead. 3) We have designed the "two-step-draft-then-verify" generation strategies using both greedy search and nucleus sampling. Experiments conducted on the CodeLlama-13B and 7B models have yielded speed increases of over 3.5X and 3.0X, respectively. Ple
&lt;/p&gt;</description></item><item><title>&#24635;&#20307;&#32780;&#35328;&#65292;&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#20013;&#25991;&#20013;&#26816;&#27979; offensive &#35821;&#35328;&#30340;&#25361;&#25112;&#65292;&#24182;&#24378;&#35843;&#20102;&#24320;&#21457;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#29305;&#23450;&#27169;&#22411;&#21644;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2403.18314</link><description>&lt;p&gt;
&#20013;&#22269; offensive &#35821;&#35328;&#26816;&#27979;&#65306;&#29616;&#29366;&#19982;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Chinese Offensive Language Detection:Current Status and Future Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18314
&lt;/p&gt;
&lt;p&gt;
&#24635;&#20307;&#32780;&#35328;&#65292;&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#20013;&#25991;&#20013;&#26816;&#27979; offensive &#35821;&#35328;&#30340;&#25361;&#25112;&#65292;&#24182;&#24378;&#35843;&#20102;&#24320;&#21457;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#29305;&#23450;&#27169;&#22411;&#21644;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#27491;&#22312;&#20570;&#20986;&#30456;&#24403;&#22823;&#30340;&#21162;&#21147;&#30417;&#27979;&#21644;&#35268;&#33539;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#65292;&#20294;&#22312;&#25968;&#23383;&#31354;&#38388;&#20013;&#65292;&#24694;&#24847;&#35821;&#35328;&#65288;&#22914;&#20167;&#24680;&#35328;&#35770;&#25110;&#32593;&#32476;&#27450;&#20940;&#65289;&#30340;&#26222;&#36941;&#23384;&#22312;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#37492;&#20110;&#32500;&#25252;&#25991;&#26126;&#21644;&#23562;&#37325;&#30340;&#22312;&#32447;&#29615;&#22659;&#30340;&#37325;&#35201;&#24615;&#65292;&#36843;&#20999;&#38656;&#35201;&#33021;&#22815;&#23454;&#26102;&#26816;&#27979;&#24694;&#24847;&#35328;&#35770;&#30340;&#33258;&#21160;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#24320;&#21457;&#22788;&#29702;&#27721;&#35821;&#31561;&#35821;&#35328;&#30340;&#26377;&#25928;&#31995;&#32479;&#65292;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#20123;&#35821;&#35328;&#30340;&#22797;&#26434;&#21644;&#24494;&#22937;&#24615;&#20351;&#24471;&#33258;&#21160;&#22788;&#29702;&#21464;&#24471;&#22256;&#38590;&#12290;&#26412;&#25991;&#20840;&#38754;&#24635;&#32467;&#20102;&#20013;&#22269; offensive &#35821;&#35328;&#26816;&#27979;&#24773;&#20917;&#65292;&#23457;&#26597;&#20102;&#24403;&#21069;&#30340;&#22522;&#20934;&#21644;&#26041;&#27861;&#65292;&#24182;&#37325;&#28857;&#20171;&#32461;&#20102;&#29992;&#20110;&#35299;&#20915;&#22312;&#36825;&#31181;&#22797;&#26434;&#35821;&#35328;&#20013;&#26816;&#27979;&#24694;&#24847;&#35821;&#35328;&#30340;&#29420;&#29305;&#25361;&#25112;&#30340;&#29305;&#23450;&#27169;&#22411;&#21644;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18314v1 Announce Type: cross  Abstract: Despite the considerable efforts being made to monitor and regulate user-generated content on social media platforms, the pervasiveness of offensive language, such as hate speech or cyberbullying, in the digital space remains a significant challenge. Given the importance of maintaining a civilized and respectful online environment, there is an urgent and growing need for automatic systems capable of detecting offensive speech in real time. However, developing effective systems for processing languages such as Chinese presents a significant challenge, owing to the language's complex and nuanced nature, which makes it difficult to process automatically. This paper provides a comprehensive overview of offensive language detection in Chinese, examining current benchmarks and approaches and highlighting specific models and tools for addressing the unique challenges of detecting offensive language in this complex language. The primary object
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#21306;&#22495;&#25351;&#23548;&#26631;&#35760;&#25216;&#26415;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#23391;&#21152;&#25289;&#25991;&#26412;&#19982;&#22320;&#26041;&#26041;&#35328;&#36716;&#24405;&#20026;&#22269;&#38469;&#38899;&#26631;&#30340;&#26041;&#27861;&#65292;&#20026;&#27169;&#22411;&#25552;&#20379;&#20102;&#20851;&#20110;&#36755;&#20837;&#25991;&#26412;&#30340;&#22320;&#21306;&#26041;&#35328;&#20449;&#24687;&#65292;&#20197;&#29702;&#35299;&#19982;&#27599;&#20010;&#22320;&#21306;&#30456;&#20851;&#30340;&#29420;&#29305;&#38899;&#38901;&#27169;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.17407</link><description>&lt;p&gt;
&#20351;&#29992;&#21306;&#22495;&#25351;&#23548;&#26631;&#35760;&#23558;&#23391;&#21152;&#25289;&#25991;&#26412;&#19982;&#22320;&#26041;&#26041;&#35328;&#36716;&#24405;&#20026;&#22269;&#38469;&#38899;&#26631;
&lt;/p&gt;
&lt;p&gt;
Transcribing Bengali Text with Regional Dialects to IPA using District Guided Tokens
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17407
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#21306;&#22495;&#25351;&#23548;&#26631;&#35760;&#25216;&#26415;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#23391;&#21152;&#25289;&#25991;&#26412;&#19982;&#22320;&#26041;&#26041;&#35328;&#36716;&#24405;&#20026;&#22269;&#38469;&#38899;&#26631;&#30340;&#26041;&#27861;&#65292;&#20026;&#27169;&#22411;&#25552;&#20379;&#20102;&#20851;&#20110;&#36755;&#20837;&#25991;&#26412;&#30340;&#22320;&#21306;&#26041;&#35328;&#20449;&#24687;&#65292;&#20197;&#29702;&#35299;&#19982;&#27599;&#20010;&#22320;&#21306;&#30456;&#20851;&#30340;&#29420;&#29305;&#38899;&#38901;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23391;&#21152;&#25289;&#25991;&#26412;&#21040;&#22269;&#38469;&#38899;&#26631;&#65288;IPA&#65289;&#30340;&#20934;&#30830;&#36716;&#24405;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#35821;&#35328;&#30340;&#22797;&#26434;&#38899;&#38901;&#23398;&#21644;&#35821;&#22659;&#30456;&#20851;&#30340;&#38899;&#21464;&#12290;&#23545;&#20110;&#21306;&#22495;&#23391;&#21152;&#25289;&#26041;&#35328;&#26469;&#35828;&#65292;&#30001;&#20110;&#32570;&#20047;&#38024;&#23545;&#36825;&#20123;&#26041;&#35328;&#30340;&#26631;&#20934;&#25340;&#20889;&#32422;&#23450;&#12289;&#24403;&#22320;&#21644;&#22806;&#35821;&#22312;&#36825;&#20123;&#22320;&#21306;&#20013;&#27969;&#34892;&#30340;&#35789;&#27719;&#20197;&#21450;&#19981;&#21516;&#22320;&#21306;&#20043;&#38388;&#30340;&#38899;&#38901;&#22810;&#26679;&#24615;&#65292;&#36825;&#19968;&#25361;&#25112;&#29978;&#33267;&#26356;&#20026;&#20005;&#23803;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#38382;&#39064;&#65292;&#21363;&#22312;&#35206;&#30422;&#23391;&#21152;&#25289;&#22269;&#20845;&#20010;&#22320;&#21306;&#30340;&#26032;&#25968;&#25454;&#38598;&#19978;&#24341;&#20837;&#8220;&#21306;&#22495;&#25351;&#23548;&#26631;&#35760;&#8221;&#65288;DGT&#65289;&#25216;&#26415;&#12290;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#22312;&#29983;&#25104;IPA&#36716;&#24405;&#20043;&#21069;&#21521;&#27169;&#22411;&#25552;&#20379;&#26377;&#20851;&#36755;&#20837;&#25991;&#26412;&#30340;&#21306;&#22495;&#26041;&#35328;&#25110;&#8220;&#22320;&#21306;&#8221;&#30340;&#26126;&#30830;&#20449;&#24687;&#12290;&#36825;&#36890;&#36807;&#22312;&#36755;&#20837;&#24207;&#21015;&#21069;&#28155;&#21152;&#19968;&#20010;&#22320;&#21306;&#26631;&#35760;&#26469;&#23454;&#29616;&#65292;&#26377;&#25928;&#22320;&#24341;&#23548;&#27169;&#22411;&#29702;&#35299;&#19982;&#27599;&#20010;&#22320;&#21306;&#30456;&#20851;&#30340;&#29420;&#29305;&#38899;&#38901;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17407v1 Announce Type: cross  Abstract: Accurate transcription of Bengali text to the International Phonetic Alphabet (IPA) is a challenging task due to the complex phonology of the language and context-dependent sound changes. This challenge is even more for regional Bengali dialects due to unavailability of standardized spelling conventions for these dialects, presence of local and foreign words popular in those regions and phonological diversity across different regions. This paper presents an approach to this sequence-to-sequence problem by introducing the District Guided Tokens (DGT) technique on a new dataset spanning six districts of Bangladesh. The key idea is to provide the model with explicit information about the regional dialect or "district" of the input text before generating the IPA transcription. This is achieved by prepending a district token to the input sequence, effectively guiding the model to understand the unique phonetic patterns associated with each 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23616;&#37096;&#21487;&#24402;&#23646;&#30340;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#8220;&#20808;&#22686;&#21152;&#23646;&#24615;&#65292;&#28982;&#21518;&#29983;&#25104;&#8221;&#30340;&#26041;&#24335;&#23558;&#29983;&#25104;&#36807;&#31243;&#20998;&#20026;&#20869;&#23481;&#36873;&#25321;&#12289;&#21477;&#23376;&#35268;&#21010;&#21644;&#24207;&#21015;&#21477;&#23376;&#29983;&#25104;&#19977;&#20010;&#27493;&#39588;&#65292;&#20197;&#31616;&#21270;&#24341;&#29992;&#39564;&#35777;&#24037;&#20316;&#12290;</title><link>https://arxiv.org/abs/2403.17104</link><description>&lt;p&gt;
&#39318;&#20808;&#22686;&#21152;&#23646;&#24615;&#65292;&#28982;&#21518;&#29983;&#25104;&#65306;&#23616;&#37096;&#21487;&#24402;&#23646;&#30340;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Attribute First, then Generate: Locally-attributable Grounded Text Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17104
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23616;&#37096;&#21487;&#24402;&#23646;&#30340;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#8220;&#20808;&#22686;&#21152;&#23646;&#24615;&#65292;&#28982;&#21518;&#29983;&#25104;&#8221;&#30340;&#26041;&#24335;&#23558;&#29983;&#25104;&#36807;&#31243;&#20998;&#20026;&#20869;&#23481;&#36873;&#25321;&#12289;&#21477;&#23376;&#35268;&#21010;&#21644;&#24207;&#21015;&#21477;&#23376;&#29983;&#25104;&#19977;&#20010;&#27493;&#39588;&#65292;&#20197;&#31616;&#21270;&#24341;&#29992;&#39564;&#35777;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#24187;&#35273;&#30340;&#21162;&#21147;&#20027;&#35201;&#38598;&#20013;&#22312;&#23646;&#24615;&#25991;&#26412;&#29983;&#25104;&#19978;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#24341;&#29992;&#25903;&#25345;&#28304;&#22312;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#21152;&#20837;&#25903;&#25345;&#25991;&#26412;&#20197;&#36827;&#34892;&#20107;&#21518;&#20107;&#23454;&#26680;&#26597;&#21644;&#26356;&#27491;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24341;&#29992;&#36890;&#24120;&#25351;&#21521;&#25972;&#20010;&#25991;&#26723;&#25110;&#27573;&#33853;&#65292;&#32473;&#29992;&#25143;&#24102;&#26469;&#20102;&#32321;&#37325;&#30340;&#39564;&#35777;&#24037;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#23616;&#37096;&#21487;&#24402;&#23646;&#30340;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#37325;&#28857;&#25918;&#22312;&#31616;&#27905;&#30340;&#23646;&#24615;&#19978;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21629;&#21517;&#20026;&#8220;&#20808;&#22686;&#21152;&#23646;&#24615;&#65292;&#28982;&#21518;&#29983;&#25104;&#8221;&#65292;&#23558;&#20256;&#32479;&#30340;&#31471;&#21040;&#31471;&#29983;&#25104;&#36807;&#31243;&#20998;&#35299;&#20026;&#19977;&#20010;&#30452;&#35266;&#30340;&#27493;&#39588;&#65306;&#20869;&#23481;&#36873;&#25321;&#12289;&#21477;&#23376;&#35268;&#21010;&#21644;&#24207;&#21015;&#21477;&#23376;&#29983;&#25104;&#12290;&#36890;&#36807;&#39318;&#20808;&#35782;&#21035;&#30456;&#20851;&#26469;&#28304;&#37096;&#20998;&#65288;&#8220;&#20808;&#36873;&#25321;&#8221;&#65289;&#65292;&#28982;&#21518;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#23545;&#23427;&#20204;&#36827;&#34892;&#26465;&#20214;&#21270;&#65288;&#8220;&#28982;&#21518;&#29983;&#25104;&#8221;&#65289;&#65292;&#25105;&#20204;&#30830;&#20445;&#36825;&#20123;&#37096;&#20998;&#20063;&#20316;&#20026;&#36755;&#20986;&#30340;&#32454;&#31890;&#24230;&#23646;&#24615;&#65288;&#8220;&#36873;&#25321;&#8221;&#21464;&#20026;&#8220;&#23646;&#24615;&#8221;&#65289;&#12290; &#22312;Mu&#19978;&#32463;&#36807;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17104v1 Announce Type: new  Abstract: Recent efforts to address hallucinations in Large Language Models (LLMs) have focused on attributed text generation, which supplements generated texts with citations of supporting sources for post-generation fact-checking and corrections. Yet, these citations often point to entire documents or paragraphs, burdening users with extensive verification work. In this paper, we introduce a locally-attributable text generation approach, prioritizing concise attributions. Our method, named ``Attribute First, then Generate'', breaks down the conventional end-to-end generation process into three intuitive steps: content selection, sentence planning, and sequential sentence generation. By initially identifying relevant source segments (``select first'') and then conditioning the generation process on them (``then generate''), we ensure these segments also act as the output's fine-grained attributions (``select'' becomes ``attribute''). Tested on Mu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;IllusionVQA&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#27979;&#35797;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#38169;&#35273;&#21644;&#38590;&#35299;&#22330;&#26223;&#19979;&#30340;&#34920;&#29616;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#29702;&#35299;&#20219;&#21153;&#21644;&#23450;&#20301;&#20219;&#21153;&#19978;&#65292;&#34920;&#29616;&#26368;&#20339;&#30340;VLM&#20026;GPT4V&#65292;&#32780;&#20154;&#31867;&#34920;&#29616;&#26356;&#32988;&#19968;&#31609;&#12290;</title><link>https://arxiv.org/abs/2403.15952</link><description>&lt;p&gt;
IllusionVQA&#65306;&#19968;&#20010;&#25361;&#25112;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#38169;&#35273;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
IllusionVQA: A Challenging Optical Illusion Dataset for Vision Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15952
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;IllusionVQA&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#27979;&#35797;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#38169;&#35273;&#21644;&#38590;&#35299;&#22330;&#26223;&#19979;&#30340;&#34920;&#29616;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#29702;&#35299;&#20219;&#21153;&#21644;&#23450;&#20301;&#20219;&#21153;&#19978;&#65292;&#34920;&#29616;&#26368;&#20339;&#30340;VLM&#20026;GPT4V&#65292;&#32780;&#20154;&#31867;&#34920;&#29616;&#26356;&#32988;&#19968;&#31609;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#30340;&#20986;&#29616;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#35843;&#26597;&#31070;&#32463;&#32593;&#32476;&#30340;&#35270;&#35273;&#29702;&#35299;&#12290; VLM&#19981;&#20165;&#33021;&#22815;&#36827;&#34892;&#23545;&#35937;&#20998;&#31867;&#21644;&#26816;&#27979;&#65292;&#36824;&#33021;&#22815;&#36827;&#34892;&#35270;&#35273;&#29702;&#35299;&#21644;&#24120;&#35782;&#25512;&#29702;&#12290; &#36825;&#33258;&#28982;&#32780;&#28982;&#22320;&#24341;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#24403;&#22270;&#20687;&#26412;&#36523;&#26159;&#19981;&#21512;&#29702;&#30340;&#26102;&#65292;VLM&#20250;&#22914;&#20309;&#22238;&#24212;&#65311; &#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;IllusionVQA&#65306;&#19968;&#20010;&#21253;&#21547;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20809;&#23398;&#38169;&#35273;&#21644;&#38590;&#20197;&#35299;&#37322;&#30340;&#22330;&#26223;&#30340;&#22810;&#26679;&#25968;&#25454;&#38598;&#65292;&#20197;&#27979;&#35797;VLM&#22312;&#20004;&#31181;&#19981;&#21516;&#30340;&#22810;&#36873;VQA&#20219;&#21153; - &#29702;&#35299;&#21644;&#36719;&#23450;&#20301;&#30340;&#33021;&#21147;&#12290; &#34920;&#29616;&#26368;&#20339;&#30340;VLM GPT4V&#22312;&#29702;&#35299;&#20219;&#21153;&#65288;4-shot&#65289;&#19978;&#23454;&#29616;&#20102;62.99&#65285;&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;&#23450;&#20301;&#20219;&#21153;&#65288;4-shot&#21644;Chain-of-Thought&#65289;&#19978;&#23454;&#29616;&#20102;49.7&#65285;&#30340;&#20934;&#30830;&#29575;&#12290; &#20154;&#31867;&#35780;&#20272;&#34920;&#26126;&#65292;&#20154;&#31867;&#22312;&#29702;&#35299;&#21644;&#23450;&#20301;&#26041;&#38754;&#30340;&#20934;&#30830;&#29575;&#20998;&#21035;&#20026;91.03&#65285;&#21644;100&#65285;&#12290; &#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#21644;Chain-of-Thought&#25512;&#29702;&#26041;&#38754;&#26377;&#24456;&#22823;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15952v1 Announce Type: cross  Abstract: The advent of Vision Language Models (VLM) has allowed researchers to investigate the visual understanding of a neural network using natural language. Beyond object classification and detection, VLMs are capable of visual comprehension and common-sense reasoning. This naturally led to the question: How do VLMs respond when the image itself is inherently unreasonable? To this end, we present IllusionVQA: a diverse dataset of challenging optical illusions and hard-to-interpret scenes to test the capability of VLMs in two distinct multiple-choice VQA tasks - comprehension and soft localization. GPT4V, the best-performing VLM, achieves 62.99% accuracy (4-shot) on the comprehension task and 49.7% on the localization task (4-shot and Chain-of-Thought). Human evaluation reveals that humans achieve 91.03% and 100% accuracy in comprehension and localization. We discover that In-Context Learning (ICL) and Chain-of-Thought reasoning substantially
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#25439;&#22833;&#35282;&#24230;&#37325;&#26032;&#23450;&#20041;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#29616;&#33021;&#21147;&#65292;&#21457;&#29616;&#20855;&#26377;&#30456;&#21516;&#39044;&#35757;&#32451;&#25439;&#22833;&#30340;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#34920;&#29616;&#30456;&#20284;&#65292;&#32780;&#24403;&#39044;&#35757;&#32451;&#25439;&#22833;&#20302;&#20110;&#29305;&#23450;&#38408;&#20540;&#26102;&#65292;&#27169;&#22411;&#23558;&#23637;&#29616;&#20986;&#31361;&#29616;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.15796</link><description>&lt;p&gt;
&#20174;&#25439;&#22833;&#35282;&#24230;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#29616;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Understanding Emergent Abilities of Language Models from the Loss Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#25439;&#22833;&#35282;&#24230;&#37325;&#26032;&#23450;&#20041;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#29616;&#33021;&#21147;&#65292;&#21457;&#29616;&#20855;&#26377;&#30456;&#21516;&#39044;&#35757;&#32451;&#25439;&#22833;&#30340;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#34920;&#29616;&#30456;&#20284;&#65292;&#32780;&#24403;&#39044;&#35757;&#32451;&#25439;&#22833;&#20302;&#20110;&#29305;&#23450;&#38408;&#20540;&#26102;&#65292;&#27169;&#22411;&#23558;&#23637;&#29616;&#20986;&#31361;&#29616;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30740;&#31350;&#36136;&#30097;&#20102;&#20256;&#32479;&#35748;&#20026;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#29616;&#33021;&#21147;&#20165;&#23384;&#22312;&#20110;&#22823;&#27169;&#22411;&#20013;&#30340;&#35266;&#28857;&#12290;&#36825;&#31181;&#24576;&#30097;&#28304;&#33258;&#20004;&#28857;&#35266;&#23519;&#65306;1&#65289;&#36739;&#23567;&#30340;&#27169;&#22411;&#20063;&#33021;&#23637;&#29616;&#20986;&#23545;&#31361;&#29616;&#33021;&#21147;&#30340;&#39640;&#24615;&#33021;&#65307;2&#65289;&#36136;&#30097;&#29992;&#20110;&#27979;&#37327;&#36825;&#20123;&#33021;&#21147;&#30340;&#19981;&#36830;&#32493;&#24615;&#25351;&#26631;&#12290;&#26412;&#25991;&#25552;&#35758;&#20174;&#39044;&#35757;&#32451;&#25439;&#22833;&#30340;&#35282;&#24230;&#30740;&#31350;&#31361;&#29616;&#33021;&#21147;&#65292;&#32780;&#38750;&#27169;&#22411;&#22823;&#23567;&#25110;&#35757;&#32451;&#35745;&#31639;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20855;&#26377;&#30456;&#21516;&#39044;&#35757;&#32451;&#25439;&#22833;&#20294;&#19981;&#21516;&#27169;&#22411;&#21644;&#25968;&#25454;&#22823;&#23567;&#30340;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#34920;&#29616;&#30456;&#21516;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#24403;&#26576;&#19968;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25439;&#22833;&#20302;&#20110;&#29305;&#23450;&#38408;&#20540;&#26102;&#65292;&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#31361;&#29616;&#33021;&#21147;&#65292;&#32780;&#19981;&#35770;&#25351;&#26631;&#30340;&#36830;&#32493;&#24615;&#22914;&#20309;&#65307;&#32780;&#22312;&#36798;&#21040;&#35813;&#38408;&#20540;&#20043;&#21069;&#65292;&#20854;&#24615;&#33021;&#20173;&#20445;&#25345;&#22312;&#38543;&#26426;&#29468;&#27979;&#27700;&#24179;&#12290;&#36825;&#21551;&#21457;&#25105;&#20204;&#37325;&#26032;&#23450;&#20041;&#31361;&#29616;&#33021;&#21147;&#20026;&#37027;&#20123;......
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15796v1 Announce Type: cross  Abstract: Recent studies have put into question the belief that emergent abilities in language models are exclusive to large models. This skepticism arises from two observations: 1) smaller models can also exhibit high performance on emergent abilities and 2) there is doubt on the discontinuous metrics used to measure these abilities. In this paper, we propose to study emergent abilities in the lens of pre-training loss, instead of model size or training compute. We demonstrate that the models with the same pre-training loss, but different model and data sizes, generate the same performance on various downstream tasks. We also discover that a model exhibits emergent abilities on certain tasks -- regardless of the continuity of metrics -- when its pre-training loss falls below a specific threshold. Before reaching this threshold, its performance remains at the level of random guessing. This inspires us to redefine emergent abilities as those that
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Private Mixing of Ensemble Distributions (PMixED)&#65306;&#36890;&#36807;&#23558;&#27169;&#22411;&#30340;&#36755;&#20986;&#20998;&#24067;&#25237;&#24433;&#21040;&#20844;&#20849;LLM&#30340;&#36755;&#20986;&#20998;&#24067;&#21608;&#22260;&#30340;&#38598;&#21512;&#19978;&#65292;&#24182;&#37319;&#26679;&#24179;&#22343;&#26469;&#23454;&#29616;&#23454;&#38469;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#65292;&#20197;&#26356;&#36731;&#37327;&#21270;&#30340;&#26041;&#24335;&#23454;&#29616;&#23545;&#38544;&#31169;&#25935;&#24863;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.15638</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24046;&#20998;&#31169;&#26377;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Next-Token Prediction of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15638
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Private Mixing of Ensemble Distributions (PMixED)&#65306;&#36890;&#36807;&#23558;&#27169;&#22411;&#30340;&#36755;&#20986;&#20998;&#24067;&#25237;&#24433;&#21040;&#20844;&#20849;LLM&#30340;&#36755;&#20986;&#20998;&#24067;&#21608;&#22260;&#30340;&#38598;&#21512;&#19978;&#65292;&#24182;&#37319;&#26679;&#24179;&#22343;&#26469;&#23454;&#29616;&#23454;&#38469;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#65292;&#20197;&#26356;&#36731;&#37327;&#21270;&#30340;&#26041;&#24335;&#23454;&#29616;&#23545;&#38544;&#31169;&#25935;&#24863;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#20445;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#38544;&#31169;&#26085;&#30410;&#37325;&#35201;&#12290;DP-SGD&#26159;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#26368;&#24191;&#27867;&#37319;&#29992;&#30340;&#25216;&#26415;&#65292;&#23427;&#20197;&#19968;&#31181;&#20445;&#35777;&#24046;&#20998;&#38544;&#31169;&#30340;&#26041;&#24335;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;DP-SGD&#38656;&#35201;&#27604;SGD&#26356;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#21644;&#26356;&#22823;&#30340;&#20869;&#23384;&#38656;&#27714;&#65292;&#21516;&#26102;&#36807;&#39640;&#20272;&#35745;&#23545;&#25163;&#20855;&#26377;&#30333;&#30418;&#35775;&#38382;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#26356;&#29616;&#23454;&#30340;&#22330;&#26223;&#20551;&#35774;&#21482;&#26377;&#23545;&#38544;&#31169;&#25935;&#24863;&#30340;LLM&#36827;&#34892;&#40657;&#30418;&#35775;&#38382;&#12290;&#22312;&#36825;&#20123;&#35266;&#23519;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31169;&#26377;&#28151;&#21512;&#38598;&#21512;&#20998;&#24067;&#65288;PMixED&#65289;&#65306;&#19968;&#31181;&#36890;&#36807;&#23558;&#27169;&#22411;&#30340;&#27599;&#20010;&#36755;&#20986;&#20998;&#24067;&#20174;&#19968;&#20010;&#32463;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;LLM&#38598;&#21512;&#25237;&#24433;&#21040;&#20844;&#20849;LLM&#36755;&#20986;&#20998;&#24067;&#21608;&#22260;&#30340;&#38598;&#21512;&#19978;&#65292;&#28982;&#21518;&#23545;&#25237;&#24433;&#20998;&#24067;&#36827;&#34892;&#24179;&#22343;&#24182;&#20174;&#20013;&#25277;&#26679;&#26469;&#23454;&#29616;&#23454;&#38469;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30340;&#31169;&#26377;&#39044;&#27979;&#21327;&#35758;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;DP-SGD&#26356;&#36731;&#37327;&#21270;&#65292;&#22240;&#20026;&#23427;&#19982;&#27169;&#22411;&#26080;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15638v1 Announce Type: cross  Abstract: Ensuring the privacy of Large Language Models (LLMs) is becoming increasingly important. The most widely adopted technique to accomplish this is DP-SGD, which trains a model in such a way that guarantees Differential Privacy (DP). However, DP-SGD requires longer training times and larger memory requirements than SGD, while overestimating an adversary's capabilities in having white box access to the model. A more realistic scenario assumes only black-box access to a privacy-sensitive LLM. Motivated by these observations, we present Private Mixing of Ensemble Distributions (PMixED): a private prediction protocol that achieves practical next-token prediction by projecting each of the model's output distribution from an ensemble of fine-tuned LLMs onto a set around a public LLM's output distribution, then averaging the projected distributions and sampling from it. Our approach is more lightweight than DP-SGD in that it is model agnostic, i
&lt;/p&gt;</description></item><item><title>NaturalTurn&#26159;&#19968;&#31181;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#20934;&#30830;&#25429;&#25417;&#33258;&#28982;&#23545;&#35805;&#20132;&#27969;&#21160;&#24577;&#30340;&#36718;&#27425;&#20998;&#21106;&#31639;&#27861;&#65292;&#36890;&#36807;&#21306;&#20998;&#35828;&#35805;&#32773;&#30340;&#20027;&#35201;&#23545;&#35805;&#36718;&#27425;&#21644;&#21548;&#20247;&#30340;&#27425;&#35201;&#35805;&#35821;&#65292;&#33021;&#22815;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#22320;&#25552;&#21462;&#36716;&#24405;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.15615</link><description>&lt;p&gt;
NaturalTurn&#65306;&#19968;&#31181;&#23558;&#36716;&#24405;&#20214;&#20998;&#21106;&#25104;&#33258;&#28982;&#23545;&#35805;&#36716;&#25240;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
NaturalTurn: A Method to Segment Transcripts into Naturalistic Conversational Turns
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15615
&lt;/p&gt;
&lt;p&gt;
NaturalTurn&#26159;&#19968;&#31181;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#20934;&#30830;&#25429;&#25417;&#33258;&#28982;&#23545;&#35805;&#20132;&#27969;&#21160;&#24577;&#30340;&#36718;&#27425;&#20998;&#21106;&#31639;&#27861;&#65292;&#36890;&#36807;&#21306;&#20998;&#35828;&#35805;&#32773;&#30340;&#20027;&#35201;&#23545;&#35805;&#36718;&#27425;&#21644;&#21548;&#20247;&#30340;&#27425;&#35201;&#35805;&#35821;&#65292;&#33021;&#22815;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#22320;&#25552;&#21462;&#36716;&#24405;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15615v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#23545;&#35805;&#26159;&#31038;&#20250;&#12289;&#35748;&#30693;&#21644;&#35745;&#31639;&#31185;&#23398;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#30340;&#20027;&#39064;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#23545;&#35805;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#19981;&#26029;&#22686;&#21152;&#65292;&#30740;&#31350;&#20154;&#21592;&#32570;&#20047;&#21487;&#20280;&#32553;&#30340;&#26041;&#27861;&#23558;&#35821;&#38899;&#36716;&#24405;&#36716;&#25442;&#20026;&#20250;&#35805;&#36718;&#27425;&#8212;&#8212;&#31038;&#20250;&#20114;&#21160;&#30340;&#22522;&#26412;&#26500;&#24314;&#27169;&#22359;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#8220;NaturalTurn&#8221;&#65292;&#19968;&#31181;&#26088;&#22312;&#20934;&#30830;&#25429;&#25417;&#33258;&#28982;&#20132;&#27969;&#21160;&#24577;&#30340;&#36718;&#27425;&#20998;&#21106;&#31639;&#27861;&#12290;NaturalTurn&#36890;&#36807;&#21306;&#20998;&#35828;&#35805;&#32773;&#30340;&#20027;&#35201;&#23545;&#35805;&#36718;&#27425;&#21644;&#21548;&#20247;&#30340;&#27425;&#35201;&#35805;&#35821;&#65292;&#22914;&#32972;&#26223;&#22768;&#12289;&#31616;&#30701;&#25554;&#35805;&#21644;&#20854;&#20182;&#34920;&#29616;&#23545;&#35805;&#29305;&#24449;&#30340;&#24179;&#34892;&#35328;&#35821;&#24418;&#24335;&#65292;&#26469;&#36816;&#20316;&#12290;&#20351;&#29992;&#22823;&#22411;&#23545;&#35805;&#35821;&#26009;&#24211;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19982;&#29616;&#26377;&#26041;&#27861;&#27966;&#29983;&#30340;&#36716;&#24405;&#30456;&#27604;&#65292;NaturalTurn&#27966;&#29983;&#30340;&#36716;&#24405;&#34920;&#29616;&#20986;&#26377;&#21033;&#30340;&#32479;&#35745;&#21644;&#25512;&#26029;&#29305;&#24615;&#12290;NaturalTurn&#31639;&#27861;&#20195;&#34920;&#20102;&#19968;&#31181;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15615v1 Announce Type: new  Abstract: Conversation is the subject of increasing interest in the social, cognitive, and computational sciences. And yet, as conversational datasets continue to increase in size and complexity, researchers lack scalable methods to segment speech-to-text transcripts into conversational turns--the basic building blocks of social interaction. We introduce "NaturalTurn," a turn segmentation algorithm designed to accurately capture the dynamics of naturalistic exchange. NaturalTurn operates by distinguishing speakers' primary conversational turns from listeners' secondary utterances, such as backchannels, brief interjections, and other forms of parallel speech that characterize conversation. Using data from a large conversation corpus, we show how NaturalTurn-derived transcripts demonstrate favorable statistical and inferential characteristics compared to transcripts derived from existing methods. The NaturalTurn algorithm represents an improvement i
&lt;/p&gt;</description></item><item><title>PruMerge&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#35270;&#35273;&#20196;&#29260;&#20943;&#23569;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#30340;&#35270;&#35273;&#20196;&#29260;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.15388</link><description>&lt;p&gt;
LLaVA-PruMerge: &#33258;&#36866;&#24212;&#20196;&#29260;&#20943;&#23569;&#29992;&#20110;&#39640;&#25928;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15388
&lt;/p&gt;
&lt;p&gt;
PruMerge&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#35270;&#35273;&#20196;&#29260;&#20943;&#23569;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#30340;&#35270;&#35273;&#20196;&#29260;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;(LMMs)&#36890;&#36807;&#36830;&#25509;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20102;&#26174;&#33879;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;LMMs&#21253;&#25324;&#20102;&#26356;&#22797;&#26434;&#30340;&#35270;&#35273;&#36755;&#20837;&#65292;&#22914;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#21644;&#35270;&#39057;&#65292;&#36825;&#26174;&#33879;&#22686;&#21152;&#20102;&#35270;&#35273;&#20196;&#29260;&#30340;&#25968;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#20196;&#29260;&#20943;&#23569;&#26426;&#21046;&#65292;&#24182;&#21457;&#29616;&#31867;&#20284;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#65292;&#35768;&#22810;&#35270;&#35273;&#20196;&#29260;&#22312;&#31354;&#38388;&#19978;&#26159;&#20887;&#20313;&#30340;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PruMerge&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#35270;&#35273;&#20196;&#29260;&#20943;&#23569;&#26041;&#27861;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35270;&#35273;&#20196;&#29260;&#30340;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#21487;&#27604;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15388v1 Announce Type: cross  Abstract: Large Multimodal Models (LMMs) have shown significant reasoning capabilities by connecting a visual encoder and a large language model. LMMs typically use a fixed amount of visual tokens, such as the penultimate layer features in the CLIP visual encoder, as the prefix content. Recent LMMs incorporate more complex visual inputs, such as high-resolution images and videos, which increase the number of visual tokens significantly. However, due to the design of the Transformer architecture, computational costs associated with these models tend to increase quadratically with the number of input tokens. To tackle this problem, we explore a token reduction mechanism and find, similar to prior work, that many visual tokens are spatially redundant. Based on this, we propose PruMerge, a novel adaptive visual token reduction approach, which largely reduces the number of visual tokens while maintaining comparable model performance. We first select 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#19978;&#19979;&#25991;&#30340;&#22810;&#23457;&#38405;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#35780;&#35770;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#35780;&#20272;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2403.15351</link><description>&lt;p&gt;
&#34701;&#21512;&#19978;&#19979;&#25991;&#30340;&#22810;&#23457;&#38405;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Multi-Review Fusion-in-Context
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#19978;&#19979;&#25991;&#30340;&#22810;&#23457;&#38405;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#35780;&#35770;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#35780;&#20272;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#38754;&#25991;&#26412;&#29983;&#25104;&#28085;&#30422;&#20102;&#35832;&#22914;&#38271;&#31687;&#38382;&#31572;&#21644;&#25688;&#35201;&#31561;&#20219;&#21153;&#65292;&#38656;&#35201;&#20869;&#23481;&#36873;&#25321;&#21644;&#20869;&#23481;&#25972;&#21512;&#12290;&#24403;&#21069;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#30001;&#20110;&#19981;&#36879;&#26126;&#24615;&#32780;&#38590;&#20197;&#25511;&#21046;&#21644;&#35299;&#37322;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#26041;&#27861;&#65292;&#20026;&#27599;&#20010;&#27493;&#39588;&#37117;&#25552;&#20379;&#21333;&#29420;&#30340;&#32452;&#20214;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#29983;&#25104;&#36830;&#36143;&#25991;&#26412;&#30340;&#31532;&#20108;&#23376;&#20219;&#21153;&#65292;&#21363;&#22312;&#22810;&#25991;&#26723;&#29615;&#22659;&#20013;&#32473;&#23450;&#39044;&#36873;&#20869;&#23481;&#12290;&#25105;&#20204;&#23558;\textit{Fusion-in-Context}(FiC)&#20855;&#20307;&#21270;&#20026;&#19968;&#20010;&#29420;&#31435;&#20219;&#21153;&#65292;&#20854;&#36755;&#20837;&#21253;&#25324;&#24102;&#26377;&#30446;&#26631;&#20869;&#23481;&#39640;&#20142;&#37096;&#20998;&#30340;&#28304;&#25991;&#26412;&#12290;&#28982;&#21518;&#65292;&#27169;&#22411;&#38656;&#35201;&#29983;&#25104;&#19968;&#20010;&#21253;&#21547;&#25152;&#26377;&#19988;&#20165;&#21253;&#21547;&#30446;&#26631;&#20449;&#24687;&#30340;&#36830;&#36143;&#27573;&#33853;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#21253;&#25324;&#22312;&#35780;&#35770;&#39046;&#22495;&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;1000&#20010;&#23454;&#20363;&#30340;&#31934;&#24515;&#31574;&#21010;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#39640;&#20142;&#30495;&#23454;&#24615;&#21644;&#28085;&#30422;&#33539;&#22260;&#30340;&#26032;&#39062;&#35780;&#20272;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15351v1 Announce Type: new  Abstract: Grounded text generation, encompassing tasks such as long-form question-answering and summarization, necessitates both content selection and content consolidation. Current end-to-end methods are difficult to control and interpret due to their opaqueness. Accordingly, recent works have proposed a modular approach, with separate components for each step. Specifically, we focus on the second subtask, of generating coherent text given pre-selected content in a multi-document setting. Concretely, we formalize \textit{Fusion-in-Context} (FiC) as a standalone task, whose input consists of source texts with highlighted spans of targeted content. A model then needs to generate a coherent passage that includes all and only the target information. Our work includes the development of a curated dataset of 1000 instances in the reviews domain, alongside a novel evaluation framework for assessing the faithfulness and coverage of highlights, which stro
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#37322;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#33258;&#28982;&#35821;&#35328;&#20013;&#21517;&#35789;&#30701;&#35821;&#30340;&#27867;&#25351;&#24615;&#36827;&#34892;&#32454;&#31890;&#24230;&#24314;&#27169;&#65292;&#36890;&#36807;&#36830;&#32493;&#30340;&#35780;&#27880;&#26041;&#27861;&#25429;&#25417;&#20102;&#27867;&#25351;&#24615;&#30340;&#24494;&#22937;&#26041;&#38754;&#65292;&#20026;&#35821;&#35328;&#23398;&#23478;&#25552;&#20379;&#20102;&#23454;&#29992;&#36164;&#28304;&#12290;</title><link>https://arxiv.org/abs/2403.15278</link><description>&lt;p&gt;
&#36890;&#36807;&#21253;&#23481;&#24615;&#21644;&#25277;&#35937;&#24615;&#36830;&#32493;&#21051;&#24230;&#35268;&#33539;&#27867;&#25351;&#24615;
&lt;/p&gt;
&lt;p&gt;
Specifying Genericity through Inclusiveness and Abstractness Continuous Scales
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15278
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#37322;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#33258;&#28982;&#35821;&#35328;&#20013;&#21517;&#35789;&#30701;&#35821;&#30340;&#27867;&#25351;&#24615;&#36827;&#34892;&#32454;&#31890;&#24230;&#24314;&#27169;&#65292;&#36890;&#36807;&#36830;&#32493;&#30340;&#35780;&#27880;&#26041;&#27861;&#25429;&#25417;&#20102;&#27867;&#25351;&#24615;&#30340;&#24494;&#22937;&#26041;&#38754;&#65292;&#20026;&#35821;&#35328;&#23398;&#23478;&#25552;&#20379;&#20102;&#23454;&#29992;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#37322;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#33258;&#28982;&#35821;&#35328;&#20013;&#21517;&#35789;&#30701;&#35821;&#65288;NPs&#65289;&#30340;&#27867;&#25351;&#24615;&#36827;&#34892;&#32454;&#31890;&#24230;&#24314;&#27169;&#12290;&#35813;&#26694;&#26550;&#35774;&#35745;&#31616;&#21333;&#30452;&#35266;&#65292;&#36866;&#29992;&#20110;&#38750;&#19987;&#23478;&#27880;&#37322;&#32773;&#24182;&#36866;&#21512;&#20247;&#21253;&#20219;&#21153;&#12290;&#32467;&#21512;&#26377;&#20851;&#27867;&#25351;&#24615;&#30340;&#29702;&#35770;&#21644;&#35748;&#30693;&#25991;&#29486;&#65292;&#35813;&#26694;&#26550;&#26681;&#26893;&#20110;&#24050;&#24314;&#31435;&#30340;&#35821;&#35328;&#23398;&#29702;&#35770;&#12290;&#36890;&#36807;&#19968;&#39033;&#35797;&#28857;&#30740;&#31350;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;324&#20010;&#21477;&#23376;&#30340;&#23567;&#32780;&#20851;&#38190;&#30340;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#22880;&#23450;&#22522;&#30784;&#12290;&#20026;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#35780;&#20272;&#65292;&#27604;&#36739;&#20102;&#25105;&#20204;&#36830;&#32493;&#27880;&#37322;&#19982;&#30456;&#21516;&#25968;&#25454;&#38598;&#19978;&#29616;&#26377;&#30340;&#20108;&#20803;&#27880;&#37322;&#65292;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#22312;&#25429;&#25417;&#27867;&#25351;&#24615;&#30340;&#24494;&#22937;&#26041;&#38754;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#35821;&#35328;&#23398;&#23478;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#29992;&#36164;&#28304;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#31532;&#19968;&#20010;&#32463;&#36807;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#26088;&#22312;&#26500;&#24314;&#21487;&#29992;&#20110;&#30740;&#31350;&#30340;&#23454;&#38469;&#35821;&#35328;&#25968;&#25454;&#38598;&#30340;&#27880;&#37322;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15278v1 Announce Type: new  Abstract: This paper introduces a novel annotation framework for the fine-grained modeling of Noun Phrases' (NPs) genericity in natural language. The framework is designed to be simple and intuitive, making it accessible to non-expert annotators and suitable for crowd-sourced tasks. Drawing from theoretical and cognitive literature on genericity, this framework is grounded in established linguistic theory. Through a pilot study, we created a small but crucial annotated dataset of 324 sentences, serving as a foundation for future research. To validate our approach, we conducted an evaluation comparing our continuous annotations with existing binary annotations on the same dataset, demonstrating the framework's effectiveness in capturing nuanced aspects of genericity. Our work offers a practical resource for linguists, providing a first annotated dataset and an annotation scheme designed to build real-language datasets that can be used in studies on
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;A$^3$T&#26694;&#26550;&#65292;&#36890;&#36807;ActRe&#25552;&#31034;&#20195;&#29702;&#23454;&#29616;&#20102;ReAct&#39118;&#26684;&#20195;&#29702;&#23545;&#20195;&#29702;&#36712;&#36857;&#30340;&#33258;&#20027;&#26631;&#27880;&#65292;&#21516;&#26102;&#22686;&#24378;&#20102;&#26032;&#30340;&#36712;&#36857;&#21512;&#25104;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.14589</link><description>&lt;p&gt;
ReAct&#36935;&#19978;ActRe&#65306;&#23545;&#27604;&#24615;&#33258;&#35757;&#32451;&#20013;&#30340;&#20195;&#29702;&#36712;&#36857;&#33258;&#21160;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
ReAct Meets ActRe: Autonomous Annotations of Agent Trajectories for Contrastive Self-Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14589
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;A$^3$T&#26694;&#26550;&#65292;&#36890;&#36807;ActRe&#25552;&#31034;&#20195;&#29702;&#23454;&#29616;&#20102;ReAct&#39118;&#26684;&#20195;&#29702;&#23545;&#20195;&#29702;&#36712;&#36857;&#30340;&#33258;&#20027;&#26631;&#27880;&#65292;&#21516;&#26102;&#22686;&#24378;&#20102;&#26032;&#30340;&#36712;&#36857;&#21512;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14589v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; &#25991;&#25688;&#65306;&#35821;&#35328;&#20195;&#29702;&#36890;&#36807;&#19982;&#22522;&#30784;&#27169;&#22411;&#25512;&#29702;&#23637;&#31034;&#20102;&#33258;&#20027;&#20915;&#31574;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#33268;&#21147;&#20110;&#36890;&#36807;&#22810;&#27493;&#25512;&#29702;&#21644;&#34892;&#21160;&#36712;&#36857;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#26469;&#35757;&#32451;&#35821;&#35328;&#20195;&#29702;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#36825;&#26679;&#30340;&#36712;&#36857;&#20173;&#38656;&#35201;&#30456;&#24403;&#22823;&#30340;&#20154;&#21147;&#65292;&#26080;&#35770;&#26159;&#36890;&#36807;&#20154;&#24037;&#26631;&#27880;&#36824;&#26159;&#23454;&#26045;&#22810;&#26679;&#21270;&#25552;&#31034;&#26694;&#26550;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;A$^3$T&#65292;&#19968;&#20010;&#20801;&#35768;&#20197;ReAct&#39118;&#26684;&#33258;&#20027;&#27880;&#37322;&#20195;&#29702;&#36712;&#36857;&#30340;&#26694;&#26550;&#12290;&#20854;&#20013;&#24515;&#26159;&#19968;&#20010;ActRe&#25552;&#31034;&#20195;&#29702;&#65292;&#23427;&#35299;&#37322;&#20219;&#24847;&#21160;&#20316;&#30340;&#21407;&#22240;&#12290;&#24403;&#38543;&#26426;&#25277;&#21462;&#22806;&#37096;&#21160;&#20316;&#26102;&#65292;ReAct&#39118;&#26684;&#20195;&#29702;&#21487;&#20197;&#26597;&#35810;ActRe&#20195;&#29702;&#20197;&#33719;&#21462;&#20854;&#25991;&#26412;&#29702;&#30001;&#12290;&#26032;&#39062;&#30340;&#36712;&#36857;&#28982;&#21518;&#36890;&#36807;&#23558;ActRe&#30340;&#21518;&#39564;&#25512;&#29702;&#21069;&#32622;&#21040;&#25277;&#26679;&#21160;&#20316;&#20013;&#36827;&#34892;&#32508;&#21512;&#21512;&#25104;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;ReAct&#39118;&#26684;&#20195;&#29702;&#21487;&#25191;&#34892;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14589v1 Announce Type: new  Abstract: Language agents have demonstrated autonomous decision-making abilities by reasoning with foundation models. Recently, efforts have been made to train language agents for performance improvement, with multi-step reasoning and action trajectories as the training data. However, collecting such trajectories still requires considerable human effort, by either artificial annotations or implementations of diverse prompting frameworks. In this work, we propose A$^3$T, a framework that enables the Autonomous Annotation of Agent Trajectories in the style of ReAct. The central role is an ActRe prompting agent, which explains the reason for an arbitrary action. When randomly sampling an external action, the ReAct-style agent could query the ActRe agent with the action to obtain its textual rationales. Novel trajectories are then synthesized by prepending the posterior reasoning from ActRe to the sampled action. In this way, the ReAct-style agent exe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#25972;&#36807;&#31243;&#20013;&#36890;&#36807;&#21033;&#29992;CLIP&#30340;&#22266;&#26377;&#23646;&#24615;&#26469;&#25506;&#35752;&#26657;&#20934;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#25552;&#31034;&#36873;&#25321;&#26174;&#33879;&#24433;&#21709;&#20102;CLIP&#20013;&#30340;&#26657;&#20934;&#65292;&#20854;&#20013;&#23548;&#33268;&#26356;&#39640;&#25991;&#26412;&#29305;&#24449;&#31163;&#25955;&#24615;&#30340;&#25552;&#31034;&#20250;&#20135;&#29983;&#26356;&#22909;&#26657;&#20934;&#30340;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.14119</link><description>&lt;p&gt;
C-TPT&#65306;&#36890;&#36807;&#25991;&#26412;&#29305;&#24449;&#31163;&#25955;&#24615;&#30340;&#26657;&#20934;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#25972;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via Text Feature Dispersion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#25972;&#36807;&#31243;&#20013;&#36890;&#36807;&#21033;&#29992;CLIP&#30340;&#22266;&#26377;&#23646;&#24615;&#26469;&#25506;&#35752;&#26657;&#20934;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#25552;&#31034;&#36873;&#25321;&#26174;&#33879;&#24433;&#21709;&#20102;CLIP&#20013;&#30340;&#26657;&#20934;&#65292;&#20854;&#20013;&#23548;&#33268;&#26356;&#39640;&#25991;&#26412;&#29305;&#24449;&#31163;&#25955;&#24615;&#30340;&#25552;&#31034;&#20250;&#20135;&#29983;&#26356;&#22909;&#26657;&#20934;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#27979;&#35797;&#26102;&#36866;&#24212;&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#20316;&#20026;&#19968;&#31181;&#22312;&#19981;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;&#26041;&#27861;&#12290;&#19968;&#20010;&#20027;&#35201;&#30340;&#20363;&#35777;&#26159;&#26368;&#36817;&#25552;&#20986;&#30340;&#29992;&#20110;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#30340;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#25972;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25552;&#31034;&#20027;&#35201;&#26159;&#20026;&#20102;&#25552;&#39640;&#20934;&#30830;&#24615;&#32780;&#24320;&#21457;&#30340;&#65292;&#24573;&#35270;&#20102;&#26657;&#20934;&#30340;&#37325;&#35201;&#24615;&#8212;&#8212;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#26657;&#20934;&#26041;&#27861;&#20381;&#36182;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#22312;&#27979;&#35797;&#26102;&#22330;&#26223;&#19979;&#19981;&#20999;&#23454;&#38469;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;CLIP&#30340;&#22266;&#26377;&#23646;&#24615;&#65292;&#22312;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#25972;&#36807;&#31243;&#20013;&#25506;&#35752;&#26657;&#20934;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#35266;&#23519;&#65292;&#25105;&#20204;&#21457;&#29616;&#25552;&#31034;&#36873;&#25321;&#26174;&#33879;&#24433;&#21709;&#20102;CLIP&#20013;&#30340;&#26657;&#20934;&#65292;&#20854;&#20013;&#23548;&#33268;&#26356;&#39640;&#25991;&#26412;&#29305;&#24449;&#31163;&#25955;&#24615;&#30340;&#25552;&#31034;&#20250;&#20135;&#29983;&#26356;&#22909;&#26657;&#20934;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14119v1 Announce Type: cross  Abstract: In deep learning, test-time adaptation has gained attention as a method for model fine-tuning without the need for labeled data. A prime exemplification is the recently proposed test-time prompt tuning for large-scale vision-language models such as CLIP. Unfortunately, these prompts have been mainly developed to improve accuracy, overlooking the importance of calibration-a crucial aspect for quantifying prediction uncertainty. However, traditional calibration methods rely on substantial amounts of labeled data, making them impractical for test-time scenarios. To this end, this paper explores calibration during test-time prompt tuning by leveraging the inherent properties of CLIP. Through a series of observations, we find that the prompt choice significantly affects the calibration in CLIP, where the prompts leading to higher text feature dispersion result in better-calibrated predictions. Introducing the Average Text Feature Dispersion
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Zigzag Mamba&#30340;&#38646;&#21442;&#25968;&#26041;&#27861;&#65292;&#36890;&#36807;&#32416;&#27491;&#24403;&#21069;Mamba-based&#35270;&#35273;&#26041;&#27861;&#20013;&#23545;&#31354;&#38388;&#36830;&#32493;&#24615;&#30340;&#24573;&#35270;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#36895;&#24230;&#21644;&#20869;&#23384;&#21033;&#29992;&#65292;&#21516;&#26102;&#22312;&#22823;&#20998;&#36776;&#29575;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.13802</link><description>&lt;p&gt;
ZigMa&#65306;&#34623;&#34578;&#26364;&#24052;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ZigMa: Zigzag Mamba Diffusion Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Zigzag Mamba&#30340;&#38646;&#21442;&#25968;&#26041;&#27861;&#65292;&#36890;&#36807;&#32416;&#27491;&#24403;&#21069;Mamba-based&#35270;&#35273;&#26041;&#27861;&#20013;&#23545;&#31354;&#38388;&#36830;&#32493;&#24615;&#30340;&#24573;&#35270;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#36895;&#24230;&#21644;&#20869;&#23384;&#21033;&#29992;&#65292;&#21516;&#26102;&#22312;&#22823;&#20998;&#36776;&#29575;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#21463;&#21040;&#21487;&#20280;&#32553;&#24615;&#21644;&#20108;&#27425;&#22797;&#26434;&#24615;&#38382;&#39064;&#30340;&#22256;&#25200;&#65292;&#29305;&#21035;&#26159;&#22312;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#32467;&#26500;&#20869;&#37096;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#21033;&#29992;&#19968;&#31181;&#31216;&#20026;&#26364;&#24052;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#38271;&#24207;&#21015;&#24314;&#27169;&#33021;&#21147;&#65292;&#20197;&#25193;&#23637;&#20854;&#22312;&#35270;&#35273;&#25968;&#25454;&#29983;&#25104;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22823;&#22810;&#25968;&#24403;&#21069;&#22522;&#20110;&#26364;&#24052;&#30340;&#35270;&#35273;&#26041;&#27861;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#30095;&#24573;&#65292;&#21363;&#26364;&#24052;&#30340;&#25195;&#25551;&#26041;&#26696;&#20013;&#32570;&#20047;&#23545;&#31354;&#38388;&#36830;&#32493;&#24615;&#30340;&#32771;&#34385;&#12290;&#20854;&#27425;&#65292;&#22522;&#20110;&#36825;&#19968;&#27934;&#23519;&#21147;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Zigzag Mamba&#30340;&#31616;&#21333;&#12289;&#21363;&#25554;&#21363;&#29992;&#12289;&#38646;&#21442;&#25968;&#26041;&#27861;&#65292;&#23427;&#20248;&#20110;&#22522;&#20110;&#26364;&#24052;&#30340;&#22522;&#32447;&#65292;&#24182;&#34920;&#29616;&#20986;&#27604;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#22522;&#32447;&#26356;&#24555;&#36895;&#21644;&#26356;&#22909;&#30340;&#20869;&#23384;&#21033;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;Zigzag Mamba&#38598;&#25104;&#21040;&#38543;&#26426;&#25554;&#20540;&#26694;&#26550;&#20013;&#65292;&#20197;&#30740;&#31350;&#27169;&#22411;&#22312;&#22823;&#20998;&#36776;&#29575;&#35270;&#35273;&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;FacesHQ $1024\times 1024$&#21644;UCF101&#65292;MultiModal-CelebA-HQ&#65289;&#19978;&#30340;&#21487;&#20280;&#32553;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13802v1 Announce Type: cross  Abstract: The diffusion model has long been plagued by scalability and quadratic complexity issues, especially within transformer-based structures. In this study, we aim to leverage the long sequence modeling capability of a State-Space Model called Mamba to extend its applicability to visual data generation. Firstly, we identify a critical oversight in most current Mamba-based vision methods, namely the lack of consideration for spatial continuity in the scan scheme of Mamba. Secondly, building upon this insight, we introduce a simple, plug-and-play, zero-parameter method named Zigzag Mamba, which outperforms Mamba-based baselines and demonstrates improved speed and memory utilization compared to transformer-based baselines. Lastly, we integrate Zigzag Mamba with the Stochastic Interpolant framework to investigate the scalability of the model on large-resolution visual datasets, such as FacesHQ $1024\times 1024$ and UCF101, MultiModal-CelebA-HQ
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21019;&#24314;&#20351;&#29992; GPT-2 &#30340;&#26426;&#22120;&#20154;&#36134;&#25143;&#65292;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#22238;&#22797;&#29992;&#25143;&#30340;&#25512;&#25991;&#65292;&#40723;&#21169;&#29992;&#25143;&#25509;&#35302;&#21644;&#20851;&#27880;&#39564;&#35777;&#30340;&#12289;&#24847;&#35782;&#24418;&#24577;&#24179;&#34913;&#30340;&#26032;&#38395;&#65292;&#20197;&#22686;&#21152;&#29992;&#25143;&#25509;&#35302;&#36825;&#20123;&#26032;&#38395;&#24182;&#25552;&#39640;&#21442;&#19982;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.13362</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#30495;&#23454;&#26426;&#22120;&#20154;&#36134;&#25143;&#28608;&#21169;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#30340;&#26032;&#38395;&#28040;&#36153;
&lt;/p&gt;
&lt;p&gt;
Incentivizing News Consumption on Social Media Platforms Using Large Language Models and Realistic Bot Accounts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13362
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21019;&#24314;&#20351;&#29992; GPT-2 &#30340;&#26426;&#22120;&#20154;&#36134;&#25143;&#65292;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#22238;&#22797;&#29992;&#25143;&#30340;&#25512;&#25991;&#65292;&#40723;&#21169;&#29992;&#25143;&#25509;&#35302;&#21644;&#20851;&#27880;&#39564;&#35777;&#30340;&#12289;&#24847;&#35782;&#24418;&#24577;&#24179;&#34913;&#30340;&#26032;&#38395;&#65292;&#20197;&#22686;&#21152;&#29992;&#25143;&#25509;&#35302;&#36825;&#20123;&#26032;&#38395;&#24182;&#25552;&#39640;&#21442;&#19982;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26497;&#21270;&#12289;&#20449;&#20219;&#19979;&#38477;&#20197;&#21450;&#23545;&#27665;&#20027;&#35268;&#33539;&#25903;&#25345;&#21160;&#25671;&#26159;&#32654;&#22269;&#27665;&#20027;&#38754;&#20020;&#30340;&#32039;&#36843;&#23041;&#32961;&#12290;&#25509;&#35302;&#39564;&#35777;&#21644;&#20248;&#36136;&#26032;&#38395;&#21487;&#33021;&#38477;&#20302;&#20010;&#20154;&#23545;&#36825;&#20123;&#23041;&#32961;&#30340;&#26131;&#24863;&#24615;&#65292;&#24182;&#20351;&#20844;&#27665;&#26356;&#20855;&#25239;&#20987;&#38169;&#35823;&#20449;&#24687;&#12289;&#27665;&#31929;&#20027;&#20041;&#21644;&#26497;&#31471;&#20826;&#27966;&#35328;&#35770;&#30340;&#33021;&#21147;&#12290;&#35813;&#39033;&#30446;&#25506;&#35752;&#20102;&#22914;&#20309;&#22312;&#19968;&#20010;&#29983;&#24577;&#26377;&#25928;&#30340;&#29615;&#22659;&#20013;&#22686;&#24378;&#29992;&#25143;&#25509;&#35302;&#21644;&#21442;&#19982;&#39564;&#35777;&#30340;&#12289;&#24847;&#35782;&#24418;&#24577;&#24179;&#34913;&#30340;&#26032;&#38395;&#12290;&#25105;&#20204;&#20381;&#36182;&#20110;&#23545; 28,457 &#20010; Twitter &#29992;&#25143;&#36827;&#34892;&#30340;&#22823;&#35268;&#27169;&#20026;&#26399;&#20004;&#21608;&#30340;&#30000;&#37326;&#23454;&#39564;&#65288;&#20174; 2023 &#24180; 1 &#26376; 19 &#26085;&#21040; 2 &#26376; 3 &#26085;&#65289;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102; 28 &#20010;&#21033;&#29992; GPT-2 &#30340;&#26426;&#22120;&#20154;&#65292;&#22312;&#29992;&#25143;&#21457;&#34920;&#26377;&#20851;&#20307;&#32946;&#12289;&#23089;&#20048;&#25110;&#29983;&#27963;&#26041;&#24335;&#30340;&#25512;&#25991;&#26102;&#22238;&#22797;&#19968;&#20010;&#20869;&#23481;&#30456;&#20851;&#30340;&#22238;&#22797;&#65292;&#20854;&#20013;&#21253;&#21547;&#20004;&#20010;&#30828;&#20195;&#30721;&#20803;&#32032;&#65306;&#19968;&#20010;&#25351;&#21521;&#20248;&#36136;&#26032;&#38395;&#26426;&#26500;&#30456;&#20851;&#20027;&#39064;&#37096;&#20998;&#30340; URL &#21644;&#40723;&#21169;&#20851;&#27880;&#20854; Twitter &#36134;&#25143;&#12290;&#20026;&#36827;&#19968;&#27493;&#27979;&#35797;&#26426;&#22120;&#20154;&#23545;&#24615;&#21035;&#30340;&#24046;&#24322;&#24433;&#21709;&#65292;&#34987;&#35797;&#29992;&#25143;&#34987;&#38543;&#26426;&#20998;&#37197;&#20197;&#25509;&#21463;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13362v1 Announce Type: cross  Abstract: Polarization, declining trust, and wavering support for democratic norms are pressing threats to U.S. democracy. Exposure to verified and quality news may lower individual susceptibility to these threats and make citizens more resilient to misinformation, populism, and hyperpartisan rhetoric. This project examines how to enhance users' exposure to and engagement with verified and ideologically balanced news in an ecologically valid setting. We rely on a large-scale two-week long field experiment (from 1/19/2023 to 2/3/2023) on 28,457 Twitter users. We created 28 bots utilizing GPT-2 that replied to users tweeting about sports, entertainment, or lifestyle with a contextual reply containing two hardcoded elements: a URL to the topic-relevant section of quality news organization and an encouragement to follow its Twitter account. To further test differential effects by gender of the bots, treated users were randomly assigned to receive re
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20004;&#31181;&#25104;&#26412;&#26377;&#25928;&#30340;&#26041;&#27861;&#35299;&#20915;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#33521;&#35821;-&#38889;&#35821;-&#20013;&#25991;&#22810;&#35821;&#35328;&#12289;&#22810;&#27169;&#24577;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#24320;&#21457;&#20102;&#34920;&#29616;&#20248;&#36234;&#30340;&#21452;&#35821;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.11399</link><description>&lt;p&gt;
X-LLaVA: &#20248;&#21270;&#21452;&#35821;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
X-LLaVA: Optimizing Bilingual Large Vision-Language Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11399
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20004;&#31181;&#25104;&#26412;&#26377;&#25928;&#30340;&#26041;&#27861;&#35299;&#20915;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#33521;&#35821;-&#38889;&#35821;-&#20013;&#25991;&#22810;&#35821;&#35328;&#12289;&#22810;&#27169;&#24577;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#24320;&#21457;&#20102;&#34920;&#29616;&#20248;&#36234;&#30340;&#21452;&#35821;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26174;&#33879;&#21457;&#23637;&#27491;&#22312;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#30340;&#39046;&#22495;&#65292;&#36825;&#20123;&#27169;&#22411;&#38598;&#25104;&#20102;&#38500;&#25991;&#26412;&#20197;&#22806;&#30340;&#22810;&#31181;&#25968;&#25454;&#31867;&#22411;&#12290;&#28982;&#32780;&#65292;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#29305;&#24615;&#23548;&#33268;&#22312;&#21019;&#24314;&#35757;&#32451;&#25968;&#25454;&#26041;&#38754;&#23384;&#22312;&#26174;&#30528;&#30340;&#24320;&#38144;&#12290;&#27492;&#22806;&#65292;&#20026;LMMs&#26500;&#24314;&#22810;&#35821;&#35328;&#25968;&#25454;&#20063;&#38754;&#20020;&#30528;&#35821;&#35328;&#22810;&#26679;&#24615;&#21644;&#22797;&#26434;&#24615;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#25104;&#26412;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65306;&#65288;1&#65289;&#22810;&#35821;&#35328;LLM&#30340;&#35789;&#27719;&#25193;&#23637;&#21644;&#39044;&#35757;&#32451;&#65292;&#20197;&#21450;&#65288;2&#65289;&#20351;&#29992;GPT4-V&#33258;&#21160;&#21644;&#31934;&#24515;&#26500;&#24314;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#12290;&#22522;&#20110;&#36825;&#20123;&#26041;&#27861;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;91K&#33521;&#25991;-&#38889;&#25991;-&#20013;&#25991;&#30340;&#22810;&#35821;&#35328;&#12289;&#22810;&#27169;&#24577;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21452;&#35821;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#22312;&#38889;&#35821;&#21644;&#33521;&#35821;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11399v1 Announce Type: new  Abstract: The impressive development of large language models (LLMs) is expanding into the realm of large multimodal models (LMMs), which incorporate multiple types of data beyond text. However, the nature of multimodal models leads to significant expenses in the creation of training data. Furthermore, constructing multilingual data for LMMs presents its own set of challenges due to language diversity and complexity. Therefore, in this study, we propose two cost-effective methods to solve this problem: (1) vocabulary expansion and pretraining of multilingual LLM for specific languages, and (2) automatic and elaborate construction of multimodal datasets using GPT4-V. Based on015 these methods, we constructed a 91K English-Korean-Chinese multilingual, multimodal training dataset. Additionally, we developed a bilingual multimodal model that exhibits excellent performance in both Korean and English, surpassing existing approaches.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#25968;&#23398;&#38382;&#39064;&#30340;&#25991;&#23383;&#39064;&#30446;&#26102;&#25152;&#38754;&#20020;&#30340;&#22256;&#38590;&#65292;&#36890;&#36807;&#20998;&#26512;&#29305;&#24449;&#21644;&#35757;&#32451;&#20998;&#31867;&#22120;&#26469;&#39044;&#27979;&#27169;&#22411;&#23545;&#19981;&#21516;&#31867;&#22411;&#39064;&#30446;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.11369</link><description>&lt;p&gt;
&#20160;&#20040;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#25968;&#23398;&#38382;&#39064;&#38590;&#20197;&#24212;&#23545;&#65311;
&lt;/p&gt;
&lt;p&gt;
What Makes Math Word Problems Challenging for LLMs?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11369
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#25968;&#23398;&#38382;&#39064;&#30340;&#25991;&#23383;&#39064;&#30446;&#26102;&#25152;&#38754;&#20020;&#30340;&#22256;&#38590;&#65292;&#36890;&#36807;&#20998;&#26512;&#29305;&#24449;&#21644;&#35757;&#32451;&#20998;&#31867;&#22120;&#26469;&#39044;&#27979;&#27169;&#22411;&#23545;&#19981;&#21516;&#31867;&#22411;&#39064;&#30446;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#32780;&#35328;&#65292;&#20160;&#20040;&#35753;&#25968;&#23398;&#38382;&#39064;&#30340;&#25991;&#23383;&#39064;&#30446;(math word problems, MWPs)&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#28145;&#20837;&#20998;&#26512;&#20102;MWPs&#30340;&#20851;&#38190;&#35821;&#35328;&#21644;&#25968;&#23398;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#22522;&#20110;&#29305;&#24449;&#30340;&#20998;&#31867;&#22120;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#27599;&#20010;&#29305;&#24449;&#23545;&#20110;LLMs&#26085;&#24120;&#20219;&#21153;&#20013;MWPs&#30340;&#25972;&#20307;&#38590;&#24230;&#30340;&#24433;&#21709;&#65292;&#24182;&#25506;&#31350;&#36825;&#26159;&#21542;&#26377;&#21161;&#20110;&#39044;&#27979;LLMs&#22312;&#29305;&#23450;&#31867;&#21035;&#30340;MWPs&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11369v1 Announce Type: new  Abstract: This paper investigates the question of what makes math word problems (MWPs) challenging for large language models (LLMs). We conduct an in-depth analysis of the key linguistic and mathematical characteristics of MWPs. In addition, we train feature-based classifiers to better understand the impact of each feature on the overall difficulty of MWPs for prominent LLMs and investigate whether this helps predict how well LLMs fare against specific categories of MWPs.
&lt;/p&gt;</description></item><item><title>&#26356;&#22810;&#30340;&#21709;&#24212;&#20294;&#26356;&#23569;&#30340;&#25552;&#31034;&#21487;&#20197;&#26356;&#22909;&#22320;&#35302;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20154;&#31867;&#23545;&#40784;&#65292;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#22810;&#26679;&#24615;&#20844;&#24335;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#24433;&#21709;LLMs&#30340;&#26368;&#32456;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.11124</link><description>&lt;p&gt;
&#22312;&#20154;&#31867;&#23545;&#40784;&#20013;&#25193;&#23637;&#25968;&#25454;&#22810;&#26679;&#24615;&#20197;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Scaling Data Diversity for Fine-Tuning Language Models in Human Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11124
&lt;/p&gt;
&lt;p&gt;
&#26356;&#22810;&#30340;&#21709;&#24212;&#20294;&#26356;&#23569;&#30340;&#25552;&#31034;&#21487;&#20197;&#26356;&#22909;&#22320;&#35302;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20154;&#31867;&#23545;&#40784;&#65292;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#22810;&#26679;&#24615;&#20844;&#24335;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#24433;&#21709;LLMs&#30340;&#26368;&#32456;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#21487;&#20197;&#38450;&#27490;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#35823;&#23548;&#24615;&#25110;&#26377;&#27602;&#20869;&#23481;&#65292;&#21516;&#26102;&#38656;&#35201;&#39640;&#25104;&#26412;&#30340;&#20154;&#31867;&#21453;&#39304;&#12290;&#20551;&#35774;&#20154;&#24037;&#27880;&#37322;&#36164;&#28304;&#26377;&#38480;&#65292;&#21017;&#21487;&#20197;&#32771;&#34385;&#20004;&#31181;&#19981;&#21516;&#30340;&#20998;&#37197;&#26041;&#24335;&#65306;&#26356;&#22810;&#26679;&#21270;&#30340;&#25552;&#31034;&#25110;&#26356;&#22810;&#26679;&#21270;&#30340;&#24453;&#26631;&#35760;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#30340;&#30452;&#25509;&#27604;&#36739;&#23578;&#19981;&#23384;&#22312;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#26681;&#25454;&#24494;&#35843;&#26679;&#26412;&#25968;&#37327;&#25511;&#21046;&#21452;&#26041;&#30340;&#22810;&#26679;&#24615;&#65292;&#36825;&#21487;&#20197;&#30452;&#25509;&#21453;&#26144;&#23427;&#20204;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19982;&#22823;&#37327;&#25552;&#31034;&#19981;&#21516;&#65292;&#26356;&#22810;&#30340;&#21709;&#24212;&#20294;&#26159;&#26356;&#23569;&#30340;&#25552;&#31034;&#26356;&#33021;&#28608;&#21457;LLMs&#36827;&#34892;&#20154;&#31867;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#25552;&#31034;&#30340;&#22810;&#26679;&#24615;&#27010;&#24565;&#21487;&#33021;&#27604;&#36890;&#24120;&#30001;&#21333;&#20010;&#25968;&#23383;&#37327;&#21270;&#30340;&#21709;&#24212;&#26356;&#22797;&#26434;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#25552;&#31034;&#22810;&#26679;&#24615;&#30340;&#26032;&#20844;&#24335;&#65292;&#36827;&#19968;&#27493;&#26263;&#31034;&#19982;&#24494;&#35843;&#21518;LLMs&#26368;&#32456;&#24615;&#33021;&#30340;&#32447;&#24615;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11124v1 Announce Type: cross  Abstract: Alignment with human preference prevents large language models (LLMs) from generating misleading or toxic content while requiring high-cost human feedback. Assuming resources of human annotation are limited, there are two different ways of allocating considered: more diverse PROMPTS or more diverse RESPONSES to be labeled. Nonetheless, a straightforward comparison between their impact is absent. In this work, we first control the diversity of both sides according to the number of samples for fine-tuning, which can directly reflect their influence. We find that instead of numerous prompts, more responses but fewer prompts better trigger LLMs for human alignment. Additionally, the concept of diversity for prompts can be more complex than responses that are typically quantified by single digits. Consequently, a new formulation of prompt diversity is proposed, further implying a linear correlation with the final performance of LLMs after f
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MT-Patcher&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21040;&#20013;&#31561;&#35268;&#27169;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#26377;&#36873;&#25321;&#24615;&#12289;&#20840;&#38754;&#21644;&#20027;&#21160;&#30340;&#30693;&#35782;&#36801;&#31227;</title><link>https://arxiv.org/abs/2403.09522</link><description>&lt;p&gt;
MT-PATCHER&#65306;&#26469;&#33258;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#36873;&#25321;&#24615;&#21644;&#21487;&#25193;&#23637;&#30340;&#30693;&#35782;&#33976;&#39311;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
MT-PATCHER: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09522
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MT-Patcher&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21040;&#20013;&#31561;&#35268;&#27169;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#26377;&#36873;&#25321;&#24615;&#12289;&#20840;&#38754;&#21644;&#20027;&#21160;&#30340;&#30693;&#35782;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#39046;&#22495;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#38754;&#20020;&#30528;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#24310;&#36831;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#23558;&#32763;&#35793;&#30693;&#35782;&#20174;&#24040;&#22411;LLM&#36716;&#31227;&#21040;&#20013;&#31561;&#35268;&#27169;&#30340;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MT-Patcher&#30340;&#26694;&#26550;&#65292;&#20197;&#36873;&#25321;&#24615;&#12289;&#20840;&#38754;&#21644;&#20027;&#21160;&#30340;&#26041;&#24335;&#23558;&#30693;&#35782;&#20174;LLMs&#36716;&#31227;&#21040;&#29616;&#26377;&#30340;MT&#27169;&#22411;&#20013;&#12290;&#32771;&#34385;&#21040;&#23398;&#29983;MT&#27169;&#22411;&#24403;&#21069;&#30340;&#32763;&#35793;&#33021;&#21147;&#65292;&#25105;&#20204;&#20165;&#35782;&#21035;&#21644;&#32416;&#27491;&#20854;&#32763;&#35793;&#38169;&#35823;&#65292;&#32780;&#19981;&#26159;&#20174;&#32769;&#24072;&#37027;&#37324;&#33976;&#39311;&#25972;&#20010;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09522v1 Announce Type: new  Abstract: Large Language Models (LLM) have demonstrated their strong ability in the field of machine translation (MT), yet they suffer from high computational cost and latency. Therefore, transferring translation knowledge from giant LLMs to medium-sized machine translation models is a promising research direction. However, traditional knowledge distillation methods do not take the capability of student and teacher models into consideration, therefore repeatedly teaching student models on the knowledge they have learned, and failing to extend to novel contexts and knowledge. In this paper, we propose a framework called MT-Patcher, which transfers knowledge from LLMs to existing MT models in a selective, comprehensive and proactive manner. Considering the current translation ability of student MT models, we only identify and correct their translation errors, instead of distilling the whole translation from the teacher. Leveraging the strong languag
&lt;/p&gt;</description></item><item><title>Ethos&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20219;&#21153;&#21521;&#37327;&#19978;&#36827;&#34892;&#30699;&#27491;LMs&#20197;&#20943;&#36731;&#20135;&#29983;&#27602;&#24615;&#21644;&#20559;&#35265;&#36755;&#20986;&#20197;&#21450;&#36991;&#20813;&#38544;&#31169;&#27844;&#38706;&#12290;</title><link>https://arxiv.org/abs/2403.08994</link><description>&lt;p&gt;
Ethos&#65306;&#22312;&#27491;&#20132;&#21442;&#25968;&#31354;&#38388;&#20013;&#30699;&#27491;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Ethos: Rectifying Language Models in Orthogonal Parameter Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08994
&lt;/p&gt;
&lt;p&gt;
Ethos&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20219;&#21153;&#21521;&#37327;&#19978;&#36827;&#34892;&#30699;&#27491;LMs&#20197;&#20943;&#36731;&#20135;&#29983;&#27602;&#24615;&#21644;&#20559;&#35265;&#36755;&#20986;&#20197;&#21450;&#36991;&#20813;&#38544;&#31169;&#27844;&#38706;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#26497;&#22823;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;LMs&#20063;&#24341;&#21457;&#20102;&#20851;&#20110;&#29983;&#25104;&#20559;&#35265;&#25110;&#26377;&#27602;&#20869;&#23481;&#20197;&#21450;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#31169;&#20154;&#20449;&#24687;&#21487;&#33021;&#27844;&#38706;&#30340;&#25285;&#24551;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;Ethos&#65292;&#36890;&#36807;&#22312;&#20219;&#21153;&#21521;&#37327;&#19978;&#36827;&#34892;&#30699;&#27491;LMs&#20197;&#20943;&#36731;&#20135;&#29983;&#27602;&#24615;&#21644;&#20559;&#35265;&#36755;&#20986;&#20197;&#21450;&#36991;&#20813;&#38544;&#31169;&#27844;&#38706;&#12290;Ethos&#24314;&#31435;&#22312;&#20219;&#21153;&#31639;&#26415;&#22522;&#30784;&#19978;&#12290;&#28982;&#32780;&#65292;&#19982;&#24403;&#21069;&#30340;&#20219;&#21153;&#31639;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;Ethos&#22312;&#37325;&#26500;&#20219;&#21153;&#21521;&#37327;&#26102;&#21306;&#20998;&#20102;&#19968;&#33324;&#26377;&#30410;&#21644;&#19981;&#33391;&#30693;&#35782;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;Ethos&#39318;&#20808;&#20351;&#29992;&#22855;&#24322;&#20540;&#20998;&#35299;&#20174;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#33719;&#24471;&#19968;&#32452;&#20027;&#25104;&#20998;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#21521;&#37327;&#25237;&#24433;&#21040;&#20027;&#25104;&#20998;&#19978;&#65292;Ethos&#35782;&#21035;&#32534;&#30721;&#19968;&#33324;&#25110;&#19981;&#33391;&#30693;&#35782;&#30340;&#20027;&#25104;&#20998;&#12290;Ethos&#20165;&#20351;&#29992;&#24102;&#26377;&#19981;&#33391;&#30693;&#35782;&#30340;&#20219;&#21153;&#21521;&#37327;&#36827;&#34892;&#21542;&#23450;&#65292;&#20174;&#32780;&#26368;&#23567;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08994v1 Announce Type: new  Abstract: Language models (LMs) have greatly propelled the research on natural language processing. However, LMs also raise concerns regarding the generation of biased or toxic content and the potential disclosure of private information from the training dataset. In this work, we present a new efficient approach, Ethos, that rectifies LMs to mitigate toxicity and bias in outputs and avoid privacy leakage. Ethos is built on task arithmetic. However, unlike current task arithmetic algorithms, Ethos distinguishes general beneficial and undesired knowledge when reconstructing task vectors. Specifically, Ethos first obtains a set of principal components from the pre-trained models using singular value decomposition. Then, by projecting the task vector onto principal components, Ethos identifies the principal components that encode general or undesired knowledge. Ethos performs negating using the task vector with undesired knowledge only, thereby minimi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#30697;&#38453;&#21464;&#25442;&#30340;&#20302;&#31209;&#35843;&#25972;&#65288;MTLoRA&#65289;&#26041;&#27861;&#65292;&#21463;&#22823;&#33041;&#21551;&#21457;&#65292;&#29992;&#20110;&#25552;&#39640;&#24494;&#35843;&#25216;&#26415;&#30340;&#22797;&#26434;&#20219;&#21153;&#36866;&#24212;&#24615;&#12289;&#24615;&#33021;&#12289;&#31283;&#23450;&#24615;&#21644;&#31639;&#27861;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.07440</link><description>&lt;p&gt;
&#22522;&#20110;&#30697;&#38453;&#21464;&#25442;&#30340;&#20302;&#31209;&#35843;&#25972;&#65288;MTLoRA&#65289;&#65306;&#19968;&#31181;&#21463;&#22823;&#33041;&#21551;&#21457;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Matrix-Transformation Based Low-Rank Adaptation (MTLoRA): A Brain-Inspired Method for Parameter-Efficient Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07440
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#30697;&#38453;&#21464;&#25442;&#30340;&#20302;&#31209;&#35843;&#25972;&#65288;MTLoRA&#65289;&#26041;&#27861;&#65292;&#21463;&#22823;&#33041;&#21551;&#21457;&#65292;&#29992;&#20110;&#25552;&#39640;&#24494;&#35843;&#25216;&#26415;&#30340;&#22797;&#26434;&#20219;&#21153;&#36866;&#24212;&#24615;&#12289;&#24615;&#33021;&#12289;&#31283;&#23450;&#24615;&#21644;&#31639;&#27861;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LPLMs&#65289;&#30340;&#24494;&#35843;&#25216;&#26415;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#26377;&#25928;&#25511;&#21046;LPLMs&#30340;&#36755;&#20986;&#34892;&#20026;&#12290;&#26412;&#25991;&#21463;&#22823;&#33041;&#21151;&#33021;&#21463;&#20854;&#20960;&#20309;&#32467;&#26500;&#22609;&#36896;&#30340;&#21551;&#21457;&#65292;&#23558;&#36825;&#19968;&#24605;&#24819;&#34701;&#20837;LoRA&#25216;&#26415;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#30697;&#38453;&#21464;&#25442;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#20197;&#20943;&#23569;&#22797;&#26434;&#20219;&#21153;&#36866;&#24212;&#24615;&#12289;&#24615;&#33021;&#12289;&#31283;&#23450;&#24615;&#21644;&#31639;&#27861;&#22797;&#26434;&#24615;&#26041;&#38754;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07440v1 Announce Type: cross  Abstract: Fine-tuning techniques based on Large Pretrained Language Models (LPLMs) have been proven to significantly enhance model performance on a variety of downstream tasks and effectively control the output behaviors of LPLMs. Recent studies have proposed numerous methods for fine-tuning a small number of parameters based on open-source LPLMs, reducing the demand for computational and storage resources. Among these, reparameterization fine-tuning methods represented by LoRA (Low-Rank Adaptation) have gained popularity. We find that although these methods perform well in many aspects, there is still considerable room for improvement in terms of complex task adaptability, performance, stability, and algorithm complexity. In response to this, inspired by the idea that the functions of the brain are shaped by its geometric structure, this paper integrates this idea into LoRA technology and proposes a new matrix transformation-based reparameteriz
&lt;/p&gt;</description></item><item><title>SVD-LLM&#26159;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;SVD&#30340;LLM&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#25130;&#26029;&#24863;&#30693;&#25968;&#25454;&#30333;&#21270;&#31574;&#30053;&#21644;&#36880;&#23618;&#38381;&#24335;&#27169;&#22411;&#21442;&#25968;&#26356;&#26032;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#23454;&#29616;&#20102;&#30452;&#25509;&#26144;&#23556;&#22855;&#24322;&#20540;&#21644;&#21387;&#32553;&#25439;&#22833;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.07378</link><description>&lt;p&gt;
SVD-LLM: &#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#30340;&#25130;&#26029;&#24863;&#30693;&#22855;&#24322;&#20540;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07378
&lt;/p&gt;
&lt;p&gt;
SVD-LLM&#26159;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;SVD&#30340;LLM&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#25130;&#26029;&#24863;&#30693;&#25968;&#25454;&#30333;&#21270;&#31574;&#30053;&#21644;&#36880;&#23618;&#38381;&#24335;&#27169;&#22411;&#21442;&#25968;&#26356;&#26032;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#23454;&#29616;&#20102;&#30452;&#25509;&#26144;&#23556;&#22855;&#24322;&#20540;&#21644;&#21387;&#32553;&#25439;&#22833;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#21463;&#21040;&#20854;&#24222;&#22823;&#23610;&#23544;&#30340;&#38480;&#21046;&#65292;&#36825;&#38656;&#35201;LLM&#21387;&#32553;&#26041;&#27861;&#20197;&#23454;&#29616;&#23454;&#38469;&#37096;&#32626;&#12290;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#20026;LLM&#21387;&#32553;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;SVD&#30340;LLM&#21387;&#32553;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38480;&#21046;&#65306;&#25130;&#26029;&#36739;&#23567;&#30340;&#22855;&#24322;&#20540;&#21487;&#33021;&#23548;&#33268;&#26356;&#39640;&#30340;&#21387;&#32553;&#25439;&#22833;&#65292;&#24182;&#19988;&#22312;SVD&#25130;&#26029;&#21518;&#21097;&#20313;&#27169;&#22411;&#21442;&#25968;&#30340;&#26356;&#26032;&#32570;&#22833;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SVD-LLM&#65292;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;SVD&#30340;LLM&#21387;&#32553;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#38480;&#21046;&#12290;SVD-LLM&#37319;&#29992;&#20102;&#19968;&#31181;&#25130;&#26029;&#24863;&#30693;&#30340;&#25968;&#25454;&#30333;&#21270;&#31574;&#30053;&#65292;&#20197;&#30830;&#20445;&#22855;&#24322;&#20540;&#21644;&#21387;&#32553;&#25439;&#22833;&#20043;&#38388;&#30340;&#30452;&#25509;&#26144;&#23556;&#12290;&#27492;&#22806;&#65292;SVD-LLM&#37319;&#29992;&#19968;&#31181;&#36880;&#23618;&#38381;&#24335;&#27169;&#22411;&#21442;&#25968;&#26356;&#26032;&#31574;&#30053;&#65292;&#20197;&#24357;&#34917;SVD&#25130;&#26029;&#24341;&#36215;&#30340;&#20934;&#30830;&#24615;&#38477;&#20302;&#12290;&#25105;&#20204;&#22312;&#24635;&#20849;11&#20010;&#25968;&#25454;&#38598;&#21644;&#19971;&#20010;m&#19978;&#35780;&#20272;&#20102;SVD-LLM&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07378v1 Announce Type: new  Abstract: The advancements in Large Language Models (LLMs) have been hindered by their substantial sizes, which necessitate LLM compression methods for practical deployment. Singular Value Decomposition (SVD) offers a promising solution for LLM compression. However, state-of-the-art SVD-based LLM compression methods have two key limitations: truncating smaller singular values may lead to higher compression loss, and the lack of update on the remaining model parameters after SVD truncation. In this work, we propose SVD-LLM, a new SVD-based LLM compression method that addresses the limitations of existing methods. SVD-LLM incorporates a truncation-aware data whitening strategy to ensure a direct mapping between singular values and compression loss. Moreover, SVD-LLM adopts a layer-wise closed-form model parameter update strategy to compensate for accuracy degradation caused by SVD truncation. We evaluate SVD-LLM on a total of 11 datasets and seven m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#36890;&#36807;&#36981;&#24490;&#39044;&#23450;&#20041;&#27969;&#31243;&#21644;&#20445;&#30041;API&#20381;&#36182;&#24615;&#35299;&#20915;&#29992;&#25143;&#24847;&#22270;&#30340;&#24544;&#23454;&#35268;&#21010;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21069;&#30651;&#21551;&#21457;&#24335;&#30340;&#21463;&#38480;&#35299;&#30721;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.05766</link><description>&lt;p&gt;
FLAP: &#22312;LLMs&#20013;&#20855;&#26377;&#21463;&#38480;&#35299;&#30721;&#30340;&#27969;&#31243;&#36981;&#24490;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
FLAP: Flow Adhering Planning with Constrained Decoding in LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#36890;&#36807;&#36981;&#24490;&#39044;&#23450;&#20041;&#27969;&#31243;&#21644;&#20445;&#30041;API&#20381;&#36182;&#24615;&#35299;&#20915;&#29992;&#25143;&#24847;&#22270;&#30340;&#24544;&#23454;&#35268;&#21010;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21069;&#30651;&#21551;&#21457;&#24335;&#30340;&#21463;&#38480;&#35299;&#30721;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#21010;&#23545;&#20110;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#30340;&#20195;&#29702;&#20154;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#20154;&#31867;&#20195;&#29702;&#20154;&#36890;&#24120;&#36890;&#36807;&#36981;&#24490;&#39044;&#23450;&#20041;&#30340;&#24037;&#20316;&#27969;&#31243;&#35299;&#20915;&#29992;&#25143;&#38382;&#39064;&#65292;&#23558;&#24037;&#20316;&#27969;&#31243;&#27493;&#39588;&#20998;&#35299;&#20026;&#21487;&#25805;&#20316;&#39033;&#30446;&#65292;&#24182;&#36890;&#36807;&#25191;&#34892;API&#25191;&#34892;&#25805;&#20316;&#65307;&#25152;&#26377;&#36825;&#20123;&#37117;&#38656;&#35201;&#25512;&#29702;&#21644;&#35268;&#21010;&#12290;&#37492;&#20110;LLMs&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#22810;&#22320;&#23581;&#35797;&#20351;&#29992;LLMs&#36827;&#34892;&#20219;&#21153;&#35268;&#21010;&#21644;API&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;LLMs&#20559;&#21521;&#39044;&#35757;&#32451;&#25968;&#25454;&#65292;&#35745;&#21010;&#19982;&#39044;&#23450;&#20041;&#24037;&#20316;&#27969;&#31243;&#21644;API&#20381;&#36182;&#24615;&#30340;&#24544;&#23454;&#24615;&#24182;&#19981;&#34987;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#22312;&#29616;&#23454;&#29983;&#27963;&#20013;&#65292;&#24037;&#20316;&#27969;&#31243;&#26159;&#33258;&#23450;&#20041;&#30340;&#24182;&#19988;&#23481;&#26131;&#26356;&#25913;&#65292;&#22240;&#27492;&#65292;&#24555;&#36895;&#20351;&#20195;&#29702;&#20154;&#36866;&#24212;&#21464;&#21270;&#26159;&#21487;&#21462;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#36890;&#36807;&#36981;&#24490;&#39044;&#23450;&#20041;&#27969;&#31243;&#21644;&#20445;&#30041;API&#20381;&#36182;&#24615;&#35299;&#20915;&#29992;&#25143;&#24847;&#22270;&#30340;&#24544;&#23454;&#35268;&#21010;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21069;&#30651;&#21551;&#21457;&#24335;&#30340;&#21463;&#38480;&#35299;&#30721;&#31639;&#27861;&#29992;&#20110;&#24544;&#23454;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05766v1 Announce Type: new  Abstract: Planning is a crucial task for agents in task oriented dialogs (TODs). Human agents typically resolve user issues by following predefined workflows, decomposing workflow steps into actionable items, and performing actions by executing APIs in order; all of which require reasoning and planning. With the recent advances in LLMs, there have been increasing attempts to use LLMs for task planning and API usage. However, the faithfulness of the plans to predefined workflows and API dependencies, is not guaranteed with LLMs because of their bias towards pretraining data. Moreover, in real life, workflows are custom-defined and prone to change, hence, quickly adapting agents to the changes is desirable. In this paper, we study faithful planning in TODs to resolve user intents by following predefined flows and preserving API dependencies. We propose a constrained decoding algorithm based on lookahead heuristic for faithful planning. Our algorithm
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;LLM&#20195;&#29702;&#36827;&#34892;&#40657;&#30418;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#21463;&#23475;&#20195;&#29702;&#20013;&#26356;&#39640;&#32423;&#21035;&#30340;&#35760;&#24518;&#21270;&#65292;&#30456;&#27604;&#30452;&#25509;&#29992;&#35757;&#32451;&#25968;&#25454;&#25552;&#31034;&#30446;&#26631;&#27169;&#22411;&#65292;&#36825;&#31181;&#26041;&#27861;&#26356;&#26377;&#25928;&#65292;&#33021;&#26356;&#22909;&#22320;&#37327;&#21270;LLMs&#30340;&#35760;&#24518;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.04801</link><description>&lt;p&gt;
Alpaca&#23545;&#25239;Vicuna&#65306;&#20351;&#29992;LLMs&#25581;&#31034;LLMs&#30340;&#35760;&#24518;&#21270;
&lt;/p&gt;
&lt;p&gt;
Alpaca against Vicuna: Using LLMs to Uncover Memorization of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04801
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;LLM&#20195;&#29702;&#36827;&#34892;&#40657;&#30418;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#21463;&#23475;&#20195;&#29702;&#20013;&#26356;&#39640;&#32423;&#21035;&#30340;&#35760;&#24518;&#21270;&#65292;&#30456;&#27604;&#30452;&#25509;&#29992;&#35757;&#32451;&#25968;&#25454;&#25552;&#31034;&#30446;&#26631;&#27169;&#22411;&#65292;&#36825;&#31181;&#26041;&#27861;&#26356;&#26377;&#25928;&#65292;&#33021;&#26356;&#22909;&#22320;&#37327;&#21270;LLMs&#30340;&#35760;&#24518;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#40657;&#30418;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#25915;&#20987;&#32773;LLM&#20195;&#29702;&#26469;&#25581;&#31034;&#21463;&#23475;&#20195;&#29702;&#20013;&#26356;&#39640;&#32423;&#21035;&#30340;&#35760;&#24518;&#21270;&#65292;&#19982;&#30452;&#25509;&#29992;&#35757;&#32451;&#25968;&#25454;&#25552;&#31034;&#30446;&#26631;&#27169;&#22411;&#30456;&#27604;&#65292;&#36825;&#26159;&#37327;&#21270;LLMs&#35760;&#24518;&#21270;&#30340;&#20027;&#23548;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#36845;&#20195;&#30340;&#25298;&#32477;&#25277;&#26679;&#20248;&#21270;&#36807;&#31243;&#26469;&#25214;&#21040;&#22522;&#20110;&#25351;&#20196;&#30340;&#25552;&#31034;&#65292;&#20855;&#26377;&#20004;&#20010;&#20027;&#35201;&#29305;&#24449;&#65306;(1)&#19982;&#35757;&#32451;&#25968;&#25454;&#26368;&#23567;&#37325;&#21472;&#65292;&#20197;&#36991;&#20813;&#30452;&#25509;&#21521;&#27169;&#22411;&#21576;&#29616;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#21450;(2)&#21463;&#23475;&#27169;&#22411;&#36755;&#20986;&#19982;&#35757;&#32451;&#25968;&#25454;&#30340;&#26368;&#22823;&#37325;&#21472;&#65292;&#26088;&#22312;&#35825;&#20351;&#21463;&#23475;&#32773;&#21520;&#20986;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#25105;&#20204;&#22522;&#20110;&#25351;&#20196;&#30340;&#25552;&#31034;&#29983;&#25104;&#30340;&#36755;&#20986;&#19982;&#35757;&#32451;&#25968;&#25454;&#37325;&#21472;&#31243;&#24230;&#27604;&#22522;&#32447;&#21069;&#32512;&#21518;&#32512;&#27979;&#37327;&#39640;&#20986;23.7&#65285;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;(1)&#32463;&#36807;&#25351;&#20196;&#35843;&#25972;&#30340;&#27169;&#22411;&#21487;&#20197;&#26292;&#38706;&#19982;&#20182;&#20204;&#30340;&#22522;&#26412;&#27169;&#22411;&#19968;&#26679;&#22810;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04801v1 Announce Type: new  Abstract: In this paper, we introduce a black-box prompt optimization method that uses an attacker LLM agent to uncover higher levels of memorization in a victim agent, compared to what is revealed by prompting the target model with the training data directly, which is the dominant approach of quantifying memorization in LLMs. We use an iterative rejection-sampling optimization process to find instruction-based prompts with two main characteristics: (1) minimal overlap with the training data to avoid presenting the solution directly to the model, and (2) maximal overlap between the victim model's output and the training data, aiming to induce the victim to spit out training data. We observe that our instruction-based prompts generate outputs with 23.7% higher overlap with training data compared to the baseline prefix-suffix measurements. Our findings show that (1) instruction-tuned models can expose pre-training data as much as their base-models, 
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;ParallelPARC&#27969;&#27700;&#32447;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22797;&#26434;&#27573;&#33853;&#31867;&#27604;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#21508;&#31181;&#31867;&#27604;&#31867;&#22411;&#65292;&#24182;&#23637;&#31034;&#20986;&#20154;&#31867;&#22312;&#31867;&#27604;&#35782;&#21035;&#20013;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.01139</link><description>&lt;p&gt;
ParallelPARC: &#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#31867;&#27604;&#30340;&#21487;&#25193;&#23637;&#27969;&#27700;&#32447;
&lt;/p&gt;
&lt;p&gt;
ParallelPARC: A Scalable Pipeline for Generating Natural-Language Analogies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01139
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;ParallelPARC&#27969;&#27700;&#32447;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22797;&#26434;&#27573;&#33853;&#31867;&#27604;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#21508;&#31181;&#31867;&#27604;&#31867;&#22411;&#65292;&#24182;&#23637;&#31034;&#20986;&#20154;&#31867;&#22312;&#31867;&#27604;&#35782;&#21035;&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Analogy-making&#23545;&#20110;&#20154;&#31867;&#35748;&#30693;&#33267;&#20851;&#37325;&#35201;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#36866;&#24212;&#26032;&#39062;&#24773;&#22659;--&#36825;&#26159;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20173;&#28982;&#32570;&#20047;&#30340;&#33021;&#21147;&#12290;&#22823;&#22810;&#25968;&#31867;&#27604;&#25968;&#25454;&#38598;&#20170;&#22825;&#20851;&#27880;&#31616;&#21333;&#30340;&#31867;&#27604;&#65288;&#20363;&#22914;&#65292;&#35789;&#31867;&#27604;&#65289;&#65307;&#21253;&#21547;&#22797;&#26434;&#31867;&#22411;&#31867;&#27604;&#30340;&#25968;&#25454;&#38598;&#36890;&#24120;&#26159;&#25163;&#24037;&#31574;&#21010;&#30340;&#65292;&#24182;&#19988;&#38750;&#24120;&#23567;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#38480;&#21046;&#20102;&#35745;&#31639;&#31867;&#27604;&#30340;&#36827;&#23637;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#25968;&#25454;&#29983;&#25104;&#27969;&#27700;&#32447;&#65292;ParallelPARC&#65288;Parallel Paragraph Creator&#65289;&#65292;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#21019;&#24314;&#22522;&#20110;&#27573;&#33853;&#30340;&#22797;&#26434;&#31867;&#27604;&#65292;&#20197;&#21450;&#31616;&#21333;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24178;&#25200;&#39033;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27969;&#27700;&#32447;&#65292;&#24182;&#21019;&#24314;&#20102;ProPara-Logy&#65292;&#19968;&#20010;&#20851;&#20110;&#31185;&#23398;&#36807;&#31243;&#38388;&#31867;&#27604;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#30001;&#20154;&#31867;&#39564;&#35777;&#36807;&#30340;&#37329;&#26631;&#20934;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#20010;&#33258;&#21160;&#29983;&#25104;&#30340;&#38134;&#26631;&#20934;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#22312;&#20108;&#36827;&#21046;&#21644;&#22810;&#36873;&#29615;&#22659;&#20013;&#27979;&#35797;&#20102;LLMs&#21644;&#20154;&#31867;&#23545;&#31867;&#27604;&#30340;&#35782;&#21035;&#65292;&#21457;&#29616;&#20154;&#31867;&#32988;&#36807;&#26368;&#20339;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01139v1 Announce Type: cross  Abstract: Analogy-making is central to human cognition, allowing us to adapt to novel situations -- an ability that current AI systems still lack. Most analogy datasets today focus on simple analogies (e.g., word analogies); datasets including complex types of analogies are typically manually curated and very small. We believe that this holds back progress in computational analogy. In this work, we design a data generation pipeline, ParallelPARC (Parallel Paragraph Creator) leveraging state-of-the-art Large Language Models (LLMs) to create complex, paragraph-based analogies, as well as distractors, both simple and challenging. We demonstrate our pipeline and create ProPara-Logy, a dataset of analogies between scientific processes. We publish a gold-set, validated by humans, and a silver-set, generated automatically. We test LLMs' and humans' analogy recognition in binary and multiple-choice settings, and found that humans outperform the best mod
&lt;/p&gt;</description></item><item><title>&#37117;&#24066;GPT&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#20855;&#26377;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#30340;&#26102;&#31354;&#27169;&#22411;&#65292;&#20511;&#37492;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25104;&#23601;&#12290;</title><link>https://arxiv.org/abs/2403.00813</link><description>&lt;p&gt;
UrbanGPT: &#26102;&#31354;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
UrbanGPT: Spatio-Temporal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00813
&lt;/p&gt;
&lt;p&gt;
&#37117;&#24066;GPT&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#20855;&#26377;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#30340;&#26102;&#31354;&#27169;&#22411;&#65292;&#20511;&#37492;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25104;&#23601;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37117;&#24066;GPT&#26088;&#22312;&#39044;&#27979;&#24182;&#27934;&#23519;&#22478;&#24066;&#29615;&#22659;&#22312;&#26102;&#38388;&#21644;&#31354;&#38388;&#19978;&#19981;&#26029;&#21464;&#21270;&#30340;&#21160;&#24577;&#12290;&#20854;&#30446;&#30340;&#26159;&#39044;&#27979;&#37117;&#24066;&#29983;&#27963;&#21508;&#20010;&#26041;&#38754;&#30340;&#26410;&#26469;&#27169;&#24335;&#12289;&#36235;&#21183;&#21644;&#20107;&#20214;&#65292;&#21253;&#25324;&#20132;&#36890;&#12289;&#20154;&#21475;&#27969;&#21160;&#21644;&#29359;&#32618;&#29575;&#31561;&#12290;&#23613;&#31649;&#24050;&#32463;&#20184;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#24320;&#21457;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#20197;&#20934;&#30830;&#39044;&#27979;&#26102;&#31354;&#25968;&#25454;&#65292;&#20294;&#38656;&#27880;&#24847;&#21040;&#24456;&#22810;&#26041;&#27861;&#22312;&#29983;&#25104;&#31934;&#30830;&#30340;&#26102;&#31354;&#34920;&#31034;&#26102;&#20005;&#37325;&#20381;&#36182;&#20110;&#26377;&#36275;&#22815;&#26631;&#35760;&#30340;&#25968;&#25454;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22312;&#23454;&#38469;&#37117;&#24066;&#24863;&#30693;&#22330;&#26223;&#20013;&#65292;&#25968;&#25454;&#31232;&#32570;&#26159;&#19968;&#20010;&#26222;&#36941;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#24314;&#31435;&#19968;&#20010;&#20855;&#26377;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#30340;&#26102;&#31354;&#27169;&#22411;&#36328;&#36234;&#22810;&#26679;&#26102;&#31354;&#23398;&#20064;&#22330;&#26223;&#26159;&#24517;&#35201;&#30340;&#12290;&#21463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#21331;&#36234;&#25104;&#23601;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00813v1 Announce Type: cross  Abstract: Spatio-temporal prediction aims to forecast and gain insights into the ever-changing dynamics of urban environments across both time and space. Its purpose is to anticipate future patterns, trends, and events in diverse facets of urban life, including transportation, population movement, and crime rates. Although numerous efforts have been dedicated to developing neural network techniques for accurate predictions on spatio-temporal data, it is important to note that many of these methods heavily depend on having sufficient labeled data to generate precise spatio-temporal representations. Unfortunately, the issue of data scarcity is pervasive in practical urban sensing scenarios. Consequently, it becomes necessary to build a spatio-temporal model with strong generalization capabilities across diverse spatio-temporal learning scenarios. Taking inspiration from the remarkable achievements of large language models (LLMs), our objective is 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#20445;&#30041;&#21407;&#38382;&#39064;&#32467;&#26500;&#38590;&#24230;&#20294;&#38024;&#23545;LLMs&#26080;&#35299;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#26377;&#25928;&#22320;&#38477;&#20302;&#20102;LLMs&#30340;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.17916</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#25915;&#20987;&#23454;&#29616;&#25239;LLM&#30340;&#25968;&#23398;&#38382;&#39064;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
LLM-Resistant Math Word Problem Generation via Adversarial Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#20445;&#30041;&#21407;&#38382;&#39064;&#32467;&#26500;&#38590;&#24230;&#20294;&#38024;&#23545;LLMs&#26080;&#35299;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#26377;&#25928;&#22320;&#38477;&#20302;&#20102;LLMs&#30340;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26174;&#33879;&#25913;&#21464;&#20102;&#25945;&#32946;&#39046;&#22495;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#20363;&#65292;&#29983;&#25104;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#20197;&#30830;&#20445;&#20844;&#24179;&#35780;&#20272;&#65292;&#36825;&#20123;&#31034;&#20363;&#20445;&#30041;&#20102;&#21407;&#22987;&#38382;&#39064;&#30340;&#32467;&#26500;&#21644;&#38590;&#24230;&#65292;&#20294;LLMs&#26080;&#27861;&#35299;&#20915;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#25968;&#23398;&#24212;&#29992;&#39046;&#22495;&#30340;&#35789;&#38382;&#39064;&#65292;&#21033;&#29992;&#25277;&#35937;&#35821;&#27861;&#26641;&#32467;&#26500;&#29983;&#25104;&#23545;&#25239;&#31034;&#20363;&#65292;&#36890;&#36807;&#31616;&#21333;&#32534;&#36753;&#38382;&#39064;&#20013;&#30340;&#25968;&#23383;&#20540;&#65292;&#23548;&#33268;LLMs&#20135;&#29983;&#38169;&#35823;&#31572;&#26696;&#12290;&#25105;&#20204;&#23545;&#21508;&#31181;&#24320;&#28304;&#21644;&#38381;&#28304;LLMs&#36827;&#34892;&#23454;&#39564;&#65292;&#23450;&#37327;&#21644;&#23450;&#24615;&#22320;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#38477;&#20302;&#20102;&#23427;&#20204;&#30340;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17916v1 Announce Type: cross  Abstract: Large language models (LLMs) have significantly transformed the educational landscape. As current plagiarism detection tools struggle to keep pace with LLMs' rapid advancements, the educational community faces the challenge of assessing students' true problem-solving abilities in the presence of LLMs. In this work, we explore a new paradigm for ensuring fair evaluation -- generating adversarial examples which preserve the structure and difficulty of the original questions aimed for assessment, but are unsolvable by LLMs. Focusing on the domain of math word problems, we leverage abstract syntax trees to structurally generate adversarial examples that cause LLMs to produce incorrect answers by simply editing the numeric values in the problems. We conduct experiments on various open- and closed-source LLMs, quantitatively and qualitatively demonstrating that our method significantly degrades their math problem-solving ability. We identify
&lt;/p&gt;</description></item><item><title>StructLM&#31995;&#21015;&#27169;&#22411;&#22522;&#20110;&#20840;&#38754;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#36229;&#36234;&#20102;&#22823;&#22810;&#25968;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#65292;&#22312;&#32467;&#26500;&#21270;&#30693;&#35782;&#36830;&#25509;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#25104;&#23601;&#12290;</title><link>https://arxiv.org/abs/2402.16671</link><description>&lt;p&gt;
StructLM: &#26397;&#21521;&#26500;&#24314;&#32467;&#26500;&#21270;&#30693;&#35782;&#36830;&#25509;&#30340;&#36890;&#29992;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
StructLM: Towards Building Generalist Models for Structured Knowledge Grounding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16671
&lt;/p&gt;
&lt;p&gt;
StructLM&#31995;&#21015;&#27169;&#22411;&#22522;&#20110;&#20840;&#38754;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#36229;&#36234;&#20102;&#22823;&#22810;&#25968;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#65292;&#22312;&#32467;&#26500;&#21270;&#30693;&#35782;&#36830;&#25509;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#25104;&#23601;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#25968;&#25454;&#28304;&#65292;&#22914;&#34920;&#26684;&#12289;&#22270;&#24418;&#21644;&#25968;&#25454;&#24211;&#65292;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#30693;&#35782;&#28304;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#32431;&#25991;&#26412;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#22312;&#35299;&#37322;&#21644;&#21033;&#29992;&#32467;&#26500;&#21270;&#25968;&#25454;&#26041;&#38754;&#30340;&#33021;&#21147;&#20173;&#28982;&#26377;&#38480;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;LLM&#22312;&#22788;&#29702;&#32467;&#26500;&#21270;&#25968;&#25454;&#26041;&#38754;&#30340;&#26174;&#30528;&#19981;&#36275;&#65292;&#20363;&#22914;&#65292;ChatGPT&#24179;&#22343;&#33853;&#21518;&#20110;&#26368;&#20808;&#36827;&#27169;&#22411;(SoTA)35%&#12290;&#20026;&#22686;&#24378;LLM&#20013;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#36830;&#25509;&#65288;SKG&#65289;&#33021;&#21147;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;110&#19975;&#20010;&#31034;&#20363;&#30340;&#20840;&#38754;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#12290;&#21033;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#31995;&#21015;&#22522;&#20110;Code-LLaMA&#26550;&#26500;&#30340;&#27169;&#22411;&#65292;&#31216;&#20026;StructLM&#65292;&#21442;&#25968;&#33539;&#22260;&#20174;7B&#21040;34B&#12290;&#25105;&#20204;&#30340;StructLM&#31995;&#21015;&#22312;18&#20010;&#35780;&#20272;&#25968;&#25454;&#38598;&#20013;&#26377;14&#20010;&#36229;&#36234;&#20102;&#29305;&#23450;&#20219;&#21153;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;7&#20010;SKG&#20219;&#21153;&#19978;&#30830;&#31435;&#20102;&#26032;&#30340;SoTA&#25104;&#23601;&#12290;&#27492;&#22806;&#65292;StructLM&#23637;&#29616;&#20102;&#21331;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16671v1 Announce Type: new  Abstract: Structured data sources, such as tables, graphs, and databases, are ubiquitous knowledge sources. Despite the demonstrated capabilities of large language models (LLMs) on plain text, their proficiency in interpreting and utilizing structured data remains limited. Our investigation reveals a notable deficiency in LLMs' ability to process structured data, e.g., ChatGPT lags behind state-of-the-art (SoTA) model by an average of 35%. To augment the Structured Knowledge Grounding (SKG) capabilities in LLMs, we have developed a comprehensive instruction tuning dataset comprising 1.1 million examples. Utilizing this dataset, we train a series of models, referred to as StructLM, based on the Code-LLaMA architecture, ranging from 7B to 34B parameters. Our StructLM series surpasses task-specific models on 14 out of 18 evaluated datasets and establishes new SoTA achievements on 7 SKG tasks. Furthermore, StructLM demonstrates exceptional generalizat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#38463;&#25289;&#20271;&#23383;&#31526;&#30340;EEG&#25968;&#25454;&#38598;ArEEG_Chars&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#23454;&#29616;97%&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;&#33041;&#26426;&#25509;&#21475;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.15733</link><description>&lt;p&gt;
ArEEG_Chars: &#29992;&#20110;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#35774;&#24819;&#35821;&#38899;&#35782;&#21035;&#30340;&#38463;&#25289;&#20271;&#23383;&#31526;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ArEEG_Chars: Dataset for Envisioned Speech Recognition using EEG for Arabic Characters
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15733
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#38463;&#25289;&#20271;&#23383;&#31526;&#30340;EEG&#25968;&#25454;&#38598;ArEEG_Chars&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#23454;&#29616;97%&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;&#33041;&#26426;&#25509;&#21475;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#26159;&#36817;&#24180;&#26469;&#28909;&#38376;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#21487;&#20197;&#24110;&#21161;&#30251;&#30186;&#24739;&#32773;&#25913;&#21892;&#29983;&#27963;&#12290;&#26377;&#20960;&#39033;&#30740;&#31350;&#33258;&#21160;&#23558;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20449;&#21495;&#20998;&#31867;&#20026;&#33521;&#25991;&#23383;&#31526;&#21644;&#21333;&#35789;&#12290;&#38463;&#25289;&#20271;&#35821;&#26159;&#19990;&#30028;&#19978;&#20351;&#29992;&#26368;&#24191;&#27867;&#30340;&#35821;&#35328;&#20043;&#19968;&#12290;&#28982;&#32780;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#30446;&#21069;&#27809;&#26377;&#38024;&#23545;&#38463;&#25289;&#20271;&#23383;&#31526;&#30340;&#33041;&#30005;&#22270;&#20449;&#21495;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;&#38463;&#25289;&#20271;&#23383;&#31526;&#30340;EEG&#25968;&#25454;&#38598;&#65292;&#24182;&#21629;&#21517;&#20026;ArEEG_Chars&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#23545;ArEEG_Chars&#36827;&#34892;&#20102;&#22810;&#39033;&#23454;&#39564;&#12290;&#22312;&#20351;&#29992;LSTM&#26102;&#33719;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;97%&#12290;ArEEG_Chars&#25968;&#25454;&#38598;&#23558;&#23545;&#30740;&#31350;&#20154;&#21592;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15733v1 Announce Type: cross  Abstract: Brain-Computer-Interface (BCI) has been a hot research topic in the last few years that could help paralyzed people in their lives. Several researches were done to classify electroencephalography (EEG) signals automatically into English characters and words. Arabic language is one of the most used languages around the world. However, to the best of our knowledge, there is no dataset for Arabic characters EEG signals. In this paper, we have created an EEG dataset for Arabic characters and named it ArEEG_Chars. Moreover, several experiments were done on ArEEG_Chars using deep learning. Best results were achieved using LSTM and reached an accuracy of 97%. ArEEG_Chars dataset will be public for researchers.
&lt;/p&gt;</description></item><item><title>Triad&#26694;&#26550;&#21033;&#29992;&#20102;&#22522;&#20110;&#22810;&#35282;&#33394;LLM&#20195;&#29702;&#26469;&#35299;&#20915;&#30693;&#35782;&#24211;&#38382;&#31572;&#38382;&#39064;&#65292;&#36890;&#36807;&#20195;&#29702;&#30340;&#19981;&#21516;&#35282;&#33394;&#20998;&#21035;&#22788;&#29702;KBQA&#23376;&#20219;&#21153;&#65292;&#21512;&#20316;&#23436;&#25104;KBQA&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.14320</link><description>&lt;p&gt;
Triad: &#19968;&#20010;&#21033;&#29992;&#22522;&#20110;&#22810;&#35282;&#33394;LLM&#20195;&#29702;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#30693;&#35782;&#24211;&#38382;&#31572;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14320
&lt;/p&gt;
&lt;p&gt;
Triad&#26694;&#26550;&#21033;&#29992;&#20102;&#22522;&#20110;&#22810;&#35282;&#33394;LLM&#20195;&#29702;&#26469;&#35299;&#20915;&#30693;&#35782;&#24211;&#38382;&#31572;&#38382;&#39064;&#65292;&#36890;&#36807;&#20195;&#29702;&#30340;&#19981;&#21516;&#35282;&#33394;&#20998;&#21035;&#22788;&#29702;KBQA&#23376;&#20219;&#21153;&#65292;&#21512;&#20316;&#23436;&#25104;KBQA&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22522;&#20110;LLM&#20195;&#29702;&#30340;&#36827;&#23637;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#22238;&#31572;&#30693;&#35782;&#24211;&#20013;&#38382;&#39064;&#30340;&#36816;&#29992;&#20173;&#28982;&#40092;&#20026;&#20154;&#30693;&#12290;&#20351;&#29992;&#20256;&#32479;&#26041;&#27861;&#26469;&#23454;&#29616;KBQA&#31995;&#32479;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#32570;&#20047;&#29305;&#23450;&#20219;&#21153;&#35757;&#32451;&#25968;&#25454;&#20197;&#21450;&#21019;&#24314;&#20197;&#20219;&#21153;&#20026;&#20013;&#24515;&#30340;&#27169;&#22411;&#32467;&#26500;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Triad&#65292;&#19968;&#20010;&#21033;&#29992;&#20855;&#26377;&#19977;&#20010;&#35282;&#33394;&#30340;LLM&#20195;&#29702;&#30340;&#32479;&#19968;&#26694;&#26550;&#26469;&#36827;&#34892;KBQA&#20219;&#21153;&#12290;&#20195;&#29702;&#34987;&#20998;&#37197;&#19977;&#20010;&#35282;&#33394;&#26469;&#22788;&#29702;&#19981;&#21516;&#30340;KBQA&#23376;&#20219;&#21153;&#65306;&#20316;&#20026;&#25484;&#25569;&#21508;&#31181;&#23376;&#20219;&#21153;&#30340;&#36890;&#25165;&#65292;&#20316;&#20026;&#36873;&#25321;&#20505;&#36873;&#32773;&#30340;&#20915;&#31574;&#32773;&#65292;&#20197;&#21450;&#20316;&#20026;&#22238;&#31572;&#24102;&#26377;&#30693;&#35782;&#30340;&#38382;&#39064;&#30340;&#39038;&#38382;&#12290;&#25105;&#20204;&#30340;KBQA&#26694;&#26550;&#22312;&#22235;&#20010;&#38454;&#27573;&#20013;&#25191;&#34892;&#65292;&#28041;&#21450;&#20195;&#29702;&#30340;&#22810;&#37325;&#35282;&#33394;&#30340;&#21327;&#20316;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#26694;&#26550;&#32988;&#36807;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14320v1 Announce Type: cross  Abstract: Recent progress with LLM-based agents has shown promising results across various tasks. However, their use in answering questions from knowledge bases remains largely unexplored. Implementing a KBQA system using traditional methods is challenging due to the shortage of task-specific training data and the complexity of creating task-focused model structures. In this paper, we present Triad, a unified framework that utilizes an LLM-based agent with three roles for KBQA tasks. The agent is assigned three roles to tackle different KBQA subtasks: agent as a generalist for mastering various subtasks, as a decision maker for the selection of candidates, and as an advisor for answering questions with knowledge. Our KBQA framework is executed in four phases, involving the collaboration of the agent's multiple roles. We evaluated the performance of our framework using three benchmark datasets, and the results show that our framework outperforms 
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20027;&#39064;&#23545;&#35805;&#25688;&#35201;&#35780;&#20272;&#22522;&#20934;TofuEval&#65292;&#30740;&#31350;&#21457;&#29616;&#29616;&#26377;&#30340;LLMs&#22312;&#23545;&#35805;&#39046;&#22495;&#23384;&#22312;&#22823;&#37327;&#20107;&#23454;&#38169;&#35823;&#30340;&#24187;&#35273;&#65292;&#24182;&#34920;&#26126;&#24403;LLMs&#20805;&#24403;&#20107;&#23454;&#35780;&#20272;&#22120;&#26102;&#65292;&#20854;&#34920;&#29616;&#19981;&#20339;&#12290;</title><link>https://arxiv.org/abs/2402.13249</link><description>&lt;p&gt;
TofuEval&#65306;&#35780;&#20272;LLM&#22312;&#20027;&#39064;&#23545;&#35805;&#25688;&#35201;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13249
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20027;&#39064;&#23545;&#35805;&#25688;&#35201;&#35780;&#20272;&#22522;&#20934;TofuEval&#65292;&#30740;&#31350;&#21457;&#29616;&#29616;&#26377;&#30340;LLMs&#22312;&#23545;&#35805;&#39046;&#22495;&#23384;&#22312;&#22823;&#37327;&#20107;&#23454;&#38169;&#35823;&#30340;&#24187;&#35273;&#65292;&#24182;&#34920;&#26126;&#24403;LLMs&#20805;&#24403;&#20107;&#23454;&#35780;&#20272;&#22120;&#26102;&#65292;&#20854;&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#25991;&#26723;&#26032;&#38395;&#25688;&#35201;&#22312;&#24544;&#23454;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#38271;&#36275;&#36827;&#27493;&#65292;&#36825;&#24471;&#30410;&#20110;&#23545;&#20107;&#23454;&#19968;&#33268;&#24615;&#25110;&#24187;&#35273;&#35780;&#20272;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#36825;&#20123;&#36827;&#23637;&#26159;&#21542;&#33021;&#24310;&#20280;&#21040;&#20854;&#20182;&#25991;&#26412;&#25688;&#35201;&#39046;&#22495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20027;&#39064;&#23545;&#35805;&#25688;&#35201;&#35780;&#20272;&#22522;&#20934;&#65292;&#30001;&#19981;&#21516;&#35268;&#27169;&#30340;LLMs&#29983;&#25104;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#36825;&#20123;&#25688;&#35201;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#30340;&#20108;&#20803;&#21477;&#32423;&#20154;&#31867;&#27880;&#37322;&#65292;&#20197;&#21450;&#23545;&#20107;&#23454;&#19981;&#19968;&#33268;&#21477;&#23376;&#30340;&#35814;&#32454;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;LLMs&#22312;&#23545;&#35805;&#39046;&#22495;&#23384;&#22312;&#22823;&#37327;&#20107;&#23454;&#38169;&#35823;&#30340;&#24187;&#35273;&#65292;&#26080;&#35770;&#27169;&#22411;&#22823;&#23567;&#22914;&#20309;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#24403;LLMs&#65288;&#21253;&#25324;GPT-4&#65289;&#20805;&#24403;&#20108;&#20803;&#20107;&#23454;&#35780;&#20272;&#22120;&#26102;&#65292;&#23427;&#20204;&#34920;&#29616;&#19981;&#20339;&#65292;&#19988;&#21487;&#20197;&#34987;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#19987;&#38376;&#20107;&#23454;&#35780;&#20272;&#24230;&#37327;&#25152;&#36229;&#36234;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#24187;&#35273;&#31867;&#22411;&#36827;&#34892;&#20102;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13249v1 Announce Type: cross  Abstract: Single document news summarization has seen substantial progress on faithfulness in recent years, driven by research on the evaluation of factual consistency, or hallucinations. We ask whether these advances carry over to other text summarization domains. We propose a new evaluation benchmark on topic-focused dialogue summarization, generated by LLMs of varying sizes. We provide binary sentence-level human annotations of the factual consistency of these summaries along with detailed explanations of factually inconsistent sentences. Our analysis shows that existing LLMs hallucinate significant amounts of factual errors in the dialogue domain, regardless of the model's size. On the other hand, when LLMs, including GPT-4, serve as binary factual evaluators, they perform poorly and can be outperformed by prevailing state-of-the-art specialized factuality evaluation metrics. Finally, we conducted an analysis of hallucination types with a cu
&lt;/p&gt;</description></item><item><title>&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#35299;&#37322;&#25552;&#20986;&#20102;&#20840;&#38754;&#12289;&#22810;&#35282;&#24230;&#30340;&#35780;&#20272;&#65292;&#21253;&#25324;&#24544;&#23454;&#24230;&#12289;&#24378;&#20581;&#24615;&#21644;&#25928;&#29992;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#21487;&#35299;&#37322;&#24615;&#25351;&#26631;&#12290;</title><link>https://arxiv.org/abs/2402.11863</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#35299;&#37322;&#30340;&#21487;&#35299;&#37322;&#24615;&#26377;&#22810;&#39640;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Interpretable are Reasoning Explanations from Prompting Large Language Models?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11863
&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#35299;&#37322;&#25552;&#20986;&#20102;&#20840;&#38754;&#12289;&#22810;&#35282;&#24230;&#30340;&#35780;&#20272;&#65292;&#21253;&#25324;&#24544;&#23454;&#24230;&#12289;&#24378;&#20581;&#24615;&#21644;&#25928;&#29992;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#21487;&#35299;&#37322;&#24615;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt Engineering&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#26497;&#22823;&#20851;&#27880;&#65292;&#21487;&#20197;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#39033;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;Chain-of-Thought&#31561;&#25216;&#26415;&#19981;&#20165;&#22686;&#24378;&#20102;&#20219;&#21153;&#24615;&#33021;&#65292;&#36824;&#25551;&#32472;&#20102;&#28165;&#26224;&#30340;&#25512;&#29702;&#27493;&#39588;&#36712;&#36857;&#65292;&#20026;&#35266;&#20247;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#24418;&#30340;&#35299;&#37322;&#24418;&#24335;&#12290;&#25105;&#20204;&#23545;&#21487;&#35299;&#37322;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#22810;&#35282;&#24230;&#30340;&#35780;&#20272;&#65292;&#19981;&#20165;&#32771;&#34385;&#20102;&#24544;&#23454;&#24230;&#65292;&#36824;&#32771;&#34385;&#20102;&#22312;&#22810;&#20010;&#24120;&#35782;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24378;&#20581;&#24615;&#21644;&#25928;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#21487;&#35299;&#37322;&#24615;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11863v1 Announce Type: new  Abstract: Prompt Engineering has garnered significant attention for enhancing the performance of large language models across a multitude of tasks. Techniques such as the Chain-of-Thought not only bolster task performance but also delineate a clear trajectory of reasoning steps, offering a tangible form of explanation for the audience. Prior works on interpretability assess the reasoning chains yielded by Chain-of-Thought solely along a singular axis, namely faithfulness. We present a comprehensive and multifaceted evaluation of interpretability, examining not only faithfulness but also robustness and utility across multiple commonsense reasoning benchmarks. Likewise, our investigation is not confined to a single prompting technique; it expansively covers a multitude of prevalent prompting techniques employed in large language models, thereby ensuring a wide-ranging and exhaustive evaluation. In addition, we introduce a simple interpretability ali
&lt;/p&gt;</description></item><item><title>&#21512;&#25104;&#20449;&#24687;&#26356;&#21487;&#33021;&#34987;&#22823;&#22411;&#27169;&#22411;&#32435;&#20837;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#20256;&#25773;&#20013;&#65292;&#22823;&#22411;&#27169;&#22411;&#22312;&#20256;&#36882;&#20449;&#24687;&#26102;&#20542;&#21521;&#20110;&#26377;&#36873;&#25321;&#22320;&#20462;&#25913;&#21644;&#20002;&#22833;&#29305;&#23450;&#20869;&#23481;</title><link>https://arxiv.org/abs/2402.11271</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22312;&#36890;&#20449;&#26102;&#20195;&#30340;&#20114;&#21160;&#65306;&#33258;&#22124;&#20351;&#24471;&#22823;&#22411;&#27169;&#22411;&#23454;&#29616;&#23616;&#37096;&#26368;&#20248;
&lt;/p&gt;
&lt;p&gt;
Human-AI Interactions in the Communication Era: Autophagy Makes Large Models Achieving Local Optima
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11271
&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#20449;&#24687;&#26356;&#21487;&#33021;&#34987;&#22823;&#22411;&#27169;&#22411;&#32435;&#20837;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#20256;&#25773;&#20013;&#65292;&#22823;&#22411;&#27169;&#22411;&#22312;&#20256;&#36882;&#20449;&#24687;&#26102;&#20542;&#21521;&#20110;&#26377;&#36873;&#25321;&#22320;&#20462;&#25913;&#21644;&#20002;&#22833;&#29305;&#23450;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#31038;&#20250;&#20449;&#24687;&#22788;&#29702;&#20013;&#30340;&#37325;&#35201;&#24615;&#26085;&#30410;&#22686;&#21152;&#65292;&#24341;&#21457;&#20102;&#20851;&#20110;&#31038;&#20250;&#23433;&#20840;&#21644;&#20262;&#29702;&#30340;&#20105;&#35770;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#20174;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30456;&#20114;&#20316;&#29992;&#30340;&#32508;&#21512;&#35270;&#35282;&#20998;&#26512;&#36825;&#20123;&#38480;&#21046;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20154;&#31867;&#21644;&#22823;&#22411;&#27169;&#22411;&#22312;&#36890;&#20449;&#20013;&#20316;&#20026;&#20851;&#38190;&#32852;&#31995;&#30340;&#20559;&#35265;&#21644;&#20559;&#22909;&#12290;&#20026;&#23454;&#29616;&#27492;&#30446;&#30340;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#21644;&#19977;&#20010;&#19981;&#21516;&#30340;&#23454;&#39564;&#65292;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#22312;&#20854;&#20316;&#20026;&#20449;&#24687;&#29983;&#20135;&#32773;&#21644;&#20256;&#25773;&#32773;&#30340;&#35282;&#33394;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#31361;&#20986;&#26174;&#31034;&#65292;&#21512;&#25104;&#20449;&#24687;&#26356;&#26377;&#21487;&#33021;&#34987;&#32435;&#20837;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#28040;&#24687;&#20256;&#36882;&#20013;&#65292;&#32780;&#20154;&#31867;&#29983;&#25104;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#22823;&#22411;&#27169;&#22411;&#22312;&#20197;&#20449;&#24687;&#20256;&#36882;&#32773;&#30340;&#35282;&#33394;&#26102;&#65292;&#20542;&#21521;&#20110;&#26377;&#36873;&#25321;&#22320;&#20462;&#25913;&#21644;&#20002;&#22833;&#29305;&#23450;&#20869;&#23481;&#12290;&#22312;&#27010;&#24565;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#30495;&#23454;&#30340;&#33258;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11271v1 Announce Type: new  Abstract: The increasing significance of large language and multimodal models in societal information processing has ignited debates on social safety and ethics. However, few studies have approached the analysis of these limitations from the comprehensive perspective of human and artificial intelligence system interactions. This study investigates biases and preferences when humans and large models are used as key links in communication. To achieve this, we design a multimodal dataset and three different experiments to evaluate generative models in their roles as producers and disseminators of information. Our main findings highlight that synthesized information is more likely to be incorporated into model training datasets and messaging than human-generated information. Additionally, large models, when acting as transmitters of information, tend to modify and lose specific content selectively. Conceptually, we present two realistic models of auto
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RS-DPO&#30340;&#26041;&#27861;&#65292;&#23427;&#23558;&#25298;&#32477;&#37319;&#26679;&#21644;&#30452;&#25509;&#20248;&#21270;&#20559;&#22909;&#32467;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#32463;&#36807;&#30417;&#30563;&#24494;&#35843;&#30340;&#31574;&#30053;&#27169;&#22411;&#65292;&#24182;&#20174;&#35813;&#27169;&#22411;&#20013;&#30452;&#25509;&#37319;&#26679;&#21709;&#24212;&#65292;RS-DPO&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#22522;&#20110;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#30340;&#19981;&#31283;&#23450;&#24615;&#21644;&#39640;&#35745;&#31639;&#25104;&#26412;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#35782;&#21035;&#23545;&#27604;&#26679;&#26412;&#23545;&#65292;RS-DPO&#33021;&#22815;&#26356;&#22909;&#22320;&#36827;&#34892;RLHF&#12290;</title><link>https://arxiv.org/abs/2402.10038</link><description>&lt;p&gt;
RS-DPO&#65306;&#19968;&#31181;&#29992;&#20110;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28151;&#21512;&#25298;&#32477;&#37319;&#26679;&#21644;&#30452;&#25509;&#20248;&#21270;&#20559;&#22909;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization Method for Alignment of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RS-DPO&#30340;&#26041;&#27861;&#65292;&#23427;&#23558;&#25298;&#32477;&#37319;&#26679;&#21644;&#30452;&#25509;&#20248;&#21270;&#20559;&#22909;&#32467;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#32463;&#36807;&#30417;&#30563;&#24494;&#35843;&#30340;&#31574;&#30053;&#27169;&#22411;&#65292;&#24182;&#20174;&#35813;&#27169;&#22411;&#20013;&#30452;&#25509;&#37319;&#26679;&#21709;&#24212;&#65292;RS-DPO&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#22522;&#20110;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#30340;&#19981;&#31283;&#23450;&#24615;&#21644;&#39640;&#35745;&#31639;&#25104;&#26412;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#35782;&#21035;&#23545;&#27604;&#26679;&#26412;&#23545;&#65292;RS-DPO&#33021;&#22815;&#26356;&#22909;&#22320;&#36827;&#34892;RLHF&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#65288;RLHF&#65289;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#29992;&#25143;&#24847;&#22270;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#30340;RLHF&#26377;&#26102;&#19981;&#31283;&#23450;&#65292;&#38656;&#35201;&#26174;&#33879;&#30340;&#36229;&#21442;&#25968;&#24494;&#35843;&#65292;&#24182;&#19988;&#22312;&#23545;&#40784;&#36807;&#31243;&#20013;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#30452;&#25509;&#20248;&#21270;&#20559;&#22909;&#65288;DPO&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;DPO&#20381;&#36182;&#20110;&#20174;&#20154;&#31867;&#26631;&#27880;&#32773;&#21644;&#26367;&#20195;LLM&#29983;&#25104;&#30340;&#23545;&#27604;&#22238;&#22797;&#65292;&#32780;&#19981;&#26159;&#31574;&#30053;&#27169;&#22411;&#65292;&#38480;&#21046;&#20102;RLHF&#30340;&#25928;&#26524;&#12290;&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#22320;&#32467;&#21512;&#25298;&#32477;&#37319;&#26679;&#65288;RS&#65289;&#21644;DPO&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;RS-DPO&#65292;&#39318;&#20808;&#24320;&#21457;&#20986;&#19968;&#20010;&#32463;&#36807;&#30417;&#30563;&#24494;&#35843;&#30340;&#31574;&#30053;&#27169;&#22411;&#65288;SFT&#65289;&#12290;&#28982;&#21518;&#30452;&#25509;&#20174;SFT&#27169;&#22411;&#20013;&#37319;&#26679;&#27599;&#20010;&#25552;&#31034;&#30340;k&#20010;&#21709;&#24212;&#12290;RS-DPO&#22522;&#20110;&#20854;&#30456;&#20284;&#24230;&#35782;&#21035;&#23545;&#27604;&#26679;&#26412;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10038v1 Announce Type: cross  Abstract: Reinforcement learning from human feedback (RLHF) has been extensively employed to align large language models with user intent. However, proximal policy optimization (PPO) based RLHF is occasionally unstable requiring significant hyperparameter finetuning, and computationally expensive to maximize the estimated reward during alignment. Recently, direct preference optimization (DPO) is proposed to address those challenges. However, DPO relies on contrastive responses generated from human annotator and alternative LLM, instead of the policy model, limiting the effectiveness of the RLHF. In this paper, we addresses both challenges by systematically combining rejection sampling (RS) and DPO. Our proposed method, RS-DPO, initiates with the development of a supervised fine-tuned policy model (SFT). A varied set of k responses per prompt are sampled directly from the SFT model. RS-DPO identifies pairs of contrastive samples based on their re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;RiVEG&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#36830;&#25509;&#26725;&#26753;&#65292;&#23558;&#22810;&#27169;&#24577;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#37325;&#26032;&#26500;&#24314;&#20026;&#32852;&#21512;&#20219;&#21153;&#65292;&#35299;&#20915;&#20102;&#21629;&#21517;&#23454;&#20307;&#26080;&#27861;&#30830;&#23450;&#21644;&#25351;&#20195;&#34920;&#36798;&#19982;&#21629;&#21517;&#23454;&#20307;&#20043;&#38388;&#30340;&#21306;&#21035;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.09989</link><description>&lt;p&gt;
LLMs&#20316;&#20026;&#26725;&#26753;&#65306;&#37325;&#26032;&#26500;&#24314;&#22522;&#20110;&#22810;&#27169;&#24577;&#22270;&#20687;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
LLMs as Bridges: Reformulating Grounded Multimodal Named Entity Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;RiVEG&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#36830;&#25509;&#26725;&#26753;&#65292;&#23558;&#22810;&#27169;&#24577;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#37325;&#26032;&#26500;&#24314;&#20026;&#32852;&#21512;&#20219;&#21153;&#65292;&#35299;&#20915;&#20102;&#21629;&#21517;&#23454;&#20307;&#26080;&#27861;&#30830;&#23450;&#21644;&#25351;&#20195;&#34920;&#36798;&#19982;&#21629;&#21517;&#23454;&#20307;&#20043;&#38388;&#30340;&#21306;&#21035;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Grounded Multimodal Named Entity Recognition (GMNER) &#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#26088;&#22312;&#35782;&#21035;&#21629;&#21517;&#23454;&#20307;&#12289;&#23454;&#20307;&#31867;&#22411;&#21450;&#20854;&#23545;&#24212;&#30340;&#35270;&#35273;&#21306;&#22495;&#12290;GMNER&#20219;&#21153;&#20855;&#26377;&#20004;&#20010;&#25361;&#25112;&#24615;&#36136;&#65306;1&#65289;&#31038;&#20132;&#23186;&#20307;&#20013;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#24369;&#30456;&#20851;&#24615;&#23548;&#33268;&#22823;&#37096;&#20998;&#21629;&#21517;&#23454;&#20307;&#38590;&#20197;&#30830;&#23450;&#65307;2&#65289;&#24120;&#29992;&#20110;&#31867;&#20284;&#20219;&#21153;&#30340;&#31895;&#31890;&#24230;&#25351;&#20195;&#34920;&#36798;&#19982;&#32454;&#31890;&#24230;&#21629;&#21517;&#23454;&#20307;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#21306;&#21035;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;RiVEG&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#36830;&#25509;&#26725;&#26753;&#65292;&#23558;GMNER&#37325;&#26032;&#26500;&#24314;&#20026;&#32852;&#21512;MNER-VE-VG&#20219;&#21153;&#12290;&#36825;&#31181;&#37325;&#26032;&#26500;&#24314;&#24102;&#26469;&#20102;&#20004;&#20010;&#22909;&#22788;&#65306;1&#65289;&#20445;&#25345;&#20102;&#26368;&#20339;&#30340;MNER&#24615;&#33021;&#65292;&#28040;&#38500;&#20102;&#20351;&#29992;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#39044;&#25552;&#21462;&#21306;&#22495;&#29305;&#24449;&#30340;&#38656;&#27714;&#65292;&#33258;&#28982;&#35299;&#20915;&#20102;&#36825;&#20004;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09989v1 Announce Type: cross  Abstract: Grounded Multimodal Named Entity Recognition (GMNER) is a nascent multimodal task that aims to identify named entities, entity types and their corresponding visual regions. GMNER task exhibits two challenging properties: 1) The weak correlation between image-text pairs in social media results in a significant portion of named entities being ungroundable. 2) There exists a distinction between coarse-grained referring expressions commonly used in similar tasks (e.g., phrase localization, referring expression comprehension) and fine-grained named entities. In this paper, we propose RiVEG, a unified framework that reformulates GMNER into a joint MNER-VE-VG task by leveraging large language models (LLMs) as a connecting bridge. This reformulation brings two benefits: 1) It maintains the optimal MNER performance and eliminates the need for employing object detection methods to pre-extract regional features, thereby naturally addressing two m
&lt;/p&gt;</description></item><item><title>&#38271;&#25991;&#26412;&#35780;&#20272;&#27169;&#22411;&#32534;&#36753;&#65288;LEME&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#21327;&#35758;&#65292;&#29992;&#20110;&#34913;&#37327;&#27169;&#22411;&#32534;&#36753;&#22312;&#38271;&#31687;&#29983;&#25104;&#35774;&#32622;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#24433;&#21709;&#12290;&#36825;&#20010;&#21327;&#35758;&#19982;&#20808;&#21069;&#30340;&#30701;&#25991;&#26412;&#25351;&#26631;&#20960;&#20046;&#27809;&#26377;&#20851;&#31995;&#65292;&#24341;&#20837;&#20102;&#19968;&#32452;&#26032;&#30340;&#32500;&#24230;&#26469;&#29702;&#35299;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.09394</link><description>&lt;p&gt;
&#38271;&#25991;&#26412;&#35780;&#20272;&#27169;&#22411;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Long-form evaluation of model editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09394
&lt;/p&gt;
&lt;p&gt;
&#38271;&#25991;&#26412;&#35780;&#20272;&#27169;&#22411;&#32534;&#36753;&#65288;LEME&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#21327;&#35758;&#65292;&#29992;&#20110;&#34913;&#37327;&#27169;&#22411;&#32534;&#36753;&#22312;&#38271;&#31687;&#29983;&#25104;&#35774;&#32622;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#24433;&#21709;&#12290;&#36825;&#20010;&#21327;&#35758;&#19982;&#20808;&#21069;&#30340;&#30701;&#25991;&#26412;&#25351;&#26631;&#20960;&#20046;&#27809;&#26377;&#20851;&#31995;&#65292;&#24341;&#20837;&#20102;&#19968;&#32452;&#26032;&#30340;&#32500;&#24230;&#26469;&#29702;&#35299;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#23545;&#20110;&#27169;&#22411;&#32534;&#36753;&#30340;&#35780;&#20272;&#21482;&#20351;&#29992;&#20102;&#25552;&#31034;&#21518;&#30340;&#8220;&#19979;&#20960;&#20010;&#26631;&#35760;&#8221;&#30340;&#23436;&#25104;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#20110;&#26356;&#38271;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#24433;&#21709;&#22823;&#37096;&#20998;&#26159;&#26410;&#30693;&#30340;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#38271;&#25991;&#26412;&#35780;&#20272;&#27169;&#22411;&#32534;&#36753;&#65288;LEME&#65289;&#30340;&#26032;&#39062;&#35780;&#20272;&#21327;&#35758;&#65292;&#35813;&#21327;&#35758;&#22312;&#38271;&#31687;&#29983;&#25104;&#35774;&#32622;&#20013;&#34913;&#37327;&#27169;&#22411;&#32534;&#36753;&#30340;&#21151;&#25928;&#21644;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#21327;&#35758;&#21253;&#25324;&#26426;&#22120;&#35780;&#23450;&#30340;&#35843;&#26597;&#21644;&#19982;&#20154;&#31867;&#35780;&#20998;&#30456;&#20851;&#24615;&#24456;&#22909;&#30340;&#20998;&#31867;&#22120;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#35780;&#20272;&#21327;&#35758;&#19982;&#20808;&#21069;&#30340;&#30701;&#25991;&#26412;&#25351;&#26631;&#20960;&#20046;&#27809;&#26377;&#20851;&#31995;&#65288;&#23613;&#31649;&#35774;&#35745;&#20026;&#25193;&#23637;&#21151;&#25928;&#12289;&#27867;&#21270;&#24615;&#12289;&#23616;&#37096;&#24615;&#21644;&#21487;&#31227;&#26893;&#24615;&#21040;&#38271;&#25991;&#26412;&#29615;&#22659;&#20013;&#65289;&#65292;&#36825;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#32452;&#26032;&#30340;&#32500;&#24230;&#26469;&#29702;&#35299;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#12290;&#20351;&#29992;&#36825;&#20010;&#21327;&#35758;&#65292;&#25105;&#20204;&#23545;&#19968;&#20123;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#20010;&#21457;&#29616;&#65292;&#21253;&#25324;&#19968;&#20123;&#26041;&#27861;&#65288;R
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09394v1 Announce Type: new Abstract: Evaluations of model editing currently only use the `next few token' completions after a prompt. As a result, the impact of these methods on longer natural language generation is largely unknown. We introduce long-form evaluation of model editing (\textbf{\textit{LEME}}) a novel evaluation protocol that measures the efficacy and impact of model editing in long-form generative settings. Our protocol consists of a machine-rated survey and a classifier which correlates well with human ratings. Importantly, we find that our protocol has very little relationship with previous short-form metrics (despite being designed to extend efficacy, generalization, locality, and portability into a long-form setting), indicating that our method introduces a novel set of dimensions for understanding model editing methods. Using this protocol, we benchmark a number of model editing techniques and present several findings including that, while some methods (R
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;LlaSMol&#65292;&#23427;&#26159;&#19968;&#31181;&#25512;&#36827;&#21270;&#23398;&#39046;&#22495;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#22823;&#35268;&#27169;&#12289;&#20840;&#38754;&#12289;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;LlaSMol&#22312;&#21270;&#23398;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;GPT-4&#24182;&#25509;&#36817;&#20110;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.09391</link><description>&lt;p&gt;
LlaSMol:&#21033;&#29992;&#22823;&#35268;&#27169;&#12289;&#20840;&#38754;&#12289;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#25512;&#36827;&#21270;&#23398;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;LlaSMol&#65292;&#23427;&#26159;&#19968;&#31181;&#25512;&#36827;&#21270;&#23398;&#39046;&#22495;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#22823;&#35268;&#27169;&#12289;&#20840;&#38754;&#12289;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;LlaSMol&#22312;&#21270;&#23398;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;GPT-4&#24182;&#25509;&#36817;&#20110;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21270;&#23398;&#22312;&#33647;&#29289;&#30740;&#21457;&#21644;&#26448;&#26009;&#31185;&#23398;&#31561;&#35768;&#22810;&#39046;&#22495;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#23613;&#31649;&#35832;&#22914;GPT-4&#20043;&#31867;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#65292;&#20294;&#29616;&#26377;&#24037;&#20316;&#34920;&#26126;&#23427;&#20204;&#22312;&#21270;&#23398;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#20196;&#20154;&#22833;&#26395;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#24320;&#21457;&#30340;LLM&#22312;&#19968;&#31995;&#21015;&#21270;&#23398;&#20219;&#21153;&#19978;&#21487;&#20197;&#21462;&#24471;&#38750;&#24120;&#24378;&#22823;&#30340;&#32467;&#26524;&#65292;&#22312;&#25152;&#26377;&#20219;&#21153;&#19978;&#37117;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;GPT-4&#65292;&#24182;&#25509;&#36817;SoTA&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#12290;&#25105;&#20204;&#21462;&#24471;&#25104;&#21151;&#30340;&#20851;&#38190;&#26159;&#19968;&#20010;&#21517;&#20026;SMolInstruct&#30340;&#22823;&#35268;&#27169;&#12289;&#20840;&#38754;&#12289;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#12290;&#23427;&#21253;&#21547;&#20102;14&#20010;&#32463;&#36807;&#31934;&#24515;&#25361;&#36873;&#30340;&#21270;&#23398;&#20219;&#21153;&#21644;&#36229;&#36807;&#19977;&#30334;&#19975;&#20010;&#39640;&#36136;&#37327;&#26679;&#26412;&#65292;&#20026;&#35757;&#32451;&#21644;&#35780;&#20272;&#21270;&#23398;LLM&#22880;&#23450;&#20102;&#22362;&#23454;&#22522;&#30784;&#12290;&#22522;&#20110;SMolInstruct&#65292;&#25105;&#20204;&#23545;&#19968;&#32452;&#24320;&#28304;LLM&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#20854;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;Mistral ser&#26159;&#26368;&#20339;&#24615;&#33021;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09391v1 Announce Type: new Abstract: Chemistry plays a crucial role in many domains, such as drug discovery and material science. While large language models (LLMs) such as GPT-4 exhibit remarkable capabilities on natural language processing tasks, existing work shows their performance on chemistry tasks is discouragingly low. In this paper, however, we demonstrate that our developed LLMs can achieve very strong results on a comprehensive set of chemistry tasks, outperforming the most advanced GPT-4 across all the tasks by a substantial margin and approaching the SoTA task-specific models. The key to our success is a large-scale, comprehensive, high-quality dataset for instruction tuning named SMolInstruct. It contains 14 meticulously selected chemistry tasks and over three million high-quality samples, laying a solid foundation for training and evaluating LLMs for chemistry. Based on SMolInstruct, we fine-tune a set of open-source LLMs, among which, we find that Mistral ser
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#35777;&#25454;&#21644;&#39118;&#26684;&#30340;&#21453;&#39539;&#65292;&#35813;&#25968;&#25454;&#38598;&#22522;&#20110;Reddit ChangeMyView&#25968;&#25454;&#38598;&#20013;&#30340;&#24086;&#23376;&#65292;&#24182;&#21487;&#29992;&#20110;&#35770;&#35777;&#30340;&#25913;&#36827;&#21644;&#35780;&#20272;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-3.5 turbo&#27169;&#22411;&#22312;&#35770;&#35777;&#36136;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#20855;&#26377;&#24456;&#39640;&#30340;&#39118;&#26684;&#34701;&#21512;&#33021;&#21147;&#12290;&#20114;&#24800;&#24335;&#21453;&#39539;&#30340;&#25928;&#26524;&#26368;&#20339;&#12290;</title><link>https://arxiv.org/abs/2402.08498</link><description>&lt;p&gt;
&#23457;&#35745;&#21453;&#28779;&#65306;&#35780;&#20272;&#20855;&#26377;&#35777;&#25454;&#21644;&#39118;&#26684;&#30340;&#20808;&#36827;&#21453;&#39539;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Auditing Counterfire: Evaluating Advanced Counterargument Generation with Evidence and Style
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08498
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#35777;&#25454;&#21644;&#39118;&#26684;&#30340;&#21453;&#39539;&#65292;&#35813;&#25968;&#25454;&#38598;&#22522;&#20110;Reddit ChangeMyView&#25968;&#25454;&#38598;&#20013;&#30340;&#24086;&#23376;&#65292;&#24182;&#21487;&#29992;&#20110;&#35770;&#35777;&#30340;&#25913;&#36827;&#21644;&#35780;&#20272;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-3.5 turbo&#27169;&#22411;&#22312;&#35770;&#35777;&#36136;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#20855;&#26377;&#24456;&#39640;&#30340;&#39118;&#26684;&#34701;&#21512;&#33021;&#21147;&#12290;&#20114;&#24800;&#24335;&#21453;&#39539;&#30340;&#25928;&#26524;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25511;&#21046;&#24615;&#21453;&#39539;&#30340;&#21512;&#25104;&#65292;&#26088;&#22312;&#36827;&#19968;&#27493;&#24212;&#29992;&#20110;&#35770;&#35777;&#30340;&#25913;&#36827;&#12289;&#25366;&#25496;&#21644;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#19982;Reddit ChangeMyView&#25968;&#25454;&#38598;&#20013;&#30340;&#24086;&#23376;&#30456;&#32467;&#21512;&#30340;&#20016;&#23500;&#30340;&#21453;&#39539;&#65292;&#36825;&#20123;&#21453;&#39539;&#34701;&#20837;&#20102;&#20174;&#39640;&#36136;&#37327;&#26469;&#28304;&#20013;&#26816;&#32034;&#21040;&#30340;&#35777;&#25454;&#65292;&#24182;&#26681;&#25454;&#29992;&#25143;&#20559;&#22909;&#29983;&#25104;&#65292;&#35843;&#25972;&#20102;&#35777;&#25454;&#21644;&#35770;&#35777;&#39118;&#26684;&#30340;&#20851;&#38190;&#23646;&#24615;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;Counterfire&#35821;&#26009;&#24211;&#21253;&#25324;&#20174;GPT-3.5 turbo&#12289;Koala&#21644;PaLM 2&#27169;&#22411;&#20197;&#21450;&#23427;&#20204;&#30340;&#20004;&#20010;&#24494;&#35843;&#21464;&#20307;&#29983;&#25104;&#30340;&#35770;&#35777;&#65288;N = 32,000&#65289;&#12290;&#27169;&#22411;&#35780;&#20272;&#34920;&#26126;&#65292;&#22312;&#35777;&#25454;&#26041;&#38754;&#20855;&#26377;&#24378;&#22823;&#30340;&#25913;&#20889;&#33021;&#21147;&#65292;&#23613;&#31649;&#35789;&#27719;&#37325;&#21472;&#26377;&#38480;&#65292;&#21516;&#26102;&#34920;&#29616;&#20986;&#39640;&#24230;&#30340;&#39118;&#26684;&#34701;&#21512;&#65288;&#23545;&#20110;&#8220;&#20114;&#24800;&#8221;&#30340;&#24471;&#20998;&#20026;0.9682&#65289;&#65292;&#26174;&#31034;&#20102;LLM&#34701;&#21512;&#22810;&#26679;&#39118;&#26684;&#30340;&#33021;&#21147;&#12290;&#22312;&#25152;&#26377;&#27169;&#22411;&#20013;&#65292;GPT-3.5 turbo&#22312;&#35770;&#35777;&#36136;&#37327;&#35780;&#20272;&#20013;&#26174;&#31034;&#20986;&#26368;&#39640;&#20998;&#25968;&#65292;&#34920;&#29616;&#20986;&#19968;&#33268;&#20934;&#30830;&#24615;&#65288;&#24471;&#20998; &gt;0.8&#65289;&#12290;&#22312;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#20013;&#65292;&#20114;&#24800;&#24335;&#21453;&#39539;&#35777;&#26126;&#25928;&#26524;&#26368;&#20339;&#65292;&#33021;&#22815;&#20135;&#29983;&#26356;&#22909;&#30340;&#35770;&#35777;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel dataset for the controlled composition of counterarguments designed for further applications in argument refining, mining, and evaluation. Our dataset constitutes enriched counter-arguments to posts in the Reddit ChangeMyView dataset that are integrated with evidence retrieved from high-quality sources and generated based on user preferences, adjusting the critical attributes of evidence and argument style. The resultant Counterfire corpus comprises arguments generated from GPT-3.5 turbo, Koala, and PaLM 2 models and two of their finetuned variants (N = 32,000). Model evaluation indicates strong paraphrasing abilities with evidence, albeit limited word overlap, while demonstrating high style integration (0.9682 for 'reciprocity'), showing the ability of LLM to assimilate diverse styles. Of all models, GPT-3.5 turbo showed the highest scores in argument quality evaluation, showing consistent accuracy (score &gt;0.8). In further analyses, reciprocity-style counterargument
&lt;/p&gt;</description></item><item><title>X-LoRA&#26159;&#19968;&#31181;&#28789;&#27963;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#21033;&#29992;&#20302;&#31209;&#36866;&#37197;&#22120;&#19987;&#23478;&#30340;&#28151;&#21512;&#31574;&#30053;&#65292;&#21487;&#20197;&#21019;&#24314;&#31934;&#32454;&#35843;&#25972;&#30340;&#27169;&#22411;&#24182;&#22312;&#34507;&#30333;&#36136;&#21147;&#23398;&#21644;&#35774;&#35745;&#39046;&#22495;&#24212;&#29992;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#28145;&#23618;&#36880;&#23618;&#36866;&#24212;&#30340;&#32452;&#21512;&#26469;&#35299;&#20915;&#29305;&#23450;&#20219;&#21153;&#65292;&#24182;&#21463;&#21040;&#29983;&#29289;&#23398;&#21407;&#29702;&#30340;&#21551;&#21457;&#12290;&#26080;&#38656;&#20462;&#25913;&#24213;&#23618;&#32467;&#26500;&#21363;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.07148</link><description>&lt;p&gt;
X-LoRA: &#19968;&#31181;&#28789;&#27963;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#21033;&#29992;&#20302;&#31209;&#36866;&#37197;&#22120;&#19987;&#23478;&#30340;&#28151;&#21512;&#31574;&#30053;&#22312;&#34507;&#30333;&#36136;&#21147;&#23398;&#21644;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
X-LoRA: Mixture of Low-Rank Adapter Experts, a Flexible Framework for Large Language Models with Applications in Protein Mechanics and Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07148
&lt;/p&gt;
&lt;p&gt;
X-LoRA&#26159;&#19968;&#31181;&#28789;&#27963;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#21033;&#29992;&#20302;&#31209;&#36866;&#37197;&#22120;&#19987;&#23478;&#30340;&#28151;&#21512;&#31574;&#30053;&#65292;&#21487;&#20197;&#21019;&#24314;&#31934;&#32454;&#35843;&#25972;&#30340;&#27169;&#22411;&#24182;&#22312;&#34507;&#30333;&#36136;&#21147;&#23398;&#21644;&#35774;&#35745;&#39046;&#22495;&#24212;&#29992;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#28145;&#23618;&#36880;&#23618;&#36866;&#24212;&#30340;&#32452;&#21512;&#26469;&#35299;&#20915;&#29305;&#23450;&#20219;&#21153;&#65292;&#24182;&#21463;&#21040;&#29983;&#29289;&#23398;&#21407;&#29702;&#30340;&#21551;&#21457;&#12290;&#26080;&#38656;&#20462;&#25913;&#24213;&#23618;&#32467;&#26500;&#21363;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25253;&#36947;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#23618;&#36880;&#23618;&#22522;&#20110;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#30340;&#26032;&#39062;&#39044;&#35757;&#32451;&#36866;&#37197;&#22120;&#30340;&#28151;&#21512;&#19987;&#23478;&#31574;&#30053;&#65292;&#29992;&#20110;&#21019;&#24314;&#31934;&#32454;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38544;&#34255;&#29366;&#24577;&#21160;&#24577;&#28151;&#21512;&#32463;&#36807;&#36866;&#24212;&#30340;&#23618;&#30340;&#38376;&#25511;&#31574;&#30053;&#65292;&#20801;&#35768;&#24471;&#21040;&#30340;X-LoRA&#27169;&#22411;&#21033;&#29992;&#19981;&#21516;&#30340;&#33021;&#21147;&#24182;&#21019;&#24314;&#20197;&#21069;&#26410;&#20351;&#29992;&#30340;&#28145;&#23618;&#36880;&#23618;&#36866;&#24212;&#30340;&#32452;&#21512;&#26469;&#35299;&#20915;&#29305;&#23450;&#20219;&#21153;&#12290;&#35813;&#35774;&#35745;&#21463;&#21040;&#20102;&#29983;&#29289;&#26222;&#36941;&#24615;&#21644;&#22810;&#26679;&#24615;&#30340;&#29983;&#29289;&#23398;&#21407;&#29702;&#30340;&#21551;&#21457;&#65292;&#20854;&#20013;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#22359;&#22312;&#19981;&#21516;&#30340;&#20998;&#23618;&#34920;&#31034;&#20013;&#34987;&#37325;&#22797;&#20351;&#29992;&#12290;&#22240;&#27492;&#65292;X-LoRA&#27169;&#22411;&#21487;&#20197;&#36731;&#26494;&#29992;&#20110;&#20219;&#20309;&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#26080;&#38656;&#20462;&#25913;&#24213;&#23618;&#32467;&#26500;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;X-LoRA&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#21253;&#25324;&#21069;&#21521;/&#36870;&#21521;&#20998;&#26512;&#20219;&#21153;&#21644;&#22686;&#24378;&#25512;&#29702;&#33021;&#21147;&#22312;&#20869;&#30340;&#31185;&#23398;&#33021;&#21147;&#65292;&#37325;&#28857;&#26159;&#29983;&#29289;&#26448;&#26009;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We report a mixture of expert strategy to create fine-tuned large language models using a deep layer-wise token-level approach based on low-rank adaptation (LoRA). Starting with a set of pre-trained LoRA adapters, we propose a gating strategy that uses the hidden states to dynamically mix adapted layers, allowing the resulting X-LoRA model to draw upon different capabilities and create never-before-used deep layer-wise combinations of adaptations are established to solve specific tasks. The design is inspired by the biological principles of universality and diversity, where neural network building blocks are reused in different hierarchical manifestations. Hence, the X-LoRA model can be easily implemented for any existing large language model (LLM) without a need for modifications of the underlying structure. We develop a tailored X-LoRA model that offers scientific capabilities including forward/inverse analysis tasks and enhanced reasoning capability, focused on biomaterial analysis,
&lt;/p&gt;</description></item><item><title>TinyLLM&#26159;&#19968;&#31181;&#20174;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#23567;&#22411;&#23398;&#29983;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#33539;&#24335;&#65292;&#26088;&#22312;&#35299;&#20915;&#30693;&#35782;&#22810;&#26679;&#24615;&#26377;&#38480;&#21644;&#32570;&#20047;&#19978;&#19979;&#25991;&#20449;&#24687;&#31561;&#38382;&#39064;&#65292;&#24182;&#40723;&#21169;&#23398;&#29983;&#27169;&#22411;&#29702;&#35299;&#31572;&#26696;&#32972;&#21518;&#30340;&#21407;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.04616</link><description>&lt;p&gt;
TinyLLM: &#20174;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#19968;&#20010;&#23567;&#22411;&#23398;&#29983;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TinyLLM: Learning a Small Student from Multiple Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04616
&lt;/p&gt;
&lt;p&gt;
TinyLLM&#26159;&#19968;&#31181;&#20174;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#23567;&#22411;&#23398;&#29983;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#33539;&#24335;&#65292;&#26088;&#22312;&#35299;&#20915;&#30693;&#35782;&#22810;&#26679;&#24615;&#26377;&#38480;&#21644;&#32570;&#20047;&#19978;&#19979;&#25991;&#20449;&#24687;&#31561;&#38382;&#39064;&#65292;&#24182;&#40723;&#21169;&#23398;&#29983;&#27169;&#22411;&#29702;&#35299;&#31572;&#26696;&#32972;&#21518;&#30340;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#26356;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#36716;&#31227;&#21040;&#36739;&#23567;&#30340;&#27169;&#22411;&#19978;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;&#36739;&#23567;&#30340;LLMs&#26356;&#28789;&#27963;&#65292;&#25104;&#26412;&#26356;&#20302;&#12290;&#22312;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#20013;&#65292;&#30693;&#35782;&#33976;&#39311;&#22240;&#20854;&#20986;&#33394;&#30340;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#32780;&#33073;&#39062;&#32780;&#20986;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#32570;&#28857;&#65292;&#21253;&#25324;&#30693;&#35782;&#22810;&#26679;&#24615;&#26377;&#38480;&#21644;&#32570;&#20047;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#24182;&#20419;&#36827;&#32039;&#20945;&#35821;&#35328;&#27169;&#22411;&#30340;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TinyLLM&#65292;&#19968;&#31181;&#20174;&#22810;&#20010;&#22823;&#22411;&#25945;&#24072;LLMs&#20013;&#23398;&#20064;&#23567;&#22411;&#23398;&#29983;LLM&#30340;&#26032;&#22411;&#30693;&#35782;&#33976;&#39311;&#33539;&#24335;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#40723;&#21169;&#23398;&#29983;LLM&#19981;&#20165;&#29983;&#25104;&#27491;&#30830;&#31572;&#26696;&#65292;&#32780;&#19988;&#29702;&#35299;&#36825;&#20123;&#31572;&#26696;&#32972;&#21518;&#30340;&#21407;&#29702;&#12290;&#37492;&#20110;&#19981;&#21516;&#30340;LLMs&#20855;&#26377;&#19981;&#21516;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#23548;&#23398;&#29983;&#27169;&#22411;&#21560;&#25910;&#26469;&#33258;&#22810;&#20010;&#25945;&#24072;LLMs&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#31034;&#20363;&#29983;&#25104;&#22120;&#21644;&#19968;&#20010;&#32769;&#24072;&#24378;&#21046;&#27169;&#22359;...
&lt;/p&gt;
&lt;p&gt;
Transferring the reasoning capability from stronger large language models (LLMs) to smaller ones has been quite appealing, as smaller LLMs are more flexible to deploy with less expense. Among the existing solutions, knowledge distillation stands out due to its outstanding efficiency and generalization. However, existing methods suffer from several drawbacks, including limited knowledge diversity and the lack of rich contextual information. To solve the problems and facilitate the learning of compact language models, we propose TinyLLM, a novel knowledge distillation paradigm to learn a small student LLM from multiple large teacher LLMs. In particular, we encourage the student LLM to not only generate the correct answers but also understand the rationales behind these answers. Given that different LLMs possess diverse reasoning skills, we guide the student model to assimilate knowledge from various teacher LLMs. We further introduce an in-context example generator and a teacher-forcing 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32593;&#39029;&#25130;&#22270;&#20013;&#20803;&#32032;&#30340;&#8220;&#21452;&#35270;&#22270;&#8221;&#26469;&#36827;&#34892; HTML &#20803;&#32032;&#30340;&#32972;&#26223;&#21270;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#23558;&#27599;&#20010;&#20803;&#32032;&#19982;&#20854;&#37051;&#23621;&#20803;&#32032;&#36827;&#34892;&#32972;&#26223;&#21270;&#65292;&#20351;&#29992;&#25991;&#26412;&#21644;&#35270;&#35273;&#29305;&#24449;&#65292;&#20351;&#24471;HTML&#20803;&#32032;&#30340;&#32467;&#26524;&#34920;&#31034;&#23545;&#20110;&#20195;&#29702;&#25191;&#34892;&#25805;&#20316;&#26356;&#21152;&#20449;&#24687;&#20016;&#23500;&#12290;</title><link>https://arxiv.org/abs/2402.04476</link><description>&lt;p&gt;
&#21452;&#35270;&#22270;&#35270;&#35273;&#32972;&#26223;&#21270;&#30340;&#32593;&#39029;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Dual-View Visual Contextualization for Web Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32593;&#39029;&#25130;&#22270;&#20013;&#20803;&#32032;&#30340;&#8220;&#21452;&#35270;&#22270;&#8221;&#26469;&#36827;&#34892; HTML &#20803;&#32032;&#30340;&#32972;&#26223;&#21270;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#23558;&#27599;&#20010;&#20803;&#32032;&#19982;&#20854;&#37051;&#23621;&#20803;&#32032;&#36827;&#34892;&#32972;&#26223;&#21270;&#65292;&#20351;&#29992;&#25991;&#26412;&#21644;&#35270;&#35273;&#29305;&#24449;&#65292;&#20351;&#24471;HTML&#20803;&#32032;&#30340;&#32467;&#26524;&#34920;&#31034;&#23545;&#20110;&#20195;&#29702;&#25191;&#34892;&#25805;&#20316;&#26356;&#21152;&#20449;&#24687;&#20016;&#23500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#32593;&#39029;&#23548;&#33322;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#21487;&#20197;&#26681;&#25454;&#35821;&#35328;&#25351;&#20196;&#22312;&#23454;&#38469;&#32593;&#31449;&#19978;&#25191;&#34892;&#22797;&#26434;&#21644;&#22810;&#26679;&#20219;&#21153;&#30340;&#32593;&#32476;&#20195;&#29702;&#12290;&#29616;&#26377;&#24037;&#20316;&#20027;&#35201;&#26159;&#20197; HTML &#25991;&#26723;&#20316;&#20026;&#36755;&#20837;&#65292;HTML &#25991;&#26723;&#23450;&#20041;&#20102;&#32593;&#39029;&#30340;&#20869;&#23481;&#21644;&#25805;&#20316;&#31354;&#38388;&#65288;&#21363;&#21487;&#25805;&#20316;&#20803;&#32032;&#21644;&#25805;&#20316;&#65289;&#12290;&#28982;&#32780;&#65292;HTML &#25991;&#26723;&#21487;&#33021;&#26080;&#27861;&#20026;&#27599;&#20010;&#20803;&#32032;&#25552;&#20379;&#28165;&#26224;&#30340;&#20219;&#21153;&#30456;&#20851;&#32972;&#26223;&#65292;&#20351;&#24471;&#36873;&#25321;&#27491;&#30830;&#30340;&#65288;&#19968;&#31995;&#21015;&#30340;&#65289;&#25805;&#20316;&#21464;&#24471;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#32593;&#39029;&#25130;&#22270;&#20013;&#20803;&#32032;&#30340;&#8220;&#21452;&#35270;&#22270;&#8221;&#26469;&#36827;&#34892; HTML &#20803;&#32032;&#30340;&#32972;&#26223;&#21270;&#65306;&#27599;&#20010; HTML &#20803;&#32032;&#22312;&#25130;&#22270;&#20013;&#26377;&#20854;&#23545;&#24212;&#30340;&#36793;&#30028;&#26694;&#21644;&#35270;&#35273;&#20869;&#23481;&#12290;&#25105;&#20204;&#22522;&#20110;&#19968;&#20010;&#27934;&#23519;&#21147;&#8212;&#8212;&#32593;&#39029;&#24320;&#21457;&#32773;&#20542;&#21521;&#20110;&#22312;&#32593;&#39029;&#19978;&#23558;&#20219;&#21153;&#30456;&#20851;&#20803;&#32032;&#25918;&#32622;&#22312;&#38468;&#36817;&#20197;&#22686;&#24378;&#29992;&#25143;&#20307;&#39564;&#65292;&#24182;&#25552;&#20986;&#23558;&#27599;&#20010;&#20803;&#32032;&#19982;&#20854;&#37051;&#23621;&#20803;&#32032;&#36827;&#34892;&#32972;&#26223;&#21270;&#65292;&#20351;&#29992;&#25991;&#26412;&#21644;&#35270;&#35273;&#29305;&#24449;&#12290;HTML &#20803;&#32032;&#30340;&#32467;&#26524;&#34920;&#31034;&#23545;&#20110;&#20195;&#29702;&#25191;&#34892;&#25805;&#20316;&#26356;&#21152;&#20449;&#24687;&#20016;&#23500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic web navigation aims to build a web agent that can follow language instructions to execute complex and diverse tasks on real-world websites. Existing work primarily takes HTML documents as input, which define the contents and action spaces (i.e., actionable elements and operations) of webpages. Nevertheless, HTML documents may not provide a clear task-related context for each element, making it hard to select the right (sequence of) actions. In this paper, we propose to contextualize HTML elements through their "dual views" in webpage screenshots: each HTML element has its corresponding bounding box and visual content in the screenshot. We build upon the insight -- web developers tend to arrange task-related elements nearby on webpages to enhance user experiences -- and propose to contextualize each element with its neighbor elements, using both textual and visual features. The resulting representations of HTML elements are more informative for the agent to take action. We val
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20840;&#38754;&#35299;&#37322;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#27599;&#20010;&#28508;&#22312;&#22240;&#32032;&#12290;&#23545;&#20110;&#19981;&#21516;&#29983;&#25104;&#27169;&#22411;&#30340;&#35299;&#37322;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#35299;&#37322;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#27169;&#22411;&#36827;&#34892;&#20102;&#24615;&#33021;&#35780;&#20272;&#65292;&#19988;&#23450;&#24615;&#22320;&#23637;&#31034;&#20102;&#28508;&#22312;&#22240;&#32032;&#30340;&#21464;&#21270;&#25928;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#35299;&#37322;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01858</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#35299;&#37322;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Explaining latent representations of generative models with large multimodal models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01858
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20840;&#38754;&#35299;&#37322;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#27599;&#20010;&#28508;&#22312;&#22240;&#32032;&#12290;&#23545;&#20110;&#19981;&#21516;&#29983;&#25104;&#27169;&#22411;&#30340;&#35299;&#37322;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#35299;&#37322;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#27169;&#22411;&#36827;&#34892;&#20102;&#24615;&#33021;&#35780;&#20272;&#65292;&#19988;&#23450;&#24615;&#22320;&#23637;&#31034;&#20102;&#28508;&#22312;&#22240;&#32032;&#30340;&#21464;&#21270;&#25928;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#35299;&#37322;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#25968;&#25454;&#29983;&#25104;&#28508;&#22312;&#22240;&#32032;&#34920;&#31034;&#26159;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#30340;&#37325;&#35201;&#35838;&#39064;&#12290;&#38543;&#30528;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#20852;&#36215;&#65292;&#23427;&#21487;&#20197;&#23558;&#22270;&#20687;&#19982;&#25991;&#26412;&#23545;&#40784;&#20197;&#29983;&#25104;&#31572;&#26696;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20840;&#38754;&#35299;&#37322;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#27599;&#20010;&#28508;&#22312;&#22240;&#32032;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#37327;&#21270;&#35780;&#20272;&#20102;&#25105;&#20204;&#29983;&#25104;&#30340;&#35299;&#37322;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#22312;&#22810;&#20010;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20043;&#38388;&#35780;&#20272;&#20102;&#35299;&#37322;&#29983;&#25104;&#30340;&#24615;&#33021;&#65292;&#24182;&#23450;&#24615;&#22320;&#21487;&#35270;&#21270;&#20102;&#27599;&#20010;&#28508;&#22312;&#22240;&#32032;&#30340;&#21464;&#21270;&#65292;&#20197;&#23398;&#20064;&#19981;&#21516;&#29983;&#25104;&#27169;&#22411;&#23545;&#35299;&#37322;&#30340;&#35299;&#32544;&#25928;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#35299;&#37322;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning interpretable representations of data generative latent factors is an important topic for the development of artificial intelligence. With the rise of the large multimodal model, it can align images with text to generate answers. In this work, we propose a framework to comprehensively explain each latent factor in the generative models using a large multimodal model. We further measure the uncertainty of our generated explanations, quantitatively evaluate the performance of explanation generation among multiple large multimodal models, and qualitatively visualize the variations of each latent factor to learn the disentanglement effects of different generative models on explanations. Finally, we discuss the explanatory capabilities and limitations of state-of-the-art large multimodal models.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#20195;&#29702;&#35843;&#25972;&#30340;&#36731;&#37327;&#32423;&#35299;&#30721;&#26102;&#31639;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#23545;&#23567;&#22411;&#35843;&#25972;&#21518;&#30340;LM&#30340;&#39044;&#27979;&#19982;&#26410;&#35843;&#25972;LM&#30340;&#39044;&#27979;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#35843;&#25972;&#22823;&#22411;&#39044;&#35757;&#32451;LM&#30340;&#39044;&#27979;&#65292;&#20174;&#32780;&#23454;&#29616;&#36164;&#28304;&#33410;&#32422;&#21644;&#20445;&#30041;&#26356;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#22909;&#22788;&#12290;</title><link>https://arxiv.org/abs/2401.08565</link><description>&lt;p&gt;
&#36890;&#36807;&#20195;&#29702;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Tuning Language Models by Proxy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.08565
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#20195;&#29702;&#35843;&#25972;&#30340;&#36731;&#37327;&#32423;&#35299;&#30721;&#26102;&#31639;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#23545;&#23567;&#22411;&#35843;&#25972;&#21518;&#30340;LM&#30340;&#39044;&#27979;&#19982;&#26410;&#35843;&#25972;LM&#30340;&#39044;&#27979;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#35843;&#25972;&#22823;&#22411;&#39044;&#35757;&#32451;LM&#30340;&#39044;&#27979;&#65292;&#20174;&#32780;&#23454;&#29616;&#36164;&#28304;&#33410;&#32422;&#21644;&#20445;&#30041;&#26356;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#19968;&#33324;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22987;&#32456;&#21463;&#30410;&#20110;&#36827;&#19968;&#27493;&#35843;&#25972;&#20197;&#26356;&#22909;&#22320;&#23454;&#29616;&#25152;&#38656;&#30340;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#35843;&#25972;&#36825;&#20123;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#28040;&#32791;&#36164;&#28304;&#65292;&#25110;&#32773;&#22312;&#27169;&#22411;&#26435;&#37325;&#26159;&#31169;&#26377;&#30340;&#24773;&#20917;&#19979;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20195;&#29702;&#35843;&#25972;&#65292;&#36825;&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#35299;&#30721;&#26102;&#31639;&#27861;&#65292;&#23427;&#22312;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#36816;&#34892;&#65292;&#20197;&#23454;&#29616;&#19982;&#30452;&#25509;&#35843;&#25972;&#30456;&#21516;&#30340;&#30446;&#30340;&#65292;&#20294;&#21482;&#35775;&#38382;&#20854;&#22312;&#36755;&#20986;&#35789;&#27719;&#19978;&#30340;&#39044;&#27979;&#65292;&#32780;&#19981;&#26159;&#20854;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35843;&#25972;&#20102;&#19968;&#20010;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#28982;&#21518;&#23558;&#32463;&#36807;&#35843;&#25972;&#21644;&#26410;&#32463;&#35843;&#25972;&#30340;&#23567;&#27169;&#22411;&#30340;&#39044;&#27979;&#20043;&#38388;&#30340;&#24046;&#24322;&#24212;&#29992;&#20110;&#23558;&#26356;&#22823;&#30340;&#26410;&#35843;&#25972;&#27169;&#22411;&#30340;&#21407;&#22987;&#39044;&#27979;&#36716;&#31227;&#21040;&#35843;&#25972;&#26041;&#21521;&#65292;&#21516;&#26102;&#20445;&#30041;&#36739;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#22909;&#22788;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#24403;&#25105;&#20204;&#20351;&#29992;&#20165;&#20026;7B&#22823;&#23567;&#30340;&#20195;&#29702;&#23545;Llama2-70B&#24212;&#29992;&#20195;&#29702;&#35843;&#25972;&#26102;&#65292;&#25105;&#20204;&#21487;&#20197;&#20851;&#38381;88% Llama2-70B &#19982;&#20854;&#30495;&#27491;&#35843;&#25972;&#36807;&#30340;&#32842;&#22825;&#29256;&#26412;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.08565v2 Announce Type: replace  Abstract: Despite the general capabilities of large pretrained language models, they consistently benefit from further adaptation to better achieve desired behaviors. However, tuning these models has become increasingly resource-intensive, or impossible when model weights are private. We introduce proxy-tuning, a lightweight decoding-time algorithm that operates on top of black-box LMs to achieve the same end as direct tuning, but by accessing only its predictions over the output vocabulary, not its parameters. Our method tunes a smaller LM, then applies the difference between the predictions of the small tuned and untuned LMs to shift the original predictions of the larger untuned model in the direction of tuning, while retaining the benefits of larger-scale pretraining. In experiments, when we apply proxy-tuning to Llama2-70B using proxies of only 7B size, we can close 88% of the gap between Llama2-70B and its truly-tuned chat version, when 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#32467;&#26500;&#32422;&#26463;&#21644;&#30001;&#27492;&#34893;&#29983;&#30340;&#32452;&#21512;&#25512;&#29702;&#65292;&#21487;&#20197;&#36807;&#28388;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#30340;&#19981;&#19968;&#33268;&#32467;&#26500;&#65292;&#20174;&#32780;&#26500;&#24314;&#26377;&#25928;&#30340;&#32467;&#26500;&#21270;&#36755;&#20986;&#65292;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.06877</link><description>&lt;p&gt;
&#21450;&#26102;&#39044;&#27979;&#32467;&#26500;&#65306;&#25512;&#29702;&#30340;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Promptly Predicting Structures: The Return of Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.06877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#32467;&#26500;&#32422;&#26463;&#21644;&#30001;&#27492;&#34893;&#29983;&#30340;&#32452;&#21512;&#25512;&#29702;&#65292;&#21487;&#20197;&#36807;&#28388;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#30340;&#19981;&#19968;&#33268;&#32467;&#26500;&#65292;&#20174;&#32780;&#26500;&#24314;&#26377;&#25928;&#30340;&#32467;&#26500;&#21270;&#36755;&#20986;&#65292;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#34987;&#24191;&#27867;&#29992;&#20110;&#26500;&#24314;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#26631;&#31614;&#39044;&#27979;&#22120;&#12290;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20855;&#26377;&#32467;&#26500;&#29305;&#28857;&#65306;&#21363;&#23427;&#20204;&#30340;&#36755;&#20986;&#30001;&#22810;&#20010;&#30456;&#20114;&#32422;&#26463;&#30340;&#26631;&#31614;&#32452;&#25104;&#12290;&#20026;&#36825;&#31867;&#20219;&#21153;&#26631;&#27880;&#25968;&#25454;&#21487;&#33021;&#20250;&#24456;&#32321;&#29712;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22522;&#20110;&#25552;&#31034;&#30340;&#33539;&#24335;&#33021;&#21542;&#25193;&#23637;&#21040;&#36825;&#31181;&#32467;&#26500;&#21270;&#36755;&#20986;&#20219;&#21153;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26500;&#24314;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#35821;&#35328;&#32467;&#26500;&#39044;&#27979;&#22120;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#28857;&#26159;&#65292;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;&#32467;&#26500;&#32422;&#26463;&#21644;&#20174;&#20013;&#24471;&#20986;&#30340;&#32452;&#21512;&#25512;&#29702;&#26469;&#36807;&#28388;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#30340;&#19981;&#19968;&#33268;&#32467;&#26500;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#32467;&#26500;&#21270;&#39044;&#27979;&#20219;&#21153;&#21644;&#20116;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#20363;&#21270;&#20102;&#36825;&#20010;&#26694;&#26550;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#65292;&#24378;&#21046;&#23454;&#26045;&#19968;&#33268;&#24615;&#19981;&#20165;&#26500;&#36896;&#20102;&#32467;&#26500;&#26377;&#25928;&#30340;&#36755;&#20986;&#65292;&#32780;&#19988;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#19981;&#21463;&#32422;&#26463;&#30340;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.06877v2 Announce Type: replace  Abstract: Prompt-based methods have been used extensively across NLP to build zero- and few-shot label predictors. Many NLP tasks are naturally structured: that is, their outputs consist of multiple labels which constrain each other. Annotating data for such tasks can be cumbersome. Can the promise of the prompt-based paradigm be extended to such structured outputs? In this paper, we present a framework for constructing zero- and few-shot linguistic structure predictors. Our key insight is that we can use structural constraints -- and combinatorial inference derived from them -- to filter out inconsistent structures predicted by large language models. We instantiated this framework on two structured prediction tasks, and five datasets. Across all cases, our results show that enforcing consistency not only constructs structurally valid outputs, but also improves performance over the unconstrained variants.
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#20174;&#20301;&#32622;&#32534;&#30721;&#30340;&#35282;&#24230;&#24635;&#32467;&#20102;&#29992;&#20110;&#22686;&#24378;Transformer&#38271;&#24230;&#22806;&#25512;&#33021;&#21147;&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#32477;&#23545;&#21644;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#20197;&#21450;&#22522;&#20110;&#23427;&#20204;&#30340;&#22806;&#25512;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2312.17044</link><description>&lt;p&gt;
Transformer&#38271;&#24230;&#22806;&#25512;&#65306;&#20174;&#20301;&#32622;&#32534;&#30721;&#30340;&#35282;&#24230;&#36827;&#34892;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Length Extrapolation of Transformers: A Survey from the Perspective of Position Encoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.17044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#20174;&#20301;&#32622;&#32534;&#30721;&#30340;&#35282;&#24230;&#24635;&#32467;&#20102;&#29992;&#20110;&#22686;&#24378;Transformer&#38271;&#24230;&#22806;&#25512;&#33021;&#21147;&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#32477;&#23545;&#21644;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#20197;&#21450;&#22522;&#20110;&#23427;&#20204;&#30340;&#22806;&#25512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#33258;&#35806;&#29983;&#20197;&#26469;&#24050;&#32463;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#25472;&#36215;&#20102;&#19968;&#32929;&#39118;&#26292;&#12290;&#24314;&#31435;&#22312;&#20854;&#22522;&#30784;&#19978;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#20840;&#29699;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#21253;&#25324;&#36825;&#20123;&#24378;&#22823;&#30340;LLMs&#22312;&#20869;&#30340;&#25152;&#26377;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#37117;&#21463;&#21046;&#20110;&#39044;&#35774;&#30340;&#38271;&#24230;&#38480;&#21046;&#65292;&#24456;&#38590;&#20174;&#30701;&#35757;&#32451;&#24207;&#21015;&#25512;&#24191;&#21040;&#26356;&#38271;&#30340;&#25512;&#26029;&#24207;&#21015;&#65292;&#21363;&#23427;&#20204;&#26080;&#27861;&#36827;&#34892;&#38271;&#24230;&#22806;&#25512;&#12290;&#22240;&#27492;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#22823;&#37327;&#26041;&#27861;&#26469;&#22686;&#24378;Transformer&#30340;&#38271;&#24230;&#22806;&#25512;&#33021;&#21147;&#65292;&#20854;&#20013;&#20301;&#32622;&#32534;&#30721;&#65288;PE&#65289;&#34987;&#35748;&#20026;&#26159;&#20027;&#35201;&#22240;&#32032;&#12290; &#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#20174;PE&#30340;&#35282;&#24230;&#20197;&#32479;&#19968;&#31526;&#21495;&#20171;&#32461;&#20102;&#36825;&#20123;&#20851;&#20110;&#38271;&#24230;&#22806;&#25512;&#30340;&#36827;&#23637;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#21487;&#22806;&#25512;&#30340;PE&#65292;&#21253;&#25324;&#32477;&#23545;&#21644;&#30456;&#23545;PE&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#22522;&#20110;&#23427;&#20204;&#30340;&#22806;&#25512;&#26041;&#27861;&#65292;&#28085;&#30422;&#20102;&#20301;&#32622;&#25554;&#20540;&#21644;&#38543;&#26426;&#21270;&#20301;&#32622;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.17044v3 Announce Type: replace  Abstract: Transformer has taken the field of natural language processing (NLP) by storm since its birth. Further, Large language models (LLMs) built upon it have captured worldwide attention due to its superior abilities. Nevertheless, all Transformer-based models including these powerful LLMs suffer from a preset length limit and can hardly generalize from short training sequences to longer inference ones, namely, they can not perform length extrapolation. Hence, a plethora of methods have been proposed to enhance length extrapolation of Transformer, in which the positional encoding (PE) is recognized as the major factor. In this survey, we present these advances towards length extrapolation in a unified notation from the perspective of PE. Specifically, we first introduce extrapolatable PEs, including absolute and relative PEs. Then, we dive into extrapolation methods based on them, covering position interpolation and randomized position met
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22522;&#20110;&#27169;&#24335;&#30340;&#25490;&#21517;&#26041;&#27861;&#23545;&#32908;&#32905;&#39592;&#39612;&#30142;&#30149;&#30340;&#39118;&#38505;&#22240;&#32032;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#25490;&#21517;&#65292;&#20197;&#25552;&#39640;&#23545;&#20854;&#29702;&#35299;&#12289;&#20998;&#31867;&#21644;&#20248;&#20808;&#32771;&#34385;&#39044;&#38450;&#21644;&#27835;&#30103;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2312.11517</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#32908;&#32905;&#39592;&#39612;&#30142;&#30149;&#39118;&#38505;&#22240;&#32032;&#20998;&#31867;&#19982;&#22522;&#20110;&#27169;&#24335;&#30340;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
A Natural Language Processing-Based Classification and Mode-Based Ranking of Musculoskeletal Disorder Risk Factors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22522;&#20110;&#27169;&#24335;&#30340;&#25490;&#21517;&#26041;&#27861;&#23545;&#32908;&#32905;&#39592;&#39612;&#30142;&#30149;&#30340;&#39118;&#38505;&#22240;&#32032;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#25490;&#21517;&#65292;&#20197;&#25552;&#39640;&#23545;&#20854;&#29702;&#35299;&#12289;&#20998;&#31867;&#21644;&#20248;&#20808;&#32771;&#34385;&#39044;&#38450;&#21644;&#27835;&#30103;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#32908;&#32905;&#39592;&#39612;&#30142;&#30149;&#65288;MSD&#65289;&#39118;&#38505;&#22240;&#32032;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#22522;&#20110;&#27169;&#24335;&#30340;&#25490;&#21517;&#30456;&#32467;&#21512;&#12290;&#26088;&#22312;&#31934;&#32454;&#21270;&#29702;&#35299;&#12289;&#20998;&#31867;&#21644;&#20248;&#20808;&#32771;&#34385;&#38024;&#23545;&#24615;&#39044;&#38450;&#21644;&#27835;&#30103;&#12290;&#35780;&#20272;&#20102;&#20843;&#20010;NLP&#27169;&#22411;&#65292;&#32467;&#21512;&#39044;&#35757;&#32451;&#30340;&#36716;&#25442;&#22120;&#12289;&#20313;&#24358;&#30456;&#20284;&#24230;&#21644;&#36317;&#31163;&#24230;&#37327;&#23558;&#22240;&#32032;&#20998;&#31867;&#20026;&#20010;&#20154;&#12289;&#29983;&#29289;&#21147;&#23398;&#12289;&#24037;&#20316;&#22330;&#25152;&#12289;&#24515;&#29702;&#21644;&#32452;&#32455;&#31561;&#31867;&#21035;&#12290;BERT&#19982;&#20313;&#24358;&#30456;&#20284;&#24230;&#36798;&#21040;28%&#30340;&#20934;&#30830;&#29575;&#65307;&#21477;&#23376;&#36716;&#25442;&#22120;&#19982;&#27431;&#27663;&#12289;&#24067;&#38647;&#26354;&#33922;&#26031;&#21644;&#38389;&#21487;&#22827;&#26031;&#22522;&#36317;&#31163;&#24471;&#20998;&#20026;100%&#12290;&#36890;&#36807;10&#20493;&#20132;&#21449;&#39564;&#35777;&#65292;&#32479;&#35745;&#26816;&#39564;&#30830;&#20445;&#40065;&#26834;&#32467;&#26524;&#12290;&#35843;&#26597;&#25968;&#25454;&#21644;&#22522;&#20110;&#27169;&#24335;&#30340;&#25490;&#21517;&#30830;&#23450;&#20102;&#20005;&#37325;&#24615;&#31561;&#32423;&#65292;&#19982;&#25991;&#29486;&#30456;&#19968;&#33268;&#12290;"&#24037;&#20316;&#23039;&#21183;"&#26159;&#26368;&#20005;&#37325;&#30340;&#65292;&#20984;&#26174;&#20102;&#23039;&#21183;&#30340;&#20316;&#29992;&#12290;&#35843;&#26597;&#32467;&#26524;&#24378;&#35843;&#20102;"&#24037;&#20316;&#19981;&#31283;&#23450;&#24615;"&#12289;"&#24037;&#20316;&#21162;&#21147;&#21644;&#22238;&#25253;&#19981;&#24179;&#34913;"&#21644;"&#21592;&#24037;&#35774;&#26045;&#24046;"&#31561;&#22240;&#32032;&#30340;&#26174;&#33879;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11517v3 Announce Type: replace  Abstract: This research delves into Musculoskeletal Disorder (MSD) risk factors, using a blend of Natural Language Processing (NLP) and mode-based ranking. The aim is to refine understanding, classification, and prioritization for focused prevention and treatment. Eight NLP models are evaluated, combining pre-trained transformers, cosine similarity, and distance metrics to categorize factors into personal, biomechanical, workplace, psychological, and organizational classes. BERT with cosine similarity achieves 28% accuracy; sentence transformer with Euclidean, Bray-Curtis, and Minkowski distances scores 100%. With 10-fold cross-validation, statistical tests ensure robust results. Survey data and mode-based ranking determine severity hierarchy, aligning with the literature. "Working posture" is the most severe, highlighting posture's role. Survey insights emphasize "Job insecurity," "Effort reward imbalance," and "Poor employee facility" as sig
&lt;/p&gt;</description></item><item><title>ComplexityNet&#36890;&#36807;&#23398;&#20064;&#20219;&#21153;&#22797;&#26434;&#24615;&#65292;&#25552;&#39640;&#20102;LLM&#25512;&#29702;&#25928;&#29575;&#65292;&#36890;&#36807;&#39044;&#27979;&#20219;&#21153;&#30340;&#20934;&#30830;&#36755;&#20986;&#27010;&#29575;&#65292;&#25104;&#21151;&#38477;&#20302;&#20102;90%&#30340;&#35745;&#31639;&#36164;&#28304;&#20351;&#29992;&#65292;&#24182;&#22312;&#20219;&#21153;&#22797;&#26434;&#24615;&#30830;&#23450;&#26041;&#38754;&#21462;&#24471;&#20102;79%&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2312.11511</link><description>&lt;p&gt;
ComplexityNet: &#36890;&#36807;&#23398;&#20064;&#20219;&#21153;&#22797;&#26434;&#24615;&#25552;&#39640;LLM&#25512;&#29702;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
ComplexityNet: Increasing LLM Inference Efficiency by Learning Task Complexity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11511
&lt;/p&gt;
&lt;p&gt;
ComplexityNet&#36890;&#36807;&#23398;&#20064;&#20219;&#21153;&#22797;&#26434;&#24615;&#65292;&#25552;&#39640;&#20102;LLM&#25512;&#29702;&#25928;&#29575;&#65292;&#36890;&#36807;&#39044;&#27979;&#20219;&#21153;&#30340;&#20934;&#30830;&#36755;&#20986;&#27010;&#29575;&#65292;&#25104;&#21151;&#38477;&#20302;&#20102;90%&#30340;&#35745;&#31639;&#36164;&#28304;&#20351;&#29992;&#65292;&#24182;&#22312;&#20219;&#21153;&#22797;&#26434;&#24615;&#30830;&#23450;&#26041;&#38754;&#21462;&#24471;&#20102;79%&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;ComplexityNet&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;&#35780;&#20272;&#20219;&#21153;&#22797;&#26434;&#24615;&#32780;&#35774;&#35745;&#30340;&#31616;&#21270;&#35821;&#35328;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#19981;&#21516;&#33021;&#21147;&#30340;&#21508;&#31181;&#35821;&#35328;&#27169;&#22411;&#26469;&#39044;&#27979;&#20934;&#30830;&#36755;&#20986;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#39318;&#27425;&#22312;Mostly Basic Python Problems&#65288;MBPP&#65289;&#25968;&#25454;&#38598;&#19978;&#24212;&#29992;&#20102;ComplexityNet&#12290;&#25105;&#20204;&#24320;&#21019;&#24615;&#22320;&#21019;&#24314;&#20102;&#31532;&#19968;&#32452;&#26631;&#31614;&#26469;&#23450;&#20041;&#20219;&#21153;&#22797;&#26434;&#24615;&#12290;ComplexityNet&#22312;&#30830;&#23450;&#20219;&#21153;&#22797;&#26434;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;79%&#20934;&#30830;&#29575;&#65292;&#36739;&#21407;&#22987;&#12289;&#38750;&#24494;&#35843;&#27169;&#22411;&#30340;34%&#20934;&#30830;&#29575;&#26377;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#19982;&#20351;&#29992;&#26368;&#39640;&#22797;&#26434;&#24615;&#27169;&#22411;&#30456;&#27604;&#65292;ComplexityNet&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;90%&#30340;&#35745;&#31639;&#36164;&#28304;&#20351;&#29992;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;86.7%&#30340;&#39640;&#20195;&#30721;&#29983;&#25104;&#20934;&#30830;&#29575;&#12290;&#36825;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#24494;&#35843;&#36739;&#23567;&#30340;&#27169;&#22411;&#26469;&#23545;&#20219;&#21153;&#36827;&#34892;&#20998;&#31867;&#65292;&#21487;&#20197;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#26356;&#24179;&#34913;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11511v2 Announce Type: replace-cross  Abstract: We present ComplexityNet, a streamlined language model designed for assessing task complexity. This model predicts the likelihood of accurate output by various language models, each with different capabilities. Our initial application of ComplexityNet involves the Mostly Basic Python Problems (MBPP) dataset. We pioneered the creation of the first set of labels to define task complexity. ComplexityNet achieved a notable 79% accuracy in determining task complexity, a significant improvement over the 34% accuracy of the original, non fine-tuned model. Furthermore, ComplexityNet effectively reduces computational resource usage by 90% compared to using the highest complexity model, while maintaining a high code generation accuracy of 86.7%. This study demonstrates that fine-tuning smaller models to categorize tasks based on their complexity can lead to a more balanced trade-off between accuracy and efficiency in the use of Large Lan
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#21363;&#35270;&#39057;&#31505;&#22768;&#25512;&#29702;&#65292;&#26088;&#22312;&#35299;&#37322;&#29305;&#23450;&#35270;&#39057;&#20013;&#20154;&#20204;&#31505;&#30340;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#25968;&#25454;&#38598;SMILE&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#35270;&#39057;&#34920;&#31034;&#65292;&#25105;&#20204;&#30340;&#22522;&#32447;&#33021;&#22815;&#29983;&#25104;&#21512;&#29702;&#30340;&#31505;&#22768;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2312.09818</link><description>&lt;p&gt;
SMILE&#65306;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#35270;&#39057;&#20013;&#30340;&#31505;&#22768;
&lt;/p&gt;
&lt;p&gt;
SMILE: Multimodal Dataset for Understanding Laughter in Video with Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#21363;&#35270;&#39057;&#31505;&#22768;&#25512;&#29702;&#65292;&#26088;&#22312;&#35299;&#37322;&#29305;&#23450;&#35270;&#39057;&#20013;&#20154;&#20204;&#31505;&#30340;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#25968;&#25454;&#38598;SMILE&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#35270;&#39057;&#34920;&#31034;&#65292;&#25105;&#20204;&#30340;&#22522;&#32447;&#33021;&#22815;&#29983;&#25104;&#21512;&#29702;&#30340;&#31505;&#22768;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20154;&#24037;&#26234;&#33021;&#26368;&#36817;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#26500;&#24314;&#31038;&#20132;&#26234;&#33021;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20854;&#20013;&#65292;&#31505;&#22768;&#26159;&#20154;&#31867;&#31038;&#20132;&#20114;&#21160;&#20013;&#21457;&#29983;&#30340;&#29420;&#29305;&#34920;&#36798;&#20043;&#19968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#38754;&#23545;&#20102;&#26426;&#22120;&#29702;&#35299;&#35270;&#39057;&#20013;&#31505;&#22768;&#32972;&#21518;&#29702;&#30001;&#30340;&#26032;&#25361;&#25112;&#65292;&#21363;&#35270;&#39057;&#31505;&#22768;&#25512;&#29702;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#36825;&#19968;&#26032;&#20219;&#21153;&#65292;&#35299;&#37322;&#20154;&#20204;&#22312;&#29305;&#23450;&#35270;&#39057;&#20013;&#20026;&#20160;&#20040;&#20250;&#31505;&#30340;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#36825;&#19968;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;SMILE&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#32447;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#25991;&#26412;&#35270;&#39057;&#34920;&#31034;&#29983;&#25104;&#21512;&#29702;&#30340;&#31505;&#22768;&#35299;&#37322;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#22522;&#32447;&#21487;&#20197;&#29983;&#25104;&#21487;&#20449;&#30340;&#31505;&#22768;&#35299;&#37322;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#25105;&#20204;&#30340;&#22522;&#32447;&#22312;&#25506;&#27979;&#20854;&#20182;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#21644;&#37326;&#22806;&#35270;&#39057;&#26041;&#38754;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12289;&#20195;&#30721;&#21644;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09818v2 Announce Type: replace-cross  Abstract: Despite the recent advances of the artificial intelligence, building social intelligence remains a challenge. Among social signals, laughter is one of the distinctive expressions that occurs during social interactions between humans. In this work, we tackle a new challenge for machines to understand the rationale behind laughter in video, Video Laugh Reasoning. We introduce this new task to explain why people laugh in a particular video and a dataset for this task. Our proposed dataset, SMILE, comprises video clips and language descriptions of why people laugh. We propose a baseline by leveraging the reasoning capacity of large language models (LLMs) with textual video representation. Experiments show that our baseline can generate plausible explanations for laughter. We further investigate the scalability of our baseline by probing other video understanding tasks and in-the-wild videos. We release our dataset, code, and model 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Auto MC-Reward&#30340;&#20808;&#36827;&#23398;&#20064;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#35774;&#35745;&#31264;&#23494;&#22870;&#21169;&#20989;&#25968;&#26469;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;</title><link>https://arxiv.org/abs/2312.09238</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#20026;Minecraft&#33258;&#21160;&#35774;&#35745;&#31264;&#23494;&#22870;&#21169;&#30340;Auto MC-Reward
&lt;/p&gt;
&lt;p&gt;
Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Auto MC-Reward&#30340;&#20808;&#36827;&#23398;&#20064;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#35774;&#35745;&#31264;&#23494;&#22870;&#21169;&#20989;&#25968;&#26469;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#65288;&#20363;&#22914;Minecraft&#65289;&#20165;&#25552;&#20379;&#25351;&#31034;&#20219;&#21153;&#23436;&#25104;&#25110;&#22833;&#36133;&#30340;&#31232;&#30095;&#22870;&#21169;&#65292;&#36825;&#20123;&#22870;&#21169;&#20197;&#20108;&#36827;&#21046;&#20540;&#34920;&#31034;&#12290;&#22312;&#36825;&#31181;&#29615;&#22659;&#20013;&#25506;&#32034;&#25928;&#29575;&#30340;&#25361;&#25112;&#20351;&#24471;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20195;&#29702;&#31243;&#24207;&#38590;&#20197;&#23398;&#20064;&#22797;&#26434;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Auto MC-Reward&#30340;&#20808;&#36827;&#23398;&#20064;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#21160;&#35774;&#35745;&#31264;&#23494;&#22870;&#21169;&#20989;&#25968;&#65292;&#20174;&#32780;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#12290;Auto MC-Reward&#21253;&#25324;&#19977;&#20010;&#37325;&#35201;&#32452;&#20214;&#65306;&#22870;&#21169;&#35774;&#35745;&#32773;&#12289;&#22870;&#21169;&#35780;&#35770;&#23478;&#21644;&#36712;&#36857;&#20998;&#26512;&#22120;&#12290;&#32473;&#23450;&#29615;&#22659;&#20449;&#24687;&#21644;&#20219;&#21153;&#25551;&#36848;&#65292;&#22870;&#21169;&#35774;&#35745;&#32773;&#39318;&#20808;&#36890;&#36807;&#32534;&#20889;&#21487;&#25191;&#34892;&#30340;Python&#20989;&#25968;&#21644;&#39044;&#23450;&#20041;&#30340;&#35266;&#27979;&#36755;&#20837;&#26469;&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30340;&#22870;&#21169;&#35780;&#35770;&#23478;&#23558;&#36127;&#36131;&#39564;&#35777;&#20195;&#30721;&#65292;&#26816;&#26597;&#20195;&#30721;&#26159;&#21542;&#33258;&#27965;&#19988;&#26080;&#35821;&#27861;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09238v2 Announce Type: replace  Abstract: Many reinforcement learning environments (e.g., Minecraft) provide only sparse rewards that indicate task completion or failure with binary values. The challenge in exploration efficiency in such environments makes it difficult for reinforcement-learning-based agents to learn complex tasks. To address this, this paper introduces an advanced learning system, named Auto MC-Reward, that leverages Large Language Models (LLMs) to automatically design dense reward functions, thereby enhancing the learning efficiency. Auto MC-Reward consists of three important components: Reward Designer, Reward Critic, and Trajectory Analyzer. Given the environment information and task descriptions, the Reward Designer first design the reward function by coding an executable Python function with predefined observation inputs. Then, our Reward Critic will be responsible for verifying the code, checking whether the code is self-consistent and free of syntax 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26082;&#28789;&#27963;&#21448;&#22686;&#24378;&#23616;&#37096;&#24615;&#30340;&#26032;&#22411;&#25237;&#24433;&#20202;&#35774;&#35745;&#65292;&#26377;&#21161;&#20110;&#36830;&#25509;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#31354;&#38388;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2312.06742</link><description>&lt;p&gt;
&#34588;&#34562;&#65306;&#22810;&#27169;&#24577;LLM&#30340;&#22686;&#24378;&#23616;&#37096;&#25237;&#24433;&#20202;
&lt;/p&gt;
&lt;p&gt;
Honeybee: Locality-enhanced Projector for Multimodal LLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06742
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26082;&#28789;&#27963;&#21448;&#22686;&#24378;&#23616;&#37096;&#24615;&#30340;&#26032;&#22411;&#25237;&#24433;&#20202;&#35774;&#35745;&#65292;&#26377;&#21161;&#20110;&#36830;&#25509;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#31354;&#38388;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#20013;&#65292;&#35270;&#35273;&#25237;&#24433;&#20202;&#22312;&#36830;&#25509;&#39044;&#20808;&#35757;&#32451;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#19982;LLMs&#20043;&#38388;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#23454;&#29616;&#28145;&#20837;&#30340;&#35270;&#35273;&#29702;&#35299;&#24182;&#21033;&#29992;LLMs&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#23613;&#31649;&#35270;&#35273;&#25237;&#24433;&#20202;&#30340;&#37325;&#35201;&#24615;&#19981;&#35328;&#32780;&#21947;&#65292;&#20294;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;&#20004;&#20010;&#20851;&#38190;&#30340;&#25237;&#24433;&#20202;&#23646;&#24615;&#65306;&#65288;i&#65289;&#28789;&#27963;&#24615;&#20197;&#31649;&#29702;&#35270;&#35273;&#20195;&#24065;&#30340;&#25968;&#37327;&#65292;&#23545;&#20110;MLLMs&#30340;&#25972;&#20307;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#65307;&#65288;ii&#65289;&#20445;&#30041;&#26469;&#33258;&#35270;&#35273;&#29305;&#24449;&#30340;&#23616;&#37096;&#19978;&#19979;&#25991;&#65292;&#23545;&#20110;&#31354;&#38388;&#29702;&#35299;&#33267;&#20851;&#37325;&#35201;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26082;&#28789;&#27963;&#21448;&#22686;&#24378;&#23616;&#37096;&#24615;&#30340;&#26032;&#22411;&#25237;&#24433;&#20202;&#35774;&#35745;&#65292;&#26377;&#25928;&#22320;&#28385;&#36275;&#20102;&#36825;&#20004;&#31181;&#29702;&#24819;&#23646;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20840;&#38754;&#30340;&#31574;&#30053;&#65292;&#20197;&#26377;&#25928;&#21033;&#29992;&#22810;&#20010;&#21644;&#22810;&#26041;&#38754;&#30340;&#25351;&#23548;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#26816;&#39564;&#20102;&#21508;&#31181;&#35774;&#35745;&#36873;&#25321;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.06742v2 Announce Type: replace-cross  Abstract: In Multimodal Large Language Models (MLLMs), a visual projector plays a crucial role in bridging pre-trained vision encoders with LLMs, enabling profound visual understanding while harnessing the LLMs' robust capabilities. Despite the importance of the visual projector, it has been relatively less explored. In this study, we first identify two essential projector properties: (i) flexibility in managing the number of visual tokens, crucial for MLLMs' overall efficiency, and (ii) preservation of local context from visual features, vital for spatial understanding. Based on these findings, we propose a novel projector design that is both flexible and locality-enhanced, effectively satisfying the two desirable properties. Additionally, we present comprehensive strategies to effectively utilize multiple and multifaceted instruction datasets. Through extensive experiments, we examine the impact of individual design choices. Finally, o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#24335;&#24605;&#32500;&#25552;&#31034;&#65288;CCoT&#65289;&#65292;&#20197;&#20811;&#26381;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#38590;&#20197;&#25429;&#25417;&#21040;&#32452;&#21512;&#35270;&#35273;&#25512;&#29702;&#26041;&#38754;&#30340;&#32454;&#33410;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.17076</link><description>&lt;p&gt;
&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#32452;&#21512;&#24335;&#24605;&#32500;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Compositional Chain-of-Thought Prompting for Large Multimodal Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17076
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#24335;&#24605;&#32500;&#25552;&#31034;&#65288;CCoT&#65289;&#65292;&#20197;&#20811;&#26381;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#38590;&#20197;&#25429;&#25417;&#21040;&#32452;&#21512;&#35270;&#35273;&#25512;&#29702;&#26041;&#38754;&#30340;&#32454;&#33410;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#22823;&#30340;&#35270;&#35273;&#39592;&#24178;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLM)&#25512;&#29702;&#30340;&#32467;&#21512;&#24050;&#32463;&#23548;&#33268;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;(LMM)&#25104;&#20026;&#24403;&#21069;&#24191;&#27867;&#35270;&#35273;&#21644;&#35821;&#35328;(VL)&#20219;&#21153;&#30340;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;LMM&#20173;&#28982;&#38590;&#20197;&#25429;&#25417;&#21040;&#32452;&#21512;&#35270;&#35273;&#25512;&#29702;&#26041;&#38754;&#30340;&#32454;&#33410;&#65292;&#27604;&#22914;&#23545;&#35937;&#20043;&#38388;&#30340;&#23646;&#24615;&#21644;&#20851;&#31995;&#12290;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#26159;&#21033;&#29992;&#22330;&#26223;&#22270;(SGs)&#8212;&#8212;&#23545;&#35937;&#21450;&#20854;&#20851;&#31995;&#21644;&#23646;&#24615;&#30340;&#24418;&#24335;&#21270;&#34920;&#36798;&#65292;&#23427;&#24050;&#34987;&#24191;&#27867;&#29992;&#20316;&#35270;&#35273;&#21644;&#25991;&#26412;&#39046;&#22495;&#20043;&#38388;&#30340;&#26725;&#26753;&#12290;&#28982;&#32780;&#65292;&#22330;&#26223;&#22270;&#25968;&#25454;&#38656;&#35201;&#22330;&#26223;&#22270;&#27880;&#37322;&#65292;&#36825;&#31181;&#25968;&#25454;&#25910;&#38598;&#25104;&#26412;&#39640;&#26114;&#65292;&#22240;&#27492;&#38590;&#20197;&#25193;&#23637;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#22330;&#26223;&#22270;&#25968;&#25454;&#24494;&#35843;LMM&#21487;&#33021;&#23548;&#33268;&#39044;&#35757;&#32451;&#30446;&#26631;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#65292;&#21463;&#21040;&#24605;&#32500;&#38142;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#24335;&#24605;&#32500;&#25552;&#31034;&#65288;CCoT&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17076v2 Announce Type: replace-cross  Abstract: The combination of strong visual backbones and Large Language Model (LLM) reasoning has led to Large Multimodal Models (LMMs) becoming the current standard for a wide range of vision and language (VL) tasks. However, recent research has shown that even the most advanced LMMs still struggle to capture aspects of compositional visual reasoning, such as attributes and relationships between objects. One solution is to utilize scene graphs (SGs)--a formalization of objects and their relations and attributes that has been extensively used as a bridge between the visual and textual domains. Yet, scene graph data requires scene graph annotations, which are expensive to collect and thus not easily scalable. Moreover, finetuning an LMM based on SG data can lead to catastrophic forgetting of the pretraining objective. To overcome this, inspired by chain-of-thought methods, we propose Compositional Chain-of-Thought (CCoT), a novel zero-sho
&lt;/p&gt;</description></item><item><title>MobileCLIP&#36890;&#36807;&#22810;&#27169;&#24577;&#24378;&#21270;&#35757;&#32451;&#23454;&#29616;&#20102;&#39640;&#25928;&#29575;&#22270;&#20687;-&#25991;&#26412;&#27169;&#22411;&#65292;&#22312;&#38646;-shot&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#24615;&#33021;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2311.17049</link><description>&lt;p&gt;
MobileCLIP: &#36890;&#36807;&#22810;&#27169;&#24577;&#24378;&#21270;&#35757;&#32451;&#23454;&#29616;&#24555;&#36895;&#22270;&#20687;-&#25991;&#26412;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17049
&lt;/p&gt;
&lt;p&gt;
MobileCLIP&#36890;&#36807;&#22810;&#27169;&#24577;&#24378;&#21270;&#35757;&#32451;&#23454;&#29616;&#20102;&#39640;&#25928;&#29575;&#22270;&#20687;-&#25991;&#26412;&#27169;&#22411;&#65292;&#22312;&#38646;-shot&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#24615;&#33021;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#22270;&#20687;-&#25991;&#26412;&#22522;&#30784;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#36827;&#34892;&#23545;&#27604;&#39044;&#35757;&#32451;&#34920;&#26126;&#65292;&#22312;&#24191;&#27867;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#30340;&#38646;-shot&#24615;&#33021;&#21644;&#25913;&#21892;&#30340;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#21033;&#29992;&#22823;&#22411;&#22522;&#20110;transformer&#30340;&#32534;&#30721;&#22120;&#65292;&#20855;&#26377;&#26174;&#33879;&#30340;&#20869;&#23384;&#21644;&#24310;&#36831;&#24320;&#38144;&#65292;&#36825;&#20250;&#32473;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#37096;&#32626;&#24102;&#26469;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MobileCLIP--&#19968;&#31181;&#20248;&#21270;&#20102;&#36816;&#34892;&#26102;&#24615;&#33021;&#30340;&#39640;&#25928;&#22270;&#20687;-&#25991;&#26412;&#27169;&#22411;&#23478;&#26063;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#39062;&#32780;&#39640;&#25928;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#21363;&#22810;&#27169;&#24577;&#24378;&#21270;&#35757;&#32451;&#12290;&#25152;&#25552;&#20986;&#30340;&#35757;&#32451;&#26041;&#27861;&#21033;&#29992;&#26469;&#33258;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#21644;&#24378;CLIP&#32534;&#30721;&#22120;&#38598;&#25104;&#30340;&#30693;&#35782;&#36716;&#31227;&#26469;&#25552;&#39640;&#39640;&#25928;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#23558;&#39069;&#22806;&#30340;&#30693;&#35782;&#23384;&#20648;&#22312;&#24378;&#21270;&#25968;&#25454;&#38598;&#20013;&#36991;&#20813;&#20102;&#35757;&#32451;&#26102;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;MobileCLIP&#20026;&#38646;-shot&#20998;&#31867;&#21644;&#26816;&#32034;&#35774;&#32622;&#20102;&#19968;&#20010;&#26032;&#30340;&#24310;&#36831;-&#20934;&#30830;&#24615;&#26435;&#34913;&#30340;&#26368;&#26032;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17049v2 Announce Type: replace-cross  Abstract: Contrastive pretraining of image-text foundation models, such as CLIP, demonstrated excellent zero-shot performance and improved robustness on a wide range of downstream tasks. However, these models utilize large transformer-based encoders with significant memory and latency overhead which pose challenges for deployment on mobile devices. In this work, we introduce MobileCLIP -- a new family of efficient image-text models optimized for runtime performance along with a novel and efficient training approach, namely multi-modal reinforced training. The proposed training approach leverages knowledge transfer from an image captioning model and an ensemble of strong CLIP encoders to improve the accuracy of efficient models. Our approach avoids train-time compute overhead by storing the additional knowledge in a reinforced dataset. MobileCLIP sets a new state-of-the-art latency-accuracy tradeoff for zero-shot classification and retrie
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;Outcome-supervised Value Model (OVM)&#21033;&#29992;&#32467;&#26524;&#30417;&#30563;&#35757;&#32451;&#20215;&#20540;&#27169;&#22411;&#65292;&#20197;&#22312;&#25968;&#23398;&#25512;&#29702;&#20013;&#20943;&#23569;&#38169;&#35823;&#20256;&#25773;&#65292;&#23558;&#20219;&#21153;&#36716;&#21270;&#20026;&#20215;&#20540;&#20272;&#35745;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.09724</link><description>&lt;p&gt;
OVM&#65292;&#32467;&#26524;&#30417;&#30563;&#20215;&#20540;&#27169;&#22411;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
OVM, Outcome-supervised Value Models for Planning in Mathematical Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09724
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;Outcome-supervised Value Model (OVM)&#21033;&#29992;&#32467;&#26524;&#30417;&#30563;&#35757;&#32451;&#20215;&#20540;&#27169;&#22411;&#65292;&#20197;&#22312;&#25968;&#23398;&#25512;&#29702;&#20013;&#20943;&#23569;&#38169;&#35823;&#20256;&#25773;&#65292;&#23558;&#20219;&#21153;&#36716;&#21270;&#20026;&#20215;&#20540;&#20272;&#35745;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#32463;&#24120;&#22312;&#22810;&#20010;&#25512;&#29702;&#27493;&#39588;&#20013;&#20445;&#25345;&#20934;&#30830;&#24615;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#23398;&#25512;&#29702;&#20013;&#65292;&#26089;&#26399;&#27493;&#39588;&#20013;&#30340;&#38169;&#35823;&#21487;&#33021;&#20256;&#25773;&#21040;&#21518;&#32493;&#27493;&#39588;&#65292;&#26368;&#32456;&#23548;&#33268;&#38169;&#35823;&#31572;&#26696;&#12290;&#20026;&#20102;&#20943;&#23569;&#38169;&#35823;&#20256;&#25773;&#65292;&#24341;&#20837;&#20102;&#24341;&#23548;&#35299;&#30721;&#20197;&#36880;&#27493;&#25351;&#23548;LM&#35299;&#30721;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#24341;&#23548;&#35299;&#30721;&#20013;&#65292;&#35780;&#20272;&#19981;&#23436;&#25972;&#25512;&#29702;&#36335;&#24452;&#30340;&#28508;&#21147;&#21487;&#33021;&#27604;&#20165;&#30830;&#20445;&#27599;&#20010;&#27493;&#39588;&#30340;&#27491;&#30830;&#24615;&#26356;&#26377;&#20248;&#21183;&#65292;&#22240;&#20026;&#21069;&#19968;&#31181;&#26041;&#27861;&#20250;&#23548;&#21521;&#27491;&#30830;&#30340;&#26368;&#32456;&#31572;&#26696;&#12290;&#36825;&#23558;&#20219;&#21153;&#36716;&#21270;&#20026;&#35745;&#21010;&#20013;&#30340;&#20215;&#20540;&#20272;&#35745;&#38382;&#39064;&#12290;&#21463;&#21040;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#21363;$\textit{&#24341;&#23548;&#35299;&#30721;&#30340;&#32467;&#26524;&#30417;&#30563;&#26412;&#36136;&#19978;&#20805;&#24403;&#20215;&#20540;&#27169;&#22411;}$&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Outcome-supervised Value Model (OVM)&#65292;&#23427;&#37319;&#29992;&#32467;&#26524;&#30417;&#30563;&#26469;&#35757;&#32451;&#20215;&#20540;&#27169;&#22411;&#65292;&#20248;&#20808;&#32771;&#34385;&#23548;&#33268;&#20934;&#30830;&#24615;&#30340;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09724v2 Announce Type: replace  Abstract: Large language models (LLMs) often struggle with maintaining accuracy throughout multiple multiple reasoning steps, especially in mathematical reasoning where an error in earlier steps can propagate to subsequent ones and it ultimately leading to an incorrect answer. To reduce error propagation, guided decoding is employed to direct the LM decoding on a step-by-step basis. We argue that in guided decoding, assessing the potential of an incomplete reasoning path can be more advantageous than simply ensuring per-step correctness, as the former approach leads towards a correct final answer. This transforms the task into a $\textit{value estimation}$ problem in planning.   Inspired by the findings that $\textit{outcome supervision for guided decoding essentially acts as a value model}$, we propose Outcome-supervised Value Model (OVM) that employs outcome supervision for training a value model, which prioritizes steps that lead to accurat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20154;&#21475;&#30340;&#26032;&#26041;&#27861;&#26469;&#27169;&#25311;&#24847;&#35265;&#21160;&#24577;&#65292;&#21457;&#29616;LLM&#20195;&#29702;&#23384;&#22312;&#22266;&#26377;&#20559;&#35265;&#23548;&#33268;&#27169;&#25311;&#20195;&#29702;&#36235;&#21521;&#20110;&#31185;&#23398;&#29616;&#23454;&#19968;&#33268;&#30340;&#20849;&#35782;&#65292;&#20294;&#24341;&#20837;&#30830;&#35748;&#20559;&#35265;&#21518;&#35266;&#23519;&#21040;&#24847;&#35265;&#20998;&#35010;&#65292;&#31361;&#26174;&#20102;LLM&#20195;&#29702;&#22312;&#35813;&#39046;&#22495;&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.09618</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#32593;&#32476;&#27169;&#25311;&#24847;&#35265;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Simulating Opinion Dynamics with Networks of LLM-based Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09618
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20154;&#21475;&#30340;&#26032;&#26041;&#27861;&#26469;&#27169;&#25311;&#24847;&#35265;&#21160;&#24577;&#65292;&#21457;&#29616;LLM&#20195;&#29702;&#23384;&#22312;&#22266;&#26377;&#20559;&#35265;&#23548;&#33268;&#27169;&#25311;&#20195;&#29702;&#36235;&#21521;&#20110;&#31185;&#23398;&#29616;&#23454;&#19968;&#33268;&#30340;&#20849;&#35782;&#65292;&#20294;&#24341;&#20837;&#30830;&#35748;&#20559;&#35265;&#21518;&#35266;&#23519;&#21040;&#24847;&#35265;&#20998;&#35010;&#65292;&#31361;&#26174;&#20102;LLM&#20195;&#29702;&#22312;&#35813;&#39046;&#22495;&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#27169;&#25311;&#20154;&#31867;&#24847;&#35265;&#21160;&#24577;&#23545;&#20110;&#29702;&#35299;&#21508;&#31181;&#31038;&#20250;&#29616;&#35937;&#33267;&#20851;&#37325;&#35201;&#65292;&#21253;&#25324;&#26497;&#21270;&#21644;&#38169;&#35823;&#20449;&#24687;&#30340;&#20256;&#25773;&#12290;&#28982;&#32780;&#65292;&#24120;&#29992;&#20110;&#27492;&#31867;&#27169;&#25311;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;&#27169;&#22411;&#65288;ABM&#65289;&#32463;&#24120;&#20250;&#36807;&#20998;&#31616;&#21270;&#20154;&#31867;&#34892;&#20026;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20154;&#21475;&#30340;&#27169;&#25311;&#24847;&#35265;&#21160;&#24577;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;LLM&#20195;&#29702;&#23384;&#22312;&#19968;&#31181;&#23545;&#20135;&#29983;&#20934;&#30830;&#20449;&#24687;&#30340;&#24378;&#28872;&#22266;&#26377;&#20559;&#35265;&#65292;&#23548;&#33268;&#27169;&#25311;&#20195;&#29702;&#36235;&#21521;&#20110;&#19982;&#31185;&#23398;&#29616;&#23454;&#19968;&#33268;&#30340;&#20849;&#35782;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20559;&#35265;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#29702;&#35299;&#27668;&#20505;&#21464;&#21270;&#31561;&#38382;&#39064;&#19978;&#25269;&#21046;&#20849;&#35782;&#35266;&#28857;&#30340;&#25928;&#29992;&#12290;&#36890;&#36807;&#24341;&#20837;&#25552;&#31034;&#24037;&#31243;&#35825;&#23548;&#30830;&#35748;&#20559;&#35265;&#21518;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#19982;&#29616;&#26377;&#22522;&#20110;&#20195;&#29702;&#27169;&#22411;&#21644;&#24847;&#35265;&#21160;&#24577;&#30740;&#31350;&#19968;&#33268;&#30340;&#24847;&#35265;&#20998;&#35010;&#12290;&#36825;&#20123;&#35265;&#35299;&#31361;&#26174;&#20102;LLM&#20195;&#29702;&#22312;&#35813;&#39046;&#22495;&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#26465;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09618v2 Announce Type: replace-cross  Abstract: Accurately simulating human opinion dynamics is crucial for understanding a variety of societal phenomena, including polarization and the spread of misinformation. However, the agent-based models (ABMs) commonly used for such simulations often over-simplify human behavior. We propose a new approach to simulating opinion dynamics based on populations of Large Language Models (LLMs). Our findings reveal a strong inherent bias in LLM agents towards producing accurate information, leading simulated agents to consensus in line with scientific reality. This bias limits their utility for understanding resistance to consensus views on issues like climate change. After inducing confirmation bias through prompt engineering, however, we observed opinion fragmentation in line with existing agent-based modeling and opinion dynamics research. These insights highlight the promise and limitations of LLM agents in this domain and suggest a path
&lt;/p&gt;</description></item><item><title>ARES&#26159;&#29992;&#20110;&#35780;&#20272;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#30340;&#33258;&#21160;&#21270;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#21019;&#24314;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#21644;&#24494;&#35843;&#35780;&#20272;&#22120;&#65292;&#26377;&#25928;&#35780;&#20272;RAG&#31995;&#32479;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2311.09476</link><description>&lt;p&gt;
ARES: &#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#30340;&#33258;&#21160;&#21270;&#35780;&#20272;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09476
&lt;/p&gt;
&lt;p&gt;
ARES&#26159;&#29992;&#20110;&#35780;&#20272;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#30340;&#33258;&#21160;&#21270;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#21019;&#24314;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#21644;&#24494;&#35843;&#35780;&#20272;&#22120;&#65292;&#26377;&#25928;&#35780;&#20272;RAG&#31995;&#32479;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#31995;&#32479;&#20256;&#32479;&#19978;&#20381;&#36182;&#20110;&#25163;&#21160;&#27880;&#37322;&#36755;&#20837;&#26597;&#35810;&#12289;&#26816;&#32034;&#27573;&#33853;&#21644;&#29983;&#25104;&#21709;&#24212;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;ARES&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;RAG&#31995;&#32479;&#30340;&#33258;&#21160;&#21270;&#35780;&#20272;&#31995;&#32479;&#65292;&#35780;&#20272;&#32500;&#24230;&#21253;&#25324;&#19978;&#19979;&#25991;&#30456;&#20851;&#24615;&#12289;&#31572;&#26696;&#24544;&#23454;&#24230;&#21644;&#31572;&#26696;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#21019;&#24314;&#33258;&#24049;&#30340;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;ARES&#24494;&#35843;&#36731;&#37327;&#32423;LM&#35780;&#20272;&#22120;&#20197;&#35780;&#20272;&#21333;&#20010;RAG&#32452;&#20214;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#20943;&#23569;&#28508;&#22312;&#30340;&#39044;&#27979;&#38169;&#35823;&#65292;ARES&#21033;&#29992;&#23569;&#37327;&#20154;&#24037;&#27880;&#37322;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#27979;&#39537;&#21160;&#25512;&#29702;&#65288;PPI&#65289;&#12290;&#22312;KILT&#12289;SuperGLUE&#21644;AIS&#30340;&#20843;&#20010;&#19981;&#21516;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#65292;ARES&#22312;&#35780;&#20272;&#36807;&#31243;&#20013;&#20165;&#20351;&#29992;&#23569;&#37327;&#20154;&#24037;&#27880;&#37322;&#23601;&#20934;&#30830;&#35780;&#20272;RAG&#31995;&#32479;&#12290;&#27492;&#22806;&#65292;ARES&#35780;&#20272;&#22120;&#22312;&#39046;&#22495;&#36716;&#31227;&#20013;&#20173;&#28982;&#26377;&#25928;&#65292;&#21363;&#20351;&#22312;&#26356;&#25913;&#29992;&#20110;&#35780;&#20272;&#30340;&#26597;&#35810;&#21644;/&#25110;&#25991;&#26723;&#31867;&#22411;&#21518;&#20173;&#28982;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09476v2 Announce Type: replace-cross  Abstract: Evaluating retrieval-augmented generation (RAG) systems traditionally relies on hand annotations for input queries, passages to retrieve, and responses to generate. We introduce ARES, an Automated RAG Evaluation System, for evaluating RAG systems along the dimensions of context relevance, answer faithfulness, and answer relevance. By creating its own synthetic training data, ARES finetunes lightweight LM judges to assess the quality of individual RAG components. To mitigate potential prediction errors, ARES utilizes a small set of human-annotated datapoints for prediction-powered inference (PPI). Across eight different knowledge-intensive tasks in KILT, SuperGLUE, and AIS, ARES accurately evaluates RAG systems while using only a few hundred human annotations during evaluation. Furthermore, ARES judges remain effective across domain shifts, proving accurate even after changing the type of queries and/or documents used in the eva
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;AbsPyramid&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#34164;&#28085;&#22270;&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#25277;&#35937;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;LLMs&#21487;&#20197;&#36890;&#36807;&#22312;&#20016;&#23500;&#25277;&#35937;&#30693;&#35782;&#19978;&#36827;&#34892;&#35757;&#32451;&#26469;&#33719;&#24471;&#22522;&#26412;&#30340;&#25277;&#35937;&#33021;&#21147;&#65292;&#24182;&#27867;&#21270;&#21040;&#26410;&#35265;&#30340;&#20107;&#20214;&#12290;</title><link>https://arxiv.org/abs/2311.09174</link><description>&lt;p&gt;
AbsPyramid&#65306;&#20351;&#29992;&#32479;&#19968;&#30340;&#34164;&#28085;&#22270;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#25277;&#35937;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
AbsPyramid: Benchmarking the Abstraction Ability of Language Models with a Unified Entailment Graph
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09174
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;AbsPyramid&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#34164;&#28085;&#22270;&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#25277;&#35937;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;LLMs&#21487;&#20197;&#36890;&#36807;&#22312;&#20016;&#23500;&#25277;&#35937;&#30693;&#35782;&#19978;&#36827;&#34892;&#35757;&#32451;&#26469;&#33719;&#24471;&#22522;&#26412;&#30340;&#25277;&#35937;&#33021;&#21147;&#65292;&#24182;&#27867;&#21270;&#21040;&#26410;&#35265;&#30340;&#20107;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#30693;&#30740;&#31350;&#34920;&#26126;&#65292;&#25277;&#35937;&#33021;&#21147;&#23545;&#20154;&#31867;&#26234;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#36825;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#20173;&#28982;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;AbsPyramid&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;221K&#20010;&#25991;&#26412;&#25551;&#36848;&#30340;&#32479;&#19968;&#34164;&#28085;&#22270;&#65292;&#29992;&#20110;&#25910;&#38598;&#24191;&#27867;&#20107;&#20214;&#19977;&#20010;&#32452;&#25104;&#37096;&#20998;&#30340;&#25277;&#35937;&#30693;&#35782;&#65292;&#20197;&#20840;&#38754;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#24320;&#25918;&#39046;&#22495;&#20013;&#30340;&#25277;&#35937;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30446;&#21069;&#30340;LLMs&#22312;&#38646;&#30701;&#21644;&#23569;&#37327;&#25968;&#25454;&#24773;&#20917;&#19979;&#38754;&#20020;&#29702;&#35299;&#25277;&#35937;&#30693;&#35782;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#22312;&#25105;&#20204;&#20016;&#23500;&#30340;&#25277;&#35937;&#30693;&#35782;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#21487;&#20197;&#33719;&#24471;&#22522;&#26412;&#30340;&#25277;&#35937;&#33021;&#21147;&#65292;&#24182;&#27867;&#21270;&#21040;&#26410;&#35265;&#30340;&#20107;&#20214;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#23454;&#35777;&#34920;&#26126;&#25105;&#20204;&#30340;&#22522;&#20934;&#21487;&#20197;&#20840;&#38754;&#25552;&#39640;LLMs&#22312;&#20004;&#20010;&#20808;&#21069;&#30340;&#25277;&#35937;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09174v3 Announce Type: replace-cross  Abstract: Cognitive research indicates that abstraction ability is essential in human intelligence, which remains under-explored in language models. In this paper, we present AbsPyramid, a unified entailment graph of 221K textual descriptions of abstraction knowledge. While existing resources only touch nouns or verbs within simplified events or specific domains, AbsPyramid collects abstract knowledge for three components of diverse events to comprehensively evaluate the abstraction ability of language models in the open domain. Experimental results demonstrate that current LLMs face challenges comprehending abstraction knowledge in zero-shot and few-shot settings. By training on our rich abstraction knowledge, we find LLMs can acquire basic abstraction abilities and generalize to unseen events. In the meantime, we empirically show that our benchmark is comprehensive to enhance LLMs across two previous abstraction tasks.
&lt;/p&gt;</description></item><item><title>R-Spin&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#39046;&#22495;&#29305;&#23450;&#33258;&#25105;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#31163;&#25955;&#30340;&#22768;&#23398;&#21333;&#20803;&#26469;&#23454;&#29616;&#35828;&#35805;&#32773;&#21644;&#22122;&#22768;&#19981;&#21464;&#30340;&#35821;&#38899;&#34920;&#31034;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#22312;&#35745;&#31639;&#36164;&#28304;&#19978;&#20943;&#23569;12&#20493;&#21516;&#26102;&#22312;&#20005;&#37325;&#22833;&#30495;&#35821;&#38899;&#22330;&#26223;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>https://arxiv.org/abs/2311.09117</link><description>&lt;p&gt;
R-Spin: &#39640;&#25928;&#30340;&#35828;&#35805;&#32773;&#21644;&#22122;&#22768;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#19982;&#22768;&#23398;&#29255;&#27573;
&lt;/p&gt;
&lt;p&gt;
R-Spin: Efficient Speaker and Noise-invariant Representation Learning with Acoustic Pieces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09117
&lt;/p&gt;
&lt;p&gt;
R-Spin&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#39046;&#22495;&#29305;&#23450;&#33258;&#25105;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#31163;&#25955;&#30340;&#22768;&#23398;&#21333;&#20803;&#26469;&#23454;&#29616;&#35828;&#35805;&#32773;&#21644;&#22122;&#22768;&#19981;&#21464;&#30340;&#35821;&#38899;&#34920;&#31034;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#22312;&#35745;&#31639;&#36164;&#28304;&#19978;&#20943;&#23569;12&#20493;&#21516;&#26102;&#22312;&#20005;&#37325;&#22833;&#30495;&#35821;&#38899;&#22330;&#26223;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Robust Spin&#65288;R-Spin&#65289;&#65292;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;&#30340;&#39046;&#22495;&#29305;&#23450;&#30340;&#33258;&#25105;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#36890;&#36807;&#23398;&#20064;&#20855;&#26377;&#35828;&#35805;&#32773;&#19981;&#21464;&#32858;&#31867;&#65288;Spin&#65289;&#30340;&#31163;&#25955;&#22768;&#23398;&#21333;&#20803;&#26469;&#23454;&#29616;&#35828;&#35805;&#32773;&#21644;&#22122;&#22768;&#19981;&#21464;&#30340;&#35821;&#38899;&#34920;&#31034;&#12290; R-Spin&#36890;&#36807;&#23398;&#20064;&#39044;&#27979;&#22768;&#23398;&#29255;&#27573;&#35299;&#20915;&#20102;Spin&#30340;&#38382;&#39064;&#24182;&#22686;&#24378;&#20102;&#20869;&#23481;&#34920;&#31034;&#12290;&#19982;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;R-Spin&#22312;&#35745;&#31639;&#36164;&#28304;&#19978;&#23454;&#29616;&#20102;12&#20493;&#30340;&#20943;&#23569;&#65292;&#21516;&#26102;&#22312;&#20005;&#37325;&#22833;&#30495;&#30340;&#35821;&#38899;&#22330;&#26223;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#31163;&#25955;&#21333;&#20803;&#22914;&#20309;&#26377;&#21161;&#20110;&#35821;&#38899;&#32534;&#30721;&#22120;&#35757;&#32451;&#20197;&#21450;&#22312;&#22810;&#26679;&#30340;&#22768;&#23398;&#29615;&#22659;&#20013;&#25552;&#39640;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09117v2 Announce Type: replace  Abstract: This paper introduces Robust Spin (R-Spin), a data-efficient domain-specific self-supervision method for speaker and noise-invariant speech representations by learning discrete acoustic units with speaker-invariant clustering (Spin). R-Spin resolves Spin's issues and enhances content representations by learning to predict acoustic pieces. R-Spin offers a 12X reduction in computational resources compared to previous state-of-the-art methods while outperforming them in severely distorted speech scenarios. This paper provides detailed analyses to show how discrete units contribute to speech encoder training and improving robustness in diverse acoustic environments.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32447;&#24615;&#20851;&#31995;&#27010;&#24565;(LRC)&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#34255;&#28608;&#27963;&#28508;&#22312;&#31354;&#38388;&#20013;&#25214;&#21040;&#19982;&#20154;&#31867;&#21487;&#35299;&#37322;&#27010;&#24565;&#23545;&#24212;&#30340;&#27010;&#24565;&#26041;&#21521;&#65292;&#36825;&#31181;&#25216;&#26415;&#36229;&#36234;&#20102;&#26631;&#20934;&#40657;&#30418;&#25506;&#27979;&#20998;&#31867;&#22120;&#12290;</title><link>https://arxiv.org/abs/2311.08968</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35782;&#21035;&#32447;&#24615;&#20851;&#31995;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Identifying Linear Relational Concepts in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08968
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32447;&#24615;&#20851;&#31995;&#27010;&#24565;(LRC)&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#34255;&#28608;&#27963;&#28508;&#22312;&#31354;&#38388;&#20013;&#25214;&#21040;&#19982;&#20154;&#31867;&#21487;&#35299;&#37322;&#27010;&#24565;&#23545;&#24212;&#30340;&#27010;&#24565;&#26041;&#21521;&#65292;&#36825;&#31181;&#25216;&#26415;&#36229;&#36234;&#20102;&#26631;&#20934;&#40657;&#30418;&#25506;&#27979;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#35821;&#35328;&#27169;&#22411;(LMs)&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#23558;&#27010;&#24565;&#34920;&#31034;&#20026;&#38544;&#34255;&#28608;&#27963;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20219;&#20309;&#21487;&#30001;&#20154;&#31867;&#35299;&#37322;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#22914;&#20309;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#25214;&#21040;&#20854;&#26041;&#21521;&#21602;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#32447;&#24615;&#20851;&#31995;&#27010;&#24565;(LRC)&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#39318;&#20808;&#24314;&#27169;&#20027;&#20307;&#21644;&#23458;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#20026;&#32447;&#24615;&#20851;&#31995;&#23884;&#20837;(LRE)&#26469;&#25214;&#21040;&#19982;&#20154;&#31867;&#21487;&#35299;&#37322;&#27010;&#24565;&#23545;&#24212;&#30340;&#27010;&#24565;&#26041;&#21521;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21453;&#36716;LRE&#24182;&#20351;&#29992;&#36739;&#26089;&#30340;&#23458;&#20307;&#23618;&#20250;&#23548;&#33268;&#19968;&#31181;&#24378;&#22823;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#25214;&#21040;&#32988;&#36807;&#26631;&#20934;&#40657;&#30418;&#25506;&#27979;&#20998;&#31867;&#22120;&#30340;&#27010;&#24565;&#26041;&#21521;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;LRC&#20316;&#20026;&#27010;&#24565;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;&#20197;&#21450;&#23427;&#20204;&#25913;&#21464;&#27169;&#22411;&#36755;&#20986;&#30340;&#22240;&#26524;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08968v2 Announce Type: replace-cross  Abstract: Transformer language models (LMs) have been shown to represent concepts as directions in the latent space of hidden activations. However, for any human-interpretable concept, how can we find its direction in the latent space? We present a technique called linear relational concepts (LRC) for finding concept directions corresponding to human-interpretable concepts by first modeling the relation between subject and object as a linear relational embedding (LRE). We find that inverting the LRE and using earlier object layers results in a powerful technique for finding concept directions that outperforms standard black-box probing classifiers. We evaluate LRCs on their performance as concept classifiers as well as their ability to causally change model output.
&lt;/p&gt;</description></item><item><title>Safer-Instruct&#36890;&#36807;&#21453;&#21521;&#25351;&#23548;&#35843;&#25972;&#12289;&#25351;&#23548;&#24863;&#24212;&#21644;&#19987;&#23478;&#27169;&#22411;&#35780;&#20272;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#26500;&#24314;&#22823;&#35268;&#27169;&#20559;&#22909;&#25968;&#25454;&#30340;&#30446;&#30340;&#65292;&#20174;&#32780;&#22312;&#27809;&#26377;&#20154;&#24037;&#26631;&#27880;&#32773;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20559;&#22909;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2311.08685</link><description>&lt;p&gt;
Safer-Instruct: &#20351;&#29992;&#33258;&#21160;&#21270;&#20559;&#22909;&#25968;&#25454;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Safer-Instruct: Aligning Language Models with Automated Preference Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08685
&lt;/p&gt;
&lt;p&gt;
Safer-Instruct&#36890;&#36807;&#21453;&#21521;&#25351;&#23548;&#35843;&#25972;&#12289;&#25351;&#23548;&#24863;&#24212;&#21644;&#19987;&#23478;&#27169;&#22411;&#35780;&#20272;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#26500;&#24314;&#22823;&#35268;&#27169;&#20559;&#22909;&#25968;&#25454;&#30340;&#30446;&#30340;&#65292;&#20174;&#32780;&#22312;&#27809;&#26377;&#20154;&#24037;&#26631;&#27880;&#32773;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20559;&#22909;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26159;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;&#37325;&#35201;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#20026;RLHF&#26631;&#27880;&#20559;&#22909;&#25968;&#25454;&#26159;&#19968;&#39033;&#36164;&#28304;&#23494;&#38598;&#19988;&#38656;&#35201;&#21019;&#36896;&#21147;&#30340;&#36807;&#31243;&#65292;&#32780;&#29616;&#26377;&#30340;&#33258;&#21160;&#29983;&#25104;&#26041;&#27861;&#22312;&#25968;&#25454;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Safer-Instruct&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#26500;&#24314;&#22823;&#35268;&#27169;&#20559;&#22909;&#25968;&#25454;&#30340;&#20840;&#26032;&#27969;&#27700;&#32447;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#21453;&#21521;&#25351;&#23548;&#35843;&#25972;&#12289;&#25351;&#23548;&#24863;&#24212;&#21644;&#19987;&#23478;&#27169;&#22411;&#35780;&#20272;&#65292;&#20197;&#39640;&#25928;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20559;&#22909;&#25968;&#25454;&#65292;&#26080;&#38656;&#20154;&#24037;&#26631;&#27880;&#32773;&#12290;&#20026;&#20102;&#39564;&#35777;Safer-Instruct&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23558;&#35813;&#27969;&#27700;&#32447;&#24212;&#29992;&#20110;&#26500;&#24314;&#19968;&#20010;&#23433;&#20840;&#20559;&#22909;&#25968;&#25454;&#38598;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#22312;&#36825;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;Alpaca&#27169;&#22411;&#19981;&#20165;&#23637;&#31034;&#20986;&#26356;&#22909;&#30340;&#26080;&#23475;&#24615;&#65292;&#36824;&#34920;&#29616;&#20986;&#20248;&#20110;&#22312;&#20154;&#24037;&#26631;&#27880;&#30340;&#23433;&#20840;&#20559;&#22909;&#25968;&#25454;&#19978;&#24494;&#35843;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25345;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08685v2 Announce Type: replace-cross  Abstract: Reinforcement learning from human feedback (RLHF) is a vital strategy for enhancing model capability in language models. However, annotating preference data for RLHF is a resource-intensive and creativity-demanding process, while existing automatic generation methods face limitations in data diversity and quality. In response, we present Safer-Instruct, a novel pipeline for automatically constructing large-scale preference data. Our approach leverages reversed instruction tuning, instruction induction, and expert model evaluation to efficiently generate high-quality preference data without human annotators. To verify the effectiveness of Safer-Instruct, we apply the pipeline to construct a safety preference dataset as a case study. Finetuning an Alpaca model on this synthetic dataset not only demonstrates improved harmlessness but also outperforms models fine-tuned on human-annotated safety preference data, all the while mainta
&lt;/p&gt;</description></item><item><title>LoRA &#22312;&#22810;&#35821;&#35328;&#25688;&#35201;&#26041;&#38754;&#31454;&#20105;&#21147;&#24378;&#65292;&#29305;&#21035;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#21644;&#36328;&#35821;&#35328;&#36716;&#31227;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2311.08572</link><description>&lt;p&gt;
&#20302;&#31209;&#36866;&#24212;&#29992;&#20110;&#22810;&#35821;&#35328;&#25688;&#35201;&#65306;&#19968;&#39033;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Low-Rank Adaptation for Multilingual Summarization: An Empirical Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08572
&lt;/p&gt;
&lt;p&gt;
LoRA &#22312;&#22810;&#35821;&#35328;&#25688;&#35201;&#26041;&#38754;&#31454;&#20105;&#21147;&#24378;&#65292;&#29305;&#21035;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#21644;&#36328;&#35821;&#35328;&#36716;&#31227;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#26174;&#33879;&#21152;&#36895;&#20102;&#36817;&#24180;&#26469;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#36827;&#27493;&#65292;&#20294;&#23427;&#20204;&#19981;&#26029;&#22686;&#38271;&#30340;&#20307;&#31215;&#23545;&#20256;&#32479;&#30340;&#24494;&#35843;&#25552;&#20986;&#20102;&#37325;&#35201;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#20869;&#23384;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#28508;&#21147;&#65292;&#37325;&#28857;&#26159;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#65292;&#28041;&#21450;&#22810;&#35821;&#35328;&#25688;&#35201;&#39046;&#22495;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65288;&#22240;&#20026;&#36755;&#20837;&#36890;&#24120;&#24456;&#38271;&#65289;&#65292;&#19988;&#30456;&#23545;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#28085;&#30422;&#19981;&#21516;&#25968;&#25454;&#21487;&#29992;&#24615;&#22330;&#26223;&#65292;&#21253;&#25324;&#39640;&#25968;&#25454;&#21644;&#20302;&#25968;&#25454;&#35774;&#32622;&#65292;&#20197;&#21450;&#36328;&#35821;&#35328;&#36716;&#31227;&#65292;&#21033;&#29992;&#19981;&#21516;&#35268;&#27169;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#24403;&#20351;&#29992;&#22823;&#37327;&#25968;&#25454;&#35757;&#32451;&#26102;&#65292;LoRA&#19982;&#23436;&#20840;&#24494;&#35843;&#31454;&#20105;&#28608;&#28872;&#65292;&#24182;&#19988;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#21644;&#36328;&#35821;&#35328;&#36716;&#31227;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#23569;&#25968;&#25454;&#28857;&#36328;&#35821;&#35328;&#36716;&#31227;&#31574;&#30053;&#65292;&#21457;&#29616;&#25345;&#32493;&#30340;LoRA&#35843;&#20248;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08572v2 Announce Type: replace-cross  Abstract: Although the advancements of pre-trained Large Language Models have significantly accelerated recent progress in NLP, their ever-increasing size poses significant challenges for conventional fine-tuning, especially in memory-intensive tasks. We investigate the potential of Parameter-Efficient Fine-Tuning, focusing on Low-Rank Adaptation (LoRA), in the domain of multilingual summarization, a task that is both challenging (due to typically long inputs), and relatively unexplored. We conduct an extensive study across different data availability scenarios, including high- and low-data settings, and cross-lingual transfer, leveraging models of different sizes. Our findings reveal that LoRA is competitive with full fine-tuning when trained with high quantities of data, and excels in low-data scenarios and cross-lingual transfer. We also study different strategies for few-shot cross-lingual transfer, finding that continued LoRA tuning
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#25277;&#35937;&#24635;&#32467;&#20013;&#23454;&#29616;&#20844;&#24179;&#24615;&#65292;&#25552;&#20986;&#20102;&#22235;&#20010;&#26080;&#21442;&#32771;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#65292;&#35780;&#20272;&#20102;&#20061;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#22312;&#22810;&#20010;&#26469;&#28304;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2311.07884</link><description>&lt;p&gt;
&#22810;&#20803;&#35270;&#35282;&#30340;&#20844;&#24179;&#25277;&#35937;&#24635;&#32467;
&lt;/p&gt;
&lt;p&gt;
Fair Abstractive Summarization of Diverse Perspectives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07884
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#25277;&#35937;&#24635;&#32467;&#20013;&#23454;&#29616;&#20844;&#24179;&#24615;&#65292;&#25552;&#20986;&#20102;&#22235;&#20010;&#26080;&#21442;&#32771;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#65292;&#35780;&#20272;&#20102;&#20061;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#22312;&#22810;&#20010;&#26469;&#28304;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26469;&#33258;&#19981;&#21516;&#31038;&#20250;&#21644;&#20154;&#21475;&#32676;&#20307;&#30340;&#20154;&#20204;&#23545;&#20135;&#21697;&#35780;&#35770;&#12289;&#21307;&#30103;&#20445;&#20581;&#12289;&#27861;&#24459;&#21644;&#25919;&#27835;&#31561;&#19968;&#31995;&#21015;&#20027;&#39064;&#34920;&#36798;&#20102;&#22810;&#20803;&#21270;&#30340;&#35266;&#28857;&#21644;&#30456;&#20114;&#20914;&#31361;&#30340;&#24847;&#35265;&#12290; &#20844;&#24179;&#30340;&#24635;&#32467;&#24212;&#25552;&#20379;&#23545;&#22810;&#20803;&#21270;&#35266;&#28857;&#30340;&#20840;&#38754;&#35206;&#30422;&#65292;&#32780;&#19981;&#20250;&#20943;&#23569;&#26576;&#20123;&#32676;&#20307;&#30340;&#20195;&#34920;&#24615;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20851;&#20110;&#25688;&#35201;&#24230;&#37327;&#26631;&#20934;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35780;&#20272;&#30340;&#24037;&#20316;&#23578;&#26410;&#25506;&#35752;&#20844;&#24179;&#30340;&#25277;&#35937;&#24635;&#32467;&#12290; &#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#29992;&#25143;&#29983;&#25104;&#25968;&#25454;&#30340;&#20844;&#24179;&#25277;&#35937;&#24635;&#32467;&#12290; &#25105;&#20204;&#39318;&#20808;&#27491;&#24335;&#23450;&#20041;&#20102;&#25277;&#35937;&#24635;&#32467;&#20013;&#30340;&#20844;&#24179;&#24615;&#65292;&#21363;&#19981;&#20943;&#23569;&#20219;&#20309;&#32676;&#20307;&#30340;&#35266;&#28857;&#65292;&#28982;&#21518;&#36890;&#36807;&#34913;&#37327;&#30446;&#26631;&#21644;&#28304;&#35266;&#28857;&#20043;&#38388;&#30340;&#24046;&#24322;&#25552;&#20986;&#20102;&#22235;&#20010;&#26080;&#21442;&#32771;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#12290; &#25105;&#20204;&#35780;&#20272;&#20102;&#20061;&#20010;LLM&#65292;&#21253;&#25324;&#19977;&#20010;GPT&#27169;&#22411;&#65292;&#22235;&#20010;LLaMA&#27169;&#22411;&#65292;PaLM 2 &#21644; Claude&#65292;&#22312;&#26469;&#33258;&#31038;&#20132;&#23186;&#20307;&#12289;&#22312;&#32447;&#35780;&#35770;&#31561;&#20845;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07884v2 Announce Type: replace  Abstract: People from different social and demographic groups express diverse perspectives and conflicting opinions on a broad set of topics such as product reviews, healthcare, law, and politics. A fair summary should provide a comprehensive coverage of diverse perspectives without underrepresenting certain groups. However, current work in summarization metrics and Large Language Models (LLMs) evaluation has not explored fair abstractive summarization. In this paper, we systematically investigate fair abstractive summarization for user-generated data. We first formally define fairness in abstractive summarization as not underrepresenting perspectives of any groups of people, and we propose four reference-free automatic metrics by measuring the differences between target and source perspectives. We evaluate nine LLMs, including three GPT models, four LLaMA models, PaLM 2, and Claude, on six datasets collected from social media, online reviews,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22312;&#29616;&#23454;NLP&#20219;&#21153;&#21644;&#27169;&#22411;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#19982;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#20043;&#38388;&#30340;&#32852;&#31995;&#35777;&#25454;&#65292;&#25214;&#21040;&#20102;&#35780;&#20272;&#19978;&#30340;&#19981;&#36275;&#65292;&#25552;&#20986;&#20102;&#36981;&#24490;&#23618;&#27425;&#22240;&#26524;&#20851;&#31995;&#30340;&#31616;&#21333;GD&#20248;&#21270;&#31243;&#24207;&#26469;&#25913;&#21892;&#30456;&#20284;&#24615;&#20998;&#25968;&#12290;</title><link>https://arxiv.org/abs/2311.07772</link><description>&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#26799;&#24230;&#19979;&#38477;&#30340;&#20877;&#23457;&#35270;
&lt;/p&gt;
&lt;p&gt;
In-context Learning and Gradient Descent Revisited
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22312;&#29616;&#23454;NLP&#20219;&#21153;&#21644;&#27169;&#22411;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#19982;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#20043;&#38388;&#30340;&#32852;&#31995;&#35777;&#25454;&#65292;&#25214;&#21040;&#20102;&#35780;&#20272;&#19978;&#30340;&#19981;&#36275;&#65292;&#25552;&#20986;&#20102;&#36981;&#24490;&#23618;&#27425;&#22240;&#26524;&#20851;&#31995;&#30340;&#31616;&#21333;GD&#20248;&#21270;&#31243;&#24207;&#26469;&#25913;&#21892;&#30456;&#20284;&#24615;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#24050;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#28982;&#32780;&#20854;&#22522;&#26412;&#26426;&#21046;&#20173;&#26410;&#34987;&#20805;&#20998;&#29702;&#35299;&#12290;&#26368;&#36817;&#30340;&#19968;&#31995;&#21015;&#30740;&#31350;&#34920;&#26126;&#65292;ICL&#38544;&#24335;&#22320;&#25191;&#34892;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#20248;&#21270;&#12290;&#23613;&#31649;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#20294;&#24456;&#22810;&#30740;&#31350;&#38598;&#20013;&#22312;&#31616;&#21270;&#35774;&#32622;&#65292;&#20854;&#20013;&#20248;&#21270;&#27973;&#23618;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#38024;&#23545;&#29616;&#23454;NLP&#20219;&#21153;&#21644;&#27169;&#22411;&#30340;ICL-GD&#23545;&#24212;&#30340;&#35777;&#25454;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#35780;&#20272;&#26041;&#38754;&#23384;&#22312;&#24046;&#36317;&#65292;&#26080;&#35770;&#26159;&#22312;&#26377;&#38382;&#39064;&#30340;&#25351;&#26631;&#36824;&#26159;&#19981;&#36275;&#30340;&#22522;&#32447;&#26041;&#38754;&#12290;&#25105;&#20204;&#21457;&#29616;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#21363;&#20351;&#26159;&#26410;&#32463;&#35757;&#32451;&#30340;&#27169;&#22411;&#20063;&#33021;&#23454;&#29616;&#21487;&#27604;&#30340;ICL-GD&#30456;&#20284;&#24615;&#20998;&#25968;&#65292;&#23613;&#31649;&#26410;&#34920;&#29616;&#20986;ICL&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#27169;&#22411;&#20013;&#20449;&#24687;&#27969;&#21160;&#22312;ICL&#21644;GD&#20043;&#38388;&#30340;&#20027;&#35201;&#24046;&#24322;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#8220;&#23618;&#22240;&#26524;&#20851;&#31995;&#8221;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23562;&#37325;&#23618;&#22240;&#26524;&#20851;&#31995;&#30340;&#31616;&#21333;GD&#20248;&#21270;&#36807;&#31243;&#65292;&#24182;&#34920;&#26126;&#23427;&#26174;&#33879;&#25913;&#21892;&#20102;&#30456;&#20284;&#24615;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07772v4 Announce Type: replace  Abstract: In-context learning (ICL) has shown impressive results in few-shot learning tasks, yet its underlying mechanism is still not fully understood. A recent line of work suggests that ICL performs gradient descent (GD)-based optimization implicitly. While appealing, much of the research focuses on simplified settings, where the parameters of a shallow model are optimized. In this work, we revisit evidence for ICL-GD correspondence on realistic NLP tasks and models. We find gaps in evaluation, both in terms of problematic metrics and insufficient baselines. We show that surprisingly, even untrained models achieve comparable ICL-GD similarity scores despite not exhibiting ICL. Next, we explore a major discrepancy in the flow of information throughout the model between ICL and GD, which we term Layer Causality. We propose a simple GD-based optimization procedure that respects layer causality, and show it improves similarity scores significan
&lt;/p&gt;</description></item><item><title>&#28779;&#23665;&#27169;&#22411;&#36890;&#36807;&#33258;&#21453;&#39304;&#24341;&#23548;&#20462;&#35746;&#30340;&#26041;&#24335;&#65292;&#26377;&#25928;&#20943;&#23569;&#22810;&#27169;&#24577;&#24187;&#35273;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#22312;&#21508;&#39033;&#35780;&#27979;&#20013;&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2311.07362</link><description>&lt;p&gt;
&#28779;&#23665;&#65306;&#36890;&#36807;&#33258;&#21453;&#39304;&#24341;&#23548;&#20462;&#35746;&#20943;&#23569;&#22810;&#27169;&#24335;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Volcano: Mitigating Multimodal Hallucination through Self-Feedback Guided Revision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07362
&lt;/p&gt;
&lt;p&gt;
&#28779;&#23665;&#27169;&#22411;&#36890;&#36807;&#33258;&#21453;&#39304;&#24341;&#23548;&#20462;&#35746;&#30340;&#26041;&#24335;&#65292;&#26377;&#25928;&#20943;&#23569;&#22810;&#27169;&#24577;&#24187;&#35273;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#22312;&#21508;&#39033;&#35780;&#27979;&#20013;&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#23384;&#22312;&#22810;&#27169;&#24577;&#24187;&#35273;&#38382;&#39064;&#65292;&#21363;&#25552;&#20379;&#19982;&#32473;&#23450;&#35270;&#35273;&#20449;&#24687;&#19981;&#31526;&#30340;&#38169;&#35823;&#21709;&#24212;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25512;&#27979;&#65292;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#21407;&#22240;&#20043;&#19968;&#26159;&#35270;&#35273;&#32534;&#30721;&#22120;&#26410;&#33021;&#27491;&#30830;&#22320;&#19982;&#22270;&#20687;&#23545;&#40784;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#21453;&#39304;&#20316;&#20026;&#35270;&#35273;&#32447;&#32034;&#30340;&#26032;&#26041;&#27861;&#12290;&#22522;&#20110;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#28779;&#23665;&#65292;&#19968;&#20010;&#22810;&#27169;&#24577;&#33258;&#21453;&#39304;&#24341;&#23548;&#20462;&#35746;&#27169;&#22411;&#12290;&#28779;&#23665;&#26681;&#25454;&#25552;&#20379;&#30340;&#35270;&#35273;&#20449;&#24687;&#20026;&#20854;&#21021;&#22987;&#21709;&#24212;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#65292;&#24182;&#21033;&#29992;&#27492;&#21453;&#39304;&#26469;&#33258;&#25105;&#20462;&#35746;&#20854;&#21021;&#22987;&#21709;&#24212;&#12290;&#28779;&#23665;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#22810;&#27169;&#24577;&#24187;&#35273;&#65292;&#24182;&#22312;MMHal-Bench&#12289;POPE&#21644;GAVIE&#19978;&#23454;&#29616;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;&#23427;&#36824;&#25552;&#39640;&#20102;&#19968;&#33324;&#22810;&#27169;&#24577;&#33021;&#21147;&#65292;&#24182;&#22312;MM-Vet&#21644;MMBench&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#27169;&#22411;&#12290;&#36890;&#36807;&#23450;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#28779;&#23665;&#30340;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07362v3 Announce Type: replace  Abstract: Large multimodal models suffer from multimodal hallucination, where they provide incorrect responses misaligned with the given visual information. Recent works have conjectured that one of the reasons behind multimodal hallucination is due to the vision encoder failing to ground on the image properly. To mitigate this issue, we propose a novel approach that leverages self-feedback as visual cues. Building on this approach, we introduce Volcano, a multimodal self-feedback guided revision model. Volcano generates natural language feedback to its initial response based on the provided visual information and utilizes this feedback to self-revise its initial response. Volcano effectively reduces multimodal hallucination and achieves state-of-the-art on MMHal-Bench, POPE, and GAVIE. It also improves on general multimodal abilities and outperforms previous models on MM-Vet and MMBench. Through qualitative analysis, we show that Volcano's fe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#29702;&#35299;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35805;&#24635;&#32467;&#20219;&#21153;&#26816;&#27979;&#20107;&#23454;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#29992;&#20107;&#23454;&#38382;&#39064;&#26469;&#20316;&#20026;&#23545;&#35805;&#29702;&#35299;&#33021;&#21147;&#30340;&#28789;&#27963;&#34913;&#37327;&#26631;&#20934;&#65292;&#32467;&#26524;&#26174;&#31034;&#24179;&#22343;&#32422;&#26377;26.8%&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25688;&#35201;&#23384;&#22312;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.07194</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#29702;&#35299;&#20013;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring the Factual Consistency in Dialogue Comprehension of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07194
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#29702;&#35299;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35805;&#24635;&#32467;&#20219;&#21153;&#26816;&#27979;&#20107;&#23454;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#29992;&#20107;&#23454;&#38382;&#39064;&#26469;&#20316;&#20026;&#23545;&#35805;&#29702;&#35299;&#33021;&#21147;&#30340;&#28789;&#27963;&#34913;&#37327;&#26631;&#20934;&#65292;&#32467;&#26524;&#26174;&#31034;&#24179;&#22343;&#32422;&#26377;26.8%&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25688;&#35201;&#23384;&#22312;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLMs&#65288;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#36890;&#24120;&#20197;&#23545;&#35805;&#30340;&#24418;&#24335;&#19982;&#29992;&#25143;&#20132;&#20114;&#65292;&#24182;&#26681;&#25454;&#20854;&#25351;&#20196;&#29983;&#25104;&#22238;&#22797;&#65292;&#36825;&#33258;&#28982;&#38656;&#35201;&#23545;&#35805;&#29702;&#35299;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#35805;&#29702;&#35299;&#26159;&#19968;&#31181;&#38590;&#20197;&#30452;&#25509;&#35780;&#20272;&#30340;&#36890;&#29992;&#35821;&#35328;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#23545;&#35805;&#24635;&#32467;&#20219;&#21153;&#30340;&#24110;&#21161;&#65292;&#38598;&#20013;&#36827;&#34892;&#23545;&#20107;&#23454;&#19968;&#33268;&#24615;&#38382;&#39064;&#30340;&#35780;&#20272;&#12290;&#38500;&#20102;&#35780;&#20272;&#21644;&#20998;&#26512;&#19981;&#21516;LLMs&#30340;&#23545;&#35805;&#24635;&#32467;&#24615;&#33021;&#65288;DIAC-Sum&#65289;&#22806;&#65292;&#25105;&#20204;&#36824;&#20174;&#29983;&#25104;&#30340;&#25688;&#35201;&#20013;&#25552;&#21462;&#20107;&#23454;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#29992;&#20316;&#23545;&#35805;&#29702;&#35299;&#30340;&#26356;&#28789;&#27963;&#30340;&#27979;&#37327;&#65288;DIAC-QA&#65289;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#26174;&#31034;&#65292;&#24179;&#22343;&#32780;&#35328;&#65292;&#30001;LLMs&#29983;&#25104;&#30340;&#25688;&#35201;&#20013;&#26377;26.8%&#21253;&#21547;&#20107;&#23454;&#19981;&#19968;&#33268;&#12290;&#21363;&#20351;&#26159;ChatGPT&#65292;&#35780;&#20272;&#20013;&#26368;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;&#22312;&#20854;&#25688;&#35201;&#20013;&#20063;&#26377;16%&#23384;&#22312;&#27492;&#31867;&#38169;&#35823;&#12290;&#23545;&#20110;&#22238;&#31572;&#20107;&#23454;&#38382;&#39064;&#65292;&#36825;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07194v3 Announce Type: replace  Abstract: LLMs (Large Language Models) usually interact with users in the form of dialogue and generate responses following their instructions, which naturally require dialogue comprehension abilities. However, dialogue comprehension is a general language ability which is hard to be evaluated directly. In this work, we propose to perform the evaluation focusing on the factual consistency issue with the help of the dialogue summarization task. Besides evaluating and analyzing the dialogue summarization performance (DIAC-Sum) of different LLMs, we also derive factual questions from the generated summaries and use them as a more flexible measurement of dialogue comprehension (DIAC-QA). Our evaluation shows that, on average, 26.8% of the summaries generated by LLMs contain factual inconsistency. Even ChatGPT, the strongest model evaluated, has such errors in 16% of its summaries. For answering the factual questions, which is more challenging, the 
&lt;/p&gt;</description></item><item><title>&#20013;&#22269;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20215;&#20540;&#35266;&#22865;&#21512;&#24615;&#38656;&#35201;&#26356;&#21152;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"&#28779;&#28976;"&#65288;Flames&#65289;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;&#24120;&#35265;&#30340;&#26080;&#23475;&#21407;&#21017;&#21644;&#29305;&#23450;&#20013;&#22269;&#20215;&#20540;&#35266;&#65292;&#20197;&#21450;&#22797;&#26434;&#22330;&#26223;&#21644;&#38544;&#21547;&#24694;&#24847;&#30340;&#25552;&#31034;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2311.06899</link><description>&lt;p&gt;
&#28779;&#28976;: &#35780;&#20272;&#20013;&#22269;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#22865;&#21512;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Flames: Benchmarking Value Alignment of Chinese Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06899
&lt;/p&gt;
&lt;p&gt;
&#20013;&#22269;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20215;&#20540;&#35266;&#22865;&#21512;&#24615;&#38656;&#35201;&#26356;&#21152;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"&#28779;&#28976;"&#65288;Flames&#65289;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;&#24120;&#35265;&#30340;&#26080;&#23475;&#21407;&#21017;&#21644;&#29305;&#23450;&#20013;&#22269;&#20215;&#20540;&#35266;&#65292;&#20197;&#21450;&#22797;&#26434;&#22330;&#26223;&#21644;&#38544;&#21547;&#24694;&#24847;&#30340;&#25552;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#22320;&#21306;&#30340;&#24191;&#27867;&#24212;&#29992;&#24378;&#35843;&#20102;&#35780;&#20272;&#23427;&#20204;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#22865;&#21512;&#24615;&#30340;&#36843;&#20999;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#26410;&#33021;&#26377;&#25928;&#22320;&#25581;&#31034;LLMs&#20013;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;&#23613;&#31649;&#35768;&#22810;&#27169;&#22411;&#22312;&#36825;&#20123;&#35780;&#20272;&#20013;&#24471;&#20998;&#24456;&#39640;&#65292;&#19988;&#8220;&#21517;&#21015;&#21069;&#33541;&#8221;&#65292;&#20294;&#22312;LLMs&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#28145;&#23618;&#22865;&#21512;&#24615;&#21644;&#23454;&#29616;&#30495;&#27491;&#26080;&#23475;&#26041;&#38754;&#20173;&#23384;&#22312;&#37325;&#22823;&#24046;&#36317;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"&#28779;&#28976;"&#65288;Flames&#65289;&#30340;&#20215;&#20540;&#35266;&#22865;&#21512;&#24615;&#22522;&#20934;&#27979;&#35797;&#65292;&#35813;&#27979;&#35797;&#28085;&#30422;&#20102;&#24120;&#35265;&#30340;&#26080;&#23475;&#21407;&#21017;&#65292;&#20197;&#21450;&#19968;&#20010;&#25972;&#21512;&#20102;&#29305;&#23450;&#20013;&#22269;&#20215;&#20540;&#35266;&#22914;&#21644;&#35856;&#30340;&#29420;&#29305;&#36947;&#24503;&#32500;&#24230;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#20102;&#21253;&#21547;&#22797;&#26434;&#24773;&#22659;&#21644;&#22823;&#22810;&#24102;&#26377;&#38544;&#21547;&#24694;&#24847;&#30340;&#30772;&#35299;&#26041;&#27861;&#30340;&#23545;&#25239;&#24615;&#25552;&#31034;&#12290;&#36890;&#36807;&#23545;17&#20010;&#20027;&#27969;LLMs&#36827;&#34892;&#25552;&#31034;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#27169;&#22411;&#30340;&#22238;&#24212;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#35814;&#32454;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.06899v2 Announce Type: replace-cross  Abstract: The widespread adoption of large language models (LLMs) across various regions underscores the urgent need to evaluate their alignment with human values. Current benchmarks, however, fall short of effectively uncovering safety vulnerabilities in LLMs. Despite numerous models achieving high scores and 'topping the chart' in these evaluations, there is still a significant gap in LLMs' deeper alignment with human values and achieving genuine harmlessness. To this end, this paper proposes a value alignment benchmark named Flames, which encompasses both common harmlessness principles and a unique morality dimension that integrates specific Chinese values such as harmony. Accordingly, we carefully design adversarial prompts that incorporate complex scenarios and jailbreaking methods, mostly with implicit malice. By prompting 17 mainstream LLMs, we obtain model responses and rigorously annotate them for detailed evaluation. Our findin
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23433;&#20840;&#24615;&#35780;&#20272;&#20013;&#23384;&#22312;&#20551;&#23545;&#40784;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;Fake alIgNment Evaluation (FINE)&#26694;&#26550;&#21644;&#20004;&#20010;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20110;&#39564;&#35777;&#21644;&#20462;&#27491;&#36825;&#19968;&#29616;&#35937;</title><link>https://arxiv.org/abs/2311.05915</link><description>&lt;p&gt;
&#20551;&#23545;&#40784;&#65306;LLMs&#30495;&#30340;&#23545;&#40784;&#24471;&#22909;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Fake Alignment: Are LLMs Really Aligned Well?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.05915
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23433;&#20840;&#24615;&#35780;&#20272;&#20013;&#23384;&#22312;&#20551;&#23545;&#40784;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;Fake alIgNment Evaluation (FINE)&#26694;&#26550;&#21644;&#20004;&#20010;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20110;&#39564;&#35777;&#21644;&#20462;&#27491;&#36825;&#19968;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23433;&#20840;&#38382;&#39064;&#30340;&#20851;&#27880;&#26085;&#30410;&#22686;&#24378;&#65292;&#20154;&#20204;&#23545;&#23433;&#20840;&#35780;&#20272;&#20135;&#29983;&#20102;&#30456;&#24403;&#22823;&#30340;&#20852;&#36259;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35780;&#20272;LLMs&#30340;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#30340;&#38382;&#39064;&#65292;&#21363;&#22810;&#39033;&#36873;&#25321;&#39064;&#21644;&#24320;&#25918;&#24335;&#38382;&#39064;&#20043;&#38388;&#30340;&#34920;&#29616;&#24046;&#24322;&#12290;&#21463;&#21040;&#36234;&#29425;&#25915;&#20987;&#27169;&#24335;&#30740;&#31350;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#30001;&#20110;&#27867;&#21270;&#19981;&#21305;&#37197;&#25152;&#24341;&#36215;&#30340;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;LLMs&#21482;&#35760;&#20303;&#20102;&#24320;&#25918;&#24335;&#23433;&#20840;&#38382;&#39064;&#30340;&#31572;&#26696;&#39118;&#26684;&#65292;&#36825;&#20351;&#20854;&#26080;&#27861;&#35299;&#20915;&#20854;&#20182;&#24418;&#24335;&#30340;&#23433;&#20840;&#27979;&#35797;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#29616;&#35937;&#31216;&#20026;&#20551;&#23545;&#40784;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#27604;&#36739;&#22522;&#20934;&#26469;&#22312;LLMs&#20013;&#32463;&#39564;&#24615;&#22320;&#39564;&#35777;&#20854;&#23384;&#22312;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;Fake alIgNment Evaluation (FINE)&#26694;&#26550;&#21644;&#20004;&#20010;&#26032;&#39062;&#30340;&#24230;&#37327;&#26631;&#20934;--&#19968;&#33268;&#24615;&#20998;&#25968;&#65288;CS&#65289;&#21644;&#19968;&#33268;&#30340;&#23433;&#20840;&#20998;&#25968;&#65288;CSS&#65289;&#65292;&#23427;&#20204;&#20849;&#21516;&#35780;&#20272;&#20004;&#31181;&#20114;&#34917;&#24418;&#24335;&#30340;&#35780;&#20272;&#65292;&#20197;&#37327;&#21270;&#20551;&#23545;&#40784;&#24182;&#33719;&#24471;&#26356;&#27491;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.05915v3 Announce Type: replace-cross  Abstract: The growing awareness of safety concerns in large language models (LLMs) has sparked considerable interest in the evaluation of safety. This study investigates an under-explored issue about the evaluation of LLMs, namely the substantial discrepancy in performance between multiple-choice questions and open-ended questions. Inspired by research on jailbreak attack patterns, we argue this is caused by mismatched generalization. That is, LLM only remembers the answer style for open-ended safety questions, which makes it unable to solve other forms of safety tests. We refer to this phenomenon as fake alignment and construct a comparative benchmark to empirically verify its existence in LLMs. We introduce a Fake alIgNment Evaluation (FINE) framework and two novel metrics--Consistency Score (CS) and Consistent Safety Score (CSS), which jointly assess two complementary forms of evaluation to quantify fake alignment and obtain corrected
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#30340;Large Language Models&#22312;&#35782;&#21035;&#20167;&#24680;&#35328;&#35770;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#25581;&#31034;&#20102;&#21363;&#20351;&#27809;&#26377;&#39044;&#35757;&#32451;&#65292;LLMs&#22312;&#24615;&#33021;&#19978;&#20173;&#28982;&#20855;&#26377;&#26497;&#22823;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2310.18964</link><description>&lt;p&gt;
LLMs&#19982;Fine-tuning: &#23545;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#36328;&#39046;&#22495;&#24615;&#33021;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
LLMs and Finetuning: Benchmarking cross-domain performance for hate speech detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.18964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#30340;Large Language Models&#22312;&#35782;&#21035;&#20167;&#24680;&#35328;&#35770;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#25581;&#31034;&#20102;&#21363;&#20351;&#27809;&#26377;&#39044;&#35757;&#32451;&#65292;LLMs&#22312;&#24615;&#33021;&#19978;&#20173;&#28982;&#20855;&#26377;&#26497;&#22823;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#20132;&#27969;&#19981;&#26029;&#21457;&#23637;&#30340;&#29615;&#22659;&#20013;&#65292;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20173;&#28982;&#26159;&#19968;&#20010;&#20005;&#23803;&#30340;&#25361;&#25112;&#65292;&#25968;&#23383;&#24179;&#21488;&#30340;&#22810;&#26679;&#24615;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#36825;&#19968;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35782;&#21035;&#20167;&#24680;&#35328;&#35770;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#20197;&#35299;&#20915;&#20004;&#20010;&#26680;&#24515;&#38382;&#39064;&#65306;&#65288;1&#65289;&#27169;&#22411;&#24615;&#33021;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#24494;&#35843;&#21644;&#35757;&#32451;&#21442;&#25968;&#65311;&#65288;2&#65289;&#27169;&#22411;&#22312;&#36328;&#39046;&#22495;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20013;&#30340;&#27867;&#21270;&#31243;&#24230;&#22914;&#20309;&#65311;&#20197;&#21450;&#65288;3&#65289;&#24433;&#21709;&#27867;&#21270;&#28508;&#21147;&#30340;&#25968;&#25454;&#38598;&#25110;&#27169;&#22411;&#30340;&#20855;&#20307;&#29305;&#24449;&#26159;&#20160;&#20040;&#65311;&#23454;&#39564;&#35777;&#26126;&#65292;&#21363;&#20351;&#27809;&#26377;&#39044;&#35757;&#32451;&#65292;LLMs&#20063;&#27604;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20855;&#26377;&#24040;&#22823;&#20248;&#21183;&#12290;&#20026;&#20102;&#22238;&#31572;&#38382;&#39064;&#65288;1&#65289;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;36&#20010;&#39046;&#22495;&#20869;&#20998;&#31867;&#22120;&#65292;&#28085;&#30422;&#20102;LLaMA&#12289;Vicuna&#21450;&#20854;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#29366;&#24577;&#65292;&#36328;&#36234;&#20102;&#20061;&#20010;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.18964v2 Announce Type: replace  Abstract: In the evolving landscape of online communication, hate speech detection remains a formidable challenge, further compounded by the diversity of digital platforms. This study investigates the effectiveness and adaptability of pre-trained and fine-tuned Large Language Models (LLMs) in identifying hate speech, to address two central questions: (1) To what extent does the model performance depend on the fine-tuning and training parameters?, (2) To what extent do models generalize to cross-domain hate speech detection? and (3) What are the specific features of the datasets or models that influence the generalization potential? The experiment shows that LLMs offer a huge advantage over the state-of-the-art even without pretraining. To answer (1) we analyze 36 in-domain classifiers comprising LLaMA, Vicuna, and their variations in pre-trained and fine-tuned states across nine publicly available datasets that span a wide range of platforms a
&lt;/p&gt;</description></item><item><title>DistillSpec&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#25913;&#36827;&#20102;&#25237;&#26426;&#24615;&#35299;&#30721;&#65292;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;10-45%&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2310.08461</link><description>&lt;p&gt;
DistillSpec&#65306;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#25913;&#36827;&#25237;&#26426;&#24615;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
DistillSpec: Improving Speculative Decoding via Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.08461
&lt;/p&gt;
&lt;p&gt;
DistillSpec&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#25913;&#36827;&#20102;&#25237;&#26426;&#24615;&#35299;&#30721;&#65292;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;10-45%&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25237;&#26426;&#24615;&#35299;&#30721;&#65288;SD&#65289;&#36890;&#36807;&#20351;&#29992;&#26356;&#24555;&#30340;&#33609;&#31295;&#27169;&#22411;&#29983;&#25104;&#22810;&#20010;&#26631;&#35760;&#65292;&#28982;&#21518;&#30001;&#26356;&#22823;&#30340;&#30446;&#26631;&#27169;&#22411;&#24182;&#34892;&#39564;&#35777;&#36825;&#20123;&#26631;&#35760;&#65292;&#20174;&#32780;&#29983;&#25104;&#31526;&#21512;&#30446;&#26631;&#27169;&#22411;&#20998;&#24067;&#30340;&#25991;&#26412;&#65292;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#25214;&#21040;&#19982;&#30446;&#26631;&#27169;&#22411;&#33391;&#22909;&#23545;&#40784;&#30340;&#32039;&#20945;&#33609;&#31295;&#27169;&#22411;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DistillSpec&#65292;&#23427;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#26469;&#26356;&#22909;&#22320;&#23558;&#33609;&#31295;&#27169;&#22411;&#19982;&#30446;&#26631;&#27169;&#22411;&#23545;&#40784;&#65292;&#28982;&#21518;&#24212;&#29992;SD&#12290;DistillSpec&#20570;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#35774;&#35745;&#36873;&#25321;&#65292;&#25105;&#20204;&#36890;&#36807;&#31995;&#32479;&#30740;&#31350;&#35777;&#26126;&#36825;&#23545;&#25913;&#36827;&#33609;&#31295;&#21644;&#30446;&#26631;&#23545;&#40784;&#33267;&#20851;&#37325;&#35201;&#65306;&#21033;&#29992;&#26469;&#33258;&#33609;&#31295;&#27169;&#22411;&#30340;on-policy&#25968;&#25454;&#29983;&#25104;&#65292;&#20197;&#21450;&#23558;&#21457;&#25955;&#20989;&#25968;&#23450;&#21046;&#21040;&#20219;&#21153;&#21644;&#35299;&#30721;&#31574;&#30053;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;DistillSpec&#22312;&#19968;&#31995;&#21015;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#19978;&#27604;&#26631;&#20934;SD&#33719;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;10-45%&#30340;&#21152;&#36895;&#65292;&#20351;&#29992;&#36138;&#23146;&#21644;&#38750;&#36138;&#23146;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.08461v2 Announce Type: replace-cross  Abstract: Speculative decoding (SD) accelerates large language model inference by employing a faster draft model for generating multiple tokens, which are then verified in parallel by the larger target model, resulting in the text generated according to the target model distribution. However, identifying a compact draft model that is well-aligned with the target model is challenging. To tackle this issue, we propose DistillSpec that uses knowledge distillation to better align the draft model with the target model, before applying SD. DistillSpec makes two key design choices, which we demonstrate via systematic study to be crucial to improving the draft and target alignment: utilizing on-policy data generation from the draft model, and tailoring the divergence function to the task and decoding strategy. Notably, DistillSpec yields impressive 10 - 45% speedups over standard SD on a range of standard benchmarks, using both greedy and non-gr
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#22797;&#21512;&#21518;&#38376;&#25915;&#20987;(CBA)&#23637;&#31034;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26893;&#20837;&#22810;&#20010;&#35302;&#21457;&#20851;&#38190;&#35789;&#30340;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#26041;&#27861;&#26356;&#20026;&#38544;&#34109;&#65292;&#24182;&#30830;&#20445;&#21482;&#26377;&#24403;&#25152;&#26377;&#35302;&#21457;&#20851;&#38190;&#35789;&#21516;&#26102;&#20986;&#29616;&#26102;&#21518;&#38376;&#25165;&#20250;&#34987;&#28608;&#27963;&#12290;</title><link>https://arxiv.org/abs/2310.07676</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22797;&#21512;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Composite Backdoor Attacks Against Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.07676
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#22797;&#21512;&#21518;&#38376;&#25915;&#20987;(CBA)&#23637;&#31034;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26893;&#20837;&#22810;&#20010;&#35302;&#21457;&#20851;&#38190;&#35789;&#30340;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#26041;&#27861;&#26356;&#20026;&#38544;&#34109;&#65292;&#24182;&#30830;&#20445;&#21482;&#26377;&#24403;&#25152;&#26377;&#35302;&#21457;&#20851;&#38190;&#35789;&#21516;&#26102;&#20986;&#29616;&#26102;&#21518;&#38376;&#25165;&#20250;&#34987;&#28608;&#27963;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#36890;&#24120;&#20316;&#20026;&#35768;&#22810;&#30740;&#31350;&#21644;&#26381;&#21153;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#19981;&#21487;&#20449;&#20219;&#30340;&#31532;&#19977;&#26041;LLMs&#21487;&#33021;&#20250;&#26263;&#20013;&#20026;&#19979;&#28216;&#20219;&#21153;&#24341;&#20837;&#28431;&#27934;&#12290;&#26412;&#25991;&#36890;&#36807;&#21518;&#38376;&#25915;&#20987;&#30340;&#35270;&#35282;&#25506;&#35752;&#20102;LLMs&#30340;&#33030;&#24369;&#24615;&#12290;&#19982;&#29616;&#26377;&#30340;&#23545;LLMs&#30340;&#21518;&#38376;&#25915;&#20987;&#19981;&#21516;&#65292;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;&#25552;&#31034;&#32452;&#20214;&#20013;&#20998;&#25955;&#22810;&#20010;&#35302;&#21457;&#20851;&#38190;&#35789;&#12290;&#36825;&#31181;&#22797;&#21512;&#21518;&#38376;&#25915;&#20987;&#65288;CBA&#65289;&#34987;&#35777;&#26126;&#27604;&#20165;&#22312;&#21333;&#20010;&#32452;&#20214;&#20013;&#26893;&#20837;&#30456;&#21516;&#30340;&#22810;&#20010;&#35302;&#21457;&#20851;&#38190;&#35789;&#26356;&#38544;&#34109;&#12290;CBA&#30830;&#20445;&#21482;&#26377;&#24403;&#25152;&#26377;&#35302;&#21457;&#20851;&#38190;&#35789;&#20986;&#29616;&#26102;&#21518;&#38376;&#25165;&#20250;&#34987;&#28608;&#27963;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;CBA&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#22810;&#27169;&#24335;&#20219;&#21153;&#20013;&#37117;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.07676v2 Announce Type: replace-cross  Abstract: Large language models (LLMs) have demonstrated superior performance compared to previous methods on various tasks, and often serve as the foundation models for many researches and services. However, the untrustworthy third-party LLMs may covertly introduce vulnerabilities for downstream tasks. In this paper, we explore the vulnerability of LLMs through the lens of backdoor attacks. Different from existing backdoor attacks against LLMs, ours scatters multiple trigger keys in different prompt components. Such a Composite Backdoor Attack (CBA) is shown to be stealthier than implanting the same multiple trigger keys in only a single component. CBA ensures that the backdoor is activated only when all trigger keys appear. Our experiments demonstrate that CBA is effective in both natural language processing (NLP) and multimodal tasks. For instance, with $3\%$ poisoning samples against the LLaMA-7B model on the Emotion dataset, our att
&lt;/p&gt;</description></item><item><title>&#35757;&#32451;&#21160;&#24577;&#21487;&#22312;&#19981;&#21516;&#27169;&#22411;&#22823;&#23567;&#21644;&#39044;&#35757;&#32451;&#26041;&#27861;&#20043;&#38388;&#36827;&#34892;&#36716;&#31227;&#65292;&#36890;&#36807;&#36873;&#23450;&#30340;&#35757;&#32451;&#23454;&#20363;&#24494;&#35843;&#20027;&#27169;&#22411;&#23454;&#29616;&#27604;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#26356;&#39640;&#30340;&#35757;&#32451;&#25928;&#29575;</title><link>https://arxiv.org/abs/2310.06588</link><description>&lt;p&gt;
FTFT:&#36890;&#36807;&#36716;&#31227;&#35757;&#32451;&#21160;&#24577;&#23454;&#29616;&#39640;&#25928;&#19988;&#31283;&#20581;&#30340;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
FTFT: Efficient and Robust Fine-Tuning by Transferring Training Dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.06588
&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#21160;&#24577;&#21487;&#22312;&#19981;&#21516;&#27169;&#22411;&#22823;&#23567;&#21644;&#39044;&#35757;&#32451;&#26041;&#27861;&#20043;&#38388;&#36827;&#34892;&#36716;&#31227;&#65292;&#36890;&#36807;&#36873;&#23450;&#30340;&#35757;&#32451;&#23454;&#20363;&#24494;&#35843;&#20027;&#27169;&#22411;&#23454;&#29616;&#27604;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#26356;&#39640;&#30340;&#35757;&#32451;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#20998;&#24067;&#22806;&#36755;&#20837;&#30340;&#24433;&#21709;&#12290; &#25968;&#25454;&#38598;&#21046;&#22270;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21452;&#27169;&#22411;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#24494;&#35843;PLMs&#30340;&#40065;&#26834;&#24615;&#12290; &#23427;&#28041;&#21450;&#22312;&#21407;&#22987;&#35757;&#32451;&#38598;&#19978;&#24494;&#35843;&#27169;&#22411;&#65288;&#21363;&#21442;&#32771;&#27169;&#22411;&#65289;&#65292;&#26681;&#25454;&#35757;&#32451;&#21160;&#24577;&#36873;&#25321;&#19968;&#20123;&#37325;&#35201;&#30340;&#35757;&#32451;&#23454;&#20363;&#65292;&#24182;&#20165;&#23545;&#36825;&#20123;&#36873;&#23450;&#30340;&#31034;&#20363;&#20877;&#27425;&#36827;&#34892;&#24494;&#35843;&#65288;&#21363;&#20027;&#27169;&#22411;&#65289;&#12290; &#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#38656;&#35201;&#23545;&#21516;&#19968;&#27169;&#22411;&#36827;&#34892;&#20004;&#27425;&#24494;&#35843;&#65292;&#36825;&#23545;&#20110;&#22823;&#22411;PLMs&#32780;&#35328;&#22312;&#35745;&#31639;&#19978;&#26159;&#26114;&#36149;&#30340;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#65288;1&#65289;&#35757;&#32451;&#21160;&#24577;&#22312;&#27169;&#22411;&#22823;&#23567;&#21644;&#39044;&#35757;&#32451;&#26041;&#27861;&#20043;&#38388;&#20855;&#26377;&#39640;&#24230;&#21487;&#20256;&#36882;&#24615;&#65292;&#20197;&#21450;&#65288;2&#65289;&#20351;&#29992;&#36825;&#20123;&#36873;&#23450;&#30340;&#35757;&#32451;&#23454;&#20363;&#23545;&#20027;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#27604;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#23454;&#29616;&#26356;&#39640;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290; &#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24494;&#35843;&#26041;&#27861;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.06588v2 Announce Type: replace  Abstract: Despite the massive success of fine-tuning Pre-trained Language Models (PLMs), they remain susceptible to out-of-distribution input. Dataset cartography is a simple yet effective dual-model approach that improves the robustness of fine-tuned PLMs. It involves fine-tuning a model on the original training set (i.e. reference model), selecting a subset of important training instances based on the training dynamics, and fine-tuning again only on these selected examples (i.e. main model). However, this approach requires fine-tuning the same model twice, which is computationally expensive for large PLMs. In this paper, we show that (1) training dynamics are highly transferable across model sizes and pre-training methods, and that (2) fine-tuning main models using these selected training instances achieves higher training efficiency than empirical risk minimization (ERM). Building on these observations, we propose a novel fine-tuning approa
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#29702;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20379;&#20102;&#21517;&#20026;LogiGLUE&#30340;&#22522;&#20934;&#65292;&#20197;&#30740;&#31350;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#12289;&#20219;&#21153;&#21644;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2310.00836</link><description>&lt;p&gt;
&#36808;&#21521;LogiGLUE&#65306;&#23545;&#35821;&#35328;&#27169;&#22411;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#30340;&#31616;&#35201;&#35843;&#26597;&#21644;&#22522;&#20934;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing Logical Reasoning Capabilities of Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.00836
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#29702;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20379;&#20102;&#21517;&#20026;LogiGLUE&#30340;&#22522;&#20934;&#65292;&#20197;&#30740;&#31350;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#12289;&#20219;&#21153;&#21644;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36923;&#36753;&#25512;&#29702;&#23545;&#20154;&#31867;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#21364;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#30740;&#31350;&#20154;&#21592;&#26368;&#21021;&#20351;&#29992;&#30340;&#30693;&#35782;&#34920;&#31034;&#19982;&#25512;&#29702;&#31995;&#32479;&#24182;&#19981;&#33021;&#24456;&#22909;&#22320;&#25193;&#23637;&#65292;&#24182;&#19988;&#38656;&#35201;&#22823;&#37327;&#25163;&#21160;&#24037;&#20316;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#20986;&#29616;&#34920;&#26126;&#20102;&#20854;&#33021;&#22815;&#20811;&#26381;&#24418;&#24335;&#21270;&#30693;&#35782;&#34920;&#31034;&#31995;&#32479;&#30340;&#21508;&#31181;&#23616;&#38480;&#24615;&#12290;&#22240;&#27492;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;&#30340;LLMs&#22791;&#21463;&#20851;&#27880;&#12290;&#35813;&#24037;&#20316;&#26088;&#22312;&#36890;&#36807;&#23545;&#27492;&#39046;&#22495;&#26368;&#26032;&#36827;&#23637;&#30340;&#31616;&#35201;&#22238;&#39038;&#26469;&#20102;&#35299;LLMs&#22312;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#65307;&#37325;&#28857;&#20851;&#27880;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#12289;&#20219;&#21153;&#20197;&#21450;&#37319;&#29992;&#30340;&#21033;&#29992;LLMs&#36827;&#34892;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#25552;&#20379;&#20840;&#38754;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#32534;&#21046;&#20102;&#19968;&#20010;&#21517;&#20026;LogiGLUE&#30340;&#22522;&#20934;&#12290;&#20854;&#20013;&#21253;&#25324;24&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#28436;&#32462;&#12289;&#36827;&#38454;&#21644;&#24402;&#32435;&#25512;&#29702;&#31561;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.00836v3 Announce Type: replace-cross  Abstract: Logical reasoning is fundamental for humans yet presents a substantial challenge in the domain of Artificial Intelligence. Initially, researchers used Knowledge Representation and Reasoning (KR) systems that did not scale and required non-trivial manual effort. Recently, the emergence of large language models (LLMs) has demonstrated the ability to overcome various limitations of formal Knowledge Representation (KR) systems. Consequently, there's a growing interest in using LLMs for logical reasoning via natural language. This work strives to understand the proficiency of LLMs in logical reasoning by offering a brief review of the latest progress in this area; with a focus on the logical reasoning datasets, tasks, and the methods adopted to utilize LLMs for reasoning. To offer a thorough analysis, we have compiled a benchmark titled LogiGLUE. This includes 24 varied datasets encompassing deductive, abductive, and inductive reaso
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26816;&#39564;&#20102;&#29983;&#25104;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#21512;&#24615;&#65292;&#21457;&#29616;&#24403;&#21069;&#22522;&#20934;&#20013;&#23384;&#22312;&#21477;&#27861;&#20559;&#35265;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;SyntaxBias Score&#21644;&#26032;&#30340;&#22522;&#20934;SyntActically D&#12290;</title><link>https://arxiv.org/abs/2308.10509</link><description>&lt;p&gt;
&#23545;&#22823;&#35268;&#27169;&#29983;&#25104;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#21512;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Examination of the Compositionality of Large Generative Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.10509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26816;&#39564;&#20102;&#29983;&#25104;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#21512;&#24615;&#65292;&#21457;&#29616;&#24403;&#21069;&#22522;&#20934;&#20013;&#23384;&#22312;&#21477;&#27861;&#20559;&#35265;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;SyntaxBias Score&#21644;&#26032;&#30340;&#22522;&#20934;SyntActically D&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25104;&#21151;&#65292;&#35768;&#22810;&#29983;&#25104;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;GVLMs&#65289;&#36890;&#36807;&#22810;&#27169;&#24577;&#25351;&#23548;&#35843;&#25972;&#24471;&#20197;&#26500;&#24314;&#12290;&#28982;&#32780;&#65292;GVLMs&#22312;&#22810;&#27169;&#24577;&#32452;&#21512;&#25512;&#29702;&#20013;&#30340;&#24615;&#33021;&#36824;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#26088;&#22312;&#26816;&#39564;&#35780;&#20272;GVLMs&#32452;&#21512;&#24615;&#30340;&#35780;&#20272;&#25351;&#26631;&#65288;VisualGPTScore&#31561;&#65289;&#21644;&#24403;&#21069;&#22522;&#20934;&#12290;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#22522;&#20934;&#20013;&#30340;&#21477;&#27861;&#20559;&#35265;&#65292;&#34987;GVLMs&#30340;&#35821;&#35328;&#33021;&#21147;&#25152;&#21033;&#29992;&#12290;&#36825;&#31181;&#20559;&#35265;&#20351;&#24471;VisualGPTScore&#25104;&#20026;&#35780;&#20272;GVLMs&#30340;&#19981;&#36275;&#25351;&#26631;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#20010;SyntaxBias Score&#65292;&#21033;&#29992;LLMs&#37327;&#21270;&#27492;&#31867;&#20559;&#35265;&#20197;&#36827;&#34892;&#32531;&#35299;&#12290;&#38543;&#21518;&#28155;&#21152;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26032;&#20219;&#21153;&#65292;&#20197;&#35780;&#20272;GVLMs&#23545;&#22266;&#26377;&#20542;&#21521;&#20110;&#21477;&#27861;&#27491;&#30830;&#24615;&#30340;&#20581;&#22766;&#24615;&#12290;&#21033;&#29992;&#32531;&#35299;&#20559;&#35265;&#30340;&#25968;&#25454;&#38598;&#21644;&#26032;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#21363;SyntActically D
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.10509v2 Announce Type: replace-cross  Abstract: With the success of Large Language Models (LLMs), many Generative Vision-Language Models (GVLMs) have been constructed via multimodal instruction tuning. However, the performance of GVLMs in multimodal compositional reasoning remains under-explored. In this paper, we examine both the evaluation metrics (VisualGPTScore, etc.) and current benchmarks for evaluating the compositionality of GVLMs. We identify the syntactical bias in current benchmarks, which is exploited by the linguistic capability of GVLMs. The bias renders VisualGPTScore an insufficient metric for assessing GVLMs. To combat this, we first introduce a SyntaxBias Score, leveraging LLMs to quantify such bias for mitigation. A challenging new task is subsequently added to evaluate the robustness of GVLMs against inherent inclination toward syntactical correctness. Using the bias-mitigated datasets and the new task, we propose a novel benchmark, namely SyntActically D
&lt;/p&gt;</description></item><item><title>&#24341;&#25991;&#34987;&#30830;&#23450;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20851;&#38190;&#20294;&#32570;&#22833;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#20854;&#24341;&#20837;&#21487;&#20197;&#22686;&#24378;&#20869;&#23481;&#30340;&#36879;&#26126;&#24615;&#21644;&#21487;&#39564;&#35777;&#24615;&#20197;&#24212;&#23545;&#30693;&#35782;&#20135;&#26435;&#21644;&#20262;&#29702;&#38382;&#39064;&#65292;&#25552;&#35758;LLMs&#30340;&#20840;&#38754;&#24341;&#25991;&#26426;&#21046;&#24212;&#32771;&#34385;&#38750;&#21442;&#25968;&#21270;&#21644;&#21442;&#25968;&#21270;&#20869;&#23481;&#65292;&#23613;&#31649;&#23454;&#26045;&#24341;&#25991;&#26426;&#21046;&#22797;&#26434;&#19988;&#23384;&#22312;&#28508;&#22312;&#32570;&#38519;&#65292;&#20173;&#20027;&#24352;&#25512;&#21160;&#20854;&#21457;&#23637;&#24182;&#27010;&#36848;&#26410;&#26469;&#30740;&#31350;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2307.02185</link><description>&lt;p&gt;
&#24341;&#25991;&#65306;&#26500;&#24314;&#36127;&#36131;&#20219;&#21644;&#21487;&#35745;&#31639;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;
&lt;/p&gt;
&lt;p&gt;
Citation: A Key to Building Responsible and Accountable Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.02185
&lt;/p&gt;
&lt;p&gt;
&#24341;&#25991;&#34987;&#30830;&#23450;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20851;&#38190;&#20294;&#32570;&#22833;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#20854;&#24341;&#20837;&#21487;&#20197;&#22686;&#24378;&#20869;&#23481;&#30340;&#36879;&#26126;&#24615;&#21644;&#21487;&#39564;&#35777;&#24615;&#20197;&#24212;&#23545;&#30693;&#35782;&#20135;&#26435;&#21644;&#20262;&#29702;&#38382;&#39064;&#65292;&#25552;&#35758;LLMs&#30340;&#20840;&#38754;&#24341;&#25991;&#26426;&#21046;&#24212;&#32771;&#34385;&#38750;&#21442;&#25968;&#21270;&#21644;&#21442;&#25968;&#21270;&#20869;&#23481;&#65292;&#23613;&#31649;&#23454;&#26045;&#24341;&#25991;&#26426;&#21046;&#22797;&#26434;&#19988;&#23384;&#22312;&#28508;&#22312;&#32570;&#38519;&#65292;&#20173;&#20027;&#24352;&#25512;&#21160;&#20854;&#21457;&#23637;&#24182;&#27010;&#36848;&#26410;&#26469;&#30740;&#31350;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24102;&#26469;&#20102;&#21464;&#38761;&#24615;&#30340;&#22909;&#22788;&#65292;&#21516;&#26102;&#20063;&#24102;&#26469;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#30693;&#35782;&#20135;&#26435;&#65288;IP&#65289;&#21644;&#20262;&#29702;&#20851;&#20999;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#32531;&#35299;&#36825;&#20123;&#39118;&#38505;&#30340;&#26032;&#24605;&#36335;&#65292;&#20174;LLMs&#21644;&#24050;&#24314;&#31435;&#30340;Web&#31995;&#32479;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#20986;&#21457;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#8220;&#24341;&#25991;&#8221; - &#23545;&#26469;&#28304;&#25110;&#35777;&#25454;&#30340;&#25215;&#35748;&#25110;&#24341;&#29992; - &#22312;LLMs&#20013;&#30340;&#20851;&#38190;&#32570;&#22833;&#32452;&#25104;&#37096;&#20998;&#12290;&#24341;&#20837;&#24341;&#25991;&#21487;&#20197;&#22686;&#24378;&#20869;&#23481;&#30340;&#36879;&#26126;&#24615;&#21644;&#21487;&#39564;&#35777;&#24615;&#65292;&#20174;&#32780;&#24212;&#23545;LLMs&#30340;&#30693;&#35782;&#20135;&#26435;&#21644;&#20262;&#29702;&#38382;&#39064;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#35758;&#65292;LLMs&#30340;&#20840;&#38754;&#24341;&#25991;&#26426;&#21046;&#24212;&#32771;&#34385;&#38750;&#21442;&#25968;&#21270;&#21644;&#21442;&#25968;&#21270;&#20869;&#23481;&#12290;&#23613;&#31649;&#23454;&#26045;&#36825;&#26679;&#30340;&#24341;&#25991;&#26426;&#21046;&#30340;&#22797;&#26434;&#24615;&#21450;&#28508;&#22312;&#32570;&#38519;&#65292;&#25105;&#20204;&#20027;&#24352;&#25512;&#21160;&#20854;&#21457;&#23637;&#12290;&#22522;&#20110;&#36825;&#19968;&#22522;&#30784;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#35813;&#39046;&#22495;&#20013;&#30340;&#20960;&#20010;&#30740;&#31350;&#38382;&#39064;&#65292;&#26088;&#22312;&#24341;&#23548;&#26410;&#26469;&#30340;&#25506;&#32034;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2307.02185v3 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) bring transformative benefits alongside unique challenges, including intellectual property (IP) and ethical concerns. This position paper explores a novel angle to mitigate these risks, drawing parallels between LLMs and established web systems. We identify "citation" - the acknowledgement or reference to a source or evidence - as a crucial yet missing component in LLMs. Incorporating citation could enhance content transparency and verifiability, thereby confronting the IP and ethical issues in the deployment of LLMs. We further propose that a comprehensive citation mechanism for LLMs should account for both non-parametric and parametric content. Despite the complexity of implementing such a citation mechanism, along with the potential pitfalls, we advocate for its development. Building on this foundation, we outline several research problems in this area, aiming to guide future explorations towards
&lt;/p&gt;</description></item><item><title>TextFormer&#25552;&#20986;&#20102;&#22522;&#20110;&#26597;&#35810;&#30340;&#31471;&#21040;&#31471;&#25991;&#26412;&#35782;&#21035;&#22120;&#65292;&#37319;&#29992;Transformer&#26550;&#26500;&#65292;&#36890;&#36807;&#26597;&#35810;&#23884;&#20837;&#23454;&#29616;&#20102;&#32852;&#21512;&#35821;&#20041;&#29702;&#35299;&#65292;&#20801;&#35768;&#22810;&#20219;&#21153;&#24314;&#27169;&#20013;&#30340;&#28145;&#23618;&#29305;&#24449;&#20849;&#20139;&#65292;&#24182;&#35774;&#35745;&#20102;&#33258;&#36866;&#24212;&#20840;&#23616;&#32858;&#21512;&#65288;AGG&#65289;&#27169;&#22359;&#29992;&#20110;&#35835;&#21462;&#20219;&#24847;&#24418;&#29366;&#30340;&#25991;&#26412;&#12290;</title><link>https://arxiv.org/abs/2306.03377</link><description>&lt;p&gt;
TextFormer&#65306;&#22522;&#20110;&#26597;&#35810;&#30340;&#31471;&#21040;&#31471;&#25991;&#26412;&#35782;&#21035;&#22120;&#19982;&#28151;&#21512;&#30417;&#30563;
&lt;/p&gt;
&lt;p&gt;
TextFormer: A Query-based End-to-End Text Spotter with Mixed Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.03377
&lt;/p&gt;
&lt;p&gt;
TextFormer&#25552;&#20986;&#20102;&#22522;&#20110;&#26597;&#35810;&#30340;&#31471;&#21040;&#31471;&#25991;&#26412;&#35782;&#21035;&#22120;&#65292;&#37319;&#29992;Transformer&#26550;&#26500;&#65292;&#36890;&#36807;&#26597;&#35810;&#23884;&#20837;&#23454;&#29616;&#20102;&#32852;&#21512;&#35821;&#20041;&#29702;&#35299;&#65292;&#20801;&#35768;&#22810;&#20219;&#21153;&#24314;&#27169;&#20013;&#30340;&#28145;&#23618;&#29305;&#24449;&#20849;&#20139;&#65292;&#24182;&#35774;&#35745;&#20102;&#33258;&#36866;&#24212;&#20840;&#23616;&#32858;&#21512;&#65288;AGG&#65289;&#27169;&#22359;&#29992;&#20110;&#35835;&#21462;&#20219;&#24847;&#24418;&#29366;&#30340;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31471;&#21040;&#31471;&#25991;&#26412;&#35782;&#21035;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#65292;&#26088;&#22312;&#23558;&#22330;&#26223;&#25991;&#26412;&#26816;&#27979;&#21644;&#35782;&#21035;&#25972;&#21512;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#20013;&#12290;&#20856;&#22411;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#24863;&#20852;&#36259;&#21306;&#22495;&#65288;RoI&#65289;&#25805;&#20316;&#26469;&#25552;&#21462;&#23616;&#37096;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#22797;&#26434;&#30340;&#21518;&#22788;&#29702;&#27493;&#39588;&#29983;&#25104;&#26368;&#32456;&#39044;&#27979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TextFormer&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#22522;&#20110;&#26597;&#35810;&#30340;&#31471;&#21040;&#31471;&#25991;&#26412;&#35782;&#21035;&#22120;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20351;&#29992;&#27599;&#20010;&#25991;&#26412;&#23454;&#20363;&#30340;&#26597;&#35810;&#23884;&#20837;&#65292;TextFormer&#22312;&#22270;&#20687;&#32534;&#30721;&#22120;&#21644;&#25991;&#26412;&#35299;&#30721;&#22120;&#20043;&#19978;&#26500;&#24314;&#65292;&#20197;&#23398;&#20064;&#29992;&#20110;&#22810;&#20219;&#21153;&#24314;&#27169;&#30340;&#32852;&#21512;&#35821;&#20041;&#29702;&#35299;&#12290;&#23427;&#20801;&#35768;&#23545;&#20998;&#31867;&#12289;&#20998;&#21106;&#21644;&#35782;&#21035;&#20998;&#25903;&#36827;&#34892;&#30456;&#20114;&#35757;&#32451;&#21644;&#20248;&#21270;&#65292;&#23454;&#29616;&#26356;&#28145;&#30340;&#29305;&#24449;&#20849;&#20139;&#65292;&#32780;&#19981;&#20250;&#29306;&#29298;&#28789;&#27963;&#24615;&#25110;&#31616;&#21333;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#20840;&#23616;&#32858;&#21512;&#65288;AGG&#65289;&#27169;&#22359;&#65292;&#23558;&#20840;&#23616;&#29305;&#24449;&#36716;&#25442;&#20026;&#39034;&#24207;&#29305;&#24449;&#65292;&#20197;&#35835;&#21462;&#20219;&#24847;&#24418;&#29366;&#30340;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.03377v2 Announce Type: replace-cross  Abstract: End-to-end text spotting is a vital computer vision task that aims to integrate scene text detection and recognition into a unified framework. Typical methods heavily rely on Region-of-Interest (RoI) operations to extract local features and complex post-processing steps to produce final predictions. To address these limitations, we propose TextFormer, a query-based end-to-end text spotter with Transformer architecture. Specifically, using query embedding per text instance, TextFormer builds upon an image encoder and a text decoder to learn a joint semantic understanding for multi-task modeling. It allows for mutual training and optimization of classification, segmentation, and recognition branches, resulting in deeper feature sharing without sacrificing flexibility or simplicity. Additionally, we design an Adaptive Global aGgregation (AGG) module to transfer global features into sequential features for reading arbitrarily-shape
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;MCTS&#65292;&#19968;&#20010;&#22810;&#21442;&#32771;&#30340;&#20013;&#25991;&#25991;&#26412;&#31616;&#21270;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#35780;&#20272;&#25968;&#25454;&#21644;&#24615;&#33021;&#20998;&#26512;&#65292;&#21516;&#26102;&#36824;&#21457;&#24067;&#20102;&#29992;&#20110;&#35757;&#32451;&#30340;&#20013;&#25991;&#25991;&#26412;&#31616;&#21270;&#24179;&#34892;&#25968;&#25454;&#65292;&#20026;&#26410;&#26469;&#20013;&#25991;&#25991;&#26412;&#31616;&#21270;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#30784;&#21644;&#21442;&#32771;&#12290;</title><link>https://arxiv.org/abs/2306.02796</link><description>&lt;p&gt;
MCTS&#65306;&#19968;&#20010;&#22810;&#21442;&#32771;&#30340;&#20013;&#25991;&#25991;&#26412;&#31616;&#21270;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MCTS: A Multi-Reference Chinese Text Simplification Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.02796
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;MCTS&#65292;&#19968;&#20010;&#22810;&#21442;&#32771;&#30340;&#20013;&#25991;&#25991;&#26412;&#31616;&#21270;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#35780;&#20272;&#25968;&#25454;&#21644;&#24615;&#33021;&#20998;&#26512;&#65292;&#21516;&#26102;&#36824;&#21457;&#24067;&#20102;&#29992;&#20110;&#35757;&#32451;&#30340;&#20013;&#25991;&#25991;&#26412;&#31616;&#21270;&#24179;&#34892;&#25968;&#25454;&#65292;&#20026;&#26410;&#26469;&#20013;&#25991;&#25991;&#26412;&#31616;&#21270;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#30784;&#21644;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#31616;&#21270;&#26088;&#22312;&#36890;&#36807;&#24212;&#29992;&#37325;&#20889;&#36716;&#25442;&#20351;&#25991;&#26412;&#26356;&#23481;&#26131;&#29702;&#35299;&#12290;&#38271;&#26399;&#20197;&#26469;&#65292;&#20851;&#20110;&#20013;&#25991;&#25991;&#26412;&#31616;&#21270;&#30340;&#30740;&#31350;&#24456;&#23569;&#12290;&#36890;&#29992;&#35780;&#20272;&#25968;&#25454;&#30340;&#32570;&#20047;&#26159;&#36825;&#31181;&#29616;&#35937;&#30340;&#19968;&#20010;&#37325;&#35201;&#21407;&#22240;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;MCTS&#65292;&#19968;&#20010;&#22810;&#21442;&#32771;&#30340;&#20013;&#25991;&#25991;&#26412;&#31616;&#21270;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#25968;&#25454;&#38598;&#30340;&#27880;&#37322;&#36807;&#31243;&#24182;&#25552;&#20379;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20960;&#31181;&#26080;&#30417;&#30563;&#26041;&#27861;&#21644;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#30340;&#20013;&#25991;&#25991;&#26412;&#31616;&#21270;&#24179;&#34892;&#25968;&#25454;&#65292;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#32763;&#35793;&#21644;&#33521;&#25991;&#25991;&#26412;&#31616;&#21270;&#33719;&#24471;&#12290;&#25105;&#20204;&#24076;&#26395;&#36890;&#36807;&#22522;&#30784;&#24037;&#20316;&#24314;&#31435;&#23545;&#20013;&#25991;&#25991;&#26412;&#31616;&#21270;&#30340;&#22522;&#26412;&#29702;&#35299;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#21442;&#32771;&#12290;&#25152;&#26377;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#24050;&#22312;https://github.com/blcuicall/mcts/&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.02796v2 Announce Type: replace  Abstract: Text simplification aims to make the text easier to understand by applying rewriting transformations. There has been very little research on Chinese text simplification for a long time. The lack of generic evaluation data is an essential reason for this phenomenon. In this paper, we introduce MCTS, a multi-reference Chinese text simplification dataset. We describe the annotation process of the dataset and provide a detailed analysis. Furthermore, we evaluate the performance of several unsupervised methods and advanced large language models. We additionally provide Chinese text simplification parallel data that can be used for training, acquired by utilizing machine translation and English text simplification. We hope to build a basic understanding of Chinese text simplification through the foundational work and provide references for future research. All of the code and data are released at https://github.com/blcuicall/mcts/.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;BootAug&#65292;&#19968;&#20010;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#28151;&#21512;&#23454;&#20363;&#36807;&#28388;&#26694;&#26550;&#65292;&#33021;&#22815;&#25913;&#36827;&#25991;&#26412;&#22686;&#24378;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#25552;&#39640;&#20102;&#32422;2-3%&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2210.02941</link><description>&lt;p&gt;
BootAug: &#36890;&#36807;&#28151;&#21512;&#23454;&#20363;&#36807;&#28388;&#26694;&#26550;&#22686;&#24378;&#25991;&#26412;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
BootAug: Boosting Text Augmentation via Hybrid Instance Filtering Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.02941
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;BootAug&#65292;&#19968;&#20010;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#28151;&#21512;&#23454;&#20363;&#36807;&#28388;&#26694;&#26550;&#65292;&#33021;&#22815;&#25913;&#36827;&#25991;&#26412;&#22686;&#24378;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#25552;&#39640;&#20102;&#32422;2-3%&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#22686;&#24378;&#26159;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#25968;&#25454;&#19981;&#36275;&#38382;&#39064;&#30340;&#26377;&#25928;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25991;&#26412;&#22686;&#24378;&#26041;&#27861;&#24448;&#24448;&#19987;&#27880;&#20110;&#23569;&#26679;&#26412;&#22330;&#26223;&#65292;&#24182;&#19988;&#36890;&#24120;&#22312;&#22823;&#35268;&#27169;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#22686;&#24378;&#26041;&#27861;&#24448;&#24448;&#29983;&#25104;&#20855;&#26377;&#31227;&#20301;&#29305;&#24449;&#31354;&#38388;&#30340;&#23454;&#20363;&#65292;&#23548;&#33268;&#22312;&#22686;&#24378;&#25968;&#25454;&#19978;&#34920;&#29616;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#28151;&#21512;&#23454;&#20363;&#36807;&#28388;&#26694;&#26550;&#65288;BootAug&#65289;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20445;&#25345;&#19982;&#33258;&#28982;&#25968;&#25454;&#38598;&#31867;&#20284;&#30340;&#29305;&#24449;&#31354;&#38388;&#12290;BootAug&#21487;&#20197;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#25991;&#26412;&#22686;&#24378;&#26041;&#27861;&#65288;&#22914;&#21516;&#20041;&#35789;&#26367;&#25442;&#21644;&#21453;&#21521;&#32763;&#35793;&#65289;&#65292;&#24182;&#22312;&#20998;&#31867;&#20934;&#30830;&#29575;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#32422; 2-3% &#30340;&#22686;&#24378;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.02941v2 Announce Type: replace  Abstract: Text augmentation is an effective technique for addressing the problem of insufficient data in natural language processing. However, existing text augmentation methods tend to focus on few-shot scenarios and usually perform poorly on large public datasets. Our research indicates that existing augmentation methods often generate instances with shifted feature spaces, which leads to a drop in performance on the augmented data (for example, EDA generally loses $\approx 2\%$ in aspect-based sentiment classification). To address this problem, we propose a hybrid instance-filtering framework (BootAug) based on pre-trained language models that can maintain a similar feature space with natural datasets. BootAug is transferable to existing text augmentation methods (such as synonym substitution and back translation) and significantly improves the augmentation performance by $\approx 2-3\%$ in classification accuracy. Our experimental results 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;IMA-GloVe-GA&#65292;&#19968;&#20010;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#22810;&#27493;&#25512;&#29702;&#30340;&#36845;&#20195;&#31070;&#32463;&#25512;&#29702;&#32593;&#32476;&#65292;&#22312;&#36229;&#39046;&#22495;&#27867;&#21270;&#26041;&#38754;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2207.14000</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#19978;&#30340;&#22810;&#27493;&#28436;&#32462;&#25512;&#29702;&#65306;&#22522;&#20110;&#36229;&#39046;&#22495;&#27867;&#21270;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study on Out-of-Distribution Generalisation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2207.14000
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;IMA-GloVe-GA&#65292;&#19968;&#20010;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#22810;&#27493;&#25512;&#29702;&#30340;&#36845;&#20195;&#31070;&#32463;&#25512;&#29702;&#32593;&#32476;&#65292;&#22312;&#36229;&#39046;&#22495;&#27867;&#21270;&#26041;&#38754;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#28145;&#24230;&#23398;&#20064;&#19982;&#31526;&#21495;&#36923;&#36753;&#25512;&#29702;&#32467;&#21512;&#36215;&#26469;&#65292;&#26088;&#22312;&#20805;&#20998;&#21033;&#29992;&#36825;&#20004;&#20010;&#39046;&#22495;&#30340;&#25104;&#21151;&#65292;&#24182;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#21463;DeepLogic&#21551;&#21457;&#65292;&#35813;&#27169;&#22411;&#32463;&#36807;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#29992;&#20110;&#25191;&#34892;&#36923;&#36753;&#31243;&#24207;&#25512;&#29702;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;IMA-GloVe-GA&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#22810;&#27493;&#25512;&#29702;&#30340;&#36845;&#20195;&#31070;&#32463;&#25512;&#29702;&#32593;&#32476;&#12290;&#22312;&#25105;&#20204;&#30340;&#27169;&#22411;&#20013;&#65292;&#25512;&#29702;&#26159;&#20351;&#29992;&#22522;&#20110;RNN&#30340;&#36845;&#20195;&#20869;&#23384;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#30340;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#38376;&#20851;&#27880;&#26426;&#21046;&#12290;&#25105;&#20204;&#22312;PARARULES&#12289;CONCEPTRULES V1&#21644;CONCEPTRULES V2&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;IMA-GloVe-GA&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24102;&#26377;&#38376;&#20851;&#27880;&#26426;&#21046;&#30340;DeepLogic&#27604;DeepLogic&#21644;&#20854;&#20182;RNN&#22522;&#32447;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#26356;&#39640;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#35268;&#21017;&#34987;&#25171;&#20081;&#26102;&#27604;RoBERTa-Large&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#36229;&#39046;&#22495;&#27867;&#21270;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35299;&#20915;&#24403;&#21069;&#22810;&#27493;&#25512;&#29702;&#25968;&#25454;&#38598;&#20013;&#25512;&#29702;&#28145;&#24230;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2207.14000v2 Announce Type: replace-cross  Abstract: Combining deep learning with symbolic logic reasoning aims to capitalize on the success of both fields and is drawing increasing attention. Inspired by DeepLogic, an end-to-end model trained to perform inference on logic programs, we introduce IMA-GloVe-GA, an iterative neural inference network for multi-step reasoning expressed in natural language. In our model, reasoning is performed using an iterative memory neural network based on RNN with a gate attention mechanism. We evaluate IMA-GloVe-GA on three datasets: PARARULES, CONCEPTRULES V1 and CONCEPTRULES V2. Experimental results show DeepLogic with gate attention can achieve higher test accuracy than DeepLogic and other RNN baseline models. Our model achieves better out-of-distribution generalisation than RoBERTa-Large when the rules have been shuffled. Furthermore, to address the issue of unbalanced distribution of reasoning depths in the current multi-step reasoning datase
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#20581;&#24247;&#25991;&#26412;&#31616;&#21270;&#30740;&#31350;&#30340;&#28040;&#21270;&#30284;&#30151;&#25945;&#32946;&#26448;&#26009;&#30340;&#27880;&#37322;&#35821;&#26009;&#24211;&#65292;&#24182;&#25506;&#32034;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21270;&#26041;&#27861;&#65292;&#21253;&#25324;&#24494;&#35843;&#12289;&#22686;&#24378;&#23398;&#20064;&#12289;&#22686;&#24378;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#12289;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.15043</link><description>&lt;p&gt;
&#20581;&#24247;&#25991;&#26412;&#31616;&#21270;&#65306;&#28040;&#21270;&#30284;&#30151;&#25945;&#32946;&#30340;&#27880;&#37322;&#35821;&#26009;&#24211;&#21644;&#22686;&#24378;&#23398;&#20064;&#30340;&#26032;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Health Text Simplification: An Annotated Corpus for Digestive Cancer Education and Novel Strategies for Reinforcement Learning. (arXiv:2401.15043v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15043
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#20581;&#24247;&#25991;&#26412;&#31616;&#21270;&#30740;&#31350;&#30340;&#28040;&#21270;&#30284;&#30151;&#25945;&#32946;&#26448;&#26009;&#30340;&#27880;&#37322;&#35821;&#26009;&#24211;&#65292;&#24182;&#25506;&#32034;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21270;&#26041;&#27861;&#65292;&#21253;&#25324;&#24494;&#35843;&#12289;&#22686;&#24378;&#23398;&#20064;&#12289;&#22686;&#24378;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#12289;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#20581;&#24247;&#25945;&#32946;&#26448;&#26009;&#30340;&#38405;&#35835;&#27700;&#24179;&#26174;&#33879;&#24433;&#21709;&#20449;&#24687;&#30340;&#21487;&#29702;&#35299;&#24615;&#21644;&#21487;&#25509;&#35302;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23569;&#25968;&#26063;&#35028;&#20154;&#32676;&#12290;&#35768;&#22810;&#24739;&#32773;&#25945;&#32946;&#36164;&#28304;&#36229;&#36807;&#20102;&#24191;&#27867;&#25509;&#21463;&#30340;&#26631;&#20934;&#30340;&#38405;&#35835;&#27700;&#24179;&#21644;&#22797;&#26434;&#24615;&#12290;&#22312;&#20581;&#24247;&#20449;&#24687;&#20013;&#65292;&#24613;&#38656;&#39640;&#24615;&#33021;&#30340;&#25991;&#26412;&#31616;&#21270;&#27169;&#22411;&#20197;&#22686;&#24378;&#20256;&#25773;&#21644;&#35782;&#23383;&#33021;&#21147;&#12290;&#36825;&#31181;&#38656;&#35201;&#22312;&#30284;&#30151;&#25945;&#32946;&#20013;&#23588;&#20026;&#36843;&#20999;&#65292;&#26377;&#25928;&#30340;&#39044;&#38450;&#21644;&#31579;&#26597;&#25945;&#32946;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#21457;&#30149;&#29575;&#21644;&#27515;&#20129;&#29575;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#24341;&#20837;&#20102;&#31616;&#21270;&#30340;&#28040;&#21270;&#30284;&#30151;&#65288;SimpleDC&#65289;&#24182;&#34892;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#20581;&#24247;&#25991;&#26412;&#31616;&#21270;&#30740;&#31350;&#12290;&#21033;&#29992;SimpleDC&#21644;&#29616;&#26377;&#30340;Med-EASi&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#31616;&#21270;&#26041;&#27861;&#65292;&#21253;&#25324;&#24494;&#35843;&#12289;&#22686;&#24378;&#23398;&#20064;&#65288;RL&#65289;&#12289;&#22686;&#24378;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#12289;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: The reading level of health educational materials significantly influences information understandability and accessibility, particularly for minoritized populations. Many patient educational resources surpass the reading level and complexity of widely accepted standards. There is a critical need for high-performing text simplification models in health information to enhance dissemination and literacy. This need is particularly acute in cancer education, where effective prevention and screening education can substantially reduce morbidity and mortality.  Methods: We introduce Simplified Digestive Cancer (SimpleDC), a parallel corpus of cancer education materials tailored for health text simplification research. Utilizing SimpleDC alongside the existing Med-EASi corpus, we explore Large Language Model (LLM)-based simplification methods, including fine-tuning, reinforcement learning (RL), reinforcement learning with human feedback (RLHF), domain adaptation, and prompt-based app
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#32467;&#21512;&#32467;&#26500;&#30340;&#25552;&#31034;&#24037;&#31243;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#24615;&#33021;&#26041;&#38754;&#30340;&#21069;&#26223;&#65292;&#36890;&#36807;&#24605;&#32500;&#38142;&#12289;&#24605;&#32500;&#26641;&#25110;&#24605;&#32500;&#22270;&#30340;&#35774;&#35745;&#26469;&#24341;&#23548;&#25972;&#20307;&#25512;&#29702;&#36807;&#31243;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#20363;&#65292;&#36825;&#31181;&#33539;&#24335;&#26174;&#33879;&#22686;&#24378;&#20102;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#34013;&#22270;&#65292;&#20026;&#26410;&#26469;&#30340;&#21457;&#23637;&#38138;&#24179;&#36947;&#36335;&#12290;</title><link>http://arxiv.org/abs/2401.14295</link><description>&lt;p&gt;
&#25512;&#29702;&#30340;&#25299;&#25169;&#23398;&#65306;&#25581;&#31192;&#24605;&#32500;&#38142;&#12289;&#26641;&#21644;&#22270;
&lt;/p&gt;
&lt;p&gt;
Topologies of Reasoning: Demystifying Chains, Trees, and Graphs of Thoughts. (arXiv:2401.14295v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14295
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#32467;&#21512;&#32467;&#26500;&#30340;&#25552;&#31034;&#24037;&#31243;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#24615;&#33021;&#26041;&#38754;&#30340;&#21069;&#26223;&#65292;&#36890;&#36807;&#24605;&#32500;&#38142;&#12289;&#24605;&#32500;&#26641;&#25110;&#24605;&#32500;&#22270;&#30340;&#35774;&#35745;&#26469;&#24341;&#23548;&#25972;&#20307;&#25512;&#29702;&#36807;&#31243;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#20363;&#65292;&#36825;&#31181;&#33539;&#24335;&#26174;&#33879;&#22686;&#24378;&#20102;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#34013;&#22270;&#65292;&#20026;&#26410;&#26469;&#30340;&#21457;&#23637;&#38138;&#24179;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#36890;&#36807;&#21019;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24615;&#33021;&#26041;&#38754;&#12290;&#20854;&#20013;&#65292;&#19982;&#32467;&#26500;&#30456;&#32467;&#21512;&#30340;&#25552;&#31034;&#24037;&#31243;&#34987;&#35270;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#33539;&#24335;&#65292;&#20854;&#35774;&#35745;&#22914;&#24605;&#32500;&#38142;&#12289;&#24605;&#32500;&#26641;&#25110;&#24605;&#32500;&#22270;&#31561;&#65292;&#36890;&#36807;&#32467;&#26500;&#25351;&#23548;&#25972;&#20307;LLM&#25512;&#29702;&#36807;&#31243;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#20363;&#30340;&#35828;&#26126;&#65292;&#36825;&#31181;&#33539;&#24335;&#26174;&#33879;&#22686;&#24378;&#20102;LLM&#22312;&#36923;&#36753;&#25110;&#25968;&#23398;&#25512;&#29702;&#12289;&#35268;&#21010;&#25110;&#21019;&#36896;&#24615;&#20889;&#20316;&#31561;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#26041;&#20415;&#29702;&#35299;&#36825;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#24182;&#20026;&#26410;&#26469;&#30340;&#21457;&#23637;&#38138;&#24179;&#36947;&#36335;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;LLM&#25512;&#29702;&#26041;&#26696;&#30340;&#36890;&#29992;&#34013;&#22270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23545;&#25552;&#31034;&#25191;&#34892;&#27969;&#31243;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#28548;&#28165;&#24182;&#26126;&#30830;&#23450;&#20041;&#20102;&#19981;&#21516;&#30340;&#27010;&#24565;&#12290;&#28982;&#21518;&#25105;&#20204;&#24314;&#31435;&#31532;&#19968;&#20010;&#20998;&#31867;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
The field of natural language processing (NLP) has witnessed significant progress in recent years, with a notable focus on improving large language models' (LLM) performance through innovative prompting techniques. Among these, prompt engineering coupled with structures has emerged as a promising paradigm, with designs such as Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts, in which the overall LLM reasoning is guided by a structure such as a graph. As illustrated with numerous examples, this paradigm significantly enhances the LLM's capability to solve numerous tasks, ranging from logical or mathematical reasoning to planning or creative writing. To facilitate the understanding of this growing field and pave the way for future developments, we devise a general blueprint for effective and efficient LLM reasoning schemes. For this, we conduct an in-depth analysis of the prompt execution pipeline, clarifying and clearly defining different concepts. We then build the first taxon
&lt;/p&gt;</description></item><item><title>TelME&#26159;&#19968;&#31181;&#25945;&#24072;&#23548;&#21521;&#30340;&#22810;&#27169;&#34701;&#21512;&#32593;&#32476;&#65292;&#36890;&#36807;&#36328;&#27169;&#24577;&#30693;&#35782;&#33976;&#39311;&#23454;&#29616;&#23545;&#35805;&#20013;&#24773;&#32490;&#35782;&#21035;&#30340;&#20248;&#21270;&#65292;&#21462;&#24471;&#20102;&#22312;&#22810;&#35828;&#35805;&#20154;&#25968;&#25454;&#38598;MELD&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.12987</link><description>&lt;p&gt;
TelME&#65306;&#25945;&#24072;&#23548;&#21521;&#30340;&#22810;&#27169;&#34701;&#21512;&#32593;&#32476;&#29992;&#20110;&#23545;&#35805;&#20013;&#30340;&#24773;&#32490;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
TelME: Teacher-leading Multimodal Fusion Network for Emotion Recognition in Conversation. (arXiv:2401.12987v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12987
&lt;/p&gt;
&lt;p&gt;
TelME&#26159;&#19968;&#31181;&#25945;&#24072;&#23548;&#21521;&#30340;&#22810;&#27169;&#34701;&#21512;&#32593;&#32476;&#65292;&#36890;&#36807;&#36328;&#27169;&#24577;&#30693;&#35782;&#33976;&#39311;&#23454;&#29616;&#23545;&#35805;&#20013;&#24773;&#32490;&#35782;&#21035;&#30340;&#20248;&#21270;&#65292;&#21462;&#24471;&#20102;&#22312;&#22810;&#35828;&#35805;&#20154;&#25968;&#25454;&#38598;MELD&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#20013;&#30340;&#24773;&#32490;&#35782;&#21035;&#22312;&#20351;&#23545;&#35805;&#31995;&#32479;&#33021;&#22815;&#26377;&#25928;&#22238;&#24212;&#29992;&#25143;&#35831;&#27714;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#23545;&#35805;&#20013;&#30340;&#24773;&#32490;&#21487;&#20197;&#36890;&#36807;&#38899;&#39057;&#12289;&#35270;&#35273;&#21644;&#25991;&#26412;&#31561;&#22810;&#31181;&#27169;&#24577;&#30340;&#34920;&#31034;&#36827;&#34892;&#35782;&#21035;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38750;&#35821;&#35328;&#27169;&#24577;&#23545;&#35782;&#21035;&#24773;&#32490;&#30340;&#36129;&#29486;&#36739;&#24369;&#65292;&#22810;&#27169;&#24577;&#24773;&#32490;&#35782;&#21035;&#19968;&#30452;&#34987;&#35748;&#20026;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23545;&#35805;&#20013;&#24773;&#32490;&#35782;&#21035;&#30340;&#25945;&#24072;&#23548;&#21521;&#22810;&#27169;&#34701;&#21512;&#32593;&#32476;&#65288;TelME&#65289;&#12290;TelME&#36890;&#36807;&#36328;&#27169;&#24577;&#30693;&#35782;&#33976;&#39311;&#23558;&#20449;&#24687;&#20174;&#20316;&#20026;&#25945;&#24072;&#30340;&#35821;&#35328;&#27169;&#22411;&#20256;&#36882;&#32473;&#38750;&#35821;&#35328;&#30340;&#23398;&#29983;&#65292;&#20174;&#32780;&#20248;&#21270;&#20102;&#24369;&#27169;&#24577;&#30340;&#25928;&#33021;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#31227;&#21160;&#34701;&#21512;&#26041;&#27861;&#23558;&#22810;&#27169;&#24577;&#29305;&#24449;&#32452;&#21512;&#36215;&#26469;&#65292;&#20854;&#20013;&#23398;&#29983;&#32593;&#32476;&#25903;&#25345;&#25945;&#24072;&#12290;TelME&#22312;MELD&#65288;&#19968;&#31181;&#29992;&#20110;&#23545;&#35805;&#24773;&#32490;&#35782;&#21035;&#30340;&#22810;&#35828;&#35805;&#20154;&#25968;&#25454;&#38598;&#65289;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#39069;&#22806;&#30340;&#23454;&#39564;&#35770;&#35777;&#20102;&#25105;&#20204;&#32452;&#20214;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion Recognition in Conversation (ERC) plays a crucial role in enabling dialogue systems to effectively respond to user requests. The emotions in a conversation can be identified by the representations from various modalities, such as audio, visual, and text. However, due to the weak contribution of non-verbal modalities to recognize emotions, multimodal ERC has always been considered a challenging task. In this paper, we propose Teacher-leading Multimodal fusion network for ERC (TelME). TelME incorporates cross-modal knowledge distillation to transfer information from a language model acting as the teacher to the non-verbal students, thereby optimizing the efficacy of the weak modalities. We then combine multimodal features using a shifting fusion approach in which student networks support the teacher. TelME achieves state-of-the-art performance in MELD, a multi-speaker conversation dataset for ERC. Finally, we demonstrate the effectiveness of our components through additional expe
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#24230;&#37327;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#29983;&#21629;&#31185;&#23398;&#39046;&#22495;&#19979;&#39046;&#22495;&#28418;&#31227;&#30340;&#21629;&#21517;&#26631;&#31614;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#28304;&#23454;&#20307;&#21644;&#30446;&#26631;&#23454;&#20307;&#25237;&#24433;&#21040;&#29305;&#24449;&#31354;&#38388;&#30340;&#19981;&#21516;&#21306;&#22495;&#26469;&#20943;&#36731;&#28304;&#23454;&#20307;&#38169;&#35823;&#26631;&#35760;&#20026;&#30446;&#26631;&#23454;&#20307;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.10472</link><description>&lt;p&gt;
&#29983;&#21629;&#31185;&#23398;&#39046;&#22495;&#36890;&#36807;&#24230;&#37327;&#23398;&#20064;&#23545;&#39046;&#22495;&#28418;&#31227;&#19979;&#30340;&#21629;&#21517;&#26631;&#31614;&#36827;&#34892;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Name Tagging Under Domain Shift via Metric Learning for Life Sciences. (arXiv:2401.10472v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#24230;&#37327;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#29983;&#21629;&#31185;&#23398;&#39046;&#22495;&#19979;&#39046;&#22495;&#28418;&#31227;&#30340;&#21629;&#21517;&#26631;&#31614;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#28304;&#23454;&#20307;&#21644;&#30446;&#26631;&#23454;&#20307;&#25237;&#24433;&#21040;&#29305;&#24449;&#31354;&#38388;&#30340;&#19981;&#21516;&#21306;&#22495;&#26469;&#20943;&#36731;&#28304;&#23454;&#20307;&#38169;&#35823;&#26631;&#35760;&#20026;&#30446;&#26631;&#23454;&#20307;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#26631;&#31614;&#26159;&#20449;&#24687;&#25277;&#21462;&#65288;IE&#65289;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#23588;&#20854;&#22312;&#29983;&#29289;&#21307;&#23398;&#21644;&#21270;&#23398;&#31561;&#31185;&#23398;&#39046;&#22495;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#20363;&#22914;ChatGPT&#65292;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#36801;&#31227;&#23398;&#20064;&#24212;&#29992;&#20110;&#22686;&#24378;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#65288;&#28304;&#39046;&#22495;&#65289;&#35757;&#32451;&#30340;&#21629;&#21517;&#26631;&#31614;&#27169;&#22411;&#22312;&#21270;&#23398;&#39046;&#22495;&#65288;&#30446;&#26631;&#39046;&#22495;&#65289;&#20013;&#30340;&#20351;&#29992;&#12290;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#35774;&#32622;&#20013;&#35757;&#32451;&#36825;&#26679;&#30340;&#27169;&#22411;&#30340;&#19968;&#31181;&#24120;&#35265;&#20570;&#27861;&#26159;&#22312;&#26631;&#35760;&#30340;&#28304;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#28982;&#21518;&#22312;&#23569;&#37327;&#26631;&#35760;&#30340;&#30446;&#26631;&#31034;&#20363;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#26679;&#30340;&#27169;&#22411;&#23481;&#26131;&#23558;&#28304;&#23454;&#20307;&#38169;&#35823;&#26631;&#35760;&#20026;&#30446;&#26631;&#23454;&#20307;&#65292;&#22240;&#20026;&#28304;&#23454;&#20307;&#32463;&#24120;&#20986;&#29616;&#22312;&#25991;&#26412;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#65292;&#23558;&#30693;&#35782;&#20174;&#28304;&#39046;&#22495;&#36716;&#31227;&#21040;&#30446;&#26631;&#39046;&#22495;&#65292;&#20294;&#21516;&#26102;&#23558;&#28304;&#23454;&#20307;&#21644;&#30446;&#26631;&#23454;&#20307;&#25237;&#24433;&#21040;&#29305;&#24449;&#31354;&#38388;&#30340;&#19981;&#21516;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Name tagging is a key component of Information Extraction (IE), particularly in scientific domains such as biomedicine and chemistry, where large language models (LLMs), e.g., ChatGPT, fall short. We investigate the applicability of transfer learning for enhancing a name tagging model trained in the biomedical domain (the source domain) to be used in the chemical domain (the target domain). A common practice for training such a model in a few-shot learning setting is to pretrain the model on the labeled source data, and then, to finetune it on a hand-full of labeled target examples. In our experiments we observed that such a model is prone to mis-labeling the source entities, which can often appear in the text, as the target entities. To alleviate this problem, we propose a model to transfer the knowledge from the source domain to the target domain, however, at the same time, to project the source entities and target entities into separate regions of the feature space. This diminishes 
&lt;/p&gt;</description></item><item><title>DeepEdit&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#22909;&#30340;&#25512;&#29702;&#19968;&#33268;&#24615;&#21644;&#23545;&#26356;&#26032;&#30693;&#35782;&#30340;&#24847;&#35782;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#33021;&#21147;&#65292;&#23545;&#22810;&#36339;&#38382;&#39064;&#25968;&#25454;&#38598;MQuaKE&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2401.10471</link><description>&lt;p&gt;
DeepEdit: &#24102;&#26377;&#32422;&#26463;&#30340;&#35299;&#30721;&#24335;&#30693;&#35782;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
DeepEdit: Knowledge Editing as Decoding with Constraints. (arXiv:2401.10471v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10471
&lt;/p&gt;
&lt;p&gt;
DeepEdit&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#22909;&#30340;&#25512;&#29702;&#19968;&#33268;&#24615;&#21644;&#23545;&#26356;&#26032;&#30693;&#35782;&#30340;&#24847;&#35782;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#33021;&#21147;&#65292;&#23545;&#22810;&#36339;&#38382;&#39064;&#25968;&#25454;&#38598;MQuaKE&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30693;&#35782;&#32534;&#36753;&#35270;&#20026;&#24102;&#26377;&#32422;&#26463;&#30340;&#35299;&#30721;&#36807;&#31243;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DeepEdit&#65288;&#22522;&#20110;&#28145;&#24230;&#20248;&#20808;&#25628;&#32034;&#30340;&#28176;&#36827;&#24335;&#35299;&#30721;&#30693;&#35782;&#32534;&#36753;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#22909;&#30340;&#25512;&#29702;&#19968;&#33268;&#24615;&#12289;&#38382;&#39064;&#30456;&#20851;&#24615;&#21644;&#23545;&#26356;&#26032;&#30693;&#35782;&#30340;&#24847;&#35782;&#26469;&#25913;&#36827;&#30693;&#35782;&#32534;&#36753;&#12290;DeepEdit&#21487;&#28789;&#27963;&#24212;&#29992;&#20110;&#25152;&#26377;&#40657;&#30418;LLMs&#65306;&#19981;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#12289;&#34920;&#31034;&#25110;&#36755;&#20986;&#35789;&#27719;&#20998;&#24067;&#12290;DeepEdit&#36880;&#27493;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#25512;&#29702;&#27493;&#39588;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#30693;&#35782;&#32534;&#36753;&#12290;&#23427;&#21033;&#29992;&#28145;&#24230;&#20248;&#20808;&#25628;&#32034;&#26469;&#20462;&#25913;LLMs&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#25552;&#39640;&#36755;&#20986;&#23545;&#38382;&#39064;&#30340;&#30456;&#20851;&#24615;&#21644;&#23545;&#26356;&#26032;&#30693;&#35782;&#30340;&#24847;&#35782;&#12290;&#22312;&#30693;&#35782;&#32534;&#36753;&#26041;&#38754;&#65292;DeepEdit&#22312;&#25511;&#21046;LLMs&#20135;&#29983;&#26356;&#31616;&#27905;&#30340;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#22312;MQuaKE&#19978;&#65292;DeepEdit&#22312;&#23450;&#37327;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#36339;&#38382;&#39064;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a new perspective of knowledge editing for large language models (LLMs) as decoding with constraints. We propose DeepEdit (Depth-first Search based Progressive Decoding for Knowledge Editing), a neuro-symbolic method that improves knowledge editing with better coherence of reasoning, relevance to the question, and awareness of updated knowledge. DeepEdit can be flexibly applied to all black-box LLMs: it does not require any access to the model parameters, representations, or output vocabulary distributions. DeepEdit progressively produces the high-quality reasoning steps towards effective knowledge editing. It utilizes a depth-first search to revise the LLMs' output, which improves the output's informativeness to the input question and awareness of the updated knowledge. Qualitatively, DeepEdit effectively controls LLMs to produce more succinct reasoning in accord with knowledge editing. Quantitatively, DeepEdit yields significant gains on MQuaKE, a challenging multi-hop que
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#28151;&#21512;&#22823;&#23567;&#20889;&#65288;mixcase&#65289;&#30340;&#27010;&#24565;&#65292;&#25506;&#35752;&#20102;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#21644;&#20154;&#24037;&#29983;&#25104;&#25991;&#26412;&#30340;&#28151;&#21512;&#24773;&#26223;&#65292;&#24182;&#26500;&#24314;&#20102;&#36866;&#29992;&#20110;&#30740;&#31350;&#36825;&#20123;&#24773;&#26223;&#30340;&#25968;&#25454;&#38598;MixSet&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#30446;&#21069;&#30340;MGT&#26816;&#27979;&#22120;&#23545;&#20110;&#28151;&#21512;&#25991;&#26412;&#30340;&#26816;&#27979;&#25928;&#26524;&#19981;&#20339;&#12290;</title><link>http://arxiv.org/abs/2401.05952</link><description>&lt;p&gt;
LLM&#20316;&#20026;&#21512;&#33879;&#32773;&#65306;&#26816;&#27979;LLM-Human&#28151;&#21512;&#25991;&#26412;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
LLM-as-a-Coauthor: The Challenges of Detecting LLM-Human Mixcase. (arXiv:2401.05952v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#28151;&#21512;&#22823;&#23567;&#20889;&#65288;mixcase&#65289;&#30340;&#27010;&#24565;&#65292;&#25506;&#35752;&#20102;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#21644;&#20154;&#24037;&#29983;&#25104;&#25991;&#26412;&#30340;&#28151;&#21512;&#24773;&#26223;&#65292;&#24182;&#26500;&#24314;&#20102;&#36866;&#29992;&#20110;&#30740;&#31350;&#36825;&#20123;&#24773;&#26223;&#30340;&#25968;&#25454;&#38598;MixSet&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#30446;&#21069;&#30340;MGT&#26816;&#27979;&#22120;&#23545;&#20110;&#28151;&#21512;&#25991;&#26412;&#30340;&#26816;&#27979;&#25928;&#26524;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26174;&#33879;&#21457;&#23637;&#21644;&#24191;&#27867;&#24212;&#29992;&#65292;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#65288;MGT&#65289;&#30340;&#20351;&#29992;&#27491;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#12290;&#36825;&#19968;&#36235;&#21183;&#24102;&#26469;&#20102;&#28508;&#22312;&#30340;&#39118;&#38505;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26032;&#38395;&#21644;&#25945;&#32946;&#31561;&#39046;&#22495;&#20449;&#24687;&#30340;&#36136;&#37327;&#21644;&#23436;&#25972;&#24615;&#32780;&#35328;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#35299;&#20915;&#26816;&#27979;&#32431;MGT&#32780;&#26410;&#20805;&#20998;&#35299;&#20915;&#21253;&#25324;AI&#20462;&#25913;&#30340;&#20154;&#24037;&#25991;&#26412;&#65288;HWT&#65289;&#25110;&#32463;&#20154;&#24037;&#20462;&#25913;&#30340;MGT&#22312;&#20869;&#30340;&#28151;&#21512;&#24773;&#26223;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#28151;&#21512;&#22823;&#23567;&#20889;&#65288;mixcase&#65289;&#36825;&#19968;&#26032;&#27010;&#24565;&#65292;&#34920;&#31034;&#19968;&#31181;&#21516;&#26102;&#28041;&#21450;&#26426;&#22120;&#29983;&#25104;&#21644;&#20154;&#24037;&#29983;&#25104;&#20869;&#23481;&#30340;&#28151;&#21512;&#25991;&#26412;&#24418;&#24335;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#26469;&#33258;&#22810;&#20010;&#26085;&#24120;&#25991;&#26412;&#32534;&#36753;&#22330;&#26223;&#30340;mixcase&#23454;&#20363;&#65292;&#24182;&#26500;&#24314;&#20102;MixSet&#65292;&#36825;&#26159;&#19987;&#29992;&#20110;&#30740;&#31350;&#36825;&#20123;&#28151;&#21512;&#20462;&#25913;&#24773;&#26223;&#30340;&#31532;&#19968;&#20010;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36827;&#34892;&#23454;&#39564;&#26469;&#35780;&#20272;&#27969;&#34892;&#30340;MGT&#26816;&#27979;&#22120;&#30340;&#25928;&#26524;&#12289;&#31283;&#20581;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#29616;&#26377;&#30340;MGT&#26816;&#27979;&#22120;&#23545;&#20110;&#28151;&#21512;&#25991;&#26412;&#30340;&#26816;&#27979;&#25928;&#26524;&#19981;&#29702;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the remarkable development and widespread applications of large language models (LLMs), the use of machine-generated text (MGT) is becoming increasingly common. This trend brings potential risks, particularly to the quality and completeness of information in fields such as news and education. Current research predominantly addresses the detection of pure MGT without adequately addressing mixed scenarios including AI-revised Human-Written Text (HWT) or human-revised MGT. To confront this challenge, we introduce mixcase, a novel concept representing a hybrid text form involving both machine-generated and human-generated content. We collected mixcase instances generated from multiple daily text-editing scenarios and composed MixSet, the first dataset dedicated to studying these mixed modification scenarios. We conduct experiments to evaluate the efficacy of popular MGT detectors, assessing their effectiveness, robustness, and generalization performance. Our findings reveal that exist
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#29992;&#25143;&#29983;&#25104;&#25968;&#25454;&#38598;&#20013;&#31579;&#36873;&#24182;&#27880;&#37322;&#20102;10K&#20010;YouTube&#19978;&#30340;&#26377;&#36259;&#22810;&#27169;&#24577;&#35270;&#39057;&#65292;&#20511;&#21161;GPT-3.5&#39564;&#35777;&#20102;&#35821;&#35328;&#21644;&#35270;&#35273;&#20803;&#32032;&#23545;&#24189;&#40664;&#30340;&#36129;&#29486;&#12290;&#27492;&#22806;&#65292;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#38646;-shot&#35270;&#39057;&#21040;&#25991;&#26412;&#25552;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#35270;&#39057;&#24189;&#40664;&#30340;&#29702;&#35299;&#12290;&#36825;&#20010;&#30740;&#31350;&#22635;&#34917;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#23545;&#22810;&#39046;&#22495;&#22810;&#27169;&#24577;&#24189;&#40664;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2310.14159</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#22066;&#31505;YouTube&#30701;&#35270;&#39057;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Language Models Laugh at YouTube Short-form Videos?. (arXiv:2310.14159v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14159
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#29992;&#25143;&#29983;&#25104;&#25968;&#25454;&#38598;&#20013;&#31579;&#36873;&#24182;&#27880;&#37322;&#20102;10K&#20010;YouTube&#19978;&#30340;&#26377;&#36259;&#22810;&#27169;&#24577;&#35270;&#39057;&#65292;&#20511;&#21161;GPT-3.5&#39564;&#35777;&#20102;&#35821;&#35328;&#21644;&#35270;&#35273;&#20803;&#32032;&#23545;&#24189;&#40664;&#30340;&#36129;&#29486;&#12290;&#27492;&#22806;&#65292;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#38646;-shot&#35270;&#39057;&#21040;&#25991;&#26412;&#25552;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#35270;&#39057;&#24189;&#40664;&#30340;&#29702;&#35299;&#12290;&#36825;&#20010;&#30740;&#31350;&#22635;&#34917;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#23545;&#22810;&#39046;&#22495;&#22810;&#27169;&#24577;&#24189;&#40664;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31038;&#20132;&#32593;&#32476;&#19978;&#30701;&#35270;&#39057;&#30340;&#27969;&#34892;&#65292;&#35201;&#27714;AI&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#20123;&#35270;&#39057;&#20197;&#19982;&#20154;&#31867;&#36827;&#34892;&#26356;&#22909;&#30340;&#20132;&#27969;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#35270;&#39057;&#24189;&#40664;&#25968;&#25454;&#38598;&#20027;&#35201;&#38024;&#23545;&#29305;&#23450;&#39046;&#22495;&#65292;&#22914;&#28436;&#35762;&#25110;&#24773;&#26223;&#21916;&#21095;&#65292;&#24182;&#19988;&#22823;&#22810;&#20851;&#27880;&#35821;&#35328;&#32447;&#32034;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;YouTube&#30340;10K&#20010;&#22810;&#27169;&#24577;&#26377;&#36259;&#35270;&#39057;&#30340;&#29992;&#25143;&#29983;&#25104;&#25968;&#25454;&#38598;&#65292;&#31216;&#20026;ExFunTube&#12290;&#20351;&#29992;&#22522;&#20110;GPT-3.5&#30340;&#35270;&#39057;&#36807;&#28388;&#27969;&#31243;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#35821;&#35328;&#21644;&#35270;&#35273;&#20803;&#32032;&#23545;&#24189;&#40664;&#30340;&#36129;&#29486;&#12290;&#22312;&#36807;&#28388;&#21518;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;&#35270;&#39057;&#30340;&#26377;&#36259;&#26102;&#21051;&#21152;&#19978;&#20102;&#26102;&#38388;&#25139;&#21644;&#25991;&#26412;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;ExFunTube&#22312;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#29420;&#29305;&#20043;&#22788;&#22312;&#20110;&#65292;&#25105;&#20204;&#30340;&#35270;&#39057;&#28085;&#30422;&#20102;&#21508;&#31181;&#31867;&#22411;&#24189;&#40664;&#30340;&#24191;&#27867;&#39046;&#22495;&#65292;&#38656;&#35201;&#23545;&#20869;&#23481;&#36827;&#34892;&#22810;&#27169;&#24577;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#38646;-shot&#35270;&#39057;&#21040;&#25991;&#26412;&#25552;&#31034;&#65292;&#20197;&#26368;&#22823;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#35270;&#39057;&#24189;&#40664;&#30340;&#29702;&#35299;&#12290;&#20351;&#29992;&#33258;&#21160;&#35780;&#20998;&#12289;&#21407;&#29702;&#36136;&#37327;&#23454;&#39564;&#20197;&#21450;&#20154;&#31867;&#35780;&#20215;&#26041;&#27861;&#36827;&#34892;&#19977;&#31181;&#19981;&#21516;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
As short-form funny videos on social networks are gaining popularity, it becomes demanding for AI models to understand them for better communication with humans. Unfortunately, previous video humor datasets target specific domains, such as speeches or sitcoms, and mostly focus on verbal cues. We curate a user-generated dataset of 10K multimodal funny videos from YouTube, called ExFunTube. Using a video filtering pipeline with GPT-3.5, we verify both verbal and visual elements contributing to humor. After filtering, we annotate each video with timestamps and text explanations for funny moments. Our ExFunTube is unique over existing datasets in that our videos cover a wide range of domains with various types of humor that necessitate a multimodal understanding of the content. Also, we develop a zero-shot video-to-text prompting to maximize video humor understanding of large language models (LLMs). With three different evaluation methods using automatic scores, rationale quality experimen
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20114;&#21160;&#28436;&#31034;&#30340;&#26041;&#24335;&#65292;&#25945;&#23548;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#33258;&#25105;&#25552;&#21319;&#33021;&#21147;&#65292;&#20943;&#23567;&#20102;&#26368;&#20808;&#36827;&#27169;&#22411;&#19982;&#25104;&#26412;&#25928;&#30410;&#26356;&#39640;&#27169;&#22411;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2310.13522</link><description>&lt;p&gt;
&#25945;&#23548;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#20114;&#21160;&#28436;&#31034;&#33258;&#25105;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;
Teaching Language Models to Self-Improve through Interactive Demonstrations. (arXiv:2310.13522v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13522
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20114;&#21160;&#28436;&#31034;&#30340;&#26041;&#24335;&#65292;&#25945;&#23548;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#33258;&#25105;&#25552;&#21319;&#33021;&#21147;&#65292;&#20943;&#23567;&#20102;&#26368;&#20808;&#36827;&#27169;&#22411;&#19982;&#25104;&#26412;&#25928;&#30410;&#26356;&#39640;&#27169;&#22411;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#25552;&#31034;&#20854;&#20998;&#26512;&#21644;&#20462;&#35746;&#33258;&#24049;&#30340;&#36755;&#20986;&#26469;&#23454;&#29616;&#33258;&#25105;&#25552;&#21319;&#30340;&#33021;&#21147;&#65292;&#36817;&#24180;&#26469;&#22312;&#30740;&#31350;&#20013;&#24341;&#36215;&#20102;&#26174;&#33879;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33021;&#21147;&#22312;&#36739;&#23567;&#30340;&#27169;&#22411;&#20013;&#34987;&#35777;&#26126;&#26159;&#32570;&#22833;&#19988;&#38590;&#20197;&#23398;&#20064;&#30340;&#65292;&#20174;&#32780;&#25193;&#22823;&#20102;&#26368;&#20808;&#36827;&#30340;LLM&#19982;&#25104;&#26412;&#25928;&#30410;&#26356;&#39640;&#19988;&#36895;&#24230;&#26356;&#24555;&#30340;&#27169;&#22411;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#20026;&#20102;&#20943;&#23567;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;TriPosT&#65292;&#19968;&#31181;&#35757;&#32451;&#31639;&#27861;&#65292;&#36171;&#20104;&#36739;&#23567;&#30340;&#27169;&#22411;&#36825;&#31181;&#33258;&#25105;&#25552;&#21319;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23558;LLaMA-7b&#22312;&#25968;&#23398;&#21644;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#39640;&#36798;7.13%&#12290;&#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#36739;&#23567;&#30340;&#27169;&#22411;&#19982;LLM&#36827;&#34892;&#20114;&#21160;&#20197;&#25910;&#38598;&#21453;&#39304;&#21644;&#25913;&#36827;&#33258;&#36523;&#29983;&#25104;&#30340;&#32467;&#26524;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37325;&#25773;&#36825;&#19968;&#32463;&#39564;&#26469;&#35757;&#32451;&#23567;&#22411;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#25968;&#23398;&#21644;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20174;&#24182;&#32416;&#27491;&#33258;&#24049;&#30340;&#38169;&#35823;&#20013;&#36827;&#34892;&#20114;&#21160;&#23398;&#20064;&#30340;&#32463;&#39564;&#23545;&#20110;&#23567;&#22411;&#27169;&#22411;&#30340;&#25552;&#21319;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
The self-improving ability of large language models (LLMs), enabled by prompting them to analyze and revise their own outputs, has garnered significant interest in recent research. However, this ability has been shown to be absent and difficult to learn for smaller models, thus widening the performance gap between state-of-the-art LLMs and more cost-effective and faster ones. To reduce this gap, we introduce TriPosT, a training algorithm that endows smaller models with such self-improvement ability, and show that our approach can improve a LLaMA-7b's performance on math and reasoning tasks by up to 7.13%. In contrast to prior work, we achieve this by using the smaller model to interact with LLMs to collect feedback and improvements on its own generations. We then replay this experience to train the small model. Our experiments on four math and reasoning datasets show that the interactive experience of learning from and correcting its own mistakes is crucial for small models to improve 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24212;&#29992;&#32467;&#26500;&#21270;&#35748;&#30693;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#24230;&#32467;&#26500;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#38543;&#26426;&#21270;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.09518</link><description>&lt;p&gt;
&#20154;&#31867;&#35838;&#31243;&#25351;&#23548;&#19979;&#30340;&#25351;&#20196;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Instruction Tuning with Human Curriculum. (arXiv:2310.09518v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24212;&#29992;&#32467;&#26500;&#21270;&#35748;&#30693;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#24230;&#32467;&#26500;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#38543;&#26426;&#21270;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#30340;&#20027;&#27969;&#33539;&#24335;&#26159;&#38543;&#26426;&#27927;&#29260;&#35757;&#32451;&#26368;&#22823;&#22810;&#26679;&#21270;&#25351;&#20196;-&#21709;&#24212;&#23545;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#24403;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;ChatGPT&#21644;GPT-4&#20013;&#24212;&#29992;&#32467;&#26500;&#21270;&#35748;&#30693;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#30340;&#28508;&#22312;&#22909;&#22788;&#12290;&#19982;&#20197;&#24448;&#20256;&#32479;&#30340;&#38543;&#26426;&#21270;&#25351;&#20196;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#24230;&#32467;&#26500;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#27169;&#25311;&#20102;&#20154;&#31867;&#25945;&#32946;&#30340;&#28176;&#36827;&#24615;&#21644;&#26377;&#32452;&#32455;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#25968;&#25454;&#38598;&#19982;&#25945;&#32946;&#26694;&#26550;&#23545;&#40784;&#26469;&#31574;&#21010;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#65292;&#20026;&#27599;&#20010;&#26679;&#26412;&#21253;&#25324;&#20027;&#39064;&#21644;&#35748;&#30693;&#20005;&#35880;&#31243;&#24230;&#31561;&#20803;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#20174;&#20013;&#23398;&#21040;&#30740;&#31350;&#29983;&#38454;&#27573;&#30340;&#20840;&#38754;&#32454;&#31890;&#24230;&#20027;&#39064;&#65292;&#27599;&#20010;&#20027;&#39064;&#37117;&#26377;&#21508;&#31181;&#38382;&#39064;&#65292;&#20197;&#21033;&#29992;&#24067;&#40065;&#22982;&#30340;&#35748;&#30693;&#20998;&#32423;&#27861;&#25552;&#39640;&#27010;&#24565;&#28145;&#24230;&#65292;&#35813;&#20998;&#32423;&#27861;&#29992;&#20110;&#21306;&#20998;&#27599;&#20010;&#27010;&#24565;&#30340;&#19981;&#21516;&#20154;&#31867;&#35748;&#30693;&#27700;&#24179;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#35748;&#30693;&#23398;&#20064;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#38543;&#26426;&#21270;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dominant paradigm for instruction tuning is the random-shuffled training of maximally diverse instruction-response pairs. This paper explores the potential benefits of applying a structured cognitive learning approach to instruction tuning in contemporary large language models like ChatGPT and GPT-4. Unlike the previous conventional randomized instruction dataset, we propose a highly structured synthetic dataset that mimics the progressive and organized nature of human education. We curate our dataset by aligning it with educational frameworks, incorporating meta information including its topic and cognitive rigor level for each sample. Our dataset covers comprehensive fine-grained topics spanning diverse educational stages (from middle school to graduate school) with various questions for each topic to enhance conceptual depth using Bloom's taxonomy-a classification framework distinguishing various levels of human cognition for each concept. The results demonstrate that this cogni
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#20998;&#24067;&#24335;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#22788;&#29702;&#25105;&#20204;&#26032;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#26102;&#37117;&#23384;&#22312;&#22256;&#38590;&#65292;&#23613;&#31649;&#23427;&#20204;&#22312;&#20854;&#20182;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#36825;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#30340;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#20173;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2310.09430</link><description>&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#20998;&#24067;&#24335;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#31995;&#32479;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Systematic Evaluation of Large Language Models on Out-of-Distribution Logical Reasoning Tasks. (arXiv:2310.09430v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09430
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#20998;&#24067;&#24335;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#22788;&#29702;&#25105;&#20204;&#26032;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#26102;&#37117;&#23384;&#22312;&#22256;&#38590;&#65292;&#23613;&#31649;&#23427;&#20204;&#22312;&#20854;&#20182;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#36825;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#30340;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#20173;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT-3.5&#21644;GPT-4&#65292;&#24050;&#32463;&#23558;&#20154;&#24037;&#31995;&#32479;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#21040;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#30340;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#35780;&#20272;&#12290;&#20026;&#20102;&#25506;&#32034;&#36825;&#31181;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#26032;&#30340;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#20998;&#21035;&#21517;&#20026;"ReClor-plus"&#12289;"LogiQA-plus"&#21644;"LogiQAv2-plus"&#65292;&#27599;&#20010;&#25968;&#25454;&#38598;&#37117;&#21253;&#21547;&#19977;&#20010;&#23376;&#38598;&#65306;&#31532;&#19968;&#20010;&#26159;&#36873;&#39033;&#38543;&#26426;&#25171;&#20081;&#65292;&#31532;&#20108;&#20010;&#26159;&#23558;&#27491;&#30830;&#36873;&#39033;&#26367;&#25442;&#20026;"&#27809;&#26377;&#20854;&#20182;&#36873;&#39033;&#26159;&#27491;&#30830;&#30340;"&#65292;&#31532;&#19977;&#20010;&#26159;&#21069;&#20004;&#20010;&#23376;&#38598;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20351;&#29992;&#20102;&#37492;&#21035;&#21644;&#29983;&#25104;&#22411;&#30340;LLMs&#65292;&#24182;&#34920;&#26126;&#36825;&#20123;&#31616;&#21333;&#30340;&#25216;&#24039;&#26497;&#22823;&#22320;&#38459;&#30861;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#22312;&#21407;&#22987;&#30340;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#25152;&#26377;&#27169;&#22411;&#37117;&#24456;&#38590;&#22238;&#31572;&#25105;&#20204;&#26032;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#25200;&#21160;&#24341;&#20837;&#20219;&#21153;&#21464;&#21270;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), such as GPT-3.5 and GPT-4, have greatly advanced the performance of artificial systems on various natural language processing tasks to human-like levels. However, their generalisation and robustness to perform logical reasoning remain under-evaluated. To probe this ability, we propose three new logical reasoning datasets named "ReClor-plus", "LogiQA-plus" and "LogiQAv2-plus", each featuring three subsets: the first with randomly shuffled options, the second with the correct choices replaced by "none of the other options are correct", and a combination of the previous two subsets. We carry out experiments on these datasets with both discriminative and generative LLMs and show that these simple tricks greatly hinder the performance of the language models. Despite their superior performance on the original publicly available datasets, we find that all models struggle to answer our newly constructed datasets. We show that introducing task variations by perturb
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#23558;&#35821;&#35328;&#20316;&#20026;&#23548;&#33322;&#30340;&#24863;&#30693;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#29616;&#25104;&#30340;&#35270;&#35273;&#31995;&#32479;&#23558;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#30340;&#35270;&#22270;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36873;&#25321;&#26368;&#20339;&#30340;&#34892;&#21160;&#26469;&#28385;&#36275;&#23548;&#33322;&#25351;&#20196;&#12290;&#26377;&#20004;&#31181;&#29992;&#20363;&#30340;&#23454;&#39564;&#23545;&#36825;&#31181;&#22522;&#20110;&#35821;&#35328;&#30340;&#23548;&#33322;&#26041;&#27861;&#36827;&#34892;&#20102;&#25506;&#32034;&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#36712;&#36857;&#20197;&#24494;&#35843;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#21450;&#27169;&#25311;&#21040;&#23454;&#38469;&#30340;&#36716;&#25442;&#12290;</title><link>http://arxiv.org/abs/2310.07889</link><description>&lt;p&gt;
LangNav: &#35821;&#35328;&#20316;&#20026;&#23548;&#33322;&#30340;&#24863;&#30693;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
LangNav: Language as a Perceptual Representation for Navigation. (arXiv:2310.07889v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07889
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#23558;&#35821;&#35328;&#20316;&#20026;&#23548;&#33322;&#30340;&#24863;&#30693;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#29616;&#25104;&#30340;&#35270;&#35273;&#31995;&#32479;&#23558;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#30340;&#35270;&#22270;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36873;&#25321;&#26368;&#20339;&#30340;&#34892;&#21160;&#26469;&#28385;&#36275;&#23548;&#33322;&#25351;&#20196;&#12290;&#26377;&#20004;&#31181;&#29992;&#20363;&#30340;&#23454;&#39564;&#23545;&#36825;&#31181;&#22522;&#20110;&#35821;&#35328;&#30340;&#23548;&#33322;&#26041;&#27861;&#36827;&#34892;&#20102;&#25506;&#32034;&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#36712;&#36857;&#20197;&#24494;&#35843;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#21450;&#27169;&#25311;&#21040;&#23454;&#38469;&#30340;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#32034;&#23558;&#35821;&#35328;&#20316;&#20026;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#30340;&#24863;&#30693;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#29616;&#25104;&#30340;&#35270;&#35273;&#31995;&#32479;&#65288;&#29992;&#20110;&#22270;&#20687;&#23383;&#24149;&#21644;&#29289;&#20307;&#26816;&#27979;&#65289;&#23558;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#20013;&#20195;&#29702;&#20154;&#30340;&#33258;&#25105;&#20013;&#24515;&#20840;&#26223;&#35270;&#22270;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#26681;&#25454;&#24403;&#21069;&#35270;&#22270;&#21644;&#36712;&#36857;&#21382;&#21490;&#36873;&#25321;&#26368;&#20339;&#30340;&#34892;&#21160;&#26469;&#28385;&#36275;&#23548;&#33322;&#25351;&#20196;&#12290;&#19982;&#26631;&#20934;&#35774;&#32622;&#30456;&#27604;&#65292;&#26631;&#20934;&#35774;&#32622;&#23558;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#20110;&#19982;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#27169;&#22411;&#36830;&#32493;&#35270;&#35273;&#29305;&#24449;&#30452;&#25509;&#37197;&#21512;&#20351;&#29992;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#65288;&#31163;&#25955;&#30340;&#65289;&#35821;&#35328;&#20316;&#20026;&#24863;&#30693;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;R2R&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#22522;&#20934;&#27979;&#35797;&#20013;&#25506;&#32034;&#20102;&#20004;&#20010;&#29992;&#20363;&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;GPT-4&#65289;&#29983;&#25104;&#21512;&#25104;&#36712;&#36857;&#65292;&#20197;&#20415;&#24494;&#35843;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#65307;&#20197;&#21450;&#27169;&#25311;&#21040;&#23454;&#38469;&#30340;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the use of language as a perceptual representation for vision-and-language navigation. Our approach uses off-the-shelf vision systems (for image captioning and object detection) to convert an agent's egocentric panoramic view at each time step into natural language descriptions. We then finetune a pretrained language model to select an action, based on the current view and the trajectory history, that would best fulfill the navigation instructions. In contrast to the standard setup which adapts a pretrained language model to work directly with continuous visual features from pretrained vision models, our approach instead uses (discrete) language as the perceptual representation. We explore two use cases of our language-based navigation (LangNav) approach on the R2R vision-and-language navigation benchmark: generating synthetic trajectories from a prompted large language model (GPT-4) with which to finetune a smaller language model; and sim-to-real transfer where we transfer 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32418;&#38431;&#28216;&#25103;&#65288;RTG&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#21338;&#24328;&#35770;&#20998;&#26512;&#20102;&#32418;&#38431;&#35821;&#35328;&#27169;&#22411;&#65288;RLM&#65289;&#19982;&#34013;&#38431;&#35821;&#35328;&#27169;&#22411;&#65288;BLM&#65289;&#20043;&#38388;&#30340;&#22810;&#36718;&#25915;&#38450;&#20114;&#21160;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#28216;&#25103;&#21270;&#32418;&#38431;&#27714;&#35299;&#22120;&#65288;GRTS&#65289;&#26469;&#25552;&#20379;&#33258;&#21160;&#21270;&#30340;&#32418;&#38431;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2310.00322</link><description>&lt;p&gt;
&#32418;&#38431;&#28216;&#25103;&#65306;&#32418;&#38431;&#35821;&#35328;&#27169;&#22411;&#30340;&#21338;&#24328;&#35770;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Red Teaming Game: A Game-Theoretic Framework for Red Teaming Language Models. (arXiv:2310.00322v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00322
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32418;&#38431;&#28216;&#25103;&#65288;RTG&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#21338;&#24328;&#35770;&#20998;&#26512;&#20102;&#32418;&#38431;&#35821;&#35328;&#27169;&#22411;&#65288;RLM&#65289;&#19982;&#34013;&#38431;&#35821;&#35328;&#27169;&#22411;&#65288;BLM&#65289;&#20043;&#38388;&#30340;&#22810;&#36718;&#25915;&#38450;&#20114;&#21160;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#28216;&#25103;&#21270;&#32418;&#38431;&#27714;&#35299;&#22120;&#65288;GRTS&#65289;&#26469;&#25552;&#20379;&#33258;&#21160;&#21270;&#30340;&#32418;&#38431;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#37096;&#32626;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24517;&#39035;&#31526;&#21512;&#26377;&#30410;&#21644;&#26080;&#23475;&#24615;&#30340;&#26631;&#20934;&#65292;&#20174;&#32780;&#23454;&#29616;LLM&#36755;&#20986;&#19982;&#20154;&#31867;&#20215;&#20540;&#30340;&#19968;&#33268;&#24615;&#12290;&#32418;&#38431;&#25216;&#26415;&#26159;&#23454;&#29616;&#36825;&#19968;&#26631;&#20934;&#30340;&#20851;&#38190;&#36884;&#24452;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#20165;&#20381;&#36182;&#20110;&#25163;&#21160;&#32418;&#38431;&#35774;&#35745;&#21644;&#21551;&#21457;&#24335;&#23545;&#25239;&#25552;&#31034;&#36827;&#34892;&#28431;&#27934;&#26816;&#27979;&#21644;&#20248;&#21270;&#12290;&#36825;&#20123;&#26041;&#27861;&#32570;&#20047;&#20005;&#26684;&#30340;&#25968;&#23398;&#24418;&#24335;&#21270;&#65292;&#38480;&#21046;&#20102;&#22312;&#21487;&#37327;&#21270;&#24230;&#37327;&#21644;&#25910;&#25947;&#20445;&#35777;&#19979;&#23545;LLM&#36827;&#34892;&#22810;&#26679;&#25915;&#20987;&#31574;&#30053;&#30340;&#25506;&#32034;&#21644;&#20248;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32418;&#38431;&#28216;&#25103;&#65288;RTG&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#26080;&#38656;&#25163;&#21160;&#26631;&#27880;&#30340;&#21338;&#24328;&#35770;&#26694;&#26550;&#12290;RTG&#26088;&#22312;&#20998;&#26512;&#32418;&#38431;&#35821;&#35328;&#27169;&#22411;&#65288;RLM&#65289;&#19982;&#34013;&#38431;&#35821;&#35328;&#27169;&#22411;&#65288;BLM&#65289;&#20043;&#38388;&#30340;&#22810;&#36718;&#25915;&#38450;&#20114;&#21160;&#12290;&#22312;RTG&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#35821;&#20041;&#31354;&#38388;&#22810;&#26679;&#24615;&#24230;&#37327;&#30340;&#28216;&#25103;&#21270;&#32418;&#38431;&#27714;&#35299;&#22120;&#65288;GRTS&#65289;&#12290;GRTS&#26159;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#32418;&#38431;&#25216;&#26415;&#65292;&#29992;&#20110;&#35299;&#20915;&#32418;&#38431;&#28216;&#25103;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deployable Large Language Models (LLMs) must conform to the criterion of helpfulness and harmlessness, thereby achieving consistency between LLMs outputs and human values. Red-teaming techniques constitute a critical way towards this criterion. Existing work rely solely on manual red team designs and heuristic adversarial prompts for vulnerability detection and optimization. These approaches lack rigorous mathematical formulation, thus limiting the exploration of diverse attack strategy within quantifiable measure and optimization of LLMs under convergence guarantees. In this paper, we present Red-teaming Game (RTG), a general game-theoretic framework without manual annotation. RTG is designed for analyzing the multi-turn attack and defense interactions between Red-team language Models (RLMs) and Blue-team Language Model (BLM). Within the RTG, we propose Gamified Red-teaming Solver (GRTS) with diversity measure of the semantic space. GRTS is an automated red teaming technique to solve 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20107;&#23454;&#30693;&#35782;&#25552;&#21319;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#25928;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#30693;&#35782;&#19978;&#19979;&#25991;&#35843;&#20248;&#26694;&#26550;&#26469;&#25913;&#21892;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.14771</link><description>&lt;p&gt;
&#20351;&#29992;&#20107;&#23454;&#30693;&#35782;&#25552;&#21319;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Boosting In-Context Learning with Factual Knowledge. (arXiv:2309.14771v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20107;&#23454;&#30693;&#35782;&#25552;&#21319;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#25928;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#30693;&#35782;&#19978;&#19979;&#25991;&#35843;&#20248;&#26694;&#26550;&#26469;&#25913;&#21892;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#26088;&#22312;&#36890;&#36807;&#20381;&#36182;&#20110;&#23569;&#37327;&#30340;&#35757;&#32451;&#31034;&#20363;&#35299;&#20915;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#65292;&#20174;&#32780;&#28040;&#38500;&#21442;&#25968;&#26356;&#26032;&#30340;&#38656;&#27714;&#65292;&#24182;&#23454;&#29616;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#20107;&#23454;&#30693;&#35782;&#22312;ICL&#30340;&#24615;&#33021;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#21253;&#25324;&#22312;LLM&#20013;&#23398;&#21040;&#30340;&#22266;&#26377;&#30693;&#35782;&#65292;&#20174;&#25152;&#36873;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#20013;&#24471;&#20986;&#30340;&#20107;&#23454;&#30693;&#35782;&#65292;&#20197;&#21450;LLM&#22312;&#36755;&#20986;&#29983;&#25104;&#20013;&#30340;&#30693;&#35782;&#20559;&#24046;&#12290;&#20026;&#20102;&#21457;&#25381;LLM&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#20013;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#30693;&#35782;&#19978;&#19979;&#25991;&#35843;&#20248;&#65288;KICT&#65289;&#26694;&#26550;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;ICL&#30340;&#24615;&#33021;&#65306;1&#65289;&#22312;&#25345;&#32493;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26399;&#38388;&#21521;LLM&#27880;&#20837;&#20107;&#23454;&#30693;&#35782;&#65292;2&#65289;&#35880;&#24910;&#36873;&#25321;&#20855;&#26377;&#39640;&#30693;&#35782;&#30456;&#20851;&#24615;&#30340;&#31034;&#20363;&#65292;3&#65289;&#26681;&#25454;&#20808;&#21069;&#30340;&#30693;&#35782;&#23545;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#26657;&#20934;&#12290;&#25105;&#20204;&#22312;&#33258;&#22238;&#24402;LLM&#65288;&#22914;GPT&#39118;&#26684;&#27169;&#22411;&#65289;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-Context Learning (ICL) over Large language models (LLMs) aims at solving previously unseen tasks by conditioning on a few training examples, eliminating the need for parameter updates and achieving competitive performance. In this paper, we demonstrate that factual knowledge is imperative for the performance of ICL in three core facets, i.e., the inherent knowledge learned in LLMs, the factual knowledge derived from the selected in-context examples, and the knowledge biases in LLMs for output generation. To unleash the power of LLMs in few-shot learning scenarios, we introduce a novel Knowledgeable In-Context Tuning (KICT) framework to further improve the performance of ICL: 1) injecting factual knowledge to LLMs during continual self-supervised pre-training, 2) judiciously selecting the examples with high knowledge relevance, and 3) calibrating the prediction results based on prior knowledge. We evaluate the proposed approaches on auto-regressive LLMs (e.g., GPT-style models) over 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;FP8&#26684;&#24335;&#22312;&#21518;&#35757;&#32451;&#37327;&#21270;&#20013;&#30340;&#20248;&#21183;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#37327;&#21270;&#24037;&#20316;&#27969;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FP8&#30456;&#23545;&#20110;INT8&#20855;&#26377;&#26356;&#22909;&#30340;&#24037;&#20316;&#36127;&#36733;&#35206;&#30422;&#29575;&#21644;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14592</link><description>&lt;p&gt;
&#20351;&#29992;FP8&#26684;&#24335;&#30340;&#39640;&#25928;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Post-training Quantization with FP8 Formats. (arXiv:2309.14592v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;FP8&#26684;&#24335;&#22312;&#21518;&#35757;&#32451;&#37327;&#21270;&#20013;&#30340;&#20248;&#21183;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#37327;&#21270;&#24037;&#20316;&#27969;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FP8&#30456;&#23545;&#20110;INT8&#20855;&#26377;&#26356;&#22909;&#30340;&#24037;&#20316;&#36127;&#36733;&#35206;&#30422;&#29575;&#21644;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#36827;&#23637;&#65292;&#22914;LLMs&#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#23545;&#25913;&#36827;&#30340;&#37327;&#21270;&#26041;&#27861;&#30340;&#38656;&#27714;&#65292;&#20197;&#28385;&#36275;&#36825;&#20123;&#29616;&#20195;&#26550;&#26500;&#30340;&#35745;&#31639;&#38656;&#27714;&#65292;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;FP8&#25968;&#25454;&#26684;&#24335;&#22312;75&#20010;&#19981;&#21516;&#32593;&#32476;&#26550;&#26500;&#19978;&#36827;&#34892;&#21518;&#35757;&#32451;&#37327;&#21270;&#30340;&#20248;&#21183;&#65292;&#36825;&#20123;&#32593;&#32476;&#26550;&#26500;&#28085;&#30422;&#20102;&#22810;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#26426;&#22120;&#32763;&#35793;&#12289;&#35821;&#35328;&#24314;&#27169;&#12289;&#25991;&#26412;&#29983;&#25104;&#12289;&#22270;&#20687;&#20998;&#31867;&#12289;&#29983;&#25104;&#21644;&#20998;&#21106;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;FP8&#34920;&#31034;&#65288;E5M2&#12289;E4M3&#21644;E3M4&#65289;&#65292;&#20197;&#30740;&#31350;&#22312;&#21160;&#24577;&#33539;&#22260;&#21644;&#31934;&#24230;&#20043;&#38388;&#19981;&#21516;&#26435;&#34913;&#31243;&#24230;&#23545;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#24191;&#27867;&#30740;&#31350;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#37327;&#21270;&#24037;&#20316;&#27969;&#31243;&#65292;&#21487;&#20197;&#27010;&#25324;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#32593;&#32476;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;FP8&#26684;&#24335;&#22312;&#22810;&#20010;&#26041;&#38754;&#20248;&#20110;INT8&#65292;&#21253;&#25324;&#24037;&#20316;&#36127;&#36733;&#35206;&#30422;&#29575;&#65288;92.64&#65285;&#23545;65.87&#65285;&#65289;&#12289;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#36866;&#29992;&#20110;&#26356;&#24191;&#27867;&#30340;&#25805;&#20316;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in deep learning methods such as LLMs and Diffusion models have created a need for improved quantization methods that can meet the computational demands of these modern architectures while maintaining accuracy. Towards this goal, we study the advantages of FP8 data formats for post-training quantization across 75 unique network architectures covering a wide range of tasks, including machine translation, language modeling, text generation, image classification, generation, and segmentation. We examine three different FP8 representations (E5M2, E4M3, and E3M4) to study the effects of varying degrees of trade-off between dynamic range and precision on model accuracy. Based on our extensive study, we developed a quantization workflow that generalizes across different network architectures. Our empirical results show that FP8 formats outperform INT8 in multiple aspects, including workload coverage (92.64% vs. 65.87%), model accuracy and suitability for a broader range of ope
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;LLMs&#22312;&#23545;&#35805;&#29615;&#22659;&#20013;&#36816;&#29992;&#35866;&#35821;&#21644;&#20439;&#35821;&#36827;&#34892;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;mLLMs&#22312;&#29702;&#35299;&#27604;&#21947;&#24615;&#35866;&#35821;&#12289;&#36873;&#25321;&#27491;&#30830;&#31572;&#26696;&#21644;&#25512;&#29702;&#20854;&#20182;&#35821;&#35328;&#30340;&#35866;&#35821;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2309.08591</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;LLMs&#26159;&#21542;&#20855;&#26377;&#25991;&#21270;&#22810;&#26679;&#24615;&#30340;&#25512;&#29702;&#33021;&#21147;&#65311;&#23545;&#22810;&#20803;&#25991;&#21270;&#35866;&#35821;&#21644;&#20439;&#35821;&#30340;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Are Multilingual LLMs Culturally-Diverse Reasoners? An Investigation into Multicultural Proverbs and Sayings. (arXiv:2309.08591v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08591
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;LLMs&#22312;&#23545;&#35805;&#29615;&#22659;&#20013;&#36816;&#29992;&#35866;&#35821;&#21644;&#20439;&#35821;&#36827;&#34892;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;mLLMs&#22312;&#29702;&#35299;&#27604;&#21947;&#24615;&#35866;&#35821;&#12289;&#36873;&#25321;&#27491;&#30830;&#31572;&#26696;&#21644;&#25512;&#29702;&#20854;&#20182;&#35821;&#35328;&#30340;&#35866;&#35821;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#38382;&#31572;&#21644;&#25512;&#29702;&#20219;&#21153;&#26041;&#38754;&#38750;&#24120;&#25797;&#38271;&#65292;&#20294;&#22312;&#24773;&#22659;&#32972;&#26223;&#19979;&#36827;&#34892;&#25512;&#29702;&#26102;&#65292;&#20154;&#31867;&#30340;&#26399;&#26395;&#22240;&#30456;&#20851;&#25991;&#21270;&#20849;&#21516;&#28857;&#32780;&#24322;&#12290;&#30001;&#20110;&#20154;&#31867;&#35821;&#35328;&#19982;&#22810;&#31181;&#25991;&#21270;&#30456;&#20851;&#32852;&#65292;LLMs&#20063;&#24212;&#35813;&#26159;&#20855;&#26377;&#25991;&#21270;&#22810;&#26679;&#24615;&#30340;&#25512;&#29702;&#32773;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31995;&#21015;&#26368;&#20808;&#36827;&#30340;&#22810;&#35821;&#35328;LLMs&#65288;mLLMs&#65289;&#22312;&#23545;&#35805;&#29615;&#22659;&#20013;&#36816;&#29992;&#35866;&#35821;&#21644;&#20439;&#35821;&#36827;&#34892;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65306;&#65288;1&#65289;mLLMs&#21482;&#8220;&#30693;&#36947;&#8221;&#26377;&#38480;&#30340;&#35866;&#35821;&#65292;&#24182;&#19988;&#20165;&#20165;&#35760;&#20303;&#35866;&#35821;&#24182;&#19981;&#33021;&#22312;&#23545;&#35805;&#29615;&#22659;&#20013;&#29702;&#35299;&#23427;&#20204;&#65307;&#65288;2&#65289;mLLMs&#22312;&#25512;&#29702;&#27604;&#21947;&#24615;&#30340;&#35866;&#35821;&#21644;&#20439;&#35821;&#26102;&#36935;&#21040;&#22256;&#38590;&#65292;&#24403;&#34987;&#35201;&#27714;&#36873;&#25321;&#38169;&#35823;&#31572;&#26696;&#26102;&#65292;&#32780;&#19981;&#26159;&#36873;&#25321;&#27491;&#30830;&#31572;&#26696;&#65307;&#65288;3&#65289;&#22312;&#25512;&#29702;&#26469;&#33258;&#20854;&#20182;&#35821;&#35328;&#30340;&#35866;&#35821;&#21644;&#20439;&#35821;&#26102;&#65292;mLLMs&#23384;&#22312;&#8220;&#25991;&#21270;&#24046;&#36317;&#8221;&#12290;&#25105;&#20204;&#26500;&#24314;&#24182;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;MAPS&#65288;&#22810;&#20803;&#25991;&#21270;&#35866;&#35821;&#21644;&#20439;&#35821;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are highly adept at question answering and reasoning tasks, but when reasoning in situational context, human expectations vary depending on the relevant cultural common ground. As human languages are associated with diverse cultures, LLMs should also be culturally-diverse reasoners. In this paper, we study the ability of a wide range of state-of-the-art multilingual LLMs (mLLMs) to reason with proverbs and sayings in a conversational context. Our experiments reveal that: (1) mLLMs 'knows' limited proverbs and memorizing proverbs does not mean understanding them within a conversational context; (2) mLLMs struggle to reason with figurative proverbs and sayings, and when asked to select the wrong answer (instead of asking it to select the correct answer); and (3) there is a "culture gap" in mLLMs when reasoning about proverbs and sayings translated from other languages. We construct and release our evaluation dataset MAPS (MulticultrAl Proverbs and Sayings) fo
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#19968;&#20010;&#26032;&#39062;&#30340;&#27602;&#24615;&#20820;&#23376;&#27934;&#26694;&#26550;&#23545;PaLM 2&#30340;&#23433;&#20840;&#21453;&#39304;&#36827;&#34892;&#20102;&#31283;&#20581;&#24615;&#23457;&#35745;&#65292;&#25581;&#31034;&#20102;PaLM 2&#29983;&#25104;&#30340;&#39640;&#24230;&#20196;&#20154;&#19981;&#23433;&#30340;&#27602;&#24615;&#20869;&#23481;&#26410;&#34987;&#23433;&#20840;&#23432;&#25252;&#26639;&#35780;&#20272;&#20026;&#39640;&#24230;&#19981;&#23433;&#20840;&#12290;</title><link>http://arxiv.org/abs/2309.06415</link><description>&lt;p&gt;
&#28145;&#20837;&#27602;&#24615;&#20820;&#23376;&#27934;&#65306;&#36890;&#36807;PaLM 2&#30340;&#23432;&#25252;&#26639;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Down the Toxicity Rabbit Hole: Investigating PaLM 2 Guardrails. (arXiv:2309.06415v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06415
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#19968;&#20010;&#26032;&#39062;&#30340;&#27602;&#24615;&#20820;&#23376;&#27934;&#26694;&#26550;&#23545;PaLM 2&#30340;&#23433;&#20840;&#21453;&#39304;&#36827;&#34892;&#20102;&#31283;&#20581;&#24615;&#23457;&#35745;&#65292;&#25581;&#31034;&#20102;PaLM 2&#29983;&#25104;&#30340;&#39640;&#24230;&#20196;&#20154;&#19981;&#23433;&#30340;&#27602;&#24615;&#20869;&#23481;&#26410;&#34987;&#23433;&#20840;&#23432;&#25252;&#26639;&#35780;&#20272;&#20026;&#39640;&#24230;&#19981;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#21517;&#20026;&#8220;&#27602;&#24615;&#20820;&#23376;&#27934;&#8221;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#23545;PaLM 2&#30340;&#23433;&#20840;&#21453;&#39304;&#36827;&#34892;&#20102;&#24378;&#21270;&#31283;&#20581;&#24615;&#23457;&#35745;&#12290;&#20174;&#19968;&#20010;&#21051;&#26495;&#21360;&#35937;&#24320;&#22987;&#65292;&#35813;&#26694;&#26550;&#25351;&#31034;PaLM 2&#29983;&#25104;&#27604;&#21051;&#26495;&#21360;&#35937;&#26356;&#20855;&#26377;&#27602;&#24615;&#30340;&#20869;&#23481;&#12290;&#27599;&#19968;&#27425;&#36845;&#20195;&#65292;&#23427;&#37117;&#35201;&#27714;PaLM 2&#29983;&#25104;&#27604;&#19978;&#19968;&#27425;&#36845;&#20195;&#26356;&#20855;&#26377;&#27602;&#24615;&#30340;&#20869;&#23481;&#65292;&#30452;&#21040;PaLM 2&#30340;&#23433;&#20840;&#23432;&#25252;&#26639;&#21457;&#20986;&#23433;&#20840;&#36829;&#35268;&#35686;&#25253;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#26497;&#20854;&#20196;&#20154;&#19981;&#23433;&#30340;&#21453;&#29369;&#22826;&#20027;&#20041;&#12289;&#20234;&#26031;&#20848;&#24656;&#24807;&#30151;&#12289;&#31181;&#26063;&#20027;&#20041;&#12289;&#24656;&#21516;&#21644;&#21388;&#22899;&#24773;&#32490;&#65288;&#20165;&#21015;&#20030;&#20960;&#31181;&#65289;&#30340;&#29983;&#25104;&#20869;&#23481;&#65292;&#24182;&#19988;&#36825;&#20123;&#20869;&#23481;&#22312;PaLM 2&#30340;&#23433;&#20840;&#23432;&#25252;&#26639;&#35780;&#20272;&#20013;&#24182;&#26410;&#34987;&#35270;&#20026;&#39640;&#24230;&#19981;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper conducts a robustness audit of the safety feedback of PaLM 2 through a novel toxicity rabbit hole framework introduced here. Starting with a stereotype, the framework instructs PaLM 2 to generate more toxic content than the stereotype. Every subsequent iteration it continues instructing PaLM 2 to generate more toxic content than the previous iteration until PaLM 2 safety guardrails throw a safety violation. Our experiments uncover highly disturbing antisemitic, Islamophobic, racist, homophobic, and misogynistic (to list a few) generated content that PaLM 2 safety guardrails do not evaluate as highly unsafe.
&lt;/p&gt;</description></item><item><title>SeaEval&#26159;&#19968;&#20010;&#35780;&#20272;&#22810;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#30740;&#31350;&#20102;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#25512;&#29702;&#20197;&#21450;&#23545;&#25991;&#21270;&#23454;&#36341;&#12289;&#32454;&#24494;&#24046;&#21035;&#21644;&#20215;&#20540;&#35266;&#30340;&#29702;&#35299;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#12290;&#37325;&#35201;&#21457;&#29616;&#21253;&#25324;&#27169;&#22411;&#22312;&#32473;&#20986;&#25913;&#20889;&#25351;&#20196;&#26102;&#34892;&#20026;&#21508;&#24322;&#65292;&#21463;&#21040;&#26292;&#38706;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#23545;&#20110;&#35821;&#20041;&#31561;&#20215;&#30340;&#22810;&#35821;&#35328;&#26597;&#35810;&#30340;&#22238;&#31572;&#19981;&#19968;&#33268;&#65292;&#20197;&#21450;&#27169;&#22411;&#22312;&#24773;&#24863;&#30456;&#20851;&#38382;&#39064;&#19978;&#30340;&#19968;&#33268;&#24615;&#19981;&#21516;&#12290;</title><link>http://arxiv.org/abs/2309.04766</link><description>&lt;p&gt;
SeaEval&#22810;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65306;&#20174;&#36328;&#35821;&#35328;&#23545;&#40784;&#21040;&#25991;&#21270;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
SeaEval for Multilingual Foundation Models: From Cross-Lingual Alignment to Cultural Reasoning. (arXiv:2309.04766v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04766
&lt;/p&gt;
&lt;p&gt;
SeaEval&#26159;&#19968;&#20010;&#35780;&#20272;&#22810;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#30740;&#31350;&#20102;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#25512;&#29702;&#20197;&#21450;&#23545;&#25991;&#21270;&#23454;&#36341;&#12289;&#32454;&#24494;&#24046;&#21035;&#21644;&#20215;&#20540;&#35266;&#30340;&#29702;&#35299;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#12290;&#37325;&#35201;&#21457;&#29616;&#21253;&#25324;&#27169;&#22411;&#22312;&#32473;&#20986;&#25913;&#20889;&#25351;&#20196;&#26102;&#34892;&#20026;&#21508;&#24322;&#65292;&#21463;&#21040;&#26292;&#38706;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#23545;&#20110;&#35821;&#20041;&#31561;&#20215;&#30340;&#22810;&#35821;&#35328;&#26597;&#35810;&#30340;&#22238;&#31572;&#19981;&#19968;&#33268;&#65292;&#20197;&#21450;&#27169;&#22411;&#22312;&#24773;&#24863;&#30456;&#20851;&#38382;&#39064;&#19978;&#30340;&#19968;&#33268;&#24615;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340;SeaEval&#22522;&#20934;&#27979;&#35797;&#12290;&#38500;&#20102;&#34920;&#24449;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#29702;&#35299;&#21644;&#25512;&#29702;&#33258;&#28982;&#35821;&#35328;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#23427;&#20204;&#23545;&#25991;&#21270;&#23454;&#36341;&#12289;&#32454;&#24494;&#24046;&#21035;&#21644;&#20215;&#20540;&#35266;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#38500;&#20102;&#26631;&#20934;&#30340;&#20934;&#30830;&#24230;&#25351;&#26631;&#65292;&#25105;&#20204;&#36824;&#35843;&#26597;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#35821;&#20041;&#21644;&#22810;&#35821;&#35328;&#24615;&#32500;&#24230;&#19978;&#30340;&#33030;&#24369;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#28085;&#30422;&#20102;&#24320;&#28304;&#21644;&#38381;&#28304;&#27169;&#22411;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#22312;&#32463;&#20856;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12289;&#25512;&#29702;&#21644;&#25991;&#21270;&#29702;&#35299;&#26041;&#38754;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;&#37325;&#35201;&#21457;&#29616;&#21253;&#25324;&#65306;&#65288;1&#65289;&#22823;&#22810;&#25968;&#27169;&#22411;&#22312;&#32473;&#20986;&#25913;&#20889;&#25351;&#20196;&#26102;&#30340;&#34892;&#20026;&#21508;&#24322;&#65307;&#65288;2&#65289;&#35768;&#22810;&#27169;&#22411;&#20173;&#28982;&#21463;&#21040;&#26292;&#38706;&#20559;&#24046;&#30340;&#24433;&#21709;&#65288;&#22914;&#20301;&#32622;&#20559;&#24046;&#12289;&#22823;&#22810;&#25968;&#26631;&#31614;&#20559;&#24046;&#65289;&#65307;&#65288;3&#65289;&#23545;&#20110;&#26681;&#28304;&#20110;&#20107;&#23454;&#12289;&#31185;&#23398;&#21644;&#24120;&#35782;&#30693;&#35782;&#30340;&#38382;&#39064;&#65292;&#39044;&#26399;&#22312;&#35821;&#20041;&#19978;&#31561;&#20215;&#30340;&#22810;&#35821;&#35328;&#26597;&#35810;&#24212;&#35813;&#24471;&#21040;&#19968;&#33268;&#30340;&#22238;&#31572;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#22312;&#36825;&#20123;&#26597;&#35810;&#19978;&#34920;&#29616;&#20986;&#20196;&#20154;&#24847;&#22806;&#30340;&#19981;&#19968;&#33268;&#24615;&#65307;&#65288;4&#65289;&#22810;&#35821;&#35328;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#23545;&#20110;&#24773;&#24863;&#30456;&#20851;&#30340;&#38382;&#39064;&#34920;&#29616;&#20986;&#19981;&#21516;&#31243;&#24230;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present SeaEval, a benchmark for multilingual foundation models. In addition to characterizing how these models understand and reason with natural language, we also investigate how well they comprehend cultural practices, nuances, and values. Alongside standard accuracy metrics, we investigate the brittleness of foundation models in the dimensions of semantics and multilinguality. Our analyses span both open-sourced and closed models, leading to empirical results across classic NLP tasks, reasoning, and cultural comprehension. Key findings indicate (1) Most models exhibit varied behavior when given paraphrased instructions. (2) Many models still suffer from exposure bias (e.g., positional bias, majority label bias). (3) For questions rooted in factual, scientific, and commonsense knowledge, consistent responses are expected across multilingual queries that are semantically equivalent. Yet, most models surprisingly demonstrate inconsistent performance on these queries. (4) Multilingu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;&#19987;&#23478;&#25511;&#21046;&#22120;(MoEController)&#30340;&#25351;&#20196;&#39537;&#21160;&#20219;&#24847;&#22270;&#20687;&#25805;&#20316;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#20154;&#31867;&#25351;&#20196;&#36827;&#34892;&#36866;&#37197;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#21508;&#31181;&#24320;&#25918;&#22495;&#22270;&#20687;&#25805;&#20316;&#20219;&#21153;&#12290;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#26465;&#20214;&#22270;&#20687;&#21512;&#25104;&#27169;&#22411;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#24182;&#37319;&#29992;MOE&#25216;&#26415;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#36866;&#24212;&#24615;&#35757;&#32451;&#23545;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2309.04372</link><description>&lt;p&gt;
MoEController: &#20351;&#29992;&#28151;&#21512;&#19987;&#23478;&#25511;&#21046;&#22120;&#30340;&#22522;&#20110;&#25351;&#20196;&#30340;&#20219;&#24847;&#22270;&#20687;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
MoEController: Instruction-based Arbitrary Image Manipulation with Mixture-of-Expert Controllers. (arXiv:2309.04372v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04372
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;&#19987;&#23478;&#25511;&#21046;&#22120;(MoEController)&#30340;&#25351;&#20196;&#39537;&#21160;&#20219;&#24847;&#22270;&#20687;&#25805;&#20316;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#20154;&#31867;&#25351;&#20196;&#36827;&#34892;&#36866;&#37197;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#21508;&#31181;&#24320;&#25918;&#22495;&#22270;&#20687;&#25805;&#20316;&#20219;&#21153;&#12290;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#26465;&#20214;&#22270;&#20687;&#21512;&#25104;&#27169;&#22411;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#24182;&#37319;&#29992;MOE&#25216;&#26415;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#36866;&#24212;&#24615;&#35757;&#32451;&#23545;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#29983;&#25104;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#36827;&#23637;&#65292;&#22312;&#24320;&#25918;&#22495;&#22270;&#20687;&#25805;&#20316;&#20219;&#21153;&#20013;&#20135;&#29983;&#20102;&#20196;&#20154;&#30528;&#36855;&#30340;&#32467;&#26524;&#12290; &#28982;&#32780;&#65292;&#30001;&#20110;&#22270;&#20687;&#25805;&#20316;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#30446;&#21069;&#24456;&#23569;&#26377;&#27169;&#22411;&#20855;&#26377;&#23436;&#20840;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#65292;&#26082;&#21487;&#20197;&#36827;&#34892;&#20840;&#23616;&#25805;&#20316;&#65292;&#21448;&#21487;&#20197;&#36827;&#34892;&#23616;&#37096;&#22270;&#20687;&#32534;&#36753;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28151;&#21512;&#19987;&#23478;&#25511;&#21046;&#22120;&#65288;MOE&#65289;&#30340;&#26041;&#27861;&#65292;&#23558;&#25193;&#25955;&#27169;&#22411;&#30340;&#25991;&#26412;&#24341;&#23548;&#33021;&#21147;&#19982;&#19981;&#21516;&#31867;&#22411;&#30340;&#20154;&#31867;&#25351;&#20196;&#30456;&#23545;&#40784;&#65292;&#20351;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#22788;&#29702;&#21508;&#31181;&#24320;&#25918;&#22495;&#22270;&#20687;&#25805;&#20316;&#20219;&#21153;&#12290; &#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;ChatGPT&#65289;&#21644;&#26465;&#20214;&#22270;&#20687;&#21512;&#25104;&#27169;&#22411;&#65288;ControlNet&#65289;&#29983;&#25104;&#22823;&#37327;&#20840;&#23616;&#22270;&#20687;&#36716;&#25442;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#22522;&#20110;&#25351;&#20196;&#30340;&#23616;&#37096;&#22270;&#20687;&#32534;&#36753;&#25968;&#25454;&#38598;&#12290; &#28982;&#21518;&#65292;&#20351;&#29992;MOE&#25216;&#26415;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#36866;&#24212;&#24615;&#35757;&#32451;&#23545;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#23545;&#22270;&#20687;&#36827;&#34892;&#20840;&#23616;&#21644;&#23616;&#37096;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-model-based text-guided image generation has recently made astounding progress, producing fascinating results in open-domain image manipulation tasks. Few models, however, currently have complete zero-shot capabilities for both global and local image editing due to the complexity and diversity of image manipulation tasks. In this work, we propose a method with a mixture-of-expert (MOE) controllers to align the text-guided capacity of diffusion models with different kinds of human instructions, enabling our model to handle various open-domain image manipulation tasks with natural language instructions. First, we use large language models (ChatGPT) and conditional image synthesis models (ControlNet) to generate a large number of global image transfer dataset in addition to the instruction-based local image editing dataset. Then, using an MOE technique and task-specific adaptation training on a large-scale dataset, our conditional diffusion model can edit images globally and loc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;OpenAI&#30340;GPT-4&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21457;&#29616;&#22522;&#22240;&#38598;&#21512;&#21151;&#33021;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#23427;&#33021;&#26681;&#25454;&#23884;&#20837;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#29983;&#25104;&#19982;Gene Ontology&#20013;&#20855;&#21517;&#22522;&#22240;&#38598;&#21512;&#38750;&#24120;&#30456;&#20284;&#30340;&#21517;&#31216;&#65292;&#21516;&#26102;&#22312;&#22522;&#22240;&#32452;&#23398;&#25968;&#25454;&#20013;&#21457;&#29616;&#30340;&#22522;&#22240;&#38598;&#21512;&#20013;&#65292;GPT-4&#30340;&#21629;&#21517;&#26356;&#20855;&#20449;&#24687;&#37327;&#65292;&#24471;&#21040;&#20102;&#20154;&#24037;&#23457;&#26680;&#30340;&#22522;&#26412;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2309.04019</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21457;&#29616;&#22522;&#22240;&#38598;&#21512;&#21151;&#33021;&#26041;&#38754;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Evaluation of large language models for discovery of gene set function. (arXiv:2309.04019v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;OpenAI&#30340;GPT-4&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21457;&#29616;&#22522;&#22240;&#38598;&#21512;&#21151;&#33021;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#23427;&#33021;&#26681;&#25454;&#23884;&#20837;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#29983;&#25104;&#19982;Gene Ontology&#20013;&#20855;&#21517;&#22522;&#22240;&#38598;&#21512;&#38750;&#24120;&#30456;&#20284;&#30340;&#21517;&#31216;&#65292;&#21516;&#26102;&#22312;&#22522;&#22240;&#32452;&#23398;&#25968;&#25454;&#20013;&#21457;&#29616;&#30340;&#22522;&#22240;&#38598;&#21512;&#20013;&#65292;GPT-4&#30340;&#21629;&#21517;&#26356;&#20855;&#20449;&#24687;&#37327;&#65292;&#24471;&#21040;&#20102;&#20154;&#24037;&#23457;&#26680;&#30340;&#22522;&#26412;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#22240;&#38598;&#21512;&#20998;&#26512;&#26159;&#21151;&#33021;&#22522;&#22240;&#32452;&#23398;&#30340;&#37325;&#35201;&#26041;&#27861;&#65292;&#20294;&#23427;&#20381;&#36182;&#20110;&#25163;&#21160;&#21019;&#24314;&#30340;&#22522;&#22240;&#21151;&#33021;&#25968;&#25454;&#24211;&#65292;&#36825;&#20123;&#25968;&#25454;&#24211;&#30340;&#19981;&#23436;&#25972;&#21644;&#19981;&#20855;&#22791;&#29983;&#29289;&#23398;&#19978;&#19979;&#25991;&#30340;&#29305;&#28857;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;OpenAI&#30340;GPT-4&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#20174;&#20854;&#23884;&#20837;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#20013;&#21457;&#23637;&#20986;&#26377;&#20851;&#24120;&#35265;&#22522;&#22240;&#21151;&#33021;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;GPT-4&#27969;&#27700;&#32447;&#65292;&#29992;&#20110;&#29992;&#24635;&#32467;&#20854;&#20849;&#35782;&#21151;&#33021;&#30340;&#21517;&#31216;&#26631;&#35760;&#22522;&#22240;&#38598;&#21512;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#25991;&#26412;&#21644;&#24341;&#25991;&#36827;&#34892;&#35777;&#23454;&#12290;&#22312;&#19982;Gene Ontology&#20013;&#30340;&#20855;&#21517;&#22522;&#22240;&#38598;&#21512;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#26102;&#65292;GPT-4&#22312;50%&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#20102;&#38750;&#24120;&#30456;&#20284;&#30340;&#21517;&#31216;&#65292;&#32780;&#22312;&#22823;&#22810;&#25968;&#20854;&#20182;&#24773;&#20917;&#19979;&#65292;&#21017;&#24674;&#22797;&#20102;&#26356;&#19968;&#33324;&#27010;&#24565;&#30340;&#21517;&#31216;&#12290;&#22312;&#22522;&#22240;&#32452;&#23398;&#25968;&#25454;&#20013;&#21457;&#29616;&#30340;&#22522;&#22240;&#38598;&#21512;&#20013;&#65292;&#19982;&#22522;&#22240;&#38598;&#21512;&#23500;&#38598;&#30456;&#27604;&#65292;GPT-4&#30340;&#21629;&#21517;&#26356;&#20855;&#20449;&#24687;&#37327;&#65292;&#20854;&#25903;&#25345;&#24615;&#38472;&#36848;&#21644;&#24341;&#25991;&#22312;&#20154;&#24037;&#23457;&#26680;&#20013;&#24471;&#21040;&#20102;&#22522;&#26412;&#39564;&#35777;&#12290;&#24555;&#36895;&#32508;&#21512;&#24120;&#35265;&#22522;&#22240;&#21151;&#33021;&#30340;&#33021;&#21147;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#26377;&#20215;&#20540;&#30340;&#21151;&#33021;&#22522;&#22240;&#32452;&#23398;&#21161;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gene set analysis is a mainstay of functional genomics, but it relies on manually curated databases of gene functions that are incomplete and unaware of biological context. Here we evaluate the ability of OpenAI's GPT-4, a Large Language Model (LLM), to develop hypotheses about common gene functions from its embedded biomedical knowledge. We created a GPT-4 pipeline to label gene sets with names that summarize their consensus functions, substantiated by analysis text and citations. Benchmarking against named gene sets in the Gene Ontology, GPT-4 generated very similar names in 50% of cases, while in most remaining cases it recovered the name of a more general concept. In gene sets discovered in 'omics data, GPT-4 names were more informative than gene set enrichment, with supporting statements and citations that largely verified in human review. The ability to rapidly synthesize common gene functions positions LLMs as valuable functional genomics assistants.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Efficient Benchmarking"&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#26234;&#33021;&#22320;&#20943;&#23569;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35745;&#31639;&#25104;&#26412;&#32780;&#19981;&#38477;&#20302;&#21487;&#38752;&#24615;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;Decision Impact on Reliability&#65288;DIoR&#65289;&#30340;&#26032;&#24230;&#37327;&#26469;&#35780;&#20272;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;HELM&#22522;&#20934;&#27979;&#35797;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#21482;&#38656;&#21024;&#38500;&#19968;&#20010;&#20302;&#25490;&#21517;&#27169;&#22411;&#21363;&#21487;&#25913;&#21464;&#39046;&#20808;&#32773;&#65292;&#24182;&#20165;&#38656;&#23569;&#37327;&#31034;&#20363;&#21363;&#21487;&#24471;&#21040;&#27491;&#30830;&#30340;&#22522;&#20934;&#27979;&#35797;&#25490;&#21517;&#12290;</title><link>http://arxiv.org/abs/2308.11696</link><description>&lt;p&gt;
&#26377;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Efficient Benchmarking (of Language Models). (arXiv:2308.11696v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Efficient Benchmarking"&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#26234;&#33021;&#22320;&#20943;&#23569;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35745;&#31639;&#25104;&#26412;&#32780;&#19981;&#38477;&#20302;&#21487;&#38752;&#24615;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;Decision Impact on Reliability&#65288;DIoR&#65289;&#30340;&#26032;&#24230;&#37327;&#26469;&#35780;&#20272;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;HELM&#22522;&#20934;&#27979;&#35797;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#21482;&#38656;&#21024;&#38500;&#19968;&#20010;&#20302;&#25490;&#21517;&#27169;&#22411;&#21363;&#21487;&#25913;&#21464;&#39046;&#20808;&#32773;&#65292;&#24182;&#20165;&#38656;&#23569;&#37327;&#31034;&#20363;&#21363;&#21487;&#24471;&#21040;&#27491;&#30830;&#30340;&#22522;&#20934;&#27979;&#35797;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#21151;&#33021;&#24615;&#22686;&#21152;&#23548;&#33268;&#20102;&#19968;&#31867;&#20840;&#38754;&#35780;&#20272;&#24191;&#27867;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;&#30340;&#20986;&#29616;&#12290;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#19982;&#22823;&#35268;&#27169;&#35745;&#31639;&#25104;&#26412;&#30456;&#20851;&#65292;&#27599;&#20010;&#27169;&#22411;&#38656;&#35201;&#25968;&#21315;&#20010;GPU&#23567;&#26102;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#35780;&#20272;&#25928;&#29575;&#26041;&#38754;&#30340;&#38382;&#39064;&#22312;&#25991;&#29486;&#20013;&#35752;&#35770;&#36739;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Efficient Benchmarking"&#30340;&#38382;&#39064;&#65292;&#21363;&#22312;&#19981;&#25439;&#23475;&#21487;&#38752;&#24615;&#30340;&#24773;&#20917;&#19979;&#26234;&#33021;&#22320;&#20943;&#23569;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#36890;&#36807;&#20351;&#29992;HELM&#22522;&#20934;&#27979;&#35797;&#20316;&#20026;&#31034;&#20363;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#35774;&#35745;&#36873;&#25321;&#22914;&#20309;&#24433;&#21709;&#35745;&#31639;-&#21487;&#38752;&#24615;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;Decision Impact on Reliability&#65288;DIoR&#65289;&#30340;&#26032;&#24230;&#37327;&#26469;&#35780;&#20272;&#36825;&#20123;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21457;&#29616;&#20165;&#36890;&#36807;&#20174;&#22522;&#20934;&#27979;&#35797;&#20013;&#21024;&#38500;&#19968;&#20010;&#20302;&#25490;&#21517;&#27169;&#22411;&#65292;&#24403;&#21069;&#22312;HELM&#19978;&#30340;&#39046;&#20808;&#32773;&#21487;&#33021;&#20250;&#25913;&#21464;&#65292;&#24182;&#19988;&#35266;&#23519;&#21040;&#21482;&#38656;&#19968;&#23567;&#37096;&#20998;&#31034;&#20363;&#21363;&#21487;&#33719;&#24471;&#27491;&#30830;&#30340;&#22522;&#20934;&#27979;&#35797;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing versatility of language models LMs has given rise to a new class of benchmarks that comprehensively assess a broad range of capabilities. Such benchmarks are associated with massive computational costs reaching thousands of GPU hours per model. However the efficiency aspect of these evaluation efforts had raised little discussion in the literature. In this work we present the problem of Efficient Benchmarking namely intelligently reducing the computation costs of LM evaluation without compromising reliability. Using the HELM benchmark as a test case we investigate how different benchmark design choices affect the computation-reliability tradeoff. We propose to evaluate the reliability of such decisions by using a new measure Decision Impact on Reliability DIoR for short. We find for example that the current leader on HELM may change by merely removing a low-ranked model from the benchmark and observe that a handful of examples suffice to obtain the correct benchmark rank
&lt;/p&gt;</description></item><item><title>Beam Retrieval&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#31471;&#21040;&#31471;&#26816;&#32034;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#38454;&#27573;&#38382;&#31572;&#12290;&#23427;&#36890;&#36807;&#20445;&#25345;&#22810;&#20010;&#30456;&#20851;&#25991;&#27573;&#30340;&#20551;&#35774;&#21644;&#36890;&#36807;&#26368;&#23567;&#21270;&#32452;&#21512;&#25439;&#22833;&#26469;&#20248;&#21270;&#32534;&#30721;&#22120;&#21644;&#20998;&#31867;&#22836;&#65292;&#23454;&#29616;&#20102;&#36817;50%&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.08973</link><description>&lt;p&gt;
Beam Retrieval: &#36890;&#29992;&#30340;&#31471;&#21040;&#31471;&#26816;&#32034;&#29992;&#20110;&#22810;&#38454;&#27573;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Beam Retrieval: General End-to-End Retrieval for Multi-Hop Question Answering. (arXiv:2308.08973v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08973
&lt;/p&gt;
&lt;p&gt;
Beam Retrieval&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#31471;&#21040;&#31471;&#26816;&#32034;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#38454;&#27573;&#38382;&#31572;&#12290;&#23427;&#36890;&#36807;&#20445;&#25345;&#22810;&#20010;&#30456;&#20851;&#25991;&#27573;&#30340;&#20551;&#35774;&#21644;&#36890;&#36807;&#26368;&#23567;&#21270;&#32452;&#21512;&#25439;&#22833;&#26469;&#20248;&#21270;&#32534;&#30721;&#22120;&#21644;&#20998;&#31867;&#22836;&#65292;&#23454;&#29616;&#20102;&#36817;50%&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#38454;&#27573;&#38382;&#31572;&#28041;&#21450;&#26597;&#25214;&#22810;&#20010;&#30456;&#20851;&#25991;&#27573;&#21644;&#36880;&#27493;&#25512;&#29702;&#20197;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#26041;&#27861;&#24050;&#32463;&#24320;&#21457;&#20102;&#29992;&#20110;&#36873;&#25321;&#30456;&#20851;&#25991;&#27573;&#30340;&#26816;&#32034;&#27169;&#22359;&#65292;&#20294;&#26159;&#23427;&#20204;&#22312;&#36229;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#22330;&#26223;&#20013;&#38754;&#20020;&#25361;&#25112;&#65292;&#21407;&#22240;&#26159;&#19968;&#27493;&#26041;&#27861;&#30340;&#24615;&#33021;&#26377;&#38480;&#65292;&#20004;&#27493;&#26041;&#27861;&#22312;&#26089;&#26399;&#38454;&#27573;&#36873;&#25321;&#26080;&#20851;&#25991;&#27573;&#26102;&#22833;&#36133;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Beam Retrieval&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#22810;&#38454;&#27573;&#38382;&#31572;&#30340;&#31471;&#21040;&#31471;&#26816;&#32034;&#26694;&#26550;&#12290;&#35813;&#26041;&#27861;&#22312;&#27599;&#20010;&#38454;&#27573;&#20445;&#25345;&#22810;&#20010;&#30456;&#20851;&#25991;&#27573;&#30340;&#20551;&#35774;&#65292;&#25193;&#23637;&#20102;&#25628;&#32034;&#31354;&#38388;&#65292;&#38477;&#20302;&#20102;&#38169;&#36807;&#30456;&#20851;&#25991;&#27573;&#30340;&#39118;&#38505;&#12290;&#27492;&#22806;&#65292;Beam Retrieval&#36890;&#36807;&#26368;&#23567;&#21270;&#25152;&#26377;&#38454;&#27573;&#30340;&#32452;&#21512;&#25439;&#22833;&#26469;&#32852;&#21512;&#20248;&#21270;&#32534;&#30721;&#22120;&#21644;&#20004;&#20010;&#20998;&#31867;&#22836;&#12290;&#20026;&#20102;&#24314;&#31435;&#19968;&#20010;&#23436;&#25972;&#30340;&#38382;&#31572;&#31995;&#32479;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26377;&#30417;&#30563;&#30340;&#38405;&#35835;&#22120;&#25110;&#38646;-shot GPT-3.5&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#20934;&#26041;&#27861;&#30456;&#27604;&#65292;Beam Retrieval&#21462;&#24471;&#20102;&#36817;50%&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-hop QA involves finding multiple relevant passages and step-by-step reasoning to answer complex questions. While previous approaches have developed retrieval modules for selecting relevant passages, they face challenges in scenarios beyond two hops, owing to the limited performance of one-step methods and the failure of two-step methods when selecting irrelevant passages in earlier stages. In this work, we introduce Beam Retrieval, a general end-to-end retrieval framework for multi-hop QA. This approach maintains multiple partial hypotheses of relevant passages at each step, expanding the search space and reducing the risk of missing relevant passages. Moreover, Beam Retrieval jointly optimizes an encoder and two classification heads by minimizing the combined loss across all hops. To establish a complete QA system, we incorporate a supervised reader or a zero-shot GPT-3.5. Experimental results demonstrate that Beam Retrieval achieves a nearly 50% improvement compared with baseli
&lt;/p&gt;</description></item><item><title>DiagGPT&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25193;&#23637;&#21040;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#22330;&#26223;&#65292;&#25552;&#20379;&#20102;&#22312;&#22797;&#26434;&#35786;&#26029;&#22330;&#26223;&#20013;&#20027;&#21160;&#25552;&#38382;&#21644;&#24341;&#23548;&#29992;&#25143;&#23436;&#25104;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.08043</link><description>&lt;p&gt;
DiagGPT:&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
DiagGPT: An LLM-based Chatbot with Automatic Topic Management for Task-Oriented Dialogue. (arXiv:2308.08043v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08043
&lt;/p&gt;
&lt;p&gt;
DiagGPT&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25193;&#23637;&#21040;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#22330;&#26223;&#65292;&#25552;&#20379;&#20102;&#22312;&#22797;&#26434;&#35786;&#26029;&#22330;&#26223;&#20013;&#20027;&#21160;&#25552;&#38382;&#21644;&#24341;&#23548;&#29992;&#25143;&#23436;&#25104;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22914;ChatGPT&#27491;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#23637;&#31034;&#20986;&#19982;&#20154;&#31867;&#30456;&#20284;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;AI&#27169;&#22411;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#36741;&#21161;&#20154;&#31867;&#23436;&#25104;&#21508;&#31181;&#20219;&#21153;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;AI&#20316;&#20026;&#32842;&#22825;&#20195;&#29702;&#20154;&#30340;&#37325;&#35201;&#24212;&#29992;&#26159;&#22238;&#31572;&#20154;&#31867;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#38382;&#39064;&#12290;&#30446;&#21069;&#30340;LLMs&#22312;&#22238;&#31572;&#19968;&#33324;&#38382;&#39064;&#26041;&#38754;&#24050;&#32463;&#26174;&#31034;&#20986;&#29087;&#32451;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#30340;&#35786;&#26029;&#22330;&#26223;(&#22914;&#27861;&#24459;&#25110;&#21307;&#30103;&#21672;&#35810;)&#20013;&#65292;&#22522;&#26412;&#30340;&#38382;&#31572;&#23545;&#35805;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#20123;&#22330;&#26223;&#36890;&#24120;&#38656;&#35201;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;(TOD)&#65292;&#20854;&#20013;AI&#32842;&#22825;&#20195;&#29702;&#38656;&#35201;&#20027;&#21160;&#25552;&#38382;&#24182;&#24341;&#23548;&#29992;&#25143;&#23436;&#25104;&#29305;&#23450;&#20219;&#21153;&#12290;&#20197;&#21069;&#30340;&#24494;&#35843;&#27169;&#22411;&#22312;TOD&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;&#24403;&#21069;&#30340;LLMs&#24182;&#26410;&#22266;&#26377;&#36825;&#31181;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DiagGPT (Diagnosis GPT)&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#23427;&#23558;LLMs&#25512;&#24191;&#21040;TOD&#22330;&#26223;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), such as ChatGPT, are becoming increasingly sophisticated, demonstrating capabilities that closely resemble those of humans. These AI models are playing an essential role in assisting humans with a wide array of tasks in daily life. A significant application of AI is its use as a chat agent, responding to human inquiries across various domains. Current LLMs have shown proficiency in answering general questions. However, basic question-answering dialogue often falls short in complex diagnostic scenarios, such as legal or medical consultations. These scenarios typically necessitate Task-Oriented Dialogue (TOD), wherein an AI chat agent needs to proactively pose questions and guide users towards specific task completion. Previous fine-tuning models have underperformed in TOD, and current LLMs do not inherently possess this capability. In this paper, we introduce DiagGPT (Dialogue in Diagnosis GPT), an innovative method that extends LLMs to TOD scenarios. Our e
&lt;/p&gt;</description></item><item><title>RAVEN&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#33945;&#29305;&#21345;&#27931;&#35821;&#35328;&#24314;&#27169;&#21644;&#21069;&#32512;&#35821;&#35328;&#24314;&#27169;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#19978;&#19979;&#25991;&#34701;&#21512;&#23398;&#20064;&#65292;&#23427;&#33021;&#22815;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#27604;ATLAS&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.07922</link><description>&lt;p&gt;
RAVEN&#65306;&#19978;&#19979;&#25991;&#23398;&#20064;&#19982;&#26816;&#32034;&#22686;&#24378;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder Language Models. (arXiv:2308.07922v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07922
&lt;/p&gt;
&lt;p&gt;
RAVEN&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#33945;&#29305;&#21345;&#27931;&#35821;&#35328;&#24314;&#27169;&#21644;&#21069;&#32512;&#35821;&#35328;&#24314;&#27169;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#19978;&#19979;&#25991;&#34701;&#21512;&#23398;&#20064;&#65292;&#23427;&#33021;&#22815;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#27604;ATLAS&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#29616;&#26377;&#30340;ATLAS&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#21457;&#29616;&#20854;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;&#39044;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#23384;&#22312;&#19981;&#21305;&#37197;&#65292;&#20197;&#21450;&#19978;&#19979;&#25991;&#38271;&#24230;&#21463;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RAVEN&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#33945;&#29305;&#21345;&#27931;&#35821;&#35328;&#24314;&#27169;&#21644;&#21069;&#32512;&#35821;&#35328;&#24314;&#27169;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19978;&#19979;&#25991;&#34701;&#21512;&#23398;&#20064;&#65292;&#36890;&#36807;&#20351;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#26356;&#22810;&#19978;&#19979;&#25991;&#31034;&#20363;&#32780;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#25110;&#27169;&#22411;&#20462;&#25913;&#26469;&#25552;&#39640;&#23569;&#26679;&#26412;&#24615;&#33021;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;RAVEN&#22312;&#26576;&#20123;&#22330;&#26223;&#19979;&#26126;&#26174;&#20248;&#20110;ATLAS&#65292;&#24182;&#36798;&#21040;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#23613;&#31649;&#21442;&#25968;&#25968;&#37327;&#26174;&#33879;&#36739;&#23569;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24378;&#35843;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the in-context learning ability of retrieval-augmented encoder-decoder language models. We first conduct a comprehensive analysis of the state-of-the-art ATLAS model and identify its limitations in in-context learning, primarily due to a mismatch between pretraining and testing, as well as a restricted context length. To address these issues, we propose RAVEN, a model that combines retrieval-augmented masked language modeling and prefix language modeling. We further introduce Fusion-in-Context Learning to enhance the few-shot performance by enabling the model to leverage more in-context examples without requiring additional training or model modifications. Through extensive experiments, we demonstrate that RAVEN significantly outperforms ATLAS and achieves results comparable to the most advanced language models in certain scenarios, despite having substantially fewer parameters. Our work underscores the potential of retrieval-augmented encoder-decoder lang
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;XSTest&#30340;&#27979;&#35797;&#22871;&#20214;&#65292;&#26088;&#22312;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#22840;&#22823;&#30340;&#23433;&#20840;&#34892;&#20026;&#12290;&#35813;&#22871;&#20214;&#30001;200&#20010;&#23433;&#20840;&#25552;&#31034;&#32452;&#25104;&#65292;&#28085;&#30422;&#21313;&#31181;&#25552;&#31034;&#31867;&#22411;&#65292;&#26088;&#22312;&#24341;&#20986;&#27169;&#22411;&#30340;&#31995;&#32479;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.01263</link><description>&lt;p&gt;
XSTest: &#29992;&#20110;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#22840;&#22823;&#23433;&#20840;&#34892;&#20026;&#30340;&#27979;&#35797;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models. (arXiv:2308.01263v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;XSTest&#30340;&#27979;&#35797;&#22871;&#20214;&#65292;&#26088;&#22312;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#22840;&#22823;&#30340;&#23433;&#20840;&#34892;&#20026;&#12290;&#35813;&#22871;&#20214;&#30001;200&#20010;&#23433;&#20840;&#25552;&#31034;&#32452;&#25104;&#65292;&#28085;&#30422;&#21313;&#31181;&#25552;&#31034;&#31867;&#22411;&#65292;&#26088;&#22312;&#24341;&#20986;&#27169;&#22411;&#30340;&#31995;&#32479;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27809;&#26377;&#36866;&#24403;&#30340;&#20445;&#25252;&#25514;&#26045;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24456;&#23481;&#26131;&#36981;&#24490;&#24694;&#24847;&#25351;&#20196;&#24182;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#12290;&#36825;&#28608;&#21457;&#20102;&#23433;&#20840;&#24037;&#20316;&#65292;&#22914;&#32418;&#38431;&#27979;&#35797;&#21644;&#22823;&#35268;&#27169;&#21453;&#39304;&#23398;&#20064;&#65292;&#26088;&#22312;&#20351;&#27169;&#22411;&#26082;&#26377;&#29992;&#21448;&#26080;&#23475;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#20010;&#30446;&#26631;&#20043;&#38388;&#23384;&#22312;&#19968;&#31181;&#32039;&#24352;&#20851;&#31995;&#65292;&#22240;&#20026;&#26080;&#23475;&#24615;&#35201;&#27714;&#27169;&#22411;&#25298;&#32477;&#36981;&#20174;&#19981;&#23433;&#20840;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#26080;&#27861;&#25552;&#20379;&#24110;&#21161;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#35777;&#25454;&#34920;&#26126;&#65292;&#19968;&#20123;&#27169;&#22411;&#21487;&#33021;&#22312;&#24179;&#34913;&#19978;&#23384;&#22312;&#38382;&#39064;&#65292;&#20197;&#33267;&#20110;&#21363;&#20351;&#20351;&#29992;&#31867;&#20284;&#19981;&#23433;&#20840;&#25552;&#31034;&#30340;&#35821;&#35328;&#25110;&#25552;&#21450;&#25935;&#24863;&#20027;&#39064;&#30340;&#26126;&#26174;&#23433;&#20840;&#25552;&#31034;&#20063;&#20250;&#34987;&#25298;&#32477;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;XSTest&#30340;&#26032;&#27979;&#35797;&#22871;&#20214;&#65292;&#20197;&#31995;&#32479;&#21270;&#21644;&#32467;&#26500;&#21270;&#30340;&#26041;&#24335;&#35782;&#21035;&#36825;&#31181;&#22840;&#24352;&#30340;&#23433;&#20840;&#34892;&#20026;&#12290;&#30446;&#21069;&#65292;XSTest&#21253;&#25324;200&#20010;&#23433;&#20840;&#25552;&#31034;&#65292;&#28085;&#30422;&#21313;&#31181;&#25552;&#31034;&#31867;&#22411;&#65292;&#33391;&#22909;&#26657;&#20934;&#30340;&#27169;&#22411;&#19981;&#24212;&#35813;&#25298;&#32477;&#36981;&#24490;&#36825;&#20123;&#25552;&#31034;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;XSTest&#30340;&#21019;&#24314;&#21644;&#32452;&#25104;&#65292;&#24182;&#20351;&#29992;&#27979;&#35797;&#22871;&#20214;&#31361;&#26174;&#31995;&#32479;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Without proper safeguards, large language models will readily follow malicious instructions and generate toxic content. This motivates safety efforts such as red-teaming and large-scale feedback learning, which aim to make models both helpful and harmless. However, there is a tension between these two objectives, since harmlessness requires models to refuse complying with unsafe prompts, and thus not be helpful. Recent anecdotal evidence suggests that some models may have struck a poor balance, so that even clearly safe prompts are refused if they use similar language to unsafe prompts or mention sensitive topics. In this paper, we introduce a new test suite called XSTest to identify such eXaggerated Safety behaviours in a structured and systematic way. In its current form, XSTest comprises 200 safe prompts across ten prompt types that well-calibrated models should not refuse to comply with. We describe XSTest's creation and composition, and use the test suite to highlight systematic f
&lt;/p&gt;</description></item><item><title>SAS&#35270;&#39057;&#38382;&#31572;&#36890;&#36807;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#35299;&#20915;&#20102;&#35270;&#39057;&#38382;&#31572;&#20013;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;</title><link>http://arxiv.org/abs/2307.04192</link><description>&lt;p&gt;
SAS&#35270;&#39057;&#38382;&#31572;&#65306;&#33258;&#36866;&#24212;&#37319;&#26679;&#29992;&#20110;&#39640;&#25928;&#35270;&#39057;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
SAS Video-QA: Self-Adaptive Sampling for Efficient Video Question-Answering. (arXiv:2307.04192v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04192
&lt;/p&gt;
&lt;p&gt;
SAS&#35270;&#39057;&#38382;&#31572;&#36890;&#36807;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#35299;&#20915;&#20102;&#35270;&#39057;&#38382;&#31572;&#20013;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#38382;&#31572;&#26159;&#35270;&#39057;&#29702;&#35299;&#39046;&#22495;&#30340;&#19968;&#39033;&#22522;&#30784;&#20219;&#21153;&#12290;&#23613;&#31649;&#24403;&#21069;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(VLMs)&#37197;&#22791;&#20102;&#35270;&#39057;&#21464;&#25442;&#22120;(Video Transformers)&#65292;&#23454;&#29616;&#20102;&#26102;&#38388;&#24314;&#27169;&#24182;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#32467;&#26524;&#65292;&#20294;&#20195;&#20215;&#26159;&#24040;&#22823;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#22240;&#27492;&#22312;&#23454;&#26102;&#24212;&#29992;&#22330;&#26223;&#20013;&#36807;&#20110;&#26114;&#36149;&#12290;&#19968;&#31181;&#32463;&#27982;&#30340;&#35299;&#20915;&#26041;&#27861;&#26159;&#21482;&#23545;&#35270;&#39057;&#30340;&#19968;&#23567;&#37096;&#20998;&#24103;&#36827;&#34892;&#37319;&#26679;&#65292;&#26469;&#20195;&#34920;&#35270;&#39057;&#30340;&#20027;&#35201;&#20869;&#23481;&#65292;&#24182;&#22312;&#36825;&#20123;&#37319;&#26679;&#24103;&#19978;&#35843;&#25972;&#22270;&#20687;-&#25991;&#26412;&#27169;&#22411;&#12290;&#26368;&#36817;&#30340;&#35270;&#39057;&#29702;&#35299;&#27169;&#22411;&#36890;&#24120;&#38543;&#26426;&#37319;&#26679;&#19968;&#32452;&#24103;&#25110;&#29255;&#27573;&#65292;&#32780;&#19981;&#32771;&#34385;&#23427;&#20204;&#30340;&#20869;&#37096;&#20851;&#32852;&#24615;&#21644;&#19982;&#38382;&#39064;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#26080;&#30446;&#26631;&#30340;&#37319;&#26679;&#21487;&#33021;&#20250;&#36951;&#28431;&#21487;&#20197;&#25512;&#23548;&#20986;&#27491;&#30830;&#31572;&#26696;&#30340;&#20851;&#38190;&#24103;&#65292;&#22312;&#37319;&#26679;&#31232;&#30095;&#31243;&#24230;&#22686;&#21152;&#26102;&#65292;&#24773;&#20917;&#20250;&#21464;&#24471;&#26356;&#31967;&#65292;&#32780;&#38543;&#30528;&#35270;&#39057;&#38271;&#24230;&#30340;&#22686;&#21152;&#65292;&#37319;&#26679;&#31232;&#30095;&#31243;&#24230;&#20063;&#20250;&#22686;&#21152;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#24103;&#37319;&#26679;&#31574;&#30053;&#65292;&#20998;&#21035;&#26159;
&lt;/p&gt;
&lt;p&gt;
Video question--answering is a fundamental task in the field of video understanding. Although current vision--language models (VLMs) equipped with Video Transformers have enabled temporal modeling and yielded superior results, they are at the cost of huge computational power and thus too expensive to deploy in real-time application scenarios. An economical workaround only samples a small portion of frames to represent the main content of that video and tune an image--text model on these sampled frames. Recent video understanding models usually randomly sample a set of frames or clips, regardless of internal correlations between their visual contents, nor their relevance to the problem. We argue that such kinds of aimless sampling may omit the key frames from which the correct answer can be deduced, and the situation gets worse when the sampling sparsity increases, which always happens as the video lengths increase. To mitigate this issue, we propose two frame sampling strategies, namel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36861;&#36394;&#21644;&#24635;&#32467;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#25972;&#12289;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#22810;&#27169;&#24577;&#24605;&#32500;&#38142;&#21644;LLM&#36741;&#21161;&#35270;&#35273;&#25512;&#29702;&#31561;&#24212;&#29992;&#65292;&#25351;&#20986;&#20102;&#29616;&#26377;&#25361;&#25112;&#21644;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.13549</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Multimodal Large Language Models. (arXiv:2306.13549v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36861;&#36394;&#21644;&#24635;&#32467;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#25972;&#12289;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#22810;&#27169;&#24577;&#24605;&#32500;&#38142;&#21644;LLM&#36741;&#21161;&#35270;&#35273;&#25512;&#29702;&#31561;&#24212;&#29992;&#65292;&#25351;&#20986;&#20102;&#29616;&#26377;&#25361;&#25112;&#21644;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#30740;&#31350;&#28909;&#28857;&#65292;&#20351;&#29992;&#24378;&#22823;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#22823;&#33041;&#25191;&#34892;&#22810;&#27169;&#24577;&#20219;&#21153;&#12290;MLLM &#30340;&#24778;&#20154;&#33021;&#21147;&#65292;&#22914;&#22522;&#20110;&#22270;&#20687;&#32534;&#20889;&#25925;&#20107;&#21644;&#26080;OCR&#25968;&#23398;&#25512;&#29702;&#31561;&#65292;&#22312;&#20256;&#32479;&#26041;&#27861;&#20013;&#24456;&#23569;&#35265;&#65292;&#34920;&#26126;&#20102;&#36890;&#21521;&#20154;&#24037;&#26234;&#33021;&#30340;&#28508;&#22312;&#36335;&#24452;&#12290;&#26412;&#25991;&#26088;&#22312;&#36861;&#36394;&#21644;&#24635;&#32467; MLLM &#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; MLLM &#30340;&#26500;&#25104;&#65292;&#27010;&#36848;&#20102;&#30456;&#20851;&#27010;&#24565;&#12290;&#28982;&#21518;&#65292;&#35752;&#35770;&#20102;&#20851;&#38190;&#25216;&#26415;&#21644;&#24212;&#29992;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#25972;&#65288;M-IT&#65289;&#12289;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;M-ICL&#65289;&#12289;&#22810;&#27169;&#24577;&#24605;&#32500;&#38142;&#65288;M-CoT&#65289;&#21644;LLM&#36741;&#21161;&#35270;&#35273;&#25512;&#29702;&#65288;LAVR&#65289;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#29616;&#26377;&#30340;&#25361;&#25112;&#65292;&#24182;&#25351;&#20986;&#20102;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#37492;&#20110; MLLM &#26102;&#20195;&#25165;&#21018;&#21018;&#24320;&#22987;&#65292;&#25105;&#20204;&#20250;&#19981;&#26029;&#26356;&#26032;&#36825;&#20010;&#32508;&#36848;&#65292;&#24182;&#24076;&#26395;&#33021;&#28608;&#21457;&#26356;&#22810;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Large Language Model (MLLM) recently has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional methods, suggesting a potential path to artificial general intelligence. In this paper, we aim to trace and summarize the recent progress of MLLM. First of all, we present the formulation of MLLM and delineate its related concepts. Then, we discuss the key techniques and applications, including Multimodal Instruction Tuning (M-IT), Multimodal In-Context Learning (M-ICL), Multimodal Chain of Thought (M-CoT), and LLM-Aided Visual Reasoning (LAVR). Finally, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;Text-to-SQL&#27169;&#22411;SQL-PaLM&#65292;&#20351;&#29992;&#20102;&#38754;&#21521;Text-to-SQL&#30340;&#22522;&#20110;&#25191;&#34892;&#30340;&#33258;&#19968;&#33268;&#25552;&#31034;&#26041;&#27861;&#65292;&#22312;Spider&#19978;&#23454;&#29616;&#20102;77.3%&#30340;&#27979;&#35797;&#22871;&#20214;&#20934;&#30830;&#24230;&#65292;&#24182;&#26174;&#30528;&#36229;&#36234;&#20197;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.00739</link><description>&lt;p&gt;
SQL-PaLM&#65306;&#38024;&#23545;Text-to-SQL&#30340;&#25913;&#36827;&#22823;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
SQL-PaLM: Improved Large Language Model Adaptation for Text-to-SQL. (arXiv:2306.00739v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;Text-to-SQL&#27169;&#22411;SQL-PaLM&#65292;&#20351;&#29992;&#20102;&#38754;&#21521;Text-to-SQL&#30340;&#22522;&#20110;&#25191;&#34892;&#30340;&#33258;&#19968;&#33268;&#25552;&#31034;&#26041;&#27861;&#65292;&#22312;Spider&#19978;&#23454;&#29616;&#20102;77.3%&#30340;&#27979;&#35797;&#22871;&#20214;&#20934;&#30830;&#24230;&#65292;&#24182;&#26174;&#30528;&#36229;&#36234;&#20197;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#19968;&#20010;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26032;&#20852;&#21151;&#33021;&#26159;&#29983;&#25104;&#20195;&#30721;&#65292;&#21253;&#25324;&#29992;&#20110;&#25968;&#25454;&#24211;&#30340;&#32467;&#26500;&#21270;&#26597;&#35810;&#35821;&#35328;&#65288;SQL&#65289;&#12290;&#23545;&#20110;&#23558;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#36716;&#25442;&#20026;SQL&#26597;&#35810;&#30340;&#20219;&#21153;&#65292;&#21363;Text-to-SQL&#65292;LLMs&#30340;&#36866;&#24212;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#20351;&#29992;&#30340;&#36866;&#24212;&#24615;&#25968;&#25454;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;Text-to-SQL&#27169;&#22411;SQL-PaLM&#65292;&#21033;&#29992;&#20102;PaLM-2&#65292;&#25512;&#21160;&#20102;&#20004;&#31181;&#35774;&#32622;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;Few-shot SQL-PaLM&#22522;&#20110;&#38754;&#21521;Text-to-SQL&#30340;&#22522;&#20110;&#25191;&#34892;&#30340;&#33258;&#19968;&#33268;&#25552;&#31034;&#26041;&#27861;&#65292;&#21487;&#22312;Spider&#19978;&#23454;&#29616;77.3%&#30340;&#27979;&#35797;&#22871;&#20214;&#20934;&#30830;&#24230;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#36890;&#36807;&#26174;&#30528;&#36739;&#22823;&#30340;&#24494;&#35843;&#36229;&#36234;&#20197;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#32463;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;SQL-PALM&#21487;&#36827;&#19968;&#27493;&#25552;&#39640;1%&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#23558;SQL-PaLM&#24212;&#29992;&#20110;&#23454;&#38469;&#22330;&#26223;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35780;&#20272;&#20102;&#20854;&#23545;&#20854;&#20182;&#25361;&#25112;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
One impressive emergent capability of large language models (LLMs) is generation of code, including Structured Query Language (SQL) for databases. For the task of converting natural language text to SQL queries, Text-to-SQL, adaptation of LLMs is of paramount importance, both in in-context learning and fine-tuning settings, depending on the amount of adaptation data used. In this paper, we propose an LLM-based Text-to-SQL model SQL-PaLM, leveraging on PaLM-2, that pushes the state-of-the-art in both settings. Few-shot SQL-PaLM is based on an execution-based self-consistency prompting approach designed for Text-to-SQL, and achieves 77.3% in test-suite accuracy on Spider, which to our best knowledge is the first to outperform previous state-of-the-art with fine-tuning by a significant margin, 4%. Furthermore, we demonstrate that the fine-tuned SQL-PALM outperforms it further by another 1%. Towards applying SQL-PaLM to real-world scenarios we further evaluate its robustness on other chall
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Self-Checker&#26694;&#26550;&#65292;&#23427;&#30001;&#21363;&#25554;&#21363;&#29992;&#30340;&#27169;&#22359;&#32452;&#25104;&#65292;&#33021;&#22815;&#22312;&#20960;&#20046;&#38646;&#27425;&#21551;&#21160;&#30340;&#24773;&#20917;&#19979;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24555;&#36895;&#39640;&#25928;&#30340;&#20107;&#23454;&#26816;&#26597;&#65292;&#36825;&#23545;&#20110;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#19979;&#26500;&#24314;&#20107;&#23454;&#26816;&#26597;&#31995;&#32479;&#38750;&#24120;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.14623</link><description>&lt;p&gt;
Self-Checker&#65306;&#29992;&#20110;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#20107;&#23454;&#26816;&#26597;&#30340;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
Self-Checker: Plug-and-Play Modules for Fact-Checking with Large Language Models. (arXiv:2305.14623v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14623
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Self-Checker&#26694;&#26550;&#65292;&#23427;&#30001;&#21363;&#25554;&#21363;&#29992;&#30340;&#27169;&#22359;&#32452;&#25104;&#65292;&#33021;&#22815;&#22312;&#20960;&#20046;&#38646;&#27425;&#21551;&#21160;&#30340;&#24773;&#20917;&#19979;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24555;&#36895;&#39640;&#25928;&#30340;&#20107;&#23454;&#26816;&#26597;&#65292;&#36825;&#23545;&#20110;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#19979;&#26500;&#24314;&#20107;&#23454;&#26816;&#26597;&#31995;&#32479;&#38750;&#24120;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#23454;&#26816;&#26597;&#26159;NLP&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20219;&#21153;&#65292;&#36890;&#24120;&#29992;&#20110;&#39564;&#35777;&#20027;&#24352;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#23545;&#29305;&#23450;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#19978;&#65292;&#36825;&#21487;&#33021;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#26102;&#38388;&#12290;&#38543;&#30528;&#20687;ChatGPT&#21644;GPT-3&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#30740;&#31350;&#20154;&#21592;&#29616;&#22312;&#27491;&#22312;&#25506;&#32034;&#23427;&#20204;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#20197;&#25191;&#34892;&#21508;&#31181;&#20219;&#21153;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Self-Checker&#65292;&#36825;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#21253;&#25324;&#19968;&#32452;&#21363;&#25554;&#21363;&#29992;&#30340;&#27169;&#22359;&#65292;&#36890;&#36807;&#22312;&#20960;&#20046;&#38646;&#27425;&#21551;&#21160;&#30340;&#24773;&#20917;&#19979;&#20165;&#25552;&#31034;LLMs&#65292;&#20174;&#32780;&#20415;&#20110;&#23545;&#20107;&#23454;&#36827;&#34892;&#26816;&#26597;&#12290;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#26500;&#24314;&#20107;&#23454;&#26816;&#26597;&#31995;&#32479;&#30340;&#24555;&#36895;&#39640;&#25928;&#26041;&#27861;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;Self-Checker&#22312;&#21033;&#29992;LLMs&#36827;&#34892;&#20107;&#23454;&#26816;&#26597;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#19982;SOTA&#24494;&#35843;&#27169;&#22411;&#30456;&#27604;&#20173;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#65292;&#36825;&#34920;&#26126;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fact-checking is an essential task in NLP that is commonly utilized for validating the factual accuracy of claims. Prior work has mainly focused on fine-tuning pre-trained languages models on specific datasets, which can be computationally intensive and time-consuming. With the rapid development of large language models (LLMs), such as ChatGPT and GPT-3, researchers are now exploring their in-context learning capabilities for a wide range of tasks. In this paper, we aim to assess the capacity of LLMs for fact-checking by introducing Self-Checker, a framework comprising a set of plug-and-play modules that facilitate fact-checking by purely prompting LLMs in an almost zero-shot setting. This framework provides a fast and efficient way to construct fact-checking systems in low-resource environments. Empirical results demonstrate the potential of Self-Checker in utilizing LLMs for fact-checking. However, there is still significant room for improvement compared to SOTA fine-tuned models, wh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21333;&#35821;&#35328;&#25968;&#25454;&#22312;&#22810;&#35821;&#35328;&#32763;&#35793;&#20013;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#21333;&#35821;&#35328;&#25968;&#25454;&#36890;&#24120;&#26377;&#21161;&#20110;&#22810;&#35821;&#35328;&#32763;&#35793;&#65292;&#20294;&#27169;&#22411;&#23545;&#39046;&#22495;&#19981;&#21305;&#37197;&#30340;&#23481;&#24525;&#24615;&#36739;&#24046;&#65292;&#23588;&#20854;&#22312;&#36739;&#23567;&#30340;&#27169;&#22411;&#35268;&#27169;&#19979;&#12290;&#22238;&#35793;&#22312;&#25968;&#25454;&#28304;&#30456;&#20284;&#30340;&#24773;&#20917;&#19979;&#26159;&#26377;&#30410;&#30340;&#65292;&#20294;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#21487;&#33021;&#26159;&#26377;&#23475;&#30340;&#65292;&#32780;&#21435;&#22122;&#33258;&#32534;&#30721;&#30340;&#25928;&#26524;&#19981;&#22914;&#20808;&#21069;&#25253;&#21578;&#30340;&#22909;&#12290;&#35268;&#27169;&#23545;&#20004;&#31181;&#26041;&#27861;&#37117;&#24456;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2305.14124</link><description>&lt;p&gt;
&#21333;&#35821;&#35328;&#25968;&#25454;&#20309;&#26102;&#26377;&#21161;&#20110;&#22810;&#35821;&#35328;&#32763;&#35793;&#65306;&#39046;&#22495;&#21644;&#27169;&#22411;&#35268;&#27169;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
When Does Monolingual Data Help Multilingual Translation: The Role of Domain and Model Scale. (arXiv:2305.14124v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14124
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21333;&#35821;&#35328;&#25968;&#25454;&#22312;&#22810;&#35821;&#35328;&#32763;&#35793;&#20013;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#21333;&#35821;&#35328;&#25968;&#25454;&#36890;&#24120;&#26377;&#21161;&#20110;&#22810;&#35821;&#35328;&#32763;&#35793;&#65292;&#20294;&#27169;&#22411;&#23545;&#39046;&#22495;&#19981;&#21305;&#37197;&#30340;&#23481;&#24525;&#24615;&#36739;&#24046;&#65292;&#23588;&#20854;&#22312;&#36739;&#23567;&#30340;&#27169;&#22411;&#35268;&#27169;&#19979;&#12290;&#22238;&#35793;&#22312;&#25968;&#25454;&#28304;&#30456;&#20284;&#30340;&#24773;&#20917;&#19979;&#26159;&#26377;&#30410;&#30340;&#65292;&#20294;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#21487;&#33021;&#26159;&#26377;&#23475;&#30340;&#65292;&#32780;&#21435;&#22122;&#33258;&#32534;&#30721;&#30340;&#25928;&#26524;&#19981;&#22914;&#20808;&#21069;&#25253;&#21578;&#30340;&#22909;&#12290;&#35268;&#27169;&#23545;&#20004;&#31181;&#26041;&#27861;&#37117;&#24456;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#65288;MMT&#65289;&#26159;&#36890;&#36807;&#28151;&#21512;&#24179;&#34892;&#21644;&#21333;&#35821;&#35328;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#25552;&#39640;&#20302;&#36164;&#28304;&#35821;&#35328;&#23545;&#32763;&#35793;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#23545;&#20110;&#21253;&#21547;&#21333;&#35821;&#35328;&#25968;&#25454;&#30340;&#19981;&#21516;&#26041;&#27861;&#30340;&#34920;&#29616;&#23384;&#22312;&#20105;&#35758;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21435;&#22122;&#33258;&#32534;&#30721;&#65288;DAE&#65289;&#21644;&#22238;&#35793;&#65288;BT&#65289;&#22312;&#19981;&#21516;&#25968;&#25454;&#26465;&#20214;&#21644;&#27169;&#22411;&#35268;&#27169;&#19979;&#23545;MMT&#30340;&#24433;&#21709;&#12290;&#19982;&#20808;&#21069;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;100&#20010;&#32763;&#35793;&#26041;&#21521;&#65292;&#24182;&#32771;&#34385;&#20102;&#35768;&#22810;&#21333;&#35821;&#35328;&#21644;&#27979;&#35797;&#25968;&#25454;&#30340;&#39046;&#22495;&#32452;&#21512;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21333;&#35821;&#35328;&#25968;&#25454;&#36890;&#24120;&#26377;&#21161;&#20110;MMT&#65292;&#20294;&#27169;&#22411;&#23545;&#39046;&#22495;&#19981;&#21305;&#37197;&#30340;&#23481;&#24525;&#24615;&#20986;&#20046;&#24847;&#26009;&#22320;&#36739;&#24046;&#65292;&#23588;&#20854;&#22312;&#36739;&#23567;&#30340;&#27169;&#22411;&#35268;&#27169;&#19979;&#12290;&#24403;&#24179;&#34892;&#12289;&#21333;&#35821;&#35328;&#21644;&#27979;&#35797;&#25968;&#25454;&#28304;&#30456;&#20284;&#26102;&#65292;&#22238;&#35793;&#26159;&#26377;&#30410;&#30340;&#65292;&#20294;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#21487;&#33021;&#26159;&#26377;&#23475;&#30340;&#65292;&#32780;DAE&#30340;&#25928;&#26524;&#19981;&#22914;&#20808;&#21069;&#25253;&#21578;&#30340;&#22909;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#35268;&#27169;&#65288;&#20174;90M&#21040;1.6B&#21442;&#25968;&#65289;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#23427;&#23545;&#20004;&#31181;&#26041;&#27861;&#37117;&#24456;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual machine translation (MMT), trained on a mixture of parallel and monolingual data, is key for improving translation in low-resource language pairs. However, the literature offers conflicting results on the performance of different methods of including monolingual data. To resolve this, we examine how denoising autoencoding (DAE) and backtranslation (BT) impact MMT under different data conditions and model scales. Unlike prior studies, we use a realistic dataset of 100 translation directions and consider many domain combinations of monolingual and test data. We find that monolingual data generally helps MMT, but models are surprisingly brittle to domain mismatches, especially at smaller model scales. BT is beneficial when the parallel, monolingual, and test data sources are similar but can be detrimental otherwise, while DAE is less effective than previously reported. Next, we analyze the impact of scale (from 90M to 1.6B parameters) and find it is important for both methods
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#36923;&#36753;&#39537;&#21160;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;&#21407;&#22987;&#25991;&#26412;&#36716;&#25442;&#20026;&#25277;&#35937;&#24847;&#20041;&#34920;&#36848;&#22270;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#36923;&#36753;&#20462;&#25913;&#21644;&#36716;&#25442;&#65292;&#29983;&#25104;&#22686;&#24378;&#25968;&#25454;&#65292;&#20174;&#32780;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#20307;&#31995;&#32467;&#26500;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.12599</link><description>&lt;p&gt;
&#36890;&#36807;&#36923;&#36753;&#39537;&#21160;&#30340;&#25968;&#25454;&#22686;&#24378;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Enhancing Logical Reasoning of Large Language Models through Logic-Driven Data Augmentation. (arXiv:2305.12599v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#36923;&#36753;&#39537;&#21160;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;&#21407;&#22987;&#25991;&#26412;&#36716;&#25442;&#20026;&#25277;&#35937;&#24847;&#20041;&#34920;&#36848;&#22270;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#36923;&#36753;&#20462;&#25913;&#21644;&#36716;&#25442;&#65292;&#29983;&#25104;&#22686;&#24378;&#25968;&#25454;&#65292;&#20174;&#32780;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#20307;&#31995;&#32467;&#26500;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#36923;&#36753;&#25512;&#29702;&#30456;&#32467;&#21512;&#21487;&#20197;&#22686;&#24378;&#23427;&#20204;&#22312;&#38382;&#39064;&#35299;&#20915;&#20013;&#30340;&#33021;&#21147;&#65292;&#20351;&#20854;&#26356;&#21152;&#24378;&#22823;&#21644;&#21487;&#38752;&#12290;&#28982;&#32780;&#65292;&#36923;&#36753;&#25512;&#29702;&#30340;&#22797;&#26434;&#24615;&#20351;&#24471;&#20174;&#32593;&#39029;&#19978;&#25910;&#38598;&#21487;&#38752;&#30340;&#25968;&#25454;&#26469;&#24314;&#31435;&#20840;&#38754;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#38754;&#20020;&#22256;&#38590;&#65292;&#36827;&#32780;&#24433;&#21709;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36923;&#36753;&#39537;&#21160;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;AMR-LDA&#12290;AMR-LDA&#23558;&#21407;&#22987;&#25991;&#26412;&#36716;&#25442;&#25104;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#65288;AMR&#65289;&#22270;&#65292;&#36825;&#26159;&#19968;&#31181;&#32467;&#26500;&#21270;&#30340;&#35821;&#20041;&#34920;&#31034;&#65292;&#21253;&#21547;&#20102;&#21477;&#23376;&#30340;&#36923;&#36753;&#32467;&#26500;&#65292;&#28982;&#21518;&#23545;&#35813;&#22270;&#36827;&#34892;&#25805;&#20316;&#20197;&#29983;&#25104;&#36923;&#36753;&#20462;&#25913;&#21518;&#30340;AMR&#22270;&#12290;&#20462;&#25913;&#21518;&#30340;AMR&#22270;&#38543;&#21518;&#34987;&#36716;&#25442;&#22238;&#25991;&#26412;&#65292;&#20174;&#32780;&#21019;&#24314;&#22686;&#24378;&#25968;&#25454;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20307;&#31995;&#32467;&#26500;&#26080;&#20851;&#65292;&#24182;&#36890;&#36807;&#25552;&#31034;&#22686;&#24378;&#26469;&#22686;&#24378;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT-3.5&#21644;GPT-4&#65289;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#26469;&#22686;&#24378;&#21028;&#21035;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Combining large language models with logical reasoning enhance their capacity to address problems in a robust and reliable manner. Nevertheless, the intricate nature of logical reasoning poses challenges to gathering reliable data from web for building comprehensive training datasets, subsequently affecting the performance on downstream tasks. To address this, we introduce a novel logic-driven data augmentation approach, AMR-LDA. AMR-LDA converts the original text into an Abstract Meaning Representation (AMR) graph, a structured semantic representation that encapsulates the logic structure of the sentence, upon which operations are performed to generate logically modified AMR graphs. The modified AMR graphs are subsequently converted back into texts to create augmented data. Notably, our methodology is architecture-agnostic and enhances generative large language models, such as GPT-3.5 and GPT-4, through prompt augmentation, and fine-tuning discriminative large language models through 
&lt;/p&gt;</description></item><item><title>SpecInfer&#26159;&#19968;&#31181;LLM&#26381;&#21153;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#25512;&#27979;&#25512;&#26029;&#21644;&#20196;&#29260;&#26641;&#39564;&#35777;&#26469;&#21152;&#36895;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#26029;&#36807;&#31243;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#20026;&#23427;&#20204;&#25552;&#20379;&#26381;&#21153;&#25152;&#38656;&#30340;&#31471;&#21040;&#31471;&#24310;&#36831;&#21644;&#35745;&#31639;&#35201;&#27714;&#65292;&#21516;&#26102;&#30830;&#20445;&#27169;&#22411;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.09781</link><description>&lt;p&gt;
SpecInfer&#65306;&#21033;&#29992;&#25512;&#27979;&#25512;&#26029;&#21644;&#20196;&#29260;&#26641;&#39564;&#35777;&#21152;&#36895;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification. (arXiv:2305.09781v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09781
&lt;/p&gt;
&lt;p&gt;
SpecInfer&#26159;&#19968;&#31181;LLM&#26381;&#21153;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#25512;&#27979;&#25512;&#26029;&#21644;&#20196;&#29260;&#26641;&#39564;&#35777;&#26469;&#21152;&#36895;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#26029;&#36807;&#31243;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#20026;&#23427;&#20204;&#25552;&#20379;&#26381;&#21153;&#25152;&#38656;&#30340;&#31471;&#21040;&#31471;&#24310;&#36831;&#21644;&#35745;&#31639;&#35201;&#27714;&#65292;&#21516;&#26102;&#30830;&#20445;&#27169;&#22411;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38656;&#35201;&#39640;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#65292;&#22240;&#27492;&#24555;&#36895;&#21644;&#24265;&#20215;&#22320;&#20026;&#23427;&#20204;&#25552;&#20379;&#26381;&#21153;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;SpecInfer&#65292;&#19968;&#20010;LLM&#26381;&#21153;&#31995;&#32479;&#65292;&#23427;&#21033;&#29992;&#25512;&#27979;&#25512;&#26029;&#21644;&#20196;&#29260;&#26641;&#39564;&#35777;&#21152;&#36895;&#29983;&#25104;&#24335;LLM&#25512;&#26029;&#12290;SpecInfer&#32972;&#21518;&#30340;&#20851;&#38190;&#26159;&#23558;&#21508;&#31181;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38598;&#20307;&#25552;&#21319;&#35843;&#25972;&#65292;&#20849;&#21516;&#39044;&#27979;LLM&#30340;&#36755;&#20986;&#65307; &#39044;&#27979;&#32467;&#26524;&#32452;&#32455;&#25104;&#19968;&#20010;&#20196;&#29260;&#26641;&#65292;&#20854;&#20013;&#27599;&#20010;&#33410;&#28857;&#37117;&#34920;&#31034;&#20505;&#36873;&#20196;&#29260;&#24207;&#21015;&#12290;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26641;&#30340;&#24182;&#34892;&#35299;&#30721;&#26426;&#21046;&#65292;&#20197;LMM&#20316;&#20026;&#20196;&#29260;&#26641;&#39564;&#35777;&#22120;&#26469;&#39564;&#35777;&#20196;&#29260;&#26641;&#25152;&#20195;&#34920;&#30340;&#25152;&#26377;&#20505;&#36873;&#20196;&#29260;&#24207;&#21015;&#30340;&#27491;&#30830;&#24615;&#12290;SpecInfer&#20351;&#29992;LLM&#20316;&#20026;&#20196;&#29260;&#26641;&#39564;&#35777;&#22120;&#65292;&#32780;&#19981;&#26159;&#22686;&#37327;&#35299;&#30721;&#22120;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#20026;&#29983;&#25104;&#24335;LLM&#25552;&#20379;&#26381;&#21153;&#25152;&#38656;&#30340;&#31471;&#21040;&#31471;&#24310;&#36831;&#21644;&#35745;&#31639;&#35201;&#27714;&#65292;&#21516;&#26102;&#21487;&#30830;&#20445;&#27169;&#22411;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The high computational and memory requirements of generative large language models (LLMs) make it challenging to serve them quickly and cheaply. This paper introduces SpecInfer, an LLM serving system that accelerates generative LLM inference with speculative inference and token tree verification. A key insight behind SpecInfer is to combine various collectively boost-tuned small language models to jointly predict the LLM's outputs; the predictions are organized as a token tree, whose nodes each represent a candidate token sequence. The correctness of all candidate token sequences represented by a token tree is verified by the LLM in parallel using a novel tree-based parallel decoding mechanism. SpecInfer uses an LLM as a token tree verifier instead of an incremental decoder, which significantly reduces the end-to-end latency and computational requirement for serving generative LLMs while provably preserving model quality.
&lt;/p&gt;</description></item><item><title>RPD&#23545;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#28431;&#27934;&#36827;&#34892;&#20102;&#38450;&#24481;&#65292;&#36890;&#36807;&#35782;&#21035;&#23545;&#25239;&#24615;&#31034;&#20363;&#24182;&#27880;&#20837;&#23433;&#20840;&#25200;&#21160;&#26469;&#20943;&#23569;&#35823;&#38450;&#24481;&#65292;&#25104;&#21151;&#22320;&#20462;&#22797;&#20102;&#39640;&#36798;97%&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#22312;&#33258;&#28982;&#31034;&#20363;&#30340;&#24615;&#33021;&#20165;&#38477;&#20302;&#20102;&#32422;2%&#12290;</title><link>http://arxiv.org/abs/2305.04067</link><description>&lt;p&gt;
&#21453;&#24212;&#24615;&#25668;&#20837;&#25200;&#21160;&#65306;&#23545;&#25239;&#25991;&#26412;&#38450;&#24481;&#30340;&#19968;&#31181;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Reactive Perturbation Defocusing for Textual Adversarial Defense. (arXiv:2305.04067v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04067
&lt;/p&gt;
&lt;p&gt;
RPD&#23545;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#28431;&#27934;&#36827;&#34892;&#20102;&#38450;&#24481;&#65292;&#36890;&#36807;&#35782;&#21035;&#23545;&#25239;&#24615;&#31034;&#20363;&#24182;&#27880;&#20837;&#23433;&#20840;&#25200;&#21160;&#26469;&#20943;&#23569;&#35823;&#38450;&#24481;&#65292;&#25104;&#21151;&#22320;&#20462;&#22797;&#20102;&#39640;&#36798;97%&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#22312;&#33258;&#28982;&#31034;&#20363;&#30340;&#24615;&#33021;&#20165;&#38477;&#20302;&#20102;&#32422;2%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#35797;&#22270;&#37325;&#26500;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#22312;&#38450;&#24481;&#23545;&#25239;&#24615;&#31034;&#20363;&#26041;&#38754;&#24615;&#33021;&#26377;&#38480;&#65292;&#21516;&#26102;&#20063;&#20250;&#23545;&#33258;&#28982;&#31034;&#20363;&#30340;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21453;&#24212;&#24615;&#25668;&#20837;&#25200;&#21160; (RPD) &#30340;&#26041;&#27861;&#12290;RPD &#20351;&#29992;&#23545;&#25239;&#26816;&#27979;&#22120;&#35782;&#21035;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#24182;&#20943;&#23569;&#22312;&#33258;&#28982;&#31034;&#20363;&#19978;&#30340;&#35823;&#38450;&#24481;&#12290;RPD &#19981;&#26159;&#37325;&#26500;&#23545;&#25163;&#65292;&#32780;&#26159;&#22312;&#23545;&#25239;&#24615;&#31034;&#20363;&#20013;&#27880;&#20837;&#23433;&#20840;&#25200;&#21160;&#65292;&#20197;&#20998;&#25955;&#30446;&#26631;&#27169;&#22411;&#23545;&#24694;&#24847;&#25200;&#21160;&#30340;&#27880;&#24847;&#21147;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#12289;&#20004;&#20010;&#30446;&#26631;&#27169;&#22411;&#21644;&#21508;&#31181;&#23545;&#25239;&#24615;&#25915;&#20987;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#25104;&#21151;&#22320;&#20462;&#22797;&#20102;&#22823;&#32422; 97% &#30340;&#27491;&#30830;&#35782;&#21035;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#24182;&#19988;&#33258;&#28982;&#31034;&#20363;&#30340;&#24615;&#33021;&#20165;&#38477;&#20302;&#20102;&#32422; 2%&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have shown that large pre-trained language models are vulnerable to adversarial attacks. Existing methods attempt to reconstruct the adversarial examples. However, these methods usually have limited performance in defense against adversarial examples, while also negatively impacting the performance on natural examples. To overcome this problem, we propose a method called Reactive Perturbation Defocusing (RPD). RPD uses an adversarial detector to identify adversarial examples and reduce false defenses on natural examples. Instead of reconstructing the adversaries, RPD injects safe perturbations into adversarial examples to distract the objective models from the malicious perturbations. Our experiments on three datasets, two objective models, and various adversarial attacks show that our proposed framework successfully repairs up to approximately 97% of correctly identified adversarial examples with only about a 2% performance decrease on natural examples. We also provide 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;NAIST-SIC-Aligned&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#21160;&#23545;&#40784;&#30340;&#33521;&#26085;&#24179;&#34892;&#21516;&#22768;&#20256;&#35793;&#25968;&#25454;&#38598;&#12290;&#35813;&#35770;&#25991;&#20351;&#29992;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#23545;&#40784;&#26041;&#27861;&#65292;&#32463;&#36807;&#23450;&#37327;&#25110;&#23450;&#24615;&#39564;&#35777;&#30340;&#27599;&#20010;&#27493;&#39588;&#65292;&#20197;&#30830;&#20445;&#35821;&#26009;&#24211;&#30340;&#36136;&#37327;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#24320;&#28304;&#30340;&#22823;&#35268;&#27169;&#24179;&#34892;SI&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2304.11766</link><description>&lt;p&gt;
NAIST-SIC-Aligned&#65306;&#33258;&#21160;&#23545;&#40784;&#30340;&#33521;&#26085;&#21516;&#22768;&#20256;&#35793;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
NAIST-SIC-Aligned: Automatically-Aligned English-Japanese Simultaneous Interpretation Corpus. (arXiv:2304.11766v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;NAIST-SIC-Aligned&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#21160;&#23545;&#40784;&#30340;&#33521;&#26085;&#24179;&#34892;&#21516;&#22768;&#20256;&#35793;&#25968;&#25454;&#38598;&#12290;&#35813;&#35770;&#25991;&#20351;&#29992;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#23545;&#40784;&#26041;&#27861;&#65292;&#32463;&#36807;&#23450;&#37327;&#25110;&#23450;&#24615;&#39564;&#35777;&#30340;&#27599;&#20010;&#27493;&#39588;&#65292;&#20197;&#30830;&#20445;&#35821;&#26009;&#24211;&#30340;&#36136;&#37327;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#24320;&#28304;&#30340;&#22823;&#35268;&#27169;&#24179;&#34892;SI&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#21033;&#29992;&#21516;&#22768;&#20256;&#35793;&#65288;SI&#65289;&#25968;&#25454;&#26469;&#24433;&#21709;&#21516;&#22768;&#26426;&#22120;&#32763;&#35793;&#65288;SiMT&#65289;&#20173;&#28982;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#30001;&#20110;&#32570;&#20047;&#22823;&#35268;&#27169;&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#65292;&#30740;&#31350;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;NAIST-SIC-Aligned&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#21160;&#23545;&#40784;&#30340;&#33521;&#26085;&#24179;&#34892;&#21516;&#22768;&#20256;&#35793;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#19968;&#20010;&#38750;&#23545;&#40784;&#35821;&#26009;&#24211;NAIST-SIC&#24320;&#22987;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#23545;&#40784;&#26041;&#27861;&#65292;&#20351;&#35821;&#26009;&#24211;&#20855;&#26377;&#24179;&#34892;&#24615;&#65292;&#20174;&#32780;&#36866;&#21512;&#27169;&#22411;&#35757;&#32451;&#12290;&#31532;&#19968;&#38454;&#27573;&#26159;&#31895;&#30053;&#23545;&#40784;&#65292;&#22312;&#27492;&#27493;&#39588;&#20013;&#65292;&#25105;&#20204;&#22312;&#28304;&#35821;&#35328;&#21644;&#30446;&#26631;&#35821;&#35328;&#20043;&#38388;&#25191;&#34892;&#19968;&#20010;&#22810;&#23545;&#22810;&#30340;&#26144;&#23556;&#65307;&#31532;&#20108;&#38454;&#27573;&#26159;&#32454;&#31890;&#24230;&#23545;&#40784;&#65292;&#22312;&#27492;&#27493;&#39588;&#20013;&#65292;&#25105;&#20204;&#25191;&#34892;&#35821;&#21477;&#20869;&#37096;&#21644;&#35821;&#21477;&#38388;&#36807;&#28388;&#26469;&#25552;&#39640;&#23545;&#40784;&#23545;&#30340;&#36136;&#37327;&#12290;&#20026;&#30830;&#20445;&#35821;&#26009;&#24211;&#30340;&#36136;&#37327;&#65292;&#27599;&#20010;&#27493;&#39588;&#37117;&#32463;&#36807;&#20102;&#23450;&#37327;&#25110;&#23450;&#24615;&#30340;&#39564;&#35777;&#12290;&#36825;&#26159;&#25991;&#29486;&#20013;&#31532;&#19968;&#20010;&#24320;&#28304;&#30340;&#22823;&#35268;&#27169;&#24179;&#34892;SI&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36824;&#25163;&#21160;&#31934;&#36873;&#20102;&#19968;&#20010;&#23567;&#22411;&#27979;&#35797;&#38598;&#29992;&#20110;&#35780;&#20272;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
It remains a question that how simultaneous interpretation (SI) data affects simultaneous machine translation (SiMT). Research has been limited due to the lack of a large-scale training corpus. In this work, we aim to fill in the gap by introducing NAIST-SIC-Aligned, which is an automatically-aligned parallel English-Japanese SI dataset. Starting with a non-aligned corpus NAIST-SIC, we propose a two-stage alignment approach to make the corpus parallel and thus suitable for model training. The first stage is coarse alignment where we perform a many-to-many mapping between source and target sentences, and the second stage is fine-grained alignment where we perform intra- and inter-sentence filtering to improve the quality of aligned pairs. To ensure the quality of the corpus, each step has been validated either quantitatively or qualitatively. This is the first open-sourced large-scale parallel SI dataset in the literature. We also manually curated a small test set for evaluation purpose
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20154;-LLM&#20132;&#20114;&#26694;&#26550;&#65292;&#20302;&#20195;&#30721;LLM&#65292;&#35813;&#26694;&#26550;&#21487;&#36890;&#36807;&#20845;&#31181;&#31867;&#22411;&#30340;&#31616;&#21333;&#20302;&#20195;&#30721;&#21487;&#35270;&#21270;&#32534;&#31243;&#20132;&#20114;&#23454;&#29616;&#26356;&#21487;&#25511;&#21644;&#31283;&#23450;&#30340;&#21709;&#24212;&#65292;&#20855;&#26377;&#21487;&#25511;&#24615;&#24378;&#12289;&#29992;&#25143;&#21451;&#22909;&#30340;&#20132;&#20114;&#26041;&#24335;&#21644;&#24191;&#27867;&#30340;&#24212;&#29992;&#33539;&#22260;&#30340;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2304.08103</link><description>&lt;p&gt;
&#20302;&#20195;&#30721;LLM&#65306;LLM&#19978;&#30340;&#21487;&#35270;&#21270;&#32534;&#31243;
&lt;/p&gt;
&lt;p&gt;
Low-code LLM: Visual Programming over LLMs. (arXiv:2304.08103v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20154;-LLM&#20132;&#20114;&#26694;&#26550;&#65292;&#20302;&#20195;&#30721;LLM&#65292;&#35813;&#26694;&#26550;&#21487;&#36890;&#36807;&#20845;&#31181;&#31867;&#22411;&#30340;&#31616;&#21333;&#20302;&#20195;&#30721;&#21487;&#35270;&#21270;&#32534;&#31243;&#20132;&#20114;&#23454;&#29616;&#26356;&#21487;&#25511;&#21644;&#31283;&#23450;&#30340;&#21709;&#24212;&#65292;&#20855;&#26377;&#21487;&#25511;&#24615;&#24378;&#12289;&#29992;&#25143;&#21451;&#22909;&#30340;&#20132;&#20114;&#26041;&#24335;&#21644;&#24191;&#27867;&#30340;&#24212;&#29992;&#33539;&#22260;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#22320;&#21033;&#29992;LLM&#26469;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#36890;&#24120;&#38656;&#35201;&#36827;&#34892;&#32791;&#26102;&#32780;&#38590;&#20197;&#25484;&#25511;&#30340;&#25552;&#31034;&#24037;&#31243;&#22788;&#29702;&#36807;&#31243;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20154;-LLM&#20132;&#20114;&#26694;&#26550;&#65292;&#21363;&#20302;&#20195;&#30721;LLM&#12290;&#23427;&#21253;&#25324;&#20845;&#31181;&#31867;&#22411;&#30340;&#31616;&#21333;&#20302;&#20195;&#30721;&#21487;&#35270;&#21270;&#32534;&#31243;&#20132;&#20114;&#65292;&#20840;&#37096;&#25903;&#25345;&#28857;&#20987;&#12289;&#25302;&#25918;&#25110;&#25991;&#26412;&#32534;&#36753;&#65292;&#20197;&#23454;&#29616;&#26356;&#21487;&#25511;&#21644;&#31283;&#23450;&#30340;&#21709;&#24212;&#12290;&#36890;&#36807;&#19982;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#30340;&#35270;&#35273;&#20132;&#20114;&#65292;&#29992;&#25143;&#21487;&#20197;&#23558;&#20854;&#24819;&#27861;&#32435;&#20837;&#24037;&#20316;&#27969;&#31243;&#65292;&#32780;&#19981;&#24517;&#32534;&#20889;&#29712;&#30862;&#30340;&#25552;&#31034;&#12290;&#25552;&#20986;&#30340;&#20302;&#20195;&#30721;LLM&#26694;&#26550;&#30001;&#35268;&#21010;LLM&#21644;&#25191;&#34892;LLM&#20004;&#37096;&#20998;&#32452;&#25104;&#65292;&#35268;&#21010;LLM&#20026;&#22797;&#26434;&#20219;&#21153;&#35774;&#35745;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#35268;&#21010;&#24037;&#20316;&#27969;&#31243;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#20302;&#20195;&#30721;&#21487;&#35270;&#21270;&#32534;&#31243;&#25805;&#20316;&#30456;&#24212;&#22320;&#36827;&#34892;&#32534;&#36753;&#21644;&#30830;&#35748;&#65292;&#32780;&#25191;&#34892;LLM&#21017;&#25353;&#29031;&#29992;&#25143;&#30830;&#35748;&#30340;&#24037;&#20316;&#27969;&#31243;&#29983;&#25104;&#21709;&#24212;&#12290;&#25105;&#20204;&#24378;&#35843;&#20302;&#20195;&#30721;LLM&#30340;&#19977;&#20010;&#20248;&#28857;&#65306;&#21487;&#25511;&#30340;&#29983;&#25104;&#32467;&#26524;&#12289;&#29992;&#25143;&#21451;&#22909;&#30340;&#20154;-LLM&#20132;&#20114;&#20197;&#21450;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effectively utilizing LLMs for complex tasks is challenging, often involving a time-consuming and uncontrollable prompt engineering process. This paper introduces a novel human-LLM interaction framework, Low-code LLM. It incorporates six types of simple low-code visual programming interactions, all supported by clicking, dragging, or text editing, to achieve more controllable and stable responses. Through visual interaction with a graphical user interface, users can incorporate their ideas into the workflow without writing trivial prompts. The proposed Low-code LLM framework consists of a Planning LLM that designs a structured planning workflow for complex tasks, which can be correspondingly edited and confirmed by users through low-code visual programming operations, and an Executing LLM that generates responses following the user-confirmed workflow. We highlight three advantages of the low-code LLM: controllable generation results, user-friendly human-LLM interaction, and broadly app
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#19982;&#20998;&#31867;&#20102;Transformer&#27169;&#22411;&#31995;&#21015;&#20013;&#26368;&#27969;&#34892;&#30340;&#27169;&#22411;&#65292;&#21253;&#25324;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#20154;&#31867;&#21442;&#19982;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#24182;&#23545;&#20854;&#20013;&#21019;&#26032;&#24615;&#30340;&#26041;&#38754;&#20570;&#20102;&#20171;&#32461;&#12290;</title><link>http://arxiv.org/abs/2302.07730</link><description>&lt;p&gt;
Transformer&#27169;&#22411;&#65306;&#20171;&#32461;&#19982;&#30446;&#24405;
&lt;/p&gt;
&lt;p&gt;
Transformer models: an introduction and catalog. (arXiv:2302.07730v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07730
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#19982;&#20998;&#31867;&#20102;Transformer&#27169;&#22411;&#31995;&#21015;&#20013;&#26368;&#27969;&#34892;&#30340;&#27169;&#22411;&#65292;&#21253;&#25324;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#20154;&#31867;&#21442;&#19982;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#24182;&#23545;&#20854;&#20013;&#21019;&#26032;&#24615;&#30340;&#26041;&#38754;&#20570;&#20102;&#20171;&#32461;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;Transformer&#31995;&#21015;&#30340;&#22522;&#30784;&#27169;&#22411;&#22914;&#38632;&#21518;&#26149;&#31499;&#33324;&#28044;&#29616;&#20986;&#26469;&#65292;&#23427;&#20204;&#20013;&#26377;&#20123;&#20855;&#26377;&#20196;&#20154;&#38590;&#24536;&#30340;&#12289;&#26377;&#26102;&#29978;&#33267;&#28369;&#31293;&#26377;&#36259;&#20294;&#21364;&#19981;&#20855;&#33258;&#35299;&#37322;&#24615;&#30340;&#21517;&#31216;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#30456;&#23545;&#20840;&#38754;&#20294;&#31616;&#21333;&#30340;Transformer&#27169;&#22411;&#30446;&#24405;&#21644;&#20998;&#31867;&#65292;&#24182;&#20171;&#32461;Transformer&#27169;&#22411;&#20013;&#26368;&#37325;&#35201;&#30340;&#26041;&#38754;&#21644;&#21019;&#26032;&#12290;&#30446;&#24405;&#20013;&#30340;&#27169;&#22411;&#21253;&#25324;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#65288;&#20363;&#22914;BERT&#25110;GPT3&#65289;&#30340;&#27169;&#22411;&#65292;&#20197;&#21450;&#36827;&#19968;&#27493;&#36890;&#36807;&#20154;&#31867;&#21442;&#19982;&#35757;&#32451;&#65288;&#20363;&#22914;ChatGPT&#20351;&#29992;&#30340;InstructGPT&#27169;&#22411;&#65289;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the past few years we have seen the meteoric appearance of dozens of foundation models of the Transformer family, all of which have memorable and sometimes funny, but not self-explanatory, names. The goal of this paper is to offer a somewhat comprehensive but simple catalog and classification of the most popular Transformer models. The paper also includes an introduction to the most important aspects and innovations in Transformer models. Our catalog will include models that are trained using self-supervised learning (e.g., BERT or GPT3) as well as those that are further trained using a human-in-the-loop (e.g. the InstructGPT model used by ChatGPT).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;16&#20010;&#20420;&#32599;&#26031;&#23186;&#20307;&#26426;&#26500;&#21644;732&#20010;&#30005;&#25253;&#39057;&#36947;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#21457;&#29616;&#26032;&#38395;&#23186;&#20307;&#19981;&#20165;&#36890;&#36807;&#30005;&#25253;&#20256;&#25773;&#29616;&#26377;&#30340;&#21465;&#20107;&#65292;&#32780;&#19988;&#20250;&#20174;&#30005;&#25253;&#24179;&#21488;&#28304;&#26448;&#26009;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;2.3&#65285;&#33267;26.7&#65285;&#30340;&#25991;&#31456;&#23558;&#20027;&#39064;&#24402;&#22240;&#20110;&#30005;&#25253;&#27963;&#21160;&#12290;</title><link>http://arxiv.org/abs/2301.10856</link><description>&lt;p&gt;
&#37096;&#20998;&#21160;&#21592;&#65306;&#36319;&#36394;&#20420;&#32599;&#26031;&#23186;&#20307;&#21644;&#30005;&#25253;&#20043;&#38388;&#30340;&#22810;&#35821;&#35328;&#20449;&#24687;&#27969;
&lt;/p&gt;
&lt;p&gt;
Partial Mobilization: Tracking Multilingual Information Flows Amongst Russian Media Outlets and Telegram. (arXiv:2301.10856v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;16&#20010;&#20420;&#32599;&#26031;&#23186;&#20307;&#26426;&#26500;&#21644;732&#20010;&#30005;&#25253;&#39057;&#36947;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#21457;&#29616;&#26032;&#38395;&#23186;&#20307;&#19981;&#20165;&#36890;&#36807;&#30005;&#25253;&#20256;&#25773;&#29616;&#26377;&#30340;&#21465;&#20107;&#65292;&#32780;&#19988;&#20250;&#20174;&#30005;&#25253;&#24179;&#21488;&#28304;&#26448;&#26009;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;2.3&#65285;&#33267;26.7&#65285;&#30340;&#25991;&#31456;&#23558;&#20027;&#39064;&#24402;&#22240;&#20110;&#30005;&#25253;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20420;&#32599;&#26031;&#20837;&#20405;&#20044;&#20811;&#20848;&#21518;&#65292;&#38024;&#23545;&#20420;&#32599;&#26031;&#22312;&#32447;&#23186;&#20307;&#30340;&#34394;&#20551;&#20449;&#24687;&#21644;&#23459;&#20256;&#65292;&#21253;&#25324;&#20420;&#32599;&#26031;&#20043;&#22768;&#21644;&#21355;&#26143;&#26032;&#38395;&#22312;&#20869;&#30340;&#20420;&#32599;&#26031;&#23186;&#20307;&#22312;&#27431;&#27954;&#36973;&#21040;&#31105;&#27490;&#12290;&#20026;&#20102;&#20445;&#25345;&#35266;&#20247;&#25968;&#37327;&#65292;&#35768;&#22810;&#20420;&#32599;&#26031;&#23186;&#20307;&#24320;&#22987;&#22312;&#30005;&#25253;&#31561;&#28040;&#24687;&#26381;&#21153;&#19978;&#22823;&#21147;&#23459;&#20256;&#20854;&#20869;&#23481;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;2022&#24180;&#26399;&#38388;16&#23478;&#20420;&#32599;&#26031;&#23186;&#20307;&#26426;&#26500;&#22914;&#20309;&#19982;732&#20010;&#30005;&#25253;&#39057;&#36947;&#20114;&#21160;&#21644;&#21033;&#29992;&#12290;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;MPNet&#12289;DP-means&#32858;&#31867;&#21644;Hawkes&#36807;&#31243;&#65292;&#25105;&#20204;&#36319;&#36394;&#26032;&#38395;&#32593;&#31449;&#21644;&#30005;&#25253;&#39057;&#36947;&#20043;&#38388;&#30340;&#21465;&#20107;&#20256;&#25773;&#24773;&#20917;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#26032;&#38395;&#23186;&#20307;&#19981;&#20165;&#36890;&#36807;&#30005;&#25253;&#20256;&#25773;&#29616;&#26377;&#30340;&#21465;&#20107;&#65292;&#32780;&#19988;&#20182;&#20204;&#20250;&#20174;&#30005;&#25253;&#24179;&#21488;&#28304;&#26448;&#26009;&#12290;&#22312;&#25105;&#20204;&#30740;&#31350;&#30340;&#32593;&#31449;&#20013;&#65292;2.3&#65285;&#65288;ura.news&#65289;&#33267;26.7&#65285;&#65288;ukraina.ru&#65289;&#30340;&#25991;&#31456;&#35752;&#35770;&#20102;&#28304;&#20110;/&#23548;&#33268;&#30005;&#25253;&#27963;&#21160;&#30340;&#20869;&#23481;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#36319;&#36394;&#20010;&#21035;&#20027;&#39064;&#30340;&#25193;&#25955;&#65292;&#25105;&#20204;&#27979;&#37327;&#26032;&#38395;&#32593;&#31449;&#21457;&#34920;&#25991;&#31456;&#30340;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In response to disinformation and propaganda from Russian online media following the Russian invasion of Ukraine, Russian outlets including Russia Today and Sputnik News were banned throughout Europe. To maintain viewership, many of these Russian outlets began to heavily promote their content on messaging services like Telegram. In this work, we study how 16 Russian media outlets interacted with and utilized 732 Telegram channels throughout 2022. Leveraging the foundational model MPNet, DP-means clustering, and Hawkes Processes, we trace how narratives spread between news sites and Telegram channels. We show that news outlets not only propagate existing narratives through Telegram, but that they source material from the messaging platform. Across the sites in our study, between 2.3% (ura.news) and 26.7% (ukraina.ru) of articles discuss content that originated/resulted from activity on Telegram. Finally, tracking the spread of individual topics, we measure the rate at which news website
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#35828;&#26381;&#24615;&#20889;&#20316;&#25216;&#24039;&#30340;&#25991;&#26412;&#27573;&#33853;&#36827;&#34892;&#20998;&#31867;&#26469;&#22686;&#21152;&#33258;&#21160;&#21270;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#30340;&#26032;&#23618;&#27425;&#65292;&#20197;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#29702;&#30001;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#24120;&#35265;&#35828;&#26381;&#24615;&#20889;&#20316;&#31574;&#30053;&#30340;&#27880;&#37322;&#26041;&#26696;&#21644;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992; RoBERTa &#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2211.05985</link><description>&lt;p&gt;
&#20351;&#29992;&#35828;&#26381;&#24615;&#20889;&#20316;&#31574;&#30053;&#26469;&#35299;&#37322;&#21644;&#26816;&#27979;&#20581;&#24247;&#38169;&#35823;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Using Persuasive Writing Strategies to Explain and Detect Health Misinformation. (arXiv:2211.05985v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05985
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#35828;&#26381;&#24615;&#20889;&#20316;&#25216;&#24039;&#30340;&#25991;&#26412;&#27573;&#33853;&#36827;&#34892;&#20998;&#31867;&#26469;&#22686;&#21152;&#33258;&#21160;&#21270;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#30340;&#26032;&#23618;&#27425;&#65292;&#20197;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#29702;&#30001;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#24120;&#35265;&#35828;&#26381;&#24615;&#20889;&#20316;&#31574;&#30053;&#30340;&#27880;&#37322;&#26041;&#26696;&#21644;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992; RoBERTa &#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#20551;&#20449;&#24687;&#30340;&#20256;&#25773;&#26159;&#24403;&#20170;&#31038;&#20250;&#30340;&#19968;&#22823;&#38382;&#39064;&#65292;&#35768;&#22810;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#30740;&#31350;&#20154;&#21592;&#27491;&#22312;&#21162;&#21147;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#30001;&#20110;&#27599;&#22825;&#21019;&#36896;&#30340;&#34394;&#20551;&#20449;&#24687;&#25968;&#37327;&#24040;&#22823;&#65292;&#23558;&#27492;&#20219;&#21153;&#30041;&#32473;&#20154;&#24037;&#20107;&#23454;&#26816;&#26597;&#21592;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#25968;&#25454;&#31185;&#23398;&#23478;&#21644;&#30740;&#31350;&#20154;&#21592;&#22810;&#24180;&#26469;&#19968;&#30452;&#33268;&#21147;&#20110;&#33258;&#21160;&#21270;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#65292;&#20294;&#20170;&#22825;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#30446;&#26631;&#26159;&#20026;&#33258;&#21160;&#21270;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#28155;&#21152;&#19968;&#20010;&#26032;&#23618;&#27425;&#65307;&#20351;&#29992;&#20855;&#26377;&#35828;&#26381;&#24615;&#20889;&#20316;&#25216;&#24039;&#30340;&#25991;&#26412;&#27573;&#33853;&#36827;&#34892;&#20998;&#31867;&#65292;&#20197;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#29702;&#30001;&#65292;&#35828;&#26126;&#20026;&#20160;&#20040;&#36825;&#31687;&#25991;&#31456;&#21487;&#20197;&#26631;&#35760;&#20026;&#34394;&#20551;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#35768;&#22810;&#24120;&#35265;&#35828;&#26381;&#24615;&#20889;&#20316;&#31574;&#30053;&#30340;&#26032;&#27880;&#37322;&#26041;&#26696;&#65292;&#20197;&#21450;&#30456;&#24212;&#30340;&#20154;&#24037;&#27880;&#37322;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992; RoBERTa &#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#26469;&#23436;&#25104;&#27492;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#20855;&#26377;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20960;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#32467;&#26524;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The spread of misinformation is a prominent problem in today's society, and many researchers in academia and industry are trying to combat it. Due to the vast amount of misinformation that is created every day, it is unrealistic to leave this task to human fact-checkers. Data scientists and researchers have been working on automated misinformation detection for years, and it is still a challenging problem today. The goal of our research is to add a new level to automated misinformation detection; classifying segments of text with persuasive writing techniques in order to produce interpretable reasoning for why an article can be marked as misinformation. To accomplish this, we present a novel annotation scheme containing many common persuasive writing tactics, along with a dataset with human annotations accordingly. For this task, we make use of a RoBERTa model for text classification, due to its high performance in NLP. We develop several language model-based baselines and present the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;LUT&#30340;&#37327;&#21270;&#30697;&#38453;&#20056;&#27861;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#25512;&#29702;&#12290;&#37319;&#29992;&#20165;&#38024;&#23545;&#26435;&#37325;&#30340;&#37327;&#21270;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;LUT-GEMM&#20869;&#26680;&#21152;&#36895;&#37327;&#21270;&#30697;&#38453;&#20056;&#27861;&#65292;&#23454;&#29616;&#21387;&#32553;&#27604;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#28789;&#27963;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2206.09557</link><description>&lt;p&gt;
LUT-GEMM&#65306;&#22522;&#20110;LUT&#30340;&#37327;&#21270;&#30697;&#38453;&#20056;&#27861;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
LUT-GEMM: Quantized Matrix Multiplication based on LUTs for Efficient Inference in Large-Scale Generative Language Models. (arXiv:2206.09557v3 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.09557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;LUT&#30340;&#37327;&#21270;&#30697;&#38453;&#20056;&#27861;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#25512;&#29702;&#12290;&#37319;&#29992;&#20165;&#38024;&#23545;&#26435;&#37325;&#30340;&#37327;&#21270;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;LUT-GEMM&#20869;&#26680;&#21152;&#36895;&#37327;&#21270;&#30697;&#38453;&#20056;&#27861;&#65292;&#23454;&#29616;&#21387;&#32553;&#27604;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#28789;&#27963;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#20808;&#36827;&#25216;&#26415;&#19982;Transformer&#26550;&#26500;&#30340;&#32467;&#21512;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24378;&#22823;&#30340;NLP&#27169;&#22411;&#38656;&#35201;&#36234;&#26469;&#36234;&#22823;&#30340;&#27169;&#22411;&#23610;&#23544;&#65292;&#20174;&#32780;&#23548;&#33268;&#22823;&#37327;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#35268;&#27169;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#25512;&#29702;&#26694;&#26550;&#12290;&#20026;&#20102;&#20943;&#23569;&#27169;&#22411;&#22823;&#23567;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#20165;&#38024;&#23545;&#26435;&#37325;&#30340;&#37327;&#21270;&#31574;&#30053;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#28608;&#27963;&#20989;&#25968;&#30340;&#23436;&#25972;&#31934;&#24230;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#38750;&#22343;&#21248;&#25110;&#22343;&#21248;&#37327;&#21270;&#25216;&#26415;&#33719;&#24471;&#27599;&#20010;&#26435;&#37325;&#30340;&#20302;&#20110;4&#20301;&#30340;&#37327;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;LUT-GEMM&#20869;&#26680;&#21152;&#36895;&#20102;&#37327;&#21270;&#30697;&#38453;&#20056;&#27861;&#65292;&#25552;&#20379;&#20102;&#21387;&#32553;&#27604;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#28789;&#27963;&#24179;&#34913;&#12290;&#19982;&#26089;&#26399;&#20165;&#36866;&#29992;&#20110;&#26435;&#37325;&#37327;&#21270;&#30340;&#30697;&#38453;&#20056;&#27861;&#20869;&#26680;&#19981;&#21516;&#65292;LUT-GEMM&#26377;&#25928;&#22320;&#28040;&#38500;&#20102;&#36164;&#28304;&#28040;&#32791;&#26356;&#22823;&#30340;&#21453;&#37327;&#21270;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advancements in self-supervised learning, combined with the Transformer architecture, have enabled natural language processing (NLP) to achieve remarkably low perplexity. However, powerful NLP models necessitate increasing model size, leading to substantial computational and memory requirements. In this paper, we introduce an efficient inference framework tailored for large-scale generative language models. To reduce the model size, we employ a weight-only quantization strategy while preserving full precision for activations. As a result, we attain sub-4-bit quantization for each weight through non-uniform or uniform quantization techniques. Our proposed kernel, called LUT-GEMM, then accelerates quantized matrix multiplications, offering a flexible balance between compression ratio and accuracy. Unlike earlier matrix multiplication kernels that accommodated weight-only quantization, LUT-GEMM efficiently eliminates the resource-demanding dequantization process for both unifor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26412;&#22320;&#24773;&#24863;&#32858;&#21512;&#33539;&#24335;&#65292;&#21487;&#20197;&#25552;&#39640;&#24773;&#24863;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#24314;&#27169;&#26041;&#38754;&#24773;&#24863;&#30456;&#24178;&#24615;&#65292;&#24182;&#22312;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2110.08604</link><description>&lt;p&gt;
&#36890;&#36807;&#26412;&#22320;&#24773;&#24863;&#32858;&#21512;&#25913;&#36827;&#38544;&#24335;&#24773;&#24863;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Improving Implicit Sentiment Learning via Local Sentiment Aggregation. (arXiv:2110.08604v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.08604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26412;&#22320;&#24773;&#24863;&#32858;&#21512;&#33539;&#24335;&#65292;&#21487;&#20197;&#25552;&#39640;&#24773;&#24863;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#24314;&#27169;&#26041;&#38754;&#24773;&#24863;&#30456;&#24178;&#24615;&#65292;&#24182;&#22312;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#31867;&#65288;ABSC&#65289;&#25581;&#31034;&#20102;&#19981;&#21516;&#26041;&#38754;&#20043;&#38388;&#24773;&#24863;&#26497;&#24615;&#30340;&#28508;&#22312;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#36825;&#19968;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#30456;&#37051;&#26041;&#38754;&#32463;&#24120;&#34920;&#29616;&#20986;&#30456;&#20284;&#24773;&#24863;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#26041;&#38754;&#24773;&#24863;&#30456;&#24178;&#24615;&#8221;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#24403;&#21069;&#30340;&#30740;&#31350;&#39046;&#22495;&#36824;&#27809;&#26377;&#20805;&#20998;&#37325;&#35270;&#24314;&#27169;&#26041;&#38754;&#24773;&#24863;&#30456;&#24178;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26412;&#22320;&#24773;&#24863;&#32858;&#21512;&#33539;&#24335;&#65288;LSA&#65289;&#65292;&#21487;&#20197;&#20419;&#36827;&#31934;&#32454;&#30340;&#24773;&#24863;&#30456;&#24178;&#24615;&#24314;&#27169;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#24471;&#21487;&#20197;&#25552;&#21462;&#32570;&#20047;&#26174;&#24335;&#24773;&#24863;&#25551;&#36848;&#30340;&#26041;&#38754;&#30340;&#38544;&#21547;&#24773;&#24863;&#12290;&#21033;&#29992;&#26799;&#24230;&#19979;&#38477;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#24046;&#20998;&#21152;&#26435;&#24773;&#24863;&#32858;&#21512;&#31383;&#21475;&#65292;&#26469;&#25351;&#23548;&#26041;&#38754;&#24773;&#24863;&#30456;&#24178;&#24615;&#30340;&#24314;&#27169;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LSA&#22312;&#23398;&#20064;&#24773;&#24863;&#30456;&#24178;&#24615;&#26041;&#38754;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#65292;&#22312;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#24773;&#24863;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aspect-based sentiment classification (ABSC) has revealed the potential dependency of sentiment polarities among different aspects. Our study further explores this phenomenon, positing that adjacent aspects often exhibit similar sentiments, a concept we term "aspect sentiment coherency." We argue that the current research landscape has not fully appreciated the significance of modeling aspect sentiment coherency. To address this gap, we introduce a local sentiment aggregation paradigm (LSA) that facilitates fine-grained sentiment coherency modeling. This approach enables the extraction of implicit sentiments for aspects lacking explicit sentiment descriptions. Leveraging gradient descent, we design a differential-weighted sentiment aggregation window that guides the modeling of aspect sentiment coherency. Experimental results affirm the efficacy of LSA in learning sentiment coherency, as it achieves state-of-the-art performance across three public datasets, thus significantly enhancing
&lt;/p&gt;</description></item></channel></rss>