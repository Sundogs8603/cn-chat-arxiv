<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>AutoRT&#26159;&#19968;&#20010;&#21033;&#29992;&#29616;&#26377;&#30340;&#22522;&#30784;&#27169;&#22411;&#26469;&#25193;&#23637;&#26426;&#22120;&#20154;&#22312;&#26410;&#30693;&#22330;&#26223;&#20013;&#30340;&#37096;&#32626;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#22810;&#26679;&#21270;&#21644;&#26032;&#39062;&#30340;&#25351;&#20196;&#65292;&#24182;&#26377;&#25928;&#22320;&#25512;&#29702;&#33258;&#20027;&#26435;&#21644;&#23433;&#20840;&#24615;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2401.12963</link><description>&lt;p&gt;
AutoRT&#65306;&#22823;&#35268;&#27169;&#32534;&#25490;&#26426;&#22120;&#20154;&#20195;&#29702;&#30340;&#20855;&#36523;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents. (arXiv:2401.12963v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12963
&lt;/p&gt;
&lt;p&gt;
AutoRT&#26159;&#19968;&#20010;&#21033;&#29992;&#29616;&#26377;&#30340;&#22522;&#30784;&#27169;&#22411;&#26469;&#25193;&#23637;&#26426;&#22120;&#20154;&#22312;&#26410;&#30693;&#22330;&#26223;&#20013;&#30340;&#37096;&#32626;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#22810;&#26679;&#21270;&#21644;&#26032;&#39062;&#30340;&#25351;&#20196;&#65292;&#24182;&#26377;&#25928;&#22320;&#25512;&#29702;&#33258;&#20027;&#26435;&#21644;&#23433;&#20840;&#24615;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25317;&#26377;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#34892;&#21160;&#31561;&#21151;&#33021;&#30340;&#20855;&#36523;&#22522;&#30784;&#27169;&#22411;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#21033;&#29992;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#25968;&#25454;&#26469;&#25512;&#29702;&#26377;&#29992;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#20855;&#36523;&#22522;&#30784;&#27169;&#22411;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#32570;&#20047;&#22522;&#20110;&#29289;&#29702;&#19990;&#30028;&#30340;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AutoRT&#65292;&#19968;&#20010;&#21033;&#29992;&#29616;&#26377;&#30340;&#22522;&#30784;&#27169;&#22411;&#26469;&#25193;&#23637;&#23436;&#20840;&#26410;&#30693;&#22330;&#26223;&#20013;&#25805;&#20316;&#26426;&#22120;&#20154;&#30340;&#37096;&#32626;&#30340;&#31995;&#32479;&#65292;&#21482;&#38656;&#35201;&#26368;&#23569;&#30340;&#20154;&#24037;&#30417;&#30563;&#12290;AutoRT&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(VLMs)&#23454;&#29616;&#22330;&#26223;&#29702;&#35299;&#21644;&#22522;&#30784;&#32465;&#23450;&#65292;&#24182;&#36827;&#19968;&#27493;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25552;&#20986;&#22810;&#26679;&#21270;&#21644;&#26032;&#39062;&#30340;&#25351;&#20196;&#65292;&#20379;&#19968;&#32452;&#26426;&#22120;&#20154;&#25191;&#34892;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#30693;&#35782;&#26469;&#25351;&#23548;&#25968;&#25454;&#25910;&#38598;&#65292;AutoRT&#33021;&#22815;&#26377;&#25928;&#22320;&#25512;&#29702;&#33258;&#20027;&#26435;&#21644;&#23433;&#20840;&#24615;&#30340;&#26435;&#34913;&#65292;&#21516;&#26102;&#26174;&#33879;&#25193;&#22823;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#25968;&#25454;&#25910;&#38598;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;AutoRT&#21521;20&#22810;&#20010;&#26426;&#22120;&#20154;&#25552;&#35758;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models that incorporate language, vision, and more recently actions have revolutionized the ability to harness internet scale data to reason about useful tasks. However, one of the key challenges of training embodied foundation models is the lack of data grounded in the physical world. In this paper, we propose AutoRT, a system that leverages existing foundation models to scale up the deployment of operational robots in completely unseen scenarios with minimal human supervision. AutoRT leverages vision-language models (VLMs) for scene understanding and grounding, and further uses large language models (LLMs) for proposing diverse and novel instructions to be performed by a fleet of robots. Guiding data collection by tapping into the knowledge of foundation models enables AutoRT to effectively reason about autonomy tradeoffs and safety while significantly scaling up data collection for robot learning. We demonstrate AutoRT proposing instructions to over 20 robots across multi
&lt;/p&gt;</description></item><item><title>&#20803;&#25552;&#31034;&#26159;&#19968;&#31181;&#25903;&#25345;&#25216;&#26415;&#65292;&#23558;&#21333;&#20010;&#35821;&#35328;&#27169;&#22411;&#36716;&#21270;&#20026;&#22810;&#38754;&#25163;&#25351;&#25381;&#32773;&#65292;&#36890;&#36807;&#39640;&#32423;&#25351;&#20196;&#23558;&#22797;&#26434;&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#20219;&#21153;&#65292;&#24182;&#30001;&#19981;&#21516;&#30340;&#19987;&#23478;&#27169;&#22411;&#22788;&#29702;&#65292;&#26368;&#32456;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.12954</link><description>&lt;p&gt;
&#20803;&#25552;&#31034;&#65306;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#20219;&#21153;&#26080;&#20851;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Meta-Prompting: Enhancing Language Models with Task-Agnostic Scaffolding. (arXiv:2401.12954v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12954
&lt;/p&gt;
&lt;p&gt;
&#20803;&#25552;&#31034;&#26159;&#19968;&#31181;&#25903;&#25345;&#25216;&#26415;&#65292;&#23558;&#21333;&#20010;&#35821;&#35328;&#27169;&#22411;&#36716;&#21270;&#20026;&#22810;&#38754;&#25163;&#25351;&#25381;&#32773;&#65292;&#36890;&#36807;&#39640;&#32423;&#25351;&#20196;&#23558;&#22797;&#26434;&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#20219;&#21153;&#65292;&#24182;&#30001;&#19981;&#21516;&#30340;&#19987;&#23478;&#27169;&#22411;&#22788;&#29702;&#65292;&#26368;&#32456;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#20803;&#25552;&#31034;&#65292;&#19968;&#31181;&#26377;&#25928;&#30340;&#25903;&#25345;&#25216;&#26415;&#65292;&#26088;&#22312;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#30340;&#21151;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;&#21333;&#20010;LM&#36716;&#21270;&#20026;&#22810;&#38754;&#25163;&#25351;&#25381;&#32773;&#65292;&#21892;&#20110;&#31649;&#29702;&#21644;&#25972;&#21512;&#22810;&#20010;&#29420;&#31435;&#30340;LM&#26597;&#35810;&#12290;&#36890;&#36807;&#20351;&#29992;&#39640;&#32423;&#25351;&#20196;&#65292;&#20803;&#25552;&#31034;&#25351;&#23548;LM&#23558;&#22797;&#26434;&#20219;&#21153;&#20998;&#35299;&#20026;&#26356;&#23567;&#12289;&#26356;&#21487;&#31649;&#29702;&#30340;&#23376;&#20219;&#21153;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#23376;&#20219;&#21153;&#30001;&#30456;&#21516;LM&#30340;&#19981;&#21516;&#8220;&#19987;&#23478;&#8221;&#23454;&#20363;&#22788;&#29702;&#65292;&#27599;&#20010;&#23454;&#20363;&#37117;&#22312;&#29305;&#23450;&#30340;&#12289;&#23450;&#21046;&#21270;&#30340;&#25351;&#23548;&#19979;&#25805;&#20316;&#12290;&#35813;&#36807;&#31243;&#30340;&#26680;&#24515;&#26159;LM&#26412;&#36523;&#65292;&#23427;&#20316;&#20026;&#25351;&#25381;&#32773;&#30830;&#20445;&#36825;&#20123;&#19987;&#23478;&#27169;&#22411;&#30340;&#36755;&#20986;&#26080;&#32541;&#27807;&#36890;&#21644;&#26377;&#25928;&#38598;&#25104;&#12290;&#23427;&#36824;&#21033;&#29992;&#20854;&#22266;&#26377;&#30340;&#25209;&#21028;&#24615;&#24605;&#32500;&#21644;&#24378;&#22823;&#30340;&#39564;&#35777;&#36807;&#31243;&#26469;&#23436;&#21892;&#21644;&#39564;&#35777;&#26368;&#32456;&#32467;&#26524;&#12290;&#36825;&#31181;&#21327;&#20316;&#25552;&#31034;&#26041;&#27861;&#20351;&#21333;&#20010;LM&#33021;&#22815;&#21516;&#26102;&#20805;&#24403;&#20840;&#38754;&#30340;&#32534;&#25490;&#32773;&#21644;&#22810;&#26679;&#21270;&#30340;&#19987;&#23478;&#23567;&#32452;&#65292;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce meta-prompting, an effective scaffolding technique designed to enhance the functionality of language models (LMs). This approach transforms a single LM into a multi-faceted conductor, adept at managing and integrating multiple independent LM queries. By employing high-level instructions, meta-prompting guides the LM to break down complex tasks into smaller, more manageable subtasks. These subtasks are then handled by distinct "expert" instances of the same LM, each operating under specific, tailored instructions. Central to this process is the LM itself, in its role as the conductor, which ensures seamless communication and effective integration of the outputs from these expert models. It additionally employs its inherent critical thinking and robust verification processes to refine and authenticate the end result. This collaborative prompting approach empowers a single LM to simultaneously act as a comprehensive orchestrator and a panel of diverse experts, significantly e
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;Transformer&#27169;&#22411;&#23398;&#20064;&#32467;&#26500;&#36882;&#24402;&#30340;&#33021;&#21147;&#65292;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#23558;&#32534;&#31243;&#35821;&#35328;&#39046;&#22495;&#20013;&#30340;&#25277;&#35937;&#27010;&#24565;&#19982;&#20855;&#20307;&#30340;&#24207;&#21015;&#24314;&#27169;&#38382;&#39064;&#21644;&#23398;&#20064;&#27169;&#22411;&#34892;&#20026;&#30456;&#36830;&#25509;&#12290;</title><link>http://arxiv.org/abs/2401.12947</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#27169;&#22411;&#22312;&#23398;&#20064;&#27169;&#25311;&#32467;&#26500;&#36882;&#24402;&#26041;&#38754;&#23578;&#26410;&#23436;&#32654;
&lt;/p&gt;
&lt;p&gt;
Transformer-Based Models Are Not Yet Perfect At Learning to Emulate Structural Recursion. (arXiv:2401.12947v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;Transformer&#27169;&#22411;&#23398;&#20064;&#32467;&#26500;&#36882;&#24402;&#30340;&#33021;&#21147;&#65292;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#23558;&#32534;&#31243;&#35821;&#35328;&#39046;&#22495;&#20013;&#30340;&#25277;&#35937;&#27010;&#24565;&#19982;&#20855;&#20307;&#30340;&#24207;&#21015;&#24314;&#27169;&#38382;&#39064;&#21644;&#23398;&#20064;&#27169;&#22411;&#34892;&#20026;&#30456;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;Transformer&#27169;&#22411;&#23398;&#20064;&#32467;&#26500;&#36882;&#24402;&#30340;&#33021;&#21147;&#12290;&#36882;&#24402;&#26159;&#33258;&#28982;&#35821;&#35328;&#21644;&#24418;&#24335;&#35821;&#35328;&#20013;&#30340;&#19968;&#31181;&#36890;&#29992;&#27010;&#24565;&#12290;&#32467;&#26500;&#36882;&#24402;&#22312;&#32534;&#31243;&#35821;&#35328;&#21644;&#24418;&#24335;&#25968;&#23398;&#20219;&#21153;&#20013;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#32780;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#65292;&#31526;&#21495;&#24037;&#20855;&#30446;&#21069;&#22312;&#31070;&#32463;&#27169;&#22411;&#20043;&#19978;&#26377;&#20248;&#21183;&#65292;&#27604;&#22914;&#25512;&#26029;&#25968;&#25454;&#31867;&#22411;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#21644;&#27169;&#25311;&#31243;&#24207;&#34892;&#20026;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#23558;&#32534;&#31243;&#35821;&#35328;&#39046;&#22495;&#20013;&#30340;&#25277;&#35937;&#27010;&#24565;&#19982;&#20855;&#20307;&#30340;&#24207;&#21015;&#24314;&#27169;&#38382;&#39064;&#21644;&#23398;&#20064;&#27169;&#22411;&#34892;&#20026;&#30456;&#36830;&#25509;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#19968;&#20010;&#25429;&#25417;&#32467;&#26500;&#36882;&#24402;&#19968;&#33324;"&#35821;&#27861;"&#30340;&#34920;&#31034;&#65292;&#20197;&#21450;&#20004;&#31181;&#19981;&#21516;&#30340;&#26694;&#26550;&#26469;&#29702;&#35299;&#23427;&#20204;&#30340;"&#35821;&#20041;"&#8212;&#8212;&#19968;&#31181;&#26356;&#31526;&#21512;&#32534;&#31243;&#35821;&#35328;&#35270;&#35282;&#30340;&#33258;&#28982;&#26041;&#24335;&#65292;&#20197;&#21450;&#19968;&#31181;&#26377;&#21161;&#20110;&#23558;&#35813;&#35270;&#35282;&#19982;&#24213;&#23618;Transformer&#27169;&#22411;&#30340;&#26426;&#26800;&#29702;&#35299;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the ability of transformer-based models to learn structural recursion from examples. Recursion is a universal concept in both natural and formal languages. Structural recursion is central to the programming language and formal mathematics tasks where symbolic tools currently excel beyond neural models, such as inferring semantic relations between datatypes and emulating program behavior. We introduce a general framework that nicely connects the abstract concepts of structural recursion in the programming language domain to concrete sequence modeling problems and learned models' behavior. The framework includes a representation that captures the general \textit{syntax} of structural recursion, coupled with two different frameworks for understanding their \textit{semantics} -- one that is more natural from a programming languages perspective and one that helps bridge that perspective with a mechanistic understanding of the underlying transformer architecture.  Wit
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20197;&#24448;&#26410;&#35265;&#21517;&#23383;&#30340;&#22810;&#20803;&#25991;&#21270;&#21517;&#23383;&#35782;&#21035;&#65292;&#36890;&#36807;&#25913;&#36827;&#35757;&#32451;&#25968;&#25454;&#21644;&#36755;&#20837;&#32467;&#26500;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#26469;&#33258;&#19981;&#21516;&#25991;&#21270;&#32972;&#26223;&#30340;&#21517;&#23383;&#30340;&#35782;&#21035;&#33021;&#21147;&#65292;&#36991;&#20813;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#25991;&#21270;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2401.12941</link><description>&lt;p&gt;
&#20197;&#24448;&#26410;&#35265;&#21517;&#23383;&#30340;&#22810;&#20803;&#25991;&#21270;&#21517;&#23383;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Multicultural Name Recognition For Previously Unseen Names. (arXiv:2401.12941v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12941
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20197;&#24448;&#26410;&#35265;&#21517;&#23383;&#30340;&#22810;&#20803;&#25991;&#21270;&#21517;&#23383;&#35782;&#21035;&#65292;&#36890;&#36807;&#25913;&#36827;&#35757;&#32451;&#25968;&#25454;&#21644;&#36755;&#20837;&#32467;&#26500;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#26469;&#33258;&#19981;&#21516;&#25991;&#21270;&#32972;&#26223;&#30340;&#21517;&#23383;&#30340;&#35782;&#21035;&#33021;&#21147;&#65292;&#36991;&#20813;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#25991;&#21270;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#27169;&#22411;&#22312;&#25552;&#21462;&#23646;&#20110;&#20301;&#32622;&#12289;&#32452;&#32455;&#12289;&#26102;&#38388;&#21644;&#20154;&#21592;&#31561;&#26631;&#31614;&#30340;&#25991;&#26412;&#24120;&#35265;&#30701;&#35821;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#35265;&#36807;&#29305;&#23450;&#23454;&#20307;&#20197;&#20415;&#23545;&#23454;&#20307;&#36827;&#34892;&#26631;&#35760;&#30340;NER&#31995;&#32479;&#22312;&#32597;&#35265;&#25110;&#26410;&#30693;&#23454;&#20307;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#35797;&#22270;&#25552;&#39640;&#23545;&#20010;&#20154;&#22995;&#21517;&#30340;&#35782;&#21035;&#33021;&#21147;&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#26679;&#21270;&#30340;&#31867;&#21035;&#65292;&#21487;&#33021;&#38543;&#26102;&#26377;&#20154;&#20986;&#29983;&#25110;&#25913;&#21464;&#22995;&#21517;&#12290;&#20026;&#20102;&#36991;&#20813;&#19979;&#28216;&#20219;&#21153;&#20986;&#29616;&#22522;&#20110;&#25991;&#21270;&#32972;&#26223;&#30340;&#20559;&#35265;&#65292;&#27169;&#22411;&#24212;&#35813;&#22312;&#21508;&#31181;&#32972;&#26223;&#30340;&#21517;&#23383;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#26412;&#25991;&#23581;&#35797;&#36890;&#36807;&#23454;&#39564;&#33521;&#35821;Bi-LSTM&#22995;&#21517;&#35782;&#21035;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#36755;&#20837;&#32467;&#26500;&#65292;&#27604;&#36739;&#27169;&#22411;&#22312;&#26469;&#33258;103&#20010;&#22269;&#23478;&#30340;&#21517;&#23383;&#19978;&#30340;&#34920;&#29616;&#65292;&#29305;&#21035;&#26159;&#22312;&#36328;&#25991;&#21270;&#21517;&#23383;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
State of the art Named Entity Recognition (NER) models have achieved an impressive ability to extract common phrases from text that belong to labels such as location, organization, time, and person. However, typical NER systems that rely on having seen a specific entity in their training data in order to label an entity perform poorly on rare or unseen entities ta in order to label an entity perform poorly on rare or unseen entities (Derczynski et al., 2017). This paper attempts to improve recognition of person names, a diverse category that can grow any time someone is born or changes their name. In order for downstream tasks to not exhibit bias based on cultural background, a model should perform well on names from a variety of backgrounds. In this paper I experiment with the training data and input structure of an English Bi-LSTM name recognition model. I look at names from 103 countries to compare how well the model performs on names from different cultures, specifically in the con
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#26032;&#39062;&#30340;&#32418;&#38431;&#25968;&#25454;&#38598;RTVLM&#65292;&#35780;&#20272;&#20102;&#24403;&#21069;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;VLMs&#22312;&#24544;&#23454;&#24230;&#12289;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#20844;&#24179;&#24615;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#24182;&#19982;GPT-4V&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;10&#20010;&#30693;&#21517;&#30340;VLMs&#22312;&#32418;&#38431;&#34892;&#21160;&#20013;&#23384;&#22312;&#19981;&#21516;&#31243;&#24230;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2401.12915</link><description>&lt;p&gt;
&#32418;&#38431;&#38024;&#23545;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Red Teaming Visual Language Models. (arXiv:2401.12915v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#26032;&#39062;&#30340;&#32418;&#38431;&#25968;&#25454;&#38598;RTVLM&#65292;&#35780;&#20272;&#20102;&#24403;&#21069;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;VLMs&#22312;&#24544;&#23454;&#24230;&#12289;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#20844;&#24179;&#24615;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#24182;&#19982;GPT-4V&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;10&#20010;&#30693;&#21517;&#30340;VLMs&#22312;&#32418;&#38431;&#34892;&#21160;&#20013;&#23384;&#22312;&#19981;&#21516;&#31243;&#24230;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;(VLMs)&#25193;&#23637;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25509;&#21463;&#22810;&#27169;&#24577;&#36755;&#20837;&#30340;&#33021;&#21147;&#12290;&#30001;&#20110;&#24050;&#32463;&#39564;&#35777;LLMs&#21487;&#20197;&#36890;&#36807;&#29305;&#23450;&#27979;&#35797;&#29992;&#20363;&#20135;&#29983;&#26377;&#23475;&#25110;&#19981;&#20934;&#30830;&#30340;&#20869;&#23481;(&#31216;&#20026;&#32418;&#38431;&#34892;&#21160;)&#65292;VLMs&#22312;&#31867;&#20284;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#22914;&#20309;&#65292;&#29305;&#21035;&#26159;&#22312;&#25991;&#26412;&#21644;&#35270;&#35273;&#36755;&#20837;&#30340;&#32452;&#21512;&#20013;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#20026;&#20102;&#25506;&#32034;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32418;&#38431;&#25968;&#25454;&#38598;RTVLM&#65292;&#21253;&#21547;4&#20010;&#20027;&#35201;&#26041;&#38754;(&#24544;&#23454;&#24230;&#12289;&#38544;&#31169;&#12289;&#23433;&#20840;&#12289;&#20844;&#24179;)&#19979;&#30340;10&#20010;&#23376;&#20219;&#21153;(&#22914;&#22270;&#20687;&#35823;&#23548;&#12289;&#22810;&#27169;&#24577;&#36234;&#29425;&#12289;&#33080;&#37096;&#20844;&#24179;&#31561;)&#12290;&#25105;&#20204;&#30340;RTVLM&#26159;&#31532;&#19968;&#20010;&#20174;&#36825;&#22235;&#20010;&#19981;&#21516;&#26041;&#38754;&#35780;&#20272;&#24403;&#21069;VLMs&#30340;&#32418;&#38431;&#25968;&#25454;&#38598;&#12290;&#35814;&#32454;&#20998;&#26512;&#34920;&#26126;&#65292;10&#20010;&#30693;&#21517;&#30340;&#24320;&#25918;&#28304;&#20195;&#30721;VLMs&#22312;&#32418;&#38431;&#34892;&#21160;&#20013;&#36935;&#21040;&#19981;&#21516;&#31243;&#24230;&#30340;&#22256;&#38590;&#65292;&#24182;&#19988;&#19982;GPT-4V&#30456;&#27604;&#65292;&#24615;&#33021;&#24046;&#36317;&#39640;&#36798;31%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;RT&#31616;&#21333;&#22320;&#23558;&#32418;&#38431;&#34892;&#21160;&#23545;&#40784;&#21040;LLaVA-v1.5&#19978;&#65292;&#20351;&#29992;&#30417;&#30563;&#24494;&#35843;(SFT)&#12290;
&lt;/p&gt;
&lt;p&gt;
VLMs (Vision-Language Models) extend the capabilities of LLMs (Large Language Models) to accept multimodal inputs. Since it has been verified that LLMs can be induced to generate harmful or inaccurate content through specific test cases (termed as Red Teaming), how VLMs perform in similar scenarios, especially with their combination of textual and visual inputs, remains a question. To explore this problem, we present a novel red teaming dataset RTVLM, which encompasses 10 subtasks (e.g., image misleading, multi-modal jail-breaking, face fairness, etc) under 4 primary aspects (faithfulness, privacy, safety, fairness). Our RTVLM is the first red-teaming dataset to benchmark current VLMs in terms of these 4 different aspects. Detailed analysis shows that 10 prominent open-sourced VLMs struggle with the red teaming in different degrees and have up to 31% performance gap with GPT-4V. Additionally, we simply apply red teaming alignment to LLaVA-v1.5 with Supervised Fine-tuning (SFT) using RT
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#35299;&#37322;&#24615;&#30340;&#26032;&#20852;&#39046;&#22495;&#65292;&#24378;&#35843;&#20102;&#22312;LLMs&#20013;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;&#30340;&#24517;&#35201;&#24615;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#24191;&#22823;&#20844;&#20247;&#23545;&#20854;&#20449;&#20219;&#21644;&#25216;&#26415;&#30028;&#23545;&#36825;&#20123;&#27169;&#22411;&#26356;&#28145;&#29702;&#35299;&#30340;&#38656;&#27714;&#12290;&#35813;&#32508;&#36848;&#23545;&#29616;&#26377;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#22312;&#25552;&#39640;&#27169;&#22411;&#36879;&#26126;&#24230;&#21644;&#21487;&#38752;&#24615;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#26088;&#22312;&#24357;&#21512;&#29702;&#35770;&#29702;&#35299;&#21644;&#23454;&#38469;&#24212;&#29992;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2401.12874</link><description>&lt;p&gt;
&#20174;&#29702;&#35299;&#21040;&#24212;&#29992;&#65306;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
From Understanding to Utilization: A Survey on Explainability for Large Language Models. (arXiv:2401.12874v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#35299;&#37322;&#24615;&#30340;&#26032;&#20852;&#39046;&#22495;&#65292;&#24378;&#35843;&#20102;&#22312;LLMs&#20013;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;&#30340;&#24517;&#35201;&#24615;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#24191;&#22823;&#20844;&#20247;&#23545;&#20854;&#20449;&#20219;&#21644;&#25216;&#26415;&#30028;&#23545;&#36825;&#20123;&#27169;&#22411;&#26356;&#28145;&#29702;&#35299;&#30340;&#38656;&#27714;&#12290;&#35813;&#32508;&#36848;&#23545;&#29616;&#26377;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#22312;&#25552;&#39640;&#27169;&#22411;&#36879;&#26126;&#24230;&#21644;&#21487;&#38752;&#24615;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#26088;&#22312;&#24357;&#21512;&#29702;&#35770;&#29702;&#35299;&#21644;&#23454;&#38469;&#24212;&#29992;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35770;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#35299;&#37322;&#24615;&#30340;&#26032;&#20852;&#39046;&#22495;&#65292;&#36825;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#19968;&#20010;&#20851;&#38190;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26041;&#38754;&#12290;LLMs&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20294;&#20854;&#8220;&#40657;&#30418;&#8221;&#24615;&#36136;&#24341;&#21457;&#20102;&#23545;&#36879;&#26126;&#24615;&#21644;&#20262;&#29702;&#20351;&#29992;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#24378;&#35843;&#20102;&#22312;LLMs&#20013;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;&#30340;&#24517;&#35201;&#24615;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#24191;&#22823;&#20844;&#20247;&#23545;&#20854;&#20449;&#20219;&#21644;&#25216;&#26415;&#30028;&#23545;&#36825;&#20123;&#27169;&#22411;&#26356;&#28145;&#29702;&#35299;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#38598;&#20013;&#22312;&#39044;&#35757;&#32451;&#30340;&#22522;&#20110;Transformer&#30340;LLMs&#65292;&#22914;LLaMA&#65292;&#20854;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#20351;&#20854;&#38754;&#20020;&#29420;&#29305;&#30340;&#21487;&#35299;&#37322;&#24615;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#32508;&#36848;&#23545;&#29616;&#26377;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#22312;&#25552;&#39640;&#27169;&#22411;&#36879;&#26126;&#24230;&#21644;&#21487;&#38752;&#24615;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#20195;&#34920;&#24615;&#30340;&#35780;&#20215;&#26041;&#27861;&#65292;&#24378;&#35843;&#20102;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;&#26412;&#32508;&#36848;&#30340;&#30446;&#26631;&#26159;&#24357;&#21512;&#29702;&#35770;&#29702;&#35299;&#21644;&#23454;&#38469;&#24212;&#29992;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25552;&#20379;&#20174;&#25216;&#26415;&#35282;&#24230;&#24635;&#32467;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#30340;&#20840;&#38754;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
This survey paper delves into the burgeoning field of explainability for Large Language Models (LLMs), a critical yet challenging aspect of natural language processing. With LLMs playing a pivotal role in various applications, their "black-box" nature raises concerns about transparency and ethical use. This paper emphasizes the necessity for enhanced explainability in LLMs, addressing both the general public's trust and the technical community's need for a deeper understanding of these models. We concentrate on pre-trained Transformer-based LLMs, such as LLaMA, which present unique interpretability challenges due to their scale and complexity. Our review categorizes existing explainability methods and discusses their application in improving model transparency and reliability. We also discuss representative evaluation methods, highlighting their strengths and limitations. The goal of this survey is to bridge the gap between theoretical understanding and practical application, offering 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#36136;&#37327;&#20272;&#35745;&#20316;&#20026;&#22870;&#21169;&#27169;&#22411;&#26469;&#39044;&#27979;&#20154;&#31867;&#20559;&#22909;&#20197;&#25913;&#21892;&#26426;&#22120;&#32763;&#35793;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#36136;&#37327;&#20272;&#35745;&#30340;&#21453;&#39304;&#35757;&#32451;&#23384;&#22312;&#36807;&#24230;&#20248;&#21270;&#38382;&#39064;&#65292;&#37319;&#29992;&#21551;&#21457;&#24335;&#35268;&#21017;&#26469;&#26816;&#27979;&#38169;&#35823;&#32763;&#35793;&#24182;&#23545;&#36136;&#37327;&#20272;&#35745;&#27169;&#22411;&#36827;&#34892;&#24809;&#32602;&#20197;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.12873</link><description>&lt;p&gt;
&#36890;&#36807;&#20154;&#30340;&#21453;&#39304;&#25913;&#21892;&#26426;&#22120;&#32763;&#35793;: &#23558;&#36136;&#37327;&#20272;&#35745;&#20316;&#20026;&#22870;&#21169;&#27169;&#22411;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Improving Machine Translation with Human Feedback: An Exploration of Quality Estimation as a Reward Model. (arXiv:2401.12873v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#36136;&#37327;&#20272;&#35745;&#20316;&#20026;&#22870;&#21169;&#27169;&#22411;&#26469;&#39044;&#27979;&#20154;&#31867;&#20559;&#22909;&#20197;&#25913;&#21892;&#26426;&#22120;&#32763;&#35793;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#36136;&#37327;&#20272;&#35745;&#30340;&#21453;&#39304;&#35757;&#32451;&#23384;&#22312;&#36807;&#24230;&#20248;&#21270;&#38382;&#39064;&#65292;&#37319;&#29992;&#21551;&#21457;&#24335;&#35268;&#21017;&#26469;&#26816;&#27979;&#38169;&#35823;&#32763;&#35793;&#24182;&#23545;&#36136;&#37327;&#20272;&#35745;&#27169;&#22411;&#36827;&#34892;&#24809;&#32602;&#20197;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#20805;&#20998;&#24314;&#27169;&#20154;&#31867;&#20559;&#22909;&#23548;&#33268;&#22870;&#21169;&#27169;&#22411;&#22312;&#21033;&#29992;&#20154;&#30340;&#21453;&#39304;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#26041;&#38754;&#25104;&#20026;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#36136;&#37327;&#20272;&#35745;(QE)&#22312;&#36807;&#21435;&#20004;&#24180;&#20013;&#26080;&#38656;&#21442;&#32771;&#25991;&#29486;&#23601;&#33021;&#20934;&#30830;&#39044;&#27979;&#32473;&#23450;&#32763;&#35793;&#30340;&#36136;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#23558;QE&#27169;&#22411;&#20316;&#20026;&#22870;&#21169;&#27169;&#22411;(&#22522;&#20110;QE&#30340;&#22870;&#21169;&#27169;&#22411;)&#26469;&#39044;&#27979;&#20154;&#30340;&#20559;&#22909;&#20197;&#36827;&#34892;&#21453;&#39304;&#35757;&#32451;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#21457;&#29616;&#20102;&#22312;&#22522;&#20110;QE&#30340;&#21453;&#39304;&#35757;&#32451;&#20013;&#30340;&#36807;&#24230;&#20248;&#21270;&#38382;&#39064;&#65292;&#34920;&#29616;&#20026;&#22870;&#21169;&#30340;&#22686;&#21152;&#32780;&#32763;&#35793;&#36136;&#37327;&#19979;&#38477;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#35748;&#20026;QE&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#32763;&#35793;&#30340;&#39640;&#22870;&#21169;&#65292;&#20174;&#32780;&#23548;&#33268;&#36807;&#24230;&#20248;&#21270;&#21644;&#38169;&#35823;&#20256;&#25773;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21551;&#21457;&#24335;&#35268;&#21017;&#26816;&#27979;&#38169;&#35823;&#32763;&#35793;&#65292;&#24182;&#20026;QE&#27169;&#22411;&#28155;&#21152;&#20102;&#19968;&#20010;&#24809;&#32602;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
Insufficient modeling of human preferences within the reward model is a major obstacle for leveraging human feedback to improve translation quality. Fortunately, quality estimation (QE), which predicts the quality of a given translation without reference, has achieved impressive alignment with human evaluations in the last two years. In this work, we investigate the potential of employing the QE model as the reward model (the QE-based reward model) to predict human preferences for feedback training. We first identify the overoptimization problem during QE-based feedback training, manifested as an increase in reward while translation quality declines. We examine the problem and argue that the vulnerability of the QE model might lead to high rewards for incorrect translations, resulting in overoptimization and error propagation. To address the problem, we adopt a simple yet effective method that uses heuristic rules to detect the incorrect translations and assigns a penalty term to the Q
&lt;/p&gt;</description></item><item><title>KAM-CoT&#26159;&#19968;&#20010;&#30693;&#35782;&#22686;&#24378;&#30340;&#22810;&#27169;&#24335;&#24605;&#32500;&#38142;&#25512;&#29702;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#24341;&#20837;&#22806;&#37096;&#30693;&#35782;&#22270;&#35889;&#26469;&#25552;&#39640;&#27169;&#22411;&#23545;&#22810;&#27169;&#24335;&#20219;&#21153;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#31572;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.12863</link><description>&lt;p&gt;
KAM-CoT: &#30693;&#35782;&#22686;&#24378;&#30340;&#22810;&#27169;&#24335;&#24605;&#32500;&#38142;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
KAM-CoT: Knowledge Augmented Multimodal Chain-of-Thoughts Reasoning. (arXiv:2401.12863v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12863
&lt;/p&gt;
&lt;p&gt;
KAM-CoT&#26159;&#19968;&#20010;&#30693;&#35782;&#22686;&#24378;&#30340;&#22810;&#27169;&#24335;&#24605;&#32500;&#38142;&#25512;&#29702;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#24341;&#20837;&#22806;&#37096;&#30693;&#35782;&#22270;&#35889;&#26469;&#25552;&#39640;&#27169;&#22411;&#23545;&#22810;&#27169;&#24335;&#20219;&#21153;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#21033;&#29992;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#23454;&#29616;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#65292;&#23558;LLMs&#25193;&#23637;&#20026;&#22810;&#27169;&#24335;&#33021;&#21147;&#26159;&#19968;&#20010;&#26377;&#21560;&#24341;&#21147;&#30340;&#26041;&#21521;&#65292;&#20294;&#20250;&#22686;&#21152;&#35745;&#31639;&#25104;&#26412;&#24182;&#38656;&#35201;&#22823;&#37327;&#30828;&#20214;&#36164;&#28304;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;KAM-CoT&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#34701;&#21512;&#20102;CoT&#25512;&#29702;&#12289;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#21644;&#22810;&#31181;&#27169;&#24335;&#65292;&#20197;&#20840;&#38754;&#29702;&#35299;&#22810;&#27169;&#24335;&#20219;&#21153;&#12290;KAM-CoT&#37319;&#29992;&#20004;&#38454;&#27573;&#35757;&#32451;&#36807;&#31243;&#65292;&#36890;&#36807;KG&#22522;&#30784;&#29983;&#25104;&#26377;&#25928;&#30340;&#29702;&#30001;&#21644;&#31572;&#26696;&#12290;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#24341;&#20837;&#26469;&#33258;&#30693;&#35782;&#22270;&#35889;&#30340;&#22806;&#37096;&#30693;&#35782;&#65292;&#27169;&#22411;&#33719;&#24471;&#20102;&#26356;&#28145;&#20837;&#30340;&#35821;&#22659;&#29702;&#35299;&#65292;&#20943;&#23569;&#20102;&#34394;&#26500;&#21644;&#25913;&#21892;&#20102;&#31572;&#26696;&#30340;&#36136;&#37327;&#12290;&#36825;&#31181;&#30693;&#35782;&#22686;&#24378;&#30340;CoT&#25512;&#29702;&#20351;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#38656;&#35201;&#22806;&#37096;&#19978;&#19979;&#25991;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#26356;&#26377;&#26681;&#25454;&#30340;&#31572;&#26696;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;KAM-CoT&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated impressive performance in natural language processing tasks by leveraging chain of thought (CoT) that enables step-by-step thinking. Extending LLMs with multimodal capabilities is the recent interest, but incurs computational cost and requires substantial hardware resources. To address these challenges, we propose KAM-CoT a framework that integrates CoT reasoning, Knowledge Graphs (KGs), and multiple modalities for a comprehensive understanding of multimodal tasks. KAM-CoT adopts a two-stage training process with KG grounding to generate effective rationales and answers. By incorporating external knowledge from KGs during reasoning, the model gains a deeper contextual understanding reducing hallucinations and enhancing the quality of answers. This knowledge-augmented CoT reasoning empowers the model to handle questions requiring external context, providing more informed answers. Experimental findings show KAM-CoT outperforms the state-of-t
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#35299;&#20915;&#23454;&#20307;&#23545;&#40784;&#35299;&#30721;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#33021;&#37327;&#26469;&#20248;&#21270;&#35299;&#30721;&#36807;&#31243;&#65292;&#20197;&#23454;&#29616;&#22270;&#21516;&#36136;&#24615;&#65292;&#24182;&#19988;&#20165;&#20381;&#36182;&#20110;&#23454;&#20307;&#23884;&#20837;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#36890;&#29992;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.12798</link><description>&lt;p&gt;
&#33021;&#37327;&#30340;&#26799;&#24230;&#27969;&#65306;&#23454;&#20307;&#23545;&#40784;&#35299;&#30721;&#30340;&#36890;&#29992;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Gradient Flow of Energy: A General and Efficient Approach for Entity Alignment Decoding. (arXiv:2401.12798v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12798
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#35299;&#20915;&#23454;&#20307;&#23545;&#40784;&#35299;&#30721;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#33021;&#37327;&#26469;&#20248;&#21270;&#35299;&#30721;&#36807;&#31243;&#65292;&#20197;&#23454;&#29616;&#22270;&#21516;&#36136;&#24615;&#65292;&#24182;&#19988;&#20165;&#20381;&#36182;&#20110;&#23454;&#20307;&#23884;&#20837;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#36890;&#29992;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#23545;&#40784;&#65288;EA&#65289;&#26159;&#22312;&#38598;&#25104;&#22810;&#28304;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#20013;&#30340;&#20851;&#38190;&#36807;&#31243;&#65292;&#26088;&#22312;&#35782;&#21035;&#36825;&#20123;&#22270;&#35889;&#20013;&#30340;&#31561;&#20215;&#23454;&#20307;&#23545;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#23558;EA&#35270;&#20026;&#22270;&#34920;&#31034;&#23398;&#20064;&#20219;&#21153;&#65292;&#19987;&#27880;&#20110;&#22686;&#24378;&#22270;&#32534;&#30721;&#22120;&#12290;&#28982;&#32780;&#65292;&#22312;EA&#20013;&#35299;&#30721;&#36807;&#31243;-&#23545;&#20110;&#26377;&#25928;&#30340;&#25805;&#20316;&#21644;&#23545;&#40784;&#20934;&#30830;&#24615;&#33267;&#20851;&#37325;&#35201;-&#24471;&#21040;&#20102;&#26377;&#38480;&#30340;&#20851;&#27880;&#65292;&#24182;&#19988;&#20173;&#28982;&#38024;&#23545;&#29305;&#23450;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#26550;&#26500;&#36827;&#34892;&#23450;&#21046;&#65292;&#38656;&#35201;&#23454;&#20307;&#21644;&#39069;&#22806;&#30340;&#26174;&#24335;&#20851;&#31995;&#23884;&#20837;&#12290;&#36825;&#31181;&#29305;&#27530;&#24615;&#38480;&#21046;&#20102;&#23427;&#30340;&#36866;&#29992;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#22522;&#20110;GNN&#30340;&#27169;&#22411;&#20013;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#36890;&#29992;&#21644;&#39640;&#25928;&#30340;EA&#35299;&#30721;&#26041;&#27861;&#65292;&#20165;&#20381;&#36182;&#20110;&#23454;&#20307;&#23884;&#20837;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#29380;&#21033;&#20811;&#38647;&#33021;&#37327;&#26469;&#20248;&#21270;&#35299;&#30721;&#36807;&#31243;&#65292;&#22312;&#22270;&#20869;&#24341;&#23548;&#26799;&#24230;&#27969;&#65292;&#20197;&#20419;&#36827;&#22270;&#21516;&#36136;&#24615;&#12290;&#26799;&#24230;&#27969;&#30340;&#31163;&#25955;&#21270;&#20135;&#29983;&#20102;&#19968;&#31181;&#24555;&#36895;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#19977;&#20803;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity alignment (EA), a pivotal process in integrating multi-source Knowledge Graphs (KGs), seeks to identify equivalent entity pairs across these graphs. Most existing approaches regard EA as a graph representation learning task, concentrating on enhancing graph encoders. However, the decoding process in EA - essential for effective operation and alignment accuracy - has received limited attention and remains tailored to specific datasets and model architectures, necessitating both entity and additional explicit relation embeddings. This specificity limits its applicability, particularly in GNN-based models. To address this gap, we introduce a novel, generalized, and efficient decoding approach for EA, relying solely on entity embeddings. Our method optimizes the decoding process by minimizing Dirichlet energy, leading to the gradient flow within the graph, to promote graph homophily. The discretization of the gradient flow produces a fast and scalable approach, termed Triple Feature
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26469;&#23545;LLMs&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#35780;&#20272;&#25351;&#26631;UAcc&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#39640;&#20934;&#30830;&#24230;&#30340;LLMs&#21487;&#33021;&#20855;&#26377;&#36739;&#20302;&#30340;&#30830;&#23450;&#24615;&#65292;&#32780;&#22823;&#35268;&#27169;LLMs&#21487;&#33021;&#27604;&#36739;&#23567;&#35268;&#27169;LLMs&#26356;&#19981;&#30830;&#23450;&#12290;&#25351;&#20196;&#24494;&#35843;&#20542;&#21521;&#20110;&#22686;&#21152;LLMs&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.12794</link><description>&lt;p&gt;
&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#23545;LLMs&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking LLMs via Uncertainty Quantification. (arXiv:2401.12794v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12794
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26469;&#23545;LLMs&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#35780;&#20272;&#25351;&#26631;UAcc&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#39640;&#20934;&#30830;&#24230;&#30340;LLMs&#21487;&#33021;&#20855;&#26377;&#36739;&#20302;&#30340;&#30830;&#23450;&#24615;&#65292;&#32780;&#22823;&#35268;&#27169;LLMs&#21487;&#33021;&#27604;&#36739;&#23567;&#35268;&#27169;LLMs&#26356;&#19981;&#30830;&#23450;&#12290;&#25351;&#20196;&#24494;&#35843;&#20542;&#21521;&#20110;&#22686;&#21152;LLMs&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21508;&#20010;&#26426;&#26500;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22686;&#22810;&#65292;&#24432;&#26174;&#20102;&#23545;&#32508;&#21512;&#35780;&#20272;&#26041;&#27861;&#30340;&#36843;&#20999;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#35780;&#20272;&#24179;&#21488;&#65292;&#22914;&#24191;&#20026;&#20154;&#30693;&#30340;HuggingFace&#24320;&#25918;&#30340;LLM&#25490;&#34892;&#27036;&#65292;&#24573;&#35270;&#20102;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;--&#19981;&#30830;&#23450;&#24615;&#65292;&#32780;&#19981;&#30830;&#23450;&#24615;&#23545;&#20110;&#20840;&#38754;&#35780;&#20272;LLMs&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;LLMs&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#65292;&#23558;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#38598;&#25104;&#36827;&#21435;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#28085;&#30422;&#20102;&#20116;&#20010;&#20856;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#20843;&#20010;LLMs&#65288;LLM&#31995;&#21015;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#32771;&#34385;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#35780;&#20272;&#25351;&#26631;UAcc&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65306;I&#65289;&#20934;&#30830;&#24230;&#36234;&#39640;&#30340;LLMs&#21487;&#33021;&#20250;&#34920;&#29616;&#20986;&#36739;&#20302;&#30340;&#30830;&#23450;&#24615;&#65307;II&#65289;&#35268;&#27169;&#36739;&#22823;&#30340;LLMs&#21487;&#33021;&#19982;&#36739;&#23567;&#30340;LLMs&#30456;&#27604;&#26174;&#31034;&#20986;&#26356;&#22823;&#30340;&#19981;&#30830;&#23450;&#24615;&#65307;III&#65289;&#25351;&#20196;&#24494;&#35843;&#20542;&#21521;&#20110;&#22686;&#21152;LLMs&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#36890;&#36807;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26356;&#20840;&#38754;&#22320;&#35780;&#20272;LLMs&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of open-source Large Language Models (LLMs) from various institutions has highlighted the urgent need for comprehensive evaluation methods. However, current evaluation platforms, such as the widely recognized HuggingFace open LLM leaderboard, neglect a crucial aspect -- uncertainty, which is vital for thoroughly assessing LLMs. To bridge this gap, we introduce a new benchmarking approach for LLMs that integrates uncertainty quantification. Our examination involves eight LLMs (LLM series) spanning five representative natural language processing tasks. Additionally, we introduce an uncertainty-aware evaluation metric, UAcc, which takes into account both prediction accuracy and prediction uncertainty. Our findings reveal that: I) LLMs with higher accuracy may exhibit lower certainty; II) Larger-scale LLMs may display greater uncertainty compared to their smaller counterparts; and III) Instruction-finetuning tends to increase the uncertainty of LLMs. By taking uncertainty
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35821;&#31181;&#38750;&#33258;&#22238;&#24402;ASR&#31995;&#32479;&#65292;&#36890;&#36807;&#34701;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#35821;&#35328;&#19978;&#30340;&#24179;&#22343;&#30456;&#23545;WER&#25913;&#21892;&#65292;&#24182;&#36890;&#36807;&#20840;&#38754;&#30340;&#28040;&#34701;&#30740;&#31350;&#20998;&#26512;&#20102;&#24433;&#21709;&#23454;&#38469;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#34701;&#21512;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#25928;&#26524;&#30340;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2401.12789</link><description>&lt;p&gt;
&#22810;&#35821;&#31181;&#38750;&#33258;&#22238;&#24402;ASR&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34701;&#21512;&#65306;&#19968;&#39033;&#20840;&#38754;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Multilingual and Fully Non-Autoregressive ASR with Large Language Model Fusion: A Comprehensive Study. (arXiv:2401.12789v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12789
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35821;&#31181;&#38750;&#33258;&#22238;&#24402;ASR&#31995;&#32479;&#65292;&#36890;&#36807;&#34701;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#35821;&#35328;&#19978;&#30340;&#24179;&#22343;&#30456;&#23545;WER&#25913;&#21892;&#65292;&#24182;&#36890;&#36807;&#20840;&#38754;&#30340;&#28040;&#34701;&#30740;&#31350;&#20998;&#26512;&#20102;&#24433;&#21709;&#23454;&#38469;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#34701;&#21512;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#25928;&#26524;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#27169;&#22411;&#26102;&#20195;&#65292;&#35299;&#30721;&#30340;&#33258;&#22238;&#24402;&#29305;&#24615;&#32463;&#24120;&#23548;&#33268;&#24310;&#36831;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#29942;&#39048;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#30340;&#35821;&#35328;&#27169;&#22411;&#34701;&#21512;ASR&#31995;&#32479;&#65292;&#26377;&#25928;&#22320;&#21033;&#29992;&#20102;&#21152;&#36895;&#22120;&#30828;&#20214;&#30340;&#24182;&#34892;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#36890;&#29992;&#35821;&#38899;&#27169;&#22411;&#65288;USM&#65289;&#21644;PaLM 2&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#22312;&#27599;&#20010;&#29255;&#27573;&#30340;&#35780;&#20998;&#27169;&#24335;&#19979;&#65292;&#23454;&#29616;&#20102;&#22312;&#25152;&#26377;&#35821;&#35328;&#19978;&#30340;&#24179;&#22343;&#30456;&#23545;WER&#25913;&#21892;10.8%&#30340;FLEURS&#65292;&#24182;&#22312;YouTube&#23383;&#24149;&#19978;&#23454;&#29616;&#20102;3.6%&#30340;&#25913;&#21892;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20840;&#38754;&#28040;&#34701;&#30740;&#31350;&#20998;&#26512;&#20102;&#20851;&#38190;&#21442;&#25968;&#65292;&#22914;LLM&#22823;&#23567;&#12289;&#19978;&#19979;&#25991;&#38271;&#24230;&#12289;&#35789;&#27719;&#34920;&#22823;&#23567;&#21644;&#34701;&#21512;&#26041;&#27861;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLM&#22823;&#23567;&#20174;128M&#21040;340B&#21442;&#25968;&#23545;ASR&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#23545;&#24433;&#21709;&#23454;&#38469;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#34701;&#21512;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#25928;&#26524;&#30340;&#22240;&#32032;&#30340;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of large models, the autoregressive nature of decoding often results in latency serving as a significant bottleneck. We propose a non-autoregressive LM-fused ASR system that effectively leverages the parallelization capabilities of accelerator hardware. Our approach combines the Universal Speech Model (USM) and the PaLM 2 language model in per-segment scoring mode, achieving an average relative WER improvement across all languages of 10.8% on FLEURS and 3.6% on YouTube captioning. Furthermore, our comprehensive ablation study analyzes key parameters such as LLM size, context length, vocabulary size, fusion methodology. For instance, we explore the impact of LLM size ranging from 128M to 340B parameters on ASR performance. This study provides valuable insights into the factors influencing the effectiveness of practical large-scale LM-fused speech recognition systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38646;&#26679;&#26412;&#27169;&#22359;&#32452;&#21512;&#26694;&#26550;&#65292;&#32479;&#19968;&#20102;&#36873;&#25321;&#12289;&#21152;&#26435;&#21644;&#32452;&#21512;&#21442;&#25968;&#27169;&#22359;&#30340;&#21508;&#31181;&#21464;&#21270;&#12290;&#20197;&#39046;&#22495;&#30693;&#35782;&#21644;&#36866;&#37197;&#22120;&#23618;&#20026;&#22330;&#26223;&#65292;&#36890;&#36807;&#31995;&#32479;&#21270;&#30340;&#32479;&#19968;&#27010;&#24565;&#65292;&#36827;&#34892;&#20102;&#39318;&#27425;&#20840;&#38754;&#30340;&#38646;&#26679;&#26412;&#30693;&#35782;&#32452;&#21512;&#30340;&#22522;&#20934;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2401.12756</link><description>&lt;p&gt;
What the Weight?! &#38646;&#26679;&#26412;&#30693;&#35782;&#32452;&#21512;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
What the Weight?! A Unified Framework for Zero-Shot Knowledge Composition. (arXiv:2401.12756v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12756
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38646;&#26679;&#26412;&#27169;&#22359;&#32452;&#21512;&#26694;&#26550;&#65292;&#32479;&#19968;&#20102;&#36873;&#25321;&#12289;&#21152;&#26435;&#21644;&#32452;&#21512;&#21442;&#25968;&#27169;&#22359;&#30340;&#21508;&#31181;&#21464;&#21270;&#12290;&#20197;&#39046;&#22495;&#30693;&#35782;&#21644;&#36866;&#37197;&#22120;&#23618;&#20026;&#22330;&#26223;&#65292;&#36890;&#36807;&#31995;&#32479;&#21270;&#30340;&#32479;&#19968;&#27010;&#24565;&#65292;&#36827;&#34892;&#20102;&#39318;&#27425;&#20840;&#38754;&#30340;&#38646;&#26679;&#26412;&#30693;&#35782;&#32452;&#21512;&#30340;&#22522;&#20934;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#20013;&#25152;&#23553;&#35013;&#30340;&#30693;&#35782;&#26159;&#30830;&#23450;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#26368;&#32456;&#24615;&#33021;&#30340;&#26680;&#24515;&#22240;&#32032;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#35768;&#22810;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#23384;&#20648;&#21644;&#35843;&#25972;&#19981;&#21516;&#31867;&#22411;&#30693;&#35782;&#30340;&#26377;&#25928;&#26041;&#27861;&#19978;&#65292;&#20363;&#22914;&#22312;&#19987;&#29992;&#30340;&#27169;&#22359;&#21270;&#32467;&#26500;&#20013;&#65292;&#20197;&#21450;&#22914;&#20309;&#36890;&#36807;&#23398;&#20064;&#39069;&#22806;&#30340;&#21442;&#25968;&#26469;&#26377;&#25928;&#22320;&#32452;&#21512;&#36825;&#20123;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#37492;&#20110;&#23384;&#22312;&#35768;&#22810;&#21487;&#33021;&#30340;&#36873;&#39033;&#65292;&#23545;&#20110;&#36825;&#20123;&#32452;&#21512;&#20013;&#28041;&#21450;&#30340;&#26426;&#21046;&#32570;&#20047;&#20840;&#38754;&#30340;&#29702;&#35299;&#65292;&#22240;&#27492;&#30446;&#21069;&#20173;&#19981;&#28165;&#26970;&#24212;&#35813;&#20351;&#29992;&#21738;&#20123;&#31574;&#30053;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38646;&#26679;&#26412;&#27169;&#22359;&#32452;&#21512;&#26694;&#26550;&#65292;&#23427;&#28085;&#30422;&#20102;&#29616;&#26377;&#30340;&#19968;&#20123;&#36873;&#25321;&#12289;&#21152;&#26435;&#21644;&#32452;&#21512;&#21442;&#25968;&#27169;&#22359;&#30340;&#21464;&#21270;&#65292;&#32479;&#19968;&#20102;&#36825;&#20123;&#27010;&#24565;&#12290;&#22312;&#32858;&#28966;&#39046;&#22495;&#30693;&#35782;&#21644;&#36866;&#37197;&#22120;&#23618;&#30340;&#24773;&#26223;&#19979;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#20010;&#31995;&#32479;&#21270;&#30340;&#32479;&#19968;&#27010;&#24565;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#36827;&#34892;&#39318;&#27425;&#20840;&#38754;&#30340;&#21508;&#31181;&#38646;&#26679;&#26412;&#30693;&#35782;&#32452;&#21512;&#30340;&#22522;&#20934;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
The knowledge encapsulated in a model is the core factor determining its final performance on downstream tasks. Much research in NLP has focused on efficient methods for storing and adapting different types of knowledge, e.g., in dedicated modularized structures, and on how to effectively combine these, e.g., by learning additional parameters. However, given the many possible options, a thorough understanding of the mechanisms involved in these compositions is missing, and hence it remains unclear which strategies to utilize. To address this research gap, we propose a novel framework for zero-shot module composition, which encompasses existing and some novel variations for selecting, weighting, and combining parameter modules under a single unified notion. Focusing on the scenario of domain knowledge and adapter layers, our framework provides a systematic unification of concepts, allowing us to conduct the first comprehensive benchmarking study of various zero-shot knowledge compositio
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20840;&#38754;&#35266;&#23519;&#20102;&#24773;&#24863;&#21644;&#27602;&#24615;&#20998;&#26512;&#26041;&#27861;&#23545;&#24102;&#26377;&#38750;&#27954;&#35028;&#32654;&#22269;&#33521;&#35821;&#34920;&#36798;&#30340;&#35805;&#35821;&#20013;&#30340;&#20559;&#35265;&#12290;&#23454;&#39564;&#32467;&#26524;&#21457;&#29616;&#30001;&#20110;AI&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#65292;&#23548;&#33268;&#36825;&#20123;&#26041;&#27861;&#22312;&#23545;&#24453;&#23545;&#8220;&#40657;&#39740;&#8221;&#19968;&#35789;&#30340;&#20877;&#36866;&#24212;&#26102;&#23384;&#22312;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2401.12720</link><description>&lt;p&gt;
&#23545;&#26377;&#30528;&#38750;&#27954;&#35028;&#32654;&#22269;&#33521;&#35821;&#34920;&#36798;&#30340;&#35770;&#36848;&#30340;&#27602;&#24615;&#21644;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#30340;&#20559;&#35265;&#30340;&#20840;&#38754;&#35266;&#23519;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive View of the Biases of Toxicity and Sentiment Analysis Methods Towards Utterances with African American English Expressions. (arXiv:2401.12720v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12720
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20840;&#38754;&#35266;&#23519;&#20102;&#24773;&#24863;&#21644;&#27602;&#24615;&#20998;&#26512;&#26041;&#27861;&#23545;&#24102;&#26377;&#38750;&#27954;&#35028;&#32654;&#22269;&#33521;&#35821;&#34920;&#36798;&#30340;&#35805;&#35821;&#20013;&#30340;&#20559;&#35265;&#12290;&#23454;&#39564;&#32467;&#26524;&#21457;&#29616;&#30001;&#20110;AI&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#65292;&#23548;&#33268;&#36825;&#20123;&#26041;&#27861;&#22312;&#23545;&#24453;&#23545;&#8220;&#40657;&#39740;&#8221;&#19968;&#35789;&#30340;&#20877;&#36866;&#24212;&#26102;&#23384;&#22312;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#26159;&#25105;&#20204;&#25991;&#21270;&#30340;&#21160;&#24577;&#26041;&#38754;&#65292;&#19981;&#21516;&#30340;&#25216;&#26415;/&#31038;&#21306;&#20013;&#34920;&#36798;&#23601;&#20250;&#26377;&#25152;&#21464;&#21270;&#12290;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#20351;&#24471;&#19981;&#21516;&#26041;&#35328;&#65292;&#21253;&#25324;&#38750;&#27954;&#35028;&#32654;&#22269;&#33521;&#35821;&#65288;AAE&#65289;&#24471;&#20197;&#20256;&#25773;&#21644;&#28436;&#21464;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22686;&#21152;&#30340;&#20351;&#29992;&#20063;&#23384;&#22312;&#38556;&#30861;&#12290;&#20854;&#20013;&#19968;&#20010;&#38556;&#30861;&#26159;&#24773;&#24863;&#65288;Vader&#65292;TextBlob&#21644;Flair&#65289;&#21644;&#27602;&#24615;&#65288;&#35895;&#27468;&#30340;Perspective&#21644;&#24320;&#28304;&#30340;Detoxify&#65289;&#26041;&#27861;&#22312;&#23545;&#24102;&#26377;AAE&#34920;&#36798;&#30340;&#35805;&#35821;&#19978;&#21576;&#29616;&#20559;&#35265;&#12290;&#20197;&#35895;&#27468;&#30340;Perspective&#20026;&#20363;&#26469;&#29702;&#35299;&#20559;&#35265;&#12290;&#22312;&#36825;&#37324;&#65292;&#20687;&#8220;&#25152;&#26377;&#40657;&#39740;&#37117;&#24212;&#35813;&#21463;&#21040;&#23562;&#37325;&#22320;&#27515;&#21435;&#12290;&#35686;&#23519;&#35851;&#26432;&#25105;&#20204;&#12290;&#8221;&#36825;&#26679;&#30340;&#35805;&#35821;&#27604;&#8220;&#38750;&#27954;&#35028;&#32654;&#22269;&#20154;&#24212;&#35813;&#21463;&#21040;&#23562;&#37325;&#22320;&#27515;&#21435;&#12290;&#35686;&#23519;&#35851;&#26432;&#25105;&#20204;&#12290;&#8221;&#30340;&#27602;&#24615;&#24471;&#20998;&#26356;&#39640;&#12290;&#36825;&#31181;&#24471;&#20998;&#24046;&#24322;&#24456;&#21487;&#33021;&#26159;&#22240;&#20026;&#35813;&#24037;&#20855;&#19981;&#33021;&#29702;&#35299;&#23545;&#8220;&#40657;&#39740;&#8221;&#19968;&#35789;&#30340;&#20877;&#36866;&#24212;&#12290;&#23545;&#36825;&#31181;&#20559;&#35265;&#30340;&#19968;&#20010;&#35299;&#37322;&#26159;&#65292;AI&#27169;&#22411;&#26159;&#22522;&#20110;&#26377;&#38480;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#20351;&#29992;&#36825;&#26679;&#30340;&#26415;&#35821;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#26356;&#26377;&#21487;&#33021;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language is a dynamic aspect of our culture that changes when expressed in different technologies/communities. Online social networks have enabled the diffusion and evolution of different dialects, including African American English (AAE). However, this increased usage is not without barriers. One particular barrier is how sentiment (Vader, TextBlob, and Flair) and toxicity (Google's Perspective and the open-source Detoxify) methods present biases towards utterances with AAE expressions. Consider Google's Perspective to understand bias. Here, an utterance such as ``All n*ggers deserve to die respectfully. The police murder us.'' it reaches a higher toxicity than ``African-Americans deserve to die respectfully. The police murder us.''. This score difference likely arises because the tool cannot understand the re-appropriation of the term ``n*gger''. One explanation for this bias is that AI models are trained on limited datasets, and using such a term in training data is more likely to a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35328;&#36766;&#36827;&#34892;&#35780;&#20998;&#24182;&#29983;&#25104;&#35299;&#37322;&#24615;&#25688;&#35201;&#65292;&#26469;&#22686;&#21152;&#35875;&#35328;&#39564;&#35777;&#30340;&#20449;&#24687;&#20016;&#23500;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.12713</link><description>&lt;p&gt;
&#29983;&#25104;&#26080;&#30417;&#30563;&#30340;&#35328;&#36766;&#35299;&#37322;&#29992;&#20110;&#35875;&#35328;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Generating Unsupervised Abstractive Explanations for Rumour Verification. (arXiv:2401.12713v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12713
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35328;&#36766;&#36827;&#34892;&#35780;&#20998;&#24182;&#29983;&#25104;&#35299;&#37322;&#24615;&#25688;&#35201;&#65292;&#26469;&#22686;&#21152;&#35875;&#35328;&#39564;&#35777;&#30340;&#20449;&#24687;&#20016;&#23500;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#36827;&#34892;&#35875;&#35328;&#39564;&#35777;&#30340;&#20219;&#21153;&#28041;&#21450;&#26681;&#25454;&#30001;&#35813;&#35875;&#35328;&#24341;&#36215;&#30340;&#23545;&#35805;&#32447;&#31243;&#35780;&#20272;&#20854;&#30495;&#23454;&#24615;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#20043;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#19987;&#27880;&#20110;&#39044;&#27979;&#30495;&#23454;&#24615;&#26631;&#31614;&#65292;&#20294;&#25105;&#20204;&#22312;&#36825;&#37324;&#37325;&#26032;&#21046;&#23450;&#20102;&#20219;&#21153;&#65292;&#20197;&#29983;&#25104;&#19982;&#27169;&#22411;&#30456;&#20851;&#30340;&#33258;&#30001;&#25991;&#26412;&#35299;&#37322;&#35875;&#35328;&#30340;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#37319;&#29992;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#39318;&#20808;&#21033;&#29992;&#20107;&#21518;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#23545;&#32447;&#31243;&#20013;&#26368;&#37325;&#35201;&#30340;&#24086;&#23376;&#36827;&#34892;&#35780;&#20998;&#65292;&#28982;&#21518;&#20351;&#29992;&#36825;&#20123;&#24086;&#23376;&#36890;&#36807;&#20351;&#29992;&#27169;&#26495;&#24341;&#23548;&#24635;&#32467;&#29983;&#25104;&#20449;&#24687;&#20016;&#23500;&#30340;&#35299;&#37322;&#24615;&#25688;&#35201;&#12290;&#20026;&#20102;&#35780;&#20272;&#35299;&#37322;&#24615;&#25688;&#35201;&#30340;&#20449;&#24687;&#37327;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35821;&#35328;&#27169;&#22411;&#22312;&#35780;&#20272;&#25688;&#35201;&#26102;&#21487;&#20197;&#19982;&#20154;&#31867;&#36798;&#21040;&#31867;&#20284;&#30340;&#19968;&#33268;&#24615;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35299;&#37322;&#24615;&#30340;&#27010;&#25324;&#25688;&#35201;&#27604;&#20165;&#20351;&#29992;&#32447;&#31243;&#20013;&#25490;&#21517;&#26368;&#39640;&#30340;&#24086;&#23376;&#26356;&#20855;&#20449;&#24687;&#37327;&#65292;&#24182;&#26356;&#22909;&#22320;&#21453;&#26144;&#20102;&#39044;&#27979;&#30340;&#35875;&#35328;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of rumour verification in social media concerns assessing the veracity of a claim on the basis of conversation threads that result from it. While previous work has focused on predicting a veracity label, here we reformulate the task to generate model-centric, free-text explanations of a rumour's veracity. We follow an unsupervised approach by first utilising post-hoc explainability methods to score the most important posts within a thread and then we use these posts to generate informative explanatory summaries by employing template-guided summarisation. To evaluate the informativeness of the explanatory summaries, we exploit the few-shot learning capabilities of a large language model (LLM). Our experiments show that LLMs can have similar agreement to humans in evaluating summaries. Importantly, we show that explanatory abstractive summaries are more informative and better reflect the predicted rumour veracity than just using the highest ranking posts in the thread.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#30340;&#33258;&#21160;&#21270;&#27169;&#22411;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#20851;&#20110;&#20010;&#20307;&#26679;&#26412;&#30456;&#20851;&#20449;&#24687;&#30340;&#20803;&#20998;&#24067;&#32479;&#35745;&#37327;&#65292;&#33021;&#22815;&#26356;&#39640;&#25928;&#21644;&#26377;&#25928;&#22320;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;AutoEval&#26694;&#26550;&#20013;&#30340;&#36807;&#24230;&#33258;&#20449;&#12289;&#23384;&#20648;&#21644;&#35745;&#31639;&#25104;&#26412;&#39640;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.12689</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#30340;&#33258;&#21160;&#21270;&#27169;&#22411;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Energy-based Automated Model Evaluation. (arXiv:2401.12689v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12689
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#30340;&#33258;&#21160;&#21270;&#27169;&#22411;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#20851;&#20110;&#20010;&#20307;&#26679;&#26412;&#30456;&#20851;&#20449;&#24687;&#30340;&#20803;&#20998;&#24067;&#32479;&#35745;&#37327;&#65292;&#33021;&#22815;&#26356;&#39640;&#25928;&#21644;&#26377;&#25928;&#22320;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;AutoEval&#26694;&#26550;&#20013;&#30340;&#36807;&#24230;&#33258;&#20449;&#12289;&#23384;&#20648;&#21644;&#35745;&#31639;&#25104;&#26412;&#39640;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;&#21327;&#35758;&#20381;&#36182;&#20110;&#26631;&#35760;&#30340;&#12289;&#20551;&#35774;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#32780;&#36825;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#24448;&#24448;&#24182;&#19981;&#24120;&#35265;&#12290;&#33258;&#21160;&#27169;&#22411;&#35780;&#20272;&#65288;AutoEval&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#20256;&#32479;&#24037;&#20316;&#27969;&#31243;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24418;&#25104;&#19968;&#20010;&#25509;&#36817;&#39044;&#27979;&#24615;&#33021;&#30340;&#27979;&#35797;&#31649;&#32447;&#65292;&#32780;&#26080;&#38656;&#30495;&#23454;&#26631;&#31614;&#30340;&#23384;&#22312;&#12290;&#23613;&#31649;AutoEval&#26694;&#26550;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#19968;&#20123;&#25104;&#21151;&#65292;&#20294;&#20173;&#23384;&#22312;&#36807;&#24230;&#33258;&#20449;&#12289;&#23384;&#20648;&#21644;&#35745;&#31639;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24230;&#37327;&#26041;&#24335;&#8212;&#8212;&#20803;&#20998;&#24067;&#33021;&#37327;&#65288;MDE&#65289;&#65292;&#23427;&#21487;&#20197;&#20351;AutoEval&#26694;&#26550;&#26356;&#21152;&#39640;&#25928;&#21644;&#26377;&#25928;&#12290;MDE&#30340;&#26680;&#24515;&#26159;&#24314;&#31435;&#19968;&#20010;&#20851;&#20110;&#20010;&#20307;&#26679;&#26412;&#30456;&#20851;&#20449;&#24687;&#65288;&#33021;&#37327;&#65289;&#30340;&#20803;&#20998;&#24067;&#32479;&#35745;&#37327;&#65292;&#28982;&#21518;&#36890;&#36807;&#22522;&#20110;&#33021;&#37327;&#30340;&#23398;&#20064;&#25552;&#20379;&#26356;&#24179;&#28369;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;MDE&#19982;&#20998;&#31867;&#25439;&#22833;&#30456;&#36830;&#25509;&#65292;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#29702;&#35770;&#27934;&#35265;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#25454;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The conventional evaluation protocols on machine learning models rely heavily on a labeled, i.i.d-assumed testing dataset, which is not often present in real world applications. The Automated Model Evaluation (AutoEval) shows an alternative to this traditional workflow, by forming a proximal prediction pipeline of the testing performance without the presence of ground-truth labels. Despite its recent successes, the AutoEval frameworks still suffer from an overconfidence issue, substantial storage and computational cost. In that regard, we propose a novel measure -- Meta-Distribution Energy (MDE) -- that allows the AutoEval framework to be both more efficient and effective. The core of the MDE is to establish a meta-distribution statistic, on the information (energy) associated with individual samples, then offer a smoother representation enabled by energy-based learning. We further provide our theoretical insights by connecting the MDE with the classification loss. We provide extensive
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32467;&#21512;&#22270;&#39537;&#21160;&#30340;&#19978;&#19979;&#25991;&#26816;&#32034;&#21644;&#30693;&#35782;&#22270;&#32467;&#26500;&#22686;&#24378;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#39640;LLMs&#30340;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#19978;&#65292;&#26356;&#22909;&#22320;&#22238;&#31572;&#24320;&#25918;&#24335;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.12671</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#30340;&#37325;&#35201;&#24615;&#65306;&#36890;&#36807;&#22270;&#32467;&#26500;&#21270;&#30693;&#35782;&#19978;&#19979;&#25991;&#25512;&#21160;&#24320;&#25918;&#24335;&#31572;&#26696;&#29983;&#25104;&#30340;&#36793;&#30028;
&lt;/p&gt;
&lt;p&gt;
Context Matters: Pushing the Boundaries of Open-Ended Answer Generation with Graph-Structured Knowledge Context. (arXiv:2401.12671v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32467;&#21512;&#22270;&#39537;&#21160;&#30340;&#19978;&#19979;&#25991;&#26816;&#32034;&#21644;&#30693;&#35782;&#22270;&#32467;&#26500;&#22686;&#24378;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#39640;LLMs&#30340;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#19978;&#65292;&#26356;&#22909;&#22320;&#22238;&#31572;&#24320;&#25918;&#24335;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#26029;&#21457;&#23637;&#30340;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#26500;&#24314;&#19978;&#19979;&#25991;&#20016;&#23500;&#12289;&#26377;&#24847;&#20041;&#30340;&#22238;&#31572;&#33267;&#20851;&#37325;&#35201;&#12290;&#30740;&#31350;&#20154;&#21592;&#36234;&#26469;&#36234;&#24847;&#35782;&#21040;&#24403;LLMs&#30340;&#21442;&#25968;&#36739;&#23569;&#26102;&#65292;&#23581;&#35797;&#25552;&#20379;&#21512;&#36866;&#31572;&#26696;&#32473;&#24320;&#25918;&#24335;&#38382;&#39064;&#26102;&#20250;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38556;&#30861;&#65292;&#23558;&#20808;&#36827;&#30340;&#31574;&#30053;&#19982;&#20016;&#23500;&#30340;&#22806;&#37096;&#39046;&#22495;&#30693;&#35782;&#19982;LLMs&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#31572;&#26696;&#30340;&#36136;&#37327;&#12290;&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23558;&#22522;&#20110;&#22270;&#30340;&#19978;&#19979;&#25991;&#26816;&#32034;&#19982;&#30693;&#35782;&#22270;&#32467;&#26500;&#22686;&#24378;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;LLMs&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#30340;&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#65292;&#22914;AskUbuntu&#12289;Unix&#21644;ServerFault&#12290;&#25105;&#20204;&#23545;&#19981;&#21516;&#21442;&#25968;&#22823;&#23567;&#30340;&#21508;&#31181;LLMs&#36827;&#34892;&#23454;&#39564;&#65292;&#35780;&#20272;&#23427;&#20204;&#22312;&#24320;&#25918;&#24335;&#38382;&#39064;&#30340;&#22238;&#31572;&#20013;&#30340;&#30693;&#35782;&#30830;&#23450;&#33021;&#21147;&#21644;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;GraphContextGen&#22312;&#22522;&#20110;&#25991;&#26412;&#30340;&#29616;&#26377;&#26041;&#27861;&#19978;&#25345;&#32493;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the continuously advancing AI landscape, crafting context-rich and meaningful responses via Large Language Models (LLMs) is essential. Researchers are becoming more aware of the challenges that LLMs with fewer parameters encounter when trying to provide suitable answers to open-ended questions. To address these hurdles, the integration of cutting-edge strategies, augmentation of rich external domain knowledge to LLMs, offers significant improvements. This paper introduces a novel framework that combines graph-driven context retrieval in conjunction to knowledge graphs based enhancement, honing the proficiency of LLMs, especially in domain specific community question answering platforms like AskUbuntu, Unix, and ServerFault. We conduct experiments on various LLMs with different parameter sizes to evaluate their ability to ground knowledge and determine factual accuracy in answers to open-ended questions. Our methodology GraphContextGen consistently outperforms dominant text-based ret
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#24212;&#20102;Makelov&#31561;&#20154;(2023)&#30340;&#35770;&#25991;&#65292;&#35813;&#35770;&#25991;&#35780;&#36848;&#20102;&#23376;&#31354;&#38388;&#20132;&#25442;&#24178;&#39044;&#26041;&#27861;&#30340;"&#35299;&#37322;&#24615;&#38169;&#35273;"&#38382;&#39064;&#12290;&#25105;&#20204;&#25351;&#20986;&#65292;&#25152;&#35859;&#30340;"&#35299;&#37322;&#24615;&#38169;&#35273;"&#21487;&#20197;&#21253;&#25324;&#30452;&#35266;&#21644;&#21487;&#21462;&#30340;&#35299;&#37322;&#65292;&#32780;Makelov&#31561;&#20154;(2023)&#21457;&#29616;&#30340;"&#38169;&#35273;"&#26159;&#20182;&#20204;&#35757;&#32451;&#21644;&#35780;&#20272;&#33539;&#20363;&#30340;&#20135;&#29289;&#12290;&#23613;&#31649;&#25105;&#20204;&#19981;&#21516;&#24847;&#20182;&#20204;&#30340;&#26680;&#24515;&#34920;&#36848;&#65292;&#20294;&#20182;&#20204;&#30340;&#20363;&#23376;&#21644;&#35752;&#35770;&#25512;&#21160;&#20102;&#21487;&#35299;&#37322;&#24615;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2401.12631</link><description>&lt;p&gt;
&#23545;Makelov&#31561;&#20154;(2023)&#30340;&#12298;&#21487;&#35299;&#37322;&#24615;&#38169;&#35273;&#12299;&#35770;&#28857;&#30340;&#22238;&#24212;
&lt;/p&gt;
&lt;p&gt;
A Reply to Makelov et al. (2023)'s "Interpretability Illusion" Arguments. (arXiv:2401.12631v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#24212;&#20102;Makelov&#31561;&#20154;(2023)&#30340;&#35770;&#25991;&#65292;&#35813;&#35770;&#25991;&#35780;&#36848;&#20102;&#23376;&#31354;&#38388;&#20132;&#25442;&#24178;&#39044;&#26041;&#27861;&#30340;"&#35299;&#37322;&#24615;&#38169;&#35273;"&#38382;&#39064;&#12290;&#25105;&#20204;&#25351;&#20986;&#65292;&#25152;&#35859;&#30340;"&#35299;&#37322;&#24615;&#38169;&#35273;"&#21487;&#20197;&#21253;&#25324;&#30452;&#35266;&#21644;&#21487;&#21462;&#30340;&#35299;&#37322;&#65292;&#32780;Makelov&#31561;&#20154;(2023)&#21457;&#29616;&#30340;"&#38169;&#35273;"&#26159;&#20182;&#20204;&#35757;&#32451;&#21644;&#35780;&#20272;&#33539;&#20363;&#30340;&#20135;&#29289;&#12290;&#23613;&#31649;&#25105;&#20204;&#19981;&#21516;&#24847;&#20182;&#20204;&#30340;&#26680;&#24515;&#34920;&#36848;&#65292;&#20294;&#20182;&#20204;&#30340;&#20363;&#23376;&#21644;&#35752;&#35770;&#25512;&#21160;&#20102;&#21487;&#35299;&#37322;&#24615;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22238;&#24212;&#20102;Makelov&#31561;&#20154;(2023)&#30340;&#26368;&#26032;&#35770;&#25991;&#65292;&#35813;&#35770;&#25991;&#35780;&#36848;&#20102;&#35832;&#22914;&#20998;&#24067;&#24335;&#23545;&#40784;&#25628;&#32034;(DAS; Geiger&#31561;&#20154;&#65292;2023)&#36825;&#26679;&#30340;&#23376;&#31354;&#38388;&#20132;&#25442;&#24178;&#39044;&#26041;&#27861;&#65292;&#24182;&#22768;&#31216;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#24341;&#36215;"&#35299;&#37322;&#24615;&#38169;&#35273;"&#12290;&#25105;&#20204;&#39318;&#20808;&#22238;&#39038;&#20102;Makelov&#31561;&#20154;(2023)&#23545;"&#35299;&#37322;&#24615;&#38169;&#35273;"&#30340;&#25216;&#26415;&#27010;&#24565;&#65292;&#28982;&#21518;&#23637;&#31034;&#20102;&#21363;&#20351;&#30452;&#35266;&#21644;&#21487;&#21462;&#30340;&#35299;&#37322;&#22312;&#36825;&#20010;&#24847;&#20041;&#19978;&#20063;&#21487;&#33021;&#25104;&#20026;&#38169;&#35273;&#12290;&#22240;&#27492;&#65292;&#20182;&#20204;&#21457;&#29616;"&#38169;&#35273;"&#30340;&#26041;&#27861;&#21487;&#33021;&#20250;&#25298;&#32477;&#20182;&#20204;&#35748;&#20026;"&#38750;&#38169;&#35273;"&#30340;&#35299;&#37322;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#35748;&#20026;Makelov&#31561;&#20154;(2023)&#22312;&#23454;&#36341;&#20013;&#30475;&#21040;&#30340;"&#38169;&#35273;"&#26159;&#20182;&#20204;&#35757;&#32451;&#21644;&#35780;&#20272;&#33539;&#20363;&#30340;&#20135;&#29289;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24378;&#35843;&#65292;&#23613;&#31649;&#25105;&#20204;&#19981;&#21516;&#24847;&#20182;&#20204;&#30340;&#26680;&#24515;&#34920;&#36848;&#65292;&#20294;Makelov&#31561;&#20154;(2023)&#30340;&#20363;&#23376;&#21644;&#35752;&#35770;&#26080;&#30097;&#25512;&#21160;&#20102;&#21487;&#35299;&#37322;&#24615;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
We respond to the recent paper by Makelov et al. (2023), which reviews subspace interchange intervention methods like distributed alignment search (DAS; Geiger et al. 2023) and claims that these methods potentially cause "interpretability illusions". We first review Makelov et al. (2023)'s technical notion of what an "interpretability illusion" is, and then we show that even intuitive and desirable explanations can qualify as illusions in this sense. As a result, their method of discovering "illusions" can reject explanations they consider "non-illusory". We then argue that the illusions Makelov et al. (2023) see in practice are artifacts of their training and evaluation paradigms. We close by emphasizing that, though we disagree with their core characterization, Makelov et al. (2023)'s examples and discussion have undoubtedly pushed the field of interpretability forward.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;SLANG&#65292;&#26088;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#23545;&#20114;&#32852;&#32593;&#19978;&#26032;&#27010;&#24565;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#30340;&#22522;&#20934;&#26041;&#27861;FOCUS&#65292;&#33021;&#24110;&#21161;LLMs&#26356;&#22909;&#22320;&#29702;&#35299;&#26032;&#30340;&#30701;&#35821;&#21644;&#29992;&#27861;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2401.12585</link><description>&lt;p&gt;
SLANG: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#26032;&#27010;&#24565;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
SLANG: New Concept Comprehension of Large Language Models. (arXiv:2401.12585v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;SLANG&#65292;&#26088;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#23545;&#20114;&#32852;&#32593;&#19978;&#26032;&#27010;&#24565;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#30340;&#22522;&#20934;&#26041;&#27861;FOCUS&#65292;&#33021;&#24110;&#21161;LLMs&#26356;&#22909;&#22320;&#29702;&#35299;&#26032;&#30340;&#30701;&#35821;&#21644;&#29992;&#27861;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#30340;&#21160;&#24577;&#24615;&#65292;&#23588;&#20854;&#22312;&#20114;&#32852;&#32593;&#19978;&#30340;&#20442;&#35821;&#21644;&#34920;&#24773;&#21253;&#31561;&#26041;&#38754;&#30340;&#20307;&#29616;&#65292;&#32473;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36866;&#24212;&#24615;&#24102;&#26469;&#20102;&#20005;&#23803;&#25361;&#25112;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#20165;&#32465;&#23450;&#22312;&#38745;&#24577;&#25968;&#25454;&#38598;&#19978;&#65292;&#24456;&#38590;&#36319;&#19978;&#22312;&#32447;&#31038;&#21306;&#20013;&#24555;&#36895;&#35821;&#35328;&#36827;&#21270;&#30340;&#27493;&#20240;&#12290;&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#30340;&#36843;&#20999;&#38656;&#27714;&#65292;&#26088;&#22312;&#22686;&#24378;LLMs&#23545;&#20114;&#32852;&#32593;&#19978;&#26032;&#27010;&#24565;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#21516;&#26102;&#36991;&#20813;&#39640;&#25104;&#26412;&#21644;&#19981;&#20999;&#23454;&#38469;&#30340;&#25345;&#32493;&#37325;&#35757;&#32451;&#12290;&#20026;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;LLMs&#22312;&#29702;&#35299;&#26032;&#20852;&#35821;&#35328;&#36235;&#21183;&#26041;&#38754;&#33021;&#21147;&#30340;&#22522;&#20934; - SLANG&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#30340;&#22522;&#20934;&#26041;&#27861; FOCUS&#65292;&#23427;&#33021;&#22686;&#24378;LLMs&#23545;&#26032;&#30340;&#30701;&#35821;&#21644;&#29992;&#27861;&#27169;&#24335;&#30340;&#29702;&#35299;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#23545;&#35821;&#35328;&#36716;&#21464;&#30340;&#30495;&#23454;&#19990;&#30028;&#23454;&#20363;&#36827;&#34892;&#35814;&#32454;&#30740;&#31350;&#65292;&#20316;&#20026;&#32972;&#26223;&#20381;&#25454;&#65292;&#20197;&#24418;&#25104;&#26356;&#31934;&#30830;&#21644;&#20855;&#26377;&#19978;&#19979;&#25991;&#30456;&#20851;&#24615;&#30340;&#26032;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dynamic nature of language, particularly evident in the realm of slang and memes on the Internet, poses serious challenges to the adaptability of large language models (LLMs). Traditionally anchored to static datasets, these models often struggle to keep up with the rapid linguistic evolution characteristic of online communities. This research addresses the critical need to bridge this gap, aiming to enhance LLMs' comprehension of evolving new concepts on the internet, without the high cost and impracticality of continual retraining. To address this issue, we propose a new benchmark $\textbf{SLANG}$ to assess LLMs' proficiency in comprehending emerging linguistic trends and a baseline approach $\textbf{FOCUS}$, which uses causal inference to enhance LLMs to understand new phrases and usage patterns. This approach involves scrutinizing real-world instances of linguistic shifts, serving as contextual beacons, to form more precise and contextually relevant connections between newly em
&lt;/p&gt;</description></item><item><title>LLMCheckup&#26159;&#19968;&#20010;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#65292;&#36890;&#36807;&#36830;&#25509;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#21487;&#35299;&#37322;&#30340;AI&#24037;&#20855;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#19982;&#27169;&#22411;&#36827;&#34892;&#23545;&#35805;&#65292;&#29983;&#25104;&#33258;&#25105;&#35299;&#37322;&#24182;&#25552;&#20379;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2401.12576</link><description>&lt;p&gt;
LLMCheckup&#65306;&#36890;&#36807;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23545;&#35805;&#24335;&#26816;&#26597;
&lt;/p&gt;
&lt;p&gt;
LLMCheckup: Conversational Examination of Large Language Models via Interpretability Tools. (arXiv:2401.12576v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12576
&lt;/p&gt;
&lt;p&gt;
LLMCheckup&#26159;&#19968;&#20010;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#65292;&#36890;&#36807;&#36830;&#25509;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#21487;&#35299;&#37322;&#30340;AI&#24037;&#20855;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#19982;&#27169;&#22411;&#36827;&#34892;&#23545;&#35805;&#65292;&#29983;&#25104;&#33258;&#25105;&#35299;&#37322;&#24182;&#25552;&#20379;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#20197;&#23545;&#35805;&#24418;&#24335;&#36827;&#34892;&#35299;&#37322;&#30340;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#24050;&#32463;&#35777;&#26126;&#22312;&#22686;&#24378;&#29992;&#25143;&#29702;&#35299;&#26041;&#38754;&#20855;&#26377;&#25928;&#26524;&#65292;&#22240;&#20026;&#19968;&#27425;&#24615;&#35299;&#37322;&#26377;&#26102;&#26080;&#27861;&#25552;&#20379;&#36275;&#22815;&#30340;&#20449;&#24687;&#32473;&#29992;&#25143;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#22522;&#20110;&#23545;&#35805;&#30340;&#35299;&#37322;&#26041;&#26696;&#38656;&#35201;&#35768;&#22810;&#20381;&#36182;&#39033;&#65292;&#24182;&#19988;&#19981;&#23481;&#26131;&#36716;&#31227;&#21040;&#23427;&#20204;&#26410;&#35774;&#35745;&#30340;&#20219;&#21153;&#19978;&#12290;&#36890;&#36807;LLMCheckup&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26131;&#20110;&#35775;&#38382;&#30340;&#24037;&#20855;&#65292;&#20801;&#35768;&#29992;&#25143;&#19982;&#20219;&#20309;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#23545;&#35805;&#20197;&#20102;&#35299;&#20854;&#34892;&#20026;&#12290;&#25105;&#20204;&#20351;LLMs&#33021;&#22815;&#33258;&#34892;&#29983;&#25104;&#25152;&#26377;&#35299;&#37322;&#65292;&#24182;&#36890;&#36807;&#19982;&#19968;&#31995;&#21015;&#21487;&#35299;&#37322;&#24615;AI&#65288;XAI&#65289;&#24037;&#20855;&#65288;&#20363;&#22914;&#29305;&#24449;&#24402;&#22240;&#12289;&#22522;&#20110;&#23884;&#20837;&#30340;&#30456;&#20284;&#24615;&#20197;&#21450;&#21453;&#20107;&#23454;&#21644;&#22522;&#20110;&#29702;&#30001;&#29983;&#25104;&#30340;&#25552;&#31034;&#31574;&#30053;&#65289;&#36830;&#25509;&#65292;&#20197;&#23436;&#25104;&#24847;&#22270;&#35782;&#21035;&#32780;&#26080;&#38656;&#24494;&#35843;&#12290;LLM&#65288;&#33258;&#25105;&#65289;&#35299;&#37322;&#20197;&#20132;&#20114;&#23545;&#35805;&#30340;&#24418;&#24335;&#21576;&#29616;&#65292;&#25903;&#25345;&#21518;&#32493;&#38382;&#39064;&#21644;&#29983;&#25104;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretability tools that offer explanations in the form of a dialogue have demonstrated their efficacy in enhancing users' understanding, as one-off explanations may occasionally fall short in providing sufficient information to the user. Current solutions for dialogue-based explanations, however, require many dependencies and are not easily transferable to tasks they were not designed for. With LLMCheckup, we present an easily accessible tool that allows users to chat with any state-of-the-art large language model (LLM) about its behavior. We enable LLMs to generate all explanations by themselves and take care of intent recognition without fine-tuning, by connecting them with a broad spectrum of Explainable AI (XAI) tools, e.g. feature attributions, embedding-based similarity, and prompting strategies for counterfactual and rationale generation. LLM (self-)explanations are presented as an interactive dialogue that supports follow-up questions and generates suggestions. LLMCheckup p
&lt;/p&gt;</description></item><item><title>Climinator&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#33258;&#21160;&#26680;&#26597;&#27668;&#20505;&#21464;&#21270;&#22768;&#26126;&#12290;&#23427;&#21033;&#29992;&#22810;&#31181;&#31185;&#23398;&#26469;&#28304;&#30340;&#20449;&#24687;&#65292;&#37319;&#29992;&#35843;&#35299;&#32773;-&#20513;&#23548;&#32773;&#26694;&#26550;&#65292;&#26377;&#25928;&#32508;&#21512;&#19981;&#21516;&#35266;&#28857;&#65292;&#20174;&#32780;&#24471;&#20986;&#22522;&#20110;&#31185;&#23398;&#12289;&#20107;&#23454;&#30340;&#35780;&#20272;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.12566</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#27668;&#20505;&#21464;&#21270;&#22768;&#26126;
&lt;/p&gt;
&lt;p&gt;
Automated Fact-Checking of Climate Change Claims with Large Language Models. (arXiv:2401.12566v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12566
&lt;/p&gt;
&lt;p&gt;
Climinator&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#33258;&#21160;&#26680;&#26597;&#27668;&#20505;&#21464;&#21270;&#22768;&#26126;&#12290;&#23427;&#21033;&#29992;&#22810;&#31181;&#31185;&#23398;&#26469;&#28304;&#30340;&#20449;&#24687;&#65292;&#37319;&#29992;&#35843;&#35299;&#32773;-&#20513;&#23548;&#32773;&#26694;&#26550;&#65292;&#26377;&#25928;&#32508;&#21512;&#19981;&#21516;&#35266;&#28857;&#65292;&#20174;&#32780;&#24471;&#20986;&#22522;&#20110;&#31185;&#23398;&#12289;&#20107;&#23454;&#30340;&#35780;&#20272;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Climinator&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#26680;&#26597;&#27668;&#20505;&#21464;&#21270;&#22768;&#26126;&#30340;&#20107;&#23454;&#12290;Climinator&#21033;&#29992;&#22823;&#37327;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#24182;&#20197;IPCC&#25253;&#21578;&#21644;&#21516;&#34892;&#35780;&#23457;&#30340;&#31185;&#23398;&#25991;&#29486;&#31561;&#26435;&#23041;&#26469;&#28304;&#20026;&#22522;&#30784;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35843;&#35299;&#32773;-&#20513;&#23548;&#32773;&#26694;&#26550;&#12290;&#35813;&#35774;&#35745;&#20351;Climinator&#33021;&#22815;&#26377;&#25928;&#22320;&#32508;&#21512;&#19981;&#21516;&#30340;&#31185;&#23398;&#35266;&#28857;&#65292;&#20174;&#32780;&#24471;&#20986;&#31283;&#20581;&#30340;&#12289;&#22522;&#20110;&#35777;&#25454;&#30340;&#35780;&#20272;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20174;Climate Feedback&#21644;Skeptical Science&#25910;&#38598;&#30340;&#22768;&#26126;&#36827;&#34892;&#27979;&#35797;&#26102;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#32467;&#21512;&#20855;&#26377;&#27668;&#20505;&#31185;&#23398;&#21542;&#23450;&#35266;&#28857;&#30340;&#20513;&#23548;&#32773;&#26102;&#65292;Climinator&#30340;&#36845;&#20195;&#36777;&#35770;&#36807;&#31243;&#21487;&#38752;&#22320;&#36235;&#21521;&#20110;&#31185;&#23398;&#20849;&#35782;&#65292;&#31361;&#26174;&#20102;&#23427;&#22312;&#23558;&#19981;&#21516;&#35266;&#28857;&#21327;&#35843;&#20026;&#22522;&#20110;&#31185;&#23398;&#12289;&#20107;&#23454;&#24615;&#32467;&#35770;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#34429;&#28982;&#25105;&#20204;&#30340;&#30740;&#31350;&#23384;&#22312;&#19968;&#23450;&#30340;&#38480;&#21046;&#65292;&#24182;&#38656;&#35201;&#35880;&#24910;&#35299;&#37322;&#65292;&#20294;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#37325;&#35201;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents Climinator, a novel AI-based tool designed to automate the fact-checking of climate change claims. Utilizing an array of Large Language Models (LLMs) informed by authoritative sources like the IPCC reports and peer-reviewed scientific literature, Climinator employs an innovative Mediator-Advocate framework. This design allows Climinator to effectively synthesize varying scientific perspectives, leading to robust, evidence-based evaluations. Our model demonstrates remarkable accuracy when testing claims collected from Climate Feedback and Skeptical Science. Notably, when integrating an advocate with a climate science denial perspective in our framework, Climinator's iterative debate process reliably converges towards scientific consensus, underscoring its adeptness at reconciling diverse viewpoints into science-based, factual conclusions. While our research is subject to certain limitations and necessitates careful interpretation, our approach holds significant poten
&lt;/p&gt;</description></item><item><title>DREditor&#26159;&#19968;&#31181;&#26102;&#38388;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#26657;&#20934;&#29616;&#26377;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#36755;&#20986;&#23884;&#20837;&#65292;&#20351;&#29992;&#32447;&#24615;&#26144;&#23556;&#21644;&#32534;&#36753;&#25805;&#20316;&#31526;&#26469;&#32534;&#36753;&#21305;&#37197;&#35268;&#21017;&#65292;&#20197;&#25552;&#39640;&#29305;&#23450;&#39046;&#22495;&#30340;&#26816;&#32034;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.12540</link><description>&lt;p&gt;
DREditor:&#19968;&#31181;&#29992;&#20110;&#26500;&#24314;&#29305;&#23450;&#39046;&#22495;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#26102;&#38388;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DREditor: An Time-efficient Approach for Building a Domain-specific Dense Retrieval Model. (arXiv:2401.12540v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12540
&lt;/p&gt;
&lt;p&gt;
DREditor&#26159;&#19968;&#31181;&#26102;&#38388;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#26657;&#20934;&#29616;&#26377;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#36755;&#20986;&#23884;&#20837;&#65292;&#20351;&#29992;&#32447;&#24615;&#26144;&#23556;&#21644;&#32534;&#36753;&#25805;&#20316;&#31526;&#26469;&#32534;&#36753;&#21305;&#37197;&#35268;&#21017;&#65292;&#20197;&#25552;&#39640;&#29305;&#23450;&#39046;&#22495;&#30340;&#26816;&#32034;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#21516;&#34892;&#19994;&#20013;&#65292;&#26377;&#25928;&#37096;&#32626;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#36825;&#22312;&#20225;&#19994;&#25628;&#32034;&#26381;&#21153;&#20013;&#23588;&#20026;&#22914;&#27492;&#65292;&#22240;&#20026;&#38656;&#27714;&#19981;&#21516;&#39046;&#22495;&#30340;&#19981;&#21516;&#20225;&#19994;&#30340;&#26102;&#38388;&#38656;&#27714;&#19981;&#21516;&#12290;&#20986;&#20110;&#36825;&#20010;&#21407;&#22240;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;DREditor&#30340;&#26102;&#38388;&#39640;&#25928;&#26041;&#27861;&#65292;&#29992;&#20110;&#32534;&#36753;&#29616;&#25104;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#21305;&#37197;&#35268;&#21017;&#65292;&#20197;&#36866;&#24212;&#29305;&#23450;&#39046;&#22495;&#12290;&#36890;&#36807;&#30452;&#25509;&#26657;&#20934;&#27169;&#22411;&#30340;&#36755;&#20986;&#23884;&#20837;&#65292;&#20351;&#29992;&#19968;&#31181;&#39640;&#25928;&#32780;&#26377;&#25928;&#30340;&#32447;&#24615;&#26144;&#23556;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#36825;&#31181;&#26144;&#23556;&#26159;&#30001;&#35299;&#20915;&#19968;&#20010;&#29305;&#27530;&#26500;&#24314;&#30340;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#33719;&#24471;&#30340;&#32534;&#36753;&#25805;&#20316;&#31526;&#39537;&#21160;&#30340;&#12290;&#19982;&#36890;&#36807;&#38271;&#26102;&#38388;&#24494;&#35843;&#36827;&#34892;&#38544;&#24335;&#35268;&#21017;&#20462;&#25913;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DREditor&#22312;&#19981;&#21516;&#30340;&#29305;&#23450;&#39046;&#22495;&#25968;&#25454;&#38598;&#12289;&#25968;&#25454;&#38598;&#26469;&#28304;&#12289;&#26816;&#32034;&#27169;&#22411;&#21644;&#35745;&#31639;&#35774;&#22791;&#19978;&#37117;&#25552;&#20379;&#20102;&#26174;&#33879;&#20248;&#21183;&#12290;&#23427;&#22987;&#32456;&#21487;&#20197;&#25552;&#39640;&#26102;&#38388;&#25928;&#29575;100-300&#20493;&#65292;&#21516;&#26102;&#20445;&#25345;&#19981;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deploying dense retrieval models efficiently is becoming increasingly important across various industries. This is especially true for enterprise search services, where customizing search engines to meet the time demands of different enterprises in different domains is crucial. Motivated by this, we develop a time-efficient approach called DREditor to edit the matching rule of an off-the-shelf dense retrieval model to suit a specific domain. This is achieved by directly calibrating the output embeddings of the model using an efficient and effective linear mapping. This mapping is powered by an edit operator that is obtained by solving a specially constructed least squares problem. Compared to implicit rule modification via long-time finetuning, our experimental results show that DREditor provides significant advantages on different domain-specific datasets, dataset sources, retrieval models, and computing devices. It consistently enhances time efficiency by 100-300 times while maintain
&lt;/p&gt;</description></item><item><title>BiTA&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21452;&#21521;&#35843;&#25972;&#23454;&#29616;&#20102;&#26080;&#25439;&#21152;&#36895;&#12290;&#23427;&#37319;&#29992;&#31616;&#21270;&#30340;&#21322;&#33258;&#22238;&#24402;&#29983;&#25104;&#21644;&#33609;&#31295;&#39564;&#35777;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#22522;&#20110;&#26641;&#30340;&#35299;&#30721;&#21516;&#26102;&#36827;&#34892;&#20505;&#36873;&#29983;&#25104;&#21644;&#39564;&#35777;&#65292;&#25552;&#39640;&#20102;&#25512;&#29702;&#25928;&#29575;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#36741;&#21161;&#27169;&#22411;&#25110;&#26174;&#33879;&#30340;&#39069;&#22806;&#20869;&#23384;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2401.12522</link><description>&lt;p&gt;
BiTA: &#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#26080;&#25439;&#21152;&#36895;&#30340;&#21452;&#21521;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
BiTA: Bi-Directional Tuning for Lossless Acceleration in Large Language Models. (arXiv:2401.12522v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12522
&lt;/p&gt;
&lt;p&gt;
BiTA&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21452;&#21521;&#35843;&#25972;&#23454;&#29616;&#20102;&#26080;&#25439;&#21152;&#36895;&#12290;&#23427;&#37319;&#29992;&#31616;&#21270;&#30340;&#21322;&#33258;&#22238;&#24402;&#29983;&#25104;&#21644;&#33609;&#31295;&#39564;&#35777;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#22522;&#20110;&#26641;&#30340;&#35299;&#30721;&#21516;&#26102;&#36827;&#34892;&#20505;&#36873;&#29983;&#25104;&#21644;&#39564;&#35777;&#65292;&#25552;&#39640;&#20102;&#25512;&#29702;&#25928;&#29575;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#36741;&#21161;&#27169;&#22411;&#25110;&#26174;&#33879;&#30340;&#39069;&#22806;&#20869;&#23384;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20351;&#29992;&#33258;&#22238;&#24402;&#29983;&#25104;&#65292;&#23548;&#33268;&#39640;&#20869;&#23384;&#24102;&#23485;&#38656;&#27714;&#21644;&#24310;&#36831;&#24310;&#38271;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#31181;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#8212;&#8212;&#21452;&#21521;&#35843;&#25972;&#20197;&#23454;&#29616;&#26080;&#25439;&#21152;&#36895;&#65288;BiTA&#65289;&#65292;&#36890;&#36807;&#31616;&#21270;&#30340;&#21322;&#33258;&#22238;&#24402;&#29983;&#25104;&#21644;&#33609;&#31295;&#39564;&#35777;&#26469;&#21152;&#36895;LLMs&#12290;&#21463;&#21551;&#21457;&#20110;&#25552;&#31034;&#35843;&#25972;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#35774;&#35745;&#65292;&#31216;&#20026;&#21452;&#21521;&#35843;&#25972;&#65292;&#26469;&#22686;&#24378;LLMs&#22312;&#21322;&#33258;&#22238;&#24402;&#29983;&#25104;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#37319;&#29992;&#39640;&#25928;&#30340;&#22522;&#20110;&#26641;&#30340;&#35299;&#30721;&#65292;&#27169;&#22411;&#21487;&#20197;&#21516;&#26102;&#36827;&#34892;&#33609;&#31295;&#20505;&#36873;&#29983;&#25104;&#21644;&#39564;&#35777;&#65292;&#30830;&#20445;&#36755;&#20986;&#32467;&#26524;&#19982;&#23427;&#20204;&#30340;&#33258;&#22238;&#24402;&#23545;&#24212;&#29289;&#22312;&#36138;&#23146;&#25277;&#26679;&#19979;&#23436;&#20840;&#30456;&#21516;&#12290;BiTA&#20316;&#20026;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#25554;&#20214;&#27169;&#22359;&#65292;&#21487;&#20197;&#26080;&#32541;&#22686;&#24378;&#29616;&#26377;LLMs&#30340;&#25512;&#29702;&#25928;&#29575;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#36741;&#21161;&#27169;&#22411;&#25110;&#25215;&#25285;&#26174;&#33879;&#30340;&#39069;&#22806;&#20869;&#23384;&#24320;&#38144;&#12290;&#36890;&#36807;&#24212;&#29992;&#25552;&#20986;&#30340;BiTA&#65292;LLaMA-2-70B-Chat&#23454;&#29616;&#20102;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) commonly employ autoregressive generation during inference, leading to high memory bandwidth demand and consequently extended latency. To mitigate this inefficiency, we present Bi-directional Tuning for lossless Acceleration (BiTA), an innovative method expediting LLMs via streamlined semi-autoregressive generation and draft verification. Inspired by the concept of prompt tuning, we enhance LLMs with a parameter-efficient design called bi-directional tuning for the capability in semi-autoregressive generation. Employing efficient tree-based decoding, the models perform draft candidate generation and verification in parallel, ensuring outputs identical to their autoregressive counterparts under greedy sampling. BiTA serves as a lightweight plug-in module, seamlessly boosting the inference efficiency of existing LLMs without requiring additional assistance models or incurring significant extra memory costs. Applying the proposed BiTA, LLaMA-2-70B-Chat achieve
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#38271;&#25991;&#26412;&#20998;&#31867;&#21644;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23884;&#20837;&#25216;&#26415;&#23545;&#38271;&#25991;&#26412;&#36827;&#34892;&#21387;&#32553;&#65292;&#28982;&#21518;&#37319;&#29992;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#26469;&#33258;Transformers&#30340;&#23884;&#20837;&#26041;&#27861;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#35757;&#32451;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#22312;&#20248;&#24800;&#36152;&#26131;&#21327;&#23450;&#30340;&#38271;&#25991;&#26412;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.12520</link><description>&lt;p&gt;
&#23545;&#20248;&#24800;&#36152;&#26131;&#21327;&#23450;&#30340;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20869;&#23481;&#36827;&#34892;&#20851;&#38190;&#20449;&#24687;&#26816;&#32034;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Key Information Retrieval to Classify the Unstructured Data Content of Preferential Trade Agreements. (arXiv:2401.12520v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#38271;&#25991;&#26412;&#20998;&#31867;&#21644;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23884;&#20837;&#25216;&#26415;&#23545;&#38271;&#25991;&#26412;&#36827;&#34892;&#21387;&#32553;&#65292;&#28982;&#21518;&#37319;&#29992;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#26469;&#33258;Transformers&#30340;&#23884;&#20837;&#26041;&#27861;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#35757;&#32451;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#22312;&#20248;&#24800;&#36152;&#26131;&#21327;&#23450;&#30340;&#38271;&#25991;&#26412;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25991;&#26412;&#25968;&#25454;&#30340;&#36805;&#36895;&#22686;&#38271;&#65292;&#39044;&#27979;&#38271;&#25991;&#26412;&#24050;&#32463;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;&#25991;&#26412;&#39044;&#27979;&#26041;&#27861;&#22312;&#22788;&#29702;&#38271;&#25991;&#26412;&#26102;&#36935;&#21040;&#22256;&#38590;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#20887;&#20313;&#21644;&#26080;&#20851;&#20449;&#24687;&#30340;&#23384;&#22312;&#65292;&#36825;&#24433;&#21709;&#20102;&#27169;&#22411;&#20174;&#25991;&#26412;&#20013;&#25429;&#25417;&#37325;&#35201;&#35265;&#35299;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38271;&#25991;&#26412;&#20998;&#31867;&#21644;&#39044;&#27979;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#37319;&#29992;&#23884;&#20837;&#25216;&#26415;&#26469;&#23545;&#38271;&#25991;&#26412;&#36827;&#34892;&#21387;&#32553;&#65292;&#20197;&#20943;&#23569;&#20854;&#20013;&#30340;&#20887;&#20313;&#20449;&#24687;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#26469;&#33258;Transformers&#65288;BERT&#65289;&#30340;&#23884;&#20837;&#26041;&#27861;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20248;&#24800;&#36152;&#26131;&#21327;&#23450;&#30340;&#38271;&#25991;&#26412;&#20998;&#31867;&#26041;&#38754;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23884;&#20837;&#26041;&#27861;&#23545;&#25991;&#26412;&#36827;&#34892;&#21387;&#32553;&#19981;&#20165;&#22686;&#24378;&#20102;&#39044;&#27979;&#34920;&#29616;&#65292;&#32780;&#19988;&#26377;&#21161;&#20110;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid proliferation of textual data, predicting long texts has emerged as a significant challenge in the domain of natural language processing. Traditional text prediction methods encounter substantial difficulties when grappling with long texts, primarily due to the presence of redundant and irrelevant information, which impedes the model's capacity to capture pivotal insights from the text. To address this issue, we introduce a novel approach to long-text classification and prediction. Initially, we employ embedding techniques to condense the long texts, aiming to diminish the redundancy therein. Subsequently,the Bidirectional Encoder Representations from Transformers (BERT) embedding method is utilized for text classification training. Experimental outcomes indicate that our method realizes considerable performance enhancements in classifying long texts of Preferential Trade Agreements. Furthermore, the condensation of text through embedding methods not only augments predic
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20197;&#32676;&#20307;&#23646;&#24615;&#12289;&#20010;&#20307;&#29992;&#25143;&#21644;&#32452;&#21512;&#26041;&#27861;&#26469;&#27169;&#25311;&#20154;&#30340;&#19978;&#19979;&#25991;&#12290;&#21512;&#24182;&#32676;&#20307;&#21644;&#20010;&#20307;&#29305;&#24449;&#26174;&#33879;&#25552;&#39640;&#20102;&#29992;&#25143;&#32423;&#22238;&#24402;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#32780;&#27169;&#25311;&#20010;&#20307;&#29992;&#25143;&#21017;&#26174;&#33879;&#25552;&#39640;&#20102;&#21333;&#20010;&#25991;&#26723;&#32423;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.12492</link><description>&lt;p&gt;
&#27604;&#36739;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35821;&#35328;&#24314;&#27169;&#65306;&#27169;&#25311;&#32676;&#20307;&#12289;&#20010;&#20307;&#29305;&#28857;&#36824;&#26159;&#20004;&#32773;&#20860;&#39038;&#65311;
&lt;/p&gt;
&lt;p&gt;
Comparing Human-Centered Language Modeling: Is it Better to Model Groups, Individual Traits, or Both?. (arXiv:2401.12492v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20197;&#32676;&#20307;&#23646;&#24615;&#12289;&#20010;&#20307;&#29992;&#25143;&#21644;&#32452;&#21512;&#26041;&#27861;&#26469;&#27169;&#25311;&#20154;&#30340;&#19978;&#19979;&#25991;&#12290;&#21512;&#24182;&#32676;&#20307;&#21644;&#20010;&#20307;&#29305;&#24449;&#26174;&#33879;&#25552;&#39640;&#20102;&#29992;&#25143;&#32423;&#22238;&#24402;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#32780;&#27169;&#25311;&#20010;&#20307;&#29992;&#25143;&#21017;&#26174;&#33879;&#25552;&#39640;&#20102;&#21333;&#20010;&#25991;&#26723;&#32423;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#23558;&#20154;&#30340;&#19978;&#19979;&#25991;&#32435;&#20837;&#20854;&#27169;&#22411;&#20013;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#20351;&#29992;&#32676;&#20307;&#23646;&#24615;&#65288;&#22914;45&#23681;&#20197;&#19978;&#30340;&#20154;&#32676;&#65289;&#36824;&#26159;&#27169;&#25311;&#20010;&#20307;&#20154;&#29289;&#26356;&#26377;&#25928;&#30340;&#38382;&#39064;&#23578;&#26410;&#30830;&#23450;&#12290;&#32676;&#20307;&#23646;&#24615;&#22312;&#25216;&#26415;&#19978;&#26356;&#23481;&#26131;&#23454;&#29616;&#65292;&#20294;&#26159;&#36807;&#20110;&#31895;&#31961;&#65306;&#24182;&#38750;&#25152;&#26377;45&#23681;&#20197;&#19978;&#30340;&#20154;&#37117;&#20197;&#30456;&#21516;&#30340;&#26041;&#24335;&#20070;&#20889;&#12290;&#30456;&#21453;&#65292;&#27169;&#25311;&#20010;&#20307;&#20154;&#29289;&#33021;&#22815;&#25429;&#25417;&#27599;&#20010;&#20154;&#36523;&#20221;&#30340;&#22797;&#26434;&#24615;&#65292;&#20801;&#35768;&#26356;&#20010;&#24615;&#21270;&#30340;&#34920;&#31034;&#65292;&#20294;&#25105;&#20204;&#21487;&#33021;&#38656;&#35201;&#27169;&#25311;&#26080;&#38480;&#25968;&#37327;&#30340;&#29992;&#25143;&#24182;&#19988;&#38656;&#35201;&#21487;&#33021;&#26080;&#27861;&#33719;&#21462;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#36890;&#36807;&#32676;&#20307;&#23646;&#24615;&#12289;&#20010;&#20307;&#29992;&#25143;&#21644;&#32452;&#21512;&#26041;&#27861;&#26469;&#27169;&#25311;&#20154;&#30340;&#19978;&#19979;&#25991;&#12290;&#23558;&#32676;&#20307;&#21644;&#20010;&#20307;&#29305;&#24449;&#32467;&#21512;&#36215;&#26469;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22522;&#20110;&#29992;&#25143;&#25991;&#26723;&#30340;&#29992;&#25143;&#32423;&#22238;&#24402;&#20219;&#21153;&#65288;&#22914;&#24180;&#40836;&#20272;&#35745;&#25110;&#20010;&#24615;&#35780;&#20272;&#65289;&#30340;&#24615;&#33021;&#12290;&#27169;&#25311;&#20010;&#20307;&#29992;&#25143;&#26174;&#33879;&#25552;&#39640;&#20102;&#21333;&#20010;&#25991;&#26723;&#32423;&#20998;&#31867;&#20219;&#21153;&#65288;&#22914;&#31435;&#22330;&#21644;&#20027;&#39064;&#26816;&#27979;&#65289;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language processing has made progress in incorporating human context into its models, but whether it is more effective to use group-wise attributes (e.g., over-45-year-olds) or model individuals remains open. Group attributes are technically easier but coarse: not all 45-year-olds write the same way. In contrast, modeling individuals captures the complexity of each person's identity. It allows for a more personalized representation, but we may have to model an infinite number of users and require data that may be impossible to get. We compare modeling human context via group attributes, individual users, and combined approaches. Combining group and individual features significantly benefits user-level regression tasks like age estimation or personality assessment from a user's documents. Modeling individual users significantly improves the performance of single document-level classification tasks like stance and topic detection. We also find that individual-user modeling does w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21019;&#36896;&#21147;&#27700;&#24179;&#30340;&#39640;&#25928;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#35780;&#20272;&#26041;&#27861;&#21644;&#19968;&#20010;&#21253;&#21547;700&#20010;&#38382;&#39064;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#12290;&#30740;&#31350;&#21457;&#29616;&#21019;&#36896;&#21147;&#27700;&#24179;&#21463;&#21040;&#20219;&#21153;&#24046;&#24322;&#21644;LLM&#27169;&#22411;&#21442;&#25968;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.12491</link><description>&lt;p&gt;
&#35780;&#20272;&#21644;&#29702;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21019;&#36896;&#21147;
&lt;/p&gt;
&lt;p&gt;
Assessing and Understanding Creativity in Large Language Models. (arXiv:2401.12491v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21019;&#36896;&#21147;&#27700;&#24179;&#30340;&#39640;&#25928;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#35780;&#20272;&#26041;&#27861;&#21644;&#19968;&#20010;&#21253;&#21547;700&#20010;&#38382;&#39064;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#12290;&#30740;&#31350;&#21457;&#29616;&#21019;&#36896;&#21147;&#27700;&#24179;&#21463;&#21040;&#20219;&#21153;&#24046;&#24322;&#21644;LLM&#27169;&#22411;&#21442;&#25968;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;LLMs&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#39640;&#27700;&#24179;&#30340;&#21019;&#36896;&#21147;&#65292;&#20294;&#35780;&#20272;&#36825;&#31181;&#21019;&#36896;&#21147;&#30340;&#26041;&#27861;&#23578;&#19981;&#23436;&#21892;&#12290;&#35780;&#20272;LLM&#30340;&#21019;&#36896;&#21147;&#38656;&#35201;&#32771;&#34385;&#19982;&#20154;&#31867;&#30340;&#24046;&#24322;&#65292;&#38656;&#35201;&#36827;&#34892;&#22810;&#32500;&#24230;&#30340;&#27979;&#37327;&#65292;&#21516;&#26102;&#24179;&#34913;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#26412;&#25991;&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#35780;&#20272;LLM&#21019;&#36896;&#21147;&#27700;&#24179;&#30340;&#39640;&#25928;&#26694;&#26550;&#12290;&#36890;&#36807;&#25913;&#36827;&#30340;&#25176;&#20848;&#26031;&#21019;&#36896;&#24615;&#24605;&#32500;&#27979;&#35797;&#30340;&#25913;&#32534;&#65292;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#21508;&#31181;LLM&#22312;7&#20010;&#20219;&#21153;&#20013;&#30340;&#21019;&#36896;&#24615;&#34920;&#29616;&#65292;&#24378;&#35843;&#20102;&#27969;&#30021;&#24230;&#12289;&#28789;&#27963;&#24615;&#12289;&#29420;&#21019;&#24615;&#21644;&#20016;&#23500;&#24615;&#31561;4&#20010;&#26631;&#20934;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;700&#20010;&#38382;&#39064;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#27979;&#35797;&#21644;&#22522;&#20110;LLM&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#23545;LLM&#23545;&#21508;&#31181;&#25552;&#31034;&#21644;&#35282;&#33394;&#25198;&#28436;&#24773;&#22659;&#30340;&#21453;&#24212;&#36827;&#34892;&#20102;&#26032;&#39062;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#21457;&#29616;&#21019;&#36896;&#21147;&#30340;&#27700;&#24179;&#21462;&#20915;&#20110;&#20219;&#21153;&#30340;&#19981;&#21516;&#65292;&#21516;&#26102;&#20063;&#21463;&#21040;LLM&#30340;&#27169;&#22411;&#21644;&#21442;&#25968;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of natural language processing, the rapid development of large language model (LLM) has attracted more and more attention. LLMs have shown a high level of creativity in various tasks, but the methods for assessing such creativity are inadequate. The assessment of LLM creativity needs to consider differences from humans, requiring multi-dimensional measurement while balancing accuracy and efficiency. This paper aims to establish an efficient framework for assessing the level of creativity in LLMs. By adapting the modified Torrance Tests of Creative Thinking, the research evaluates the creative performance of various LLMs across 7 tasks, emphasizing 4 criteria including Fluency, Flexibility, Originality, and Elaboration. In this context, we develop a comprehensive dataset of 700 questions for testing and an LLM-based evaluation method. In addition, this study presents a novel analysis of LLMs' responses to diverse prompts and role-play situations. We found that the creativit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Ditto&#30340;&#35282;&#33394;&#25198;&#28436;&#33258;&#25105;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35282;&#33394;&#30693;&#35782;&#30340;&#21033;&#29992;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#27169;&#25311;&#35282;&#33394;&#25198;&#28436;&#23545;&#35805;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#35282;&#33394;&#25198;&#28436;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;Ditto&#22312;&#35282;&#33394;&#25198;&#28436;&#22522;&#20934;&#21644;MT-Bench&#30340;&#35780;&#20272;&#20013;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.12474</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#25152;&#26377;&#23383;&#31526;&#30340;&#21472;&#21152;&#65306;&#36890;&#36807;&#33258;&#25105;&#23545;&#40784;&#23454;&#29616;&#20219;&#24847;&#35282;&#33394;&#25198;&#28436;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Superpositions of All Characters: Attaining Arbitrary Role-play via Self-Alignment. (arXiv:2401.12474v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Ditto&#30340;&#35282;&#33394;&#25198;&#28436;&#33258;&#25105;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35282;&#33394;&#30693;&#35782;&#30340;&#21033;&#29992;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#27169;&#25311;&#35282;&#33394;&#25198;&#28436;&#23545;&#35805;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#35282;&#33394;&#25198;&#28436;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;Ditto&#22312;&#35282;&#33394;&#25198;&#28436;&#22522;&#20934;&#21644;MT-Bench&#30340;&#35780;&#20272;&#20013;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#30340;&#24037;&#20316;&#24050;&#32463;&#25237;&#20837;&#21040;&#36890;&#36807;&#27169;&#25311;&#19987;&#26377;&#23545;&#25163;&#26469;&#22686;&#24378;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35282;&#33394;&#25198;&#28436;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;LLMs&#26412;&#36136;&#19978;&#20855;&#26377;&#35282;&#33394;&#25198;&#28436;&#33021;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#24191;&#27867;&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#34164;&#21547;&#20102;&#23545;&#35282;&#33394;&#21644;&#28508;&#22312;&#23545;&#35805;&#30340;&#24191;&#27867;&#20102;&#35299;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Ditto&#65292;&#19968;&#31181;&#29992;&#20110;&#35282;&#33394;&#25198;&#28436;&#30340;&#33258;&#25105;&#23545;&#40784;&#26041;&#27861;&#12290;Ditto&#21033;&#29992;&#35282;&#33394;&#30693;&#35782;&#65292;&#40723;&#21169;LLM&#25353;&#29031;&#25351;&#20196;&#27169;&#25311;&#35282;&#33394;&#25198;&#28436;&#23545;&#35805;&#65292;&#20316;&#20026;&#38405;&#35835;&#29702;&#35299;&#30340;&#19968;&#31181;&#21464;&#20307;&#12290;&#35813;&#26041;&#27861;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;4000&#20010;&#23383;&#31526;&#30340;&#35282;&#33394;&#25198;&#28436;&#35757;&#32451;&#38598;&#65292;&#30456;&#23545;&#20110;&#30446;&#21069;&#21487;&#29992;&#25968;&#25454;&#38598;&#30340;&#35282;&#33394;&#25968;&#37327;&#22686;&#21152;&#20102;&#21313;&#20493;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#33258;&#21160;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#23545;LLM&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#22686;&#24378;&#20854;&#35282;&#33394;&#25198;&#28436;&#33021;&#21147;&#12290;&#36890;&#36807;&#35780;&#20272;&#25105;&#20204;&#31934;&#24515;&#26500;&#24314;&#19988;&#21487;&#22797;&#21046;&#30340;&#35282;&#33394;&#25198;&#28436;&#22522;&#20934;&#21644;MT-Bench&#30340;&#35282;&#33394;&#25198;&#28436;&#23376;&#38598;&#65292;Ditto&#22312;&#21508;&#39033;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Considerable efforts have been invested in augmenting the role-playing proficiency of open-source large language models (LLMs) by emulating proprietary counterparts. Nevertheless, we posit that LLMs inherently harbor role-play capabilities, owing to the extensive knowledge of characters and potential dialogues ingrained in their vast training corpora. Thus, in this study, we introduce Ditto, a self-alignment method for role-play. Ditto capitalizes on character knowledge, encouraging an instruction-following LLM to simulate role-play dialogues as a variant of reading comprehension. This method creates a role-play training set comprising 4,000 characters, surpassing the scale of currently available datasets by tenfold regarding the number of roles. Subsequently, we fine-tune the LLM using this self-generated dataset to augment its role-playing capabilities. Upon evaluating our meticulously constructed and reproducible role-play benchmark and the roleplay subset of MT-Bench, Ditto, in var
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#36866;&#29992;&#20110;&#33976;&#39311;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23545;DistilBERT&#27169;&#22411;&#36827;&#34892;&#25913;&#36827;&#65292;&#22312;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#19988;&#29983;&#25104;&#30340;DistilFace&#27169;&#22411;&#20855;&#26377;&#36731;&#37327;&#32423;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2401.12472</link><description>&lt;p&gt;
&#23545;&#20110;&#33976;&#39311;&#27169;&#22411;&#20013;&#30340;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Contrastive Learning in Distilled Models. (arXiv:2401.12472v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#36866;&#29992;&#20110;&#33976;&#39311;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23545;DistilBERT&#27169;&#22411;&#36827;&#34892;&#25913;&#36827;&#65292;&#22312;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#19988;&#29983;&#25104;&#30340;DistilFace&#27169;&#22411;&#20855;&#26377;&#36731;&#37327;&#32423;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20687;BERT&#36825;&#26679;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#33021;&#22815;&#20026;&#19979;&#28216;NLP&#20219;&#21153;&#25552;&#20379;&#26368;&#20808;&#36827;&#30340;&#35789;&#23884;&#20837;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23578;&#26410;&#22312;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#21487;&#33021;&#36807;&#20110;&#24222;&#22823;&#65292;&#26080;&#27861;&#37096;&#32626;&#20026;&#36731;&#37327;&#32423;&#36793;&#32536;&#24212;&#29992;&#31243;&#24207;&#12290;&#25105;&#20204;&#24076;&#26395;&#23558;&#19968;&#31181;&#21512;&#36866;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#27169;&#22411;DistilBERT&#65292;&#24182;&#26681;&#25454;SimCSE&#35770;&#25991;&#23545;&#27169;&#22411;&#26550;&#26500;&#36827;&#34892;&#25913;&#36827;&#65292;&#20197;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#26368;&#32456;&#30340;&#36731;&#37327;&#32423;&#27169;&#22411;DistilFace&#22312;STS&#20219;&#21153;&#19978;&#30340;Spearman&#30456;&#20851;&#24615;&#24179;&#22343;&#36798;&#21040;72.1&#65292;&#27604;BERT base&#25552;&#39640;&#20102;34.2&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural Language Processing models like BERT can provide state-of-the-art word embeddings for downstream NLP tasks. However, these models yet to perform well on Semantic Textual Similarity, and may be too large to be deployed as lightweight edge applications. We seek to apply a suitable contrastive learning method based on the SimCSE paper, to a model architecture adapted from a knowledge distillation based model, DistilBERT, to address these two issues. Our final lightweight model DistilFace achieves an average of 72.1 in Spearman's correlation on STS tasks, a 34.2 percent improvement over BERT base.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#23545;&#25239;&#35757;&#32451;&#65288;FAT&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#22312;&#26080;&#21516;&#20041;&#35789;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21333;&#27493;&#26799;&#24230;&#19978;&#21319;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#29983;&#25104;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#20197;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2401.12461</link><description>&lt;p&gt;
&#24555;&#36895;&#23545;&#25239;&#35757;&#32451;&#19982;&#25991;&#26412;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Fast Adversarial Training against Textual Adversarial Attacks. (arXiv:2401.12461v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#23545;&#25239;&#35757;&#32451;&#65288;FAT&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#22312;&#26080;&#21516;&#20041;&#35789;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21333;&#27493;&#26799;&#24230;&#19978;&#21319;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#29983;&#25104;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#20197;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#23545;&#25239;&#24615;&#38450;&#24481;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#65292;&#20197;&#22686;&#24378;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#39044;&#35774;&#35821;&#35328;&#30693;&#35782;&#65292;&#24182;&#20551;&#35774;&#25915;&#20987;&#32773;&#20351;&#29992;&#30340;&#21516;&#20041;&#35789;&#20505;&#36873;&#35789;&#26159;&#21487;&#35775;&#38382;&#30340;&#65292;&#36825;&#26159;&#19968;&#20010;&#29702;&#24819;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#24182;&#20174;&#21333;&#27493;&#25200;&#21160;&#29983;&#25104;&#21644;&#25200;&#21160;&#21021;&#22987;&#21270;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#23545;&#25239;&#35757;&#32451;&#65288;FAT&#65289;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#22312;&#26080;&#21516;&#20041;&#35789;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#22522;&#20110;&#21333;&#27493;&#21644;&#22810;&#27493;&#26799;&#24230;&#19978;&#21319;&#29983;&#25104;&#30340;&#23545;&#25239;&#25200;&#21160;&#30456;&#20284;&#30340;&#35266;&#23519;&#65292;FAT&#20351;&#29992;&#21333;&#27493;&#26799;&#24230;&#19978;&#21319;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#29983;&#25104;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#20197;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#12290;&#22522;&#20110;&#36830;&#32493;&#32426;&#20803;&#20013;&#21516;&#19968;&#35757;&#32451;&#26679;&#26412;&#19978;&#29983;&#25104;&#30340;&#25200;&#21160;&#30456;&#20284;&#30340;&#35266;&#23519;&#65292;FAT&#20805;&#20998;&#21033;&#29992;&#21382;&#21490;&#20449;&#24687;&#26469;&#21021;&#22987;&#21270;&#25200;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many adversarial defense methods have been proposed to enhance the adversarial robustness of natural language processing models. However, most of them introduce additional pre-set linguistic knowledge and assume that the synonym candidates used by attackers are accessible, which is an ideal assumption. We delve into adversarial training in the embedding space and propose a Fast Adversarial Training (FAT) method to improve the model robustness in the synonym-unaware scenario from the perspective of single-step perturbation generation and perturbation initialization. Based on the observation that the adversarial perturbations crafted by single-step and multi-step gradient ascent are similar, FAT uses single-step gradient ascent to craft adversarial examples in the embedding space to expedite the training process. Based on the observation that the perturbations generated on the identical training sample in successive epochs are similar, FAT fully utilizes historical information when initi
&lt;/p&gt;</description></item><item><title>CIM-MLC&#26159;&#19968;&#20010;&#38754;&#21521;&#35745;&#31639;&#20869;&#23384;&#21152;&#36895;&#22120;&#30340;&#22810;&#32423;&#32534;&#35793;&#26632;&#65292;&#20026;&#20102;&#25903;&#25345;&#21508;&#31181;CIM&#26550;&#26500;&#30340;&#28508;&#21147;&#65292;&#35813;&#32534;&#35793;&#26632;&#23436;&#20840;&#20102;&#35299;CIM&#26550;&#26500;&#32454;&#33410;&#21644;&#23454;&#29616;&#22810;&#26679;&#24615;&#65292;&#24182;&#25552;&#20379;&#28789;&#27963;&#24615;&#26469;&#25903;&#25345;&#20855;&#26377;&#19981;&#21516;&#35745;&#31639;&#31890;&#24230;&#30340;CIM&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2401.12428</link><description>&lt;p&gt;
CIM-MLC:&#38754;&#21521;&#35745;&#31639;&#20869;&#23384;&#21152;&#36895;&#22120;&#30340;&#22810;&#32423;&#32534;&#35793;&#26632;
&lt;/p&gt;
&lt;p&gt;
CIM-MLC: A Multi-level Compilation Stack for Computing-In-Memory Accelerators. (arXiv:2401.12428v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12428
&lt;/p&gt;
&lt;p&gt;
CIM-MLC&#26159;&#19968;&#20010;&#38754;&#21521;&#35745;&#31639;&#20869;&#23384;&#21152;&#36895;&#22120;&#30340;&#22810;&#32423;&#32534;&#35793;&#26632;&#65292;&#20026;&#20102;&#25903;&#25345;&#21508;&#31181;CIM&#26550;&#26500;&#30340;&#28508;&#21147;&#65292;&#35813;&#32534;&#35793;&#26632;&#23436;&#20840;&#20102;&#35299;CIM&#26550;&#26500;&#32454;&#33410;&#21644;&#23454;&#29616;&#22810;&#26679;&#24615;&#65292;&#24182;&#25552;&#20379;&#28789;&#27963;&#24615;&#26469;&#25903;&#25345;&#20855;&#26377;&#19981;&#21516;&#35745;&#31639;&#31890;&#24230;&#30340;CIM&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#21508;&#31181;&#35745;&#31639;&#20869;&#23384;(CIM)&#22788;&#29702;&#22120;&#34987;&#25552;&#20986;&#65292;&#23637;&#31034;&#20986;&#20248;&#20110;&#20256;&#32479;&#26550;&#26500;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#37322;&#25918;&#21508;&#31181;CIM&#26550;&#26500;&#30340;&#28508;&#21147;&#65292;&#22914;&#35774;&#22791;&#31934;&#24230;&#12289;&#20132;&#21449;&#26639;&#22823;&#23567;&#21644;&#20132;&#21449;&#26639;&#25968;&#37327;&#65292;&#26377;&#24517;&#35201;&#24320;&#21457;&#23436;&#20840;&#20102;&#35299;CIM&#26550;&#26500;&#32454;&#33410;&#21644;&#23454;&#29616;&#22810;&#26679;&#24615;&#30340;&#32534;&#35793;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24403;&#21069;&#27969;&#34892;&#30340;&#24320;&#28304;&#32534;&#35793;&#26632;&#22312;&#26550;&#26500;&#25903;&#25345;&#26041;&#38754;&#30340;&#19981;&#36275;&#65292;&#29616;&#26377;&#30340;CIM&#35774;&#35745;&#35201;&#20040;&#25163;&#21160;&#37096;&#32626;&#32593;&#32476;&#65292;&#35201;&#20040;&#26500;&#24314;&#33258;&#24049;&#30340;&#32534;&#35793;&#22120;&#65292;&#36825;&#26159;&#32791;&#26102;&#32791;&#21147;&#30340;&#12290;&#23613;&#31649;&#19968;&#20123;&#24037;&#20316;&#23558;&#29305;&#23450;&#30340;CIM&#35774;&#22791;&#32534;&#31243;&#25509;&#21475;&#20844;&#24320;&#32473;&#32534;&#35793;&#22120;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#34987;&#32465;&#23450;&#21040;&#22266;&#23450;&#30340;CIM&#26550;&#26500;&#65292;&#32570;&#20047;&#28789;&#27963;&#24615;&#26469;&#25903;&#25345;&#20855;&#26377;&#19981;&#21516;&#35745;&#31639;&#31890;&#24230;&#30340;CIM&#26550;&#26500;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#29616;&#26377;&#30340;&#32534;&#35793;&#24037;&#20316;&#36890;&#24120;&#32771;&#34385;&#20102;&#26377;&#38480;&#30340;&#25805;&#20316;&#31867;&#22411;&#30340;&#35843;&#24230;&#65288;&#20363;&#22914;&#20132;&#21449;&#26639;&#38480;&#21046;&#30340;&#30697;&#38453;-&#21521;&#37327;&#20056;&#27861;&#65289;
&lt;/p&gt;
&lt;p&gt;
In recent years, various computing-in-memory (CIM) processors have been presented, showing superior performance over traditional architectures. To unleash the potential of various CIM architectures, such as device precision, crossbar size, and crossbar number, it is necessary to develop compilation tools that are fully aware of the CIM architectural details and implementation diversity. However, due to the lack of architectural support in current popular open-source compiling stacks, existing CIM designs either manually deploy networks or build their own compilers, which is time-consuming and labor-intensive. Although some works expose the specific CIM device programming interfaces to compilers, they are often bound to a fixed CIM architecture, lacking the flexibility to support the CIM architectures with different computing granularity. On the other hand, existing compilation works usually consider the scheduling of limited operation types (such as crossbar-bound matrix-vector multipl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#39044;&#35757;&#32451;&#25991;&#26412;&#26469;&#34913;&#37327;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#27010;&#24565;&#30340;&#39057;&#29575;&#65292;&#24182;&#21457;&#29616;&#27969;&#34892;&#30340;VLM&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#38271;&#23614;&#27010;&#24565;&#20998;&#24067;&#65292;&#36825;&#19982;&#25353;&#31867;&#21035;&#30340;&#20934;&#30830;&#29575;&#24378;&#28872;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2401.12425</link><description>&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#34987;&#24573;&#35270;&#30340;&#23614;&#37096;
&lt;/p&gt;
&lt;p&gt;
The Neglected Tails of Vision-Language Models. (arXiv:2401.12425v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#39044;&#35757;&#32451;&#25991;&#26412;&#26469;&#34913;&#37327;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#27010;&#24565;&#30340;&#39057;&#29575;&#65292;&#24182;&#21457;&#29616;&#27969;&#34892;&#30340;VLM&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#38271;&#23614;&#27010;&#24565;&#20998;&#24067;&#65292;&#36825;&#19982;&#25353;&#31867;&#21035;&#30340;&#20934;&#30830;&#29575;&#24378;&#28872;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#22312;&#38646;&#26679;&#26412;&#35782;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#35270;&#35273;&#27010;&#24565;&#19978;&#30340;&#34920;&#29616;&#26497;&#19981;&#22343;&#34913;&#12290;&#20363;&#22914;&#65292;&#23613;&#31649;CLIP&#22312;ImageNet&#19978;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24179;&#22343;&#38646;&#26679;&#26412;&#20934;&#30830;&#29575;&#65288;72.7&#65285;&#65289;&#65292;&#20294;&#22312;&#21313;&#20010;&#27010;&#24565;&#65288;&#22914;gyromitra&#21644;night snake&#65289;&#19978;&#30340;&#20934;&#30830;&#29575;&#19981;&#21040;10&#65285;&#65292;&#36825;&#21487;&#33021;&#26159;&#22240;&#20026;&#36825;&#20123;&#27010;&#24565;&#22312;VLM&#30340;&#38750;&#22343;&#34913;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#34920;&#31034;&#19981;&#36275;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#36825;&#31181;&#19981;&#24179;&#34913;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#22312;VLM&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#35745;&#31639;&#29305;&#23450;&#27010;&#24565;&#30340;&#39057;&#29575;&#26159;&#38750;&#24120;&#22797;&#26434;&#30340;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;&#20998;&#26512;&#39044;&#35757;&#32451;&#25991;&#26412;&#26469;&#27979;&#37327;&#27010;&#24565;&#39057;&#29575;&#12290;&#25105;&#20204;&#21033;&#29992;&#29616;&#25104;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#24110;&#21161;&#35745;&#31639;&#21253;&#21547;&#32473;&#23450;&#27010;&#24565;&#30340;&#21516;&#20041;&#35789;&#30340;&#30456;&#20851;&#25991;&#26412;&#65292;&#24182;&#35299;&#20915;&#35821;&#35328;&#27495;&#20041;&#12290;&#25105;&#20204;&#30830;&#35748;&#20687;LAION&#36825;&#26679;&#30340;&#27969;&#34892;&#30340;VLM&#25968;&#25454;&#38598;&#30830;&#23454;&#23637;&#31034;&#20102;&#38271;&#23614;&#27010;&#24565;&#20998;&#24067;&#65292;&#24182;&#19988;&#36825;&#19982;&#25353;&#31867;&#21035;&#30340;&#20934;&#30830;&#29575;&#24378;&#28872;&#30456;&#20851;&#12290;&#27492;&#22806;&#65292;&#24403;&#20195;&#30340;&#22810;&#27169;&#24335;&#31995;&#32479;&#65292;&#22914;&#35270;&#35273;&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#25991;&#26412;-&#35270;&#35273;&#25512;&#29702;&#27169;&#22411;&#65292;&#22312;&#36825;&#31181;&#38271;&#23614;&#20998;&#24067;&#19979;&#32463;&#24120;&#38590;&#20197;&#36798;&#21040;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-language models (VLMs) excel in zero-shot recognition but exhibit drastically imbalanced performance across visual concepts. For example, CLIP, despite an impressive mean zero-shot accuracy on ImageNet (72.7%), yields $&lt;$10% on ten concepts (e.g., gyromitra and night snake), presumably, because these concepts are under-represented in VLMs' imbalanced pretraining data. Yet, assessing this imbalance is challenging as it is non-trivial to calculate the frequency of specific concepts within VLMs' large-scale pretraining data. Our work makes the first attempt to measure the concept frequency by analyzing pretraining texts. We use off-the-shelf language models to help count relevant texts that contain synonyms of the given concepts and resolve linguistic ambiguity. We confirm that popular VLM datasets like LAION indeed exhibit long-tailed concept distributions, which strongly correlate with per-class accuracies. Further, contemporary multimodal systems, e.g., visual chatbots and text-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#20165;&#26377;&#30340;&#23569;&#37327;&#24494;&#23567;&#22810;&#35821;&#35328;&#24179;&#34892;&#25968;&#25454;&#26469;&#22686;&#24378;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#32763;&#35793;&#33021;&#21147;&#65292;&#23454;&#29616;&#22823;&#24133;&#24230;&#30340;&#38750;&#33521;&#25991;&#25972;&#20307;&#25913;&#36827;&#65292;&#24182;&#20445;&#25345;&#33521;&#25991;&#26041;&#21521;&#19978;&#30340;&#24615;&#33021;&#33021;&#21147;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#38543;&#26426;&#25277;&#21462;&#30340;&#23569;&#37327;&#26041;&#21521;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#20063;&#21487;&#20197;&#33719;&#24471;&#21487;&#27604;&#36739;&#30340;&#25972;&#20307;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.12413</link><description>&lt;p&gt;
100&#20010;&#26679;&#26412;&#21487;&#20197;&#36208;&#22810;&#36828;&#65311;&#36890;&#36807;&#24494;&#23567;&#30340;&#22810;&#35821;&#35328;&#24179;&#34892;&#25968;&#25454;&#35299;&#38145;&#20840;&#38754;&#30340;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
How Far Can 100 Samples Go? Unlocking Overall Zero-Shot Multilingual Translation via Tiny Multi-Parallel Data. (arXiv:2401.12413v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#20165;&#26377;&#30340;&#23569;&#37327;&#24494;&#23567;&#22810;&#35821;&#35328;&#24179;&#34892;&#25968;&#25454;&#26469;&#22686;&#24378;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#32763;&#35793;&#33021;&#21147;&#65292;&#23454;&#29616;&#22823;&#24133;&#24230;&#30340;&#38750;&#33521;&#25991;&#25972;&#20307;&#25913;&#36827;&#65292;&#24182;&#20445;&#25345;&#33521;&#25991;&#26041;&#21521;&#19978;&#30340;&#24615;&#33021;&#33021;&#21147;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#38543;&#26426;&#25277;&#21462;&#30340;&#23569;&#37327;&#26041;&#21521;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#20063;&#21487;&#20197;&#33719;&#24471;&#21487;&#27604;&#36739;&#30340;&#25972;&#20307;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#32763;&#35793;&#26159;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#65292;&#26088;&#22312;&#22312;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#65288;MMT&#65289;&#20013;&#32763;&#35793;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#23545;&#12290;&#19968;&#31181;&#24120;&#35265;&#20294;&#36164;&#28304;&#28040;&#32791;&#36739;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#23613;&#21487;&#33021;&#25366;&#25496;&#26356;&#22810;&#30340;&#32763;&#35793;&#26041;&#21521;&#24182;&#28155;&#21152;&#21040;&#24179;&#34892;&#35821;&#26009;&#24211;&#20013;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#36890;&#36807;&#20351;&#29992;&#20165;&#26377;&#30340;&#23569;&#37327;&#24494;&#23567;&#22810;&#35821;&#35328;&#24179;&#34892;&#25968;&#25454;&#26469;&#20248;&#21270;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;&#22312;EC30&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20165;&#20351;&#29992;100&#20010;&#22810;&#35821;&#35328;&#24179;&#34892;&#26679;&#26412;&#23601;&#33021;&#22815;&#23454;&#29616;+21.7 ChrF&#38750;&#33521;&#25991;&#25972;&#20307;&#25913;&#36827;&#65288;870&#20010;&#26041;&#21521;&#65289;&#65292;&#21516;&#26102;&#20445;&#25345;&#22312;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#26041;&#21521;&#19978;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#24494;&#35843;&#25968;&#25454;&#30340;&#35268;&#27169;&#25928;&#24212;&#21644;&#20854;&#36716;&#31227;&#33021;&#21147;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#22312;&#19968;&#20010;&#23567;&#30340;&#12289;&#38543;&#26426;&#25277;&#21462;&#30340;&#26041;&#21521;&#38598;&#65288;10%&#65289;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#20063;&#21487;&#20197;&#33719;&#24471;&#21487;&#27604;&#36739;&#30340;&#25972;&#20307;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25152;&#24471;&#21040;&#30340;&#38750;&#33521;&#25991;&#24615;&#33021;&#19982;&#33521;&#25991;&#24615;&#33021;&#38750;&#24120;&#25509;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-shot translation is an open problem, aiming to translate between language pairs unseen during training in Multilingual Machine Translation (MMT). A common, albeit resource-consuming, solution is to mine as many translation directions as possible to add to the parallel corpus. In this paper, we show that the zero-shot capability of an English-centric model can be easily enhanced by fine-tuning with a very small amount of multi-parallel data. For example, on the EC30 dataset, we show that up to +21.7 ChrF non-English overall improvements (870 directions) can be achieved by using only 100 multi-parallel samples, meanwhile preserving capability in English-centric directions. We further study the size effect of fine-tuning data and its transfer capabilities. Surprisingly, our empirical analysis shows that comparable overall improvements can be achieved even through fine-tuning in a small, randomly sampled direction set (10\%). Also, the resulting non-English performance is quite close 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32447;&#24615;&#25506;&#27979;&#26657;&#20934;&#65288;LinC&#65289;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#26657;&#20934;&#27169;&#22411;&#30340;&#36755;&#20986;&#27010;&#29575;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#22312;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#27169;&#22411;&#19978;&#30340;&#27979;&#35797;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.12406</link><description>&lt;p&gt;
&#36890;&#36807;&#32447;&#24615;&#25506;&#27979;&#26657;&#20934;&#25552;&#39640;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Enhancing In-context Learning via Linear Probe Calibration. (arXiv:2401.12406v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32447;&#24615;&#25506;&#27979;&#26657;&#20934;&#65288;LinC&#65289;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#26657;&#20934;&#27169;&#22411;&#30340;&#36755;&#20986;&#27010;&#29575;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#22312;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#27169;&#22411;&#19978;&#30340;&#27979;&#35797;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33539;&#24335;&#65292;&#21033;&#29992;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#31561;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#29992;&#21253;&#21547;&#19978;&#19979;&#25991;&#28436;&#31034;&#30340;&#25552;&#31034;&#26469;&#20026;&#26032;&#30340;&#26597;&#35810;&#36755;&#20837;&#29983;&#25104;&#30456;&#24212;&#30340;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#24212;&#29992;ICL&#26080;&#27861;&#38543;&#30528;&#26679;&#26412;&#25968;&#37327;&#30340;&#22686;&#21152;&#32780;&#25193;&#23637;&#65292;&#24182;&#19988;&#23545;&#19981;&#21516;&#30340;&#25552;&#31034;&#27169;&#26495;&#21644;&#28436;&#31034;&#25490;&#21015;&#32570;&#20047;&#40065;&#26834;&#24615;&#12290;&#26412;&#25991;&#39318;&#20808;&#23637;&#31034;&#20102;&#20351;&#29992;ICL&#30340;GPT&#27169;&#22411;&#22522;&#20110;&#22522;&#20110;&#39321;&#20892;&#29109;&#30340;&#26032;&#24230;&#37327;&#32780;&#23548;&#33268;&#19981;&#21487;&#38752;&#30340;&#39044;&#27979;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#32447;&#24615;&#25506;&#27979;&#26657;&#20934;&#65288;LinC&#65289;&#30340;&#26032;&#25216;&#26415;&#65292;&#23427;&#21487;&#20197;&#26657;&#20934;&#27169;&#22411;&#30340;&#36755;&#20986;&#27010;&#29575;&#65292;&#20174;&#32780;&#24471;&#21040;&#21487;&#38752;&#30340;&#39044;&#27979;&#21644;&#25913;&#36827;&#30340;&#24615;&#33021;&#65292;&#19988;&#20165;&#38656;&#35201;&#26497;&#23569;&#37327;&#30340;&#39069;&#22806;&#26679;&#26412;&#65288;&#20165;&#38656;&#20116;&#20010;&#24050;&#26631;&#35760;&#30340;&#25968;&#25454;&#26679;&#26412;&#65289;&#12290;LinC&#26174;&#33879;&#25552;&#39640;&#20102;GPT&#27169;&#22411;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;ICL&#27979;&#35797;&#24615;&#33021;&#65292;&#24179;&#22343;&#25913;&#21892;&#25928;&#26524;&#24456;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning (ICL) is a new paradigm for natural language processing that utilizes Generative Pre-trained Transformer (GPT)-like models. This approach uses prompts that include in-context demonstrations to generate the corresponding output for a new query input. However, applying ICL in real cases does not scale with the number of samples, and lacks robustness to different prompt templates and demonstration permutations. In this paper, we first show that GPT-like models using ICL result in unreliable predictions based on a new metric based on Shannon entropy. Then, to solve this problem, we propose a new technique called the Linear Probe Calibration (LinC), a method that calibrates the model's output probabilities, resulting in reliable predictions and improved performance, while requiring only minimal additional samples (as few as five labeled data samples). LinC significantly enhances the ICL test performance of GPT models on various benchmark datasets, with an average improve
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#22235;&#25152;&#21152;&#25343;&#22823;&#20027;&#35201;&#22823;&#23398;&#30340;&#23398;&#29983;&#22312;Reddit&#19978;&#25776;&#20889;&#30340;&#24086;&#23376;&#36827;&#34892;&#32437;&#21521;&#24773;&#24863;&#20998;&#31867;&#12290;&#36890;&#36807;&#35843;&#25972;&#24773;&#24863;&#38408;&#20540;&#65292;&#25105;&#20204;&#25104;&#21151;&#26500;&#24314;&#20102;&#20998;&#31867;&#22120;&#65292;&#33021;&#22815;&#23558;&#24086;&#23376;&#24773;&#24863;&#20998;&#31867;&#20026;&#31215;&#26497;&#21644;&#28040;&#26497;&#31867;&#21035;&#65292;&#24182;&#19988;&#32467;&#26524;&#22312;&#19981;&#21516;&#22823;&#23398;&#25968;&#25454;&#38598;&#20013;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2401.12382</link><description>&lt;p&gt;
Reddit&#24086;&#23376;&#30340;&#32437;&#21521;&#24773;&#24863;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Longitudinal Sentiment Classification of Reddit Posts. (arXiv:2401.12382v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#22235;&#25152;&#21152;&#25343;&#22823;&#20027;&#35201;&#22823;&#23398;&#30340;&#23398;&#29983;&#22312;Reddit&#19978;&#25776;&#20889;&#30340;&#24086;&#23376;&#36827;&#34892;&#32437;&#21521;&#24773;&#24863;&#20998;&#31867;&#12290;&#36890;&#36807;&#35843;&#25972;&#24773;&#24863;&#38408;&#20540;&#65292;&#25105;&#20204;&#25104;&#21151;&#26500;&#24314;&#20102;&#20998;&#31867;&#22120;&#65292;&#33021;&#22815;&#23558;&#24086;&#23376;&#24773;&#24863;&#20998;&#31867;&#20026;&#31215;&#26497;&#21644;&#28040;&#26497;&#31867;&#21035;&#65292;&#24182;&#19988;&#32467;&#26524;&#22312;&#19981;&#21516;&#22823;&#23398;&#25968;&#25454;&#38598;&#20013;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25253;&#21578;&#20102;&#23545;&#22235;&#25152;&#21152;&#25343;&#22823;&#20027;&#35201;&#22823;&#23398;&#30340;&#23398;&#29983;&#25776;&#20889;&#30340;Reddit&#24086;&#23376;&#36827;&#34892;&#32437;&#21521;&#24773;&#24863;&#20998;&#31867;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#20351;&#29992;&#24086;&#23376;&#30340;&#25991;&#26412;&#65292;&#37325;&#28857;&#20851;&#27880;2020&#24180;&#33267;2023&#24180;&#20043;&#38388;&#30340;&#26102;&#38388;&#12290;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#24773;&#24863;&#38408;&#20540;&#22312;[-0.075, 0.075]&#33539;&#22260;&#20869;&#65292;&#25105;&#20204;&#25104;&#21151;&#26500;&#24314;&#20102;&#33021;&#22815;&#23558;&#24086;&#23376;&#24773;&#24863;&#20998;&#31867;&#20026;&#31215;&#26497;&#21644;&#28040;&#26497;&#31867;&#21035;&#30340;&#20998;&#31867;&#22120;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#24773;&#24863;&#20998;&#31867;&#32467;&#26524;&#22312;&#22235;&#20010;&#22823;&#23398;&#25968;&#25454;&#38598;&#20013;&#26159;&#19968;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We report results of a longitudinal sentiment classification of Reddit posts written by students of four major Canadian universities. We work with the texts of the posts, concentrating on the years 2020-2023. By finely tuning a sentiment threshold to a range of [-0.075,0.075], we successfully built classifiers proficient in categorizing post sentiments into positive and negative categories. Noticeably, our sentiment classification results are consistent across the four university data sets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20026;&#35270;&#21147;&#38556;&#30861;&#23398;&#29983;&#24320;&#21457;&#30340;&#22522;&#20110;NLP&#30340;&#35745;&#31639;&#26426;&#21270;&#27979;&#35797;&#25351;&#21335;&#65292;&#21033;&#29992;&#35821;&#38899;&#25216;&#26415;&#21644;NLP&#25216;&#26415;&#23454;&#26102;&#25552;&#20379;&#25903;&#25345;&#21644;&#24110;&#21161;&#65292;&#24182;&#23558;&#25991;&#26412;&#38382;&#39064;&#21644;&#36873;&#39033;&#36716;&#21270;&#20026;&#26426;&#22120;&#21487;&#35835;&#30340;&#26684;&#24335;&#12290;</title><link>http://arxiv.org/abs/2401.12375</link><description>&lt;p&gt;
&#20026;&#35270;&#21147;&#38556;&#30861;&#23398;&#29983;&#24320;&#21457;&#22522;&#20110;NLP&#30340;&#35745;&#31639;&#26426;&#21270;&#27979;&#35797;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
Development of an NLP-driven computer-based test guide for visually impaired students. (arXiv:2401.12375v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20026;&#35270;&#21147;&#38556;&#30861;&#23398;&#29983;&#24320;&#21457;&#30340;&#22522;&#20110;NLP&#30340;&#35745;&#31639;&#26426;&#21270;&#27979;&#35797;&#25351;&#21335;&#65292;&#21033;&#29992;&#35821;&#38899;&#25216;&#26415;&#21644;NLP&#25216;&#26415;&#23454;&#26102;&#25552;&#20379;&#25903;&#25345;&#21644;&#24110;&#21161;&#65292;&#24182;&#23558;&#25991;&#26412;&#38382;&#39064;&#21644;&#36873;&#39033;&#36716;&#21270;&#20026;&#26426;&#22120;&#21487;&#35835;&#30340;&#26684;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25216;&#26415;&#30340;&#36827;&#27493;&#25913;&#21464;&#20102;&#26080;&#38556;&#30861;&#21644;&#29420;&#21344;&#24615;&#27979;&#35797;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#35270;&#21147;&#38556;&#30861;&#23398;&#29983;&#65288;VIS&#65289;&#12290;&#35745;&#31639;&#26426;&#21270;&#27979;&#35797;&#65288;CBT&#65289;&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#24050;&#32463;&#26174;&#31034;&#20986;&#20102;&#22312;&#30005;&#23376;&#21270;&#32771;&#35797;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#65292;&#20351;&#32771;&#35797;&#36807;&#31243;&#26356;&#21152;&#31616;&#20415;&#65292;&#25552;&#20379;&#26356;&#24555;&#26356;&#20934;&#30830;&#30340;&#32467;&#26524;&#65292;&#24182;&#20026;&#32771;&#29983;&#25552;&#20379;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#21644;&#21487;&#35775;&#38382;&#24615;&#12290;&#28982;&#32780;&#65292;&#35270;&#21147;&#38556;&#30861;&#23398;&#29983;&#26080;&#27861;&#35775;&#38382;&#21360;&#21047;&#25991;&#20214;&#65292;&#22240;&#27492;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#35270;&#21147;&#38556;&#30861;&#23398;&#29983;&#30340;&#22522;&#20110;NLP&#30340;&#35745;&#31639;&#26426;&#21270;&#27979;&#35797;&#25351;&#21335;&#12290;&#35813;&#31995;&#32479;&#37319;&#29992;&#35821;&#38899;&#25216;&#26415;&#39044;&#20808;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#20026;&#35270;&#21147;&#38556;&#30861;&#23398;&#29983;&#23454;&#26102;&#25552;&#20379;&#25903;&#25345;&#21644;&#24110;&#21161;&#12290;&#31995;&#32479;&#21033;&#29992;NLP&#25216;&#26415;&#23558;&#22522;&#20110;&#25991;&#26412;&#30340;&#38382;&#39064;&#21644;&#30456;&#20851;&#36873;&#39033;&#36716;&#21270;&#20026;&#26426;&#22120;&#21487;&#35835;&#30340;&#26684;&#24335;&#12290;&#38543;&#21518;&#65292;&#35821;&#38899;&#25216;&#26415;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#22788;&#29702;&#36716;&#21270;&#21518;&#30340;&#25991;&#26412;&#65292;&#23454;&#29616;&#23545;&#35270;&#21147;&#38556;&#30861;&#23398;&#29983;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, advancements in Natural Language Processing (NLP) techniques have revolutionized the field of accessibility and exclusivity of testing, particularly for visually impaired students (VIS). CBT has shown in years back its relevance in terms of administering exams electronically, making the test process easier, providing quicker and more accurate results, and offering greater flexibility and accessibility for candidates. Yet, its relevance was not felt by the visually impaired students as they cannot access printed documents. Hence, in this paper, we present an NLP-driven Computer-Based Test guide for visually impaired students. It employs a speech technology pre-trained methods to provide real-time assistance and support to visually impaired students. The system utilizes NLP technologies to convert the text-based questions and the associated options in a machine-readable format. Subsequently, the speech technology pre-trained model processes the converted text enabling th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#39304;&#24341;&#23548;&#21644;&#23376;&#22270;&#25552;&#21462;&#30340;&#24046;&#20998;&#32422;&#26463;&#35843;&#24230;&#31639;&#27861;&#29992;&#20110;&#39640;&#32423;&#32508;&#21512;&#65292;&#21487;&#26174;&#33879;&#20943;&#23569;&#23492;&#23384;&#22120;&#20351;&#29992;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.12343</link><description>&lt;p&gt;
&#22522;&#20110;&#23376;&#22270;&#25552;&#21462;&#30340;&#21453;&#39304;&#24341;&#23548;&#36845;&#20195;&#35843;&#24230;&#22312;&#39640;&#32423;&#32508;&#21512;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Subgraph Extraction-based Feedback-guided Iterative Scheduling for HLS. (arXiv:2401.12343v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12343
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#39304;&#24341;&#23548;&#21644;&#23376;&#22270;&#25552;&#21462;&#30340;&#24046;&#20998;&#32422;&#26463;&#35843;&#24230;&#31639;&#27861;&#29992;&#20110;&#39640;&#32423;&#32508;&#21512;&#65292;&#21487;&#26174;&#33879;&#20943;&#23569;&#23492;&#23384;&#22120;&#20351;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21453;&#39304;&#24341;&#23548;&#36845;&#20195;&#24046;&#20998;&#32422;&#26463;&#35843;&#24230;&#31639;&#27861;ISDC&#65292;&#29992;&#20110;&#39640;&#32423;&#32508;&#21512;(HLS)&#12290;ISDC&#21033;&#29992;&#19979;&#28216;&#24037;&#20855;&#65288;&#22914;&#36923;&#36753;&#32508;&#21512;&#22120;&#65289;&#25552;&#20379;&#30340;&#23376;&#22270;&#25552;&#21462;&#30340;&#20302;&#32423;&#21453;&#39304;&#65292;&#36845;&#20195;&#20248;&#21270;HLS&#35843;&#24230;&#12290;&#25216;&#26415;&#21019;&#26032;&#21253;&#25324;&#65306;&#65288;1&#65289;&#22686;&#24378;&#30340;&#24046;&#20998;&#32422;&#26463;&#38382;&#39064;&#65288;SDC&#65289;&#24418;&#24335;&#65292;&#26377;&#25928;&#23558;&#20302;&#32423;&#21453;&#39304;&#38598;&#25104;&#21040;&#32447;&#24615;&#35268;&#21010;(LP)&#38382;&#39064;&#20013;&#65307;&#65288;2&#65289;&#22522;&#20110;&#25159;&#20986;&#21644;&#31383;&#21475;&#30340;&#23376;&#22270;&#25552;&#21462;&#26426;&#21046;&#39537;&#21160;&#21453;&#39304;&#24490;&#29615;&#65307;&#65288;3&#65289;&#19982;&#24191;&#27867;&#30340;&#19979;&#28216;&#24037;&#20855;&#21644;&#24037;&#33402;&#35774;&#35745;&#24037;&#20855;&#21253;(PDK)&#20860;&#23481;&#30340;&#26080;&#20154;&#20171;&#20837;&#30340;ISDC&#27969;&#31243;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;ISDC&#30456;&#23545;&#20110;&#24037;&#19994;&#32423;&#24320;&#28304;HLS&#24037;&#20855;&#65292;&#21487;&#20197;&#20943;&#23569;28.5%&#30340;&#23492;&#23384;&#22120;&#20351;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes ISDC, a novel feedback-guided iterative system of difference constraints (SDC) scheduling algorithm for high-level synthesis (HLS). ISDC leverages subgraph extraction-based low-level feedback from downstream tools like logic synthesizers to iteratively refine HLS scheduling. Technical innovations include: (1) An enhanced SDC formulation that effectively integrates low-level feedback into the linear-programming (LP) problem; (2) A fanout and window-based subgraph extraction mechanism driving the feedback cycle; (3) A no-human-in-loop ISDC flow compatible with a wide range of downstream tools and process design kits (PDKs). Evaluation shows that ISDC reduces register usage by 28.5% against an industrial-strength open-source HLS tool.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#22810;&#29983;&#25104;&#22120;&#12289;&#22810;&#39046;&#22495;&#21644;&#22810;&#35821;&#35328;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36716;&#25442;&#22120;&#27169;&#22411;&#29305;&#21035;&#26159;LoRA-RoBERTa&#22312;&#25928;&#26524;&#19978;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23545;&#20110;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#30340;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#35782;&#21035;&#65292;&#22810;&#25968;&#25237;&#31080;&#26041;&#27861;&#23588;&#20026;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2401.12326</link><description>&lt;p&gt;
&#23545;&#22810;&#29983;&#25104;&#22120;&#12289;&#22810;&#39046;&#22495;&#21644;&#22810;&#35821;&#35328;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning Large Language Models for Multigenerator, Multidomain, and Multilingual Machine-Generated Text Detection. (arXiv:2401.12326v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#22810;&#29983;&#25104;&#22120;&#12289;&#22810;&#39046;&#22495;&#21644;&#22810;&#35821;&#35328;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36716;&#25442;&#22120;&#27169;&#22411;&#29305;&#21035;&#26159;LoRA-RoBERTa&#22312;&#25928;&#26524;&#19978;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23545;&#20110;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#30340;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#35782;&#21035;&#65292;&#22810;&#25968;&#25237;&#31080;&#26041;&#27861;&#23588;&#20026;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
SemEval-2024&#20219;&#21153;8&#24341;&#20837;&#20102;&#20174;&#22810;&#31181;&#35821;&#35328;&#21644;&#39046;&#22495;&#30340;&#19981;&#21516;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#35782;&#21035;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#25361;&#25112;&#12290;&#35813;&#20219;&#21153;&#30001;&#19977;&#20010;&#23376;&#20219;&#21153;&#32452;&#25104;&#65306;&#21333;&#35821;&#21644;&#22810;&#35821;&#35328;&#30340;&#20108;&#20803;&#20998;&#31867;&#65288;&#23376;&#20219;&#21153;A&#65289;&#12289;&#22810;&#31867;&#21035;&#20998;&#31867;&#65288;&#23376;&#20219;&#21153;B&#65289;&#20197;&#21450;&#28151;&#21512;&#25991;&#26412;&#26816;&#27979;&#65288;&#23376;&#20219;&#21153;C&#65289;&#12290;&#26412;&#25991;&#37325;&#28857;&#20851;&#27880;&#23376;&#20219;&#21153;A&#21644;B&#12290;&#27599;&#20010;&#23376;&#20219;&#21153;&#37117;&#26377;&#19977;&#20010;&#25968;&#25454;&#38598;&#29992;&#20110;&#35757;&#32451;&#12289;&#24320;&#21457;&#21644;&#27979;&#35797;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#65292;&#37319;&#29992;&#20102;&#20004;&#31181;&#26041;&#27861;&#65306;1&#65289;&#20351;&#29992;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#39044;&#22788;&#29702;&#65288;NLP&#65289;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#65292;2&#65289;&#23545;&#25991;&#26412;&#20998;&#31867;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;LoRA-RoBERTa&#65292;&#22312;&#25928;&#26524;&#19978;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22810;&#25968;&#25237;&#31080;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#20013;&#35782;&#21035;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26041;&#38754;&#23588;&#20026;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
SemEval-2024 Task 8 introduces the challenge of identifying machine-generated texts from diverse Large Language Models (LLMs) in various languages and domains. The task comprises three subtasks: binary classification in monolingual and multilingual (Subtask A), multi-class classification (Subtask B), and mixed text detection (Subtask C). This paper focuses on Subtask A &amp; B. Each subtask is supported by three datasets for training, development, and testing. To tackle this task, two methods: 1) using traditional machine learning (ML) with natural language preprocessing (NLP) for feature extraction, and 2) fine-tuning LLMs for text classification. The results show that transformer models, particularly LoRA-RoBERTa, exceed traditional ML methods in effectiveness, with majority voting being particularly effective in multilingual contexts for identifying machine-generated texts.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#8220;&#24265;&#20215;&#8221;&#23398;&#20064;&#25216;&#26415;&#22312;&#31038;&#20250;&#31185;&#23398;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#24369;&#30417;&#30563;&#12289;&#36801;&#31227;&#23398;&#20064;&#21644;&#25552;&#31034;&#24037;&#31243;&#12290;&#29305;&#21035;&#22320;&#65292;&#36890;&#36807;&#25552;&#31034;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.12295</link><description>&lt;p&gt;
&#24265;&#20215;&#23398;&#20064;&#65306;&#26368;&#22823;&#21270;&#31038;&#20250;&#25968;&#25454;&#31185;&#23398;&#20013;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20351;&#29992;&#26368;&#23569;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cheap Learning: Maximising Performance of Language Models for Social Data Science Using Minimal Data. (arXiv:2401.12295v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#8220;&#24265;&#20215;&#8221;&#23398;&#20064;&#25216;&#26415;&#22312;&#31038;&#20250;&#31185;&#23398;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#24369;&#30417;&#30563;&#12289;&#36801;&#31227;&#23398;&#20064;&#21644;&#25552;&#31034;&#24037;&#31243;&#12290;&#29305;&#21035;&#22320;&#65292;&#36890;&#36807;&#25552;&#31034;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#22312;&#26500;&#24314;&#26032;&#27169;&#22411;&#26102;&#65292;&#26368;&#36817;&#21462;&#24471;&#20102;&#38477;&#20302;&#26631;&#27880;&#35757;&#32451;&#25968;&#25454;&#35201;&#27714;&#30340;&#37325;&#35201;&#36827;&#23637;&#12290;&#36825;&#20123;&#8220;&#24265;&#20215;&#8221;&#23398;&#20064;&#25216;&#26415;&#22312;&#31038;&#20250;&#31185;&#23398;&#39046;&#22495;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#22240;&#20026;&#24320;&#21457;&#22823;&#22411;&#26631;&#27880;&#35757;&#32451;&#25968;&#25454;&#38598;&#36890;&#24120;&#26159;&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#20998;&#26512;&#20219;&#21153;&#30340;&#23454;&#38469;&#38556;&#30861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#26368;&#36817;&#21457;&#23637;&#30340;&#19977;&#31181;&#8220;&#24265;&#20215;&#8221;&#25216;&#26415;&#65306;&#24369;&#30417;&#30563;&#12289;&#36801;&#31227;&#23398;&#20064;&#21644;&#25552;&#31034;&#24037;&#31243;&#12290;&#23545;&#20110;&#21518;&#32773;&#65292;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#25552;&#31034;&#30340;&#29305;&#27530;&#24773;&#20917;&#12290;&#38024;&#23545;&#27599;&#31181;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#24037;&#20316;&#21407;&#29702;&#30340;&#25351;&#21335;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#20845;&#20010;&#19981;&#21516;&#30340;&#23454;&#38469;&#31038;&#20250;&#31185;&#23398;&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#24212;&#29992;&#24773;&#20917;&#65288;&#20004;&#20010;&#19981;&#21516;&#20219;&#21153;&#19982;&#19977;&#31181;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#32452;&#21512;&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#26377;&#25216;&#26415;&#30340;&#33391;&#22909;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#21487;&#20197;&#23454;&#29616;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of machine learning has recently made significant progress in reducing the requirements for labelled training data when building new models. These `cheaper' learning techniques hold significant potential for the social sciences, where development of large labelled training datasets is often a significant practical impediment to the use of machine learning for analytical tasks. In this article we review three `cheap' techniques that have developed in recent years: weak supervision, transfer learning and prompt engineering. For the latter, we also review the particular case of zero-shot prompting of large language models. For each technique we provide a guide of how it works and demonstrate its application across six different realistic social science applications (two different tasks paired with three different dataset makeups). We show good performance for all techniques, and in particular we demonstrate how prompting of large language models can achieve high accuracy at very
&lt;/p&gt;</description></item><item><title>GRATH&#26159;&#19968;&#31181;&#36880;&#27493;&#33258;&#25105;&#30495;&#23454;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;&#23427;&#36890;&#36807;&#20351;&#29992;&#39046;&#22495;&#22806;&#38382;&#39064;&#25552;&#31034;&#29983;&#25104;&#31572;&#26696;&#65292;&#24182;&#36890;&#36807;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#36827;&#34892;&#33258;&#36866;&#24212;&#27169;&#22411;&#20248;&#21270;&#12290;GRATH&#22312;&#27809;&#26377;&#26631;&#27880;&#31572;&#26696;&#30340;&#24773;&#20917;&#19979;&#20197;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#30495;&#23454;&#24615;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#26469;&#36880;&#27493;&#25552;&#21319;&#27169;&#22411;&#30495;&#23454;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.12292</link><description>&lt;p&gt;
GRATH: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36880;&#28176;&#33258;&#25105;&#30495;&#23454;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GRATH: Gradual Self-Truthifying for Large Language Models. (arXiv:2401.12292v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12292
&lt;/p&gt;
&lt;p&gt;
GRATH&#26159;&#19968;&#31181;&#36880;&#27493;&#33258;&#25105;&#30495;&#23454;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;&#23427;&#36890;&#36807;&#20351;&#29992;&#39046;&#22495;&#22806;&#38382;&#39064;&#25552;&#31034;&#29983;&#25104;&#31572;&#26696;&#65292;&#24182;&#36890;&#36807;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#36827;&#34892;&#33258;&#36866;&#24212;&#27169;&#22411;&#20248;&#21270;&#12290;GRATH&#22312;&#27809;&#26377;&#26631;&#27880;&#31572;&#26696;&#30340;&#24773;&#20917;&#19979;&#20197;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#30495;&#23454;&#24615;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#26469;&#36880;&#27493;&#25552;&#21319;&#27169;&#22411;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30340;&#37096;&#32626;&#36234;&#26469;&#36234;&#22810;&#65292;&#30495;&#23454;&#24615;&#23545;&#23427;&#20204;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;LLMs&#22312;&#29983;&#25104;&#30495;&#23454;&#31572;&#26696;&#21644;&#20869;&#23481;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#65292;&#22914;&#22312;TruthfulQA&#31561;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GRAdual self-truTHifying (GRATH)&#65292;&#19968;&#31181;&#36890;&#36807;&#21518;&#22788;&#29702;&#26041;&#27861;&#25552;&#39640;LLMs&#30495;&#23454;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;GRATH&#21033;&#29992;&#39046;&#22495;&#22806;&#30340;&#38382;&#39064;&#25552;&#31034;&#29983;&#25104;&#30456;&#24212;&#30340;&#31572;&#26696;&#65292;&#24182;&#36890;&#36807;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#36827;&#34892;&#33258;&#36866;&#24212;&#27169;&#22411;&#20248;&#21270;&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;GRATH&#20197;&#26080;&#38656;&#26631;&#27880;&#31572;&#26696;&#30340;&#33258;&#25105;&#30417;&#30563;&#26041;&#24335;&#23398;&#20064;&#30495;&#23454;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;GRATH&#39318;&#20808;&#36890;&#36807;&#25552;&#31034;LLM&#33258;&#36523;&#29983;&#25104;&#25104;&#23545;&#30495;&#23454;&#24615;&#35757;&#32451;&#25968;&#25454;&#65292;&#27599;&#23545;&#21253;&#21547;&#19968;&#20010;&#38382;&#39064;&#21450;&#20854;&#27491;&#30830;&#21644;&#38169;&#35823;&#31572;&#26696;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#26469;&#24494;&#35843;&#27169;&#22411;&#65292;&#20174;&#31572;&#26696;&#23545;&#30340;&#24046;&#24322;&#20013;&#23398;&#20064;&#12290;&#38543;&#21518;&#65292;GRATH&#36845;&#20195;&#22320;&#20248;&#21270;&#27169;&#22411;&#20197;&#36880;&#28176;&#25552;&#39640;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Truthfulness is paramount for large language models (LLMs) as they are increasingly deployed in real-world applications. However, existing LLMs still struggle with generating truthful answers and content, as evidenced by their modest performance on benchmarks like TruthfulQA. To address this issue, we propose GRAdual self-truTHifying (GRATH), a novel post-processing method to enhance truthfulness of LLMs. GRATH utilizes out-of-domain question prompts to generate corresponding answers and adaptively optimizes the model via direct preference optimization (DPO). Note that during this process, GRATH learns truthfulness in a self-supervised manner without requiring annotated answers. In particular, GRATH first generates pairwise truthfulness training data by prompting the LLM itself, with each pair containing a question and its correct and incorrect answers. The model is then fine-tuned using DPO to learn from the difference between answer pairs. Subsequently, GRATH iteratively refines the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20840;&#38754;&#25506;&#35752;&#20102;&#19982;&#35821;&#35328;&#23398;&#20064;&#27169;&#22411;&#65288;LLMs&#65289;&#38754;&#20020;&#30340;&#23433;&#20840;&#23041;&#32961;&#30456;&#20851;&#30340;&#20262;&#29702;&#25361;&#25112;&#12290;&#20998;&#26512;&#20102;&#20116;&#31181;&#20027;&#35201;&#23041;&#32961;&#30340;&#20262;&#29702;&#21518;&#26524;&#65292;&#24182;&#24378;&#35843;&#20102;&#30830;&#20445;&#36825;&#20123;&#31995;&#32479;&#22312;&#20262;&#29702;&#35268;&#33539;&#33539;&#22260;&#20869;&#36816;&#20316;&#30340;&#32039;&#36843;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.12273</link><description>&lt;p&gt;
&#20132;&#20114;&#30340;&#20262;&#29702;&#38382;&#39064;&#65306;&#32531;&#35299;LLMs&#20013;&#30340;&#23433;&#20840;&#23041;&#32961;
&lt;/p&gt;
&lt;p&gt;
The Ethics of Interaction: Mitigating Security Threats in LLMs. (arXiv:2401.12273v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20840;&#38754;&#25506;&#35752;&#20102;&#19982;&#35821;&#35328;&#23398;&#20064;&#27169;&#22411;&#65288;LLMs&#65289;&#38754;&#20020;&#30340;&#23433;&#20840;&#23041;&#32961;&#30456;&#20851;&#30340;&#20262;&#29702;&#25361;&#25112;&#12290;&#20998;&#26512;&#20102;&#20116;&#31181;&#20027;&#35201;&#23041;&#32961;&#30340;&#20262;&#29702;&#21518;&#26524;&#65292;&#24182;&#24378;&#35843;&#20102;&#30830;&#20445;&#36825;&#20123;&#31995;&#32479;&#22312;&#20262;&#29702;&#35268;&#33539;&#33539;&#22260;&#20869;&#36816;&#20316;&#30340;&#32039;&#36843;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#25506;&#35752;&#20102;&#19982;&#35821;&#35328;&#23398;&#20064;&#27169;&#22411;&#65288;LLMs&#65289;&#38754;&#20020;&#30340;&#23433;&#20840;&#23041;&#32961;&#30456;&#20851;&#30340;&#20262;&#29702;&#25361;&#25112;&#12290;&#36825;&#20123;&#22797;&#26434;&#30340;&#25968;&#23383;&#23384;&#20648;&#24211;&#26085;&#30410;&#34701;&#20837;&#21040;&#25105;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#20013;&#65292;&#22240;&#27492;&#25104;&#20026;&#25915;&#20987;&#30340;&#20027;&#35201;&#30446;&#26631;&#65292;&#21487;&#33021;&#21361;&#21450;&#20854;&#35757;&#32451;&#25968;&#25454;&#21644;&#25968;&#25454;&#28304;&#30340;&#26426;&#23494;&#24615;&#12290;&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#36825;&#20123;&#23433;&#20840;&#23041;&#32961;&#23545;&#31038;&#20250;&#21644;&#20010;&#20154;&#38544;&#31169;&#30340;&#24494;&#22937;&#20262;&#29702;&#24433;&#21709;&#12290;&#25105;&#20204;&#23545;&#20116;&#20010;&#20027;&#35201;&#23041;&#32961;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65306;&#25552;&#31034;&#27880;&#20837;&#12289;&#36234;&#29425;&#12289;&#20010;&#20154;&#21487;&#35782;&#21035;&#20449;&#24687;&#65288;PII&#65289;&#26333;&#38706;&#12289;&#24615;&#21035;&#26174;&#38706;&#20869;&#23481;&#21644;&#22522;&#20110;&#20167;&#24680;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#19981;&#20165;&#20165;&#36827;&#34892;&#20102;&#35782;&#21035;&#65292;&#36824;&#35780;&#20272;&#20102;&#23427;&#20204;&#30340;&#20851;&#38190;&#20262;&#29702;&#21518;&#26524;&#20197;&#21450;&#23545;&#24378;&#21270;&#38450;&#24481;&#31574;&#30053;&#30340;&#32039;&#36843;&#24615;&#12290;&#23545;LLMs&#30340;&#19981;&#26029;&#20381;&#36182;&#20984;&#26174;&#20102;&#30830;&#20445;&#36825;&#20123;&#31995;&#32479;&#22312;&#20262;&#29702;&#35268;&#33539;&#33539;&#22260;&#20869;&#36816;&#20316;&#30340;&#37325;&#35201;&#24615;&#65292;&#29305;&#21035;&#26159;&#30001;&#20110;&#23427;&#20204;&#30340;&#28389;&#29992;&#21487;&#33021;&#23548;&#33268;&#37325;&#22823;&#31038;&#20250;&#21644;&#20010;&#20154;&#20260;&#23475;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#36825;&#20123;&#31995;&#32479;&#27010;&#24565;&#21270;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper comprehensively explores the ethical challenges arising from security threats to Language Learning Models (LLMs). These intricate digital repositories are increasingly integrated into our daily lives, making them prime targets for attacks that can compromise their training data and the confidentiality of their data sources. The paper delves into the nuanced ethical repercussions of such security threats on society and individual privacy. We scrutinize five major threats: prompt injection, jailbreaking, Personal Identifiable Information (PII) exposure, sexually explicit content, and hate based content, going beyond mere identification to assess their critical ethical consequences and the urgency they create for robust defensive strategies. The escalating reliance on LLMs underscores the crucial need for ensuring these systems operate within the bounds of ethical norms, particularly as their misuse can lead to significant societal and individual harm. We propose conceptualizin
&lt;/p&gt;</description></item><item><title>Orion-14B&#26159;&#19968;&#20010;&#20855;&#26377;140&#20159;&#21442;&#25968;&#30340;&#24320;&#28304;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#22312;&#35813;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#25968;&#25454;&#35843;&#24230;&#26041;&#27861;&#23545;&#19968;&#20010;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#29992;&#20102;&#26469;&#33258;&#22810;&#31181;&#35821;&#35328;&#30340;2.5&#19975;&#20159;&#20010;&#26631;&#35760;&#30340;&#22810;&#26679;&#21270;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#36824;&#23545;&#23545;&#35805;&#24212;&#29992;&#21644;&#20854;&#20182;&#29305;&#23450;&#29992;&#20363;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;Orion-14B&#22312;&#24191;&#27867;&#30340;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#39046;&#20808;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;Orion-14B&#27169;&#22411;&#31995;&#21015;&#21450;&#20854;&#30456;&#20851;&#20195;&#30721;&#20844;&#24320;&#65292;&#20197;&#40723;&#21169;&#26410;&#26469;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.12246</link><description>&lt;p&gt;
Orion-14B: &#24320;&#28304;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Orion-14B: Open-source Multilingual Large Language Models. (arXiv:2401.12246v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12246
&lt;/p&gt;
&lt;p&gt;
Orion-14B&#26159;&#19968;&#20010;&#20855;&#26377;140&#20159;&#21442;&#25968;&#30340;&#24320;&#28304;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#22312;&#35813;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#25968;&#25454;&#35843;&#24230;&#26041;&#27861;&#23545;&#19968;&#20010;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#29992;&#20102;&#26469;&#33258;&#22810;&#31181;&#35821;&#35328;&#30340;2.5&#19975;&#20159;&#20010;&#26631;&#35760;&#30340;&#22810;&#26679;&#21270;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#36824;&#23545;&#23545;&#35805;&#24212;&#29992;&#21644;&#20854;&#20182;&#29305;&#23450;&#29992;&#20363;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;Orion-14B&#22312;&#24191;&#27867;&#30340;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#39046;&#20808;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;Orion-14B&#27169;&#22411;&#31995;&#21015;&#21450;&#20854;&#30456;&#20851;&#20195;&#30721;&#20844;&#24320;&#65292;&#20197;&#40723;&#21169;&#26410;&#26469;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Orion-14B&#65292;&#19968;&#20010;&#20855;&#26377;140&#20159;&#21442;&#25968;&#30340;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#21512;&#12290;&#25105;&#20204;&#37319;&#29992;&#25968;&#25454;&#35843;&#24230;&#26041;&#27861;&#65292;&#22312;&#21253;&#25324;&#33521;&#35821;&#12289;&#20013;&#25991;&#12289;&#26085;&#35821;&#12289;&#38889;&#35821;&#21644;&#20854;&#20182;&#35821;&#35328;&#30340;&#25991;&#26412;&#20013;&#65292;&#23545;&#19968;&#20010;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#20351;&#29992;&#20102;&#26469;&#33258;2.5&#19975;&#20159;&#20010;&#26631;&#35760;&#30340;&#22810;&#26679;&#21270;&#35821;&#26009;&#24211;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#38024;&#23545;&#23545;&#35805;&#24212;&#29992;&#21644;&#20854;&#20182;&#29305;&#23450;&#29992;&#20363;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#27169;&#22411;&#30340;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;Orion-14B&#22312;&#24191;&#27867;&#30340;&#20219;&#21153;&#39046;&#22495;&#20013;&#23454;&#29616;&#20102;&#39046;&#20808;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;Orion-14B&#27169;&#22411;&#31995;&#21015;&#21450;&#20854;&#30456;&#20851;&#20195;&#30721;&#20844;&#24320;&#21487;&#35775;&#38382;&#65292;&#26088;&#22312;&#28608;&#21457;&#26410;&#26469;&#22312;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we introduce Orion-14B, a collection of multilingual large language models with 14 billion parameters. We utilize a data scheduling approach to train a foundational model on a diverse corpus of 2.5 trillion tokens, sourced from texts in English, Chinese, Japanese, Korean, and other languages. Additionally, we fine-tuned a series of models tailored for conversational applications and other specific use cases. Our evaluation results demonstrate that Orion-14B achieves state-of-the-art performance across a broad spectrum of tasks. We make the Orion-14B model family and its associated code publicly accessible https://github.com/OrionStarAI/Orion, aiming to inspire future research and practical applications in the field.
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35843;&#30740;&#20102;&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#20013;&#32034;&#36180;&#26816;&#27979;&#30340;&#29616;&#26377;&#24037;&#20316;&#65292;&#29305;&#21035;&#20851;&#27880;&#22810;&#35821;&#35328;&#25968;&#25454;&#21644;&#26041;&#27861;&#12290;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#23500;&#26377;&#25104;&#26524;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#38656;&#35201;&#26356;&#36890;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#23545;&#25239;&#36328;&#22810;&#35821;&#35328;&#21644;&#27169;&#24577;&#30340;&#19981;&#23454;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2401.11969</link><description>&lt;p&gt;
&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#30340;&#32034;&#36180;&#26816;&#27979;&#65306;&#20851;&#20110;&#21333;&#35821;&#12289;&#22810;&#35821;&#21644;&#36328;&#35821;&#35328;&#30740;&#31350;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Claim Detection for Automated Fact-checking: A Survey on Monolingual, Multilingual and Cross-Lingual Research. (arXiv:2401.11969v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11969
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35843;&#30740;&#20102;&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#20013;&#32034;&#36180;&#26816;&#27979;&#30340;&#29616;&#26377;&#24037;&#20316;&#65292;&#29305;&#21035;&#20851;&#27880;&#22810;&#35821;&#35328;&#25968;&#25454;&#21644;&#26041;&#27861;&#12290;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#23500;&#26377;&#25104;&#26524;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#38656;&#35201;&#26356;&#36890;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#23545;&#25239;&#36328;&#22810;&#35821;&#35328;&#21644;&#27169;&#24577;&#30340;&#19981;&#23454;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#65292;&#21407;&#22240;&#26159;&#32593;&#32476;&#24179;&#21488;&#19978;&#34394;&#20551;&#20449;&#24687;&#30340;&#20256;&#25773;&#22686;&#21152;&#20102;&#12290;&#36825;&#36890;&#24120;&#26159;&#20316;&#20026;&#19968;&#31995;&#21015;&#20219;&#21153;&#30340;&#24207;&#21015;&#26469;&#23436;&#25104;&#30340;&#65292;&#21253;&#25324;&#65288;i&#65289;&#26816;&#27979;&#22312;&#32593;&#19978;&#27969;&#20256;&#30340;&#21477;&#23376;&#65292;&#36825;&#20123;&#21477;&#23376;&#26500;&#25104;&#38656;&#35201;&#39564;&#35777;&#30340;&#32034;&#36180;&#65292;&#28982;&#21518;&#26159;&#65288;ii&#65289;&#23545;&#36825;&#20123;&#32034;&#36180;&#36827;&#34892;&#39564;&#35777;&#30340;&#36807;&#31243;&#12290;&#26412;&#32508;&#36848;&#37325;&#28857;&#35752;&#35770;&#21069;&#32773;&#65292;&#35752;&#35770;&#20102;&#29616;&#26377;&#30340;&#21162;&#21147;&#65292;&#26088;&#22312;&#26816;&#27979;&#38656;&#35201;&#20107;&#23454;&#26680;&#26597;&#30340;&#32034;&#36180;&#65292;&#29305;&#21035;&#20851;&#27880;&#22810;&#35821;&#35328;&#25968;&#25454;&#21644;&#26041;&#27861;&#12290;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#23500;&#26377;&#25104;&#26524;&#30340;&#26041;&#21521;&#65292;&#30001;&#20110;&#38382;&#39064;&#30340;&#26497;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22312;&#20154;&#31867;&#34920;&#29616;&#26041;&#38754;&#65292;&#29616;&#26377;&#26041;&#27861;&#31163;&#21305;&#37197;&#20154;&#31867;&#34920;&#29616;&#36824;&#26377;&#24456;&#38271;&#30340;&#36335;&#35201;&#36208;&#12290;&#29305;&#21035;&#26159;&#65292;&#36328;&#22810;&#20010;&#31038;&#20132;&#24179;&#21488;&#30340;&#20449;&#24687;&#20256;&#25773;&#20197;&#22810;&#31181;&#35821;&#35328;&#21644;&#27169;&#24577;&#34920;&#36798;&#65292;&#38656;&#35201;&#26356;&#21152;&#36890;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#23545;&#25239;&#19981;&#23454;&#20449;&#24687;&#12290;&#25105;&#20204;&#38024;&#23545;&#22810;&#35821;&#35328;&#19981;&#23454;&#20449;&#24687;&#65292;&#25552;&#20379;&#20102;&#19968;&#20221;&#20840;&#38754;&#30340;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated fact-checking has drawn considerable attention over the past few decades due to the increase in the diffusion of misinformation on online platforms. This is often carried out as a sequence of tasks comprising (i) the detection of sentences circulating in online platforms which constitute claims needing verification, followed by (ii) the verification process of those claims. This survey focuses on the former, by discussing existing efforts towards detecting claims needing fact-checking, with a particular focus on multilingual data and methods. This is a challenging and fertile direction where existing methods are yet far from matching human performance due to the profoundly challenging nature of the issue. Especially, the dissemination of information across multiple social platforms, articulated in multiple languages and modalities demands more generalized solutions for combating misinformation. Focusing on multilingual misinformation, we present a comprehensive survey of exis
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#19968;&#31181;&#21517;&#20026;&#26816;&#32034;&#31034;&#33539;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#29305;&#23450;&#20110;&#36755;&#20837;&#26597;&#35810;&#30340;&#31034;&#33539;&#26469;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#37327;&#26679;&#26412;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#36824;&#20943;&#23569;&#20102;&#25163;&#21160;&#31034;&#20363;&#36873;&#25321;&#20013;&#30340;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2401.11624</link><description>&lt;p&gt;
&#36890;&#36807;&#26816;&#32034;&#31034;&#33539;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
In-context Learning with Retrieved Demonstrations for Language Models: A Survey. (arXiv:2401.11624v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11624
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#19968;&#31181;&#21517;&#20026;&#26816;&#32034;&#31034;&#33539;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#29305;&#23450;&#20110;&#36755;&#20837;&#26597;&#35810;&#30340;&#31034;&#33539;&#26469;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#37327;&#26679;&#26412;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#36824;&#20943;&#23569;&#20102;&#25163;&#21160;&#31034;&#20363;&#36873;&#25321;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24050;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#22312;&#36755;&#20837;&#19978;&#19979;&#25991;&#20013;&#36827;&#34892;&#23569;&#37327;&#26679;&#26412;&#30340;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#24182;&#22312;&#26032;&#20219;&#21153;&#19978;&#20855;&#26377;&#36866;&#24212;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#30340;ICL&#33021;&#21147;&#23545;&#20110;&#23569;&#26679;&#26412;&#31034;&#33539;&#30340;&#36873;&#25321;&#26159;&#25935;&#24863;&#30340;&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#30740;&#31350;&#36827;&#23637;&#26159;&#26816;&#32034;&#38024;&#23545;&#27599;&#20010;&#36755;&#20837;&#26597;&#35810;&#23450;&#21046;&#30340;&#31034;&#33539;&#12290;&#31034;&#33539;&#26816;&#32034;&#30340;&#23454;&#29616;&#30456;&#23545;&#31616;&#21333;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#25968;&#25454;&#24211;&#21644;&#26816;&#32034;&#31995;&#32479;&#12290;&#36825;&#19981;&#20165;&#25552;&#39640;&#20102;&#23398;&#20064;&#36807;&#31243;&#30340;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#32780;&#19988;&#24050;&#32463;&#35777;&#26126;&#21487;&#20197;&#20943;&#23569;&#25163;&#21160;&#31034;&#20363;&#36873;&#25321;&#20013;&#30340;&#20559;&#35265;&#12290;&#37492;&#20110;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#21644;&#22312;&#26816;&#32034;&#31034;&#33539;&#30340;ICL&#26041;&#38754;&#19981;&#26029;&#22686;&#38271;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#32508;&#36848;&#12290;&#22312;&#36825;&#39033;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#21644;&#27604;&#36739;&#20102;&#26816;&#32034;&#27169;&#22411;&#30340;&#19981;&#21516;&#35774;&#35745;&#36873;&#25321;&#65292;&#26816;&#32034;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Language models, especially pre-trained large language models, have showcased remarkable abilities as few-shot in-context learners (ICL), adept at adapting to new tasks with just a few demonstrations in the input context. However, the model's ability to perform ICL is sensitive to the choice of the few-shot demonstrations. Instead of using a fixed set of demonstrations, one recent development is to retrieve demonstrations tailored to each input query. The implementation of demonstration retrieval is relatively straightforward, leveraging existing databases and retrieval systems. This not only improves the efficiency and scalability of the learning process but also has been shown to reduce biases inherent in manual example selection. In light of the encouraging results and growing research in ICL with retrieved demonstrations, we conduct an extensive review of studies in this area. In this survey, we discuss and compare different design choices for retrieval models, retrieval training p
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#20010;&#23558;FAIR&#25968;&#25454;&#21407;&#21017;&#23884;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#26694;&#26550;&#65292;&#24182;&#25552;&#20379;&#20102;&#25972;&#21512;&#25351;&#21335;&#21644;&#26816;&#26597;&#28165;&#21333;&#12290;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#22312;&#31526;&#21512;FAIR&#26631;&#20934;&#30340;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#21644;&#20943;&#36731;&#20559;&#35265;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.11033</link><description>&lt;p&gt;
FAIR&#21040;&#20301;&#65306;&#25105;&#20204;&#22914;&#20309;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#24320;&#21457;&#21644;&#35780;&#20272;&#31526;&#21512;FAIR&#26631;&#20934;&#30340;&#25968;&#25454;&#38598;&#65311;
&lt;/p&gt;
&lt;p&gt;
FAIR Enough: How Can We Develop and Assess a FAIR-Compliant Dataset for Large Language Models' Training?. (arXiv:2401.11033v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11033
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#20010;&#23558;FAIR&#25968;&#25454;&#21407;&#21017;&#23884;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#26694;&#26550;&#65292;&#24182;&#25552;&#20379;&#20102;&#25972;&#21512;&#25351;&#21335;&#21644;&#26816;&#26597;&#28165;&#21333;&#12290;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#22312;&#31526;&#21512;FAIR&#26631;&#20934;&#30340;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#21644;&#20943;&#36731;&#20559;&#35265;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#20984;&#26174;&#20102;&#36947;&#24503;&#23454;&#36341;&#21644;&#25968;&#25454;&#23436;&#25972;&#24615;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#23558;FAIR&#65288;&#21487;&#21457;&#29616;&#12289;&#21487;&#35775;&#38382;&#12289;&#21487;&#20114;&#25805;&#20316;&#12289;&#21487;&#37325;&#29992;&#65289;&#25968;&#25454;&#21407;&#21017;&#23884;&#20837;&#21040;LLM&#35757;&#32451;&#20013;&#30340;&#26694;&#26550;&#12290;&#36825;&#19968;&#26041;&#27861;&#26631;&#24535;&#30528;&#26397;&#30528;&#31526;&#21512;FAIR&#26631;&#20934;&#30340;&#23454;&#36341;&#30340;&#36716;&#21464;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#20379;&#20102;&#23558;FAIR&#25968;&#25454;&#21407;&#21017;&#25972;&#21512;&#21040;LLM&#35757;&#32451;&#20013;&#30340;&#25351;&#21335;&#12290;&#35813;&#20030;&#25514;&#21253;&#25324;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#20154;&#21592;&#30340;&#26816;&#26597;&#28165;&#21333;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#23427;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#37325;&#28857;&#26159;&#22312;&#25105;&#20204;&#31526;&#21512;FAIR&#26631;&#20934;&#30340;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#21644;&#20943;&#36731;&#20559;&#35265;&#12290;&#36825;&#39033;&#24037;&#20316;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#21644;&#25968;&#25454;&#31185;&#23398;&#26159;&#37325;&#35201;&#30340;&#36129;&#29486;&#65292;&#20513;&#23548;&#22312;LLMs&#20013;&#20351;&#29992;&#24179;&#34913;&#21644;&#36947;&#24503;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advancements in Large Language Models (LLMs) highlight the need for ethical practices and data integrity. We introduce a framework that embeds FAIR (Findable, Accessible, Interoperable, Reusable) data principles into LLM training. This approach marks a shift towards practices compliant with FAIR standards. Our framework presents guidelines for integrating FAIR data principles into LLM training. This initiative includes a checklist for researchers and developers. We also demonstrate its practical application through a case study focused on bias identification and mitigation in our FAIR-compliant dataset. This work is a significant contribution to AI ethics and data science, advocating for balanced and ethical training methods in LLMs.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21457;&#23637;&#20102;&#19968;&#31181;&#22810;&#35821;&#35328;&#22768;&#23398;&#35789;&#23884;&#20837;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32570;&#20047;&#26631;&#27880;&#25968;&#25454;&#30340;&#38646;&#36164;&#28304;&#35821;&#35328;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#21644;&#22810;&#35821;&#35328;&#36716;&#31227;&#65292;&#35813;&#26041;&#27861;&#22312;&#38646;&#36164;&#28304;&#35821;&#35328;&#19978;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#36824;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#21644;&#35821;&#20041;&#26597;&#35810;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.10543</link><description>&lt;p&gt;
&#38646;&#36164;&#28304;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#22768;&#23398;&#35789;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Multilingual acoustic word embeddings for zero-resource languages. (arXiv:2401.10543v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10543
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21457;&#23637;&#20102;&#19968;&#31181;&#22810;&#35821;&#35328;&#22768;&#23398;&#35789;&#23884;&#20837;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32570;&#20047;&#26631;&#27880;&#25968;&#25454;&#30340;&#38646;&#36164;&#28304;&#35821;&#35328;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#21644;&#22810;&#35821;&#35328;&#36716;&#31227;&#65292;&#35813;&#26041;&#27861;&#22312;&#38646;&#36164;&#28304;&#35821;&#35328;&#19978;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#36824;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#21644;&#35821;&#20041;&#26597;&#35810;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35299;&#20915;&#20102;&#22312;&#32570;&#20047;&#26631;&#27880;&#25968;&#25454;&#30340;&#38646;&#36164;&#28304;&#35821;&#35328;&#20013;&#24320;&#21457;&#35821;&#38899;&#24212;&#29992;&#30340;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#20351;&#29992;&#22768;&#23398;&#35789;&#23884;&#20837;&#65288;AWE&#65289;-&#23558;&#21487;&#21464;&#26102;&#38271;&#30340;&#35821;&#38899;&#29255;&#27573;&#36716;&#25442;&#20026;&#22266;&#23450;&#32500;&#24230;&#30340;&#34920;&#31034;-&#24182;&#20351;&#29992;&#22810;&#35821;&#35328;&#36716;&#31227;&#65292;&#22312;&#22810;&#20010;&#36164;&#28304;&#20016;&#23500;&#30340;&#35821;&#35328;&#30340;&#26631;&#27880;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#38646;&#36164;&#28304;&#35821;&#35328;&#19978;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;AWE&#27169;&#22411;&#12290;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#36164;&#28304;&#20016;&#23500;&#35821;&#35328;&#30340;&#36873;&#25321;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;AWE&#24212;&#29992;&#20110;&#26031;&#29926;&#24076;&#37324;&#35821;&#24191;&#25773;&#20013;&#30340;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#20851;&#38190;&#35789;&#35782;&#21035;&#31995;&#32479;&#20013;&#65292;&#23637;&#31034;&#20102;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#26032;&#39062;&#30340;&#35821;&#20041;AWE&#27169;&#22411;&#25913;&#36827;&#20102;&#35821;&#20041;&#26597;&#35810;&#31034;&#20363;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research addresses the challenge of developing speech applications for zero-resource languages that lack labelled data. It specifically uses acoustic word embedding (AWE) -- fixed-dimensional representations of variable-duration speech segments -- employing multilingual transfer, where labelled data from several well-resourced languages are used for pertaining. The study introduces a new neural network that outperforms existing AWE models on zero-resource languages. It explores the impact of the choice of well-resourced languages. AWEs are applied to a keyword-spotting system for hate speech detection in Swahili radio broadcasts, demonstrating robustness in real-world scenarios. Additionally, novel semantic AWE models improve semantic query-by-example search.
&lt;/p&gt;</description></item><item><title>ChatQA&#26159;&#19968;&#31995;&#21015;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#65292;&#21487;&#20197;&#36798;&#21040;GPT-4&#32423;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#20004;&#38454;&#27573;&#30340;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#32467;&#26524;&#12290;&#20351;&#29992;&#23494;&#38598;&#26816;&#32034;&#22120;&#36827;&#34892;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#24494;&#35843;&#21487;&#20197;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#26597;&#35810;&#37325;&#20889;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#38477;&#20302;&#37096;&#32626;&#25104;&#26412;&#12290;ChatQA-70B&#22312;10&#20010;&#23545;&#35805;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#24179;&#22343;&#24471;&#20998;&#36229;&#36807;&#20102;GPT-4&#65292;&#19988;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#26469;&#33258;OpenAI GPT&#27169;&#22411;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2401.10225</link><description>&lt;p&gt;
ChatQA: &#26500;&#24314;GPT-4&#32423;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ChatQA: Building GPT-4 Level Conversational QA Models. (arXiv:2401.10225v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10225
&lt;/p&gt;
&lt;p&gt;
ChatQA&#26159;&#19968;&#31995;&#21015;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#65292;&#21487;&#20197;&#36798;&#21040;GPT-4&#32423;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#20004;&#38454;&#27573;&#30340;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#32467;&#26524;&#12290;&#20351;&#29992;&#23494;&#38598;&#26816;&#32034;&#22120;&#36827;&#34892;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#24494;&#35843;&#21487;&#20197;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#26597;&#35810;&#37325;&#20889;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#38477;&#20302;&#37096;&#32626;&#25104;&#26412;&#12290;ChatQA-70B&#22312;10&#20010;&#23545;&#35805;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#24179;&#22343;&#24471;&#20998;&#36229;&#36807;&#20102;GPT-4&#65292;&#19988;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#26469;&#33258;OpenAI GPT&#27169;&#22411;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ChatQA&#65292;&#19968;&#31995;&#21015;&#20855;&#26377;GPT-4&#32423;&#21035;&#20934;&#30830;&#24615;&#30340;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#38646;-shot&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#22788;&#29702;&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#26816;&#32034;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#22810;&#36718;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23494;&#38598;&#26816;&#32034;&#22120;&#30340;&#24494;&#35843;&#65292;&#36825;&#26679;&#21487;&#20197;&#25552;&#20379;&#19982;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#26597;&#35810;&#37325;&#20889;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#22823;&#22823;&#38477;&#20302;&#37096;&#32626;&#25104;&#26412;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;ChatQA-70B&#21487;&#20197;&#22312;10&#20010;&#23545;&#35805;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#24179;&#22343;&#20998;&#19978;&#36229;&#36807;GPT-4&#65288;54.14 vs. 53.90&#65289;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;OpenAI GPT&#27169;&#22411;&#30340;&#20219;&#20309;&#21512;&#25104;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce ChatQA, a family of conversational question answering (QA) models, that obtain GPT-4 level accuracies. Specifically, we propose a two-stage instruction tuning method that can significantly improve the zero-shot conversational QA results from large language models (LLMs). To handle retrieval in conversational QA, we fine-tune a dense retriever on a multi-turn QA dataset, which provides comparable results to using the state-of-the-art query rewriting model while largely reducing deployment cost. Notably, our ChatQA-70B can outperform GPT-4 in terms of average score on 10 conversational QA datasets (54.14 vs. 53.90), without relying on any synthetic data from OpenAI GPT models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;-&#26102;&#38388;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;ST-LLM&#65289;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;&#65292;&#36890;&#36807;&#21442;&#25968;&#25193;&#23637;&#21644;&#39044;&#35757;&#32451;&#26469;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#21033;&#29992;&#31354;&#38388;-&#26102;&#38388;&#23884;&#20837;&#27169;&#22359;&#23398;&#20064;&#26631;&#35760;&#30340;&#31354;&#38388;&#20301;&#32622;&#21644;&#20840;&#23616;&#26102;&#38388;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2401.10134</link><description>&lt;p&gt;
&#31354;&#38388;-&#26102;&#38388;&#22823;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Spatial-Temporal Large Language Model for Traffic Prediction. (arXiv:2401.10134v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;-&#26102;&#38388;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;ST-LLM&#65289;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;&#65292;&#36890;&#36807;&#21442;&#25968;&#25193;&#23637;&#21644;&#39044;&#35757;&#32451;&#26469;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#21033;&#29992;&#31354;&#38388;-&#26102;&#38388;&#23884;&#20837;&#27169;&#22359;&#23398;&#20064;&#26631;&#35760;&#30340;&#31354;&#38388;&#20301;&#32622;&#21644;&#20840;&#23616;&#26102;&#38388;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#39044;&#27979;&#26159;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#21382;&#21490;&#25968;&#25454;&#26469;&#39044;&#27979;&#29305;&#23450;&#20301;&#32622;&#30340;&#26410;&#26469;&#20132;&#36890;&#24773;&#20917;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#20132;&#36890;&#39044;&#27979;&#27169;&#22411;&#36890;&#24120;&#24378;&#35843;&#24320;&#21457;&#22797;&#26434;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#20294;&#23427;&#20204;&#30340;&#20934;&#30830;&#24615;&#24182;&#26410;&#30456;&#24212;&#25552;&#39640;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#19982;&#29616;&#26377;&#27169;&#22411;&#19981;&#21516;&#65292;LLMs&#20027;&#35201;&#36890;&#36807;&#21442;&#25968;&#25193;&#23637;&#21644;&#24191;&#27867;&#30340;&#39044;&#35757;&#32451;&#26469;&#36827;&#27493;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#22522;&#26412;&#32467;&#26500;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;-&#26102;&#38388;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;ST-LLM&#65289;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ST-LLM&#23558;&#27599;&#20010;&#20301;&#32622;&#30340;&#26102;&#38388;&#27493;&#38271;&#23450;&#20041;&#20026;&#26631;&#35760;&#65292;&#24182;&#32467;&#21512;&#31354;&#38388;-&#26102;&#38388;&#23884;&#20837;&#27169;&#22359;&#26469;&#23398;&#20064;&#26631;&#35760;&#30340;&#31354;&#38388;&#20301;&#32622;&#21644;&#20840;&#23616;&#26102;&#38388;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#34920;&#31034;&#34987;&#34701;&#21512;&#20197;&#20026;&#27599;&#20010;&#26631;&#35760;&#25552;&#20379;&#32479;&#19968;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic prediction, a critical component for intelligent transportation systems, endeavors to foresee future traffic at specific locations using historical data. Although existing traffic prediction models often emphasize developing complex neural network structures, their accuracy has not seen improvements accordingly. Recently, Large Language Models (LLMs) have shown outstanding capabilities in time series analysis. Differing from existing models, LLMs progress mainly through parameter expansion and extensive pre-training while maintaining their fundamental structures. In this paper, we propose a Spatial-Temporal Large Language Model (ST-LLM) for traffic prediction. Specifically, ST-LLM redefines the timesteps at each location as tokens and incorporates a spatial-temporal embedding module to learn the spatial location and global temporal representations of tokens. Then these representations are fused to provide each token with unified spatial and temporal information. Furthermore, we
&lt;/p&gt;</description></item><item><title>&#37096;&#20998;&#38899;&#26631;&#21270;&#26159;&#36873;&#25321;&#26631;&#35760;&#37096;&#20998;&#23383;&#31526;&#26469;&#25552;&#39640;&#38405;&#35835;&#21487;&#35835;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;&#19978;&#19979;&#25991;&#23545;&#27604;&#30340;&#37096;&#20998;&#38899;&#26631;&#21270;&#65288;CCPD&#65289;&#38598;&#25104;&#20102;&#29616;&#26377;&#30340;&#38463;&#25289;&#20271;&#38899;&#26631;&#21270;&#31995;&#32479;&#65292;&#24182;&#36890;&#36807;&#34913;&#37327;&#37096;&#20998;&#38899;&#26631;&#21270;&#30340;&#26032;&#25351;&#26631;&#26469;&#21028;&#26029;&#38656;&#35201;&#26631;&#35760;&#21738;&#20123;&#23383;&#31526;&#12290;</title><link>http://arxiv.org/abs/2401.08919</link><description>&lt;p&gt;
&#37096;&#20998;&#38899;&#26631;&#21270;&#65306;&#19968;&#31181;&#19978;&#19979;&#25991;&#23545;&#27604;&#25512;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Partial Diacritization: A Context-Contrastive Inference Approach. (arXiv:2401.08919v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08919
&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#38899;&#26631;&#21270;&#26159;&#36873;&#25321;&#26631;&#35760;&#37096;&#20998;&#23383;&#31526;&#26469;&#25552;&#39640;&#38405;&#35835;&#21487;&#35835;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;&#19978;&#19979;&#25991;&#23545;&#27604;&#30340;&#37096;&#20998;&#38899;&#26631;&#21270;&#65288;CCPD&#65289;&#38598;&#25104;&#20102;&#29616;&#26377;&#30340;&#38463;&#25289;&#20271;&#38899;&#26631;&#21270;&#31995;&#32479;&#65292;&#24182;&#36890;&#36807;&#34913;&#37327;&#37096;&#20998;&#38899;&#26631;&#21270;&#30340;&#26032;&#25351;&#26631;&#26469;&#21028;&#26029;&#38656;&#35201;&#26631;&#35760;&#21738;&#20123;&#23383;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#26631;&#21270;&#22312;&#25552;&#39640;&#38463;&#25289;&#20271;&#25991;&#26412;&#21487;&#35835;&#24615;&#21644;&#28040;&#38500;&#27495;&#20041;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#30446;&#21069;&#30340;&#21162;&#21147;&#20027;&#35201;&#38598;&#20013;&#22312;&#26631;&#35760;&#27599;&#20010;&#31526;&#21512;&#26465;&#20214;&#30340;&#23383;&#31526;&#65288;&#20840;&#38899;&#26631;&#21270;&#65289;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#37096;&#20998;&#38899;&#26631;&#21270;&#65288;PD&#65289;&#26159;&#36873;&#25321;&#26631;&#35760;&#23376;&#38598;&#20197;&#22312;&#24517;&#35201;&#26102;&#25552;&#20379;&#24110;&#21161;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36807;&#22810;&#30340;&#38899;&#26631;&#31526;&#21495;&#20250;&#22952;&#30861;&#29087;&#32451;&#35835;&#32773;&#65292;&#38477;&#20302;&#38405;&#35835;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#34892;&#20026;&#23454;&#39564;&#65292;&#24182;&#26174;&#31034;&#20986;&#37096;&#20998;&#26631;&#35760;&#30340;&#25991;&#26412;&#36890;&#24120;&#27604;&#23436;&#20840;&#26631;&#35760;&#30340;&#25991;&#26412;&#26356;&#23481;&#26131;&#38405;&#35835;&#65292;&#26377;&#26102;&#29978;&#33267;&#27604;&#32431;&#25991;&#26412;&#26356;&#23481;&#26131;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19978;&#19979;&#25991;&#23545;&#27604;&#30340;&#37096;&#20998;&#38899;&#26631;&#21270;&#65288;CCPD&#65289;-&#19968;&#31181;&#19982;&#29616;&#26377;&#38463;&#25289;&#20271;&#38899;&#26631;&#21270;&#31995;&#32479;&#26080;&#32541;&#38598;&#25104;&#30340;&#26032;&#26041;&#27861;&#12290;CCPD&#23545;&#27599;&#20010;&#21333;&#35789;&#36827;&#34892;&#20004;&#27425;&#22788;&#29702;&#65292;&#19968;&#27425;&#26377;&#19978;&#19979;&#25991;&#65292;&#19968;&#27425;&#27809;&#26377;&#65292;&#24182;&#19988;&#21482;&#23545;&#20004;&#27425;&#25512;&#29702;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#30340;&#23383;&#31526;&#36827;&#34892;&#38899;&#26631;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#34913;&#37327;&#37096;&#20998;&#38899;&#26631;&#21270;&#30340;&#26032;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diacritization plays a pivotal role in improving readability and disambiguating the meaning of Arabic texts. Efforts have so far focused on marking every eligible character (Full Diacritization). Comparatively overlooked, Partial Diacritzation (PD) is the selection of a subset of characters to be marked to aid comprehension where needed. Research has indicated that excessive diacritic marks can hinder skilled readers--reducing reading speed and accuracy. We conduct a behavioral experiment and show that partially marked text is often easier to read than fully marked text, and sometimes easier than plain text. In this light, we introduce Context-Contrastive Partial Diacritization (CCPD)--a novel approach to PD which integrates seamlessly with existing Arabic diacritization systems. CCPD processes each word twice, once with context and once without, and diacritizes only the characters with disparities between the two inferences. Further, we introduce novel indicators for measuring partial
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#24773;&#22659;&#21270;&#30340;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#20316;&#20026;&#23398;&#29983;&#23398;&#20064;&#25512;&#33616;&#30340;&#35299;&#37322;&#24037;&#20855;&#21644;&#25351;&#23548;&#12290;&#36890;&#36807;&#23450;&#20041;&#19978;&#19979;&#25991;&#24182;&#21033;&#29992;&#20154;&#24037;&#31574;&#21010;&#30340;&#20449;&#24687;&#28304;&#26469;&#35843;&#25511;LLM&#30340;&#29983;&#25104;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#33021;&#22312;&#19982;&#23398;&#29983;&#23545;&#35805;&#20013;&#25552;&#20379;&#35299;&#37322;&#21644;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2401.08517</link><description>&lt;p&gt;
&#25903;&#25345;&#23398;&#29983;&#20915;&#31574;&#30340;&#23398;&#20064;&#25512;&#33616;&#65306;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#24773;&#22659;&#21270;&#30340;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#23454;&#29616;&#23545;&#35805;&#35299;&#37322;&#21644;&#25351;&#23548;
&lt;/p&gt;
&lt;p&gt;
Supporting Student Decisions on Learning Recommendations: An LLM-Based Chatbot with Knowledge Graph Contextualization for Conversational Explainability and Mentoring. (arXiv:2401.08517v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08517
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#24773;&#22659;&#21270;&#30340;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#20316;&#20026;&#23398;&#29983;&#23398;&#20064;&#25512;&#33616;&#30340;&#35299;&#37322;&#24037;&#20855;&#21644;&#25351;&#23548;&#12290;&#36890;&#36807;&#23450;&#20041;&#19978;&#19979;&#25991;&#24182;&#21033;&#29992;&#20154;&#24037;&#31574;&#21010;&#30340;&#20449;&#24687;&#28304;&#26469;&#35843;&#25511;LLM&#30340;&#29983;&#25104;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#33021;&#22312;&#19982;&#23398;&#29983;&#23545;&#35805;&#20013;&#25552;&#20379;&#35299;&#37322;&#21644;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#29983;&#23545;&#23398;&#20064;&#25512;&#33616;&#30340;&#20915;&#31574;&#19982;&#20854;&#29702;&#35299;&#25512;&#33616;&#21407;&#22240;&#30340;&#33021;&#21147;&#26159;&#19981;&#21487;&#20998;&#21106;&#30340;&#65307;&#20182;&#20204;&#33021;&#21542;&#26681;&#25454;&#36825;&#31181;&#29702;&#35299;&#36827;&#34892;&#20462;&#25913;&#12290;&#22312;&#21508;&#31181;&#35299;&#37322;&#24615;&#26041;&#27861;&#20013;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#20855;&#26377;&#19982;&#21516;&#34892;&#25110;&#23548;&#24072;&#35752;&#35770;&#31867;&#20284;&#30340;&#28508;&#21147;&#26469;&#19982;&#23398;&#29983;&#36827;&#34892;&#23545;&#35805;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#33021;&#21147;&#20173;&#28982;&#19981;&#36275;&#20197;&#21462;&#20195;&#20154;&#31867;&#23548;&#24072;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#32842;&#22825;&#26426;&#22120;&#20154;&#20316;&#20026;&#23545;&#35805;&#30340;&#20013;&#20171;&#21644;&#35299;&#37322;&#30340;&#26377;&#38480;&#21644;&#21463;&#25511;&#29983;&#25104;&#30340;&#26469;&#28304;&#65292;&#20197;&#21033;&#29992;LLM&#30340;&#28508;&#21147;&#30340;&#21516;&#26102;&#20943;&#23569;&#20854;&#28508;&#22312;&#39118;&#38505;&#12290;&#25152;&#25552;&#20986;&#30340;&#22522;&#20110;LLM&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#25903;&#25345;&#23398;&#29983;&#29702;&#35299;&#23398;&#20064;&#36335;&#24452;&#25512;&#33616;&#12290;&#25105;&#20204;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#20316;&#20026;&#20154;&#24037;&#31574;&#21010;&#30340;&#20449;&#24687;&#28304;&#65292;&#36890;&#36807;&#23450;&#20041;&#20854;&#25552;&#31034;&#30340;&#19978;&#19979;&#25991;&#26469;&#35843;&#25511;LLM&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Student commitment towards a learning recommendation is not separable from their understanding of the reasons it was recommended to them; and their ability to modify it based on that understanding. Among explainability approaches, chatbots offer the potential to engage the student in a conversation, similar to a discussion with a peer or a mentor. The capabilities of chatbots, however, are still not sufficient to replace a human mentor, despite the advancements of generative AI (GenAI) and large language models (LLM). Therefore, we propose an approach to utilize chatbots as mediators of the conversation and sources of limited and controlled generation of explanations, to harvest the potential of LLMs while reducing their potential risks at the same time. The proposed LLM-based chatbot supports students in understanding learning-paths recommendations. We use a knowledge graph (KG) as a human-curated source of information, to regulate the LLM's output through defining its prompt's contex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23618;&#20248;&#21270;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65292;&#36890;&#36807;&#32852;&#21512;&#26080;&#30417;&#30563;&#21644;&#30417;&#30563;&#35757;&#32451;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.06980</link><description>&lt;p&gt;
&#36890;&#36807;&#21452;&#23618;&#20248;&#21270;&#36827;&#34892;&#32852;&#21512;&#26080;&#30417;&#30563;&#21644;&#30417;&#30563;&#35757;&#32451;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Joint Unsupervised and Supervised Training for Automatic Speech Recognition via Bilevel Optimization. (arXiv:2401.06980v1 [cs.CL] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23618;&#20248;&#21270;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65292;&#36890;&#36807;&#32852;&#21512;&#26080;&#30417;&#30563;&#21644;&#30417;&#30563;&#35757;&#32451;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#21452;&#23618;&#20248;&#21270;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#20219;&#21153;&#20013;&#30340;&#22768;&#23398;&#27169;&#22411;&#35757;&#32451;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#21452;&#23618;&#32852;&#21512;&#26080;&#30417;&#30563;&#21644;&#30417;&#30563;&#35757;&#32451;&#65288;BL-JUST&#65289;&#8221;&#12290;BL-JUST&#37319;&#29992;&#19979;&#23618;&#21644;&#19978;&#23618;&#20248;&#21270;&#65292;&#20998;&#21035;&#20351;&#29992;&#26080;&#30417;&#30563;&#25439;&#22833;&#21644;&#30417;&#30563;&#25439;&#22833;&#65292;&#21033;&#29992;&#26368;&#36817;&#22312;&#24809;&#32602;&#22411;&#21452;&#23618;&#20248;&#21270;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#26469;&#35299;&#20915;&#36825;&#19968;&#20855;&#26377;&#21487;&#25215;&#21463;&#22797;&#26434;&#24230;&#21644;&#20005;&#26684;&#25910;&#25947;&#24615;&#20445;&#35777;&#30340;&#25361;&#25112;&#24615;ASR&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a novel bilevel optimization-based training approach to training acoustic models for automatic speech recognition (ASR) tasks that we term {bi-level joint unsupervised and supervised training (BL-JUST)}. {BL-JUST employs a lower and upper level optimization with an unsupervised loss and a supervised loss respectively, leveraging recent advances in penalty-based bilevel optimization to solve this challenging ASR problem with affordable complexity and rigorous convergence guarantees.} To evaluate BL-JUST, extensive experiments on the LibriSpeech and TED-LIUM v2 datasets have been conducted. BL-JUST achieves superior performance over the commonly used pre-training followed by fine-tuning strategy.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;APLe&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#20196;&#29260;&#33258;&#36866;&#24212;&#30340;&#26041;&#24335;&#35843;&#33410;CLIP&#27169;&#22411;&#20013;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24335;&#25552;&#31034;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.06827</link><description>&lt;p&gt;
APLe: &#22810;&#27169;&#24335;&#25552;&#31034;&#23398;&#20064;&#30340;&#36880;&#20196;&#29260;&#33258;&#36866;&#24212;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
APLe: Token-Wise Adaptive for Multi-Modal Prompt Learning. (arXiv:2401.06827v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;APLe&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#20196;&#29260;&#33258;&#36866;&#24212;&#30340;&#26041;&#24335;&#35843;&#33410;CLIP&#27169;&#22411;&#20013;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24335;&#25552;&#31034;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#20026;&#21516;&#31867;&#27169;&#22411;&#26641;&#31435;&#20102;&#19968;&#20010;&#22522;&#20934;&#12290;&#29616;&#26377;&#30740;&#31350;&#20013;&#24050;&#32463;&#25506;&#32034;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#35768;&#22810;&#29305;&#24449;&#65292;&#21253;&#25324;&#23545;&#25991;&#26412;&#36755;&#20837;&#30340;&#25935;&#24863;&#24615;&#21644;&#36328;&#22810;&#27169;&#24335;&#25552;&#31034;&#30340;&#35843;&#33410;&#36807;&#31243;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#20013;&#65292;&#20511;&#37492;&#20102;&#22270;&#20687;&#34701;&#21512;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#36880;&#23618;&#35757;&#32451;&#24605;&#24819;&#65292;&#20197;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#24182;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#21462;&#20195;&#20102;&#25163;&#24037;&#35774;&#35745;&#30340;&#25552;&#31034;&#65292;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#25552;&#31034;&#26469;&#25552;&#21319;&#24615;&#33021;&#12290;&#22312;&#35299;&#20915;&#22810;&#27169;&#24335;&#25552;&#31034;&#25361;&#25112;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36880;&#20196;&#29260;&#33258;&#36866;&#24212;&#26041;&#27861;(APLe)&#65292;&#20197;&#39034;&#24207;&#26041;&#24335;&#35843;&#33410;CLIP&#20013;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#20004;&#31181;&#27169;&#24335;&#25552;&#31034;&#12290;APLe&#33021;&#22815;&#39640;&#25928;&#22320;&#25913;&#36827;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Vision-Language (V-L) models set the benchmark for generalization to downstream tasks among the noteworthy contenders. Many characteristics of the V-L model have been explored in existing research including the challenge of the sensitivity to text input and the tuning process across multi-modal prompts. With the advanced utilization of the V-L model like CLIP, recent approaches deploy learnable prompts instead of hand-craft prompts to boost the generalization performance and address the aforementioned challenges. Inspired by layer-wise training, which is wildly used in image fusion, we note that using a sequential training process to adapt different modalities branches of CLIP efficiently facilitates the improvement of generalization. In the context of addressing the multi-modal prompting challenge, we propose Token-wise Adaptive for Multi-modal Prompt Learning (APLe) for tuning both modalities prompts, vision and language, as tokens in a sequential manner. APLe addresses t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#25143;&#34892;&#20026;&#19982;&#20027;&#35266;&#35780;&#20272;&#22312;&#21475;&#35821;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38388;&#25509;&#20294;&#23458;&#35266;&#35780;&#20272;&#31995;&#32479;&#30340;&#26694;&#26550;&#65292;&#24182;&#21457;&#29616;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#23545;&#35805;&#20219;&#21153;&#20013;&#65292;&#19981;&#21516;&#30340;&#29992;&#25143;&#34892;&#20026;&#25351;&#26631;&#23545;&#35780;&#20272;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.04867</link><description>&lt;p&gt;
&#23545;&#29992;&#25143;&#34892;&#20026;&#36827;&#34892;&#20998;&#26512;&#20197;&#23458;&#35266;&#35780;&#20272;&#21475;&#35821;&#23545;&#35805;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
An Analysis of User Behaviours for Objectively Evaluating Spoken Dialogue Systems. (arXiv:2401.04867v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04867
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#25143;&#34892;&#20026;&#19982;&#20027;&#35266;&#35780;&#20272;&#22312;&#21475;&#35821;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38388;&#25509;&#20294;&#23458;&#35266;&#35780;&#20272;&#31995;&#32479;&#30340;&#26694;&#26550;&#65292;&#24182;&#21457;&#29616;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#23545;&#35805;&#20219;&#21153;&#20013;&#65292;&#19981;&#21516;&#30340;&#29992;&#25143;&#34892;&#20026;&#25351;&#26631;&#23545;&#35780;&#20272;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#21475;&#35821;&#23545;&#35805;&#31995;&#32479;&#30340;&#35780;&#20272;&#26041;&#26696;&#24456;&#37325;&#35201;&#65292;&#20294;&#20063;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#34429;&#28982;&#20027;&#35266;&#35780;&#20272;&#22312;&#29992;&#25143;&#23454;&#39564;&#20013;&#24120;&#29992;&#65292;&#20294;&#23458;&#35266;&#35780;&#20272;&#23545;&#20110;&#30740;&#31350;&#27604;&#36739;&#21644;&#21487;&#22797;&#21046;&#24615;&#26159;&#24517;&#35201;&#30340;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#29992;&#25143;&#34892;&#20026;&#38388;&#25509;&#20294;&#23458;&#35266;&#22320;&#35780;&#20272;&#31995;&#32479;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#31038;&#20132;&#23545;&#35805;&#20219;&#21153;&#20013;&#29992;&#25143;&#34892;&#20026;&#19982;&#20027;&#35266;&#35780;&#20272;&#20998;&#25968;&#20043;&#38388;&#30340;&#20851;&#31995;&#65306;&#19987;&#27880;&#20542;&#21548;&#12289;&#38754;&#35797;&#21644;&#39318;&#27425;&#20250;&#35758;&#23545;&#35805;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#29992;&#25143;&#35805;&#35821;&#26159;&#20027;&#35201;&#22240;&#32032;&#30340;&#23545;&#35805;&#20219;&#21153;&#20013;&#65292;&#22914;&#19987;&#27880;&#20542;&#21548;&#21644;&#38754;&#35797;&#65292;&#35805;&#35821;&#25968;&#37327;&#21644;&#21333;&#35789;&#25968;&#37327;&#31561;&#25351;&#26631;&#22312;&#35780;&#20272;&#20013;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;&#35266;&#23519;&#35821;&#35843;&#19981;&#27969;&#30021;&#31561;&#20063;&#21487;&#20197;&#25351;&#31034;&#27491;&#24335;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#65292;&#20363;&#22914;&#38754;&#35797;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22312;&#39640;&#20114;&#21160;&#24615;&#30340;&#23545;&#35805;&#20219;&#21153;&#20013;&#65292;&#22914;&#39318;&#27425;&#20250;&#35758;&#23545;&#35805;&#65292;&#29992;&#25143;&#24773;&#32490;&#21644;&#21442;&#19982;&#31243;&#24230;&#26356;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Establishing evaluation schemes for spoken dialogue systems is important, but it can also be challenging. While subjective evaluations are commonly used in user experiments, objective evaluations are necessary for research comparison and reproducibility. To address this issue, we propose a framework for indirectly but objectively evaluating systems based on users' behaviours. In this paper, to this end, we investigate the relationship between user behaviours and subjective evaluation scores in social dialogue tasks: attentive listening, job interview, and first-meeting conversation. The results reveal that in dialogue tasks where user utterances are primary, such as attentive listening and job interview, indicators like the number of utterances and words play a significant role in evaluation. Observing disfluency also can indicate the effectiveness of formal tasks, such as job interview. On the other hand, in dialogue tasks with high interactivity, such as first-meeting conversation, b
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#28151;&#21512;&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;&#22810;&#20010;&#36866;&#24230;&#35268;&#27169;&#30340;&#32842;&#22825;AI&#27169;&#22411;&#65292;&#21487;&#20197;&#36798;&#21040;&#25110;&#36229;&#36234;&#27604;&#23427;&#20204;&#26356;&#22823;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.02994</link><description>&lt;p&gt;
&#22522;&#20110;&#28151;&#21512;&#26041;&#27861;&#30340;&#32842;&#22825;AI&#27169;&#22411;&#65306;&#30456;&#23545;&#20110;&#19975;&#20159;&#32423;&#21442;&#25968;&#27169;&#22411;&#30340;&#26356;&#24265;&#20215;&#12289;&#26356;&#22909;&#30340;&#26367;&#20195;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Blending Is All You Need: Cheaper, Better Alternative to Trillion-Parameters LLM. (arXiv:2401.02994v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#28151;&#21512;&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;&#22810;&#20010;&#36866;&#24230;&#35268;&#27169;&#30340;&#32842;&#22825;AI&#27169;&#22411;&#65292;&#21487;&#20197;&#36798;&#21040;&#25110;&#36229;&#36234;&#27604;&#23427;&#20204;&#26356;&#22823;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20250;&#35805;&#22411;AI&#30740;&#31350;&#20013;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#27169;&#22411;&#37319;&#29992;&#20102;&#26356;&#22810;&#30340;&#21442;&#25968;&#65292;&#22914;ChatGPT&#31561;&#27169;&#22411;&#12290;&#34429;&#28982;&#36825;&#20123;&#24222;&#22823;&#30340;&#27169;&#22411;&#24448;&#24448;&#33021;&#29983;&#25104;&#26356;&#22909;&#30340;&#32842;&#22825;&#22238;&#22797;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#20869;&#23384;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65306;&#33021;&#21542;&#36890;&#36807;&#32452;&#21512;&#36739;&#23567;&#30340;&#27169;&#22411;&#26469;&#36798;&#21040;&#19982;&#21333;&#20010;&#22823;&#27169;&#22411;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#28151;&#21512;&#8221;&#30340;&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#23558;&#22810;&#20010;&#32842;&#22825;AI&#38598;&#25104;&#22312;&#19968;&#36215;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;&#65292;&#24403;&#29305;&#23450;&#36739;&#23567;&#30340;&#27169;&#22411;&#21327;&#21516;&#28151;&#21512;&#26102;&#65292;&#23427;&#20204;&#21487;&#20197;&#28508;&#22312;&#22320;&#36229;&#36234;&#25110;&#21305;&#25932;&#22823;&#22411;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;&#20165;&#38598;&#25104;&#19977;&#20010;&#36866;&#24230;&#35268;&#27169;&#30340;&#27169;&#22411;&#65288;6B/13B&#21442;&#25968;&#65289;&#23601;&#21487;&#20197;&#36798;&#21040;&#25110;&#29978;&#33267;&#36229;&#36234;ChatGPT&#65288;175B+&#21442;&#25968;&#65289;&#31561;&#22823;&#22411;&#27169;&#22411;&#30340;&#24615;&#33021;&#25351;&#26631;&#12290;&#36825;&#20010;&#20551;&#35774;&#32463;&#36807;&#20102;&#20005;&#26684;&#30340;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
In conversational AI research, there's a noticeable trend towards developing models with a larger number of parameters, exemplified by models like ChatGPT. While these expansive models tend to generate increasingly better chat responses, they demand significant computational resources and memory. This study explores a pertinent question: Can a combination of smaller models collaboratively achieve comparable or enhanced performance relative to a singular large model? We introduce an approach termed "blending", a straightforward yet effective method of integrating multiple chat AIs. Our empirical evidence suggests that when specific smaller models are synergistically blended, they can potentially outperform or match the capabilities of much larger counterparts. For instance, integrating just three models of moderate size (6B/13B paramaeters) can rival or even surpass the performance metrics of a substantially larger model like ChatGPT (175B+ paramaters). This hypothesis is rigorously tes
&lt;/p&gt;</description></item><item><title>AgentCoder&#26159;&#19968;&#31181;&#22522;&#20110;&#22810;Agent&#30340;&#20195;&#30721;&#29983;&#25104;&#21644;&#27979;&#35797;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#31243;&#24207;&#21592;Agent&#12289;&#27979;&#35797;&#35774;&#35745;&#24072;Agent&#21644;&#27979;&#35797;&#25191;&#34892;Agent&#30340;&#21327;&#20316;&#65292;&#23454;&#29616;&#20102;&#22312;&#24179;&#34913;&#20195;&#30721;&#29983;&#25104;&#21644;&#26377;&#25928;&#27979;&#35797;&#29992;&#20363;&#29983;&#25104;&#19982;&#25191;&#34892;&#26041;&#38754;&#30340;&#25361;&#25112;&#20013;&#30340;&#31361;&#30772;&#12290;</title><link>http://arxiv.org/abs/2312.13010</link><description>&lt;p&gt;
AgentCoder: &#22522;&#20110;&#22810;Agent&#30340;&#20195;&#30721;&#29983;&#25104;&#21644;&#36845;&#20195;&#27979;&#35797;&#19982;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
AgentCoder: Multi-Agent-based Code Generation with Iterative Testing and Optimisation. (arXiv:2312.13010v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.13010
&lt;/p&gt;
&lt;p&gt;
AgentCoder&#26159;&#19968;&#31181;&#22522;&#20110;&#22810;Agent&#30340;&#20195;&#30721;&#29983;&#25104;&#21644;&#27979;&#35797;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#31243;&#24207;&#21592;Agent&#12289;&#27979;&#35797;&#35774;&#35745;&#24072;Agent&#21644;&#27979;&#35797;&#25191;&#34892;Agent&#30340;&#21327;&#20316;&#65292;&#23454;&#29616;&#20102;&#22312;&#24179;&#34913;&#20195;&#30721;&#29983;&#25104;&#21644;&#26377;&#25928;&#27979;&#35797;&#29992;&#20363;&#29983;&#25104;&#19982;&#25191;&#34892;&#26041;&#38754;&#30340;&#25361;&#25112;&#20013;&#30340;&#31361;&#30772;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#30340;&#21457;&#23637;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#21457;&#23637;&#25512;&#21160;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#23454;&#29616;&#20102;&#38761;&#21629;&#24615;&#30340;&#31361;&#30772;&#65292;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#25552;&#39640;&#36719;&#20214;&#30340;&#25928;&#29575;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20294;&#22312;&#24179;&#34913;&#20195;&#30721;&#27573;&#30340;&#29983;&#25104;&#19982;&#26377;&#25928;&#30340;&#27979;&#35797;&#29992;&#20363;&#29983;&#25104;&#21644;&#25191;&#34892;&#26041;&#38754;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#8212;&#8212;&#22810;Agent&#21161;&#25163;&#20195;&#30721;&#29983;&#25104;(AgentCoder)&#65292;&#35813;&#35299;&#20915;&#26041;&#26696;&#21253;&#25324;&#19968;&#20010;&#22810;Agent&#26694;&#26550;&#21644;&#19987;&#38376;&#30340;Agent&#65306;&#31243;&#24207;&#21592;Agent&#12289;&#27979;&#35797;&#35774;&#35745;&#24072;Agent&#21644;&#27979;&#35797;&#25191;&#34892;Agent&#12290;&#22312;&#32534;&#30721;&#36807;&#31243;&#20013;&#65292;&#31243;&#24207;&#21592;Agent&#23558;&#26681;&#25454;&#27979;&#35797;&#25191;&#34892;Agent&#30340;&#21453;&#39304;&#37325;&#28857;&#20851;&#27880;&#20195;&#30721;&#30340;&#29983;&#25104;&#21644;&#25913;&#36827;&#12290;&#27979;&#35797;&#35774;&#35745;&#24072;Agent&#23558;&#20026;&#29983;&#25104;&#30340;&#20195;&#30721;&#29983;&#25104;&#27979;&#35797;&#29992;&#20363;&#65292;&#27979;&#35797;&#25191;&#34892;Agent&#23558;&#20351;&#29992;&#27979;&#35797;&#29992;&#20363;&#36816;&#34892;&#20195;&#30721;&#24182;&#23558;&#21453;&#39304;&#20889;&#20837;&#21040;&#32534;&#31243;&#32773;
&lt;/p&gt;
&lt;p&gt;
The advancement of natural language processing (NLP) has been significantly boosted by the development of transformer-based large language models (LLMs). These models have revolutionized NLP tasks, particularly in code generation, aiding developers in creating software with enhanced efficiency. Despite their advancements, challenges in balancing code snippet generation with effective test case generation and execution persist. To address these issues, this paper introduces Multi-Agent Assistant Code Generation (AgentCoder), a novel solution comprising a multi-agent framework with specialized agents: the programmer agent, the test designer agent, and the test executor agent. During the coding procedure, the programmer agent will focus on the code generation and refinement based on the test executor agent's feedback. The test designer agent will generate test cases for the generated code, and the test executor agent will run the code with the test cases and write the feedback to the prog
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;&#65292;&#21253;&#25324;&#19981;&#21516;&#25216;&#26415;&#30340;&#27010;&#36848;&#21644;&#27604;&#36739;&#12289;&#31639;&#27861;&#35780;&#20272;&#26041;&#27861;&#12289;&#24212;&#29992;&#22330;&#26223;&#20197;&#21450;&#24403;&#21069;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2312.07913</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Text Watermarking in the Era of Large Language Models. (arXiv:2312.07913v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.07913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;&#65292;&#21253;&#25324;&#19981;&#21516;&#25216;&#26415;&#30340;&#27010;&#36848;&#21644;&#27604;&#36739;&#12289;&#31639;&#27861;&#35780;&#20272;&#26041;&#27861;&#12289;&#24212;&#29992;&#22330;&#26223;&#20197;&#21450;&#24403;&#21069;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#27700;&#21360;&#31639;&#27861;&#22312;&#29256;&#26435;&#20445;&#25252;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#28982;&#32780;&#20854;&#33021;&#21147;&#21644;&#24212;&#29992;&#22330;&#26223;&#19968;&#30452;&#21463;&#38480;&#12290;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#21457;&#23637;&#20026;&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;&#30340;&#36827;&#27493;&#25171;&#24320;&#20102;&#26032;&#30340;&#26426;&#20250;&#12290;&#22823;&#35821;&#35328;&#27169;&#22411;&#19981;&#20165;&#36890;&#36807;&#20854;&#25991;&#26412;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#22686;&#24378;&#20102;&#25991;&#26412;&#27700;&#21360;&#31639;&#27861;&#30340;&#33021;&#21147;&#65292;&#36824;&#38656;&#35201;&#20351;&#29992;&#25991;&#26412;&#27700;&#21360;&#31639;&#27861;&#26469;&#20445;&#25252;&#33258;&#36523;&#30340;&#29256;&#26435;&#12290;&#26412;&#25991;&#23545;&#24403;&#21069;&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;&#30340;&#29616;&#29366;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#65292;&#21253;&#25324;&#22235;&#20010;&#20027;&#35201;&#26041;&#38754;&#65306;&#65288;1&#65289;&#19981;&#21516;&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;&#30340;&#27010;&#36848;&#21644;&#27604;&#36739;&#65307;&#65288;2&#65289;&#25991;&#26412;&#27700;&#21360;&#31639;&#27861;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#21253;&#25324;&#25104;&#21151;&#29575;&#12289;&#23545;&#25991;&#26412;&#36136;&#37327;&#30340;&#24433;&#21709;&#12289;&#40065;&#26834;&#24615;&#21644;&#38450;&#31713;&#25913;&#24615;&#65307;&#65288;3&#65289;&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;&#30340;&#28508;&#22312;&#24212;&#29992;&#22330;&#26223;&#65307;&#65288;4&#65289;&#24403;&#21069;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text watermarking algorithms play a crucial role in the copyright protection of textual content, yet their capabilities and application scenarios have been limited historically. The recent developments in large language models (LLMs) have opened new opportunities for the advancement of text watermarking techniques. LLMs not only enhance the capabilities of text watermarking algorithms through their text understanding and generation abilities but also necessitate the use of text watermarking algorithms for their own copyright protection. This paper conducts a comprehensive survey of the current state of text watermarking technology, covering four main aspects: (1) an overview and comparison of different text watermarking techniques; (2) evaluation methods for text watermarking algorithms, including their success rates, impact on text quality, robustness, and unforgeability; (3) potential application scenarios for text watermarking technology; (4) current challenges and future directions
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#21521;&#37327;&#23884;&#20837;&#21644;&#38750;&#32447;&#24615;&#38477;&#32500;&#26041;&#27861;&#65292;&#21457;&#29616;GPT-2&#19982;UMAP&#30340;&#32467;&#21512;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#31163;&#21644;&#32858;&#31867;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;DistilBERT&#27169;&#22411;&#21487;&#29992;&#20110;&#35782;&#21035;&#24635;&#32479;&#21644;&#28436;&#35762;&#30340;&#24180;&#20221;&#12290;</title><link>http://arxiv.org/abs/2312.01185</link><description>&lt;p&gt;
&#26102;&#38388;&#20013;&#30340;&#28063;&#28458;&#65306;&#32654;&#22269;&#21382;&#21490;&#20013;&#30340;&#19981;&#36830;&#32493;&#24615;
&lt;/p&gt;
&lt;p&gt;
A ripple in time: a discontinuity in American history. (arXiv:2312.01185v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.01185
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#21521;&#37327;&#23884;&#20837;&#21644;&#38750;&#32447;&#24615;&#38477;&#32500;&#26041;&#27861;&#65292;&#21457;&#29616;GPT-2&#19982;UMAP&#30340;&#32467;&#21512;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#31163;&#21644;&#32858;&#31867;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;DistilBERT&#27169;&#22411;&#21487;&#29992;&#20110;&#35782;&#21035;&#24635;&#32479;&#21644;&#28436;&#35762;&#30340;&#24180;&#20221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;Kaggle&#30340;&#22269;&#24773;&#21672;&#25991;&#25968;&#25454;&#38598;&#23545;&#32654;&#22269;&#21382;&#21490;&#30340;&#24635;&#20307;&#26102;&#38388;&#32447;&#21450;&#21672;&#25991;&#26412;&#36523;&#30340;&#29305;&#28857;&#21644;&#24615;&#36136;&#36827;&#34892;&#20102;&#19968;&#20123;&#20196;&#20154;&#24778;&#35766;&#65288;&#20063;&#26377;&#20123;&#19981;&#37027;&#20040;&#20196;&#20154;&#24778;&#35766;&#65289;&#30340;&#35266;&#23519;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#26041;&#27861;&#26159;&#20351;&#29992;&#21521;&#37327;&#23884;&#20837;&#65292;&#22914;BERT&#65288;DistilBERT&#65289;&#21644;GPT-2&#12290;&#34429;&#28982;&#24191;&#27867;&#35748;&#20026;BERT&#65288;&#21450;&#20854;&#21464;&#20307;&#65289;&#26368;&#36866;&#21512;NLP&#20998;&#31867;&#20219;&#21153;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;GPT-2&#32467;&#21512;UMAP&#31561;&#38750;&#32447;&#24615;&#38477;&#32500;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#31163;&#21644;&#26356;&#24378;&#30340;&#32858;&#31867;&#25928;&#26524;&#12290;&#36825;&#20351;&#24471;GPT-2 + UMAP&#25104;&#20026;&#19968;&#20010;&#26377;&#36259;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#22312;&#25105;&#20204;&#30340;&#24773;&#20917;&#19979;&#65292;&#19981;&#38656;&#35201;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#39044;&#35757;&#32451;&#30340;GPT-2&#27169;&#22411;&#23601;&#36275;&#22815;&#22909;&#29992;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#20102;&#32463;&#36807;&#24494;&#35843;&#30340;DistilBERT&#27169;&#22411;&#26469;&#26816;&#27979;&#21738;&#20301;&#24635;&#32479;&#21457;&#34920;&#20102;&#21738;&#31687;&#28436;&#35762;&#65292;&#24182;&#21462;&#24471;&#20102;&#38750;&#24120;&#22909;&#30340;&#32467;&#26524;&#65288;&#20934;&#30830;&#29575;&#20026;93\% - 95\%&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#36816;&#34892;&#24773;&#20917;&#65289;&#12290;&#20026;&#20102;&#30830;&#23450;&#20889;&#20316;&#24180;&#20221;&#65292;&#25105;&#20204;&#36824;&#25191;&#34892;&#20102;&#19968;&#20010;&#31867;&#20284;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this note we use the State of the Union Address (SOTU) dataset from Kaggle to make some surprising (and some not so surprising) observations pertaining to the general timeline of American history, and the character and nature of the addresses themselves. Our main approach is using vector embeddings, such as BERT (DistilBERT) and GPT-2.  While it is widely believed that BERT (and its variations) is most suitable for NLP classification tasks, we find out that GPT-2 in conjunction with nonlinear dimension reduction methods such as UMAP provide better separation and stronger clustering. This makes GPT-2 + UMAP an interesting alternative. In our case, no model fine-tuning is required, and the pre-trained out-of-the-box GPT-2 model is enough.  We also used a fine-tuned DistilBERT model for classification detecting which President delivered which address, with very good results (accuracy 93\% - 95\% depending on the run). An analogous task was performed to determine the year of writing, an
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23545;&#27604;&#20102;&#19977;&#31181;&#19981;&#21516;&#26041;&#27861;&#29992;&#20110;&#26816;&#27979;&#20154;&#31867;&#29983;&#25104;&#21644;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#22312;&#24615;&#33021;&#19978;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#24320;&#21457;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#39640;&#24230;&#21306;&#20998;&#24615;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2311.12373</link><description>&lt;p&gt;
&#36229;&#36234;&#22270;&#28789;: &#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#26041;&#27861;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Beyond Turing: A Comparative Analysis of Approaches for Detecting Machine-Generated Text. (arXiv:2311.12373v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.12373
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23545;&#27604;&#20102;&#19977;&#31181;&#19981;&#21516;&#26041;&#27861;&#29992;&#20110;&#26816;&#27979;&#20154;&#31867;&#29983;&#25104;&#21644;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#22312;&#24615;&#33021;&#19978;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#24320;&#21457;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#39640;&#24230;&#21306;&#20998;&#24615;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#29983;&#25104;&#25991;&#26412;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#28982;&#32780;&#21306;&#20998;&#20154;&#31867;&#29983;&#25104;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#35770;&#25991;&#23545;&#19977;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#28145;&#20837;&#35780;&#20272;&#65292;&#29992;&#20110;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#65306;&#20256;&#32479;&#30340;&#27973;&#23618;&#23398;&#20064;&#12289;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#24494;&#35843;&#21644;&#22810;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#24191;&#27867;&#30340;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#19978;&#32463;&#36807;&#20005;&#26684;&#27979;&#35797;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#35780;&#20272;&#23427;&#20204;&#22312;&#21306;&#20998;&#20154;&#31867;&#21644;&#26426;&#22120;&#26500;&#36896;&#35821;&#35328;&#26041;&#38754;&#33021;&#21147;&#30340;&#22522;&#20934;&#12290;&#32467;&#26524;&#26174;&#31034;&#19981;&#21516;&#26041;&#27861;&#30340;&#24615;&#33021;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#22240;&#27492;&#24378;&#35843;&#20102;&#22312;&#36825;&#19968;&#20851;&#38190;NLP&#39046;&#22495;&#30340;&#25345;&#32493;&#38656;&#27714;&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#24182;&#20026;&#26410;&#26469;&#26088;&#22312;&#21019;&#24314;&#24378;&#22823;&#19988;&#39640;&#24230;&#21306;&#20998;&#24615;&#27169;&#22411;&#30340;&#30740;&#31350;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Significant progress has been made on text generation by pre-trained language models (PLMs), yet distinguishing between human and machine-generated text poses an escalating challenge. This paper offers an in-depth evaluation of three distinct methods used to address this task: traditional shallow learning, Language Model (LM) fine-tuning, and Multilingual Model fine-tuning. These approaches are rigorously tested on a wide range of machine-generated texts, providing a benchmark of their competence in distinguishing between human-authored and machine-authored linguistic constructs. The results reveal considerable differences in performance across methods, thus emphasizing the continued need for advancement in this crucial area of NLP. This study offers valuable insights and paves the way for future research aimed at creating robust and highly discriminative models.
&lt;/p&gt;</description></item><item><title>&#24322;&#24120;&#32500;&#24230;&#21487;&#20197;&#32534;&#30721;&#20851;&#38190;&#30340;&#29305;&#23450;&#20219;&#21153;&#30693;&#35782;&#65292;&#24182;&#19988;&#19968;&#20010;&#21333;&#19968;&#30340;&#24322;&#24120;&#32500;&#24230;&#21487;&#20197;&#20197;&#26368;&#23567;&#30340;&#38169;&#35823;&#29575;&#23436;&#25104;&#19979;&#28216;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.17715</link><description>&lt;p&gt;
&#24322;&#24120;&#32500;&#24230;&#32534;&#30721;&#29305;&#23450;&#20219;&#21153;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Outlier Dimensions Encode Task-Specific Knowledge. (arXiv:2310.17715v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17715
&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#32500;&#24230;&#21487;&#20197;&#32534;&#30721;&#20851;&#38190;&#30340;&#29305;&#23450;&#20219;&#21153;&#30693;&#35782;&#65292;&#24182;&#19988;&#19968;&#20010;&#21333;&#19968;&#30340;&#24322;&#24120;&#32500;&#24230;&#21487;&#20197;&#20197;&#26368;&#23567;&#30340;&#38169;&#35823;&#29575;&#23436;&#25104;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#34920;&#31034;&#34987;&#23569;&#25968;&#20960;&#20010;&#20855;&#26377;&#26497;&#39640;&#26041;&#24046;&#30340;&#24322;&#24120;&#32500;&#24230;&#25152;&#20027;&#23548;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#35748;&#20026;&#65292;&#34429;&#28982;&#21435;&#38500;LLM&#34920;&#31034;&#20013;&#30340;&#24322;&#24120;&#32500;&#24230;&#20250;&#25439;&#23475;&#19979;&#28216;&#24615;&#33021;&#65292;&#20294;&#24322;&#24120;&#32500;&#24230;&#23545;&#23884;&#20837;&#34920;&#31034;&#30340;&#36136;&#37327;&#26159;&#26377;&#23475;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24494;&#35843;&#23545;&#24322;&#24120;&#32500;&#24230;&#30340;&#24433;&#21709;&#65292;&#24182;&#23637;&#31034;&#20102;&#20197;&#19979;&#32467;&#26524;&#65306;1&#65289;&#22312;&#39044;&#35757;&#32451;&#20013;&#20986;&#29616;&#30340;&#24322;&#24120;&#32500;&#24230;&#22312;&#24494;&#35843;&#27169;&#22411;&#20013;&#20173;&#28982;&#23384;&#22312;&#65292;2&#65289;&#19968;&#20010;&#21333;&#19968;&#30340;&#24322;&#24120;&#32500;&#24230;&#21487;&#20197;&#20197;&#26368;&#23567;&#30340;&#38169;&#35823;&#29575;&#23436;&#25104;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#24322;&#24120;&#32500;&#24230;&#21487;&#20197;&#32534;&#30721;&#20851;&#38190;&#30340;&#29305;&#23450;&#20219;&#21153;&#30693;&#35782;&#65292;&#24182;&#19988;&#19968;&#20010;&#34920;&#31034;&#22312;&#21333;&#20010;&#24322;&#24120;&#32500;&#24230;&#19978;&#30340;&#20540;&#20250;&#24433;&#21709;&#19979;&#28216;&#27169;&#22411;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representations from large language models (LLMs) are known to be dominated by a small subset of dimensions with exceedingly high variance. Previous works have argued that although ablating these outlier dimensions in LLM representations hurts downstream performance, outlier dimensions are detrimental to the representational quality of embeddings. In this study, we investigate how fine-tuning impacts outlier dimensions and show that 1) outlier dimensions that occur in pre-training persist in fine-tuned models and 2) a single outlier dimension can complete downstream tasks with a minimal error rate. Our results suggest that outlier dimensions can encode crucial task-specific knowledge and that the value of a representation in a single outlier dimension drives downstream model decisions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26368;&#23567;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#39640;&#32423;&#22768;&#26126;&#20013;&#23450;&#20041;&#25152;&#38656;&#30340;&#20195;&#29702;&#20154;&#34892;&#20026;&#65292;&#28982;&#21518;&#26500;&#24314;&#35299;&#30721;&#30417;&#35270;&#22120;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#20154;&#30340;&#24555;&#36895;&#35774;&#35745;&#21644;&#23454;&#26045;&#12290;</title><link>http://arxiv.org/abs/2310.08535</link><description>&lt;p&gt;
&#27491;&#24335;&#35268;&#23450;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#20154;&#30340;&#39640;&#32423;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Formally Specifying the High-Level Behavior of LLM-Based Agents. (arXiv:2310.08535v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26368;&#23567;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#39640;&#32423;&#22768;&#26126;&#20013;&#23450;&#20041;&#25152;&#38656;&#30340;&#20195;&#29702;&#20154;&#34892;&#20026;&#65292;&#28982;&#21518;&#26500;&#24314;&#35299;&#30721;&#30417;&#35270;&#22120;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#20154;&#30340;&#24555;&#36895;&#35774;&#35745;&#21644;&#23454;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;LLM&#39537;&#21160;&#30340;&#33258;&#20027;&#30446;&#26631;&#39537;&#21160;&#22411;&#20195;&#29702;&#20154;&#24050;&#25104;&#20026;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#38382;&#39064;&#30340;&#26377;&#21069;&#36884;&#30340;&#24037;&#20855;&#65292;&#32780;&#26080;&#38656;&#33719;&#24471;&#26114;&#36149;&#30340;&#20219;&#21153;&#29305;&#23450;&#30340;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#12290;&#30446;&#21069;&#65292;&#36825;&#31867;&#20195;&#29702;&#20154;&#30340;&#35774;&#35745;&#21644;&#23454;&#26045;&#26159;&#20020;&#26102;&#24615;&#30340;&#65292;&#22240;&#20026;LLM-based&#20195;&#29702;&#20154;&#21487;&#33021;&#24212;&#29992;&#20110;&#30340;&#21508;&#31181;&#20219;&#21153;&#30340;&#24191;&#27867;&#24615;&#36136;&#24847;&#21619;&#30528;&#19981;&#33021;&#26377;&#19968;&#31181;&#36866;&#29992;&#20110;&#25152;&#26377;&#24773;&#20917;&#30340;&#20195;&#29702;&#20154;&#35774;&#35745;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#31616;&#21270;&#20195;&#29702;&#20154;&#26500;&#24314;&#36807;&#31243;&#30340;&#26368;&#23567;&#29983;&#25104;&#26694;&#26550;&#26469;&#20943;&#36731;&#35774;&#35745;&#21644;&#23454;&#26045;&#26032;&#20195;&#29702;&#20154;&#30340;&#38590;&#24230;&#12290;&#25105;&#20204;&#24341;&#20837;&#30340;&#26694;&#26550;&#20801;&#35768;&#29992;&#25143;&#20197;&#39640;&#32423;&#22768;&#26126;&#30340;&#35268;&#33539;&#26041;&#24335;&#23450;&#20041;&#25152;&#38656;&#30340;&#20195;&#29702;&#20154;&#34892;&#20026;&#65292;&#28982;&#21518;&#20351;&#29992;&#36825;&#20010;&#35268;&#33539;&#26500;&#24314;&#35299;&#30721;&#30417;&#35270;&#22120;&#65292;&#20197;&#30830;&#20445;LLM&#20250;&#20135;&#29983;&#20855;&#26377;&#25152;&#38656;&#34892;&#20026;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#22768;&#26126;&#24615;&#26041;&#27861;&#65292;&#21363;&#25551;&#36848;&#34892;&#20026;&#32780;&#19981;&#32771;&#34385;&#22914;&#20309;&#23454;&#26045;&#25110;&#24378;&#21046;&#25191;&#34892;&#65292;&#21487;&#20197;&#23454;&#29616;&#24555;&#36895;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Autonomous, goal-driven agents powered by LLMs have recently emerged as promising tools for solving challenging problems without the need for task-specific finetuned models that can be expensive to procure. Currently, the design and implementation of such agents is ad hoc, as the wide variety of tasks that LLM-based agents may be applied to naturally means there can be no one-size-fits-all approach to agent design. In this work we aim to alleviate the difficulty of designing and implementing new agents by proposing a minimalistic generation framework that simplifies the process of building agents. The framework we introduce allows the user to define desired agent behaviors in a high-level, declarative specification that is then used to construct a decoding monitor which guarantees the LLM will produce an output exhibiting the desired behavior. Our declarative approach, in which the behavior is described without concern for how it should be implemented or enforced, enables rapid design,
&lt;/p&gt;</description></item><item><title>EMO&#25552;&#20986;&#20102;&#22320;&#29699;&#31227;&#21160;&#36317;&#31163;&#20248;&#21270;&#65288;EMO&#65289;&#26469;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36864;&#21270;&#29616;&#35937;&#12290;EMO&#21033;&#29992;&#20102;&#22320;&#29699;&#31227;&#21160;&#36317;&#31163;&#30340;&#29305;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#34892;&#30340;&#19978;&#30028;&#26469;&#31616;&#21270;&#35757;&#32451;&#12290;&#32463;&#36807;&#35780;&#20272;&#65292;&#21457;&#29616;EMO&#22312;&#35821;&#35328;&#27169;&#22411;&#19978;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.04691</link><description>&lt;p&gt;
EMO: Earth Mover Distance Optimization for Auto-Regressive Language Modeling. (arXiv:2310.04691v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
EMO: Earth Mover Distance Optimization for Auto-Regressive Language Modeling. (arXiv:2310.04691v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04691
&lt;/p&gt;
&lt;p&gt;
EMO&#25552;&#20986;&#20102;&#22320;&#29699;&#31227;&#21160;&#36317;&#31163;&#20248;&#21270;&#65288;EMO&#65289;&#26469;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36864;&#21270;&#29616;&#35937;&#12290;EMO&#21033;&#29992;&#20102;&#22320;&#29699;&#31227;&#21160;&#36317;&#31163;&#30340;&#29305;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#34892;&#30340;&#19978;&#30028;&#26469;&#31616;&#21270;&#35757;&#32451;&#12290;&#32463;&#36807;&#35780;&#20272;&#65292;&#21457;&#29616;EMO&#22312;&#35821;&#35328;&#27169;&#22411;&#19978;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#26159;&#20154;&#25991;&#26412;&#30340;&#27010;&#29575;&#27169;&#22411;&#12290;&#23427;&#20204;&#20027;&#35201;&#36890;&#36807;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#26041;&#27861;&#31561;&#21516;&#20110;&#26368;&#23567;&#21270;&#32463;&#39564;&#25968;&#25454;&#20998;&#24067;&#21644;&#27169;&#22411;&#20998;&#24067;&#20043;&#38388;&#30340;&#21069;&#21521;&#20132;&#21449;&#29109;&#12290;&#28982;&#32780;&#65292;&#24403;&#20174;&#36825;&#20123;&#27169;&#22411;&#23398;&#20064;&#30340;&#20998;&#24067;&#35299;&#30721;&#26102;&#65292;&#20173;&#28982;&#32463;&#24120;&#35266;&#23519;&#21040;&#21508;&#31181;&#36864;&#21270;&#29616;&#35937;&#12290;&#25105;&#20204;&#30830;&#23450;&#21069;&#21521;&#20132;&#21449;&#29109;&#20316;&#20026;&#20154;&#19982;&#27169;&#22411;&#20998;&#24067;&#23545;&#40784;&#30340;&#36317;&#31163;&#24230;&#37327;&#26159;&#27425;&#20248;&#30340;&#65292;&#21407;&#22240;&#26377;&#65306;&#65288;1&#65289;&#21484;&#22238;&#20248;&#21270;&#65292;&#65288;2&#65289;&#36127;&#26679;&#26412;&#22810;&#26679;&#24615;&#24573;&#35270;&#21644;&#65288;3&#65289;&#35757;&#32451;&#27979;&#35797;&#19981;&#21305;&#37197;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#30340;&#22320;&#29699;&#31227;&#21160;&#36317;&#31163;&#20248;&#21270;&#65288;EMO&#65289;&#12290;EMO&#21033;&#29992;&#22320;&#29699;&#31227;&#21160;&#36317;&#31163;&#30340;&#20869;&#22312;&#29305;&#24615;&#26469;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#12290;&#30001;&#20110;&#30452;&#25509;&#35745;&#31639;&#30340;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;EMO&#19978;&#30028;&#26469;&#31616;&#21270;&#31471;&#21040;&#31471;&#35757;&#32451;&#12290;&#32463;&#36807;&#24191;&#27867;&#35780;&#20272;&#20043;&#21518;&#65292;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35821;&#35328;&#27169;&#22411;&#19978;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural language models are probabilistic models of human text. They are predominantly trained using maximum likelihood estimation (MLE), which is equivalent to minimizing the forward cross-entropy between the empirical data distribution and the model distribution. However, various degeneration phenomena are still widely observed when decoding from the distributions learned by such models. We establish that the forward cross-entropy is suboptimal as a distance metric for aligning human and model distribution due to its (1) recall-prioritization (2) negative diversity ignorance and (3) train-test mismatch. In this paper, we propose Earth Mover Distance Optimization (EMO) for auto-regressive language modeling. EMO capitalizes on the inherent properties of earth mover distance to address the aforementioned challenges. Due to the high complexity of direct computation, we further introduce a feasible upper bound for EMO to ease end-to-end training. Upon extensive evaluation of language model
&lt;/p&gt;</description></item><item><title>"&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#26816;&#32034;&#22686;&#24378;&#21644;&#38271;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21457;&#29616;&#22312;&#38271;&#19978;&#19979;&#25991;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#30340;LLM&#20351;&#29992;4K&#19978;&#19979;&#25991;&#31383;&#21475;&#21487;&#20197;&#21462;&#24471;&#19982;&#36890;&#36807;&#38271;&#19978;&#19979;&#25991;&#31383;&#21475;&#24494;&#35843;&#30340;LLM&#20351;&#29992;16K&#19978;&#19979;&#25991;&#31383;&#21475;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#35745;&#31639;&#37327;&#35201;&#23569;&#24471;&#22810;&#12290;&#27492;&#22806;&#65292;&#26080;&#35770;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#22914;&#20309;&#65292;&#26816;&#32034;&#37117;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;LLM&#30340;&#24615;&#33021;&#12290;"</title><link>http://arxiv.org/abs/2310.03025</link><description>&lt;p&gt;
"&#26816;&#32034;&#36935;&#19978;&#38271;&#31687;&#22823;&#35821;&#35328;&#27169;&#22411;"
&lt;/p&gt;
&lt;p&gt;
Retrieval meets Long Context Large Language Models. (arXiv:2310.03025v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03025
&lt;/p&gt;
&lt;p&gt;
"&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#26816;&#32034;&#22686;&#24378;&#21644;&#38271;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21457;&#29616;&#22312;&#38271;&#19978;&#19979;&#25991;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#30340;LLM&#20351;&#29992;4K&#19978;&#19979;&#25991;&#31383;&#21475;&#21487;&#20197;&#21462;&#24471;&#19982;&#36890;&#36807;&#38271;&#19978;&#19979;&#25991;&#31383;&#21475;&#24494;&#35843;&#30340;LLM&#20351;&#29992;16K&#19978;&#19979;&#25991;&#31383;&#21475;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#35745;&#31639;&#37327;&#35201;&#23569;&#24471;&#22810;&#12290;&#27492;&#22806;&#65292;&#26080;&#35770;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#22914;&#20309;&#65292;&#26816;&#32034;&#37117;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;LLM&#30340;&#24615;&#33021;&#12290;"
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
"&#26368;&#36817;&#65292;&#25193;&#23637;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#32780;&#23558;&#26816;&#32034;&#19982;LLM&#30456;&#32467;&#21512;&#30340;&#35299;&#20915;&#26041;&#26696;&#24050;&#23384;&#22312;&#22810;&#24180;&#12290;&#33258;&#28982;&#32780;&#28982;&#30340;&#38382;&#39064;&#26159;&#65306;&#26816;&#32034;&#22686;&#24378;&#19982;&#38271;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#21738;&#20010;&#23545;&#19979;&#28216;&#20219;&#21153;&#26356;&#22909;&#65311;&#36825;&#20004;&#31181;&#26041;&#27861;&#21487;&#20197;&#32467;&#21512;&#36215;&#26469;&#20197;&#20860;&#39038;&#21033;&#24330;&#21527;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;LLM&#65288;&#21363;&#19968;&#20010;&#31169;&#26377;&#30340;43B GPT&#21644;Llama2-70B&#65289;&#26469;&#30740;&#31350;&#36825;&#20004;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;&#20063;&#35768;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#38271;&#19978;&#19979;&#25991;&#20219;&#21153;&#20013;&#65292;LLM&#20351;&#29992;4K&#19978;&#19979;&#25991;&#31383;&#21475;&#24182;&#36890;&#36807;&#31616;&#21333;&#30340;&#26816;&#32034;&#22686;&#24378;&#22312;&#29983;&#25104;&#26102;&#21487;&#20197;&#36798;&#21040;&#19982;&#36890;&#36807;&#38271;&#19978;&#19979;&#25991;&#31383;&#21475;&#36827;&#34892;&#20301;&#32622;&#25554;&#20540;&#30340;&#24494;&#35843;LLM&#20351;&#29992;16K&#19978;&#19979;&#25991;&#31383;&#21475;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#35745;&#31639;&#37327;&#35201;&#23569;&#24471;&#22810;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19981;&#35770;&#20854;&#25193;&#23637;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#22914;&#20309;&#65292;&#26816;&#32034;&#37117;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;LLM&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#26368;&#22909;&#30340;&#27169;&#22411;&#26159;&#20855;&#26377;32K&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#26816;&#32034;&#22686;&#24378;Llama2-70B&#12290;"
&lt;/p&gt;
&lt;p&gt;
Extending the context window of large language models (LLMs) is getting popular recently, while the solution of augmenting LLMs with retrieval has existed for years. The natural questions are: i) Retrieval-augmentation versus long context window, which one is better for downstream tasks? ii) Can both methods be combined to get the best of both worlds? In this work, we answer these questions by studying both solutions using two state-of-the-art pretrained LLMs, i.e., a proprietary 43B GPT and Llama2-70B. Perhaps surprisingly, we find that LLM with 4K context window using simple retrieval-augmentation at generation can achieve comparable performance to finetuned LLM with 16K context window via positional interpolation on long context tasks, while taking much less computation. More importantly, we demonstrate that retrieval can significantly improve the performance of LLMs regardless of their extended context window sizes. Our best model, retrieval-augmented Llama2-70B with 32K context wi
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#28389;&#29992;&#65292;&#21628;&#21505;&#35748;&#35782;&#21040;&#36825;&#20123;&#25361;&#25112;&#30340;&#32039;&#36843;&#24615;&#12290;&#30740;&#31350;&#25581;&#31034;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#28145;&#24230;&#20266;&#36896;&#12289;&#21512;&#25104;&#36523;&#20221;&#24694;&#24847;&#27963;&#21160;&#20197;&#21450;&#34394;&#20551;&#20449;&#24687;&#21644;&#27450;&#35784;&#26041;&#38754;&#21487;&#33021;&#24102;&#26469;&#30340;&#31038;&#20250;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.00737</link><description>&lt;p&gt;
&#12298;GenAI&#23545;&#25239;&#20154;&#24615;&#65306;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37034;&#24694;&#24212;&#29992;&#12299;
&lt;/p&gt;
&lt;p&gt;
GenAI Against Humanity: Nefarious Applications of Generative Artificial Intelligence and Large Language Models. (arXiv:2310.00737v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00737
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#28389;&#29992;&#65292;&#21628;&#21505;&#35748;&#35782;&#21040;&#36825;&#20123;&#25361;&#25112;&#30340;&#32039;&#36843;&#24615;&#12290;&#30740;&#31350;&#25581;&#31034;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#28145;&#24230;&#20266;&#36896;&#12289;&#21512;&#25104;&#36523;&#20221;&#24694;&#24847;&#27963;&#21160;&#20197;&#21450;&#34394;&#20551;&#20449;&#24687;&#21644;&#27450;&#35784;&#26041;&#38754;&#21487;&#33021;&#24102;&#26469;&#30340;&#31038;&#20250;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26159;&#25216;&#26415;&#30340;&#22855;&#36857;&#65292;&#20197;&#20854;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22810;&#27169;&#24335;&#20869;&#23481;&#29983;&#25104;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#32780;&#21463;&#21040;&#36190;&#25196;&#65292;&#23427;&#20204;&#25215;&#35834;&#24102;&#26469;&#19968;&#20010;&#21464;&#38761;&#30340;&#26410;&#26469;&#12290;&#20294;&#23601;&#20687;&#25152;&#26377;&#24378;&#22823;&#30340;&#24037;&#20855;&#19968;&#26679;&#65292;&#23427;&#20204;&#20063;&#26377;&#20854;&#38452;&#24433;&#23384;&#22312;&#12290;&#24819;&#35937;&#19968;&#19979;&#29983;&#27963;&#22312;&#19968;&#20010;&#28145;&#24230;&#20266;&#36896;&#19982;&#29616;&#23454;&#26080;&#27861;&#21306;&#20998;&#12289;&#21512;&#25104;&#36523;&#20221;&#32452;&#32455;&#24694;&#24847;&#27963;&#21160;&#12289;&#20197;&#21450;&#26377;&#30528;&#26080;&#19982;&#20262;&#27604;&#31934;&#30830;&#24230;&#30340;&#26377;&#38024;&#23545;&#24615;&#30340;&#34394;&#20551;&#20449;&#24687;&#25110;&#27450;&#35784;&#25163;&#27861;&#30340;&#19990;&#30028;&#12290;&#27426;&#36814;&#26469;&#21040;GenAI&#24212;&#29992;&#30340;&#40657;&#26263;&#38754;&#12290;&#26412;&#25991;&#19981;&#20165;&#26159;&#25506;&#32034;GenAI&#21644;LLMs&#28508;&#22312;&#28389;&#29992;&#30340;&#26053;&#31243;&#65292;&#20063;&#26159;&#21628;&#21505;&#35748;&#35782;&#21040;&#38754;&#20020;&#30340;&#25361;&#25112;&#30340;&#32039;&#36843;&#24615;&#12290;&#22312;&#25105;&#20204;&#33322;&#34892;&#20110;&#34394;&#20551;&#20449;&#24687;&#27963;&#21160;&#12289;&#24694;&#24847;&#20869;&#23481;&#29983;&#25104;&#19982;&#31934;&#23494;&#24694;&#24847;&#36719;&#20214;&#26500;&#24314;&#30340;&#28023;&#27915;&#20013;&#65292;&#25105;&#20204;&#23558;&#25581;&#31034;&#36825;&#22330;&#25105;&#20204;&#27491;&#22312;&#35265;&#35777;&#30340;GenAI&#38761;&#21629;&#20013;&#30340;&#31038;&#20250;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs) are marvels of technology; celebrated for their prowess in natural language processing and multimodal content generation, they promise a transformative future. But as with all powerful tools, they come with their shadows. Picture living in a world where deepfakes are indistinguishable from reality, where synthetic identities orchestrate malicious campaigns, and where targeted misinformation or scams are crafted with unparalleled precision. Welcome to the darker side of GenAI applications. This article is not just a journey through the meanders of potential misuse of GenAI and LLMs, but also a call to recognize the urgency of the challenges ahead. As we navigate the seas of misinformation campaigns, malicious content generation, and the eerie creation of sophisticated malware, we'll uncover the societal implications that ripple through the GenAI revolution we are witnessing. From AI-powered botnets on social med
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;AutomaTikZ&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;TikZ&#20316;&#20026;&#20013;&#38388;&#34920;&#31034;&#26469;&#23454;&#29616;&#31185;&#23398;&#22270;&#24418;&#30340;&#25991;&#26412;&#24341;&#23548;&#21512;&#25104;&#12290;&#36890;&#36807;&#24494;&#35843;LLaMA&#21644;&#24341;&#20837;CLiMA&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;DaTikZ&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#22312;&#20154;&#24037;&#35780;&#20272;&#21644;&#33258;&#21160;&#35780;&#20272;&#20013;&#65292;CLiMA&#21644;LLaMA&#22312;&#19982;&#20154;&#31867;&#21019;&#24314;&#30340;&#22270;&#24418;&#30340;&#30456;&#20284;&#24230;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;CLiMA&#36824;&#25913;&#36827;&#20102;tex&#12290;</title><link>http://arxiv.org/abs/2310.00367</link><description>&lt;p&gt;
AutomaTikZ: &#20351;&#29992;TikZ&#36827;&#34892;&#31185;&#23398;&#30690;&#37327;&#22270;&#30340;&#25991;&#26412;&#24341;&#23548;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
AutomaTikZ: Text-Guided Synthesis of Scientific Vector Graphics with TikZ. (arXiv:2310.00367v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;AutomaTikZ&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;TikZ&#20316;&#20026;&#20013;&#38388;&#34920;&#31034;&#26469;&#23454;&#29616;&#31185;&#23398;&#22270;&#24418;&#30340;&#25991;&#26412;&#24341;&#23548;&#21512;&#25104;&#12290;&#36890;&#36807;&#24494;&#35843;LLaMA&#21644;&#24341;&#20837;CLiMA&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;DaTikZ&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#22312;&#20154;&#24037;&#35780;&#20272;&#21644;&#33258;&#21160;&#35780;&#20272;&#20013;&#65292;CLiMA&#21644;LLaMA&#22312;&#19982;&#20154;&#31867;&#21019;&#24314;&#30340;&#22270;&#24418;&#30340;&#30456;&#20284;&#24230;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;CLiMA&#36824;&#25913;&#36827;&#20102;tex&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#25991;&#26412;&#29983;&#25104;&#20301;&#22270;&#22270;&#24418;&#24050;&#32463;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#65292;&#20294;&#26159;&#23545;&#20110;&#31185;&#23398;&#22270;&#24418;&#65292;&#24120;&#24120;&#26356;&#21916;&#27426;&#20351;&#29992;&#30690;&#37327;&#22270;&#24418;&#12290;&#37492;&#20110;&#30690;&#37327;&#22270;&#24418;&#36890;&#24120;&#20351;&#29992;&#20302;&#32423;&#21035;&#30340;&#22270;&#24418;&#21407;&#35821;&#36827;&#34892;&#32534;&#30721;&#65292;&#30452;&#25509;&#29983;&#25104;&#23427;&#20204;&#26159;&#22256;&#38590;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;TikZ&#20316;&#20026;&#31185;&#23398;&#22270;&#24418;&#30340;&#20013;&#38388;&#34920;&#31034;&#65292;TikZ&#26159;&#19968;&#31181;&#20247;&#25152;&#21608;&#30693;&#30340;&#25277;&#35937;&#22270;&#24418;&#35821;&#35328;&#65292;&#21487;&#20197;&#32534;&#35793;&#25104;&#30690;&#37327;&#22270;&#24418;&#12290;TikZ&#25552;&#20379;&#20102;&#38754;&#21521;&#20154;&#30340;&#39640;&#32423;&#21629;&#20196;&#65292;&#20174;&#32780;&#20351;&#24471;&#22312;&#20219;&#20309;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#26465;&#20214;&#35821;&#35328;&#24314;&#27169;&#21464;&#24471;&#23481;&#26131;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DaTikZ&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21253;&#21547;120k&#20010;&#19982;&#26631;&#39064;&#23545;&#40784;&#30340;TikZ&#32472;&#22270;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#22312;DaTikZ&#19978;&#23545;LLaMA&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#25105;&#20204;&#30340;&#26032;&#27169;&#22411;CLiMA&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#20102;&#22810;&#27169;&#24577;&#30340;CLIP&#23884;&#20837;&#12290;&#22312;&#20154;&#24037;&#21644;&#33258;&#21160;&#35780;&#20272;&#20013;&#65292;CLiMA&#21644;LLaMA&#22312;&#19982;&#20154;&#31867;&#21019;&#24314;&#30340;&#22270;&#24418;&#30340;&#30456;&#20284;&#24230;&#26041;&#38754;&#37117;&#20248;&#20110;&#21830;&#19994;&#30340;GPT-4&#21644;Claude 2&#65292;&#20854;&#20013;CLiMA&#36824;&#25913;&#36827;&#20102;tex&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating bitmap graphics from text has gained considerable attention, yet for scientific figures, vector graphics are often preferred. Given that vector graphics are typically encoded using low-level graphics primitives, generating them directly is difficult. To address this, we propose the use of TikZ, a well-known abstract graphics language that can be compiled to vector graphics, as an intermediate representation of scientific figures. TikZ offers human-oriented, high-level commands, thereby facilitating conditional language modeling with any large language model. To this end, we introduce DaTikZ, the first large-scale TikZ dataset consisting of 120k TikZ drawings aligned with captions. We fine-tune LLaMA on DaTikZ, as well as our new model CLiMA, which augments LLaMA with multimodal CLIP embeddings. In both human and automatic evaluation, CLiMA and LLaMA outperform commercial GPT-4 and Claude 2 in terms of similarity to human-created figures, with CLiMA additionally improving tex
&lt;/p&gt;</description></item><item><title>CB-Whisper&#26159;&#19968;&#31181;&#22522;&#20110;OpenAI&#30340;Whisper&#27169;&#22411;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#24320;&#25918;&#35789;&#27719;&#20851;&#38190;&#35789;&#26816;&#27979;&#65288;OV-KWS&#65289;&#35782;&#21035;&#32597;&#35265;&#30340;&#21629;&#21517;&#23454;&#20307;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#23454;&#20307;&#20316;&#20026;&#25552;&#31034;&#26469;&#25913;&#36827;&#35782;&#21035;&#25928;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#23454;&#20307;&#21484;&#22238;&#29575;&#30340;&#21516;&#26102;&#20250;&#30053;&#24494;&#22686;&#21152;&#28151;&#28102;&#38169;&#35823;&#29575;&#65288;MER&#65289;&#12290;</title><link>http://arxiv.org/abs/2309.09552</link><description>&lt;p&gt;
CB-Whisper: &#20351;&#29992;&#24320;&#25918;&#35789;&#27719;&#20851;&#38190;&#35789;&#26816;&#27979;&#36827;&#34892;&#19978;&#19979;&#25991;&#20559;&#32622;&#30340;Whisper
&lt;/p&gt;
&lt;p&gt;
CB-Whisper: Contextual Biasing Whisper using Open-Vocabulary Keyword-Spotting. (arXiv:2309.09552v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09552
&lt;/p&gt;
&lt;p&gt;
CB-Whisper&#26159;&#19968;&#31181;&#22522;&#20110;OpenAI&#30340;Whisper&#27169;&#22411;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#24320;&#25918;&#35789;&#27719;&#20851;&#38190;&#35789;&#26816;&#27979;&#65288;OV-KWS&#65289;&#35782;&#21035;&#32597;&#35265;&#30340;&#21629;&#21517;&#23454;&#20307;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#23454;&#20307;&#20316;&#20026;&#25552;&#31034;&#26469;&#25913;&#36827;&#35782;&#21035;&#25928;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#23454;&#20307;&#21484;&#22238;&#29575;&#30340;&#21516;&#26102;&#20250;&#30053;&#24494;&#22686;&#21152;&#28151;&#28102;&#38169;&#35823;&#29575;&#65288;MER&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#24448;&#24448;&#38590;&#20197;&#35782;&#21035;&#32597;&#35265;&#30340;&#21629;&#21517;&#23454;&#20307;&#65292;&#22914;&#20010;&#20154;&#22995;&#21517;&#12289;&#32452;&#32455;&#26426;&#26500;&#21644;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#19981;&#32463;&#24120;&#36935;&#21040;&#30340;&#19987;&#19994;&#26415;&#35821;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;OpenAI&#30340;Whisper&#27169;&#22411;&#30340;Contextual Biasing Whisper&#65288;CB-Whisper&#65289;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;Whisper&#32534;&#30721;&#22120;&#30340;&#38544;&#34255;&#29366;&#24577;&#25191;&#34892;&#24320;&#25918;&#35789;&#27719;&#20851;&#38190;&#35789;&#26816;&#27979;&#65288;OV-KWS&#65289;&#26469;&#35782;&#21035;&#29992;&#25143;&#23450;&#20041;&#30340;&#21629;&#21517;&#23454;&#20307;&#12290;&#35782;&#21035;&#20986;&#30340;&#23454;&#20307;&#34987;&#29992;&#20316;Whisper&#35299;&#30721;&#22120;&#30340;&#25552;&#31034;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;OV-KWS&#21644;ASR&#20219;&#21153;&#36827;&#34892;&#22810;&#20219;&#21153;&#35757;&#32451;&#30340;&#26041;&#27861;&#26469;&#20248;&#21270;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#21407;&#22987;Whisper&#27169;&#22411;&#30456;&#27604;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#20013;&#22269;Aishell&#28909;&#35789;&#23376;&#38598;&#21644;&#20004;&#20010;&#20869;&#37096;&#20195;&#30721;&#20999;&#25442;&#27979;&#35797;&#38598;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#23454;&#20307;&#21484;&#22238;&#29575;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#20869;&#37096;&#27979;&#35797;&#38598;&#19978;&#28151;&#28102;&#38169;&#35823;&#29575;&#65288;MER&#65289;&#30053;&#24494;&#22686;&#21152;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#24182;&#20351;&#29992;&#19981;&#21516;&#22823;&#23567;&#30340;Whisper&#27169;&#22411;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-end automatic speech recognition (ASR) systems often struggle to recognize rare name entities, such as personal names, organizations, and terminologies not frequently encountered in the training data. This paper presents Contextual Biasing Whisper (CB-Whisper), a novel ASR system based on OpenAI's Whisper model that can recognize user-defined name entities by performing open-vocabulary keyword-spotting (OV-KWS) using the hidden states of Whisper encoder. The recognized entities are used as prompts for the Whisper decoder. We first propose a multitask training approach with OV-KWS and ASR tasks to optimize the model. Experiments show that this approach substantially improves the entity recalls compared to the original Whisper model on Chinese Aishell hot word subsets and two internal code-switch test sets. However, we observed a slight increase in mixed-error-rate (MER) on internal test sets due to catastrophic forgetting. To address this problem and use different sizes of the Wh
&lt;/p&gt;</description></item><item><title>DiariST&#26159;&#31532;&#19968;&#20010;&#27969;&#24335;&#35821;&#38899;&#32763;&#35793;&#21644;&#35828;&#35805;&#32773;&#20998;&#31163;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#38598;&#25104;&#26631;&#35760;&#32423;&#24207;&#21015;&#21270;&#36755;&#20986;&#35757;&#32451;&#21644;t&#21521;&#37327;&#65292;&#23454;&#29616;&#20102;&#24378;&#22823;&#30340;ST&#21644;SD&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.08007</link><description>&lt;p&gt;
DiariST: &#24102;&#26377;&#35828;&#35805;&#32773;&#20998;&#31163;&#30340;&#27969;&#24335;&#35821;&#38899;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
DiariST: Streaming Speech Translation with Speaker Diarization. (arXiv:2309.08007v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08007
&lt;/p&gt;
&lt;p&gt;
DiariST&#26159;&#31532;&#19968;&#20010;&#27969;&#24335;&#35821;&#38899;&#32763;&#35793;&#21644;&#35828;&#35805;&#32773;&#20998;&#31163;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#38598;&#25104;&#26631;&#35760;&#32423;&#24207;&#21015;&#21270;&#36755;&#20986;&#35757;&#32451;&#21644;t&#21521;&#37327;&#65292;&#23454;&#29616;&#20102;&#24378;&#22823;&#30340;ST&#21644;SD&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#23545;&#35805;&#24405;&#38899;&#30340;&#31471;&#21040;&#31471;&#35821;&#38899;&#32763;&#35793;&#65288;ST&#65289;&#28041;&#21450;&#19968;&#20123;&#23578;&#26410;&#20805;&#20998;&#30740;&#31350;&#30340;&#25361;&#25112;&#65292;&#22914;&#27809;&#26377;&#20934;&#30830;&#30340;&#35789;&#26102;&#38388;&#25139;&#30340;&#35828;&#35805;&#32773;&#20998;&#31163;&#65288;SD&#65289;&#21644;&#22788;&#29702;&#37325;&#21472;&#35821;&#38899;&#30340;&#27969;&#24335;&#26041;&#24335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DiariST&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#27969;&#24335;&#30340;ST&#21644;SD&#35299;&#20915;&#26041;&#26696;&#12290;&#23427;&#22522;&#20110;&#22522;&#20110;&#31070;&#32463;&#20256;&#36882;&#22120;&#30340;&#27969;&#24335;ST&#31995;&#32479;&#26500;&#24314;&#65292;&#24182;&#38598;&#25104;&#20102;&#26368;&#21021;&#29992;&#20110;&#22810;&#35828;&#35805;&#20154;&#35821;&#38899;&#35782;&#21035;&#30340;&#26631;&#35760;&#32423;&#24207;&#21015;&#21270;&#36755;&#20986;&#35757;&#32451;&#21644;t&#21521;&#37327;&#12290;&#30001;&#20110;&#35813;&#39046;&#22495;&#32570;&#20047;&#35780;&#20272;&#22522;&#20934;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;AliMeeting&#35821;&#26009;&#24211;&#30340;&#21442;&#32771;&#20013;&#25991;&#36716;&#24405;&#25104;&#33521;&#25991;&#26469;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;DiariST-AliMeeting&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#26032;&#30340;&#25351;&#26631;&#65292;&#31216;&#20026;&#38750;&#29305;&#23450;&#35828;&#35805;&#32773;BLEU&#21644;&#29305;&#23450;&#35828;&#35805;&#32773;BLEU&#65292;&#20197;&#34913;&#37327;ST&#30340;&#36136;&#37327;&#65292;&#24182;&#32771;&#34385;SD&#30340;&#20934;&#30830;&#24615;&#12290;&#19982;&#22522;&#20110;Whisper&#30340;&#31163;&#32447;&#31995;&#32479;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#36827;&#34892;&#27969;&#24335;&#25512;&#29702;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#24378;&#22823;&#30340;ST&#21644;SD&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-end speech translation (ST) for conversation recordings involves several under-explored challenges such as speaker diarization (SD) without accurate word time stamps and handling of overlapping speech in a streaming fashion. In this work, we propose DiariST, the first streaming ST and SD solution. It is built upon a neural transducer-based streaming ST system and integrates token-level serialized output training and t-vector, which were originally developed for multi-talker speech recognition. Due to the absence of evaluation benchmarks in this area, we develop a new evaluation dataset, DiariST-AliMeeting, by translating the reference Chinese transcriptions of the AliMeeting corpus into English. We also propose new metrics, called speaker-agnostic BLEU and speaker-attributed BLEU, to measure the ST quality while taking SD accuracy into account. Our system achieves a strong ST and SD capability compared to offline systems based on Whisper, while performing streaming inference for
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#35821;&#38899;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#32479;&#19968;&#35821;&#38899;&#20998;&#35789;&#22120;SpeechTokenizer&#65292;&#36890;&#36807;&#32479;&#19968;&#35821;&#20041;&#21644;&#22768;&#23398;&#26631;&#35760;&#24182;&#37319;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#23618;&#32423;&#19978;&#35299;&#32806;&#35821;&#38899;&#20449;&#24687;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#32479;&#19968;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#65288;USLM&#65289;&#12290;</title><link>http://arxiv.org/abs/2308.16692</link><description>&lt;p&gt;
SpeechTokenizer&#65306;&#38754;&#21521;&#35821;&#38899;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#32479;&#19968;&#35821;&#38899;&#20998;&#35789;&#22120;
&lt;/p&gt;
&lt;p&gt;
SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language Models. (arXiv:2308.16692v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16692
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#35821;&#38899;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#32479;&#19968;&#35821;&#38899;&#20998;&#35789;&#22120;SpeechTokenizer&#65292;&#36890;&#36807;&#32479;&#19968;&#35821;&#20041;&#21644;&#22768;&#23398;&#26631;&#35760;&#24182;&#37319;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#23618;&#32423;&#19978;&#35299;&#32806;&#35821;&#38899;&#20449;&#24687;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#32479;&#19968;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#65288;USLM&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#35821;&#38899;&#22823;&#35821;&#35328;&#27169;&#22411;&#22522;&#20110;&#31163;&#25955;&#30340;&#35821;&#38899;&#34920;&#31034;&#65292;&#21487;&#20197;&#20998;&#20026;&#35821;&#20041;&#26631;&#35760;&#21644;&#22768;&#23398;&#26631;&#35760;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35821;&#38899;&#26631;&#35760;&#24182;&#38750;&#19987;&#20026;&#35821;&#38899;&#35821;&#35328;&#24314;&#27169;&#32780;&#35774;&#35745;&#12290;&#20026;&#20102;&#35780;&#20272;&#35821;&#38899;&#26631;&#35760;&#22312;&#26500;&#24314;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#30340;&#36866;&#24212;&#24615;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#31532;&#19968;&#20010;&#22522;&#20934;&#26631;&#20934;&#65292;&#21363;SLMTokBench&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#26080;&#35770;&#26159;&#35821;&#20041;&#26631;&#35760;&#36824;&#26159;&#22768;&#23398;&#26631;&#35760;&#37117;&#19981;&#36866;&#21512;&#36825;&#20010;&#30446;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SpeechTokenizer&#65292;&#19968;&#31181;&#38754;&#21521;&#35821;&#38899;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#32479;&#19968;&#35821;&#38899;&#20998;&#35789;&#22120;&#12290;SpeechTokenizer&#37319;&#29992;&#20855;&#26377;&#27531;&#24046;&#21521;&#37327;&#37327;&#21270;&#65288;RVQ&#65289;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#12290;&#36890;&#36807;&#32479;&#19968;&#35821;&#20041;&#21644;&#22768;&#23398;&#26631;&#35760;&#65292;SpeechTokenizer&#22312;&#19981;&#21516;&#30340;RVQ&#23618;&#32423;&#19978;&#20197;&#23618;&#27425;&#26041;&#24335;&#35299;&#32806;&#35821;&#38899;&#20449;&#24687;&#30340;&#19981;&#21516;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21033;&#29992;SpeechTokenizer&#30340;&#32479;&#19968;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#65288;USLM&#65289;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;SpeechTokenizer&#22312;&#35821;&#38899;&#37325;&#24314;&#21644;...
&lt;/p&gt;
&lt;p&gt;
Current speech large language models build upon discrete speech representations, which can be categorized into semantic tokens and acoustic tokens. However, existing speech tokens are not specifically designed for speech language modeling. To assess the suitability of speech tokens for building speech language models, we established the first benchmark, SLMTokBench. Our results indicate that neither semantic nor acoustic tokens are ideal for this purpose. Therefore, we propose SpeechTokenizer, a unified speech tokenizer for speech large language models. SpeechTokenizer adopts the Encoder-Decoder architecture with residual vector quantization (RVQ). Unifying semantic and acoustic tokens, SpeechTokenizer disentangles different aspects of speech information hierarchically across different RVQ layers. Furthermore, We construct a Unified Speech Language Model (USLM) leveraging SpeechTokenizer. Experiments show that SpeechTokenizer performs comparably to EnCodec in speech reconstruction and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#27169;&#22411;&#25237;&#31080;&#25552;&#31034;(MVP)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;(FSL)&#29615;&#22659;&#19979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#26597;&#35810;&#24615;&#33021;&#12290;MVP&#36890;&#36807;&#25552;&#31034;&#22810;&#20010;LLMs&#25191;&#34892;&#30456;&#21516;&#30340;&#20219;&#21153;&#65292;&#24182;&#23545;&#29983;&#25104;&#30340;&#36755;&#20986;&#36827;&#34892;&#22810;&#25968;&#25237;&#31080;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#32597;&#35265;&#30149;&#30340;&#35782;&#21035;&#21644;&#20998;&#31867;&#20219;&#21153;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2308.12890</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25237;&#31080;&#65306;&#29992;&#20110;&#32597;&#35265;&#30149;&#35782;&#21035;&#30340;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Vote: Prompting for Rare Disease Identification. (arXiv:2308.12890v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#27169;&#22411;&#25237;&#31080;&#25552;&#31034;(MVP)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;(FSL)&#29615;&#22659;&#19979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#26597;&#35810;&#24615;&#33021;&#12290;MVP&#36890;&#36807;&#25552;&#31034;&#22810;&#20010;LLMs&#25191;&#34892;&#30456;&#21516;&#30340;&#20219;&#21153;&#65292;&#24182;&#23545;&#29983;&#25104;&#30340;&#36755;&#20986;&#36827;&#34892;&#22810;&#25968;&#25237;&#31080;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#32597;&#35265;&#30149;&#30340;&#35782;&#21035;&#21644;&#20998;&#31867;&#20219;&#21153;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#20986;&#29616;&#24378;&#35843;&#20102;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#25552;&#31034;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;LLMs&#32463;&#24120;&#24212;&#29992;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;(FSL)&#30340;&#24773;&#22659;&#20013;&#65292;&#36825;&#37324;&#20219;&#21153;&#21482;&#20351;&#29992;&#24456;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#25191;&#34892;&#12290;FSL&#22312;&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;(AI)&#23376;&#39046;&#22495;&#20013;&#21464;&#24471;&#27969;&#34892;&#65292;&#21253;&#25324;&#29992;&#20110;&#20581;&#24247;&#30340;AI&#12290;&#32597;&#35265;&#30149;&#24433;&#21709;&#20154;&#21475;&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;&#22312;&#25968;&#25454;&#21487;&#29992;&#24615;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979; inherently &#38656;&#35201;FSL&#25216;&#26415;&#65292;&#23613;&#31649;&#20154;&#24037;&#25968;&#25454;&#25910;&#38598;&#21644;&#26631;&#27880;&#36153;&#26102;&#36153;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27169;&#22411;&#25237;&#31080;&#25552;&#31034;(MVP)&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#25913;&#21892;FSL&#29615;&#22659;&#20013;LLM&#26597;&#35810;&#24615;&#33021;&#30340;&#28789;&#27963;&#25552;&#31034;&#26041;&#27861;&#12290;MVP&#36890;&#36807;&#25552;&#31034;&#22810;&#20010;LLMs&#25191;&#34892;&#30456;&#21516;&#30340;&#20219;&#21153;&#65292;&#28982;&#21518;&#23545;&#29983;&#25104;&#30340;&#36755;&#20986;&#36827;&#34892;&#22810;&#25968;&#25237;&#31080;&#26469;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#22312;&#21333;&#27425;&#32597;&#35265;&#30149;&#35782;&#21035;&#21644;&#20998;&#31867;&#20219;&#21153;&#20013;&#30456;&#23545;&#20110;&#20219;&#20309;&#21333;&#20010;&#27169;&#22411;&#22312;&#38598;&#25104;&#27169;&#22411;&#20013;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32597;&#35265;&#30149;&#25968;&#25454;&#38598;&#29992;&#20110;FSL&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of generative Large Language Models (LLMs) emphasizes the need for accurate and efficient prompting approaches. LLMs are often applied in Few-Shot Learning (FSL) contexts, where tasks are executed with minimal training data. FSL has become popular in many Artificial Intelligence (AI) subdomains, including AI for health. Rare diseases, affecting a small fraction of the population, inherently require FSL techniques due to limited data availability, though manual data collection and annotation is costly and time-consuming. In this paper, we propose Models-Vote Prompting (MVP), a flexible prompting approach for improving the performance of LLM queries in FSL settings. MVP works by prompting numerous LLMs to perform the same tasks and then conducting a majority vote on the resulting outputs. This method achieves improved results to any one model in the ensemble on one-shot rare disease identification and classification tasks. We also release a novel rare disease dataset for FS
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;AI&#30340;&#21305;&#37197;&#35774;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#19987;&#19994;&#20107;&#23454;&#26680;&#26597;&#21592;&#30340;&#21512;&#20316;&#35774;&#35745;&#65292;&#21457;&#29616;&#24182;&#35299;&#20915;&#20107;&#23454;&#26680;&#26597;&#21592;&#19982;&#25216;&#26415;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#21512;&#20316;&#35774;&#35745;&#20250;&#35758;&#20135;&#29983;&#20102;11&#20010;&#26032;&#30340;&#35774;&#35745;&#24605;&#36335;&#65292;&#21253;&#25324;&#25552;&#39640;&#25928;&#29575;&#21644;&#20010;&#24615;&#21270;&#30340;&#20107;&#23454;&#26680;&#26597;&#24037;&#20855;&#65292;&#24110;&#21161;&#20107;&#23454;&#26680;&#26597;&#21592;&#20934;&#22791;&#26410;&#26469;&#30340;&#34394;&#20551;&#20449;&#24687;&#65292;&#30417;&#27979;&#20559;&#35265;&#65292;&#20197;&#21450;&#25903;&#25345;&#20869;&#37096;&#32452;&#32455;&#12290;</title><link>http://arxiv.org/abs/2308.07213</link><description>&lt;p&gt;
&#20154;&#26412;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20107;&#23454;&#26680;&#26597;&#65306;&#20351;&#29992;AI&#30340;&#21305;&#37197;&#35774;&#35745;&#19982;&#20107;&#23454;&#26680;&#26597;&#21592;&#21512;&#20316;
&lt;/p&gt;
&lt;p&gt;
Human-centered NLP Fact-checking: Co-Designing with Fact-checkers using Matchmaking for AI. (arXiv:2308.07213v1 [cs.HC] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;AI&#30340;&#21305;&#37197;&#35774;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#19987;&#19994;&#20107;&#23454;&#26680;&#26597;&#21592;&#30340;&#21512;&#20316;&#35774;&#35745;&#65292;&#21457;&#29616;&#24182;&#35299;&#20915;&#20107;&#23454;&#26680;&#26597;&#21592;&#19982;&#25216;&#26415;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#21512;&#20316;&#35774;&#35745;&#20250;&#35758;&#20135;&#29983;&#20102;11&#20010;&#26032;&#30340;&#35774;&#35745;&#24605;&#36335;&#65292;&#21253;&#25324;&#25552;&#39640;&#25928;&#29575;&#21644;&#20010;&#24615;&#21270;&#30340;&#20107;&#23454;&#26680;&#26597;&#24037;&#20855;&#65292;&#24110;&#21161;&#20107;&#23454;&#26680;&#26597;&#21592;&#20934;&#22791;&#26410;&#26469;&#30340;&#34394;&#20551;&#20449;&#24687;&#65292;&#30417;&#27979;&#20559;&#35265;&#65292;&#20197;&#21450;&#25903;&#25345;&#20869;&#37096;&#32452;&#32455;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19987;&#19994;&#20107;&#23454;&#26680;&#26597;&#22312;&#24212;&#23545;&#22823;&#37327;&#34394;&#20551;&#20449;&#24687;&#26041;&#38754;&#23384;&#22312;&#21487;&#25193;&#23637;&#24615;&#26377;&#38480;&#30340;&#25361;&#25112;&#12290;&#34429;&#28982;&#25552;&#20986;&#20102;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#26469;&#22686;&#24378;&#20107;&#23454;&#26680;&#26597;&#30340;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#20294;&#23398;&#26415;&#30740;&#31350;&#21644;&#20107;&#23454;&#26680;&#26597;&#32452;&#32455;&#22343;&#25253;&#21578;&#20102;&#23545;&#27492;&#31867;&#24037;&#20855;&#30340;&#26377;&#38480;&#37319;&#29992;&#65292;&#22240;&#20026;&#36825;&#20123;&#24037;&#20855;&#19981;&#36275;&#20197;&#19982;&#20107;&#23454;&#26680;&#26597;&#21592;&#30340;&#23454;&#36341;&#12289;&#20215;&#20540;&#35266;&#21644;&#38656;&#27714;&#20445;&#25345;&#19968;&#33268;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#21512;&#20316;&#35774;&#35745;&#26041;&#27861;&#65292;&#21363;AI&#30340;&#21305;&#37197;&#35774;&#35745;&#65292;&#35813;&#26041;&#27861;&#20419;&#36827;&#20107;&#23454;&#26680;&#26597;&#21592;&#12289;&#35774;&#35745;&#24072;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20154;&#21592;&#20849;&#21516;&#21457;&#29616;&#24212;&#20197;&#20309;&#31181;&#26041;&#24335;&#35299;&#20915;&#20107;&#23454;&#26680;&#26597;&#21592;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#19982;22&#21517;&#19987;&#19994;&#20107;&#23454;&#26680;&#26597;&#21592;&#36827;&#34892;&#30340;&#21512;&#20316;&#35774;&#35745;&#20250;&#35758;&#20135;&#29983;&#20102;11&#20010;&#26032;&#30340;&#35774;&#35745;&#24605;&#36335;&#12290;&#36825;&#20123;&#24605;&#36335;&#26377;&#21161;&#20110;&#25552;&#39640;&#20449;&#24687;&#25628;&#32034;&#12289;&#22788;&#29702;&#21644;&#25776;&#20889;&#25928;&#29575;&#20197;&#21450;&#20010;&#24615;&#21270;&#30340;&#20107;&#23454;&#26680;&#26597;&#65307;&#24110;&#21161;&#20107;&#23454;&#26680;&#26597;&#21592;&#20027;&#21160;&#20934;&#22791;&#26410;&#26469;&#30340;&#34394;&#20551;&#20449;&#24687;&#65307;&#30417;&#27979;&#28508;&#22312;&#30340;&#20559;&#35265;&#65307;&#24182;&#25903;&#25345;&#20869;&#37096;&#32452;&#32455;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key challenge in professional fact-checking is its limited scalability in relation to the magnitude of false information. While many Natural Language Processing (NLP) tools have been proposed to enhance fact-checking efficiency and scalability, both academic research and fact-checking organizations report limited adoption of such tooling due to insufficient alignment with fact-checker practices, values, and needs. To address this gap, we investigate a co-design method, Matchmaking for AI, which facilitates fact-checkers, designers, and NLP researchers to collaboratively discover what fact-checker needs should be addressed by technology and how. Our co-design sessions with 22 professional fact-checkers yielded a set of 11 novel design ideas. They assist in information searching, processing, and writing tasks for efficient and personalized fact-checking; help fact-checkers proactively prepare for future misinformation; monitor their potential biases; and support internal organization c
&lt;/p&gt;</description></item><item><title>&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#20013;&#65292;&#35201;&#20351;&#29992;&#22810;&#20010;&#26381;&#21153;&#22120;&#36149;&#37325;&#30340;GPU&#23548;&#33268;&#26174;&#33879;&#30340;&#25104;&#26412;&#38556;&#30861;&#65292;OWQ&#25552;&#20986;&#30340;&#19968;&#31181;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#21487;&#20197;&#22312;&#26368;&#23567;&#36136;&#37327;&#25439;&#22833;&#30340;&#24773;&#20917;&#19979;&#20943;&#23569;&#36825;&#31181;&#38480;&#21046;&#12290;&#23427;&#21487;&#20197;&#36890;&#36807;&#32771;&#34385;&#28608;&#27963;&#31163;&#32676;&#20540;&#26469;&#30830;&#23450;&#26435;&#20540;&#37327;&#21270;&#35823;&#24046;&#30340;&#22240;&#32032;&#65292;&#24182;&#20026;&#26131;&#21463;&#25915;&#20987;&#30340;&#26435;&#37325;&#20998;&#37197;&#39640;&#31934;&#24230;&#65292;&#20855;&#26377;&#19982;OPTQ&#30456;&#24403;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.02272</link><description>&lt;p&gt;
OWQ&#65306;&#22823;&#35821;&#35328;&#27169;&#22411;&#26435;&#37325;&#37327;&#21270;&#20013;&#28608;&#27963;&#31163;&#32676;&#20540;&#30340;&#21551;&#31034;
&lt;/p&gt;
&lt;p&gt;
OWQ: Lessons learned from activation outliers for weight quantization in large language models. (arXiv:2306.02272v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02272
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#20013;&#65292;&#35201;&#20351;&#29992;&#22810;&#20010;&#26381;&#21153;&#22120;&#36149;&#37325;&#30340;GPU&#23548;&#33268;&#26174;&#33879;&#30340;&#25104;&#26412;&#38556;&#30861;&#65292;OWQ&#25552;&#20986;&#30340;&#19968;&#31181;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#21487;&#20197;&#22312;&#26368;&#23567;&#36136;&#37327;&#25439;&#22833;&#30340;&#24773;&#20917;&#19979;&#20943;&#23569;&#36825;&#31181;&#38480;&#21046;&#12290;&#23427;&#21487;&#20197;&#36890;&#36807;&#32771;&#34385;&#28608;&#27963;&#31163;&#32676;&#20540;&#26469;&#30830;&#23450;&#26435;&#20540;&#37327;&#21270;&#35823;&#24046;&#30340;&#22240;&#32032;&#65292;&#24182;&#20026;&#26131;&#21463;&#25915;&#20987;&#30340;&#26435;&#37325;&#20998;&#37197;&#39640;&#31934;&#24230;&#65292;&#20855;&#26377;&#19982;OPTQ&#30456;&#24403;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25317;&#26377;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36890;&#36807;&#31616;&#21333;&#30340;&#25552;&#31034;&#35843;&#25972;&#21644;&#23569;&#37327;&#30340;&#31034;&#20363;&#65292;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20196;&#20154;&#24778;&#21497;&#30340;&#32467;&#26524;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#20219;&#21153;&#29305;&#23450;&#30340;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24040;&#22823;&#30340;&#23610;&#23544;&#35201;&#27714;&#29978;&#33267;&#22312;&#25512;&#29702;&#26102;&#20351;&#29992;&#22810;&#20010;&#26381;&#21153;&#22120;&#32423;&#30340;GPU&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#26174;&#33879;&#30340;&#25104;&#26412;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#26469;&#37327;&#21270;&#26435;&#37325;&#65292;&#20943;&#23569;&#36136;&#37327;&#25439;&#22833;&#12290;&#34429;&#28982;&#24050;&#30693;&#28608;&#27963;&#31163;&#32676;&#20540;&#22312;&#28608;&#27963;&#37327;&#21270;&#20013;&#23384;&#22312;&#38382;&#39064;&#65292;&#20294;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#36890;&#36807;&#32771;&#34385;&#28608;&#27963;&#31163;&#32676;&#20540;&#65292;&#25105;&#20204;&#21487;&#20197;&#30830;&#23450;&#23548;&#33268;&#26435;&#37325;&#37327;&#21270;&#35823;&#24046;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#26696;&#65292;&#21517;&#20026;Outlier-Aware Weight Quantization (OWQ)&#65292;&#23427;&#21487;&#20197;&#35782;&#21035;&#26131;&#21463;&#25915;&#20987;&#30340;&#26435;&#37325;&#24182;&#20026;&#23427;&#20204;&#20998;&#37197;&#39640;&#31934;&#24230;&#12290;&#25105;&#20204;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;OWQ&#29983;&#25104;&#30340;3.01&#20301;&#27169;&#22411;&#20855;&#26377;&#19982;OPTQ&#29983;&#25104;&#30340;4&#20301;&#27169;&#22411;&#30456;&#24403;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) with hundreds of billions of parameters show impressive results across various language tasks using simple prompt tuning and few-shot examples, without the need for task-specific fine-tuning. However, their enormous size requires multiple server-grade GPUs even for inference, creating a significant cost barrier. To address this limitation, we introduce a novel post-training quantization method for weights with minimal quality degradation. While activation outliers are known to be problematic in activation quantization, our theoretical analysis suggests that we can identify factors contributing to weight quantization errors by considering activation outliers. We propose an innovative PTQ scheme called outlier-aware weight quantization (OWQ), which identifies vulnerable weights and allocates high-precision to them. Our extensive experiments demonstrate that the 3.01-bit models produced by OWQ exhibit comparable quality to the 4-bit models generated by OPTQ.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#29486;&#30340;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#30340;&#23398;&#20064;&#29983;&#25104;&#26032;&#30340;&#31185;&#23398;&#26041;&#21521;&#65292;&#20811;&#26381;&#20102;&#26631;&#20934;&#26041;&#27861;&#22312;&#39044;&#27979;&#20851;&#32852;&#12289;&#24573;&#30053;&#19978;&#19979;&#25991;&#31561;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#27169;&#22411;&#20351;&#29992;&#20102;&#24341;&#25991;&#21644;&#30693;&#35782;&#22270;&#20851;&#31995;&#30340;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT4&#22312;&#29983;&#25104;&#21019;&#26032;&#24605;&#24819;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2305.14259</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#25991;&#29486;&#30340;&#35821;&#22659;&#21270;&#23398;&#20064;&#29983;&#25104;&#26032;&#30340;&#31185;&#23398;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Learning to Generate Novel Scientific Directions with Contextualized Literature-based Discovery. (arXiv:2305.14259v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#29486;&#30340;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#30340;&#23398;&#20064;&#29983;&#25104;&#26032;&#30340;&#31185;&#23398;&#26041;&#21521;&#65292;&#20811;&#26381;&#20102;&#26631;&#20934;&#26041;&#27861;&#22312;&#39044;&#27979;&#20851;&#32852;&#12289;&#24573;&#30053;&#19978;&#19979;&#25991;&#31561;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#27169;&#22411;&#20351;&#29992;&#20102;&#24341;&#25991;&#21644;&#30693;&#35782;&#22270;&#20851;&#31995;&#30340;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT4&#22312;&#29983;&#25104;&#21019;&#26032;&#24605;&#24819;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25991;&#29486;&#30340;&#21457;&#29616;&#65288;LBD&#65289;&#26088;&#22312;&#36890;&#36807;&#25366;&#25496;&#35770;&#25991;&#24182;&#29983;&#25104;&#20551;&#35774;&#26469;&#21457;&#29616;&#26032;&#30340;&#31185;&#23398;&#30693;&#35782;&#12290;&#26631;&#20934;&#30340;LBD&#20165;&#38480;&#20110;&#39044;&#27979;&#31163;&#25955;&#27010;&#24565;&#20043;&#38388;&#30340;&#20004;&#20004;&#20851;&#31995;&#65288;&#20363;&#22914;&#65292;&#33647;&#29289;&#21644;&#30142;&#30149;&#30340;&#20851;&#32852;&#65289;&#12290;LBD&#36824;&#24573;&#30053;&#20102;&#20851;&#38190;&#30340;&#19978;&#19979;&#25991;&#65292;&#20363;&#22914;&#23454;&#39564;&#35774;&#32622;&#65288;&#20363;&#22914;&#65292;&#33647;&#29289;&#35780;&#20272;&#30340;&#29305;&#23450;&#24739;&#32773;&#32676;&#20307;&#65289;&#21644;&#20154;&#31867;&#31185;&#23398;&#23478;&#32771;&#34385;&#30340;&#32972;&#26223;&#30693;&#35782;&#21644;&#21160;&#26426;&#65288;&#20363;&#22914;&#65292;&#25214;&#21040;&#27809;&#26377;&#29305;&#23450;&#21103;&#20316;&#29992;&#30340;&#33647;&#29289;&#20505;&#36873;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#19978;&#19979;&#25991;&#21270;LBD&#65288;C-LBD&#65289;&#34920;&#36848;&#26469;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#65306;&#20197;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#31185;&#23398;&#20551;&#35774;&#65292;&#21516;&#26102;&#23558;&#23427;&#20204;&#32852;&#31995;&#21040;&#25511;&#21046;&#20551;&#35774;&#25628;&#32034;&#31354;&#38388;&#30340;&#19978;&#19979;&#25991;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24314;&#27169;&#26694;&#26550;&#65292;&#20351;&#29992;&#33719;&#24471;&#30340;&#24341;&#25991;&#21644;&#30693;&#35782;&#22270;&#20851;&#31995;&#30340;&#24322;&#26500;&#32593;&#32476;&#20013;&#30340;&#8220;&#28789;&#24863;&#8221;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#20174;&#35770;&#25991;&#20013;&#27966;&#29983;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT4&#20542;&#21521;&#20110;&#29983;&#25104;&#20855;&#26377;&#21019;&#26032;&#24615;&#30340;&#24605;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
Literature-Based Discovery (LBD) aims to discover new scientific knowledge by mining papers and generating hypotheses. Standard LBD is limited to predicting pairwise relations between discrete concepts (e.g., drug-disease links). LBD also ignores critical contexts like experimental settings (e.g., a specific patient population where a drug is evaluated) and background knowledge and motivations that human scientists consider (e.g., to find a drug candidate without specific side effects). We address these limitations with a novel formulation of contextualized-LBD (C-LBD): generating scientific hypotheses in natural language, while grounding them in a context that controls the hypothesis search space. We present a modeling framework using retrieval of ``inspirations'' from a heterogeneous network of citations and knowledge graph relations, and create a new dataset derived from papers. Our evaluations with powerful large language models (LLMs) reveal that GPT4 tends to generate ideas with 
&lt;/p&gt;</description></item><item><title>SpecInfer&#26159;&#19968;&#31181;LLM&#26381;&#21153;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#25512;&#27979;&#25512;&#26029;&#21644;&#20196;&#29260;&#26641;&#39564;&#35777;&#26469;&#21152;&#36895;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#26029;&#36807;&#31243;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#20026;&#23427;&#20204;&#25552;&#20379;&#26381;&#21153;&#25152;&#38656;&#30340;&#31471;&#21040;&#31471;&#24310;&#36831;&#21644;&#35745;&#31639;&#35201;&#27714;&#65292;&#21516;&#26102;&#30830;&#20445;&#27169;&#22411;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.09781</link><description>&lt;p&gt;
SpecInfer&#65306;&#21033;&#29992;&#25512;&#27979;&#25512;&#26029;&#21644;&#20196;&#29260;&#26641;&#39564;&#35777;&#21152;&#36895;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification. (arXiv:2305.09781v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09781
&lt;/p&gt;
&lt;p&gt;
SpecInfer&#26159;&#19968;&#31181;LLM&#26381;&#21153;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#25512;&#27979;&#25512;&#26029;&#21644;&#20196;&#29260;&#26641;&#39564;&#35777;&#26469;&#21152;&#36895;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#26029;&#36807;&#31243;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#20026;&#23427;&#20204;&#25552;&#20379;&#26381;&#21153;&#25152;&#38656;&#30340;&#31471;&#21040;&#31471;&#24310;&#36831;&#21644;&#35745;&#31639;&#35201;&#27714;&#65292;&#21516;&#26102;&#30830;&#20445;&#27169;&#22411;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38656;&#35201;&#39640;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#65292;&#22240;&#27492;&#24555;&#36895;&#21644;&#24265;&#20215;&#22320;&#20026;&#23427;&#20204;&#25552;&#20379;&#26381;&#21153;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;SpecInfer&#65292;&#19968;&#20010;LLM&#26381;&#21153;&#31995;&#32479;&#65292;&#23427;&#21033;&#29992;&#25512;&#27979;&#25512;&#26029;&#21644;&#20196;&#29260;&#26641;&#39564;&#35777;&#21152;&#36895;&#29983;&#25104;&#24335;LLM&#25512;&#26029;&#12290;SpecInfer&#32972;&#21518;&#30340;&#20851;&#38190;&#26159;&#23558;&#21508;&#31181;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38598;&#20307;&#25552;&#21319;&#35843;&#25972;&#65292;&#20849;&#21516;&#39044;&#27979;LLM&#30340;&#36755;&#20986;&#65307; &#39044;&#27979;&#32467;&#26524;&#32452;&#32455;&#25104;&#19968;&#20010;&#20196;&#29260;&#26641;&#65292;&#20854;&#20013;&#27599;&#20010;&#33410;&#28857;&#37117;&#34920;&#31034;&#20505;&#36873;&#20196;&#29260;&#24207;&#21015;&#12290;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26641;&#30340;&#24182;&#34892;&#35299;&#30721;&#26426;&#21046;&#65292;&#20197;LMM&#20316;&#20026;&#20196;&#29260;&#26641;&#39564;&#35777;&#22120;&#26469;&#39564;&#35777;&#20196;&#29260;&#26641;&#25152;&#20195;&#34920;&#30340;&#25152;&#26377;&#20505;&#36873;&#20196;&#29260;&#24207;&#21015;&#30340;&#27491;&#30830;&#24615;&#12290;SpecInfer&#20351;&#29992;LLM&#20316;&#20026;&#20196;&#29260;&#26641;&#39564;&#35777;&#22120;&#65292;&#32780;&#19981;&#26159;&#22686;&#37327;&#35299;&#30721;&#22120;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#20026;&#29983;&#25104;&#24335;LLM&#25552;&#20379;&#26381;&#21153;&#25152;&#38656;&#30340;&#31471;&#21040;&#31471;&#24310;&#36831;&#21644;&#35745;&#31639;&#35201;&#27714;&#65292;&#21516;&#26102;&#21487;&#30830;&#20445;&#27169;&#22411;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The high computational and memory requirements of generative large language models (LLMs) make it challenging to serve them quickly and cheaply. This paper introduces SpecInfer, an LLM serving system that accelerates generative LLM inference with speculative inference and token tree verification. A key insight behind SpecInfer is to combine various collectively boost-tuned small language models to jointly predict the LLM's outputs; the predictions are organized as a token tree, whose nodes each represent a candidate token sequence. The correctness of all candidate token sequences represented by a token tree is verified by the LLM in parallel using a novel tree-based parallel decoding mechanism. SpecInfer uses an LLM as a token tree verifier instead of an incremental decoder, which significantly reduces the end-to-end latency and computational requirement for serving generative LLMs while provably preserving model quality.
&lt;/p&gt;</description></item><item><title>VCoT&#26159;&#19968;&#31181;&#20351;&#29992;&#24605;&#32500;&#38142;&#28608;&#21169;&#21644;&#35270;&#35273;&#35821;&#35328;&#32452;&#21512;&#36882;&#24402;&#22320;&#24357;&#21512;&#26102;&#24207;&#25968;&#25454;&#20013;&#36923;&#36753;&#24046;&#36317;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#20854;&#20351;&#29992;&#35270;&#35273;&#24341;&#23548;&#29983;&#25104;&#21512;&#25104;&#30340;&#22810;&#27169;&#24577;&#22635;&#20805;&#20197;&#28155;&#21152;&#19968;&#33268;&#19988;&#26032;&#39062;&#30340;&#20449;&#24687;&#65292;&#24182;&#20943;&#23569;&#38656;&#35201;&#26102;&#24207;&#25512;&#29702;&#30340;&#36923;&#36753;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2305.02317</link><description>&lt;p&gt;
&#35270;&#35273;&#24605;&#32500;&#38142;&#65306;&#22810;&#27169;&#24577;&#22635;&#20805;&#25216;&#26415;&#24357;&#21512;&#36923;&#36753;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Visual Chain of Thought: Bridging Logical Gaps with Multimodal Infillings. (arXiv:2305.02317v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02317
&lt;/p&gt;
&lt;p&gt;
VCoT&#26159;&#19968;&#31181;&#20351;&#29992;&#24605;&#32500;&#38142;&#28608;&#21169;&#21644;&#35270;&#35273;&#35821;&#35328;&#32452;&#21512;&#36882;&#24402;&#22320;&#24357;&#21512;&#26102;&#24207;&#25968;&#25454;&#20013;&#36923;&#36753;&#24046;&#36317;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#20854;&#20351;&#29992;&#35270;&#35273;&#24341;&#23548;&#29983;&#25104;&#21512;&#25104;&#30340;&#22810;&#27169;&#24577;&#22635;&#20805;&#20197;&#28155;&#21152;&#19968;&#33268;&#19988;&#26032;&#39062;&#30340;&#20449;&#24687;&#65292;&#24182;&#20943;&#23569;&#38656;&#35201;&#26102;&#24207;&#25512;&#29702;&#30340;&#36923;&#36753;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#33258;&#28982;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#65292;&#33021;&#20197;&#20154;&#31867;&#26041;&#24335;&#20998;&#35299;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#35813;&#33539;&#20363;&#30001;&#20110;&#20854;&#21333;&#27169;&#24577;&#24615;&#36136;&#24182;&#19988;&#20027;&#35201;&#24212;&#29992;&#20110;&#38382;&#31572;&#20219;&#21153;&#32780;&#21463;&#21040;&#38480;&#21046;&#12290;&#25105;&#20204;&#35748;&#20026;&#23558;&#35270;&#35273;&#22686;&#24378;&#20869;&#23481;&#32435;&#20837;&#25512;&#29702;&#26159;&#24517;&#35201;&#30340;&#65292;&#23588;&#20854;&#26159;&#38024;&#23545;&#22797;&#26434;&#24819;&#35937;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;VCoT&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#24605;&#32500;&#38142;&#28608;&#21169;&#21644;&#35270;&#35273;&#35821;&#35328;&#32452;&#21512;&#26469;&#36882;&#24402;&#22320;&#24357;&#21512;&#26102;&#24207;&#25968;&#25454;&#20013;&#30340;&#36923;&#36753;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#35270;&#35273;&#24341;&#23548;&#29983;&#25104;&#21512;&#25104;&#30340;&#22810;&#27169;&#24577;&#22635;&#20805;&#65292;&#20197;&#28155;&#21152;&#19968;&#33268;&#19988;&#26032;&#39062;&#30340;&#20449;&#24687;&#65292;&#24182;&#20943;&#23569;&#19979;&#28216;&#20219;&#21153;&#20013;&#38656;&#35201;&#26102;&#24207;&#25512;&#29702;&#30340;&#36923;&#36753;&#24046;&#36317;&#65292;&#21516;&#26102;&#25552;&#20379;&#27169;&#22411;&#30340;&#22810;&#27493;&#25512;&#29702;&#30340;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#23558;VCoT&#24212;&#29992;&#20110;&#35270;&#35273;&#21465;&#20107;&#21644;WikiHow&#25688;&#35201;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#23637;&#31034;&#20102;&#20854;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in large language models elicit reasoning in a chain of thought that allows models to decompose problems in a human-like fashion. Though this paradigm improves multi-step reasoning ability in language models, it is limited by being unimodal and applied mainly to question-answering tasks. We claim that incorporating visual augmentation into reasoning is essential, especially for complex, imaginative tasks. Consequently, we introduce VCoT, a novel method that leverages chain of thought prompting with vision-language grounding to recursively bridge the logical gaps within sequential data. Our method uses visual guidance to generate synthetic multimodal infillings that add consistent and novel information to reduce the logical gaps for downstream tasks that can benefit from temporal reasoning, as well as provide interpretability into models' multi-step reasoning. We apply VCoT to the Visual Storytelling and WikiHow summarization datasets and demonstrate through human evalua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#22330;&#26223;&#37325;&#26032;&#25490;&#21015;&#35268;&#21010;&#22120;&#65292;&#36890;&#36807;&#35821;&#35328;&#25351;&#23548;&#30340;&#31354;&#38388;&#27010;&#24565;&#26469;&#23454;&#29616;&#38271;&#25351;&#20196;&#20197;&#21450;&#22312;&#35757;&#32451;&#26102;&#20174;&#26410;&#35265;&#36807;&#30340;&#31354;&#38388;&#27010;&#24565;&#32452;&#21512;&#12290;&#26412;&#25991;&#30340;&#27169;&#22411;&#22312;&#25351;&#20196;&#23548;&#21521;&#25805;&#20316;&#22522;&#20934;&#27979;&#35797;&#20197;&#21450;&#32452;&#21512;&#25351;&#20196;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20248;&#20110;&#22522;&#20110;&#35821;&#35328;&#34920;&#36798;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#25104;&#21151;&#22320;&#35299;&#20915;&#20043;&#21069;&#20174;&#26410;&#35265;&#36807;&#30340;&#22797;&#26434;&#25351;&#20196;&#21644;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2304.14391</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#22330;&#26223;&#37325;&#26032;&#25490;&#21015;&#35268;&#21010;&#22120;
&lt;/p&gt;
&lt;p&gt;
Energy-based Models as Zero-Shot Planners for Compositional Scene Rearrangement. (arXiv:2304.14391v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#22330;&#26223;&#37325;&#26032;&#25490;&#21015;&#35268;&#21010;&#22120;&#65292;&#36890;&#36807;&#35821;&#35328;&#25351;&#23548;&#30340;&#31354;&#38388;&#27010;&#24565;&#26469;&#23454;&#29616;&#38271;&#25351;&#20196;&#20197;&#21450;&#22312;&#35757;&#32451;&#26102;&#20174;&#26410;&#35265;&#36807;&#30340;&#31354;&#38388;&#27010;&#24565;&#32452;&#21512;&#12290;&#26412;&#25991;&#30340;&#27169;&#22411;&#22312;&#25351;&#20196;&#23548;&#21521;&#25805;&#20316;&#22522;&#20934;&#27979;&#35797;&#20197;&#21450;&#32452;&#21512;&#25351;&#20196;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20248;&#20110;&#22522;&#20110;&#35821;&#35328;&#34920;&#36798;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#25104;&#21151;&#22320;&#35299;&#20915;&#20043;&#21069;&#20174;&#26410;&#35265;&#36807;&#30340;&#22797;&#26434;&#25351;&#20196;&#21644;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#24320;&#21457;&#19968;&#20010;&#22330;&#26223;&#37325;&#25490;&#26694;&#26550;&#65292;&#21487;&#20197;&#35299;&#37322;&#38271;&#25351;&#20196;&#20197;&#21450;&#22312;&#35757;&#32451;&#26102;&#20174;&#26410;&#35265;&#36807;&#30340;&#31354;&#38388;&#27010;&#24565;&#32452;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#30456;&#23545;&#23545;&#35937;&#25490;&#21015;&#30340;&#33021;&#37327;&#20989;&#25968;&#26469;&#34920;&#31034;&#35821;&#35328;&#25351;&#23548;&#30340;&#31354;&#38388;&#27010;&#24565;&#12290;&#35821;&#35328;&#35299;&#26512;&#22120;&#23558;&#25351;&#20196;&#26144;&#23556;&#21040;&#30456;&#24212;&#30340;&#33021;&#37327;&#20989;&#25968;&#65292;&#32780;&#24320;&#25918;&#24335;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#23558;&#23427;&#20204;&#30340;&#21442;&#25968;&#22522;&#20110;&#22330;&#26223;&#20013;&#30340;&#30456;&#20851;&#23545;&#35937;&#36827;&#34892;&#20462;&#27491;&#12290;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#27714;&#35299;&#33021;&#37327;&#20989;&#25968;&#30340;&#24635;&#21644;&#65292;&#24182;&#21033;&#29992;&#22522;&#20110;&#26412;&#22320;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#31574;&#30053;&#23558;&#23545;&#35937;&#37325;&#26032;&#23450;&#20301;&#21040;&#25512;&#26029;&#30340;&#30446;&#26631;&#20301;&#32622;&#65292;&#21363;&#21487;&#29983;&#25104;&#30446;&#26631;&#22330;&#26223;&#37197;&#32622;&#12290;&#25105;&#20204;&#22312;&#24050;&#24314;&#31435;&#30340;&#25351;&#20196;&#23548;&#21521;&#25805;&#20316;&#22522;&#20934;&#27979;&#35797;&#20197;&#21450;&#25105;&#20204;&#25552;&#20986;&#30340;&#32452;&#21512;&#25351;&#20196;&#22522;&#20934;&#27979;&#35797;&#20013;&#27979;&#35797;&#20102;&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#32489;&#25928;&#20248;&#20110;&#22522;&#20110;&#35821;&#35328;&#34920;&#36798;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#25104;&#21151;&#22320;&#35299;&#20915;&#20043;&#21069;&#20174;&#26410;&#35265;&#36807;&#30340;&#22797;&#26434;&#25351;&#20196;&#21644;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language is compositional; an instruction can express multiple relation constraints to hold among objects in a scene that a robot is tasked to rearrange. Our focus in this work is an instructable scene rearranging framework that generalizes to longer instructions and to spatial concept compositions never seen at training time. We propose to represent language-instructed spatial concepts with energy functions over relative object arrangements. A language parser maps instructions to corresponding energy functions and an open-vocabulary visual-language model grounds their arguments to relevant objects in the scene. We generate goal scene configurations by gradient descent on the sum of energy functions, one per language predicate in the instruction. Local vision-based policies then relocate objects to the inferred goal locations. We test our model on established instruction-guided manipulation benchmarks, as well as benchmarks of compositional instructions we introduce. We show our model 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21152;&#24378;&#21477;&#23376;&#36873;&#25321;&#26469;&#22686;&#24378;&#25991;&#26412;&#29983;&#25104;&#30340;&#23398;&#20064;&#29983;&#25104;&#38382;&#39064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#35774;&#35745;&#36873;&#25321;&#22120;&#21644;&#29983;&#25104;&#22120;&#20004;&#20010;&#27169;&#22359;&#65292;&#20351;&#27169;&#22411;&#26356;&#20851;&#27880;&#19982;&#31572;&#26696;&#30456;&#20851;&#30340;&#21477;&#23376;&#65292;&#24182;&#38544;&#24335;&#32467;&#21512;&#23616;&#37096;&#20449;&#24687;&#21644;&#20840;&#23616;&#20449;&#24687;&#26469;&#29983;&#25104;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#38382;&#39064;&#29983;&#25104;&#20219;&#21153;&#19978;&#20248;&#20110;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2212.12192</link><description>&lt;p&gt;
&#36890;&#36807;&#21152;&#24378;&#21477;&#23376;&#36873;&#25321;&#26469;&#22686;&#24378;&#25991;&#26412;&#29983;&#25104;&#30340;&#23398;&#20064;&#29983;&#25104;&#38382;&#39064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning to Generate Questions by Enhancing Text Generation with Sentence Selection. (arXiv:2212.12192v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21152;&#24378;&#21477;&#23376;&#36873;&#25321;&#26469;&#22686;&#24378;&#25991;&#26412;&#29983;&#25104;&#30340;&#23398;&#20064;&#29983;&#25104;&#38382;&#39064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#35774;&#35745;&#36873;&#25321;&#22120;&#21644;&#29983;&#25104;&#22120;&#20004;&#20010;&#27169;&#22359;&#65292;&#20351;&#27169;&#22411;&#26356;&#20851;&#27880;&#19982;&#31572;&#26696;&#30456;&#20851;&#30340;&#21477;&#23376;&#65292;&#24182;&#38544;&#24335;&#32467;&#21512;&#23616;&#37096;&#20449;&#24687;&#21644;&#20840;&#23616;&#20449;&#24687;&#26469;&#29983;&#25104;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#38382;&#39064;&#29983;&#25104;&#20219;&#21153;&#19978;&#20248;&#20110;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22238;&#31572;&#24863;&#30693;&#30340;&#38382;&#39064;&#29983;&#25104;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22238;&#31572;&#21644;&#38382;&#39064;&#30340;&#20449;&#24687;&#21487;&#20197;&#22312;&#19978;&#19979;&#25991;&#20013;&#30340;&#19968;&#20123;&#30456;&#20851;&#21477;&#23376;&#20013;&#25214;&#21040;&#65292;&#32780;&#19981;&#20165;&#20165;&#20381;&#36182;&#20110;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#21253;&#25324;&#20004;&#20010;&#27169;&#22359;&#65306;&#36873;&#25321;&#22120;&#21644;&#29983;&#25104;&#22120;&#12290;&#36873;&#25321;&#22120;&#24378;&#21046;&#27169;&#22411;&#26356;&#21152;&#20851;&#27880;&#19982;&#31572;&#26696;&#30456;&#20851;&#30340;&#21477;&#23376;&#65292;&#20197;&#25552;&#20379;&#38544;&#21547;&#30340;&#23616;&#37096;&#20449;&#24687;&#12290;&#29983;&#25104;&#22120;&#36890;&#36807;&#23558;&#36873;&#25321;&#22120;&#25552;&#20379;&#30340;&#23616;&#37096;&#20449;&#24687;&#19982;&#32534;&#30721;&#22120;&#32534;&#30721;&#30340;&#25972;&#20010;&#19978;&#19979;&#25991;&#30340;&#20840;&#23616;&#20449;&#24687;&#38544;&#24335;&#32467;&#21512;&#26469;&#29983;&#25104;&#38382;&#39064;&#12290;&#27169;&#22411;&#32852;&#21512;&#35757;&#32451;&#20197;&#21033;&#29992;&#20004;&#20010;&#27169;&#22359;&#20043;&#38388;&#30340;&#28508;&#22312;&#20132;&#20114;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#27604;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#38382;&#39064;&#29983;&#25104;&#20219;&#21153;&#19978;&#26356;&#22909;&#12290;&#20195;&#30721;&#20063;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce an approach for the answer-aware question generation problem. Instead of only relying on the capability of strong pre-trained language models, we observe that the information of answers and questions can be found in some relevant sentences in the context. Based on that, we design a model which includes two modules: a selector and a generator. The selector forces the model to more focus on relevant sentences regarding an answer to provide implicit local information. The generator generates questions by implicitly combining local information from the selector and global information from the whole context encoded by the encoder. The model is trained jointly to take advantage of latent interactions between the two modules. Experimental results on two benchmark datasets show that our model is better than strong pre-trained models for the question generation task. The code is also available.
&lt;/p&gt;</description></item></channel></rss>