<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#20026;&#20154;&#24037;&#32534;&#20889;&#30340;&#25991;&#26412;&#28155;&#21152;&#25351;&#20196;&#26631;&#31614;&#26469;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#36319;&#36394;&#35821;&#35328;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#25105;&#22686;&#24378;&#21644;&#33258;&#25105;&#31579;&#36873;&#29983;&#25104;&#35757;&#32451;&#31034;&#20363;&#65292;&#24182;&#19988;&#22312;Alpaca&#25490;&#34892;&#27036;&#19978;&#34920;&#29616;&#20986;&#38750;&#24120;&#39640;&#25928;&#30340;&#33258;&#21160;&#23545;&#40784;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.06259</link><description>&lt;p&gt;
&#20351;&#29992;&#25351;&#20196;&#21453;&#21521;&#32763;&#35793;&#30340;&#33258;&#21160;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Self-Alignment with Instruction Backtranslation. (arXiv:2308.06259v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#20026;&#20154;&#24037;&#32534;&#20889;&#30340;&#25991;&#26412;&#28155;&#21152;&#25351;&#20196;&#26631;&#31614;&#26469;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#36319;&#36394;&#35821;&#35328;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#25105;&#22686;&#24378;&#21644;&#33258;&#25105;&#31579;&#36873;&#29983;&#25104;&#35757;&#32451;&#31034;&#20363;&#65292;&#24182;&#19988;&#22312;Alpaca&#25490;&#34892;&#27036;&#19978;&#34920;&#29616;&#20986;&#38750;&#24120;&#39640;&#25928;&#30340;&#33258;&#21160;&#23545;&#40784;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#20026;&#20154;&#24037;&#32534;&#20889;&#30340;&#25991;&#26412;&#28155;&#21152;&#30456;&#24212;&#30340;&#25351;&#20196;&#26631;&#31614;&#26469;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#36319;&#36394;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;&#25351;&#20196;&#21453;&#21521;&#32763;&#35793;&#65292;&#23427;&#20174;&#22312;&#23569;&#37327;&#31181;&#23376;&#25968;&#25454;&#21644;&#32473;&#23450;&#30340;&#32593;&#32476;&#35821;&#26009;&#24211;&#19978;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#24320;&#22987;&#12290;&#31181;&#23376;&#27169;&#22411;&#29992;&#20110;&#36890;&#36807;&#20026;&#32593;&#32476;&#25991;&#26723;&#29983;&#25104;&#25351;&#20196;&#25552;&#31034;&#65288;&#33258;&#25105;&#22686;&#24378;&#65289;&#26469;&#26500;&#24314;&#35757;&#32451;&#31034;&#20363;&#65292;&#28982;&#21518;&#20174;&#36825;&#20123;&#20505;&#36873;&#31034;&#20363;&#20013;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#31034;&#20363;&#65288;&#33258;&#25105;&#31579;&#36873;&#65289;&#12290;&#28982;&#21518;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#26469;&#24494;&#35843;&#26356;&#24378;&#30340;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#25105;&#20204;&#26041;&#27861;&#30340;&#20004;&#27425;&#36845;&#20195;&#26469;&#24494;&#35843;LLaMa&#65292;&#25105;&#20204;&#24471;&#21040;&#30340;&#27169;&#22411;&#22312;Alpaca&#25490;&#34892;&#27036;&#19978;&#20987;&#36133;&#20102;&#25152;&#26377;&#20854;&#20182;&#22522;&#20110;LLaMa&#30340;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#33976;&#39311;&#25968;&#25454;&#65292;&#23637;&#31034;&#20102;&#38750;&#24120;&#26377;&#25928;&#30340;&#33258;&#21160;&#23545;&#40784;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a scalable method to build a high quality instruction following language model by automatically labelling human-written text with corresponding instructions. Our approach, named instruction backtranslation, starts with a language model finetuned on a small amount of seed data, and a given web corpus. The seed model is used to construct training examples by generating instruction prompts for web documents (self-augmentation), and then selecting high quality examples from among these candidates (self-curation). This data is then used to finetune a stronger model. Finetuning LLaMa on two iterations of our approach yields a model that outperforms all other LLaMa-based models on the Alpaca leaderboard not relying on distillation data, demonstrating highly effective self-alignment.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20027;&#35201;&#20851;&#27880;&#21360;&#24230;COVID-19&#25512;&#25991;&#30340;&#24773;&#24863;&#20998;&#26512;&#65292;&#36890;&#36807;&#25552;&#21462;Twitter&#25968;&#25454;&#24182;&#23545;&#20854;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#65292;&#26377;&#21161;&#20110;&#20998;&#26512;&#25512;&#25991;&#20013;&#38750;&#32467;&#26500;&#21270;&#12289;&#24322;&#26500;&#30340;&#35266;&#28857;&#12290;</title><link>http://arxiv.org/abs/2308.06241</link><description>&lt;p&gt;
Covid-19&#21360;&#24230;&#25512;&#25991;&#20998;&#31867;&#30340;&#20844;&#20247;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Covid-19 Public Sentiment Analysis for Indian Tweets Classification. (arXiv:2308.06241v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#35201;&#20851;&#27880;&#21360;&#24230;COVID-19&#25512;&#25991;&#30340;&#24773;&#24863;&#20998;&#26512;&#65292;&#36890;&#36807;&#25552;&#21462;Twitter&#25968;&#25454;&#24182;&#23545;&#20854;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#65292;&#26377;&#21161;&#20110;&#20998;&#26512;&#25512;&#25991;&#20013;&#38750;&#32467;&#26500;&#21270;&#12289;&#24322;&#26500;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#19990;&#30028;&#33539;&#22260;&#20869;&#21457;&#29983;&#20219;&#20309;&#29305;&#27530;&#20107;&#20214;&#26102;&#65292;&#31038;&#20132;&#23186;&#20307;&#24448;&#24448;&#26159;&#26368;&#24555;&#30340;&#26032;&#38395;&#20256;&#25773;&#36733;&#20307;&#65292;&#21516;&#26102;&#20063;&#28041;&#21450;&#21040;&#35813;&#20107;&#20214;&#30340;&#21518;&#26524;&#12290;&#20154;&#20204;&#21487;&#20197;&#36890;&#36807;&#31038;&#20132;&#32593;&#32476;&#25910;&#38598;&#26377;&#20851;&#20154;&#20204;&#24773;&#24863;&#12289;&#34892;&#20026;&#21644;&#35266;&#28857;&#30340;&#20016;&#23500;&#20449;&#24687;&#12290;&#26412;&#25991;&#20027;&#35201;&#20851;&#27880;&#23545;&#21360;&#24230;COVID-19&#25512;&#25991;&#30340;&#24773;&#24863;&#20998;&#26512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#25552;&#21462;Twitter&#25968;&#25454;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#12290;&#36825;&#26377;&#21161;&#20110;&#20998;&#26512;&#25512;&#25991;&#20013;&#38750;&#24120;&#38750;&#32467;&#26500;&#21270;&#12289;&#24322;&#26500;&#30340;&#35266;&#28857;&#65292;&#26377;&#26102;&#26159;&#27491;&#38754;&#30340;&#12289;&#36127;&#38754;&#30340;&#65292;&#25110;&#26159;&#20013;&#24615;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
When any extraordinary event takes place in the world wide area, it is the social media that acts as the fastest carrier of the news along with the consequences dealt with that event. One can gather much information through social networks regarding the sentiments, behavior, and opinions of the people. In this paper, we focus mainly on sentiment analysis of twitter data of India which comprises of COVID-19 tweets. We show how Twitter data has been extracted and then run sentimental analysis queries on it. This is helpful to analyze the information in the tweets where opinions are highly unstructured, heterogeneous, and are either positive or negative or neutral in some cases.
&lt;/p&gt;</description></item><item><title>KETM&#26159;&#19968;&#31181;&#30693;&#35782;&#22686;&#24378;&#30340;&#25991;&#26412;&#21305;&#37197;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#22806;&#37096;&#30693;&#35782;&#28304;&#20013;&#33719;&#21462;&#24120;&#35782;&#30693;&#35782;&#26469;&#20016;&#23500;&#19978;&#19979;&#25991;&#34920;&#31034;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#24120;&#35782;&#30693;&#35782;&#25512;&#29702;&#20219;&#21153;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.06235</link><description>&lt;p&gt;
KETM:&#19968;&#31181;&#30693;&#35782;&#22686;&#24378;&#30340;&#25991;&#26412;&#21305;&#37197;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
KETM:A Knowledge-Enhanced Text Matching method. (arXiv:2308.06235v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06235
&lt;/p&gt;
&lt;p&gt;
KETM&#26159;&#19968;&#31181;&#30693;&#35782;&#22686;&#24378;&#30340;&#25991;&#26412;&#21305;&#37197;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#22806;&#37096;&#30693;&#35782;&#28304;&#20013;&#33719;&#21462;&#24120;&#35782;&#30693;&#35782;&#26469;&#20016;&#23500;&#19978;&#19979;&#25991;&#34920;&#31034;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#24120;&#35782;&#30693;&#35782;&#25512;&#29702;&#20219;&#21153;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21305;&#37197;&#26159;&#23558;&#20004;&#20010;&#25991;&#26412;&#36827;&#34892;&#21305;&#37197;&#24182;&#30830;&#23450;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#30340;&#20219;&#21153;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;&#22914;&#38405;&#35835;&#29702;&#35299;&#21644;&#38382;&#31572;&#31995;&#32479;&#20013;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#20027;&#27969;&#26041;&#27861;&#26159;&#36890;&#36807;&#35745;&#31639;&#25991;&#26412;&#34920;&#31034;&#25110;&#36890;&#36807;&#27880;&#24847;&#26426;&#21046;&#19982;&#25991;&#26412;&#36827;&#34892;&#20132;&#20114;&#65292;&#36825;&#22312;&#25991;&#26412;&#21305;&#37197;&#20219;&#21153;&#20013;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#38656;&#35201;&#22522;&#20110;&#24120;&#35782;&#30693;&#35782;&#25512;&#29702;&#30340;&#25991;&#26412;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#19981;&#36275;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25991;&#26412;&#21305;&#37197;&#27169;&#22411;&#65292;&#31216;&#20026;&#30693;&#35782;&#22686;&#24378;&#30340;&#25991;&#26412;&#21305;&#37197;&#27169;&#22411;&#65288;KETM&#65289;&#65292;&#20197;&#20174;&#22806;&#37096;&#30693;&#35782;&#28304;&#20013;&#20016;&#23500;&#19978;&#19979;&#25991;&#34920;&#31034;&#65292;&#20197;&#22686;&#24378;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;Wiktionary&#26816;&#32034;&#25991;&#26412;&#35789;&#30340;&#23450;&#20041;&#20316;&#20026;&#25105;&#20204;&#30340;&#22806;&#37096;&#30693;&#35782;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23558;&#25991;&#26412;&#21644;&#30693;&#35782;&#36755;&#20837;&#25991;&#26412;&#21305;&#37197;&#27169;&#22359;&#65292;&#25552;&#21462;&#23427;&#20204;&#30340;&#29305;&#24449;&#21521;&#37327;&#12290;&#25991;&#26412;&#21305;&#37197;&#27169;&#22359;&#20351;&#29992;&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#30340;&#29305;&#24449;&#21521;&#37327;&#36827;&#34892;&#21305;&#37197;&#65292;&#20197;&#25552;&#39640;&#21305;&#37197;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#20960;&#20010;&#25991;&#26412;&#21305;&#37197;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#65292;&#24182;&#19982;&#29616;&#26377;&#30340;&#25991;&#26412;&#21305;&#37197;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;KETM&#27169;&#22411;&#22312;&#24120;&#35782;&#30693;&#35782;&#25512;&#29702;&#20219;&#21153;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text matching is the task of matching two texts and determining the relationship between them, which has extensive applications in natural language processing tasks such as reading comprehension, and Question-Answering systems. The mainstream approach is to compute text representations or to interact with the text through attention mechanism, which is effective in text matching tasks. However, the performance of these models is insufficient for texts that require commonsense knowledge-based reasoning. To this end, in this paper, We introduce a new model for text matching called the Knowledge Enhanced Text Matching model (KETM), to enrich contextual representations with real-world common-sense knowledge from external knowledge sources to enhance our model understanding and reasoning. First, we use Wiktionary to retrieve the text word definitions as our external knowledge. Secondly, we feed text and knowledge to the text matching module to extract their feature vectors. The text matching
&lt;/p&gt;</description></item><item><title>&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22686;&#24378;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#26377;&#25928;&#22320;&#31649;&#29702;&#23376;&#20219;&#21153;&#12289;&#35299;&#20915;&#19981;&#21516;&#30340;&#23376;&#20219;&#21153;&#65292;&#24182;&#29983;&#25104;&#19982;&#29992;&#25143;&#20132;&#20114;&#30340;&#22238;&#24212;&#12290;</title><link>http://arxiv.org/abs/2308.06212</link><description>&lt;p&gt;
&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22686;&#24378;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Large Language Model Enhanced Conversational Recommender System. (arXiv:2308.06212v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06212
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22686;&#24378;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#26377;&#25928;&#22320;&#31649;&#29702;&#23376;&#20219;&#21153;&#12289;&#35299;&#20915;&#19981;&#21516;&#30340;&#23376;&#20219;&#21153;&#65292;&#24182;&#29983;&#25104;&#19982;&#29992;&#25143;&#20132;&#20114;&#30340;&#22238;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#26088;&#22312;&#36890;&#36807;&#23545;&#35805;&#30028;&#38754;&#21521;&#29992;&#25143;&#25512;&#33616;&#39640;&#36136;&#37327;&#30340;&#29289;&#21697;&#12290;&#23427;&#36890;&#24120;&#21253;&#21547;&#22810;&#20010;&#23376;&#20219;&#21153;&#65292;&#22914;&#29992;&#25143;&#20559;&#22909;&#33719;&#21462;&#12289;&#25512;&#33616;&#12289;&#35299;&#37322;&#21644;&#29289;&#21697;&#20449;&#24687;&#25628;&#32034;&#12290;&#20026;&#20102;&#24320;&#21457;&#26377;&#25928;&#30340;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#65292;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#65306;1&#65289;&#22914;&#20309;&#27491;&#30830;&#31649;&#29702;&#23376;&#20219;&#21153;&#65307;2&#65289;&#22914;&#20309;&#26377;&#25928;&#35299;&#20915;&#19981;&#21516;&#30340;&#23376;&#20219;&#21153;&#65307;3&#65289;&#22914;&#20309;&#27491;&#30830;&#29983;&#25104;&#19982;&#29992;&#25143;&#20132;&#20114;&#30340;&#22238;&#24212;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#25512;&#29702;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#20026;&#24320;&#21457;&#26356;&#24378;&#22823;&#30340;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#65292;&#31216;&#20026;LLMCRS&#65292;&#26469;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#12290;&#22312;&#23376;&#20219;&#21153;&#31649;&#29702;&#26041;&#38754;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#26469;&#26377;&#25928;&#22320;&#31649;&#29702;&#23376;&#20219;&#21153;&#12290;&#22312;&#23376;&#20219;&#21153;&#35299;&#20915;&#26041;&#38754;&#65292;&#25105;&#20204;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#19981;&#21516;&#23376;&#20219;&#21153;&#30340;&#19987;&#23478;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#22686;&#24378;&#24615;&#33021;&#12290;&#22312;&#22238;&#24212;&#29983;&#25104;&#26041;&#38754;&#65292;&#25105;&#20204;&#21033;&#29992;&#29983;&#25104;&#33021;&#21147;&#26469;&#29983;&#25104;&#22238;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational recommender systems (CRSs) aim to recommend high-quality items to users through a dialogue interface. It usually contains multiple sub-tasks, such as user preference elicitation, recommendation, explanation, and item information search. To develop effective CRSs, there are some challenges: 1) how to properly manage sub-tasks; 2) how to effectively solve different sub-tasks; and 3) how to correctly generate responses that interact with users. Recently, Large Language Models (LLMs) have exhibited an unprecedented ability to reason and generate, presenting a new opportunity to develop more powerful CRSs. In this work, we propose a new LLM-based CRS, referred to as LLMCRS, to address the above challenges. For sub-task management, we leverage the reasoning ability of LLM to effectively manage sub-task. For sub-task solving, we collaborate LLM with expert models of different sub-tasks to achieve the enhanced performance. For response generation, we utilize the generation abili
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#33268;&#21147;&#20110;&#26500;&#24314;&#19968;&#20010;&#24605;&#32500;&#33539;&#24335;&#65292;&#33021;&#22815;&#20687;&#19987;&#23478;&#19968;&#26679;&#24605;&#32771;&#65292;&#36890;&#36807;&#36229;&#22270;&#30340;&#36229;&#36793;&#36830;&#25509;&#19981;&#21516;&#39030;&#28857;&#65292;&#23454;&#29616;&#39640;&#38454;&#22810;&#36339;&#25512;&#29702;&#21644;&#22810;&#27169;&#24577;&#27604;&#36739;&#21028;&#26029;&#12290;</title><link>http://arxiv.org/abs/2308.06207</link><description>&lt;p&gt;
&#24819;&#20687;&#19968;&#20301;&#19987;&#23478;&#65306;&#22810;&#27169;&#24577;&#24605;&#32500;&#36229;&#22270;&#22312;&#25552;&#21319;&#22522;&#30784;&#27169;&#22411;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Thinking Like an Expert:Multimodal Hypergraph-of-Thought (HoT) Reasoning to boost Foundation Modals. (arXiv:2308.06207v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#26500;&#24314;&#19968;&#20010;&#24605;&#32500;&#33539;&#24335;&#65292;&#33021;&#22815;&#20687;&#19987;&#23478;&#19968;&#26679;&#24605;&#32771;&#65292;&#36890;&#36807;&#36229;&#22270;&#30340;&#36229;&#36793;&#36830;&#25509;&#19981;&#21516;&#39030;&#28857;&#65292;&#23454;&#29616;&#39640;&#38454;&#22810;&#36339;&#25512;&#29702;&#21644;&#22810;&#27169;&#24577;&#27604;&#36739;&#21028;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#33021;&#21147;&#26159;&#22522;&#30784;&#27169;&#22411;&#26368;&#20851;&#38190;&#30340;&#33021;&#21147;&#20043;&#19968;&#65292;&#34920;&#31034;&#20854;&#24212;&#23545;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#24605;&#32500;&#38142;&#25216;&#26415;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#22686;&#24378;&#22522;&#30784;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#24182;&#19988;&#21463;&#21040;&#20102;&#24456;&#22823;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#24605;&#32500;&#38142;&#30340;&#25512;&#29702;&#36807;&#31243;&#26159;&#32447;&#24615;&#30340;&#12289;&#19968;&#27493;&#19968;&#27493;&#30340;&#65292;&#31867;&#20284;&#20110;&#20010;&#20154;&#30340;&#36923;&#36753;&#25512;&#29702;&#65292;&#36866;&#29992;&#20110;&#35299;&#20915;&#19968;&#33324;&#21644;&#31245;&#24494;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;&#30456;&#21453;&#65292;&#19987;&#23478;&#30340;&#24605;&#32500;&#27169;&#24335;&#20855;&#26377;&#20004;&#20010;&#26174;&#33879;&#29305;&#28857;&#65292;&#22312;&#24605;&#32500;&#38142;&#20013;&#26080;&#27861;&#36866;&#24403;&#22788;&#29702;&#65292;&#21363;&#39640;&#38454;&#22810;&#36339;&#25512;&#29702;&#21644;&#22810;&#27169;&#24577;&#27604;&#36739;&#21028;&#26029;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#30340;&#26680;&#24515;&#21160;&#26426;&#26159;&#36229;&#36234;&#24605;&#32500;&#38142;&#65292;&#26500;&#24314;&#19968;&#20010;&#33021;&#24605;&#32771;&#20687;&#19987;&#23478;&#19968;&#26679;&#30340;&#25512;&#29702;&#33539;&#24335;&#12290;&#36229;&#22270;&#30340;&#36229;&#36793;&#21487;&#20197;&#36830;&#25509;&#19981;&#21516;&#30340;&#39030;&#28857;&#65292;&#22240;&#27492;&#33258;&#28982;&#36866;&#29992;&#20110;&#24314;&#27169;&#39640;&#38454;&#20851;&#31995;&#12290;&#22312;&#27492;&#21551;&#21457;&#19979;&#65292;&#26412;&#25991;&#21019;&#26032;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Reasoning ability is one of the most crucial capabilities of a foundation model, signifying its capacity to address complex reasoning tasks. Chain-of-Thought (CoT) technique is widely regarded as one of the effective methods for enhancing the reasoning ability of foundation models and has garnered significant attention. However, the reasoning process of CoT is linear, step-by-step, similar to personal logical reasoning, suitable for solving general and slightly complicated problems. On the contrary, the thinking pattern of an expert owns two prominent characteristics that cannot be handled appropriately in CoT, i.e., high-order multi-hop reasoning and multimodal comparative judgement. Therefore, the core motivation of this paper is transcending CoT to construct a reasoning paradigm that can think like an expert. The hyperedge of a hypergraph could connect various vertices, making it naturally suitable for modelling high-order relationships. Inspired by this, this paper innovatively pro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#20998;&#26512;&#20102;&#24739;&#32773;&#25253;&#21578;&#30340;&#32467;&#26524;&#34913;&#37327;&#20013;&#30340;&#33258;&#30001;&#25991;&#26412;&#35780;&#35770;&#65292;&#21457;&#29616;&#20851;&#38190;&#23383;&#22522;&#30784;&#30340;WSTC&#22312;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#28508;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.06199</link><description>&lt;p&gt;
&#22312;&#24739;&#32773;&#25253;&#21578;&#30340;&#32467;&#26524;&#34913;&#37327;&#20013;&#65292;&#23545;&#33258;&#30001;&#25991;&#26412;&#35780;&#35770;&#36827;&#34892;&#24369;&#30417;&#30563;&#30340;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Weakly Supervised Text Classification on Free Text Comments in Patient-Reported Outcome Measures. (arXiv:2308.06199v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06199
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#20998;&#26512;&#20102;&#24739;&#32773;&#25253;&#21578;&#30340;&#32467;&#26524;&#34913;&#37327;&#20013;&#30340;&#33258;&#30001;&#25991;&#26412;&#35780;&#35770;&#65292;&#21457;&#29616;&#20851;&#38190;&#23383;&#22522;&#30784;&#30340;WSTC&#22312;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#28508;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24739;&#32773;&#25253;&#21578;&#30340;&#32467;&#26524;&#27979;&#37327;(PROMs)&#25968;&#25454;&#20013;&#30340;&#33258;&#30001;&#25991;&#26412;&#35780;&#35770;(FTC)&#36890;&#24120;&#20351;&#29992;&#25163;&#21160;&#26041;&#27861;&#36827;&#34892;&#20998;&#26512;&#65292;&#22914;&#20869;&#23481;&#20998;&#26512;&#65292;&#36825;&#31181;&#26041;&#27861;&#32791;&#26102;&#19988;&#36153;&#21147;&#12290;&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;&#26041;&#27861;&#36890;&#24120;&#26159;&#26080;&#30417;&#30563;&#30340;&#65292;&#38656;&#35201;&#36827;&#34892;&#21518;&#26399;&#35299;&#37322;&#12290;&#24369;&#30417;&#30563;&#30340;&#25991;&#26412;&#20998;&#31867;(WSTC)&#21487;&#20197;&#26159;&#19968;&#31181;&#26377;&#20215;&#20540;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#26631;&#35760;&#25968;&#25454;&#26377;&#38480;&#30340;&#39046;&#22495;&#29305;&#23450;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#12290;&#26412;&#25991;&#23558;&#20116;&#31181;WSTC&#25216;&#26415;&#24212;&#29992;&#20110;PROMs&#25968;&#25454;&#20013;&#30340;FTC&#65292;&#20197;&#30830;&#23450;&#32467;&#30452;&#32928;&#30284;&#24739;&#32773;&#25253;&#21578;&#30340;&#19982;&#20581;&#24247;&#30456;&#20851;&#30340;&#29983;&#27963;&#36136;&#37327;(HRQoL)&#20027;&#39064;&#12290;WSTC&#26041;&#27861;&#26631;&#35760;FTC&#20013;&#25552;&#21040;&#30340;&#25152;&#26377;&#20027;&#39064;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;PROMs&#25968;&#25454;&#30340;&#24615;&#33021;&#20013;&#31561;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#27169;&#22411;&#30340;&#31934;&#30830;&#24230;&#21644;&#20027;&#39064;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#23545;&#20998;&#31867;&#24615;&#33021;&#30340;&#35780;&#20272;&#25581;&#31034;&#20102;&#22522;&#20110;&#20851;&#38190;&#23383;&#30340;WSTC&#22312;&#26631;&#35760;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#26631;&#35760;PROMs FTC&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Free text comments (FTC) in patient-reported outcome measures (PROMs) data are typically analysed using manual methods, such as content analysis, which is labour-intensive and time-consuming. Machine learning analysis methods are largely unsupervised, necessitating post-analysis interpretation. Weakly supervised text classification (WSTC) can be a valuable method of analysis to classify domain-specific text data in which there is limited labelled data. In this paper, we apply five WSTC techniques to FTC in PROMs data to identify health-related quality of life (HRQoL) themes reported by colorectal cancer patients. The WSTC methods label all the themes mentioned in the FTC. The results showed moderate performance on the PROMs data, mainly due to the precision of the models, and variation between themes. Evaluation of the classification performance illustrated the potential and limitations of keyword based WSTC to label PROMs FTC when labelled data is limited.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#35780;&#35770;&#20013;&#25552;&#21462;&#23458;&#20154;&#22269;&#31821;&#30340;&#24341;&#29992;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20010;&#30456;&#23545;&#31616;&#21333;&#30340;&#26550;&#26500;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#36816;&#34892;&#26102;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2308.06175</link><description>&lt;p&gt;
&#20174;&#37202;&#24215;&#35780;&#35770;&#20013;&#35780;&#20272;&#23458;&#20154;&#30340;&#22269;&#31821;&#26500;&#25104;
&lt;/p&gt;
&lt;p&gt;
Assessing Guest Nationality Composition from Hotel Reviews. (arXiv:2308.06175v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#35780;&#35770;&#20013;&#25552;&#21462;&#23458;&#20154;&#22269;&#31821;&#30340;&#24341;&#29992;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20010;&#30456;&#23545;&#31616;&#21333;&#30340;&#26550;&#26500;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#36816;&#34892;&#26102;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#37202;&#24215;&#36890;&#36807;&#38024;&#23545;&#29305;&#23450;&#24066;&#22330;&#30340;&#23458;&#25143;&#33719;&#21462;&#21162;&#21147;&#65292;&#20197;&#26368;&#22909;&#22320;&#39044;&#27979;&#23458;&#20154;&#30340;&#20010;&#20154;&#20559;&#22909;&#21644;&#38656;&#27714;&#12290;&#21516;&#26679;&#65292;&#36825;&#31181;&#25112;&#30053;&#23450;&#20301;&#26159;&#26377;&#25928;&#30340;&#33829;&#38144;&#39044;&#31639;&#20998;&#37197;&#30340;&#20808;&#20915;&#26465;&#20214;&#12290;&#23448;&#26041;&#32479;&#35745;&#25968;&#25454;&#25253;&#21578;&#20102;&#26469;&#33258;&#19981;&#21516;&#22269;&#23478;&#30340;&#28216;&#23458;&#25968;&#37327;&#65292;&#20294;&#27809;&#26377;&#20851;&#20110;&#20010;&#21035;&#20225;&#19994;&#23458;&#20154;&#26500;&#25104;&#30340;&#32454;&#31890;&#24230;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#31454;&#20105;&#23545;&#25163;&#12289;&#20379;&#24212;&#21830;&#12289;&#30740;&#31350;&#20154;&#21592;&#21644;&#26222;&#36890;&#20844;&#20247;&#23545;&#36825;&#20123;&#25968;&#25454;&#30340;&#20852;&#36259;&#36234;&#26469;&#36234;&#22823;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#35780;&#35770;&#20013;&#25552;&#21462;&#23458;&#20154;&#22269;&#31821;&#30340;&#24341;&#29992;&#65292;&#20197;&#21160;&#24577;&#35780;&#20272;&#21644;&#30417;&#27979;&#20010;&#21035;&#20225;&#19994;&#23458;&#20154;&#26500;&#25104;&#30340;&#21160;&#24577;&#21464;&#21270;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;&#23884;&#20837;&#21644;&#22534;&#21472;&#30340;LSTM&#23618;&#30340;&#30456;&#24403;&#31616;&#21333;&#30340;&#26550;&#26500;&#25552;&#20379;&#20102;&#27604;&#22797;&#26434;&#30340;&#26368;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#36816;&#34892;&#26102;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many hotels target guest acquisition efforts to specific markets in order to best anticipate individual preferences and needs of their guests. Likewise, such strategic positioning is a prerequisite for efficient marketing budget allocation. Official statistics report on the number of visitors from different countries, but no fine-grained information on the guest composition of individual businesses exists. There is, however, growing interest in such data from competitors, suppliers, researchers and the general public. We demonstrate how machine learning can be leveraged to extract references to guest nationalities from unstructured text reviews in order to dynamically assess and monitor the dynamics of guest composition of individual businesses. In particular, we show that a rather simple architecture of pre-trained embeddings and stacked LSTM layers provides a better performance-runtime tradeoff than more complex state-of-the-art language models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#26469;&#35299;&#20915;&#23545;&#35805;&#31995;&#32479;&#20013;&#24847;&#22270;&#36319;&#36394;&#21644;&#27133;&#20301;&#29702;&#35299;&#30340;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#22312;&#21516;&#19968;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#22810;&#20010;&#30446;&#26631;&#25512;&#29702;&#30340;&#26465;&#20214;&#25511;&#21046;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.06165</link><description>&lt;p&gt;
&#20219;&#21153;&#26465;&#20214;&#19979;&#30340;BERT&#29992;&#20110;&#32852;&#21512;&#24847;&#22270;&#26816;&#27979;&#21644;&#27133;&#20301;&#22635;&#20805;
&lt;/p&gt;
&lt;p&gt;
Task Conditioned BERT for Joint Intent Detection and Slot-filling. (arXiv:2308.06165v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#26469;&#35299;&#20915;&#23545;&#35805;&#31995;&#32479;&#20013;&#24847;&#22270;&#36319;&#36394;&#21644;&#27133;&#20301;&#29702;&#35299;&#30340;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#22312;&#21516;&#19968;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#22810;&#20010;&#30446;&#26631;&#25512;&#29702;&#30340;&#26465;&#20214;&#25511;&#21046;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#31995;&#32479;&#38656;&#35201;&#35299;&#20915;&#29992;&#25143;&#24847;&#22270;&#30340;&#19981;&#21487;&#39044;&#27979;&#24615;&#20197;&#36319;&#36394;&#23545;&#35805;&#29366;&#24577;&#65292;&#24182;&#19988;&#38656;&#35201;&#22788;&#29702;&#27133;&#20301;&#30340;&#24322;&#36136;&#24615;&#20197;&#29702;&#35299;&#29992;&#25143;&#30340;&#20559;&#22909;&#12290;&#26412;&#25991;&#30740;&#31350;&#30340;&#20551;&#35774;&#26159;&#23558;&#36825;&#20123;&#25361;&#25112;&#20316;&#20026;&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#26469;&#35299;&#20915;&#65292;&#23558;&#20801;&#35768;&#22312;&#19981;&#21516;&#30340;&#20219;&#21153;&#20043;&#38388;&#20256;&#36882;&#21442;&#25968;&#25903;&#25345;&#25968;&#25454;&#12290;&#25552;&#20986;&#30340;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20016;&#23500;&#30340;&#36755;&#20837;&#26469;&#23558;&#27169;&#22411;&#26465;&#20214;&#20110;&#30446;&#26631;&#25512;&#29702;&#12290;&#36890;&#36807;&#23545;&#30456;&#21516;&#35821;&#26009;&#24211;&#19978;&#30340;&#22810;&#20010;&#30446;&#26631;&#25512;&#29702;&#65288;&#21363;&#24847;&#22270;&#21644;&#22810;&#20010;&#27133;&#20301;&#31867;&#22411;&#65289;&#23545;Transformer&#32534;&#30721;&#22120;&#36827;&#34892;&#26465;&#20214;&#25511;&#21046;&#65292;&#20801;&#35768;&#23398;&#20064;&#21040;&#27604;&#21333;&#20219;&#21153;&#27169;&#22411;&#26356;&#20016;&#23500;&#30340;&#35821;&#35328;&#20132;&#20114;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#27169;&#22411;&#26465;&#20214;&#20110;&#36234;&#26469;&#36234;&#22810;&#30340;&#23545;&#35805;&#25512;&#29702;&#20219;&#21153;&#21487;&#20197;&#25913;&#21892;&#32467;&#26524;&#65306;&#22312;MultiWOZ&#25968;&#25454;&#38598;&#19978;&#65292;&#36890;&#36807;&#26465;&#20214;&#25511;&#21046;&#24847;&#22270;&#65292;&#32852;&#21512;&#24847;&#22270;&#21644;&#27133;&#20301;&#26816;&#27979;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;3.2%&#65292;&#36890;&#36807;&#26465;&#20214;&#25511;&#21046;&#22810;&#20010;&#27133;&#20301;&#31867;&#22411;&#65292;&#25552;&#39640;&#20102;10.8%&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue systems need to deal with the unpredictability of user intents to track dialogue state and the heterogeneity of slots to understand user preferences. In this paper we investigate the hypothesis that solving these challenges as one unified model will allow the transfer of parameter support data across the different tasks. The proposed principled model is based on a Transformer encoder, trained on multiple tasks, and leveraged by a rich input that conditions the model on the target inferences. Conditioning the Transformer encoder on multiple target inferences over the same corpus, i.e., intent and multiple slot types, allows learning richer language interactions than a single-task model would be able to. In fact, experimental results demonstrate that conditioning the model on an increasing number of dialogue inference tasks leads to improved results: on the MultiWOZ dataset, the joint intent and slot detection can be improved by 3.2\% by conditioning on intent, 10.8\% by conditi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#35789;&#34955;&#21644;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#23545;&#20195;&#30721;&#27880;&#37322;&#30340;&#30456;&#20851;&#24615;&#36827;&#34892;&#35782;&#21035;&#12290;&#22312;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#65292;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#29305;&#24449;&#24037;&#31243;&#21644;&#25991;&#26412;&#20998;&#31867;&#25216;&#26415;&#65292;&#24182;&#27604;&#36739;&#20102;&#20256;&#32479;&#35789;&#34955;&#27169;&#22411;&#21644;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.06144</link><description>&lt;p&gt;
&#20351;&#29992;&#35789;&#34955;&#21644;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#35782;&#21035;&#20195;&#30721;&#35780;&#35770;&#30340;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
Identification of the Relevance of Comments in Codes Using Bag of Words and Transformer Based Models. (arXiv:2308.06144v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06144
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#35789;&#34955;&#21644;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#23545;&#20195;&#30721;&#27880;&#37322;&#30340;&#30456;&#20851;&#24615;&#36827;&#34892;&#35782;&#21035;&#12290;&#22312;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#65292;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#29305;&#24449;&#24037;&#31243;&#21644;&#25991;&#26412;&#20998;&#31867;&#25216;&#26415;&#65292;&#24182;&#27604;&#36739;&#20102;&#20256;&#32479;&#35789;&#34955;&#27169;&#22411;&#21644;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20170;&#24180;&#65292;&#20449;&#24687;&#26816;&#32034;&#35770;&#22363;(FIRE)&#21551;&#21160;&#20102;&#19968;&#20010;&#20849;&#20139;&#20219;&#21153;&#65292;&#29992;&#20110;&#23545;&#19981;&#21516;&#20195;&#30721;&#27573;&#30340;&#35780;&#35770;&#36827;&#34892;&#20998;&#31867;&#12290;&#36825;&#26159;&#19968;&#20010;&#20108;&#20803;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#65292;&#30446;&#26631;&#26159;&#30830;&#23450;&#32473;&#23450;&#20195;&#30721;&#27573;&#30340;&#35780;&#35770;&#26159;&#21542;&#30456;&#20851;&#12290;&#21360;&#24230;&#31185;&#23398;&#25945;&#32946;&#19982;&#30740;&#31350;&#38498;&#21338;&#24085;&#23572;&#20998;&#38498;(IISERB)&#30340;BioNLP-IISERB&#23567;&#32452;&#21442;&#19982;&#20102;&#36825;&#39033;&#20219;&#21153;&#65292;&#24182;&#20026;&#20116;&#31181;&#19981;&#21516;&#30340;&#27169;&#22411;&#25552;&#20132;&#20102;&#20116;&#31181;&#36816;&#34892;&#32467;&#26524;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#27010;&#20917;&#21644;&#22312;&#35757;&#32451;&#35821;&#26009;&#24211;&#19978;&#30340;&#20854;&#20182;&#37325;&#35201;&#21457;&#29616;&#12290;&#36825;&#20123;&#26041;&#27861;&#28041;&#21450;&#19981;&#21516;&#30340;&#29305;&#24449;&#24037;&#31243;&#26041;&#26696;&#21644;&#25991;&#26412;&#20998;&#31867;&#25216;&#26415;&#12290;&#23545;&#20110;&#35789;&#34955;&#27169;&#22411;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#20998;&#31867;&#22120;&#65292;&#22914;&#38543;&#26426;&#26862;&#26519;&#12289;&#25903;&#25345;&#21521;&#37327;&#26426;&#21644;&#36923;&#36753;&#22238;&#24402;&#65292;&#20197;&#35782;&#21035;&#32473;&#23450;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#22522;&#20110;&#39044;&#35757;&#32451;Transformer&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Forum for Information Retrieval (FIRE) started a shared task this year for classification of comments of different code segments. This is binary text classification task where the objective is to identify whether comments given for certain code segments are relevant or not. The BioNLP-IISERB group at the Indian Institute of Science Education and Research Bhopal (IISERB) participated in this task and submitted five runs for five different models. The paper presents the overview of the models and other significant findings on the training corpus. The methods involve different feature engineering schemes and text classification techniques. The performance of the classical bag of words model and transformer-based models were explored to identify significant features from the given training corpus. We have explored different classifiers viz., random forest, support vector machine and logistic regression using the bag of words model. Furthermore, the pre-trained transformer based models 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#32852;&#21512;&#35821;&#38899;-&#25991;&#26412;&#34920;&#31034;&#20013;&#65292;&#24573;&#30053;&#24207;&#21015;&#38271;&#24230;&#38382;&#39064;&#33021;&#22815;&#33258;&#28982;&#22320;&#23454;&#29616;&#19968;&#33268;&#30340;&#34920;&#31034;&#65292;&#19968;&#33268;&#24615;&#25439;&#22833;&#21487;&#20197;&#25552;&#39640;&#19979;&#28216;&#30340;&#23383;&#38169;&#35823;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.06125</link><description>&lt;p&gt;
&#22312;&#19981;&#36827;&#34892;&#23545;&#40784;&#30340;&#24773;&#20917;&#19979;&#25913;&#36827;&#32852;&#21512;&#35821;&#38899;-&#25991;&#26412;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Improving Joint Speech-Text Representations Without Alignment. (arXiv:2308.06125v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#32852;&#21512;&#35821;&#38899;-&#25991;&#26412;&#34920;&#31034;&#20013;&#65292;&#24573;&#30053;&#24207;&#21015;&#38271;&#24230;&#38382;&#39064;&#33021;&#22815;&#33258;&#28982;&#22320;&#23454;&#29616;&#19968;&#33268;&#30340;&#34920;&#31034;&#65292;&#19968;&#33268;&#24615;&#25439;&#22833;&#21487;&#20197;&#25552;&#39640;&#19979;&#28216;&#30340;&#23383;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#19968;&#24180;&#65292;&#22522;&#20110;&#36328;&#27169;&#24577;&#34920;&#31034;&#31354;&#38388;&#30340;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#29983;&#25104;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#65292;&#20854;&#20013;&#25991;&#26412;&#21644;&#22270;&#20687;&#39046;&#22495;&#20197;&#32852;&#21512;&#30340;&#26041;&#24335;&#34920;&#31034;&#12290;&#22312;ASR&#20013;&#65292;&#36825;&#20010;&#24819;&#27861;&#34987;&#24212;&#29992;&#20026;&#32852;&#21512;&#35821;&#38899;-&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#21516;&#26102;&#35757;&#32451;&#19981;&#21305;&#37197;&#30340;&#35821;&#38899;&#21644;&#25991;&#26412;&#65292;&#21487;&#20197;&#25193;&#23637;&#21040;&#38750;&#24120;&#22823;&#30340;&#21442;&#25968;&#27169;&#22411;&#30340;&#23481;&#37327;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#24076;&#26395;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#29305;&#27530;&#22788;&#29702;&#35821;&#38899;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#24207;&#21015;&#38271;&#24230;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#35201;&#20040;&#36890;&#36807;&#19978;&#37319;&#26679;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#35201;&#20040;&#36890;&#36807;&#19968;&#20010;&#26174;&#24335;&#30340;&#23545;&#40784;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#65292;&#32852;&#21512;&#35821;&#38899;-&#25991;&#26412;&#32534;&#30721;&#22120;&#36890;&#36807;&#24573;&#30053;&#24207;&#21015;&#38271;&#24230;&#33258;&#28982;&#32780;&#28982;&#22320;&#23454;&#29616;&#20102;&#19968;&#33268;&#30340;&#34920;&#31034;&#65292;&#24182;&#19988;&#35748;&#20026;&#19968;&#33268;&#24615;&#25439;&#22833;&#21487;&#20197;&#24357;&#34917;&#38271;&#24230;&#24046;&#24322;&#65292;&#24182;&#31616;&#21333;&#22320;&#20551;&#35774;&#26368;&#20339;&#23545;&#40784;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#26679;&#30340;&#25439;&#22833;&#22312;&#22823;&#21442;&#25968;&#21333;&#35821;&#35328;&#21644;&#22810;&#35821;&#35328;&#31995;&#32479;&#20013;&#25552;&#39640;&#20102;&#19979;&#28216;&#30340;&#23383;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The last year has seen astonishing progress in text-prompted image generation premised on the idea of a cross-modal representation space in which the text and image domains are represented jointly. In ASR, this idea has found application as joint speech-text encoders that can scale to the capacities of very large parameter models by being trained on both unpaired speech and text. While these methods show promise, they have required special treatment of the sequence-length mismatch inherent in speech and text, either by up-sampling heuristics or an explicit alignment model. In this work, we offer evidence that joint speech-text encoders naturally achieve consistent representations across modalities by disregarding sequence length, and argue that consistency losses could forgive length differences and simply assume the best alignment. We show that such a loss improves downstream WER in both a large-parameter monolingual and multilingual system.
&lt;/p&gt;</description></item><item><title>Lip2Vec&#26159;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#20808;&#39564;&#27169;&#22411;&#23558;&#22068;&#21767;&#24207;&#21015;&#30340;&#32534;&#30721;&#34920;&#31034;&#26144;&#23556;&#21040;&#38899;&#39057;&#23545;&#30340;&#28508;&#31354;&#38388;&#65292;&#24182;&#20351;&#29992;&#38899;&#39057;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#23558;&#29983;&#25104;&#30340;&#38899;&#39057;&#34920;&#31034;&#35299;&#30721;&#20026;&#25991;&#26412;&#30340;&#39640;&#25928;&#31283;&#20581;&#30340;&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.06112</link><description>&lt;p&gt;
Lip2Vec:&#36890;&#36807;&#28508;&#31354;&#38388;&#21040;&#28508;&#31354;&#38388;&#30340;&#35270;&#21548;&#34920;&#36798;&#26144;&#23556;&#23454;&#29616;&#39640;&#25928;&#31283;&#20581;&#30340;&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Lip2Vec: Efficient and Robust Visual Speech Recognition via Latent-to-Latent Visual to Audio Representation Mapping. (arXiv:2308.06112v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06112
&lt;/p&gt;
&lt;p&gt;
Lip2Vec&#26159;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#20808;&#39564;&#27169;&#22411;&#23558;&#22068;&#21767;&#24207;&#21015;&#30340;&#32534;&#30721;&#34920;&#31034;&#26144;&#23556;&#21040;&#38899;&#39057;&#23545;&#30340;&#28508;&#31354;&#38388;&#65292;&#24182;&#20351;&#29992;&#38899;&#39057;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#23558;&#29983;&#25104;&#30340;&#38899;&#39057;&#34920;&#31034;&#35299;&#30721;&#20026;&#25991;&#26412;&#30340;&#39640;&#25928;&#31283;&#20581;&#30340;&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;(VSR)&#19982;&#24120;&#35265;&#30340;&#24863;&#30693;&#20219;&#21153;&#19981;&#21516;&#65292;&#23427;&#38656;&#35201;&#23545;&#35270;&#39057;&#24207;&#21015;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#25512;&#29702;&#65292;&#21363;&#20351;&#23545;&#20110;&#20154;&#31867;&#19987;&#23478;&#26469;&#35828;&#20063;&#26159;&#22914;&#27492;&#12290;&#23613;&#31649;VSR&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#30446;&#21069;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#26377;&#26631;&#31614;&#30340;&#25968;&#25454;&#26469;&#23436;&#20840;&#35757;&#32451;&#25110;&#24494;&#35843;&#27169;&#22411;&#20197;&#39044;&#27979;&#30446;&#26631;&#35821;&#38899;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#35757;&#32451;&#38598;&#20043;&#22806;&#30340;&#24191;&#27867;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#23548;&#33268;&#22312;&#38754;&#23545;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20998;&#24067;&#24773;&#26223;&#26102;&#24615;&#33021;&#36864;&#21270;&#12290;&#19982;&#20197;&#24448;&#28041;&#21450;&#36741;&#21161;&#25439;&#22833;&#25110;&#22797;&#26434;&#35757;&#32451;&#36807;&#31243;&#21644;&#26550;&#26500;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;Lip2Vec&#65292;&#23427;&#22522;&#20110;&#23398;&#20064;&#19968;&#20010;&#20808;&#39564;&#27169;&#22411;&#12290;&#32473;&#23450;&#19968;&#20010;&#24378;&#22823;&#30340;&#35270;&#35273;&#35821;&#38899;&#32534;&#30721;&#22120;&#65292;&#35813;&#32593;&#32476;&#23558;&#22068;&#21767;&#24207;&#21015;&#30340;&#32534;&#30721;&#28508;&#22312;&#34920;&#31034;&#26144;&#23556;&#21040;&#23427;&#20204;&#23545;&#24212;&#30340;&#38899;&#39057;&#23545;&#30340;&#28508;&#31354;&#38388;&#65292;&#36825;&#20123;&#28508;&#31354;&#38388;&#23545;&#20110;&#26377;&#25928;&#30340;&#25991;&#26412;&#35299;&#30721;&#20855;&#26377;&#36275;&#22815;&#30340;&#19981;&#21464;&#24615;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#29616;&#25104;&#30340;&#38899;&#39057;&#35821;&#38899;&#35782;&#21035;(ASR)&#27169;&#22411;&#23558;&#29983;&#25104;&#30340;&#38899;&#39057;&#34920;&#31034;&#35299;&#30721;&#20026;&#25991;&#26412;&#12290;&#35813;&#26041;&#27861;&#20855;&#22791;&#39640;&#25928;&#21644;&#31283;&#20581;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual Speech Recognition (VSR) differs from the common perception tasks as it requires deeper reasoning over the video sequence, even by human experts. Despite the recent advances in VSR, current approaches rely on labeled data to fully train or finetune their models predicting the target speech. This hinders their ability to generalize well beyond the training set and leads to performance degeneration under out-of-distribution challenging scenarios. Unlike previous works that involve auxiliary losses or complex training procedures and architectures, we propose a simple approach, named Lip2Vec that is based on learning a prior model. Given a robust visual speech encoder, this network maps the encoded latent representations of the lip sequence to their corresponding latents from the audio pair, which are sufficiently invariant for effective text decoding. The generated audio representation is then decoded to text using an off-the-shelf Audio Speech Recognition (ASR) model. The proposed
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;ZeroShotALI&#65292;&#23427;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25512;&#33616;&#31995;&#32479;&#26469;&#25913;&#36827;&#37329;&#34701;&#23457;&#35745;&#20013;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#21305;&#37197;&#12290;&#36890;&#36807;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#32463;&#36807;&#39046;&#22495;&#20248;&#21270;&#30340;&#22522;&#20110;transformer&#30340;&#25991;&#26412;&#21305;&#37197;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#31995;&#32479;&#23454;&#29616;&#20102;&#20174;&#25253;&#21578;&#20013;&#25512;&#33616;&#19982;&#27861;&#24459;&#35201;&#27714;&#30456;&#31526;&#30340;&#30456;&#20851;&#25991;&#26412;&#27573;&#33853;&#65292;&#24182;&#22312;&#29616;&#26377;&#26041;&#27861;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2308.06111</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21319;&#37329;&#34701;&#23457;&#35745;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Improving Zero-Shot Text Matching for Financial Auditing with Large Language Models. (arXiv:2308.06111v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06111
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;ZeroShotALI&#65292;&#23427;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25512;&#33616;&#31995;&#32479;&#26469;&#25913;&#36827;&#37329;&#34701;&#23457;&#35745;&#20013;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#21305;&#37197;&#12290;&#36890;&#36807;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#32463;&#36807;&#39046;&#22495;&#20248;&#21270;&#30340;&#22522;&#20110;transformer&#30340;&#25991;&#26412;&#21305;&#37197;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#31995;&#32479;&#23454;&#29616;&#20102;&#20174;&#25253;&#21578;&#20013;&#25512;&#33616;&#19982;&#27861;&#24459;&#35201;&#27714;&#30456;&#31526;&#30340;&#30456;&#20851;&#25991;&#26412;&#27573;&#33853;&#65292;&#24182;&#22312;&#29616;&#26377;&#26041;&#27861;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23457;&#35745;&#37329;&#34701;&#25991;&#20214;&#26159;&#19968;&#20010;&#38750;&#24120;&#32321;&#29712;&#21644;&#32791;&#26102;&#30340;&#36807;&#31243;&#12290;&#30446;&#21069;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#31616;&#21270;&#36825;&#19968;&#36807;&#31243;&#65292;&#20197;&#25512;&#33616;&#19982;&#20005;&#26684;&#20250;&#35745;&#26631;&#20934;&#30340;&#27861;&#24459;&#35201;&#27714;&#30456;&#31526;&#30340;&#25253;&#21578;&#20013;&#30340;&#30456;&#20851;&#25991;&#26412;&#27573;&#33853;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#23450;&#26399;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#19988;&#36890;&#24120;&#22312;&#24037;&#19994;&#29615;&#22659;&#20013;&#32570;&#20047;&#22823;&#37327;&#30340;&#27880;&#37322;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ZeroShotALI&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#21033;&#29992;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#39046;&#22495;&#29305;&#23450;&#30340;&#20248;&#21270;&#30340;&#22522;&#20110;transformer&#30340;&#25991;&#26412;&#21305;&#37197;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#39318;&#20808;&#20351;&#29992;&#33258;&#23450;&#20041;BERT&#27169;&#22411;&#26816;&#32034;&#19982;&#27861;&#24459;&#35201;&#27714;&#30456;&#31526;&#30340;&#33509;&#24178;&#26368;&#20339;&#21305;&#37197;&#30340;&#25991;&#26723;&#37096;&#20998;&#65292;&#28982;&#21518;&#20351;&#29992;LLM&#23545;&#36825;&#20123;&#36873;&#25321;&#36827;&#34892;&#36807;&#28388;&#65292;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Auditing financial documents is a very tedious and time-consuming process. As of today, it can already be simplified by employing AI-based solutions to recommend relevant text passages from a report for each legal requirement of rigorous accounting standards. However, these methods need to be fine-tuned regularly, and they require abundant annotated data, which is often lacking in industrial environments. Hence, we present ZeroShotALI, a novel recommender system that leverages a state-of-the-art large language model (LLM) in conjunction with a domain-specifically optimized transformer-based text-matching solution. We find that a two-step approach of first retrieving a number of best matching document sections per legal requirement with a custom BERT-based model and second filtering these selections using an LLM yields significant performance improvements over existing approaches.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#36848;&#30740;&#31350;&#20102;&#20197;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#24178;&#39044;&#24213;&#23618;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#22914;&#25968;&#25454;&#12289;&#35757;&#32451;&#21046;&#24230;&#25110;&#35299;&#30721;&#65292;&#26469;&#20445;&#35777;&#27169;&#22411;&#30340;&#27969;&#30021;&#24615;&#12289;&#20449;&#24687;&#20016;&#23500;&#24615;&#12289;&#19968;&#33268;&#24615;&#12289;&#36830;&#36143;&#24615;&#20197;&#21450;&#36981;&#24490;&#31038;&#20250;&#20934;&#21017;&#30340;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.06095</link><description>&lt;p&gt;
&#31070;&#32463;&#23545;&#35805;&#27169;&#22411;&#21450;&#20854;&#25511;&#21046;&#26041;&#27861;&#65306;&#22833;&#36133;&#21644;&#20462;&#22797;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Neural Conversation Models and How to Rein Them in: A Survey of Failures and Fixes. (arXiv:2308.06095v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06095
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#36848;&#30740;&#31350;&#20102;&#20197;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#24178;&#39044;&#24213;&#23618;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#22914;&#25968;&#25454;&#12289;&#35757;&#32451;&#21046;&#24230;&#25110;&#35299;&#30721;&#65292;&#26469;&#20445;&#35777;&#27169;&#22411;&#30340;&#27969;&#30021;&#24615;&#12289;&#20449;&#24687;&#20016;&#23500;&#24615;&#12289;&#19968;&#33268;&#24615;&#12289;&#36830;&#36143;&#24615;&#20197;&#21450;&#36981;&#24490;&#31038;&#20250;&#20934;&#21017;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20197;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#26465;&#20214;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20197;&#30475;&#20284;&#27969;&#21033;&#30340;&#26041;&#24335;&#24310;&#32493;&#20219;&#20309;&#31867;&#22411;&#30340;&#25991;&#26412;&#26469;&#28304;&#12290;&#36825;&#20010;&#20107;&#23454;&#20419;&#36827;&#20102;&#23545;&#22522;&#20110;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#30340;&#30740;&#31350;&#65292;&#26088;&#22312;&#36890;&#36807;&#29983;&#25104;&#36866;&#24403;&#30340;&#23545;&#35805;&#20869;&#23481;&#26469;&#27169;&#20223;&#23545;&#35805;&#26041;&#30340;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#20174;&#35821;&#35328;&#23398;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#21442;&#19982;&#23545;&#35805;&#30340;&#22797;&#26434;&#24615;&#24456;&#39640;&#12290;&#22312;&#36825;&#39033;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#20174;&#36825;&#19968;&#29305;&#23450;&#30740;&#31350;&#39046;&#22495;&#30340;&#35282;&#24230;&#35299;&#37322;&#20102;Grice&#30340;&#21512;&#20316;&#24615;&#23545;&#35805;&#26368;&#22823;&#35268;&#21017;&#65292;&#24182;&#23558;&#25991;&#29486;&#31995;&#32479;&#21270;&#22320;&#24402;&#32435;&#20026;&#19968;&#20010;&#36129;&#29486;&#20309;&#31181;&#20869;&#23481;&#26159;&#36866;&#24403;&#30340;&#26041;&#38754;&#65306;&#31070;&#32463;&#23545;&#35805;&#27169;&#22411;&#24517;&#39035;&#27969;&#30021;&#12289;&#20449;&#24687;&#20016;&#23500;&#12289;&#19968;&#33268;&#12289;&#36830;&#36143;&#65292;&#24182;&#36981;&#24490;&#31038;&#20250;&#20934;&#21017;&#12290;&#20026;&#20102;&#30830;&#20445;&#36825;&#20123;&#29305;&#24615;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#23581;&#35797;&#22312;&#25968;&#25454;&#12289;&#35757;&#32451;&#21046;&#24230;&#25110;&#35299;&#30721;&#31561;&#21508;&#20010;&#24178;&#39044;&#28857;&#19978;&#25511;&#21046;&#24213;&#23618;&#35821;&#35328;&#27169;&#22411;&#12290;&#25353;&#29031;&#36825;&#20123;&#31867;&#21035;&#21644;&#24178;&#39044;&#28857;&#36827;&#34892;&#25490;&#24207;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#19968;&#20123;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent conditional language models are able to continue any kind of text source in an often seemingly fluent way. This fact encouraged research in the area of open-domain conversational systems that are based on powerful language models and aim to imitate an interlocutor by generating appropriate contributions to a written dialogue. From a linguistic perspective, however, the complexity of contributing to a conversation is high. In this survey, we interpret Grice's maxims of cooperative conversation from the perspective of this specific research area and systematize the literature under the aspect of what makes a contribution appropriate: A neural conversation model has to be fluent, informative, consistent, coherent, and follow social norms. In order to ensure these qualities, recent approaches try to tame the underlying language models at various intervention points, such as data, training regime or decoding. Sorted by these categories and intervention points, we discuss promising at
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32463;&#27982;&#26377;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#36873;&#25321;&#26694;&#26550;&#65288;CELMOC&#65289;&#65292;&#36890;&#36807;&#20803;&#27169;&#22411;&#39044;&#27979;&#22312;&#19981;&#21516;&#36755;&#20837;&#19978;&#34920;&#29616;&#33391;&#22909;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#22312;&#20302;&#25104;&#26412;&#19979;&#23454;&#29616;&#39640;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.06077</link><description>&lt;p&gt;
&#39134;&#25293;&#25110;&#22823;&#28846;&#65311;&#36890;&#36807;&#20803;&#27169;&#22411;&#36873;&#25321;&#32463;&#27982;&#26377;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fly-Swat or Cannon? Cost-Effective Language Model Choice via Meta-Modeling. (arXiv:2308.06077v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32463;&#27982;&#26377;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#36873;&#25321;&#26694;&#26550;&#65288;CELMOC&#65289;&#65292;&#36890;&#36807;&#20803;&#27169;&#22411;&#39044;&#27979;&#22312;&#19981;&#21516;&#36755;&#20837;&#19978;&#34920;&#29616;&#33391;&#22909;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#22312;&#20302;&#25104;&#26412;&#19979;&#23454;&#29616;&#39640;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#25454;&#31185;&#23398;&#39046;&#22495;&#20013;&#21464;&#24471;&#26080;&#22788;&#19981;&#22312;&#12290;&#23545;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#21487;&#20197;&#23558;&#36755;&#20837;&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#65292;&#36890;&#36807;LM&#30340;&#36755;&#20986;&#26469;&#25552;&#21462;&#35299;&#20915;&#26041;&#26696;&#12290;LM&#30340;&#24615;&#33021;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#32780;&#19981;&#26029;&#25552;&#39640;&#65292;&#20294;&#21516;&#26102;&#26597;&#35810;&#36234;&#26469;&#36234;&#22823;&#30340;&#27169;&#22411;&#30340;&#32463;&#27982;&#25104;&#26412;&#20063;&#22312;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#19981;&#26159;&#25152;&#26377;&#30340;&#36755;&#20837;&#37117;&#24456;&#38590;&#65306;&#26377;&#20123;&#36755;&#20837;&#38656;&#35201;&#26356;&#22823;&#30340;LM&#25165;&#33021;&#33719;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#23545;&#20110;&#20854;&#20182;&#36755;&#20837;&#65292;&#36739;&#23567;&#30340;LM&#23601;&#36275;&#22815;&#20102;&#12290;&#22522;&#20110;&#36825;&#20010;&#20107;&#23454;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#32463;&#27982;&#26377;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#36873;&#25321;&#26694;&#26550;&#65288;CELMOC&#65289;&#12290;&#32473;&#23450;&#19968;&#32452;&#36755;&#20837;&#21644;&#19968;&#32452;&#20505;&#36873;LM&#65292;CELMOC&#26681;&#25454;&#25152;&#35859;&#30340;&#20803;&#27169;&#22411;&#32874;&#26126;&#22320;&#23558;&#27599;&#20010;&#36755;&#20837;&#20998;&#37197;&#32473;&#19968;&#20010;&#22312;&#35813;&#36755;&#20837;&#19978;&#39044;&#27979;&#34920;&#29616;&#33391;&#22909;&#30340;LM&#65292;&#20197;&#26399;&#22312;&#20302;&#25104;&#26412;&#19979;&#23454;&#29616;&#39640;&#25972;&#20307;&#24615;&#33021;&#12290;&#29992;&#25143;&#21487;&#20197;&#28789;&#27963;&#35843;&#25972;&#25104;&#26412;&#19982;&#24615;&#33021;&#30340;&#26435;&#34913;&#12290;&#36873;&#39033;&#21253;&#25324;&#65292;&#26368;&#22823;&#21270;&#24635;&#20307;&#24615;&#33021;&#65288;&#25110;&#22788;&#29702;&#36755;&#20837;&#30340;&#25968;&#37327;&#65289;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative language models (LMs) have become omnipresent across data science. For a wide variety of tasks, inputs can be phrased as natural language prompts for an LM, from whose output the solution can then be extracted. LM performance has consistently been increasing with model size - but so has the monetary cost of querying the ever larger models. Importantly, however, not all inputs are equally hard: some require larger LMs for obtaining a satisfactory solution, whereas for others smaller LMs suffice. Based on this fact, we design a framework for Cost-Effective Language Model Choice (CELMOC). Given a set of inputs and a set of candidate LMs, CELMOC judiciously assigns each input to an LM predicted to do well on the input according to a so-called meta-model, aiming to achieve high overall performance at low cost. The cost-performance trade-off can be flexibly tuned by the user. Options include, among others, maximizing total expected performance (or the number of processed inputs) w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#32534;&#30721;&#22120;&#30340;&#25991;&#26723;&#32423;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#19978;&#19979;&#25991;&#32534;&#30721;&#30340;&#26696;&#20363;&#65292;&#32467;&#26524;&#34920;&#26126;&#19978;&#19979;&#25991;&#23545;&#20195;&#35789;&#32763;&#35793;&#30340;&#20934;&#30830;&#24615;&#24433;&#21709;&#19981;&#22823;&#65292;&#24182;&#19988;&#19978;&#19979;&#25991;&#32534;&#30721;&#22120;&#25552;&#20379;&#20102;&#36275;&#22815;&#30340;&#20449;&#24687;&#26469;&#23398;&#20064;&#35821;&#31687;&#32423;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.06063</link><description>&lt;p&gt;
&#22810;&#32534;&#30721;&#22120;&#30340;&#25991;&#26723;&#32423;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#19978;&#19979;&#25991;&#32534;&#30721;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Case Study on Context Encoding in Multi-Encoder based Document-Level Neural Machine Translation. (arXiv:2308.06063v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#32534;&#30721;&#22120;&#30340;&#25991;&#26723;&#32423;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#19978;&#19979;&#25991;&#32534;&#30721;&#30340;&#26696;&#20363;&#65292;&#32467;&#26524;&#34920;&#26126;&#19978;&#19979;&#25991;&#23545;&#20195;&#35789;&#32763;&#35793;&#30340;&#20934;&#30830;&#24615;&#24433;&#21709;&#19981;&#22823;&#65292;&#24182;&#19988;&#19978;&#19979;&#25991;&#32534;&#30721;&#22120;&#25552;&#20379;&#20102;&#36275;&#22815;&#30340;&#20449;&#24687;&#26469;&#23398;&#20064;&#35821;&#31687;&#32423;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22810;&#32534;&#30721;&#22120;&#27169;&#22411;&#23545;&#19978;&#19979;&#25991;&#30340;&#36873;&#25321;&#19981;&#25935;&#24863;&#65292;&#24182;&#19988;&#19978;&#19979;&#25991;&#32534;&#30721;&#22120;&#20135;&#29983;&#30340;&#22122;&#22768;&#26377;&#21161;&#20110;&#25552;&#39640;&#27169;&#22411;&#30340;BLEU&#20998;&#25968;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#19977;&#31181;&#19981;&#21516;&#19978;&#19979;&#25991;&#35774;&#32622;&#19979;&#35757;&#32451;&#22810;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#24182;&#35780;&#20272;&#19978;&#19979;&#25991;&#24863;&#30693;&#20195;&#35789;&#32763;&#35793;&#27979;&#35797;&#38598;&#26469;&#36827;&#19968;&#27493;&#25506;&#32034;&#36825;&#20010;&#24819;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22312;ContraPro&#27979;&#35797;&#38598;&#19978;&#35780;&#20272;&#27169;&#22411;&#65292;&#20197;&#30740;&#31350;&#19981;&#21516;&#19978;&#19979;&#25991;&#23545;&#20195;&#35789;&#32763;&#35793;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#19978;&#19979;&#25991;&#26159;&#38543;&#26426;&#30340;&#65292;&#35813;&#27169;&#22411;&#22312;ContraPro&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#28304;&#34920;&#31034;&#20197;&#30740;&#31350;&#19978;&#19979;&#25991;&#32534;&#30721;&#22120;&#26159;&#21542;&#20135;&#29983;&#22122;&#22768;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#19978;&#19979;&#25991;&#32534;&#30721;&#22120;&#25552;&#20379;&#20102;&#36275;&#22815;&#30340;&#20449;&#24687;&#26469;&#23398;&#20064;&#35821;&#31687;&#32423;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#23558;&#36873;&#25321;&#30340;&#19978;&#19979;&#25991;&#65288;&#21363;&#21069;&#20004;&#20010;&#21477;&#23376;&#65289;&#28151;&#21512;&#22312;&#19968;&#36215;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have shown that the multi-encoder models are agnostic to the choice of context, and the context encoder generates noise which helps improve the models in terms of BLEU score. In this paper, we further explore this idea by evaluating with context-aware pronoun translation test set by training multi-encoder models trained on three different context settings viz, previous two sentences, random two sentences, and a mix of both as context. Specifically, we evaluate the models on the ContraPro test set to study how different contexts affect pronoun translation accuracy. The results show that the model can perform well on the ContraPro test set even when the context is random. We also analyze the source representations to study whether the context encoder generates noise. Our analysis shows that the context encoder provides sufficient information to learn discourse-level information. Additionally, we observe that mixing the selected context (the previous two sentences in this c
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#23398;&#20064;&#24341;&#23548;&#65288;LTG&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#20010;&#24615;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#20154;&#31867;&#19987;&#23478;&#25552;&#20379;&#26377;&#21161;&#20110;&#25351;&#23548;&#20915;&#31574;&#30340;&#25351;&#23548;&#65292;&#35299;&#20915;&#20102;&#26426;&#22120;&#20915;&#31574;&#21644;&#20154;&#31867;&#20915;&#31574;&#20043;&#38388;&#30340;&#20381;&#36182;&#38382;&#39064;&#12290;&#21033;&#29992;SLOG&#23454;&#29616;&#65292;&#35813;&#26694;&#26550;&#22312;&#21307;&#23398;&#35786;&#26029;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#21021;&#27493;&#20294;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.06039</link><description>&lt;p&gt;
&#23398;&#20064;&#36890;&#36807;&#20010;&#24615;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25351;&#23548;&#20154;&#31867;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
Learning to Guide Human Experts via Personalized Large Language Models. (arXiv:2308.06039v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06039
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#23398;&#20064;&#24341;&#23548;&#65288;LTG&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#20010;&#24615;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#20154;&#31867;&#19987;&#23478;&#25552;&#20379;&#26377;&#21161;&#20110;&#25351;&#23548;&#20915;&#31574;&#30340;&#25351;&#23548;&#65292;&#35299;&#20915;&#20102;&#26426;&#22120;&#20915;&#31574;&#21644;&#20154;&#31867;&#20915;&#31574;&#20043;&#38388;&#30340;&#20381;&#36182;&#38382;&#39064;&#12290;&#21033;&#29992;SLOG&#23454;&#29616;&#65292;&#35813;&#26694;&#26550;&#22312;&#21307;&#23398;&#35786;&#26029;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#21021;&#27493;&#20294;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23398;&#20064;&#25512;&#36831;&#30340;&#36807;&#31243;&#20013;&#65292;&#19968;&#20010;&#39044;&#27979;&#22120;&#35782;&#21035;&#20986;&#39118;&#38505;&#20915;&#31574;&#24182;&#23558;&#23427;&#20204;&#25512;&#36831;&#32473;&#20154;&#31867;&#19987;&#23478;&#12290;&#36825;&#31181;&#35774;&#32622;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#65292;&#30001;&#20110;&#38170;&#23450;&#20559;&#35265;&#65292;&#19987;&#23478;&#21487;&#33021;&#20250;&#36807;&#20998;&#20381;&#36182;&#20110;&#26426;&#22120;&#30340;&#20915;&#31574;&#12290;&#21516;&#26102;&#65292;&#27599;&#24403;&#26426;&#22120;&#36873;&#25321;&#25512;&#36831;&#36873;&#39033;&#26102;&#65292;&#19987;&#23478;&#24517;&#39035;&#23436;&#20840;&#33258;&#20027;&#22320;&#20570;&#20986;&#20915;&#31574;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#24341;&#23548;&#65288;LTG&#65289;&#30340;&#21478;&#19968;&#31181;&#26694;&#26550;&#65292;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#26426;&#22120;&#19981;&#26159;&#25552;&#20379;&#29616;&#25104;&#30340;&#20915;&#31574;&#65292;&#32780;&#26159;&#25552;&#20379;&#26377;&#21161;&#20110;&#25351;&#23548;&#20915;&#31574;&#30340;&#25351;&#23548;&#65292;&#24182;&#19988;&#20154;&#31867;&#23436;&#20840;&#36127;&#36131;&#20570;&#20986;&#20915;&#31574;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;SLOG&#65292;&#36825;&#26159;&#19968;&#20010;LTG&#23454;&#29616;&#65292;&#21033;&#29992;&#65288;&#23569;&#37327;&#65289;&#20154;&#31867;&#30417;&#30563;&#23558;&#36890;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36716;&#21270;&#20026;&#33021;&#22815;&#29983;&#25104;&#25991;&#26412;&#25351;&#23548;&#30340;&#27169;&#22359;&#65292;&#24182;&#22312;&#21307;&#23398;&#35786;&#26029;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#21021;&#27493;&#20294;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In learning to defer, a predictor identifies risky decisions and defers them to a human expert. One key issue with this setup is that the expert may end up over-relying on the machine's decisions, due to anchoring bias. At the same time, whenever the machine chooses the deferral option the expert has to take decisions entirely unassisted. As a remedy, we propose learning to guide (LTG), an alternative framework in which -- rather than suggesting ready-made decisions -- the machine provides guidance useful to guide decision-making, and the human is entirely responsible for coming up with a decision. We also introduce SLOG, an LTG implementation that leverages (a small amount of) human supervision to convert a generic large language model into a module capable of generating textual guidance, and present preliminary but promising results on a medical diagnosis task.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;mLLMs&#65289;&#22312;&#39044;&#27979;&#35821;&#35328;&#22788;&#29702;&#36807;&#31243;&#20013;&#19982;&#20154;&#31867;&#30340;&#35270;&#35273;-&#35821;&#35328;&#38598;&#25104;&#33021;&#21147;&#26159;&#21542;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;mLLMs&#30340;&#22810;&#27169;&#24577;&#36755;&#20837;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#35748;&#30693;&#36127;&#33655;&#65292;&#25552;&#39640;&#24863;&#30693;&#21644;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.06035</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#35821;&#35328;&#22788;&#29702;&#26399;&#38388;&#34920;&#29616;&#20986;&#20154;&#31867;&#35270;&#35273;-&#35821;&#35328;&#38598;&#25104;&#30340;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Evidence of Human-Like Visual-Linguistic Integration in Multimodal Large Language Models During Predictive Language Processing. (arXiv:2308.06035v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06035
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;mLLMs&#65289;&#22312;&#39044;&#27979;&#35821;&#35328;&#22788;&#29702;&#36807;&#31243;&#20013;&#19982;&#20154;&#31867;&#30340;&#35270;&#35273;-&#35821;&#35328;&#38598;&#25104;&#33021;&#21147;&#26159;&#21542;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;mLLMs&#30340;&#22810;&#27169;&#24577;&#36755;&#20837;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#35748;&#30693;&#36127;&#33655;&#65292;&#25552;&#39640;&#24863;&#30693;&#21644;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20808;&#36827;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#24341;&#21457;&#20102;&#20851;&#20110;&#23427;&#20204;&#26159;&#21542;&#33021;&#22815;&#22797;&#21046;&#20154;&#31867;&#35748;&#30693;&#36807;&#31243;&#30340;&#20105;&#35758;&#12290;LLMs&#21644;&#20154;&#31867;&#22312;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#19968;&#20010;&#21306;&#21035;&#22312;&#20110;&#65292;&#35821;&#35328;&#36755;&#20837;&#36890;&#24120;&#24314;&#31435;&#22312;&#22810;&#20010;&#30693;&#35273;&#27169;&#24577;&#19978;&#65292;&#32780;&#22823;&#22810;&#25968;LLMs&#20165;&#22788;&#29702;&#22522;&#20110;&#25991;&#26412;&#30340;&#20449;&#24687;&#12290;&#22810;&#27169;&#24577;&#22522;&#30784;&#20351;&#20154;&#31867;&#33021;&#22815;&#25972;&#21512;&#35270;&#35273;&#32972;&#26223;&#19982;&#35821;&#35328;&#20449;&#24687;&#65292;&#20174;&#32780;&#23545;&#21363;&#23558;&#20986;&#29616;&#30340;&#21333;&#35789;&#30340;&#31354;&#38388;&#26045;&#21152;&#38480;&#21046;&#65292;&#20943;&#23569;&#35748;&#30693;&#36127;&#33655;&#65292;&#25552;&#39640;&#24863;&#30693;&#21644;&#29702;&#35299;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#22810;&#27169;&#24577;LLMs&#65288;mLLMs&#65289;&#32467;&#21512;&#20102;&#35270;&#35273;&#21644;&#35821;&#35328;&#23884;&#20837;&#31354;&#38388;&#65292;&#24182;&#20351;&#29992;&#21464;&#21387;&#22120;&#31867;&#22411;&#30340;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#19979;&#19968;&#20010;&#21333;&#35789;&#30340;&#39044;&#27979;&#12290;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#65292;&#22522;&#20110;&#22810;&#27169;&#24577;&#36755;&#20837;&#30340;&#39044;&#27979;&#35821;&#35328;&#22788;&#29702;&#22312;mLLMs&#21644;&#20154;&#31867;&#20013;&#21563;&#21512;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;200&#21517;&#34987;&#35797;&#35266;&#30475;&#20102;&#30701;&#30340;&#35270;&#21548;&#21098;&#36753;&#65292;&#24182;&#20272;&#35745;&#20102;&#21363;&#23558;&#20986;&#29616;&#30340;&#21160;&#35789;&#25110;&#21517;&#35789;&#30340;&#21487;&#39044;&#27979;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advanced language processing abilities of large language models (LLMs) have stimulated debate over their capacity to replicate human-like cognitive processes. One differentiating factor between language processing in LLMs and humans is that language input is often grounded in more than one perceptual modality, whereas most LLMs process solely text-based information. Multimodal grounding allows humans to integrate - e.g. visual context with linguistic information and thereby place constraints on the space of upcoming words, reducing cognitive load and improving perception and comprehension. Recent multimodal LLMs (mLLMs) combine visual and linguistic embedding spaces with a transformer type attention mechanism for next-word prediction. To what extent does predictive language processing based on multimodal input align in mLLMs and humans? To answer this question, 200 human participants watched short audio-visual clips and estimated the predictability of an upcoming verb or noun. The 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#21152;&#23494;&#36135;&#24065;&#35777;&#21048;&#26696;&#20214;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#33021;&#22815;&#20934;&#30830;&#21028;&#26029;&#36829;&#27861;&#34892;&#20026;&#65292;&#24182;&#27604;&#36739;&#20102;&#30001;LLM&#21644;&#24459;&#24072;&#25776;&#20889;&#30340;&#25237;&#35785;&#20070;&#23545;&#38506;&#23457;&#22242;&#20915;&#31574;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30446;&#21069;&#30340;LLMs&#22312;&#27861;&#24459;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#36739;&#24369;&#65292;&#20294;&#38543;&#30528;&#26410;&#26469;&#27169;&#22411;&#30340;&#25913;&#36827;&#65292;&#20854;&#28508;&#21147;&#26377;&#26395;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2308.06032</link><description>&lt;p&gt;
&#21152;&#23494;&#36135;&#24065;&#35777;&#21048;&#26696;&#20214;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;ChatGPT&#33021;&#21542;&#21462;&#20195;&#24459;&#24072;&#65311;
&lt;/p&gt;
&lt;p&gt;
Large Language Models in Cryptocurrency Securities Cases: Can ChatGPT Replace Lawyers?. (arXiv:2308.06032v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#21152;&#23494;&#36135;&#24065;&#35777;&#21048;&#26696;&#20214;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#33021;&#22815;&#20934;&#30830;&#21028;&#26029;&#36829;&#27861;&#34892;&#20026;&#65292;&#24182;&#27604;&#36739;&#20102;&#30001;LLM&#21644;&#24459;&#24072;&#25776;&#20889;&#30340;&#25237;&#35785;&#20070;&#23545;&#38506;&#23457;&#22242;&#20915;&#31574;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30446;&#21069;&#30340;LLMs&#22312;&#27861;&#24459;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#36739;&#24369;&#65292;&#20294;&#38543;&#30528;&#26410;&#26469;&#27169;&#22411;&#30340;&#25913;&#36827;&#65292;&#20854;&#28508;&#21147;&#26377;&#26395;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#22686;&#24378;&#23545;&#27861;&#24459;&#31995;&#32479;&#30340;&#35775;&#38382;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#23427;&#20204;&#22312;&#36827;&#34892;&#27861;&#24459;&#20219;&#21153;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#30340;&#23454;&#35777;&#30740;&#31350;&#38750;&#24120;&#26377;&#38480;&#12290;&#25105;&#20204;&#30740;&#31350;&#28041;&#21450;&#21152;&#23494;&#36135;&#24065;&#30340;&#35777;&#21048;&#26696;&#20214;&#65292;&#20316;&#20026;AI&#21487;&#20197;&#25903;&#25345;&#27861;&#24459;&#36807;&#31243;&#30340;&#20247;&#22810;&#24773;&#22659;&#20043;&#19968;&#65292;&#30740;&#31350;LLMs&#30340;&#27861;&#24459;&#25512;&#29702;&#21644;&#36215;&#33609;&#33021;&#21147;&#12290;&#25105;&#20204;&#26816;&#26597;&#20197;&#19979;&#20004;&#20010;&#26041;&#38754;&#65306;a&#65289;LLM&#33021;&#21542;&#20934;&#30830;&#30830;&#23450;&#20107;&#23454;&#27169;&#24335;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#36829;&#27861;&#34892;&#20026;&#65292;b&#65289;&#22522;&#20110;LLM&#21644;&#24459;&#24072;&#25776;&#20889;&#30340;&#25237;&#35785;&#20070;&#65292;&#38506;&#23457;&#22242;&#30340;&#20915;&#31574;&#26159;&#21542;&#26377;&#25152;&#24046;&#24322;&#12290;&#25105;&#20204;&#23558;&#30495;&#23454;&#26696;&#20363;&#20013;&#30340;&#20107;&#23454;&#27169;&#24335;&#36755;&#20837;GPT-3.5&#65292;&#24182;&#35780;&#20272;&#20854;&#30830;&#23450;&#27491;&#30830;&#28508;&#22312;&#36829;&#27861;&#34892;&#20026;&#24182;&#25490;&#38500;&#34394;&#20551;&#36829;&#27861;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35831;&#27169;&#25311;&#38506;&#23457;&#21592;&#35780;&#20272;LLM&#21644;&#24459;&#24072;&#25776;&#20889;&#30340;&#25237;&#35785;&#20070;&#12290;GPT-3.5&#30340;&#27861;&#24459;&#25512;&#29702;&#33021;&#21147;&#36739;&#24369;&#65292;&#20294;&#25105;&#20204;&#39044;&#26399;&#26410;&#26469;&#27169;&#22411;&#30340;&#25913;&#36827;&#65292;&#29305;&#21035;&#26159;&#32771;&#34385;&#21040;&#23427;&#24314;&#35758;&#30340;&#36829;&#27861;&#34892;&#20026;&#24448;&#24448;&#26159;&#27491;&#30830;&#30340;&#65288;&#23427;&#20165;&#20165;&#36807;&#20110;&#20445;&#23432;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) could enhance access to the legal system. However, empirical research on their effectiveness in conducting legal tasks is scant. We study securities cases involving cryptocurrencies as one of numerous contexts where AI could support the legal process, studying LLMs' legal reasoning and drafting capabilities. We examine whether a) an LLM can accurately determine which laws are potentially being violated from a fact pattern, and b) whether there is a difference in juror decision-making based on complaints written by a lawyer compared to an LLM. We feed fact patterns from real-life cases to GPT-3.5 and evaluate its ability to determine correct potential violations from the scenario and exclude spurious violations. Second, we had mock jurors assess complaints written by the LLM and lawyers. GPT-3.5's legal reasoning skills proved weak, though we expect improvement in future models, particularly given the violations it suggested tended to be correct (it merely m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#28040;&#34701;&#23454;&#39564;&#21457;&#29616;&#65292;&#22312;&#21333;&#20010;GPU&#19978;&#65292;&#21442;&#25968;&#25968;&#37327;&#26368;&#22810;&#30340;&#32452;&#21512;&#24182;&#19981;&#19968;&#23450;&#26159;&#26368;&#26377;&#25928;&#30340;&#65292;&#36890;&#36807;&#20943;&#23569;&#21442;&#25968;&#22823;&#23567;&#21487;&#20197;&#22312;&#19981;&#38477;&#20302;&#32763;&#35793;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#22797;&#26434;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#36229;&#21442;&#25968;&#36873;&#25321;&#12289;&#27169;&#22411;&#22823;&#23567;&#21644;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2308.06017</link><description>&lt;p&gt;
&#20248;&#21270;&#21333;GPU&#35757;&#32451;&#30340;&#22522;&#20110;transformer&#30340;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#65306;&#36229;&#21442;&#25968;&#28040;&#34701;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Optimizing transformer-based machine translation model for single GPU training: a hyperparameter ablation study. (arXiv:2308.06017v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#28040;&#34701;&#23454;&#39564;&#21457;&#29616;&#65292;&#22312;&#21333;&#20010;GPU&#19978;&#65292;&#21442;&#25968;&#25968;&#37327;&#26368;&#22810;&#30340;&#32452;&#21512;&#24182;&#19981;&#19968;&#23450;&#26159;&#26368;&#26377;&#25928;&#30340;&#65292;&#36890;&#36807;&#20943;&#23569;&#21442;&#25968;&#22823;&#23567;&#21487;&#20197;&#22312;&#19981;&#38477;&#20302;&#32763;&#35793;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#22797;&#26434;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#36229;&#21442;&#25968;&#36873;&#25321;&#12289;&#27169;&#22411;&#22823;&#23567;&#21644;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#65292;&#27169;&#22411;&#22797;&#26434;&#24615;&#21644;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#32447;&#24615;&#30340;&#65292;&#39537;&#21160;&#21442;&#25968;&#25968;&#37327;&#22686;&#21152;&#24182;&#23545;&#22810;&#20010;GPU&#31561;&#35745;&#31639;&#36164;&#28304;&#25552;&#20986;&#35201;&#27714;&#12290;&#20026;&#20102;&#25506;&#32034;&#36825;&#19968;&#20551;&#35774;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#36229;&#21442;&#25968;&#28040;&#34701;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#19968;&#20010;&#22522;&#20110;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#26426;&#22120;&#32763;&#35793;&#31649;&#36947;&#22312;&#21333;&#20010;NVIDIA A100 GPU&#19978;&#30340;&#24433;&#21709;&#12290;&#19982;&#39044;&#26399;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#21457;&#29616;&#20855;&#26377;&#26368;&#22810;&#21442;&#25968;&#30340;&#32452;&#21512;&#26410;&#24517;&#26159;&#26368;&#26377;&#25928;&#30340;&#12290;&#36825;&#19968;&#24847;&#22806;&#30340;&#21457;&#29616;&#20419;&#20351;&#25105;&#20204;&#20180;&#32454;&#20943;&#23569;&#21442;&#25968;&#22823;&#23567;&#65292;&#25581;&#31034;&#20102;&#33021;&#22815;&#22312;&#21333;&#20010;GPU&#19978;&#35757;&#32451;&#22797;&#26434;&#27169;&#22411;&#32780;&#19981;&#25439;&#23475;&#32763;&#35793;&#36136;&#37327;&#30340;&#8220;&#29980;&#28857;&#8221;&#12290;&#36825;&#20123;&#21457;&#29616;&#23637;&#31034;&#20102;&#36229;&#21442;&#25968;&#36873;&#25321;&#12289;&#27169;&#22411;&#22823;&#23567;&#21644;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#35265;&#35299;&#23545;&#20110;&#21162;&#21147;&#36827;&#34892;&#26426;&#22120;&#32763;&#35793;&#30340;&#25913;&#36827;&#24037;&#20316;&#26377;&#25152;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
In machine translation tasks, the relationship between model complexity and performance is often presumed to be linear, driving an increase in the number of parameters and consequent demands for computational resources like multiple GPUs. To explore this assumption, this study systematically investigates the effects of hyperparameters through ablation on a sequence-to-sequence machine translation pipeline, utilizing a single NVIDIA A100 GPU. Contrary to expectations, our experiments reveal that combinations with the most parameters were not necessarily the most effective. This unexpected insight prompted a careful reduction in parameter sizes, uncovering "sweet spots" that enable training sophisticated models on a single GPU without compromising translation quality. The findings demonstrate an intricate relationship between hyperparameter selection, model size, and computational resource needs. The insights from this study contribute to the ongoing efforts to make machine translation m
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20351;&#29992;Viterbi&#31639;&#27861;&#21644;&#36801;&#31227;&#23398;&#20064;&#26469;&#25552;&#21462;&#25512;&#29305;&#24773;&#32490;&#65292;&#24341;&#20837;&#32622;&#20449;&#24230;&#20998;&#25968;&#21644;&#21521;&#37327;&#20316;&#20026;&#35780;&#20272;&#27169;&#22411;&#30340;&#25351;&#26631;&#65292;&#36827;&#34892;&#27169;&#22411;&#30340;&#20869;&#37096;&#35780;&#20272;&#21644;&#24494;&#35843;&#12290;</title><link>http://arxiv.org/abs/2308.05973</link><description>&lt;p&gt;
&#20351;&#29992;Viterbi&#31639;&#27861;&#21644;&#36801;&#31227;&#23398;&#20064;&#36827;&#34892;&#25512;&#29305;&#24773;&#32490;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Tweet Sentiment Extraction using Viterbi Algorithm with Transfer Learning. (arXiv:2308.05973v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05973
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20351;&#29992;Viterbi&#31639;&#27861;&#21644;&#36801;&#31227;&#23398;&#20064;&#26469;&#25552;&#21462;&#25512;&#29305;&#24773;&#32490;&#65292;&#24341;&#20837;&#32622;&#20449;&#24230;&#20998;&#25968;&#21644;&#21521;&#37327;&#20316;&#20026;&#35780;&#20272;&#27169;&#22411;&#30340;&#25351;&#26631;&#65292;&#36827;&#34892;&#27169;&#22411;&#30340;&#20869;&#37096;&#35780;&#20272;&#21644;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29305;&#24773;&#32490;&#25552;&#21462;&#26159;&#25552;&#21462;&#21477;&#23376;&#20013;&#26368;&#37325;&#35201;&#37096;&#20998;&#30340;&#36807;&#31243;&#65292;&#21028;&#26029;&#24773;&#32490;&#26159;&#31215;&#26497;&#36824;&#26159;&#28040;&#26497;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#35782;&#21035;&#25512;&#29305;&#21477;&#23376;&#20013;&#24341;&#36215;&#24773;&#24863;&#30340;&#37096;&#20998;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#32487;&#32493;&#25913;&#36827;&#20316;&#32773;&#20043;&#21069;&#20462;&#25913;&#30340;Viterbi&#31639;&#27861;&#65292;&#20351;&#20854;&#33021;&#22815;&#25509;&#25910;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#21442;&#25968;&#12290;&#25105;&#20204;&#24341;&#20837;&#32622;&#20449;&#24230;&#20998;&#25968;&#21644;&#21521;&#37327;&#20316;&#20026;&#20004;&#20010;&#25351;&#26631;&#65292;&#29992;&#20110;&#22312;&#35780;&#20272;&#26368;&#32456;&#32467;&#26524;&#20043;&#21069;&#23545;&#27169;&#22411;&#36827;&#34892;&#20869;&#37096;&#35780;&#20272;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#24494;&#35843;&#36825;&#20010;&#38750;&#21442;&#25968;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#38543;&#30528;&#32622;&#20449;&#24230;&#20998;&#25968;&#21521;&#37327;&#20934;&#30830;&#22320;&#26174;&#31034;&#20986;&#26368;&#19981;&#33258;&#20449;&#30340;&#39044;&#27979;&#29366;&#24577;&#30340;&#20301;&#32622;&#65292;&#20197;&#21450;&#20462;&#25913;&#26159;&#21542;&#25913;&#21892;&#20102;&#32622;&#20449;&#24230;&#20998;&#25968;&#25110;&#24494;&#35843;&#26159;&#21542;&#26397;&#30528;&#38169;&#35823;&#30340;&#26041;&#21521;&#36827;&#34892;&#65292;&#27169;&#22411;&#21464;&#24471;&#26356;&#26131;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tweet sentiment extraction extracts the most significant portion of the sentence, determining whether the sentiment is positive or negative. This research aims to identify the part of tweet sentences that strikes any emotion. To reach this objective, we continue improving the Viterbi algorithm previously modified by the author to make it able to receive pre-trained model parameters. We introduce the confidence score and vector as two indicators responsible for evaluating the model internally before assessing the final results. We then present a method to fine-tune this nonparametric model. We found that the model gets highly explainable as the confidence score vector reveals precisely where the least confidence predicted states are and if the modifications approved ameliorate the confidence score or if the tuning is going in the wrong direction.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#34394;&#25311;&#30340;MOOC&#21161;&#25945; LittleMu&#65292;&#36890;&#36807;&#25972;&#21512;&#24322;&#26500;&#25968;&#25454;&#28304;&#21644;&#25945;&#23398;&#25552;&#31034;&#38142;&#36335;&#26469;&#25903;&#25345;&#24191;&#27867;&#33539;&#22260;&#30340;&#20934;&#30830;&#22238;&#31572;&#21644;&#30693;&#35782;&#30456;&#20851;&#30340;&#38386;&#32842;&#26381;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.05935</link><description>&lt;p&gt;
LittleMu&#65306;&#36890;&#36807;&#24322;&#26500;&#25968;&#25454;&#28304;&#25972;&#21512;&#21644;&#25945;&#23398;&#25552;&#31034;&#38142;&#36335;&#37096;&#32626;&#22312;&#32447;&#34394;&#25311;&#21161;&#25945;
&lt;/p&gt;
&lt;p&gt;
LittleMu: Deploying an Online Virtual Teaching Assistant via Heterogeneous Sources Integration and Chain of Teach Prompts. (arXiv:2308.05935v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#34394;&#25311;&#30340;MOOC&#21161;&#25945; LittleMu&#65292;&#36890;&#36807;&#25972;&#21512;&#24322;&#26500;&#25968;&#25454;&#28304;&#21644;&#25945;&#23398;&#25552;&#31034;&#38142;&#36335;&#26469;&#25903;&#25345;&#24191;&#27867;&#33539;&#22260;&#30340;&#20934;&#30830;&#22238;&#31572;&#21644;&#30693;&#35782;&#30456;&#20851;&#30340;&#38386;&#32842;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25945;&#32946;&#30340;&#28459;&#38271;&#21382;&#21490;&#20013;&#65292;&#21161;&#25945;&#22312;&#23398;&#20064;&#20013;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#30495;&#23454;&#22312;&#32447;&#25945;&#32946;&#22330;&#26223;&#30340;&#22797;&#26434;&#24615;&#21644;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#65292;&#24456;&#23569;&#26377;MOOC&#24179;&#21488;&#25552;&#20379;&#20154;&#24037;&#25110;&#34394;&#25311;&#21161;&#25945;&#26469;&#25903;&#25345;&#22823;&#37327;&#22312;&#32447;&#23398;&#29983;&#30340;&#23398;&#20064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#34394;&#25311;&#30340;MOOC&#21161;&#25945;LittleMu&#65292;&#20165;&#20351;&#29992;&#23569;&#37327;&#26631;&#27880;&#35757;&#32451;&#25968;&#25454;&#65292;&#25552;&#20379;&#38382;&#39064;&#22238;&#31572;&#21644;&#38386;&#32842;&#26381;&#21153;&#12290;LittleMu&#30001;&#20004;&#20010;&#20132;&#20114;&#27169;&#22359;&#32452;&#25104;&#65292;&#21253;&#25324;&#24322;&#26500;&#26816;&#32034;&#21644;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#65292;&#39318;&#20808;&#25972;&#21512;&#32467;&#26500;&#21270;&#12289;&#21322;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#30340;&#30693;&#35782;&#28304;&#65292;&#25903;&#25345;&#24191;&#27867;&#33539;&#22260;&#30340;&#38382;&#39064;&#30340;&#20934;&#30830;&#22238;&#31572;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#21517;&#20026;&#8220;Chain of Teach&#8221;&#25552;&#31034;&#30340;&#31934;&#24515;&#31034;&#33539;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#22788;&#29702;&#22797;&#26434;&#30340;&#26410;&#25910;&#38598;&#38382;&#39064;&#12290;&#38500;&#20102;&#38382;&#39064;&#22238;&#31572;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#20854;&#20182;&#25945;&#32946;&#26381;&#21153;&#65292;&#22914;&#30693;&#35782;&#30456;&#20851;&#30340;&#38386;&#32842;&#12290;&#25105;&#20204;&#36890;&#36807;&#26426;&#22120;&#20154;&#27979;&#35797;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Teaching assistants have played essential roles in the long history of education. However, few MOOC platforms are providing human or virtual teaching assistants to support learning for massive online students due to the complexity of real-world online education scenarios and the lack of training data. In this paper, we present a virtual MOOC teaching assistant, LittleMu with minimum labeled training data, to provide question answering and chit-chat services. Consisting of two interactive modules of heterogeneous retrieval and language model prompting, LittleMu first integrates structural, semi- and unstructured knowledge sources to support accurate answers for a wide range of questions. Then, we design delicate demonstrations named "Chain of Teach" prompts to exploit the large-scale pre-trained model to handle complex uncollected questions. Except for question answering, we develop other educational services such as knowledge-grounded chit-chat. We test the system's performance via bot
&lt;/p&gt;</description></item><item><title>PIPPA&#26159;&#19968;&#20010;&#37096;&#20998;&#21512;&#25104;&#30340;&#25968;&#25454;&#38598;&#65292;&#20026;&#30740;&#31350;&#21644;&#24320;&#21457;&#23545;&#35805;AI&#31995;&#32479;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#36164;&#28304;&#65292;&#20197;&#24212;&#23545;&#29616;&#26377;&#25968;&#25454;&#38598;&#26080;&#27861;&#25429;&#25417;&#30495;&#23454;&#19990;&#30028;&#35282;&#33394;&#25198;&#28436;&#20132;&#20114;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.05884</link><description>&lt;p&gt;
PIPPA:&#19968;&#20010;&#37096;&#20998;&#21512;&#25104;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
PIPPA: A Partially Synthetic Conversational Dataset. (arXiv:2308.05884v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05884
&lt;/p&gt;
&lt;p&gt;
PIPPA&#26159;&#19968;&#20010;&#37096;&#20998;&#21512;&#25104;&#30340;&#25968;&#25454;&#38598;&#65292;&#20026;&#30740;&#31350;&#21644;&#24320;&#21457;&#23545;&#35805;AI&#31995;&#32479;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#36164;&#28304;&#65292;&#20197;&#24212;&#23545;&#29616;&#26377;&#25968;&#25454;&#38598;&#26080;&#27861;&#25429;&#25417;&#30495;&#23454;&#19990;&#30028;&#35282;&#33394;&#25198;&#28436;&#20132;&#20114;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36234;&#26469;&#36234;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#26377;&#20852;&#36259;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#38750;&#27491;&#24335;&#23545;&#35805;&#21644;&#35282;&#33394;&#25198;&#28436;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23545;&#35805;&#21644;&#35282;&#33394;&#25198;&#28436;&#25968;&#25454;&#38598;&#24448;&#24448;&#26080;&#27861;&#25429;&#25417;&#21040;&#30495;&#23454;&#19990;&#30028;&#35282;&#33394;&#25198;&#28436;&#21442;&#19982;&#32773;&#25152;&#23637;&#31034;&#30340;&#22810;&#26679;&#21644;&#24494;&#22937;&#30340;&#20132;&#20114;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#24182;&#20026;&#36825;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#20570;&#20986;&#36129;&#29486;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#37096;&#20998;&#21512;&#25104;&#30340;&#25968;&#25454;&#38598;PIPPA&#65288;&#20154;&#19982;AI&#20043;&#38388;&#30340;&#20010;&#20154;&#20132;&#20114;&#23545;&#65289;&#12290;PIPPA&#26159;&#19968;&#20010;&#30001;&#19968;&#32676;&#35282;&#33394;&#25198;&#28436;&#29233;&#22909;&#32773;&#21442;&#19982;&#30340;&#31038;&#21306;&#39537;&#21160;&#30340;&#20247;&#21253;&#21162;&#21147;&#30340;&#32467;&#26524;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#20998;&#24067;&#22312;26,000&#20010;&#23545;&#35805;&#20250;&#35805;&#20013;&#30340;&#36229;&#36807;100&#19975;&#20010;&#35805;&#35821;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;AI&#24320;&#21457;&#20154;&#21592;&#22312;&#35282;&#33394;&#25198;&#28436;&#22330;&#26223;&#20013;&#25506;&#32034;&#21644;&#23436;&#21892;&#23545;&#35805;AI&#31995;&#32479;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the emergence of increasingly powerful large language models, there is a burgeoning interest in leveraging these models for casual conversation and role-play applications. However, existing conversational and role-playing datasets often fail to capture the diverse and nuanced interactions typically exhibited by real-world role-play participants. To address this limitation and contribute to the rapidly growing field, we introduce a partially-synthetic dataset named PIPPA (Personal Interaction Pairs between People and AI). PIPPA is a result of a community-driven crowdsourcing effort involving a group of role-play enthusiasts. The dataset comprises over 1 million utterances that are distributed across 26,000 conversation sessions and provides a rich resource for researchers and AI developers to explore and refine conversational AI systems in the context of role-play scenarios.
&lt;/p&gt;</description></item><item><title>LLM&#21464;&#25104;DBA&#65292;&#25552;&#20379;&#25968;&#25454;&#24211;&#32500;&#25252;&#30340;&#35786;&#26029;&#21644;&#20248;&#21270;&#24314;&#35758;&#65292;&#36890;&#36807;&#20174;&#25991;&#26412;&#26469;&#28304;&#20013;&#33719;&#21462;&#32463;&#39564;&#21644;&#22810;&#20010;LLMs&#30340;&#21327;&#20316;&#35786;&#26029;&#12290;</title><link>http://arxiv.org/abs/2308.05481</link><description>&lt;p&gt;
LLM&#21464;&#25104;DBA
&lt;/p&gt;
&lt;p&gt;
LLM As DBA. (arXiv:2308.05481v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05481
&lt;/p&gt;
&lt;p&gt;
LLM&#21464;&#25104;DBA&#65292;&#25552;&#20379;&#25968;&#25454;&#24211;&#32500;&#25252;&#30340;&#35786;&#26029;&#21644;&#20248;&#21270;&#24314;&#35758;&#65292;&#36890;&#36807;&#20174;&#25991;&#26412;&#26469;&#28304;&#20013;&#33719;&#21462;&#32463;&#39564;&#21644;&#22810;&#20010;LLMs&#30340;&#21327;&#20316;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#24211;&#31649;&#29702;&#21592;&#65288;DBA&#65289;&#22312;&#31649;&#29702;&#12289;&#32500;&#25252;&#21644;&#20248;&#21270;&#25968;&#25454;&#24211;&#31995;&#32479;&#20197;&#30830;&#20445;&#25968;&#25454;&#21487;&#29992;&#24615;&#12289;&#24615;&#33021;&#21644;&#21487;&#38752;&#24615;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;DBA&#26469;&#35828;&#65292;&#31649;&#29702;&#22823;&#37327;&#25968;&#25454;&#24211;&#23454;&#20363;&#65288;&#20363;&#22914;&#65292;&#20113;&#25968;&#25454;&#24211;&#19978;&#30340;&#25968;&#30334;&#19975;&#20010;&#23454;&#20363;&#65289;&#26159;&#22256;&#38590;&#21644;&#32321;&#29712;&#30340;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#26174;&#31034;&#20986;&#20102;&#29702;&#35299;&#26377;&#20215;&#20540;&#25991;&#20214;&#24182;&#29983;&#25104;&#21512;&#29702;&#31572;&#26696;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;D-Bot&#65292;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#25968;&#25454;&#24211;&#31649;&#29702;&#21592;&#65292;&#23427;&#21487;&#20197;&#25345;&#32493;&#20174;&#25991;&#26412;&#26469;&#28304;&#20013;&#33719;&#21462;&#25968;&#25454;&#24211;&#32500;&#25252;&#32463;&#39564;&#65292;&#24182;&#20026;&#30446;&#26631;&#25968;&#25454;&#24211;&#25552;&#20379;&#21512;&#29702;&#12289;&#26377;&#29702;&#12289;&#21450;&#26102;&#30340;&#35786;&#26029;&#21644;&#20248;&#21270;&#24314;&#35758;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#38761;&#21629;&#24615;&#30340;&#20197;LLM&#20026;&#20013;&#24515;&#30340;&#25968;&#25454;&#24211;&#32500;&#25252;&#26694;&#26550;&#65292;&#21253;&#25324;&#65288;i&#65289;&#20174;&#25991;&#26723;&#21644;&#24037;&#20855;&#20013;&#26816;&#27979;&#25968;&#25454;&#24211;&#32500;&#25252;&#30693;&#35782;&#65292;&#65288;ii&#65289;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#30340;&#24605;&#32500;&#26641;&#65292;&#21644;&#65288;iii&#65289;&#22810;&#20010;LLM&#20043;&#38388;&#30340;&#21327;&#20316;&#35786;&#26029;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#21021;&#27493;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Database administrators (DBAs) play a crucial role in managing, maintaining and optimizing a database system to ensure data availability, performance, and reliability. However, it is hard and tedious for DBAs to manage a large number of database instances (e.g., millions of instances on the cloud databases). Recently large language models (LLMs) have shown great potential to understand valuable documents and accordingly generate reasonable answers. Thus, we propose D-Bot, a LLM-based database administrator that can continuously acquire database maintenance experience from textual sources, and provide reasonable, well-founded, in-time diagnosis and optimization advice for target databases. This paper presents a revolutionary LLM-centric framework for database maintenance, including (i) database maintenance knowledge detection from documents and tools, (ii) tree of thought reasoning for root cause analysis, and (iii) collaborative diagnosis among multiple LLMs. Our preliminary experiment
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#26426;&#22120;&#23398;&#20064;&#21644;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#22312;&#27450;&#35784;&#24615;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#25928;&#26524;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.05476</link><description>&lt;p&gt;
&#25506;&#32034;&#26426;&#22120;&#23398;&#20064;&#21644;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#29992;&#20110;&#27450;&#35784;&#24615;&#25991;&#26412;&#20998;&#31867;&#65306;&#19968;&#39033;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Exploring Machine Learning and Transformer-based Approaches for Deceptive Text Classification: A Comparative Analysis. (arXiv:2308.05476v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#26426;&#22120;&#23398;&#20064;&#21644;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#22312;&#27450;&#35784;&#24615;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#25928;&#26524;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27450;&#35784;&#24615;&#25991;&#26412;&#20998;&#31867;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#65292;&#26088;&#22312;&#35782;&#21035;&#27450;&#35784;&#25110;&#27450;&#39575;&#24615;&#20869;&#23481;&#12290;&#26412;&#30740;&#31350;&#23545;&#26426;&#22120;&#23398;&#20064;&#21644;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#29992;&#20110;&#27450;&#35784;&#24615;&#25991;&#26412;&#20998;&#31867;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;Transformer&#27169;&#22411;&#65288;&#22914;BERT&#65292;XLNET&#65292;DistilBERT&#21644;RoBERTa&#65289;&#22312;&#26816;&#27979;&#27450;&#35784;&#24615;&#25991;&#26412;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#24102;&#26631;&#31614;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#27450;&#35784;&#24615;&#21644;&#38750;&#27450;&#35784;&#24615;&#25991;&#26412;&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#30446;&#30340;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#26041;&#27861;&#30340;&#24615;&#33021;&#25351;&#26631;&#65292;&#21253;&#25324;&#20934;&#30830;&#29575;&#65292;&#31934;&#30830;&#29575;&#65292;&#21484;&#22238;&#29575;&#21644;F1&#24471;&#20998;&#12290;&#26412;&#30740;&#31350;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#22312;&#27450;&#35784;&#24615;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#33021;&#22815;&#22312;&#22788;&#29702;&#27450;&#35784;&#20869;&#23481;&#26102;&#20570;&#20986;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deceptive text classification is a critical task in natural language processing that aims to identify deceptive or fraudulent content. This study presents a comparative analysis of machine learning and transformer-based approaches for deceptive text classification. We investigate the effectiveness of traditional machine learning algorithms and state-of-the-art transformer models, such as BERT, XLNET, DistilBERT, and RoBERTa, in detecting deceptive text. A labeled dataset consisting of deceptive and non-deceptive texts is used for training and evaluation purposes. Through extensive experimentation, we compare the performance metrics, including accuracy, precision, recall, and F1 score, of the different approaches. The results of this study shed light on the strengths and limitations of machine learning and transformer-based methods for deceptive text classification, enabling researchers and practitioners to make informed decisions when dealing with deceptive content
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#23545;&#22823;&#22411;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#23398;&#31185;&#39046;&#22495;&#30340;&#29983;&#25104;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;Gscore&#20316;&#20026;&#34913;&#37327;&#29983;&#25104;&#32467;&#26524;&#36136;&#37327;&#30340;&#32508;&#21512;&#25351;&#25968;&#12290;</title><link>http://arxiv.org/abs/2308.04823</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Generation Capabilities of Large Chinese Language Models. (arXiv:2308.04823v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#23545;&#22823;&#22411;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#23398;&#31185;&#39046;&#22495;&#30340;&#29983;&#25104;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;Gscore&#20316;&#20026;&#34913;&#37327;&#29983;&#25104;&#32467;&#26524;&#36136;&#37327;&#30340;&#32508;&#21512;&#25351;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;CG-Eval&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23545;&#22823;&#22411;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#23398;&#31185;&#39046;&#22495;&#29983;&#25104;&#33021;&#21147;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#30340;&#30740;&#31350;&#12290;&#36890;&#36807;&#22312;&#31185;&#23398;&#24037;&#31243;&#12289;&#20154;&#25991;&#31038;&#31185;&#12289;&#25968;&#23398;&#35745;&#31639;&#12289;&#21307;&#24072;&#36164;&#26684;&#32771;&#35797;&#12289;&#21496;&#27861;&#32771;&#35797;&#21644;&#27880;&#20876;&#20250;&#35745;&#24072;&#32771;&#35797;&#20845;&#20010;&#23398;&#31185;&#20013;&#29983;&#25104;&#20934;&#30830;&#21644;&#30456;&#20851;&#30340;&#22238;&#31572;&#65292;&#35780;&#20272;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;Gscore&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#22810;&#20010;&#24230;&#37327;&#25351;&#26631;&#21152;&#26435;&#27714;&#21644;&#24471;&#21040;&#30340;&#32508;&#21512;&#25351;&#25968;&#65292;&#29992;&#20110;&#34913;&#37327;&#27169;&#22411;&#29983;&#25104;&#32467;&#26524;&#19982;&#21442;&#32771;&#31572;&#26696;&#30340;&#36136;&#37327;&#12290;&#27979;&#35797;&#25968;&#25454;&#21644;&#27979;&#35797;&#32467;&#26524;&#21487;&#22312;&#27492;http URL&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents CG-Eval, the first comprehensive evaluation of the generation capabilities of large Chinese language models across a wide range of academic disciplines. The models' performance was assessed based on their ability to generate accurate and relevant responses to different types of questions in six disciplines, namely, Science and Engineering, Humanities and Social Sciences, Mathematical Calculations, Medical Practitioner Qualification Examination, Judicial Examination, and Certified Public Accountant Examination. This paper also presents Gscore, a composite index derived from the weighted sum of multiple metrics to measure the quality of model's generation against a reference. The test data and test results can be found at this http URL
&lt;/p&gt;</description></item><item><title>CLASSLA-Stanza&#26159;&#19968;&#20010;&#20026;&#21335;&#26031;&#25289;&#22827;&#35821;&#35328;&#25552;&#20379;&#33258;&#21160;&#35821;&#35328;&#27880;&#37322;&#30340;&#27969;&#27700;&#32447;&#65292;&#30456;&#23545;&#20110;Stanza&#65292;&#22312;&#24615;&#33021;&#21644;&#21151;&#33021;&#19978;&#26377;&#22810;&#20010;&#25913;&#36827;&#65292;&#24182;&#21462;&#24471;&#20102;&#22987;&#32456;&#22914;&#19968;&#30340;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.04255</link><description>&lt;p&gt;
CLASSLA-Stanza: &#21335;&#26031;&#25289;&#22827;&#35821;&#35328;&#30340;&#35821;&#35328;&#22788;&#29702;&#30340;&#19979;&#19968;&#27493;
&lt;/p&gt;
&lt;p&gt;
CLASSLA-Stanza: The Next Step for Linguistic Processing of South Slavic Languages. (arXiv:2308.04255v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04255
&lt;/p&gt;
&lt;p&gt;
CLASSLA-Stanza&#26159;&#19968;&#20010;&#20026;&#21335;&#26031;&#25289;&#22827;&#35821;&#35328;&#25552;&#20379;&#33258;&#21160;&#35821;&#35328;&#27880;&#37322;&#30340;&#27969;&#27700;&#32447;&#65292;&#30456;&#23545;&#20110;Stanza&#65292;&#22312;&#24615;&#33021;&#21644;&#21151;&#33021;&#19978;&#26377;&#22810;&#20010;&#25913;&#36827;&#65292;&#24182;&#21462;&#24471;&#20102;&#22987;&#32456;&#22914;&#19968;&#30340;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;CLASSLA-Stanza&#65292;&#19968;&#20010;&#29992;&#20110;&#21335;&#26031;&#25289;&#22827;&#35821;&#35328;&#30340;&#33258;&#21160;&#35821;&#35328;&#27880;&#37322;&#27969;&#27700;&#32447;&#65292;&#35813;&#27969;&#27700;&#32447;&#22522;&#20110;Stanza&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27969;&#27700;&#32447;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;CLASSLA-Stanza&#30456;&#23545;&#20110;Stanza&#30340;&#20027;&#35201;&#25913;&#36827;&#65292;&#24182;&#35814;&#32454;&#25551;&#36848;&#20102;&#26368;&#26032;2.1&#29256;&#26412;&#27969;&#27700;&#32447;&#30340;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#12290;&#25105;&#20204;&#36824;&#25253;&#21578;&#20102;&#27969;&#27700;&#32447;&#23545;&#19981;&#21516;&#35821;&#35328;&#21644;&#21464;&#31181;&#30340;&#24615;&#33021;&#35780;&#20998;&#12290;CLASSLA-Stanza&#22312;&#25152;&#26377;&#25903;&#25345;&#30340;&#35821;&#35328;&#19978;&#34920;&#29616;&#20986;&#22987;&#32456;&#22914;&#19968;&#30340;&#39640;&#24615;&#33021;&#65292;&#24182;&#22312;&#25152;&#26377;&#25903;&#25345;&#30340;&#20219;&#21153;&#19978;&#20248;&#20110;&#25110;&#25193;&#23637;&#20102;&#20854;&#29238;&#27969;&#27700;&#32447;Stanza&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#27969;&#27700;&#32447;&#30340;&#26032;&#21151;&#33021;&#65292;&#20351;&#20854;&#33021;&#22815;&#39640;&#25928;&#22788;&#29702;&#32593;&#32476;&#25968;&#25454;&#65292;&#24182;&#35299;&#37322;&#20102;&#23548;&#33268;&#20854;&#23454;&#29616;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present CLASSLA-Stanza, a pipeline for automatic linguistic annotation of the South Slavic languages, which is based on the Stanza natural language processing pipeline. We describe the main improvements in CLASSLA-Stanza with respect to Stanza, and give a detailed description of the model training process for the latest 2.1 release of the pipeline. We also report performance scores produced by the pipeline for different languages and varieties. CLASSLA-Stanza exhibits consistently high performance across all the supported languages and outperforms or expands its parent pipeline Stanza at all the supported tasks. We also present the pipeline's new functionality enabling efficient processing of web data and the reasons that led to its implementation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;3D-EX&#30340;&#25968;&#25454;&#38598;&#65292;&#23427;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#23450;&#20041;&#21644;&#35789;&#20856;&#31034;&#20363;&#30340;&#30693;&#35782;&#23384;&#20648;&#24211;&#65292;&#21487;&#20197;&#29992;&#20110;&#25913;&#36827;&#35789;&#21521;&#37327;&#25110;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#65292;&#24182;&#19988;&#22312;&#19979;&#28216;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2308.03043</link><description>&lt;p&gt;
3D-EX&#65306;&#19968;&#31181;&#32479;&#19968;&#30340;&#23450;&#20041;&#21644;&#35789;&#20856;&#31034;&#20363;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
3D-EX : A Unified Dataset of Definitions and Dictionary Examples. (arXiv:2308.03043v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;3D-EX&#30340;&#25968;&#25454;&#38598;&#65292;&#23427;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#23450;&#20041;&#21644;&#35789;&#20856;&#31034;&#20363;&#30340;&#30693;&#35782;&#23384;&#20648;&#24211;&#65292;&#21487;&#20197;&#29992;&#20110;&#25913;&#36827;&#35789;&#21521;&#37327;&#25110;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#65292;&#24182;&#19988;&#22312;&#19979;&#28216;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23450;&#20041;&#26159;&#35789;&#20856;&#23398;&#12289;&#35821;&#35328;&#23398;&#21644;&#35745;&#31639;&#35821;&#20041;&#23398;&#20013;&#30340;&#22522;&#26412;&#26500;&#24314;&#22359;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#23427;&#20204;&#34987;&#29992;&#20110;&#25913;&#36827;&#35789;&#21521;&#37327;&#25110;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#21253;&#21547;&#23450;&#20041;&#30340;&#35789;&#20856;&#36164;&#28304;&#20855;&#26377;&#21508;&#31181;&#29305;&#24615;&#65292;&#36825;&#23545;&#20110;&#22312;&#20854;&#19978;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#27169;&#22411;&#34892;&#20026;&#26377;&#24433;&#21709;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;3D-EX&#65292;&#19968;&#20010;&#26088;&#22312;&#23558;&#20247;&#25152;&#21608;&#30693;&#30340;&#33521;&#25991;&#36164;&#28304;&#32452;&#21512;&#25104;&#19968;&#20010;&#38598;&#20013;&#30340;&#30693;&#35782;&#23384;&#20648;&#24211;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&lt;&#26415;&#35821;&#65292;&#23450;&#20041;&#65292;&#31034;&#20363;&gt;&#19977;&#20803;&#32452;&#30340;&#24418;&#24335;&#21576;&#29616;&#12290;3D-EX&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#20855;&#26377;&#32463;&#36807;&#31934;&#24515;&#39044;&#20808;&#35745;&#31639;&#30340;&#35757;&#32451;/&#39564;&#35777;/&#27979;&#35797;&#21010;&#20998;&#65292;&#20197;&#38450;&#27490;&#35760;&#24518;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#25968;&#25454;&#38598;&#22312;&#19979;&#28216;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#20855;&#26377;&#26377;&#25928;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#22312;https://github.com/F-Almeman/3D-EX &#19978;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Definitions are a fundamental building block in lexicography, linguistics and computational semantics. In NLP, they have been used for retrofitting word embeddings or augmenting contextual representations in language models. However, lexical resources containing definitions exhibit a wide range of properties, which has implications in the behaviour of models trained and evaluated on them. In this paper, we introduce 3D- EX , a dataset that aims to fill this gap by combining well-known English resources into one centralized knowledge repository in the form of &lt;term, definition, example&gt; triples. 3D- EX is a unified evaluation framework with carefully pre-computed train/validation/test splits to prevent memorization. We report experimental results that suggest that this dataset could be effectively leveraged in downstream NLP tasks. Code and data are available at https://github.com/F-Almeman/3D-EX .
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#20026;&#25918;&#23556;&#23398;&#26500;&#24314;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#21307;&#23398;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#21644;&#25903;&#25345;&#19981;&#21516;&#25918;&#23556;&#23398;&#20219;&#21153;&#30340;&#26550;&#26500;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2308.02463</link><description>&lt;p&gt;
&#20026;&#25918;&#23556;&#23398;&#26500;&#24314;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Towards Generalist Foundation Model for Radiology. (arXiv:2308.02463v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#20026;&#25918;&#23556;&#23398;&#26500;&#24314;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#21307;&#23398;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#21644;&#25903;&#25345;&#19981;&#21516;&#25918;&#23556;&#23398;&#20219;&#21153;&#30340;&#26550;&#26500;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#21551;&#21160;&#25918;&#23556;&#23398;&#22522;&#30784;&#27169;&#22411;&#30340;&#24320;&#21457;&#65292;&#31216;&#20026;RadFM&#12290;&#25105;&#20204;&#20174;&#25968;&#25454;&#12289;&#27169;&#22411;&#35774;&#35745;&#21644;&#35780;&#20272;&#30340;&#35282;&#24230;&#20840;&#38754;&#32771;&#34385;&#20102;&#22522;&#30784;&#27169;&#22411;&#30340;&#26500;&#24314;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#21487;&#24635;&#32467;&#22914;&#19979;&#65306;&#65288;i&#65289;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#21307;&#23398;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;MedMD&#65292;&#21253;&#25324;1600&#19975;&#20010;2D&#21644;3D&#21307;&#23398;&#25195;&#25551;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21253;&#21547;3D&#21307;&#23398;&#25195;&#25551;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#12290;&#65288;ii&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26550;&#26500;&#65292;&#20351;&#24471;&#21487;&#35270;&#26465;&#20214;&#29983;&#25104;&#39044;&#35757;&#32451;&#25104;&#20026;&#21487;&#33021;&#65292;&#21487;&#20197;&#23558;&#25991;&#26412;&#36755;&#20837;&#19982;2D&#25110;3D&#21307;&#23398;&#25195;&#25551;&#20132;&#38169;&#65292;&#29983;&#25104;&#19981;&#21516;&#25918;&#23556;&#23398;&#20219;&#21153;&#30340;&#21709;&#24212;&#12290;&#35813;&#27169;&#22411;&#39318;&#20808;&#22312;MedMD&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;RadMD&#19978;&#36827;&#34892;&#20102;&#29305;&#23450;&#39046;&#22495;&#30340;&#24494;&#35843;&#65292;RadMD&#26159;MedMD&#30340;&#25918;&#23556;&#23398;&#28165;&#29702;&#29256;&#26412;&#65292;&#21253;&#21547;300&#19975;&#20010;&#25918;&#23556;&#23398;&#30340;&#35270;&#35273;&#35821;&#35328;&#23545;&#12290;&#65288;iii&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#21253;&#25324;&#20116;&#20010;&#20219;&#21153;&#65292;&#26088;&#22312;&#20840;&#38754;&#35780;&#20272;&#35813;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we aim to initiate the development of Radiology Foundation Model, termed as RadFM.We consider the construction of foundational models from the perspectives of data, model design, and evaluation thoroughly. Our contribution can be concluded as follows: (i), we construct a large-scale Medical Multi-modal Dataset, MedMD, consisting of 16M 2D and 3D medical scans. To the best of our knowledge, this is the first multi-modal dataset containing 3D medical scans. (ii), We propose an architecture that enables visually conditioned generative pre-training, allowing for the integration of text input interleaved with 2D or 3D medical scans to generate response for diverse radiologic tasks. The model was initially pre-trained on MedMD and subsequently domain-specific fine-tuned on RadMD, a radiologic cleaned version of MedMD, containing 3M radiologic visual-language pairs. (iii), we propose a new evaluation benchmark that comprises five tasks, aiming to comprehensively assess the capa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#24179;&#21488;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#35299;&#32544;&#36755;&#20837;&#34920;&#31034;&#20026;&#19981;&#21464;&#29305;&#24449;&#21644;&#24179;&#21488;&#30456;&#20851;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#20010;&#26410;&#35265;&#24179;&#21488;&#30340;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.02080</link><description>&lt;p&gt;
&#36328;&#24179;&#21488;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20013;&#30340;&#22240;&#26524;&#24341;&#23548;&#35299;&#32544;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Causality Guided Disentanglement for Cross-Platform Hate Speech Detection. (arXiv:2308.02080v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02080
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#24179;&#21488;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#35299;&#32544;&#36755;&#20837;&#34920;&#31034;&#20026;&#19981;&#21464;&#29305;&#24449;&#21644;&#24179;&#21488;&#30456;&#20851;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#20010;&#26410;&#35265;&#24179;&#21488;&#30340;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#22312;&#20419;&#36827;&#20844;&#24320;&#23545;&#35805;&#26041;&#38754;&#20855;&#26377;&#20215;&#20540;&#65292;&#20294;&#20182;&#20204;&#32463;&#24120;&#34987;&#21033;&#29992;&#26469;&#20256;&#25773;&#26377;&#23475;&#20869;&#23481;&#12290;&#30446;&#21069;&#29992;&#20110;&#26816;&#27979;&#36825;&#31181;&#26377;&#23475;&#20869;&#23481;&#30340;&#28145;&#24230;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#36807;&#24230;&#20381;&#36182;&#20110;&#39046;&#22495;&#29305;&#23450;&#26415;&#35821;&#65292;&#24433;&#21709;&#21040;&#20102;&#23427;&#20204;&#36866;&#24212;&#27867;&#21270;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#33021;&#21147;&#12290;&#36825;&#26159;&#22240;&#20026;&#23427;&#20204;&#20542;&#21521;&#20110;&#36807;&#20110;&#29421;&#38552;&#22320;&#20851;&#27880;&#29305;&#23450;&#30340;&#35821;&#35328;&#20449;&#21495;&#25110;&#26576;&#20123;&#35789;&#35821;&#31867;&#21035;&#30340;&#20351;&#29992;&#12290;&#24403;&#24179;&#21488;&#32570;&#20047;&#39640;&#36136;&#37327;&#30340;&#26631;&#35760;&#25968;&#25454;&#29992;&#20110;&#35757;&#32451;&#26102;&#65292;&#21478;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#20986;&#29616;&#20102;&#65292;&#38656;&#35201;&#36328;&#24179;&#21488;&#27169;&#22411;&#26469;&#36866;&#24212;&#19981;&#21516;&#30340;&#20998;&#24067;&#36716;&#21270;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#36328;&#24179;&#21488;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#19968;&#20010;&#24179;&#21488;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#24182;&#25512;&#24191;&#21040;&#22810;&#20010;&#26410;&#35265;&#24179;&#21488;&#12290;&#20026;&#20102;&#23454;&#29616;&#23545;&#19981;&#21516;&#24179;&#21488;&#30340;&#33391;&#22909;&#27867;&#21270;&#24615;&#33021;&#65292;&#19968;&#31181;&#26041;&#27861;&#26159;&#23558;&#36755;&#20837;&#34920;&#31034;&#35299;&#32544;&#20026;&#19981;&#21464;&#29305;&#24449;&#21644;&#24179;&#21488;&#30456;&#20851;&#29305;&#24449;&#12290;&#25105;&#20204;&#36824;&#35748;&#20026;&#23398;&#20064;&#22240;&#26524;&#20851;&#31995;&#26159;&#25552;&#20379;&#26356;&#22909;&#35299;&#32544;&#21644;&#27867;&#21270;&#24615;&#33021;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social media platforms, despite their value in promoting open discourse, are often exploited to spread harmful content. Current deep learning and natural language processing models used for detecting this harmful content overly rely on domain-specific terms affecting their capabilities to adapt to generalizable hate speech detection. This is because they tend to focus too narrowly on particular linguistic signals or the use of certain categories of words. Another significant challenge arises when platforms lack high-quality annotated data for training, leading to a need for cross-platform models that can adapt to different distribution shifts. Our research introduces a cross-platform hate speech detection model capable of being trained on one platform's data and generalizing to multiple unseen platforms. To achieve good generalizability across platforms, one way is to disentangle the input representations into invariant and platform-dependent features. We also argue that learning causa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;Fine-Tuned&#30340;OpenAI LLM&#36827;&#34892;&#32763;&#35793;&#36136;&#37327;&#20272;&#35745;&#30340;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;&#21487;&#20197;&#36890;&#36807;Fine-Tuned&#30340;ChatGPT&#26469;&#39044;&#27979;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2308.00158</link><description>&lt;p&gt;
&#20351;&#29992;Fine-Tuned&#30340;OpenAI LLM&#39044;&#27979;&#26426;&#22120;&#32763;&#35793;&#36755;&#20986;&#20013;&#30340;&#23436;&#32654;&#36136;&#37327;&#27573;&#33853;&#65306;&#26159;&#21542;&#21487;&#20197;&#20174;&#21382;&#21490;&#25968;&#25454;&#20013;&#25429;&#25417;&#32534;&#36753;&#36317;&#31163;&#27169;&#24335;&#65311;
&lt;/p&gt;
&lt;p&gt;
Predicting Perfect Quality Segments in MT Output with Fine-Tuned OpenAI LLM: Is it possible to capture editing distance patterns from historical data?. (arXiv:2308.00158v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;Fine-Tuned&#30340;OpenAI LLM&#36827;&#34892;&#32763;&#35793;&#36136;&#37327;&#20272;&#35745;&#30340;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;&#21487;&#20197;&#36890;&#36807;Fine-Tuned&#30340;ChatGPT&#26469;&#39044;&#27979;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32763;&#35793;&#36136;&#37327;&#20272;&#35745;&#65288;TQE&#65289;&#26159;&#23558;&#36755;&#20986;&#32763;&#35793;&#37096;&#32626;&#21040;&#20351;&#29992;&#20013;&#20043;&#21069;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290; TQE&#23545;&#20110;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#21644;&#20154;&#24037;&#32763;&#35793;&#65288;HT&#65289;&#30340;&#36136;&#37327;&#20063;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#32780;&#19981;&#38656;&#35201;&#26597;&#30475;&#21442;&#32771;&#32763;&#35793;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26816;&#26597;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#21487;&#20197;&#20026;TQE&#20219;&#21153;&#21644;&#23427;&#20204;&#30340;&#33021;&#21147;&#36827;&#34892;Fine-Tune&#12290;&#25105;&#20204;&#20197;ChatGPT&#20026;&#20363;&#65292;&#23558;TQE&#35270;&#20026;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#12290;&#20351;&#29992;&#33521;&#24847;&#21644;&#33521;&#24503;&#35757;&#32451;&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36890;&#36807;ChatGPT&#30340;API Fine-Tuned&#21487;&#20197;&#22312;&#39044;&#27979;&#32763;&#35793;&#36136;&#37327;&#26041;&#38754;&#33719;&#24471;&#30456;&#23545;&#36739;&#39640;&#30340;&#24471;&#20998;&#65292;&#21363;&#26159;&#21542;&#38656;&#35201;&#32534;&#36753;&#32763;&#35793;&#65292;&#20294;&#32943;&#23450;&#26377;&#25913;&#36827;&#20934;&#30830;&#24615;&#30340;&#31354;&#38388;&#12290;&#33521;&#24847;&#21452;&#35821;&#25688;&#35201;&#21487;&#22312;&#35770;&#25991;&#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Translation Quality Estimation (TQE) is an important step before deploying the output translation into usage. TQE is also critical in assessing machine translation (MT) and human translation (HT) quality without seeing the reference translations. In this work, we examine if the state-of-the-art large language models (LLMs) can be fine-tuned for the TQE task and their capability. We take ChatGPT as one example and approach TQE as a binary classification task. Using English-Italian and English-German training corpus, our experimental results show that fine-tuned ChatGPT via its API can achieve a relatively high score on predicting translation quality, i.e. if the translation needs to be edited, but there is definitely space to improve the accuracy. English-Italiano bilingual Abstract is available in the paper.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#25945;&#32946;&#39046;&#22495;&#20013;&#65292;&#30001;&#20154;&#31867;&#21644;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#21327;&#20316;&#32534;&#20889;&#30340;&#28151;&#21512;&#25991;&#26412;&#30340;AI&#20869;&#23481;&#26816;&#27979;&#26041;&#27861;&#65292;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#35782;&#21035;&#36716;&#25442;&#28857;&#30340;&#20219;&#21153;&#65292;&#20197;&#21306;&#20998;&#20154;&#31867;&#32534;&#20889;&#21644;AI&#29983;&#25104;&#30340;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2307.12267</link><description>&lt;p&gt;
&#38754;&#21521;&#25945;&#32946;&#20013;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#28151;&#21512;&#35770;&#25991;&#30340;&#33258;&#21160;&#36793;&#30028;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Towards Automatic Boundary Detection for Human-AI Collaborative Hybrid Essay in Education. (arXiv:2307.12267v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#25945;&#32946;&#39046;&#22495;&#20013;&#65292;&#30001;&#20154;&#31867;&#21644;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#21327;&#20316;&#32534;&#20889;&#30340;&#28151;&#21512;&#25991;&#26412;&#30340;AI&#20869;&#23481;&#26816;&#27979;&#26041;&#27861;&#65292;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#35782;&#21035;&#36716;&#25442;&#28857;&#30340;&#20219;&#21153;&#65292;&#20197;&#21306;&#20998;&#20154;&#31867;&#32534;&#20889;&#21644;AI&#29983;&#25104;&#30340;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#33021;&#22815;&#22312;&#25552;&#20379;&#20855;&#20307;&#25351;&#23548;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#27969;&#30021;&#22238;&#31572;&#12290;&#23613;&#31649;&#25215;&#35748;&#25216;&#26415;&#36827;&#27493;&#24102;&#26469;&#30340;&#20415;&#21033;&#65292;&#25945;&#32946;&#32773;&#20063;&#25285;&#24515;&#23398;&#29983;&#21487;&#33021;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#23436;&#25104;&#20889;&#20316;&#20219;&#21153;&#24182;&#23558;&#20854;&#20551;&#20882;&#20026;&#33258;&#24049;&#30340;&#21407;&#21019;&#20316;&#21697;&#12290;&#34429;&#28982;&#26377;&#24456;&#22810;AI&#20869;&#23481;&#26816;&#27979;&#30740;&#31350;&#26159;&#22522;&#20110;&#36825;&#20123;&#25285;&#24551;&#36827;&#34892;&#30340;&#65292;&#20294;&#22823;&#22810;&#25968;&#20043;&#21069;&#30340;&#30740;&#31350;&#23558;AI&#20869;&#23481;&#26816;&#27979;&#24314;&#27169;&#20026;&#19968;&#20010;&#20998;&#31867;&#38382;&#39064;&#65292;&#20551;&#35774;&#19968;&#20010;&#25991;&#26412;&#35201;&#20040;&#23436;&#20840;&#30001;&#20154;&#31867;&#32534;&#20889;&#65292;&#35201;&#20040;&#23436;&#20840;&#30001;AI&#29983;&#25104;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;AI&#20869;&#23481;&#26816;&#27979;&#22312;&#19968;&#20010;&#23569;&#26377;&#25506;&#32034;&#20294;&#21364;&#29616;&#23454;&#30340;&#24773;&#20917;&#19979;&#65292;&#21363;&#26816;&#27979;&#30340;&#25991;&#26412;&#30001;&#20154;&#31867;&#21644;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#65288;&#21363;&#28151;&#21512;&#25991;&#26412;&#65289;&#21327;&#20316;&#32534;&#20889;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#26816;&#27979;&#20219;&#21153;&#24418;&#24335;&#21270;&#20026;&#20174;&#32473;&#23450;&#30340;&#28151;&#21512;&#25991;&#26412;&#20013;&#35782;&#21035;&#20154;&#31867;&#32534;&#20889;&#20869;&#23481;&#21644;AI&#29983;&#25104;&#20869;&#23481;&#20043;&#38388;&#30340;&#36716;&#25442;&#28857;&#65288;&#36793;&#30028;&#26816;&#27979;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent large language models (LLMs), e.g., ChatGPT, have been able to generate human-like and fluent responses when provided with specific instructions. While admitting the convenience brought by technological advancement, educators also have concerns that students might leverage LLMs to complete their writing assignments and pass them off as their original work. Although many AI content detection studies have been conducted as a result of such concerns, most of these prior studies modeled AI content detection as a classification problem, assuming that a text is either entirely human-written or entirely AI-generated. In this study, we investigated AI content detection in a rarely explored yet realistic setting where the text to be detected is collaboratively written by human and generative LLMs (i.e., hybrid text). We first formalized the detection task as identifying the transition points between human-written content and AI-generated content from a given hybrid text (boundary det
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35757;&#32451;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#26469;&#39640;&#25928;&#22495;&#33258;&#36866;&#24212;&#21477;&#23376;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#24494;&#35843;&#25972;&#20010;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#30340;&#36164;&#28304;&#28040;&#32791;&#12290;&#36890;&#36807;&#35757;&#32451;&#29305;&#23450;&#39046;&#22495;&#30340;&#36866;&#37197;&#22120;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#20351;&#29992;&#21516;&#19968;&#27169;&#22411;&#33719;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.03104</link><description>&lt;p&gt;
&#20351;&#29992;&#36866;&#37197;&#22120;&#39640;&#25928;&#22495;&#33258;&#36866;&#24212;&#21477;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Efficient Domain Adaptation of Sentence Embeddings using Adapters. (arXiv:2307.03104v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35757;&#32451;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#26469;&#39640;&#25928;&#22495;&#33258;&#36866;&#24212;&#21477;&#23376;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#24494;&#35843;&#25972;&#20010;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#30340;&#36164;&#28304;&#28040;&#32791;&#12290;&#36890;&#36807;&#35757;&#32451;&#29305;&#23450;&#39046;&#22495;&#30340;&#36866;&#37197;&#22120;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#20351;&#29992;&#21516;&#19968;&#27169;&#22411;&#33719;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21477;&#23376;&#23884;&#20837;&#20351;&#25105;&#20204;&#33021;&#22815;&#25429;&#25417;&#30701;&#25991;&#26412;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;&#22823;&#22810;&#25968;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#26159;&#38024;&#23545;&#19968;&#33324;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#65288;STS&#65289;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#22240;&#27492;&#65292;&#35201;&#22312;&#29305;&#23450;&#39046;&#22495;&#20013;&#20351;&#29992;&#21477;&#23376;&#23884;&#20837;&#65292;&#24517;&#39035;&#23558;&#27169;&#22411;&#36866;&#24212;&#20110;&#35813;&#39046;&#22495;&#20197;&#33719;&#24471;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#36890;&#24120;&#65292;&#36825;&#26159;&#36890;&#36807;&#23545;&#24863;&#20852;&#36259;&#30340;&#22495;&#23545;&#25972;&#20010;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#26469;&#23454;&#29616;&#30340;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20135;&#29983;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20294;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#26356;&#26032;&#20102;&#25152;&#26377;&#27169;&#22411;&#30340;&#26435;&#37325;&#65292;&#20351;&#35813;&#26041;&#27861;&#22312;&#36164;&#28304;&#19978;&#35201;&#27714;&#36739;&#39640;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35757;&#32451;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#26159;&#21333;&#29420;&#20026;&#27599;&#20010;&#30446;&#26631;&#39046;&#22495;&#24494;&#35843;&#25972;&#20010;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#12290;&#36825;&#20123;&#29305;&#23450;&#39046;&#22495;&#30340;&#36866;&#37197;&#22120;&#19981;&#38656;&#35201;&#24494;&#35843;&#25152;&#26377;&#24213;&#23618;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#21482;&#35757;&#32451;&#23569;&#37327;&#30340;&#39069;&#22806;&#21442;&#25968;&#65292;&#21516;&#26102;&#20445;&#25345;&#24213;&#23618;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#30340;&#26435;&#37325;&#19981;&#21464;&#12290;&#35757;&#32451;&#29305;&#23450;&#39046;&#22495;&#30340;&#36866;&#37197;&#22120;&#21487;&#20197;&#22987;&#32456;&#20351;&#29992;&#21516;&#19968;&#27169;&#22411;&#24182;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#33719;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentence embeddings enable us to capture the semantic similarity of short texts. Most sentence embedding models are trained for general semantic textual similarity (STS) tasks. Therefore, to use sentence embeddings in a particular domain, the model must be adapted to it in order to achieve good results. Usually, this is done by fine-tuning the entire sentence embedding model for the domain of interest. While this approach yields state-of-the-art results, all of the model's weights are updated during fine-tuning, making this method resource-intensive. Therefore, instead of fine-tuning entire sentence embedding models for each target domain individually, we propose to train lightweight adapters. These domain-specific adapters do not require fine-tuning all underlying sentence embedding model parameters. Instead, we only train a small number of additional parameters while keeping the weights of the underlying sentence embedding model fixed. Training domain-specific adapters allows always 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#22312;PubMed&#25688;&#35201;&#20013;&#20351;&#29992;SNOMED CT&#29256;&#26412;&#65292;&#35299;&#20915;&#20102;&#20808;&#21069;&#25968;&#25454;&#38598;&#25152;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#29992;&#20110;&#33258;&#21160;&#21270;&#22320;&#21457;&#29616;&#21644;&#25918;&#32622;&#26032;&#27010;&#24565;&#21040;&#30693;&#35782;&#24211;&#20013;&#12290;</title><link>http://arxiv.org/abs/2306.14704</link><description>&lt;p&gt;
&#20174;&#25991;&#26412;&#20013;&#20016;&#23500;&#26412;&#20307;&#30693;&#35782;&#65306;&#19968;&#31181;&#29992;&#20110;&#27010;&#24565;&#21457;&#29616;&#21644;&#25918;&#32622;&#30340;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Ontology Enrichment from Texts: A Biomedical Dataset for Concept Discovery and Placement. (arXiv:2306.14704v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#22312;PubMed&#25688;&#35201;&#20013;&#20351;&#29992;SNOMED CT&#29256;&#26412;&#65292;&#35299;&#20915;&#20102;&#20808;&#21069;&#25968;&#25454;&#38598;&#25152;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#29992;&#20110;&#33258;&#21160;&#21270;&#22320;&#21457;&#29616;&#21644;&#25918;&#32622;&#26032;&#27010;&#24565;&#21040;&#30693;&#35782;&#24211;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#27010;&#24565;&#30340;&#25552;&#21450;&#32463;&#24120;&#20986;&#29616;&#22312;&#25991;&#26412;&#20013;&#65292;&#38656;&#35201;&#33258;&#21160;&#21270;&#30340;&#26041;&#27861;&#23558;&#20854;&#25910;&#38598;&#24182;&#25918;&#32622;&#21040;&#30693;&#35782;&#24211;&#65288;&#20363;&#22914;&#26412;&#20307;&#21644;&#20998;&#31867;&#31995;&#32479;&#65289;&#20013;&#12290;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#23384;&#22312;&#19977;&#20010;&#38382;&#39064;&#65306;&#65288;&#19968;&#65289;&#22823;&#37096;&#20998;&#20551;&#35774;&#26032;&#27010;&#24565;&#24050;&#32463;&#34987;&#21457;&#29616;&#65292;&#19981;&#33021;&#25903;&#25345;&#27010;&#24565;&#21457;&#29616;&#65307;&#65288;&#20108;&#65289;&#21482;&#20351;&#29992;&#27010;&#24565;&#26631;&#31614;&#20316;&#20026;&#36755;&#20837;&#65292;&#32570;&#20047;&#27010;&#24565;&#26631;&#31614;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65307;&#65288;&#19977;&#65289;&#20027;&#35201;&#20851;&#27880;&#19982;&#21407;&#23376;&#27010;&#24565;&#30340;&#20998;&#31867;&#65292;&#32780;&#19981;&#26159;&#22797;&#26434;&#27010;&#24565;&#65288;&#21253;&#21547;&#36923;&#36753;&#36816;&#31639;&#31526;&#65289;&#30340;&#25918;&#32622;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#20351;&#29992;2014&#24180;&#21644;2017&#24180;&#30340;SNOMED CT&#29256;&#26412;&#36866;&#37197;MedMentions&#25968;&#25454;&#38598;&#65288;PubMed&#25688;&#35201;&#65289;&#65292;&#28085;&#30422;&#30142;&#30149;&#23376;&#31867;&#21035;&#21644;&#26356;&#24191;&#27867;&#30340;&#20020;&#24202;&#21457;&#29616;&#12289;&#36807;&#31243;&#20197;&#21450;&#21046;&#33647;/&#29983;&#29289;&#20135;&#21697;&#31867;&#21035;&#12290;&#25105;&#20204;&#29992;&#35813;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#27010;&#24565;&#21457;&#29616;&#21644;&#25918;&#32622;&#30340;&#24037;&#20316;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#20351;&#29992;&#26041;&#24335;&#30340;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mentions of new concepts appear regularly in texts and require automated approaches to harvest and place them into Knowledge Bases (KB), e.g., ontologies and taxonomies. Existing datasets suffer from three issues, (i) mostly assuming that a new concept is pre-discovered and cannot support out-of-KB mention discovery; (ii) only using the concept label as the input along with the KB and thus lacking the contexts of a concept label; and (iii) mostly focusing on concept placement w.r.t a taxonomy of atomic concepts, instead of complex concepts, i.e., with logical operators. To address these issues, we propose a new benchmark, adapting MedMentions dataset (PubMed abstracts) with SNOMED CT versions in 2014 and 2017 under the Diseases sub-category and the broader categories of Clinical finding, Procedure, and Pharmaceutical / biologic product. We provide usage on the evaluation with the dataset for out-of-KB mention discovery and concept placement, adapting recent Large Language Model based m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#22312;&#20855;&#26377;&#21333;&#23618;&#32447;&#24615;&#33258;&#27880;&#24847;&#23618;&#30340;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#19978;&#36890;&#36807;&#26799;&#24230;&#27969;&#36827;&#34892;&#35757;&#32451;&#30340;ICL&#26426;&#21046;&#65292;&#25581;&#31034;&#20102;&#26799;&#24230;&#27969;&#20855;&#26377;&#25214;&#21040;&#30446;&#26631;&#20989;&#25968;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.09927</link><description>&lt;p&gt;
&#35757;&#32451;&#22909;&#30340;Transformer&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#32447;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Trained Transformers Learn Linear Models In-Context. (arXiv:2306.09927v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09927
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#22312;&#20855;&#26377;&#21333;&#23618;&#32447;&#24615;&#33258;&#27880;&#24847;&#23618;&#30340;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#19978;&#36890;&#36807;&#26799;&#24230;&#27969;&#36827;&#34892;&#35757;&#32451;&#30340;ICL&#26426;&#21046;&#65292;&#25581;&#31034;&#20102;&#26799;&#24230;&#27969;&#20855;&#26377;&#25214;&#21040;&#30446;&#26631;&#20989;&#25968;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20363;&#22914;Transformers&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#65306;&#32473;&#23450;&#19968;&#20010;&#26469;&#33258;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#30340;&#30701;&#35821;&#24207;&#21015;&#30340;&#25552;&#31034;&#65292;&#23427;&#20204;&#21487;&#20197;&#21046;&#23450;&#30456;&#20851;&#30340;&#27599;&#20010;&#20196;&#29260;&#21644;&#19979;&#19968;&#20010;&#20196;&#29260;&#30340;&#39044;&#27979;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#21442;&#25968;&#26356;&#26032;&#12290;&#36890;&#36807;&#23558;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#26410;&#26631;&#35760;&#30340;&#27979;&#35797;&#25968;&#25454;&#24207;&#21015;&#23884;&#20837;&#21040;&#25552;&#31034;&#20013;&#65292;&#36825;&#20351;&#24471;Transformer&#34920;&#29616;&#24471;&#20687;&#26377;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#12290;&#20107;&#23454;&#19978;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#22312;&#38543;&#26426;&#23454;&#20363;&#19978;&#35757;&#32451;Transformer&#20307;&#31995;&#32467;&#26500;&#30340;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#39044;&#27979;&#20250;&#27169;&#20223;&#26222;&#36890;&#26368;&#23567;&#20108;&#20056;&#27861;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention-based neural networks such as transformers have demonstrated a remarkable ability to exhibit in-context learning (ICL): Given a short prompt sequence of tokens from an unseen task, they can formulate relevant per-token and next-token predictions without any parameter updates. By embedding a sequence of labeled training data and unlabeled test data as a prompt, this allows for transformers to behave like supervised learning algorithms. Indeed, recent work has shown that when training transformer architectures over random instances of linear regression problems, these models' predictions mimic those of ordinary least squares.  Towards understanding the mechanisms underlying this phenomenon, we investigate the dynamics of ICL in transformers with a single linear self-attention layer trained by gradient flow on linear regression tasks. We show that despite non-convexity, gradient flow with a suitable random initialization finds a global minimum of the objective function. At this 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35206;&#30422;143&#31181;&#35821;&#35328;&#12289;&#29992;&#20110;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#22522;&#20934;&#30340;&#22810;&#35821;&#31181;&#35821;&#38899;&#22522;&#20934; ML-SUPERB&#65292;&#24182;&#21457;&#29616;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#19988;&#22810;&#35821;&#31181;&#27169;&#22411;&#19981;&#24635;&#26159;&#27604;&#21333;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.10615</link><description>&lt;p&gt;
ML-SUPERB: &#22810;&#35821;&#31181;&#35821;&#38899;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#24615;&#33021;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ML-SUPERB: Multilingual Speech Universal PERformance Benchmark. (arXiv:2305.10615v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10615
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35206;&#30422;143&#31181;&#35821;&#35328;&#12289;&#29992;&#20110;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#22522;&#20934;&#30340;&#22810;&#35821;&#31181;&#35821;&#38899;&#22522;&#20934; ML-SUPERB&#65292;&#24182;&#21457;&#29616;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#19988;&#22810;&#35821;&#31181;&#27169;&#22411;&#19981;&#24635;&#26159;&#27604;&#21333;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#22788;&#29702;Universal PERformance Benchmark (SUPERB)&#26159;&#19968;&#20010;&#29992;&#20110;&#21508;&#31181;&#35821;&#38899;&#22788;&#29702;&#20219;&#21153;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#22522;&#20934;&#30340;&#25490;&#34892;&#27036;&#12290;&#28982;&#32780;&#65292;SUPERB&#22312;&#35780;&#20272;&#20013;&#20027;&#35201;&#32771;&#34385;&#33521;&#35821;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22810;&#35821;&#31181;SUPERB (ML-SUPERB)&#65292;&#35206;&#30422;&#20102;143&#31181;&#35821;&#35328;&#65288;&#20174;&#39640;&#36164;&#28304;&#21040;&#28626;&#21361;&#35821;&#35328;&#65289;&#65292;&#32771;&#34385;&#20102;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#21644;&#35821;&#35328;&#35782;&#21035;&#12290;&#19982;SUPERB&#27010;&#24565;&#31867;&#20284;&#65292;ML-SUPERB&#21033;&#29992;&#20923;&#32467;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#27973;&#23618;&#19979;&#28216;&#27169;&#22411;&#30340;&#31616;&#21333;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#35821;&#31181;&#20219;&#21153;&#12290;&#19982;SUPERB&#22522;&#20934;&#31867;&#20284;&#65292;&#25105;&#20204;&#21457;&#29616;&#35821;&#38899;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#19982;FBANK&#29305;&#24449;&#30456;&#27604;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22810;&#35821;&#31181;&#27169;&#22411;&#24182;&#19981;&#24635;&#26159;&#27604;&#21333;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;&#25105;&#20204;&#23558;&#21457;&#24067;ML-SUPERB&#20316;&#20026;&#19968;&#20010;&#25361;&#25112;&#65292;&#25552;&#20379;&#32452;&#32455;&#22909;&#30340;&#25968;&#25454;&#38598;&#21644;&#21487;&#37325;&#29616;&#30340;&#35757;&#32451;&#33050;&#26412;&#65292;&#29992;&#20110;&#26410;&#26469;&#30340;&#22810;&#35821;&#31181;&#34920;&#31034;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech processing Universal PERformance Benchmark (SUPERB) is a leaderboard to benchmark the performance of Self-Supervised Learning (SSL) models on various speech processing tasks. However, SUPERB largely considers English speech in its evaluation. This paper presents multilingual SUPERB (ML-SUPERB), covering 143 languages (ranging from high-resource to endangered), and considering both automatic speech recognition and language identification. Following the concept of SUPERB, ML-SUPERB utilizes frozen SSL features and employs a simple framework for multilingual tasks by learning a shallow downstream model. Similar to the SUPERB benchmark, we find speech SSL models can significantly improve performance compared to FBANK features. Furthermore, we find that multilingual models do not always perform better than their monolingual counterparts. We will release ML-SUPERB as a challenge with organized datasets and reproducible training scripts for future multilingual representation research.
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#20016;&#23500;&#30340;&#20803;&#25968;&#25454;&#27880;&#37322;&#30340;&#20449;&#24687;&#36827;&#34892;&#23631;&#24149;&#35282;&#33394;&#30340;&#20010;&#24615;&#21270;&#35821;&#35328;&#24314;&#27169;&#65292;&#27979;&#35797;&#34920;&#26126;&#36825;&#26679;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#26500;&#24314;&#65292;&#21363;&#20351;&#23545;&#20110;&#38646;&#26679;&#26412;&#30340;&#28436;&#35828;&#23478;&#20063;&#21487;&#20197;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.16618</link><description>&lt;p&gt;
&#20351;&#29992;&#20016;&#23500;&#30340;&#20803;&#25968;&#25454;&#27880;&#37322;&#30340;&#23631;&#24149;&#35282;&#33394;&#30340;&#20010;&#24615;&#21270;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Personalised Language Modelling of Screen Characters Using Rich Metadata Annotations. (arXiv:2303.16618v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#20016;&#23500;&#30340;&#20803;&#25968;&#25454;&#27880;&#37322;&#30340;&#20449;&#24687;&#36827;&#34892;&#23631;&#24149;&#35282;&#33394;&#30340;&#20010;&#24615;&#21270;&#35821;&#35328;&#24314;&#27169;&#65292;&#27979;&#35797;&#34920;&#26126;&#36825;&#26679;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#26500;&#24314;&#65292;&#21363;&#20351;&#23545;&#20110;&#38646;&#26679;&#26412;&#30340;&#28436;&#35828;&#23478;&#20063;&#21487;&#20197;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#20010;&#24615;&#21270;&#20026;&#23545;&#35805;&#25935;&#24863;&#65292;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#29305;&#23450;&#29305;&#24449;&#30340;&#20154;&#21592;&#21644;/&#25110;&#29305;&#23450;&#29615;&#22659;&#20013;&#30340;&#35828;&#35805;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#20016;&#23500;&#30340;&#35282;&#33394;&#27880;&#37322;&#38590;&#20197;&#24471;&#21040;&#21644;&#25104;&#21151;&#21033;&#29992;&#12290;&#22312;&#27492;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21457;&#24067;&#24182;&#25551;&#36848;&#20102;&#19968;&#32452;&#26032;&#39062;&#30340;&#25163;&#21160;&#27880;&#37322;&#65292;&#28085;&#30422;&#20102;&#26469;&#33258;&#27969;&#34892;&#30340; Cornell &#30005;&#24433;&#23545;&#35805;&#35821;&#26009;&#24211;&#30340; 863 &#21517;&#28436;&#35762;&#32773;&#65292;&#21253;&#25324;&#29305;&#24449;&#24341;&#29992;&#21644;&#35282;&#33394;&#25551;&#36848;&#65292;&#20197;&#21450;&#36229;&#36807; 95&#65285; &#30340;&#29305;&#33394;&#30005;&#24433;&#30340;&#19968;&#32452;&#33258;&#21160;&#25552;&#21462;&#30340;&#20845;&#20010;&#20803;&#25968;&#25454;&#12290;&#25105;&#20204;&#23545;&#20004;&#20010;&#35821;&#26009;&#24211;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#24182;&#34920;&#26126;&#21487;&#20197;&#26377;&#25928;&#22320;&#20351;&#29992;&#27492;&#31867;&#27880;&#37322;&#26469;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#65292;&#23558;&#22256;&#24785;&#20943;&#23569;&#39640;&#36798; 8.5&#65285;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29978;&#33267;&#21487;&#20197;&#24212;&#29992;&#20110;&#38646;&#26679;&#26412;&#30340;&#28436;&#35762;&#32773;&#65292;&#21363;&#23545;&#20110;&#27809;&#26377;&#20808;&#21069;&#22521;&#35757;&#25968;&#25454;&#30340;&#28436;&#35762;&#32773;&#65292;&#20381;&#36182;&#20110;&#35282;&#33394;&#30340;&#20154;&#21475;&#29305;&#24449;&#30340;&#32452;&#21512;&#12290;&#30001;&#20110;&#25910;&#38598;&#27492;&#31867;&#20803;&#25968;&#25454;&#25104;&#26412;&#39640;&#26114;&#65292;&#22240;&#27492;&#25105;&#20204;&#36824;&#36129;&#29486;&#20102;&#19968;&#39033;&#25104;&#26412;&#25928;&#30410;&#20998;&#26512;&#65292;&#20197;&#31361;&#20986;&#26174;&#31034;
&lt;/p&gt;
&lt;p&gt;
Personalisation of language models for dialogue sensitises them to better capture the speaking patterns of people of specific characteristics, and/or in specific environments. However, rich character annotations are difficult to come by and to successfully leverage. In this work, we release and describe a novel set of manual annotations for 863 speakers from the popular Cornell Movie Dialog Corpus, including features like characteristic quotes and character descriptions, and a set of six automatically extracted metadata for over 95% of the featured films. We perform extensive experiments on two corpora and show that such annotations can be effectively used to personalise language models, reducing perplexity by up to 8.5%. Our method can be applied even zero-shot for speakers for whom no prior training data is available, by relying on combinations of characters' demographic characteristics. Since collecting such metadata is costly, we also contribute a cost-benefit analysis to highlight
&lt;/p&gt;</description></item><item><title>BODEGA&#26159;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#27169;&#25311;&#30495;&#23454;&#30340;&#20869;&#23481;&#31649;&#29702;&#22330;&#26223;&#65292;&#22312;&#22235;&#20010;&#35823;&#20256;&#26816;&#27979;&#20219;&#21153;&#19978;&#27979;&#35797;&#21463;&#23475;&#27169;&#22411;&#21644;&#25915;&#20987;&#26041;&#27861;&#12290;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#36827;&#34892;&#24494;&#23567;&#30340;&#25991;&#26412;&#20462;&#25913;&#65292;&#20063;&#21487;&#20197;&#27450;&#39575;&#26368;&#20934;&#30830;&#30340;&#20998;&#31867;&#22120;&#12290;</title><link>http://arxiv.org/abs/2303.08032</link><description>&lt;p&gt;
BODEGA: &#38024;&#23545;&#21487;&#20449;&#24230;&#35780;&#20272;&#20013;&#23545;&#25239;&#24615;&#26679;&#26412;&#29983;&#25104;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
BODEGA: Benchmark for Adversarial Example Generation in Credibility Assessment. (arXiv:2303.08032v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08032
&lt;/p&gt;
&lt;p&gt;
BODEGA&#26159;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#27169;&#25311;&#30495;&#23454;&#30340;&#20869;&#23481;&#31649;&#29702;&#22330;&#26223;&#65292;&#22312;&#22235;&#20010;&#35823;&#20256;&#26816;&#27979;&#20219;&#21153;&#19978;&#27979;&#35797;&#21463;&#23475;&#27169;&#22411;&#21644;&#25915;&#20987;&#26041;&#27861;&#12290;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#36827;&#34892;&#24494;&#23567;&#30340;&#25991;&#26412;&#20462;&#25913;&#65292;&#20063;&#21487;&#20197;&#27450;&#39575;&#26368;&#20934;&#30830;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#26816;&#27979;&#19981;&#21487;&#20449;&#20869;&#23481;&#65292;&#22914;&#20551;&#26032;&#38395;&#12289;&#31038;&#20132;&#23186;&#20307;&#26426;&#22120;&#20154;&#12289;&#23459;&#20256;&#31561;&#12290;&#36739;&#20026;&#20934;&#30830;&#30340;&#27169;&#22411;&#65288;&#21487;&#33021;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65289;&#26377;&#21161;&#20110;&#31649;&#29702;&#20844;&#20849;&#30005;&#23376;&#24179;&#21488;&#65292;&#24182;&#32463;&#24120;&#23548;&#33268;&#20869;&#23481;&#21019;&#24314;&#32773;&#38754;&#20020;&#25552;&#20132;&#25298;&#32477;&#25110;&#24050;&#21457;&#24067;&#25991;&#26412;&#30340;&#25764;&#19979;&#12290;&#20026;&#20102;&#36991;&#20813;&#36827;&#19968;&#27493;&#34987;&#26816;&#27979;&#65292;&#20869;&#23481;&#21019;&#24314;&#32773;&#23581;&#35797;&#20135;&#29983;&#19968;&#20010;&#31245;&#24494;&#20462;&#25913;&#36807;&#30340;&#25991;&#26412;&#29256;&#26412;&#65288;&#21363;&#25915;&#20987;&#23545;&#25239;&#24615;&#26679;&#26412;&#65289;&#65292;&#21033;&#29992;&#20998;&#31867;&#22120;&#30340;&#24369;&#28857;&#23548;&#33268;&#19981;&#21516;&#30340;&#36755;&#20986;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;BODEGA&#65306;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#22312;&#27169;&#25311;&#20869;&#23481;&#31649;&#29702;&#30340;&#30495;&#23454;&#29992;&#20363;&#20013;&#27979;&#35797;&#21463;&#23475;&#27169;&#22411;&#21644;&#25915;&#20987;&#26041;&#27861;&#22312;&#22235;&#20010;&#35823;&#20256;&#26816;&#27979;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#36824;&#31995;&#32479;&#22320;&#27979;&#35797;&#20102;&#21463;&#27426;&#36814;&#30340;&#25991;&#26412;&#20998;&#31867;&#22120;&#23545;&#21487;&#29992;&#25915;&#20987;&#25216;&#26415;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#21457;&#29616;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#22312;&#25991;&#26412;&#20013;&#36827;&#34892;&#24494;&#23567;&#30340;&#20462;&#25913;&#20063;&#21487;&#20197;&#27450;&#39575;&#26368;&#20934;&#30830;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text classification methods have been widely investigated as a way to detect content of low credibility: fake news, social media bots, propaganda, etc. Quite accurate models (likely based on deep neural networks) help in moderating public electronic platforms and often cause content creators to face rejection of their submissions or removal of already published texts. Having the incentive to evade further detection, content creators try to come up with a slightly modified version of the text (known as an attack with an adversarial example) that exploit the weaknesses of classifiers and result in a different output. Here we introduce BODEGA: a benchmark for testing both victim models and attack methods on four misinformation detection tasks in an evaluation framework designed to simulate real use-cases of content moderation. We also systematically test the robustness of popular text classifiers against available attacking techniques and discover that, indeed, in some cases barely signif
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;COOLANT&#65292;&#19968;&#20010;&#29992;&#20110;&#36328;&#27169;&#24577;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#21319;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#23545;&#40784;&#31934;&#24230;&#65292;&#24182;&#36890;&#36807;&#36328;&#27169;&#24577;&#34701;&#21512;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#23454;&#29616;&#26356;&#20934;&#30830;&#21644;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#32858;&#21512;&#12290;</title><link>http://arxiv.org/abs/2302.14057</link><description>&lt;p&gt;
&#36328;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#22810;&#27169;&#24577;&#20551;&#26032;&#38395;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Cross-modal Contrastive Learning for Multimodal Fake News Detection. (arXiv:2302.14057v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14057
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;COOLANT&#65292;&#19968;&#20010;&#29992;&#20110;&#36328;&#27169;&#24577;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#21319;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#23545;&#40784;&#31934;&#24230;&#65292;&#24182;&#36890;&#36807;&#36328;&#27169;&#24577;&#34701;&#21512;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#23454;&#29616;&#26356;&#20934;&#30830;&#21644;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#32858;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#26816;&#27979;&#22810;&#27169;&#24577;&#20551;&#26032;&#38395;&#36817;&#26469;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#35768;&#22810;&#29616;&#26377;&#26041;&#27861;&#33268;&#21147;&#20110;&#34701;&#21512;&#21333;&#27169;&#29305;&#24449;&#20197;&#20135;&#29983;&#22810;&#27169;&#24577;&#26032;&#38395;&#34920;&#36798;&#12290;&#28982;&#32780;&#65292;&#24378;&#22823;&#30340;&#36328;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#22312;&#20551;&#26032;&#38395;&#26816;&#27979;&#26041;&#38754;&#30340;&#28508;&#21147;&#23578;&#26410;&#20805;&#20998;&#21033;&#29992;&#12290;&#27492;&#22806;&#65292;&#22914;&#20309;&#32858;&#21512;&#19981;&#21516;&#27169;&#24577;&#30340;&#29305;&#24449;&#20197;&#25552;&#21319;&#20915;&#31574;&#36807;&#31243;&#30340;&#24615;&#33021;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;COOLANT&#65292;&#19968;&#20010;&#29992;&#20110;&#22810;&#27169;&#24577;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#36328;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#40784;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#23545;&#40784;&#31934;&#24230;&#65292;&#25105;&#20204;&#21033;&#29992;&#36741;&#21161;&#20219;&#21153;&#22312;&#23545;&#27604;&#36807;&#31243;&#20013;&#36719;&#21270;&#36127;&#26679;&#26412;&#30340;&#25439;&#22833;&#39033;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#36328;&#27169;&#24577;&#34701;&#21512;&#27169;&#22359;&#26469;&#23398;&#20064;&#36328;&#27169;&#24577;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;&#24102;&#26377;&#27880;&#24847;&#21147;&#24341;&#23548;&#27169;&#22359;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20197;&#24110;&#21161;&#26377;&#25928;&#19988;&#21487;&#35299;&#37322;&#22320;&#32858;&#21512;&#23545;&#40784;&#30340;&#21333;&#27169;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic detection of multimodal fake news has gained a widespread attention recently. Many existing approaches seek to fuse unimodal features to produce multimodal news representations. However, the potential of powerful cross-modal contrastive learning methods for fake news detection has not been well exploited. Besides, how to aggregate features from different modalities to boost the performance of the decision-making process is still an open question. To address that, we propose COOLANT, a cross-modal contrastive learning framework for multimodal fake news detection, aiming to achieve more accurate image-text alignment. To further improve the alignment precision, we leverage an auxiliary task to soften the loss term of negative samples during the contrast process. A cross-modal fusion module is developed to learn the cross-modality correlations. An attention mechanism with an attention guidance module is implemented to help effectively and interpretably aggregate the aligned unimo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;BERT&#30340;&#23454;&#20307;&#38142;&#25509;&#26041;&#27861;BLINKout&#65292;&#36890;&#36807;&#19982;&#29305;&#27530;NIL&#23454;&#20307;&#21305;&#37197;&#26469;&#35782;&#21035;&#27809;&#26377;&#30456;&#24212;KB&#23454;&#20307;&#30340;&#23454;&#20307;&#25552;&#21450;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2302.07189</link><description>&lt;p&gt;
&#25581;&#31034;&#26410;&#30693;&#65306;&#22522;&#20110;&#23454;&#20307;&#38142;&#25509;&#30340;&#30693;&#35782;&#24211;&#22806;&#25552;&#21450;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Reveal the Unknown: Out-of-Knowledge-Base Mention Discovery with Entity Linking. (arXiv:2302.07189v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;BERT&#30340;&#23454;&#20307;&#38142;&#25509;&#26041;&#27861;BLINKout&#65292;&#36890;&#36807;&#19982;&#29305;&#27530;NIL&#23454;&#20307;&#21305;&#37197;&#26469;&#35782;&#21035;&#27809;&#26377;&#30456;&#24212;KB&#23454;&#20307;&#30340;&#23454;&#20307;&#25552;&#21450;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#25991;&#26412;&#20013;&#21457;&#29616;&#30693;&#35782;&#24211;&#65288;KB&#65289;&#22806;&#30340;&#23454;&#20307;&#25552;&#21450;&#65292;&#22312;KB&#32500;&#25252;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#20294;&#24182;&#26410;&#34987;&#23436;&#20840;&#24320;&#21457;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#23616;&#38480;&#20110;&#31616;&#21333;&#30340;&#22522;&#20110;&#38408;&#20540;&#30340;&#26041;&#27861;&#21644;&#22522;&#20110;&#29305;&#24449;&#30340;&#20998;&#31867;&#65292;&#24182;&#19988;&#29992;&#20110;&#35780;&#20272;&#30340;&#25968;&#25454;&#38598;&#30456;&#23545;&#36739;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;BLINKout&#65292;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;BERT&#30340;&#23454;&#20307;&#38142;&#25509;&#65288;EL&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#23558;&#25552;&#21450;&#19982;&#29305;&#27530;&#30340;NIL&#23454;&#20307;&#36827;&#34892;&#21305;&#37197;&#26469;&#35782;&#21035;&#27809;&#26377;&#30456;&#24212;KB&#23454;&#20307;&#30340;&#25552;&#21450;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#21033;&#29992;BERT&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21253;&#25324;NIL&#23454;&#20307;&#34920;&#31034;&#21644;&#20998;&#31867;&#22312;&#20869;&#30340;&#26032;&#25216;&#26415;&#65292;&#24182;&#22686;&#24378;&#20102;&#20854;&#21516;&#20041;&#35789;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;KB&#20462;&#21098;&#21644;&#29256;&#26412;&#25511;&#21046;&#31574;&#30053;&#65292;&#20197;&#33258;&#21160;&#20174;&#24120;&#35265;&#30340;KB EL&#25968;&#25454;&#38598;&#26500;&#24314;&#20986;KB&#22806;&#30340;&#25968;&#25454;&#38598;&#12290;&#22312;&#21307;&#23398;&#26412;&#20307;&#35770;&#12289;UMLS&#12289;SNOMED CT&#31561;&#20116;&#20010;&#19981;&#21516;&#39046;&#22495;&#20013;&#65292;&#23545;&#20020;&#24202;&#31508;&#35760;&#12289;&#29983;&#29289;&#21307;&#23398;&#20986;&#29256;&#29289;&#21644;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;BLINKout&#22312;&#35782;&#21035;&#30693;&#35782;&#24211;&#22806;&#25552;&#21450;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering entity mentions that are out of a Knowledge Base (KB) from texts plays a critical role in KB maintenance, but has not yet been fully explored. The current methods are mostly limited to the simple threshold-based approach and feature-based classification, and the datasets for evaluation are relatively rare. We propose BLINKout, a new BERT-based Entity Linking (EL) method which can identify mentions that do not have corresponding KB entities by matching them to a special NIL entity. To better utilize BERT, we propose new techniques including NIL entity representation and classification, with synonym enhancement. We also propose KB Pruning and Versioning strategies to automatically construct out-of-KB datasets from common in-KB EL datasets. Results on five datasets of clinical notes, biomedical publications, and Wikipedia articles in various domains show the advantages of BLINKout over existing methods to identify out-of-KB mentions for the medical ontologies, UMLS, SNOMED CT,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;CodeBert&#22312;&#28304;&#30721;&#34920;&#31034;&#23398;&#20064;&#20013;&#23398;&#21040;&#20102;&#21738;&#20123;&#29305;&#24449;&#65292;&#21457;&#29616;&#24403;&#21069;&#26041;&#27861;&#26080;&#27861;&#26377;&#25928;&#29702;&#35299;&#28304;&#20195;&#30721;&#30340;&#36923;&#36753;&#65292;&#32780;&#28304;&#20195;&#30721;&#30340;&#34920;&#31034;&#20381;&#36182;&#20110;&#31243;&#24207;&#21592;&#23450;&#20041;&#30340;&#21464;&#37327;&#21644;&#20989;&#25968;&#21517;&#12290;</title><link>http://arxiv.org/abs/2301.08427</link><description>&lt;p&gt;
CodeBert&#33021;&#23398;&#21040;&#21738;&#20123;&#29305;&#24449;&#65306;BERT&#22522;&#20110;&#28304;&#30721;&#34920;&#31034;&#23398;&#20064;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Which Features are Learned by CodeBert: An Empirical Study of the BERT-based Source Code Representation Learning. (arXiv:2301.08427v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08427
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;CodeBert&#22312;&#28304;&#30721;&#34920;&#31034;&#23398;&#20064;&#20013;&#23398;&#21040;&#20102;&#21738;&#20123;&#29305;&#24449;&#65292;&#21457;&#29616;&#24403;&#21069;&#26041;&#27861;&#26080;&#27861;&#26377;&#25928;&#29702;&#35299;&#28304;&#20195;&#30721;&#30340;&#36923;&#36753;&#65292;&#32780;&#28304;&#20195;&#30721;&#30340;&#34920;&#31034;&#20381;&#36182;&#20110;&#31243;&#24207;&#21592;&#23450;&#20041;&#30340;&#21464;&#37327;&#21644;&#20989;&#25968;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#36716;&#25442;(BERT)&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#39046;&#22495;&#25552;&#20986;&#65292;&#24182;&#26174;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#26368;&#36817;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;BERT&#24212;&#29992;&#20110;&#28304;&#30721;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#22312;&#20960;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#19968;&#20123;&#22909;&#28040;&#24687;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35828;&#26126;&#20102;&#24403;&#21069;&#26041;&#27861;&#26080;&#27861;&#26377;&#25928;&#29702;&#35299;&#28304;&#20195;&#30721;&#30340;&#36923;&#36753;&#12290;&#28304;&#20195;&#30721;&#30340;&#34920;&#31034;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#31243;&#24207;&#21592;&#23450;&#20041;&#30340;&#21464;&#37327;&#21644;&#20989;&#25968;&#21517;&#12290;&#25105;&#20204;&#35774;&#35745;&#24182;&#23454;&#26045;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#29468;&#24819;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#24037;&#20316;&#25552;&#20379;&#19968;&#20123;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Bidirectional Encoder Representations from Transformers (BERT) were proposed in the natural language process (NLP) and shows promising results. Recently researchers applied the BERT to source-code representation learning and reported some good news on several downstream tasks. However, in this paper, we illustrated that current methods cannot effectively understand the logic of source codes. The representation of source code heavily relies on the programmer-defined variable and function names. We design and implement a set of experiments to demonstrate our conjecture and provide some insights for future works.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26426;&#22120;&#20154;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#12289;&#20219;&#21153;&#26080;&#20851;&#30340;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#30693;&#35782;&#65292;&#24182;&#32467;&#21512;&#39640;&#23481;&#37327;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#22312;&#23454;&#38469;&#25511;&#21046;&#39046;&#22495;&#30340;&#39640;&#24615;&#33021;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.06817</link><description>&lt;p&gt;
RT-1: &#29992;&#20110;&#23454;&#38469;&#25511;&#21046;&#30340;&#26426;&#22120;&#20154;&#21464;&#21387;&#22120;.
&lt;/p&gt;
&lt;p&gt;
RT-1: Robotics Transformer for Real-World Control at Scale. (arXiv:2212.06817v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26426;&#22120;&#20154;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#12289;&#20219;&#21153;&#26080;&#20851;&#30340;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#30693;&#35782;&#65292;&#24182;&#32467;&#21512;&#39640;&#23481;&#37327;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#22312;&#23454;&#38469;&#25511;&#21046;&#39046;&#22495;&#30340;&#39640;&#24615;&#33021;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20174;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#12289;&#20219;&#21153;&#26080;&#20851;&#30340;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#30693;&#35782;&#65292;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#22312;&#38646;&#26679;&#26412;&#23398;&#20064;&#25110;&#20351;&#29992;&#23569;&#37327;&#29305;&#23450;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#26469;&#39640;&#27700;&#24179;&#22320;&#35299;&#20915;&#20855;&#20307;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#34429;&#28982;&#36825;&#31181;&#33021;&#21147;&#24050;&#32463;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25110;&#35821;&#38899;&#35782;&#21035;&#31561;&#20854;&#20182;&#39046;&#22495;&#24471;&#21040;&#35777;&#26126;&#65292;&#20294;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#23578;&#26410;&#23637;&#31034;&#20986;&#26469;&#12290;&#36825;&#26159;&#22240;&#20026;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#23588;&#20026;&#20851;&#38190;&#65292;&#30001;&#20110;&#25910;&#38598;&#29616;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#20154;&#25968;&#25454;&#30340;&#38590;&#24230;&#36739;&#22823;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#31181;&#36890;&#29992;&#26426;&#22120;&#20154;&#27169;&#22411;&#25104;&#21151;&#30340;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#22312;&#20110;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#24320;&#25918;&#24335;&#35757;&#32451;&#65292;&#32467;&#21512;&#21487;&#20197;&#21560;&#25910;&#25152;&#26377;&#22810;&#26679;&#21270;&#26426;&#22120;&#20154;&#25968;&#25454;&#30340;&#39640;&#23481;&#37327;&#26550;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#26426;&#22120;&#20154;&#21464;&#21387;&#22120;&#30340;&#27169;&#22411;&#31867;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#27169;&#22411;&#29305;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#19981;&#21516;&#27169;&#22411;&#31867;&#21035;&#21450;&#20854;&#38543;&#25968;&#25454;&#22823;&#23567;&#32780;&#25512;&#24191;&#30340;&#33021;&#21147;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
By transferring knowledge from large, diverse, task-agnostic datasets, modern machine learning models can solve specific downstream tasks either zero-shot or with small task-specific datasets to a high level of performance. While this capability has been demonstrated in other fields such as computer vision, natural language processing or speech recognition, it remains to be shown in robotics, where the generalization capabilities of the models are particularly critical due to the difficulty of collecting real-world robotic data. We argue that one of the keys to the success of such general robotic models lies with open-ended task-agnostic training, combined with high-capacity architectures that can absorb all of the diverse, robotic data. In this paper, we present a model class, dubbed Robotics Transformer, that exhibits promising scalable model properties. We verify our conclusions in a study of different model classes and their ability to generalize as a function of the data size, mod
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#30701;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;Transformers&#22312;&#30701;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#29575;&#65292;&#24341;&#21457;&#20102;&#26159;&#21542;&#38656;&#35201;&#19987;&#38376;&#30340;&#30701;&#25991;&#26412;&#25216;&#26415;&#30340;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2211.16878</link><description>&lt;p&gt;
Transformers&#26159;&#30701;&#25991;&#26412;&#20998;&#31867;&#22120;&#65306;&#22522;&#20110;&#22522;&#20934;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#30340;&#24402;&#32435;&#30701;&#25991;&#26412;&#20998;&#31867;&#22120;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Transformers are Short Text Classifiers: A Study of Inductive Short Text Classifiers on Benchmarks and Real-world Datasets. (arXiv:2211.16878v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16878
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#30701;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;Transformers&#22312;&#30701;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#29575;&#65292;&#24341;&#21457;&#20102;&#26159;&#21542;&#38656;&#35201;&#19987;&#38376;&#30340;&#30701;&#25991;&#26412;&#25216;&#26415;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30701;&#25991;&#26412;&#20998;&#31867;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20851;&#38190;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#23384;&#22312;&#35768;&#22810;&#39640;&#24230;&#19987;&#38376;&#21270;&#30340;&#30701;&#25991;&#26412;&#20998;&#31867;&#22120;&#12290;&#28982;&#32780;&#65292;&#22312;&#26368;&#36817;&#30340;&#30701;&#25991;&#26412;&#30740;&#31350;&#20013;&#65292;&#20256;&#32479;&#25991;&#26412;&#20998;&#31867;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#32431;&#31929;&#20351;&#29992;Transformers&#30340;&#26041;&#27861;&#65292;&#23578;&#26410;&#34987;&#20805;&#20998;&#21033;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26816;&#26597;&#20102;&#22810;&#31181;&#30701;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;&#20197;&#21450;&#25490;&#21517;&#38752;&#21069;&#30340;&#20256;&#32479;&#25991;&#26412;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#20004;&#20010;&#26032;&#30340;&#23454;&#38469;&#30701;&#25991;&#26412;&#25968;&#25454;&#38598;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#20197;&#35299;&#20915;&#23545;&#20110;&#20165;&#20855;&#26377;&#26377;&#38480;&#29305;&#24449;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#36807;&#24230;&#20381;&#36182;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;Transformers&#22312;&#30701;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#29575;&#65292;&#24341;&#21457;&#20102;&#26159;&#21542;&#38656;&#35201;&#19987;&#38376;&#30340;&#30701;&#25991;&#26412;&#25216;&#26415;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Short text classification is a crucial and challenging aspect of Natural Language Processing. For this reason, there are numerous highly specialized short text classifiers. However, in recent short text research, State of the Art (SOTA) methods for traditional text classification, particularly the pure use of Transformers, have been unexploited. In this work, we examine the performance of a variety of short text classifiers as well as the top performing traditional text classifier. We further investigate the effects on two new real-world short text datasets in an effort to address the issue of becoming overly dependent on benchmark datasets with a limited number of characteristics. Our experiments unambiguously demonstrate that Transformers achieve SOTA accuracy on short text classification tasks, raising the question of whether specialized short text techniques are necessary.
&lt;/p&gt;</description></item><item><title>Kuaipedia&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#22810;&#27169;&#24335;&#30701;&#35270;&#39057;&#30334;&#31185;&#20840;&#20070;&#65292;&#36890;&#36807;&#30693;&#35782;&#35270;&#39057;&#30340;&#24418;&#24335;&#65292;&#33021;&#22815;&#36731;&#26494;&#34920;&#36798;&#32593;&#27665;&#23545;&#26576;&#20010;&#39033;&#30446;&#30340;&#21508;&#20010;&#26041;&#38754;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2211.00732</link><description>&lt;p&gt;
Kuaipedia:&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#22810;&#27169;&#24335;&#30701;&#35270;&#39057;&#30334;&#31185;&#20840;&#20070;
&lt;/p&gt;
&lt;p&gt;
Kuaipedia: a Large-scale Multi-modal Short-video Encyclopedia. (arXiv:2211.00732v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00732
&lt;/p&gt;
&lt;p&gt;
Kuaipedia&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#22810;&#27169;&#24335;&#30701;&#35270;&#39057;&#30334;&#31185;&#20840;&#20070;&#65292;&#36890;&#36807;&#30693;&#35782;&#35270;&#39057;&#30340;&#24418;&#24335;&#65292;&#33021;&#22815;&#36731;&#26494;&#34920;&#36798;&#32593;&#27665;&#23545;&#26576;&#20010;&#39033;&#30446;&#30340;&#21508;&#20010;&#26041;&#38754;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;20&#24180;&#20013;&#65292;&#22312;&#32447;&#30334;&#31185;&#20840;&#20070;&#65288;&#22914;&#32500;&#22522;&#30334;&#31185;&#65289;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#21457;&#23637;&#21644;&#30740;&#31350;&#12290;&#20154;&#20204;&#21487;&#20197;&#22312;&#30001;&#24535;&#24895;&#32773;&#31038;&#21306;&#32534;&#36753;&#30340;&#32500;&#22522;&#39029;&#38754;&#19978;&#25214;&#21040;&#32500;&#22522;&#39033;&#30340;&#20219;&#20309;&#23646;&#24615;&#25110;&#20854;&#20182;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#25991;&#26412;&#12289;&#22270;&#29255;&#21644;&#34920;&#26684;&#24456;&#38590;&#34920;&#36798;&#32500;&#22522;&#39033;&#30340;&#26576;&#20123;&#26041;&#38754;&#12290;&#20363;&#22914;&#65292;&#24403;&#25105;&#20204;&#35848;&#35770;&#8220;&#26612;&#29356;&#8221;&#26102;&#65292;&#20154;&#20204;&#21487;&#33021;&#26356;&#20851;&#24515;&#8220;&#22914;&#20309;&#21890;&#20859;&#23427;&#8221;&#25110;&#8220;&#22914;&#20309;&#35757;&#32451;&#23427;&#19981;&#20445;&#25252;&#39135;&#29289;&#8221;&#12290;&#30446;&#21069;&#65292;&#30701;&#35270;&#39057;&#24179;&#21488;&#24050;&#25104;&#20026;&#22312;&#32447;&#19990;&#30028;&#30340;&#26631;&#24535;&#12290;&#26080;&#35770;&#20320;&#20351;&#29992;&#30340;&#26159;TikTok&#12289;Instagram&#12289;&#24555;&#25163;&#36824;&#26159;YouTube Shorts&#65292;&#30701;&#35270;&#39057;&#24212;&#29992;&#31243;&#24207;&#24050;&#25913;&#21464;&#20102;&#25105;&#20204;&#20170;&#22825;&#30340;&#20869;&#23481;&#28040;&#36153;&#21644;&#21019;&#20316;&#26041;&#24335;&#12290;&#38500;&#20102;&#20026;&#23089;&#20048;&#21046;&#20316;&#30701;&#35270;&#39057;&#22806;&#65292;&#25105;&#20204;&#36234;&#26469;&#36234;&#22810;&#22320;&#30475;&#21040;&#20316;&#32773;&#20204;&#22312;&#21508;&#34892;&#21508;&#19994;&#24191;&#27867;&#20998;&#20139;&#26377;&#35265;&#35299;&#30340;&#30693;&#35782;&#12290;&#36825;&#20123;&#30701;&#35270;&#39057;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#30693;&#35782;&#35270;&#39057;&#65292;&#21487;&#20197;&#36731;&#26494;&#34920;&#36798;&#28040;&#36153;&#32773;&#24819;&#20102;&#35299;&#26377;&#20851;&#26576;&#20010;&#39033;&#30446;&#65288;&#20363;&#22914;&#26612;&#29356;&#65289;&#30340;&#20219;&#20309;&#26041;&#38754;&#65288;&#20363;&#22914;&#27611;&#21457;&#25110;&#22914;&#20309;&#21890;&#20859;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online encyclopedias, such as Wikipedia, have been well-developed and researched in the last two decades. One can find any attributes or other information of a wiki item on a wiki page edited by a community of volunteers. However, the traditional text, images and tables can hardly express some aspects of an wiki item. For example, when we talk about ``Shiba Inu'', one may care more about ``How to feed it'' or ``How to train it not to protect its food''. Currently, short-video platforms have become a hallmark in the online world. Whether you're on TikTok, Instagram, Kuaishou, or YouTube Shorts, short-video apps have changed how we consume and create content today. Except for producing short videos for entertainment, we can find more and more authors sharing insightful knowledge widely across all walks of life. These short videos, which we call knowledge videos, can easily express any aspects (e.g. hair or how-to-feed) consumers want to know about an item (e.g. Shiba Inu), and they can b
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;Whisper&#27169;&#22411;&#34429;&#28982;&#22312;&#20998;&#24067;&#22806;&#36755;&#20837;&#21644;&#38543;&#26426;&#22122;&#22768;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#20986;&#33394;&#30340;&#31283;&#20581;&#24615;&#65292;&#20294;&#21364;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24178;&#25200;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#29983;&#25104;&#26497;&#23567;&#30340;&#36755;&#20837;&#25200;&#21160;&#26469;&#22823;&#24133;&#38477;&#20302;Whisper&#30340;&#24615;&#33021;&#65292;&#24182;&#23545;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#20102;&#24433;&#21709;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#20110;&#23454;&#38469;&#30340;&#23433;&#20840;&#38382;&#39064;&#21644;&#23545;&#25239;&#24615;&#31283;&#20581;ASR&#30340;&#38656;&#27714;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2210.17316</link><description>&lt;p&gt;
&#26377;&#19981;&#27490;&#19968;&#31181;&#31283;&#20581;&#24615;&#65306;&#29992;&#23545;&#25239;&#26679;&#26412;&#27450;&#39575;Whisper&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
There is more than one kind of robustness: Fooling Whisper with adversarial examples. (arXiv:2210.17316v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;Whisper&#27169;&#22411;&#34429;&#28982;&#22312;&#20998;&#24067;&#22806;&#36755;&#20837;&#21644;&#38543;&#26426;&#22122;&#22768;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#20986;&#33394;&#30340;&#31283;&#20581;&#24615;&#65292;&#20294;&#21364;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24178;&#25200;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#29983;&#25104;&#26497;&#23567;&#30340;&#36755;&#20837;&#25200;&#21160;&#26469;&#22823;&#24133;&#38477;&#20302;Whisper&#30340;&#24615;&#33021;&#65292;&#24182;&#23545;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#20102;&#24433;&#21709;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#20110;&#23454;&#38469;&#30340;&#23433;&#20840;&#38382;&#39064;&#21644;&#23545;&#25239;&#24615;&#31283;&#20581;ASR&#30340;&#38656;&#27714;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Whisper&#26159;&#19968;&#31181;&#26368;&#36817;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#65292;&#23545;&#20110;&#20998;&#24067;&#22806;&#36755;&#20837;&#21644;&#38543;&#26426;&#22122;&#22768;&#37117;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#31283;&#20581;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#31283;&#20581;&#24615;&#22312;&#23545;&#25239;&#24178;&#25200;&#19979;&#24182;&#19981;&#36866;&#29992;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#29983;&#25104;&#26497;&#23567;&#30340;&#36755;&#20837;&#25200;&#21160;&#65288;&#20449;&#22122;&#27604;&#20026;35-45dB&#65289;&#65292;&#25105;&#20204;&#21487;&#20197;&#22823;&#24133;&#38477;&#20302;Whisper&#30340;&#24615;&#33021;&#65292;&#29978;&#33267;&#36716;&#24405;&#25105;&#20204;&#36873;&#25321;&#30340;&#30446;&#26631;&#21477;&#23376;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36890;&#36807;&#27450;&#39575;Whisper&#35821;&#35328;&#26816;&#27979;&#22120;&#65292;&#25105;&#20204;&#21487;&#20197;&#36731;&#26494;&#38477;&#20302;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#23545;&#19968;&#20010;&#24191;&#21463;&#27426;&#36814;&#30340;&#24320;&#28304;&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;&#20855;&#26377;&#23454;&#38469;&#30340;&#23433;&#20840;&#24433;&#21709;&#65292;&#24182;&#24378;&#35843;&#20102;&#23545;&#25239;&#24615;&#31283;&#20581;ASR&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whisper is a recent Automatic Speech Recognition (ASR) model displaying impressive robustness to both out-of-distribution inputs and random noise. In this work, we show that this robustness does not carry over to adversarial noise. We show that we can degrade Whisper performance dramatically, or even transcribe a target sentence of our choice, by generating very small input perturbations with Signal Noise Ratio of 35-45dB. We also show that by fooling the Whisper language detector we can very easily degrade the performance of multilingual models. These vulnerabilities of a widely popular open-source model have practical security implications and emphasize the need for adversarially robust ASR.
&lt;/p&gt;</description></item><item><title>TitaNet-LID&#26159;&#19968;&#31181;&#32039;&#20945;&#19988;&#20855;&#26377;&#23616;&#37096;&#21644;&#20840;&#23616;&#19978;&#19979;&#25991;&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36866;&#29992;&#20110;&#21475;&#35821;&#35821;&#35328;&#35782;&#21035;&#12290;&#23613;&#31649;&#23610;&#23544;&#36739;&#23567;&#65292;&#20294;&#23427;&#21487;&#20197;&#22312;&#19981;&#38477;&#20302;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#20284;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#22768;&#23398;&#26465;&#20214;&#21644;&#35821;&#35328;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#33391;&#22909;&#30340;&#25193;&#23637;&#24615;&#21644;&#22788;&#29702;&#30701;&#35821;&#36755;&#20837;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2210.15781</link><description>&lt;p&gt;
&#29992;&#20110;&#21475;&#35821;&#35821;&#35328;&#35782;&#21035;&#30340;&#20855;&#26377;&#23616;&#37096;&#21644;&#20840;&#23616;&#19978;&#19979;&#25991;&#30340;&#32039;&#20945;&#22411;&#31471;&#21040;&#31471;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Compact End-to-End Model with Local and Global Context for Spoken Language Identification. (arXiv:2210.15781v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15781
&lt;/p&gt;
&lt;p&gt;
TitaNet-LID&#26159;&#19968;&#31181;&#32039;&#20945;&#19988;&#20855;&#26377;&#23616;&#37096;&#21644;&#20840;&#23616;&#19978;&#19979;&#25991;&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36866;&#29992;&#20110;&#21475;&#35821;&#35821;&#35328;&#35782;&#21035;&#12290;&#23613;&#31649;&#23610;&#23544;&#36739;&#23567;&#65292;&#20294;&#23427;&#21487;&#20197;&#22312;&#19981;&#38477;&#20302;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#20284;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#22768;&#23398;&#26465;&#20214;&#21644;&#35821;&#35328;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#33391;&#22909;&#30340;&#25193;&#23637;&#24615;&#21644;&#22788;&#29702;&#30701;&#35821;&#36755;&#20837;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;TitaNet-LID&#65292;&#19968;&#31181;&#22522;&#20110;ContextNet&#26550;&#26500;&#30340;&#29992;&#20110;&#21475;&#35821;&#35821;&#35328;&#35782;&#21035;&#65288;LID&#65289;&#30340;&#32039;&#20945;&#22411;&#31471;&#21040;&#31471;&#31070;&#32463;&#32593;&#32476;&#12290;TitaNet-LID&#20351;&#29992;1D&#28145;&#24230;&#21487;&#20998;&#31163;&#21367;&#31215;&#21644;Squeeze-and-Excitation&#23618;&#65292;&#26377;&#25928;&#22320;&#25429;&#25417;&#20102;&#35805;&#35821;&#20013;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#19978;&#19979;&#25991;&#12290;&#23613;&#31649;&#23610;&#23544;&#36739;&#23567;&#65292;TitaNet-LID&#22312;VoxLingua107&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#19982;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#30456;&#20284;&#65292;&#21516;&#26102;&#23610;&#23544;&#26356;&#23567;10&#20493;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#24494;&#35843;&#65292;&#23427;&#21487;&#20197;&#36731;&#26494;&#36866;&#24212;&#26032;&#30340;&#22768;&#23398;&#26465;&#20214;&#21644;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#65292;&#22312;FLEURS&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#20102;88.2%&#30340;&#26368;&#20808;&#36827;&#20934;&#30830;&#29575;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65292;&#21487;&#20197;&#22312;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#20043;&#38388;&#21462;&#24471;&#26356;&#22909;&#30340;&#24179;&#34913;&#12290;TitaNet-LID&#22312;&#38271;&#24230;&#19981;&#36275;5&#31186;&#30340;&#30701;&#35821;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#34920;&#26126;&#23427;&#23545;&#36755;&#20837;&#38271;&#24230;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce TitaNet-LID, a compact end-to-end neural network for Spoken Language Identification (LID) that is based on the ContextNet architecture. TitaNet-LID employs 1D depth-wise separable convolutions and Squeeze-and-Excitation layers to effectively capture local and global context within an utterance. Despite its small size, TitaNet-LID achieves performance similar to state-of-the-art models on the VoxLingua107 dataset while being 10 times smaller. Furthermore, it can be easily adapted to new acoustic conditions and unseen languages through simple fine-tuning, achieving a state-of-the-art accuracy of 88.2% on the FLEURS benchmark. Our model is scalable and can achieve a better trade-off between accuracy and speed. TitaNet-LID performs well even on short utterances less than 5s in length, indicating its robustness to input length.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#32447;&#24615;&#38142;&#26465;&#20214;&#38543;&#26426;&#22330;&#65288;CRFs&#65289;&#38480;&#21046;&#21040;&#27491;&#21017;&#35821;&#35328;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23545;&#24191;&#27867;&#30340;&#32422;&#26463;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#20801;&#35768;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24341;&#20837;&#36825;&#20123;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2106.07306</link><description>&lt;p&gt;
&#38480;&#21046;&#32447;&#24615;&#38142;&#26465;&#20214;&#38543;&#26426;&#22330;&#21040;&#27491;&#21017;&#35821;&#35328;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Constraining Linear-chain CRFs to Regular Languages. (arXiv:2106.07306v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.07306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#32447;&#24615;&#38142;&#26465;&#20214;&#38543;&#26426;&#22330;&#65288;CRFs&#65289;&#38480;&#21046;&#21040;&#27491;&#21017;&#35821;&#35328;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23545;&#24191;&#27867;&#30340;&#32422;&#26463;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#20801;&#35768;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24341;&#20837;&#36825;&#20123;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#39044;&#27979;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#34920;&#31034;&#36755;&#20986;&#32467;&#26500;&#20013;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#12290;&#24403;&#36755;&#20986;&#20197;&#24207;&#21015;&#24418;&#24335;&#32467;&#26500;&#21270;&#26102;&#65292;&#32447;&#24615;&#38142;&#26465;&#20214;&#38543;&#26426;&#22330;&#65288;CRFs&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#27169;&#22411;&#31867;&#65292;&#21487;&#20197;&#23398;&#20064;&#36755;&#20986;&#20013;&#30340;&#8220;&#23616;&#37096;&#8221;&#20381;&#36182;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;CRF&#30340;&#39532;&#23572;&#21487;&#22827;&#20551;&#35774;&#20351;&#24471;CRFs&#26080;&#27861;&#34920;&#31034;&#20855;&#26377;&#8220;&#38750;&#23616;&#37096;&#8221;&#20381;&#36182;&#20851;&#31995;&#30340;&#20998;&#24067;&#65292;&#24182;&#19988;&#26631;&#20934;CRFs&#26080;&#27861;&#28385;&#36275;&#25968;&#25454;&#30340;&#38750;&#23616;&#37096;&#32422;&#26463;&#65288;&#20363;&#22914;&#36755;&#20986;&#26631;&#31614;&#30340;&#20840;&#23616;&#24615;&#32422;&#26463;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;CRFs&#30340;&#25512;&#24191;&#24418;&#24335;&#65292;&#21487;&#20197;&#36890;&#36807;&#23558;&#21487;&#33021;&#30340;&#36755;&#20986;&#32467;&#26500;&#31354;&#38388;&#25351;&#23450;&#20026;&#27491;&#21017;&#35821;&#35328;$\mathcal{L}$&#26469;&#24378;&#21046;&#25191;&#34892;&#24191;&#27867;&#30340;&#32422;&#26463;&#65292;&#21253;&#25324;&#38750;&#23616;&#37096;&#32422;&#26463;&#12290;&#32467;&#26524;&#30340;&#27491;&#21017;&#32422;&#26463;CRF&#65288;RegCCRF&#65289;&#20855;&#26377;&#19982;&#26631;&#20934;CRF&#30456;&#21516;&#30340;&#24418;&#24335;&#23646;&#24615;&#65292;&#20294;&#23545;&#20110;&#19981;&#22312;$\mathcal{L}$&#20013;&#30340;&#25152;&#26377;&#26631;&#31614;&#24207;&#21015;&#20998;&#37197;&#38646;&#27010;&#29575;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;RegCCRFs&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24341;&#20837;&#32422;&#26463;&#65292;&#19982;&#30456;&#20851;&#27169;&#22411;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
A major challenge in structured prediction is to represent the interdependencies within output structures. When outputs are structured as sequences, linear-chain conditional random fields (CRFs) are a widely used model class which can learn \textit{local} dependencies in the output. However, the CRF's Markov assumption makes it impossible for CRFs to represent distributions with \textit{nonlocal} dependencies, and standard CRFs are unable to respect nonlocal constraints of the data (such as global arity constraints on output labels). We present a generalization of CRFs that can enforce a broad class of constraints, including nonlocal ones, by specifying the space of possible output structures as a regular language $\mathcal{L}$. The resulting regular-constrained CRF (RegCCRF) has the same formal properties as a standard CRF, but assigns zero probability to all label sequences not in $\mathcal{L}$. Notably, RegCCRFs can incorporate their constraints during training, while related models
&lt;/p&gt;</description></item></channel></rss>