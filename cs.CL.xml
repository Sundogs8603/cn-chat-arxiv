<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>MathVerse&#26159;&#19968;&#20010;&#20840;&#26041;&#20301;&#30340;&#35270;&#35273;&#25968;&#23398;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#20844;&#24179;&#32780;&#28145;&#20837;&#22320;&#35780;&#20272;MLLMs&#22312;&#35270;&#35273;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.14624</link><description>&lt;p&gt;
MathVerse&#65306;&#24744;&#30340;&#22810;&#27169;&#24335;LLM&#26159;&#21542;&#30495;&#27491;&#30475;&#21040;&#20102;&#35270;&#35273;&#25968;&#23398;&#38382;&#39064;&#20013;&#30340;&#22270;&#34920;&#65311;
&lt;/p&gt;
&lt;p&gt;
MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14624
&lt;/p&gt;
&lt;p&gt;
MathVerse&#26159;&#19968;&#20010;&#20840;&#26041;&#20301;&#30340;&#35270;&#35273;&#25968;&#23398;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#20844;&#24179;&#32780;&#28145;&#20837;&#22320;&#35780;&#20272;MLLMs&#22312;&#35270;&#35273;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#22312;&#35270;&#35273;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#28982;&#32780;&#23427;&#20204;&#22312;&#35270;&#35273;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#26041;&#38754;&#30340;&#33021;&#21147;&#20173;&#26410;&#20805;&#20998;&#35780;&#20272;&#21644;&#29702;&#35299;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#24403;&#21069;&#22522;&#20934;&#27979;&#35797;&#65292;&#23558;&#36807;&#22810;&#30340;&#35270;&#35273;&#20869;&#23481;&#34701;&#20837;&#25991;&#26412;&#38382;&#39064;&#20013;&#65292;&#36825;&#26377;&#21161;&#20110;MLLM&#22312;&#19981;&#30495;&#27491;&#35299;&#37322;&#36755;&#20837;&#22270;&#34920;&#30340;&#24773;&#20917;&#19979;&#25512;&#23548;&#31572;&#26696;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MathVerse&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#26041;&#20301;&#30340;&#35270;&#35273;&#25968;&#23398;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#20844;&#24179;&#32780;&#28145;&#20837;&#22320;&#35780;&#20272;MLLMs&#12290;&#25105;&#20204;&#31934;&#24515;&#25910;&#38598;&#20102;2,612&#20010;&#39640;&#36136;&#37327;&#30340;&#22810;&#23398;&#31185;&#25968;&#23398;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#21547;&#22270;&#34920;&#65292;&#26469;&#28304;&#20110;&#20844;&#24320;&#28192;&#36947;&#12290;&#28982;&#21518;&#65292;&#27599;&#20010;&#38382;&#39064;&#30001;&#20154;&#24037;&#27880;&#37322;&#32773;&#36716;&#21270;&#20026;&#20845;&#20010;&#19981;&#21516;&#29256;&#26412;&#65292;&#27599;&#20010;&#29256;&#26412;&#22312;&#22810;&#27169;&#24335;&#20013;&#25552;&#20379;&#19981;&#21516;&#31243;&#24230;&#30340;&#20449;&#24687;&#20869;&#23481;&#65292;&#20849;&#36129;&#29486;&#20102;15K&#20010;&#27979;&#35797;&#26679;&#26412;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#24471;MathVerse&#33021;&#22815;&#21516;&#26102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14624v1 Announce Type: cross  Abstract: The remarkable progress of Multi-modal Large Language Models (MLLMs) has garnered unparalleled attention, due to their superior performance in visual contexts. However, their capabilities in visual math problem-solving remain insufficiently evaluated and understood. We investigate current benchmarks to incorporate excessive visual content within textual questions, which potentially assist MLLMs in deducing answers without truly interpreting the input diagrams. To this end, we introduce MathVerse, an all-around visual math benchmark designed for an equitable and in-depth evaluation of MLLMs. We meticulously collect 2,612 high-quality, multi-subject math problems with diagrams from publicly available sources. Each problem is then transformed by human annotators into six distinct versions, each offering varying degrees of information content in multi-modality, contributing to 15K test samples in total. This approach allows MathVerse to co
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;A$^3$T&#26694;&#26550;&#65292;&#36890;&#36807;ActRe&#25552;&#31034;&#20195;&#29702;&#23454;&#29616;&#20102;ReAct&#39118;&#26684;&#20195;&#29702;&#23545;&#20195;&#29702;&#36712;&#36857;&#30340;&#33258;&#20027;&#26631;&#27880;&#65292;&#21516;&#26102;&#22686;&#24378;&#20102;&#26032;&#30340;&#36712;&#36857;&#21512;&#25104;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.14589</link><description>&lt;p&gt;
ReAct&#36935;&#19978;ActRe&#65306;&#23545;&#27604;&#24615;&#33258;&#35757;&#32451;&#20013;&#30340;&#20195;&#29702;&#36712;&#36857;&#33258;&#21160;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
ReAct Meets ActRe: Autonomous Annotations of Agent Trajectories for Contrastive Self-Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14589
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;A$^3$T&#26694;&#26550;&#65292;&#36890;&#36807;ActRe&#25552;&#31034;&#20195;&#29702;&#23454;&#29616;&#20102;ReAct&#39118;&#26684;&#20195;&#29702;&#23545;&#20195;&#29702;&#36712;&#36857;&#30340;&#33258;&#20027;&#26631;&#27880;&#65292;&#21516;&#26102;&#22686;&#24378;&#20102;&#26032;&#30340;&#36712;&#36857;&#21512;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14589v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; &#25991;&#25688;&#65306;&#35821;&#35328;&#20195;&#29702;&#36890;&#36807;&#19982;&#22522;&#30784;&#27169;&#22411;&#25512;&#29702;&#23637;&#31034;&#20102;&#33258;&#20027;&#20915;&#31574;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#33268;&#21147;&#20110;&#36890;&#36807;&#22810;&#27493;&#25512;&#29702;&#21644;&#34892;&#21160;&#36712;&#36857;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#26469;&#35757;&#32451;&#35821;&#35328;&#20195;&#29702;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#36825;&#26679;&#30340;&#36712;&#36857;&#20173;&#38656;&#35201;&#30456;&#24403;&#22823;&#30340;&#20154;&#21147;&#65292;&#26080;&#35770;&#26159;&#36890;&#36807;&#20154;&#24037;&#26631;&#27880;&#36824;&#26159;&#23454;&#26045;&#22810;&#26679;&#21270;&#25552;&#31034;&#26694;&#26550;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;A$^3$T&#65292;&#19968;&#20010;&#20801;&#35768;&#20197;ReAct&#39118;&#26684;&#33258;&#20027;&#27880;&#37322;&#20195;&#29702;&#36712;&#36857;&#30340;&#26694;&#26550;&#12290;&#20854;&#20013;&#24515;&#26159;&#19968;&#20010;ActRe&#25552;&#31034;&#20195;&#29702;&#65292;&#23427;&#35299;&#37322;&#20219;&#24847;&#21160;&#20316;&#30340;&#21407;&#22240;&#12290;&#24403;&#38543;&#26426;&#25277;&#21462;&#22806;&#37096;&#21160;&#20316;&#26102;&#65292;ReAct&#39118;&#26684;&#20195;&#29702;&#21487;&#20197;&#26597;&#35810;ActRe&#20195;&#29702;&#20197;&#33719;&#21462;&#20854;&#25991;&#26412;&#29702;&#30001;&#12290;&#26032;&#39062;&#30340;&#36712;&#36857;&#28982;&#21518;&#36890;&#36807;&#23558;ActRe&#30340;&#21518;&#39564;&#25512;&#29702;&#21069;&#32622;&#21040;&#25277;&#26679;&#21160;&#20316;&#20013;&#36827;&#34892;&#32508;&#21512;&#21512;&#25104;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;ReAct&#39118;&#26684;&#20195;&#29702;&#21487;&#25191;&#34892;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14589v1 Announce Type: new  Abstract: Language agents have demonstrated autonomous decision-making abilities by reasoning with foundation models. Recently, efforts have been made to train language agents for performance improvement, with multi-step reasoning and action trajectories as the training data. However, collecting such trajectories still requires considerable human effort, by either artificial annotations or implementations of diverse prompting frameworks. In this work, we propose A$^3$T, a framework that enables the Autonomous Annotation of Agent Trajectories in the style of ReAct. The central role is an ActRe prompting agent, which explains the reason for an arbitrary action. When randomly sampling an external action, the ReAct-style agent could query the ActRe agent with the action to obtain its textual rationales. Novel trajectories are then synthesized by prepending the posterior reasoning from ActRe to the sampled action. In this way, the ReAct-style agent exe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#36873;&#39064;&#25968;&#25454;&#19978;&#30340;&#35757;&#32451;&#25928;&#26524;&#65292;&#24182;&#21033;&#29992;MQ&#24207;&#21015;BERT&#26041;&#27861;&#65292;&#22312;&#21307;&#23398;&#31185;&#30446;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#26368;&#20808;&#36827;&#32467;&#26524;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.14582</link><description>&lt;p&gt;
&#29992;&#20110;&#21307;&#23398;&#31185;&#30446;&#22810;&#36873;&#39064;&#20998;&#31867;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Multi-Choice Question Classification of Medical Subjects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#36873;&#39064;&#25968;&#25454;&#19978;&#30340;&#35757;&#32451;&#25928;&#26524;&#65292;&#24182;&#21033;&#29992;MQ&#24207;&#21015;BERT&#26041;&#27861;&#65292;&#22312;&#21307;&#23398;&#31185;&#30446;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#26368;&#20808;&#36827;&#32467;&#26524;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#35780;&#20272;&#26159;&#21542;&#21487;&#20197;&#21033;&#29992;&#22312;&#22810;&#36873;&#39064;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#21306;&#20998;&#21307;&#23398;&#31185;&#30446;&#12290;&#36825;&#23545;&#20110;&#33258;&#21160;&#38382;&#31572;&#26159;&#19968;&#39033;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#23558;&#38382;&#39064;&#22810;&#31867;&#21035;&#20998;&#31867;&#20026;&#25512;&#26029;&#30340;&#21307;&#23398;&#31185;&#30446;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#22810;&#38382;&#39064;(MQ)&#24207;&#21015;BERT&#26041;&#27861;&#65292;&#22312;MedMCQA&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#20998;&#21035;&#22312;&#20854;&#24320;&#21457;&#21644;&#27979;&#35797;&#38598;&#19978;&#20855;&#26377;0.68&#21644;0.60&#30340;&#20934;&#30830;&#29575;&#12290;&#20174;&#36825;&#20010;&#24847;&#20041;&#19978;&#35828;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#29305;&#21035;&#26159;LLMs&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#29992;&#20110;&#22810;&#20998;&#31867;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14582v1 Announce Type: cross  Abstract: The aim of this paper is to evaluate whether large language models trained on multi-choice question data can be used to discriminate between medical subjects. This is an important and challenging task for automatic question answering. To achieve this goal, we train deep neural networks for multi-class classification of questions into the inferred medical subjects. Using our Multi-Question (MQ) Sequence-BERT method, we outperform the state-of-the-art results on the MedMCQA dataset with an accuracy of 0.68 and 0.60 on their development and test sets, respectively. In this sense, we show the capability of AI and LLMs in particular for multi-classification tasks in the Healthcare domain.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#33258;&#21160;&#35780;&#20272;&#20013;&#23398;&#29983;&#31185;&#23398;&#24418;&#25104;&#24615;&#35780;&#20272;&#22238;&#31572;&#65292;&#32467;&#21512;&#24605;&#32500;&#38142;&#25512;&#29702;&#65292;&#20154;&#26426;&#21327;&#21516;&#26041;&#27861;&#25104;&#21151;&#35780;&#20998;&#24182;&#25552;&#20379;&#26377;&#24847;&#20041;&#35299;&#37322;&#65292;&#26377;&#26395;&#25552;&#21319;&#24320;&#25918;&#24615;&#31185;&#23398;&#27979;&#35780;&#30340;&#33258;&#21160;&#35780;&#20998;&#12290;</title><link>https://arxiv.org/abs/2403.14565</link><description>&lt;p&gt;
&#22522;&#20110;LLMs&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;&#26041;&#27861;&#29992;&#20110;&#35780;&#20272;&#31185;&#23398;&#20013;&#23398;&#29983;&#24418;&#25104;&#24615;&#35780;&#20272;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
A Chain-of-Thought Prompting Approach with LLMs for Evaluating Students' Formative Assessment Responses in Science
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14565
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#33258;&#21160;&#35780;&#20272;&#20013;&#23398;&#29983;&#31185;&#23398;&#24418;&#25104;&#24615;&#35780;&#20272;&#22238;&#31572;&#65292;&#32467;&#21512;&#24605;&#32500;&#38142;&#25512;&#29702;&#65292;&#20154;&#26426;&#21327;&#21516;&#26041;&#27861;&#25104;&#21151;&#35780;&#20998;&#24182;&#25552;&#20379;&#26377;&#24847;&#20041;&#35299;&#37322;&#65292;&#26377;&#26395;&#25552;&#21319;&#24320;&#25918;&#24615;&#31185;&#23398;&#27979;&#35780;&#30340;&#33258;&#21160;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;K-12&#31185;&#23398;&#20013;&#30340;&#31616;&#31572;&#27979;&#35780;&#36827;&#34892;&#35780;&#20998;&#21644;&#35299;&#37322;&#12290;&#34429;&#28982;&#29616;&#26377;&#26041;&#27861;&#21487;&#20197;&#20026;&#26356;&#32467;&#26500;&#21270;&#30340;&#25968;&#23398;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#27979;&#35780;&#25171;&#20998;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#19981;&#25552;&#20379;&#20998;&#25968;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20391;&#37325;&#20110;&#22312;&#20013;&#23398;&#22320;&#29699;&#31185;&#23398;&#20013;&#24212;&#29992;GPT-4&#36827;&#34892;&#33258;&#21160;&#35780;&#20272;&#65292;&#23558;&#23569;&#26679;&#26412;&#23398;&#20064;&#21644;&#20027;&#21160;&#23398;&#20064;&#19982;&#24605;&#32500;&#38142;&#25512;&#29702;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#20154;&#26426;&#21327;&#21516;&#26041;&#27861;&#65292;&#25105;&#20204;&#25104;&#21151;&#23545;&#24418;&#25104;&#24615;&#35780;&#20272;&#22238;&#31572;&#36827;&#34892;&#35780;&#20998;&#24182;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#31995;&#32479;&#20998;&#26512;&#25581;&#31034;&#20102;&#20154;&#26426;&#21327;&#21516;&#25216;&#26415;&#22686;&#24378;&#24320;&#25918;&#24615;&#31185;&#23398;&#27979;&#35780;&#30340;&#33258;&#21160;&#35780;&#20998;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14565v1 Announce Type: new  Abstract: This paper explores the use of large language models (LLMs) to score and explain short-answer assessments in K-12 science. While existing methods can score more structured math and computer science assessments, they often do not provide explanations for the scores. Our study focuses on employing GPT-4 for automated assessment in middle school Earth Science, combining few-shot and active learning with chain-of-thought reasoning. Using a human-in-the-loop approach, we successfully score and provide meaningful explanations for formative assessment responses. A systematic analysis of our method's pros and cons sheds light on the potential for human-in-the-loop techniques to enhance automated grading for open-ended science assessments.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35821;&#20041;&#35299;&#30721;&#30340;&#26032;&#35266;&#28857;&#65292;&#23558;LLM&#12289;&#20154;&#31867;&#36755;&#20837;&#21644;&#21508;&#31181;&#24037;&#20855;&#20043;&#38388;&#30340;&#21327;&#20316;&#36807;&#31243;&#26500;&#24314;&#20026;&#35821;&#20041;&#31354;&#38388;&#20013;&#30340;&#20248;&#21270;&#36807;&#31243;&#65292;&#20419;&#36827;&#20102;&#39640;&#25928;&#36755;&#20986;&#30340;&#26500;&#24314;&#12290;</title><link>https://arxiv.org/abs/2403.14562</link><description>&lt;p&gt;
&#35821;&#20041;&#35299;&#30721;&#26102;&#20195;
&lt;/p&gt;
&lt;p&gt;
The Era of Semantic Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14562
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35821;&#20041;&#35299;&#30721;&#30340;&#26032;&#35266;&#28857;&#65292;&#23558;LLM&#12289;&#20154;&#31867;&#36755;&#20837;&#21644;&#21508;&#31181;&#24037;&#20855;&#20043;&#38388;&#30340;&#21327;&#20316;&#36807;&#31243;&#26500;&#24314;&#20026;&#35821;&#20041;&#31354;&#38388;&#20013;&#30340;&#20248;&#21270;&#36807;&#31243;&#65292;&#20419;&#36827;&#20102;&#39640;&#25928;&#36755;&#20986;&#30340;&#26500;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#23637;&#29616;&#20102;&#22312;LLM&#65288;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#12289;&#20154;&#31867;&#36755;&#20837;&#21644;&#21508;&#31181;&#24037;&#20855;&#20043;&#38388;&#32534;&#25490;&#21327;&#20316;&#20197;&#35299;&#20915;LLM&#22266;&#26377;&#23616;&#38480;&#24615;&#30340;&#24819;&#27861;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#35821;&#20041;&#35299;&#30721;&#30340;&#26032;&#35266;&#28857;&#65292;&#23558;&#36825;&#20123;&#21327;&#20316;&#36807;&#31243;&#26500;&#24314;&#20026;&#35821;&#20041;&#31354;&#38388;&#20013;&#30340;&#20248;&#21270;&#36807;&#31243;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;LLM&#27010;&#24565;&#21270;&#20026;&#25805;&#32437;&#25105;&#20204;&#31216;&#20043;&#20026;&#35821;&#20041;&#26631;&#35760;&#65288;&#24050;&#30693;&#24605;&#24819;&#65289;&#30340;&#26377;&#24847;&#20041;&#20449;&#24687;&#29255;&#27573;&#30340;&#35821;&#20041;&#22788;&#29702;&#22120;&#12290;LLM&#26159;&#20247;&#22810;&#20854;&#20182;&#35821;&#20041;&#22788;&#29702;&#22120;&#20043;&#19968;&#65292;&#21253;&#25324;&#20154;&#31867;&#21644;&#24037;&#20855;&#65292;&#27604;&#22914;&#25628;&#32034;&#24341;&#25806;&#25110;&#20195;&#30721;&#25191;&#34892;&#22120;&#12290;&#35821;&#20041;&#22788;&#29702;&#22120;&#38598;&#20307;&#21442;&#19982;&#35821;&#20041;&#26631;&#35760;&#30340;&#21160;&#24577;&#20132;&#27969;&#65292;&#36880;&#27493;&#26500;&#24314;&#39640;&#25928;&#36755;&#20986;&#12290;&#25105;&#20204;&#31216;&#36825;&#20123;&#22312;&#35821;&#20041;&#31354;&#38388;&#20013;&#36827;&#34892;&#20248;&#21270;&#21644;&#25628;&#32034;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#20026;&#35821;&#20041;&#35299;&#30721;&#31639;&#27861;&#12290;&#36825;&#20010;&#27010;&#24565;&#19982;&#24050;&#24191;&#20026;&#30740;&#31350;&#30340;&#35821;&#20041;&#35299;&#30721;&#38382;&#39064;&#30452;&#25509;&#24179;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14562v1 Announce Type: cross  Abstract: Recent work demonstrated great promise in the idea of orchestrating collaborations between LLMs, human input, and various tools to address the inherent limitations of LLMs. We propose a novel perspective called semantic decoding, which frames these collaborative processes as optimization procedures in semantic space. Specifically, we conceptualize LLMs as semantic processors that manipulate meaningful pieces of information that we call semantic tokens (known thoughts). LLMs are among a large pool of other semantic processors, including humans and tools, such as search engines or code executors. Collectively, semantic processors engage in dynamic exchanges of semantic tokens to progressively construct high-utility outputs. We refer to these orchestrated interactions among semantic processors, optimizing and searching in semantic space, as semantic decoding algorithms. This concept draws a direct parallel to the well-studied problem of s
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;LexiContrastive Grounding (LCG)&#65292;&#23427;&#32467;&#21512;&#20102;&#35270;&#35273;&#30417;&#30563;&#21644;&#25991;&#26412;&#34920;&#31034;&#25913;&#36827;&#31574;&#30053;&#65292;&#22312;&#22810;&#20010;&#21333;&#35789;&#23398;&#20064;&#21644;&#21477;&#23376;&#29702;&#35299;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#27604;&#26631;&#20934;&#35821;&#35328;&#27169;&#22411;&#26356;&#39640;&#30340;&#23398;&#20064;&#25928;&#29575;&#21644;&#36827;&#27493;&#12290;</title><link>https://arxiv.org/abs/2403.14551</link><description>&lt;p&gt;
&#35789;&#27719;&#32423;&#23545;&#27604;&#35270;&#35273;&#22522;&#30784;&#25913;&#36827;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Lexicon-Level Contrastive Visual-Grounding Improves Language Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14551
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;LexiContrastive Grounding (LCG)&#65292;&#23427;&#32467;&#21512;&#20102;&#35270;&#35273;&#30417;&#30563;&#21644;&#25991;&#26412;&#34920;&#31034;&#25913;&#36827;&#31574;&#30053;&#65292;&#22312;&#22810;&#20010;&#21333;&#35789;&#23398;&#20064;&#21644;&#21477;&#23376;&#29702;&#35299;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#27604;&#26631;&#20934;&#35821;&#35328;&#27169;&#22411;&#26356;&#39640;&#30340;&#23398;&#20064;&#25928;&#29575;&#21644;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20170;&#22825;&#26368;&#20934;&#30830;&#30340;&#35821;&#35328;&#27169;&#22411;&#26159;&#22312;&#27604;&#20154;&#31867;&#35821;&#35328;&#23398;&#20064;&#32773;&#25509;&#25910;&#21040;&#30340;&#35821;&#35328;&#25968;&#25454;&#37327;&#22810;&#24471;&#22810;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#30340;&#65292;&#20294;&#24182;&#27809;&#26377;&#26469;&#33258;&#22312;&#20154;&#31867;&#23398;&#20064;&#20013;&#36215;&#20851;&#38190;&#20316;&#29992;&#30340;&#20854;&#20182;&#24863;&#23448;&#27169;&#24335;&#30340;&#30417;&#30563;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;LexiContrastive Grounding (LCG)&#65292;&#19968;&#31181;&#21033;&#29992;&#35270;&#35273;&#30417;&#30563;&#26469;&#25913;&#36827;&#25991;&#26412;&#34920;&#24449;&#30340;&#22522;&#20110;&#22320;&#38754;&#35821;&#35328;&#23398;&#20064;&#31243;&#24207;&#12290;LexiContrastive Grounding&#23558;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#31574;&#30053;&#19982;&#23545;&#27604;&#35270;&#35273;&#22522;&#30784;&#30446;&#26631;&#32467;&#21512;&#36215;&#26469;&#65292;&#37325;&#28857;&#25918;&#22312;&#32534;&#30721;&#35789;&#27719;&#20449;&#24687;&#30340;&#26089;&#26399;&#23618;&#34920;&#31034;&#19978;&#12290;&#22312;&#22810;&#20010;&#21333;&#35789;&#23398;&#20064;&#21644;&#21477;&#23376;&#29702;&#35299;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;LexiContrastive Grounding&#19981;&#20165;&#22312;&#23398;&#20064;&#25928;&#29575;&#19978;&#20248;&#20110;&#26631;&#20934;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#19988;&#22312;&#35270;&#35273;&#19982;&#35821;&#35328;&#23398;&#20064;&#31243;&#24207;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14551v1 Announce Type: cross  Abstract: Today's most accurate language models are trained on orders of magnitude more language data than human language learners receive - but with no supervision from other sensory modalities that play a crucial role in human learning. Can we make LMs' representations and predictions more accurate (and more human-like) with more ecologically plausible supervision? This paper describes LexiContrastive Grounding (LCG), a grounded language learning procedure that leverages visual supervision to improve textual representations. LexiContrastive Grounding combines a next token prediction strategy with a contrastive visual grounding objective, focusing on early-layer representations that encode lexical information. Across multiple word-learning and sentence-understanding benchmarks, LexiContrastive Grounding not only outperforms standard language-only models in learning efficiency, but also improves upon vision-and-language learning procedures inclu
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#22522;&#20110;&#29109;&#30340;&#21160;&#24577;&#28201;&#24230;&#37319;&#26679;&#26041;&#27861;&#65292;&#26412;&#25991;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#20013;&#23454;&#29616;&#20102;&#26356;&#24179;&#34913;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#24182;&#22312;&#22235;&#20010;&#19981;&#21516;&#29983;&#25104;&#22522;&#20934;&#19978;&#23637;&#31034;&#20102;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#31574;&#30053;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.14541</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#29109;&#30340;&#21160;&#24577;&#28201;&#24230;&#37319;&#26679;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
EDT: Improving Large Language Models' Generation by Entropy-based Dynamic Temperature Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14541
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#22522;&#20110;&#29109;&#30340;&#21160;&#24577;&#28201;&#24230;&#37319;&#26679;&#26041;&#27861;&#65292;&#26412;&#25991;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#20013;&#23454;&#29616;&#20102;&#26356;&#24179;&#34913;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#24182;&#22312;&#22235;&#20010;&#19981;&#21516;&#29983;&#25104;&#22522;&#20934;&#19978;&#23637;&#31034;&#20102;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#31574;&#30053;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#19979;&#28216;&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#28201;&#24230;&#37319;&#26679;&#26159;LLMs&#29983;&#25104;&#36807;&#31243;&#20013;&#24120;&#29992;&#30340;&#35299;&#30721;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#20351;&#29992;&#22266;&#23450;&#30340;&#28201;&#24230;&#21442;&#25968;&#65292;&#36825;&#21487;&#33021;&#24182;&#38750;&#22987;&#32456;&#26159;&#24179;&#34913;&#29983;&#25104;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#30340;&#26368;&#20339;&#36873;&#25321;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#22522;&#20110;&#29109;&#30340;&#21160;&#24577;&#28201;&#24230;&#65288;EDT&#65289;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#36873;&#25321;&#28201;&#24230;&#21442;&#25968;&#23454;&#29616;&#22312;&#29983;&#25104;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#26356;&#24179;&#34913;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;4&#20010;&#19981;&#21516;&#29983;&#25104;&#22522;&#20934;&#30340;&#27169;&#22411;&#24615;&#33021;&#21644;&#20840;&#38754;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;EDT&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14541v1 Announce Type: new  Abstract: Recently, Large Language Models (LLMs) have demonstrated outstanding performance across a wide range of downstream language tasks. Temperature sampling is a commonly used decoding strategy for LLMs' generation process. However, a fixed temperature parameter is used in most cases, which may not always be an optimal choice for balancing generation quality and diversity. In this paper, we propose an effective Entropy-based Dynamic Temperature (EDT) Sampling method, to achieve a more balanced performance in terms of both generation quality and diversity by dynamically selecting the temperature parameter. Additionally, we also show model performance and comprehensive analyses for 4 different generation benchmarks. Our experiments show that EDT significantly outperforms the existing strategies across different tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23581;&#35797;&#26500;&#24314;&#19968;&#27454;&#29992;&#20110;&#24052;&#35199;&#22303;&#33879;&#35821;&#35328;&#23398;&#20064;&#30340;&#35821;&#35328;&#23398;&#20064;&#28216;&#25103;&#65292;&#24182;&#25506;&#35752;&#20102;&#30456;&#20851;&#25361;&#25112;&#65292;&#35774;&#35745;&#20102;&#20855;&#26377;&#28216;&#25103;&#21270;&#29305;&#28857;&#30340;&#24037;&#20855;&#65292;&#23454;&#29616;&#20102;&#20174;&#20381;&#23384;&#26641;&#24211;&#21644;&#22270;&#27604;&#23433;&#35821;&#35328;&#30340;&#35789;&#27719;&#25968;&#25454;&#24211;&#33258;&#21160;&#29983;&#25104;&#35821;&#35328;&#32451;&#20064;&#21644;&#38382;&#39064;&#65292;&#24182;&#24378;&#35843;&#20102;&#19982;&#22303;&#33879;&#31038;&#21306;&#24314;&#31435;&#21512;&#20316;&#20851;&#31995;&#21644;&#20197;&#25945;&#32946;&#20026;&#30446;&#30340;&#30340;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.14515</link><description>&lt;p&gt;
&#20026;&#24052;&#35199;&#22303;&#33879;&#35821;&#35328;&#26500;&#24314;&#35821;&#35328;&#23398;&#20064;&#28216;&#25103;&#65306;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Building a Language-Learning Game for Brazilian Indigenous Languages: A Case of Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23581;&#35797;&#26500;&#24314;&#19968;&#27454;&#29992;&#20110;&#24052;&#35199;&#22303;&#33879;&#35821;&#35328;&#23398;&#20064;&#30340;&#35821;&#35328;&#23398;&#20064;&#28216;&#25103;&#65292;&#24182;&#25506;&#35752;&#20102;&#30456;&#20851;&#25361;&#25112;&#65292;&#35774;&#35745;&#20102;&#20855;&#26377;&#28216;&#25103;&#21270;&#29305;&#28857;&#30340;&#24037;&#20855;&#65292;&#23454;&#29616;&#20102;&#20174;&#20381;&#23384;&#26641;&#24211;&#21644;&#22270;&#27604;&#23433;&#35821;&#35328;&#30340;&#35789;&#27719;&#25968;&#25454;&#24211;&#33258;&#21160;&#29983;&#25104;&#35821;&#35328;&#32451;&#20064;&#21644;&#38382;&#39064;&#65292;&#24182;&#24378;&#35843;&#20102;&#19982;&#22303;&#33879;&#31038;&#21306;&#24314;&#31435;&#21512;&#20316;&#20851;&#31995;&#21644;&#20197;&#25945;&#32946;&#20026;&#30446;&#30340;&#30340;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20026;&#24052;&#35199;&#22303;&#33879;&#35821;&#35328;&#26500;&#24314;&#35821;&#35328;&#23398;&#20064;&#28216;&#25103;&#30340;&#21021;&#27493;&#23581;&#35797;&#20197;&#21450;&#22260;&#32469;&#27492;&#38382;&#39064;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#21576;&#29616;&#20102;&#19968;&#31181;&#24102;&#26377;&#28216;&#25103;&#21270;&#29305;&#28857;&#30340;&#24037;&#20855;&#35774;&#35745;&#12290;&#28982;&#21518;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#36807;&#31243;&#65292;&#20174;&#20381;&#23384;&#26641;&#24211;&#21644;&#19968;&#20010;&#22270;&#27604;&#23433;&#35821;&#35328;&#30340;&#35789;&#27719;&#25968;&#25454;&#24211;&#33258;&#21160;&#29983;&#25104;&#35821;&#35328;&#32451;&#20064;&#21644;&#38382;&#39064;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#21407;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#31361;&#20986;&#20102;&#20262;&#29702;&#21644;&#23454;&#38469;&#23454;&#26045;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#26032;&#30340;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#24212;&#19982;&#22303;&#33879;&#31038;&#21306;&#24314;&#31435;&#21512;&#20316;&#20851;&#31995;&#65292;&#24182;&#20197;&#25945;&#32946;&#20026;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14515v1 Announce Type: new  Abstract: In this paper we discuss a first attempt to build a language learning game for brazilian indigenous languages and the challenges around it. We present a design for the tool with gamification aspects. Then we describe a process to automatically generate language exercises and questions from a dependency treebank and a lexical database for Tupian languages. We discuss the limitations of our prototype highlighting ethical and practical implementation concerns. Finally, we conclude that new data gathering processes should be established in partnership with indigenous communities and oriented for educational purposes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21435;&#27602;&#21270;&#65292;&#22312;&#26500;&#24314;&#20102;SafeEdit&#22522;&#20934;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861; DINM&#65292;&#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#35843;&#25972;&#27493;&#39588;&#20943;&#23569;&#27169;&#22411;&#30340;&#27602;&#24615;&#65292;&#21516;&#26102;&#23545;&#21508;&#31181;&#21435;&#27602;&#26041;&#27861;&#30340;&#20869;&#37096;&#26426;&#21046;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.14472</link><description>&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#32534;&#36753;&#23454;&#29616;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21435;&#27602;&#21270;
&lt;/p&gt;
&lt;p&gt;
Detoxifying Large Language Models via Knowledge Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21435;&#27602;&#21270;&#65292;&#22312;&#26500;&#24314;&#20102;SafeEdit&#22522;&#20934;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861; DINM&#65292;&#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#35843;&#25972;&#27493;&#39588;&#20943;&#23569;&#27169;&#22411;&#30340;&#27602;&#24615;&#65292;&#21516;&#26102;&#23545;&#21508;&#31181;&#21435;&#27602;&#26041;&#27861;&#30340;&#20869;&#37096;&#26426;&#21046;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#26469;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#21435;&#27602;&#21270;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;SafeEdit&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#20061;&#31181;&#19981;&#23433;&#20840;&#31867;&#21035;&#65292;&#20855;&#26377;&#21508;&#31181;&#24378;&#22823;&#30340;&#25915;&#20987;&#25552;&#31034;&#65292;&#24182;&#37197;&#22791;&#20102;&#20840;&#38754;&#30340;&#24230;&#37327;&#26631;&#20934;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#19982;&#20043;&#21069;&#30340;&#22522;&#20934;&#32447;&#65292;&#32467;&#26524;&#34920;&#26126;&#30693;&#35782;&#32534;&#36753;&#26377;&#28508;&#21147;&#22312;&#23545;LLMs&#36827;&#34892;&#21435;&#27602;&#21270;&#26102;&#65292;&#22312;&#23545;&#19968;&#33324;&#24615;&#33021;&#30340;&#24433;&#21709;&#30456;&#23545;&#26377;&#38480;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#22522;&#20934;&#32447;&#65292;&#31216;&#20026;&#36890;&#36807;&#26415;&#20013;&#31070;&#32463;&#30417;&#27979;&#21435;&#27602;&#21270;&#65288;DINM&#65289;&#65292;&#36890;&#36807;&#20165;&#19968;&#27425;&#23454;&#20363;&#30340;&#23569;&#37327;&#35843;&#25972;&#27493;&#39588;&#20943;&#23569;LLMs&#30340;&#27602;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;&#21508;&#31181;&#21435;&#27602;&#26041;&#27861;&#30340;&#20869;&#37096;&#26426;&#21046;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#34920;&#26126;&#20808;&#21069;&#30340;&#26041;&#27861;&#22914;SFT&#21644;DPO&#21487;&#33021;&#20165;&#25233;&#21046;&#26377;&#27602;&#21442;&#25968;&#30340;&#28608;&#27963;&#65292;&#32780;DINM&#21017;&#20943;&#36731;&#26377;&#27602;&#21442;&#25968;&#30340;&#27602;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14472v1 Announce Type: cross  Abstract: This paper investigates using knowledge editing techniques to detoxify Large Language Models (LLMs). We construct a benchmark, SafeEdit, which covers nine unsafe categories with various powerful attack prompts and equips comprehensive metrics for systematic evaluation. We conduct experiments to compare knowledge editing approaches with previous baselines, indicating that knowledge editing has the potential to efficiently detoxify LLMs with limited impact on general performance. Then, we propose a simple yet effective baseline, dubbed Detoxifying with Intraoperative Neural Monitoring (DINM), to diminish the toxicity of LLMs within a few tuning steps via only one instance. We further provide an in-depth analysis of the internal mechanism for various detoxify approaches, demonstrating that previous methods like SFT and DPO may merely suppress the activations of toxic parameters, while DINM mitigates the toxicity of the toxic parameters to
&lt;/p&gt;</description></item><item><title>&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#39046;&#22495;&#23637;&#29616;&#20986;&#24378;&#22823;&#33021;&#21147;&#65292;ChatGPT&#26159;&#19968;&#20010;&#22522;&#20110;LLMs&#30340;&#24378;&#22823;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#21512;&#20316;&#25512;&#21160;&#19979;&#65292;LLM&#30740;&#31350;&#39046;&#22495;&#19981;&#26029;&#21462;&#24471;&#26032;&#31361;&#30772;&#65292;&#39044;&#31034;&#30528;&#20154;&#24037;&#26234;&#33021;&#31038;&#21306;&#23558;&#36814;&#26469;&#38761;&#21629;&#24615;&#21464;&#38761;&#12290;</title><link>https://arxiv.org/abs/2403.14469</link><description>&lt;p&gt;
ChatGPT&#22791;&#36873;&#26041;&#26696;&#65306;&#22823;&#35821;&#35328;&#27169;&#22411;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
ChatGPT Alternative Solutions: Large Language Models Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14469
&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#39046;&#22495;&#23637;&#29616;&#20986;&#24378;&#22823;&#33021;&#21147;&#65292;ChatGPT&#26159;&#19968;&#20010;&#22522;&#20110;LLMs&#30340;&#24378;&#22823;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#21512;&#20316;&#25512;&#21160;&#19979;&#65292;LLM&#30740;&#31350;&#39046;&#22495;&#19981;&#26029;&#21462;&#24471;&#26032;&#31361;&#30772;&#65292;&#39044;&#31034;&#30528;&#20154;&#24037;&#26234;&#33021;&#31038;&#21306;&#23558;&#36814;&#26469;&#38761;&#21629;&#24615;&#21464;&#38761;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#30340;&#26102;&#20195;&#65292;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22766;&#20029;&#19981;&#20165;&#38378;&#32768;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#32780;&#19988;&#20063;&#23558;&#20854;&#20809;&#36745;&#27922;&#21521;&#24191;&#27867;&#30340;&#24212;&#29992;&#39046;&#22495;&#12290;&#36825;&#31181;&#23545;LLM&#33021;&#21147;&#30340;&#26174;&#33879;&#23637;&#31034;&#24341;&#21457;&#20102;&#35813;&#39046;&#22495;&#20869;&#30740;&#31350;&#36129;&#29486;&#30340;&#28608;&#22686;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#20027;&#39064;&#30340;&#22810;&#26679;&#21270;&#20809;&#35889;&#12290;&#36825;&#20123;&#36129;&#29486;&#28085;&#30422;&#20102;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#36827;&#27493;&#12289;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#22686;&#24378;&#12289;&#27169;&#22411;&#23545;&#40784;&#12289;&#35757;&#32451;&#25968;&#25454;&#38598;&#12289;&#22522;&#20934;&#27979;&#35797;&#12289;&#25928;&#29575;&#25913;&#36827;&#31561;&#12290;&#36817;&#24180;&#26469;&#65292;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#20043;&#38388;&#24418;&#25104;&#20102;&#21160;&#24577;&#30340;&#21327;&#21516;&#20851;&#31995;&#65292;&#25512;&#21160;&#20102;LLM&#30740;&#31350;&#39046;&#22495;&#21521;&#26032;&#39640;&#24230;&#36808;&#36827;&#12290;&#36825;&#19968;&#26053;&#31243;&#20013;&#30340;&#19968;&#20010;&#20540;&#24471;&#27880;&#24847;&#30340;&#37324;&#31243;&#30865;&#26159;ChatGPT&#30340;&#25512;&#20986;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;LLMs&#30340;&#24378;&#22823;&#20154;&#24037;&#26234;&#33021;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#31038;&#20250;&#20851;&#27880;&#12290;LLMs&#30340;&#19981;&#26029;&#21457;&#23637;&#25216;&#26415;&#24050;&#32463;&#24320;&#22987;&#37325;&#22609;&#25972;&#20010;&#20154;&#24037;&#26234;&#33021;&#31038;&#21306;&#30340;&#26684;&#23616;&#65292;&#25215;&#35834;&#36827;&#34892;&#38761;&#21629;&#24615;&#30340;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14469v1 Announce Type: cross  Abstract: In recent times, the grandeur of Large Language Models (LLMs) has not only shone in the realm of natural language processing but has also cast its brilliance across a vast array of applications. This remarkable display of LLM capabilities has ignited a surge in research contributions within this domain, spanning a diverse spectrum of topics. These contributions encompass advancements in neural network architecture, context length enhancements, model alignment, training datasets, benchmarking, efficiency improvements, and more. Recent years have witnessed a dynamic synergy between academia and industry, propelling the field of LLM research to new heights. A notable milestone in this journey is the introduction of ChatGPT, a powerful AI chatbot grounded in LLMs, which has garnered widespread societal attention. The evolving technology of LLMs has begun to reshape the landscape of the entire AI community, promising a revolutionary shift i
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#23558;&#31639;&#27861;&#24615;&#32034;&#36180;&#30340;&#27010;&#24565;&#24310;&#20280;&#21040;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#21160;&#24577;&#35774;&#32622;&#27602;&#24615;&#36807;&#28388;&#38408;&#20540;&#30340;&#26032;&#26426;&#21046;&#65292;&#20351;&#20182;&#20204;&#33021;&#22815;&#23454;&#29616;&#20182;&#20204;&#26399;&#26395;&#30340;&#39044;&#27979;&#65292;&#20174;&#32780;&#22686;&#21152;&#20182;&#20204;&#30340;&#20195;&#29702;&#26435;&#12290;</title><link>https://arxiv.org/abs/2403.14467</link><description>&lt;p&gt;
&#37325;&#26032;&#32034;&#21462;&#30340;&#26435;&#21033;&#65306;&#19982;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
Recourse for reclamation: Chatting with generative language models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14467
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#23558;&#31639;&#27861;&#24615;&#32034;&#36180;&#30340;&#27010;&#24565;&#24310;&#20280;&#21040;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#21160;&#24577;&#35774;&#32622;&#27602;&#24615;&#36807;&#28388;&#38408;&#20540;&#30340;&#26032;&#26426;&#21046;&#65292;&#20351;&#20182;&#20204;&#33021;&#22815;&#23454;&#29616;&#20182;&#20204;&#26399;&#26395;&#30340;&#39044;&#27979;&#65292;&#20174;&#32780;&#22686;&#21152;&#20182;&#20204;&#30340;&#20195;&#29702;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#32773;&#36234;&#26469;&#36234;&#22810;&#22320;&#20381;&#36182;&#27602;&#24615;&#35780;&#20998;&#26469;&#35843;&#33410;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#22312;&#23458;&#25143;&#26381;&#21153;&#12289;&#20449;&#24687;&#26816;&#32034;&#21644;&#20869;&#23481;&#29983;&#25104;&#31561;&#22330;&#26223;&#20013;&#12290;&#28982;&#32780;&#65292;&#27602;&#24615;&#35780;&#20998;&#21487;&#33021;&#20351;&#30456;&#20851;&#20449;&#24687;&#26080;&#27861;&#35775;&#38382;&#65292;&#20351;&#25991;&#21270;&#35268;&#33539;&#20725;&#21270;&#25110;&#8220;&#20215;&#20540;&#38145;&#23450;&#8221;&#65292;&#38459;&#30861;&#35821;&#35328;&#37325;&#26032;&#32034;&#21462;&#36807;&#31243;&#65292;&#29305;&#21035;&#26159;&#23545;&#36793;&#32536;&#21270;&#32676;&#20307;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#31639;&#27861;&#24615;&#32034;&#36180;&#30340;&#27010;&#24565;&#24310;&#20280;&#21040;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65306;&#25105;&#20204;&#20026;&#29992;&#25143;&#25552;&#20379;&#19968;&#31181;&#26032;&#39062;&#26426;&#21046;&#65292;&#36890;&#36807;&#21160;&#24577;&#35774;&#32622;&#27602;&#24615;&#36807;&#28388;&#30340;&#38408;&#20540;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#23454;&#29616;&#20182;&#20204;&#25152;&#26399;&#26395;&#30340;&#39044;&#27979;&#12290;&#29992;&#25143;&#22240;&#27492;&#30456;&#23545;&#20110;&#19982;&#22522;&#32447;&#31995;&#32479;&#30340;&#20132;&#20114;&#65292;&#21487;&#20197;&#34892;&#20351;&#26356;&#22810;&#30340;&#20195;&#29702;&#26435;&#12290;&#19968;&#39033;&#35797;&#28857;&#30740;&#31350;($n=30$)&#25903;&#25345;&#25105;&#20204;&#25552;&#20986;&#30340;&#32034;&#36180;&#26426;&#21046;&#30340;&#28508;&#21147;&#65292;&#34920;&#26126;&#19982;&#22266;&#23450;&#38408;&#20540;&#27602;&#24615;&#36807;&#28388;&#27169;&#22411;&#36755;&#20986;&#30456;&#27604;&#65292;&#22312;&#21487;&#29992;&#24615;&#26041;&#38754;&#26377;&#25152;&#25913;&#36827;&#12290;&#26410;&#26469;&#30340;&#24037;&#20316;&#24212;&#35813;&#25506;&#32034;&#27602;&#24615;&#35780;&#20998;&#19982;&#35821;&#35328;&#37325;&#26032;&#32034;&#21462;&#20043;&#38388;&#30340;&#20132;&#21449;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14467v1 Announce Type: cross  Abstract: Researchers and developers increasingly rely on toxicity scoring to moderate generative language model outputs, in settings such as customer service, information retrieval, and content generation. However, toxicity scoring may render pertinent information inaccessible, rigidify or "value-lock" cultural norms, and prevent language reclamation processes, particularly for marginalized people. In this work, we extend the concept of algorithmic recourse to generative language models: we provide users a novel mechanism to achieve their desired prediction by dynamically setting thresholds for toxicity filtering. Users thereby exercise increased agency relative to interactions with the baseline system. A pilot study ($n = 30$) supports the potential of our proposed recourse mechanism, indicating improvements in usability compared to fixed-threshold toxicity-filtering of model outputs. Future work should explore the intersection of toxicity sco
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#21644;&#29305;&#24449;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#25628;&#32034;&#21644;&#20248;&#21270;&#30340;&#36807;&#31243;&#20013;&#20986;&#29616;&#26368;&#32456;&#26550;&#26500;&#65292;&#21516;&#26102;&#20445;&#25345;&#21333;&#19968;&#31995;&#32479;&#38169;&#35273;&#29305;&#24615;&#65292;&#24182;&#23558;&#29616;&#20195;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24341;&#20837;&#20854;&#20013;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#36710;&#36742;&#36719;&#20214;&#31995;&#32479;&#30340;&#33258;&#21160;&#21270;&#24320;&#21457;&#12290;</title><link>https://arxiv.org/abs/2403.14460</link><description>&lt;p&gt;
&#23454;&#29616;&#36719;&#20214;&#23450;&#20041;&#36710;&#36742;&#20013;&#30340;&#21333;&#19968;&#31995;&#32479;&#38169;&#35273; -- &#33258;&#21160;&#21270;&#12289;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#24037;&#20316;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
Towards Single-System Illusion in Software-Defined Vehicles -- Automated, AI-Powered Workflow
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14460
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#21644;&#29305;&#24449;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#25628;&#32034;&#21644;&#20248;&#21270;&#30340;&#36807;&#31243;&#20013;&#20986;&#29616;&#26368;&#32456;&#26550;&#26500;&#65292;&#21516;&#26102;&#20445;&#25345;&#21333;&#19968;&#31995;&#32479;&#38169;&#35273;&#29305;&#24615;&#65292;&#24182;&#23558;&#29616;&#20195;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24341;&#20837;&#20854;&#20013;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#36710;&#36742;&#36719;&#20214;&#31995;&#32479;&#30340;&#33258;&#21160;&#21270;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#21644;&#29305;&#24449;&#30340;&#26032;&#39062;&#26041;&#27861;&#26469;&#24320;&#21457;&#36710;&#36742;&#36719;&#20214;&#31995;&#32479;&#65292;&#20854;&#20013;&#26368;&#32456;&#26550;&#26500;&#24182;&#26410;&#26126;&#30830;&#23450;&#20041;&#12290;&#30456;&#21453;&#65292;&#23427;&#26159;&#20174;&#19968;&#31995;&#21015;&#25628;&#32034;&#21644;&#20248;&#21270;&#30340;&#36845;&#20195;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#65292;&#26377;&#29305;&#23450;&#30340;&#32422;&#26463;&#12289;&#38656;&#27714;&#21644;&#30828;&#20214;&#26550;&#26500;&#65292;&#21516;&#26102;&#20445;&#25345;&#21333;&#19968;&#31995;&#32479;&#38169;&#35273;&#30340;&#29305;&#24615;&#65292;&#20854;&#20013;&#24212;&#29992;&#22312;&#36923;&#36753;&#19978;&#32479;&#19968;&#30340;&#29615;&#22659;&#20013;&#36816;&#34892;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#28857;&#26159;&#22312;&#24490;&#29615;&#20013;&#21253;&#21547;&#29616;&#20195;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#38543;&#30528;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25105;&#20204;&#26399;&#26395;LLMs&#33021;&#22815;&#36741;&#21161;&#22788;&#29702;&#38656;&#27714;&#12289;&#29983;&#25104;&#24418;&#24335;&#31995;&#32479;&#27169;&#22411;&#65292;&#20197;&#21450;&#29983;&#25104;&#36719;&#20214;&#37096;&#32626;&#35268;&#33539;&#21644;&#27979;&#35797;&#20195;&#30721;&#12290;&#32467;&#26524;&#31649;&#36947;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#33258;&#21160;&#21270;&#30340;&#65292;&#27599;&#19968;&#27493;&#37117;&#20250;&#29983;&#25104;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14460v1 Announce Type: cross  Abstract: We propose a novel model- and feature-based approach to development of vehicle software systems, where the end architecture is not explicitly defined. Instead, it emerges from an iterative process of search and optimization given certain constraints, requirements and hardware architecture, while retaining the property of single-system illusion, where applications run in a logically uniform environment. One of the key points of the presented approach is the inclusion of modern generative AI, specifically Large Language Models (LLMs), in the loop. With the recent advances in the field, we expect that the LLMs will be able to assist in processing of requirements, generation of formal system models, as well as generation of software deployment specification and test code. The resulting pipeline is automated to a large extent, with feedback being generated at each step.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MExGen&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26631;&#37327;&#21270;&#27010;&#24565;&#21644;&#22810;&#32423;&#26041;&#27861;&#22788;&#29702;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#25361;&#25112;&#65292;&#35777;&#26126;&#21487;&#20197;&#25552;&#20379;&#26356;&#36148;&#36817;&#26412;&#22320;&#30340;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2403.14459</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#32423;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Multi-Level Explanations for Generative Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MExGen&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26631;&#37327;&#21270;&#27010;&#24565;&#21644;&#22810;&#32423;&#26041;&#27861;&#22788;&#29702;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#25361;&#25112;&#65292;&#35777;&#26126;&#21487;&#20197;&#25552;&#20379;&#26356;&#36148;&#36817;&#26412;&#22320;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25200;&#21160;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#22914;LIME&#21644;SHAP&#65292;&#36890;&#24120;&#24212;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#12290;&#26412;&#25991;&#20851;&#27880;&#23427;&#20204;&#22914;&#20309;&#25193;&#23637;&#21040;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#25991;&#26412;&#20316;&#20026;&#36755;&#20986;&#21644;&#38271;&#25991;&#26412;&#36755;&#20837;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MExGen&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#21487;&#20197;&#29992;&#19981;&#21516;&#30340;&#24402;&#22240;&#31639;&#27861;&#23454;&#20363;&#21270;&#12290;&#20026;&#20102;&#22788;&#29702;&#25991;&#26412;&#36755;&#20986;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23558;&#25991;&#26412;&#26144;&#23556;&#21040;&#23454;&#25968;&#30340;&#26631;&#37327;&#21270;&#27010;&#24565;&#65292;&#24182;&#25506;&#35752;&#20102;&#22810;&#31181;&#21487;&#33021;&#24615;&#12290;&#20026;&#20102;&#22788;&#29702;&#38271;&#36755;&#20837;&#65292;&#25105;&#20204;&#37319;&#29992;&#22810;&#32423;&#26041;&#27861;&#65292;&#20174;&#31895;&#31890;&#24230;&#21040;&#32454;&#31890;&#24230;&#65292;&#37325;&#28857;&#20851;&#27880;&#20855;&#26377;&#27169;&#22411;&#26597;&#35810;&#32447;&#24615;&#32553;&#25918;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#23545;&#22522;&#20110;&#25200;&#21160;&#30340;&#24402;&#22240;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#65292;&#21253;&#25324;&#33258;&#21160;&#21270;&#21644;&#20154;&#24037;&#35780;&#20272;&#65292;&#29992;&#20110;&#25688;&#35201;&#21644;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#38382;&#31572;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#25552;&#20379;&#26356;&#21152;&#36148;&#36817;&#26412;&#22320;&#30340;&#29983;&#25104;&#24335;&#36755;&#20986;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14459v1 Announce Type: cross  Abstract: Perturbation-based explanation methods such as LIME and SHAP are commonly applied to text classification. This work focuses on their extension to generative language models. To address the challenges of text as output and long text inputs, we propose a general framework called MExGen that can be instantiated with different attribution algorithms. To handle text output, we introduce the notion of scalarizers for mapping text to real numbers and investigate multiple possibilities. To handle long inputs, we take a multi-level approach, proceeding from coarser levels of granularity to finer ones, and focus on algorithms with linear scaling in model queries. We conduct a systematic evaluation, both automated and human, of perturbation-based attribution methods for summarization and context-grounded question answering. The results show that our framework can provide more locally faithful explanations of generated outputs.
&lt;/p&gt;</description></item><item><title>gTBLS&#36890;&#36807;&#20004;&#38454;&#27573;&#26041;&#27861;&#20174;&#25991;&#26412;&#20013;&#29983;&#25104;&#34920;&#26684;&#65292;&#31532;&#19968;&#38454;&#27573;&#25512;&#26029;&#34920;&#26684;&#32467;&#26500;&#65292;&#31532;&#20108;&#38454;&#27573;&#21033;&#29992;&#32467;&#26500;&#25552;&#20986;&#38382;&#39064;&#24182;&#36890;&#36807;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#26469;&#22238;&#31572;&#65292;&#33021;&#22815;&#22312;&#38646;&#30701;&#37197;&#32622;&#19979;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25913;&#36827;&#20102;&#20808;&#21069;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.14457</link><description>&lt;p&gt;
gTBLS&#65306;&#36890;&#36807;&#26465;&#20214;&#38382;&#31572;&#20174;&#25991;&#26412;&#20013;&#29983;&#25104;&#34920;&#26684;
&lt;/p&gt;
&lt;p&gt;
gTBLS: Generating Tables from Text by Conditional Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14457
&lt;/p&gt;
&lt;p&gt;
gTBLS&#36890;&#36807;&#20004;&#38454;&#27573;&#26041;&#27861;&#20174;&#25991;&#26412;&#20013;&#29983;&#25104;&#34920;&#26684;&#65292;&#31532;&#19968;&#38454;&#27573;&#25512;&#26029;&#34920;&#26684;&#32467;&#26500;&#65292;&#31532;&#20108;&#38454;&#27573;&#21033;&#29992;&#32467;&#26500;&#25552;&#20986;&#38382;&#39064;&#24182;&#36890;&#36807;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#26469;&#22238;&#31572;&#65292;&#33021;&#22815;&#22312;&#38646;&#30701;&#37197;&#32622;&#19979;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25913;&#36827;&#20102;&#20808;&#21069;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14457v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#23558;&#22823;&#27573;&#30340;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#25552;&#28860;&#20026;&#32467;&#26500;&#21270;&#12289;&#31616;&#21270;&#30340;&#24418;&#24335;&#65292;&#22914;&#34920;&#26684;&#65292;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#33258;&#21160;&#29983;&#25104;&#34920;&#26684;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26159;&#30830;&#20445;&#20854;&#21477;&#27861;&#26377;&#25928;&#24615;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;Transformer&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#21253;&#21547;&#39069;&#22806;&#30340;&#21442;&#25968;&#65292;&#20197;&#20415;&#27880;&#24847;&#29305;&#23450;&#30340;&#34892;&#21644;&#21015;&#26631;&#39064;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#19982;&#36825;&#31181;&#21333;&#38454;&#27573;&#26041;&#27861;&#30456;&#21453;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#29983;&#25104;&#24335;&#34920;&#26684;&#65288;gTBLS&#65289;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#12290;&#31532;&#19968;&#38454;&#27573;&#20174;&#25991;&#26412;&#20013;&#25512;&#26029;&#34920;&#26684;&#32467;&#26500;&#65288;&#34892;&#21644;&#21015;&#26631;&#39064;&#65289;&#12290;&#31532;&#20108;&#38454;&#27573;&#21033;&#29992;&#36825;&#20123;&#26631;&#39064;&#25552;&#20986;&#38382;&#39064;&#65292;&#24182;&#24494;&#35843;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#26469;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;gTBLS&#26041;&#27861;&#26131;&#20110;&#22312;&#38646;&#30701;&#37197;&#32622;&#20013;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20026;&#22312;&#19981;&#33021;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#34920;&#26684;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;gTBLS&#25552;&#39640;&#20102;&#20043;&#21069;&#26041;&#27861;&#36798;&#21040;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14457v1 Announce Type: new  Abstract: Distilling large, unstructured text into a structured, condensed form such as tables is an open research problem. One of the primary challenges in automatically generating tables is ensuring their syntactic validity. Prior approaches address this challenge by including additional parameters in the Transformer's attention mechanism to attend to specific rows and column headers. In contrast to this single-stage method, this paper presents a two-stage approach called Generative Tables (gTBLS). The first stage infers table structure (row and column headers) from the text. The second stage formulates questions using these headers and fine-tunes a causal language model to answer them. Furthermore, the gTBLS approach is amenable to the utilization of pre-trained Large Language Models in a zero-shot configuration, presenting a solution for table generation in situations where fine-tuning is not feasible. gTBLS improves prior approaches by up to 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#38024;&#23545;&#32763;&#35793;&#36807;&#31243;&#30340;&#32763;&#35793;&#25216;&#26415;&#39044;&#27979;&#65292;&#21457;&#29616;&#22312;&#33258;&#21160;&#35782;&#21035;&#21644;&#24212;&#29992;&#36866;&#24403;&#30340;&#32763;&#35793;&#25216;&#26415;&#26041;&#38754;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#20248;&#21270;&#26426;&#22120;&#32763;&#35793;&#65292;&#21462;&#24471;&#20102;82%&#21040;93%&#30340;&#39044;&#27979;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.14454</link><description>&lt;p&gt;
&#38024;&#23545;&#32763;&#35793;&#36807;&#31243;&#30340;&#32763;&#35793;&#25216;&#26415;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Prediction of Translation Techniques for the Translation Process
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14454
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#38024;&#23545;&#32763;&#35793;&#36807;&#31243;&#30340;&#32763;&#35793;&#25216;&#26415;&#39044;&#27979;&#65292;&#21457;&#29616;&#22312;&#33258;&#21160;&#35782;&#21035;&#21644;&#24212;&#29992;&#36866;&#24403;&#30340;&#32763;&#35793;&#25216;&#26415;&#26041;&#38754;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#20248;&#21270;&#26426;&#22120;&#32763;&#35793;&#65292;&#21462;&#24471;&#20102;82%&#21040;93%&#30340;&#39044;&#27979;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#28085;&#30422;&#20102;&#19968;&#31995;&#21015;&#26088;&#22312;&#25552;&#39640;&#32763;&#35793;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20154;&#24037;&#32763;&#35793;&#30340;&#36807;&#31243;&#20381;&#36182;&#20110;&#19968;&#31995;&#21015;&#32763;&#35793;&#25216;&#26415;&#65292;&#36825;&#20123;&#25216;&#26415;&#23545;&#30830;&#20445;&#35821;&#35328;&#30340;&#20805;&#20998;&#24615;&#21644;&#27969;&#30021;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#22914;&#26524;&#33021;&#22312;&#24212;&#29992;&#20043;&#21069;&#33258;&#21160;&#35782;&#21035;&#36825;&#20123;&#32763;&#35793;&#25216;&#26415;&#65292;&#23427;&#20204;&#21487;&#33021;&#36827;&#19968;&#27493;&#20248;&#21270;&#26426;&#22120;&#32763;&#35793;&#65292;&#20197;&#26377;&#25928;&#22320;&#24341;&#23548;&#32763;&#35793;&#36807;&#31243;&#12290;&#30740;&#31350;&#21306;&#20998;&#20102;&#32763;&#35793;&#36807;&#31243;&#30340;&#20004;&#31181;&#22330;&#26223;&#65306;&#20174;&#22836;&#24320;&#22987;&#32763;&#35793;&#21644;&#21518;&#26399;&#32534;&#36753;&#12290;&#38024;&#23545;&#27599;&#31181;&#22330;&#26223;&#65292;&#35774;&#35745;&#20102;&#19968;&#31995;&#21015;&#29305;&#23450;&#23454;&#39564;&#65292;&#20197;&#39044;&#27979;&#26368;&#36866;&#21512;&#30340;&#32763;&#35793;&#25216;&#26415;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20174;&#22836;&#24320;&#22987;&#32763;&#35793;&#30340;&#39044;&#27979;&#20934;&#30830;&#29575;&#36798;&#21040;82&#65285;&#65292;&#32780;&#21518;&#26399;&#32534;&#36753;&#36807;&#31243;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#28508;&#21147;&#65292;&#23454;&#29616;&#20102;93&#65285;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14454v1 Announce Type: new  Abstract: Machine translation (MT) encompasses a variety of methodologies aimed at enhancing the accuracy of translations. In contrast, the process of human-generated translation relies on a wide range of translation techniques, which are crucial for ensuring linguistic adequacy and fluency. This study suggests that these translation techniques could further optimize machine translation if they are automatically identified before being applied to guide the translation process effectively. The study differentiates between two scenarios of the translation process: from-scratch translation and post-editing. For each scenario, a specific set of experiments has been designed to forecast the most appropriate translation techniques. The findings indicate that the predictive accuracy for from-scratch translation reaches 82%, while the post-editing process exhibits even greater potential, achieving an accuracy rate of 93%.
&lt;/p&gt;</description></item><item><title>NMS&#21644;Morfessor&#22312;&#20998;&#21106;&#30001;&#19981;&#21516;&#24418;&#24577;&#36807;&#31243;&#24418;&#25104;&#30340;&#21333;&#35789;&#20013;&#34920;&#29616;&#20986;&#25104;&#21151;&#65292;NMS&#23545;&#27169;&#26495;&#21644;&#20854;&#20182;&#24418;&#24577;&#32467;&#26500;&#32447;&#32034;&#20063;&#34920;&#29616;&#20986;&#25935;&#24863;&#65292;&#26174;&#31034;&#20986;&#20182;&#20204;&#30340;&#23398;&#20064;&#36807;&#31243;&#19981;&#20165;&#20165;&#21463;&#21040;&#32479;&#35745;&#37325;&#22797;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.14444</link><description>&lt;p&gt;
&#36229;&#36234;&#32479;&#35745;&#37325;&#22797;&#65306;&#20154;&#31867;&#21644;&#26426;&#22120;&#26080;&#30417;&#30563;&#23398;&#20064;&#36328;&#24418;&#24577;&#36807;&#31243;&#30340;&#27611;&#21033;&#35821;&#35789;&#35821;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
More than Just Statistical Recurrence: Human and Machine Unsupervised Learning of M\=aori Word Segmentation across Morphological Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14444
&lt;/p&gt;
&lt;p&gt;
NMS&#21644;Morfessor&#22312;&#20998;&#21106;&#30001;&#19981;&#21516;&#24418;&#24577;&#36807;&#31243;&#24418;&#25104;&#30340;&#21333;&#35789;&#20013;&#34920;&#29616;&#20986;&#25104;&#21151;&#65292;NMS&#23545;&#27169;&#26495;&#21644;&#20854;&#20182;&#24418;&#24577;&#32467;&#26500;&#32447;&#32034;&#20063;&#34920;&#29616;&#20986;&#25935;&#24863;&#65292;&#26174;&#31034;&#20986;&#20182;&#20204;&#30340;&#23398;&#20064;&#36807;&#31243;&#19981;&#20165;&#20165;&#21463;&#21040;&#32479;&#35745;&#37325;&#22797;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#27611;&#21033;&#26063;&#35028;&#30340;&#26032;&#35199;&#20848;&#20154;(NMS)&#33021;&#22815;&#20197;&#19968;&#31181;&#19982;&#27969;&#21033;&#30340;&#27611;&#21033;&#35821;&#20351;&#29992;&#32773;&#65288;Panther&#31561;&#20154;&#65292;2024&#65289;&#38750;&#24120;&#30456;&#20284;&#30340;&#26041;&#24335;&#20998;&#21106;&#27611;&#21033;&#35821;&#21333;&#35789;&#12290;&#36825;&#31181;&#33021;&#21147;&#34987;&#35748;&#20026;&#26159;&#36890;&#36807;&#35782;&#21035;&#21644;&#25552;&#21462;&#32479;&#35745;&#37325;&#22797;&#24418;&#24335;&#32780;&#33719;&#24471;&#30340;&#12290;&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;NMS&#21644;Morfessor&#23545;&#30001;&#21508;&#31181;&#24418;&#24577;&#36807;&#31243;&#24418;&#25104;&#30340;&#21333;&#35789;&#30340;&#20998;&#21106;&#65292;&#26469;&#26816;&#39564;&#36825;&#19968;&#20551;&#35774;&#12290;NMS&#21644;Morfessor&#22343;&#25104;&#21151;&#22320;&#20998;&#21106;&#30001;&#36830;&#25509;&#36807;&#31243;&#65288;&#26500;&#35789;&#21644;&#35789;&#32512;&#21270;&#27809;&#26377;&#21464;&#38899;&#30340;&#36807;&#31243;&#65289;&#24418;&#25104;&#30340;&#21333;&#35789;&#65292;&#20294;NMS&#36824;&#25104;&#21151;&#22320;&#20998;&#21106;&#28041;&#21450;&#27169;&#26495;&#65288;&#37325;&#22797;&#21644;&#21464;&#38899;&#65289;&#21644;&#20854;&#20182;&#24418;&#24577;&#32467;&#26500;&#32447;&#32034;&#30340;&#21333;&#35789;&#65292;&#36825;&#24847;&#21619;&#30528;&#20182;&#20204;&#30340;&#23398;&#20064;&#36807;&#31243;&#23545;&#19981;&#20165;&#20165;&#26159;&#32479;&#35745;&#37325;&#22797;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14444v1 Announce Type: new  Abstract: Non-M\=aori-speaking New Zealanders (NMS)are able to segment M\=aori words in a highlysimilar way to fluent speakers (Panther et al.,2024). This ability is assumed to derive through the identification and extraction of statistically recurrent forms. We examine this assumption by asking how NMS segmentations compare to those produced by Morfessor, an unsupervised machine learning model that operates based on statistical recurrence, across words formed by a variety of morphological processes. Both NMS and Morfessor succeed in segmenting words formed by concatenative processes (compounding and affixation without allomorphy), but NMS also succeed for words that invoke templates (reduplication and allomorphy) and other cues to morphological structure, implying that their learning process is sensitive to more than just statistical recurrence.
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#26234;&#33021;&#20195;&#29702;&#22312;&#27169;&#25311;&#25968;&#23383;&#24066;&#22330;&#20013;&#23436;&#25104;&#20080;&#21334;&#20449;&#24687;&#30340;&#20219;&#21153;&#65292;&#36890;&#36807;&#20855;&#22791;&#35780;&#20272;&#20449;&#24687;&#36136;&#37327;&#21644;&#36951;&#24536;&#33021;&#21147;&#30340;&#29305;&#28857;&#65292;&#25104;&#21151;&#38477;&#20302;&#20102;&#20449;&#24687;&#24066;&#22330;&#30340;&#20080;&#26041;&#26816;&#26597;&#24726;&#35770;&#12290;</title><link>https://arxiv.org/abs/2403.14443</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#38477;&#20302;&#20449;&#24687;&#24066;&#22330;&#30340;&#19981;&#23545;&#31216;&#24615;
&lt;/p&gt;
&lt;p&gt;
Language Models Can Reduce Asymmetry in Information Markets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14443
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#26234;&#33021;&#20195;&#29702;&#22312;&#27169;&#25311;&#25968;&#23383;&#24066;&#22330;&#20013;&#23436;&#25104;&#20080;&#21334;&#20449;&#24687;&#30340;&#20219;&#21153;&#65292;&#36890;&#36807;&#20855;&#22791;&#35780;&#20272;&#20449;&#24687;&#36136;&#37327;&#21644;&#36951;&#24536;&#33021;&#21147;&#30340;&#29305;&#28857;&#65292;&#25104;&#21151;&#38477;&#20302;&#20102;&#20449;&#24687;&#24066;&#22330;&#30340;&#20080;&#26041;&#26816;&#26597;&#24726;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#35299;&#20915;&#20102;&#20449;&#24687;&#24066;&#22330;&#20013;&#20080;&#26041;&#26816;&#26597;&#24726;&#35770;&#30340;&#38382;&#39064;&#12290;&#20080;&#26041;&#38656;&#35201;&#33719;&#21462;&#20449;&#24687;&#26469;&#30830;&#23450;&#20854;&#20215;&#20540;&#65292;&#32780;&#21334;&#26041;&#38656;&#35201;&#38480;&#21046;&#35775;&#38382;&#20197;&#38450;&#27490;&#30423;&#31363;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#20010;&#24320;&#28304;&#30340;&#27169;&#25311;&#25968;&#23383;&#24066;&#22330;&#65292;&#20854;&#20013;&#30001;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#26234;&#33021;&#20195;&#29702;&#20195;&#34920;&#22806;&#37096;&#21442;&#19982;&#32773;&#20080;&#21334;&#20449;&#24687;&#12290;&#36825;&#20010;&#24066;&#22330;&#30340;&#26680;&#24515;&#26426;&#21046;&#22312;&#20110;&#20195;&#29702;&#20154;&#30340;&#21452;&#37325;&#33021;&#21147;&#65306;&#20182;&#20204;&#19981;&#20165;&#33021;&#22815;&#35780;&#20272;&#29305;&#26435;&#20449;&#24687;&#30340;&#36136;&#37327;&#65292;&#36824;&#20855;&#22791;&#36951;&#24536;&#30340;&#33021;&#21147;&#12290;&#36825;&#31181;&#35825;&#23548;&#20581;&#24536;&#30340;&#33021;&#21147;&#20351;&#20379;&#24212;&#21830;&#21487;&#20197;&#25480;&#20104;&#20020;&#26102;&#35775;&#38382;&#19987;&#26377;&#20449;&#24687;&#30340;&#26435;&#38480;&#65292;&#26174;&#33879;&#38477;&#20302;&#26410;&#32463;&#25480;&#26435;&#30340;&#20445;&#30041;&#39118;&#38505;&#65292;&#21516;&#26102;&#20351;&#20195;&#29702;&#20154;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#20449;&#24687;&#23545;&#29305;&#23450;&#26597;&#35810;&#25110;&#20219;&#21153;&#30340;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#34920;&#29616;&#20248;&#24322;&#65292;&#20195;&#29702;&#24517;&#39035;&#20570;&#20986;&#29702;&#24615;&#20915;&#31574;&#65292;&#25112;&#30053;&#24615;&#22320;&#25506;&#32034;&#24066;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14443v1 Announce Type: new  Abstract: This work addresses the buyer's inspection paradox for information markets. The paradox is that buyers need to access information to determine its value, while sellers need to limit access to prevent theft. To study this, we introduce an open-source simulated digital marketplace where intelligent agents, powered by language models, buy and sell information on behalf of external participants. The central mechanism enabling this marketplace is the agents' dual capabilities: they not only have the capacity to assess the quality of privileged information but also come equipped with the ability to forget. This ability to induce amnesia allows vendors to grant temporary access to proprietary information, significantly reducing the risk of unauthorized retention while enabling agents to accurately gauge the information's relevance to specific queries or tasks. To perform well, agents must make rational decisions, strategically explore the marke
&lt;/p&gt;</description></item><item><title>&#25506;&#32034;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35774;&#22791;&#23450;&#21521;&#35821;&#38899;&#26816;&#27979;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#25991;&#26412;&#21644;&#38899;&#39057;&#27169;&#22411;&#65292;&#20351;&#29992;&#22810;&#27169;&#24577;&#20449;&#24687;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#30456;&#31561;&#38169;&#35823;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.14438</link><description>&lt;p&gt;
&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35774;&#22791;&#23450;&#21521;&#35821;&#38899;&#26816;&#27979;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Multimodal Approach to Device-Directed Speech Detection with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14438
&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35774;&#22791;&#23450;&#21521;&#35821;&#38899;&#26816;&#27979;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#25991;&#26412;&#21644;&#38899;&#39057;&#27169;&#22411;&#65292;&#20351;&#29992;&#22810;&#27169;&#24577;&#20449;&#24687;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#30456;&#31561;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#25311;&#21161;&#25163;&#30340;&#20132;&#20114;&#36890;&#24120;&#20174;&#39044;&#23450;&#20041;&#35302;&#21457;&#30701;&#35821;&#24320;&#22987;&#65292;&#28982;&#21518;&#26159;&#29992;&#25143;&#21629;&#20196;&#12290;&#20026;&#20102;&#20351;&#19982;&#21161;&#25163;&#30340;&#20132;&#20114;&#26356;&#30452;&#35266;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26159;&#21542;&#21487;&#20197;&#25918;&#24323;&#29992;&#25143;&#24517;&#39035;&#29992;&#35302;&#21457;&#30701;&#35821;&#24320;&#22987;&#27599;&#20010;&#21629;&#20196;&#30340;&#35201;&#27714;&#12290;&#25105;&#20204;&#36890;&#36807;&#19977;&#31181;&#26041;&#24335;&#25506;&#32034;&#20102;&#36825;&#20010;&#20219;&#21153;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#20165;&#20351;&#29992;&#20174;&#38899;&#39057;&#27874;&#24418;&#20013;&#33719;&#24471;&#30340;&#22768;&#23398;&#20449;&#24687;&#35757;&#32451;&#20998;&#31867;&#22120;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23558;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#30340;&#35299;&#30721;&#22120;&#36755;&#20986;&#65292;&#20363;&#22914;1-best&#20551;&#35774;&#65292;&#20316;&#20026;&#36755;&#20837;&#29305;&#24449;&#36755;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#31995;&#32479;&#65292;&#23558;&#22768;&#23398;&#21644;&#35789;&#27719;&#29305;&#24449;&#20197;&#21450;ASR&#35299;&#30721;&#22120;&#20449;&#21495;&#32467;&#21512;&#22312;LLM&#20013;&#12290;&#20351;&#29992;&#22810;&#27169;&#24577;&#20449;&#24687;&#30456;&#23545;&#20110;&#20165;&#25991;&#26412;&#21644;&#20165;&#38899;&#39057;&#27169;&#22411;&#25552;&#39640;&#20102;&#30456;&#31561;&#38169;&#35823;&#29575;&#39640;&#36798;39%&#21644;61%&#12290;&#22686;&#21152;LLM&#30340;&#22823;&#23567;&#24182;&#36890;&#36807;&#20302;&#31209;&#35843;&#25972;&#36827;&#34892;&#35757;&#32451;&#36827;&#19968;&#27493;&#20943;&#23569;&#20102;&#30456;&#23545;EER&#20540;&#30340;&#20943;&#23569;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14438v1 Announce Type: new  Abstract: Interactions with virtual assistants typically start with a predefined trigger phrase followed by the user command. To make interactions with the assistant more intuitive, we explore whether it is feasible to drop the requirement that users must begin each command with a trigger phrase. We explore this task in three ways: First, we train classifiers using only acoustic information obtained from the audio waveform. Second, we take the decoder outputs of an automatic speech recognition (ASR) system, such as 1-best hypotheses, as input features to a large language model (LLM). Finally, we explore a multimodal system that combines acoustic and lexical features, as well as ASR decoder signals in an LLM. Using multimodal information yields relative equal-error-rate improvements over text-only and audio-only models of up to 39% and 61%. Increasing the size of the LLM and training with low-rank adaption leads to further relative EER reductions o
&lt;/p&gt;</description></item><item><title>&#20174;&#35821;&#35328;&#36827;&#21270;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#26032;&#20852;&#27807;&#36890;&#25991;&#29486;&#65292;&#21457;&#29616;&#20854;&#22312;&#35774;&#35745;&#21644;&#35843;&#25972;&#27169;&#22411;&#20197;&#24674;&#22797;&#33258;&#28982;&#35821;&#35328;&#20013;&#21021;&#22987;&#32570;&#22833;&#30340;&#35821;&#35328;&#29616;&#35937;&#26041;&#38754;&#34920;&#29616;&#20248;&#31168;&#65292;&#25581;&#31034;&#20102;&#20851;&#38190;&#21387;&#21147;&#20419;&#20351;&#24674;&#22797;&#26368;&#21021;&#19981;&#26174;&#29616;&#30340;&#20154;&#31867;&#27169;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.14427</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32039;&#24613;&#27807;&#36890;&#21644;&#23398;&#20064;&#21387;&#21147;&#65306;&#35821;&#35328;&#36827;&#21270;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Emergent communication and learning pressures in language models: a language evolution perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14427
&lt;/p&gt;
&lt;p&gt;
&#20174;&#35821;&#35328;&#36827;&#21270;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#26032;&#20852;&#27807;&#36890;&#25991;&#29486;&#65292;&#21457;&#29616;&#20854;&#22312;&#35774;&#35745;&#21644;&#35843;&#25972;&#27169;&#22411;&#20197;&#24674;&#22797;&#33258;&#28982;&#35821;&#35328;&#20013;&#21021;&#22987;&#32570;&#22833;&#30340;&#35821;&#35328;&#29616;&#35937;&#26041;&#38754;&#34920;&#29616;&#20248;&#31168;&#65292;&#25581;&#31034;&#20102;&#20851;&#38190;&#21387;&#21147;&#20419;&#20351;&#24674;&#22797;&#26368;&#21021;&#19981;&#26174;&#29616;&#30340;&#20154;&#31867;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#31867;&#26159;&#20004;&#31181;&#23398;&#20064;&#31995;&#32479;&#12290;&#21457;&#29616;&#25110;&#20419;&#36827;&#20108;&#32773;&#20043;&#38388;&#30340;&#20849;&#21516;&#28857;&#21487;&#33021;&#20250;&#22312;&#25105;&#20204;&#29702;&#35299;&#35821;&#35328;&#30340;&#20064;&#24471;&#21644;&#28436;&#21270;&#26041;&#38754;&#21462;&#24471;&#37325;&#22823;&#31361;&#30772;&#12290;&#35768;&#22810;&#35821;&#35328;&#36827;&#21270;&#29702;&#35770;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#23398;&#20064;&#20559;&#22909;&#21644;&#23398;&#20064;&#21387;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23398;&#20064;&#21387;&#21147;&#23384;&#22312;&#30528;&#37325;&#22823;&#24046;&#24322;&#65292;&#23545;&#20110;&#20154;&#31867;&#21644;&#26426;&#22120;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26159;&#21542;&#36275;&#20197;&#21551;&#21457;&#27934;&#35265;&#24182;&#20540;&#24471;&#19982;&#20154;&#31867;&#21442;&#19982;&#32773;&#19968;&#36215;&#36827;&#34892;&#27979;&#35797;&#26159;&#20540;&#24471;&#24576;&#30097;&#30340;&#12290;&#26412;&#25991;&#20174;&#35821;&#35328;&#36827;&#21270;&#30340;&#35282;&#24230;&#23457;&#35270;&#20102;&#26032;&#20852;&#27807;&#36890;&#25991;&#29486;&#65292;&#36825;&#26159;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#19968;&#20010;&#23376;&#39046;&#22495;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#26032;&#20852;&#27807;&#36890;&#25991;&#29486;&#22312;&#35774;&#35745;&#21644;&#35843;&#25972;&#27169;&#22411;&#20197;&#24674;&#22797;&#33258;&#28982;&#35821;&#35328;&#30340;&#26368;&#21021;&#19981;&#26174;&#29616;&#30340;&#35821;&#35328;&#29616;&#35937;&#26041;&#38754;&#26377;&#26480;&#20986;&#34920;&#29616;&#12290;&#26681;&#25454;&#23545;&#25991;&#29486;&#30340;&#31616;&#35201;&#22238;&#39038;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20123;&#22312;&#26032;&#20852;&#27807;&#36890;&#20013;&#24674;&#22797;&#26368;&#21021;&#19981;&#26174;&#29616;&#30340;&#20154;&#31867;&#27169;&#24335;&#30340;&#20851;&#38190;&#21387;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14427v1 Announce Type: new  Abstract: Language models and humans are two types of learning systems. Finding or facilitating commonalities could enable major breakthroughs in our understanding of the acquisition and evolution of language. Many theories of language evolution rely heavily on learning biases and learning pressures. Yet due to substantial differences in learning pressures, it is questionable whether the similarity between humans and machines is sufficient for insights to carry over and to be worth testing with human participants. Here, we review the emergent communication literature, a subfield of multi-agent reinforcement learning, from a language evolution perspective. We find that the emergent communication literature excels at designing and adapting models to recover initially absent linguistic phenomena of natural languages. Based on a short literature review, we identify key pressures that have recovered initially absent human patterns in emergent communica
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#23450;&#20301;&#21644;&#20943;&#36731;&#20559;&#35265;&#36807;&#31243;&#34701;&#20837;&#32479;&#19968;&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#36861;&#36394;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#21516;&#32452;&#20214;&#28608;&#27963;&#30340;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20943;&#36731;&#32844;&#19994;&#20195;&#35789;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#22522;&#20110;&#30693;&#35782;&#32534;&#36753;&#30340;LSDM&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.14409</link><description>&lt;p&gt;
&#23450;&#20301;&#21644;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Locating and Mitigating Gender Bias in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14409
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#23450;&#20301;&#21644;&#20943;&#36731;&#20559;&#35265;&#36807;&#31243;&#34701;&#20837;&#32479;&#19968;&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#36861;&#36394;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#21516;&#32452;&#20214;&#28608;&#27963;&#30340;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20943;&#36731;&#32844;&#19994;&#20195;&#35789;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#22522;&#20110;&#30693;&#35782;&#32534;&#36753;&#30340;LSDM&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#24191;&#27867;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#23398;&#20064;&#20107;&#23454;&#21644;&#20154;&#31867;&#35748;&#30693;&#65292;&#20854;&#20013;&#21253;&#21547;&#20154;&#31867;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36807;&#31243;&#21487;&#33021;&#26080;&#24847;&#20013;&#23548;&#33268;&#36825;&#20123;&#27169;&#22411;&#33719;&#24471;&#31038;&#20250;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#20559;&#35265;&#21644;&#21051;&#26495;&#21360;&#35937;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#36890;&#36807;&#21333;&#19968;&#35270;&#35282;&#22788;&#29702;&#20559;&#35265;&#38382;&#39064;&#65292;&#38598;&#20013;&#20110;&#23450;&#20301;&#25110;&#20943;&#36731;&#20559;&#35265;&#12290;&#36825;&#31181;&#26377;&#38480;&#30340;&#35270;&#35282;&#22312;&#20419;&#36827;&#20559;&#35265;&#30740;&#31350;&#26041;&#38754;&#36896;&#25104;&#20102;&#38556;&#30861;&#65292;&#26080;&#27861;&#21327;&#21516;&#20114;&#34917;&#24182;&#36880;&#27493;&#31215;&#32047;&#32463;&#39564;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#23450;&#20301;&#21644;&#20943;&#36731;&#20559;&#35265;&#30340;&#36807;&#31243;&#34701;&#20837;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#20869;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#26469;&#36319;&#36394;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#21516;&#32452;&#20214;&#28608;&#27963;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;LSDM&#65288;&#26368;&#23567;&#20108;&#20056;&#20943;&#20559;&#26041;&#27861;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#32534;&#36753;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#36731;&#32844;&#19994;&#20195;&#35789;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#21644;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14409v1 Announce Type: cross  Abstract: Large language models(LLM) are pre-trained on extensive corpora to learn facts and human cognition which contain human preferences. However, this process can inadvertently lead to these models acquiring biases and stereotypes prevalent in society. Prior research has typically tackled the issue of bias through a one-dimensional perspective, concentrating either on locating or mitigating it. This limited perspective has created obstacles in facilitating research on bias to synergistically complement and progressively build upon one another. In this study, we integrate the processes of locating and mitigating bias within a unified framework. Initially, we use causal mediation analysis to trace the causal effects of different components' activation within a large language model. Building on this, we propose the LSDM (Least Square Debias Method), a knowledge-editing based method for mitigating gender bias in occupational pronouns, and compa
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;QA&#26694;&#26550;&#65292;&#26681;&#25454;&#26597;&#35810;&#30340;&#22797;&#26434;&#24230;&#21160;&#24577;&#36873;&#25321;&#36866;&#21512;&#30340;&#26816;&#32034;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#22238;&#31572;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.14403</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#26816;&#32034;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#36890;&#36807;&#38382;&#39064;&#22797;&#26434;&#24230;&#23398;&#20064;&#35843;&#36866;
&lt;/p&gt;
&lt;p&gt;
Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14403
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;QA&#26694;&#26550;&#65292;&#26681;&#25454;&#26597;&#35810;&#30340;&#22797;&#26434;&#24230;&#21160;&#24577;&#36873;&#25321;&#36866;&#21512;&#30340;&#26816;&#32034;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#22238;&#31572;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23558;&#22806;&#37096;&#30693;&#35782;&#24211;&#20013;&#30340;&#38750;&#21442;&#25968;&#30693;&#35782;&#32435;&#20837;LLMs&#65292;&#24050;&#25104;&#20026;&#25552;&#39640;&#22810;&#31181;&#20219;&#21153;&#20013;&#22238;&#31572;&#20934;&#30830;&#24615;&#30340;&#26377;&#24076;&#26395;&#26041;&#27861;&#65292;&#22914;&#38382;&#31572;&#65288;QA&#65289;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26377;&#21508;&#31181;&#26041;&#27861;&#22788;&#29702;&#19981;&#21516;&#22797;&#26434;&#24230;&#30340;&#26597;&#35810;&#65292;&#20294;&#23427;&#20204;&#35201;&#20040;&#22788;&#29702;&#31616;&#21333;&#26597;&#35810;&#26102;&#20135;&#29983;&#19981;&#24517;&#35201;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#35201;&#20040;&#26410;&#33021;&#20805;&#20998;&#35299;&#20915;&#22797;&#26434;&#30340;&#22810;&#27493;&#26597;&#35810;&#65307;&#28982;&#32780;&#65292;&#24182;&#38750;&#25152;&#26377;&#29992;&#25143;&#35831;&#27714;&#37117;&#21482;&#33021;&#21010;&#20998;&#20026;&#31616;&#21333;&#25110;&#22797;&#26434;&#31867;&#21035;&#20013;&#30340;&#19968;&#31181;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;QA&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#21160;&#24577;&#36873;&#25321;&#20174;&#26368;&#31616;&#21333;&#21040;&#26368;&#22797;&#26434;&#30340;&#65288;&#26816;&#32034;&#22686;&#24378;&#65289;LLMs&#31574;&#30053;&#65292;&#36825;&#21462;&#20915;&#20110;&#26597;&#35810;&#30340;&#22797;&#26434;&#24230;&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;&#36873;&#25321;&#36807;&#31243;&#26159;&#36890;&#36807;&#19968;&#20010;&#20998;&#31867;&#22120;&#23454;&#29616;&#30340;&#65292;&#35813;&#20998;&#31867;&#22120;&#26159;&#19968;&#20010;&#36739;&#23567;&#30340;LM&#65292;&#35757;&#32451;&#20197;&#39044;&#27979;&#20256;&#20837;&#26597;&#35810;&#30340;&#22797;&#26434;&#24230;&#32423;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14403v1 Announce Type: cross  Abstract: Retrieval-Augmented Large Language Models (LLMs), which incorporate the non-parametric knowledge from external knowledge bases into LLMs, have emerged as a promising approach to enhancing response accuracy in several tasks, such as Question-Answering (QA). However, even though there are various approaches dealing with queries of different complexities, they either handle simple queries with unnecessary computational overhead or fail to adequately address complex multi-step queries; yet, not all user requests fall into only one of the simple or complex categories. In this work, we propose a novel adaptive QA framework, that can dynamically select the most suitable strategy for (retrieval-augmented) LLMs from the simplest to the most sophisticated ones based on the query complexity. Also, this selection process is operationalized with a classifier, which is a smaller LM trained to predict the complexity level of incoming queries with aut
&lt;/p&gt;</description></item><item><title>XLAVS-R&#26159;&#19968;&#20010;&#36328;&#35821;&#35328;&#35270;&#21548;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#26377;&#38480;&#30340;&#22810;&#35821;&#35328;AV&#39044;&#35757;&#32451;&#25968;&#25454;&#65292;&#31616;&#21270;&#39044;&#35757;&#32451;&#26041;&#26696;&#65292;&#20197;&#25552;&#39640;&#23545;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#65292;&#22312;&#19979;&#28216;&#38899;&#39057;-&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;&#21644;&#32763;&#35793;&#20219;&#21153;&#20013;&#27604;&#20808;&#21069;&#26368;&#20808;&#36827;&#25216;&#26415;&#25552;&#21319;&#20102;&#39640;&#36798;18.5% WER&#21644;4.7 BLEU&#12290;</title><link>https://arxiv.org/abs/2403.14402</link><description>&lt;p&gt;
XLAVS-R: &#36328;&#35821;&#35328;&#35270;&#21548;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#22122;&#22768;&#40065;&#26834;&#35821;&#38899;&#30693;&#35273;
&lt;/p&gt;
&lt;p&gt;
XLAVS-R: Cross-Lingual Audio-Visual Speech Representation Learning for Noise-Robust Speech Perception
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14402
&lt;/p&gt;
&lt;p&gt;
XLAVS-R&#26159;&#19968;&#20010;&#36328;&#35821;&#35328;&#35270;&#21548;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#26377;&#38480;&#30340;&#22810;&#35821;&#35328;AV&#39044;&#35757;&#32451;&#25968;&#25454;&#65292;&#31616;&#21270;&#39044;&#35757;&#32451;&#26041;&#26696;&#65292;&#20197;&#25552;&#39640;&#23545;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#65292;&#22312;&#19979;&#28216;&#38899;&#39057;-&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;&#21644;&#32763;&#35793;&#20219;&#21153;&#20013;&#27604;&#20808;&#21069;&#26368;&#20808;&#36827;&#25216;&#26415;&#25552;&#21319;&#20102;&#39640;&#36798;18.5% WER&#21644;4.7 BLEU&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#35782;&#21035;&#21644;&#32763;&#35793;&#31995;&#32479;&#23545;&#22024;&#26434;&#30340;&#36755;&#20837;&#34920;&#29616;&#19981;&#20339;&#65292;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#32463;&#24120;&#20986;&#29616;&#12290;&#36890;&#36807;&#35270;&#35273;&#20449;&#21495;&#22686;&#24378;&#36825;&#20123;&#31995;&#32479;&#26377;&#28508;&#21147;&#25552;&#39640;&#23545;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#35270;&#21548;&#65288;AV&#65289;&#25968;&#25454;&#20165;&#26377;&#38480;&#21487;&#29992;&#65292;&#24182;&#19988;&#27604;&#20165;&#26377;&#38899;&#39057;&#36164;&#28304;&#30340;&#35821;&#35328;&#26356;&#23569;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;XLAVS-R&#65292;&#19968;&#20010;&#36328;&#35821;&#35328;&#35270;&#21548;&#35821;&#38899;&#34920;&#31034;&#27169;&#22411;&#65292;&#29992;&#20110;&#36229;&#36807;100&#31181;&#35821;&#35328;&#30340;&#22122;&#22768;&#40065;&#26834;&#35821;&#38899;&#35782;&#21035;&#21644;&#32763;&#35793;&#12290;&#23427;&#26088;&#22312;&#26368;&#22823;&#31243;&#24230;&#21033;&#29992;&#26377;&#38480;&#30340;&#22810;&#35821;&#35328;AV&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#30410;&#22788;&#65292;&#36890;&#36807;&#22312;&#38899;&#39057;-&#20165;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#19978;&#26500;&#24314;&#65292;&#24182;&#31616;&#21270;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#26041;&#26696;&#12290;&#22312;MuAViC&#22522;&#20934;&#35780;&#20272;&#19978;&#23545;XLAVS-R&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#26174;&#31034;&#20102;&#20854;&#22312;&#19979;&#28216;&#38899;&#39057;-&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;&#21644;&#32763;&#35793;&#20219;&#21153;&#19978;&#30340;&#20248;&#21183;&#65292;&#22312;&#32473;&#20986;&#22024;&#26434;&#30340;AV&#36755;&#20837;&#26102;&#65292;&#20854;&#20248;&#20110;&#20808;&#21069;&#26368;&#20808;&#36827;&#25216;&#26415;&#26368;&#39640;&#36798;&#21040;18.5% WER&#21644;4.7 BLEU&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14402v1 Announce Type: cross  Abstract: Speech recognition and translation systems perform poorly on noisy inputs, which are frequent in realistic environments. Augmenting these systems with visual signals has the potential to improve robustness to noise. However, audio-visual (AV) data is only available in limited amounts and for fewer languages than audio-only resources. To address this gap, we present XLAVS-R, a cross-lingual audio-visual speech representation model for noise-robust speech recognition and translation in over 100 languages. It is designed to maximize the benefits of limited multilingual AV pre-training data, by building on top of audio-only multilingual pre-training and simplifying existing pre-training schemes. Extensive evaluation on the MuAViC benchmark shows the strength of XLAVS-R on downstream audio-visual speech recognition and translation tasks, where it outperforms the previous state of the art by up to 18.5% WER and 4.7 BLEU given noisy AV inputs
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35774;&#35745;&#20004;&#38454;&#27573;&#24494;&#35843;&#31639;&#27861;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#39640;LLMs&#36981;&#24490;&#32763;&#35793;&#25351;&#20196;&#30340;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#32763;&#35793;&#26041;&#21521;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.14399</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#24863;&#30693;&#35843;&#33410;&#30340;&#31934;&#30830;&#32763;&#35793;&#23450;&#21046;LLMs&#30340;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Building Accurate Translation-Tailored LLMs with Language Aware Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14399
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35774;&#35745;&#20004;&#38454;&#27573;&#24494;&#35843;&#31639;&#27861;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#39640;LLMs&#36981;&#24490;&#32763;&#35793;&#25351;&#20196;&#30340;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#32763;&#35793;&#26041;&#21521;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32763;&#35793;&#23450;&#21046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#29616;&#20986;&#20986;&#33394;&#30340;&#32763;&#35793;&#33021;&#21147;&#65292;&#29978;&#33267;&#33021;&#19982;&#21830;&#19994;&#32763;&#35793;&#31995;&#32479;&#30456;&#23218;&#32654;&#12290;&#28982;&#32780;&#65292;&#35823;&#32763;&#35793;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#38459;&#30861;&#25105;&#20204;&#24320;&#21457;&#20934;&#30830;&#30340;&#22522;&#20110;LLMs&#30340;&#32763;&#35793;&#27169;&#22411;&#12290;&#20026;&#20102;&#32531;&#35299;&#35823;&#32763;&#35793;&#38382;&#39064;&#24182;&#25552;&#21319;LLMs&#22312;&#32763;&#35793;&#19978;&#30340;&#24615;&#33021;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#35201;&#20040;&#35774;&#35745;&#20102;&#20808;&#36827;&#30340;&#25552;&#31034;&#31574;&#30053;&#20197;&#31361;&#20986;&#32763;&#35793;&#25351;&#20196;&#30340;&#21151;&#33021;&#65292;&#35201;&#20040;&#21033;&#29992;LLMs&#30340;&#29616;&#22330;&#23398;&#20064;&#33021;&#21147;&#36890;&#36807;&#23569;&#37327;&#31034;&#33539;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22522;&#26412;&#19978;&#27809;&#26377;&#25552;&#39640;LLMs&#36981;&#24490;&#32763;&#35793;&#25351;&#20196;&#30340;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#35821;&#35328;&#26041;&#21521;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#24494;&#35843;&#31639;&#27861;&#26469;&#25552;&#39640;LLMs&#30340;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#65288;&#29305;&#21035;&#26159;&#32763;&#35793;&#26041;&#21521;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14399v1 Announce Type: cross  Abstract: Translation-tailored Large language models (LLMs) exhibit remarkable translation capabilities, even competing with supervised-trained commercial translation systems. However, off-target translation remains an unsolved problem, especially for low-resource languages, hindering us from developing accurate LLMs-based translation models. To mitigate the off-target translation problem and enhance the performance of LLMs on translation, recent works have either designed advanced prompting strategies to highlight the functionality of translation instructions or exploited the in-context learning ability of LLMs by feeding few-shot demonstrations. However, these methods essentially do not improve LLM's ability to follow translation instructions, especially the language direction information. In this work, we design a two-stage fine-tuning algorithm to improve the instruction-following ability (especially the translation direction) of LLMs. Speci
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#24039;&#22937;&#22320;&#23558;&#25968;&#23398;&#19987;&#19994;&#30693;&#35782;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36716;&#31227;&#21040;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;</title><link>https://arxiv.org/abs/2403.14390</link><description>&lt;p&gt;
&#20174;&#22823;&#21040;&#23567;&#65306;&#21033;&#29992;&#24369;&#30417;&#30563;&#23545;&#25968;&#23398;&#38382;&#39064;&#36827;&#34892;&#30693;&#35782;&#25552;&#28860;&#21644;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
From Large to Tiny: Distilling and Refining Mathematical Expertise for Math Word Problems with Weakly Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14390
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#24039;&#22937;&#22320;&#23558;&#25968;&#23398;&#19987;&#19994;&#30693;&#35782;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36716;&#31227;&#21040;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#20013;&#39640;&#26114;&#27880;&#37322;&#25104;&#26412;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#20351;&#29992;&#20013;&#38388;&#26041;&#31243;&#24335;&#36827;&#34892;&#20840;&#38754;&#30417;&#30563;&#30340;&#26368;&#36817;&#30740;&#31350;&#25552;&#20986;&#20102;&#20381;&#36182;&#26368;&#32456;&#31572;&#26696;&#20316;&#20026;&#30417;&#30563;&#20449;&#21495;&#30340;&#24369;&#30417;&#30563;&#20219;&#21153;&#35774;&#32622;&#12290;&#29616;&#26377;&#30340;&#20027;&#23548;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#21508;&#31181;&#25628;&#32034;&#25216;&#26415;&#26469;&#25512;&#26029;&#20013;&#38388;&#26041;&#31243;&#24335;&#65292;&#20294;&#26080;&#27861;&#20445;&#35777;&#20854;&#19982;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#35821;&#20041;&#19968;&#33268;&#24615;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#30340;&#23835;&#36215;&#20026;&#30452;&#25509;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#25171;&#24320;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;&#28982;&#32780;&#65292;LLMs&#30340;&#35745;&#31639;&#35201;&#27714;&#20351;&#23427;&#20204;&#22312;&#36164;&#28304;&#32039;&#24352;&#30340;&#29615;&#22659;&#20013;&#20351;&#29992;&#26102;&#24182;&#19981;&#29702;&#24819;&#12290;&#37492;&#20110;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#24039;&#22937;&#22320;&#23558;&#25968;&#23398;&#19987;&#19994;&#30693;&#35782;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36716;&#31227;&#21040;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#22312;\emph{&#33976;&#39311;&#38454;&#27573;}&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#28385;&#36275;MWP&#23646;&#24615;&#30340;&#25552;&#21462;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14390v1 Announce Type: new  Abstract: Addressing the challenge of high annotation costs in solving Math Word Problems (MWPs) through full supervision with intermediate equations, recent works have proposed weakly supervised task settings that rely solely on the final answer as a supervised signal. Existing leading approaches typically employ various search techniques to infer intermediate equations, but cannot ensure their semantic consistency with natural language descriptions. The rise of Large Language Models (LLMs) like ChatGPT has opened up new possibilities for addressing MWPs directly. However, the computational demands of LLMs make them less than ideal for use in settings where resources are tight. In light of these challenges, we introduce an innovative two-stage framework that adeptly transfers mathematical Expertise from large to tiny language models. In \emph{Distillation Stage}, we propose a series of extraction processes that satisfy the properties of MWPs to d
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;PSPEM&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#34920;&#36848;&#21069;&#32512;&#25552;&#31034;&#26469;&#32534;&#36753;&#35821;&#35328;Lodel&#30340;&#30693;&#35782;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#20013;&#30340;&#20302;&#25928;&#24615;&#12289;&#36890;&#29992;&#24615;&#38382;&#39064;&#65292;&#20197;&#21450;&#25552;&#31034;&#24037;&#31243;&#30340;&#19981;&#36879;&#26126;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.14381</link><description>&lt;p&gt;
&#36890;&#36807;&#37325;&#26032;&#34920;&#36848;&#21069;&#32512;&#25552;&#31034;&#26469;&#32534;&#36753;&#35821;&#35328;Lodel&#30340;&#30693;&#35782;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Editing Knowledge Representation of Language Lodel via Rephrased Prefix Prompts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14381
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;PSPEM&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#34920;&#36848;&#21069;&#32512;&#25552;&#31034;&#26469;&#32534;&#36753;&#35821;&#35328;Lodel&#30340;&#30693;&#35782;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#20013;&#30340;&#20302;&#25928;&#24615;&#12289;&#36890;&#29992;&#24615;&#38382;&#39064;&#65292;&#20197;&#21450;&#25552;&#31034;&#24037;&#31243;&#30340;&#19981;&#36879;&#26126;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#24050;&#22312;&#24191;&#27867;&#30340;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#22521;&#35757;&#65292;&#20197;&#23384;&#20648;&#20851;&#20110;&#25991;&#26412;&#25551;&#36848;&#30340;&#19990;&#30028;&#21508;&#20010;&#26041;&#38754;&#30340;&#20107;&#23454;&#30693;&#35782;&#12290;&#24403;&#21069;&#25216;&#26415;&#36890;&#24120;&#37319;&#29992;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#25110;&#29305;&#23450;&#25552;&#31034;&#26469;&#20462;&#25913;LM&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#25104;&#26412;&#39640;&#26114;&#19988;&#20302;&#25928;&#65292;&#38590;&#20197;&#20135;&#29983;&#36866;&#24403;&#30340;&#25991;&#26412;&#12290;&#27492;&#22806;&#65292;&#25552;&#31034;&#24037;&#31243;&#26159;&#19981;&#36879;&#26126;&#30340;&#65292;&#38656;&#35201;&#22823;&#37327;&#21162;&#21147;&#25214;&#21040;&#21512;&#36866;&#30340;&#25552;&#31034;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;PSPEM&#65288;&#21069;&#32512;&#36719;&#25552;&#31034;&#32534;&#36753;&#26041;&#27861;&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#20165;&#36890;&#36807;&#19968;&#27425;&#35757;&#32451;&#32780;&#32456;&#36523;&#20351;&#29992;&#12290;&#23427;&#35299;&#20915;&#20102;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#20013;&#30340;&#20302;&#25928;&#24615;&#21644;&#36890;&#29992;&#24615;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#23547;&#25214;&#26368;&#20339;&#36719;&#25552;&#31034;&#26469;&#20811;&#26381;&#25552;&#31034;&#24037;&#31243;&#30340;&#19981;&#36879;&#26126;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;PSPEM&#21033;&#29992;&#25552;&#31034;&#32534;&#30721;&#22120;&#21644;&#32534;&#30721;&#36716;&#25442;&#22120;&#26469;&#31934;&#28860;&#25552;&#31034;&#20013;&#30340;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#25552;&#31034;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14381v1 Announce Type: cross  Abstract: Neural language models (LMs) have been extensively trained on vast corpora to store factual knowledge about various aspects of the world described in texts. Current technologies typically employ knowledge editing methods or specific prompts to modify LM outputs. However, existing knowledge editing methods are costly and inefficient, struggling to produce appropriate text. Additionally, prompt engineering is opaque and requires significant effort to find suitable prompts. To address these issues, we introduce a new method called PSPEM (Prefix Soft Prompt Editing Method), that can be used for a lifetime with just one training. It resolves the inefficiencies and generalizability issues in knowledge editing methods and overcomes the opacity of prompt engineering by automatically seeking optimal soft prompts. Specifically, PSPEM utilizes a prompt encoder and an encoding converter to refine key information in prompts and uses prompt alignmen
&lt;/p&gt;</description></item><item><title>FIT-RAG&#20351;&#29992;&#20107;&#23454;&#20449;&#24687;&#21644;&#20196;&#29260;&#20943;&#23569;&#35299;&#20915;&#20102;&#40657;&#21283;&#23376;RAG&#31995;&#32479;&#20013;&#23384;&#22312;&#30340;&#20107;&#23454;&#20449;&#24687;&#24573;&#35270;&#21644;&#20196;&#29260;&#28010;&#36153;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.14374</link><description>&lt;p&gt;
FIT-RAG: &#20855;&#26377;&#20107;&#23454;&#20449;&#24687;&#21644;&#20196;&#29260;&#20943;&#23569;&#30340;&#40657;&#21283;&#23376;RAG
&lt;/p&gt;
&lt;p&gt;
FIT-RAG: Black-Box RAG with Factual Information and Token Reduction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14374
&lt;/p&gt;
&lt;p&gt;
FIT-RAG&#20351;&#29992;&#20107;&#23454;&#20449;&#24687;&#21644;&#20196;&#29260;&#20943;&#23569;&#35299;&#20915;&#20102;&#40657;&#21283;&#23376;RAG&#31995;&#32479;&#20013;&#23384;&#22312;&#30340;&#20107;&#23454;&#20449;&#24687;&#24573;&#35270;&#21644;&#20196;&#29260;&#28010;&#36153;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21442;&#25968;&#25968;&#37327;&#24222;&#22823;&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#24494;&#35843;&#20197;&#26356;&#26032;&#38271;&#23614;&#25110;&#36807;&#26102;&#30693;&#35782;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#37117;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#20026;&#20102;&#36991;&#20813;&#24494;&#35843;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;LLM&#35270;&#20026;&#19968;&#20010;&#40657;&#21283;&#23376;&#65288;&#21363;&#65292;&#20923;&#32467;LLM&#30340;&#21442;&#25968;&#65289;&#24182;&#22686;&#21152;&#19968;&#20010;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#31995;&#32479;&#65292;&#21363;&#40657;&#21283;&#23376;RAG&#12290;&#26368;&#36817;&#65292;&#40657;&#21283;&#23376;RAG&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#24182;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#40657;&#21283;&#23376;RAG&#26041;&#27861;&#36890;&#24120;&#24494;&#35843;&#26816;&#32034;&#22120;&#20197;&#36814;&#21512;LLMs&#30340;&#20559;&#22909;&#65292;&#24182;&#23558;&#25152;&#26377;&#26816;&#32034;&#21040;&#30340;&#25991;&#26723;&#20018;&#32852;&#22312;&#19968;&#36215;&#20316;&#20026;&#36755;&#20837;&#65292;&#20294;&#23384;&#22312;&#20004;&#20010;&#38382;&#39064;:&#65288;1&#65289;&#23545;&#20107;&#23454;&#20449;&#24687;&#30340;&#24573;&#35270;&#12290;LLM&#20559;&#22909;&#30340;&#25991;&#26723;&#21487;&#33021;&#19981;&#21253;&#21547;&#32473;&#23450;&#38382;&#39064;&#30340;&#20107;&#23454;&#20449;&#24687;&#65292;&#36825;&#21487;&#33021;&#20250;&#35823;&#23548;&#26816;&#32034;&#22120;&#65292;&#24182;&#25439;&#23475;&#40657;&#21283;&#23376;RAG&#30340;&#26377;&#25928;&#24615;;&#65288;2&#65289;&#20196;&#29260;&#30340;&#28010;&#36153;&#12290;&#31616;&#21333;&#22320;&#23558;&#25152;&#26377;&#26816;&#32034;&#21040;&#30340;&#25991;&#26723;&#20018;&#32852;&#22312;&#19968;&#36215;&#20250;&#24102;&#26469;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14374v1 Announce Type: new  Abstract: Due to the extraordinarily large number of parameters, fine-tuning Large Language Models (LLMs) to update long-tail or out-of-date knowledge is impractical in lots of applications. To avoid fine-tuning, we can alternatively treat a LLM as a black-box (i.e., freeze the parameters of the LLM) and augment it with a Retrieval-Augmented Generation (RAG) system, namely black-box RAG. Recently, black-box RAG has achieved success in knowledge-intensive tasks and has gained much attention. Existing black-box RAG methods typically fine-tune the retriever to cater to LLMs' preferences and concatenate all the retrieved documents as the input, which suffers from two issues: (1) Ignorance of Factual Information. The LLM preferred documents may not contain the factual information for the given question, which can mislead the retriever and hurt the effectiveness of black-box RAG; (2) Waste of Tokens. Simply concatenating all the retrieved documents brin
&lt;/p&gt;</description></item><item><title>WikiFactDiff&#26159;&#19968;&#20010;&#29992;&#20110;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21407;&#23376;&#20107;&#23454;&#30693;&#35782;&#26356;&#26032;&#30340;&#22823;&#22411;&#12289;&#29616;&#23454;&#19988;&#26102;&#38388;&#19978;&#21487;&#36866;&#24212;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#25551;&#36848;&#20107;&#23454;&#30693;&#35782;&#22312;&#20004;&#20010;&#26085;&#26399;&#20043;&#38388;&#30340;&#28436;&#21464;&#65292;&#24182;&#25552;&#20379;&#19981;&#21516;&#31867;&#22411;&#22522;&#26412;&#26356;&#26032;&#30340;&#26356;&#26032;&#26041;&#26696;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.14364</link><description>&lt;p&gt;
WikiFactDiff: &#19968;&#20010;&#22823;&#22411;&#12289;&#29616;&#23454;&#19988;&#26102;&#38388;&#19978;&#21487;&#36866;&#24212;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21407;&#23376;&#20107;&#23454;&#30693;&#35782;&#26356;&#26032;
&lt;/p&gt;
&lt;p&gt;
WikiFactDiff: A Large, Realistic, and Temporally Adaptable Dataset for Atomic Factual Knowledge Update in Causal Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14364
&lt;/p&gt;
&lt;p&gt;
WikiFactDiff&#26159;&#19968;&#20010;&#29992;&#20110;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21407;&#23376;&#20107;&#23454;&#30693;&#35782;&#26356;&#26032;&#30340;&#22823;&#22411;&#12289;&#29616;&#23454;&#19988;&#26102;&#38388;&#19978;&#21487;&#36866;&#24212;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#25551;&#36848;&#20107;&#23454;&#30693;&#35782;&#22312;&#20004;&#20010;&#26085;&#26399;&#20043;&#38388;&#30340;&#28436;&#21464;&#65292;&#24182;&#25552;&#20379;&#19981;&#21516;&#31867;&#22411;&#22522;&#26412;&#26356;&#26032;&#30340;&#26356;&#26032;&#26041;&#26696;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20107;&#23454;&#24615;&#38543;&#26102;&#38388;&#34928;&#20943;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#35757;&#32451;&#20043;&#21518;&#30340;&#20107;&#20214;&#23545;&#23427;&#20204;&#26469;&#35828;&#26159;&#8220;&#26410;&#30693;&#8221;&#30340;&#12290;&#20026;&#20102;&#20445;&#25345;&#27169;&#22411;&#30340;&#26368;&#26032;&#65292;&#19968;&#31181;&#26041;&#27861;&#21487;&#33021;&#26159;&#36890;&#36807;&#20107;&#23454;&#26356;&#26032;&#65306;&#21363;&#22312;&#27169;&#22411;&#20013;&#25554;&#20837;&#12289;&#26367;&#25442;&#25110;&#21024;&#38500;&#26576;&#20123;&#31616;&#21333;&#65288;&#21407;&#23376;&#65289;&#20107;&#23454;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#19968;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;WikiFactDiff&#65292;&#36825;&#26159;&#19968;&#20010;&#25551;&#36848;&#20107;&#23454;&#30693;&#35782;&#22312;&#20004;&#20010;&#26085;&#26399;&#20043;&#38388;&#28436;&#21464;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#34987;&#20998;&#20026;&#19977;&#31867;&#30340;&#19968;&#31995;&#21015;&#31616;&#21333;&#20107;&#23454;&#65306;&#26032;&#20107;&#23454;&#12289;&#36807;&#26102;&#20107;&#23454;&#21644;&#38745;&#24577;&#20107;&#23454;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#30001;&#36825;&#19977;&#31181;&#22522;&#26412;&#26356;&#26032;&#31867;&#22411;&#30340;&#21508;&#31181;&#32452;&#21512;&#20135;&#29983;&#30340;&#22810;&#20010;&#26356;&#26032;&#26041;&#26696;&#12290;&#36825;&#20123;&#20107;&#23454;&#30001;&#20027;-&#35859;-&#23486;&#19977;&#20803;&#32452;&#34920;&#31034;&#65307;&#30830;&#23454;&#65292;WikiFactDiff&#26159;&#36890;&#36807;&#27604;&#36739;Wikidata&#30693;&#35782;&#24211;&#22312;2021&#24180;1&#26376;4&#26085;&#21644;2023&#24180;2&#26376;27&#26085;&#26399;&#38388;&#30340;&#29366;&#24577;&#32780;&#26500;&#24314;&#30340;&#12290;&#36825;&#20123;&#20107;&#23454;&#20276;&#38543;&#26377;&#29992;&#20110;&#36816;&#34892;&#26356;&#26032;&#31639;&#27861;&#21644;&#35780;&#20272;&#25351;&#26631;&#30340;&#25991;&#26412;&#21270;&#27169;&#26495;&#21644;&#22635;&#31354;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14364v1 Announce Type: new  Abstract: The factuality of large language model (LLMs) tends to decay over time since events posterior to their training are "unknown" to them. One way to keep models up-to-date could be factual update: the task of inserting, replacing, or removing certain simple (atomic) facts within the model. To study this task, we present WikiFactDiff, a dataset that describes the evolution of factual knowledge between two dates as a collection of simple facts divided into three categories: new, obsolete, and static. We describe several update scenarios arising from various combinations of these three types of basic update. The facts are represented by subject-relation-object triples; indeed, WikiFactDiff was constructed by comparing the state of the Wikidata knowledge base at 4 January 2021 and 27 February 2023. Those fact are accompanied by verbalization templates and cloze tests that enable running update algorithms and their evaluation metrics. Contrary t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Financial-STS&#20219;&#21153;&#65292;&#26088;&#22312;&#34913;&#37327;&#37329;&#34701;&#21465;&#36848;&#23545;&#20043;&#38388;&#24494;&#22937;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;LLM&#22686;&#24378;&#22411;&#27969;&#31243;&#65292;&#20197;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.14341</link><description>&lt;p&gt;
&#36229;&#36234;&#34920;&#38754;&#30456;&#20284;&#24615;&#65306;&#26816;&#27979;&#37329;&#34701;&#21465;&#36848;&#20013;&#24494;&#22937;&#30340;&#35821;&#20041;&#36716;&#21464;
&lt;/p&gt;
&lt;p&gt;
Beyond Surface Similarity: Detecting Subtle Semantic Shifts in Financial Narratives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Financial-STS&#20219;&#21153;&#65292;&#26088;&#22312;&#34913;&#37327;&#37329;&#34701;&#21465;&#36848;&#23545;&#20043;&#38388;&#24494;&#22937;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;LLM&#22686;&#24378;&#22411;&#27969;&#31243;&#65292;&#20197;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Financial-STS&#20219;&#21153;&#65292;&#36825;&#26159;&#19968;&#20010;&#37329;&#34701;&#39046;&#22495;&#29305;&#23450;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#26088;&#22312;&#34913;&#37327;&#37329;&#34701;&#21465;&#36848;&#23545;&#20043;&#38388;&#24494;&#22937;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;&#36825;&#20123;&#21465;&#36848;&#26469;&#33258;&#21516;&#19968;&#20844;&#21496;&#30340;&#36130;&#21153;&#25253;&#34920;&#65292;&#20294;&#23545;&#24212;&#19981;&#21516;&#26102;&#26399;&#65292;&#20363;&#22914;&#24180;&#24230;&#27604;&#36739;&#12290;&#34913;&#37327;&#36825;&#20123;&#37197;&#23545;&#21465;&#36848;&#20043;&#38388;&#24494;&#22937;&#30340;&#35821;&#20041;&#24046;&#24322;&#20351;&#24471;&#24066;&#22330;&#21033;&#30410;&#30456;&#20851;&#32773;&#33021;&#22815;&#35780;&#20272;&#20844;&#21496;&#36130;&#21153;&#21644;&#36816;&#33829;&#24773;&#20917;&#38543;&#26102;&#38388;&#30340;&#21464;&#21270;&#65292;&#36825;&#23545;&#20110;&#37329;&#34701;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#23884;&#20837;&#27169;&#22411;&#21644;LLM&#23884;&#20837;&#22312;&#35782;&#21035;&#36825;&#20123;&#24494;&#22937;&#30340;&#37329;&#34701;&#21465;&#36848;&#36716;&#21464;&#26041;&#38754;&#34920;&#29616;&#19981;&#36275;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;Financial-STS&#20219;&#21153;&#35774;&#35745;&#30340;LLM&#22686;&#24378;&#22411;&#27969;&#31243;&#12290;&#23545;&#19968;&#20010;&#20154;&#24037;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#22312;&#32463;&#20856;STS&#20219;&#21153;&#21644;gen&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14341v1 Announce Type: new  Abstract: In this paper, we introduce the Financial-STS task, a financial domain-specific NLP task designed to measure the nuanced semantic similarity between pairs of financial narratives. These narratives originate from the financial statements of the same company but correspond to different periods, such as year-over-year comparisons. Measuring the subtle semantic differences between these paired narratives enables market stakeholders to gauge changes over time in the company's financial and operational situations, which is critical for financial decision-making. We find that existing pretrained embedding models and LLM embeddings fall short in discerning these subtle financial narrative shifts. To address this gap, we propose an LLM-augmented pipeline specifically designed for the Financial-STS task. Evaluation on a human-annotated dataset demonstrates that our proposed method outperforms existing methods trained on classic STS tasks and gener
&lt;/p&gt;</description></item><item><title>$\nabla \tau$ &#26159;&#19968;&#31181;&#26088;&#22312;&#39640;&#25928;&#28040;&#38500;&#37096;&#20998;&#35757;&#32451;&#25968;&#25454;&#24433;&#21709;&#30340;&#26426;&#22120;&#36951;&#24536;&#20248;&#21270;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2403.14339</link><description>&lt;p&gt;
$\nabla \tau$: &#22522;&#20110;&#26799;&#24230;&#19988;&#20219;&#21153;&#26080;&#20851;&#30340;&#26426;&#22120;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
$\nabla \tau$: Gradient-based and Task-Agnostic machine Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14339
&lt;/p&gt;
&lt;p&gt;
$\nabla \tau$ &#26159;&#19968;&#31181;&#26088;&#22312;&#39640;&#25928;&#28040;&#38500;&#37096;&#20998;&#35757;&#32451;&#25968;&#25454;&#24433;&#21709;&#30340;&#26426;&#22120;&#36951;&#24536;&#20248;&#21270;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#36951;&#24536;&#26159;&#19968;&#31181;&#26377;&#36873;&#25321;&#24615;&#22320;&#28040;&#38500;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#26576;&#20123;&#25968;&#25454;&#31034;&#20363;&#24433;&#21709;&#30340;&#36807;&#31243;&#65292;&#20316;&#20026;&#20174;&#19994;&#32773;&#36981;&#23432;&#26368;&#36817;&#30340;&#25968;&#25454;&#20445;&#25252;&#27861;&#35268;&#30340;&#25163;&#27573;&#65292;&#24050;&#32463;&#24341;&#36215;&#20102;&#26174;&#33879;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#36951;&#24536;&#26041;&#27861;&#38754;&#20020;&#30528;&#20851;&#38190;&#32570;&#28857;&#65292;&#21253;&#25324;&#20854;&#25104;&#26412;&#36807;&#39640;&#65292;&#36890;&#24120;&#19982;&#22823;&#37327;&#36229;&#21442;&#25968;&#30456;&#20851;&#65292;&#20197;&#21450;&#20165;&#24536;&#35760;&#30456;&#23545;&#36739;&#23567;&#25968;&#25454;&#37096;&#20998;&#30340;&#38480;&#21046;&#12290;&#36825;&#32463;&#24120;&#23548;&#33268;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#25104;&#20026;&#26356;&#24555;&#36895;&#21644;&#26356;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22522;&#20110;&#26799;&#24230;&#19988;&#20219;&#21153;&#26080;&#20851;&#30340;&#26426;&#22120;&#36951;&#24536;&#65288;$\nabla \tau$&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26088;&#22312;&#39640;&#25928;&#28040;&#38500;&#37096;&#20998;&#35757;&#32451;&#25968;&#25454;&#24433;&#21709;&#30340;&#20248;&#21270;&#26694;&#26550;&#12290;&#23427;&#23545;&#24453;&#36951;&#24536;&#30340;&#25968;&#25454;&#24212;&#29992;&#33258;&#36866;&#24212;&#26799;&#24230;&#19978;&#21319;&#65292;&#21516;&#26102;&#23545;&#20854;&#20313;&#25968;&#25454;&#20351;&#29992;&#26631;&#20934;&#26799;&#24230;&#19979;&#38477;&#12290;$\nabla \tau$&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#25552;&#20379;&#20102;&#22810;&#31181;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14339v1 Announce Type: cross  Abstract: Machine Unlearning, the process of selectively eliminating the influence of certain data examples used during a model's training, has gained significant attention as a means for practitioners to comply with recent data protection regulations. However, existing unlearning methods face critical drawbacks, including their prohibitively high cost, often associated with a large number of hyperparameters, and the limitation of forgetting only relatively small data portions. This often makes retraining the model from scratch a quicker and more effective solution. In this study, we introduce Gradient-based and Task-Agnostic machine Unlearning ($\nabla \tau$), an optimization framework designed to remove the influence of a subset of training data efficiently. It applies adaptive gradient ascent to the data to be forgotten while using standard gradient descent for the remaining data. $\nabla \tau$ offers multiple benefits over existing approache
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;CoTGenius&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#20248;&#36136;CoT&#25552;&#31034;&#65292;&#24182;&#36890;&#36807;&#23427;&#21019;&#24314;&#20102;&#24222;&#22823;&#30340;CoT&#25968;&#25454;&#38598;&#20197;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;</title><link>https://arxiv.org/abs/2403.14312</link><description>&lt;p&gt;
ChainLM&#65306;&#20511;&#21161;&#25913;&#36827;&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;&#36171;&#33021;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ChainLM: Empowering Large Language Models with Improved Chain-of-Thought Prompting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14312
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;CoTGenius&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#20248;&#36136;CoT&#25552;&#31034;&#65292;&#24182;&#36890;&#36807;&#23427;&#21019;&#24314;&#20102;&#24222;&#22823;&#30340;CoT&#25968;&#25454;&#38598;&#20197;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought (CoT)&#25552;&#31034;&#21487;&#20197;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#26159;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;&#20027;&#35201;&#26041;&#27861;&#20043;&#19968;&#12290;&#29616;&#26377;&#30340;CoT&#21512;&#25104;&#26041;&#27861;&#36890;&#24120;&#19987;&#27880;&#20110;&#26356;&#31616;&#21333;&#30340;&#25512;&#29702;&#20219;&#21153;&#65292;&#22240;&#27492;&#23548;&#33268;CoT&#25552;&#31034;&#36136;&#37327;&#20302;&#19988;&#19981;&#19968;&#33268;&#12290;&#38024;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#23545;CoT&#25552;&#31034;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;CoTGenius&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#33258;&#21160;&#29983;&#25104;&#20248;&#36136;CoT&#25552;&#31034;&#30340;&#26032;&#22411;&#26694;&#26550;&#12290;CoTGenius&#22522;&#20110;&#19977;&#31181;&#20027;&#35201;&#30340;&#36827;&#21270;&#31574;&#30053;&#21457;&#23637;&#32780;&#26469;&#65292;&#21363;&#22797;&#26434;&#21270;&#12289;&#22810;&#26679;&#21270;&#21644;&#20855;&#20307;&#21270;&#65292;&#21516;&#26102;&#37197;&#22791;&#20004;&#31181;&#36807;&#28388;&#26426;&#21046;&#65306;&#36827;&#21270;&#25104;&#21151;&#35780;&#21028;&#21644;&#27491;&#30830;&#24615;&#39564;&#35777;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21033;&#29992;CoTGenius&#21019;&#24314;&#20102;&#19968;&#20010;&#24222;&#22823;&#30340;CoT&#25968;&#25454;&#38598;&#65292;&#24182;&#38543;&#21518;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#23545;Llama 2-Chat 7B&#21644;13B&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#26368;&#32456;&#24471;&#21040;&#30340;&#27169;&#22411;&#34987;&#21629;&#21517;&#20026;ChainLM&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14312v1 Announce Type: new  Abstract: Chain-of-Thought (CoT) prompting can enhance the reasoning capabilities of large language models (LLMs), establishing itself as a primary approach to solving complex reasoning tasks. Existing CoT synthesis approaches usually focus on simpler reasoning tasks and thus result in low-quality and inconsistent CoT prompts. In response to this challenge, we present an empirical investigation of CoT prompting and introduce CoTGenius, a novel framework designed for the automatic generation of superior CoT prompts. CoTGenius is developed based on three major evolution strategies, i.e., complicate, diversify, and specify-alongside two filtering mechanisms: evolutionary success judgement and correctness verification. We further employ CoTGenius to create an extensive CoT dataset, and subsequently fine-tune the Llama 2-Chat 7B and 13B models on this dataset. We call the resulting model ChainLM. To deal with the cumulative error issue in reasoning ste
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#31995;&#32479;&#35780;&#20272;&#20013;&#65292;&#26080;&#21442;&#32771;&#25351;&#26631;&#19982;&#20154;&#31867;&#21028;&#26029;&#26356;&#30456;&#20851;&#65292;&#23545;&#35821;&#35328;&#36136;&#37327;&#30340;&#19981;&#36275;&#26356;&#20026;&#25935;&#24863;&#65292;&#20294;&#20854;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#23384;&#22312;&#24046;&#24322;&#65292;&#24182;&#21463;&#21040;&#20505;&#36873;&#25991;&#26412;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.14275</link><description>&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#31995;&#32479;&#35780;&#20272;&#20013;&#26159;&#21542;&#38656;&#35201;&#21442;&#32771;&#65311;&#20309;&#26102;&#20309;&#22320;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Reference Necessary in the Evaluation of NLG Systems? When and Where?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#31995;&#32479;&#35780;&#20272;&#20013;&#65292;&#26080;&#21442;&#32771;&#25351;&#26631;&#19982;&#20154;&#31867;&#21028;&#26029;&#26356;&#30456;&#20851;&#65292;&#23545;&#35821;&#35328;&#36136;&#37327;&#30340;&#19981;&#36275;&#26356;&#20026;&#25935;&#24863;&#65292;&#20294;&#20854;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#23384;&#22312;&#24046;&#24322;&#65292;&#24182;&#21463;&#21040;&#20505;&#36873;&#25991;&#26412;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#31995;&#32479;&#30340;&#33258;&#21160;&#25351;&#26631;&#37117;&#26159;&#22522;&#20110;&#21442;&#32771;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#22330;&#26223;&#20013;&#65292;&#30001;&#20110;&#25910;&#38598;&#20154;&#31867;&#27880;&#37322;&#32467;&#26524;&#30340;&#25361;&#25112;&#65292;&#32570;&#20047;&#21487;&#38752;&#30340;&#21442;&#32771;&#12290;&#23613;&#31649;&#26368;&#36817;&#22312;&#26080;&#21442;&#32771;&#25351;&#26631;&#26041;&#38754;&#26377;&#20102;&#36827;&#23637;&#65292;&#20294;&#20309;&#26102;&#20309;&#22320;&#21487;&#20197;&#23558;&#20854;&#20316;&#20026;&#21442;&#32771;&#25351;&#26631;&#30340;&#26367;&#20195;&#21697;&#36824;&#19981;&#26159;&#24456;&#28165;&#26970;&#12290;&#36890;&#36807;&#37319;&#29992;&#22810;&#31181;&#20998;&#26512;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;&#20004;&#31181;&#25351;&#26631;&#22312;&#24191;&#27867;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#28085;&#30422;&#20102;&#20843;&#20010;&#25968;&#25454;&#38598;&#21644;&#20843;&#20010;&#35780;&#20272;&#27169;&#22411;&#12290;&#22522;&#20110;&#25166;&#23454;&#30340;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#26080;&#21442;&#32771;&#25351;&#26631;&#19982;&#20154;&#31867;&#21028;&#26029;&#20043;&#38388;&#23384;&#22312;&#26356;&#39640;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#23545;&#35821;&#35328;&#36136;&#37327;&#30340;&#19981;&#36275;&#26356;&#20026;&#25935;&#24863;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#22312;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#21464;&#21270;&#65292;&#24182;&#21463;&#21040;&#20505;&#36873;&#25991;&#26412;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#37325;&#35201;&#30340;&#26159;&#22312;&#20351;&#29992;&#26080;&#21442;&#32771;&#25351;&#26631;&#20043;&#21069;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14275v1 Announce Type: new  Abstract: The majority of automatic metrics for evaluating NLG systems are reference-based. However, the challenge of collecting human annotation results in a lack of reliable references in numerous application scenarios. Despite recent advancements in reference-free metrics, it has not been well understood when and where they can be used as an alternative to reference-based metrics. In this study, by employing diverse analytical approaches, we comprehensively assess the performance of both metrics across a wide range of NLG tasks, encompassing eight datasets and eight evaluation models. Based on solid experiments, the results show that reference-free metrics exhibit a higher correlation with human judgment and greater sensitivity to deficiencies in language quality. However, their effectiveness varies across tasks and is influenced by the quality of candidate texts. Therefore, it's important to assess the performance of reference-free metrics bef
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#26080;&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#24320;&#25918;&#35789;&#27719;&#30340;&#35270;&#35273;&#20851;&#31995;&#26816;&#27979;&#65292;&#36890;&#36807;Transformer-based&#22270;&#20687;&#32534;&#30721;&#22120;&#38544;&#24335;&#24314;&#27169;&#23545;&#35937;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#21462;&#20851;&#31995;&#20449;&#24687;&#65292;&#22312;&#28151;&#21512;&#25968;&#25454;&#19978;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20851;&#31995;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.14270</link><description>&lt;p&gt;
&#22330;&#26223;&#22270;ViT&#65306;&#31471;&#21040;&#31471;&#30340;&#24320;&#25918;&#35789;&#27719;&#35270;&#35273;&#20851;&#31995;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Scene-Graph ViT: End-to-End Open-Vocabulary Visual Relationship Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14270
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#26080;&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#24320;&#25918;&#35789;&#27719;&#30340;&#35270;&#35273;&#20851;&#31995;&#26816;&#27979;&#65292;&#36890;&#36807;Transformer-based&#22270;&#20687;&#32534;&#30721;&#22120;&#38544;&#24335;&#24314;&#27169;&#23545;&#35937;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#21462;&#20851;&#31995;&#20449;&#24687;&#65292;&#22312;&#28151;&#21512;&#25968;&#25454;&#19978;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20851;&#31995;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#20851;&#31995;&#26816;&#27979;&#26088;&#22312;&#35782;&#21035;&#22270;&#20687;&#20013;&#30340;&#23545;&#35937;&#21450;&#20854;&#20851;&#31995;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;&#29616;&#26377;&#30446;&#26631;&#26816;&#27979;&#26550;&#26500;&#20013;&#28155;&#21152;&#21333;&#29420;&#30340;&#20851;&#31995;&#27169;&#22359;&#25110;&#35299;&#30721;&#22120;&#26469;&#22788;&#29702;&#27492;&#20219;&#21153;&#12290;&#36825;&#31181;&#20998;&#31163;&#22686;&#21152;&#20102;&#22797;&#26434;&#24615;&#65292;&#38459;&#30861;&#20102;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#38480;&#21046;&#20102;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#39640;&#25928;&#30340;&#26080;&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#24320;&#25918;&#35789;&#27719;&#30340;&#35270;&#35273;&#20851;&#31995;&#26816;&#27979;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#30001;&#22522;&#20110;Transformer&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#32452;&#25104;&#65292;&#23558;&#23545;&#35937;&#34920;&#31034;&#20026;&#26631;&#35760;&#65292;&#24182;&#38544;&#21547;&#22320;&#24314;&#27169;&#23427;&#20204;&#30340;&#20851;&#31995;&#12290;&#20026;&#20102;&#25552;&#21462;&#20851;&#31995;&#20449;&#24687;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36873;&#25321;&#21487;&#33021;&#24418;&#25104;&#20851;&#31995;&#30340;&#23545;&#35937;&#23545;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21333;&#38454;&#27573;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#28151;&#21512;&#23545;&#35937;&#21644;&#20851;&#31995;&#26816;&#27979;&#25968;&#25454;&#19978;&#35757;&#32451;&#27492;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Visual Genome&#21644;&#22823;&#35789;&#27719;GQA&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20851;&#31995;&#26816;&#27979;&#24615;&#33021;&#65292;&#21487;&#23454;&#29616;&#23454;&#26102;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14270v1 Announce Type: cross  Abstract: Visual relationship detection aims to identify objects and their relationships in images. Prior methods approach this task by adding separate relationship modules or decoders to existing object detection architectures. This separation increases complexity and hinders end-to-end training, which limits performance. We propose a simple and highly efficient decoder-free architecture for open-vocabulary visual relationship detection. Our model consists of a Transformer-based image encoder that represents objects as tokens and models their relationships implicitly. To extract relationship information, we introduce an attention mechanism that selects object pairs likely to form a relationship. We provide a single-stage recipe to train this model on a mixture of object and relationship detection data. Our approach achieves state-of-the-art relationship detection performance on Visual Genome and on the large-vocabulary GQA benchmark at real-tim
&lt;/p&gt;</description></item><item><title>&#19987;&#21033;&#21644;&#25216;&#26415;&#30683;&#30462;&#25552;&#21462;&#26159;&#21019;&#26032;&#20135;&#21697;&#24320;&#21457;&#20013;&#30340;&#37325;&#35201;&#28789;&#24863;&#26469;&#28304;&#65292;&#32780;&#22522;&#20110;LLM&#30340;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#36229;&#36234;&#20256;&#32479;&#20851;&#38190;&#35789;&#26041;&#27861;&#65292;&#23454;&#29616;&#19987;&#21033;&#25688;&#35201;&#19982;&#20851;&#38190;&#27010;&#24565;&#25552;&#21462;&#12290;</title><link>https://arxiv.org/abs/2403.14258</link><description>&lt;p&gt;
&#22522;&#20110;LLM&#30340;&#19987;&#21033;&#30683;&#30462;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
LLM-based Extraction of Contradictions from Patents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14258
&lt;/p&gt;
&lt;p&gt;
&#19987;&#21033;&#21644;&#25216;&#26415;&#30683;&#30462;&#25552;&#21462;&#26159;&#21019;&#26032;&#20135;&#21697;&#24320;&#21457;&#20013;&#30340;&#37325;&#35201;&#28789;&#24863;&#26469;&#28304;&#65292;&#32780;&#22522;&#20110;LLM&#30340;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#36229;&#36234;&#20256;&#32479;&#20851;&#38190;&#35789;&#26041;&#27861;&#65292;&#23454;&#29616;&#19987;&#21033;&#25688;&#35201;&#19982;&#20851;&#38190;&#27010;&#24565;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;1950&#24180;&#20195;&#36215;&#65292;TRIZ&#23601;&#34920;&#26126;&#19987;&#21033;&#21450;&#20854;&#35299;&#20915;&#30340;&#25216;&#26415;&#30683;&#30462;&#26159;&#21019;&#26032;&#20135;&#21697;&#24320;&#21457;&#20013;&#37325;&#35201;&#30340;&#28789;&#24863;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;TRIZ&#26159;&#19968;&#31181;&#22522;&#20110;&#21382;&#21490;&#19987;&#21033;&#20998;&#26512;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24182;&#26410;&#21033;&#29992;&#24403;&#21069;&#19987;&#21033;&#20013;&#26085;&#30410;&#22686;&#22810;&#30340;&#26368;&#26032;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#12290;&#30001;&#20110;&#19987;&#21033;&#25968;&#37327;&#24222;&#22823;&#12289;&#38271;&#24230;&#38271;&#65292;&#19988;&#22797;&#26434;&#24615;&#39640;&#65292;&#29616;&#20195;&#19987;&#21033;&#26816;&#32034;&#21644;&#19987;&#21033;&#20998;&#26512;&#38656;&#35201;&#36229;&#36234;&#22522;&#20110;&#20851;&#38190;&#35789;&#30340;&#26041;&#27861;&#12290;&#26368;&#36817;&#19987;&#21033;&#26816;&#32034;&#21644;&#20998;&#26512;&#30340;&#36827;&#23637;&#20027;&#35201;&#38598;&#20013;&#22312;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;AI&#21464;&#21387;&#22120;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;Google BERT&#65289;&#30340;&#23494;&#38598;&#21521;&#37327;&#19978;&#12290;&#20363;&#22914;&#65292;&#23427;&#20204;&#29992;&#20110;&#23494;&#38598;&#26816;&#32034;&#12289;&#38382;&#39064;&#22238;&#31572;&#12289;&#25688;&#35201;&#21644;&#20851;&#38190;&#27010;&#24565;&#25552;&#21462;&#12290;&#22312;&#19987;&#21033;&#25688;&#35201;&#21644;&#20851;&#38190;&#27010;&#24565;&#25552;&#21462;&#26041;&#27861;&#20013;&#30340;&#19968;&#20010;&#30740;&#31350;&#37325;&#28857;&#26159;&#36890;&#29992;&#30340;&#21019;&#26032;&#27010;&#24565;&#65292;&#20197;&#21450;TRIZ&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14258v1 Announce Type: new  Abstract: Already since the 1950s TRIZ shows that patents and the technical contradictions they solve are an important source of inspiration for the development of innovative products. However, TRIZ is a heuristic based on a historic patent analysis and does not make use of the ever-increasing number of latest technological solutions in current patents. Because of the huge number of patents, their length, and, last but not least, their complexity there is a need for modern patent retrieval and patent analysis to go beyond keyword-oriented methods. Recent advances in patent retrieval and analysis mainly focus on dense vectors based on neural AI Transformer language models like Google BERT. They are, for example, used for dense retrieval, question answering or summarization and key concept extraction. A research focus within the methods for patent summarization and key concept extraction are generic inventive concepts respectively TRIZ concepts like
&lt;/p&gt;</description></item><item><title>ERD&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#21462;&#19982;&#35748;&#30693;&#22833;&#35843;&#30456;&#20851;&#30340;&#37096;&#20998;&#21644;&#36890;&#36807;&#22810;&#20010;&#20195;&#29702;&#20154;&#36827;&#34892;&#25512;&#29702;&#27493;&#39588;&#30340;&#36777;&#35770;&#65292;&#25913;&#36827;&#20102;&#22522;&#20110;LLM&#30340;&#35748;&#30693;&#22833;&#35843;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.14255</link><description>&lt;p&gt;
ERD&#65306;&#19968;&#20010;&#29992;&#20110;&#25913;&#21892;&#35748;&#30693;&#22833;&#35843;&#20998;&#31867;&#30340;LLM&#25512;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ERD: A Framework for Improving LLM Reasoning for Cognitive Distortion Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14255
&lt;/p&gt;
&lt;p&gt;
ERD&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#21462;&#19982;&#35748;&#30693;&#22833;&#35843;&#30456;&#20851;&#30340;&#37096;&#20998;&#21644;&#36890;&#36807;&#22810;&#20010;&#20195;&#29702;&#20154;&#36827;&#34892;&#25512;&#29702;&#27493;&#39588;&#30340;&#36777;&#35770;&#65292;&#25913;&#36827;&#20102;&#22522;&#20110;LLM&#30340;&#35748;&#30693;&#22833;&#35843;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25913;&#36827;&#24515;&#29702;&#27835;&#30103;&#30340;&#21487;&#35775;&#38382;&#24615;&#36817;&#24180;&#26469;&#21463;&#21040;&#20102;&#37325;&#35270;&#12290;&#20174;&#21463;&#35775;&#32773;&#30340;&#35805;&#35821;&#20013;&#35782;&#21035;&#35748;&#30693;&#22833;&#35843;&#21487;&#20197;&#26159;&#24515;&#29702;&#27835;&#30103;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#35748;&#30693;&#34892;&#20026;&#30103;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ERD&#65292;&#36890;&#36807;&#39069;&#22806;&#27169;&#22359;&#30340;&#65288;1&#65289;&#25552;&#21462;&#19982;&#35748;&#30693;&#22833;&#35843;&#30456;&#20851;&#30340;&#37096;&#20998;&#21644;&#65288;2&#65289;&#36890;&#36807;&#22810;&#20010;&#20195;&#29702;&#20154;&#36827;&#34892;&#25512;&#29702;&#27493;&#39588;&#30340;&#36777;&#35770;&#65292;&#25552;&#39640;&#20102;&#22522;&#20110;LLM&#30340;&#35748;&#30693;&#22833;&#35843;&#20998;&#31867;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;ERD&#25552;&#39640;&#20102;&#22810;&#31867;F1&#20998;&#25968;&#20197;&#21450;&#20108;&#20803;&#29305;&#24322;&#24615;&#20998;&#25968;&#12290;&#20851;&#20110;&#21518;&#32773;&#30340;&#20998;&#25968;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21521;LLMs&#25552;&#20379;&#22810;&#20195;&#29702;&#20154;&#36777;&#35770;&#25688;&#35201;&#26102;&#26377;&#25928;&#22320;&#28040;&#38500;&#20102;&#22522;&#20934;&#26041;&#27861;&#30340;&#20559;&#35265;&#65292;&#23588;&#20854;&#26159;&#24403;&#35813;&#25688;&#35201;&#34987;&#25552;&#20379;&#32473;LLMs&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14255v1 Announce Type: new  Abstract: Improving the accessibility of psychotherapy with the aid of Large Language Models (LLMs) is garnering a significant attention in recent years. Recognizing cognitive distortions from the interviewee's utterances can be an essential part of psychotherapy, especially for cognitive behavioral therapy. In this paper, we propose ERD, which improves LLM-based cognitive distortion classification performance with the aid of additional modules of (1) extracting the parts related to cognitive distortion, and (2) debating the reasoning steps by multiple agents. Our experimental results on a public dataset show that ERD improves the multi-class F1 score as well as binary specificity score. Regarding the latter score, it turns out that our method is effective in debiasing the baseline method which has high false positive rate, especially when the summary of multi-agent debate is provided to LLMs.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#38024;&#23545;&#38388;&#25509;&#24773;&#24863;&#34920;&#36798;&#30340;&#38889;&#22269;&#24120;&#35782;&#30693;&#35782;&#22270;&#35889;K-Act2Emo&#65292;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20854;&#22312;&#35757;&#32451;&#24773;&#24863;&#25512;&#26029;&#27169;&#22411;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24494;&#35843;&#21518;&#30340;BART&#30693;&#35782;&#27169;&#22411;&#34920;&#29616;&#20248;&#24322;&#65292;&#36798;&#21040;&#20102;&#19982;GPT-4 Turbo&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2403.14253</link><description>&lt;p&gt;
K-Act2Emo&#65306;&#38024;&#23545;&#38388;&#25509;&#24773;&#24863;&#34920;&#36798;&#30340;&#38889;&#22269;&#24120;&#35782;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
K-Act2Emo: Korean Commonsense Knowledge Graph for Indirect Emotional Expression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14253
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#38024;&#23545;&#38388;&#25509;&#24773;&#24863;&#34920;&#36798;&#30340;&#38889;&#22269;&#24120;&#35782;&#30693;&#35782;&#22270;&#35889;K-Act2Emo&#65292;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20854;&#22312;&#35757;&#32451;&#24773;&#24863;&#25512;&#26029;&#27169;&#22411;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24494;&#35843;&#21518;&#30340;BART&#30693;&#35782;&#27169;&#22411;&#34920;&#29616;&#20248;&#24322;&#65292;&#36798;&#21040;&#20102;&#19982;GPT-4 Turbo&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#25991;&#23398;&#20316;&#21697;&#20013;&#65292;&#24773;&#32490;&#36890;&#36807;&#23545;&#34892;&#20026;&#12289;&#38754;&#37096;&#34920;&#24773;&#21644;&#22806;&#34920;&#30340;&#25551;&#36848;&#38388;&#25509;&#20256;&#36798;&#65292;&#38656;&#35201;&#23545;&#21465;&#20107;&#29702;&#35299;&#36827;&#34892;&#24773;&#24863;&#25512;&#26029;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;K-Act2Emo&#65292;&#19968;&#20010;&#21253;&#21547;1,900&#31181;&#38388;&#25509;&#24773;&#24863;&#34920;&#36798;&#21644;&#21487;&#25512;&#26029;&#20986;&#30340;&#24773;&#32490;&#30340;&#38889;&#22269;&#24120;&#35782;&#30693;&#35782;&#22270;&#35889;&#12290;&#25105;&#20204;&#23558;&#25512;&#29702;&#31867;&#22411;&#20998;&#20026;&#27491;&#38754;&#24773;&#22659;&#30340;&#25512;&#29702;&#12289;&#36127;&#38754;&#24773;&#22659;&#30340;&#25512;&#29702;&#21644;&#34920;&#36798;&#19981;&#26159;&#24773;&#24863;&#32447;&#32034;&#26102;&#30340;&#25512;&#29702;&#12290;&#19982;&#29616;&#26377;&#30340;&#24120;&#35782;&#30693;&#35782;&#22270;&#35889;&#19981;&#21516;&#65292;K-Act2Emo&#19987;&#27880;&#20110;&#24773;&#24863;&#32972;&#26223;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#23427;&#23545;&#35757;&#32451;&#24773;&#24863;&#25512;&#26029;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20351;&#29992;K-Act2Emo&#24494;&#35843;&#30340;&#22522;&#20110;BART&#30340;&#30693;&#35782;&#27169;&#22411;&#32988;&#36807;&#20102;&#21508;&#31181;&#29616;&#26377;&#30340;&#38889;&#22269;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#24615;&#33021;&#19978;&#36798;&#21040;&#20102;&#19982;GPT-4 Turbo&#21487;&#27604;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14253v1 Announce Type: new  Abstract: In many literary texts, emotions are indirectly conveyed through descriptions of actions, facial expressions, and appearances, necessitating emotion inference for narrative understanding. In this paper, we introduce K-Act2Emo, a Korean commonsense knowledge graph (CSKG) comprising 1,900 indirect emotional expressions and the emotions inferable from them. We categorize reasoning types into inferences in positive situations, inferences in negative situations, and inferences when expressions do not serve as emotional cues. Unlike existing CSKGs, K-Act2Emo specializes in emotional contexts, and experimental results validate its effectiveness for training emotion inference models. Significantly, the BART-based knowledge model fine-tuned with K-Act2Emo outperforms various existing Korean large language models, achieving performance levels comparable to GPT-4 Turbo.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;LayoutLLM&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21644;&#25991;&#26723;&#22270;&#20687;&#29702;&#35299;&#30340;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#23545;&#25991;&#26723;&#22270;&#20687;&#30340;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.14252</link><description>&lt;p&gt;
LayoutLLM&#65306;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25351;&#20196;&#35843;&#25972;&#29992;&#20110;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
LayoutLLM: Large Language Model Instruction Tuning for Visually Rich Document Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14252
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;LayoutLLM&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21644;&#25991;&#26723;&#22270;&#20687;&#29702;&#35299;&#30340;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#23545;&#25991;&#26723;&#22270;&#20687;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;LayoutLLM&#65292;&#19968;&#31181;&#26356;&#28789;&#27963;&#30340;&#25991;&#26723;&#20998;&#26512;&#26041;&#27861;&#65292;&#29992;&#20110;&#29702;&#35299;&#22270;&#20687;&#25991;&#26723;&#12290;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#29702;&#35299;&#20219;&#21153;&#65292;&#22914;&#25991;&#26723;&#22270;&#20687;&#20998;&#31867;&#21644;&#20449;&#24687;&#25552;&#21462;&#65292;&#30001;&#20110;&#20854;&#37325;&#35201;&#24615;&#32780;&#21463;&#21040;&#37325;&#35270;&#12290;&#29616;&#26377;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#25972;&#21512;&#23545;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#24067;&#23616;&#32467;&#26500;&#30340;&#39044;&#35757;&#32451;&#24847;&#35782;&#26469;&#25552;&#21319;&#25991;&#26723;&#29702;&#35299;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#38024;&#23545;&#27599;&#20010;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#32780;&#19988;&#27169;&#22411;&#35757;&#32451;&#21644;&#25805;&#20316;&#25104;&#26412;&#39640;&#26114;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;LayoutLLM&#65292;&#23558;&#36825;&#20123;&#19982;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30456;&#38598;&#25104;&#12290;&#36890;&#36807;&#21033;&#29992;&#29616;&#26377;&#30740;&#31350;&#22312;&#25991;&#26723;&#22270;&#20687;&#29702;&#35299;&#21644;LLMs&#21331;&#36234;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#22810;&#27169;&#24577;&#25351;&#20196;&#25968;&#25454;&#38598;&#30340;&#24494;&#35843;&#19979;&#65292;&#21487;&#20197;&#36890;&#36807;&#21333;&#20010;&#27169;&#22411;&#29702;&#35299;&#25991;&#26723;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14252v1 Announce Type: cross  Abstract: This paper proposes LayoutLLM, a more flexible document analysis method for understanding imaged documents. Visually Rich Document Understanding tasks, such as document image classification and information extraction, have gained significant attention due to their importance. Existing methods have been developed to enhance document comprehension by incorporating pre-training awareness of images, text, and layout structure. However, these methods require fine-tuning for each task and dataset, and the models are expensive to train and operate. To overcome this limitation, we propose a new LayoutLLM that integrates these with large-scale language models (LLMs). By leveraging the strengths of existing research in document image understanding and LLMs' superior language understanding capabilities, the proposed model, fine-tuned with multimodal instruction datasets, performs an understanding of document images in a single model. Our experime
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22312;&#36828;&#31243;&#30382;&#32932;&#30149;&#23398;&#20013;&#25972;&#21512;&#20102;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#26426;&#22120;&#23398;&#20064;&#65292;&#26088;&#22312;&#36890;&#36807;&#32508;&#21512;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;Transformer&#35270;&#35273;&#27169;&#22411;&#21644;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#65292;&#36741;&#21161;&#35786;&#26029;&#30382;&#32932;&#30149;&#21464;&#21644;&#20854;&#20182;&#30382;&#32932;&#29366;&#20917;&#65292;&#20174;&#32780;&#20840;&#38754;&#35299;&#20915;&#35813;&#39046;&#22495;&#30340;&#35786;&#26029;&#27969;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.14243</link><description>&lt;p&gt;
Dermacen Analytica: &#19968;&#31181;&#23558;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#26426;&#22120;&#23398;&#20064;&#30456;&#25972;&#21512;&#30340;&#26032;&#26041;&#27861;&#35770;&#22312;&#36828;&#31243;&#30382;&#32932;&#30149;&#23398;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Dermacen Analytica: A Novel Methodology Integrating Multi-Modal Large Language Models with Machine Learning in tele-dermatology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14243
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22312;&#36828;&#31243;&#30382;&#32932;&#30149;&#23398;&#20013;&#25972;&#21512;&#20102;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#26426;&#22120;&#23398;&#20064;&#65292;&#26088;&#22312;&#36890;&#36807;&#32508;&#21512;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;Transformer&#35270;&#35273;&#27169;&#22411;&#21644;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#65292;&#36741;&#21161;&#35786;&#26029;&#30382;&#32932;&#30149;&#21464;&#21644;&#20854;&#20182;&#30382;&#32932;&#29366;&#20917;&#65292;&#20174;&#32780;&#20840;&#38754;&#35299;&#20915;&#35813;&#39046;&#22495;&#30340;&#35786;&#26029;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#23835;&#36215;&#22312;&#21307;&#23398;&#21457;&#29616;&#12289;&#35786;&#26029;&#21644;&#24739;&#32773;&#31649;&#29702;&#39046;&#22495;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#24076;&#26395;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#21307;&#23398;&#39046;&#22495;&#30340;&#24040;&#22823;&#22797;&#26434;&#24615;&#35201;&#27714;&#37319;&#29992;&#19968;&#31181;&#26356;&#22797;&#26434;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12289;&#20998;&#31867;&#22120;&#12289;&#20998;&#21106;&#31639;&#27861;&#20197;&#21450;&#36817;&#26469;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#26412;&#25991;&#25551;&#36848;&#12289;&#23454;&#26045;&#21644;&#35780;&#20272;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#31995;&#32479;&#21644;&#26041;&#27861;&#35770;&#65292;&#26088;&#22312;&#21327;&#21161;&#30382;&#32932;&#30149;&#23398;&#39046;&#22495;&#30340;&#30382;&#32932;&#30149;&#21464;&#21644;&#20854;&#20182;&#30382;&#32932;&#29366;&#20917;&#30340;&#35786;&#26029;&#36807;&#31243;&#65292;&#26088;&#22312;&#20840;&#38754;&#35299;&#20915;&#35813;&#39046;&#22495;&#30340;&#35786;&#26029;&#27969;&#31243;&#12290;&#35813;&#24037;&#20316;&#27969;&#31243;&#25972;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#12289;&#22522;&#20110;Transformer&#30340;&#35270;&#35273;&#27169;&#22411;&#21644;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#12290;&#36825;&#31181;&#20840;&#38754;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#30382;&#32932;&#30149;&#21464;&#30340;&#24494;&#22937;&#35299;&#37322;&#65292;&#27169;&#25311;&#24182;&#20419;&#36827;&#20102;&#30382;&#32932;&#31185;&#21307;&#29983;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;&#25105;&#20204;&#36890;&#36807;&#24443;&#24213;&#30340;&#20132;&#21449;&#27169;&#24335;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14243v1 Announce Type: cross  Abstract: The rise of Artificial Intelligence creates great promise in the field of medical discovery, diagnostics and patient management. However, the vast complexity of all medical domains require a more complex approach that combines machine learning algorithms, classifiers, segmentation algorithms and, lately, large language models. In this paper, we describe, implement and assess an Artificial Intelligence-empowered system and methodology aimed at assisting the diagnosis process of skin lesions and other skin conditions within the field of dermatology that aims to holistically address the diagnostic process in this domain. The workflow integrates large language, transformer-based vision models and sophisticated machine learning tools. This holistic approach achieves a nuanced interpretation of dermatological conditions that simulates and facilitates a dermatologist's workflow. We assess our proposed methodology through a thorough cross-mode
&lt;/p&gt;</description></item><item><title>RLRF&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#21453;&#39304;&#21644;&#33258;&#25105;&#21453;&#24605;&#26426;&#21046;&#65292;&#21487;&#20197;&#25913;&#36827;LLMs&#30340;&#26680;&#24515;&#33021;&#21147;&#65292;&#36229;&#36234;&#34920;&#38754;&#35843;&#25972;&#12290;</title><link>https://arxiv.org/abs/2403.14238</link><description>&lt;p&gt;
&#21453;&#24605;&#21453;&#39304;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLRF&#65289;&#65306;&#36890;&#36807;&#32454;&#31890;&#24230;&#33258;&#25105;&#21453;&#24605;&#23545;LLMs&#36827;&#34892;&#35843;&#25972;&#21644;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Reflective Feedback (RLRF): Aligning and Improving LLMs via Fine-Grained Self-Reflection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14238
&lt;/p&gt;
&lt;p&gt;
RLRF&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#21453;&#39304;&#21644;&#33258;&#25105;&#21453;&#24605;&#26426;&#21046;&#65292;&#21487;&#20197;&#25913;&#36827;LLMs&#30340;&#26680;&#24515;&#33021;&#21147;&#65292;&#36229;&#36234;&#34920;&#38754;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;RLHF&#22312;&#23558;LLMs&#19982;&#20154;&#31867;&#20559;&#22909;&#36827;&#34892;&#35843;&#25972;&#26041;&#38754;&#24456;&#26377;&#21069;&#26223;&#65292;&#20294;&#24448;&#24448;&#20250;&#23548;&#33268;&#34920;&#38754;&#35843;&#25972;&#65292;&#20248;&#20808;&#32771;&#34385;&#39118;&#26684;&#21464;&#21270;&#32780;&#38750;&#25913;&#21892;LLMs&#30340;&#19979;&#28216;&#24615;&#33021;&#12290;&#26410;&#26126;&#30830;&#35268;&#23450;&#30340;&#20559;&#22909;&#21487;&#33021;&#20250;&#27169;&#31946;&#23545;&#40784;&#27169;&#22411;&#30340;&#26041;&#21521;&#12290;&#32570;&#20047;&#25506;&#32034;&#20250;&#38480;&#21046;&#35782;&#21035;&#20986;&#25913;&#21892;&#27169;&#22411;&#30340;&#29702;&#24819;&#36755;&#20986;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65306;&#21453;&#24605;&#21453;&#39304;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLRF&#65289;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#22522;&#20110;&#35814;&#32454;&#26631;&#20934;&#30340;&#32454;&#31890;&#24230;&#21453;&#39304;&#65292;&#20197;&#25913;&#36827;LLMs&#30340;&#26680;&#24515;&#33021;&#21147;&#12290;RLRF&#37319;&#29992;&#33258;&#25105;&#21453;&#24605;&#26426;&#21046;&#31995;&#32479;&#22320;&#25506;&#32034;&#21644;&#23436;&#21892;LLMs&#30340;&#22238;&#24212;&#65292;&#28982;&#21518;&#36890;&#36807;RL&#31639;&#27861;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#21516;&#26102;&#32467;&#21512;&#26377;&#24076;&#26395;&#30340;&#22238;&#24212;&#12290;&#22312;Just-Eval&#12289;Factuality&#21644;Mathematical Reasoning&#31561;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;RLRF&#30340;&#21151;&#25928;&#21644;&#36229;&#36234;&#34920;&#38754;&#35843;&#25972;&#30340;&#36716;&#21464;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14238v1 Announce Type: cross  Abstract: Despite the promise of RLHF in aligning LLMs with human preferences, it often leads to superficial alignment, prioritizing stylistic changes over improving downstream performance of LLMs. Underspecified preferences could obscure directions to align the models. Lacking exploration restricts identification of desirable outputs to improve the models. To overcome these challenges, we propose a novel framework: Reinforcement Learning from Reflective Feedback (RLRF), which leverages fine-grained feedback based on detailed criteria to improve the core capabilities of LLMs. RLRF employs a self-reflection mechanism to systematically explore and refine LLM responses, then fine-tuning the models via a RL algorithm along with promising responses. Our experiments across Just-Eval, Factuality, and Mathematical Reasoning demonstrate the efficacy and transformative potential of RLRF beyond superficial surface-level adjustment.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#32479;&#19968;&#26694;&#26550;&#32467;&#21512;&#20102;&#8220;&#23450;&#20301;&#21644;&#32534;&#36753;&#8221;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#65292;&#26368;&#22823;&#21270;&#20445;&#30041;&#26576;&#20123;&#21521;&#37327;&#34920;&#31034;&#24182;&#35760;&#24518;&#26032;&#20107;&#23454;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.14236</link><description>&lt;p&gt;
&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#32534;&#36753;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Unified Framework for Model Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14236
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#32479;&#19968;&#26694;&#26550;&#32467;&#21512;&#20102;&#8220;&#23450;&#20301;&#21644;&#32534;&#36753;&#8221;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#65292;&#26368;&#22823;&#21270;&#20445;&#30041;&#26576;&#20123;&#21521;&#37327;&#34920;&#31034;&#24182;&#35760;&#24518;&#26032;&#20107;&#23454;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#26159;&#19968;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#19987;&#27880;&#20110;&#26356;&#26032;&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#30693;&#35782;&#12290;&#22312;&#21508;&#31181;&#26041;&#27861;&#20013;&#65292;ROME&#21644;MEMIT&#20316;&#20026;&#20027;&#35201;&#30340;&#8220;&#23450;&#20301;&#21644;&#32534;&#36753;&#8221;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#33073;&#39062;&#32780;&#20986;&#12290;&#32780;MEMIT&#21487;&#20197;&#25209;&#37327;&#32534;&#36753;&#35760;&#24518;&#65292;ROME&#21017;&#19968;&#27425;&#21482;&#33021;&#25913;&#21464;&#19968;&#20010;&#20107;&#23454;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#23558;ROME&#21644;MEMIT&#32435;&#20837;&#19968;&#20010;&#21333;&#19968;&#30340;&#27010;&#24565;&#26694;&#26550;&#65292;&#20248;&#21270;&#21516;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#20445;&#23384;-&#35760;&#24518;&#8221;&#30446;&#26631;&#12290;&#35813;&#30446;&#26631;&#26088;&#22312;&#22312;&#35760;&#24518;&#26032;&#20107;&#23454;&#20449;&#24687;&#30340;&#21516;&#26102;&#20445;&#30041;&#26576;&#20123;&#36873;&#23450;&#21521;&#37327;&#30340;&#34920;&#31034;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;ROME&#20351;&#29992;&#31561;&#24335;&#32422;&#26463;&#20248;&#21270;&#27492;&#30446;&#26631;&#65292;&#32780;MEMIT&#37319;&#29992;&#26356;&#28789;&#27963;&#30340;&#26368;&#23567;&#20108;&#20056;&#32422;&#26463;&#12290;&#38500;&#20102;&#25209;&#37327;&#32534;&#36753;&#22806;&#65292;MEMIT&#36824;&#21487;&#20197;&#22312;&#22810;&#20010;&#23618;&#38754;&#32534;&#36753;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#32534;&#36753;&#30340;&#20998;&#24067;&#20174;&#22810;&#20010;&#23618;&#38754;&#20998;&#24320;&#65292;&#21306;&#21035;&#20110;&#20248;&#21270;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14236v1 Announce Type: cross  Abstract: Model editing is a growing area focused on updating the knowledge embedded within models. Among the various methodologies, ROME and MEMIT stand out as leading "locate-and-edit" model editing techniques. While MEMIT enables batched editing of memories, ROME is limited to changing one fact at a time. This paper introduces a unifying framework that brings ROME and MEMIT under a single conceptual umbrella, optimizing for the same goal, which we call the "preservation-memorization" objective. This objective aims to preserve the representations of certain selected vectors while memorizing the representations of new factual information. Specifically, ROME optimizes this objective using an equality constraint, whereas MEMIT employs a more flexible least-square constraint. In addition to making batched edits, MEMIT also edits the model at multiple layers. We disentangle the distribution of edits to multiple layers from the optimization objectiv
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22823;&#35268;&#27169;&#25193;&#23637;&#23454;&#20307;&#31867;&#22411;&#25968;&#37327;&#21644;&#31890;&#24230;&#65292;&#30740;&#31350;&#20102;&#24378;&#35821;&#20041;&#20808;&#39564;&#23545;&#20110;&#35299;&#37322;&#23454;&#20307;&#31867;&#22411;&#25991;&#26412;&#21270;&#25551;&#36848;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.14222</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#26631;&#31614;&#35299;&#37322;&#23398;&#20064;&#29992;&#20110;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Large-Scale Label Interpretation Learning for Few-Shot Named Entity Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14222
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22823;&#35268;&#27169;&#25193;&#23637;&#23454;&#20307;&#31867;&#22411;&#25968;&#37327;&#21644;&#31890;&#24230;&#65292;&#30740;&#31350;&#20102;&#24378;&#35821;&#20041;&#20808;&#39564;&#23545;&#20110;&#35299;&#37322;&#23454;&#20307;&#31867;&#22411;&#25991;&#26412;&#21270;&#25551;&#36848;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#20165;&#20351;&#29992;&#23569;&#37327;&#27880;&#37322;&#31034;&#20363;&#22312;&#25991;&#26412;&#20013;&#26816;&#27979;&#21629;&#21517;&#23454;&#20307;&#12290; &#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#26159;&#21033;&#29992;&#27599;&#31181;&#23454;&#20307;&#31867;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65306;&#20363;&#22914;&#65292;&#24120;&#35265;&#26631;&#31614;PER&#21487;&#33021;&#34987;&#34920;&#36798;&#20026;&#8220;&#20154;&#29289;&#23454;&#20307;&#8221;&#12290; &#22312;&#21021;&#22987;&#26631;&#31614;&#35299;&#37322;&#23398;&#20064;&#38454;&#27573;&#65292;&#27169;&#22411;&#23398;&#20064;&#35299;&#37322;&#36825;&#20123;&#23454;&#20307;&#31867;&#22411;&#30340;&#25991;&#26412;&#21270;&#25551;&#36848;&#12290; &#22312;&#38543;&#21518;&#30340;&#23569;&#26679;&#26412;&#26631;&#31614;&#38598;&#25193;&#23637;&#38454;&#27573;&#65292;&#35813;&#27169;&#22411;&#28982;&#21518;&#32473;&#20986;&#20808;&#21069;&#26410;&#35265;&#23454;&#20307;&#31867;&#22411;&#30340;&#25551;&#36848;&#65288;&#20363;&#22914;&#8220;&#38899;&#20048;&#19987;&#36753;&#8221;&#65289;&#20197;&#21450;&#21487;&#36873;&#30340;&#23569;&#37327;&#35757;&#32451;&#31034;&#20363;&#65292;&#25191;&#34892;&#35813;&#31867;&#22411;&#30340;&#23569;&#26679;&#26412;NER&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#25506;&#35752;&#20102;&#24378;&#35821;&#20041;&#20808;&#39564;&#23545;&#20110;&#36890;&#36807;&#22823;&#35268;&#27169;&#25193;&#23637;&#29992;&#20110;&#26631;&#31614;&#35299;&#37322;&#23398;&#20064;&#30340;&#23454;&#20307;&#31867;&#22411;&#25968;&#37327;&#21644;&#31890;&#24230;&#30340;&#24433;&#21709;&#12290; &#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#23454;&#20307;&#38142;&#25509;&#22522;&#20934;&#26469;&#21019;&#24314;&#19968;&#20010;&#25968;&#25454;&#38598;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14222v1 Announce Type: new  Abstract: Few-shot named entity recognition (NER) detects named entities within text using only a few annotated examples. One promising line of research is to leverage natural language descriptions of each entity type: the common label PER might, for example, be verbalized as ''person entity.'' In an initial label interpretation learning phase, the model learns to interpret such verbalized descriptions of entity types. In a subsequent few-shot tagset extension phase, this model is then given a description of a previously unseen entity type (such as ''music album'') and optionally a few training examples to perform few-shot NER for this type. In this paper, we systematically explore the impact of a strong semantic prior to interpret verbalizations of new entity types by massively scaling up the number and granularity of entity types used for label interpretation learning. To this end, we leverage an entity linking benchmark to create a dataset with
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19968;&#33268;&#24615;&#23545;&#40784;&#35757;&#32451;&#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#23545;&#25351;&#20196;&#30340;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.14221</link><description>&lt;p&gt;
&#36890;&#36807;&#19968;&#33268;&#24615;&#23545;&#40784;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving the Robustness of Large Language Models via Consistency Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14221
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19968;&#33268;&#24615;&#23545;&#40784;&#35757;&#32451;&#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#23545;&#25351;&#20196;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#36981;&#24490;&#29992;&#25143;&#25351;&#20196;&#21644;&#29983;&#25104;&#26377;&#29992;&#21709;&#24212;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#20173;&#36828;&#26410;&#36798;&#21040;&#26368;&#20339;&#29366;&#24577;&#65292;&#22240;&#20026;&#21487;&#33021;&#30001;&#20110;&#21475;&#22836;&#25351;&#20196;&#20013;&#30340;&#32454;&#24494;&#26356;&#25913;&#32780;&#20135;&#29983;&#26126;&#26174;&#19981;&#19968;&#33268;&#30340;&#21709;&#24212;&#12290;&#26368;&#36817;&#30340;&#25991;&#29486;&#25506;&#35752;&#20102;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#32487;&#32493;&#25913;&#21892;&#21709;&#24212;&#29983;&#25104;&#30340;&#40065;&#26834;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#23545;&#31995;&#32479;&#24615;&#20998;&#26512;&#21644;&#35299;&#20915;&#26041;&#26696;&#20173;&#28982;&#32570;&#20047;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23450;&#37327;&#23450;&#20041;&#20102;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#30001;&#25351;&#20196;&#22686;&#24378;&#30417;&#30563;&#24494;&#35843;&#21644;&#19968;&#33268;&#24615;&#23545;&#40784;&#35757;&#32451;&#32452;&#25104;&#30340;&#20004;&#38454;&#27573;&#35757;&#32451;&#26694;&#26550;&#12290;&#31532;&#19968;&#38454;&#27573;&#36890;&#36807;&#31867;&#20284;&#25351;&#20196;&#22686;&#24378;&#24110;&#21161;&#27169;&#22411;&#22312;&#36981;&#24490;&#25351;&#20196;&#26102;&#27867;&#21270;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;&#25552;&#39640;&#20102;&#22810;&#26679;&#24615;&#65292;&#24182;&#24110;&#21161;&#27169;&#22411;&#29702;&#35299;&#21738;&#20123;&#21709;&#24212;&#19982;&#20154;&#31867;&#26399;&#26395;&#26356;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14221v1 Announce Type: new  Abstract: Large language models (LLMs) have shown tremendous success in following user instructions and generating helpful responses. Nevertheless, their robustness is still far from optimal, as they may generate significantly inconsistent responses due to minor changes in the verbalized instructions. Recent literature has explored this inconsistency issue, highlighting the importance of continued improvement in the robustness of response generation. However, systematic analysis and solutions are still lacking. In this paper, we quantitatively define the inconsistency problem and propose a two-stage training framework consisting of instruction-augmented supervised fine-tuning and consistency alignment training. The first stage helps a model generalize on following instructions via similar instruction augmentations. In the second stage, we improve the diversity and help the model understand which responses are more aligned with human expectations b
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38024;&#23545;&#20799;&#31461;-&#30475;&#25252;&#32773;&#23545;&#35805;&#20013;&#19978;&#19979;&#25991;&#20381;&#36182;&#35821;&#27861;&#30340;&#32534;&#30721;&#26041;&#26696;&#65292;&#24182;&#35757;&#32451;&#24182;&#35780;&#20272;&#20102;&#19968;&#31995;&#21015;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#65292;&#32467;&#26524;&#26174;&#31034;&#32463;&#36807;&#24494;&#35843;&#30340;Transformer&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>https://arxiv.org/abs/2403.14208</link><description>&lt;p&gt;
&#20799;&#31461;-&#30475;&#25252;&#32773;&#23545;&#35805;&#20013;&#35821;&#27861;&#27491;&#30830;&#24615;&#30340;&#33258;&#21160;&#27880;&#37322;
&lt;/p&gt;
&lt;p&gt;
Automatic Annotation of Grammaticality in Child-Caregiver Conversations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14208
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38024;&#23545;&#20799;&#31461;-&#30475;&#25252;&#32773;&#23545;&#35805;&#20013;&#19978;&#19979;&#25991;&#20381;&#36182;&#35821;&#27861;&#30340;&#32534;&#30721;&#26041;&#26696;&#65292;&#24182;&#35757;&#32451;&#24182;&#35780;&#20272;&#20102;&#19968;&#31995;&#21015;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#65292;&#32467;&#26524;&#26174;&#31034;&#32463;&#36807;&#24494;&#35843;&#30340;Transformer&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#27861;&#30340;&#20064;&#24471;&#19968;&#30452;&#26159;&#35821;&#35328;&#20064;&#24471;&#29702;&#35770;&#20043;&#38388;&#30340;&#19968;&#39033;&#26680;&#24515;&#38382;&#39064;&#12290;&#20026;&#20102;&#22312;&#20799;&#31461;-&#30475;&#25252;&#32773;&#23545;&#35805;&#20013;&#36827;&#34892;&#26356;&#24555;&#36895;&#12289;&#26356;&#21487;&#37325;&#22797;&#12289;&#26356;&#22823;&#35268;&#27169;&#30340;&#35821;&#27861;&#30740;&#31350;&#65292;&#33258;&#21160;&#27880;&#37322;&#24037;&#20855;&#21487;&#20197;&#20026;&#32321;&#29712;&#30340;&#25163;&#21160;&#27880;&#37322;&#25552;&#20379;&#26377;&#25928;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#20799;&#31461;-&#30475;&#25252;&#32773;&#23545;&#35805;&#20013;&#30340;&#19978;&#19979;&#25991;&#20381;&#36182;&#35821;&#27861;&#30340;&#32534;&#30721;&#26041;&#26696;&#65292;&#24182;&#23545;&#22823;&#37327;&#36716;&#24405;&#23545;&#35805;&#35821;&#26009;&#24211;&#20013;&#30340;4000&#22810;&#20010;&#35805;&#35821;&#36827;&#34892;&#20102;&#27880;&#37322;&#12290;&#22522;&#20110;&#36825;&#20123;&#27880;&#37322;&#65292;&#25105;&#20204;&#35757;&#32451;&#21644;&#35780;&#20272;&#20102;&#19968;&#31995;&#21015;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#65292;&#36798;&#21040;&#20102;&#20154;&#31867;&#20043;&#38388;&#30340;&#27880;&#37322;&#19968;&#33268;&#24615;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14208v1 Announce Type: new  Abstract: The acquisition of grammar has been a central question to adjudicate between theories of language acquisition. In order to conduct faster, more reproducible, and larger-scale corpus studies on grammaticality in child-caregiver conversations, tools for automatic annotation can offer an effective alternative to tedious manual annotation. We propose a coding scheme for context-dependent grammaticality in child-caregiver conversations and annotate more than 4,000 utterances from a large corpus of transcribed conversations. Based on these annotations, we train and evaluate a range of NLP models. Our results show that fine-tuned Transformer-based models perform best, achieving human inter-annotation agreement levels.As a first application and sanity check of this tool, we use the trained models to annotate a corpus almost two orders of magnitude larger than the manually annotated data and verify that children's grammaticality shows a steady in
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#35757;&#32451; Fusion-in-Decoder&#65288;FiD&#65289;&#26102;&#19978;&#19979;&#25991;&#25968;&#37327;&#21644;&#36136;&#37327;&#23545;&#25552;&#21462;&#24335;&#24320;&#25918;&#22495;&#38382;&#31572;&#20219;&#21153;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;FiD&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#33021;&#20250;&#36807;&#25311;&#21512;&#21040;&#19978;&#19979;&#25991;&#36136;&#37327;&#65292;&#23548;&#33268;&#22312;&#19981;&#21516;&#36136;&#37327;&#19978;&#19979;&#25991;&#35780;&#20272;&#26102;&#34920;&#29616;&#20986;&#20122;&#20248;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.14197</link><description>&lt;p&gt;
&#22312;&#25552;&#21462;&#24335;&#24320;&#25918;&#22495;&#38382;&#31572;&#20013;&#65292;&#35757;&#32451;&#35299;&#30721;&#22120;&#20013;&#30340;&#34701;&#21512;&#25216;&#26415;&#26102;&#65292;&#19978;&#19979;&#25991;&#36136;&#37327;&#24456;&#37325;&#35201;
&lt;/p&gt;
&lt;p&gt;
Context Quality Matters in Training Fusion-in-Decoder for Extractive Open-Domain Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14197
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#35757;&#32451; Fusion-in-Decoder&#65288;FiD&#65289;&#26102;&#19978;&#19979;&#25991;&#25968;&#37327;&#21644;&#36136;&#37327;&#23545;&#25552;&#21462;&#24335;&#24320;&#25918;&#22495;&#38382;&#31572;&#20219;&#21153;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;FiD&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#33021;&#20250;&#36807;&#25311;&#21512;&#21040;&#19978;&#19979;&#25991;&#36136;&#37327;&#65292;&#23548;&#33268;&#22312;&#19981;&#21516;&#36136;&#37327;&#19978;&#19979;&#25991;&#35780;&#20272;&#26102;&#34920;&#29616;&#20986;&#20122;&#20248;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#27169;&#22411;&#36890;&#36807;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#25552;&#20379;&#39069;&#22806;&#30456;&#20851;&#30340;&#22806;&#37096;&#30693;&#35782;&#65288;&#19978;&#19979;&#25991;&#65289;&#26469;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#20013;&#32534;&#30721;&#30340;&#30693;&#35782;&#12290;&#23613;&#31649;&#24050;&#32463;&#26174;&#31034;&#20986;&#19978;&#19979;&#25991;&#30340;&#25968;&#37327;&#21644;&#36136;&#37327;&#20250;&#24433;&#21709;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#27169;&#22411;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#30340;&#24615;&#33021;&#65292;&#20294;&#26377;&#38480;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#36825;&#20123;&#29305;&#24449;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#35757;&#32451;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#19978;&#19979;&#25991;&#25968;&#37327;&#21644;&#36136;&#37327;&#22914;&#20309;&#24433;&#21709;&#34701;&#21512;&#35299;&#30721;&#22120;&#65288;FiD&#65289;&#22312;&#25552;&#21462;&#24335;&#24320;&#25918;&#22495;&#38382;&#31572;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;FiD&#26159;&#30446;&#21069;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FiD&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36807;&#25311;&#21512;&#21040;&#19978;&#19979;&#25991;&#36136;&#37327;&#65292;&#24182;&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#36136;&#37327;&#30340;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#20122;&#20248;&#24615;&#33021;&#12290;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#36824;&#25581;&#31034;&#20102;&#20351;&#29992;&#19981;&#21516;&#19978;&#19979;&#25991;&#36136;&#37327;&#35757;&#32451;&#30340;FiD&#27169;&#22411;&#20855;&#26377;&#19981;&#21516;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#20998;&#24067;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14197v1 Announce Type: new  Abstract: Retrieval-augmented generation models augment knowledge encoded in a language model by providing additional relevant external knowledge (context) during generation. Although it has been shown that the quantity and quality of context impact the performance of retrieval-augmented generation models during inference, limited research explores how these characteristics affect model training. This paper explores how context quantity and quality during model training affect the performance of Fusion-in-Decoder (FiD), the state-of-the-art retrieval-augmented generation model, in extractive open-domain question answering tasks. Experimental results suggest that FiD models overfit to context quality during training and show suboptimal performance when evaluated on different context quality. Through the experimental results, we also reveal FiD models trained with different context quality have different cross-attention distribution patterns. Specif
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MMIDR&#26694;&#26550;&#65292;&#29992;&#20110;&#25945;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#35299;&#37322;&#20854;&#22810;&#27169;&#24577;&#34394;&#20551;&#20449;&#24687;&#20915;&#31574;&#36807;&#31243;&#30340;&#25991;&#26412;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2403.14171</link><description>&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#22810;&#27169;&#24577;&#34394;&#20551;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
MMIDR: Teaching Large Language Model to Interpret Multimodal Misinformation via Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14171
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MMIDR&#26694;&#26550;&#65292;&#29992;&#20110;&#25945;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#35299;&#37322;&#20854;&#22810;&#27169;&#24577;&#34394;&#20551;&#20449;&#24687;&#20915;&#31574;&#36807;&#31243;&#30340;&#25991;&#26412;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22810;&#27169;&#24577;&#34394;&#20551;&#20449;&#24687;&#30340;&#33258;&#21160;&#26816;&#27979;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22810;&#27169;&#24577;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#26041;&#38754;&#30340;&#28508;&#21147;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#21457;&#25496;&#12290;&#27492;&#22806;&#65292;&#22914;&#20309;&#20197;&#25104;&#26412;&#25928;&#30410;&#21644;&#26131;&#20110;&#35775;&#38382;&#30340;&#26041;&#24335;&#25945;&#23548;LLMs&#35299;&#37322;&#22810;&#27169;&#24577;&#34394;&#20551;&#20449;&#24687;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MMIDR&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#25945;&#23548;LLMs&#20026;&#20854;&#22810;&#27169;&#24577;&#34394;&#20551;&#20449;&#24687;&#20915;&#31574;&#36807;&#31243;&#25552;&#20379;&#27969;&#30021;&#21644;&#39640;&#36136;&#37327;&#25991;&#26412;&#35299;&#37322;&#30340;&#26694;&#26550;&#12290;&#20026;&#20102;&#23558;&#22810;&#27169;&#24577;&#34394;&#20551;&#20449;&#24687;&#36716;&#21270;&#20026;&#36866;&#24403;&#30340;&#25351;&#20196;&#25191;&#34892;&#26684;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#22686;&#24378;&#35270;&#35282;&#21644;&#31649;&#36947;&#12290;&#35813;&#31649;&#36947;&#21253;&#25324;&#19968;&#20010;&#35270;&#35273;&#20449;&#24687;&#22788;&#29702;&#27169;&#22359;&#21644;&#19968;&#20010;&#35777;&#25454;&#26816;&#32034;&#27169;&#22359;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#22788;&#29702;&#36807;&#30340;&#20869;&#23481;&#25552;&#31034;&#19987;&#26377;&#30340;LLMs&#20026;&#35299;&#37322;&#22810;&#27169;&#24577;&#34394;&#20551;&#20449;&#24687;&#30340;&#30495;&#23454;&#24615;&#25552;&#21462;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14171v1 Announce Type: new  Abstract: Automatic detection of multimodal misinformation has gained a widespread attention recently. However, the potential of powerful Large Language Models (LLMs) for multimodal misinformation detection remains underexplored. Besides, how to teach LLMs to interpret multimodal misinformation in cost-effective and accessible way is still an open question. To address that, we propose MMIDR, a framework designed to teach LLMs in providing fluent and high-quality textual explanations for their decision-making process of multimodal misinformation. To convert multimodal misinformation into an appropriate instruction-following format, we present a data augmentation perspective and pipeline. This pipeline consists of a visual information processing module and an evidence retrieval module. Subsequently, we prompt the proprietary LLMs with processed contents to extract rationales for interpreting the authenticity of multimodal misinformation. Furthermore
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;M$^3$AV&#38899;&#35270;&#39057;&#23398;&#26415;&#35762;&#24231;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#22810;&#27169;&#24577;&#12289;&#22810;&#20307;&#35009;&#21644;&#39640;&#36136;&#37327;&#20154;&#24037;&#27880;&#37322;&#65292;&#21487;&#29992;&#20110;&#22810;&#31181;&#38899;&#35270;&#39057;&#35782;&#21035;&#20219;&#21153;</title><link>https://arxiv.org/abs/2403.14168</link><description>&lt;p&gt;
M$^3$AV&#65306;&#19968;&#31181;&#22810;&#27169;&#24577;&#12289;&#22810;&#20307;&#35009;&#21644;&#22810;&#29992;&#36884;&#30340;&#38899;&#35270;&#39057;&#23398;&#26415;&#35762;&#24231;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
M$^3$AV: A Multimodal, Multigenre, and Multipurpose Audio-Visual Academic Lecture Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14168
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;M$^3$AV&#38899;&#35270;&#39057;&#23398;&#26415;&#35762;&#24231;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#22810;&#27169;&#24577;&#12289;&#22810;&#20307;&#35009;&#21644;&#39640;&#36136;&#37327;&#20154;&#24037;&#27880;&#37322;&#65292;&#21487;&#29992;&#20110;&#22810;&#31181;&#38899;&#35270;&#39057;&#35782;&#21035;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14168v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#21457;&#24067;&#24320;&#28304;&#23398;&#26415;&#35270;&#39057;&#24405;&#20687;&#26159;&#22312;&#32447;&#20998;&#20139;&#30693;&#35782;&#30340;&#19968;&#31181;&#26032;&#20852;&#21644;&#26222;&#36941;&#26041;&#27861;&#12290;&#36825;&#20123;&#35270;&#39057;&#21253;&#21547;&#20016;&#23500;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#21253;&#25324;&#28436;&#35762;&#32773;&#30340;&#35821;&#38899;&#12289;&#38754;&#37096;&#21644;&#36523;&#20307;&#21160;&#20316;&#65292;&#20197;&#21450;&#24187;&#28783;&#29255;&#20013;&#30340;&#25991;&#26412;&#21644;&#22270;&#29255;&#65292;&#29978;&#33267;&#21487;&#33021;&#21253;&#25324;&#35770;&#25991;&#20869;&#23481;&#12290;&#23613;&#31649;&#24050;&#26500;&#24314;&#21644;&#21457;&#24067;&#20102;&#22810;&#20010;&#23398;&#26415;&#35270;&#39057;&#25968;&#25454;&#38598;&#65292;&#20294;&#24456;&#23569;&#26377;&#25968;&#25454;&#38598;&#25903;&#25345;&#22810;&#27169;&#24577;&#20869;&#23481;&#35782;&#21035;&#21644;&#29702;&#35299;&#20219;&#21153;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#32570;&#20047;&#39640;&#36136;&#37327;&#30340;&#20154;&#24037;&#27880;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#12289;&#22810;&#20307;&#35009;&#21644;&#22810;&#29992;&#36884;&#30340;&#38899;&#35270;&#39057;&#23398;&#26415;&#35762;&#24231;&#25968;&#25454;&#38598;(M$^3$AV)&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#26469;&#33258;&#20116;&#20010;&#26469;&#28304;&#30340;&#36817;367&#23567;&#26102;&#30340;&#35270;&#39057;&#65292;&#28085;&#30422;&#35745;&#31639;&#26426;&#31185;&#23398;&#12289;&#25968;&#23398;&#20197;&#21450;&#21307;&#23398;&#21644;&#29983;&#29289;&#23398;&#31561;&#20027;&#39064;&#12290;&#36890;&#36807;&#23545;&#35328;&#35821;&#21644;&#20070;&#38754;&#25991;&#23383;&#65288;&#23588;&#20854;&#26159;&#39640;&#20215;&#20540;&#21517;&#31216;&#23454;&#20307;&#65289;&#30340;&#39640;&#36136;&#37327;&#20154;&#24037;&#27880;&#37322;&#65292;&#35813;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#22810;&#31181;&#38899;&#35270;&#39057;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14168v1 Announce Type: new  Abstract: Publishing open-source academic video recordings is an emergent and prevalent approach to sharing knowledge online. Such videos carry rich multimodal information including speech, the facial and body movements of the speakers, as well as the texts and pictures in the slides and possibly even the papers. Although multiple academic video datasets have been constructed and released, few of them support both multimodal content recognition and understanding tasks, which is partially due to the lack of high-quality human annotations. In this paper, we propose a novel multimodal, multigenre, and multipurpose audio-visual academic lecture dataset (M$^3$AV), which has almost 367 hours of videos from five sources covering computer science, mathematics, and medical and biology topics. With high-quality human annotations of the spoken and written words, in particular high-valued name entities, the dataset can be used for multiple audio-visual recogn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#25972;&#36807;&#31243;&#20013;&#36890;&#36807;&#21033;&#29992;CLIP&#30340;&#22266;&#26377;&#23646;&#24615;&#26469;&#25506;&#35752;&#26657;&#20934;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#25552;&#31034;&#36873;&#25321;&#26174;&#33879;&#24433;&#21709;&#20102;CLIP&#20013;&#30340;&#26657;&#20934;&#65292;&#20854;&#20013;&#23548;&#33268;&#26356;&#39640;&#25991;&#26412;&#29305;&#24449;&#31163;&#25955;&#24615;&#30340;&#25552;&#31034;&#20250;&#20135;&#29983;&#26356;&#22909;&#26657;&#20934;&#30340;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.14119</link><description>&lt;p&gt;
C-TPT&#65306;&#36890;&#36807;&#25991;&#26412;&#29305;&#24449;&#31163;&#25955;&#24615;&#30340;&#26657;&#20934;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#25972;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via Text Feature Dispersion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#25972;&#36807;&#31243;&#20013;&#36890;&#36807;&#21033;&#29992;CLIP&#30340;&#22266;&#26377;&#23646;&#24615;&#26469;&#25506;&#35752;&#26657;&#20934;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#25552;&#31034;&#36873;&#25321;&#26174;&#33879;&#24433;&#21709;&#20102;CLIP&#20013;&#30340;&#26657;&#20934;&#65292;&#20854;&#20013;&#23548;&#33268;&#26356;&#39640;&#25991;&#26412;&#29305;&#24449;&#31163;&#25955;&#24615;&#30340;&#25552;&#31034;&#20250;&#20135;&#29983;&#26356;&#22909;&#26657;&#20934;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#27979;&#35797;&#26102;&#36866;&#24212;&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#20316;&#20026;&#19968;&#31181;&#22312;&#19981;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;&#26041;&#27861;&#12290;&#19968;&#20010;&#20027;&#35201;&#30340;&#20363;&#35777;&#26159;&#26368;&#36817;&#25552;&#20986;&#30340;&#29992;&#20110;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#30340;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#25972;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25552;&#31034;&#20027;&#35201;&#26159;&#20026;&#20102;&#25552;&#39640;&#20934;&#30830;&#24615;&#32780;&#24320;&#21457;&#30340;&#65292;&#24573;&#35270;&#20102;&#26657;&#20934;&#30340;&#37325;&#35201;&#24615;&#8212;&#8212;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#26657;&#20934;&#26041;&#27861;&#20381;&#36182;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#22312;&#27979;&#35797;&#26102;&#22330;&#26223;&#19979;&#19981;&#20999;&#23454;&#38469;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;CLIP&#30340;&#22266;&#26377;&#23646;&#24615;&#65292;&#22312;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#25972;&#36807;&#31243;&#20013;&#25506;&#35752;&#26657;&#20934;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#35266;&#23519;&#65292;&#25105;&#20204;&#21457;&#29616;&#25552;&#31034;&#36873;&#25321;&#26174;&#33879;&#24433;&#21709;&#20102;CLIP&#20013;&#30340;&#26657;&#20934;&#65292;&#20854;&#20013;&#23548;&#33268;&#26356;&#39640;&#25991;&#26412;&#29305;&#24449;&#31163;&#25955;&#24615;&#30340;&#25552;&#31034;&#20250;&#20135;&#29983;&#26356;&#22909;&#26657;&#20934;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14119v1 Announce Type: cross  Abstract: In deep learning, test-time adaptation has gained attention as a method for model fine-tuning without the need for labeled data. A prime exemplification is the recently proposed test-time prompt tuning for large-scale vision-language models such as CLIP. Unfortunately, these prompts have been mainly developed to improve accuracy, overlooking the importance of calibration-a crucial aspect for quantifying prediction uncertainty. However, traditional calibration methods rely on substantial amounts of labeled data, making them impractical for test-time scenarios. To this end, this paper explores calibration during test-time prompt tuning by leveraging the inherent properties of CLIP. Through a series of observations, we find that the prompt choice significantly affects the calibration in CLIP, where the prompts leading to higher text feature dispersion result in better-calibrated predictions. Introducing the Average Text Feature Dispersion
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#35780;&#20272;&#39046;&#22495;&#30340;&#21457;&#23637;&#21382;&#21490;&#36827;&#34892;&#20102;&#20840;&#38754;&#27010;&#36848;&#65292;&#23558;&#26041;&#27861;&#20998;&#20026;&#22522;&#20110;&#25163;&#24037;&#29305;&#24449;&#12289;&#28145;&#24230;&#23398;&#20064;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19977;&#31867;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2403.14118</link><description>&lt;p&gt;
&#20174;&#25163;&#24037;&#29305;&#24449;&#21040;LLMs&#65306;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#35780;&#20272;&#31616;&#35201;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14118
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#35780;&#20272;&#39046;&#22495;&#30340;&#21457;&#23637;&#21382;&#21490;&#36827;&#34892;&#20102;&#20840;&#38754;&#27010;&#36848;&#65292;&#23558;&#26041;&#27861;&#20998;&#20026;&#22522;&#20110;&#25163;&#24037;&#29305;&#24449;&#12289;&#28145;&#24230;&#23398;&#20064;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19977;&#31867;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#35780;&#20272;&#65288;MTQE&#65289;&#26159;&#22312;&#23454;&#26102;&#29615;&#22659;&#20013;&#20272;&#35745;&#26426;&#22120;&#32763;&#35793;&#25991;&#26412;&#36136;&#37327;&#30340;&#20219;&#21153;&#65292;&#26080;&#38656;&#21442;&#32771;&#32763;&#35793;&#65292;&#23545;&#20110;MT&#30340;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#23545;QE&#25968;&#25454;&#38598;&#12289;&#26631;&#27880;&#26041;&#27861;&#12289;&#20849;&#20139;&#20219;&#21153;&#12289;&#26041;&#27861;&#35770;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#36827;&#34892;&#20102;&#20840;&#38754;&#27010;&#36848;&#12290;&#23427;&#20174;&#20171;&#32461;QE&#30340;&#32972;&#26223;&#21644;&#24847;&#20041;&#24320;&#22987;&#65292;&#28982;&#21518;&#35299;&#37322;&#20102;&#35789;&#32423;QE&#12289;&#21477;&#32423;QE&#12289;&#25991;&#26723;&#32423;QE&#21644;&#21487;&#35299;&#37322;QE&#30340;&#27010;&#24565;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#25991;&#31456;&#23558;QE&#21457;&#23637;&#21382;&#21490;&#20013;&#20135;&#29983;&#30340;&#26041;&#27861;&#20998;&#20026;&#22522;&#20110;&#25163;&#24037;&#29305;&#24449;&#12289;&#28145;&#24230;&#23398;&#20064;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26041;&#27861;&#65292;&#36827;&#19968;&#27493;&#23558;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#20998;&#20026;&#32463;&#20856;&#28145;&#24230;&#23398;&#20064;&#21644;&#21253;&#21547;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14118v1 Announce Type: new  Abstract: Machine Translation Quality Estimation (MTQE) is the task of estimating the quality of machine-translated text in real time without the need for reference translations, which is of great importance for the development of MT. After two decades of evolution, QE has yielded a wealth of results. This article provides a comprehensive overview of QE datasets, annotation methods, shared tasks, methodologies, challenges, and future research directions. It begins with an introduction to the background and significance of QE, followed by an explanation of the concepts and evaluation metrics for word-level QE, sentence-level QE, document-level QE, and explainable QE. The paper categorizes the methods developed throughout the history of QE into those based on handcrafted features, deep learning, and Large Language Models (LLMs), with a further division of deep learning-based methods into classic deep learning and those incorporating pre-trained lang
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#35774;&#35745;&#31354;&#38388;&#65292;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#35774;&#35745;&#24072;&#22312;&#22810;&#32500;&#31354;&#38388;&#20013;&#26816;&#39564;&#21644;&#25506;&#32034;&#26234;&#33021;&#20132;&#20114;&#24335;&#20889;&#20316;&#21161;&#25163;&#30340;&#21508;&#31181;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.14117</link><description>&lt;p&gt;
&#19968;&#31181;&#26234;&#33021;&#20132;&#20114;&#24335;&#20889;&#20316;&#21161;&#25163;&#30340;&#35774;&#35745;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
A Design Space for Intelligent and Interactive Writing Assistants
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14117
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#35774;&#35745;&#31354;&#38388;&#65292;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#35774;&#35745;&#24072;&#22312;&#22810;&#32500;&#31354;&#38388;&#20013;&#26816;&#39564;&#21644;&#25506;&#32034;&#26234;&#33021;&#20132;&#20114;&#24335;&#20889;&#20316;&#21161;&#25163;&#30340;&#21508;&#31181;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25105;&#20204;&#36825;&#20010;&#24555;&#36895;&#31185;&#25216;&#21457;&#23637;&#30340;&#26102;&#20195;&#65292;&#20889;&#20316;&#21161;&#25163;&#30340;&#30740;&#31350;&#39046;&#22495;&#24050;&#32463;&#22312;&#21508;&#20010;&#30740;&#31350;&#31038;&#21306;&#20013;&#21464;&#24471;&#26085;&#30410;&#20998;&#25955;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#35774;&#35745;&#31354;&#38388;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#20316;&#20026;&#19968;&#31181;&#32467;&#26500;&#21270;&#30340;&#26041;&#27861;&#26469;&#26816;&#39564;&#21644;&#25506;&#32034;&#26234;&#33021;&#20132;&#20114;&#24335;&#20889;&#20316;&#21161;&#25163;&#30340;&#22810;&#32500;&#31354;&#38388;&#12290;&#36890;&#36807;&#22823;&#22411;&#31038;&#21306;&#21327;&#20316;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20889;&#20316;&#21161;&#25163;&#30340;&#20116;&#20010;&#26041;&#38754;&#65306;&#20219;&#21153;&#12289;&#29992;&#25143;&#12289;&#25216;&#26415;&#12289;&#20132;&#20114;&#21644;&#29983;&#24577;&#31995;&#32479;&#12290;&#22312;&#27599;&#20010;&#26041;&#38754;&#65292;&#25105;&#20204;&#36890;&#36807;&#31995;&#32479;&#23457;&#38405;115&#31687;&#35770;&#25991;&#23450;&#20041;&#20102;&#32500;&#24230;&#65288;&#21363;&#26041;&#38754;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#65289;&#21644;&#20195;&#30721;&#65288;&#21363;&#27599;&#20010;&#32500;&#24230;&#30340;&#21487;&#33021;&#36873;&#39033;&#65289;&#12290;&#25105;&#20204;&#30340;&#35774;&#35745;&#31354;&#38388;&#26088;&#22312;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#35774;&#35745;&#24072;&#25552;&#20379;&#19968;&#20010;&#23454;&#29992;&#24037;&#20855;&#65292;&#24110;&#21161;&#20182;&#20204;&#23548;&#33322;&#12289;&#29702;&#35299;&#21644;&#27604;&#36739;&#20889;&#20316;&#21161;&#25163;&#30340;&#21508;&#31181;&#21487;&#33021;&#24615;&#65292;&#24182;&#24110;&#21161;&#26500;&#24605;&#21644;&#35774;&#35745;&#26032;&#30340;&#20889;&#20316;&#21161;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14117v1 Announce Type: cross  Abstract: In our era of rapid technological advancement, the research landscape for writing assistants has become increasingly fragmented across various research communities. We seek to address this challenge by proposing a design space as a structured way to examine and explore the multidimensional space of intelligent and interactive writing assistants. Through a large community collaboration, we explore five aspects of writing assistants: task, user, technology, interaction, and ecosystem. Within each aspect, we define dimensions (i.e., fundamental components of an aspect) and codes (i.e., potential options for each dimension) by systematically reviewing 115 papers. Our design space aims to offer researchers and designers a practical tool to navigate, comprehend, and compare the various possibilities of writing assistants, and aid in the envisioning and design of new writing assistants.
&lt;/p&gt;</description></item><item><title>CHARM&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#20840;&#38754;&#28145;&#20837;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25991;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#30740;&#31350;&#21457;&#29616;LLM&#30340;&#35821;&#35328;&#23548;&#21521;&#24615;&#21644;&#20219;&#21153;&#39046;&#22495;&#20250;&#24433;&#21709;&#25552;&#31034;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25351;&#20986;&#19968;&#20123;LLMs&#22312;&#35760;&#24518;&#20013;&#25991;&#24120;&#35782;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#32780;&#20854;&#20182;&#19968;&#20123;LLMs&#22312;&#25512;&#29702;&#19978;&#34920;&#29616;&#23384;&#22312;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.14112</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25991;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#65306;&#20174;&#20013;&#25991;&#29305;&#23450;&#21040;&#25512;&#29702;-&#35760;&#24518;&#20851;&#32852;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14112
&lt;/p&gt;
&lt;p&gt;
CHARM&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#20840;&#38754;&#28145;&#20837;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25991;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#30740;&#31350;&#21457;&#29616;LLM&#30340;&#35821;&#35328;&#23548;&#21521;&#24615;&#21644;&#20219;&#21153;&#39046;&#22495;&#20250;&#24433;&#21709;&#25552;&#31034;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25351;&#20986;&#19968;&#20123;LLMs&#22312;&#35760;&#24518;&#20013;&#25991;&#24120;&#35782;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#32780;&#20854;&#20182;&#19968;&#20123;LLMs&#22312;&#25512;&#29702;&#19978;&#34920;&#29616;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;CHARM&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#20840;&#38754;&#28145;&#20837;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#25991;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#20840;&#29699;&#24050;&#30693;&#21644;&#20013;&#25991;&#29305;&#26377;&#30340;&#24120;&#35782;&#12290;&#22312;CHARM&#19978;&#35780;&#20272;&#20102;7&#20010;&#33521;&#25991;&#21644;12&#20010;&#20013;&#25991;&#23450;&#21521;LLMs&#65292;&#37319;&#29992;&#20102;5&#31181;&#20195;&#34920;&#24615;&#25552;&#31034;&#31574;&#30053;&#26469;&#25552;&#39640;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#27604;&#22914;&#24605;&#32500;&#38142;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLM&#30340;&#35821;&#35328;&#23548;&#21521;&#24615;&#21644;&#20219;&#21153;&#39046;&#22495;&#24433;&#21709;&#20102;&#25552;&#31034;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#36825;&#20016;&#23500;&#20102;&#20197;&#24448;&#30340;&#30740;&#31350;&#32467;&#26524;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#32039;&#23494;&#20851;&#32852;&#30340;&#25512;&#29702;&#21644;&#35760;&#24518;&#20219;&#21153;&#65292;&#24182;&#21457;&#29616;&#19968;&#20123;LLMs&#22312;&#35760;&#24518;&#20013;&#25991;&#24120;&#35782;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#24433;&#21709;&#20102;&#23427;&#20204;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#32780;&#20854;&#20182;&#19968;&#20123;LLMs&#22312;&#25512;&#29702;&#19978;&#34920;&#29616;&#23384;&#22312;&#24046;&#24322;&#65292;&#23613;&#31649;&#35760;&#24518;&#34920;&#29616;&#30456;&#20284;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;LLMs&#30340;&#19982;&#35760;&#24518;&#26080;&#20851;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#20998;&#26512;&#20102;&#20856;&#22411;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14112v1 Announce Type: new  Abstract: We introduce CHARM, the first benchmark for comprehensively and in-depth evaluating the commonsense reasoning ability of large language models (LLMs) in Chinese, which covers both globally known and Chinese-specific commonsense. We evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5 representative prompt strategies for improving LLMs' reasoning ability, such as Chain-of-Thought. Our findings indicate that the LLM's language orientation and the task's domain influence the effectiveness of the prompt strategy, which enriches previous research findings. We built closely-interconnected reasoning and memorization tasks, and found that some LLMs struggle with memorizing Chinese commonsense, affecting their reasoning ability, while others show differences in reasoning despite similar memorization performance. We also evaluated the LLMs' memorization-independent reasoning abilities and analyzed the typical errors. Our study pr
&lt;/p&gt;</description></item><item><title>M3&#26159;&#19968;&#20010;&#22810;&#20219;&#21153;&#28151;&#21512;&#30446;&#26631;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#20165;&#20381;&#36182;&#23545;&#27604;&#23398;&#20064;&#21487;&#33021;&#23548;&#33268;&#30340;&#27425;&#20248;&#26816;&#32034;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#22312;FEVER&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.14074</link><description>&lt;p&gt;
M3: &#29992;&#20110;&#24320;&#25918;&#39046;&#22495;&#22810;&#36339;&#23494;&#38598;&#21477;&#23376;&#26816;&#32034;&#30340;&#22810;&#20219;&#21153;&#28151;&#21512;&#30446;&#26631;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
M3: A Multi-Task Mixed-Objective Learning Framework for Open-Domain Multi-Hop Dense Sentence Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14074
&lt;/p&gt;
&lt;p&gt;
M3&#26159;&#19968;&#20010;&#22810;&#20219;&#21153;&#28151;&#21512;&#30446;&#26631;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#20165;&#20381;&#36182;&#23545;&#27604;&#23398;&#20064;&#21487;&#33021;&#23548;&#33268;&#30340;&#27425;&#20248;&#26816;&#32034;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#22312;FEVER&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#65292;&#23545;&#27604;&#23398;&#20064;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#38750;&#24120;&#26377;&#25928;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#24191;&#27867;&#29992;&#20110;&#23494;&#38598;&#26816;&#32034;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#20165;&#20381;&#36182;&#23545;&#27604;&#23398;&#20064;&#21487;&#33021;&#20250;&#23548;&#33268;&#27425;&#20248;&#30340;&#26816;&#32034;&#24615;&#33021;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23613;&#31649;&#35768;&#22810;&#26816;&#32034;&#25968;&#25454;&#38598;&#25903;&#25345;&#21508;&#31181;&#36229;&#36234;&#23545;&#27604;&#23398;&#20064;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#20294;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#22330;&#26223;&#20013;&#39640;&#25928;&#22320;&#32452;&#21512;&#23427;&#20204;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;M3&#65292;&#36825;&#26159;&#19968;&#20010;&#20808;&#36827;&#30340;&#36882;&#24402;&#22810;&#36339;&#23494;&#38598;&#21477;&#23376;&#26816;&#32034;&#31995;&#32479;&#65292;&#23427;&#24314;&#31435;&#22312;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20219;&#21153;&#28151;&#21512;&#30446;&#26631;&#26041;&#27861;&#20043;&#19978;&#65292;&#29992;&#20110;&#23494;&#38598;&#25991;&#26412;&#34920;&#31034;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#19978;&#36848;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#24320;&#25918;&#39046;&#22495;&#20107;&#23454;&#39564;&#35777;&#22522;&#20934;&#25968;&#25454;&#38598;FEVER&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#22312;&#20197;&#19979;&#38142;&#25509;&#33719;&#21462;: https://github.com/TonyBY/M3
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14074v1 Announce Type: cross  Abstract: In recent research, contrastive learning has proven to be a highly effective method for representation learning and is widely used for dense retrieval. However, we identify that relying solely on contrastive learning can lead to suboptimal retrieval performance. On the other hand, despite many retrieval datasets supporting various learning objectives beyond contrastive learning, combining them efficiently in multi-task learning scenarios can be challenging. In this paper, we introduce M3, an advanced recursive Multi-hop dense sentence retrieval system built upon a novel Multi-task Mixed-objective approach for dense text representation learning, addressing the aforementioned challenges. Our approach yields state-of-the-art performance on a large-scale open-domain fact verification benchmark dataset, FEVER. Code and data are available at: https://github.com/TonyBY/M3
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33521;&#35821;&#20013;&#24120;&#35265;&#30340;&#27495;&#20041;&#31867;&#22411;&#20998;&#31867;&#65292;&#26088;&#22312;&#20419;&#36827;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20998;&#26512;&#65292;&#26377;&#21161;&#20110;&#26356;&#32454;&#33268;&#22320;&#35780;&#20272;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.14072</link><description>&lt;p&gt;
&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#27495;&#20041;&#31867;&#22411;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
A Taxonomy of Ambiguity Types for NLP
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14072
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33521;&#35821;&#20013;&#24120;&#35265;&#30340;&#27495;&#20041;&#31867;&#22411;&#20998;&#31867;&#65292;&#26088;&#22312;&#20419;&#36827;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20998;&#26512;&#65292;&#26377;&#21161;&#20110;&#26356;&#32454;&#33268;&#22320;&#35780;&#20272;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27495;&#20041;&#26159;&#35821;&#35328;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#23427;&#33021;&#22815;&#23454;&#29616;&#35828;&#35805;&#32773;&#20043;&#38388;&#26356;&#26377;&#25928;&#30340;&#20132;&#27969;&#65292;&#20294;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#32463;&#24120;&#34987;&#24573;&#30053;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#21487;&#33021;&#38590;&#20197;&#29702;&#35299;&#20154;&#31867;&#35821;&#35328;&#29702;&#35299;&#30340;&#26576;&#20123;&#35201;&#32032;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#33021;&#26080;&#27861;&#20687;&#20154;&#31867;&#22312;&#20132;&#27969;&#20013;&#33258;&#28982;&#22320;&#22788;&#29702;&#37027;&#26679;&#22788;&#29702;&#27495;&#20041;&#12290;&#27492;&#22806;&#65292;&#19981;&#21516;&#31867;&#22411;&#30340;&#27495;&#20041;&#21487;&#33021;&#26377;&#19981;&#21516;&#30340;&#20316;&#29992;&#65292;&#24182;&#38656;&#35201;&#19981;&#21516;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#25105;&#20204;&#26088;&#22312;&#25506;&#31350;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#27495;&#20041;&#31867;&#22411;&#19978;&#30340;&#33021;&#21147;&#22914;&#20309;&#21464;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33521;&#35821;&#20013;&#24120;&#35265;&#30340;&#27495;&#20041;&#31867;&#22411;&#20998;&#31867;&#65292;&#20197;&#20419;&#36827;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#20998;&#31867;&#21487;&#20197;&#24110;&#21161;&#22312;&#35821;&#35328;&#27495;&#20041;&#25968;&#25454;&#20013;&#36827;&#34892;&#26377;&#24847;&#20041;&#30340;&#21010;&#20998;&#65292;&#20174;&#32780;&#26356;&#32454;&#33268;&#22320;&#35780;&#20272;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14072v1 Announce Type: new  Abstract: Ambiguity is an critical component of language that allows for more effective communication between speakers, but is often ignored in NLP. Recent work suggests that NLP systems may struggle to grasp certain elements of human language understanding because they may not handle ambiguities at the level that humans naturally do in communication. Additionally, different types of ambiguity may serve different purposes and require different approaches for resolution, and we aim to investigate how language models' abilities vary across types. We propose a taxonomy of ambiguity types as seen in English to facilitate NLP analysis. Our taxonomy can help make meaningful splits in language ambiguity data, allowing for more fine-grained assessments of both datasets and model performance.
&lt;/p&gt;</description></item><item><title>NeurIPS 2023&#38899;&#39057;&#26426;&#22120;&#23398;&#20064;&#30740;&#35752;&#20250;&#32858;&#38598;&#20102;&#38899;&#39057;&#39046;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;&#19987;&#23478;&#65292;&#20026;&#35299;&#20915;&#38899;&#39057;&#25968;&#25454;&#33719;&#21462;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#22810;&#20010;&#24320;&#28304;&#25968;&#25454;&#38598;&#21644;&#19987;&#26377;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.14048</link><description>&lt;p&gt;
NeurIPS 2023&#38899;&#39057;&#26426;&#22120;&#23398;&#20064;&#30740;&#35752;&#20250;&#65306;&#24773;&#24863;&#38899;&#39057;&#22522;&#20934;&#21644;&#26032;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
The NeurIPS 2023 Machine Learning for Audio Workshop: Affective Audio Benchmarks and Novel Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14048
&lt;/p&gt;
&lt;p&gt;
NeurIPS 2023&#38899;&#39057;&#26426;&#22120;&#23398;&#20064;&#30740;&#35752;&#20250;&#32858;&#38598;&#20102;&#38899;&#39057;&#39046;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;&#19987;&#23478;&#65292;&#20026;&#35299;&#20915;&#38899;&#39057;&#25968;&#25454;&#33719;&#21462;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#22810;&#20010;&#24320;&#28304;&#25968;&#25454;&#38598;&#21644;&#19987;&#26377;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
NeurIPS 2023&#38899;&#39057;&#26426;&#22120;&#23398;&#20064;&#30740;&#35752;&#20250;&#27719;&#38598;&#20102;&#26469;&#33258;&#21508;&#31181;&#38899;&#39057;&#39046;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#19987;&#23478;&#12290; &#25991;&#31456;&#25351;&#20986;&#65292;&#26377;&#35768;&#22810;&#26377;&#20215;&#20540;&#30340;&#38899;&#39057;&#39537;&#21160;&#30340;ML&#20219;&#21153;&#65292;&#20174;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#21040;&#38899;&#39057;&#20107;&#20214;&#26816;&#27979;&#65292;&#20294;&#30456;&#27604;&#20110;&#20854;&#20182;ML&#39046;&#22495;&#65288;&#22914;&#35745;&#31639;&#26426;&#35270;&#35273;&#25110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65289;&#65292;&#38899;&#39057;&#31038;&#21306;&#30456;&#23545;&#36739;&#23569;&#12290; &#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#38480;&#21046;&#26159;&#21487;&#29992;&#30340;&#25968;&#25454;&#65307; &#30001;&#20110;&#38899;&#39057;&#26159;&#19968;&#31181;&#26102;&#38388;&#30456;&#20851;&#30340;&#27169;&#24577;&#65292;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#25910;&#38598;&#32791;&#26102;&#19988;&#26114;&#36149;&#65292;&#36825;&#20351;&#24471;&#23398;&#26415;&#22242;&#20307;&#38590;&#20197;&#23558;&#20182;&#20204;&#30340;&#26368;&#20808;&#36827;&#31574;&#30053;&#24212;&#29992;&#20110;&#19968;&#20010;&#26356;&#22823;&#65292;&#26356;&#20855;&#26222;&#36941;&#24615;&#30340;&#25968;&#25454;&#38598;&#12290; &#20026;&#40723;&#21169;&#37027;&#20123;&#33719;&#21462;&#22823;&#22411;&#25968;&#25454;&#38598;&#22256;&#38590;&#30340;&#30740;&#31350;&#20154;&#21592;&#65292;&#30740;&#35752;&#20250;&#32452;&#32455;&#32773;&#39318;&#20808;&#27010;&#36848;&#20102;&#20960;&#20010;&#21487;&#20379;&#31038;&#21306;&#20351;&#29992;&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#30740;&#35752;&#20250;&#26399;&#38388;&#25552;&#20379;&#20102;&#20960;&#20010;&#19987;&#26377;&#25968;&#25454;&#38598;&#12290; &#20855;&#20307;&#26469;&#35828;&#65292;&#21253;&#25324;&#19977;&#20010;&#21457;&#22768;&#25968;&#25454;&#38598;&#65292;Hume-Prosody&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14048v1 Announce Type: cross  Abstract: The NeurIPS 2023 Machine Learning for Audio Workshop brings together machine learning (ML) experts from various audio domains. There are several valuable audio-driven ML tasks, from speech emotion recognition to audio event detection, but the community is sparse compared to other ML areas, e.g., computer vision or natural language processing. A major limitation with audio is the available data; with audio being a time-dependent modality, high-quality data collection is time-consuming and costly, making it challenging for academic groups to apply their often state-of-the-art strategies to a larger, more generalizable dataset. In this short white paper, to encourage researchers with limited access to large-datasets, the organizers first outline several open-source datasets that are available to the community, and for the duration of the workshop are making several propriety datasets available. Namely, three vocal datasets, Hume-Prosody, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20044;&#23572;&#37117;&#35821;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#39318;&#20010;&#26368;&#22823;&#35268;&#27169;&#20844;&#24320;&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;&#22320;&#21306;&#35821;&#35328;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#39046;&#22495;&#25968;&#25454;&#38598;&#35268;&#27169;&#26377;&#38480;&#30340;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2403.14037</link><description>&lt;p&gt;
Ax-to-Grind Urdu: &#20044;&#23572;&#37117;&#35821;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Ax-to-Grind Urdu: Benchmark Dataset for Urdu Fake News Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20044;&#23572;&#37117;&#35821;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#39318;&#20010;&#26368;&#22823;&#35268;&#27169;&#20844;&#24320;&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;&#22320;&#21306;&#35821;&#35328;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#39046;&#22495;&#25968;&#25454;&#38598;&#35268;&#27169;&#26377;&#38480;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35823;&#20256;&#20449;&#24687;&#21487;&#33021;&#20005;&#37325;&#24433;&#21709;&#31038;&#20250;&#65292;&#24433;&#21709;&#20174;&#20844;&#20247;&#33286;&#35770;&#21040;&#26426;&#26500;&#20449;&#24515;&#21644;&#19968;&#20010;&#22269;&#23478;&#30340;&#25919;&#27835;&#21069;&#26223;&#31561;&#21508;&#20010;&#26041;&#38754;&#12290;&#22312;&#32447;&#32593;&#31449;&#21644;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#19978;&#30340;&#34394;&#20551;&#26032;&#38395;&#20256;&#25773;&#21576;&#29616;&#22823;&#37327;&#22686;&#38271;&#12290;&#21508;&#31181;&#20107;&#23454;&#26680;&#26597;&#32593;&#31449;&#30340;&#26032;&#38395;&#20027;&#35201;&#20197;&#33521;&#35821;&#21457;&#24067;&#65292;&#20960;&#20046;&#19981;&#25552;&#20379;&#26377;&#20851;&#22320;&#21306;&#35821;&#35328;&#34394;&#20551;&#26032;&#38395;&#30340;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#26080;&#27861;&#36890;&#36807;&#20107;&#23454;&#26680;&#26597;&#38376;&#25143;&#32593;&#31449;&#26469;&#35782;&#21035;&#20044;&#23572;&#37117;&#35821;&#34394;&#20551;&#26032;&#38395;&#20256;&#25773;&#32773;&#12290;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;(SOTA)&#26041;&#27861;&#20381;&#36182;&#20110;&#36866;&#24403;&#26631;&#35760;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#22320;&#21306;&#21644;&#36164;&#28304;&#21463;&#38480;&#35821;&#35328;&#30340;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#30001;&#20110;&#25968;&#25454;&#38598;&#35268;&#27169;&#26377;&#38480;&#21644;&#21512;&#27861;&#35789;&#27719;&#36164;&#28304;&#30340;&#32570;&#20047;&#32780;&#28382;&#21518;&#12290;&#20808;&#21069;&#29992;&#20110;&#20044;&#23572;&#37117;&#35821;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#25968;&#25454;&#38598;&#35268;&#27169;&#26377;&#38480;&#12289;&#39046;&#22495;&#21463;&#38480;&#12289;&#19981;&#20844;&#24320;&#19988;&#26410;&#32463;&#20154;&#24037;&#39564;&#35777;&#65292;&#20854;&#20013;&#26032;&#38395;&#26159;&#20174;&#33521;&#35821;&#32763;&#35793;&#20026;&#20044;&#23572;&#37117;&#35821;&#12290;&#26412;&#25991;&#31574;&#21010;&#24182;&#36129;&#29486;&#20102;&#39318;&#20010;&#26368;&#22823;&#35268;&#27169;&#30340;&#20844;&#24320;&#21487;&#29992;&#30340;&#20044;&#23572;&#37117;&#35821;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14037v1 Announce Type: cross  Abstract: Misinformation can seriously impact society, affecting anything from public opinion to institutional confidence and the political horizon of a state. Fake News (FN) proliferation on online websites and Online Social Networks (OSNs) has increased profusely. Various fact-checking websites include news in English and barely provide information about FN in regional languages. Thus the Urdu FN purveyors cannot be discerned using factchecking portals. SOTA approaches for Fake News Detection (FND) count upon appropriately labelled and large datasets. FND in regional and resource-constrained languages lags due to the lack of limited-sized datasets and legitimate lexical resources. The previous datasets for Urdu FND are limited-sized, domain-restricted, publicly unavailable and not manually verified where the news is translated from English into Urdu. In this paper, we curate and contribute the first largest publicly available dataset for Urdu 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;HPLT&#35821;&#35328;&#36164;&#28304;&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#25324;&#21333;&#35821;&#21644;&#21452;&#35821;&#35821;&#26009;&#24211;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#36164;&#28304;&#29992;&#20110;&#35821;&#35328;&#24314;&#27169;&#21644;&#26426;&#22120;&#32763;&#35793;&#35757;&#32451;</title><link>https://arxiv.org/abs/2403.14009</link><description>&lt;p&gt;
&#19968;&#20221;&#26032;&#30340;&#29992;&#20110;&#39640;&#24615;&#33021;&#35821;&#35328;&#25216;&#26415;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A New Massive Multilingual Dataset for High-Performance Language Technologies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14009
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;HPLT&#35821;&#35328;&#36164;&#28304;&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#25324;&#21333;&#35821;&#21644;&#21452;&#35821;&#35821;&#26009;&#24211;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#36164;&#28304;&#29992;&#20110;&#35821;&#35328;&#24314;&#27169;&#21644;&#26426;&#22120;&#32763;&#35793;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;HPLT&#65288;High Performance Language Technologies&#65289;&#35821;&#35328;&#36164;&#28304;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#20174;CommonCrawl&#21644;&#20197;&#21069;&#26410;&#20351;&#29992;&#36807;&#30340;&#20114;&#32852;&#32593;&#26723;&#26696;&#39302;&#20013;&#25552;&#21462;&#30340;&#21333;&#35821;&#21644;&#21452;&#35821;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#25105;&#20204;&#29992;&#20110;&#33719;&#21462;&#12289;&#31649;&#29702;&#21644;&#22788;&#29702;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#24320;&#28304;&#36719;&#20214;&#24037;&#20855;&#21644;&#39640;&#24615;&#33021;&#35745;&#31639;&#12290;&#25105;&#20204;&#30340;&#21333;&#35821;&#25968;&#25454;&#38598;&#20391;&#37325;&#20110;&#36164;&#28304;&#36739;&#20302;&#33267;&#36164;&#28304;&#20013;&#31561;&#30340;&#35821;&#35328;&#65292;&#28085;&#30422;&#20102;75&#31181;&#35821;&#35328;&#21644;&#22823;&#32422;5.6&#19975;&#20159;&#20010;&#35789;&#26631;&#35760;&#65292;&#22312;&#25991;&#26723;&#32423;&#21035;&#19978;&#21435;&#37325;&#12290;&#25105;&#20204;&#30340;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#24179;&#34892;&#35821;&#26009;&#24211;&#26469;&#28304;&#20110;&#20854;&#21333;&#35821;&#23545;&#24212;&#37096;&#20998;&#65292;&#28085;&#30422;&#20102;18&#31181;&#35821;&#35328;&#23545;&#21644;&#36229;&#36807;9600&#19975;&#23545;&#40784;&#30340;&#21477;&#23376;&#23545;&#65292;&#22823;&#32422;&#26377;14&#20159;&#20010;&#33521;&#35821;&#26631;&#35760;&#12290;HPLT&#35821;&#35328;&#36164;&#28304;&#26159;&#26377;&#21490;&#20197;&#26469;&#21457;&#24067;&#30340;&#26368;&#22823;&#24320;&#25918;&#25991;&#26412;&#35821;&#26009;&#24211;&#20043;&#19968;&#65292;&#20026;&#35821;&#35328;&#24314;&#27169;&#21644;&#26426;&#22120;&#32763;&#35793;&#35757;&#32451;&#25552;&#20379;&#20102;&#37325;&#35201;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14009v1 Announce Type: new  Abstract: We present the HPLT (High Performance Language Technologies) language resources, a new massive multilingual dataset including both monolingual and bilingual corpora extracted from CommonCrawl and previously unused web crawls from the Internet Archive. We describe our methods for data acquisition, management and processing of large corpora, which rely on open-source software tools and high-performance computing. Our monolingual collection focuses on low- to medium-resourced languages and covers 75 languages and a total of ~5.6 trillion word tokens de-duplicated on the document level. Our English-centric parallel corpus is derived from its monolingual counterpart and covers 18 language pairs and more than 96 million aligned sentence pairs with roughly 1.4 billion English tokens. The HPLT language resources are one of the largest open text corpora ever released, providing a great resource for language modeling and machine translation traini
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22522;&#30784;&#27169;&#22411;&#24615;&#33021;&#25935;&#24863;&#24615;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;ChatGPT&#22312;&#24773;&#24863;&#35745;&#31639;&#20013;&#30340;&#25552;&#31034;&#25935;&#24863;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#21644;&#35780;&#20272;&#65292;&#28085;&#30422;&#24773;&#32490;&#20998;&#26512;&#12289;&#27602;&#24615;&#26816;&#27979;&#21644;&#35773;&#21050;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.14006</link><description>&lt;p&gt;
&#35770;ChatGPT&#22312;&#24773;&#24863;&#35745;&#31639;&#20013;&#30340;&#25552;&#31034;&#25935;&#24863;&#24615;
&lt;/p&gt;
&lt;p&gt;
On Prompt Sensitivity of ChatGPT in Affective Computing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14006
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22522;&#30784;&#27169;&#22411;&#24615;&#33021;&#25935;&#24863;&#24615;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;ChatGPT&#22312;&#24773;&#24863;&#35745;&#31639;&#20013;&#30340;&#25552;&#31034;&#25935;&#24863;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#21644;&#35780;&#20272;&#65292;&#28085;&#30422;&#24773;&#32490;&#20998;&#26512;&#12289;&#27602;&#24615;&#26816;&#27979;&#21644;&#35773;&#21050;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#23637;&#31034;&#20102;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22522;&#30784;&#27169;&#22411;&#22312;&#22810;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#24773;&#24863;&#35745;&#31639;&#20013;&#30340;&#26032;&#20852;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#35775;&#38382;&#36825;&#20123;&#26032;&#20852;&#33021;&#21147;&#26159;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#26469;&#23454;&#29616;&#30340;&#12290;&#23613;&#31649;&#23384;&#22312;&#19968;&#20123;&#25552;&#31034;&#25216;&#26415;&#65292;&#20294;&#35813;&#39046;&#22495;&#20173;&#22312;&#36805;&#36895;&#21457;&#23637;&#65292;&#35768;&#22810;&#25552;&#31034;&#24819;&#27861;&#20173;&#38656;&#35201;&#35843;&#26597;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#24182;&#35843;&#26597;&#22522;&#20110;&#19981;&#21516;&#25552;&#31034;&#25110;&#29983;&#25104;&#21442;&#25968;&#30340;&#22522;&#30784;&#27169;&#22411;&#24615;&#33021;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#22312;&#24773;&#24863;&#35745;&#31639;&#33539;&#22260;&#20869;&#38024;&#23545;ChatGPT&#25191;&#34892;&#25105;&#20204;&#30340;&#35780;&#20272;&#65292;&#35299;&#20915;&#20102;&#19977;&#20010;&#20027;&#35201;&#38382;&#39064;&#65292;&#21363;&#24773;&#32490;&#20998;&#26512;&#12289;&#27602;&#24615;&#26816;&#27979;&#21644;&#35773;&#21050;&#26816;&#27979;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23545;&#33258;&#22238;&#24402;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#20851;&#38190;&#21442;&#25968;&#36827;&#34892;&#20102;&#25935;&#24863;&#24615;&#20998;&#26512;&#65292;&#29305;&#21035;&#26159;&#28201;&#24230;&#21442;&#25968;$T$&#21644;Nucleus&#25277;&#26679;&#20013;&#30340;top-$p$&#21442;&#25968;&#65292;&#25351;&#23548;&#30528;&#20445;&#23432;&#25110;&#21019;&#36896;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14006v1 Announce Type: cross  Abstract: Recent studies have demonstrated the emerging capabilities of foundation models like ChatGPT in several fields, including affective computing. However, accessing these emerging capabilities is facilitated through prompt engineering. Despite the existence of some prompting techniques, the field is still rapidly evolving and many prompting ideas still require investigation. In this work, we introduce a method to evaluate and investigate the sensitivity of the performance of foundation models based on different prompts or generation parameters. We perform our evaluation on ChatGPT within the scope of affective computing on three major problems, namely sentiment analysis, toxicity detection, and sarcasm detection. First, we carry out a sensitivity analysis on pivotal parameters in auto-regressive text generation, specifically the temperature parameter $T$ and the top-$p$ parameter in Nucleus sampling, dictating how conservative or creative
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#20351;&#29992;&#26080;&#30417;&#30563;&#38477;&#32500;&#26041;&#27861;&#20943;&#23567;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21477;&#23376;&#23884;&#20837;&#32500;&#24230;&#65292;&#24182;&#21457;&#29616;&#23545;&#20110;&#19968;&#20123;&#27169;&#22411;&#65292;&#22312;&#38477;&#32500;&#21518;&#24615;&#33021;&#21453;&#32780;&#25552;&#39640;&#20102;&#12290;</title><link>https://arxiv.org/abs/2403.14001</link><description>&lt;p&gt;
&#35780;&#20272;&#39044;&#35757;&#32451;&#21477;&#23376;&#23884;&#20837;&#30340;&#26080;&#30417;&#30563;&#38477;&#32500;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Evaluating Unsupervised Dimensionality Reduction Methods for Pretrained Sentence Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14001
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20351;&#29992;&#26080;&#30417;&#30563;&#38477;&#32500;&#26041;&#27861;&#20943;&#23567;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21477;&#23376;&#23884;&#20837;&#32500;&#24230;&#65292;&#24182;&#21457;&#29616;&#23545;&#20110;&#19968;&#20123;&#27169;&#22411;&#65292;&#22312;&#38477;&#32500;&#21518;&#24615;&#33021;&#21453;&#32780;&#25552;&#39640;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#29983;&#25104;&#30340;&#21477;&#23376;&#23884;&#20837;&#30001;&#20110;&#22312;&#20247;&#22810;&#19979;&#28216;&#24212;&#29992;&#20013;&#20195;&#34920;&#25991;&#26412;&#26102;&#34920;&#29616;&#20986;&#33394;&#65292;&#21463;&#21040;NLP&#31038;&#21306;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;PLMs&#29983;&#25104;&#30340;&#21477;&#23376;&#23884;&#20837;&#30340;&#39640;&#32500;&#24230;&#22312;&#20195;&#34920;&#22823;&#37327;&#21477;&#23376;&#26102;&#23384;&#22312;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#20869;&#23384;&#25110;&#35745;&#31639;&#21463;&#38480;&#35774;&#22791;&#19978;&#12290;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#26080;&#30417;&#30563;&#38477;&#32500;&#26041;&#27861;&#65292;&#20197;&#20943;&#23567;PLMs&#29983;&#25104;&#30340;&#21477;&#23376;&#23884;&#20837;&#30340;&#32500;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#31616;&#21333;&#30340;&#26041;&#27861;&#22914;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#21487;&#20197;&#23558;&#21477;&#23376;&#23884;&#20837;&#30340;&#32500;&#24230;&#20943;&#23569;&#32422;50&#65285;&#65292;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#24615;&#33021;&#26410;&#21463;&#21040;&#26174;&#30528;&#25439;&#22833;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23545;&#19968;&#20123;PLMs&#29983;&#25104;&#30340;&#21477;&#23376;&#23884;&#20837;&#36827;&#19968;&#27493;&#38477;&#20302;&#32500;&#24230;&#21487;&#20197;&#25913;&#21892;&#24615;&#33021;&#65292;&#36229;&#36807;&#21407;&#22987;&#39640;&#32500;&#29256;&#26412;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14001v1 Announce Type: new  Abstract: Sentence embeddings produced by Pretrained Language Models (PLMs) have received wide attention from the NLP community due to their superior performance when representing texts in numerous downstream applications. However, the high dimensionality of the sentence embeddings produced by PLMs is problematic when representing large numbers of sentences in memory- or compute-constrained devices. As a solution, we evaluate unsupervised dimensionality reduction methods to reduce the dimensionality of sentence embeddings produced by PLMs. Our experimental results show that simple methods such as Principal Component Analysis (PCA) can reduce the dimensionality of sentence embeddings by almost $50\%$, without incurring a significant loss in performance in multiple downstream tasks. Surprisingly, reducing the dimensionality further improves performance over the original high-dimensional versions for the sentence embeddings produced by some PLMs in s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#8220;&#21463;&#38480;&#34892;&#19994;&#8221;&#36827;&#34892;&#33258;&#21160;&#25968;&#25454;&#38598;&#22686;&#24378;&#20197;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20559;&#35265;&#30340;&#26032;&#26426;&#21046;&#65292;&#24182;&#21019;&#24314;&#20102;mb-index&#21644;db-index&#20004;&#20010;&#26032;&#30340;&#20559;&#35265;&#37327;&#21270;&#25351;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.13925</link><description>&lt;p&gt;
&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20559;&#35265;&#65306;&#37325;&#28857;&#20851;&#27880;&#8220;&#21463;&#38480;&#34892;&#19994;&#8221;&#30340;&#33258;&#21160;&#25968;&#25454;&#38598;&#22686;&#24378;&#21644;&#20559;&#35265;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Reducing Large Language Model Bias with Emphasis on 'Restricted Industries': Automated Dataset Augmentation and Prejudice Quantification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#8220;&#21463;&#38480;&#34892;&#19994;&#8221;&#36827;&#34892;&#33258;&#21160;&#25968;&#25454;&#38598;&#22686;&#24378;&#20197;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20559;&#35265;&#30340;&#26032;&#26426;&#21046;&#65292;&#24182;&#21019;&#24314;&#20102;mb-index&#21644;db-index&#20004;&#20010;&#26032;&#30340;&#20559;&#35265;&#37327;&#21270;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#19981;&#26029;&#22686;&#24378;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#23545;&#20854;&#20135;&#29983;&#20559;&#35265;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#8220;&#21463;&#38480;&#34892;&#19994;&#8221;&#22312;&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#36890;&#36807;&#25351;&#23450;&#25968;&#25454;&#38598;&#22686;&#24378;&#26469;&#21435;&#20559;&#35265;&#30340;&#26032;&#39062;&#33258;&#21160;&#26426;&#21046;&#12290;&#25105;&#20204;&#36824;&#21019;&#24314;&#20102;&#20004;&#20010;&#26032;&#30340;&#34913;&#37327;&#25351;&#26631;mb-index&#21644;db-index&#26469;&#37327;&#21270;&#20559;&#35265;&#65292;&#32771;&#34385;&#21040;&#20559;&#35265;&#26159;&#30001;&#20869;&#22312;&#27169;&#22411;&#26550;&#26500;&#21644;&#25968;&#25454;&#38598;&#20849;&#21516;&#23548;&#33268;&#30340;&#36825;&#19968;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13925v1 Announce Type: cross  Abstract: Despite the growing capabilities of large language models, there exists concerns about the biases they develop. In this paper, we propose a novel, automated mechanism for debiasing through specified dataset augmentation in the lens of bias producers and in the context of 'restricted industries' with limited data. We additionally create two new additional metrics, the mb-index and db-index, to quantify bias, considering the idea that bias occurs due to both intrinsic model architecture and dataset.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#35270;&#35273;&#24341;&#23548;&#30340;&#35821;&#38899;&#27169;&#22411;&#20013;&#30340;&#30456;&#20114;&#25490;&#20182;&#24615;&#20559;&#35265;&#65292;&#24182;&#21457;&#29616;&#22312;&#20855;&#26377;&#26356;&#22810;&#35270;&#35273;&#30693;&#35782;&#30340;&#27169;&#22411;&#20013;&#23384;&#22312;&#26356;&#24378;&#30340;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2403.13922</link><description>&lt;p&gt;
&#35270;&#35273;&#24341;&#23548;&#30340;&#35821;&#38899;&#27169;&#22411;&#23384;&#22312;&#30456;&#20114;&#25490;&#20182;&#24615;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Visually Grounded Speech Models have a Mutual Exclusivity Bias
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13922
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#35270;&#35273;&#24341;&#23548;&#30340;&#35821;&#38899;&#27169;&#22411;&#20013;&#30340;&#30456;&#20114;&#25490;&#20182;&#24615;&#20559;&#35265;&#65292;&#24182;&#21457;&#29616;&#22312;&#20855;&#26377;&#26356;&#22810;&#35270;&#35273;&#30693;&#35782;&#30340;&#27169;&#22411;&#20013;&#23384;&#22312;&#26356;&#24378;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#23401;&#23376;&#23398;&#20064;&#26032;&#21333;&#35789;&#26102;&#65292;&#20182;&#20204;&#20250;&#36816;&#29992;&#30456;&#20114;&#25490;&#20182;&#24615;&#65288;ME&#65289;&#20559;&#35265;&#36825;&#26679;&#30340;&#32422;&#26463;&#65306;&#19968;&#20010;&#26032;&#21333;&#35789;&#20250;&#26144;&#23556;&#21040;&#19968;&#20010;&#26032;&#23545;&#35937;&#32780;&#19981;&#26159;&#19968;&#20010;&#29087;&#24713;&#30340;&#23545;&#35937;&#12290;&#36825;&#31181;&#20559;&#35265;&#24050;&#32463;&#22312;&#35745;&#31639;&#27169;&#22411;&#20013;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20294;&#21482;&#22312;&#20351;&#29992;&#31163;&#25955;&#35789;&#34920;&#31034;&#20316;&#20026;&#36755;&#20837;&#30340;&#27169;&#22411;&#20013;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24573;&#30053;&#20102;&#21475;&#35821;&#35789;&#30340;&#39640;&#21464;&#24322;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20174;&#33258;&#28982;&#22270;&#20687;&#21644;&#36830;&#32493;&#35821;&#38899;&#38899;&#39057;&#20013;&#23398;&#20064;&#30340;&#35270;&#35273;&#24341;&#23548;&#30340;&#35821;&#38899;&#27169;&#22411;&#20013;&#30340;ME&#20559;&#35265;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#20197;&#29087;&#24713;&#30340;&#21333;&#35789;&#65292;&#28982;&#21518;&#36890;&#36807;&#35810;&#38382;&#27169;&#22411;&#36873;&#25321;&#19968;&#20010;&#26032;&#23545;&#35937;&#21644;&#19968;&#20010;&#29087;&#24713;&#23545;&#35937;&#30340;&#26041;&#24335;&#26469;&#27979;&#35797;&#20854;ME&#20559;&#35265;&#12290;&#20026;&#20102;&#27169;&#25311;&#20808;&#21069;&#30340;&#22768;&#23398;&#21644;&#35270;&#35273;&#30693;&#35782;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#20960;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#38899;&#21644;&#35270;&#35273;&#32593;&#32476;&#30340;&#21021;&#22987;&#21270;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#19981;&#21516;&#21021;&#22987;&#21270;&#26041;&#27861;&#19979;&#30340;ME&#20559;&#35265;&#65292;&#20197;&#21450;&#22312;&#20855;&#26377;&#26356;&#22810;&#20808;&#21069;&#30693;&#35782;&#65288;&#29305;&#21035;&#26159;&#35270;&#35273;&#30693;&#35782;&#65289;&#30340;&#27169;&#22411;&#20013;&#26356;&#24378;&#30340;&#20559;&#35265;&#12290;&#39069;&#22806;&#30340;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13922v1 Announce Type: new  Abstract: When children learn new words, they employ constraints such as the mutual exclusivity (ME) bias: a novel word is mapped to a novel object rather than a familiar one. This bias has been studied computationally, but only in models that use discrete word representations as input, ignoring the high variability of spoken words. We investigate the ME bias in the context of visually grounded speech models that learn from natural images and continuous speech audio. Concretely, we train a model on familiar words and test its ME bias by asking it to select between a novel and a familiar object when queried with a novel word. To simulate prior acoustic and visual knowledge, we experiment with several initialisation strategies using pretrained speech and vision networks. Our findings reveal the ME bias across the different initialisation approaches, with a stronger bias in models with more prior (in particular, visual) knowledge. Additional tests co
&lt;/p&gt;</description></item><item><title>&#39318;&#27425;&#21033;&#29992;Seq2Seq PLM&#19982;&#35821;&#35328;&#29305;&#24449;&#30456;&#32467;&#21512;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;OIE&#26550;&#26500;&#30340;&#24615;&#33021;&#65292;&#20351;&#20854;&#21516;&#26102;&#33719;&#30410;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#35328;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2403.13903</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#35328;&#22686;&#24378;&#23884;&#20837;&#36827;&#34892;&#24320;&#25918;&#20449;&#24687;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Leveraging Linguistically Enhanced Embeddings for Open Information Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13903
&lt;/p&gt;
&lt;p&gt;
&#39318;&#27425;&#21033;&#29992;Seq2Seq PLM&#19982;&#35821;&#35328;&#29305;&#24449;&#30456;&#32467;&#21512;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;OIE&#26550;&#26500;&#30340;&#24615;&#33021;&#65292;&#20351;&#20854;&#21516;&#26102;&#33719;&#30410;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#35328;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#20449;&#24687;&#25277;&#21462;&#65288;OIE&#65289;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#19968;&#39033;&#32467;&#26500;&#21270;&#39044;&#27979;&#65288;SP&#65289;&#20219;&#21153;&#65292;&#26088;&#22312;&#20174;&#33258;&#30001;&#25991;&#26412;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#30340;$n$-&#20803;&#32452;&#65292;&#36890;&#24120;&#26159;&#20027;-&#35859;-&#23486;&#19977;&#20803;&#32452;&#12290;&#36755;&#20837;&#25991;&#26412;&#20013;&#30340;&#35789;&#23884;&#20837;&#21487;&#20197;&#36890;&#36807;&#35821;&#35328;&#29305;&#24449;&#65288;&#36890;&#24120;&#26159;&#35789;&#24615;&#65288;PoS&#65289;&#21644;&#21477;&#27861;&#20381;&#23384;&#35299;&#26512;&#65288;SynDP&#65289;&#26631;&#31614;&#65289;&#36827;&#34892;&#22686;&#24378;&#12290;&#28982;&#32780;&#65292;&#36807;&#21435;&#30340;&#22686;&#24378;&#25216;&#26415;&#26080;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#30340;&#24378;&#22823;&#21151;&#33021;&#65292;&#32780;PLMs&#26412;&#36523;&#24456;&#23569;&#29992;&#20110;OIE&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#39318;&#27425;&#21033;&#29992;Seq2Seq PLM&#19982;&#35821;&#35328;&#29305;&#24449;&#36827;&#34892;OIE&#65292;&#36890;&#36807;&#24341;&#20837;&#21152;&#26435;&#30456;&#21152;&#21644;&#32447;&#24615;&#21270;&#36830;&#25509;&#20004;&#31181;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#21487;&#20197;&#20351;&#20219;&#20309;&#31070;&#32463;&#32593;&#32476;OIE&#26550;&#26500;&#19968;&#27425;&#24615;&#33719;&#24471;PLMs&#21644;&#35821;&#35328;&#29305;&#24449;&#30340;&#20851;&#38190;&#24615;&#33021;&#25552;&#21319;&#12290;&#22312;&#25105;&#20204;&#30340;&#35774;&#32622;&#20013;&#65292;&#36825;&#26174;&#31034;&#20102;&#23545;Precision&#12289;Recall&#21644;F1&#24471;&#20998;&#30340;&#24191;&#27867;&#25552;&#39640;&#65292;&#20998;&#21035;&#36798;&#21040;24.9%&#12289;27.3%&#21644;14.9%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13903v1 Announce Type: new  Abstract: Open Information Extraction (OIE) is a structured prediction (SP) task in Natural Language Processing (NLP) that aims to extract structured $n$-ary tuples - usually subject-relation-object triples - from free text. The word embeddings in the input text can be enhanced with linguistic features, usually Part-of-Speech (PoS) and Syntactic Dependency Parse (SynDP) labels. However, past enhancement techniques cannot leverage the power of pretrained language models (PLMs), which themselves have been hardly used for OIE. To bridge this gap, we are the first to leverage linguistic features with a Seq2Seq PLM for OIE. We do so by introducing two methods - Weighted Addition and Linearized Concatenation. Our work can give any neural OIE architecture the key performance boost from both PLMs and linguistic features in one go. In our settings, this shows wide improvements of up to 24.9%, 27.3% and 14.9% on Precision, Recall and F1 scores respectively 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20027;&#39064;&#21644;&#37322;&#20041;&#29983;&#25104;&#22522;&#20110;&#38899;&#38901;&#23398;&#30340;&#32469;&#21475;&#20196;&#30340;&#26032;&#26041;&#27861;&#65292;&#29983;&#25104;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#32469;&#21475;&#20196;&#25968;&#25454;&#38598;TwistList 2.0&#65292;&#24182;&#36827;&#34892;&#20102;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.13901</link><description>&lt;p&gt;
&#35757;&#32451;&#19982;&#38480;&#21046;&#65306;&#20174;&#20027;&#39064;&#21644;&#37322;&#20041;&#29983;&#25104;&#22522;&#20110;&#38899;&#38901;&#23398;&#30340;&#32469;&#21475;&#20196;
&lt;/p&gt;
&lt;p&gt;
Train &amp; Constrain: Phonologically Informed Tongue-Twister Generation from Topics and Paraphrases
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20027;&#39064;&#21644;&#37322;&#20041;&#29983;&#25104;&#22522;&#20110;&#38899;&#38901;&#23398;&#30340;&#32469;&#21475;&#20196;&#30340;&#26032;&#26041;&#27861;&#65292;&#29983;&#25104;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#32469;&#21475;&#20196;&#25968;&#25454;&#38598;TwistList 2.0&#65292;&#24182;&#36827;&#34892;&#20102;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#22312;&#38899;&#38901;&#21644;&#35821;&#38899;&#22522;&#30784;&#30340;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#39046;&#22495;&#65292;&#22914;&#21452;&#20851;&#35821;&#21644;&#35799;&#27468;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20135;&#29983;&#32469;&#21475;&#20196;&#30340;&#26032;&#24037;&#20316;-&#36825;&#31181;&#35821;&#35328;&#24418;&#24335;&#38656;&#35201;&#22312;&#38899;&#32032;&#32423;&#21035;&#19978;&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#65292;&#20197;&#26368;&#22823;&#31243;&#24230;&#22320;&#23454;&#29616;&#22768;&#38899;&#37325;&#21472;&#65292;&#21516;&#26102;&#19982;&#36755;&#20837;&#20027;&#39064;&#20445;&#25345;&#35821;&#20041;&#19968;&#33268;&#65292;&#20173;&#28982;&#20445;&#25345;&#35821;&#27861;&#27491;&#30830;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;TwisterLister&#65292;&#36825;&#26159;&#19968;&#20010;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#29983;&#25104;&#22522;&#20110;&#38899;&#38901;&#23398;&#30340;&#32469;&#21475;&#20196;&#30340;&#27969;&#31243;&#65292;&#25105;&#20204;&#29992;&#23427;&#26469;&#29983;&#25104;TwistList 2.0&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#26368;&#22823;&#30340;&#19968;&#20010;&#24050;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#26469;&#33258;&#20154;&#31867;&#21644;LLM&#20316;&#32773;&#21512;&#20316;&#30340;&#36229;&#36807;17K&#20010;&#20363;&#23376;&#12290;&#25105;&#20204;&#30340;&#29983;&#25104;&#27969;&#31243;&#28041;&#21450;&#20351;&#29992;&#38899;&#38901;&#21463;&#38480;&#35789;&#27719;&#20197;&#21450;LLM&#25552;&#31034;&#26469;&#29983;&#25104;&#26032;&#39062;&#30340;&#12289;&#38750;&#34893;&#29983;&#30340;&#32469;&#21475;&#20196;&#23454;&#20363;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#23545;&#36739;&#23567;&#35268;&#27169;&#30340;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13901v1 Announce Type: new  Abstract: Previous work in phonologically and phonetically grounded language generation has mainly focused on domains such as puns and poetry. In this article, we present new work on the generation of tongue-twisters - a form of language that is required to be conditioned on a phoneme level to maximize sound overlap, whilst maintaining semantic consistency with an input topic and still being grammatically correct. We present TwisterLister, a pipeline for generating phonologically informed tongue-twisters from Large Language Models (LLMs) that we use to generate TwistList 2.0, the largest annotated dataset of tongue-twisters to date, consisting of 17K+ examples from a combination of human and LLM authors. Our generation pipeline involves the use of a phonologically constrained vocabulary alongside LLM prompting to generate novel, non-derivative tongue-twister examples. We additionally present the results of automatic and human evaluation of smaller
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#23450;&#37327;&#26694;&#26550;&#21644;&#27969;&#31243;&#65292;&#31995;&#32479;&#35843;&#26597;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25919;&#27835;&#21462;&#21521;&#65292;&#32467;&#26524;&#26174;&#31034;&#36825;&#20123;&#27169;&#22411;&#20542;&#21521;&#20110;&#25552;&#20379;&#19982;&#33258;&#30001;&#20027;&#20041;&#25110;&#24038;&#20542;&#35266;&#28857;&#26356;&#20026;&#25509;&#36817;&#30340;&#22238;&#24212;&#12290;</title><link>https://arxiv.org/abs/2403.13840</link><description>&lt;p&gt;
&#20320;&#31449;&#22312;&#21738;&#19968;&#36793;&#65311;&#35843;&#26597;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25919;&#27835;&#31435;&#22330;
&lt;/p&gt;
&lt;p&gt;
Whose Side Are You On? Investigating the Political Stance of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#23450;&#37327;&#26694;&#26550;&#21644;&#27969;&#31243;&#65292;&#31995;&#32479;&#35843;&#26597;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25919;&#27835;&#21462;&#21521;&#65292;&#32467;&#26524;&#26174;&#31034;&#36825;&#20123;&#27169;&#22411;&#20542;&#21521;&#20110;&#25552;&#20379;&#19982;&#33258;&#30001;&#20027;&#20041;&#25110;&#24038;&#20542;&#35266;&#28857;&#26356;&#20026;&#25509;&#36817;&#30340;&#22238;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22240;&#20854;&#22312;&#25991;&#26412;&#29983;&#25104;&#12289;&#25688;&#35201;&#21644;&#20449;&#24687;&#26816;&#32034;&#31561;&#26085;&#24120;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#32780;&#22791;&#21463;&#27426;&#36814;&#12290;&#38543;&#30528;LLMs&#30340;&#24191;&#27867;&#24212;&#29992;&#19981;&#26029;&#22686;&#38271;&#65292;&#30830;&#20445;&#36825;&#20123;&#27169;&#22411;&#20135;&#29983;&#25919;&#27835;&#20013;&#31435;&#30340;&#22238;&#24212;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#26088;&#22312;&#36991;&#20813;&#20449;&#24687;&#27873;&#27819;&#65292;&#32500;&#25252;&#20195;&#34920;&#20844;&#24179;&#65292;&#24182;&#20943;&#36731;&#30830;&#35748;&#20559;&#35265;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23450;&#37327;&#26694;&#26550;&#21644;&#27969;&#31243;&#65292;&#26088;&#22312;&#31995;&#32479;&#22320;&#35843;&#26597;LLMs&#30340;&#25919;&#27835;&#21462;&#21521;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#28145;&#20837;&#25506;&#35752;LLMs&#22312;&#20843;&#20010;&#26497;&#21270;&#35805;&#39064;&#30340;&#25919;&#27835;&#21462;&#21521;&#65292;&#20174;&#22549;&#32974;&#21040;LGBTQ&#38382;&#39064;&#36328;&#36234;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#20010;&#35805;&#39064;&#19978;&#65292;LLMs&#20542;&#21521;&#20110;&#25552;&#20379;&#19982;&#33258;&#30001;&#20027;&#20041;&#25110;&#24038;&#20542;&#35266;&#28857;&#26356;&#20026;&#25509;&#36817;&#30340;&#22238;&#24212;&#65292;&#32780;&#19981;&#26159;&#19982;&#20445;&#23432;&#20027;&#20041;&#25110;&#21491;&#20542;&#35266;&#28857;&#26356;&#20026;&#25509;&#36817;&#30340;&#22238;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13840v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have gained significant popularity for their application in various everyday tasks such as text generation, summarization, and information retrieval. As the widespread adoption of LLMs continues to surge, it becomes increasingly crucial to ensure that these models yield responses that are politically impartial, with the aim of preventing information bubbles, upholding fairness in representation, and mitigating confirmation bias. In this paper, we propose a quantitative framework and pipeline designed to systematically investigate the political orientation of LLMs. Our investigation delves into the political alignment of LLMs across a spectrum of eight polarizing topics, spanning from abortion to LGBTQ issues. Across topics, the results indicate that LLMs exhibit a tendency to provide responses that closely align with liberal or left-leaning perspectives rather than conservative or right-leaning ones when us
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;SMART&#26694;&#26550;&#65292;&#21487;&#36890;&#36807;&#20934;&#30830;&#24615;&#32422;&#26463;&#26368;&#23567;&#21270;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#25512;&#29702;&#25104;&#26412;&#65292;&#21516;&#26102;&#30830;&#20445;&#32467;&#26524;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.13835</link><description>&lt;p&gt;
&#20351;&#29992;&#20934;&#30830;&#24615;&#20445;&#35777;&#33258;&#21160;&#32553;&#20943;&#35821;&#35328;&#27169;&#22411;&#35268;&#27169;&#20197;&#38477;&#20302;&#22788;&#29702;&#36153;&#29992;&#30340;SMART
&lt;/p&gt;
&lt;p&gt;
SMART: Automatically Scaling Down Language Models with Accuracy Guarantees for Reduced Processing Fees
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13835
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SMART&#26694;&#26550;&#65292;&#21487;&#36890;&#36807;&#20934;&#30830;&#24615;&#32422;&#26463;&#26368;&#23567;&#21270;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#25512;&#29702;&#25104;&#26412;&#65292;&#21516;&#26102;&#30830;&#20445;&#32467;&#26524;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#27493;&#26174;&#33879;&#25552;&#39640;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#37096;&#32626;&#39640;&#24615;&#33021;LLMs&#20250;&#20135;&#29983;&#24040;&#22823;&#30340;&#25104;&#26412;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#22686;&#21152;&#30340;&#21442;&#25968;&#25968;&#37327;&#26088;&#22312;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;&#36825;&#20351;&#24471;&#26368;&#20808;&#36827;&#30340;LLMs&#23545;&#32456;&#31471;&#29992;&#25143;&#32780;&#35328;&#21464;&#24471;&#26356;&#21152;&#26114;&#36149;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;SMART&#65292;&#21363;&#20026;&#38477;&#20302;&#26631;&#35760;&#36153;&#29992;&#32780;&#33258;&#36866;&#24212;&#32553;&#25918;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;LLM&#26694;&#26550;&#65292;&#26088;&#22312;&#26368;&#22823;&#31243;&#24230;&#22320;&#38477;&#20302;NLP&#20219;&#21153;&#30340;&#25512;&#29702;&#25104;&#26412;&#65292;&#21516;&#26102;&#30830;&#20445;&#36275;&#22815;&#30340;&#32467;&#26524;&#36136;&#37327;&#12290;&#23427;&#20351;&#29992;&#25143;&#33021;&#22815;&#20197;&#36755;&#20986;&#30340;&#31561;&#25928;&#24615;&#25351;&#23450;&#20934;&#30830;&#24615;&#32422;&#26463;&#19982;&#26368;&#24378;&#22823;&#30340;LLM&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13835v1 Announce Type: cross  Abstract: The advancement of Large Language Models (LLMs) has significantly boosted performance in natural language processing (NLP) tasks. However, the deployment of high-performance LLMs incurs substantial costs, primarily due to the increased number of parameters aimed at enhancing model performance. This has made the use of state-of-the-art LLMs more expensive for end-users. AI service providers, such as OpenAI and Anthropic, often offer multiple versions of LLMs with varying prices and performance. However, end-users still face challenges in choosing the appropriate LLM for their tasks that balance result quality with cost.   We introduce SMART, Scaling Models Adaptively for Reduced Token Fees, a novel LLM framework designed to minimize the inference costs of NLP tasks while ensuring sufficient result quality. It enables users to specify an accuracy constraint in terms of the equivalence of outputs to those of the most powerful LLM. SMART t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#31995;&#32479;&#35843;&#30740;&#20102;&#38024;&#23545;&#20998;&#23376;&#30740;&#31350;&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#25991;&#26412;&#19982;&#20998;&#23376;&#20043;&#38388;&#30340;&#20851;&#32852;&#12289;&#19981;&#21516;&#27169;&#22411;&#26550;&#26500;&#21644;&#39044;&#35757;&#32451;&#20219;&#21153;&#12290;&#21516;&#26102;&#36824;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#21450;&#25552;&#31034;&#25216;&#26415;&#22312;&#20998;&#23376;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.13830</link><description>&lt;p&gt;
&#25991;&#26412;&#19982;&#20998;&#23376;&#20043;&#38388;&#30340;&#26725;&#26753;&#65306;&#20998;&#23376;&#22810;&#27169;&#24577;&#26694;&#26550;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Bridging Text and Molecule: A Survey on Multimodal Frameworks for Molecule
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#31995;&#32479;&#35843;&#30740;&#20102;&#38024;&#23545;&#20998;&#23376;&#30740;&#31350;&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#25991;&#26412;&#19982;&#20998;&#23376;&#20043;&#38388;&#30340;&#20851;&#32852;&#12289;&#19981;&#21516;&#27169;&#22411;&#26550;&#26500;&#21644;&#39044;&#35757;&#32451;&#20219;&#21153;&#12290;&#21516;&#26102;&#36824;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#21450;&#25552;&#31034;&#25216;&#26415;&#22312;&#20998;&#23376;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#31185;&#23398;&#30740;&#31350;&#20013;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#22312;&#20998;&#23376;&#31185;&#23398;&#39046;&#22495;&#65292;&#23427;&#27491;&#22312;&#25913;&#21464;&#20256;&#32479;&#30340;&#35745;&#31639;&#26426;&#36741;&#21161;&#33539;&#24335;&#65292;&#24341;&#39046;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#26102;&#20195;&#12290;&#38543;&#30528;&#22810;&#27169;&#24577;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#19968;&#31181;&#26032;&#20852;&#36235;&#21183;&#26159;&#26500;&#24314;&#22810;&#27169;&#24577;&#26694;&#26550;&#65292;&#20197;&#20849;&#21516;&#24314;&#27169;&#20998;&#23376;&#21644;&#25991;&#26412;&#39046;&#22495;&#30693;&#35782;&#12290;&#26412;&#25991;&#39318;&#27425;&#31995;&#32479;&#22320;&#35843;&#30740;&#20102;&#38024;&#23545;&#20998;&#23376;&#30740;&#31350;&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20174;&#20998;&#23376;&#28145;&#24230;&#23398;&#20064;&#30340;&#21457;&#23637;&#20837;&#25163;&#65292;&#25351;&#20986;&#28041;&#21450;&#25991;&#26412;&#27169;&#24577;&#30340;&#24517;&#35201;&#24615;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20851;&#27880;&#20102;&#25991;&#26412;-&#20998;&#23376;&#23545;&#40784;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#26681;&#25454;&#23427;&#20204;&#30340;&#26550;&#26500;&#23558;&#24403;&#21069;&#27169;&#22411;&#20998;&#20026;&#20004;&#32452;&#65292;&#24182;&#21015;&#20986;&#30456;&#20851;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25552;&#31034;&#25216;&#26415;&#22312;&#20998;&#23376;&#20219;&#21153;&#21644;&#39044;&#35757;&#32451;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13830v1 Announce Type: cross  Abstract: Artificial intelligence has demonstrated immense potential in scientific research. Within molecular science, it is revolutionizing the traditional computer-aided paradigm, ushering in a new era of deep learning. With recent progress in multimodal learning and natural language processing, an emerging trend has targeted at building multimodal frameworks to jointly model molecules with textual domain knowledge. In this paper, we present the first systematic survey on multimodal frameworks for molecules research. Specifically,we begin with the development of molecular deep learning and point out the necessity to involve textual modality. Next, we focus on recent advances in text-molecule alignment methods, categorizing current models into two groups based on their architectures and listing relevant pre-training tasks. Furthermore, we delves into the utilization of large language models and prompting techniques for molecular tasks and prese
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#29109;&#30340;&#26041;&#27861;&#65292;&#20197;&#27604;&#36739;&#22270;&#20687;&#38598;&#21512;&#20043;&#38388;&#30340;&#22810;&#26679;&#24615;&#65292;&#32780;&#26080;&#38656;&#30495;&#23454;&#26631;&#20934;&#30693;&#35782;&#19988;&#26131;&#20110;&#35745;&#31639;&#65292;&#24182;&#23637;&#31034;&#20102;&#19981;&#21516;&#39044;&#35757;&#32451;&#32593;&#32476;&#36873;&#25321;&#23545;&#25105;&#20204;&#35201;&#35780;&#20272;&#30340;&#22810;&#26679;&#24615;&#27010;&#24565;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.13826</link><description>&lt;p&gt;
&#22312;&#20849;&#21019;&#22270;&#20687;&#29983;&#25104;&#20013;&#34913;&#37327;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
Measuring Diversity in Co-creative Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13826
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#29109;&#30340;&#26041;&#27861;&#65292;&#20197;&#27604;&#36739;&#22270;&#20687;&#38598;&#21512;&#20043;&#38388;&#30340;&#22810;&#26679;&#24615;&#65292;&#32780;&#26080;&#38656;&#30495;&#23454;&#26631;&#20934;&#30693;&#35782;&#19988;&#26131;&#20110;&#35745;&#31639;&#65292;&#24182;&#23637;&#31034;&#20102;&#19981;&#21516;&#39044;&#35757;&#32451;&#32593;&#32476;&#36873;&#25321;&#23545;&#25105;&#20204;&#35201;&#35780;&#20272;&#30340;&#22810;&#26679;&#24615;&#27010;&#24565;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#24050;&#34987;&#25552;&#20986;&#20316;&#20026;&#35780;&#20272;&#20849;&#21019;&#31995;&#32479;&#29983;&#25104;&#20869;&#23481;&#30340;&#21512;&#29702;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#65292;&#20154;&#20204;&#23545;&#21518;&#32773;&#30340;&#26500;&#25104;&#20197;&#21450;&#22914;&#20309;&#34913;&#37327;&#23427;&#23578;&#26080;&#32479;&#19968;&#24847;&#35265;&#12290;&#30446;&#21069;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#22810;&#26679;&#24615;&#30340;&#26041;&#27861;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#22312;&#22823;&#22411;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#23558;&#27169;&#22411;&#36755;&#20986;&#19982;&#19968;&#20010;&#21487;&#33021;&#19981;&#23384;&#22312;&#30340;&#30495;&#23454;&#26631;&#20934;&#36827;&#34892;&#27604;&#36739;&#65292;&#25110;&#32773;&#28041;&#21450;&#21040;&#19981;&#20999;&#23454;&#38469;&#30340;&#35745;&#31639;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#29109;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#29992;&#20110;&#27604;&#36739;&#22270;&#20687;&#38598;&#21512;&#20043;&#38388;&#30340;&#22810;&#26679;&#24615;&#65292;&#32780;&#26080;&#38656;&#30495;&#23454;&#26631;&#20934;&#30693;&#35782;&#19988;&#26131;&#20110;&#35745;&#31639;&#12290;&#25105;&#20204;&#36824;&#27604;&#36739;&#20102;&#20004;&#20010;&#39044;&#35757;&#32451;&#32593;&#32476;&#65292;&#24182;&#23637;&#31034;&#20102;&#36873;&#25321;&#22914;&#20309;&#19982;&#25105;&#20204;&#24819;&#35201;&#35780;&#20272;&#30340;&#22810;&#26679;&#24615;&#27010;&#24565;&#30456;&#20851;&#32852;&#12290;&#26368;&#21518;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#20123;&#27979;&#37327;&#26041;&#27861;&#22312;&#20132;&#20114;&#24335;&#31995;&#32479;&#30340;&#26500;&#24605;&#12289;&#27169;&#22411;&#35780;&#20272;&#20197;&#21450;&#20854;&#20182;&#24212;&#29992;&#39046;&#22495;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13826v1 Announce Type: cross  Abstract: Quality and diversity have been proposed as reasonable heuristics for assessing content generated by co-creative systems, but to date there has been little agreement around what constitutes the latter or how to measure it. Proposed approaches for assessing generative models in terms of diversity have limitations in that they compare the model's outputs to a ground truth that in the era of large pre-trained generative models might not be available, or entail an impractical number of computations. We propose an alternative based on entropy of neural network encodings for comparing diversity between sets of images that does not require ground-truth knowledge and is easy to compute. We also compare two pre-trained networks and show how the choice relates to the notion of diversity that we want to evaluate. We conclude with a discussion of the potential applications of these measures for ideation in interactive systems, model evaluation, an
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#33021;&#22815;&#26816;&#27979;Arxiv&#25237;&#31295;&#20013;AI&#25104;&#20998;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#29289;&#29702;&#12289;&#25968;&#23398;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#25991;&#31456;&#21019;&#24314;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;Originality.ai&#36827;&#34892;&#20998;&#26512;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;98%&#12290;</title><link>https://arxiv.org/abs/2403.13812</link><description>&lt;p&gt;
AI&#29983;&#25104;&#25991;&#26412;&#22312;&#23398;&#26415;&#30740;&#31350;&#20013;&#30340;&#23450;&#37327;&#20998;&#26512;&#65306;&#21033;&#29992;AI&#26816;&#27979;&#24037;&#20855;&#30740;&#31350;Arxiv&#25237;&#31295;&#20013;&#30340;AI&#23384;&#22312;&#24615;
&lt;/p&gt;
&lt;p&gt;
Quantitative Analysis of AI-Generated Texts in Academic Research: A Study of AI Presence in Arxiv Submissions using AI Detection Tool
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13812
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#33021;&#22815;&#26816;&#27979;Arxiv&#25237;&#31295;&#20013;AI&#25104;&#20998;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#29289;&#29702;&#12289;&#25968;&#23398;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#25991;&#31456;&#21019;&#24314;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;Originality.ai&#36827;&#34892;&#20998;&#26512;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;98%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20154;&#23545;ChatGPT&#24863;&#20852;&#36259;&#65292;&#22240;&#20026;&#23427;&#24050;&#25104;&#20026;&#19968;&#20010;&#31361;&#20986;&#30340;AIGC&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#21508;&#31181;&#24773;&#22659;&#19979;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#21709;&#24212;&#65292;&#22914;&#36719;&#20214;&#24320;&#21457;&#21644;&#32500;&#25252;&#12290;ChatGPT&#30340;&#35823;&#29992;&#21487;&#33021;&#24341;&#36215;&#37325;&#22823;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#20844;&#20849;&#23433;&#20840;&#21644;&#25945;&#32946;&#39046;&#22495;&#65292;&#23613;&#31649;&#20854;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#30740;&#31350;&#20154;&#21592;&#22823;&#22810;&#36873;&#25321;&#22312;Arxiv&#19978;&#21457;&#34920;&#20182;&#20204;&#30340;&#20316;&#21697;&#12290;&#26410;&#26469;&#24037;&#20316;&#30340;&#26377;&#25928;&#24615;&#21644;&#29420;&#21019;&#24615;&#21462;&#20915;&#20110;&#22312;&#36825;&#20123;&#36129;&#29486;&#20013;&#26816;&#27979;&#21040;AI&#32452;&#20214;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#19968;&#38656;&#27714;&#65292;&#26412;&#30740;&#31350;&#23558;&#20998;&#26512;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#26597;&#30475;&#23398;&#26415;&#26426;&#26500;&#29992;&#20110;&#22312;Arxiv&#19978;&#21457;&#24067;&#30340;&#21051;&#24847;&#21046;&#36896;&#30340;&#20869;&#23481;&#12290;&#20026;&#20102;&#36827;&#34892;&#36825;&#39033;&#30740;&#31350;&#65292;&#20351;&#29992;&#20102;&#29289;&#29702;&#12289;&#25968;&#23398;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#25991;&#31456;&#21019;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#12290;&#21033;&#29992;&#26032;&#24314;&#31435;&#30340;&#25968;&#25454;&#38598;&#65292;&#25509;&#19979;&#26469;&#30340;&#27493;&#39588;&#26159;&#23558;originality.ai&#25237;&#20837;&#20351;&#29992;&#12290;&#32479;&#35745;&#20998;&#26512;&#26174;&#31034;&#65292;Originality.ai&#38750;&#24120;&#31934;&#20934;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;98%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13812v1 Announce Type: cross  Abstract: Many people are interested in ChatGPT since it has become a prominent AIGC model that provides high-quality responses in various contexts, such as software development and maintenance. Misuse of ChatGPT might cause significant issues, particularly in public safety and education, despite its immense potential. The majority of researchers choose to publish their work on Arxiv. The effectiveness and originality of future work depend on the ability to detect AI components in such contributions. To address this need, this study will analyze a method that can see purposely manufactured content that academic organizations use to post on Arxiv. For this study, a dataset was created using physics, mathematics, and computer science articles. Using the newly built dataset, the following step is to put originality.ai through its paces. The statistical analysis shows that Originality.ai is very accurate, with a rate of 98%.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;RoleInteract&#65292;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#35282;&#33394;&#25198;&#28436;&#23545;&#35805;&#20195;&#29702;&#31038;&#20132;&#24615;&#30340;&#22522;&#20934;&#65292;&#35206;&#30422;&#20102;500&#20010;&#35282;&#33394;&#12289;6000&#22810;&#20010;&#38382;&#39064;&#25552;&#31034;&#21644;30800&#20010;&#23545;&#35805;&#35805;&#35821;&#12290;</title><link>https://arxiv.org/abs/2403.13679</link><description>&lt;p&gt;
RoleInteract&#65306;&#35780;&#20272;&#35282;&#33394;&#25198;&#28436;&#20195;&#29702;&#30340;&#31038;&#20132;&#20114;&#21160;
&lt;/p&gt;
&lt;p&gt;
RoleInteract: Evaluating the Social Interaction of Role-Playing Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13679
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;RoleInteract&#65292;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#35282;&#33394;&#25198;&#28436;&#23545;&#35805;&#20195;&#29702;&#31038;&#20132;&#24615;&#30340;&#22522;&#20934;&#65292;&#35206;&#30422;&#20102;500&#20010;&#35282;&#33394;&#12289;6000&#22810;&#20010;&#38382;&#39064;&#25552;&#31034;&#21644;30800&#20010;&#23545;&#35805;&#35805;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25512;&#21160;&#20102;&#21508;&#31181;AI&#23545;&#35805;&#20195;&#29702;&#30340;&#21457;&#23637;&#65292;&#21253;&#25324;&#27169;&#20223;&#19981;&#21516;&#35282;&#33394;&#21644;&#20154;&#31867;&#34892;&#20026;&#30340;&#35282;&#33394;&#25198;&#28436;&#23545;&#35805;&#20195;&#29702;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;RoleInteract&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#26088;&#22312;&#31995;&#32479;&#35780;&#20272;&#35282;&#33394;&#25198;&#28436;&#23545;&#35805;&#20195;&#29702;&#22312;&#31038;&#20132;&#26041;&#38754;&#34920;&#29616;&#30340;&#22522;&#20934;&#12290;&#35813;&#22522;&#20934;&#20174;&#21508;&#31181;&#26469;&#28304;&#26500;&#24314;&#65292;&#28085;&#30422;&#20102;&#36229;&#36807;500&#20010;&#35282;&#33394;&#12289;6000&#22810;&#20010;&#38382;&#39064;&#25552;&#31034;&#21644;30800&#20010;&#22810;&#36718;&#35282;&#33394;&#25198;&#28436;&#35805;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13679v1 Announce Type: new  Abstract: Large language models (LLMs) have advanced the development of various AI conversational agents, including role-playing conversational agents that mimic diverse characters and human behaviors. While prior research has predominantly focused on enhancing the conversational capability, role-specific knowledge, and stylistic attributes of these agents, there has been a noticeable gap in assessing their social intelligence. In this paper, we introduce RoleInteract, the first benchmark designed to systematically evaluate the sociality of role-playing conversational agents at both individual and group levels of social interactions. The benchmark is constructed from a variety of sources and covers a wide range of 500 characters and over 6,000 question prompts and 30,800 multi-turn role-playing utterances. We conduct comprehensive evaluations on this benchmark using mainstream open-source and closed-source LLMs. We find that agents excelling in in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;Translationese&#21512;&#25104;&#25968;&#25454;&#20316;&#20026;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#29992;&#24615;&#65292;&#23637;&#31034;&#20102;&#22312;&#33521;&#35821;&#20197;&#22806;&#30340;&#35821;&#35328;&#20013;&#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#21019;&#24314;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;LMs&#39044;&#35757;&#32451;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#36807;&#20351;&#29992;&#36731;&#37327;&#32423;TinyLMs&#39044;&#35757;&#32451;&#26469;&#36807;&#28388;&#21512;&#25104;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.13638</link><description>&lt;p&gt;
&#19981;&#24517;&#25285;&#24515;&#22914;&#26524;&#24744;&#27809;&#26377;&#25968;&#25454;&#65306;&#21033;&#29992;Translationese&#26500;&#24314;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Do Not Worry if You Do Not Have Data: Building Pretrained Language Models Using Translationese
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;Translationese&#21512;&#25104;&#25968;&#25454;&#20316;&#20026;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#29992;&#24615;&#65292;&#23637;&#31034;&#20102;&#22312;&#33521;&#35821;&#20197;&#22806;&#30340;&#35821;&#35328;&#20013;&#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#21019;&#24314;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;LMs&#39044;&#35757;&#32451;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#36807;&#20351;&#29992;&#36731;&#37327;&#32423;TinyLMs&#39044;&#35757;&#32451;&#26469;&#36807;&#28388;&#21512;&#25104;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#23558;&#26426;&#22120;&#32763;&#35793;&#21019;&#24314;&#30340;&#21512;&#25104;&#25968;&#25454;Translationese&#29992;&#20316;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#23454;&#29992;&#24615;&#12290;&#39044;&#35757;&#32451;&#38656;&#35201;&#22823;&#37327;&#30340;&#21333;&#35821;&#25968;&#25454;&#65292;&#23545;&#20110;&#33521;&#35821;&#20197;&#22806;&#30340;&#35821;&#35328;&#65292;&#36825;&#20123;&#25968;&#25454;&#22823;&#37096;&#20998;&#26159;&#19981;&#21487;&#29992;&#30340;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#26469;&#35299;&#20915;&#36825;&#31181;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#20197;&#33521;&#35821;&#21644;Indic&#35821;&#35328;&#20026;&#20363;&#65292;&#23558;&#32593;&#32476;&#25235;&#21462;&#30340;&#21333;&#35821;&#25991;&#26723;&#65288;&#24178;&#20928;&#30340;&#65289;&#32763;&#35793;&#25104;&#30446;&#26631;&#35821;&#35328;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#36825;&#20123;Translationese&#25968;&#25454;&#65288;&#21512;&#25104;&#25968;&#25454;&#65289;&#19978;&#35757;&#32451;&#21253;&#21547;28M&#21644;85M&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#19982;&#22312;&#24178;&#20928;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#30340;LMs&#30456;&#27604;&#65292;NLU&#20219;&#21153;&#30340;&#24615;&#33021;&#20165;&#24046;3.56&#65285;&#65292;NLG&#20219;&#21153;&#30340;&#24046;&#24322;&#20026;1.51&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#22312;&#24178;&#20928;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#30340;&#36731;&#37327;&#32423;TinyLMs&#26469;&#39640;&#25928;&#36807;&#28388;&#21512;&#25104;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#36825;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13638v1 Announce Type: new  Abstract: In this paper, we explore the utility of \textit{Translationese} as synthetic data created using machine translation for pre-training language models (LMs). Pre-training requires vast amounts of monolingual data, which is mostly unavailable for languages other than English. Recently, there has been a growing interest in using synthetic data to address this data scarcity. We take the case of English and Indic languages and translate web-crawled monolingual documents (clean) into the target language. Then, we train language models containing 28M and 85M parameters on this translationese data (synthetic). We show that their performance on downstream natural language understanding and generative tasks is only 3.56\% poorer on NLU tasks and 1.51\% on NLG tasks than LMs pre-trained on clean data. Further, we propose the use of lightweight \textit{TinyLMs} pre-trained on clean data to filter synthetic data efficiently which significantly improv
&lt;/p&gt;</description></item><item><title>LlamaFactory&#26159;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#25972;&#21512;&#20102;&#19968;&#31995;&#21015;&#21069;&#27839;&#30340;&#39640;&#25928;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#32534;&#30721;&#30340;&#24773;&#20917;&#19979;&#28789;&#27963;&#23450;&#21046;100&#22810;&#31181;LLMs&#30340;&#24494;&#35843;&#12290;</title><link>https://arxiv.org/abs/2403.13372</link><description>&lt;p&gt;
LlamaFactory&#65306;100&#22810;&#31181;&#35821;&#35328;&#27169;&#22411;&#30340;&#32479;&#19968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13372
&lt;/p&gt;
&lt;p&gt;
LlamaFactory&#26159;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#25972;&#21512;&#20102;&#19968;&#31995;&#21015;&#21069;&#27839;&#30340;&#39640;&#25928;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#32534;&#30721;&#30340;&#24773;&#20917;&#19979;&#28789;&#27963;&#23450;&#21046;100&#22810;&#31181;LLMs&#30340;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#30340;&#24494;&#35843;&#23545;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#19981;&#21516;&#27169;&#22411;&#19978;&#23454;&#29616;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#38750;&#24179;&#20961;&#30340;&#21162;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LlamaFactory&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#38598;&#25104;&#20102;&#19968;&#22871;&#21069;&#27839;&#30340;&#39640;&#25928;&#35757;&#32451;&#26041;&#27861;&#12290;&#23427;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#20869;&#32622;&#30340;Web UI LlamaBoard &#28789;&#27963;&#23450;&#21046;100&#22810;&#31181;LLMs&#30340;&#24494;&#35843;&#65292;&#26080;&#38656;&#32534;&#30721;&#12290;&#25105;&#20204;&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#19978;&#32463;&#39564;&#24615;&#22320;&#39564;&#35777;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;&#24050;&#21457;&#24067;&#22312; https://github.com/hiyouga/LLaMA-Factory&#65292;&#24182;&#24050;&#33719;&#24471;&#36229;&#36807;13,000&#39063;&#26143;&#21644;1,600&#20010;&#20998;&#25903;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13372v1 Announce Type: new  Abstract: Efficient fine-tuning is vital for adapting large language models (LLMs) to downstream tasks. However, it requires non-trivial efforts to implement these methods on different models. We present LlamaFactory, a unified framework that integrates a suite of cutting-edge efficient training methods. It allows users to flexibly customize the fine-tuning of 100+ LLMs without the need for coding through the built-in web UI LlamaBoard. We empirically validate the efficiency and effectiveness of our framework on language modeling and text generation tasks. It has been released at https://github.com/hiyouga/LLaMA-Factory and already received over 13,000 stars and 1,600 forks.
&lt;/p&gt;</description></item><item><title>&#21512;&#24182;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#21363;&#21487;&#21019;&#24314;&#22810;&#20219;&#21153;&#27169;&#22411;&#65292;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#35299;&#20915;AI&#20013;&#30340;&#22797;&#26434;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.13257</link><description>&lt;p&gt;
Arcee&#30340;MergeKit&#65306;&#29992;&#20110;&#21512;&#24182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
Arcee's MergeKit: A Toolkit for Merging Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13257
&lt;/p&gt;
&lt;p&gt;
&#21512;&#24182;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#21363;&#21487;&#21019;&#24314;&#22810;&#20219;&#21153;&#27169;&#22411;&#65292;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#35299;&#20915;AI&#20013;&#30340;&#22797;&#26434;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#30340;&#24555;&#36895;&#25193;&#24352;&#20026;&#36890;&#36807;&#21512;&#24182;&#20854;&#21442;&#25968;&#26469;&#32467;&#21512;&#36825;&#20123;&#27169;&#22411;&#26816;&#26597;&#28857;&#30340;&#33021;&#21147;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#36801;&#31227;&#23398;&#20064;&#30340;&#36827;&#27493;&#23548;&#33268;&#20102;&#22823;&#37327;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#30340;&#24320;&#21457;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#19987;&#38376;&#38024;&#23545;&#20010;&#21035;&#20219;&#21153;&#36827;&#34892;&#19987;&#38376;&#21270;&#65292;&#26080;&#27861;&#21033;&#29992;&#24444;&#27492;&#30340;&#20248;&#21183;&#12290;&#27169;&#22411;&#21512;&#24182;&#20419;&#36827;&#20102;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#21019;&#24314;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#65292;&#20026;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#21644;&#22810;&#21151;&#33021;&#24615;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;&#36890;&#36807;&#20445;&#30041;&#21407;&#22987;&#27169;&#22411;&#30340;&#22266;&#26377;&#33021;&#21147;&#65292;&#27169;&#22411;&#21512;&#24182;&#35299;&#20915;&#20102;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#22797;&#26434;&#25361;&#25112;&#65292;&#21253;&#25324;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#22256;&#38590;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#19968;&#19981;&#26029;&#25193;&#22823;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MergeKit&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#12289;&#24320;&#28304;&#30340;&#24211;&#65292;&#26088;&#22312;&#20419;&#36827;&#27169;&#22411;&#21512;&#24182;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13257v1 Announce Type: new  Abstract: The rapid expansion of the open-source language model landscape presents an opportunity to merge the competencies of these model checkpoints by combining their parameters. Advances in transfer learning, the process of fine-tuning pre-trained models for specific tasks, has resulted in the development of vast amounts of task-specific models, typically specialized in individual tasks and unable to utilize each other's strengths. Model merging facilitates the creation of multitask models without the need for additional training, offering a promising avenue for enhancing model performance and versatility. By preserving the intrinsic capabilities of the original models, model merging addresses complex challenges in AI - including the difficulties of catastrophic forgetting and multi-task learning. To support this expanding area of research, we introduce MergeKit, a comprehensive, open-source library designed to facilitate the application of mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#38024;&#23545;&#34920;&#29616;&#24615;&#20260;&#23475;&#21644;&#26381;&#21153;&#36136;&#37327;&#20260;&#23475;&#30340;&#32650;&#39548;2&#23433;&#20840;&#20445;&#38556;&#25514;&#26045;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25351;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.13213</link><description>&lt;p&gt;
&#20174;&#34920;&#29616;&#24615;&#20260;&#23475;&#21040;&#26381;&#21153;&#36136;&#37327;&#20260;&#23475;:&#32650;&#39548;2&#23433;&#20840;&#20445;&#38556;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
From Representational Harms to Quality-of-Service Harms: A Case Study on Llama 2 Safety Safeguards
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#38024;&#23545;&#34920;&#29616;&#24615;&#20260;&#23475;&#21644;&#26381;&#21153;&#36136;&#37327;&#20260;&#23475;&#30340;&#32650;&#39548;2&#23433;&#20840;&#20445;&#38556;&#25514;&#26045;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25351;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#23548;&#33268;&#23427;&#20204;&#22312;&#21508;&#20010;&#39046;&#22495;&#34987;&#24191;&#27867;&#37319;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#36827;&#27493;&#20063;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#23433;&#20840;&#39118;&#38505;&#65292;&#24182;&#24341;&#21457;&#20102;&#23545;&#20854;&#23545;&#24050;&#32463;&#36793;&#32536;&#21270;&#20154;&#32676;&#30340;&#19981;&#21033;&#24433;&#21709;&#30340;&#25285;&#24551;&#12290;&#23613;&#31649;&#23384;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#20943;&#36731;&#25514;&#26045;&#26469;&#24320;&#21457;&#23433;&#20840;&#20445;&#38556;&#25514;&#26045;&#65292;&#27604;&#22914;&#30417;&#30563;&#24335;&#30340;&#23433;&#20840;&#23450;&#21521;&#24494;&#35843;&#21644;&#21033;&#29992;&#26469;&#33258;&#20154;&#31867;&#21453;&#39304;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#65292;&#20294;&#20851;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#21644;&#20869;&#22312;&#20559;&#35265;&#20173;&#23384;&#22312;&#22810;&#37325;&#20851;&#27880;&#12290;&#27492;&#22806;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#65292;&#20026;&#20102;&#23433;&#20840;&#32780;&#20248;&#21270;&#30340;&#27169;&#22411;&#36890;&#24120;&#20250;&#23637;&#31034;&#22840;&#22823;&#30340;&#23433;&#20840;&#34892;&#20026;&#65292;&#27604;&#22914;&#20986;&#20110;&#39044;&#38450;&#25514;&#26045;&#32780;&#20542;&#21521;&#20110;&#19981;&#22238;&#24212;&#26576;&#20123;&#35831;&#27714;&#12290;&#22240;&#27492;&#65292;&#25991;&#29486;&#20013;&#24050;&#32463;&#35760;&#24405;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#23454;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#20043;&#38388;&#30340;&#26126;&#26174;&#26435;&#34913;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#23433;&#20840;&#25514;&#26045;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#35780;&#20272;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13213v1 Announce Type: cross  Abstract: Recent progress in large language models (LLMs) has led to their widespread adoption in various domains. However, these advancements have also introduced additional safety risks and raised concerns regarding their detrimental impact on already marginalized populations. Despite growing mitigation efforts to develop safety safeguards, such as supervised safety-oriented fine-tuning and leveraging safe reinforcement learning from human feedback, multiple concerns regarding the safety and ingrained biases in these models remain. Furthermore, previous work has demonstrated that models optimized for safety often display exaggerated safety behaviors, such as a tendency to refrain from responding to certain requests as a precautionary measure. As such, a clear trade-off between the helpfulness and safety of these models has been documented in the literature. In this paper, we further investigate the effectiveness of safety measures by evaluatin
&lt;/p&gt;</description></item><item><title>RankPrompt &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#33258;&#25105;&#25490;&#24207;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.12373</link><description>&lt;p&gt;
RankPrompt&#65306;&#36880;&#27493;&#27604;&#36739;&#20351;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#26356;&#22909;&#30340;&#25512;&#29702;&#32773;
&lt;/p&gt;
&lt;p&gt;
RankPrompt: Step-by-Step Comparisons Make Language Models Better Reasoners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12373
&lt;/p&gt;
&lt;p&gt;
RankPrompt &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#33258;&#25105;&#25490;&#24207;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#20687;ChatGPT&#36825;&#26679;&#30340;&#26368;&#20808;&#36827;&#30340;LLMs&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20063;&#23481;&#26131;&#20986;&#29616;&#36923;&#36753;&#38169;&#35823;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#37096;&#32626;&#29305;&#23450;&#20110;&#20219;&#21153;&#30340;&#39564;&#35777;&#22120;&#25110;&#22312;&#22810;&#20010;&#25512;&#29702;&#36335;&#24452;&#19978;&#25237;&#31080;&#65292;&#35201;&#20040;&#38656;&#35201;&#22823;&#37327;&#20154;&#31867;&#27880;&#37322;&#65292;&#35201;&#20040;&#22312;&#23384;&#22312;&#19981;&#19968;&#33268;&#21709;&#24212;&#30340;&#22330;&#26223;&#20013;&#22833;&#36133;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;RankPrompt&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#20351;LLMs&#33021;&#22815;&#33258;&#34892;&#23545;&#20854;&#21709;&#24212;&#36827;&#34892;&#25490;&#24207;&#32780;&#26080;&#38656;&#39069;&#22806;&#36164;&#28304;&#12290;RankPrompt&#23558;&#25490;&#24207;&#38382;&#39064;&#20998;&#35299;&#20026;&#22810;&#20010;&#21709;&#24212;&#20043;&#38388;&#30340;&#19968;&#31995;&#21015;&#27604;&#36739;&#65292;&#21033;&#29992;LLMs&#33258;&#21160;&#29983;&#25104;&#27604;&#36739;&#38142;&#20316;&#20026;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#22266;&#26377;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;11&#20010;&#31639;&#26415;&#25512;&#29702;&#21644;&#24120;&#35782;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;RankPrompt&#26174;&#33879;&#25552;&#39640;&#20102;ChatGPT&#21644;GPT-4&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12373v1 Announce Type: new  Abstract: Large Language Models (LLMs) have achieved impressive performance across various reasoning tasks. However, even state-of-the-art LLMs such as ChatGPT are prone to logical errors during their reasoning processes. Existing solutions, which include deploying task-specific verifiers or voting over multiple reasoning paths, either require extensive human annotations or fail in scenarios with inconsistent responses. To address these challenges, we introduce RankPrompt, a new prompting method that enables LLMs to self-rank their responses without additional resources. RankPrompt breaks down the ranking problem into a series of comparisons among diverse responses, leveraging the inherent capabilities of LLMs to generate chains of comparison as contextual exemplars. Our experiments across 11 arithmetic and commonsense reasoning tasks show that RankPrompt significantly enhances the reasoning performance of ChatGPT and GPT-4, with improvements of u
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#36718;&#23545;&#35805;&#31435;&#22330;&#26816;&#27979;&#25968;&#25454;&#38598;&#65288;MT-CSD&#65289;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#23616;&#23616;&#37096;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;GLAN&#65289;&#26469;&#35299;&#20915;&#23545;&#35805;&#25968;&#25454;&#20013;&#30340;&#38271;&#36317;&#31163;&#21644;&#30701;&#36317;&#31163;&#20381;&#36182;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.11145</link><description>&lt;p&gt;
&#19968;&#39033;&#29992;&#20110;&#23545;&#35805;&#31435;&#22330;&#26816;&#27979;&#30340;&#25361;&#25112;&#25968;&#25454;&#38598;&#21644;&#26377;&#25928;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Challenge Dataset and Effective Models for Conversational Stance Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11145
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#36718;&#23545;&#35805;&#31435;&#22330;&#26816;&#27979;&#25968;&#25454;&#38598;&#65288;MT-CSD&#65289;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#23616;&#23616;&#37096;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;GLAN&#65289;&#26469;&#35299;&#20915;&#23545;&#35805;&#25968;&#25454;&#20013;&#30340;&#38271;&#36317;&#31163;&#21644;&#30701;&#36317;&#31163;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#31435;&#22330;&#26816;&#27979;&#30740;&#31350;&#36890;&#24120;&#38598;&#20013;&#22312;&#35780;&#20272;&#21333;&#20010;&#23454;&#20363;&#20869;&#30340;&#31435;&#22330;&#65292;&#20174;&#32780;&#22312;&#26377;&#25928;&#24314;&#27169;&#20851;&#20110;&#21516;&#19968;&#29305;&#23450;&#20027;&#39064;&#30340;&#22810;&#26041;&#35752;&#35770;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#36825;&#22312;&#30495;&#23454;&#31038;&#20132;&#23186;&#20307;&#20114;&#21160;&#20013;&#33258;&#28982;&#21457;&#29983;&#12290;&#36825;&#31181;&#38480;&#21046;&#20027;&#35201;&#26159;&#30001;&#20110;&#32570;&#20047;&#30495;&#23454;&#22797;&#21046;&#30495;&#23454;&#31038;&#20132;&#23186;&#20307;&#32972;&#26223;&#30340;&#25968;&#25454;&#38598;&#65292;&#38459;&#30861;&#20102;&#23545;&#35805;&#31435;&#22330;&#26816;&#27979;&#30340;&#30740;&#31350;&#36827;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#36718;&#23545;&#35805;&#31435;&#22330;&#26816;&#27979;&#25968;&#25454;&#38598;&#65288;&#31216;&#20026;\textbf{MT-CSD}&#65289;&#65292;&#28085;&#30422;&#20102;&#29992;&#20110;&#23545;&#35805;&#31435;&#22330;&#26816;&#27979;&#30340;&#22810;&#20010;&#30446;&#26631;&#12290;&#20026;&#20102;&#20174;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#31435;&#22330;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#23616;&#23616;&#37096;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;\textbf{GLAN}&#65289;&#65292;&#20197;&#35299;&#20915;&#23545;&#35805;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#38271;&#36317;&#31163;&#21644;&#30701;&#36317;&#31163;&#20381;&#36182;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21363;&#20351;&#26159;GLAN&#36825;&#26679;&#30340;&#26368;&#20808;&#36827;&#30340;&#31435;&#22330;&#26816;&#27979;&#26041;&#27861;&#65292;&#22312;......
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11145v1 Announce Type: new  Abstract: Previous stance detection studies typically concentrate on evaluating stances within individual instances, thereby exhibiting limitations in effectively modeling multi-party discussions concerning the same specific topic, as naturally transpire in authentic social media interactions. This constraint arises primarily due to the scarcity of datasets that authentically replicate real social media contexts, hindering the research progress of conversational stance detection. In this paper, we introduce a new multi-turn conversation stance detection dataset (called \textbf{MT-CSD}), which encompasses multiple targets for conversational stance detection. To derive stances from this challenging dataset, we propose a global-local attention network (\textbf{GLAN}) to address both long and short-range dependencies inherent in conversational data. Notably, even state-of-the-art stance detection methods, exemplified by GLAN, exhibit an accuracy of on
&lt;/p&gt;</description></item><item><title>m&amp;m's&#24341;&#20837;&#20102;&#19968;&#20010;&#21253;&#21547;4K+&#22810;&#27493;&#39588;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#28041;&#21450;33&#31181;&#24037;&#20855;&#65292;&#29992;&#20110;&#35780;&#20272;LLM&#20316;&#20026;&#35268;&#21010;&#22120;&#30340;&#35774;&#35745;&#20915;&#31574;&#12290;</title><link>https://arxiv.org/abs/2403.11085</link><description>&lt;p&gt;
m&amp;m's: &#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22810;&#27493;&#39588;&#22810;&#27169;&#24577;&#20219;&#21153;&#24037;&#20855;&#20351;&#29992;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
m&amp;m's: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11085
&lt;/p&gt;
&lt;p&gt;
m&amp;m's&#24341;&#20837;&#20102;&#19968;&#20010;&#21253;&#21547;4K+&#22810;&#27493;&#39588;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#28041;&#21450;33&#31181;&#24037;&#20855;&#65292;&#29992;&#20110;&#35780;&#20272;LLM&#20316;&#20026;&#35268;&#21010;&#22120;&#30340;&#35774;&#35745;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22810;&#27169;&#24577;&#38382;&#39064;&#24456;&#23569;&#30001;&#21333;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35299;&#20915;&#65292;&#36890;&#24120;&#38656;&#35201;&#22810;&#27493;&#39588;&#35745;&#31639;&#35745;&#21010;&#65292;&#28041;&#21450;&#25340;&#25509;&#22810;&#20010;&#27169;&#22411;&#12290; &#24037;&#20855;&#22686;&#24378;&#22411;LLM&#26497;&#26377;&#21487;&#33021;&#33258;&#21160;&#21270;&#29983;&#25104;&#36825;&#31181;&#35745;&#31639;&#35745;&#21010;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#29992;&#20110;&#35780;&#20272;LLM&#20316;&#20026;&#22810;&#27493;&#39588;&#22810;&#27169;&#24577;&#20219;&#21153;&#35268;&#21010;&#22120;&#30340;&#26631;&#20934;&#21270;&#22522;&#20934;&#65292;&#38459;&#30861;&#20102;&#23545;&#35268;&#21010;&#22120;&#35774;&#35745;&#20915;&#31574;&#30340;&#31995;&#32479;&#30740;&#31350;&#12290;LLM&#26159;&#21542;&#24212;&#19968;&#27425;&#24615;&#29983;&#25104;&#25972;&#20010;&#35745;&#21010;&#36824;&#26159;&#36880;&#27493;&#29983;&#25104;&#65311;&#23427;&#20204;&#26159;&#21542;&#24212;&#35813;&#30452;&#25509;&#20351;&#29992;Python&#20195;&#30721;&#35843;&#29992;&#24037;&#20855;&#65292;&#36824;&#26159;&#36890;&#36807;&#31867;&#20284;JSON&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#26684;&#24335;&#65311;&#21453;&#39304;&#26159;&#21542;&#25913;&#21892;&#35268;&#21010;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#20197;&#21450;&#26356;&#22810;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;m&amp;m's&#65306;&#19968;&#20010;&#22522;&#20934;&#65292;&#21253;&#21547;4K+&#20010;&#28041;&#21450;33&#31181;&#24037;&#20855;&#30340;&#22810;&#27493;&#39588;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#20854;&#20013;&#21253;&#25324;&#22810;&#27169;&#24577;&#27169;&#22411;&#12289;(&#20813;&#36153;)&#20844;&#20849;API&#21644;&#22270;&#20687;&#22788;&#29702;&#27169;&#22359;&#12290;&#23545;&#20110;&#27599;&#20010;&#20219;&#21153;&#26597;&#35810;&#65292;&#25105;&#20204;&#25552;&#20379;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#33258;&#21160;&#29983;&#25104;&#30340;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11085v1 Announce Type: cross  Abstract: Real-world multi-modal problems are rarely solved by a single machine learning model, and often require multi-step computational plans that involve stitching several models. Tool-augmented LLMs hold tremendous promise for automating the generation of such computational plans. However, the lack of standardized benchmarks for evaluating LLMs as planners for multi-step multi-modal tasks has prevented a systematic study of planner design decisions. Should LLMs generate a full plan in a single shot or step-by-step? Should they invoke tools directly with Python code or through structured data formats like JSON? Does feedback improve planning? To answer these questions and more, we introduce m&amp;m's: a benchmark containing 4K+ multi-step multi-modal tasks involving 33 tools that include multi-modal models, (free) public APIs, and image processing modules. For each of these task queries, we provide automatically generated plans using this realis
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19977;&#31181;&#31574;&#30053;&#26469;&#22686;&#24378;&#22522;&#20110;&#20844;&#24320;&#21487;&#29992;MLLMs&#30340;&#36164;&#28304;&#36739;&#23569;&#30340;&#35821;&#35328;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#25193;&#23637;&#35789;&#27719;&#12289;&#21452;&#35821;&#25968;&#25454;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#24494;&#35843;&#12290;</title><link>https://arxiv.org/abs/2403.10882</link><description>&lt;p&gt;
&#20248;&#21270;&#22810;&#35821;&#35328;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#22686;&#24378;&#65306;&#20197;&#38889;&#35821;&#20026;&#20363;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Optimizing Language Augmentation for Multilingual Large Language Models: A Case Study on Korean
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10882
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19977;&#31181;&#31574;&#30053;&#26469;&#22686;&#24378;&#22522;&#20110;&#20844;&#24320;&#21487;&#29992;MLLMs&#30340;&#36164;&#28304;&#36739;&#23569;&#30340;&#35821;&#35328;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#25193;&#23637;&#35789;&#27719;&#12289;&#21452;&#35821;&#25968;&#25454;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20351;&#29992;&#39044;&#35757;&#32451;&#26469;&#39044;&#27979;&#19979;&#19968;&#20010;&#21333;&#35789;&#65307;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#25193;&#23637;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#35768;&#22810;&#22823;&#22411;&#31185;&#25216;&#20844;&#21496;&#21644;&#30740;&#31350;&#26426;&#26500;&#24050;&#32463;&#24320;&#21457;&#20102;&#22810;&#35821;&#35328;LLMs&#65288;MLLMs&#65289;&#20197;&#28385;&#36275;&#24403;&#21069;&#38656;&#27714;&#65292;&#20294;&#24573;&#35270;&#20102;&#36164;&#28304;&#36739;&#23569;&#30340;&#35821;&#35328;&#65288;LRLs&#65289;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19977;&#31181;&#31574;&#30053;&#26469;&#22686;&#24378;&#22522;&#20110;&#20844;&#24320;&#21487;&#29992;MLLMs&#30340;LRLs&#30340;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#25193;&#23637;LRLs&#30340;MLLM&#35789;&#27719;&#20197;&#22686;&#24378;&#34920;&#36798;&#24615;&#12290;&#20854;&#27425;&#65292;&#20351;&#29992;&#21452;&#35821;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#20197;&#23545;&#40784;&#39640;&#36164;&#28304;&#35821;&#35328;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#31532;&#19977;&#65292;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#23567;&#35268;&#27169;&#25351;&#23548;&#25968;&#25454;&#38598;&#65292;&#24182;&#36827;&#34892;&#25351;&#23548;&#24494;&#35843;&#20197;&#22686;&#24378;LRL&#12290;&#23454;&#39564;&#37319;&#29992;&#20102;Llama2&#27169;&#22411;&#65292;&#20197;&#38889;&#35821;&#20316;&#20026;LRL&#65292;&#24182;&#22312;&#20843;&#39033;&#20219;&#21153;&#20013;&#23545;&#20854;&#19982;&#20854;&#20182;&#24050;&#24320;&#21457;&#30340;LLMs&#36827;&#34892;&#20102;&#23450;&#37327;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#20154;&#31867;&#35780;&#20272;&#36827;&#34892;&#20102;&#23450;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10882v1 Announce Type: cross  Abstract: Large language models (LLMs) use pretraining to predict the subsequent word; however, their expansion requires significant computing resources. Numerous big tech companies and research institutes have developed multilingual LLMs (MLLMs) to meet current demands, overlooking less-resourced languages (LRLs). This study proposed three strategies to enhance the performance of LRLs based on the publicly available MLLMs. First, the MLLM vocabularies of LRLs were expanded to enhance expressiveness. Second, bilingual data were used for pretraining to align the high- and less-resourced languages. Third, a high-quality small-scale instruction dataset was constructed and instruction-tuning was performed to augment the LRL. The experiments employed the Llama2 model and Korean was used as the LRL, which was quantitatively evaluated against other developed LLMs across eight tasks. Furthermore, a qualitative assessment was performed based on human eva
&lt;/p&gt;</description></item><item><title>&#35270;&#35273;&#25351;&#23548;&#35843;&#25972;&#26102;&#38656;&#35201;&#36827;&#34892;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#65292;&#36890;&#36807;&#26032;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;TIVE&#65292;&#26681;&#25454;&#20219;&#21153;&#32423;&#21644;&#23454;&#20363;&#32423;&#20215;&#20540;&#26469;&#28040;&#38500;&#35270;&#35273;&#25351;&#23548;&#25968;&#25454;&#20013;&#30340;&#20887;&#20313;&#12290;</title><link>https://arxiv.org/abs/2403.09559</link><description>&lt;p&gt;
&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#23545;&#35270;&#35273;&#25351;&#23548;&#35843;&#25972;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Less is More: Data Value Estimation for Visual Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09559
&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#25351;&#23548;&#35843;&#25972;&#26102;&#38656;&#35201;&#36827;&#34892;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#65292;&#36890;&#36807;&#26032;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;TIVE&#65292;&#26681;&#25454;&#20219;&#21153;&#32423;&#21644;&#23454;&#20363;&#32423;&#20215;&#20540;&#26469;&#28040;&#38500;&#35270;&#35273;&#25351;&#23548;&#25968;&#25454;&#20013;&#30340;&#20887;&#20313;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#25351;&#23548;&#35843;&#25972;&#26159;&#26500;&#24314;&#22810;&#27169;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#20851;&#38190;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35270;&#35273;&#22330;&#26223;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MLLMs&#20027;&#35201;&#20381;&#36182;&#20110;&#22810;&#20010;&#39640;&#24230;&#22810;&#26679;&#21270;&#30340;&#35270;&#35273;&#25351;&#23548;&#25968;&#25454;&#38598;&#30340;&#28151;&#21512;&#35757;&#32451;&#65288;&#29978;&#33267;&#36229;&#36807;&#19968;&#30334;&#19975;&#26465;&#25351;&#23548;&#65289;&#65292;&#36825;&#21487;&#33021;&#24341;&#20837;&#25968;&#25454;&#20887;&#20313;&#12290;&#20026;&#20102;&#35843;&#26597;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#35777;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#35270;&#35273;&#25351;&#23548;&#25968;&#25454;&#38598;&#20869;&#23384;&#22312;&#26174;&#33879;&#20887;&#20313;&#65292;&#24182;&#26174;&#31034;&#22823;&#22823;&#20943;&#23569;&#20960;&#20010;&#25351;&#23548;&#25968;&#25454;&#38598;&#30340;&#25968;&#37327;&#29978;&#33267;&#19981;&#20250;&#24433;&#21709;&#24615;&#33021;&#12290;&#26681;&#25454;&#30740;&#31350;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;TIVE&#65292;&#20197;&#28040;&#38500;&#35270;&#35273;&#25351;&#23548;&#25968;&#25454;&#20013;&#30340;&#20887;&#20313;&#12290;TIVE&#39318;&#20808;&#26681;&#25454;&#35745;&#31639;&#30340;&#26799;&#24230;&#20272;&#35745;&#35270;&#35273;&#25351;&#23548;&#30340;&#20219;&#21153;&#32423;&#21644;&#23454;&#20363;&#32423;&#20215;&#20540;&#12290;&#28982;&#21518;&#65292;&#26681;&#25454;&#20272;&#35745;&#30340;&#20215;&#20540;&#65292;TIVE&#30830;&#23450;&#20102;&#20219;&#21153;&#32423;&#21644;&#23454;&#20363;&#32423;&#25351;&#23548;&#36873;&#25321;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09559v1 Announce Type: new  Abstract: Visual instruction tuning is the key to building multimodal large language models (MLLMs), which greatly improves the reasoning capabilities of large language models (LLMs) in vision scenario. However, existing MLLMs mostly rely on a mixture of multiple highly diverse visual instruction datasets for training (even more than a million instructions), which may introduce data redundancy. To investigate this issue, we conduct a series of empirical studies, which reveal a significant redundancy within the visual instruction datasets, and show that greatly reducing the amount of several instruction dataset even do not affect the performance. Based on the findings, we propose a new data selection approach TIVE, to eliminate redundancy within visual instruction data. TIVE first estimates the task-level and instance-level value of the visual instructions based on computed gradients. Then, according to the estimated values, TIVE determines the tas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#21069;&#27839;&#27169;&#22411;&#23578;&#23384;&#22312;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#24046;&#36317;&#65292;&#25506;&#35752;&#20102;&#35757;&#32451;&#24320;&#28304;&#23567;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20197;&#24357;&#34917;&#20020;&#24202;&#38656;&#27714;&#30340;&#29983;&#29289;&#21307;&#23398;&#33021;&#21147;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.08002</link><description>&lt;p&gt;
&#35757;&#32451;&#23567;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20197;&#22635;&#34917;&#29983;&#29289;&#21307;&#23398;&#33021;&#21147;&#24046;&#36317;&#65306;&#20197;&#25918;&#23556;&#23398;&#25104;&#20687;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Training Small Multimodal Models to Bridge Biomedical Competency Gap: A Case Study in Radiology Imaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#21069;&#27839;&#27169;&#22411;&#23578;&#23384;&#22312;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#24046;&#36317;&#65292;&#25506;&#35752;&#20102;&#35757;&#32451;&#24320;&#28304;&#23567;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20197;&#24357;&#34917;&#20020;&#24202;&#38656;&#27714;&#30340;&#29983;&#29289;&#21307;&#23398;&#33021;&#21147;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25918;&#22823;&#22522;&#30784;&#27169;&#22411;&#30340;&#23610;&#24230;&#35268;&#24459;&#21644;&#38750;&#20961;&#34920;&#29616;&#28608;&#21169;&#20102;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#24320;&#21457;&#21644;&#21033;&#29992;&#36825;&#20123;&#22823;&#22411;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22312;&#19968;&#20123;&#29983;&#29289;&#21307;&#23398;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26089;&#26399;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#22312;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#24212;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20043;&#21069;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#37325;&#22823;&#25361;&#25112;&#12290;&#20687;GPT-4V&#36825;&#26679;&#30340;&#21069;&#27839;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#20173;&#23384;&#22312;&#37325;&#22823;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#35775;&#38382;&#12289;&#25104;&#26412;&#12289;&#24310;&#36831;&#21644;&#21512;&#35268;&#31561;&#23454;&#38469;&#38382;&#39064;&#20351;&#20020;&#24202;&#21307;&#29983;&#38590;&#20197;&#30452;&#25509;&#22312;&#31169;&#20154;&#24739;&#32773;&#25968;&#25454;&#19978;&#20351;&#29992;&#31169;&#20154;&#25176;&#31649;&#30340;&#26368;&#20808;&#36827;&#22823;&#22411;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#35757;&#32451;&#24320;&#28304;&#23567;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;SMMs&#65289;&#26469;&#22635;&#34917;&#26410;&#28385;&#36275;&#30340;&#20020;&#24202;&#38656;&#27714;&#30340;&#29983;&#29289;&#21307;&#23398;&#33021;&#21147;&#24046;&#36317;&#12290;&#20026;&#20102;&#26368;&#22823;&#21270;&#25968;&#25454;&#25928;&#29575;&#65292;&#25105;&#20204;&#37319;&#29992;&#27169;&#22359;&#21270;&#26041;&#27861;&#65292;&#23558;&#29992;&#20110;&#22270;&#20687;&#21644;&#25991;&#26412;&#27169;&#24577;&#30340;&#26368;&#20808;&#36827;&#39044;&#35757;&#32451;&#27169;&#22411;&#32435;&#20837;&#65292;&#24182;&#20391;&#37325;&#20110;t
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08002v1 Announce Type: new  Abstract: The scaling laws and extraordinary performance of large foundation models motivate the development and utilization of such large models in biomedicine. However, despite early promising results on some biomedical benchmarks, there are still major challenges that need to be addressed before these models can be used in real-world applications. Frontier models such as GPT-4V still have major competency gaps in multimodal capabilities for biomedical applications. Moreover, pragmatic issues such as access, cost, latency, and compliance make it hard for clinicians to use privately-hosted state-of-the-art large models directly on private patient data. In this paper, we explore training open-source small multimodal models (SMMs) to bridge biomedical competency gaps for unmet clinical needs. To maximize data efficiency, we adopt a modular approach by incorporating state-of-the-art pre-trained models for image and text modalities, and focusing on t
&lt;/p&gt;</description></item><item><title>&#30830;&#35748;&#32553;&#25918;&#23450;&#24459;&#21407;&#21017;&#22312;&#27169;&#22411;&#39044;&#35757;&#32451;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#25581;&#31034;OpenAI&#21407;&#22987;&#32553;&#25918;&#23450;&#24459;&#35770;&#25991;&#30340;&#19981;&#23436;&#25972;&#32454;&#33410;&#65292;&#24182;&#25506;&#31350;&#39044;&#27979;&#27979;&#35797;&#25439;&#22833;&#36712;&#36857;&#21487;&#38752;&#20844;&#24335;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2403.06563</link><description>&lt;p&gt;
&#25581;&#24320;&#32553;&#25918;&#23450;&#24459;&#20043;&#35868;&#65306;&#31532;&#19968;&#37096;&#20998;
&lt;/p&gt;
&lt;p&gt;
Unraveling the Mystery of Scaling Laws: Part I
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06563
&lt;/p&gt;
&lt;p&gt;
&#30830;&#35748;&#32553;&#25918;&#23450;&#24459;&#21407;&#21017;&#22312;&#27169;&#22411;&#39044;&#35757;&#32451;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#25581;&#31034;OpenAI&#21407;&#22987;&#32553;&#25918;&#23450;&#24459;&#35770;&#25991;&#30340;&#19981;&#23436;&#25972;&#32454;&#33410;&#65292;&#24182;&#25506;&#31350;&#39044;&#27979;&#27979;&#35797;&#25439;&#22833;&#36712;&#36857;&#21487;&#38752;&#20844;&#24335;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32553;&#25918;&#23450;&#24459;&#21407;&#21017;&#34920;&#26126;&#22312;&#27169;&#22411;&#22823;&#23567;&#12289;&#25968;&#25454;&#38598;&#22823;&#23567;&#21644;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;&#35745;&#31639;&#36164;&#28304;&#31561;&#21464;&#37327;&#20043;&#38388;&#23384;&#22312;&#24130;&#23450;&#24459;&#30456;&#20851;&#24615;&#12290;&#36825;&#20123;&#21407;&#21017;&#22312;&#20248;&#21270;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#21508;&#20010;&#26041;&#38754;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#26368;&#32456;&#26377;&#21161;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT-4&#12289;Llama&#21644;Gemini&#65289;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;OpenAI&#30340;&#21407;&#22987;&#32553;&#25918;&#23450;&#24459;&#35770;&#25991;&#24182;&#26410;&#25259;&#38706;&#25512;&#23548;&#31934;&#30830;&#32553;&#25918;&#23450;&#24459;&#20844;&#24335;&#25152;&#24517;&#38656;&#30340;&#23436;&#25972;&#32454;&#33410;&#65292;&#20182;&#20204;&#30340;&#32467;&#35770;&#20165;&#22522;&#20110;&#21253;&#21547;&#39640;&#36798;15&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#12290;&#23613;&#31649;&#19968;&#20123;&#21518;&#32493;&#20316;&#21697;&#35797;&#22270;&#25581;&#31034;&#36825;&#20123;&#32454;&#33410;&#24182;&#25193;&#23637;&#21040;&#26356;&#22823;&#30340;&#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#24573;&#30053;&#20102;&#37325;&#35201;&#22240;&#32032;&#30340;&#35757;&#32451;&#20381;&#36182;&#24615;&#65292;&#22914;&#23398;&#20064;&#36895;&#29575;&#12289;&#19978;&#19979;&#25991;&#38271;&#24230;&#21644;&#25209;&#37327;&#22823;&#23567;&#65292;&#23548;&#33268;&#23427;&#20204;&#26410;&#33021;&#24314;&#31435;&#19968;&#20010;&#21487;&#38752;&#30340;&#39044;&#27979;&#27979;&#35797;&#25439;&#22833;&#36712;&#36857;&#30340;&#20844;&#24335;&#12290;&#22312;&#26412;&#25216;&#26415;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#30830;&#35748;&#20102;&#32553;&#25918;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06563v1 Announce Type: cross  Abstract: Scaling law principles indicate a power-law correlation between loss and variables such as model size, dataset size, and computational resources utilized during training. These principles play a vital role in optimizing various aspects of model pre-training, ultimately contributing to the success of large language models such as GPT-4, Llama and Gemini. However, the original scaling law paper by OpenAI did not disclose the complete details necessary to derive the precise scaling law formulas, and their conclusions are only based on models containing up to 1.5 billion parameters. Though some subsequent works attempt to unveil these details and scale to larger models, they often neglect the training dependency of important factors such as the learning rate, context length and batch size, leading to their failure to establish a reliable formula for predicting the test loss trajectory. In this technical report, we confirm that the scaling 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;LLMs&#36827;&#34892;&#31038;&#20132;&#20114;&#21160;&#30340;&#20840;&#30693;&#27169;&#25311;&#27604;&#38750;&#20840;&#30693;&#27169;&#25311;&#26356;&#23481;&#26131;&#23454;&#29616;&#31038;&#20132;&#30446;&#26631;&#65292;&#23613;&#31649;&#38750;&#20840;&#30693;&#27169;&#25311;&#26356;&#25509;&#36817;&#23454;&#38469;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2403.05020</link><description>&lt;p&gt;
&#27169;&#25311;&#31038;&#20132;&#20114;&#21160;&#25104;&#21151;&#24615;&#30340;&#35823;&#23548;&#24615;&#65306;&#20197;LLMs&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Is this the real life? Is this just fantasy? The Misleading Success of Simulating Social Interactions With LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05020
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;LLMs&#36827;&#34892;&#31038;&#20132;&#20114;&#21160;&#30340;&#20840;&#30693;&#27169;&#25311;&#27604;&#38750;&#20840;&#30693;&#27169;&#25311;&#26356;&#23481;&#26131;&#23454;&#29616;&#31038;&#20132;&#30446;&#26631;&#65292;&#23613;&#31649;&#38750;&#20840;&#30693;&#27169;&#25311;&#26356;&#25509;&#36817;&#23454;&#38469;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#20351;&#24471;&#31038;&#20132;&#27169;&#25311;&#26356;&#21152;&#20016;&#23500;&#65292;&#33021;&#22815;&#20351;&#29992;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#20154;&#30740;&#31350;&#21508;&#31181;&#31038;&#20132;&#29616;&#35937;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24037;&#20316;&#22312;&#36825;&#20123;&#27169;&#25311;&#20013;&#37319;&#29992;&#20102;&#19968;&#31181;&#20840;&#30693;&#30340;&#36879;&#35270;&#65288;&#20363;&#22914;&#65292;&#21333;&#20010;LLM&#29983;&#25104;&#25152;&#26377;&#20132;&#35848;&#32773;&#65289;&#65292;&#36825;&#19982;&#20154;&#31867;&#20855;&#26377;&#30340;&#38750;&#20840;&#30693;&#12289;&#20449;&#24687;&#19981;&#23545;&#31216;&#30340;&#20114;&#21160;&#26681;&#26412;&#19981;&#31526;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20123;&#24046;&#24322;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#22312;&#21508;&#31181;&#35774;&#23450;&#65288;&#20840;&#30693;&#12289;&#38750;&#20840;&#30693;&#65289;&#20013;&#20351;&#29992;LLMs&#27169;&#25311;&#31038;&#20132;&#20114;&#21160;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36890;&#36807;&#20840;&#30693;&#26041;&#24335;&#27169;&#25311;&#30340;&#20132;&#35848;&#32773;&#22312;&#23454;&#29616;&#31038;&#20132;&#30446;&#26631;&#26041;&#38754;&#27604;&#38750;&#20840;&#30693;&#20195;&#29702;&#20154;&#26356;&#25104;&#21151;&#65292;&#23613;&#31649;&#21518;&#32773;&#26356;&#31526;&#21512;&#29616;&#23454;&#35774;&#32622;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#26126;&#20174;&#20840;&#30693;&#27169;&#25311;&#20013;&#23398;&#20064;&#21487;&#20197;&#25913;&#21892;&#20132;&#20114;&#30340;&#33258;&#28982;&#24615;&#65292;&#20294;&#22312;&#21512;&#20316;&#22330;&#26223;&#20013;&#20960;&#20046;&#19981;&#33021;&#22686;&#24378;&#30446;&#26631;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05020v1 Announce Type: cross  Abstract: Recent advances in large language models (LLM) have enabled richer social simulations, allowing for the study of various social phenomena with LLM-based agents. However, most work has used an omniscient perspective on these simulations (e.g., single LLM to generate all interlocutors), which is fundamentally at odds with the non-omniscient, information asymmetric interactions that humans have. To examine these differences, we develop an evaluation framework to simulate social interactions with LLMs in various settings (omniscient, non-omniscient). Our experiments show that interlocutors simulated omnisciently are much more successful at accomplishing social goals compared to non-omniscient agents, despite the latter being the more realistic setting. Furthermore, we demonstrate that learning from omniscient simulations improves the apparent naturalness of interactions but scarcely enhances goal achievement in cooperative scenarios. Our f
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#23569;&#26679;&#26412;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26694;&#26550;&#20197;&#27169;&#25311;&#23454;&#20307;&#21644;&#19977;&#20803;&#32452;&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#23398;&#20064;&#26381;&#20174;&#39640;&#26031;&#20998;&#24067;&#30340;&#34920;&#31034;&#26469;&#26356;&#22909;&#22320;&#29702;&#35299;&#26377;&#38480;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2403.04521</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20851;&#31995;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#23569;&#26679;&#26412;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Relational Graph Neural Network for Few-Shot Knowledge Graph Completion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04521
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#23569;&#26679;&#26412;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26694;&#26550;&#20197;&#27169;&#25311;&#23454;&#20307;&#21644;&#19977;&#20803;&#32452;&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#23398;&#20064;&#26381;&#20174;&#39640;&#26031;&#20998;&#24067;&#30340;&#34920;&#31034;&#26469;&#26356;&#22909;&#22320;&#29702;&#35299;&#26377;&#38480;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26088;&#22312;&#22312;&#32473;&#23450;&#23569;&#37327;&#26696;&#20363;&#21442;&#32771;&#23454;&#20307;&#23545;&#30340;&#24773;&#20917;&#19979;&#26597;&#35810;&#26576;&#31181;&#20851;&#31995;&#30340;&#26410;&#30693;&#20107;&#23454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#23569;&#26679;&#26412;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26694;&#26550;&#65288;UFKGC&#65289;&#65292;&#20197;&#27169;&#25311;&#19981;&#30830;&#23450;&#24615;&#65292;&#26356;&#22909;&#22320;&#29702;&#35299;&#26377;&#38480;&#25968;&#25454;&#65292;&#36890;&#36807;&#23398;&#20064;&#26381;&#20174;&#39640;&#26031;&#20998;&#24067;&#30340;&#34920;&#31034;&#26469;&#36991;&#20813;&#30001;&#23454;&#20307;&#21644;&#19977;&#20803;&#32452;&#19981;&#30830;&#23450;&#24615;&#23548;&#33268;&#30340;&#22122;&#22768;&#21103;&#20316;&#29992;&#65292;&#20808;&#20026;&#28304;&#23454;&#20307;&#23545;&#30340;&#19981;&#30830;&#23450;&#24615;&#33539;&#22260;&#35774;&#35745;&#19981;&#30830;&#23450;&#24615;&#34920;&#31034;&#65292;&#28982;&#21518;&#23558;&#29305;&#24449;&#34920;&#31034;&#36716;&#25442;&#20026;&#39640;&#26031;&#20998;&#24067;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#25972;&#21512;&#20855;&#26377;&#19981;&#30830;&#23450;&#29305;&#24449;&#30340;&#37051;&#23621;&#20197;&#29992;&#20110;&#23454;&#20307;&#29305;&#24449;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#20851;&#31995;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;UR-GNN&#65289;&#36827;&#34892;&#21367;&#31215;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04521v1 Announce Type: new  Abstract: Few-shot knowledge graph completion (FKGC) aims to query the unseen facts of a relation given its few-shot reference entity pairs. The side effect of noises due to the uncertainty of entities and triples may limit the few-shot learning, but existing FKGC works neglect such uncertainty, which leads them more susceptible to limited reference samples with noises. In this paper, we propose a novel uncertainty-aware few-shot KG completion framework (UFKGC) to model uncertainty for a better understanding of the limited data by learning representations under Gaussian distribution. Uncertainty representation is first designed for estimating the uncertainty scope of the entity pairs after transferring feature representations into a Gaussian distribution. Further, to better integrate the neighbors with uncertainty characteristics for entity features, we design an uncertainty-aware relational graph neural network (UR-GNN) to conduct convolution ope
&lt;/p&gt;</description></item><item><title>NewsBench&#26159;&#19968;&#20010;&#35780;&#20272;LLMs&#22312;&#20013;&#22269;&#26032;&#38395;&#20889;&#20316;&#27700;&#24179;&#21644;&#23433;&#20840;&#24615;&#36981;&#20174;&#33021;&#21147;&#30340;&#22522;&#20934;&#26694;&#26550;&#65292;&#25581;&#31034;&#20102;&#22312;&#21019;&#36896;&#24615;&#20889;&#20316;&#20219;&#21153;&#20013;LLMs&#30456;&#23545;&#19981;&#36275;&#30340;&#26032;&#38395;&#20262;&#29702;&#36981;&#23432;&#26041;&#38754;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2403.00862</link><description>&lt;p&gt;
NewsBench&#65306;&#31995;&#32479;&#24615;&#35780;&#20272;LLM&#22312;&#20013;&#22269;&#26032;&#38395;&#32534;&#36753;&#24212;&#29992;&#20013;&#30340;&#20889;&#20316;&#27700;&#24179;&#21644;&#23433;&#20840;&#24615;&#36981;&#20174;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
NewsBench: Systematic Evaluation of LLMs for Writing Proficiency and Safety Adherence in Chinese Journalistic Editorial Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00862
&lt;/p&gt;
&lt;p&gt;
NewsBench&#26159;&#19968;&#20010;&#35780;&#20272;LLMs&#22312;&#20013;&#22269;&#26032;&#38395;&#20889;&#20316;&#27700;&#24179;&#21644;&#23433;&#20840;&#24615;&#36981;&#20174;&#33021;&#21147;&#30340;&#22522;&#20934;&#26694;&#26550;&#65292;&#25581;&#31034;&#20102;&#22312;&#21019;&#36896;&#24615;&#20889;&#20316;&#20219;&#21153;&#20013;LLMs&#30456;&#23545;&#19981;&#36275;&#30340;&#26032;&#38395;&#20262;&#29702;&#36981;&#23432;&#26041;&#38754;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;NewsBench&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#26694;&#26550;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20013;&#22269;&#26032;&#38395;&#20889;&#20316;&#27700;&#24179;&#65288;JWP&#65289;&#21644;&#23433;&#20840;&#24615;&#36981;&#20174;&#65288;SA&#65289;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24357;&#34917;&#20102;&#26032;&#38395;&#20262;&#29702;&#19982;&#20154;&#24037;&#26234;&#33021;&#21033;&#29992;&#39118;&#38505;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;NewsBench&#21253;&#25324;5&#20010;&#32534;&#36753;&#24212;&#29992;&#20013;&#30340;1,267&#39033;&#20219;&#21153;&#65292;7&#20010;&#26041;&#38754;&#65288;&#21253;&#25324;&#23433;&#20840;&#24615;&#21644;&#26032;&#38395;&#20889;&#20316;&#65292;&#20197;&#21450;4&#20010;&#35814;&#32454;&#35201;&#38754;&#65289;&#65292;&#28085;&#30422;24&#20010;&#26032;&#38395;&#20027;&#39064;&#39046;&#22495;&#65292;&#37319;&#29992;&#22522;&#20110;&#20004;&#31181;GPT-4&#30340;&#33258;&#21160;&#35780;&#20272;&#21327;&#35758;&#65292;&#24182;&#32463;&#36807;&#20154;&#31867;&#35780;&#20272;&#39564;&#35777;&#12290;&#25105;&#20204;&#23545;11&#20010;LLM&#30340;&#20840;&#38754;&#20998;&#26512;&#31361;&#20986;&#20102;GPT-4&#21644;ERNIE Bot&#20316;&#20026;&#34920;&#29616;&#26368;&#20339;&#65292;&#20294;&#22312;&#21019;&#36896;&#24615;&#20889;&#20316;&#20219;&#21153;&#20013;&#25581;&#31034;&#20102;&#26032;&#38395;&#20262;&#29702;&#36981;&#23432;&#26041;&#38754;&#30340;&#30456;&#23545;&#19981;&#36275;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;AI&#29983;&#25104;&#30340;&#26032;&#38395;&#20869;&#23481;&#38656;&#35201;&#25552;&#39640;&#20262;&#29702;&#25351;&#23548;&#65292;&#26631;&#24535;&#30528;&#20197;&#26032;&#38395;&#26631;&#20934;&#21644;&#23433;&#20840;&#24615;&#23545;&#40784;AI&#33021;&#21147;&#36808;&#20986;&#20102;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00862v1 Announce Type: cross  Abstract: This study presents NewsBench, a novel benchmark framework developed to evaluate the capability of Large Language Models (LLMs) in Chinese Journalistic Writing Proficiency (JWP) and their Safety Adherence (SA), addressing the gap between journalistic ethics and the risks associated with AI utilization. Comprising 1,267 tasks across 5 editorial applications, 7 aspects (including safety and journalistic writing with 4 detailed facets), and spanning 24 news topics domains, NewsBench employs two GPT-4 based automatic evaluation protocols validated by human assessment. Our comprehensive analysis of 11 LLMs highlighted GPT-4 and ERNIE Bot as top performers, yet revealed a relative deficiency in journalistic ethic adherence during creative writing tasks. These findings underscore the need for enhanced ethical guidance in AI-generated journalistic content, marking a step forward in aligning AI capabilities with journalistic standards and safet
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;OSCaR&#65292;&#26088;&#22312;&#35299;&#20915;&#25551;&#36848;&#22797;&#26434;&#35270;&#35273;&#29615;&#22659;&#20013;&#23545;&#35937;&#29366;&#24577;&#21464;&#21270;&#30340;&#38382;&#39064;&#65292;&#20026;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#39564;&#24179;&#21488;&#12290;</title><link>https://arxiv.org/abs/2402.17128</link><description>&lt;p&gt;
OSCaR:&#23545;&#35937;&#29366;&#24577;&#23383;&#24149;&#21644;&#29366;&#24577;&#21464;&#21270;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
OSCaR: Object State Captioning and State Change Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17128
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;OSCaR&#65292;&#26088;&#22312;&#35299;&#20915;&#25551;&#36848;&#22797;&#26434;&#35270;&#35273;&#29615;&#22659;&#20013;&#23545;&#35937;&#29366;&#24577;&#21464;&#21270;&#30340;&#38382;&#39064;&#65292;&#20026;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#39564;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17128v3 &#20844;&#21578;&#31867;&#22411;: &#36328; &#38754;&#21521;&#20154;&#31867;&#22312;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#30340;&#20132;&#20114;&#35270;&#35282;&#65292;&#26234;&#33021;&#27169;&#22411;&#25512;&#26029;&#21644;&#29702;&#35299;&#23545;&#35937;&#29366;&#24577;&#30340;&#21464;&#21270;&#33021;&#21147;&#26159;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#19968;&#20010;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26041;&#38754;&#12290;&#35813;&#20219;&#21153;&#28041;&#21450;&#25551;&#36848;&#22797;&#26434;&#30340;&#35270;&#35273;&#29615;&#22659;&#65292;&#35782;&#21035;&#27963;&#36291;&#23545;&#35937;&#65292;&#20197;&#21450;&#36890;&#36807;&#35821;&#35328;&#35299;&#37322;&#23427;&#20204;&#30340;&#21464;&#21270;&#12290;&#20256;&#32479;&#26041;&#27861;&#23558;&#23545;&#35937;&#23383;&#24149;&#21644;&#29366;&#24577;&#21464;&#21270;&#26816;&#27979;&#36827;&#34892;&#38548;&#31163;&#65292;&#25552;&#20379;&#20102;&#23545;&#21160;&#24577;&#29615;&#22659;&#30340;&#26377;&#38480;&#35270;&#22270;&#12290;&#27492;&#22806;&#65292;&#20381;&#36182;&#20110;&#19968;&#23567;&#22871;&#31526;&#21495;&#21270;&#35789;&#27719;&#26469;&#34920;&#31034;&#21464;&#21270;&#38480;&#21046;&#20102;&#35821;&#35328;&#30340;&#34920;&#36798;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#23545;&#35937;&#29366;&#24577;&#23383;&#24149;&#21644;&#29366;&#24577;&#21464;&#21270;&#34920;&#31034;&#65288;OSCaR&#65289;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#12290;OSCaR&#21253;&#25324;&#26469;&#33258;&#21508;&#31181;&#20027;&#35266;&#35270;&#35282;&#35270;&#39057;&#38598;&#21512;&#30340;14,084&#20010;&#24102;&#27880;&#37322;&#35270;&#39057;&#29255;&#27573;&#65292;&#28085;&#30422;&#36817;1,000&#20010;&#29420;&#29305;&#23545;&#35937;&#12290;&#23427;&#20026;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#39564;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17128v3 Announce Type: cross  Abstract: The capability of intelligent models to extrapolate and comprehend changes in object states is a crucial yet demanding aspect of AI research, particularly through the lens of human interaction in real-world settings. This task involves describing complex visual environments, identifying active objects, and interpreting their changes as conveyed through language. Traditional methods, which isolate object captioning and state change detection, offer a limited view of dynamic environments. Moreover, relying on a small set of symbolic words to represent changes has restricted the expressiveness of the language. To address these challenges, in this paper, we introduce the Object State Captioning and State Change Representation (OSCaR) dataset and benchmark. OSCaR consists of 14,084 annotated video segments with nearly 1,000 unique objects from various egocentric video collections. It sets a new testbed for evaluating multimodal large langua
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36923;&#36753;&#25512;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;ChatGPT&#22312;&#22768;&#26126;&#39564;&#35777;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#21457;&#29616;&#20854;&#22312;&#24402;&#32435;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32531;&#35299;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10735</link><description>&lt;p&gt;
&#22312;&#22768;&#26126;&#39564;&#35777;&#30340;&#32972;&#26223;&#19979;&#35780;&#20272;ChatGPT&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Assessing the Reasoning Abilities of ChatGPT in the Context of Claim Verification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10735
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36923;&#36753;&#25512;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;ChatGPT&#22312;&#22768;&#26126;&#39564;&#35777;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#21457;&#29616;&#20854;&#22312;&#24402;&#32435;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32531;&#35299;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#26377;&#20851;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#30340;&#36777;&#35770;&#27491;&#22312;&#26085;&#30410;&#28608;&#28872;&#12290;&#25105;&#20204;&#20174;&#22768;&#26126;/&#35875;&#35328;&#39564;&#35777;&#30340;&#35282;&#24230;&#26469;&#23457;&#35270;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36923;&#36753;&#25512;&#29702;&#26694;&#26550;&#65292;&#26088;&#22312;&#23558;&#20219;&#20309;&#22768;&#26126;&#25110;&#20256;&#35328;&#19982;&#35777;&#25454;&#32467;&#21512;&#65292;&#25286;&#20998;&#25104;&#39564;&#35777;&#25152;&#38656;&#30340;&#22522;&#26412;&#25512;&#29702;&#27493;&#39588;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#25972;&#29702;&#20102;&#20004;&#20010;&#27880;&#37322;&#38598;&#21512;&#65292;&#20854;&#20013;&#21253;&#25324;&#26469;&#33258;&#32500;&#22522;&#30334;&#31185;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#28304;&#33258;Twitter&#19978;&#27969;&#20256;&#30340;&#35875;&#35328;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;&#23427;&#20204;&#26469;&#35780;&#20272;GPT-3.5-Turbo&#21644;GPT-4&#65288;&#20197;&#19979;&#31616;&#31216;&#20026;ChatGPT&#65289;&#22312;&#25105;&#20204;&#26694;&#26550;&#30340;&#32972;&#26223;&#19979;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#24443;&#24213;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;ChatGPT&#22312;&#24402;&#32435;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#23613;&#31649;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#25163;&#21160;&#30340;&#24605;&#32500;&#38142;&#36335;&#65288;Chain of Thought&#65292;CoT&#65289;&#26469;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#32780;&#38750;&#38646;&#32534;&#30721;&#65288;Zero Shot&#65292;ZS&#65289;&#21644;ZS CoT&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26377;&#21161;&#20110;&#19981;&#26029;&#22686;&#38271;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#34920;&#26126;Cha
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10735v1 Announce Type: new  Abstract: The reasoning capabilities of LLMs are currently hotly debated. We examine the issue from the perspective of claim/rumour verification. We propose the first logical reasoning framework designed to break down any claim or rumor paired with evidence into the atomic reasoning steps necessary for verification. Based on our framework, we curate two annotated collections of such claim/evidence pairs: a synthetic dataset from Wikipedia and a real-world set stemming from rumours circulating on Twitter. We use them to evaluate the reasoning capabilities of GPT-3.5-Turbo and GPT-4 (hereinafter referred to as ChatGPT) within the context of our framework, providing a thorough analysis. Our results show that ChatGPT struggles in abductive reasoning, although this can be somewhat mitigated by using manual Chain of Thought (CoT) as opposed to Zero Shot (ZS) and ZS CoT approaches. Our study contributes to the growing body of research suggesting that Cha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#19977;&#20010;&#25104;&#29087;&#30340;&#20154;&#31867;&#20915;&#31574;&#29702;&#35770;&#25972;&#21512;&#21040;&#19968;&#36215;&#65292;&#24418;&#25104;&#20102;&#19968;&#20010;&#30446;&#30340;&#24615;&#20154;&#31867;&#34892;&#21160;&#27169;&#22411;&#12290;&#21516;&#26102;&#65292;&#23558;&#35821;&#35328;&#20316;&#20026;&#34892;&#21160;&#30340;&#35266;&#28857;&#24212;&#29992;&#20110;&#23545;&#35805;&#29992;&#25143;&#30028;&#38754;&#12290;&#36890;&#36807;&#29702;&#35299;ChatGPT&#30340;&#26234;&#33021;&#26469;&#28304;&#65292;&#21487;&#20197;&#22312;&#20943;&#23569;&#36164;&#28304;&#30340;&#21516;&#26102;&#33719;&#24471;&#23545;&#25105;&#20204;&#20043;&#38388;&#20851;&#31995;&#30340;&#35748;&#35782;&#12290;</title><link>https://arxiv.org/abs/2402.08403</link><description>&lt;p&gt;
LLMs&#21644;&#20154;&#31867;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
LLMs and the Human Condition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#19977;&#20010;&#25104;&#29087;&#30340;&#20154;&#31867;&#20915;&#31574;&#29702;&#35770;&#25972;&#21512;&#21040;&#19968;&#36215;&#65292;&#24418;&#25104;&#20102;&#19968;&#20010;&#30446;&#30340;&#24615;&#20154;&#31867;&#34892;&#21160;&#27169;&#22411;&#12290;&#21516;&#26102;&#65292;&#23558;&#35821;&#35328;&#20316;&#20026;&#34892;&#21160;&#30340;&#35266;&#28857;&#24212;&#29992;&#20110;&#23545;&#35805;&#29992;&#25143;&#30028;&#38754;&#12290;&#36890;&#36807;&#29702;&#35299;ChatGPT&#30340;&#26234;&#33021;&#26469;&#28304;&#65292;&#21487;&#20197;&#22312;&#20943;&#23569;&#36164;&#28304;&#30340;&#21516;&#26102;&#33719;&#24471;&#23545;&#25105;&#20204;&#20043;&#38388;&#20851;&#31995;&#30340;&#35748;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20154;&#31867;&#20915;&#31574;&#30340;&#19977;&#20010;&#25104;&#29087;&#29702;&#35770;&#65292;&#24182;&#25551;&#36848;&#20102;&#22914;&#20309;&#23558;&#23427;&#20204;&#25972;&#21512;&#36215;&#26469;&#25552;&#20379;&#19968;&#20010;&#30446;&#30340;&#24615;&#20154;&#31867;&#34892;&#21160;&#30340;&#27169;&#22411;&#12290;&#21516;&#26102;&#65292;&#23558;&#35821;&#35328;&#20316;&#20026;&#34892;&#21160;&#30340;&#35266;&#28857;&#24212;&#29992;&#20110;&#23545;&#35805;&#29992;&#25143;&#30028;&#38754;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#29702;&#35770;&#30340;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#36935;&#21040;&#20102;&#22256;&#38590;&#65292;&#26412;&#25991;&#26088;&#22312;&#37325;&#26032;&#28608;&#21457;&#23545;&#29702;&#35299;LLMs&#23454;&#38469;&#25191;&#34892;&#30340;&#20852;&#36259;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#22312;&#25152;&#26377;&#25968;&#25454;&#19978;&#36816;&#34892;&#38590;&#20197;&#29702;&#35299;&#30340;&#26426;&#22120;&#23398;&#20064;&#20363;&#31243;&#12290;&#24403;&#19968;&#21488;&#21806;&#20215;&#19981;&#21040;50&#32654;&#20803;&#30340;&#26641;&#33683;&#27966;&#30005;&#33041;&#27604;&#31532;&#19968;&#21488;&#21830;&#19994;Cray&#36229;&#32423;&#35745;&#31639;&#26426;&#24555;400&#20493;&#26102;&#65292;&#22823;&#22411;&#31185;&#25216;&#20844;&#21496;&#21487;&#20197;&#25509;&#36817;&#25317;&#26377;&#26080;&#25968;&#38543;&#26426;&#25171;&#23383;&#24182;&#29983;&#25104;&#26377;&#24847;&#20041;&#25991;&#23383;&#30340;&#29492;&#23376;&#12290;&#36890;&#36807;&#29702;&#35299;ChatGPT&#30340;&#34920;&#29616;&#26234;&#33021;&#30340;&#26469;&#28304;&#65292;&#20063;&#35768;&#25105;&#20204;&#21487;&#20197;&#29992;&#26356;&#23569;&#30340;&#36164;&#28304;&#36827;&#34892;&#21516;&#26679;&#30340;&#39764;&#26415;&#65292;&#24182;&#22312;&#27492;&#36807;&#31243;&#20013;&#33719;&#24471;&#19968;&#20123;&#20851;&#20110;&#25105;&#20204;&#20043;&#38388;&#20851;&#31995;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents three established theories of human decision-making and describes how they can be integrated to provide a model of purposive human action. Taking seriously the idea of language as action the model is then applied to the conversational user interfaces. Theory based AI research has had a hard time recently and the aim here is to revitalise interest in understanding what LLMs are actually doing other than running poorly understood machine learning routines over all the data the relevant Big Tech company can hoover up. When a raspberry pi computer for under 50USD is up to 400 times faster than the first commercial Cray super computer~\cite{crayVpi}, Big Tech can get really close to having an infinite number of monkeys typing at random and producing text, some of which will make sense. By understanding where ChatGPT's apparent intelligence comes from, perhaps we can perform the magic with fewer resources and at the same time gain some understanding about our relationship
&lt;/p&gt;</description></item><item><title>ANLS*&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#22411;&#27169;&#22411;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;&#65292;&#38024;&#23545;&#21508;&#31181;&#20219;&#21153;&#21253;&#25324;&#20449;&#24687;&#25552;&#21462;&#21644;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;&#23427;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;ANLS&#24230;&#37327;&#26041;&#27861;&#65292;&#21487;&#20197;&#20316;&#20026;&#26367;&#20195;&#26041;&#26696;&#20351;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.03848</link><description>&lt;p&gt;
ANLS* -- &#19968;&#31181;&#36866;&#29992;&#20110;&#29983;&#25104;&#22411;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#25991;&#26723;&#22788;&#29702;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ANLS* -- A Universal Document Processing Metric for Generative Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03848
&lt;/p&gt;
&lt;p&gt;
ANLS*&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#22411;&#27169;&#22411;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;&#65292;&#38024;&#23545;&#21508;&#31181;&#20219;&#21153;&#21253;&#25324;&#20449;&#24687;&#25552;&#21462;&#21644;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;&#23427;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;ANLS&#24230;&#37327;&#26041;&#27861;&#65292;&#21487;&#20197;&#20316;&#20026;&#26367;&#20195;&#26041;&#26696;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#22312;&#25991;&#26723;&#20998;&#31867;&#21644;&#20449;&#24687;&#25552;&#21462;&#31561;&#20219;&#21153;&#20013;&#65292;&#21306;&#20998;&#27169;&#22411;&#19968;&#30452;&#26159;&#20027;&#35201;&#36873;&#25321;&#12290;&#36825;&#20123;&#27169;&#22411;&#20570;&#20986;&#30340;&#39044;&#27979;&#21487;&#20197;&#20998;&#20026;&#26377;&#38480;&#25968;&#37327;&#30340;&#39044;&#23450;&#20041;&#31867;&#21035;&#65292;&#20415;&#20110;&#36827;&#34892;&#20108;&#20803;&#30495;&#20551;&#35780;&#20272;&#65292;&#24182;&#33021;&#30452;&#25509;&#35745;&#31639;F1&#20998;&#25968;&#31561;&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#22411;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;GLLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#20419;&#20351;&#39046;&#22495;&#21457;&#29983;&#20102;&#36716;&#21464;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#22791;&#20102;&#24378;&#22823;&#30340;&#38646;-shot&#33021;&#21147;&#65292;&#28040;&#38500;&#20102;&#19979;&#28216;&#25968;&#25454;&#38598;&#21644;&#35745;&#31639;&#26114;&#36149;&#30340;&#24494;&#35843;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;GLLMs&#23384;&#22312;&#25361;&#25112;&#65292;&#22240;&#20026;&#23545;&#20110;GLLMs&#30340;&#39044;&#27979;&#65292;&#19981;&#33021;&#24212;&#29992;&#20110;&#21306;&#20998;&#27169;&#22411;&#25152;&#20351;&#29992;&#30340;&#20108;&#20803;&#30495;&#20551;&#35780;&#20272;&#26041;&#27861;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#22411;&#27169;&#22411;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;ANLS*&#65292;&#29992;&#20110;&#35780;&#20272;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#20449;&#24687;&#25552;&#21462;&#21644;&#20998;&#31867;&#20219;&#21153;&#12290;ANLS*&#24230;&#37327;&#26041;&#27861;&#25193;&#23637;&#20102;&#29616;&#26377;ANLS&#24230;&#37327;&#26041;&#27861;&#65292;&#21487;&#20316;&#20026;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditionally, discriminative models have been the predominant choice for tasks like document classification and information extraction. These models make predictions that fall into a limited number of predefined classes, facilitating a binary true or false evaluation and enabling the direct calculation of metrics such as the F1 score. However, recent advancements in generative large language models (GLLMs) have prompted a shift in the field due to their enhanced zero-shot capabilities, which eliminate the need for a downstream dataset and computationally expensive fine-tuning. However, evaluating GLLMs presents a challenge as the binary true or false evaluation used for discriminative models is not applicable to the predictions made by GLLMs. This paper introduces a new metric for generative models called ANLS* for evaluating a wide variety of tasks, including information extraction and classification tasks. The ANLS* metric extends existing ANLS metrics as a drop-in-replacement and i
&lt;/p&gt;</description></item><item><title>EasyInstruct&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#22788;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#22359;&#21270;&#25351;&#20196;&#29983;&#25104;&#12289;&#36873;&#25321;&#21644;&#25552;&#31034;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#30340;&#32452;&#21512;&#21644;&#20132;&#20114;&#65292;&#20351;&#25351;&#20196;&#22788;&#29702;&#26356;&#21152;&#26041;&#20415;&#21644;&#39640;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.03049</link><description>&lt;p&gt;
EasyInstruct&#65306;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#22788;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
EasyInstruct: An Easy-to-use Instruction Processing Framework for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03049
&lt;/p&gt;
&lt;p&gt;
EasyInstruct&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#22788;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#22359;&#21270;&#25351;&#20196;&#29983;&#25104;&#12289;&#36873;&#25321;&#21644;&#25552;&#31034;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#30340;&#32452;&#21512;&#21644;&#20132;&#20114;&#65292;&#20351;&#25351;&#20196;&#22788;&#29702;&#26356;&#21152;&#26041;&#20415;&#21644;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25351;&#20196;&#35843;&#25972;&#24050;&#32463;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#24182;&#25104;&#20026;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#21147;&#30340;&#19968;&#31181;&#20851;&#38190;&#25216;&#26415;&#12290;&#20026;&#20102;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#25351;&#20196;&#22788;&#29702;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#25968;&#25454;&#25968;&#37327;&#21644;&#25968;&#25454;&#36136;&#37327;&#20043;&#38388;&#36798;&#21040;&#31934;&#24039;&#30340;&#24179;&#34913;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21508;&#31181;&#25351;&#20196;&#22788;&#29702;&#26041;&#27861;&#20043;&#38388;&#20173;&#28982;&#23384;&#22312;&#19981;&#19968;&#33268;&#65292;&#30446;&#21069;&#27809;&#26377;&#26631;&#20934;&#30340;&#24320;&#28304;&#25351;&#20196;&#22788;&#29702;&#23454;&#29616;&#26694;&#26550;&#21487;&#20379;&#31038;&#21306;&#20351;&#29992;&#65292;&#36825;&#20351;&#24471;&#20174;&#19994;&#32773;&#26080;&#27861;&#36827;&#19968;&#27493;&#24320;&#21457;&#21644;&#25512;&#36827;&#12290;&#20026;&#20102;&#20419;&#36827;&#25351;&#20196;&#22788;&#29702;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EasyInstruct&#65292;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#20110;LLMs&#30340;&#25351;&#20196;&#22788;&#29702;&#26694;&#26550;&#65292;&#23427;&#23558;&#25351;&#20196;&#29983;&#25104;&#12289;&#36873;&#25321;&#21644;&#25552;&#31034;&#27169;&#22359;&#21270;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#30340;&#32452;&#21512;&#21644;&#20132;&#20114;&#12290;EasyInstruct&#24050;&#32463;&#22312;https://github.com/zjunlp/EasyInstruct&#19978;&#20844;&#24320;&#21457;&#24067;&#65292;&#24182;&#24471;&#21040;&#20102;&#31215;&#26497;&#32500;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, instruction tuning has gained increasing attention and emerged as a crucial technique to enhance the capabilities of Large Language Models (LLMs). To construct high-quality instruction datasets, many instruction processing approaches have been proposed, aiming to achieve a delicate balance between data quantity and data quality. Nevertheless, due to inconsistencies that persist among various instruction processing methods, there is no standard open-source instruction processing implementation framework available for the community, which hinders practitioners from further developing and advancing. To facilitate instruction processing research and development, we present EasyInstruct, an easy-to-use instruction processing framework for LLMs, which modularizes instruction generation, selection, and prompting, while also considering their combination and interaction. EasyInstruct is publicly released and actively maintained at https://github.com/zjunlp/EasyInstruct, along 
&lt;/p&gt;</description></item><item><title>DenseFormer&#26159;&#23545;Transformer&#30340;&#31616;&#21333;&#20462;&#25913;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;transformer&#22359;&#20043;&#21518;&#36827;&#34892;&#28145;&#24230;&#21152;&#26435;&#24179;&#22343;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#22256;&#24785;&#24230;&#12290;&#23398;&#21040;&#30340;&#21152;&#26435;&#24179;&#22343;&#26435;&#37325;&#25581;&#31034;&#20102;&#20449;&#24687;&#27969;&#30340;&#36830;&#36143;&#27169;&#24335;&#65292;&#20351;&#24471;DenseFormer&#20855;&#26377;&#26356;&#39640;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#19988;&#22312;&#30456;&#21516;&#22256;&#24785;&#24230;&#19979;&#32988;&#36807;&#20256;&#32479;&#30340;Transformer&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.02622</link><description>&lt;p&gt;
DenseFormer: &#36890;&#36807;&#28145;&#24230;&#21152;&#26435;&#24179;&#22343;&#22686;&#24378;Transformer&#20013;&#30340;&#20449;&#24687;&#27969;
&lt;/p&gt;
&lt;p&gt;
DenseFormer: Enhancing Information Flow in Transformers via Depth Weighted Averaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02622
&lt;/p&gt;
&lt;p&gt;
DenseFormer&#26159;&#23545;Transformer&#30340;&#31616;&#21333;&#20462;&#25913;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;transformer&#22359;&#20043;&#21518;&#36827;&#34892;&#28145;&#24230;&#21152;&#26435;&#24179;&#22343;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#22256;&#24785;&#24230;&#12290;&#23398;&#21040;&#30340;&#21152;&#26435;&#24179;&#22343;&#26435;&#37325;&#25581;&#31034;&#20102;&#20449;&#24687;&#27969;&#30340;&#36830;&#36143;&#27169;&#24335;&#65292;&#20351;&#24471;DenseFormer&#20855;&#26377;&#26356;&#39640;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#19988;&#22312;&#30456;&#21516;&#22256;&#24785;&#24230;&#19979;&#32988;&#36807;&#20256;&#32479;&#30340;Transformer&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;Vaswani&#31561;&#20154;&#65288;2017&#65289;&#30340;Transformer&#26550;&#26500;&#29616;&#24050;&#26222;&#36941;&#24212;&#29992;&#20110;&#21508;&#20010;&#24212;&#29992;&#39046;&#22495;&#65292;&#20174;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21040;&#35821;&#38899;&#22788;&#29702;&#21644;&#22270;&#20687;&#29702;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DenseFormer&#65292;&#36825;&#26159;&#23545;&#26631;&#20934;&#26550;&#26500;&#30340;&#31616;&#21333;&#20462;&#25913;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#22256;&#24785;&#24230;&#65292;&#32780;&#19981;&#22686;&#21152;&#20854;&#22823;&#23567;-&#23545;&#20110;&#25317;&#26377;100B&#21442;&#25968;&#33539;&#22260;&#30340;&#22823;&#35268;&#27169;&#27169;&#22411;&#65292;&#21482;&#38656;&#28155;&#21152;&#20960;&#21315;&#20010;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27599;&#20010;transformer&#22359;&#20043;&#21518;&#20381;&#38752;&#39069;&#22806;&#30340;&#24179;&#22343;&#27493;&#39588;&#65292;&#35745;&#31639;&#24403;&#21069;&#21644;&#36807;&#21435;&#34920;&#31034;&#30340;&#21152;&#26435;&#24179;&#22343;-&#25105;&#20204;&#23558;&#36825;&#20010;&#25805;&#20316;&#31216;&#20026;&#28145;&#24230;&#21152;&#26435;&#24179;&#22343;&#65288;DWA&#65289;&#12290;&#23398;&#21040;&#30340;DWA&#26435;&#37325;&#23637;&#29616;&#20102;&#20449;&#24687;&#27969;&#30340;&#36830;&#36143;&#27169;&#24335;&#65292;&#25581;&#31034;&#20102;&#26469;&#33258;&#36828;&#23618;&#30340;&#28608;&#27963;&#30340;&#24378;&#22823;&#19988;&#32467;&#26500;&#21270;&#30340;&#37325;&#22797;&#20351;&#29992;&#12290;&#23454;&#39564;&#35777;&#26126;DenseFormer&#20855;&#26377;&#26356;&#39640;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#33021;&#22815;&#36798;&#21040;&#27604;&#26356;&#28145;&#30340;transformer&#27169;&#22411;&#30456;&#21516;&#30340;&#22256;&#24785;&#24230;&#65292;&#24182;&#19988;&#22312;&#30456;&#21516;&#22256;&#24785;&#24230;&#19979;&#65292;&#36825;&#20123;&#26032;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;transformer&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The transformer architecture from Vaswani et al. (2017) is now ubiquitous across application domains, from natural language processing to speech processing and image understanding. We propose DenseFormer, a simple modification to the standard architecture that improves the perplexity of the model without increasing its size -- adding a few thousand parameters for large-scale models in the 100B parameters range. Our approach relies on an additional averaging step after each transformer block, which computes a weighted average of current and past representations -- we refer to this operation as Depth-Weighted-Average (DWA). The learned DWA weights exhibit coherent patterns of information flow, revealing the strong and structured reuse of activations from distant layers. Experiments demonstrate that DenseFormer is more data efficient, reaching the same perplexity of much deeper transformer models, and that for the same perplexity, these new models outperform transformer baselines in terms
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#21387;&#32553;&#29616;&#26377;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#25913;&#36827;&#26435;&#37325;&#26356;&#26032;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#20197;&#26356;&#26377;&#25928;&#22320;&#25429;&#25417;&#26356;&#22810;&#26435;&#37325;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2312.17244</link><description>&lt;p&gt;
LLM&#22806;&#31185;&#21307;&#29983;
&lt;/p&gt;
&lt;p&gt;
The LLM Surgeon
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.17244
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#21387;&#32553;&#29616;&#26377;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#25913;&#36827;&#26435;&#37325;&#26356;&#26032;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#20197;&#26356;&#26377;&#25928;&#22320;&#25429;&#25417;&#26356;&#22810;&#26435;&#37325;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#36234;&#26469;&#36234;&#24222;&#22823;&#65292;&#20197;&#26399;&#22312;&#22823;&#37327;&#21487;&#29992;&#30340;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;Transformer&#26550;&#26500;&#30340;&#24040;&#22823;&#35268;&#27169;&#20351;&#24471;&#22312;&#35745;&#31639;&#12289;&#29615;&#22659;&#25110;&#35774;&#22791;&#29305;&#23450;&#32422;&#26463;&#19979;&#37096;&#32626;&#27169;&#22411;&#21464;&#24471;&#22256;&#38590;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#23545;&#29616;&#26377;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#39537;&#21160;&#21387;&#32553;&#20316;&#20026;&#35757;&#32451;&#36739;&#23567;&#27169;&#22411;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#30446;&#26631;&#25439;&#22833;&#26223;&#35266;&#30340;Kronecker&#20998;&#35299;&#26354;&#29575;&#36817;&#20284;&#25193;&#23637;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#26082;&#21487;&#20197;&#35745;&#31639;&#21487;&#21024;&#38500;&#32467;&#26500;&#30340;&#21160;&#24577;&#20998;&#37197;&#65292;&#20063;&#21487;&#20197;&#26356;&#26032;&#21097;&#20313;&#26435;&#37325;&#20197;&#32771;&#34385;&#21024;&#38500;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#38750;&#32467;&#26500;&#21270;&#12289;&#21322;&#32467;&#26500;&#21270;&#21644;&#32467;&#26500;&#21270;&#20462;&#21098;&#65292;&#24182;&#25913;&#36827;&#20102;&#26435;&#37325;&#26356;&#26032;&#20197;&#25429;&#25417;&#26356;&#22810;&#26435;&#37325;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.17244v2 Announce Type: replace-cross  Abstract: State-of-the-art language models are becoming increasingly large in an effort to achieve the highest performance on large corpora of available textual data. However, the sheer size of the Transformer architectures makes it difficult to deploy models within computational, environmental or device-specific constraints. We explore data-driven compression of existing pretrained models as an alternative to training smaller models from scratch. To do so, we scale Kronecker-factored curvature approximations of the target loss landscape to large language models. In doing so, we can compute both the dynamic allocation of structures that can be removed as well as updates of remaining weights that account for the removal. We provide a general framework for unstructured, semi-structured and structured pruning and improve upon weight updates to capture more correlations between weights, while remaining computationally efficient. Experimental
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#29983;&#25104;&#35299;&#37322;&#20197;&#29702;&#35299;&#21644;&#20462;&#22797;&#22522;&#20110;&#23884;&#20837;&#30340;&#23454;&#20307;&#23545;&#40784;&#32467;&#26524;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#26500;&#24314;&#21305;&#37197;&#23376;&#22270;&#21644;&#23545;&#40784;&#20381;&#36182;&#22270;&#65292;&#35299;&#20915;&#23545;&#40784;&#20914;&#31361;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#26377;&#25928;&#24615;&#21644;&#27867;&#21270;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.04877</link><description>&lt;p&gt;
&#29983;&#25104;&#35299;&#37322;&#20197;&#29702;&#35299;&#21644;&#20462;&#22797;&#22522;&#20110;&#23884;&#20837;&#30340;&#23454;&#20307;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Generating Explanations to Understand and Repair Embedding-based Entity Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04877
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#29983;&#25104;&#35299;&#37322;&#20197;&#29702;&#35299;&#21644;&#20462;&#22797;&#22522;&#20110;&#23884;&#20837;&#30340;&#23454;&#20307;&#23545;&#40784;&#32467;&#26524;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#26500;&#24314;&#21305;&#37197;&#23376;&#22270;&#21644;&#23545;&#40784;&#20381;&#36182;&#22270;&#65292;&#35299;&#20915;&#23545;&#40784;&#20914;&#31361;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#26377;&#25928;&#24615;&#21644;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#23545;&#40784;&#65288;EA&#65289;&#26088;&#22312;&#22312;&#19981;&#21516;&#30693;&#35782;&#22270;&#20013;&#23547;&#25214;&#30456;&#21516;&#30340;&#23454;&#20307;&#65292;&#36825;&#26159;&#25968;&#25454;&#24211;&#30740;&#31350;&#20013;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#23558;&#23454;&#20307;&#23884;&#20837;&#21521;&#37327;&#31354;&#38388;&#65292;&#24182;&#36890;&#36807;&#26368;&#36817;&#37051;&#25628;&#32034;&#26469;&#23545;&#40784;&#23427;&#20204;&#12290;&#23613;&#31649;&#22522;&#20110;&#23884;&#20837;&#30340;EA&#22312;&#36817;&#24180;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#32570;&#20047;&#23545;&#40784;&#20915;&#31574;&#30340;&#35299;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#20026;&#29702;&#35299;&#21644;&#20462;&#22797;&#22522;&#20110;&#23884;&#20837;&#30340;EA&#32467;&#26524;&#29983;&#25104;&#35299;&#37322;&#30340;&#26694;&#26550;&#12290;&#32473;&#23450;&#30001;&#23884;&#20837;&#27169;&#22411;&#20135;&#29983;&#30340;EA&#23545;&#65292;&#25105;&#20204;&#39318;&#20808;&#27604;&#36739;&#20854;&#37051;&#36817;&#23454;&#20307;&#21644;&#20851;&#31995;&#65292;&#26500;&#24314;&#19968;&#20010;&#21305;&#37197;&#23376;&#22270;&#20316;&#20026;&#23616;&#37096;&#35299;&#37322;&#12290;&#28982;&#21518;&#26500;&#24314;&#19968;&#20010;&#23545;&#40784;&#20381;&#36182;&#22270;&#65292;&#20197;&#25277;&#35937;&#30340;&#35282;&#24230;&#29702;&#35299;&#35813;&#23545;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#20381;&#36182;&#22270;&#35299;&#20915;&#19977;&#31181;&#31867;&#22411;&#30340;&#23545;&#40784;&#20914;&#31361;&#26469;&#20462;&#22797;&#36825;&#23545;&#12290;&#23545;&#21508;&#31181;EA&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.04877v3 Announce Type: replace  Abstract: Entity alignment (EA) seeks identical entities in different knowledge graphs, which is a long-standing task in the database research. Recent work leverages deep learning to embed entities in vector space and align them via nearest neighbor search. Although embedding-based EA has gained marked success in recent years, it lacks explanations for alignment decisions. In this paper, we present the first framework that can generate explanations for understanding and repairing embedding-based EA results. Given an EA pair produced by an embedding model, we first compare its neighbor entities and relations to build a matching subgraph as a local explanation. We then construct an alignment dependency graph to understand the pair from an abstract perspective. Finally, we repair the pair by resolving three types of alignment conflicts based on dependency graphs. Experiments on a variety of EA datasets demonstrate the effectiveness, generalizatio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Prompt Highlighter&#30340;&#26032;&#22411;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#31361;&#20986;&#26174;&#31034;&#29305;&#23450;&#25552;&#31034;&#36328;&#24230;&#65292;&#23454;&#29616;&#29992;&#25143;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#20132;&#20114;&#25511;&#21046;&#28966;&#28857;&#65292;&#24182;&#22522;&#20110;&#39640;&#20142;&#26631;&#35760;&#24418;&#25104;&#27491;&#35268;&#19988;&#26080;&#26465;&#20214;&#30340;&#19978;&#19979;&#25991;&#23545;&#65292;&#20174;&#32780;&#22312;&#27809;&#26377;&#20998;&#31867;&#22120;&#30340;&#24773;&#20917;&#19979;&#24341;&#23548;&#27169;&#22411;&#30340;&#33258;&#22238;&#24402;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2312.04302</link><description>&lt;p&gt;
Prompt Highlighter: &#22810;&#27169;&#24577;LLM&#20114;&#21160;&#25511;&#21046;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Prompt Highlighter: Interactive Control for Multi-Modal LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04302
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Prompt Highlighter&#30340;&#26032;&#22411;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#31361;&#20986;&#26174;&#31034;&#29305;&#23450;&#25552;&#31034;&#36328;&#24230;&#65292;&#23454;&#29616;&#29992;&#25143;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#20132;&#20114;&#25511;&#21046;&#28966;&#28857;&#65292;&#24182;&#22522;&#20110;&#39640;&#20142;&#26631;&#35760;&#24418;&#25104;&#27491;&#35268;&#19988;&#26080;&#26465;&#20214;&#30340;&#19978;&#19979;&#25991;&#23545;&#65292;&#20174;&#32780;&#22312;&#27809;&#26377;&#20998;&#31867;&#22120;&#30340;&#24773;&#20917;&#19979;&#24341;&#23548;&#27169;&#22411;&#30340;&#33258;&#22238;&#24402;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20851;&#27880;&#22810;&#27169;&#24577;LLMs&#65288;LLMs&#21644;VLMs&#65289;&#25512;&#29702;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#65306;&#26174;&#24335;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#12290;&#22810;&#27169;&#24577;LLMs&#36890;&#36807;&#35821;&#20041;&#29983;&#25104;&#30340;&#33021;&#21147;&#22686;&#24378;&#20102;&#22810;&#27169;&#24577;&#29702;&#35299;&#65292;&#20294;&#30001;&#20110;&#20854;&#33258;&#22238;&#24402;&#29983;&#25104;&#30340;&#29305;&#24615;&#65292;&#35299;&#37322;&#24615;&#36739;&#24046;&#19988;&#26356;&#21152;&#20381;&#36182;&#25552;&#31034;&#20869;&#23481;&#12290;&#23613;&#31649;&#25805;&#20316;&#25552;&#31034;&#26684;&#24335;&#21487;&#20197;&#25913;&#36827;&#36755;&#20986;&#65292;&#20294;&#35774;&#35745;&#38024;&#23545;&#27599;&#20010;&#20219;&#21153;&#30340;&#20855;&#20307;&#21644;&#31934;&#30830;&#25552;&#31034;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#25928;&#26524;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;Prompt Highlighter&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#31361;&#20986;&#26174;&#31034;&#29305;&#23450;&#25552;&#31034;&#36328;&#24230;&#20197;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#20132;&#20114;&#25511;&#21046;&#28966;&#28857;&#12290;&#21463;&#26080;&#20998;&#31867;&#22120;&#25193;&#25955;&#24341;&#23548;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#22522;&#20110;&#31361;&#20986;&#26174;&#31034;&#30340;&#26631;&#35760;&#24418;&#25104;&#24120;&#35268;&#21644;&#26080;&#26465;&#20214;&#30340;&#19978;&#19979;&#25991;&#23545;&#65292;&#35777;&#26126;&#20102;&#27169;&#22411;&#20013;&#30340;&#33258;&#22238;&#24402;&#29983;&#25104;&#21487;&#20197;&#20197;&#26080;&#20998;&#31867;&#22120;&#30340;&#26041;&#24335;&#36827;&#34892;&#24341;&#23548;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#24341;&#23548;&#36807;&#31243;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#36755;&#20986;&#36136;&#37327;&#24182;&#22686;&#24378;&#27169;&#22411;&#30340;&#35299;&#37322;&#33021;&#21147;&#65292;&#21516;&#26102;&#20943;&#36731;&#20102;&#23545;&#25552;&#31034;&#20869;&#23481;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.04302v2 Announce Type: replace-cross  Abstract: This study targets a critical aspect of multi-modal LLMs' (LLMs&amp;VLMs) inference: explicit controllable text generation. Multi-modal LLMs empower multi-modality understanding with the capability of semantic generation yet bring less explainability and heavier reliance on prompt contents due to their autoregressive generative nature. While manipulating prompt formats could improve outputs, designing specific and precise prompts per task can be challenging and ineffective. To tackle this issue, we introduce a novel inference method, Prompt Highlighter, which enables users to highlight specific prompt spans to interactively control the focus during generation. Motivated by the classifier-free diffusion guidance, we form regular and unconditional context pairs based on highlighted tokens, demonstrating that the autoregressive generation in models can be guided in a classifier-free way. Notably, we find that, during inference, guidin
&lt;/p&gt;</description></item><item><title>CoachLM &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#26469;&#22686;&#24378;&#25351;&#23548;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#65292;&#36890;&#36807;&#33258;&#21160;&#20462;&#35746;&#26679;&#26412;&#32780;&#38750;&#20002;&#24323;&#20302;&#36136;&#37327;&#26679;&#26412;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.13246</link><description>&lt;p&gt;
CoachLM&#65306;&#33258;&#21160;&#25351;&#23548;&#20462;&#35746;&#25552;&#39640;LLM&#25351;&#23548;&#35843;&#25972;&#20013;&#30340;&#25968;&#25454;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
CoachLM: Automatic Instruction Revisions Improve the Data Quality in LLM Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13246
&lt;/p&gt;
&lt;p&gt;
CoachLM &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#26469;&#22686;&#24378;&#25351;&#23548;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#65292;&#36890;&#36807;&#33258;&#21160;&#20462;&#35746;&#26679;&#26412;&#32780;&#38750;&#20002;&#24323;&#20302;&#36136;&#37327;&#26679;&#26412;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#23548;&#35843;&#25972;&#23545;&#20110;&#20351;&#35821;&#35328;&#23398;&#20064;&#27169;&#22411;&#65288;LLM&#65289;&#33021;&#22815;&#22238;&#24212;&#20154;&#31867;&#25351;&#20196;&#33267;&#20851;&#37325;&#35201;&#12290;&#29992;&#20110;&#35843;&#25972;&#30340;&#25351;&#20196;&#23545;&#30340;&#36136;&#37327;&#26497;&#22823;&#24433;&#21709;LLM&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#21019;&#24314;&#39640;&#36136;&#37327;&#25351;&#20196;&#25968;&#25454;&#38598;&#25104;&#26412;&#39640;&#65292;&#23548;&#33268;LLM&#30340;&#33258;&#21160;&#29983;&#25104;&#25351;&#20196;&#23545;&#34987;&#24191;&#27867;&#37319;&#32435;&#20316;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#20026;&#20102;&#30830;&#20445;LLM&#29983;&#25104;&#30340;&#25351;&#23548;&#25968;&#25454;&#38598;&#30340;&#39640;&#36136;&#37327;&#65292;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#36890;&#36807;&#36807;&#28388;&#22823;&#37327;&#26679;&#26412;&#26469;&#38477;&#20302;&#25968;&#25454;&#38598;&#23436;&#25972;&#24615;&#65292;&#35201;&#20040;&#19981;&#36866;&#21512;&#24037;&#19994;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;CoachLM&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25968;&#25454;&#38598;&#20013;&#30340;&#26679;&#26412;&#36827;&#34892;&#33258;&#21160;&#20462;&#35746;&#26469;&#22686;&#24378;&#25351;&#23548;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#12290;CoachLM&#26159;&#20174;&#19987;&#23478;&#20462;&#35746;&#30340;&#26679;&#26412;&#20013;&#35757;&#32451;&#20986;&#26469;&#30340;&#65292;&#24182;&#26174;&#33879;&#22686;&#21152;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13246v2 Announce Type: replace  Abstract: Instruction tuning is crucial for enabling Language Learning Models (LLMs) in responding to human instructions. The quality of instruction pairs used for tuning greatly affects the performance of LLMs. However, the manual creation of high-quality instruction datasets is costly, leading to the adoption of automatic generation of instruction pairs by LLMs as a popular alternative. To ensure the high quality of LLM-generated instruction datasets, several approaches have been proposed. Nevertheless, existing methods either compromise dataset integrity by filtering a large proportion of samples, or are unsuitable for industrial applications. In this paper, instead of discarding low-quality samples, we propose CoachLM, a novel approach to enhance the quality of instruction datasets through automatic revisions on samples in the dataset. CoachLM is trained from the samples revised by human experts and significantly increases the proportion o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#29992;&#20110;&#21508;&#31181;&#22522;&#20110;&#34920;&#26684;&#20219;&#21153;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#26500;&#24314;&#26032;&#25968;&#25454;&#38598;TableInstruct&#21644;&#24320;&#21457;&#31532;&#19968;&#20010;&#38754;&#21521;&#34920;&#26684;&#30340;&#24320;&#28304;&#36890;&#29992;&#27169;&#22411;TableLlama&#65292;&#22312;&#34920;&#29616;&#26041;&#38754;&#21462;&#24471;&#20102;&#21487;&#27604;&#25110;&#26356;&#22909;&#30340;&#25104;&#26524;&#12290;</title><link>https://arxiv.org/abs/2311.09206</link><description>&lt;p&gt;
TableLlama&#65306;&#38754;&#21521;&#34920;&#26684;&#30340;&#24320;&#25918;&#22823;&#22411;&#36890;&#29992;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TableLlama: Towards Open Large Generalist Models for Tables
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#29992;&#20110;&#21508;&#31181;&#22522;&#20110;&#34920;&#26684;&#20219;&#21153;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#26500;&#24314;&#26032;&#25968;&#25454;&#38598;TableInstruct&#21644;&#24320;&#21457;&#31532;&#19968;&#20010;&#38754;&#21521;&#34920;&#26684;&#30340;&#24320;&#28304;&#36890;&#29992;&#27169;&#22411;TableLlama&#65292;&#22312;&#34920;&#29616;&#26041;&#38754;&#21462;&#24471;&#20102;&#21487;&#27604;&#25110;&#26356;&#22909;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#32467;&#26500;&#21270;&#34920;&#26684;&#26159;&#26080;&#22788;&#19981;&#22312;&#30340;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#23545;&#34920;&#26684;&#36827;&#34892;&#39044;&#35757;&#32451;&#25110;&#29305;&#27530;&#30340;&#27169;&#22411;&#26550;&#26500;&#35774;&#35745;&#65292;&#21463;&#38480;&#20110;&#29305;&#23450;&#30340;&#34920;&#26684;&#31867;&#22411;&#65292;&#25110;&#23545;&#34920;&#26684;&#21644;&#20219;&#21153;&#26377;&#31616;&#21270;&#30340;&#20551;&#35774;&#12290;&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#24320;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20316;&#20026;&#21508;&#31181;&#22522;&#20110;&#34920;&#26684;&#20219;&#21153;&#30340;&#36890;&#29992;&#24037;&#20855;&#30340;&#31532;&#19968;&#27493;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#21508;&#31181;&#30495;&#23454;&#34920;&#26684;&#21644;&#20219;&#21153;&#30340;&#26032;&#25968;&#25454;&#38598;TableInstruct&#65292;&#20197;&#29992;&#20110;&#25351;&#20196;&#35843;&#25972;&#21644;&#35780;&#20272;LLMs&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21033;&#29992;LongLoRA&#23545;Llama 2 (7B)&#36827;&#34892;&#24494;&#35843;&#65292;&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#38754;&#21521;&#34920;&#26684;&#30340;&#24320;&#28304;&#36890;&#29992;&#27169;&#22411;TableLlama&#65292;&#20197;&#24212;&#23545;&#38271;&#19978;&#19979;&#25991;&#25361;&#25112;&#12290;&#25105;&#20204;&#22312;&#21516;&#39046;&#22495;&#21644;&#36328;&#39046;&#22495;&#29615;&#22659;&#19979;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#22312;8&#20010;&#21516;&#39046;&#22495;&#20219;&#21153;&#20013;&#30340;7&#20010;&#20219;&#21153;&#20013;&#65292;TableLlama&#22312;&#24615;&#33021;&#19978;&#23454;&#29616;&#20102;&#19982;SOT&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09206v2 Announce Type: replace  Abstract: Semi-structured tables are ubiquitous. There has been a variety of tasks that aim to automatically interpret, augment, and query tables. Current methods often require pretraining on tables or special model architecture design, are restricted to specific table types, or have simplifying assumptions about tables and tasks. This paper makes the first step towards developing open-source large language models (LLMs) as generalists for a diversity of table-based tasks. Towards that end, we construct TableInstruct, a new dataset with a variety of realistic tables and tasks, for instruction tuning and evaluating LLMs. We further develop the first open-source generalist model for tables, TableLlama, by fine-tuning Llama 2 (7B) with LongLoRA to address the long context challenge. We experiment under both in-domain setting and out-of-domain setting. On 7 out of 8 in-domain tasks, TableLlama achieves comparable or better performance than the SOT
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#35757;&#32451;&#30340;&#33258;&#25105;&#25913;&#36827;&#26694;&#26550;&#65292;&#21033;&#29992;&#26410;&#26631;&#35760;&#35821;&#26009;&#24211;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#23398;&#20064;&#33021;&#21147;&#65292;&#20174;&#32780;&#25512;&#21160;&#38646;-shot&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#24615;&#33021;&#36793;&#30028;&#12290;</title><link>https://arxiv.org/abs/2311.08921</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38646;-shot&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#33258;&#25105;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Self-Improving for Zero-Shot Named Entity Recognition with Large Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08921
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#35757;&#32451;&#30340;&#33258;&#25105;&#25913;&#36827;&#26694;&#26550;&#65292;&#21033;&#29992;&#26410;&#26631;&#35760;&#35821;&#26009;&#24211;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#23398;&#20064;&#33021;&#21147;&#65292;&#20174;&#32780;&#25512;&#21160;&#38646;-shot&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#24615;&#33021;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25506;&#32034;&#23558;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24212;&#29992;&#20110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;(NER)&#20219;&#21153;&#24341;&#36215;&#20102;&#24456;&#22823;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#26080;&#38656;&#35757;&#32451;&#30340;&#33258;&#25105;&#25913;&#36827;&#26694;&#26550;&#65292;&#21033;&#29992;&#26410;&#26631;&#35760;&#35821;&#26009;&#24211;&#26469;&#28608;&#21457;LLMs&#30340;&#33258;&#25105;&#23398;&#20064;&#33021;&#21147;&#65292;&#20174;&#32780;&#25512;&#21160;LLMs&#22312;&#38646;-shot NER&#19978;&#30340;&#24615;&#33021;&#36793;&#30028;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;LLMs&#23545;&#26410;&#26631;&#27880;&#35821;&#26009;&#24211;&#36827;&#34892;&#33258;&#19968;&#33268;&#24615;&#39044;&#27979;&#65292;&#24182;&#33719;&#24471;&#33258;&#25105;&#27880;&#37322;&#25968;&#25454;&#38598;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25506;&#32034;&#21508;&#31181;&#31574;&#30053;&#26469;&#36873;&#25321;&#21487;&#38752;&#30340;&#27880;&#37322;&#65292;&#24418;&#25104;&#19968;&#20010;&#21487;&#38752;&#30340;&#33258;&#25105;&#27880;&#37322;&#25968;&#25454;&#38598;&#12290;&#26368;&#21518;&#65292;&#23545;&#20110;&#27599;&#20010;&#27979;&#35797;&#36755;&#20837;&#65292;&#25105;&#20204;&#20174;&#21487;&#38752;&#30340;&#33258;&#25105;&#27880;&#37322;&#25968;&#25454;&#38598;&#20013;&#26816;&#32034;&#28436;&#31034;&#65292;&#24182;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#36827;&#34892;&#25512;&#26029;&#12290;&#22312;&#22235;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#22686;&#21152;&#26410;&#26631;&#35760;&#35821;&#26009;&#24211;&#30340;&#35268;&#27169;&#25110;&#36845;&#20195;&#27425;&#25968;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08921v2 Announce Type: replace  Abstract: Exploring the application of powerful large language models (LLMs) on the named entity recognition (NER) task has drawn much attention recently. This work pushes the performance boundary of zero-shot NER with LLMs by proposing a training-free self-improving framework, which utilizes an unlabeled corpus to stimulate the self-learning ability of LLMs. First, we use the LLM to make predictions on the unlabeled corpus using self-consistency and obtain a self-annotated dataset. Second, we explore various strategies to select reliable annotations to form a reliable self-annotated dataset. Finally, for each test input, we retrieve demonstrations from the reliable self-annotated dataset and perform inference via in-context learning. Experiments on four benchmarks show substantial performance improvements achieved by our framework. Through comprehensive experimental analysis, we find that increasing the size of unlabeled corpus or iterations 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;Reddit&#19978;&#30340;&#36947;&#24503;&#21028;&#26029;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#31038;&#20132;&#24120;&#35782;&#21644;&#35821;&#35328;&#20449;&#21495;&#23545;&#20110;&#36947;&#24503;&#28779;&#33457;&#30340;&#24433;&#21709;&#65292;&#20026;&#20154;&#31867;&#36947;&#24503;&#21028;&#26029;&#25552;&#20379;&#20102;&#28145;&#20837;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2310.19268</link><description>&lt;p&gt;
Reddit&#19978;&#20851;&#20110;&#21465;&#20107;&#20013;&#30340;&#36947;&#24503;&#21028;&#26029;&#65306;&#36890;&#36807;&#31038;&#20132;&#24120;&#35782;&#21644;&#35821;&#35328;&#20449;&#21495;&#35843;&#26597;&#36947;&#24503;&#28779;&#33457;
&lt;/p&gt;
&lt;p&gt;
Moral Judgments in Narratives on Reddit: Investigating Moral Sparks via Social Commonsense and Linguistic Signals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.19268
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;Reddit&#19978;&#30340;&#36947;&#24503;&#21028;&#26029;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#31038;&#20132;&#24120;&#35782;&#21644;&#35821;&#35328;&#20449;&#21495;&#23545;&#20110;&#36947;&#24503;&#28779;&#33457;&#30340;&#24433;&#21709;&#65292;&#20026;&#20154;&#31867;&#36947;&#24503;&#21028;&#26029;&#25552;&#20379;&#20102;&#28145;&#20837;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2310.19268v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;-&#36328; &#25991;&#25688;&#65306;&#26426;&#22120;&#20262;&#29702;&#30830;&#20445;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#21644;&#20195;&#29702;&#30340;&#36947;&#24503;&#34892;&#20026;&#12290;&#30740;&#31350;&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#24212;&#29992;&#26377;&#21161;&#20110;&#23398;&#20064;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#23454;&#36341;&#36947;&#24503;&#65292;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#19981;&#21516;&#32972;&#26223;&#19979;&#30340;&#20154;&#31867;&#36947;&#24503;&#22797;&#26434;&#24615;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#65292;&#20197;&#29702;&#35299;&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#20262;&#29702;&#24773;&#26223;&#21644;&#20154;&#31867;&#36947;&#24503;&#21028;&#26029;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#21517;&#20026;r/AmITheAsshole&#30340;&#28909;&#38376;Reddit&#23376;&#31038;&#21306;&#20013;&#30340;&#24086;&#23376;&#65292;&#20316;&#32773;&#21644;&#35780;&#35770;&#32773;&#22312;&#36825;&#37324;&#20998;&#20139;&#35841;&#24212;&#35813;&#21463;&#21040;&#36131;&#22791;&#30340;&#36947;&#24503;&#21028;&#26029;&#12290;&#25105;&#20204;&#37319;&#29992;&#35745;&#31639;&#25216;&#26415;&#26469;&#30740;&#31350;&#24433;&#21709;&#36947;&#24503;&#21028;&#26029;&#30340;&#28508;&#22312;&#25512;&#29702;&#12290;&#25105;&#20204;&#20851;&#27880;&#21407;&#22987;&#24086;&#23376;&#20013;&#30340;&#33410;&#36873;&#65292;&#25105;&#20204;&#23558;&#20854;&#21629;&#21517;&#20026;&#36947;&#24503;&#28779;&#33457;&#65292;&#35780;&#35770;&#32773;&#21253;&#25324;&#36825;&#20123;&#33410;&#36873;&#20197;&#34920;&#26126;&#20182;&#20204;&#21028;&#26029;&#30340;&#21160;&#26426;&#26159;&#20160;&#20040;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#65288;1&#65289;&#28608;&#27963;&#31038;&#20132;&#24120;&#35782;&#30340;&#20107;&#20214;&#21644;&#65288;2&#65289;&#35821;&#35328;&#20449;&#21495;&#22914;&#20309;&#24433;&#21709;&#36947;&#24503;&#28779;&#33457;&#30340;&#20998;&#37197;&#21450;&#20854;&#23376;&#21021;&#22987;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.19268v2 Announce Type: replace-cross  Abstract: Machine ethics ensures ethical conduct in Artificial Intelligence (AI) models and agents. Examining real-life applications benefit learning practical ethics in many situations, offering valuable data to grasp the complexities of human ethics in diverse contexts. In this paper, we examine social media platforms for understanding real-life ethical scenarios and human moral judgments. We examine posts from a popular Reddit subreddit (i.e., a subcommunity) called r/AmITheAsshole, where authors and commenters share their moral judgments on who is blameworthy. We employ computational techniques to investigate the underlying reasoning influencing moral judgments. We focus on excerpts-which we term moral sparks-from original posts that commenters include to indicate what motivates their judgments. To this end, we examine how (1) events activating social commonsense and (2) linguistic signals affect moral sparks assignment and their sub
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#24187;&#35273;&#29616;&#35937;&#65292;&#20294;&#30740;&#31350;&#34920;&#26126;&#23427;&#20204;&#21487;&#20197;&#20316;&#20026;&#26377;&#25928;&#30340;&#20107;&#23454;&#39564;&#35777;&#22120;&#65292;&#29978;&#33267;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#32988;&#36807;&#26356;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2310.14564</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#24187;&#35273;&#65292;&#20294;&#22312;&#20107;&#23454;&#39564;&#35777;&#26041;&#38754;&#21487;&#33021;&#34920;&#29616;&#20986;&#33394;
&lt;/p&gt;
&lt;p&gt;
Language Models Hallucinate, but May Excel at Fact Verification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.14564
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#24187;&#35273;&#29616;&#35937;&#65292;&#20294;&#30740;&#31350;&#34920;&#26126;&#23427;&#20204;&#21487;&#20197;&#20316;&#20026;&#26377;&#25928;&#30340;&#20107;&#23454;&#39564;&#35777;&#22120;&#65292;&#29978;&#33267;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#32988;&#36807;&#26356;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#30340;&#26368;&#26032;&#36827;&#23637;&#24456;&#22823;&#31243;&#24230;&#19978;&#35201;&#24402;&#21151;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#26174;&#33879;&#36827;&#27493;&#12290;&#28982;&#32780;&#65292;LLMs&#32463;&#24120;&#20250;&#8220;&#24187;&#35273;&#8221;&#65292;&#23548;&#33268;&#20135;&#29983;&#19982;&#20107;&#23454;&#19981;&#31526;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#30340;&#20154;&#31867;&#35780;&#20272;&#35777;&#23454;&#20102;&#20005;&#37325;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#21457;&#29616;&#21363;&#20351;GPT-3.5&#30340;&#20107;&#23454;&#36755;&#20986;&#19981;&#21040;25%&#12290;&#36825;&#20984;&#26174;&#20102;&#20107;&#23454;&#39564;&#35777;&#22120;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#34913;&#37327;&#21644;&#28608;&#21169;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#35843;&#26597;&#30830;&#35748;&#20102;LLMs&#21487;&#20197;&#34987;&#37325;&#26032;&#29992;&#20316;&#26377;&#25928;&#30340;&#20107;&#23454;&#39564;&#35777;&#22120;&#65292;&#19982;&#20154;&#31867;&#21028;&#26029;&#20855;&#26377;&#24456;&#24378;&#30340;&#30456;&#20851;&#24615;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#34920;&#29616;&#26368;&#24046;&#30340;&#29983;&#25104;&#22120;FLAN-T5-11B&#20316;&#20026;&#20107;&#23454;&#39564;&#35777;&#22120;&#34920;&#29616;&#26368;&#20339;&#65292;&#29978;&#33267;&#20248;&#20110;GPT3.5&#21644;ChatGPT&#31561;&#26356;&#20248;&#31168;&#30340;LLMs&#12290;&#28145;&#20837;&#30740;&#31350;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20123;LLMs&#23545;&#39640;&#36136;&#37327;&#35777;&#25454;&#30340;&#20381;&#36182;&#65292;&#20197;&#21450;&#23427;&#20204;&#22312;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20123;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.14564v2 Announce Type: replace  Abstract: Recent progress in natural language processing (NLP) owes much to remarkable advances in large language models (LLMs). Nevertheless, LLMs frequently "hallucinate," resulting in non-factual outputs. Our carefully-designed human evaluation substantiates the serious hallucination issue, revealing that even GPT-3.5 produces factual outputs less than 25% of the time. This underscores the importance of fact verifiers in order to measure and incentivize progress. Our systematic investigation affirms that LLMs can be repurposed as effective fact verifiers with strong correlations with human judgments. Surprisingly, FLAN-T5-11B, the least factual generator in our study, performs the best as a fact verifier, even outperforming more capable LLMs like GPT3.5 and ChatGPT. Delving deeper, we analyze the reliance of these LLMs on high-quality evidence, as well as their deficiencies in robustness and generalization ability. Our study presents insigh
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#25552;&#31034;&#27744;&#21644;&#26500;&#24314;&#22522;&#20110;&#23454;&#20363;&#30340;&#25552;&#31034;&#20197;&#21450;&#24341;&#20837;&#26032;&#39062;&#30340;&#36719;&#35821;&#35328;&#21270;&#22120;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20803;&#23398;&#20064;&#21644;&#20195;&#34920;&#24615;&#35821;&#35328;&#21270;&#22120;&#23454;&#29616;&#26377;&#25928;&#30340;&#32467;&#26500;&#21270;&#25552;&#31034;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2306.00618</link><description>&lt;p&gt;
&#36890;&#36807;&#20803;&#23398;&#20064;&#21644;&#20195;&#34920;&#24615;&#35821;&#35328;&#21270;&#22120;&#23454;&#29616;&#26377;&#25928;&#30340;&#32467;&#26500;&#21270;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Effective Structured Prompting by Meta-Learning and Representative Verbalizer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.00618
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#25552;&#31034;&#27744;&#21644;&#26500;&#24314;&#22522;&#20110;&#23454;&#20363;&#30340;&#25552;&#31034;&#20197;&#21450;&#24341;&#20837;&#26032;&#39062;&#30340;&#36719;&#35821;&#35328;&#21270;&#22120;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20803;&#23398;&#20064;&#21644;&#20195;&#34920;&#24615;&#35821;&#35328;&#21270;&#22120;&#23454;&#29616;&#26377;&#25928;&#30340;&#32467;&#26500;&#21270;&#25552;&#31034;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#65288;MLM&#65289;&#30340;&#25552;&#31034;&#35843;&#25972;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#26377;&#38480;&#26631;&#35760;&#31034;&#20363;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;&#23427;&#20026;&#19979;&#28216;&#20219;&#21153;&#35843;&#25972;&#25552;&#31034;&#65292;&#24182;&#20351;&#29992;&#35821;&#35328;&#21270;&#22120;&#26469;&#36830;&#25509;&#39044;&#27979;&#30340;&#26631;&#35760;&#21644;&#26631;&#31614;&#39044;&#27979;&#12290;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#65292;&#25552;&#31034;&#21021;&#22987;&#21270;&#23545;&#20110;&#25552;&#31034;&#35843;&#25972;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;MetaPrompting&#65288;Hou&#31561;&#65292;2022&#65289;&#20351;&#29992;&#20803;&#23398;&#20064;&#26469;&#23398;&#20064;&#25152;&#26377;&#29305;&#23450;&#20219;&#21153;&#25552;&#31034;&#30340;&#20849;&#20139;&#21021;&#22987;&#21270;&#12290;&#28982;&#32780;&#65292;&#24403;&#20219;&#21153;&#22797;&#26434;&#26102;&#65292;&#21333;&#19968;&#21021;&#22987;&#21270;&#26080;&#27861;&#33719;&#24471;&#25152;&#26377;&#20219;&#21153;&#21644;&#26679;&#26412;&#30340;&#33391;&#22909;&#25552;&#31034;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;MLM&#36890;&#24120;&#24456;&#22823;&#65292;MetaPrompting&#38656;&#35201;&#35843;&#25972;&#25972;&#20010;MLM&#65292;&#23548;&#33268;&#35745;&#31639;&#21644;&#20869;&#23384;&#36127;&#25285;&#27785;&#37325;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#25552;&#31034;&#27744;&#25552;&#21462;&#26356;&#22810;&#20219;&#21153;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#27880;&#24847;&#21147;&#26500;&#24314;&#22522;&#20110;&#23454;&#20363;&#30340;&#25552;&#31034;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36719;&#35821;&#35328;&#21270;&#22120;&#65288;RepVerb&#65289;...&#65288;&#21097;&#20313;&#20869;&#23481;&#26410;&#25552;&#20379;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.00618v2 Announce Type: replace-cross  Abstract: Prompt tuning for pre-trained masked language models (MLM) has shown promising performance in natural language processing tasks with few labeled examples. It tunes a prompt for the downstream task, and a verbalizer is used to bridge the predicted token and label prediction. Due to the limited training data, prompt initialization is crucial for prompt tuning. Recently, MetaPrompting (Hou et al., 2022) uses meta-learning to learn a shared initialization for all task-specific prompts. However, a single initialization is insufficient to obtain good prompts for all tasks and samples when the tasks are complex. Moreover, MetaPrompting requires tuning the whole MLM, causing a heavy burden on computation and memory as the MLM is usually large. To address these issues, we use a prompt pool to extract more task knowledge and construct instance-dependent prompts via attention. We further propose a novel soft verbalizer (RepVerb) which con
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MulCo&#65306;&#22810;&#23610;&#24230;&#23545;&#27604;&#30693;&#35782;&#20849;&#21516;&#33976;&#39311;&#29992;&#20110;&#20840;&#38754;&#25552;&#39640;&#25152;&#26377;&#31867;&#22411;&#26102;&#38388;&#25968;&#25454;&#38598;&#24615;&#33021;</title><link>https://arxiv.org/abs/2209.00568</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#23545;&#27604;&#30693;&#35782;&#20849;&#21516;&#33976;&#39311;&#29992;&#20110;&#20107;&#20214;&#26102;&#38388;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Multi-Scale Contrastive Knowledge Co-Distillation for Event Temporal Relation Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.00568
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MulCo&#65306;&#22810;&#23610;&#24230;&#23545;&#27604;&#30693;&#35782;&#20849;&#21516;&#33976;&#39311;&#29992;&#20110;&#20840;&#38754;&#25552;&#39640;&#25152;&#26377;&#31867;&#22411;&#26102;&#38388;&#25968;&#25454;&#38598;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#26102;&#38388;&#20851;&#31995;&#25277;&#21462;&#65288;ETRE&#65289;&#26159;&#19968;&#20010;&#20851;&#38190;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20107;&#20214;&#23545;&#20301;&#20110;&#19981;&#21516;&#36317;&#31163;&#30340;&#35805;&#35821;&#20013;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#25509;&#36817;&#24615;&#24102;&#12290;&#20851;&#20110;&#20301;&#20110;&#26356;&#36828;&#65288;&#21363;&#8220;&#38271;&#8221;&#65289;&#25110;&#26356;&#36817;&#65288;&#21363;&#8220;&#30701;&#8221;&#65289;&#25509;&#36817;&#24615;&#24102;&#30340;&#20107;&#20214;&#23545;&#30340;&#26102;&#38388;&#39034;&#24207;&#20256;&#36798;&#26041;&#24335;&#19981;&#21516;&#12290;&#30446;&#21069;ETRE&#27169;&#22411;&#24448;&#24448;&#22312;&#20301;&#20110;&#30701;&#25110;&#38271;&#25509;&#36817;&#24615;&#24102;&#30340;&#20107;&#20214;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#19981;&#33021;&#21516;&#26102;&#34920;&#29616;&#33391;&#22909;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#33258;&#28982;&#25991;&#26412;&#21253;&#21547;&#25152;&#26377;&#31867;&#22411;&#30340;&#26102;&#38388;&#20107;&#20214;&#23545;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MulCo&#65306;&#22810;&#23610;&#24230;&#23545;&#27604;&#30693;&#35782;&#20849;&#21516;&#33976;&#39311;&#65292;&#36825;&#26159;&#19968;&#31181;&#34701;&#21512;&#26041;&#27861;&#65292;&#21487;&#20197;&#36328;&#22810;&#20010;&#20107;&#20214;&#23545;&#25509;&#36817;&#24615;&#24102;&#20849;&#20139;&#30693;&#35782;&#65292;&#20197;&#25552;&#39640;&#23545;&#25152;&#26377;&#31867;&#22411;&#26102;&#38388;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;MulCo&#25104;&#21151;&#22320;&#25972;&#21512;&#20102;&#36328;&#30701;&#21644;&#38271;&#25509;&#36817;&#24615;&#24102;&#30340;&#19982;&#26102;&#38388;&#25512;&#29702;&#30456;&#20851;&#30340;&#35821;&#35328;&#32447;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.00568v2 Announce Type: replace-cross  Abstract: Event Temporal Relation Extraction (ETRE) is a crucial yet challenging problem. Event pairs are situated within a discourse at different distances, which we refer to as proximity bands. The temporal ordering communicated about event pairs situated at more remote (i.e., ``long'') or less remote (i.e., ``short'') proximity bands is encoded differently. SOTA ETRE models have tended to perform well on events situated at either short or long proximity bands, but not both. Yet, real-world, natural texts contain all types of temporal event-pairs. In this paper, we present MulCo: Multi-Scale Contrastive Knowledge Co-Distillation, a fusion approach that shares knowledge across multiple event pair proximity bands in order to improve performance on all types of temporal datasets. Our experimental results show that MulCo successfully integrates linguistic cues pertaining to temporal reasoning across both short and long proximity bands and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HKLM&#30340;&#24322;&#26500;&#30693;&#35782;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#32479;&#19968;&#22788;&#29702;&#21253;&#25324;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#12289;&#21322;&#32467;&#26500;&#21270;&#25991;&#26412;&#21644;&#32467;&#26500;&#21270;&#25991;&#26412;&#22312;&#20869;&#30340;&#25152;&#26377;&#25991;&#26412;&#24418;&#24335;&#65292;&#36890;&#36807;&#19981;&#21516;&#30340;&#30446;&#26631;&#23398;&#20064;&#35789;&#30693;&#35782;&#12289;&#23454;&#20307;&#30693;&#35782;&#21644;&#20027;&#39064;&#30693;&#35782;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#24615;&#33021;&#20248;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2109.01048</link><description>&lt;p&gt;
&#23558;&#39046;&#22495;&#29305;&#23450;&#30340;&#24322;&#26500;&#30693;&#35782;&#34701;&#20837;&#32479;&#19968;&#34920;&#31034;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Pre-training Language Model Incorporating Domain-specific Heterogeneous Knowledge into A Unified Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2109.01048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HKLM&#30340;&#24322;&#26500;&#30693;&#35782;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#32479;&#19968;&#22788;&#29702;&#21253;&#25324;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#12289;&#21322;&#32467;&#26500;&#21270;&#25991;&#26412;&#21644;&#32467;&#26500;&#21270;&#25991;&#26412;&#22312;&#20869;&#30340;&#25152;&#26377;&#25991;&#26412;&#24418;&#24335;&#65292;&#36890;&#36807;&#19981;&#21516;&#30340;&#30446;&#26631;&#23398;&#20064;&#35789;&#30693;&#35782;&#12289;&#23454;&#20307;&#30693;&#35782;&#21644;&#20027;&#39064;&#30693;&#35782;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#24615;&#33021;&#20248;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#25216;&#26415;&#20174;&#19981;&#21516;&#35282;&#24230;&#25193;&#23637;&#20102;BERT&#65292;&#20363;&#22914;&#35774;&#35745;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#12289;&#19981;&#21516;&#30340;&#35821;&#20041;&#31890;&#24230;&#21644;&#19981;&#21516;&#30340;&#27169;&#22411;&#26550;&#26500;&#12290;&#24456;&#23569;&#26377;&#27169;&#22411;&#32771;&#34385;&#20174;&#19981;&#21516;&#30340;&#25991;&#26412;&#26684;&#24335;&#25193;&#23637;BERT&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#26500;&#30693;&#35782;&#35821;&#35328;&#27169;&#22411;&#65288;HKLM&#65289;&#65292;&#19968;&#31181;&#32479;&#19968;&#30340;&#38754;&#21521;&#25152;&#26377;&#24418;&#24335;&#25991;&#26412;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#65292;&#21253;&#25324;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#12289;&#21322;&#32467;&#26500;&#21270;&#25991;&#26412;&#21644;&#32467;&#26500;&#21270;&#25991;&#26412;&#12290;&#20026;&#20102;&#25429;&#25417;&#36825;&#20123;&#22810;&#26684;&#24335;&#30693;&#35782;&#20043;&#38388;&#30340;&#30456;&#24212;&#20851;&#31995;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#30446;&#26631;&#26469;&#23398;&#20064;&#35789;&#30693;&#35782;&#65292;&#20351;&#29992;&#19977;&#20803;&#20998;&#31867;&#30446;&#26631;&#21644;&#26631;&#39064;&#21305;&#37197;&#30446;&#26631;&#20998;&#21035;&#23398;&#20064;&#23454;&#20307;&#30693;&#35782;&#21644;&#20027;&#39064;&#30693;&#35782;&#12290;&#20026;&#20102;&#33719;&#24471;&#21069;&#36848;&#22810;&#26684;&#24335;&#25991;&#26412;&#65292;&#25105;&#20204;&#22312;&#26053;&#28216;&#39046;&#22495;&#26500;&#24314;&#20102;&#19968;&#20010;&#35821;&#26009;&#24211;&#65292;&#24182;&#22312;5&#20010;&#26053;&#28216;NLP&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32988;&#36807;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2109.01048v3 Announce Type: replace  Abstract: Existing technologies expand BERT from different perspectives, e.g. designing different pre-training tasks, different semantic granularities, and different model architectures. Few models consider expanding BERT from different text formats. In this paper, we propose a heterogeneous knowledge language model (\textbf{HKLM}), a unified pre-trained language model (PLM) for all forms of text, including unstructured text, semi-structured text, and well-structured text. To capture the corresponding relations among these multi-format knowledge, our approach uses masked language model objective to learn word knowledge, uses triple classification objective and title matching objective to learn entity knowledge and topic knowledge respectively. To obtain the aforementioned multi-format text, we construct a corpus in the tourism domain and conduct experiments on 5 tourism NLP datasets. The results show that our approach outperforms the pre-train
&lt;/p&gt;</description></item><item><title>VQPy&#26159;&#19968;&#31181;&#38754;&#21521;&#23545;&#35937;&#30340;&#35270;&#39057;&#20998;&#26512;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;Python&#21464;&#20307;&#20316;&#20026;&#21069;&#31471;&#65292;&#24182;&#20855;&#26377;&#21487;&#25193;&#23637;&#30340;&#21518;&#31471;&#65292;&#21487;&#20197;&#33258;&#21160;&#26500;&#24314;&#21644;&#20248;&#21270;&#22522;&#20110;&#35270;&#39057;&#23545;&#35937;&#30340;&#22788;&#29702;&#27969;&#31243;&#12290;</title><link>http://arxiv.org/abs/2311.01623</link><description>&lt;p&gt;
VQPy&#65306;&#19968;&#31181;&#38754;&#21521;&#29616;&#20195;&#35270;&#39057;&#20998;&#26512;&#30340;&#38754;&#21521;&#23545;&#35937;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
VQPy: An Object-Oriented Approach to Modern Video Analytics. (arXiv:2311.01623v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01623
&lt;/p&gt;
&lt;p&gt;
VQPy&#26159;&#19968;&#31181;&#38754;&#21521;&#23545;&#35937;&#30340;&#35270;&#39057;&#20998;&#26512;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;Python&#21464;&#20307;&#20316;&#20026;&#21069;&#31471;&#65292;&#24182;&#20855;&#26377;&#21487;&#25193;&#23637;&#30340;&#21518;&#31471;&#65292;&#21487;&#20197;&#33258;&#21160;&#26500;&#24314;&#21644;&#20248;&#21270;&#22522;&#20110;&#35270;&#39057;&#23545;&#35937;&#30340;&#22788;&#29702;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#20998;&#26512;&#24191;&#27867;&#24212;&#29992;&#20110;&#24403;&#20170;&#31995;&#32479;&#21644;&#26381;&#21153;&#20013;&#12290;&#22312;&#35270;&#39057;&#20998;&#26512;&#30340;&#21069;&#27839;&#26159;&#29992;&#25143;&#24320;&#21457;&#30340;&#35270;&#39057;&#26597;&#35810;&#65292;&#20197;&#25214;&#21040;&#29305;&#23450;&#24863;&#20852;&#36259;&#30340;&#23545;&#35937;&#12290;&#22522;&#20110;&#35270;&#39057;&#23545;&#35937;&#65288;&#20363;&#22914;&#20154;&#65292;&#21160;&#29289;&#65292;&#27773;&#36710;&#31561;&#65289;&#19982;&#20256;&#32479;&#38754;&#21521;&#23545;&#35937;&#35821;&#35328;&#24314;&#27169;&#30340;&#23545;&#35937;&#30456;&#20284;&#30340;&#27934;&#23519;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#35270;&#39057;&#20998;&#26512;&#30340;&#38754;&#21521;&#23545;&#35937;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21517;&#20026;VQPy&#65292;&#21253;&#25324;&#19968;&#20010;&#21069;&#31471;&#65288;&#19968;&#31181;Python&#21464;&#20307;&#65292;&#20854;&#20013;&#21253;&#21547;&#29992;&#25143;&#21487;&#20197;&#34920;&#36798;&#35270;&#39057;&#23545;&#35937;&#21450;&#20854;&#20132;&#20114;&#30340;&#32467;&#26500;&#65289;&#21644;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#21518;&#31471;&#65292;&#21487;&#20197;&#22522;&#20110;&#35270;&#39057;&#23545;&#35937;&#33258;&#21160;&#29983;&#25104;&#21644;&#20248;&#21270;&#31649;&#36947;&#12290;&#25105;&#20204;&#24050;&#32463;&#23454;&#26045;&#21644;&#24320;&#28304;&#20102;VQPy&#65292;&#23427;&#24050;&#32463;&#20316;&#20026;Cisco DeepVision&#26694;&#26550;&#30340;&#19968;&#37096;&#20998;&#20135;&#21697;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video analytics is widely used in contemporary systems and services. At the forefront of video analytics are video queries that users develop to find objects of particular interest. Building upon the insight that video objects (e.g., human, animals, cars, etc.), the center of video analytics, are similar in spirit to objects modeled by traditional object-oriented languages, we propose to develop an object-oriented approach to video analytics. This approach, named VQPy, consists of a frontend$\unicode{x2015}$a Python variant with constructs that make it easy for users to express video objects and their interactions$\unicode{x2015}$as well as an extensible backend that can automatically construct and optimize pipelines based on video objects. We have implemented and open-sourced VQPy, which has been productized in Cisco as part of its DeepVision framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#25105;&#26816;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#21028;&#26029;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#26080;&#27861;&#22238;&#31572;&#30340;&#38382;&#39064;&#65292;&#20197;&#36991;&#20813;&#29983;&#25104;&#38750;&#20107;&#23454;&#24615;&#30340;&#22238;&#31572;&#12290;&#36890;&#36807;&#22810;&#26679;&#21270;&#38382;&#39064;&#30340;&#25991;&#26412;&#34920;&#36798;&#65292;&#25910;&#38598;&#31572;&#26696;&#65292;&#24182;&#26816;&#26597;&#29983;&#25104;&#30340;&#31572;&#26696;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#21487;&#33021;&#29983;&#25104;&#34394;&#20551;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21482;&#38656;&#35201;&#21033;&#29992;LLMs&#33258;&#36523;&#65292;&#26080;&#38656;&#20854;&#20182;&#22806;&#37096;&#36164;&#28304;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;Vicuna&#12289;ChatGPT&#21644;GPT-4&#31561;&#26368;&#26032;&#21457;&#24067;&#30340;LLMs&#19978;&#24471;&#21040;&#20102;&#26377;&#25928;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.17918</link><description>&lt;p&gt;
&#30693;&#36947;LLMs&#19981;&#30693;&#36947;&#20160;&#20040;&#65306;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33258;&#25105;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Knowing What LLMs DO NOT Know: A Simple Yet Effective Self-Detection Method. (arXiv:2310.17918v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#25105;&#26816;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#21028;&#26029;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#26080;&#27861;&#22238;&#31572;&#30340;&#38382;&#39064;&#65292;&#20197;&#36991;&#20813;&#29983;&#25104;&#38750;&#20107;&#23454;&#24615;&#30340;&#22238;&#31572;&#12290;&#36890;&#36807;&#22810;&#26679;&#21270;&#38382;&#39064;&#30340;&#25991;&#26412;&#34920;&#36798;&#65292;&#25910;&#38598;&#31572;&#26696;&#65292;&#24182;&#26816;&#26597;&#29983;&#25104;&#30340;&#31572;&#26696;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#21487;&#33021;&#29983;&#25104;&#34394;&#20551;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21482;&#38656;&#35201;&#21033;&#29992;LLMs&#33258;&#36523;&#65292;&#26080;&#38656;&#20854;&#20182;&#22806;&#37096;&#36164;&#28304;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;Vicuna&#12289;ChatGPT&#21644;GPT-4&#31561;&#26368;&#26032;&#21457;&#24067;&#30340;LLMs&#19978;&#24471;&#21040;&#20102;&#26377;&#25928;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#25991;&#29486;&#25581;&#31034;&#20102;LLMs&#20250;&#20598;&#23572;&#29983;&#25104;&#38750;&#20107;&#23454;&#24615;&#30340;&#22238;&#31572;&#65292;&#36825;&#24433;&#21709;&#20102;&#23427;&#20204;&#36827;&#19968;&#27493;&#21033;&#29992;&#30340;&#21487;&#38752;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#25105;&#26816;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;LLMs&#19981;&#30693;&#36947;&#30340;&#38382;&#39064;&#65292;&#20197;&#36991;&#20813;&#29983;&#25104;&#38750;&#20107;&#23454;&#24615;&#30340;&#32467;&#26524;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#32473;&#23450;&#38382;&#39064;&#30340;&#25991;&#26412;&#34920;&#36798;&#22810;&#26679;&#21270;&#65292;&#24182;&#25910;&#38598;&#30456;&#24212;&#30340;&#31572;&#26696;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26816;&#26597;&#29983;&#25104;&#30340;&#31572;&#26696;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20197;&#35782;&#21035;&#27169;&#22411;&#21487;&#33021;&#29983;&#25104;&#34394;&#20551;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#25152;&#26377;&#20197;&#19978;&#27493;&#39588;&#37117;&#21487;&#20197;&#36890;&#36807;&#25552;&#31034;LLMs&#33258;&#36523;&#26469;&#23436;&#25104;&#65292;&#32780;&#26080;&#38656;&#21442;&#32771;&#20219;&#20309;&#20854;&#20182;&#22806;&#37096;&#36164;&#28304;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#26368;&#36817;&#21457;&#24067;&#30340;LLMs&#65288;&#22914;Vicuna&#12289;ChatGPT&#21644;GPT-4&#65289;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown great potential in Natural Language Processing (NLP) tasks. However, recent literature reveals that LLMs generate nonfactual responses intermittently, which impedes the LLMs' reliability for further utilization. In this paper, we propose a novel self-detection method to detect which questions that a LLM does not know that are prone to generate nonfactual results. Specifically, we first diversify the textual expressions for a given question and collect the corresponding answers. Then we examine the divergencies between the generated answers to identify the questions that the model may generate falsehoods. All of the above steps can be accomplished by prompting the LLMs themselves without referring to any other external resources. We conduct comprehensive experiments and demonstrate the effectiveness of our method on recently released LLMs, e.g., Vicuna, ChatGPT, and GPT-4.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32422;&#26463;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#30740;&#31350;&#20102;LLM&#30340;&#33021;&#21147;&#21644;&#19981;&#36275;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#21457;&#23637;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.16343</link><description>&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32422;&#26463;&#25991;&#26412;&#29983;&#25104;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Evaluation of Constrained Text Generation for Large Language Models. (arXiv:2310.16343v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16343
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32422;&#26463;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#30740;&#31350;&#20102;LLM&#30340;&#33021;&#21147;&#21644;&#19981;&#36275;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#21457;&#23637;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29983;&#25104; (NLG) &#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#30340;&#36827;&#27493;&#20351;&#24471;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#33021;&#22815;&#29983;&#25104;&#29087;&#32451;&#30340;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;LLM&#30340;&#19981;&#36879;&#26126;&#24615;&#65292;&#23558;&#22797;&#26434;&#30340;&#32422;&#26463;&#38598;&#25104;&#21040;&#31070;&#32463;&#25991;&#26412;&#29983;&#25104;&#20013;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;LLM&#30340;&#32422;&#26463;&#25991;&#26412;&#29983;&#25104;&#65292;&#20854;&#20013;&#22312;LLM&#30340;&#29983;&#25104;&#36807;&#31243;&#20013;&#24212;&#29992;&#20102;&#39044;&#23450;&#20041;&#30340;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32771;&#23519;&#20102;&#22810;&#20010;LLM&#65292;&#21253;&#25324;ChatGPT&#21644;GPT-4&#65292;&#24182;&#23558;&#32422;&#26463;&#20998;&#20026;&#35789;&#27719;&#12289;&#32467;&#26500;&#21644;&#20851;&#31995;&#31867;&#22411;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#21508;&#31181;&#22522;&#20934;&#26469;&#20419;&#36827;&#20844;&#24179;&#35780;&#20272;&#12290;&#30740;&#31350;&#35299;&#20915;&#20102;&#19968;&#20123;&#20851;&#38190;&#30740;&#31350;&#38382;&#39064;&#65292;&#21253;&#25324;LLM&#19982;&#32422;&#26463;&#30340;&#36981;&#23432;&#31243;&#24230;&#12290;&#32467;&#26524;&#25581;&#31034;&#20102;LLM&#38598;&#25104;&#32422;&#26463;&#30340;&#33021;&#21147;&#21644;&#19981;&#36275;&#65292;&#24182;&#20026;&#26410;&#26469;&#21457;&#23637;&#32422;&#26463;&#25991;&#26412;&#29983;&#25104;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#23558;&#22312;&#34987;&#25509;&#21463;&#21518;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advancements in natural language generation (NLG) and large language models (LLMs) have led to proficient text generation in various tasks. However, integrating intricate constraints into neural text generation, due to LLMs' opacity, remains challenging. This study investigates constrained text generation for LLMs, where predefined constraints are applied during LLM's generation process. Our research examines multiple LLMs, including ChatGPT and GPT-4, categorizing constraints into lexical, structural, and relation-based types. We also present various benchmarks to facilitate fair evaluation. The study addresses some key research questions, including the extent of LLMs' compliance with constraints. Results illuminate LLMs' capacity and deficiency to incorporate constraints and provide insights for future developments in constrained text generation. Codes and datasets will be released upon acceptance.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#26102;&#38388;&#36830;&#32493; (TiC) &#22522;&#20934;&#65292;&#20351;&#29992;&#36825;&#20123;&#22522;&#20934;&#35780;&#20272;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#26102;&#38388;&#40065;&#26834;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#25490;&#32451;&#26041;&#27861;&#26469;&#25345;&#32493;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.16226</link><description>&lt;p&gt;
TiC-CLIP: CLIP&#27169;&#22411;&#30340;&#25345;&#32493;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
TiC-CLIP: Continual Training of CLIP Models. (arXiv:2310.16226v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16226
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#26102;&#38388;&#36830;&#32493; (TiC) &#22522;&#20934;&#65292;&#20351;&#29992;&#36825;&#20123;&#22522;&#20934;&#35780;&#20272;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#26102;&#38388;&#40065;&#26834;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#25490;&#32451;&#26041;&#27861;&#26469;&#25345;&#32493;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20445;&#25345;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#19982;&#26368;&#26032;&#25968;&#25454;&#20445;&#25345;&#21516;&#27493;&#26412;&#36523;&#23601;&#26159;&#26114;&#36149;&#30340;&#12290;&#20026;&#20102;&#36991;&#20813;&#19981;&#26029;&#37325;&#26032;&#35757;&#32451;&#30340;&#39640;&#25104;&#26412;&#65292;&#25345;&#32493;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#20010;&#38382;&#39064;&#34987;&#32570;&#20047;&#22823;&#35268;&#27169;&#36830;&#32493;&#23398;&#20064;&#22522;&#20934;&#25110;&#22522;&#32447;&#25152;&#21152;&#21095;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#31532;&#19968;&#25209; Web &#35268;&#27169;&#26102;&#38388;&#36830;&#32493;&#65288;TiC&#65289;&#22522;&#20934;&#65306;TiC-DataCompt&#12289;TiC-YFCC &#21644; TiC-RedCaps&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807; 127 &#20159;&#20010;&#26102;&#38388;&#25139;&#22270;&#20687;-&#25991;&#26412;&#23545;&#65292;&#36328;&#36234;&#20102; 9 &#24180;&#30340;&#26102;&#38388;&#65288;2014-2022&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#36825;&#20123;&#22522;&#20934;&#26469;&#31574;&#21010;&#21508;&#31181;&#21160;&#24577;&#35780;&#20272;&#65292;&#20197;&#34913;&#37327;&#29616;&#26377;&#27169;&#22411;&#30340;&#26102;&#38388;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; OpenAI &#30340; CLIP &#27169;&#22411;&#65288;&#20351;&#29992; 2020 &#24180;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65289;&#22312;&#25105;&#20204;&#31574;&#21010;&#30340;&#20174; 2021 &#24180;&#21040; 2022 &#24180;&#30340;&#26816;&#32034;&#20219;&#21153;&#20013;&#65292;&#22833;&#21435;&#20102;&#32422; 8% &#30340;&#38646;-shot&#20934;&#30830;&#29575;&#65292;&#32780;&#19982; OpenCLIP &#23384;&#20648;&#24211;&#20013;&#26368;&#36817;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#27604;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#22914;&#20309;&#39640;&#25928;&#22320;&#23545;&#26102;&#38388;&#36830;&#32493;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25490;&#32451;&#26041;&#27861;&#65292;&#20174;&#19978;&#27425;&#30340;&#35757;&#32451;&#20013;&#32487;&#32493;&#35757;&#32451;&#65292;&#21487;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Keeping large foundation models up to date on latest data is inherently expensive. To avoid the prohibitive costs of constantly retraining, it is imperative to continually train these models. This problem is exacerbated by the lack of any large scale continual learning benchmarks or baselines. We introduce the first set of web-scale Time-Continual (TiC) benchmarks for training vision-language models: TiC-DataCompt, TiC-YFCC, and TiC-RedCaps with over 12.7B timestamped image-text pairs spanning 9 years (2014--2022). We first use our benchmarks to curate various dynamic evaluations to measure temporal robustness of existing models. We show OpenAI's CLIP (trained on data up to 2020) loses $\approx 8\%$ zero-shot accuracy on our curated retrieval task from 2021--2022 compared with more recently trained models in OpenCLIP repository. We then study how to efficiently train models on time-continuous data. We demonstrate that a simple rehearsal-based approach that continues training from the l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;AutoDAN&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#22312;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#33258;&#21160;&#29983;&#25104;&#38544;&#34109;&#30340;&#36234;&#29425;&#25552;&#31034;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#36234;&#29425;&#25216;&#26415;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#38544;&#34109;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.04451</link><description>&lt;p&gt;
AutoDAN: &#22312;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#29983;&#25104;&#38544;&#34109;&#30340;&#36234;&#29425;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models. (arXiv:2310.04451v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04451
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;AutoDAN&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#22312;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#33258;&#21160;&#29983;&#25104;&#38544;&#34109;&#30340;&#36234;&#29425;&#25552;&#31034;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#36234;&#29425;&#25216;&#26415;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#38544;&#34109;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#26159;&#24378;&#22823;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#20915;&#31574;&#24037;&#20855;&#65292;&#36890;&#36807;&#19982;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#24191;&#27867;&#23545;&#40784;&#32780;&#21019;&#24314;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22823;&#22411;&#27169;&#22411;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#36234;&#29425;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#25805;&#32437;&#25552;&#31034;&#26469;&#24341;&#21457;&#23545;&#40784;&#30340;LLM&#19981;&#24212;&#32473;&#20986;&#30340;&#24694;&#24847;&#36755;&#20986;&#12290;&#30740;&#31350;&#36234;&#29425;&#25552;&#31034;&#21487;&#20197;&#35753;&#25105;&#20204;&#28145;&#20837;&#20102;&#35299;LLM&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#36827;&#19968;&#27493;&#25351;&#23548;&#25105;&#20204;&#22914;&#20309;&#20445;&#25252;&#23427;&#20204;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29616;&#26377;&#30340;&#36234;&#29425;&#25216;&#26415;&#23384;&#22312;&#20197;&#19979;&#38382;&#39064;&#65306;(1) &#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#25915;&#20987;&#22823;&#37327;&#20381;&#36182;&#25163;&#24037;&#21046;&#20316;&#25552;&#31034;&#65307;(2) &#38544;&#34109;&#24615;&#38382;&#39064;&#65292;&#25915;&#20987;&#20381;&#36182;&#22522;&#20110;&#26631;&#35760;&#30340;&#31639;&#27861;&#29983;&#25104;&#24120;&#24120;&#35821;&#20041;&#26080;&#24847;&#20041;&#30340;&#25552;&#31034;&#65292;&#23481;&#26131;&#36890;&#36807;&#22522;&#26412;&#22256;&#24785;&#24230;&#27979;&#35797;&#26816;&#27979;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24819;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65306;&#33021;&#21542;&#24320;&#21457;&#19968;&#31181;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#38544;&#34109;&#36234;&#29425;&#25552;&#31034;&#30340;&#26041;&#27861;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AutoDAN&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aligned Large Language Models (LLMs) are powerful language understanding and decision-making tools that are created through extensive alignment with human feedback. However, these large models remain susceptible to jailbreak attacks, where adversaries manipulate prompts to elicit malicious outputs that should not be given by aligned LLMs. Investigating jailbreak prompts can lead us to delve into the limitations of LLMs and further guide us to secure them. Unfortunately, existing jailbreak techniques suffer from either (1) scalability issues, where attacks heavily rely on manual crafting of prompts, or (2) stealthiness problems, as attacks depend on token-based algorithms to generate prompts that are often semantically meaningless, making them susceptible to detection through basic perplexity testing. In light of these challenges, we intend to answer this question: Can we develop an approach that can automatically generate stealthy jailbreak prompts? In this paper, we introduce Auto
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#21477;&#23376;&#32423;&#21035;&#36827;&#34892;&#35821;&#20041;&#20998;&#31867;&#65292;&#20197;&#21152;&#36895;&#20154;&#25991;&#23398;&#31185;&#21644;&#35821;&#35328;&#23398;&#39046;&#22495;&#20013;&#35821;&#26009;&#24211;&#24314;&#35774;&#30340;&#36807;&#31243;&#12290;&#32463;&#36807;&#35780;&#20272;&#65292;&#35813;&#26041;&#27861;&#22312;&#26816;&#27979;&#24615;&#20869;&#23481;&#26041;&#38754;&#34920;&#29616;&#20986;&#39640;&#31934;&#24230;&#21644;&#30495;&#38451;&#24615;&#29575;&#65292;&#24182;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#36755;&#20837;&#23884;&#20837;&#23618;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.14974</link><description>&lt;p&gt;
&#22312;&#19968;&#21315;&#24180;&#21069;&#30340;&#25289;&#19969;&#25991;&#26412;&#20013;&#26816;&#27979;&#21477;&#23376;&#32423;&#21035;&#30340;&#24615;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
Detecting Sexual Content at the Sentence Level in First Millennium Latin Texts. (arXiv:2309.14974v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14974
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#21477;&#23376;&#32423;&#21035;&#36827;&#34892;&#35821;&#20041;&#20998;&#31867;&#65292;&#20197;&#21152;&#36895;&#20154;&#25991;&#23398;&#31185;&#21644;&#35821;&#35328;&#23398;&#39046;&#22495;&#20013;&#35821;&#26009;&#24211;&#24314;&#35774;&#30340;&#36807;&#31243;&#12290;&#32463;&#36807;&#35780;&#20272;&#65292;&#35813;&#26041;&#27861;&#22312;&#26816;&#27979;&#24615;&#20869;&#23481;&#26041;&#38754;&#34920;&#29616;&#20986;&#39640;&#31934;&#24230;&#21644;&#30495;&#38451;&#24615;&#29575;&#65292;&#24182;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#36755;&#20837;&#23884;&#20837;&#23618;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#21477;&#23376;&#32423;&#21035;&#36827;&#34892;&#35821;&#20041;&#20998;&#31867;&#65292;&#20197;&#21152;&#24555;&#20154;&#25991;&#23398;&#31185;&#21644;&#35821;&#35328;&#23398;&#39046;&#22495;&#20013;&#35821;&#26009;&#24211;&#24314;&#35774;&#30340;&#36807;&#31243;&#65292;&#36825;&#26159;&#19968;&#39033;&#20256;&#32479;&#19988;&#32791;&#26102;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35821;&#26009;&#24211;&#65292;&#21253;&#25324;&#32422;2500&#20010;&#21477;&#23376;&#65292;&#28085;&#30422;&#20102;&#20174;&#20844;&#20803;&#21069;300&#24180;&#21040;&#20844;&#20803;900&#24180;&#30340;&#24615;&#35821;&#20041;&#23398;&#65288;&#21307;&#23398;&#65292;&#24773;&#33394;&#31561;&#65289;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181;&#21477;&#23376;&#20998;&#31867;&#26041;&#27861;&#21644;&#19981;&#21516;&#30340;&#36755;&#20837;&#23884;&#20837;&#23618;&#65292;&#24182;&#34920;&#26126;&#23427;&#20204;&#37117;&#27604;&#31616;&#21333;&#30340;&#22522;&#20110;&#26631;&#35760;&#30340;&#25628;&#32034;&#26041;&#27861;&#26356;&#22909;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#20010;&#20154;&#35328;&#35821;&#21644;&#31038;&#20250;&#35328;&#35821;&#20803;&#25968;&#25454;&#23884;&#20837;&#65288;&#19990;&#32426;&#65292;&#20316;&#32773;&#65292;&#20889;&#20316;&#31867;&#22411;&#65289;&#30340;&#25972;&#21512;&#65292;&#20294;&#21457;&#29616;&#36825;&#23548;&#33268;&#20102;&#36807;&#25311;&#21512;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20351;&#29992;HAN&#20998;&#21035;&#36798;&#21040;&#20102;70.60%&#30340;&#39640;&#31934;&#24230;&#21644;86.33%&#30340;&#30495;&#38451;&#24615;&#29575;&#65288;TPR&#65289;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25968;&#25454;&#38598;&#22823;&#23567;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65288;420&#32780;&#19981;&#26159;2013&#65289;&#65292;&#24182;&#26174;&#31034;&#20986;&#65292;&#23613;&#31649;&#25105;&#20204;&#30340;&#27169;&#22411;&#24615;&#33021;&#21487;&#33021;&#31245;&#26377;&#19979;&#38477;&#65292;&#20294;&#24615;&#33021;&#20173;&#28982;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we propose to evaluate the use of deep learning methods for semantic classification at the sentence level to accelerate the process of corpus building in the field of humanities and linguistics, a traditional and time-consuming task. We introduce a novel corpus comprising around 2500 sentences spanning from 300 BCE to 900 CE including sexual semantics (medical, erotica, etc.). We evaluate various sentence classification approaches and different input embedding layers, and show that all consistently outperform simple token-based searches. We explore the integration of idiolectal and sociolectal metadata embeddings (centuries, author, type of writing), but find that it leads to overfitting. Our results demonstrate the effectiveness of this approach, achieving high precision and true positive rates (TPR) of respectively 70.60% and 86.33% using HAN. We evaluate the impact of the dataset size on the model performances (420 instead of 2013), and show that, while our models per
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;SignBank+&#65292;&#36825;&#26159;&#19968;&#20010;&#32463;&#36807;&#20248;&#21270;&#30340;&#25163;&#35821;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#31616;&#21270;&#25991;&#26412;&#23545;&#25991;&#26412;&#32763;&#35793;&#26041;&#27861;&#65292;&#25552;&#21319;&#20102;&#25163;&#35821;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#24320;&#25918;&#30340;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2309.11566</link><description>&lt;p&gt;
SignBank +&#65306;&#22810;&#35821;&#31181;&#25163;&#35821;&#32763;&#35793;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SignBank+: Multilingual Sign Language Translation Dataset. (arXiv:2309.11566v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11566
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;SignBank+&#65292;&#36825;&#26159;&#19968;&#20010;&#32463;&#36807;&#20248;&#21270;&#30340;&#25163;&#35821;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#31616;&#21270;&#25991;&#26412;&#23545;&#25991;&#26412;&#32763;&#35793;&#26041;&#27861;&#65292;&#25552;&#21319;&#20102;&#25163;&#35821;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#24320;&#25918;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#20851;&#27880;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#21644;&#31616;&#21270;&#32763;&#35793;&#31995;&#32479;&#65292;&#25512;&#36827;&#25163;&#35821;&#26426;&#22120;&#32763;&#35793;&#39046;&#22495;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;SignBank+&#65292;&#36825;&#26159;SignBank&#25968;&#25454;&#38598;&#30340;&#20248;&#21270;&#29256;&#26412;&#65292;&#32463;&#36807;&#28165;&#29702;&#20197;&#36866;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#12290;&#19982;&#20197;&#24448;&#37319;&#29992;&#22797;&#26434;&#30340;&#20998;&#35299;&#25216;&#26415;&#36827;&#34892;&#32763;&#35793;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#20027;&#24352;&#37319;&#29992;&#31616;&#21270;&#30340;&#25991;&#26412;&#23545;&#25991;&#26412;&#32763;&#35793;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#22312;SignBank+&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#36229;&#36807;&#20102;&#21407;&#22987;&#25968;&#25454;&#38598;&#19978;&#30340;&#27169;&#22411;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#24320;&#25918;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work advances the field of sign language machine translation by focusing on dataset quality and simplification of the translation system. We introduce SignBank+, a clean version of the SignBank dataset, optimized for machine translation. Contrary to previous works that employ complex factorization techniques for translation, we advocate for a simplified text-to-text translation approach. Our evaluation shows that models trained on SignBank+ surpass those on the original dataset, establishing a new benchmark and providing an open resource for future research.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#35199;&#29677;&#29273;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#21508;&#31181;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#31454;&#20105;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;BART&#12289;T5&#21644;BERT2BERT-style&#27169;&#22411;&#30340;&#35199;&#29677;&#29273;&#29256;&#26412;&#12290;</title><link>http://arxiv.org/abs/2309.11259</link><description>&lt;p&gt;
&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#35199;&#29677;&#29273;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Sequence-to-Sequence Spanish Pre-trained Language Models. (arXiv:2309.11259v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11259
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#35199;&#29677;&#29273;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#21508;&#31181;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#31454;&#20105;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;BART&#12289;T5&#21644;BERT2BERT-style&#27169;&#22411;&#30340;&#35199;&#29677;&#29273;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#22823;&#36827;&#23637;&#20026;&#35768;&#22810;&#38750;&#33521;&#35821;&#35821;&#35328;&#29256;&#26412;&#30340;&#24320;&#21457;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#20854;&#20013;&#29305;&#21035;&#20851;&#27880;&#20102;&#20165;&#32534;&#30721;&#22120;&#21644;&#20165;&#35299;&#30721;&#22120;&#30340;&#26550;&#26500;&#12290;&#34429;&#28982;&#35199;&#29677;&#29273;&#35821;&#35821;&#35328;&#27169;&#22411;&#21253;&#25324;BERT&#12289;RoBERTa&#21644;GPT&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#20248;&#21183;&#65292;&#20294;&#22312;&#28041;&#21450;&#36755;&#20837;&#36755;&#20986;&#23545;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#20013;&#65292;&#32570;&#20047;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#23454;&#26045;&#21644;&#35780;&#20272;&#33879;&#21517;&#30340;&#20165;&#22312;&#35199;&#29677;&#29273;&#35821;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#24320;&#21019;&#20102;&#26032;&#30340;&#39046;&#22495;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BART&#12289;T5&#21644;BERT2BERT&#39118;&#26684;&#27169;&#22411;&#30340;&#35199;&#29677;&#29273;&#35821;&#29256;&#26412;&#65292;&#24182;&#23545;&#23427;&#20204;&#22312;&#21508;&#31181;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21253;&#25324;&#25688;&#35201;&#12289;&#37325;&#36848;&#21644;&#29983;&#25104;&#24335;&#38382;&#31572;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#25152;&#26377;&#27169;&#22411;&#30340;&#31454;&#20105;&#24615;&#33021;&#65292;&#20854;&#20013;BART&#21644;T5&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, substantial advancements in pre-trained language models have paved the way for the development of numerous non-English language versions, with a particular focus on encoder-only and decoder-only architectures. While Spanish language models encompassing BERT, RoBERTa, and GPT have exhibited prowess in natural language understanding and generation, there remains a scarcity of encoder-decoder models designed for sequence-to-sequence tasks involving input-output pairs. This paper breaks new ground by introducing the implementation and evaluation of renowned encoder-decoder architectures, exclusively pre-trained on Spanish corpora. Specifically, we present Spanish versions of BART, T5, and BERT2BERT-style models and subject them to a comprehensive assessment across a diverse range of sequence-to-sequence tasks, spanning summarization, rephrasing, and generative question answering. Our findings underscore the competitive performance of all models, with BART and T5 emerging a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;CensorChat&#65292;&#19968;&#20010;&#29992;&#20110;&#30417;&#27979;NSFW&#23545;&#35805;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#26500;&#24314;&#20102;&#39640;&#25928;&#30340;NSFW&#20869;&#23481;&#26816;&#27979;&#22120;&#12290;</title><link>http://arxiv.org/abs/2309.09749</link><description>&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#20419;&#36827;&#24320;&#25918;&#22495;&#23545;&#35805;&#31995;&#32479;&#20013;NSFW&#25991;&#26412;&#30340;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Facilitating NSFW Text Detection in Open-Domain Dialogue Systems via Knowledge Distillation. (arXiv:2309.09749v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09749
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;CensorChat&#65292;&#19968;&#20010;&#29992;&#20110;&#30417;&#27979;NSFW&#23545;&#35805;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#26500;&#24314;&#20102;&#39640;&#25928;&#30340;NSFW&#20869;&#23481;&#26816;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#25918;&#22495;&#23545;&#35805;&#31995;&#32479;&#20013;&#65292;&#23545;&#35805;&#20013;&#30340;NSFW&#65288;&#19981;&#36866;&#21512;&#19978;&#29677;&#65289;&#20869;&#23481;&#21487;&#33021;&#23545;&#29992;&#25143;&#20135;&#29983;&#20005;&#37325;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#22312;&#23545;&#35805;&#19978;&#19979;&#25991;&#20013;&#26816;&#27979;NSFW&#35821;&#35328;&#65292;&#23588;&#20854;&#26159;&#24615;&#29233;&#20869;&#23481;&#26041;&#38754;&#30340;&#30740;&#31350;&#26126;&#26174;&#28382;&#21518;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CensorChat&#65292;&#19968;&#20010;&#26088;&#22312;&#26816;&#27979;NSFW&#23545;&#35805;&#30340;&#23545;&#35805;&#30417;&#25511;&#25968;&#25454;&#38598;&#12290;&#21033;&#29992;&#28041;&#21450;GPT-4&#21644;ChatGPT&#30340;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#65292;&#35813;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#19968;&#31181;&#32463;&#27982;&#39640;&#25928;&#30340;&#26500;&#24314;NSFW&#20869;&#23481;&#26816;&#27979;&#22120;&#30340;&#26041;&#27861;&#12290;&#35813;&#36807;&#31243;&#28041;&#21450;&#25910;&#38598;&#30495;&#23454;&#30340;&#20154;&#26426;&#20132;&#20114;&#25968;&#25454;&#65292;&#24182;&#23558;&#20854;&#20998;&#35299;&#20026;&#21333;&#20010;&#35805;&#35821;&#21644;&#21333;&#36718;&#23545;&#35805;&#65292;&#20854;&#20013;&#32842;&#22825;&#26426;&#22120;&#20154;&#25552;&#20379;&#26368;&#21518;&#19968;&#21477;&#35805;&#12290;&#20351;&#29992;ChatGPT&#23545;&#26080;&#26631;&#31614;&#25968;&#25454;&#36827;&#34892;&#27880;&#37322;&#65292;&#20316;&#20026;&#35757;&#32451;&#38598;&#12290;&#20351;&#29992;ChatGPT&#21644;GPT-4&#20316;&#20026;&#27880;&#37322;&#22120;&#26500;&#24314;&#20102;&#21512;&#29702;&#24615;&#39564;&#35777;&#38598;&#21644;&#27979;&#35797;&#38598;&#65292;&#24182;&#20351;&#29992;&#33258;&#25105;&#25209;&#35780;&#31574;&#30053;&#35299;&#20915;&#26631;&#35760;&#20013;&#30340;&#24046;&#24322;&#12290;BERT&#27169;&#22411;&#29992;&#20110;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
NSFW (Not Safe for Work) content, in the context of a dialogue, can have severe side effects on users in open-domain dialogue systems. However, research on detecting NSFW language, especially sexually explicit content, within a dialogue context has significantly lagged behind. To address this issue, we introduce CensorChat, a dialogue monitoring dataset aimed at NSFW dialogue detection. Leveraging knowledge distillation techniques involving GPT-4 and ChatGPT, this dataset offers a cost-effective means of constructing NSFW content detectors. The process entails collecting real-life human-machine interaction data and breaking it down into single utterances and single-turn dialogues, with the chatbot delivering the final utterance. ChatGPT is employed to annotate unlabeled data, serving as a training set. Rationale validation and test sets are constructed using ChatGPT and GPT-4 as annotators, with a self-criticism strategy for resolving discrepancies in labeling. A BERT model is fine-tun
&lt;/p&gt;</description></item><item><title>RoDia&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#32599;&#39532;&#23612;&#20122;&#26041;&#35328;&#35782;&#21035;&#30340;&#35821;&#38899;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#26469;&#33258;&#20116;&#20010;&#19981;&#21516;&#22320;&#21306;&#30340;2&#23567;&#26102;&#25163;&#21160;&#26631;&#27880;&#25968;&#25454;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#32452;&#31454;&#20105;&#27169;&#22411;&#20316;&#20026;&#26410;&#26469;&#30740;&#31350;&#30340;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2309.03378</link><description>&lt;p&gt;
RoDia: &#19968;&#20221;&#29992;&#20110;&#32599;&#39532;&#23612;&#20122;&#26041;&#35328;&#35782;&#21035;&#30340;&#26032;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
RoDia: A New Dataset for Romanian Dialect Identification from Speech. (arXiv:2309.03378v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03378
&lt;/p&gt;
&lt;p&gt;
RoDia&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#32599;&#39532;&#23612;&#20122;&#26041;&#35328;&#35782;&#21035;&#30340;&#35821;&#38899;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#26469;&#33258;&#20116;&#20010;&#19981;&#21516;&#22320;&#21306;&#30340;2&#23567;&#26102;&#25163;&#21160;&#26631;&#27880;&#25968;&#25454;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#32452;&#31454;&#20105;&#27169;&#22411;&#20316;&#20026;&#26410;&#26469;&#30740;&#31350;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26041;&#35328;&#35782;&#21035;&#26159;&#35821;&#38899;&#22788;&#29702;&#21644;&#35821;&#35328;&#25216;&#26415;&#20013;&#20851;&#38190;&#30340;&#20219;&#21153;&#65292;&#21487;&#20197;&#22686;&#24378;&#35832;&#22914;&#35821;&#38899;&#35782;&#21035;&#12289;&#35828;&#35805;&#20154;&#39564;&#35777;&#31561;&#21508;&#31181;&#24212;&#29992;&#12290;&#23613;&#31649;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#24191;&#20026;&#20351;&#29992;&#30340;&#35821;&#35328;&#30340;&#26041;&#35328;&#35782;&#21035;&#19978;&#65292;&#20294;&#23545;&#20110;&#32599;&#39532;&#23612;&#20122;&#36825;&#31181;&#36164;&#28304;&#26377;&#38480;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#26041;&#35328;&#35782;&#21035;&#21364;&#27809;&#26377;&#24471;&#21040;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;RoDia&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#32599;&#39532;&#23612;&#20122;&#26041;&#35328;&#35782;&#21035;&#30340;&#35821;&#38899;&#25968;&#25454;&#38598;&#12290;RoDia&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#26469;&#33258;&#32599;&#39532;&#23612;&#20122;&#20116;&#20010;&#19981;&#21516;&#22320;&#21306;&#30340;&#21508;&#31181;&#35821;&#38899;&#26679;&#26412;&#65292;&#28085;&#30422;&#20102;&#22478;&#24066;&#21644;&#20892;&#26449;&#29615;&#22659;&#65292;&#24635;&#20849;&#26377;2&#23567;&#26102;&#30340;&#25163;&#21160;&#26631;&#27880;&#35821;&#38899;&#25968;&#25454;&#12290;&#38500;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#32452;&#21487;&#20316;&#20026;&#26410;&#26469;&#30740;&#31350;&#22522;&#20934;&#30340;&#31454;&#20105;&#27169;&#22411;&#12290;&#26368;&#39640;&#24471;&#20998;&#30340;&#27169;&#22411;&#30340;&#23439;F1&#20998;&#25968;&#20026;59.83%&#65292;&#24494;F1&#20998;&#25968;&#20026;62.08%&#65292;&#35828;&#26126;&#35813;&#20219;&#21153;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35748;&#20026;RoDia&#26159;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialect identification is a critical task in speech processing and language technology, enhancing various applications such as speech recognition, speaker verification, and many others. While most research studies have been dedicated to dialect identification in widely spoken languages, limited attention has been given to dialect identification in low-resource languages, such as Romanian. To address this research gap, we introduce RoDia, the first dataset for Romanian dialect identification from speech. The RoDia dataset includes a varied compilation of speech samples from five distinct regions of Romania, covering both urban and rural environments, totaling 2 hours of manually annotated speech data. Along with our dataset, we introduce a set of competitive models to be used as baselines for future research. The top scoring model achieves a macro F1 score of 59.83% and a micro F1 score of 62.08%, indicating that the task is challenging. We thus believe that RoDia is a valuable resource
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21015;&#34920;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#31895;&#21040;&#32454;&#31070;&#32463;&#26816;&#32034;&#22120;&#26469;&#37325;&#26032;&#25490;&#24207;&#27573;&#33853;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#20854;&#20182;&#20505;&#36873;&#21477;&#23376;&#30340;&#21015;&#34920;&#19978;&#19979;&#25991;&#20449;&#24687;&#32435;&#20837;&#27573;&#33853;&#34920;&#31034;&#20013;&#65292;&#22686;&#24378;&#20102;&#27573;&#33853;&#34920;&#31034;&#12290;&#32780;&#19988;&#65292;&#35813;&#26041;&#27861;&#23558;&#21015;&#34920;&#19978;&#19979;&#25991;&#24314;&#27169;&#36807;&#31243;&#20998;&#20026;&#20004;&#20010;&#23376;&#36807;&#31243;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#27573;&#33853;&#27880;&#24847;&#26426;&#21046;&#30340;&#20869;&#23384;&#38480;&#21046;&#38382;&#39064;&#65292;&#20801;&#35768;&#39640;&#25928;&#32534;&#30721;&#22823;&#37327;&#20505;&#36873;&#31572;&#26696;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.12022</link><description>&lt;p&gt;
&#21033;&#29992;&#21015;&#34920;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#31895;&#21040;&#32454;&#31070;&#32463;&#26816;&#32034;&#22120;&#23545;&#27573;&#33853;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Reranking Passages with Coarse-to-Fine Neural Retriever using List-Context Information. (arXiv:2308.12022v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21015;&#34920;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#31895;&#21040;&#32454;&#31070;&#32463;&#26816;&#32034;&#22120;&#26469;&#37325;&#26032;&#25490;&#24207;&#27573;&#33853;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#20854;&#20182;&#20505;&#36873;&#21477;&#23376;&#30340;&#21015;&#34920;&#19978;&#19979;&#25991;&#20449;&#24687;&#32435;&#20837;&#27573;&#33853;&#34920;&#31034;&#20013;&#65292;&#22686;&#24378;&#20102;&#27573;&#33853;&#34920;&#31034;&#12290;&#32780;&#19988;&#65292;&#35813;&#26041;&#27861;&#23558;&#21015;&#34920;&#19978;&#19979;&#25991;&#24314;&#27169;&#36807;&#31243;&#20998;&#20026;&#20004;&#20010;&#23376;&#36807;&#31243;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#27573;&#33853;&#27880;&#24847;&#26426;&#21046;&#30340;&#20869;&#23384;&#38480;&#21046;&#38382;&#39064;&#65292;&#20801;&#35768;&#39640;&#25928;&#32534;&#30721;&#22823;&#37327;&#20505;&#36873;&#31572;&#26696;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27573;&#33853;&#37325;&#26032;&#25490;&#24207;&#26159;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#25991;&#26723;&#26102;&#12290;&#20256;&#32479;&#30340;&#31070;&#32463;&#26550;&#26500;&#22312;&#20026;&#38382;&#39064;&#26816;&#32034;&#26368;&#20339;&#27573;&#33853;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#23558;&#38382;&#39064;&#19982;&#27599;&#20010;&#27573;&#33853;&#20998;&#24320;&#21305;&#37197;&#65292;&#24456;&#23569;&#32771;&#34385;&#20854;&#20182;&#27573;&#33853;&#20013;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#32780;&#36825;&#20123;&#20449;&#24687;&#21487;&#20197;&#25552;&#20379;&#27604;&#36739;&#21644;&#21442;&#32771;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21015;&#34920;&#19978;&#19979;&#25991;&#27880;&#24847;&#26426;&#21046;&#65292;&#36890;&#36807;&#23558;&#26469;&#33258;&#20854;&#20182;&#20505;&#36873;&#21477;&#23376;&#30340;&#21015;&#34920;&#19978;&#19979;&#25991;&#20449;&#24687;&#32435;&#20837;&#27573;&#33853;&#34920;&#31034;&#20013;&#26469;&#22686;&#24378;&#27573;&#33853;&#34920;&#31034;&#12290;&#25152;&#25552;&#20986;&#30340;&#31895;&#21040;&#32454;&#65288;C2F&#65289;&#31070;&#32463;&#26816;&#32034;&#22120;&#36890;&#36807;&#23558;&#21015;&#34920;&#19978;&#19979;&#25991;&#24314;&#27169;&#36807;&#31243;&#20998;&#20026;&#20004;&#20010;&#23376;&#36807;&#31243;&#26469;&#35299;&#20915;&#27573;&#33853;&#27880;&#24847;&#26426;&#21046;&#30340;&#20869;&#23384;&#38480;&#21046;&#38382;&#39064;&#65292;&#20174;&#32780;&#20801;&#35768;&#23545;&#22823;&#37327;&#20505;&#36873;&#31572;&#26696;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#36827;&#34892;&#39640;&#25928;&#32534;&#30721;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#24191;&#27867;&#29992;&#20110;&#19968;&#27425;&#24615;&#32534;&#30721;&#20219;&#24847;&#25968;&#37327;&#30340;&#20505;&#36873;&#31572;&#26696;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Passage reranking is a crucial task in many applications, particularly when dealing with large-scale documents. Traditional neural architectures are limited in retrieving the best passage for a question because they usually match the question to each passage separately, seldom considering contextual information in other passages that can provide comparison and reference information. This paper presents a list-context attention mechanism to augment the passage representation by incorporating the list-context information from other candidates. The proposed coarse-to-fine (C2F) neural retriever addresses the out-of-memory limitation of the passage attention mechanism by dividing the list-context modeling process into two sub-processes, allowing for efficient encoding of context information from a large number of candidate answers. This method can be generally used to encode context information from any number of candidate answers in one pass. Different from most multi-stage information re
&lt;/p&gt;</description></item><item><title>&#20803;&#35748;&#30693;&#25552;&#31034; (MP) &#26159;&#19968;&#31181;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#29702;&#35299;&#33021;&#21147;&#30340;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;MP&#30340;PaLM&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#25509;&#36817;&#20110;GPT-4&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2308.05342</link><description>&lt;p&gt;
&#20803;&#35748;&#30693;&#25552;&#31034;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Metacognitive Prompting Improves Understanding in Large Language Models. (arXiv:2308.05342v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05342
&lt;/p&gt;
&lt;p&gt;
&#20803;&#35748;&#30693;&#25552;&#31034; (MP) &#26159;&#19968;&#31181;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#29702;&#35299;&#33021;&#21147;&#30340;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;MP&#30340;PaLM&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#25509;&#36817;&#20110;GPT-4&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#20013;&#65292;&#36890;&#36807;&#26377;&#25928;&#30340;&#25552;&#31034;&#35774;&#35745;&#65292;&#20219;&#21153;&#29305;&#23450;&#24615;&#33021;&#19968;&#30452;&#22312;&#19981;&#26029;&#25552;&#39640;&#12290;&#23613;&#31649;&#26368;&#36817;&#20851;&#20110;&#25552;&#31034;&#30340;&#30740;&#31350;&#22686;&#24378;&#20102;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#22312;&#36827;&#19968;&#27493;&#25552;&#39640;&#23427;&#20204;&#30340;&#29702;&#35299;&#33021;&#21147;&#26041;&#38754;&#20173;&#23384;&#22312;&#24046;&#36317;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20803;&#35748;&#30693;&#25552;&#31034; (MP)&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#20154;&#31867;&#20869;&#30465;&#25512;&#29702;&#36807;&#31243;&#21551;&#21457;&#30340;&#31574;&#30053;&#12290;&#20351;&#29992;MP&#65292;LLMs&#32463;&#21382;&#19968;&#31995;&#21015;&#26377;&#32467;&#26500;&#12289;&#33258;&#25105;&#24847;&#35782;&#30340;&#35780;&#20272;&#65292;&#21033;&#29992;&#20854;&#20016;&#23500;&#30340;&#20869;&#22312;&#30693;&#35782;&#21644;&#26032;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#28041;&#21450;&#20116;&#20010;&#24120;&#35265;&#30340;LLMs&#65306;Llama2&#12289;Vicuna&#12289;PaLM&#12289;GPT-3.5&#21644;GPT-4&#65292;&#23427;&#20204;&#37117;&#28085;&#30422;&#20102;&#26469;&#33258;GLUE&#21644;SuperGLUE&#22522;&#20934;&#27979;&#35797;&#30340;&#21508;&#31181;&#36890;&#29992;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299; (NLU) &#20219;&#21153;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;GPT-4&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#22987;&#32456;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#37197;&#22791;MP&#30340;PaLM&#25509;&#36817;&#20854;&#24615;&#33021;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#36328;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#65292;MP&#22987;&#32456;&#20248;&#20110;&#29616;&#26377;&#30340;&#25552;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Large Language Models (LLMs), there have been consistent advancements in task-specific performance, largely influenced by effective prompt design. While recent research on prompting has enhanced the reasoning capabilities of LLMs, a gap remains in further improving their understanding abilities. In this study, we introduce metacognitive prompting (MP), a strategy inspired by human introspective reasoning processes. Using MP, LLMs undergo a systematic series of structured, self-aware evaluations, drawing on both their vast inherent knowledge and new insights. Our experiments involve five prevalent LLMs: Llama2, Vicuna, PaLM, GPT-3.5, and GPT-4, all of which span various general natural language understanding (NLU) tasks from the GLUE and SuperGLUE benchmarks. Results indicate that, although GPT-4 consistently excels in most tasks, PaLM, when equipped with MP, approaches its performance level. Furthermore, across models and datasets, MP consistently outperforms existing prompting meth
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35760;&#24518;&#22686;&#24378;&#30340;&#36866;&#37197;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25554;&#25300;&#30340;&#26041;&#27861;&#26469;&#25511;&#21046;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#29983;&#25104;&#34892;&#20026;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#32988;&#36807;&#20960;&#20010;&#20195;&#34920;&#24615;&#30340;&#21487;&#25554;&#25300;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.06029</link><description>&lt;p&gt;
&#36890;&#36807;&#35760;&#24518;&#22686;&#24378;&#30340;&#36866;&#37197;&#22120;&#23454;&#29616;&#21487;&#25554;&#25300;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Pluggable Neural Machine Translation Models via Memory-augmented Adapters. (arXiv:2307.06029v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06029
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35760;&#24518;&#22686;&#24378;&#30340;&#36866;&#37197;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25554;&#25300;&#30340;&#26041;&#27861;&#26469;&#25511;&#21046;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#29983;&#25104;&#34892;&#20026;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#32988;&#36807;&#20960;&#20010;&#20195;&#34920;&#24615;&#30340;&#21487;&#25554;&#25300;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#27169;&#22411;&#22312;&#26222;&#36890;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26159;&#25511;&#21046;&#20854;&#29983;&#25104;&#34892;&#20026;&#20197;&#28385;&#36275;&#19981;&#21516;&#29992;&#25143;&#38656;&#27714;&#20173;&#28982;&#20855;&#26377;&#19968;&#23450;&#25361;&#25112;&#24615;&#12290;&#37492;&#20110;&#27599;&#20010;&#29992;&#25143;&#38656;&#27714;&#37117;&#38656;&#35201;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#26032;&#27169;&#22411;&#30340;&#39640;&#26114;&#35757;&#32451;&#25104;&#26412;&#21644;&#25968;&#25454;&#31232;&#32570;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35760;&#24518;&#22686;&#24378;&#36866;&#37197;&#22120;&#65292;&#20197;&#21487;&#25554;&#25300;&#30340;&#26041;&#24335;&#24341;&#23548;&#39044;&#35757;&#32451;&#30340;NMT&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22522;&#20110;&#29992;&#25143;&#25552;&#20379;&#30340;&#25991;&#26412;&#26679;&#26412;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#31890;&#24230;&#35760;&#24518;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#37197;&#22120;&#26550;&#26500;&#26469;&#32467;&#21512;&#27169;&#22411;&#34920;&#31034;&#21644;&#26816;&#32034;&#32467;&#26524;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35760;&#24518;&#20002;&#24323;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#20943;&#23569;NMT&#27169;&#22411;&#21644;&#35760;&#24518;&#20043;&#38388;&#30340;&#34394;&#20551;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;&#39118;&#26684;&#21644;&#39046;&#22495;&#29305;&#23450;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#32988;&#36807;&#20960;&#20010;&#20195;&#34920;&#24615;&#30340;&#21487;&#25554;&#25300;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although neural machine translation (NMT) models perform well in the general domain, it remains rather challenging to control their generation behavior to satisfy the requirement of different users. Given the expensive training cost and the data scarcity challenge of learning a new model from scratch for each user requirement, we propose a memory-augmented adapter to steer pretrained NMT models in a pluggable manner. Specifically, we construct a multi-granular memory based on the user-provided text samples and propose a new adapter architecture to combine the model representations and the retrieved results. We also propose a training strategy using memory dropout to reduce spurious dependencies between the NMT model and the memory. We validate our approach on both style- and domain-specific experiments and the results indicate that our method can outperform several representative pluggable baselines.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Gamma&#38899;&#22270;&#34920;&#31034;&#35821;&#38899;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20102;&#35821;&#38899;&#35782;&#21035;&#12289;&#35828;&#35805;&#20154;&#35782;&#21035;&#21644;&#21487;&#29702;&#35299;&#24615;&#35780;&#20272;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.03296</link><description>&lt;p&gt;
&#29992;&#20110;&#31471;&#21040;&#31471;&#21151;&#33021;&#24615;&#35328;&#35821;&#22788;&#29702;&#20219;&#21153;&#30340;Gamma&#38899;&#22270;&#34920;&#31034;&#65306;&#35821;&#38899;&#35782;&#21035;&#12289;&#35828;&#35805;&#20154;&#35782;&#21035;&#21644;&#21487;&#29702;&#35299;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Gammatonegram Representation for End-to-End Dysarthric Speech Processing Tasks: Speech Recognition, Speaker Identification, and Intelligibility Assessment. (arXiv:2307.03296v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03296
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Gamma&#38899;&#22270;&#34920;&#31034;&#35821;&#38899;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20102;&#35821;&#38899;&#35782;&#21035;&#12289;&#35828;&#35805;&#20154;&#35782;&#21035;&#21644;&#21487;&#29702;&#35299;&#24615;&#35780;&#20272;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#38899;&#38556;&#30861;&#26159;&#19968;&#31181;&#24433;&#21709;&#20154;&#31867;&#35328;&#35821;&#31995;&#32479;&#24182;&#38477;&#20302;&#20010;&#20154;&#21457;&#38899;&#36136;&#37327;&#21644;&#21487;&#29702;&#35299;&#24615;&#30340;&#27531;&#30142;&#12290;&#30001;&#20110;&#36825;&#31181;&#24433;&#21709;&#65292;&#24120;&#35268;&#30340;&#35328;&#35821;&#22788;&#29702;&#31995;&#32479;&#26080;&#27861;&#22312;&#21463;&#25439;&#30340;&#35328;&#35821;&#19978;&#27491;&#24120;&#24037;&#20316;&#12290;&#36825;&#31181;&#27531;&#30142;&#36890;&#24120;&#19982;&#36523;&#20307;&#27531;&#30142;&#30456;&#20851;&#12290;&#22240;&#27492;&#65292;&#35774;&#35745;&#19968;&#20010;&#33021;&#22815;&#36890;&#36807;&#25509;&#25910;&#35821;&#38899;&#21629;&#20196;&#22312;&#26234;&#33021;&#23478;&#23621;&#20013;&#25191;&#34892;&#19968;&#20123;&#20219;&#21153;&#30340;&#31995;&#32479;&#65292;&#23558;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25104;&#23601;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Gamma&#38899;&#22270;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#34920;&#31034;&#20855;&#26377;&#21306;&#20998;&#24615;&#32454;&#33410;&#30340;&#38899;&#39057;&#25991;&#20214;&#65292;&#35813;&#26041;&#27861;&#29992;&#20316;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#25105;&#20204;&#23558;&#27599;&#20010;&#35821;&#38899;&#25991;&#20214;&#36716;&#25442;&#25104;&#22270;&#20687;&#65292;&#24182;&#25552;&#20986;&#20102;&#22270;&#20687;&#35782;&#21035;&#31995;&#32479;&#26469;&#23545;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#35821;&#38899;&#36827;&#34892;&#20998;&#31867;&#12290;&#25152;&#25552;&#20986;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;Alexnet&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#31995;&#32479;&#22312;&#35821;&#38899;&#35782;&#21035;&#12289;&#35828;&#35805;&#20154;&#35782;&#21035;&#21644;&#21487;&#29702;&#35299;&#24615;&#35780;&#20272;&#26041;&#38754;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dysarthria is a disability that causes a disturbance in the human speech system and reduces the quality and intelligibility of a person's speech. Because of this effect, the normal speech processing systems can not work properly on impaired speech. This disability is usually associated with physical disabilities. Therefore, designing a system that can perform some tasks by receiving voice commands in the smart home can be a significant achievement. In this work, we introduce gammatonegram as an effective method to represent audio files with discriminative details, which is used as input for the convolutional neural network. On the other word, we convert each speech file into an image and propose image recognition system to classify speech in different scenarios. Proposed CNN is based on the transfer learning method on the pre-trained Alexnet. In this research, the efficiency of the proposed system for speech recognition, speaker identification, and intelligibility assessment is evaluat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#20030;&#21150;&#22312;2023 IEEE&#28216;&#25103;&#20250;&#35758;&#19978;&#30340;&#31532;&#19968;&#23626;ChatGPT4PCG&#27604;&#36187;&#65292;&#30446;&#26631;&#26159;&#35753;ChatGPT&#29983;&#25104;&#20855;&#26377;&#39640;&#31283;&#23450;&#24615;&#21644;&#31867;&#20284;&#35282;&#33394;&#30340;&#29305;&#36136;&#26469;&#29983;&#25104;&#20855;&#26377;&#31185;&#23398;&#40479;&#35282;&#33394;&#32423;&#27700;&#24179;&#30340;&#20851;&#21345;&#12290;</title><link>http://arxiv.org/abs/2303.15662</link><description>&lt;p&gt;
ChatGPT4PCG&#27604;&#36187;&#65306;&#31185;&#23398;&#40479;&#35282;&#33394;&#32423;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
ChatGPT4PCG Competition: Character-like Level Generation for Science Birds. (arXiv:2303.15662v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15662
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#20030;&#21150;&#22312;2023 IEEE&#28216;&#25103;&#20250;&#35758;&#19978;&#30340;&#31532;&#19968;&#23626;ChatGPT4PCG&#27604;&#36187;&#65292;&#30446;&#26631;&#26159;&#35753;ChatGPT&#29983;&#25104;&#20855;&#26377;&#39640;&#31283;&#23450;&#24615;&#21644;&#31867;&#20284;&#35282;&#33394;&#30340;&#29305;&#36136;&#26469;&#29983;&#25104;&#20855;&#26377;&#31185;&#23398;&#40479;&#35282;&#33394;&#32423;&#27700;&#24179;&#30340;&#20851;&#21345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;2023&#24180;IEEE&#28216;&#25103;&#20250;&#35758;&#19978;&#30340;&#31532;&#19968;&#23626;ChatGPT4PCG&#27604;&#36187;&#12290;&#26412;&#27425;&#27604;&#36187;&#30340;&#30446;&#26631;&#26159;&#35753;&#21442;&#36187;&#32773;&#36890;&#36807;&#21019;&#36896;&#24615;&#21644;&#25552;&#31034;&#24037;&#31243;&#25216;&#33021;&#65292;&#20026;ChatGPT&#21019;&#24314;&#26377;&#25928;&#30340;&#25552;&#31034;&#65292;&#20351;&#20854;&#33021;&#22815;&#20855;&#26377;&#39640;&#31283;&#23450;&#24615;&#21644;&#31867;&#20284;&#35282;&#33394;&#30340;&#29305;&#36136;&#26469;&#29983;&#25104;&#20855;&#26377;&#31185;&#23398;&#40479;&#35282;&#33394;&#32423;&#27700;&#24179;&#30340;&#20851;&#21345;&#12290;&#20026;&#20102;&#38477;&#20302;&#21442;&#36187;&#38376;&#27099;&#65292;&#25105;&#20204;&#23558;&#20219;&#21153;&#38480;&#21046;&#22312;&#29983;&#25104;&#22823;&#20889;&#33521;&#25991;&#23383;&#27597;&#12290;&#21442;&#36187;&#20316;&#21697;&#30340;&#36136;&#37327;&#30001;&#20854;&#31283;&#23450;&#24615;&#21644;&#19982;&#32473;&#23450;&#23383;&#31526;&#30340;&#30456;&#20284;&#24615;&#20915;&#23450;&#12290;&#32473;&#21442;&#36187;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#26679;&#20363;&#25552;&#31034;&#20379;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the first ChatGPT4PCG Competition at the 2023 IEEE Conference on Games. The objective of this competition is for participants to create effective prompts for ChatGPT--enabling it to generate Science Birds levels with high stability and character-like qualities--fully using their creativity as well as prompt engineering skills. ChatGPT is a conversational agent developed by OpenAI. Science Birds is selected as the competition platform because designing an Angry Birds-like level is not a trivial task due to the in-game gravity; the playability of the levels is determined by their stability. To lower the entry barrier to the competition, we limit the task to the generation of capitalized English alphabetical characters. Here, the quality of the generated levels is determined by their stability and similarity to the given characters. A sample prompt is provided to participants for their reference. An experiment is conducted to determine the effectiveness of its modified
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#35757;&#32451;&#30340;&#32452;&#21512;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#27169;&#22411; ComCLIP&#65292;&#36890;&#36807;&#23558;&#36755;&#20837;&#22270;&#20687;&#20998;&#35299;&#20026;&#20027;&#20307;&#12289;&#23545;&#35937;&#21644;&#21160;&#20316;&#23376;&#22270;&#20687;&#65292;&#24182;&#32467;&#21512;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#25991;&#26412;&#32534;&#30721;&#22120;&#36827;&#34892;&#36880;&#27493;&#21305;&#37197;&#65292;&#20197;&#35299;&#20915;&#32452;&#21512;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#20013;&#30340;&#20266;&#21305;&#37197;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.13854</link><description>&lt;p&gt;
ComCLIP: &#26080;&#38656;&#35757;&#32451;&#30340;&#32452;&#21512;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
ComCLIP: Training-Free Compositional Image and Text Matching. (arXiv:2211.13854v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#35757;&#32451;&#30340;&#32452;&#21512;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#27169;&#22411; ComCLIP&#65292;&#36890;&#36807;&#23558;&#36755;&#20837;&#22270;&#20687;&#20998;&#35299;&#20026;&#20027;&#20307;&#12289;&#23545;&#35937;&#21644;&#21160;&#20316;&#23376;&#22270;&#20687;&#65292;&#24182;&#32467;&#21512;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#25991;&#26412;&#32534;&#30721;&#22120;&#36827;&#34892;&#36880;&#27493;&#21305;&#37197;&#65292;&#20197;&#35299;&#20915;&#32452;&#21512;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#20013;&#30340;&#20266;&#21305;&#37197;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#26041;&#38754;&#30340;&#24456;&#22909;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23558; CLIP &#36825;&#26679;&#30340;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#20110;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#32452;&#21512;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#36825;&#38656;&#35201;&#27169;&#22411;&#29702;&#35299;&#32452;&#21512;&#35789;&#27010;&#24565;&#21644;&#35270;&#35273;&#32452;&#20214;&#12290;&#20026;&#20102;&#23454;&#29616;&#26356;&#22909;&#30340;&#38646;&#26679;&#26412;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#20013;&#30340;&#32452;&#21512;&#27867;&#21270;&#33021;&#21147;&#65292;&#26412;&#25991;&#20174;&#22240;&#26524;&#20851;&#31995;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#35813;&#38382;&#39064;&#65306;&#21333;&#20010;&#23454;&#20307;&#30340;&#38169;&#35823;&#35821;&#20041;&#26412;&#36136;&#19978;&#26159;&#23548;&#33268;&#21305;&#37197;&#22833;&#36133;&#30340;&#28151;&#28102;&#22240;&#32032;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#26080;&#38656;&#35757;&#32451;&#8221;&#30340;&#32452;&#21512; CLIP &#27169;&#22411;&#65288;ComCLIP&#65289;&#12290;ComCLIP&#23558;&#36755;&#20837;&#22270;&#20687;&#20998;&#35299;&#20026;&#20027;&#20307;&#12289;&#23545;&#35937;&#21644;&#21160;&#20316;&#23376;&#22270;&#20687;&#65292;&#24182;&#32452;&#21512; CLIP &#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#20197;&#22312;&#32452;&#21512;&#25991;&#26412;&#23884;&#20837;&#21644;&#23376;&#22270;&#20687;&#23884;&#20837;&#20043;&#19978;&#36827;&#34892;&#36880;&#27493;&#21305;&#37197;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;ComCLIP &#21487;&#20197;&#20943;&#36731;&#20266;&#21305;&#37197;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive Language-Image Pretraining (CLIP) has demonstrated great zero-shot performance for matching images and text. However, it is still challenging to adapt vision-lanaguage pretrained models like CLIP to compositional image and text matching -- a more challenging image and text matching task requiring the model understanding of compositional word concepts and visual components. Towards better compositional generalization in zero-shot image and text matching, in this paper, we study the problem from a causal perspective: the erroneous semantics of individual entities are essentially confounders that cause the matching failure. Therefore, we propose a novel \textbf{\textit{training-free}} compositional CLIP model (ComCLIP). ComCLIP disentangles input images into subjects, objects, and action sub-images and composes CLIP's vision encoder and text encoder to perform evolving matching over compositional text embedding and sub-image embeddings. In this way, ComCLIP can mitigate spurio
&lt;/p&gt;</description></item></channel></rss>