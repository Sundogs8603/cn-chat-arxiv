<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;In-context Autoencoder (ICAE)&#30340;&#19978;&#19979;&#25991;&#33258;&#32534;&#30721;&#22120;&#65292;&#23427;&#36890;&#36807;&#23558;&#38271;&#19978;&#19979;&#25991;&#21387;&#32553;&#20026;&#26377;&#38480;&#25968;&#37327;&#30340;&#20869;&#23384;&#27133;&#65292;&#23454;&#29616;&#20102;$4\times$&#30340;&#19978;&#19979;&#25991;&#21387;&#32553;&#65292;&#24182;&#33021;&#22815;&#26681;&#25454;&#20869;&#23384;&#27133;&#36827;&#34892;&#26465;&#20214;&#22788;&#29702;&#20197;&#21709;&#24212;&#21508;&#31181;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2307.06945</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19978;&#19979;&#25991;&#21387;&#32553;&#30340;&#19978;&#19979;&#25991;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
In-context Autoencoder for Context Compression in a Large Language Model. (arXiv:2307.06945v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06945
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;In-context Autoencoder (ICAE)&#30340;&#19978;&#19979;&#25991;&#33258;&#32534;&#30721;&#22120;&#65292;&#23427;&#36890;&#36807;&#23558;&#38271;&#19978;&#19979;&#25991;&#21387;&#32553;&#20026;&#26377;&#38480;&#25968;&#37327;&#30340;&#20869;&#23384;&#27133;&#65292;&#23454;&#29616;&#20102;$4\times$&#30340;&#19978;&#19979;&#25991;&#21387;&#32553;&#65292;&#24182;&#33021;&#22815;&#26681;&#25454;&#20869;&#23384;&#27133;&#36827;&#34892;&#26465;&#20214;&#22788;&#29702;&#20197;&#21709;&#24212;&#21508;&#31181;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#21387;&#32553;&#30340;&#19978;&#19979;&#25991;&#33258;&#32534;&#30721;&#22120;&#65288;ICAE&#65289;&#12290; ICAE&#26377;&#20004;&#20010;&#27169;&#22359;&#65306;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#20174;LLM&#20013;&#37319;&#29992;LoRA&#26041;&#24335;&#23558;&#38271;&#19978;&#19979;&#25991;&#21387;&#32553;&#20026;&#26377;&#38480;&#25968;&#37327;&#30340;&#20869;&#23384;&#27133;&#65292;&#20197;&#21450;&#19968;&#20010;&#22266;&#23450;&#30340;&#35299;&#30721;&#22120;&#65292;&#20316;&#20026;&#30446;&#26631;LLM&#65292;&#21487;&#20197;&#26681;&#25454;&#20869;&#23384;&#27133;&#26469;&#36827;&#34892;&#21508;&#31181;&#30446;&#30340;&#30340;&#26465;&#20214;&#22788;&#29702;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#33258;&#32534;&#30721;&#21644;&#35821;&#35328;&#24314;&#27169;&#30446;&#26631;&#22312;&#22823;&#35268;&#27169;&#25991;&#26412;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;ICAE&#65292;&#20351;&#20854;&#33021;&#22815;&#29983;&#25104;&#20934;&#30830;&#21644;&#20840;&#38754;&#34920;&#31034;&#21407;&#22987;&#19978;&#19979;&#25991;&#30340;&#20869;&#23384;&#27133;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#23569;&#37327;&#25351;&#23548;&#25968;&#25454;&#23545;&#39044;&#35757;&#32451;&#30340;ICAE&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#22686;&#24378;&#20854;&#19982;&#21508;&#31181;&#25552;&#31034;&#30340;&#20132;&#20114;&#65292;&#20174;&#32780;&#20135;&#29983;&#29702;&#24819;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#33539;&#24335;&#23398;&#20064;&#30340;ICAE&#21487;&#20197;&#26377;&#25928;&#22320;&#20135;&#29983;$4\times$&#19978;&#19979;&#25991;&#21387;&#32553;&#30340;&#20869;&#23384;&#27133;&#65292;&#30446;&#26631;LLM&#21487;&#20197;&#24456;&#22909;&#22320;&#23545;&#20854;&#36827;&#34892;&#26465;&#20214;&#22788;&#29702;&#65292;&#20197;&#21709;&#24212;&#21508;&#31181;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the In-context Autoencoder (ICAE) for context compression in a large language model (LLM). The ICAE has two modules: a learnable encoder adapted with LoRA from an LLM for compressing a long context into a limited number of memory slots, and a fixed decoder which is the target LLM that can condition on the memory slots for various purposes. We first pretrain the ICAE using both autoencoding and language modeling objectives on massive text data, enabling it to generate memory slots that accurately and comprehensively represent the original context. Then, we fine-tune the pretrained ICAE on a small amount of instruct data to enhance its interaction with various prompts for producing desirable responses. Our experimental results demonstrate that the ICAE learned with our proposed pretraining and fine-tuning paradigm can effectively produce memory slots with $4\times$ context compression, which can be well conditioned on by the target LLM to respond to various prompts. The promis
&lt;/p&gt;</description></item><item><title>mBLIP&#26159;&#31532;&#19968;&#20010;&#22810;&#35821;&#35328;Vision-LLM&#65292;&#36890;&#36807;&#22312;&#28040;&#36153;&#32423;&#30828;&#20214;&#19978;&#20351;&#29992;&#23569;&#37327;&#35757;&#32451;&#26679;&#20363;&#30340;&#35745;&#31639;&#19978;&#39640;&#25928;&#30340;&#26041;&#24335;&#33719;&#24471;&#12290;</title><link>http://arxiv.org/abs/2307.06930</link><description>&lt;p&gt;
mBLIP: &#22810;&#35821;&#35328;&#35270;&#35273;-LLM&#30340;&#39640;&#25928;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs. (arXiv:2307.06930v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06930
&lt;/p&gt;
&lt;p&gt;
mBLIP&#26159;&#31532;&#19968;&#20010;&#22810;&#35821;&#35328;Vision-LLM&#65292;&#36890;&#36807;&#22312;&#28040;&#36153;&#32423;&#30828;&#20214;&#19978;&#20351;&#29992;&#23569;&#37327;&#35757;&#32451;&#26679;&#20363;&#30340;&#35745;&#31639;&#19978;&#39640;&#25928;&#30340;&#26041;&#24335;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22359;&#21270;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;Vision-LLM&#65289;&#23558;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#19982;&#65288;&#39044;&#35757;&#32451;&#30340;&#65289;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23545;&#40784;&#65292;&#26159;&#19968;&#31181;&#22312;&#35745;&#31639;&#19978;&#26356;&#39640;&#25928;&#30340;&#36873;&#25321;&#65292;&#21487;&#20197;&#20195;&#26367;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#26041;&#27861;&#65292;&#32780;&#21518;&#32773;&#23545;&#20110;&#22823;&#22810;&#25968;&#20154;&#26469;&#35828;&#25104;&#26412;&#22826;&#39640;&#12290; Vision-LLM&#23558;LLM&#20107;&#21518;&#26465;&#20214;&#21270;&#20026;&#8220;&#29702;&#35299;&#8221;&#22270;&#20687;&#32534;&#30721;&#22120;&#30340;&#36755;&#20986;&#12290;&#38543;&#30528;&#29616;&#25104;&#30340;&#39640;&#36136;&#37327;&#33521;&#25991;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#20197;&#21450;&#21333;&#35821;&#33521;&#35821;LLM&#30340;&#20016;&#23500;&#24615;&#65292;&#30740;&#31350;&#37325;&#28857;&#24050;&#32463;&#25918;&#22312;&#20165;&#33521;&#25991;&#30340;Vision-LLM&#19978;&#12290;&#32780;&#22810;&#35821;&#35328;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#20027;&#35201;&#36890;&#36807;&#26114;&#36149;&#30340;&#31471;&#21040;&#31471;&#39044;&#35757;&#32451;&#33719;&#24471;&#65292;&#36825;&#23548;&#33268;&#20102;&#30456;&#23545;&#36739;&#23567;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#26377;&#38480;&#30340;&#22810;&#35821;&#35328;&#22270;&#20687;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#21516;&#26102;&#34917;&#20805;&#20102;&#20165;&#26377;&#25991;&#26412;&#30340;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;mBLIP&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22810;&#35821;&#35328;Vision-LLM&#65292;&#25105;&#20204;&#20197;&#35745;&#31639;&#19978;&#39640;&#25928;&#30340;&#26041;&#24335;&#33719;&#24471;&#65292;&#20165;&#20351;&#29992;&#20960;&#30334;&#19975;&#20010;&#35757;&#32451;&#26679;&#20363;&#22312;&#28040;&#36153;&#32423;&#30828;&#20214;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modular vision-language models (Vision-LLMs) align pretrained image encoders with (pretrained) large language models (LLMs), representing a computationally much more efficient alternative to end-to-end training of large vision-language models from scratch, which is prohibitively expensive for most. Vision-LLMs instead post-hoc condition LLMs to `understand' the output of an image encoder. With the abundance of readily available high-quality English image-text data as well as monolingual English LLMs, the research focus has been on English-only Vision-LLMs. Multilingual vision-language models are still predominantly obtained via expensive end-to-end pretraining, resulting in comparatively smaller models, trained on limited multilingual image data supplemented with text-only multilingual corpora. In this work, we present mBLIP, the first multilingual Vision-LLM, which we obtain in a computationally efficient manner -- on consumer hardware using only a few million training examples -- by 
&lt;/p&gt;</description></item><item><title>DRAGON&#26159;&#19968;&#31181;&#22522;&#20110;&#23545;&#35805;&#30340;&#23548;&#33322;&#26426;&#22120;&#20154;&#65292;&#33021;&#22815;&#29702;&#35299;&#29992;&#25143;&#30340;&#25351;&#20196;&#24182;&#36890;&#36807;&#35821;&#35328;&#19982;&#29992;&#25143;&#27807;&#36890;&#65292;&#20026;&#35270;&#21147;&#21463;&#25439;&#32773;&#25552;&#20379;&#23548;&#33322;&#21644;&#29615;&#22659;&#25551;&#36848;&#30340;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2307.06924</link><description>&lt;p&gt;
DRAGON: &#19968;&#31181;&#22522;&#20110;&#23545;&#35805;&#30340;&#24102;&#26377;&#35270;&#35273;&#35821;&#35328;&#20851;&#32852;&#30340;&#36741;&#21161;&#23548;&#33322;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
DRAGON: A Dialogue-Based Robot for Assistive Navigation with Visual Language Grounding. (arXiv:2307.06924v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06924
&lt;/p&gt;
&lt;p&gt;
DRAGON&#26159;&#19968;&#31181;&#22522;&#20110;&#23545;&#35805;&#30340;&#23548;&#33322;&#26426;&#22120;&#20154;&#65292;&#33021;&#22815;&#29702;&#35299;&#29992;&#25143;&#30340;&#25351;&#20196;&#24182;&#36890;&#36807;&#35821;&#35328;&#19982;&#29992;&#25143;&#27807;&#36890;&#65292;&#20026;&#35270;&#21147;&#21463;&#25439;&#32773;&#25552;&#20379;&#23548;&#33322;&#21644;&#29615;&#22659;&#25551;&#36848;&#30340;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#21147;&#21463;&#25439;&#32773;&#22312;&#29702;&#35299;&#21644;&#23548;&#33322;&#21608;&#22260;&#31354;&#38388;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#30446;&#21069;&#30340;&#23548;&#33322;&#25216;&#26415;&#35201;&#20040;&#21482;&#20851;&#27880;&#23548;&#33322;&#65292;&#35201;&#20040;&#25552;&#20379;&#26377;&#38480;&#30340;&#20851;&#20110;&#29615;&#22659;&#30340;&#27807;&#36890;&#12290;&#21463;&#21040;&#26368;&#36817;&#22312;&#35270;&#35273;&#35821;&#35328;&#20851;&#32852;&#21644;&#35821;&#20041;&#23548;&#33322;&#26041;&#38754;&#30340;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DRAGON&#65292;&#19968;&#31181;&#30001;&#23545;&#35805;&#31995;&#32479;&#39537;&#21160;&#30340;&#23548;&#33322;&#26426;&#22120;&#20154;&#65292;&#24182;&#20855;&#26377;&#23558;&#29615;&#22659;&#19982;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#29702;&#35299;&#29992;&#25143;&#30340;&#25351;&#20196;&#65292;DRAGON&#33021;&#22815;&#24341;&#23548;&#29992;&#25143;&#21040;&#22320;&#22270;&#19978;&#30340;&#30446;&#26631;&#22320;&#26631;&#65292;&#25551;&#36848;&#29615;&#22659;&#65292;&#24182;&#36890;&#36807;&#35270;&#35273;&#35266;&#23519;&#22238;&#31572;&#38382;&#39064;&#12290;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#23545;&#35805;&#65292;&#26426;&#22120;&#20154;&#21487;&#20197;&#23558;&#29992;&#25143;&#30340;&#33258;&#30001;&#24418;&#24335;&#25551;&#36848;&#19982;&#29615;&#22659;&#20013;&#30340;&#22320;&#26631;&#20851;&#32852;&#36215;&#26469;&#65292;&#24182;&#36890;&#36807;&#21475;&#35821;&#25552;&#20379;&#35821;&#20041;&#20449;&#24687;&#32473;&#29992;&#25143;&#12290;&#25105;&#20204;&#22312;&#26085;&#24120;&#23460;&#20869;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#30450;&#30446;&#21442;&#19982;&#32773;&#30340;&#29992;&#25143;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;DRAGON&#33021;&#22815;&#19982;&#29992;&#25143;&#39034;&#30021;&#22320;&#27807;&#36890;&#65292;
&lt;/p&gt;
&lt;p&gt;
Persons with visual impairments (PwVI) have difficulties understanding and navigating spaces around them. Current wayfinding technologies either focus solely on navigation or provide limited communication about the environment. Motivated by recent advances in visual-language grounding and semantic navigation, we propose DRAGON, a guiding robot powered by a dialogue system and the ability to associate the environment with natural language. By understanding the commands from the user, DRAGON is able to guide the user to the desired landmarks on the map, describe the environment, and answer questions from visual observations. Through effective utilization of dialogue, the robot can ground the user's free-form descriptions to landmarks in the environment, and give the user semantic information through spoken language. We conduct a user study with blindfolded participants in an everyday indoor environment. Our results demonstrate that DRAGON is able to communicate with the user smoothly, pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;LLM&#36741;&#21161;&#30340;&#30693;&#35782;&#22270;&#35889;&#24037;&#31243;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;ChatGPT&#22312;&#24320;&#21457;&#21644;&#31649;&#29702;&#30693;&#35782;&#22270;&#35889;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.06917</link><description>&lt;p&gt;
&#22522;&#20110;LLM&#36741;&#21161;&#30340;&#30693;&#35782;&#22270;&#35889;&#24037;&#31243;: ChatGPT&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
LLM-assisted Knowledge Graph Engineering: Experiments with ChatGPT. (arXiv:2307.06917v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06917
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;LLM&#36741;&#21161;&#30340;&#30693;&#35782;&#22270;&#35889;&#24037;&#31243;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;ChatGPT&#22312;&#24320;&#21457;&#21644;&#31649;&#29702;&#30693;&#35782;&#22270;&#35889;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#25552;&#20379;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#12289;&#28789;&#27963;&#12289;&#36879;&#26126;&#12289;&#36328;&#31995;&#32479;&#21644;&#21327;&#20316;&#30340;&#26041;&#24335;&#26469;&#32452;&#32455;&#31038;&#20250;&#21644;&#24037;&#19994;&#20197;&#21450;&#31185;&#23398;&#23398;&#31185;&#20013;&#21508;&#20010;&#39046;&#22495;&#30340;&#30693;&#35782;&#21644;&#25968;&#25454;&#12290;&#30693;&#35782;&#22270;&#35889;&#22312;&#26377;&#25928;&#24615;&#26041;&#38754;&#36229;&#36234;&#20102;&#20219;&#20309;&#20854;&#20182;&#24418;&#24335;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#30693;&#35782;&#22270;&#35889;&#24037;&#31243;&#38656;&#35201;&#23545;&#22270;&#32467;&#26500;&#12289;&#32593;&#32476;&#25216;&#26415;&#12289;&#29616;&#26377;&#27169;&#22411;&#21644;&#35789;&#27719;&#12289;&#35268;&#21017;&#38598;&#12289;&#36923;&#36753;&#20197;&#21450;&#26368;&#20339;&#23454;&#36341;&#26377;&#28145;&#20837;&#30340;&#20102;&#35299;&#65292;&#21516;&#26102;&#36824;&#38656;&#35201;&#22823;&#37327;&#30340;&#24037;&#20316;&#12290;&#32771;&#34385;&#21040;&#26368;&#36817;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21450;&#20854;&#25509;&#21475;&#21644;&#24212;&#29992;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#23545;ChatGPT&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#65292;&#25506;&#32034;&#20854;&#22312;&#25903;&#25345;&#30693;&#35782;&#22270;&#35889;&#24037;&#31243;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20854;&#20013;&#19968;&#20123;&#23454;&#39564;&#21450;&#20854;&#32467;&#26524;&#65292;&#20197;&#23637;&#31034;ChatGPT&#22914;&#20309;&#36741;&#21161;&#25105;&#20204;&#24320;&#21457;&#21644;&#31649;&#29702;&#30693;&#35782;&#22270;&#35889;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graphs (KG) provide us with a structured, flexible, transparent, cross-system, and collaborative way of organizing our knowledge and data across various domains in society and industrial as well as scientific disciplines. KGs surpass any other form of representation in terms of effectiveness. However, Knowledge Graph Engineering (KGE) requires in-depth experiences of graph structures, web technologies, existing models and vocabularies, rule sets, logic, as well as best practices. It also demands a significant amount of work. Considering the advancements in large language models (LLMs) and their interfaces and applications in recent years, we have conducted comprehensive experiments with ChatGPT to explore its potential in supporting KGE. In this paper, we present a selection of these experiments and their results to demonstrate how ChatGPT can assist us in the development and management of KGs.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FACTOR&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#20107;&#23454;&#24615;&#35780;&#20272;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#33258;&#21160;&#36716;&#25442;&#20107;&#23454;&#35821;&#26009;&#24211;&#65292;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#26681;&#25454;&#35821;&#26009;&#24211;&#29983;&#25104;&#30495;&#23454;&#20107;&#23454;&#30340;&#20542;&#21521;&#19982;&#29983;&#25104;&#19981;&#27491;&#30830;&#38472;&#36848;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#20998;&#25968;&#38543;&#27169;&#22411;&#22823;&#23567;&#22686;&#21152;&#32780;&#22686;&#21152;&#65292;&#22312;LM&#19982;&#26816;&#32034;&#26041;&#27861;&#32467;&#21512;&#26102;&#24615;&#33021;&#24471;&#21040;&#25913;&#21892;&#12290;&#22256;&#24785;&#24230;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#20998;&#25968;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#24615;&#65292;&#20294;&#19981;&#24635;&#26159;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2307.06908</link><description>&lt;p&gt;
&#29983;&#25104;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#20107;&#23454;&#24615;&#35780;&#20272;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Generating Benchmarks for Factuality Evaluation of Language Models. (arXiv:2307.06908v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06908
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FACTOR&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#20107;&#23454;&#24615;&#35780;&#20272;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#33258;&#21160;&#36716;&#25442;&#20107;&#23454;&#35821;&#26009;&#24211;&#65292;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#26681;&#25454;&#35821;&#26009;&#24211;&#29983;&#25104;&#30495;&#23454;&#20107;&#23454;&#30340;&#20542;&#21521;&#19982;&#29983;&#25104;&#19981;&#27491;&#30830;&#38472;&#36848;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#20998;&#25968;&#38543;&#27169;&#22411;&#22823;&#23567;&#22686;&#21152;&#32780;&#22686;&#21152;&#65292;&#22312;LM&#19982;&#26816;&#32034;&#26041;&#27861;&#32467;&#21512;&#26102;&#24615;&#33021;&#24471;&#21040;&#25913;&#21892;&#12290;&#22256;&#24785;&#24230;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#20998;&#25968;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#24615;&#65292;&#20294;&#19981;&#24635;&#26159;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23558;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#37096;&#32626;&#21040;&#29305;&#23450;&#39046;&#22495;&#20043;&#21069;&#65292;&#34913;&#37327;&#20854;&#22312;&#35813;&#39046;&#22495;&#20013;&#29983;&#25104;&#20107;&#23454;&#38169;&#35823;&#20449;&#24687;&#30340;&#20542;&#21521;&#24456;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#20107;&#23454;&#29983;&#25104;&#35780;&#20272;&#26041;&#27861;&#38598;&#20013;&#20110;&#20174;LM&#33258;&#36523;&#20013;&#37319;&#26679;&#30340;&#20107;&#23454;&#65292;&#22240;&#27492;&#26080;&#27861;&#25511;&#21046;&#35780;&#20272;&#20107;&#23454;&#30340;&#38598;&#21512;&#65292;&#24182;&#19988;&#21487;&#33021;&#20302;&#20272;&#20102;&#32597;&#35265;&#21644;&#19981;&#22826;&#21487;&#33021;&#30340;&#20107;&#23454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;FACTOR&#65306;&#36890;&#36807;&#35821;&#26009;&#24211;&#21464;&#25442;&#36827;&#34892;&#20107;&#23454;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;LM&#30340;&#20107;&#23454;&#24615;&#12290;FACTOR&#20250;&#33258;&#21160;&#23558;&#24863;&#20852;&#36259;&#30340;&#20107;&#23454;&#35821;&#26009;&#24211;&#36716;&#21270;&#20026;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;LM&#26681;&#25454;&#35821;&#26009;&#24211;&#29983;&#25104;&#30495;&#23454;&#20107;&#23454;&#30340;&#20542;&#21521;&#19982;&#29983;&#25104;&#31867;&#20284;&#20294;&#19981;&#27491;&#30830;&#30340;&#38472;&#36848;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#21019;&#24314;&#20102;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65306;Wiki-FACTOR&#21644;News-FACTOR&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65306;&#65288;i&#65289;&#25105;&#20204;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#20998;&#25968;&#38543;&#27169;&#22411;&#22823;&#23567;&#22686;&#21152;&#32780;&#22686;&#21152;&#65292;&#24182;&#19988;&#24403;LM&#19982;&#26816;&#32034;&#26041;&#27861;&#32467;&#21512;&#20351;&#29992;&#26102;&#65292;&#24615;&#33021;&#24471;&#21040;&#25913;&#21892;&#65307;&#65288;ii&#65289;&#22522;&#20934;&#25968;&#25454;&#38598;&#20998;&#25968;&#19982;&#22256;&#24785;&#24230;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#24615;&#65292;&#20294;&#36825;&#20004;&#20010;&#25351;&#26631;&#22312;&#27169;&#22411;&#25490;&#24207;&#19978;&#24182;&#19981;&#24635;&#26159;&#19968;&#33268;&#65307;&#20197;&#21450;&#65288;iii&#65289;&#24403;&#22256;&#24785;&#24230;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#20998;&#25968;&#21457;&#29983;&#20914;&#31361;&#26102;&#65292;&#22522;&#20934;&#25968;&#25454;&#38598;&#20998;&#25968;&#26356;&#33021;&#20934;&#30830;&#21453;&#26144;LM&#30340;&#20107;&#23454;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Before deploying a language model (LM) within a given domain, it is important to measure its tendency to generate factually incorrect information in that domain. Existing factual generation evaluation methods focus on facts sampled from the LM itself, and thus do not control the set of evaluated facts and might under-represent rare and unlikely facts. We propose FACTOR: Factual Assessment via Corpus TransfORmation, a scalable approach for evaluating LM factuality. FACTOR automatically transforms a factual corpus of interest into a benchmark evaluating an LM's propensity to generate true facts from the corpus vs. similar but incorrect statements. We use our framework to create two benchmarks: Wiki-FACTOR and News-FACTOR. We show that: (i) our benchmark scores increase with model size and improve when the LM is augmented with retrieval; (ii) benchmark score correlates with perplexity, but the two metrics do not always agree on model ranking; and (iii) when perplexity and benchmark score 
&lt;/p&gt;</description></item><item><title>DecompEval&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25351;&#26631;&#26469;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#65292;&#23427;&#23558;&#35780;&#20272;&#24314;&#27169;&#20026;&#19968;&#31181;&#31867;&#20284;&#25351;&#20196;&#30340;&#38382;&#31572;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#34913;&#37327;&#65292;&#20197;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.06869</link><description>&lt;p&gt;
DecompEval&#65306;&#23558;&#29983;&#25104;&#30340;&#25991;&#26412;&#20316;&#20026;&#26080;&#30417;&#30563;&#20998;&#35299;&#38382;&#31572;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
DecompEval: Evaluating Generated Texts as Unsupervised Decomposed Question Answering. (arXiv:2307.06869v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06869
&lt;/p&gt;
&lt;p&gt;
DecompEval&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25351;&#26631;&#26469;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#65292;&#23427;&#23558;&#35780;&#20272;&#24314;&#27169;&#20026;&#19968;&#31181;&#31867;&#20284;&#25351;&#20196;&#30340;&#38382;&#31572;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#34913;&#37327;&#65292;&#20197;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#20219;&#21153;&#35780;&#20272;&#25351;&#26631;&#38754;&#20020;&#30528;&#27867;&#21270;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22823;&#22810;&#25968;&#34920;&#29616;&#33391;&#22909;&#30340;&#25351;&#26631;&#38656;&#35201;&#22312;&#29305;&#23450;&#30340;NLG&#20219;&#21153;&#21644;&#35780;&#20272;&#32500;&#24230;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#23545;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#38598;&#30340;&#36807;&#25311;&#21512;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#25351;&#26631;&#20165;&#25552;&#20379;&#27599;&#20010;&#32500;&#24230;&#30340;&#35780;&#20272;&#20998;&#25968;&#65292;&#32780;&#19981;&#25581;&#31034;&#22914;&#20309;&#33719;&#24471;&#35813;&#20998;&#25968;&#30340;&#35777;&#25454;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25351;&#26631;&#31216;&#20026;DecompEval&#12290;&#36825;&#20010;&#25351;&#26631;&#23558;NLG&#35780;&#20272;&#24314;&#27169;&#20026;&#19968;&#31181;&#31867;&#20284;&#25351;&#20196;&#30340;&#38382;&#31572;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;&#32463;&#36807;&#25351;&#20196;&#35843;&#25972;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#32780;&#19981;&#26159;&#22312;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#26088;&#22312;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#20351;&#35780;&#20272;&#36807;&#31243;&#26356;&#20855;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#23558;&#20851;&#20110;&#29983;&#25104;&#25991;&#26412;&#36136;&#37327;&#30340;&#35774;&#35745;&#25351;&#20196;&#24335;&#38382;&#39064;&#20998;&#35299;&#20026;&#34913;&#37327;&#23376;&#38382;&#39064;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Existing evaluation metrics for natural language generation (NLG) tasks face the challenges on generalization ability and interpretability. Specifically, most of the well-performed metrics are required to train on evaluation datasets of specific NLG tasks and evaluation dimensions, which may cause over-fitting to task-specific datasets. Furthermore, existing metrics only provide an evaluation score for each dimension without revealing the evidence to interpret how this score is obtained. To deal with these challenges, we propose a simple yet effective metric called DecompEval. This metric formulates NLG evaluation as an instruction-style question answering task and utilizes instruction-tuned pre-trained language models (PLMs) without training on evaluation datasets, aiming to enhance the generalization ability. To make the evaluation process more interpretable, we decompose our devised instruction-style question about the quality of generated texts into the subquestions that measure th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#22320;&#34913;&#37327;&#25552;&#31034;&#25552;&#21462;&#25915;&#20987;&#25104;&#21151;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#22810;&#20010;&#23454;&#39564;&#21457;&#29616;&#65292;&#21363;&#20351;&#25552;&#31034;&#34987;&#20445;&#23494;&#65292;&#31616;&#21333;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#25915;&#20987;&#20173;&#28982;&#21487;&#20197;&#39640;&#27010;&#29575;&#22320;&#25581;&#31034;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2307.06865</link><description>&lt;p&gt;
&#25552;&#31034;&#19981;&#24212;&#34987;&#35270;&#20026;&#31192;&#23494;&#65306;&#31995;&#32479;&#22320;&#34913;&#37327;&#25552;&#31034;&#25552;&#21462;&#25915;&#20987;&#30340;&#25104;&#21151;&#24615;
&lt;/p&gt;
&lt;p&gt;
Prompts Should not be Seen as Secrets: Systematically Measuring Prompt Extraction Attack Success. (arXiv:2307.06865v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#22320;&#34913;&#37327;&#25552;&#31034;&#25552;&#21462;&#25915;&#20987;&#25104;&#21151;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#22810;&#20010;&#23454;&#39564;&#21457;&#29616;&#65292;&#21363;&#20351;&#25552;&#31034;&#34987;&#20445;&#23494;&#65292;&#31616;&#21333;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#25915;&#20987;&#20173;&#28982;&#21487;&#20197;&#39640;&#27010;&#29575;&#22320;&#25581;&#31034;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#36890;&#24120;&#36890;&#36807;&#25552;&#31034;&#25216;&#26415;&#26469;&#25511;&#21046;&#65292;&#20854;&#20013;&#29992;&#25143;&#23545;&#27169;&#22411;&#30340;&#26597;&#35810;&#20197;&#26088;&#22312;&#25351;&#23548;&#27169;&#22411;&#22312;&#35813;&#26597;&#35810;&#19978;&#30340;&#34892;&#20026;&#30340;&#25552;&#31034;&#20316;&#20026;&#21069;&#32512;&#12290;&#20844;&#21496;&#29992;&#20110;&#25351;&#23548;&#20854;&#27169;&#22411;&#30340;&#25552;&#31034;&#36890;&#24120;&#34987;&#35270;&#20026;&#31192;&#23494;&#65292;&#38544;&#34255;&#22312;&#26597;&#35810;&#30340;&#29992;&#25143;&#20043;&#22806;&#12290;&#23427;&#20204;&#29978;&#33267;&#34987;&#35270;&#20026;&#21487;&#20197;&#20080;&#21334;&#30340;&#21830;&#21697;&#12290;&#28982;&#32780;&#65292;&#26377;&#32463;&#39564;&#24615;&#30340;&#35777;&#25454;&#26174;&#31034;&#65292;&#21363;&#20351;&#25552;&#31034;&#34987;&#20445;&#23494;&#65292;&#29992;&#25143;&#20173;&#28982;&#21487;&#20197;&#25552;&#21462;&#23427;&#20204;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#22320;&#34913;&#37327;&#25552;&#31034;&#25552;&#21462;&#25915;&#20987;&#25104;&#21151;&#30340;&#26694;&#26550;&#12290;&#22312;&#20351;&#29992;&#22810;&#20010;&#25552;&#31034;&#28304;&#21644;&#22810;&#20010;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#31616;&#21333;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#25915;&#20987;&#23454;&#38469;&#19978;&#21487;&#20197;&#39640;&#27010;&#29575;&#22320;&#25581;&#31034;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generations of large language models are commonly controlled through prompting techniques, where a user's query to the model is prefixed with a prompt that aims to guide the model's behaviour on the query. The prompts used by companies to guide their models are often treated as secrets, to be hidden from the user making the query. They have even been treated as commodities to be bought and sold. However, there has been anecdotal evidence showing that the prompts can be extracted by a user even when they are kept secret. In this paper, we present a framework for systematically measuring the success of prompt extraction attacks. In experiments with multiple sources of prompts and multiple underlying language models, we find that simple text-based attacks can in fact reveal prompts with high probability.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36755;&#20986;&#36136;&#37327;&#21644;&#19968;&#33268;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#33258;&#27965;&#24615;&#26694;&#26550;&#30340;&#36866;&#29992;&#24615;&#65292;&#23454;&#29616;&#20102;&#20174;&#19968;&#20010;&#20505;&#36873;&#38598;&#20013;&#24674;&#22797;&#26368;&#20248;&#25110;&#25509;&#36817;&#26368;&#20248;&#30340;&#29983;&#25104;&#32467;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#26080;&#21442;&#25968;&#30456;&#20284;&#24615;&#20989;&#25968;&#26469;&#25913;&#36827;&#20195;&#30721;&#29983;&#25104;&#12289;&#33258;&#21160;&#24418;&#24335;&#21270;&#21644;&#25688;&#35201;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.06857</link><description>&lt;p&gt;
&#33258;&#27965;&#24615;&#26041;&#27861;&#29992;&#20110;&#26080;&#38480;&#29983;&#25104;&#38382;&#39064;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Self-consistency for open-ended generations. (arXiv:2307.06857v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36755;&#20986;&#36136;&#37327;&#21644;&#19968;&#33268;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#33258;&#27965;&#24615;&#26694;&#26550;&#30340;&#36866;&#29992;&#24615;&#65292;&#23454;&#29616;&#20102;&#20174;&#19968;&#20010;&#20505;&#36873;&#38598;&#20013;&#24674;&#22797;&#26368;&#20248;&#25110;&#25509;&#36817;&#26368;&#20248;&#30340;&#29983;&#25104;&#32467;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#26080;&#21442;&#25968;&#30456;&#20284;&#24615;&#20989;&#25968;&#26469;&#25913;&#36827;&#20195;&#30721;&#29983;&#25104;&#12289;&#33258;&#21160;&#24418;&#24335;&#21270;&#21644;&#25688;&#35201;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36755;&#20986;&#30340;&#36136;&#37327;&#21644;&#19968;&#33268;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;&#33258;&#27965;&#24615;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23545;&#20110;&#20855;&#26377;&#22266;&#23450;&#31572;&#26696;&#30340;&#25552;&#31034;&#65292;&#36873;&#25321;&#24471;&#31080;&#26368;&#22810;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#25512;&#24191;&#30340;&#33258;&#27965;&#24615;&#26694;&#26550;&#65292;&#25193;&#23637;&#20102;&#20854;&#36866;&#29992;&#24615;&#65292;&#36229;&#36234;&#20102;&#22266;&#23450;&#31572;&#26696;&#38382;&#39064;&#30340;&#33539;&#22260;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#27169;&#25311;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20174;&#20505;&#36873;&#38598;&#20013;&#24674;&#22797;&#26368;&#20248;&#25110;&#25509;&#36817;&#26368;&#20248;&#30340;&#29983;&#25104;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#26080;&#21442;&#25968;&#30456;&#20284;&#24615;&#20989;&#25968;&#65292;&#21363;&#20351;&#27809;&#26377;&#35775;&#38382;&#21040;&#26631;&#35760;&#30340;&#27010;&#29575;&#65292;&#20063;&#33021;&#22312;&#20195;&#30721;&#29983;&#25104;&#12289;&#33258;&#21160;&#24418;&#24335;&#21270;&#21644;&#25688;&#35201;&#20219;&#21153;&#20013;&#26174;&#33879;&#21644;&#19968;&#33268;&#22320;&#25913;&#36827;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20960;&#20046;&#27809;&#26377;&#35745;&#31639;&#24320;&#38144;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#20877;&#25490;&#24207;&#27169;&#22411;&#25110;&#23545;&#29616;&#26377;&#27169;&#22411;&#30340;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a novel approach for improving the quality and consistency of generated outputs from large-scale pre-trained language models (LLMs). Self-consistency has emerged as an effective approach for prompts with fixed answers, selecting the answer with the highest number of votes. In this paper, we introduce a generalized framework for self-consistency that extends its applicability beyond problems that have fixed-answer answers. Through extensive simulations, we demonstrate that our approach consistently recovers the optimal or near-optimal generation from a set of candidates. We also propose lightweight parameter-free similarity functions that show significant and consistent improvements across code generation, autoformalization, and summarization tasks, even without access to token log probabilities. Our method incurs minimal computational overhead, requiring no auxiliary reranker models or modifications to the existing model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#30340;&#24120;&#35782;&#30693;&#35782;&#65292;&#36890;&#36807;&#23558;&#35270;&#39057;&#36716;&#25442;&#20026;&#25991;&#26412;&#25551;&#36848;&#65292;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#25512;&#29702;&#30340;&#29359;&#32618;&#26816;&#27979;&#12290;&#29616;&#26377;&#30340;&#35270;&#39057;&#36716;&#25991;&#26412;&#26041;&#27861;&#36136;&#37327;&#19981;&#36275;&#20197;&#25903;&#25345;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2307.06844</link><description>&lt;p&gt;
&#22403;&#22334;&#36827;&#65292;&#22403;&#22334;&#20986;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#29359;&#32618;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Garbage in, garbage out: Zero-shot detection of crime using Large Language Models. (arXiv:2307.06844v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06844
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#30340;&#24120;&#35782;&#30693;&#35782;&#65292;&#36890;&#36807;&#23558;&#35270;&#39057;&#36716;&#25442;&#20026;&#25991;&#26412;&#25551;&#36848;&#65292;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#25512;&#29702;&#30340;&#29359;&#32618;&#26816;&#27979;&#12290;&#29616;&#26377;&#30340;&#35270;&#39057;&#36716;&#25991;&#26412;&#26041;&#27861;&#36136;&#37327;&#19981;&#36275;&#20197;&#25903;&#25345;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#30340;&#24120;&#35782;&#30693;&#35782;&#65292;&#22312;&#32473;&#23450;&#30417;&#25511;&#35270;&#39057;&#30340;&#25991;&#26412;&#25551;&#36848;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20851;&#20110;&#29359;&#32618;&#30340;&#38646;&#26679;&#26412;&#25512;&#29702;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#23558;&#35270;&#39057;&#25163;&#21160;&#36716;&#25442;&#20026;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#25551;&#36848;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20165;&#36890;&#36807;&#38646;&#26679;&#26412;&#25512;&#29702;&#23454;&#29616;&#20855;&#26377;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#29359;&#32618;&#26816;&#27979;&#21644;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#33258;&#21160;&#21270;&#35270;&#39057;&#36716;&#25991;&#26412;&#26041;&#27861;&#26080;&#27861;&#29983;&#25104;&#36275;&#22815;&#36136;&#37327;&#30340;&#35270;&#39057;&#25551;&#36848;&#26469;&#25903;&#25345;&#25512;&#29702;&#65288;&#22403;&#22334;&#35270;&#39057;&#25551;&#36848;&#36827;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21518;&#65292;&#20135;&#29983;&#30340;&#32467;&#26524;&#20063;&#26159;&#22403;&#22334;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes exploiting the common sense knowledge learned by large language models to perform zero-shot reasoning about crimes given textual descriptions of surveillance videos. We show that when video is (manually) converted to high quality textual descriptions, large language models are capable of detecting and classifying crimes with state-of-the-art performance using only zero-shot reasoning. However, existing automated video-to-text approaches are unable to generate video descriptions of sufficient quality to support reasoning (garbage video descriptions into the large language model, garbage out).
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#21033;&#29992;&#20010;&#24615;&#21270;&#20869;&#23481;&#25552;&#39640;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#65306;&#35789;&#34920;&#12289;&#25552;&#31034;&#21644;&#22522;&#20110;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#37117;&#26174;&#33879;&#25552;&#39640;&#20102;&#35789;&#38169;&#35823;&#29575;&#65292;&#24182;&#19988;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#19981;&#38656;&#35201;&#35757;&#32451;&#21363;&#21487;&#25913;&#21892;&#20934;&#30830;&#29575;&#12290;&#20854;&#20013;&#65292;&#35789;&#34920;&#34920;&#29616;&#26368;&#20339;&#65292;&#25552;&#39640;&#20102;10%&#30340;&#35789;&#38169;&#35823;&#29575;&#65292;&#21516;&#26102;&#22312;&#36890;&#29992;&#27979;&#35797;&#38598;&#19978;&#20063;&#26377;1%&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2307.06832</link><description>&lt;p&gt;
&#22522;&#20110;BERT&#30340;&#21306;&#20998;&#24615;&#35821;&#38899;&#35782;&#21035;&#20505;&#36873;&#20462;&#27491;&#30340;&#20010;&#24615;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Personalization for BERT-based Discriminative Speech Recognition Rescoring. (arXiv:2307.06832v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#21033;&#29992;&#20010;&#24615;&#21270;&#20869;&#23481;&#25552;&#39640;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#65306;&#35789;&#34920;&#12289;&#25552;&#31034;&#21644;&#22522;&#20110;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#37117;&#26174;&#33879;&#25552;&#39640;&#20102;&#35789;&#38169;&#35823;&#29575;&#65292;&#24182;&#19988;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#19981;&#38656;&#35201;&#35757;&#32451;&#21363;&#21487;&#25913;&#21892;&#20934;&#30830;&#29575;&#12290;&#20854;&#20013;&#65292;&#35789;&#34920;&#34920;&#29616;&#26368;&#20339;&#65292;&#25552;&#39640;&#20102;10%&#30340;&#35789;&#38169;&#35823;&#29575;&#65292;&#21516;&#26102;&#22312;&#36890;&#29992;&#27979;&#35797;&#38598;&#19978;&#20063;&#26377;1%&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#20013;&#65292;&#20010;&#24615;&#21270;&#20869;&#23481;&#30340;&#35782;&#21035;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19977;&#31181;&#21033;&#29992;&#20010;&#24615;&#21270;&#20869;&#23481;&#22312;&#31070;&#32463;&#20462;&#27491;&#27493;&#39588;&#20013;&#25552;&#39640;&#35782;&#21035;&#20934;&#30830;&#24615;&#30340;&#26032;&#26041;&#27861;&#65306;&#35789;&#34920;&#12289;&#25552;&#31034;&#21644;&#22522;&#20110;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#20869;&#37096;&#30340;&#21435;&#26631;&#35782;&#21270;&#33521;&#35821;(&#32654;&#22269;)&#20132;&#20114;&#25968;&#25454;&#65292;&#34917;&#20805;&#20010;&#24615;&#21270;&#21629;&#21517;&#23454;&#20307;&#65292;&#20197;&#27604;&#36739;&#36825;&#20123;&#26041;&#27861;&#12290;&#22312;&#19968;&#20010;&#21253;&#21547;&#20010;&#24615;&#21270;&#21629;&#21517;&#23454;&#20307;&#30340;&#27979;&#35797;&#38598;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#27599;&#31181;&#26041;&#27861;&#30456;&#23545;&#20110;&#31070;&#32463;&#20462;&#27491;&#22522;&#32447;&#21487;&#20197;&#23558;&#35789;&#38169;&#35823;&#29575;&#25552;&#39640;&#36229;&#36807;10%&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#36825;&#20010;&#27979;&#35797;&#38598;&#19978;&#65292;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#21487;&#20197;&#25552;&#39640;&#35789;&#38169;&#35823;&#29575;7%&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#35757;&#32451;&#65292;&#24182;&#19988;&#22312;&#27867;&#21270;&#26041;&#38754;&#21482;&#26377;&#24494;&#23567;&#30340;&#25439;&#22833;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#35789;&#34920;&#30340;&#34920;&#29616;&#26368;&#22909;&#65292;&#35789;&#38169;&#35823;&#29575;&#25552;&#39640;&#20102;10%&#65292;&#22312;&#19968;&#20010;&#36890;&#29992;&#27979;&#35797;&#38598;&#19978;&#36824;&#25552;&#39640;&#20102;1%&#12290;
&lt;/p&gt;
&lt;p&gt;
Recognition of personalized content remains a challenge in end-to-end speech recognition. We explore three novel approaches that use personalized content in a neural rescoring step to improve recognition: gazetteers, prompting, and a cross-attention based encoder-decoder model. We use internal de-identified en-US data from interactions with a virtual voice assistant supplemented with personalized named entities to compare these approaches. On a test set with personalized named entities, we show that each of these approaches improves word error rate by over 10%, against a neural rescoring baseline. We also show that on this test set, natural language prompts can improve word error rate by 7% without any training and with a marginal loss in generalization. Overall, gazetteers were found to perform the best with a 10% improvement in word error rate (WER), while also improving WER on a general test set by 1%.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#21542;&#23450;&#24335;&#20114;&#34917;&#38382;&#39064;&#30340;&#24615;&#33021;&#12290;&#20316;&#32773;&#21457;&#29616;&#65292;&#36825;&#31181;&#31867;&#22411;&#30340;&#38382;&#39064;&#20250;&#23545;&#27169;&#22411;&#30340;&#21709;&#24212;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#29983;&#25104;&#26041;&#38754;&#20248;&#20110;GPT-3&#65292;&#24182;&#24378;&#35843;&#20102;&#30740;&#31350;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#21542;&#23450;&#24335;&#20114;&#34917;&#38382;&#39064;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.06794</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#21542;&#23450;&#24335;&#20114;&#34917;&#24120;&#35782;
&lt;/p&gt;
&lt;p&gt;
Negated Complementary Commonsense using Large Language Models. (arXiv:2307.06794v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#21542;&#23450;&#24335;&#20114;&#34917;&#38382;&#39064;&#30340;&#24615;&#33021;&#12290;&#20316;&#32773;&#21457;&#29616;&#65292;&#36825;&#31181;&#31867;&#22411;&#30340;&#38382;&#39064;&#20250;&#23545;&#27169;&#22411;&#30340;&#21709;&#24212;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#29983;&#25104;&#26041;&#38754;&#20248;&#20110;GPT-3&#65292;&#24182;&#24378;&#35843;&#20102;&#30740;&#31350;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#21542;&#23450;&#24335;&#20114;&#34917;&#38382;&#39064;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;GPT-3&#65292;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#38750;&#24120;&#35268;&#38382;&#39064;&#21487;&#33021;&#20250;&#20351;&#27169;&#22411;&#22833;&#21435;&#35686;&#35273;&#12290;&#26412;&#25991;&#20027;&#35201;&#20851;&#27880;&#22312;&#24120;&#35782;&#24773;&#26223;&#20013;&#23547;&#25214;&#21542;&#23450;&#24335;&#20114;&#34917;&#38382;&#39064;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#38416;&#36848;&#20102;&#36825;&#31867;&#38382;&#39064;&#23545;&#27169;&#22411;&#21709;&#24212;&#30340;&#19981;&#21033;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#22312;&#21542;&#23450;&#24335;&#20114;&#34917;&#24773;&#26223;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20174;GPT-3&#36827;&#34892;&#23569;&#26679;&#26412;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#65288;&#36229;&#36807;11&#20010;&#28857;&#65289;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#24378;&#35843;&#20102;&#30740;&#31350;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#21542;&#23450;&#24335;&#20114;&#34917;&#38382;&#39064;&#20013;&#30340;&#21709;&#24212;&#30340;&#37325;&#35201;&#24615;&#12290;&#20195;&#30721;&#12289;&#25968;&#25454;&#21644;&#23454;&#39564;&#21487;&#22312;&#20197;&#19979;&#38142;&#25509;&#25214;&#21040;&#65306;https://github.com/navidre/negated_complementary_commonsense&#12290;
&lt;/p&gt;
&lt;p&gt;
Larger language models, such as GPT-3, have shown to be excellent in many tasks. However, we demonstrate that out-of-ordinary questions can throw the model off guard. This work focuses on finding answers to negated complementary questions in commonsense scenarios. We illustrate how such questions adversely affect the model responses. We propose a model-agnostic methodology to improve the performance in negated complementary scenarios. Our method outperforms few-shot generation from GPT-3 (by more than 11 points) and, more importantly, highlights the significance of studying the response of large language models in negated complementary questions. The code, data, and experiments are available under: https://github.com/navidre/negated_complementary_commonsense.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23558;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#30340;&#20869;&#23481;&#12290;&#26368;&#26377;&#25928;&#30340;&#27169;&#22411;&#26159;RoBERTa&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#21644;MaxViT&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#34701;&#21512;&#27169;&#22411;&#65292;&#20934;&#30830;&#29575;&#21644;F1&#20998;&#25968;&#20998;&#21035;&#36798;&#21040;95.9%&#21644;0.959&#12290;</title><link>http://arxiv.org/abs/2307.06775</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#22411;&#30340;&#19982;&#24179;&#21488;&#26080;&#20851;&#30340;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#35782;&#21035;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
A Novel Site-Agnostic Multimodal Deep Learning Model to Identify Pro-Eating Disorder Content on Social Media. (arXiv:2307.06775v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23558;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#30340;&#20869;&#23481;&#12290;&#26368;&#26377;&#25928;&#30340;&#27169;&#22411;&#26159;RoBERTa&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#21644;MaxViT&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#34701;&#21512;&#27169;&#22411;&#65292;&#20934;&#30830;&#29575;&#21644;F1&#20998;&#25968;&#20998;&#21035;&#36798;&#21040;95.9%&#21644;0.959&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#39278;&#39135;&#32010;&#20081;&#30340;&#35786;&#26029;&#21644;&#19982;&#20043;&#30456;&#20851;&#30340;&#27515;&#20129;&#25968;&#37327;&#22823;&#24133;&#22686;&#21152;&#65292;&#23588;&#20854;&#26159;&#22312;&#26032;&#20896;&#30123;&#24773;&#26399;&#38388;&#12290;&#36825;&#31181;&#24040;&#22823;&#22686;&#38271;&#37096;&#20998;&#26469;&#28304;&#20110;&#30123;&#24773;&#30340;&#21387;&#21147;&#65292;&#20294;&#20063;&#19982;&#31038;&#20132;&#23186;&#20307;&#30340;&#26292;&#38706;&#22686;&#21152;&#26377;&#20851;&#65292;&#31038;&#20132;&#23186;&#20307;&#19978;&#20805;&#26021;&#30528;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#30340;&#20869;&#23481;&#12290;&#36825;&#20123;&#20869;&#23481;&#21487;&#20197;&#35825;&#21457;&#35266;&#30475;&#32773;&#30340;&#39278;&#39135;&#32010;&#20081;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#21019;&#24314;&#19968;&#20010;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#22522;&#20110;&#35270;&#35273;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#32452;&#21512;&#21028;&#26029;&#32473;&#23450;&#30340;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#26159;&#21542;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#12290;&#20174;Twitter&#25910;&#38598;&#20102;&#19968;&#20010;&#24102;&#26377;&#26631;&#31614;&#30340;&#25512;&#25991;&#25968;&#25454;&#38598;&#65292;&#23545;&#20854;&#36827;&#34892;&#20102;&#21313;&#20108;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#26681;&#25454;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#26368;&#26377;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26159;RoBERTa&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#21644;MaxViT&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;&#65292;&#20934;&#30830;&#29575;&#21644;F1&#20998;&#25968;&#20998;&#21035;&#36798;&#21040;95.9%&#21644;0.959&#12290;RoBERTa&#21644;MaxViT&#34701;&#21512;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the last decade, there has been a vast increase in eating disorder diagnoses and eating disorder-attributed deaths, reaching their zenith during the Covid-19 pandemic. This immense growth derived in part from the stressors of the pandemic but also from increased exposure to social media, which is rife with content that promotes eating disorders. Such content can induce eating disorders in viewers. This study aimed to create a multimodal deep learning model capable of determining whether a given social media post promotes eating disorders based on a combination of visual and textual data. A labeled dataset of Tweets was collected from Twitter, upon which twelve deep learning models were trained and tested. Based on model performance, the most effective deep learning model was the multimodal fusion of the RoBERTa natural language processing model and the MaxViT image classification model, attaining accuracy and F1 scores of 95.9% and 0.959 respectively. The RoBERTa and MaxViT fusion
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#23545;&#35805;&#31574;&#30053;&#21644;&#22870;&#21169;&#20272;&#35745;&#22120;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#35299;&#37322;&#20102;&#23545;&#25239;&#23398;&#20064;&#22312;&#23545;&#35805;&#31574;&#30053;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.06721</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#23548;&#21521;&#24335;&#23545;&#35805;&#31574;&#30053;&#23398;&#20064;&#34920;&#29616;&#20248;&#31168;&#65311;&#29702;&#35299;&#23545;&#25239;&#23398;&#20064;&#21450;&#20854;&#26367;&#20195;&#26041;&#27861;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Why Guided Dialog Policy Learning performs well? Understanding the role of adversarial learning and its alternative. (arXiv:2307.06721v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#23545;&#35805;&#31574;&#30053;&#21644;&#22870;&#21169;&#20272;&#35745;&#22120;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#35299;&#37322;&#20102;&#23545;&#25239;&#23398;&#20064;&#22312;&#23545;&#35805;&#31574;&#30053;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#31574;&#30053;&#26159;&#26681;&#25454;&#27599;&#20010;&#23545;&#35805;&#36718;&#27425;&#30340;&#24403;&#21069;&#29366;&#24577;&#30830;&#23450;&#31995;&#32479;&#21160;&#20316;&#30340;&#20851;&#38190;&#12290;&#36817;&#24180;&#26469;&#65292;&#24378;&#21270;&#23398;&#20064; (RL) &#24050;&#25104;&#20026;&#23545;&#35805;&#31574;&#30053;&#23398;&#20064; (DPL) &#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#36873;&#25321;&#12290;&#22312;&#22522;&#20110; RL &#30340; DPL &#20013;&#65292;&#26681;&#25454;&#22870;&#21169;&#26356;&#26032;&#23545;&#35805;&#31574;&#30053;&#12290;&#23545;&#20110;&#20855;&#26377;&#22823;&#37327;&#29366;&#24577;&#21160;&#20316;&#23545;&#32452;&#21512;&#30340;&#22810;&#39046;&#22495;&#20219;&#21153;&#23548;&#21521;&#22411;&#23545;&#35805;&#22330;&#26223;&#65292;&#31934;&#32454;&#26500;&#24314;&#20687;&#29366;&#24577;-&#21160;&#20316;&#30456;&#20851;&#30340;&#22870;&#21169;&#26469;&#26377;&#25928;&#25351;&#23548;&#23545;&#35805;&#31574;&#30053;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#19968;&#31181;&#20174;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#20013;&#20272;&#35745;&#22870;&#21169;&#30340;&#26041;&#24335;&#26159;&#20351;&#29992;&#23545;&#25239;&#23398;&#20064; (AL) &#21516;&#26102;&#35757;&#32451;&#22870;&#21169;&#20272;&#35745;&#22120;&#21644;&#23545;&#35805;&#31574;&#30053;&#12290;&#23613;&#31649;&#36825;&#31181;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#20854;&#22266;&#26377;&#30340;&#23545;&#25239;&#23398;&#20064;&#38382;&#39064;&#65288;&#20363;&#22914;&#27169;&#24335;&#22349;&#32553;&#65289;&#20063;&#21313;&#20998;&#26840;&#25163;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#23545;&#35805;&#31574;&#30053;&#21644;&#22870;&#21169;&#20272;&#35745;&#22120;&#30340;&#30446;&#26631;&#20989;&#25968;&#36827;&#34892;&#35814;&#32454;&#20998;&#26512;&#65292;&#39318;&#20808;&#30830;&#23450;&#20102; AL &#22312; DPL &#20013;&#30340;&#20316;&#29992;&#12290;&#25509;&#19979;&#26469;&#65292;&#22522;&#20110;&#27492;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialog policies, which determine a system's action based on the current state at each dialog turn, are crucial to the success of the dialog. In recent years, reinforcement learning (RL) has emerged as a promising option for dialog policy learning (DPL). In RL-based DPL, dialog policies are updated according to rewards. The manual construction of fine-grained rewards, such as state-action-based ones, to effectively guide the dialog policy is challenging in multi-domain task-oriented dialog scenarios with numerous state-action pair combinations. One way to estimate rewards from collected data is to train the reward estimator and dialog policy simultaneously using adversarial learning (AL). Although this method has demonstrated superior performance experimentally, it is fraught with the inherent problems of AL, such as mode collapse. This paper first identifies the role of AL in DPL through detailed analyses of the objective functions of dialog policy and reward estimator. Next, based on 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#30340;&#26080;&#30417;&#30563;&#26657;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#20808;&#39564;&#31867;&#21035;&#20998;&#24067;&#65292;&#23454;&#29616;&#22312;&#27809;&#26377;&#26631;&#35760;&#26679;&#26412;&#21644;&#20165;&#23569;&#37327;&#39046;&#22495;&#20869;&#26679;&#26412;&#26597;&#35810;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.06713</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#26080;&#30417;&#30563;&#26657;&#20934;&#30340;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#30340;&#20808;&#39564;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Calibration through Prior Adaptation for Text Classification using Large Language Models. (arXiv:2307.06713v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#30340;&#26080;&#30417;&#30563;&#26657;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#20808;&#39564;&#31867;&#21035;&#20998;&#24067;&#65292;&#23454;&#29616;&#22312;&#27809;&#26377;&#26631;&#35760;&#26679;&#26412;&#21644;&#20165;&#23569;&#37327;&#39046;&#22495;&#20869;&#26679;&#26412;&#26597;&#35810;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#26377;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#27491;&#22312;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#30740;&#31350;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#36890;&#36807;&#22823;&#37327;&#26080;&#30417;&#30563;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#12289;&#26657;&#20934;&#25110;&#19978;&#19979;&#25991;&#23398;&#20064;&#31561;&#26041;&#27861;&#36827;&#34892;&#36866;&#24212;&#20197;&#25191;&#34892;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#20808;&#39564;&#31867;&#21035;&#20998;&#24067;&#65292;&#23454;&#29616;&#22312;&#27809;&#26377;&#26631;&#35760;&#26679;&#26412;&#21644;&#20165;&#23569;&#37327;&#39046;&#22495;&#20869;&#26679;&#26412;&#26597;&#35810;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#23558;LLM&#35270;&#20026;&#40657;&#30418;&#65292;&#22312;&#27169;&#22411;&#23631;&#38556;&#20013;&#28155;&#21152;&#20102;&#19968;&#20010;&#38454;&#27573;&#65292;&#29992;&#20110;&#26657;&#20934;&#27169;&#22411;&#21518;&#39564;&#20197;&#23436;&#25104;&#20219;&#21153;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#37327;&#30340;&#25552;&#31034;&#35757;&#32451;&#26679;&#26412;&#21644;&#26080;&#36866;&#24212;&#25968;&#25454;&#19979;&#30340;&#26657;&#20934;&#26041;&#27861;&#20013;&#20248;&#20110;&#26410;&#36866;&#24212;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
A wide variety of natural language tasks are currently being addressed with large-scale language models (LLMs). These models are usually trained with a very large amount of unsupervised text data and adapted to perform a downstream natural language task using methods like fine-tuning, calibration or in-context learning. In this work, we propose an approach to adapt the prior class distribution to perform text classification tasks without the need for labelled samples and only few in-domain sample queries. The proposed approach treats the LLM as a black box, adding a stage where the model posteriors are calibrated to the task. Results show that these methods outperform the un-adapted model for different number of training shots in the prompt and a previous approach were calibration is performed without using any adaptation data.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#26222;&#36890;&#20154;&#22312;&#38754;&#20020;&#38544;&#31169;&#23041;&#32961;&#24773;&#22659;&#26102;&#30340;&#20915;&#31574;&#34892;&#20026;&#65292;&#20197;&#21450;&#20182;&#20204;&#24895;&#24847;&#20026;&#32473;&#20104;&#24046;&#20998;&#38544;&#31169;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#25552;&#20379;&#25935;&#24863;&#25968;&#25454;&#25152;&#25215;&#25285;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2307.06708</link><description>&lt;p&gt;
&#26159;&#21542;&#20998;&#20139;&#65311;&#32473;&#20104;&#24046;&#20998;&#38544;&#31169;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#25935;&#24863;&#25968;&#25454;&#30340;&#26222;&#36890;&#20154;&#25509;&#21463;&#20160;&#20040;&#39118;&#38505;&#65311;
&lt;/p&gt;
&lt;p&gt;
To share or not to share: What risks would laypeople accept to give sensitive data to differentially-private NLP systems?. (arXiv:2307.06708v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06708
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#26222;&#36890;&#20154;&#22312;&#38754;&#20020;&#38544;&#31169;&#23041;&#32961;&#24773;&#22659;&#26102;&#30340;&#20915;&#31574;&#34892;&#20026;&#65292;&#20197;&#21450;&#20182;&#20204;&#24895;&#24847;&#20026;&#32473;&#20104;&#24046;&#20998;&#38544;&#31169;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#25552;&#20379;&#25935;&#24863;&#25968;&#25454;&#25152;&#25215;&#25285;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;NLP&#31038;&#21306;&#24050;&#32463;&#37319;&#29992;&#20013;&#24515;&#24046;&#20998;&#38544;&#31169;&#20316;&#20026;&#20445;&#25252;&#38544;&#31169;&#30340;&#27169;&#22411;&#35757;&#32451;&#25110;&#25968;&#25454;&#20849;&#20139;&#30340;&#39318;&#36873;&#26694;&#26550;&#65292;&#20294;&#20915;&#23450;&#24615;&#30340;&#20851;&#38190;&#21442;&#25968;&#8212;&#8212;&#25511;&#21046;&#38544;&#31169;&#20445;&#25252;&#24378;&#24230;&#30340;&#38544;&#31169;&#39044;&#31639;&#949;&#30340;&#36873;&#25321;&#21644;&#35299;&#37322;&#20173;&#28982;&#30456;&#24403;&#38543;&#24847;&#12290;&#25105;&#20204;&#35748;&#20026;&#30830;&#23450;&#949;&#20540;&#19981;&#24212;&#35813;&#20165;&#30001;&#30740;&#31350;&#20154;&#21592;&#25110;&#31995;&#32479;&#24320;&#21457;&#32773;&#20915;&#23450;&#65292;&#36824;&#24517;&#39035;&#32771;&#34385;&#37027;&#20123;&#20849;&#20139;&#20182;&#20204;&#28508;&#22312;&#25935;&#24863;&#25968;&#25454;&#30340;&#20154;&#12290;&#25442;&#21477;&#35805;&#35828;&#65306;&#20320;&#24895;&#24847;&#20026;&#949;&#20540;&#20026;10&#32780;&#20998;&#20139;&#20320;&#30340;&#21363;&#26102;&#28040;&#24687;&#21527;&#65311;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#12289;&#23454;&#26045;&#21644;&#36827;&#34892;&#34892;&#20026;&#23454;&#39564;(311&#21517;&#26222;&#36890;&#21442;&#19982;&#32773;)&#26469;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#30740;&#31350;&#20154;&#20204;&#22312;&#19981;&#30830;&#23450;&#20915;&#31574;&#29615;&#22659;&#19979;&#38754;&#23545;&#23041;&#32961;&#38544;&#31169;&#30340;&#24773;&#22659;&#26102;&#30340;&#34892;&#20026;&#12290;&#36890;&#36807;&#23558;&#39118;&#38505;&#24863;&#30693;&#26694;&#26550;&#21270;&#20026;&#20004;&#20010;&#29616;&#23454;&#30340;NLP&#22330;&#26223;&#65292;&#24182;&#20351;&#29992;&#24773;&#33410;&#34892;&#20026;&#30740;&#31350;&#65292;&#25105;&#20204;&#33021;&#22815;&#30830;&#23450;&#21738;&#20123;&#949;&#38408;&#20540;&#23558;&#23548;&#33268;&#20849;&#20139;&#34892;&#20026;&#30340;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although the NLP community has adopted central differential privacy as a go-to framework for privacy-preserving model training or data sharing, the choice and interpretation of the key parameter, privacy budget $\varepsilon$ that governs the strength of privacy protection, remains largely arbitrary. We argue that determining the $\varepsilon$ value should not be solely in the hands of researchers or system developers, but must also take into account the actual people who share their potentially sensitive data. In other words: Would you share your instant messages for $\varepsilon$ of 10? We address this research gap by designing, implementing, and conducting a behavioral experiment (311 lay participants) to study the behavior of people in uncertain decision-making situations with respect to privacy-threatening situations. Framing the risk perception in terms of two realistic NLP scenarios and using a vignette behavioral study help us determine what $\varepsilon$ thresholds would lead l
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#24847;&#22270;&#26657;&#20934;&#30340;&#33258;&#25105;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#20013;&#31572;&#26696;&#36873;&#25321;&#30340;&#36136;&#37327;&#12290;&#36890;&#36807;&#20351;&#29992;&#24847;&#22270;&#26631;&#31614;&#26469;&#26657;&#20934;&#31572;&#26696;&#26631;&#31614;&#65292;&#20316;&#32773;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#25928;&#26524;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.06703</link><description>&lt;p&gt;
&#24847;&#22270;&#26657;&#20934;&#30340;&#33258;&#25105;&#35757;&#32451;&#29992;&#20110;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#20013;&#30340;&#31572;&#26696;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Intent-calibrated Self-training for Answer Selection in Open-domain Dialogues. (arXiv:2307.06703v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06703
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#24847;&#22270;&#26657;&#20934;&#30340;&#33258;&#25105;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#20013;&#31572;&#26696;&#36873;&#25321;&#30340;&#36136;&#37327;&#12290;&#36890;&#36807;&#20351;&#29992;&#24847;&#22270;&#26631;&#31614;&#26469;&#26657;&#20934;&#31572;&#26696;&#26631;&#31614;&#65292;&#20316;&#32773;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#25928;&#26524;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#20013;&#30340;&#31572;&#26696;&#36873;&#25321;&#26088;&#22312;&#20174;&#20505;&#36873;&#31572;&#26696;&#20013;&#36873;&#25321;&#20934;&#30830;&#30340;&#31572;&#26696;&#12290;&#26368;&#36817;&#31572;&#26696;&#36873;&#25321;&#27169;&#22411;&#30340;&#25104;&#21151;&#20381;&#36182;&#20110;&#20351;&#29992;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#26159;&#19968;&#39033;&#32791;&#26102;&#19988;&#36153;&#21147;&#30340;&#24037;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#39044;&#27979;&#30340;&#24847;&#22270;&#26631;&#31614;&#20197;&#26657;&#20934;&#33258;&#25105;&#35757;&#32451;&#33539;&#24335;&#20013;&#30340;&#31572;&#26696;&#26631;&#31614;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24847;&#22270;&#26657;&#20934;&#30340;&#33258;&#25105;&#35757;&#32451;&#65288;ICAST&#65289;&#26469;&#36890;&#36807;&#24847;&#22270;&#26657;&#20934;&#30340;&#31572;&#26696;&#36873;&#25321;&#33539;&#24335;&#25552;&#39640;&#20266;&#31572;&#26696;&#26631;&#31614;&#30340;&#36136;&#37327;&#65292;&#20854;&#20013;&#25105;&#20204;&#20351;&#29992;&#20266;&#24847;&#22270;&#26631;&#31614;&#26469;&#24110;&#21161;&#25913;&#36827;&#20266;&#31572;&#26696;&#26631;&#31614;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#28041;&#21450;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#21482;&#26377;1&#65285;&#12289;5&#65285;&#21644;10&#65285;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;ICAST&#22987;&#32456;&#20248;&#20110;&#22522;&#32447;&#12290;&#29305;&#21035;&#26159;&#19982;&#20165;&#26377;5&#65285;&#26631;&#35760;&#25968;&#25454;&#30340;&#26368;&#24378;&#22522;&#32447;&#30456;&#27604;&#65292;&#23427;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#23558;F1&#24471;&#20998;&#20998;&#21035;&#25552;&#39640;&#20102;2.06&#65285;&#21644;1.00&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Answer selection in open-domain dialogues aims to select an accurate answer from candidates. Recent success of answer selection models hinges on training with large amounts of labeled data. However, collecting large-scale labeled data is labor-intensive and time-consuming. In this paper, we introduce the predicted intent labels to calibrate answer labels in a self-training paradigm. Specifically, we propose the intent-calibrated self-training (ICAST) to improve the quality of pseudo answer labels through the intent-calibrated answer selection paradigm, in which we employ pseudo intent labels to help improve pseudo answer labels. We carry out extensive experiments on two benchmark datasets with open-domain dialogues. The experimental results show that ICAST outperforms baselines consistently with 1%, 5% and 10% labeled data. Specifically, it improves 2.06% and 1.00% of F1 score on the two datasets, compared with the strongest baseline with only 5% labeled data.
&lt;/p&gt;</description></item><item><title>Parmesan&#26159;&#19968;&#20010;&#21407;&#22411;&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;&#19978;&#19979;&#25991;&#20013;&#25628;&#32034;&#21644;&#23450;&#20041;&#25968;&#23398;&#27010;&#24565;&#65292;&#29305;&#21035;&#20851;&#27880;&#33539;&#30068;&#35770;&#39046;&#22495;&#12290;&#35813;&#31995;&#32479;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#32452;&#20214;&#36827;&#34892;&#27010;&#24565;&#25552;&#21462;&#12289;&#20851;&#31995;&#25552;&#21462;&#12289;&#23450;&#20041;&#25552;&#21462;&#21644;&#23454;&#20307;&#38142;&#25509;&#12290;&#36890;&#36807;&#35813;&#31995;&#32479;&#30340;&#24320;&#21457;&#65292;&#21487;&#20197;&#35299;&#20915;&#29616;&#26377;&#25216;&#26415;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#33539;&#30068;&#35770;&#39046;&#22495;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#20004;&#20010;&#25968;&#23398;&#35821;&#26009;&#24211;&#20197;&#25903;&#25345;&#31995;&#32479;&#30340;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.06699</link><description>&lt;p&gt;
Parmesan&#65306;&#25945;&#32946;&#20013;&#30340;&#25968;&#23398;&#27010;&#24565;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Parmesan: mathematical concept extraction for education. (arXiv:2307.06699v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06699
&lt;/p&gt;
&lt;p&gt;
Parmesan&#26159;&#19968;&#20010;&#21407;&#22411;&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;&#19978;&#19979;&#25991;&#20013;&#25628;&#32034;&#21644;&#23450;&#20041;&#25968;&#23398;&#27010;&#24565;&#65292;&#29305;&#21035;&#20851;&#27880;&#33539;&#30068;&#35770;&#39046;&#22495;&#12290;&#35813;&#31995;&#32479;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#32452;&#20214;&#36827;&#34892;&#27010;&#24565;&#25552;&#21462;&#12289;&#20851;&#31995;&#25552;&#21462;&#12289;&#23450;&#20041;&#25552;&#21462;&#21644;&#23454;&#20307;&#38142;&#25509;&#12290;&#36890;&#36807;&#35813;&#31995;&#32479;&#30340;&#24320;&#21457;&#65292;&#21487;&#20197;&#35299;&#20915;&#29616;&#26377;&#25216;&#26415;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#33539;&#30068;&#35770;&#39046;&#22495;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#20004;&#20010;&#25968;&#23398;&#35821;&#26009;&#24211;&#20197;&#25903;&#25345;&#31995;&#32479;&#30340;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#26159;&#19968;&#20010;&#39640;&#24230;&#19987;&#19994;&#21270;&#30340;&#39046;&#22495;&#65292;&#20855;&#26377;&#33258;&#24049;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#20294;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#21364;&#26377;&#38480;&#12290;&#28982;&#32780;&#65292;&#25968;&#23398;&#22312;&#35768;&#22810;&#19981;&#21516;&#39046;&#22495;&#30340;&#36328;&#23398;&#31185;&#30740;&#31350;&#20013;&#32463;&#24120;&#20381;&#36182;&#20110;&#23545;&#25968;&#23398;&#27010;&#24565;&#30340;&#29702;&#35299;&#12290;&#20026;&#20102;&#24110;&#21161;&#26469;&#33258;&#20854;&#20182;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21407;&#22411;&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;&#19978;&#19979;&#25991;&#20013;&#25628;&#32034;&#21644;&#23450;&#20041;&#25968;&#23398;&#27010;&#24565;&#65292;&#37325;&#28857;&#20851;&#27880;&#33539;&#30068;&#35770;&#39046;&#22495;&#12290;&#36825;&#20010;&#31995;&#32479;&#21517;&#20026;Parmesan&#65292;&#20381;&#36182;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#32452;&#20214;&#65292;&#21253;&#25324;&#27010;&#24565;&#25552;&#21462;&#12289;&#20851;&#31995;&#25552;&#21462;&#12289;&#23450;&#20041;&#25552;&#21462;&#21644;&#23454;&#20307;&#38142;&#25509;&#12290;&#22312;&#24320;&#21457;&#36825;&#20010;&#31995;&#32479;&#30340;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#26377;&#25216;&#26415;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#33539;&#30068;&#35770;&#39046;&#22495;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#25216;&#26415;&#65292;&#36825;&#31181;&#25216;&#26415;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#25105;&#20204;&#39044;&#35745;&#31995;&#32479;&#23558;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#32780;&#19981;&#26029;&#28436;&#21464;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20004;&#20010;&#28165;&#29702;&#36807;&#30340;&#25968;&#23398;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#25903;&#25345;&#21407;&#22411;&#31995;&#32479;&#65292;&#36825;&#20123;&#35821;&#26009;&#24211;&#22522;&#20110;&#26399;&#21002;&#25991;&#31456;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mathematics is a highly specialized domain with its own unique set of challenges that has seen limited study in natural language processing. However, mathematics is used in a wide variety of fields and multidisciplinary research in many different domains often relies on an understanding of mathematical concepts. To aid researchers coming from other fields, we develop a prototype system for searching for and defining mathematical concepts in context, focusing on the field of category theory. This system, Parmesan, depends on natural language processing components including concept extraction, relation extraction, definition extraction, and entity linking. In developing this system, we show that existing techniques cannot be applied directly to the category theory domain, and suggest hybrid techniques that do perform well, though we expect the system to evolve over time. We also provide two cleaned mathematical corpora that power the prototype system, which are based on journal articles 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;GLORY&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20840;&#23616;&#22270;&#19982;&#26412;&#22320;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#22686;&#24378;&#20102;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#26500;&#24314;&#20840;&#23616;&#24863;&#30693;&#21382;&#21490;&#26032;&#38395;&#32534;&#30721;&#22120;&#26469;&#34701;&#21512;&#21382;&#21490;&#26032;&#38395;&#34920;&#31034;&#65292;&#24182;&#32771;&#34385;&#20102;&#29992;&#25143;&#38544;&#34255;&#30340;&#21160;&#26426;&#21644;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2307.06576</link><description>&lt;p&gt;
&#36229;&#36234;&#26412;&#22320;&#33539;&#22260;&#65306;&#20840;&#29699;&#22270;&#22686;&#24378;&#20010;&#24615;&#21270;&#26032;&#38395;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Going Beyond Local: Global Graph-Enhanced Personalized News Recommendations. (arXiv:2307.06576v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06576
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;GLORY&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20840;&#23616;&#22270;&#19982;&#26412;&#22320;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#22686;&#24378;&#20102;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#26500;&#24314;&#20840;&#23616;&#24863;&#30693;&#21382;&#21490;&#26032;&#38395;&#32534;&#30721;&#22120;&#26469;&#34701;&#21512;&#21382;&#21490;&#26032;&#38395;&#34920;&#31034;&#65292;&#24182;&#32771;&#34385;&#20102;&#29992;&#25143;&#38544;&#34255;&#30340;&#21160;&#26426;&#21644;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#22320;&#21521;&#29992;&#25143;&#25512;&#33616;&#20505;&#36873;&#26032;&#38395;&#25991;&#31456;&#19968;&#30452;&#26159;&#20010;&#24615;&#21270;&#26032;&#38395;&#25512;&#33616;&#31995;&#32479;&#30340;&#26680;&#24515;&#25361;&#25112;&#12290;&#22823;&#22810;&#25968;&#36817;&#26399;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20351;&#29992;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#20174;&#20016;&#23500;&#30340;&#25991;&#26412;&#25968;&#25454;&#20013;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#65292;&#20351;&#29992;&#20174;&#26412;&#22320;&#21382;&#21490;&#26032;&#38395;&#27966;&#29983;&#30340;&#22522;&#20110;&#20869;&#23481;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#32570;&#20047;&#20840;&#23616;&#35270;&#35282;&#65292;&#26410;&#33021;&#32771;&#34385;&#29992;&#25143;&#38544;&#34255;&#30340;&#21160;&#26426;&#21644;&#34892;&#20026;&#65292;&#36229;&#36234;&#35821;&#20041;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411; GLORY&#65288;Global-LOcal news Recommendation sYstem&#65289;&#65292;&#23427;&#32467;&#21512;&#20102;&#20174;&#20854;&#20182;&#29992;&#25143;&#23398;&#21040;&#30340;&#20840;&#23616;&#34920;&#31034;&#21644;&#26412;&#22320;&#34920;&#31034;&#65292;&#26469;&#22686;&#24378;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#20840;&#23616;&#24863;&#30693;&#21382;&#21490;&#26032;&#38395;&#32534;&#30721;&#22120;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#20840;&#23616;&#26032;&#38395;&#22270;&#65292;&#24182;&#20351;&#29992;&#38376;&#25511;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#20016;&#23500;&#26032;&#38395;&#34920;&#31034;&#65292;&#20174;&#32780;&#36890;&#36807;&#21382;&#21490;&#26032;&#38395;&#32858;&#21512;&#22120;&#34701;&#21512;&#21382;&#21490;&#26032;&#38395;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Precisely recommending candidate news articles to users has always been a core challenge for personalized news recommendation systems. Most recent works primarily focus on using advanced natural language processing techniques to extract semantic information from rich textual data, employing content-based methods derived from local historical news. However, this approach lacks a global perspective, failing to account for users' hidden motivations and behaviors beyond semantic information. To address this challenge, we propose a novel model called GLORY (Global-LOcal news Recommendation sYstem), which combines global representations learned from other users with local representations to enhance personalized recommendation systems. We accomplish this by constructing a Global-aware Historical News Encoder, which includes a global news graph and employs gated graph neural networks to enrich news representations, thereby fusing historical news representations by a historical news aggregator.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#24494;&#21338;&#25968;&#25454;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#65292;&#21462;&#24471;&#20102;&#32422;0.73&#30340;&#23439;&#24179;&#22343;F1&#20998;&#25968;&#65292;&#32467;&#26524;&#34920;&#26126;&#20102;CNN&#22312;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#23545;&#31038;&#20132;&#23186;&#20307;&#20998;&#26512;&#12289;&#24066;&#22330;&#30740;&#31350;&#21644;&#25919;&#31574;&#30740;&#31350;&#31561;&#39046;&#22495;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2307.06540</link><description>&lt;p&gt;
&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#24494;&#21338;&#24773;&#24863;&#20998;&#26512;&#65306;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Convolutional Neural Networks for Sentiment Analysis on Weibo Data: A Natural Language Processing Approach. (arXiv:2307.06540v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#24494;&#21338;&#25968;&#25454;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#65292;&#21462;&#24471;&#20102;&#32422;0.73&#30340;&#23439;&#24179;&#22343;F1&#20998;&#25968;&#65292;&#32467;&#26524;&#34920;&#26126;&#20102;CNN&#22312;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#23545;&#31038;&#20132;&#23186;&#20307;&#20998;&#26512;&#12289;&#24066;&#22330;&#30740;&#31350;&#21644;&#25919;&#31574;&#30740;&#31350;&#31561;&#39046;&#22495;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#23545;&#26469;&#33258;&#24494;&#21338;&#30340;119,988&#26465;&#21407;&#22987;&#25512;&#25991;&#36827;&#34892;&#20102;&#24773;&#24863;&#20998;&#26512;&#65292;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;&#25968;&#25454;&#26159;&#20174;&#30334;&#24230;&#30340;PaddlePaddle AI&#24179;&#21488;&#33719;&#21462;&#30340;&#65292;&#32463;&#36807;&#20102;&#31934;&#32454;&#30340;&#39044;&#22788;&#29702;&#12289;&#20998;&#35789;&#21644;&#24773;&#24863;&#26631;&#31614;&#20998;&#31867;&#12290;&#21033;&#29992;&#22522;&#20110;&#35789;&#23884;&#20837;&#30340;CNN&#27169;&#22411;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#21644;&#24773;&#24863;&#20998;&#31867;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#22312;&#27979;&#35797;&#38598;&#19978;&#33719;&#24471;&#20102;&#32422;0.73&#30340;&#23439;&#24179;&#22343;F1&#20998;&#25968;&#65292;&#26174;&#31034;&#20102;&#23545;&#27491;&#38754;&#12289;&#20013;&#24615;&#21644;&#36127;&#38754;&#24773;&#24863;&#30340;&#24179;&#34913;&#34920;&#29616;&#12290;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;CNN&#22312;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#23545;&#31038;&#20132;&#23186;&#20307;&#20998;&#26512;&#12289;&#24066;&#22330;&#30740;&#31350;&#21644;&#25919;&#31574;&#30740;&#31350;&#31561;&#23454;&#38469;&#24212;&#29992;&#26377;&#30528;&#37325;&#35201;&#24847;&#20041;&#12290;&#23436;&#25972;&#30340;&#23454;&#39564;&#20869;&#23481;&#21644;&#20195;&#30721;&#24050;&#22312;Kaggle&#25968;&#25454;&#24179;&#21488;&#19978;&#20844;&#24320;&#25552;&#20379;&#20197;&#36827;&#34892;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study addressed the complex task of sentiment analysis on a dataset of 119,988 original tweets from Weibo using a Convolutional Neural Network (CNN), offering a new approach to Natural Language Processing (NLP). The data, sourced from Baidu's PaddlePaddle AI platform, were meticulously preprocessed, tokenized, and categorized based on sentiment labels. A CNN-based model was utilized, leveraging word embeddings for feature extraction, and trained to perform sentiment classification. The model achieved a macro-average F1-score of approximately 0.73 on the test set, showing balanced performance across positive, neutral, and negative sentiments. The findings underscore the effectiveness of CNNs for sentiment analysis tasks, with implications for practical applications in social media analysis, market research, and policy studies. The complete experimental content and code have been made publicly available on the Kaggle data platform for further research and development. Future work ma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#21040;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#30340;&#38382;&#39064;&#65292;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#36827;&#34892;ASR&#24212;&#29992;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.06530</link><description>&lt;p&gt;
&#25506;&#32034;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#21040;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20013;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploring the Integration of Large Language Models into Automatic Speech Recognition Systems: An Empirical Study. (arXiv:2307.06530v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#21040;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#30340;&#38382;&#39064;&#65292;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#36827;&#34892;ASR&#24212;&#29992;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38598;&#25104;&#21040;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#20013;&#20197;&#25552;&#39640;&#36716;&#24405;&#20934;&#30830;&#24615;&#30340;&#38382;&#39064;&#12290;LLMs&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#20197;&#20854;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#21644;&#36981;&#24490;&#25351;&#20196;&#30340;&#34892;&#20026;&#24341;&#36215;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#26497;&#22823;&#20851;&#27880;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#37325;&#28857;&#26159;&#30740;&#31350;&#21033;&#29992;LLM&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#26469;&#25552;&#21319;ASR&#31995;&#32479;&#24615;&#33021;&#30340;&#28508;&#21147;&#65292;&#30446;&#21069;&#36825;&#20123;&#31995;&#32479;&#38754;&#20020;&#29615;&#22659;&#22122;&#38899;&#12289;&#35828;&#35805;&#20154;&#21475;&#38899;&#21644;&#22797;&#26434;&#35821;&#35328;&#29615;&#22659;&#31561;&#25361;&#25112;&#12290;&#25105;&#20204;&#20351;&#29992;Aishell-1&#21644;LibriSpeech&#25968;&#25454;&#38598;&#35774;&#35745;&#20102;&#19968;&#39033;&#30740;&#31350;&#65292;ChatGPT&#21644;GPT-4&#20316;&#20026;LLM&#33021;&#21147;&#30340;&#22522;&#20934;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#21021;&#27493;&#23454;&#39564;&#32467;&#26524;&#24182;&#19981;&#29702;&#24819;&#65292;&#34920;&#26126;&#21033;&#29992;LLM&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#36827;&#34892;ASR&#24212;&#29992;&#30340;&#22797;&#26434;&#24615;&#12290;&#23613;&#31649;&#25105;&#20204;&#36827;&#19968;&#27493;&#23581;&#35797;&#20102;&#19981;&#21516;&#30340;&#35774;&#32622;&#21644;&#27169;&#22411;&#65292;&#20294;LLMs&#32416;&#27491;&#30340;&#21477;&#23376;&#39057;&#32321;&#20986;&#38169;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the integration of Large Language Models (LLMs) into Automatic Speech Recognition (ASR) systems to improve transcription accuracy. The increasing sophistication of LLMs, with their in-context learning capabilities and instruction-following behavior, has drawn significant attention in the field of Natural Language Processing (NLP). Our primary focus is to investigate the potential of using an LLM's in-context learning capabilities to enhance the performance of ASR systems, which currently face challenges such as ambient noise, speaker accents, and complex linguistic contexts. We designed a study using the Aishell-1 and LibriSpeech datasets, with ChatGPT and GPT-4 serving as benchmarks for LLM capabilities. Unfortunately, our initial experiments did not yield promising results, indicating the complexity of leveraging LLM's in-context learning for ASR applications. Despite further exploration with varied settings and models, the corrected sentences from the LLMs freque
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#20004;&#26041;&#22810;&#38382;&#39064;&#35848;&#21028;&#30340;&#21327;&#35758;&#36319;&#36394;&#20219;&#21153;&#65292;&#36890;&#36807;&#20351;&#29992;GPT-3&#26500;&#24314;&#20102;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#24182;&#36801;&#31227;&#23398;&#20064;T5&#27169;&#22411;&#65292;&#22312;&#35299;&#20915;&#32570;&#20047;&#26631;&#27880;&#25968;&#25454;&#30340;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#24378;&#26377;&#21147;&#30340;&#21021;&#22987;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2307.06524</link><description>&lt;p&gt;
&#22810;&#38382;&#39064;&#35848;&#21028;&#23545;&#35805;&#30340;&#21327;&#35758;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
Agreement Tracking for Multi-Issue Negotiation Dialogues. (arXiv:2307.06524v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06524
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#20004;&#26041;&#22810;&#38382;&#39064;&#35848;&#21028;&#30340;&#21327;&#35758;&#36319;&#36394;&#20219;&#21153;&#65292;&#36890;&#36807;&#20351;&#29992;GPT-3&#26500;&#24314;&#20102;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#24182;&#36801;&#31227;&#23398;&#20064;T5&#27169;&#22411;&#65292;&#22312;&#35299;&#20915;&#32570;&#20047;&#26631;&#27880;&#25968;&#25454;&#30340;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#24378;&#26377;&#21147;&#30340;&#21021;&#22987;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#35848;&#21028;&#25903;&#25345;&#31995;&#32479;&#30340;&#30446;&#26631;&#26159;&#24110;&#21161;&#20154;&#31867;&#35848;&#21028;&#32773;&#22312;&#22810;&#38382;&#39064;&#35848;&#21028;&#20013;&#23454;&#29616;&#26356;&#26377;&#21033;&#30340;&#32467;&#26524;&#65288;&#20363;&#22914;&#65292;&#38599;&#20027;&#21644;&#20505;&#36873;&#20154;&#22312;&#24037;&#20316;&#25552;&#35758;&#20043;&#21069;&#23601;&#34218;&#27700;&#12289;&#24037;&#26102;&#21644;&#26187;&#21319;&#31561;&#38382;&#39064;&#36827;&#34892;&#35848;&#21028;&#65289;&#12290;&#20026;&#20102;&#25104;&#21151;&#65292;&#36825;&#20123;&#31995;&#32479;&#24517;&#39035;&#23454;&#26102;&#20934;&#30830;&#22320;&#36319;&#36394;&#21442;&#19982;&#32773;&#36798;&#25104;&#30340;&#21327;&#35758;&#12290;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#38598;&#20013;&#20110;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#65292;&#35201;&#20040;&#20135;&#29983;&#38750;&#32467;&#26500;&#21270;&#30340;&#36755;&#20986;&#65292;&#20351;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#27492;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;&#20004;&#26041;&#22810;&#38382;&#39064;&#35848;&#21028;&#30340;&#21327;&#35758;&#36319;&#36394;&#30340;&#26032;&#20219;&#21153;&#65292;&#36825;&#35201;&#27714;&#22312;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#29366;&#24577;&#31354;&#38388;&#20869;&#36830;&#32493;&#30417;&#27979;&#21327;&#35758;&#12290;&#20026;&#20102;&#35299;&#20915;&#32570;&#20047;&#20855;&#26377;&#30495;&#23454;&#22810;&#38382;&#39064;&#35848;&#21028;&#23545;&#35805;&#30340;&#26631;&#27880;&#35821;&#26009;&#24211;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;GPT-3&#26500;&#24314;&#20102;GPT-Negochat&#65292;&#19968;&#20221;&#25105;&#20204;&#20844;&#24320;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;MultiWOZ 2.4&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#65292;&#32473;&#20986;&#20102;&#25105;&#20204;&#20219;&#21153;&#30340;&#24378;&#26377;&#21147;&#30340;&#21021;&#22987;&#22522;&#32447;&#65292;&#23545;T5&#27169;&#22411;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#21253;&#25324;T5-small&#21644;T5-...
&lt;/p&gt;
&lt;p&gt;
Automated negotiation support systems aim to help human negotiators reach more favorable outcomes in multi-issue negotiations (e.g., an employer and a candidate negotiating over issues such as salary, hours, and promotions before a job offer). To be successful, these systems must accurately track agreements reached by participants in real-time. Existing approaches either focus on task-oriented dialogues or produce unstructured outputs, rendering them unsuitable for this objective. Our work introduces the novel task of agreement tracking for two-party multi-issue negotiations, which requires continuous monitoring of agreements within a structured state space. To address the scarcity of annotated corpora with realistic multi-issue negotiation dialogues, we use GPT-3 to build GPT-Negochat, a synthesized dataset that we make publicly available. We present a strong initial baseline for our task by transfer-learning a T5 model trained on the MultiWOZ 2.4 corpus. Pre-training T5-small and T5-
&lt;/p&gt;</description></item><item><title>&#20256;&#25773;&#23398;&#39046;&#22495;&#20013;&#30340;&#33258;&#21160;&#21270;&#20869;&#23481;&#20998;&#26512;&#24120;&#24573;&#35270;&#20102;&#38169;&#35823;&#20998;&#31867;&#30340;&#20559;&#24046;&#65292;&#25105;&#20204;&#20171;&#32461;&#24182;&#27979;&#35797;&#20102;&#32479;&#35745;&#26041;&#27861;&#26469;&#32416;&#27491;&#36825;&#31181;&#20559;&#24046;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#20462;&#22797;&#20043;&#12290;</title><link>http://arxiv.org/abs/2307.06483</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#20869;&#23481;&#20998;&#26512;&#20013;&#30340;&#38169;&#35823;&#20998;&#31867;&#23548;&#33268;&#22238;&#24402;&#20998;&#26512;&#20013;&#30340;&#20559;&#24046;&#12290;&#25105;&#20204;&#33021;&#20462;&#22797;&#21527;&#65311;&#26159;&#30340;&#65292;&#25105;&#20204;&#33021;&#65281;
&lt;/p&gt;
&lt;p&gt;
Misclassification in Automated Content Analysis Causes Bias in Regression. Can We Fix It? Yes We Can!. (arXiv:2307.06483v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06483
&lt;/p&gt;
&lt;p&gt;
&#20256;&#25773;&#23398;&#39046;&#22495;&#20013;&#30340;&#33258;&#21160;&#21270;&#20869;&#23481;&#20998;&#26512;&#24120;&#24573;&#35270;&#20102;&#38169;&#35823;&#20998;&#31867;&#30340;&#20559;&#24046;&#65292;&#25105;&#20204;&#20171;&#32461;&#24182;&#27979;&#35797;&#20102;&#32479;&#35745;&#26041;&#27861;&#26469;&#32416;&#27491;&#36825;&#31181;&#20559;&#24046;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#20462;&#22797;&#20043;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20998;&#31867;&#22120;&#65288;ACs&#65289;&#36890;&#24120;&#36890;&#36807;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#65288;SML&#65289;&#26500;&#24314;&#65292;&#21487;&#20197;&#23545;&#20174;&#25991;&#26412;&#21040;&#22270;&#29255;&#21644;&#35270;&#39057;&#30340;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#65292;&#24050;&#32463;&#25104;&#20026;&#20256;&#25773;&#31185;&#23398;&#21644;&#30456;&#20851;&#39046;&#22495;&#20013;&#24191;&#27867;&#27969;&#34892;&#30340;&#27979;&#37327;&#35774;&#22791;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#21363;&#20351;&#26159;&#39640;&#24230;&#20934;&#30830;&#30340;&#20998;&#31867;&#22120;&#20063;&#20250;&#20135;&#29983;&#38169;&#35823;&#65292;&#36825;&#23548;&#33268;&#20102;&#38169;&#35823;&#20998;&#31867;&#30340;&#20559;&#24046;&#21644;&#19979;&#28216;&#20998;&#26512;&#20013;&#35823;&#23548;&#24615;&#30340;&#32467;&#26524;&#65292;&#38500;&#38750;&#36825;&#20123;&#20998;&#26512;&#32771;&#34385;&#21040;&#36825;&#20123;&#38169;&#35823;&#12290;&#36890;&#36807;&#23545;SML&#24212;&#29992;&#30340;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#65292;&#25105;&#20204;&#21457;&#29616;&#20256;&#25773;&#23398;&#32773;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24573;&#35270;&#20102;&#38169;&#35823;&#20998;&#31867;&#30340;&#20559;&#24046;&#12290;&#21407;&#21017;&#19978;&#65292;&#29616;&#26377;&#30340;&#32479;&#35745;&#26041;&#27861;&#21487;&#20197;&#20351;&#29992;&#8220;&#40644;&#37329;&#26631;&#20934;&#8221;&#39564;&#35777;&#25968;&#25454;&#65288;&#22914;&#30001;&#20154;&#31867;&#27880;&#37322;&#32773;&#21019;&#24314;&#30340;&#25968;&#25454;&#65289;&#26469;&#32416;&#27491;&#38169;&#35823;&#20998;&#31867;&#30340;&#20559;&#24046;&#65292;&#24182;&#20135;&#29983;&#19968;&#33268;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#20171;&#32461;&#24182;&#27979;&#35797;&#20102;&#36825;&#20123;&#26041;&#27861;&#65292;&#21253;&#25324;&#25105;&#20204;&#22312;R&#21253;misclassificationmodels&#20013;&#35774;&#35745;&#21644;&#23454;&#29616;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#27169;&#25311;&#26469;&#25581;&#31034;&#27599;&#31181;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated classifiers (ACs), often built via supervised machine learning (SML), can categorize large, statistically powerful samples of data ranging from text to images and video, and have become widely popular measurement devices in communication science and related fields. Despite this popularity, even highly accurate classifiers make errors that cause misclassification bias and misleading results in downstream analyses-unless such analyses account for these errors. As we show in a systematic literature review of SML applications, communication scholars largely ignore misclassification bias. In principle, existing statistical methods can use "gold standard" validation data, such as that created by human annotators, to correct misclassification bias and produce consistent estimates. We introduce and test such methods, including a new method we design and implement in the R package misclassificationmodels, via Monte Carlo simulations designed to reveal each method's limitations, which 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#32842;&#22825;&#22411;AI&#27169;&#22411;ChatGPT&#22312;&#31995;&#32479;&#24615;&#32508;&#36848;&#65288;SR&#65289;&#25991;&#31456;&#31579;&#36873;&#20013;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;ChatGPT&#26159;&#33258;&#21160;&#21270;SR&#36807;&#31243;&#30340;&#21487;&#34892;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2307.06464</link><description>&lt;p&gt;
&#35780;&#20272;ChatGPT&#23545;&#20110;&#31995;&#32479;&#24615;&#32508;&#36848;&#25991;&#31456;&#31579;&#36873;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Assessing the Ability of ChatGPT to Screen Articles for Systematic Reviews. (arXiv:2307.06464v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06464
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#32842;&#22825;&#22411;AI&#27169;&#22411;ChatGPT&#22312;&#31995;&#32479;&#24615;&#32508;&#36848;&#65288;SR&#65289;&#25991;&#31456;&#31579;&#36873;&#20013;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;ChatGPT&#26159;&#33258;&#21160;&#21270;SR&#36807;&#31243;&#30340;&#21487;&#34892;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#30740;&#31350;&#39046;&#22495;&#20869;&#32452;&#32455;&#30693;&#35782;&#65292;&#31995;&#32479;&#24615;&#32508;&#36848;&#65288;SR&#65289;&#20026;&#30740;&#31350;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#32447;&#32034;&#12290;&#26377;&#35777;&#25454;&#34920;&#26126;&#65292;SR&#24050;&#25104;&#20026;&#36719;&#20214;&#24037;&#31243;&#20013;&#19968;&#27969;&#30340;&#33402;&#26415;&#21697;&#12290;&#28982;&#32780;&#65292;SR&#31579;&#36873;&#38454;&#27573;&#25152;&#38656;&#30340;&#32321;&#29712;&#25163;&#21160;&#24037;&#20316;&#20351;&#24471;&#36825;&#20123;&#30740;&#31350;&#21464;&#24471;&#26114;&#36149;&#19988;&#23481;&#26131;&#20986;&#38169;&#12290;&#23613;&#31649;&#20256;&#32479;&#19978;&#35748;&#20026;&#31579;&#36873;&#19981;&#36866;&#21512;&#33258;&#21160;&#21270;&#65292;&#20294;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25903;&#25345;&#30340;&#29983;&#25104;&#24335;AI&#39537;&#21160;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#20986;&#29616;&#23558;&#25913;&#21464;&#36825;&#19968;&#24773;&#20917;&#12290;&#22312;&#26412;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36825;&#20123;&#26032;&#25216;&#26415;&#21457;&#23637;&#33258;&#21160;&#21270;SR&#31579;&#36873;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;ChatGPT&#22312;SR&#25991;&#31456;&#31579;&#36873;&#20013;&#30340;&#19968;&#33268;&#24615;&#12289;&#20998;&#31867;&#24615;&#33021;&#21644;&#25512;&#24191;&#33021;&#21147;&#65292;&#24182;&#23558;&#36825;&#20123;&#25968;&#25454;&#19982;&#20256;&#32479;&#29992;&#20110;SR&#33258;&#21160;&#21270;&#30340;&#20998;&#31867;&#22120;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#26159;&#33258;&#21160;&#21270;SR&#36807;&#31243;&#30340;&#21487;&#34892;&#36873;&#25321;&#65292;&#20294;&#24320;&#21457;&#32773;&#22312;&#38598;&#25104;&#26102;&#38656;&#35201;&#20180;&#32454;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;
By organizing knowledge within a research field, Systematic Reviews (SR) provide valuable leads to steer research. Evidence suggests that SRs have become first-class artifacts in software engineering. However, the tedious manual effort associated with the screening phase of SRs renders these studies a costly and error-prone endeavor. While screening has traditionally been considered not amenable to automation, the advent of generative AI-driven chatbots, backed with large language models is set to disrupt the field. In this report, we propose an approach to leverage these novel technological developments for automating the screening of SRs. We assess the consistency, classification performance, and generalizability of ChatGPT in screening articles for SRs and compare these figures with those of traditional classifiers used in SR automation. Our results indicate that ChatGPT is a viable option to automate the SR processes, but requires careful considerations from developers when integra
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#31639;&#27861;&#65292;&#21253;&#25324;&#21160;&#24577;&#26550;&#26500;&#65292;&#25209;&#37327;&#36873;&#25321;&#21644;&#39640;&#25928;&#20248;&#21270;&#22120;&#12290;&#28982;&#32780;&#65292;&#22312;&#20351;&#29992;&#36825;&#20123;&#31639;&#27861;&#39044;&#35757;&#32451;&#26102;&#65292;&#30456;&#23545;&#20110;&#22522;&#32447;&#26041;&#27861;&#65292;&#23427;&#20204;&#30340;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#19979;&#28216;&#25910;&#30410;&#28040;&#22833;&#20102;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#21327;&#35758;&#26469;&#36827;&#34892;&#35745;&#31639;&#65292;&#24182;&#37322;&#25918;&#20102;&#20195;&#30721;&#26469;&#20419;&#36827;&#39640;&#25928;&#35757;&#32451;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.06440</link><description>&lt;p&gt;
&#27809;&#26377;&#35757;&#32451;&#23601;&#27809;&#26377;&#25910;&#30410;&#65306;&#37325;&#26032;&#23457;&#35270;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models. (arXiv:2307.06440v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#31639;&#27861;&#65292;&#21253;&#25324;&#21160;&#24577;&#26550;&#26500;&#65292;&#25209;&#37327;&#36873;&#25321;&#21644;&#39640;&#25928;&#20248;&#21270;&#22120;&#12290;&#28982;&#32780;&#65292;&#22312;&#20351;&#29992;&#36825;&#20123;&#31639;&#27861;&#39044;&#35757;&#32451;&#26102;&#65292;&#30456;&#23545;&#20110;&#22522;&#32447;&#26041;&#27861;&#65292;&#23427;&#20204;&#30340;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#19979;&#28216;&#25910;&#30410;&#28040;&#22833;&#20102;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#21327;&#35758;&#26469;&#36827;&#34892;&#35745;&#31639;&#65292;&#24182;&#37322;&#25918;&#20102;&#20195;&#30721;&#26469;&#20419;&#36827;&#39640;&#25928;&#35757;&#32451;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35757;&#32451;Transformer-based&#35821;&#35328;&#27169;&#22411;&#25152;&#38656;&#30340;&#35745;&#31639;&#37327;&#24613;&#21095;&#22686;&#21152;&#12290;&#36825;&#19968;&#36235;&#21183;&#20419;&#20351;&#30740;&#31350;&#32773;&#20204;&#24320;&#23637;&#20102;&#38024;&#23545;&#39640;&#25928;&#35757;&#32451;&#31639;&#27861;&#30340;&#30740;&#31350;&#65292;&#26088;&#22312;&#27604;&#26631;&#20934;&#35757;&#32451;&#26356;&#24555;&#22320;&#25913;&#21892;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#19979;&#28216;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#19977;&#31867;&#36825;&#26679;&#30340;&#31639;&#27861;&#65306;&#21160;&#24577;&#26550;&#26500;&#65288;&#23618;&#21472;&#12289;&#23618;&#20002;&#24323;&#65289;&#12289;&#25209;&#37327;&#36873;&#25321;&#65288;&#36873;&#25321;&#24615;&#21453;&#21521;&#20256;&#25773;&#12289;RHO&#25439;&#22833;&#65289;&#21644;&#39640;&#25928;&#20248;&#21270;&#22120;&#65288;Lion&#12289;Sophia&#65289;&#12290;&#24403;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#22312;&#22266;&#23450;&#35745;&#31639;&#39044;&#31639;&#19979;&#23545;BERT&#21644;T5&#36827;&#34892;&#39044;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#30340;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#19979;&#28216;&#25910;&#30410;&#30456;&#23545;&#20110;&#19968;&#20010;&#20855;&#26377;&#23436;&#20840;&#34928;&#20943;&#23398;&#20064;&#29575;&#30340;&#22522;&#32447;&#32780;&#35328;&#20250;&#28040;&#22833;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#35780;&#20272;&#21327;&#35758;&#65292;&#21487;&#20197;&#36890;&#36807;&#23558;&#25152;&#26377;&#35745;&#31639;&#26102;&#38388;&#26144;&#23556;&#21040;&#19968;&#20010;&#31216;&#20026;&#21442;&#32771;&#31995;&#32479;&#26102;&#38388;&#30340;&#21442;&#32771;&#26426;&#22120;&#19978;&#65292;&#22312;&#20219;&#24847;&#26426;&#22120;&#19978;&#36827;&#34892;&#35745;&#31639;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#21327;&#35758;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#65292;&#20197;&#40723;&#21169;&#23545;&#39640;&#25928;&#35757;&#32451;&#30340;&#20005;&#26684;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
The computation necessary for training Transformer-based language models has skyrocketed in recent years. This trend has motivated research on efficient training algorithms designed to improve training, validation, and downstream performance faster than standard training. In this work, we revisit three categories of such algorithms: dynamic architectures (layer stacking, layer dropping), batch selection (selective backprop, RHO loss), and efficient optimizers (Lion, Sophia). When pre-training BERT and T5 with a fixed computation budget using such methods, we find that their training, validation, and downstream gains vanish compared to a baseline with a fully-decayed learning rate. We define an evaluation protocol that enables computation to be done on arbitrary machines by mapping all computation time to a reference machine which we call reference system time. We discuss the limitations of our proposed protocol and release our code to encourage rigorous research in efficient training p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#20026;&#29305;&#23450;&#20219;&#21153;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#25104;&#21151;&#22320;&#25552;&#21319;&#20102;&#22312;&#33647;&#29289;&#19981;&#33391;&#20107;&#20214;&#25552;&#21462;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;&#19982;&#30417;&#30563;&#24335;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#65292;&#20855;&#26377;&#25104;&#26412;&#12289;&#25928;&#29575;&#21644;&#30333;&#30418;&#27169;&#22411;&#35775;&#38382;&#31561;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.06439</link><description>&lt;p&gt;
&#20026;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#25552;&#21462;&#32780;&#33976;&#39311;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#23545;&#33647;&#29289;&#19981;&#33391;&#20107;&#20214;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Distilling Large Language Models for Biomedical Knowledge Extraction: A Case Study on Adverse Drug Events. (arXiv:2307.06439v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#20026;&#29305;&#23450;&#20219;&#21153;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#25104;&#21151;&#22320;&#25552;&#21319;&#20102;&#22312;&#33647;&#29289;&#19981;&#33391;&#20107;&#20214;&#25552;&#21462;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;&#19982;&#30417;&#30563;&#24335;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#65292;&#20855;&#26377;&#25104;&#26412;&#12289;&#25928;&#29575;&#21644;&#30333;&#30418;&#27169;&#22411;&#35775;&#38382;&#31561;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT-4&#65292;&#22312;&#21253;&#25324;&#20581;&#24247;&#24212;&#29992;&#22312;&#20869;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;LLMs&#26469;&#25193;&#23637;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#25972;&#29702;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;LLMs&#24050;&#32463;&#20855;&#22791;&#20102;&#32467;&#26500;&#21270;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#30340;&#33391;&#22909;&#33021;&#21147;&#65292;&#20294;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#23558;&#20854;&#33976;&#39311;&#20026;&#29305;&#23450;&#20219;&#21153;&#30340;&#23398;&#29983;&#27169;&#22411;&#21487;&#20197;&#21462;&#24471;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#21516;&#26102;&#20855;&#22791;&#25104;&#26412;&#12289;&#25928;&#29575;&#21644;&#30333;&#30418;&#27169;&#22411;&#35775;&#38382;&#31561;&#39069;&#22806;&#20248;&#21183;&#12290;&#25105;&#20204;&#23545;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#65288;ADE&#65289;&#25552;&#21462;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#65292;&#36825;&#26159;&#25913;&#36827;&#21307;&#30103;&#30340;&#37325;&#35201;&#39046;&#22495;&#12290;&#22312;&#26631;&#20934;ADE&#25552;&#21462;&#35780;&#20272;&#20013;&#65292;&#32463;&#33976;&#39311;&#30340;GPT-3.5 PubMedBERT&#27169;&#22411;&#22312;&#19981;&#20351;&#29992;&#20219;&#20309;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#36798;&#21040;&#20102;&#19982;&#30417;&#30563;&#24335;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#12290;&#23613;&#31649;&#20307;&#31215;&#32553;&#23567;&#20102;1000&#22810;&#20493;&#65292;&#20294;&#33976;&#39311;&#27169;&#22411;&#22312;F1&#25351;&#26631;&#19978;&#36229;&#36807;&#20102;&#20854;&#25945;&#24072;GPT-3.5&#32422;6&#20010;&#32477;&#23545;&#28857;&#65292;&#36229;&#36807;&#20102;GPT-4&#32422;5&#20010;&#32477;&#23545;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), such as GPT-4, have demonstrated remarkable capabilities across a wide range of tasks, including health applications. In this paper, we study how LLMs can be used to scale biomedical knowledge curation. We find that while LLMs already possess decent competency in structuring biomedical text, by distillation into a task-specific student model through self-supervised learning, substantial gains can be attained over out-of-box LLMs, with additional advantages such as cost, efficiency, and white-box model access.  We conduct a case study on adverse drug event (ADE) extraction, which is an important area for improving care. On standard ADE extraction evaluation, a GPT-3.5 distilled PubMedBERT model attained comparable accuracy as supervised state-of-the-art models without using any labeled data. Despite being over 1,000 times smaller, the distilled model outperformed its teacher GPT-3.5 by over 6 absolute points in F1 and GPT-4 by over 5 absolute points.  Ablat
&lt;/p&gt;</description></item><item><title>&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#20998;&#26512;&#20102;&#21508;&#31181;&#26032;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#31574;&#30053;&#65292;&#35752;&#35770;&#20102;LLM&#30340;&#29305;&#28857;&#21644;&#21151;&#33021;&#65292;&#24182;&#24635;&#32467;&#20102;&#37325;&#35201;&#30340;&#30740;&#31350;&#21457;&#29616;&#21644;&#20851;&#38190;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2307.06435</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#32508;&#21512;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Overview of Large Language Models. (arXiv:2307.06435v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06435
&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#20998;&#26512;&#20102;&#21508;&#31181;&#26032;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#31574;&#30053;&#65292;&#35752;&#35770;&#20102;LLM&#30340;&#29305;&#28857;&#21644;&#21151;&#33021;&#65292;&#24182;&#24635;&#32467;&#20102;&#37325;&#35201;&#30340;&#30740;&#31350;&#21457;&#29616;&#21644;&#20851;&#38190;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#23548;&#33268;&#20102;&#20247;&#22810;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#36825;&#20123;&#27169;&#22411;&#25552;&#20986;&#20102;&#21508;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#25913;&#36827;&#30340;&#35757;&#32451;&#31574;&#30053;&#26469;&#35843;&#25972;&#29616;&#26377;&#30340;&#26550;&#26500;&#65292;&#22686;&#21152;&#19978;&#19979;&#25991;&#38271;&#24230;&#65292;&#20351;&#29992;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#22686;&#21152;&#35757;&#32451;&#26102;&#38388;&#20197;&#36229;&#36234;&#22522;&#32447;&#12290;&#20998;&#26512;&#26032;&#30340;&#21457;&#23637;&#23545;&#20110;&#35782;&#21035;&#22686;&#24378;&#35757;&#32451;&#31283;&#23450;&#24615;&#21644;&#25913;&#36827;LLM&#27867;&#21270;&#33021;&#21147;&#30340;&#21464;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#32508;&#36848;&#35770;&#25991;&#20840;&#38754;&#20998;&#26512;&#20102;LLM&#30340;&#26550;&#26500;&#21450;&#20854;&#20998;&#31867;&#12289;&#35757;&#32451;&#31574;&#30053;&#12289;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#24615;&#33021;&#35780;&#20272;&#65292;&#24182;&#35752;&#35770;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;LLM&#30340;&#22522;&#26412;&#26500;&#24314;&#22359;&#21644;&#27010;&#24565;&#65292;&#24182;&#25552;&#20379;&#20102;LLM&#30340;&#23436;&#25972;&#27010;&#36848;&#65292;&#21253;&#25324;&#20854;&#37325;&#35201;&#29305;&#28857;&#21644;&#21151;&#33021;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#24635;&#32467;&#20102;LLM&#30740;&#31350;&#30340;&#37325;&#35201;&#21457;&#29616;&#65292;&#24182;&#25972;&#21512;&#20102;&#20851;&#38190;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown excellent generalization capabilities that have led to the development of numerous models. These models propose various new architectures, tweaking existing architectures with refined training strategies, increasing context length, using high-quality training data, and increasing training time to outperform baselines. Analyzing new developments is crucial for identifying changes that enhance training stability and improve generalization in LLMs. This survey paper comprehensively analyses the LLMs architectures and their categorization, training strategies, training datasets, and performance evaluations and discusses future research directions. Moreover, the paper also discusses the basic building blocks and concepts behind LLMs, followed by a complete overview of LLMs, including their important features and functions. Finally, the paper summarizes significant findings from LLM research and consolidates essential architectural and training strateg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35789;&#35821;&#30340;&#24418;&#24577;&#19982;&#35821;&#20041;&#20851;&#31995;&#20043;&#38388;&#30340;&#20114;&#21160;&#20851;&#31995;&#65292;&#36890;&#36807;&#25506;&#32034;&#35821;&#35328;&#24418;&#24577;&#23398;&#19982;&#35821;&#20041;&#20851;&#31995;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#32852;&#65292;&#25581;&#31034;&#20102;&#35789;&#35821;&#32467;&#26500;&#23545;&#35821;&#35328;&#29702;&#35299;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.06419</link><description>&lt;p&gt;
&#35789;&#27719;&#38388;&#35821;&#20041;&#20851;&#31995;&#30340;&#33719;&#21462;
&lt;/p&gt;
&lt;p&gt;
The Acquisition of Semantic Relationships between words. (arXiv:2307.06419v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35789;&#35821;&#30340;&#24418;&#24577;&#19982;&#35821;&#20041;&#20851;&#31995;&#20043;&#38388;&#30340;&#20114;&#21160;&#20851;&#31995;&#65292;&#36890;&#36807;&#25506;&#32034;&#35821;&#35328;&#24418;&#24577;&#23398;&#19982;&#35821;&#20041;&#20851;&#31995;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#32852;&#65292;&#25581;&#31034;&#20102;&#35789;&#35821;&#32467;&#26500;&#23545;&#35821;&#35328;&#29702;&#35299;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35821;&#20041;&#20851;&#31995;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#36825;&#20123;&#20851;&#31995;&#19982;&#35821;&#35328;&#30340;&#24418;&#24577;&#29305;&#24449;&#20043;&#38388;&#30340;&#23494;&#20999;&#32852;&#31995;&#12290;&#24418;&#24577;&#23398;&#20316;&#20026;&#35821;&#35328;&#23398;&#30340;&#19968;&#20010;&#20998;&#25903;&#65292;&#30740;&#31350;&#35789;&#35821;&#30340;&#20869;&#37096;&#32467;&#26500;&#21644;&#26500;&#25104;&#12290;&#36890;&#36807;&#28145;&#20837;&#25506;&#35752;&#35821;&#20041;&#20851;&#31995;&#21644;&#35821;&#35328;&#24418;&#24577;&#23398;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#21487;&#20197;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#35789;&#35821;&#30340;&#24213;&#23618;&#32467;&#26500;&#22914;&#20309;&#24433;&#21709;&#35821;&#35328;&#30340;&#35299;&#37322;&#21644;&#29702;&#35299;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#35821;&#20041;&#20851;&#31995;&#21644;&#19981;&#21516;&#35821;&#35328;&#30340;&#24418;&#24577;&#23398;&#20043;&#38388;&#30340;&#21160;&#24577;&#30456;&#20114;&#20316;&#29992;&#65292;&#36890;&#36807;&#30740;&#31350;&#35821;&#35328;&#24418;&#24577;&#23398;&#19982;&#35821;&#20041;&#20851;&#31995;&#20043;&#38388;&#22797;&#26434;&#30340;&#20851;&#31995;&#65292;&#21487;&#20197;&#33719;&#24471;&#26377;&#20851;&#35789;&#35821;&#32467;&#26500;&#22914;&#20309;&#24433;&#21709;&#35821;&#35328;&#29702;&#35299;&#30340;&#23453;&#36149;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study of semantic relationships has revealed a close connection between these relationships and the morphological characteristics of a language. Morphology, as a subfield of linguistics, investigates the internal structure and formation of words. By delving into the relationship between semantic relationships and language morphology, we can gain deeper insights into how the underlying structure of words contributes to the interpretation and comprehension of language. This paper explores the dynamic interplay between semantic relationships and the morphological aspects of different languages, by examining the intricate relationship between language morphology and semantic relationships, valuable insights can be gained regarding how the structure of words influences language comprehension.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24207;&#21015;&#26631;&#27880;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#20174;&#23545;&#35805;&#19978;&#19979;&#25991;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#24182;&#24341;&#20837;&#20102;&#21306;&#20998;&#35828;&#35805;&#32773;&#30340;&#23884;&#20837;&#26469;&#24314;&#27169;&#35828;&#35805;&#32773;&#30340;&#21464;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#24674;&#22797;&#24471;&#20998;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20248;&#65292;&#24182;&#22312;&#25512;&#29702;&#36895;&#24230;&#19978;&#32988;&#36807;&#22823;&#22810;&#25968;&#20043;&#21069;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.06337</link><description>&lt;p&gt;
&#19981;&#23436;&#25972;&#35805;&#35821;&#37325;&#20889;&#20316;&#20026;&#39034;&#24207;&#36138;&#23146;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
Incomplete Utterance Rewriting as Sequential Greedy Tagging. (arXiv:2307.06337v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24207;&#21015;&#26631;&#27880;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#20174;&#23545;&#35805;&#19978;&#19979;&#25991;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#24182;&#24341;&#20837;&#20102;&#21306;&#20998;&#35828;&#35805;&#32773;&#30340;&#23884;&#20837;&#26469;&#24314;&#27169;&#35828;&#35805;&#32773;&#30340;&#21464;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#24674;&#22797;&#24471;&#20998;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20248;&#65292;&#24182;&#22312;&#25512;&#29702;&#36895;&#24230;&#19978;&#32988;&#36807;&#22823;&#22810;&#25968;&#20043;&#21069;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#23436;&#25972;&#35805;&#35821;&#37325;&#20889;&#30340;&#20219;&#21153;&#26368;&#36817;&#21463;&#21040;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#20043;&#21069;&#30340;&#27169;&#22411;&#24456;&#38590;&#20174;&#23545;&#35805;&#19978;&#19979;&#25991;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#36825;&#22312;&#24674;&#22797;&#24471;&#20998;&#20302;&#30340;&#24773;&#20917;&#19979;&#24471;&#20197;&#35777;&#26126;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#24207;&#21015;&#26631;&#27880;&#30340;&#27169;&#22411;&#65292;&#26356;&#25797;&#38271;&#20174;&#19978;&#19979;&#25991;&#20013;&#25552;&#21462;&#20449;&#24687;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21306;&#20998;&#35828;&#35805;&#32773;&#30340;&#23884;&#20837;&#26469;&#24314;&#27169;&#35828;&#35805;&#32773;&#30340;&#21464;&#21270;&#12290;&#22312;&#22810;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25152;&#26377;&#20061;&#20010;&#24674;&#22797;&#24471;&#20998;&#19978;&#37117;&#36798;&#21040;&#20102;&#26368;&#20248;&#32467;&#26524;&#65292;&#21516;&#26102;&#20854;&#20182;&#25351;&#26631;&#24471;&#20998;&#20063;&#19982;&#20043;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#30456;&#24403;&#12290;&#27492;&#22806;&#65292;&#24471;&#30410;&#20110;&#27169;&#22411;&#30340;&#31616;&#21333;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25512;&#29702;&#36895;&#24230;&#19978;&#36229;&#36807;&#20102;&#22823;&#22810;&#25968;&#20043;&#21069;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of incomplete utterance rewriting has recently gotten much attention. Previous models struggled to extract information from the dialogue context, as evidenced by the low restoration scores. To address this issue, we propose a novel sequence tagging-based model, which is more adept at extracting information from context. Meanwhile, we introduce speaker-aware embedding to model speaker variation. Experiments on multiple public datasets show that our model achieves optimal results on all nine restoration scores while having other metric scores comparable to previous state-of-the-art models. Furthermore, benefitting from the model's simplicity, our approach outperforms most previous models on inference speed.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#24110;&#21161;&#35299;&#37322;&#21644;&#35299;&#35835;&#20379;&#24212;&#38142;&#20248;&#21270;&#32467;&#26524;&#30340;&#26041;&#27861;&#12290;&#20182;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#25509;&#21463;&#26222;&#36890;&#25991;&#26412;&#26597;&#35810;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#36755;&#20986;&#20851;&#20110;&#24213;&#23618;&#20248;&#21270;&#32467;&#26524;&#30340;&#27934;&#23519;&#12290;&#36890;&#36807;&#23450;&#37327;&#22238;&#31572;&#20551;&#35774;&#24773;&#20917;&#65292;&#35813;&#26694;&#26550;&#22312;&#19981;&#25918;&#24323;&#26368;&#20808;&#36827;&#30340;&#32452;&#21512;&#20248;&#21270;&#25216;&#26415;&#30340;&#24773;&#20917;&#19979;&#24110;&#21161;&#20225;&#19994;&#36816;&#33829;&#32773;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#20449;&#20219;&#20248;&#21270;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.03875</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#20379;&#24212;&#38142;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Supply Chain Optimization. (arXiv:2307.03875v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03875
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#24110;&#21161;&#35299;&#37322;&#21644;&#35299;&#35835;&#20379;&#24212;&#38142;&#20248;&#21270;&#32467;&#26524;&#30340;&#26041;&#27861;&#12290;&#20182;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#25509;&#21463;&#26222;&#36890;&#25991;&#26412;&#26597;&#35810;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#36755;&#20986;&#20851;&#20110;&#24213;&#23618;&#20248;&#21270;&#32467;&#26524;&#30340;&#27934;&#23519;&#12290;&#36890;&#36807;&#23450;&#37327;&#22238;&#31572;&#20551;&#35774;&#24773;&#20917;&#65292;&#35813;&#26694;&#26550;&#22312;&#19981;&#25918;&#24323;&#26368;&#20808;&#36827;&#30340;&#32452;&#21512;&#20248;&#21270;&#25216;&#26415;&#30340;&#24773;&#20917;&#19979;&#24110;&#21161;&#20225;&#19994;&#36816;&#33829;&#32773;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#20449;&#20219;&#20248;&#21270;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#20379;&#24212;&#38142;&#25805;&#20316;&#28041;&#21450;&#21508;&#31181;&#22797;&#26434;&#30340;&#20915;&#31574;&#38382;&#39064;&#12290;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#65292;&#20379;&#24212;&#38142;&#21463;&#30410;&#20110;&#35745;&#31639;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#20174;&#25163;&#21160;&#22788;&#29702;&#36807;&#28193;&#21040;&#33258;&#21160;&#21270;&#21644;&#25104;&#26412;&#25928;&#30410;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#20225;&#19994;&#36816;&#33829;&#32773;&#20173;&#28982;&#38656;&#35201;&#33457;&#36153;&#22823;&#37327;&#31934;&#21147;&#26469;&#35299;&#37322;&#21644;&#35299;&#35835;&#20248;&#21270;&#32467;&#26524;&#32473;&#30456;&#20851;&#20154;&#22763;&#12290;&#21463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26368;&#36817;&#30340;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#31181;&#39072;&#35206;&#24615;&#25216;&#26415;&#22914;&#20309;&#24110;&#21161;&#24357;&#21512;&#20379;&#24212;&#38142;&#33258;&#21160;&#21270;&#21644;&#20154;&#31867;&#29702;&#35299;&#19982;&#20449;&#20219;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;\name{}&#30340;&#26694;&#26550;&#65292;&#23427;&#25509;&#21463;&#26222;&#36890;&#25991;&#26412;&#26597;&#35810;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#36755;&#20986;&#20851;&#20110;&#24213;&#23618;&#20248;&#21270;&#32467;&#26524;&#30340;&#27934;&#23519;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24182;&#27809;&#26377;&#25918;&#24323;&#26368;&#20808;&#36827;&#30340;&#32452;&#21512;&#20248;&#21270;&#25216;&#26415;&#65292;&#32780;&#26159;&#21033;&#29992;&#23427;&#26469;&#23450;&#37327;&#22320;&#22238;&#31572;&#20551;&#35774;&#24773;&#20917;&#65288;&#20363;&#22914;&#65292;&#22914;&#26524;&#25105;&#20204;&#20351;&#29992;&#20379;&#24212;&#21830;B&#32780;&#19981;&#26159;&#20379;&#24212;&#21830;A&#65292;&#25104;&#26412;&#20250;&#22914;&#20309;&#21464;&#21270;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supply chain operations traditionally involve a variety of complex decision making problems. Over the last few decades, supply chains greatly benefited from advances in computation, which allowed the transition from manual processing to automation and cost-effective optimization. Nonetheless, business operators still need to spend substantial efforts in \emph{explaining} and interpreting the optimization outcomes to stakeholders. Motivated by the recent advances in Large Language Models (LLMs), we study how this disruptive technology can help bridge the gap between supply chain automation and human comprehension and trust thereof. We design \name{} -- a framework that accepts as input queries in plain text, and outputs insights about the underlying optimization outcomes. Our framework does not forgo the state-of-the-art combinatorial optimization technology, but rather leverages it to quantitatively answer what-if scenarios (e.g., how would the cost change if we used supplier B instead
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20851;&#27880;&#19977;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#35780;&#20272;&#20160;&#20040;&#12289;&#22312;&#21738;&#37324;&#35780;&#20272;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#12290;&#35780;&#20272;&#20219;&#21153;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#25512;&#29702;&#12289;&#21307;&#23398;&#24212;&#29992;&#12289;&#20262;&#29702;&#23398;&#12289;&#25945;&#32946;&#12289;&#33258;&#28982;&#21644;&#31038;&#20250;&#31185;&#23398;&#12289;&#20195;&#29702;&#24212;&#29992;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;&#26412;&#25991;&#20026;&#31038;&#20250;&#23618;&#38754;&#23545;LLMs&#28508;&#22312;&#39118;&#38505;&#30340;&#29702;&#35299;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2307.03109</link><description>&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Evaluation of Large Language Models. (arXiv:2307.03109v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20851;&#27880;&#19977;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#35780;&#20272;&#20160;&#20040;&#12289;&#22312;&#21738;&#37324;&#35780;&#20272;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#12290;&#35780;&#20272;&#20219;&#21153;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#25512;&#29702;&#12289;&#21307;&#23398;&#24212;&#29992;&#12289;&#20262;&#29702;&#23398;&#12289;&#25945;&#32946;&#12289;&#33258;&#28982;&#21644;&#31038;&#20250;&#31185;&#23398;&#12289;&#20195;&#29702;&#24212;&#29992;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;&#26412;&#25991;&#20026;&#31038;&#20250;&#23618;&#38754;&#23545;LLMs&#28508;&#22312;&#39118;&#38505;&#30340;&#29702;&#35299;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#32780;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#38543;&#30528;LLMs&#22312;&#30740;&#31350;&#21644;&#26085;&#24120;&#20351;&#29992;&#20013;&#32487;&#32493;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#23427;&#20204;&#30340;&#35780;&#20272;&#21464;&#24471;&#36234;&#26469;&#36234;&#20851;&#38190;&#65292;&#19981;&#20165;&#22312;&#20219;&#21153;&#27700;&#24179;&#19978;&#65292;&#32780;&#19988;&#22312;&#31038;&#20250;&#23618;&#38754;&#19978;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#23427;&#20204;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#37324;&#65292;&#24050;&#32463;&#20570;&#20986;&#20102;&#30456;&#24403;&#22823;&#30340;&#21162;&#21147;&#26469;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#26469;&#30740;&#31350;LLMs&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;LLMs&#30340;&#36825;&#20123;&#35780;&#20272;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#19977;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#35780;&#20272;&#20160;&#20040;&#12289;&#22312;&#21738;&#37324;&#35780;&#20272;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#35780;&#20272;&#20219;&#21153;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;&#19968;&#20010;&#27010;&#36848;&#65292;&#28085;&#30422;&#20102;&#19968;&#33324;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12289;&#25512;&#29702;&#12289;&#21307;&#23398;&#24212;&#29992;&#12289;&#20262;&#29702;&#23398;&#12289;&#25945;&#32946;&#12289;&#33258;&#28982;&#31185;&#23398;&#21644;&#31038;&#20250;&#31185;&#23398;&#12289;&#20195;&#29702;&#24212;&#29992;&#21644;&#20854;&#20182;&#39046;&#22495;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#28145;&#20837;&#25506;&#35752;&#35780;&#20272;&#26041;&#27861;&#21644;&#22522;&#20934;&#31572;&#26696;&#26469;&#22238;&#31572;&#8220;&#22312;&#21738;&#37324;&#8221;&#21644;&#8220;&#22914;&#20309;&#8221;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, educations, natural and social sciences, agent applications, and other areas. Secondly, we answer the `where' and `how' questions by diving into the evaluation methods and bench
&lt;/p&gt;</description></item><item><title>PatternGPT&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#21462;&#33021;&#21147;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#30340;&#24605;&#24819;&#23454;&#29616;&#27169;&#24335;&#20849;&#20139;&#65292;&#26368;&#32456;&#36890;&#36807;&#25628;&#32034;&#39640;&#36136;&#37327;&#27169;&#24335;&#25351;&#23548;&#29983;&#25104;&#27169;&#22411;&#12290;&#35813;&#26694;&#26550;&#20855;&#26377;&#29983;&#25104;&#22810;&#26679;&#21270;&#27169;&#24335;&#12289;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12289;&#32467;&#21512;&#22806;&#37096;&#30693;&#35782;&#31561;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.00470</link><description>&lt;p&gt;
PatternGPT: &#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#29983;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
PatternGPT :A Pattern-Driven Framework for Large Language Model Text Generation. (arXiv:2307.00470v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00470
&lt;/p&gt;
&lt;p&gt;
PatternGPT&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#21462;&#33021;&#21147;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#30340;&#24605;&#24819;&#23454;&#29616;&#27169;&#24335;&#20849;&#20139;&#65292;&#26368;&#32456;&#36890;&#36807;&#25628;&#32034;&#39640;&#36136;&#37327;&#27169;&#24335;&#25351;&#23548;&#29983;&#25104;&#27169;&#22411;&#12290;&#35813;&#26694;&#26550;&#20855;&#26377;&#29983;&#25104;&#22810;&#26679;&#21270;&#27169;&#24335;&#12289;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12289;&#32467;&#21512;&#22806;&#37096;&#30693;&#35782;&#31561;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#65292;&#33021;&#22815;&#20026;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#29983;&#25104;&#27969;&#30021;&#30340;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#20851;&#38190;&#20219;&#21153;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#23481;&#26131;&#20986;&#29616;&#24187;&#35273;&#65292;&#24182;&#19988;&#26080;&#27861;&#30452;&#25509;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#12290;&#20026;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;PatternGPT&#65292;&#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#29983;&#25104;&#26694;&#26550;&#12290;&#39318;&#20808;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#21462;&#33021;&#21147;&#29983;&#25104;&#20016;&#23500;&#22810;&#26679;&#30340;&#27169;&#24335;&#65292;&#28982;&#21518;&#20511;&#37492;&#32852;&#37030;&#23398;&#20064;&#30340;&#24605;&#24819;&#65292;&#20351;&#29992;&#22810;&#20010;&#20195;&#29702;&#23454;&#29616;&#20849;&#20139;&#20197;&#33719;&#21462;&#26356;&#22810;&#26679;&#30340;&#27169;&#24335;&#12290;&#26368;&#21518;&#65292;&#23427;&#20351;&#29992;&#21028;&#26029;&#26631;&#20934;&#21644;&#20248;&#21270;&#31639;&#27861;&#25628;&#32034;&#39640;&#36136;&#37327;&#30340;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#25628;&#32034;&#21040;&#30340;&#27169;&#24335;&#25351;&#23548;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#12290;&#35813;&#26694;&#26550;&#20855;&#26377;&#29983;&#25104;&#22810;&#26679;&#21270;&#27169;&#24335;&#12289;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12289;&#32467;&#21512;&#22806;&#37096;&#30693;&#35782;&#31561;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models(LLMS) have shown excellent text generation capabilities,capable of generating fluent responses for many downstream tasks. However,applying large language models to real-world critical tasks remains challenging due to their susceptibility to hallucinations and inability to directly use external knowledge. To address the above challenges,this paper proposes PatternGPT, a pattern-driven text generation framework for large language models. First,the framework utilizes the extraction capabilities of large language models to generate rich and diverse patterns and later draws on the idea of federated learning. Using multiple agents to achieve sharing to obtain more diverse patterns. Finally, it searches for high-quality patterns using judgment criteria and optimization algorithms and uses the searched patterns to guide the model for generation. This framework has the advantages of generating diversified patterns, protecting data privacy,combining external knowledge, and 
&lt;/p&gt;</description></item><item><title>Kosmos-2&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#24863;&#30693;&#29289;&#20307;&#25551;&#36848;&#24182;&#23558;&#25991;&#26412;&#19982;&#35270;&#35273;&#19990;&#30028;&#32852;&#31995;&#36215;&#26469;&#12290;&#23427;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#20986;&#33394;&#34920;&#29616;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#25509;&#22320;&#12289;&#22810;&#27169;&#24577;&#24341;&#29992;&#12289;&#24863;&#30693;&#35821;&#35328;&#20219;&#21153;&#20197;&#21450;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2306.14824</link><description>&lt;p&gt;
Kosmos-2: &#23558;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#19982;&#19990;&#30028;&#36830;&#25509;
&lt;/p&gt;
&lt;p&gt;
Kosmos-2: Grounding Multimodal Large Language Models to the World. (arXiv:2306.14824v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14824
&lt;/p&gt;
&lt;p&gt;
Kosmos-2&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#24863;&#30693;&#29289;&#20307;&#25551;&#36848;&#24182;&#23558;&#25991;&#26412;&#19982;&#35270;&#35273;&#19990;&#30028;&#32852;&#31995;&#36215;&#26469;&#12290;&#23427;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#20986;&#33394;&#34920;&#29616;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#25509;&#22320;&#12289;&#22810;&#27169;&#24577;&#24341;&#29992;&#12289;&#24863;&#30693;&#35821;&#35328;&#20219;&#21153;&#20197;&#21450;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Kosmos-2&#65292;&#19968;&#20010;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#65292;&#20351;&#20854;&#33021;&#22815;&#24863;&#30693;&#29289;&#20307;&#25551;&#36848;&#65288;&#20363;&#22914;&#65292;&#36793;&#30028;&#26694;&#65289;&#24182;&#23558;&#25991;&#26412;&#19982;&#35270;&#35273;&#19990;&#30028;&#32852;&#31995;&#36215;&#26469;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#24341;&#29992;&#34920;&#36798;&#24335;&#34920;&#31034;&#20026;Markdown&#20013;&#30340;&#38142;&#25509;&#65292;&#21363;``[text span](bounding boxes)''&#65292;&#20854;&#20013;&#29289;&#20307;&#25551;&#36848;&#26159;&#20301;&#32622;&#26631;&#35760;&#24207;&#21015;&#12290;&#36890;&#36807;&#19982;&#22810;&#27169;&#24577;&#35821;&#26009;&#24211;&#32467;&#21512;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#22823;&#35268;&#27169;&#30340;&#22270;&#20687;&#25991;&#26412;&#23545;&#65288;&#31216;&#20026;GrIT&#65289;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#35813;&#27169;&#22411;&#12290;&#38500;&#20102;MLLM&#30340;&#29616;&#26377;&#21151;&#33021;&#65288;&#20363;&#22914;&#65292;&#24863;&#30693;&#21508;&#31181;&#27169;&#24577;&#65292;&#36981;&#24490;&#25351;&#20196;&#21644;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65289;&#22806;&#65292;Kosmos-2&#36824;&#23558;&#25509;&#22320;&#33021;&#21147;&#38598;&#25104;&#21040;&#19979;&#28216;&#24212;&#29992;&#20013;&#12290;&#25105;&#20204;&#22312;&#24191;&#27867;&#30340;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;Kosmos-2&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#25509;&#22320;&#65288;&#20363;&#22914;&#65292;&#24341;&#29992;&#34920;&#36798;&#29702;&#35299;&#21644;&#30701;&#35821;&#25509;&#22320;&#65289;&#65292;&#22810;&#27169;&#24577;&#24341;&#29992;&#65288;&#20363;&#22914;&#65292;&#24341;&#29992;&#34920;&#36798;&#29983;&#25104;&#65289;&#65292;&#24863;&#30693;&#35821;&#35328;&#20219;&#21153;&#20197;&#21450;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new capabilities of perceiving object descriptions (e.g., bounding boxes) and grounding text to the visual world. Specifically, we represent refer expressions as links in Markdown, i.e., ``[text span](bounding boxes)'', where object descriptions are sequences of location tokens. Together with multimodal corpora, we construct large-scale data of grounded image-text pairs (called GrIT) to train the model. In addition to the existing capabilities of MLLMs (e.g., perceiving general modalities, following instructions, and performing in-context learning), Kosmos-2 integrates the grounding capability into downstream applications. We evaluate Kosmos-2 on a wide range of tasks, including (i) multimodal grounding, such as referring expression comprehension, and phrase grounding, (ii) multimodal referring, such as referring expression generation, (iii) perception-language tasks, and (iv) language understanding and generatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;GEmo-CLAP&#27169;&#22411;&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65292;&#32467;&#21512;&#20102;&#24615;&#21035;&#23646;&#24615;&#20449;&#24687;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#20808;&#36827;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#22312;IEMOCAP&#19978;&#23454;&#29616;&#20102;&#26356;&#20248;&#36234;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.07848</link><description>&lt;p&gt;
GEmo-CLAP: &#38754;&#21521;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#24615;&#21035;&#23646;&#24615;&#22686;&#24378;&#23545;&#27604;&#35821;&#38899;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GEmo-CLAP: Gender-Attribute-Enhanced Contrastive Language-Audio Pretraining for Speech Emotion Recognition. (arXiv:2306.07848v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;GEmo-CLAP&#27169;&#22411;&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65292;&#32467;&#21512;&#20102;&#24615;&#21035;&#23646;&#24615;&#20449;&#24687;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#20808;&#36827;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#22312;IEMOCAP&#19978;&#23454;&#29616;&#20102;&#26356;&#20248;&#36234;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#35821;&#38899;-&#35821;&#35328;&#39044;&#35757;&#32451;&#65288;CLAP&#65289;&#26368;&#36817;&#22312;&#19981;&#21516;&#39046;&#22495;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#25104;&#21151;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GEmo-CLAP&#30340;&#39640;&#25928;&#24615;&#21035;&#23646;&#24615;&#22686;&#24378;CLAP&#27169;&#22411;&#65292;&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65288;SER&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#21508;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#26500;&#24314;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#24773;&#24863;CLAP&#27169;&#22411;&#65288;&#31216;&#20026;Emo-CLAP&#65289;&#65292;&#29992;&#20110;SER&#12290;&#28982;&#21518;&#65292;&#32771;&#34385;&#21040;&#22312;&#35821;&#38899;&#24773;&#24863;&#24314;&#27169;&#20013;&#24615;&#21035;&#23646;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#20004;&#31181;GEmo-CLAP&#26041;&#27861;&#65292;&#26469;&#25972;&#21512;&#35821;&#38899;&#20449;&#21495;&#30340;&#24773;&#24863;&#21644;&#24615;&#21035;&#20449;&#24687;&#65292;&#24418;&#25104;&#26356;&#21512;&#29702;&#30340;&#30446;&#26631;&#12290;&#22312;IEMOCAP&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#20004;&#31181;GEmo-CLAP&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;&#22522;&#32447;Emo-CLAP&#27169;&#22411;&#65288;&#20351;&#29992;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65289;&#65292;&#21516;&#26102;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#23454;&#29616;&#20102;&#26356;&#20248;&#36234;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive Language-Audio Pretraining (CLAP) has recently exhibited impressive success in diverse fields. In this paper, we propose GEmo-CLAP, a kind of efficient gender-attribute-enhanced CLAP model for speech emotion recognition (SER). Specifically, we first build an effective emotion CLAP model termed Emo-CLAP for SER, utilizing various self-supervised learning based pre-trained models. Then, considering the importance of the gender attribute in speech emotion modeling, two GEmo-CLAP approaches are further proposed to integrate the emotion and gender information of speech signals, forming more reasonable objectives. Extensive experiments conducted on the IEMOCAP corpus demonstrate that our proposed two GEmo-CLAP approaches consistently outperform the baseline Emo-CLAP with different pre-trained models, while also achieving superior recognition performance compared with other state-of-the-art methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;SeeTRUE&#35780;&#20272;&#38598;&#21644;&#20004;&#31181;&#33258;&#21160;&#25991;&#26412;-&#22270;&#20687;&#23545;&#40784;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#21508;&#31181;&#23545;&#40784;&#20219;&#21153;&#20013;&#22343;&#21462;&#24471;&#20102;&#26174;&#30528;&#25913;&#36827;&#65292;&#22312;&#22797;&#26434;&#32452;&#21512;&#25110;&#38750;&#33258;&#28982;&#22270;&#20687;&#30340;&#25361;&#25112;&#24615;&#26696;&#20363;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2305.10400</link><description>&lt;p&gt;
&#20320;&#30475;&#21040;&#30340;&#23601;&#26159;&#20320;&#35835;&#21040;&#30340;? &#25913;&#36827;&#25991;&#26412;-&#22270;&#20687;&#23545;&#40784;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
What You See is What You Read? Improving Text-Image Alignment Evaluation. (arXiv:2305.10400v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;SeeTRUE&#35780;&#20272;&#38598;&#21644;&#20004;&#31181;&#33258;&#21160;&#25991;&#26412;-&#22270;&#20687;&#23545;&#40784;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#21508;&#31181;&#23545;&#40784;&#20219;&#21153;&#20013;&#22343;&#21462;&#24471;&#20102;&#26174;&#30528;&#25913;&#36827;&#65292;&#22312;&#22797;&#26434;&#32452;&#21512;&#25110;&#38750;&#33258;&#28982;&#22270;&#20687;&#30340;&#25361;&#25112;&#24615;&#26696;&#20363;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#30830;&#23450;&#25991;&#26412;&#21644;&#30456;&#24212;&#30340;&#22270;&#20687;&#26159;&#21542;&#35821;&#20041;&#19978;&#23545;&#40784;&#26159;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30340;&#19968;&#39033;&#37325;&#35201;&#25361;&#25112;&#65292;&#24212;&#29992;&#20110;&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#21644;&#22270;&#20687;&#21040;&#25991;&#26412;&#20219;&#21153;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#33258;&#21160;&#25991;&#26412;-&#22270;&#20687;&#23545;&#40784;&#35780;&#20272;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;SeeTRUE&#65306;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#38598;&#65292;&#28085;&#30422;&#20102;&#20174;&#25991;&#26412;&#21040;&#22270;&#20687;&#21644;&#22270;&#20687;&#21040;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#30340;&#22810;&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#20855;&#26377;&#20154;&#31867;&#30340;&#21028;&#26029;&#65292;&#21028;&#26029;&#32473;&#23450;&#30340;&#25991;&#26412;-&#22270;&#20687;&#23545;&#26159;&#21542;&#35821;&#20041;&#19978;&#23545;&#40784;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20004;&#31181;&#33258;&#21160;&#30830;&#23450;&#23545;&#40784;&#30340;&#26041;&#27861;&#65306;&#31532;&#19968;&#31181;&#26159;&#22522;&#20110;&#38382;&#39064;&#29983;&#25104;&#21644;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#27169;&#22411;&#30340;&#31649;&#36947;&#65292;&#31532;&#20108;&#31181;&#26159;&#36890;&#36807;&#24494;&#35843;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#20998;&#31867;&#26041;&#27861;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#22312;&#21508;&#31181;&#25991;&#26412;-&#22270;&#20687;&#23545;&#40784;&#20219;&#21153;&#20013;&#22343;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#22312;&#28041;&#21450;&#22797;&#26434;&#32452;&#21512;&#25110;&#38750;&#33258;&#28982;&#22270;&#20687;&#30340;&#25361;&#25112;&#24615;&#26696;&#20363;&#20013;&#26377;&#26174;&#30528;&#25913;&#36827;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#21363;&#20351;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#36824;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#65292;&#36825;&#28608;&#21169;&#20102;&#26410;&#26469;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatically determining whether a text and a corresponding image are semantically aligned is a significant challenge for vision-language models, with applications in generative text-to-image and image-to-text tasks. In this work, we study methods for automatic text-image alignment evaluation. We first introduce SeeTRUE: a comprehensive evaluation set, spanning multiple datasets from both text-to-image and image-to-text generation tasks, with human judgements for whether a given text-image pair is semantically aligned. We then describe two automatic methods to determine alignment: the first involving a pipeline based on question generation and visual question answering models, and the second employing an end-to-end classification approach by finetuning multimodal pretrained models. Both methods surpass prior approaches in various text-image alignment tasks, with significant improvements in challenging cases that involve complex composition or unnatural images. Finally, we demonstrate 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Dr. LLaMA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#65292;&#20197;&#25913;&#21892;&#23567;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#20013;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#24494;&#35843;&#21518;&#20351;&#27169;&#22411;&#24615;&#33021;&#25552;&#39640;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#29305;&#23450;&#39046;&#22495;&#38382;&#31572;&#20219;&#21153;&#20013;&#20351;&#29992;LLM&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#28508;&#22312;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.07804</link><description>&lt;p&gt;
Dr. LLaMA&#65306;&#36890;&#36807;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#25913;&#21892;&#29305;&#23450;&#39046;&#22495;QA&#20013;&#30340;&#23567;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Dr. LLaMA: Improving Small Language Models in Domain-Specific QA via Generative Data Augmentation. (arXiv:2305.07804v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07804
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Dr. LLaMA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#65292;&#20197;&#25913;&#21892;&#23567;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#20013;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#24494;&#35843;&#21518;&#20351;&#27169;&#22411;&#24615;&#33021;&#25552;&#39640;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#29305;&#23450;&#39046;&#22495;&#38382;&#31572;&#20219;&#21153;&#20013;&#20351;&#29992;LLM&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#28508;&#22312;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#38543;&#30528;&#20854;&#35268;&#27169;&#30340;&#22686;&#38271;&#65292;&#20063;&#38754;&#20020;&#30528;&#35745;&#31639;&#24320;&#38144;&#21644;&#25928;&#29575;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#20219;&#21153;&#20013;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30001;&#20110;&#23481;&#37327;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#38480;&#21046;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Dr. LLaMA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#65292;&#32858;&#28966;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#21644;PubMedQA&#25968;&#25454;&#38598;&#65292;&#20197;&#25913;&#21892;&#23567;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;LLM&#26377;&#25928;&#22320;&#32454;&#21270;&#21644;&#25193;&#23637;&#29616;&#26377;&#30340;&#38382;&#39064;-&#31572;&#26696;&#23545;&#65292;&#22312;&#24494;&#35843;&#21518;&#65292;&#20351;&#24471;&#23567;&#22411;&#27169;&#22411;&#22312;&#29305;&#23450;&#39046;&#22495;QA&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#25552;&#39640;&#12290;&#26412;&#30740;&#31350;&#24378;&#35843;&#20102;&#22312;&#29305;&#23450;&#39046;&#22495;&#38382;&#31572;&#20219;&#21153;&#20013;&#20351;&#29992;LLM&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#28508;&#22312;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#26368;&#32456;&#26088;&#22312;&#20026;&#19987;&#19994;&#24212;&#29992;&#21019;&#24314;&#26356;&#39640;&#25928;&#21644;&#33021;&#21147;&#26356;&#24378;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have made significant strides in natural language processing but face challenges in terms of computational expense and inefficiency as they grow in size, especially in domain-specific tasks. Small Language Models (SLMs), on the other hand, often struggle in these tasks due to limited capacity and training data. In this paper, we introduce Dr. LLaMA, a method for improving SLMs through generative data augmentation using LLMs, focusing on medical question-answering tasks and the PubMedQA dataset. Our findings indicate that LLMs effectively refine and diversify existing question-answer pairs, resulting in improved performance of a much smaller model on domain-specific QA datasets after fine-tuning. This study highlights the challenges of using LLMs for domain-specific question answering and suggests potential research directions to address these limitations, ultimately aiming to create more efficient and capable models for specialized applications. We have als
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36879;&#26126;&#12289;&#26080;&#30417;&#30563;&#30340;&#35789;&#32423;&#36136;&#37327;&#20272;&#35745;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#20998;&#26512;&#25200;&#21160;&#30340;&#36755;&#20837;&#28304;&#21477;&#23376;&#19978;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#36755;&#20986;&#26469;&#24037;&#20316;&#65292;&#24182;&#21487;&#20197;&#35780;&#20272;&#20219;&#20309;&#31867;&#22411;&#30340;&#40657;&#30418;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#12290;&#20351;&#29992;&#35813;&#26041;&#27861;&#20316;&#20026;&#21453;&#39304;&#20449;&#21495;&#65292;&#21487;&#20197;&#22312;&#26080;&#30417;&#30563;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#20013;&#25913;&#36827;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.07457</link><description>&lt;p&gt;
&#22522;&#20110;&#25668;&#21160;&#30340;&#36136;&#37327;&#20272;&#35745;&#65306;&#19968;&#31181;&#36879;&#26126;&#12289;&#26080;&#30417;&#30563;&#30340;&#35789;&#32423;&#36136;&#37327;&#20272;&#35745;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#40657;&#30418;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Perturbation-based QE: An Explainable, Unsupervised Word-level Quality Estimation Method for Blackbox Machine Translation. (arXiv:2305.07457v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36879;&#26126;&#12289;&#26080;&#30417;&#30563;&#30340;&#35789;&#32423;&#36136;&#37327;&#20272;&#35745;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#20998;&#26512;&#25200;&#21160;&#30340;&#36755;&#20837;&#28304;&#21477;&#23376;&#19978;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#36755;&#20986;&#26469;&#24037;&#20316;&#65292;&#24182;&#21487;&#20197;&#35780;&#20272;&#20219;&#20309;&#31867;&#22411;&#30340;&#40657;&#30418;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#12290;&#20351;&#29992;&#35813;&#26041;&#27861;&#20316;&#20026;&#21453;&#39304;&#20449;&#21495;&#65292;&#21487;&#20197;&#22312;&#26080;&#30417;&#30563;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#20013;&#25913;&#36827;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36136;&#37327;&#20272;&#35745;&#65288;QE&#65289;&#26159;&#39044;&#27979;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#31995;&#32479;&#36755;&#20986;&#36136;&#37327;&#30340;&#20219;&#21153;&#65292;&#19981;&#20351;&#29992;&#20219;&#20309;&#40644;&#37329;&#26631;&#20934;&#32763;&#35793;&#21442;&#32771;&#12290;&#30446;&#21069;&#30340;QE&#27169;&#22411;&#26159;&#30417;&#30563;&#30340;&#65306;&#23427;&#20204;&#38656;&#35201;&#23545;&#19968;&#20123;&#25968;&#25454;&#38598;&#19978;&#26576;&#20123;MT&#31995;&#32479;&#36755;&#20986;&#36827;&#34892;&#20154;&#31867;&#26631;&#27880;&#36136;&#37327;&#26469;&#36827;&#34892;&#22521;&#35757;&#65292;&#20351;&#23427;&#20204;&#19982;&#22495;&#30456;&#20851;&#21644;MT&#31995;&#32479;&#30456;&#20851;&#12290;&#26377;&#30740;&#31350;&#23545;&#26080;&#30417;&#30563;&#30340;QE&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#38656;&#35201;&#29627;&#29827;&#30418;&#35775;&#38382;MT&#31995;&#32479;&#65292;&#25110;&#32773;&#20351;&#29992;&#24182;&#34892;MT&#25968;&#25454;&#26469;&#29983;&#25104;&#21512;&#25104;&#38169;&#35823;&#20197;&#35757;&#32451;QE&#27169;&#22411;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25668;&#21160;&#30340;QE&#26041;&#27861;-&#19968;&#31181;&#35789;&#32423;&#36136;&#37327;&#20272;&#35745;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#20998;&#26512;&#25200;&#21160;&#30340;&#36755;&#20837;&#28304;&#21477;&#23376;&#19978;MT&#31995;&#32479;&#36755;&#20986;&#26469;&#24037;&#20316;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#26080;&#30417;&#30563;&#30340;&#12289;&#21487;&#35299;&#37322;&#30340;&#65292;&#24182;&#21487;&#20197;&#35780;&#20272;&#20219;&#20309;&#31867;&#22411;&#30340;&#40657;&#30418;MT&#31995;&#32479;&#65292;&#21253;&#25324;&#30446;&#21069;&#21344;&#20027;&#23548;&#22320;&#20301;&#30340;&#20855;&#26377;&#19981;&#36879;&#26126;&#20869;&#37096;&#36807;&#31243;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#23545;&#20110;&#27809;&#26377;&#26631;&#35760;QE&#25968;&#25454;&#30340;&#35821;&#35328;&#26041;&#21521;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#30456;&#20284;&#25110;&#26356;&#22909;&#30340;&#38646;-shot&#30417;&#30563;&#31995;&#32479;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#22522;&#20110;&#25668;&#21160;&#30340;QE&#20316;&#20026;&#21453;&#39304;&#20449;&#21495;&#65292;&#22312;&#26080;&#30417;&#30563;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#20013;&#25913;&#36827;MT&#31995;&#32479;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quality Estimation (QE) is the task of predicting the quality of Machine Translation (MT) system output, without using any gold-standard translation references. State-of-the-art QE models are supervised: they require human-labeled quality of some MT system output on some datasets for training, making them domain-dependent and MT-system-dependent. There has been research on unsupervised QE, which requires glass-box access to the MT systems, or parallel MT data to generate synthetic errors for training QE models. In this paper, we present Perturbation-based QE - a word-level Quality Estimation approach that works simply by analyzing MT system output on perturbed input source sentences. Our approach is unsupervised, explainable, and can evaluate any type of blackbox MT systems, including the currently prominent large language models (LLMs) with opaque internal processes. For language directions with no labeled QE data, our approach has similar or better performance than the zero-shot supe
&lt;/p&gt;</description></item></channel></rss>