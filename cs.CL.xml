<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20174;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20013;&#33258;&#21160;&#29983;&#25104;Java&#20195;&#30721;&#30340;&#26041;&#27861;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#20102;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#19977;&#31181;&#31867;&#22411;&#65306;&#32534;&#30721;&#22120;&#27169;&#22411;&#12289;&#35299;&#30721;&#22120;&#27169;&#22411;&#21644;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.06371</link><description>&lt;p&gt;
&#20174;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#29983;&#25104;Java&#20195;&#30721;&#30340;&#29616;&#26377;&#26041;&#27861;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Review of State-of-The-Art Methods for Java Code Generation from Natural Language Text. (arXiv:2306.06371v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06371
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20174;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20013;&#33258;&#21160;&#29983;&#25104;Java&#20195;&#30721;&#30340;&#26041;&#27861;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#20102;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#19977;&#31181;&#31867;&#22411;&#65306;&#32534;&#30721;&#22120;&#27169;&#22411;&#12289;&#35299;&#30721;&#22120;&#27169;&#22411;&#21644;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Java&#20195;&#30721;&#29983;&#25104;&#26159;&#20174;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20013;&#33258;&#21160;&#29983;&#25104;Java&#20195;&#30721;&#12290;&#36825;&#19968;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#26377;&#21161;&#20110;&#36890;&#36807;&#20026;&#31243;&#24207;&#21592;&#25552;&#20379;&#26368;&#31616;&#21333;&#21644;&#26368;&#37325;&#22797;&#30340;&#20219;&#21153;&#30340;&#21363;&#26102;&#35299;&#20915;&#26041;&#27861;&#26469;&#25552;&#39640;&#20854;&#29983;&#20135;&#21147;&#12290;&#20195;&#30721;&#29983;&#25104;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#38656;&#35201;&#36981;&#24490;&#20005;&#26684;&#30340;&#35821;&#27861;&#35268;&#21017;&#24182;&#28145;&#20837;&#29702;&#35299;&#32534;&#31243;&#35821;&#35328;&#30340;&#35821;&#20041;&#26041;&#38754;&#12290;&#35768;&#22810;&#30740;&#31350;&#23581;&#35797;&#20351;&#29992;&#22522;&#20110;RNN&#25110;&#21464;&#21387;&#22120;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#12290;&#21518;&#32773;&#22312;&#35813;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#21487;&#20197;&#20998;&#20026;&#19977;&#32452;&#65306;(1)&#20165;&#32534;&#30721;&#27169;&#22411;&#65292;(2)&#20165;&#35299;&#30721;&#27169;&#22411;&#21644;(3)&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#12290;&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;Java&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#28436;&#21464;&#21644;&#36827;&#23637;&#12290;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#26368;&#37325;&#35201;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#23427;&#20204;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#65292;&#20197;&#21450;&#31038;&#21306;&#20351;&#29992;&#30340;&#30446;&#26631;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#25551;&#36848;...
&lt;/p&gt;
&lt;p&gt;
Java Code Generation consists in generating automatically Java code from a Natural Language Text. This NLP task helps in increasing programmers' productivity by providing them with immediate solutions to the simplest and most repetitive tasks. Code generation is a challenging task because of the hard syntactic rules and the necessity of a deep understanding of the semantic aspect of the programming language. Many works tried to tackle this task using either RNN-based, or Transformer-based models. The latter achieved remarkable advancement in the domain and they can be divided into three groups: (1) encoder-only models, (2) decoder-only models, and (3) encoder-decoder models. In this paper, we provide a comprehensive review of the evolution and progress of deep learning models in Java code generation task. We focus on the most important methods and present their merits and limitations, as well as the objective functions used by the community. In addition, we provide a detailed descripti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20123;&#25216;&#26415;&#26469;&#25552;&#39640;&#38750;&#33258;&#22238;&#24402;&#32763;&#35793;&#27169;&#22411;&#30340;&#32763;&#35793;&#36136;&#37327;&#65292;&#22312;&#20445;&#25345;&#26174;&#30528;&#25512;&#29702;&#36895;&#24230;&#21152;&#36895;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#22810;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12289;&#37319;&#29992;MASK&#25554;&#20837;&#26041;&#26696;&#36827;&#34892;&#19978;&#37319;&#26679;&#12289;&#20197;&#21450;&#37319;&#29992;&#23884;&#20837;&#33976;&#39311;&#26041;&#27861;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#65292;&#27169;&#22411;&#34920;&#29616;&#20248;&#20110;&#22522;&#32447;&#33258;&#22238;&#24402;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.06345</link><description>&lt;p&gt;
&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12289;&#23884;&#20837;&#33976;&#39311;&#21644;&#19978;&#37319;&#26679;&#31574;&#30053;&#25913;&#21892;&#38750;&#33258;&#22238;&#24402;&#32763;&#35793;&#36136;&#37327;&#65288;arXiv:2306.06345v1 [cs.CL]&#65289;
&lt;/p&gt;
&lt;p&gt;
Improving Non-autoregressive Translation Quality with Pretrained Language Model, Embedding Distillation and Upsampling Strategy for CTC. (arXiv:2306.06345v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20123;&#25216;&#26415;&#26469;&#25552;&#39640;&#38750;&#33258;&#22238;&#24402;&#32763;&#35793;&#27169;&#22411;&#30340;&#32763;&#35793;&#36136;&#37327;&#65292;&#22312;&#20445;&#25345;&#26174;&#30528;&#25512;&#29702;&#36895;&#24230;&#21152;&#36895;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#22810;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12289;&#37319;&#29992;MASK&#25554;&#20837;&#26041;&#26696;&#36827;&#34892;&#19978;&#37319;&#26679;&#12289;&#20197;&#21450;&#37319;&#29992;&#23884;&#20837;&#33976;&#39311;&#26041;&#27861;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#65292;&#27169;&#22411;&#34920;&#29616;&#20248;&#20110;&#22522;&#32447;&#33258;&#22238;&#24402;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#33258;&#22238;&#24402;&#26041;&#27861;&#26088;&#22312;&#25552;&#39640;&#32763;&#35793;&#27169;&#22411;&#30340;&#25512;&#29702;&#36895;&#24230;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#21487;&#20197;&#19968;&#27425;&#27491;&#21521;&#20256;&#36882;&#29983;&#25104;&#36755;&#20986;&#30340;&#27169;&#22411;&#12290;&#20294;&#26159;&#65292;&#19982;&#33258;&#22238;&#24402;&#27169;&#22411;&#30456;&#27604;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#22312;&#32763;&#35793;&#36136;&#37327;&#19978;&#26377;&#26174;&#33879;&#30340;&#19979;&#38477;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#21019;&#26032;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#38750;&#33258;&#22238;&#24402;&#32763;&#35793;&#27169;&#22411;&#30340;&#32763;&#35793;&#36136;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#25512;&#29702;&#36895;&#24230;&#30340;&#26174;&#33879;&#21152;&#36895;&#12290;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;CTC&#25439;&#22833;&#24494;&#35843;&#39044;&#35757;&#32451;&#22810;&#35821;&#35328;&#27169;&#22411;&#26469;&#26377;&#25928;&#22320;&#35757;&#32451;NAT&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;MASK&#25554;&#20837;&#26041;&#26696;&#36827;&#34892;&#19978;&#37319;&#26679;&#65292;&#32780;&#19981;&#26159;&#20196;&#29260;&#22797;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23884;&#20837;&#33976;&#39311;&#26041;&#27861;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#22522;&#32447;&#33258;&#22238;&#24402;&#27169;&#22411;&#65288;Transformer base&#65289;&#65292;&#21253;&#25324;WMT'14 DE $\leftrightarrow$ EN&#12289;WMT'16 RO $\leftrightarrow$ EN&#21644;IWSLT'14 DE $\leftrightarrow$ EN&#12290;
&lt;/p&gt;
&lt;p&gt;
Non-autoregressive approaches aim to improve the inference speed of translation models, particularly those that generate output in a one-pass forward manner. However, these approaches often suffer from a significant drop in translation quality compared to autoregressive models. This paper introduces a series of innovative techniques to enhance the translation quality of Non-Autoregressive Translation (NAT) models while maintaining a substantial acceleration in inference speed. We propose fine-tuning Pretrained Multilingual Language Models (PMLMs) with the CTC loss to train NAT models effectively. Furthermore, we adopt the MASK insertion scheme for up-sampling instead of token duplication, and we present an embedding distillation method to further enhance performance. In our experiments, our model outperforms the baseline autoregressive model (Transformer \textit{base}) on multiple datasets, including WMT'14 DE$\leftrightarrow$EN, WMT'16 RO$\leftrightarrow$EN, and IWSLT'14 DE$\leftright
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#38463;&#25289;&#20271;&#35821;DL-based MSA&#39046;&#22495;&#32570;&#20047;&#26631;&#20934;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#36716;&#25442;&#22120;&#21644;&#21333;&#35789;&#23545;&#40784;&#25216;&#26415;&#20013;&#30340;&#29305;&#24449;&#25552;&#21462;&#24037;&#20855;&#26469;&#35774;&#35745;&#31649;&#36947;&#27969;&#31243;&#65292;&#26500;&#24314;&#20102;&#38463;&#25289;&#20271;&#35821;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.06322</link><description>&lt;p&gt;
&#38754;&#21521;&#38463;&#25289;&#20271;&#35821;&#24773;&#24863;&#20998;&#26512;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Towards Arabic Multimodal Dataset for Sentiment Analysis. (arXiv:2306.06322v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06322
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#38463;&#25289;&#20271;&#35821;DL-based MSA&#39046;&#22495;&#32570;&#20047;&#26631;&#20934;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#36716;&#25442;&#22120;&#21644;&#21333;&#35789;&#23545;&#40784;&#25216;&#26415;&#20013;&#30340;&#29305;&#24449;&#25552;&#21462;&#24037;&#20855;&#26469;&#35774;&#35745;&#31649;&#36947;&#27969;&#31243;&#65292;&#26500;&#24314;&#20102;&#38463;&#25289;&#20271;&#35821;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;(MSA)&#24050;&#25104;&#20026;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#30340;&#20013;&#24515;&#30740;&#31350;&#26041;&#21521;&#12290;&#36825;&#31181;&#26222;&#21450;&#24471;&#30410;&#20110;&#24847;&#35265;&#23545;&#20960;&#20046;&#25152;&#26377;&#20154;&#31867;&#27963;&#21160;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#21450;&#25104;&#20026;&#25105;&#20204;&#34892;&#20026;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;(DL)&#27169;&#22411;&#30340;&#39640;&#25928;&#24615;&#20063;&#24050;&#22312;&#22810;&#31181;&#35199;&#26041;&#35821;&#35328;&#20013;&#24471;&#21040;&#35777;&#26126;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#26631;&#20934;&#25968;&#25454;&#38598;&#65292;&#38463;&#25289;&#20271;&#35821;DL-based MSA&#20173;&#22788;&#20110;&#21021;&#22987;&#38454;&#27573;&#12290;&#26412;&#25991;&#30340;&#25506;&#31350;&#30446;&#26631;&#26377;&#20004;&#20010;&#26041;&#38754;&#65292;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31649;&#36947;&#27969;&#31243;&#65292;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#36716;&#25442;&#22120;&#20197;&#21450;&#21333;&#35789;&#23545;&#40784;&#25216;&#26415;&#20013;&#30340;&#29305;&#24449;&#25552;&#21462;&#24037;&#20855;&#65292;&#24110;&#21161;&#26500;&#24314;&#25105;&#20204;&#30340;&#38463;&#25289;&#20271;&#35821;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12290;&#23613;&#31649;&#36755;&#20986;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#36739;&#23567;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#38463;&#25289;&#20271;&#35821;&#22810;&#27169;&#24577;&#20173;&#28982;&#20855;&#26377;&#38750;&#24120;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Sentiment Analysis (MSA) has recently become a centric research direction for many real-world applications. This proliferation is due to the fact that opinions are central to almost all human activities and are key influencers of our behaviors. In addition, the recent deployment of Deep Learning-based (DL) models has proven their high efficiency for a wide range of Western languages. In contrast, Arabic DL-based multimodal sentiment analysis (MSA) is still in its infantile stage due, mainly, to the lack of standard datasets. In this paper, our investigation is twofold. First, we design a pipeline that helps building our Arabic Multimodal dataset leveraging both state-of-the-art transformers and feature extraction tools within word alignment techniques. Thereafter, we validate our dataset using state-of-the-art transformer-based model dealing with multimodality. Despite the small size of the outcome dataset, experiments show that Arabic multimodality is very promising
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#20004;&#20010;&#21327;&#35758;&#65292;&#26088;&#22312;&#20445;&#25252;LLM&#25552;&#31034;&#30340;&#30693;&#35782;&#20135;&#26435;&#65292;&#25552;&#20379;&#24320;&#25918;&#24066;&#22330;&#19978;&#20132;&#26131;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.06297</link><description>&lt;p&gt;
LLM&#24212;&#29992;&#20013;&#30340;IP&#20445;&#25252;&#21327;&#35758;
&lt;/p&gt;
&lt;p&gt;
Protect Your Prompts: Protocols for IP Protection in LLM Applications. (arXiv:2306.06297v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06297
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#20004;&#20010;&#21327;&#35758;&#65292;&#26088;&#22312;&#20445;&#25252;LLM&#25552;&#31034;&#30340;&#30693;&#35782;&#20135;&#26435;&#65292;&#25552;&#20379;&#24320;&#25918;&#24066;&#22330;&#19978;&#20132;&#26131;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24418;&#24335;&#30340;AI&#30340;&#24555;&#36895;&#37319;&#29992;&#65292;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#30340;&#28508;&#22312;&#20215;&#20540;&#21464;&#24471;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#28508;&#21147;&#65292;&#25552;&#31034;&#24212;&#35813;&#22312;&#19968;&#20010;&#20844;&#24320;&#24066;&#22330;&#19978;&#20132;&#26131;&#12290;&#30001;&#20110;&#25552;&#31034;&#30446;&#21069;&#36890;&#24120;&#26159;&#32463;&#27982;&#19978;&#19981;&#21487;&#25490;&#38500;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;&#25991;&#26412;&#30340;&#24615;&#36136;&#65292;&#22240;&#27492;&#23578;&#26410;&#24314;&#31435;&#19968;&#33324;&#30340;&#31454;&#20105;&#24066;&#22330;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#20004;&#20010;&#26088;&#22312;&#25552;&#20379;&#25552;&#31034;&#20445;&#25252;&#30340;&#21327;&#35758;&#65292;&#25552;&#39640;&#20854;&#20316;&#20026;&#30693;&#35782;&#20135;&#26435;&#30340;&#22320;&#20301;&#65292;&#20174;&#32780;&#30830;&#35748;&#25552;&#31034;&#24037;&#31243;&#24072;&#30340;&#30693;&#35782;&#20135;&#26435;&#65292;&#24182;&#21487;&#33021;&#25903;&#25345;LLM&#25552;&#31034;&#30340;&#24320;&#25918;&#24066;&#22330;&#30340;&#32321;&#33635;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid adoption of AI in the form of large language models (LLMs), the potential value of carefully engineered prompts has become significant. However, to realize this potential, prompts should be tradable on an open market. Since prompts are, at present, generally economically non-excludable, by virtue of their nature as text, no general competitive market has yet been established. This note discusses two protocols intended to provide protection of prompts, elevating their status as intellectual property, thus confirming the intellectual property rights of prompt engineers, and potentially supporting the flourishing of an open market for LLM prompts.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#35770;&#30340;&#27979;&#37327;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20107;&#23454;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#29109;&#21450;KL&#25955;&#24230;&#31561;&#24230;&#37327;&#25351;&#26631;&#36827;&#34892;&#30693;&#35782;&#20462;&#25913;&#65292;&#36229;&#36234;&#20102;&#20197;&#21069;&#30340;&#25490;&#21517;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#27979;&#37327;&#21644;&#20462;&#25913;LLMs&#20013;&#30340;&#22823;&#37327;&#20107;&#23454;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2306.06264</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#27979;&#37327;&#21644;&#20462;&#25913;&#20107;&#23454;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Measuring and Modifying Factual Knowledge in Large Language Models. (arXiv:2306.06264v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06264
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#35770;&#30340;&#27979;&#37327;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20107;&#23454;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#29109;&#21450;KL&#25955;&#24230;&#31561;&#24230;&#37327;&#25351;&#26631;&#36827;&#34892;&#30693;&#35782;&#20462;&#25913;&#65292;&#36229;&#36234;&#20102;&#20197;&#21069;&#30340;&#25490;&#21517;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#27979;&#37327;&#21644;&#20462;&#25913;LLMs&#20013;&#30340;&#22823;&#37327;&#20107;&#23454;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23384;&#20648;&#30528;&#20174;&#22823;&#37327;&#25991;&#26412;&#20013;&#33719;&#21462;&#30340;&#24191;&#27867;&#30340;&#20107;&#23454;&#30693;&#35782;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#65292;&#26377;&#21487;&#38752;&#30340;&#26041;&#27861;&#26469;&#34913;&#37327;&#23427;&#20204;&#30340;&#30693;&#35782;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30693;&#35782;&#27979;&#37327;&#26041;&#27861;&#23384;&#22312;&#26576;&#20123;&#38480;&#21046;&#65292;&#23613;&#31649;&#26368;&#36817;&#26377;&#19981;&#23569;&#21162;&#21147;&#65292;&#20294;&#23427;&#20204;&#19981;&#33021;&#25552;&#20379;&#20934;&#30830;&#30340;&#27979;&#37327;&#21644;&#20462;&#25913;LLMs&#20013;&#25152;&#38656;&#30340;&#27934;&#23519;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#20449;&#24687;&#29702;&#35770;&#30340;&#27979;&#37327;&#26041;&#27861;&#26469;&#25552;&#20379;&#19968;&#20010;&#26694;&#26550;&#26469;&#20272;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21253;&#21547;&#30340;&#20107;&#23454;&#30693;&#35782;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;LLM&#22312;&#27880;&#20837;&#30446;&#26631;&#30693;&#35782;&#21069;&#21518;&#30340;&#39044;&#27979;&#27010;&#29575;&#20998;&#24067;&#26469;&#34913;&#37327;&#30693;&#35782;&#65292;&#20351;&#29992;&#29109;&#21644;KL-&#25955;&#24230;&#31561;&#24230;&#37327;&#26631;&#20934;&#12290;&#39318;&#20808;&#20171;&#32461;&#25105;&#20204;&#30340;&#25351;&#26631;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#39033;&#21512;&#25104;&#23454;&#39564;&#65292;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#19982;&#20197;&#21069;&#30340;&#25490;&#21517;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#36229;&#36807;&#20102;&#23427;&#20204;35&#65285;&#20197;&#19978;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#36825;&#20123;&#25351;&#26631;&#22914;&#20309;&#29992;&#20110;&#30693;&#35782;&#20462;&#25913;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36873;&#25321;&#24615;&#20462;&#25913;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23454;&#38469;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#23453;&#36149;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#27979;&#37327;&#21644;&#20462;&#25913;LLMs&#20013;&#30340;&#22823;&#37327;&#20107;&#23454;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) store an extensive amount of factual knowledge obtained from vast collections of text. To effectively utilize these models for downstream tasks, it is crucial to have reliable methods for measuring their knowledge. However, existing approaches for knowledge measurement have certain limitations, and despite recent efforts, they fail to provide accurate measurements and the necessary insights for modifying the knowledge within LLMs. In this work, we employ information theory-based measurements to provide a framework estimating the factual knowledge contained within large language models. More specifically, we measure knowledge by analyzing the LLM's prediction probability distribution before and after instilling the target knowledge, employing metrics such as entropy and KL-divergence. Introducing our metrics, we first assess their accuracy in comparison to previous ranking-based methods, surpassing them by over $35\%$ in a synthetic experiment. Then, we expl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35760;&#24405;&#37325;&#22797;&#28040;&#38500;&#26469;&#27169;&#25311;&#23458;&#25143;&#30495;&#23454;&#35831;&#27714;&#30340;&#23454;&#20307;&#20998;&#24067;&#65292;&#20174;&#32780;&#35299;&#20915;ASR&#38169;&#35823;&#35782;&#21035;&#24102;&#26469;&#30340;&#23454;&#20307;&#20998;&#24067;&#22797;&#26434;&#38382;&#39064;&#65292;&#24182;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#35821;&#22659;&#20559;&#32622;&#20013;&#65292;&#26174;&#31034;&#20986;&#20272;&#35745;&#30340;&#21333;&#35789;&#38169;&#35823;&#29575;&#38477;&#20302;5%&#12290;</title><link>http://arxiv.org/abs/2306.06246</link><description>&lt;p&gt;
ASR&#36716;&#24405;&#20013;&#23454;&#20307;&#20998;&#24067;&#24314;&#27169;&#30340;&#35760;&#24405;&#37325;&#22797;&#28040;&#38500;
&lt;/p&gt;
&lt;p&gt;
Record Deduplication for Entity Distribution Modeling in ASR Transcripts. (arXiv:2306.06246v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35760;&#24405;&#37325;&#22797;&#28040;&#38500;&#26469;&#27169;&#25311;&#23458;&#25143;&#30495;&#23454;&#35831;&#27714;&#30340;&#23454;&#20307;&#20998;&#24067;&#65292;&#20174;&#32780;&#35299;&#20915;ASR&#38169;&#35823;&#35782;&#21035;&#24102;&#26469;&#30340;&#23454;&#20307;&#20998;&#24067;&#22797;&#26434;&#38382;&#39064;&#65292;&#24182;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#35821;&#22659;&#20559;&#32622;&#20013;&#65292;&#26174;&#31034;&#20986;&#20272;&#35745;&#30340;&#21333;&#35789;&#38169;&#35823;&#29575;&#38477;&#20302;5%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#25968;&#23383;&#21161;&#25163;&#24517;&#39035;&#36319;&#19978;&#28909;&#38376;&#25628;&#32034;&#26597;&#35810;&#12290;&#25105;&#20204;&#20381;&#36182;&#20110;&#20351;&#29992;&#24555;&#36895;&#26356;&#26032;&#30340;&#23454;&#20307;&#38598;&#21512;&#36827;&#34892;&#35821;&#22659;&#20559;&#32622;&#30340;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#32780;&#19981;&#26159;&#39057;&#32321;&#30340;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#65292;&#20174;&#32780;&#36319;&#19978;&#36235;&#21183;&#12290;&#36825;&#31181;&#26041;&#27861;&#23384;&#22312;&#33509;&#24178;&#25361;&#25112;&#65306;&#65288;1&#65289;&#23454;&#20307;&#38598;&#24517;&#39035;&#39057;&#32321;&#37325;&#26500;&#65292;&#65288;2&#65289;&#30001;&#20110;&#24310;&#36831;&#21644;&#20934;&#30830;&#24615;&#26435;&#34913;&#65292;&#23454;&#20307;&#38598;&#21512;&#30340;&#22823;&#23567;&#21463;&#38480;&#65292;&#65288;3&#65289;&#30001;&#20110;ASR&#38169;&#35823;&#35782;&#21035;&#65292;&#23547;&#25214;&#30495;&#23454;&#23454;&#20307;&#20998;&#24067;&#26159;&#22797;&#26434;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#23454;&#20307;&#35299;&#26512;&#39046;&#22495;&#30340;&#19968;&#31181;&#25216;&#26415;-&#35760;&#24405;&#37325;&#22797;&#28040;&#38500;&#65292;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#24182;&#23450;&#20041;&#23454;&#20307;&#38598;&#12290;&#36890;&#36807;&#24314;&#27169;&#23458;&#25143;&#20174;ASR&#36755;&#20986;&#20013;&#30495;&#23454;&#35831;&#27714;&#30340;&#23454;&#20307;&#20998;&#24067;&#65292;&#35760;&#24405;&#37325;&#22797;&#28040;&#38500;&#35299;&#20915;&#25110;&#28040;&#38500;&#20102;&#30456;&#21516;&#28508;&#22312;&#23454;&#20307;&#30340;&#26680;&#24515;&#24341;&#29992;&#65292;&#21253;&#25324;&#38169;&#35823;&#35782;&#21035;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#26816;&#32034;&#21040;&#20102;95%&#30340;&#38169;&#35823;&#35782;&#21035;&#23454;&#20307;&#65292;&#24182;&#19988;&#22312;&#29992;&#20110;&#35821;&#22659;&#20559;&#32622;&#26102;&#65292;&#26174;&#31034;&#20986;&#20272;&#35745;&#30340;5%&#30340;&#30456;&#23545;&#21333;&#35789;&#38169;&#35823;&#29575;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Voice digital assistants must keep up with trending search queries. We rely on a speech recognition model using contextual biasing with a rapidly updated set of entities, instead of frequent model retraining, to keep up with trends. There are several challenges with this approach: (1) the entity set must be frequently reconstructed, (2) the entity set is of limited size due to latency and accuracy trade-offs, and (3) finding the true entity distribution for biasing is complicated by ASR misrecognition. We address these challenges and define an entity set by modeling customers true requested entity distribution from ASR output in production using record deduplication, a technique from the field of entity resolution. Record deduplication resolves or deduplicates coreferences, including misrecognitions, of the same latent entity. Our method successfully retrieves 95% of misrecognized entities and when used for contextual biasing shows an estimated 5% relative word error rate reduction.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#22312;&#26497;&#23569;&#30417;&#30563;&#19979;&#26816;&#27979;&#25919;&#31574;&#36829;&#35268;&#65292;&#21019;&#26032;&#24615;&#22320;&#23558;&#24605;&#32500;&#38142;&#25552;&#31034;&#24341;&#20837;&#25919;&#31574;&#36829;&#35268;&#20219;&#21153;&#65292;&#21516;&#26102;&#23558;&#30828;&#25552;&#31034;&#19982;&#36719;&#25552;&#31034;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#39640;&#20934;&#30830;&#24230;&#22320;&#29983;&#25104;&#21512;&#29702;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2306.06234</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#22312;&#26497;&#23569;&#30417;&#30563;&#19979;&#26816;&#27979;&#25919;&#31574;&#36829;&#35268;
&lt;/p&gt;
&lt;p&gt;
Using Foundation Models to Detect Policy Violations with Minimal Supervision. (arXiv:2306.06234v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#22312;&#26497;&#23569;&#30417;&#30563;&#19979;&#26816;&#27979;&#25919;&#31574;&#36829;&#35268;&#65292;&#21019;&#26032;&#24615;&#22320;&#23558;&#24605;&#32500;&#38142;&#25552;&#31034;&#24341;&#20837;&#25919;&#31574;&#36829;&#35268;&#20219;&#21153;&#65292;&#21516;&#26102;&#23558;&#30828;&#25552;&#31034;&#19982;&#36719;&#25552;&#31034;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#39640;&#20934;&#30830;&#24230;&#22320;&#29983;&#25104;&#21512;&#29702;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#65292;&#21363;&#39044;&#35757;&#32451;&#20110;&#22823;&#22411;&#25991;&#26412;&#35821;&#26009;&#24211;&#30340;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#65292;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12290;&#23427;&#20204;&#21487;&#20197;&#30452;&#25509;&#25351;&#23548;&#65292;&#20363;&#22914;&#30828;&#25552;&#31034;(&#20363;&#22914;(arXiv:2005.14165))&#65292;&#20063;&#21487;&#20197;&#20351;&#29992;&#26497;&#23569;&#30340;&#25968;&#25454;&#36827;&#34892;&#35843;&#25972;&#65292;&#36825;&#31181;&#25216;&#26415;&#34987;&#31216;&#20026;&#36719;&#25552;&#31034;&#65292;&#25105;&#20204;&#35797;&#22270;&#21033;&#29992;&#23427;&#20204;&#30340;&#33021;&#21147;&#26469;&#26816;&#27979;&#25919;&#31574;&#36829;&#35268;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#28857;&#26159;&#65306;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#31181;&#30828;&#25552;&#31034;&#65292;&#23558;&#24605;&#32500;&#38142;&#25552;&#31034;&#36866;&#24212;&#20110;&#25919;&#31574;&#36829;&#35268;&#20219;&#21153;&#12290;&#36825;&#20010;&#25552;&#31034;&#29983;&#25104;&#25919;&#31574;&#36829;&#35268;&#20998;&#31867;&#20197;&#21450;&#25552;&#21462;&#24335;&#35299;&#37322;&#26469;&#35777;&#26126;&#20998;&#31867;&#30340;&#21512;&#29702;&#24615;&#12290;&#25105;&#20204;&#23558;&#30828;&#25552;&#31034;&#19982;&#36719;&#25552;&#31034;&#35843;&#25972;&#30456;&#32467;&#21512;&#65292;&#20351;&#29992;&#38750;&#24120;&#23569;&#30340;&#30417;&#30563;&#26469;&#29983;&#25104;&#19968;&#20010;&#20998;&#31867;&#22120;&#65292;&#21516;&#26102;&#35813;&#20998;&#31867;&#22120;&#36824;&#21487;&#20197;&#20135;&#29983;&#35299;&#37322;&#12290;&#34429;&#28982;&#30417;&#30563;&#21482;&#20316;&#29992;&#20110;&#20998;&#31867;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#20462;&#25913;&#21518;&#30340;&#35299;&#37322;&#19982;(&#35843;&#25972;&#21518;&#30340;)&#27169;&#22411;&#21709;&#24212;&#20445;&#25345;&#19968;&#33268;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20123;&#20196;&#20154;&#36153;&#35299;&#30340;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models, i.e. large neural networks pre-trained on large text corpora, have revolutionized NLP. They can be instructed directly (e.g. (arXiv:2005.14165)) - this is called hard prompting - and they can be tuned using very little data (e.g. (arXiv:2104.08691)) - this technique is called soft prompting. We seek to leverage their capabilities to detect policy violations. Our contributions are: We identify a hard prompt that adapts chain-of-thought prompting to policy violation tasks. This prompt produces policy violation classifications, along with extractive explanations that justify the classification. We compose the hard-prompts with soft prompt tuning to produce a classifier that attains high accuracy with very little supervision; the same classifier also produces explanations. Though the supervision only acts on the classifications, we find that the modified explanations remain consistent with the (tuned) model's response. Along the way, we identify several unintuitive aspec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#20013;&#30340;&#38899;&#32032;&#21644;&#38899;&#20301;&#20449;&#24687;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#26089;&#26399;&#23618;&#23601;&#33021;&#22815;&#24456;&#22909;&#22320;&#34920;&#31034;&#36825;&#20123;&#21306;&#21035;&#65292;&#24182;&#19988;&#36825;&#31181;&#34920;&#31034;&#22312;&#26356;&#28145;&#23618;&#30340;&#34920;&#31034;&#20013;&#24471;&#20197;&#20445;&#30041;&#65292;&#26159;&#30001;&#20110;&#27169;&#22411;&#22312;&#35821;&#38899;&#25968;&#25454;&#19978;&#30340;&#20248;&#21270;&#21644;&#39640;&#32500;&#24230;&#30340;&#20307;&#31995;&#32467;&#26500;&#30340;&#20849;&#21516;&#20316;&#29992;&#25152;&#33268;&#12290;</title><link>http://arxiv.org/abs/2306.06232</link><description>&lt;p&gt;
&#25506;&#31350;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#20013;&#30340;&#38899;&#32032;&#21644;&#38899;&#20301;&#20449;&#24687;&#65306;&#20197;&#36865;&#27668;&#29616;&#35937;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Probing self-supervised speech models for phonetic and phonemic information: a case study in aspiration. (arXiv:2306.06232v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#20013;&#30340;&#38899;&#32032;&#21644;&#38899;&#20301;&#20449;&#24687;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#26089;&#26399;&#23618;&#23601;&#33021;&#22815;&#24456;&#22909;&#22320;&#34920;&#31034;&#36825;&#20123;&#21306;&#21035;&#65292;&#24182;&#19988;&#36825;&#31181;&#34920;&#31034;&#22312;&#26356;&#28145;&#23618;&#30340;&#34920;&#31034;&#20013;&#24471;&#20197;&#20445;&#30041;&#65292;&#26159;&#30001;&#20110;&#27169;&#22411;&#22312;&#35821;&#38899;&#25968;&#25454;&#19978;&#30340;&#20248;&#21270;&#21644;&#39640;&#32500;&#24230;&#30340;&#20307;&#31995;&#32467;&#26500;&#30340;&#20849;&#21516;&#20316;&#29992;&#25152;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26080;&#38656;&#25991;&#26412;&#30340;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#30340;&#33021;&#21147;&#19981;&#26029;&#25552;&#39640;&#65292;&#20294;&#23427;&#20204;&#25152;&#32534;&#30721;&#30340;&#35821;&#35328;&#20449;&#24687;&#30340;&#26412;&#36136;&#36824;&#26410;&#24471;&#21040;&#24443;&#24213;&#30740;&#31350;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;&#36825;&#20123;&#27169;&#22411;&#23398;&#20064;&#34920;&#31034;&#19982;&#20154;&#31867;&#22522;&#26412;&#34920;&#31034;&#21306;&#21035;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#38598;&#20013;&#30740;&#31350;&#20102;&#19968;&#32452;&#21021;&#22987;&#35789;&#20572;&#39039;&#20013;&#20855;&#20307;&#34920;&#29616;&#30340;&#38899;&#32032;&#65288;&#20302;&#23618;&#65289;&#21644;&#38899;&#20301;&#65288;&#26356;&#25277;&#35937;&#65289;&#23545;&#27604;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#36825;&#20123;&#27169;&#22411;&#30340;&#20307;&#31995;&#32467;&#26500;&#30340;&#26089;&#26399;&#23618;&#20013;&#65292;&#20986;&#29616;&#20102;&#20851;&#20110;&#38899;&#32032;&#21644;&#38899;&#20301;&#21306;&#21035;&#30340;&#24378;&#22823;&#34920;&#31034;&#65292;&#24182;&#22312;&#26356;&#28145;&#23618;&#30340;&#20027;&#35201;&#25104;&#20998;&#34920;&#31034;&#20013;&#20445;&#30041;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#36825;&#19968;&#25104;&#21151;&#30340;&#21407;&#22240;&#22312;&#20110;&#20004;&#26041;&#38754;&#65306;&#19968;&#20123;&#21487;&#24402;&#22240;&#20110;&#27169;&#22411;&#22312;&#35821;&#38899;&#25968;&#25454;&#19978;&#30340;&#20248;&#21270;&#65292;&#32780;&#21478;&#19968;&#20123;&#21487;&#24402;&#22240;&#20110;&#36825;&#20123;&#27169;&#22411;&#39640;&#32500;&#24230;&#30340;&#20307;&#31995;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#32463;&#36807;&#35821;&#38899;&#35757;&#32451;&#30340; HuBERT &#24471;&#20986;&#20102;&#19982;&#25277;&#35937;&#30340;&#38899;&#20301;&#21306;&#21035;&#30456;&#24212;&#30340;&#20302;&#22122;&#22768;&#21644;&#20302;&#32500;&#24230;&#23376;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Textless self-supervised speech models have grown in capabilities in recent years, but the nature of the linguistic information they encode has not yet been thoroughly examined. We evaluate the extent to which these models' learned representations align with basic representational distinctions made by humans, focusing on a set of phonetic (low-level) and phonemic (more abstract) contrasts instantiated in word-initial stops. We find that robust representations of both phonetic and phonemic distinctions emerge in early layers of these models' architectures, and are preserved in the principal components of deeper layer representations. Our analyses suggest two sources for this success: some can only be explained by the optimization of the models on speech data, while some can be attributed to these models' high-dimensional architectures. Our findings show that speech-trained HuBERT derives a low-noise and low-dimensional subspace corresponding to abstract phonological distinctions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20998;&#24067;&#26041;&#27861;&#8212;&#8212;&#35268;&#33539;&#21270;&#39044;&#27979;&#65292;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#32622;&#20449;&#21306;&#38388;&#30340;&#35745;&#31639;&#65292;&#24182;&#35777;&#26126;&#20854;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#35206;&#30422;&#29575;&#65292;&#21487;&#20197;&#32416;&#27491;&#20854;&#20182;&#26041;&#27861;&#30340;&#35823;&#24046;&#12290;&#24212;&#29992;&#26465;&#20214;&#35268;&#33539;&#21270;&#39044;&#27979;&#25216;&#26415;&#65292;&#33719;&#24471;&#24179;&#31561;&#35206;&#30422;&#30340;&#26657;&#20934;&#23376;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.06221</link><description>&lt;p&gt;
&#35268;&#33539;&#21270;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Conformalizing Machine Translation Evaluation. (arXiv:2306.06221v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06221
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20998;&#24067;&#26041;&#27861;&#8212;&#8212;&#35268;&#33539;&#21270;&#39044;&#27979;&#65292;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#32622;&#20449;&#21306;&#38388;&#30340;&#35745;&#31639;&#65292;&#24182;&#35777;&#26126;&#20854;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#35206;&#30422;&#29575;&#65292;&#21487;&#20197;&#32416;&#27491;&#20854;&#20182;&#26041;&#27861;&#30340;&#35823;&#24046;&#12290;&#24212;&#29992;&#26465;&#20214;&#35268;&#33539;&#21270;&#39044;&#27979;&#25216;&#26415;&#65292;&#33719;&#24471;&#24179;&#31561;&#35206;&#30422;&#30340;&#26657;&#20934;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#20102;&#22810;&#31181;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#19968;&#20010;&#26377;&#29992;&#30340;&#25351;&#31034;&#65292;&#26469;&#21028;&#26029;&#20309;&#26102;&#19981;&#33021;&#30456;&#20449;&#27169;&#22411;&#39044;&#27979;&#65292;&#20294;&#26412;&#25991;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#22823;&#37096;&#20998;&#20542;&#21521;&#20110;&#20302;&#20272;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#32467;&#26524;&#24448;&#24448;&#20135;&#29983;&#20855;&#26377;&#35823;&#23548;&#24615;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#32780;&#36825;&#20123;&#32622;&#20449;&#21306;&#38388;&#26410;&#33021;&#35206;&#30422;&#30495;&#23454;&#20540;&#12290;&#25105;&#20204;&#25552;&#20986;&#29992;&#35268;&#33539;&#21270;&#39044;&#27979;&#20316;&#20026;&#26367;&#20195;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#26080;&#20998;&#24067;&#26041;&#27861;&#65292;&#29992;&#20110;&#33719;&#24471;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#35206;&#30422;&#29575;&#30340;&#32622;&#20449;&#21306;&#38388;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20998;&#35010;&#35268;&#33539;&#21270;&#39044;&#27979;&#21487;&#20197;&#8220;&#20462;&#27491;&#8221;&#20043;&#21069;&#26041;&#27861;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#20197;&#20135;&#29983;&#25152;&#38656;&#30340;&#35206;&#30422;&#29575;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#31361;&#20986;&#26174;&#31034;&#20102;&#20272;&#35745;&#32622;&#20449;&#21306;&#38388;&#30340;&#20559;&#24046;&#65292;&#26080;&#35770;&#26159;&#22312;&#32763;&#35793;&#35821;&#35328;&#23545;&#26041;&#38754;&#36824;&#26159;&#22312;&#32763;&#35793;&#36136;&#37327;&#26041;&#38754;&#12290;&#25105;&#20204;&#24212;&#29992;&#26465;&#20214;&#35268;&#33539;&#21270;&#39044;&#27979;&#25216;&#26415;&#65292;&#20026;&#27599;&#20010;&#25968;&#25454;&#23376;&#32452;&#33719;&#24471;&#26657;&#20934;&#23376;&#38598;&#65292;&#20174;&#32780;&#23454;&#29616;&#24179;&#31561;&#35206;&#30422;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several uncertainty estimation methods have been recently proposed for machine translation evaluation. While these methods can provide a useful indication of when not to trust model predictions, we show in this paper that the majority of them tend to underestimate model uncertainty, and as a result they often produce misleading confidence intervals that do not cover the ground truth. We propose as an alternative the use of conformal prediction, a distribution-free method to obtain confidence intervals with a theoretically established guarantee on coverage. First, we demonstrate that split conformal prediction can ``correct'' the confidence intervals of previous methods to yield a desired coverage level. Then, we highlight biases in estimated confidence intervals, both in terms of the translation language pairs and the quality of translations. We apply conditional conformal prediction techniques to obtain calibration subsets for each data subgroup, leading to equalized coverage.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24191;&#27867;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22312;42&#31181;&#35821;&#35328;&#20013;&#25506;&#27979;&#35821;&#35328;&#27169;&#22411;&#30340;&#24418;&#24577;&#21477;&#27861;&#20449;&#24687;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#65288;mBERT&#21644;XLM-RoBERTa&#65289;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#21069;&#38754;&#30340;&#19978;&#19979;&#25991;&#27604;&#21518;&#38754;&#30340;&#19978;&#19979;&#25991;&#21253;&#21547;&#26356;&#22810;&#19982;&#39044;&#27979;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2306.06205</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;BERT&#27169;&#22411;&#30340;&#24418;&#24577;&#21477;&#27861;&#25506;&#27979;
&lt;/p&gt;
&lt;p&gt;
Morphosyntactic probing of multilingual BERT models. (arXiv:2306.06205v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06205
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24191;&#27867;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22312;42&#31181;&#35821;&#35328;&#20013;&#25506;&#27979;&#35821;&#35328;&#27169;&#22411;&#30340;&#24418;&#24577;&#21477;&#27861;&#20449;&#24687;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#65288;mBERT&#21644;XLM-RoBERTa&#65289;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#21069;&#38754;&#30340;&#19978;&#19979;&#25991;&#27604;&#21518;&#38754;&#30340;&#19978;&#19979;&#25991;&#21253;&#21547;&#26356;&#22810;&#19982;&#39044;&#27979;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#24191;&#27867;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#23545;&#35821;&#35328;&#27169;&#22411;&#20013;&#24418;&#24577;&#20449;&#24687;&#36827;&#34892;&#22810;&#35821;&#35328;&#25506;&#27979;&#65288;&#26469;&#33258;10&#20010;&#26063;&#32676;&#30340;42&#31181;&#35821;&#35328;&#20013;&#30340;247&#39033;&#20219;&#21153;&#65289;&#65292;&#27599;&#20010;&#20219;&#21153;&#21253;&#25324;&#19968;&#20010;&#24102;&#26377;&#30446;&#26631;&#21333;&#35789;&#21644;&#24418;&#24577;&#26631;&#31614;&#30340;&#21477;&#23376;&#20316;&#20026;&#26399;&#26395;&#30340;&#26631;&#31614;&#65292;&#26469;&#33258;Universal Dependencies&#26641;&#24211;&#12290;&#25105;&#20204;&#21457;&#29616;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#65288;mBERT&#21644;XLM-RoBERTa&#65289;&#23398;&#20064;&#30340;&#29305;&#24449;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#20855;&#26377;&#24378;&#22823;&#30340;&#34920;&#29616;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#23450;&#20301;&#27599;&#20010;&#25506;&#27979;&#20219;&#21153;&#20013;&#30340;&#20915;&#31574;&#20449;&#24687;&#25152;&#22312;&#30340;&#20301;&#32622;&#12290;&#31532;&#19968;&#31181;&#26159;&#19968;&#31181;&#26032;&#30340;&#25200;&#21160;&#26041;&#27861;&#65292;&#21487;&#20197;&#36974;&#34109;&#19978;&#19979;&#25991;&#30340;&#21508;&#20010;&#37096;&#20998;&#65307;&#31532;&#20108;&#31181;&#26159;Shapley&#20540;&#30340;&#32463;&#20856;&#26041;&#27861;&#12290;&#26368;&#24341;&#20154;&#27880;&#30446;&#30340;&#21457;&#29616;&#26159;&#65292;&#21069;&#38754;&#30340;&#19978;&#19979;&#25991;&#27604;&#21518;&#38754;&#30340;&#19978;&#19979;&#25991;&#21253;&#21547;&#26356;&#22810;&#19982;&#39044;&#27979;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce an extensive dataset for multilingual probing of morphological information in language models (247 tasks across 42 languages from 10 families), each consisting of a sentence with a target word and a morphological tag as the desired label, derived from the Universal Dependencies treebanks. We find that pre-trained Transformer models (mBERT and XLM-RoBERTa) learn features that attain strong performance across these tasks. We then apply two methods to locate, for each probing task, where the disambiguating information resides in the input. The first is a new perturbation method that masks various parts of context; the second is the classical method of Shapley values. The most intriguing finding that emerges is a strong tendency for the preceding context to hold more information relevant to the prediction than the following context.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-3&#23545;&#25935;&#24863;&#35805;&#39064;&#21644;&#25552;&#31034;&#25514;&#36766;&#30340;&#21453;&#24212;&#65292;&#21457;&#29616;&#20854;&#22312;&#38452;&#35851;&#35770;&#21644;&#21051;&#26495;&#21360;&#35937;&#26041;&#38754;&#26377;&#27491;&#30830;&#30340;&#21453;&#24212;&#65292;&#20294;&#22312;&#35823;&#35299;&#21644;&#20105;&#35758;&#26041;&#38754;&#23384;&#22312;&#38169;&#35823;&#65292;&#24182;&#20855;&#26377;&#19981;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.06199</link><description>&lt;p&gt;
&#21487;&#38752;&#24615;&#26816;&#26597;&#65306;&#23545;GPT-3&#22312;&#25935;&#24863;&#35805;&#39064;&#21644;&#25552;&#31034;&#25514;&#36766;&#26041;&#38754;&#30340;&#21453;&#24212;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Reliability Check: An Analysis of GPT-3's Response to Sensitive Topics and Prompt Wording. (arXiv:2306.06199v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06199
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-3&#23545;&#25935;&#24863;&#35805;&#39064;&#21644;&#25552;&#31034;&#25514;&#36766;&#30340;&#21453;&#24212;&#65292;&#21457;&#29616;&#20854;&#22312;&#38452;&#35851;&#35770;&#21644;&#21051;&#26495;&#21360;&#35937;&#26041;&#38754;&#26377;&#27491;&#30830;&#30340;&#21453;&#24212;&#65292;&#20294;&#22312;&#35823;&#35299;&#21644;&#20105;&#35758;&#26041;&#38754;&#23384;&#22312;&#38169;&#35823;&#65292;&#24182;&#20855;&#26377;&#19981;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#25104;&#20026;&#20027;&#27969;&#25216;&#26415;&#65292;&#20855;&#26377;&#22810;&#31181;&#29992;&#36884;&#21644;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#26377;&#26080;&#25968;&#30340;&#24212;&#29992;&#65292;&#20294;LLMs&#20173;&#28982;&#19981;&#26159;&#21487;&#38752;&#30340;&#12290;&#36890;&#36807;&#24494;&#35843;&#12289;&#25552;&#31034;&#21644;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#31561;&#26041;&#27861;&#65292;&#27491;&#22312;&#36827;&#34892;&#22823;&#37327;&#24037;&#20316;&#26469;&#25552;&#39640;&#36825;&#20123;&#27169;&#22411;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#36947;&#24503;&#26631;&#20934;&#65292;&#20294;&#32570;&#20047;&#23545;&#36825;&#20123;&#27169;&#22411;&#23545;&#19981;&#21516;&#35821;&#21477;&#31867;&#21035;&#30340;&#21453;&#24212;&#25110;&#22312;&#31616;&#21333;&#25552;&#31034;&#21464;&#21270;&#19979;&#21487;&#33021;&#23384;&#22312;&#30340;&#28431;&#27934;&#30340;&#31995;&#32479;&#20998;&#26512;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20160;&#20040;&#20250;&#35753;GPT-3&#22256;&#24785;&#65306;&#27169;&#22411;&#22914;&#20309;&#21709;&#24212;&#26576;&#20123;&#25935;&#24863;&#35805;&#39064;&#20197;&#21450;&#25552;&#31034;&#25514;&#36766;&#23545;&#27169;&#22411;&#21709;&#24212;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;GPT-3&#27491;&#30830;&#22320;&#21453;&#23545;&#26126;&#26174;&#30340;&#38452;&#35851;&#35770;&#21644;&#21051;&#26495;&#21360;&#35937;&#65292;&#20294;&#22312;&#26222;&#36941;&#30340;&#35823;&#35299;&#21644;&#20105;&#35758;&#20013;&#29359;&#20102;&#38169;&#35823;&#12290;&#27169;&#22411;&#21709;&#24212;&#22312;&#25552;&#31034;&#21644;&#35774;&#32622;&#19978;&#19981;&#19968;&#33268;&#65292;&#31361;&#26174;&#20986;GPT-3&#30340;&#19981;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have become mainstream technology with their versatile use cases and impressive performance. Despite the countless out-of-the-box applications, LLMs are still not reliable. A lot of work is being done to improve the factual accuracy, consistency, and ethical standards of these models through fine-tuning, prompting, and Reinforcement Learning with Human Feedback (RLHF), but no systematic analysis of the responses of these models to different categories of statements, or on their potential vulnerabilities to simple prompting changes is available. In this work, we analyze what confuses GPT-3: how the model responds to certain sensitive topics and what effects the prompt wording has on the model response. We find that GPT-3 correctly disagrees with obvious Conspiracies and Stereotypes but makes mistakes with common Misconceptions and Controversies. The model responses are inconsistent across prompts and settings, highlighting GPT-3's unreliability. Dataset and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;$FPDM$&#65292;&#20351;&#29992;&#25991;&#26723;&#20803;&#25968;&#25454;&#21644;&#39046;&#22495;&#29305;&#23450;&#20998;&#31867;&#20316;&#20026;&#30417;&#30563;&#20449;&#21495;&#65292;&#23545;&#39046;&#22495;&#29305;&#23450;&#35821;&#26009;&#24211;&#36827;&#34892;transformer&#32534;&#30721;&#22120;&#30340;&#39044;&#35757;&#32451;&#12290;$FPDM$&#36890;&#36807;&#21477;&#23376;&#32423;&#21035;&#30340;&#36755;&#20837;&#39044;&#35757;&#32451;&#24320;&#25918;&#39046;&#22495;&#30340;&#32534;&#30721;&#22120;&#65292;&#22312;&#24494;&#35843;&#26102;&#20351;&#29992;&#35789;&#27719;&#32423;&#21035;&#30340;&#36755;&#20837;&#65292;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.06190</link><description>&lt;p&gt;
&#20351;&#29992;&#25991;&#26723;&#32423;&#20803;&#25968;&#25454;&#30340;&#39046;&#22495;&#29305;&#23450;&#24555;&#36895;&#39044;&#35757;&#32451;&#25216;&#26415;$FPDM$
&lt;/p&gt;
&lt;p&gt;
$FPDM$: Domain-Specific Fast Pre-training Technique using Document-Level Metadata. (arXiv:2306.06190v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;$FPDM$&#65292;&#20351;&#29992;&#25991;&#26723;&#20803;&#25968;&#25454;&#21644;&#39046;&#22495;&#29305;&#23450;&#20998;&#31867;&#20316;&#20026;&#30417;&#30563;&#20449;&#21495;&#65292;&#23545;&#39046;&#22495;&#29305;&#23450;&#35821;&#26009;&#24211;&#36827;&#34892;transformer&#32534;&#30721;&#22120;&#30340;&#39044;&#35757;&#32451;&#12290;$FPDM$&#36890;&#36807;&#21477;&#23376;&#32423;&#21035;&#30340;&#36755;&#20837;&#39044;&#35757;&#32451;&#24320;&#25918;&#39046;&#22495;&#30340;&#32534;&#30721;&#22120;&#65292;&#22312;&#24494;&#35843;&#26102;&#20351;&#29992;&#35789;&#27719;&#32423;&#21035;&#30340;&#36755;&#20837;&#65292;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#24050;&#26174;&#31034;&#20986;&#22312;&#24320;&#25918;&#39046;&#22495;&#21644;&#39046;&#22495;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#26368;&#20808;&#36827;&#30340;transformers&#38656;&#35201;&#22823;&#37327;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$FPDM$&#65288;Fast Pre-training Technique using Document Level Metadata&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#12289;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#25991;&#26723;&#20803;&#25968;&#25454;&#21644;&#39046;&#22495;&#29305;&#23450;&#30340;&#20998;&#31867;&#20316;&#20026;&#30417;&#30563;&#20449;&#21495;&#65292;&#23545;&#39046;&#22495;&#29305;&#23450;&#35821;&#26009;&#24211;&#36827;&#34892;transformer&#32534;&#30721;&#22120;&#30340;&#39044;&#35757;&#32451;&#12290;&#26368;&#20027;&#35201;&#30340;&#21019;&#26032;&#22312;&#20110;&#65292;&#22312;&#39046;&#22495;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#20351;&#29992;&#21477;&#23376;&#32423;&#21035;&#30340;&#23884;&#20837;&#20316;&#20026;&#36755;&#20837;&#65292;&#25345;&#32493;&#23545;&#24320;&#25918;&#39046;&#22495;&#30340;&#32534;&#30721;&#22120;&#36827;&#34892;&#39044;&#35757;&#32451;&#65288;&#20197;&#36866;&#24212;&#38271;&#25991;&#26723;&#65289;&#65292;&#20294;&#22312;&#23545;&#35813;&#32534;&#30721;&#22120;&#36827;&#34892;&#24494;&#35843;&#26102;&#65292;&#21017;&#20351;&#29992;&#35789;&#27719;&#32423;&#21035;&#23884;&#20837;&#20316;&#20026;&#36755;&#20837;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;$FPDM$&#22312;&#23458;&#25143;&#25903;&#25345;&#12289;&#31185;&#23398;&#21644;&#27861;&#24459;&#31561;&#39046;&#22495;&#30340;&#23383;&#31526;&#32423;F1&#20998;&#25968;&#21644;&#20854;&#20182;&#33258;&#21160;&#21270;&#25351;&#26631;&#26041;&#38754;&#20248;&#20110;&#20960;&#31181;&#22522;&#20110;transformer&#30340;&#22522;&#20934;&#65292;&#19988;&#22312;&#19979;&#28216;&#20219;&#21153;&#24494;&#35843;&#21518;&#24615;&#33021;&#19979;&#38477;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-training Transformers has shown promising results on open-domain and domain-specific downstream tasks. However, state-of-the-art Transformers require an unreasonably large amount of pre-training data and compute. In this paper, we propose $FPDM$ (Fast Pre-training Technique using Document Level Metadata), a novel, compute-efficient framework that utilizes Document metadata and Domain-Specific Taxonomy as supervision signals to pre-train transformer encoder on a domain-specific corpus. The main innovation is that during domain-specific pretraining, an open-domain encoder is continually pre-trained using sentence-level embeddings as inputs (to accommodate long documents), however, fine-tuning is done with token-level embeddings as inputs to this encoder. We show that $FPDM$ outperforms several transformer-based baselines in terms of character-level F1 scores and other automated metrics in the Customer Support, Scientific, and Legal Domains, and shows a negligible drop in performance 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;SentiGOLD&#65292;&#19968;&#20010;&#23391;&#21152;&#25289;&#35821;&#22810;&#39046;&#22495;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#30001;70,000&#20010;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#26679;&#26412;&#32452;&#25104;&#65292;&#36981;&#23432;&#20102;&#25919;&#24220;&#21644;&#35821;&#35328;&#23398;&#22996;&#21592;&#20250;&#21830;&#23450;&#30340;&#35821;&#35328;&#32422;&#23450;&#65292;&#21253;&#25324;&#20102;30&#20010;&#39046;&#22495;&#21644;5&#20010;&#24773;&#24863;&#31867;&#21035;&#12290;&#22312;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#27880;&#37322;&#26041;&#26696;&#19979;&#65292;&#35813;&#25968;&#25454;&#38598;&#30340;&#20114;&#35780;&#19968;&#33268;&#24615;&#34920;&#29616;&#20986;&#33394;&#65292;&#21487;&#29992;&#20110;&#24314;&#31435;&#23391;&#21152;&#25289;&#35821;&#24773;&#24863;&#20998;&#26512;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.06147</link><description>&lt;p&gt;
SentiGOLD&#65306;&#19968;&#20010;&#22823;&#22411;&#23391;&#21152;&#25289;&#35821;&#22810;&#39046;&#22495;&#24773;&#24863;&#20998;&#26512;&#40644;&#37329;&#26631;&#20934;&#25968;&#25454;&#38598;&#21450;&#20854;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
SentiGOLD: A Large Bangla Gold Standard Multi-Domain Sentiment Analysis Dataset and its Evaluation. (arXiv:2306.06147v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SentiGOLD&#65292;&#19968;&#20010;&#23391;&#21152;&#25289;&#35821;&#22810;&#39046;&#22495;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#30001;70,000&#20010;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#26679;&#26412;&#32452;&#25104;&#65292;&#36981;&#23432;&#20102;&#25919;&#24220;&#21644;&#35821;&#35328;&#23398;&#22996;&#21592;&#20250;&#21830;&#23450;&#30340;&#35821;&#35328;&#32422;&#23450;&#65292;&#21253;&#25324;&#20102;30&#20010;&#39046;&#22495;&#21644;5&#20010;&#24773;&#24863;&#31867;&#21035;&#12290;&#22312;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#27880;&#37322;&#26041;&#26696;&#19979;&#65292;&#35813;&#25968;&#25454;&#38598;&#30340;&#20114;&#35780;&#19968;&#33268;&#24615;&#34920;&#29616;&#20986;&#33394;&#65292;&#21487;&#29992;&#20110;&#24314;&#31435;&#23391;&#21152;&#25289;&#35821;&#24773;&#24863;&#20998;&#26512;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;SentiGOLD&#65292;&#19968;&#20010;&#23391;&#21152;&#25289;&#35821;&#22810;&#39046;&#22495;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;&#12290;&#23427;&#30001;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;70,000&#20010;&#26679;&#26412;&#32452;&#25104;&#65292;&#24182;&#30001;&#19968;&#32452;&#24615;&#21035;&#24179;&#34913;&#30340;&#35821;&#35328;&#23398;&#23478;&#36827;&#34892;&#27880;&#37322;&#12290;SentiGOLD&#36981;&#23432;&#23391;&#21152;&#25289;&#22269;&#25919;&#24220;&#21644;&#23391;&#21152;&#25289;&#35821;&#35328;&#23398;&#22996;&#21592;&#20250;&#21830;&#23450;&#30340;&#26082;&#23450;&#35821;&#35328;&#32422;&#23450;&#12290;&#19982;&#33521;&#35821;&#21644;&#20854;&#20182;&#35821;&#35328;&#19981;&#21516;&#65292;&#30001;&#20110;&#32570;&#20047;&#22269;&#23478;&#35821;&#35328;&#23398;&#26694;&#26550;&#65292;&#23391;&#21152;&#25289;&#35821;&#32570;&#20047;&#26631;&#20934;&#30340;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#22312;&#32447;&#35270;&#39057;&#35780;&#35770;&#12289;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#12289;&#21338;&#23458;&#12289;&#26032;&#38395;&#21644;&#20854;&#20182;&#26469;&#28304;&#30340;&#25968;&#25454;&#65292;&#24182;&#20005;&#26684;&#32500;&#25252;&#39046;&#22495;&#21644;&#31867;&#21035;&#20998;&#24067;&#12290;&#23427;&#28085;&#30422;&#20102;30&#20010;&#39046;&#22495;&#65288;&#22914;&#25919;&#27835;&#12289;&#23089;&#20048;&#12289;&#20307;&#32946;&#65289;&#65292;&#21253;&#25324;5&#20010;&#24773;&#24863;&#31867;&#21035;&#65288;&#24378;&#28872;&#30340;&#36127;&#38754;&#12289;&#24369;&#30340;&#36127;&#38754;&#12289;&#20013;&#24615;&#21644;&#24378;&#28872;&#30340;&#27491;&#38754;&#65289;&#12290;&#30001;&#22269;&#23478;&#35821;&#35328;&#23398;&#22996;&#21592;&#20250;&#25209;&#20934;&#30340;&#27880;&#37322;&#26041;&#26696;&#30830;&#20445;&#20855;&#26377;&#40065;&#26834;&#30340;&#20114;&#35780;&#19968;&#33268;&#24615;&#65288;IAA&#65289;&#65292;&#33778;&#21033;&#26031;kappa&#24471;&#20998;&#20026;0.88&#12290;&#20869;&#37096;&#21644;&#36328;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
This study introduces SentiGOLD, a Bangla multi-domain sentiment analysis dataset. Comprising 70,000 samples, it was created from diverse sources and annotated by a gender-balanced team of linguists. SentiGOLD adheres to established linguistic conventions agreed upon by the Government of Bangladesh and a Bangla linguistics committee. Unlike English and other languages, Bangla lacks standard sentiment analysis datasets due to the absence of a national linguistics framework. The dataset incorporates data from online video comments, social media posts, blogs, news, and other sources while maintaining domain and class distribution rigorously. It spans 30 domains (e.g., politics, entertainment, sports) and includes 5 sentiment classes (strongly negative, weakly negative, neutral, and strongly positive). The annotation scheme, approved by the national linguistics committee, ensures a robust Inter Annotator Agreement (IAA) with a Fleiss' kappa score of 0.88. Intra- and cross-dataset evaluatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#23545;&#35805;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#25429;&#25417;&#35302;&#21457;&#22120;&#24182;&#23558;&#20854;&#19982;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#20851;&#31995;&#21517;&#31216;&#30456;&#20851;&#32852;&#65292;&#33021;&#22815;&#23545;&#25512;&#26029;&#20808;&#21069;&#26410;&#35265;&#36807;&#30340;&#20851;&#31995;&#31867;&#22411;&#26497;&#20855;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2306.06141</link><description>&lt;p&gt;
&#22522;&#20110;&#21487;&#35299;&#37322;&#35302;&#21457;&#22120;&#21644;&#20851;&#31995;&#21517;&#31216;&#30456;&#20851;&#32852;&#30340;&#26080;&#30417;&#30563;&#23545;&#35805;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Dialogue Relation Extraction by Relating Explainable Triggers and Relation Names. (arXiv:2306.06141v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#23545;&#35805;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#25429;&#25417;&#35302;&#21457;&#22120;&#24182;&#23558;&#20854;&#19982;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#20851;&#31995;&#21517;&#31216;&#30456;&#20851;&#32852;&#65292;&#33021;&#22815;&#23545;&#25512;&#26029;&#20808;&#21069;&#26410;&#35265;&#36807;&#30340;&#20851;&#31995;&#31867;&#22411;&#26497;&#20855;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#23545;&#35805;&#20851;&#31995;&#25277;&#21462;&#65288;DRE&#65289;&#31995;&#32479;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#30340;&#25968;&#25454;&#65292;&#32780;&#36825;&#20123;&#25968;&#25454;&#30340;&#26631;&#27880;&#21487;&#33021;&#20250;&#24456;&#26114;&#36149;&#19988;&#32791;&#26102;&#12290;&#20026;&#20102;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#24182;&#25903;&#25345;&#22810;&#26679;&#21270;&#30340;&#12289;&#26410;&#35265;&#36807;&#30340;&#20851;&#31995;&#25277;&#21462;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25429;&#25417;&#35302;&#21457;&#22120;&#24182;&#23558;&#23427;&#20204;&#19982;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#20851;&#31995;&#21517;&#31216;&#30456;&#20851;&#32852;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#25429;&#25417;&#35302;&#21457;&#22120;&#30340;&#33021;&#21147;&#23454;&#29616;&#26080;&#30417;&#30563;&#23545;&#35805;&#20851;&#31995;&#25277;&#21462;&#12290;&#25105;&#20204;&#22312;DialogRE&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#24050;&#30693;&#21644;&#26410;&#30693;&#30340;&#20851;&#31995;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#26159;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;&#25429;&#25417;&#35302;&#21457;&#22120;&#23454;&#29616;&#26080;&#30417;&#30563;&#23545;&#35805;&#20851;&#31995;&#25277;&#21462;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#25512;&#26029;&#20808;&#21069;&#26410;&#35265;&#36807;&#30340;&#20851;&#31995;&#31867;&#22411;&#38750;&#24120;&#26377;&#25928;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#31361;&#26174;&#20102;&#36825;&#31181;&#26041;&#27861;&#25552;&#39640;DRE&#31995;&#32479;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#23454;&#29992;&#24615;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing dialogue relation extraction (DRE) systems often requires a large amount of labeled data, which can be costly and time-consuming to annotate. In order to improve scalability and support diverse, unseen relation extraction, this paper proposes a method for leveraging the ability to capture triggers and relate them to previously unseen relation names. Specifically, we introduce a model that enables zero-shot dialogue relation extraction by utilizing trigger-capturing capabilities. Our experiments on a benchmark DialogRE dataset demonstrate that the proposed model achieves significant improvements for both seen and unseen relations. Notably, this is the first attempt at zero-shot dialogue relation extraction using trigger-capturing capabilities, and our results suggest that this approach is effective for inferring previously unseen relation types. Overall, our findings highlight the potential for this method to enhance the scalability and practicality of DRE systems.
&lt;/p&gt;</description></item><item><title>INSTRUCTEVAL&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#32508;&#21512;&#22871;&#20214;&#65292;&#23427;&#37319;&#21462;&#20102;&#20840;&#38754;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#35299;&#20915;&#38382;&#39064;&#12289;&#20889;&#20316;&#33021;&#21147;&#21644;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#19968;&#33268;&#24615;&#31561;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2306.04757</link><description>&lt;p&gt;
INSTRUCTEVAL&#65306;&#38754;&#21521;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25972;&#20307;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large Language Models. (arXiv:2306.04757v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04757
&lt;/p&gt;
&lt;p&gt;
INSTRUCTEVAL&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#32508;&#21512;&#22871;&#20214;&#65292;&#23427;&#37319;&#21462;&#20102;&#20840;&#38754;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#35299;&#20915;&#38382;&#39064;&#12289;&#20889;&#20316;&#33021;&#21147;&#21644;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#19968;&#33268;&#24615;&#31561;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#20174;&#26681;&#26412;&#19978;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#24050;&#32463;&#22312;&#35832;&#22914;&#23545;&#35805;&#20195;&#29702;&#31561;&#24212;&#29992;&#20013;&#26174;&#31034;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#36825;&#20123;&#27169;&#22411;&#65292;&#22914;GPT-4&#65292;&#19981;&#20165;&#33021;&#22815;&#25484;&#25569;&#35821;&#35328;&#65292;&#32780;&#19988;&#21487;&#20197;&#35299;&#20915;&#25968;&#23398;&#12289;&#32534;&#30721;&#12289;&#21307;&#23398;&#21644;&#27861;&#24459;&#31561;&#39046;&#22495;&#30340;&#22797;&#26434;&#20219;&#21153;&#12290;&#23613;&#31649;&#23427;&#20204;&#20855;&#26377;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#20294;&#30001;&#20110;&#35768;&#22810;&#27169;&#22411;&#30340;&#40657;&#30418;&#24615;&#36136;&#21644;&#32570;&#20047;&#20840;&#38754;&#30340;&#35780;&#20272;&#30740;&#31350;&#65292;&#23545;&#23427;&#20204;&#30340;&#20840;&#37096;&#28508;&#21147;&#20173;&#28982;&#32570;&#20047;&#20840;&#38754;&#30340;&#29702;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;INSTRUCTEVAL&#65292;&#19968;&#20010;&#26356;&#20840;&#38754;&#30340;&#35780;&#20272;&#22871;&#20214;&#65292;&#19987;&#38376;&#38024;&#23545;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#19982;&#20197;&#24448;&#30340;&#20316;&#21697;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#35780;&#20272;&#21253;&#25324;&#23545;&#27169;&#22411;&#22522;&#20110;&#35299;&#20915;&#38382;&#39064;&#12289;&#20889;&#20316;&#33021;&#21147;&#21644;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#19968;&#33268;&#24615;&#30340;&#20005;&#26684;&#35780;&#20272;&#12290;&#25105;&#20204;&#37319;&#21462;&#20102;&#20840;&#38754;&#30340;&#26041;&#27861;&#26469;&#20998;&#26512;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#30340;&#21508;&#31181;&#22240;&#32032;&#65292;&#21253;&#25324;&#39044;&#35757;&#32451;&#22522;&#30784;&#12289;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#21644;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned large language models have revolutionized natural language processing and have shown great potential in applications such as conversational agents. These models, such as GPT-4, can not only master language but also solve complex tasks in areas like mathematics, coding, medicine, and law. Despite their impressive capabilities, there is still a lack of comprehensive understanding regarding their full potential, primarily due to the black-box nature of many models and the absence of holistic evaluation studies. To address these challenges, we present INSTRUCTEVAL, a more comprehensive evaluation suite designed specifically for instruction-tuned large language models. Unlike previous works, our evaluation involves a rigorous assessment of models based on problem-solving, writing ability, and alignment to human values. We take a holistic approach to analyze various factors affecting model performance, including the pretraining foundation, instruction-tuning data, and train
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#30340;&#21518;&#35757;&#32451;&#25216;&#26415;Dial-MAE&#65292;&#21033;&#29992;&#29983;&#25104;&#26041;&#27861;&#26356;&#22909;&#22320;&#21387;&#32553;&#23545;&#35805;&#35821;&#20041;&#33267;&#23494;&#38598;&#21521;&#37327;&#65292;&#24182;&#25552;&#39640;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04357</link><description>&lt;p&gt;
&#29992;&#20110;&#22522;&#20110;&#26816;&#32034;&#30340;&#23545;&#35805;&#31995;&#32479;&#30340;&#19978;&#19979;&#25991;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
ConTextual Masked Auto-Encoder for Retrieval-based Dialogue Systems. (arXiv:2306.04357v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#30340;&#21518;&#35757;&#32451;&#25216;&#26415;Dial-MAE&#65292;&#21033;&#29992;&#29983;&#25104;&#26041;&#27861;&#26356;&#22909;&#22320;&#21387;&#32553;&#23545;&#35805;&#35821;&#20041;&#33267;&#23494;&#38598;&#21521;&#37327;&#65292;&#24182;&#25552;&#39640;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#26088;&#22312;&#26681;&#25454;&#32473;&#23450;&#30340;&#29992;&#25143;&#21644;&#31995;&#32479;&#35805;&#35821;&#21382;&#21490;&#35760;&#24405;&#20174;&#20960;&#20010;&#20505;&#36873;&#21709;&#24212;&#20013;&#36873;&#25321;&#36866;&#24403;&#30340;&#21709;&#24212;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#21518;&#35757;&#32451;&#22823;&#22810;&#20381;&#36182;&#20110;&#21333;&#32431;&#30340;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#26469;&#25552;&#39640;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#30340;&#20934;&#30830;&#24615;&#12290;&#20294;&#26159;&#65292;&#26368;&#36817;&#24320;&#21457;&#30340;&#29983;&#25104;&#26041;&#27861;&#22312;IR&#31038;&#21306;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#25991;&#26412;&#34920;&#31034;&#33021;&#21147;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#26356;&#22909;&#30340;&#23545;&#35805;&#35821;&#20041;&#24314;&#27169;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986; Dial-MAE&#65288;&#23545;&#35805;&#19978;&#19979;&#25991;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#38024;&#23545;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#30340;&#21518;&#35757;&#32451;&#25216;&#26415;&#12290; Dial-MAE&#20351;&#29992;&#19968;&#20010;&#19981;&#23545;&#31216;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#23398;&#20064;&#23558;&#23545;&#35805;&#30340;&#35821;&#20041;&#26356;&#22909;&#22320;&#21387;&#32553;&#21040;&#23494;&#38598;&#21521;&#37327;&#20013;&#12290; Dial-MAE&#30340;&#36807;&#31243;&#21253;&#25324;&#30001;&#28145;&#24230;&#32534;&#30721;&#22120;&#21019;&#24314;&#24102;&#26377;&#25513;&#30721;&#23545;&#35805;&#19978;&#19979;&#25991;&#30340;&#23545;&#35805;&#23884;&#20837;&#65292;&#28982;&#21518;&#26159;&#27973;&#35299;&#30721;&#22120;&#65292;&#35813;&#35299;&#30721;&#22120;&#20351;&#29992;&#27492;&#23884;&#20837;&#20197;&#21450;&#19978;&#19979;&#25991;&#21521;&#37327;&#26469;&#29983;&#25104;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue response selection aims to select an appropriate response from several candidates based on a given user and system utterance history. Recent studies have been improving the accuracy of dialogue response selection through post-training, mostly relying on naive masked language modeling methods. However, the recently developed generative methods have shown promising text representation capabilities in IR community, which could potentially lead to better dialogue semantics modeling. Thus, in this paper, we propose Dial-MAE (Dialogue Contextual Masking Auto-encoder), a straightforward yet effective post-training technique tailored for dialogue response selection. Dial-MAE uses an asymmetric encoder-decoder architecture that learns to better compress the semantics of the dialogue into dialogue-dense vectors. The process of Dial-MAE involves a deep encoder creating a dialogue embedding with the masked dialogue context, followed by a shallow decoder that uses this embedding along with
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#35805;&#35805;&#35821;&#29305;&#24449;&#22686;&#24378;&#23545;&#35805;&#20998;&#31163;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#24322;&#26500;&#22270;&#34920;&#31034;&#21644;&#24341;&#20837;&#22686;&#24378;&#20998;&#31163;&#30446;&#26631;&#65292;&#20197;&#26356;&#22909;&#22320;&#24314;&#27169;&#23545;&#35805;&#35821;&#22659;&#21644;&#21033;&#29992;&#20869;&#22312;&#30340;&#35805;&#35821;&#32467;&#26500;&#20449;&#24687;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#23545;&#35805;&#20998;&#31163;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.03975</link><description>&lt;p&gt;
&#37325;&#22609;&#35805;&#35821;&#35821;&#31687;&#29702;&#35299;&#20197;&#20419;&#36827;&#23545;&#35805;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Revisiting Conversation Discourse for Dialogue Disentanglement. (arXiv:2306.03975v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#35805;&#35805;&#35821;&#29305;&#24449;&#22686;&#24378;&#23545;&#35805;&#20998;&#31163;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#24322;&#26500;&#22270;&#34920;&#31034;&#21644;&#24341;&#20837;&#22686;&#24378;&#20998;&#31163;&#30446;&#26631;&#65292;&#20197;&#26356;&#22909;&#22320;&#24314;&#27169;&#23545;&#35805;&#35821;&#22659;&#21644;&#21033;&#29992;&#20869;&#22312;&#30340;&#35805;&#35821;&#32467;&#26500;&#20449;&#24687;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#23545;&#35805;&#20998;&#31163;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#20998;&#31163;&#26088;&#22312;&#23558;&#26102;&#38388;&#39034;&#24207;&#25490;&#21015;&#30340;&#35805;&#35821;&#20998;&#38548;&#25104;&#20960;&#20010;&#29420;&#31435;&#30340;&#20250;&#35805;&#12290;&#23545;&#35805;&#35805;&#35821;&#26412;&#36136;&#19978;&#26159;&#30001;&#24213;&#23618;&#35821;&#31687;&#32452;&#32455;&#21644;&#25551;&#36848;&#30340;&#65292;&#22240;&#27492;&#23545;&#35805;&#20998;&#31163;&#38656;&#35201;&#23436;&#20840;&#29702;&#35299;&#21644;&#21033;&#29992;&#20869;&#22312;&#30340;&#35805;&#35821;&#23646;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#23545;&#35805;&#35805;&#35821;&#29305;&#24449;&#20840;&#38754;&#22686;&#24378;&#23545;&#35805;&#20998;&#31163;&#12290;&#22312;&#29305;&#24449;&#32534;&#30721;&#38454;&#27573;&#65292;&#25105;&#20204;&#26500;&#24314;&#24322;&#26500;&#22270;&#34920;&#31034;&#26469;&#27169;&#25311;&#21508;&#31181;&#23545;&#35805;&#29305;&#23450;&#30340;&#35805;&#35821;&#32467;&#26500;&#29305;&#24449;&#65292;&#21253;&#25324;&#38745;&#24577;&#30340;&#35762;&#35805;&#32773;&#35282;&#33394;&#32467;&#26500;&#65288;&#21363;&#35762;&#35805;&#32773;&#35805;&#35821;&#21644;&#35762;&#35805;&#32773;&#25552;&#21450;&#32467;&#26500;&#65289;&#21644;&#21160;&#24577;&#30340;&#19978;&#19979;&#25991;&#32467;&#26500;&#65288;&#21363;&#35805;&#35821;&#36317;&#31163;&#21644;&#37096;&#20998;&#22238;&#22797;&#32467;&#26500;&#65289;&#12290;&#25105;&#20204;&#28982;&#21518;&#24320;&#21457;&#20102;&#19968;&#20010;&#32467;&#26500;&#24863;&#30693;&#26694;&#26550;&#65292;&#20197;&#38598;&#25104;&#20016;&#23500;&#30340;&#32467;&#26500;&#29305;&#24449;&#65292;&#26356;&#22909;&#22320;&#24314;&#27169;&#23545;&#35805;&#35821;&#22659;&#12290;&#20854;&#27425;&#65292;&#22312;&#27169;&#22411;&#32763;&#35793;&#38454;&#27573;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378;&#20998;&#31163;&#30446;&#26631;&#65292;&#20197;&#21033;&#29992;&#20869;&#22312;&#30340;&#35805;&#35821;&#32467;&#26500;&#20449;&#24687;&#26469;&#36827;&#34892;&#20998;&#31163;&#36807;&#31243;&#12290;&#22312;&#19981;&#21516;&#30340;&#23545;&#35805;&#20998;&#31163;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue disentanglement aims to detach the chronologically ordered utterances into several independent sessions. Conversation utterances are essentially organized and described by the underlying discourse, and thus dialogue disentanglement requires the full understanding and harnessing of the intrinsic discourse attribute. In this paper, we propose enhancing dialogue disentanglement by taking full advantage of the dialogue discourse characteristics. First of all, \textbf{in feature encoding stage}, we construct the heterogeneous graph representations to model the various dialogue-specific discourse structural features, including the static speaker-role structures (i.e., speaker-utterance and speaker-mentioning structure) and the dynamic contextual structures (i.e., the utterance-distance and partial-replying structure). We then develop a structure-aware framework to integrate the rich structural features for better modeling the conversational semantic context. Second, \textbf{in model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; TKDP &#30340;&#26041;&#27861;&#65292;&#22312;&#28145;&#24230;&#25552;&#31034;&#35843;&#25972;&#20013;&#36890;&#36807;&#25972;&#21512;&#19977;&#31181;&#19981;&#21516;&#26469;&#28304;&#30340;&#30693;&#35782;&#26469;&#22686;&#24378;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#24615;&#33021;&#12290; &#22312;&#20116;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#30456;&#23545;&#20110;&#21407;&#22987;&#30340;&#28145;&#24230;&#25552;&#31034;&#26041;&#27861;&#25552;&#39640;&#20102;&#26368;&#22810; 11.53% &#30340; F1&#65292;&#24182;&#19988;&#26126;&#26174;&#20248;&#20110; 8 &#31181;&#34920;&#29616;&#24378;&#21170;&#30340; few-shot NER &#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.03974</link><description>&lt;p&gt;
TKDP: &#19977;&#37325;&#30693;&#35782;&#22686;&#24378;&#30340;&#28145;&#24230;&#25552;&#31034;&#35843;&#25972;&#22312;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
TKDP: Threefold Knowledge-enriched Deep Prompt Tuning for Few-shot Named Entity Recognition. (arXiv:2306.03974v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03974
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; TKDP &#30340;&#26041;&#27861;&#65292;&#22312;&#28145;&#24230;&#25552;&#31034;&#35843;&#25972;&#20013;&#36890;&#36807;&#25972;&#21512;&#19977;&#31181;&#19981;&#21516;&#26469;&#28304;&#30340;&#30693;&#35782;&#26469;&#22686;&#24378;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#24615;&#33021;&#12290; &#22312;&#20116;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#30456;&#23545;&#20110;&#21407;&#22987;&#30340;&#28145;&#24230;&#25552;&#31034;&#26041;&#27861;&#25552;&#39640;&#20102;&#26368;&#22810; 11.53% &#30340; F1&#65292;&#24182;&#19988;&#26126;&#26174;&#20248;&#20110; 8 &#31181;&#34920;&#29616;&#24378;&#21170;&#30340; few-shot NER &#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#36890;&#36807;&#26377;&#38480;&#27880;&#37322;&#31034;&#20363;&#26469;&#35782;&#21035;&#21629;&#21517;&#23454;&#20307;&#65292;&#22240;&#27492;&#26377;&#25928;&#22320;&#36716;&#31227;&#20869;&#37096;&#25110;&#22806;&#37096;&#36164;&#28304;&#25104;&#20026;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#20851;&#38190;&#12290;&#26412;&#25991;&#30740;&#31350;&#22312;&#28145;&#24230;&#25552;&#31034;&#35843;&#25972;&#20013;&#25972;&#21512;&#20016;&#23500;&#30340;&#30693;&#35782;&#20197;&#23454;&#29616;&#26356;&#24378;&#30340;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;&#25105;&#20204;&#25552;&#20986;&#23558;&#28145;&#24230;&#25552;&#31034;&#35843;&#25972;&#26694;&#26550;&#19982;&#19977;&#37325;&#30693;&#35782;&#65288;&#21363; TKDP&#65289;&#65292;&#21253;&#25324;&#20869;&#37096;&#30340; 1&#65289;&#19978;&#19979;&#25991;&#30693;&#35782;&#21644;&#22806;&#37096;&#30340; 2&#65289;&#26631;&#31614;&#30693;&#35782;&#21644; 3&#65289;&#20041;&#21407;&#30693;&#35782;&#30456;&#32467;&#21512;&#12290;TKDP &#23545;&#19977;&#20010;&#29305;&#24449;&#28304;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#23558;&#23427;&#20204;&#25972;&#21512;&#21040;&#36719;&#25552;&#31034;&#23884;&#20837;&#20013;&#65292;&#36827;&#32780;&#27880;&#20837;&#21040;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#20197;&#20419;&#36827;&#39044;&#27979;&#12290;&#22312;&#20116;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#30693;&#35782;&#22686;&#24378;&#27169;&#22411;&#30456;&#23545;&#20110;&#21407;&#22987;&#30340;&#28145;&#24230;&#25552;&#31034;&#26041;&#27861;&#25552;&#39640;&#20102;&#26368;&#22810; 11.53% &#30340; F1&#65292;&#24182;&#19988;&#26126;&#26174;&#20248;&#20110; 8 &#31181;&#34920;&#29616;&#24378;&#21170;&#30340; few-shot NER &#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot named entity recognition (NER) exploits limited annotated instances to identify named mentions. Effectively transferring the internal or external resources thus becomes the key to few-shot NER. While the existing prompt tuning methods have shown remarkable few-shot performances, they still fail to make full use of knowledge. In this work, we investigate the integration of rich knowledge to prompt tuning for stronger few-shot NER. We propose incorporating the deep prompt tuning framework with threefold knowledge (namely TKDP), including the internal 1) context knowledge and the external 2) label knowledge &amp; 3) sememe knowledge. TKDP encodes the three feature sources and incorporates them into the soft prompt embeddings, which are further injected into an existing pre-trained language model to facilitate predictions. On five benchmark datasets, our knowledge-enriched model boosts by at most 11.53% F1 over the raw deep prompt method, and significantly outperforms 8 strong-perform
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;-&#21407;&#22240;&#22235;&#20803;&#32452;&#25277;&#21462;&#20219;&#21153;(ECQED)&#65292;&#36890;&#36807;&#24341;&#20837;&#23545;&#35805;&#19978;&#19979;&#25991;&#21644;&#32454;&#31890;&#24230;&#30340;&#24773;&#24863;&#21644;&#21407;&#22240;&#26816;&#27979;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.03969</link><description>&lt;p&gt;
ECQED&#65306;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;-&#21407;&#22240;&#22235;&#20803;&#32452;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
ECQED: Emotion-Cause Quadruple Extraction in Dialogs. (arXiv:2306.03969v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03969
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;-&#21407;&#22240;&#22235;&#20803;&#32452;&#25277;&#21462;&#20219;&#21153;(ECQED)&#65292;&#36890;&#36807;&#24341;&#20837;&#23545;&#35805;&#19978;&#19979;&#25991;&#21644;&#32454;&#31890;&#24230;&#30340;&#24773;&#24863;&#21644;&#21407;&#22240;&#26816;&#27979;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#24773;&#24863;-&#21407;&#22240;&#23545;&#25277;&#21462;(ECPE)&#20219;&#21153;&#36951;&#25022;&#22320;&#24573;&#30053;&#20102;&#24773;&#24863;&#31867;&#22411;&#21644;&#21407;&#22240;&#31867;&#22411;&#30340;&#25552;&#21462;&#65292;&#32780;&#36825;&#20123;&#32454;&#31890;&#24230;&#30340;&#20803;&#20449;&#24687;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#33021;&#38750;&#24120;&#26377;&#29992;&#65292;&#20363;&#22914;&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#20849;&#24773;&#23545;&#35805;&#29983;&#25104;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;ECPE&#20165;&#38480;&#20110;&#21333;&#20010;&#25991;&#26412;&#29255;&#27573;&#30340;&#22330;&#26223;&#65292;&#32780;&#24573;&#30053;&#20102;&#24212;&#35813;&#20855;&#26377;&#26356;&#29616;&#23454;&#20215;&#20540;&#30340;&#23545;&#35805;&#32423;&#21035;&#30340;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#26356;&#24191;&#27867;&#30340;&#23450;&#20041;&#21644;&#22330;&#26223;&#25193;&#23637;&#20102;ECPE&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65306;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;-&#21407;&#22240;&#22235;&#20803;&#32452;&#25277;&#21462;(ECQED)&#65292;&#38656;&#35201;&#26816;&#27979;&#24773;&#24863;-&#21407;&#22240;&#35805;&#35821;&#23545;&#21644;&#24773;&#24863;&#21644;&#21407;&#22240;&#31867;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#32467;&#26500;&#21644;&#35821;&#20041;&#24322;&#26500;&#22270;&#20197;&#21450;&#24179;&#34892;&#26631;&#35760;&#26041;&#26696;&#30340;ECQED&#27169;&#22411;&#65292;&#36825;&#25552;&#39640;&#20102;&#26377;&#25928;&#22320;&#32467;&#21512;&#23545;&#35805;&#19978;&#19979;&#25991;&#32467;&#26500;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#20196;&#20154;&#22256;&#25200;&#30340;&#37325;&#21472;&#22235;&#20803;&#32452;&#38382;&#39064;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#24341;&#20837;&#32454;&#31890;&#24230;&#24773;&#24863;&#21644;&#21407;&#22240;&#26816;&#27979;&#24182;&#32771;&#34385;&#23545;&#35805;&#19978;&#19979;&#25991;&#23545;&#20110;&#23454;&#29616;ECQED&#20219;&#21153;&#30340;&#26356;&#22909;&#25928;&#26524;&#37117;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
The existing emotion-cause pair extraction (ECPE) task, unfortunately, ignores extracting the emotion type and cause type, while these fine-grained meta-information can be practically useful in real-world applications, i.e., chat robots and empathic dialog generation. Also the current ECPE is limited to the scenario of single text piece, while neglecting the studies at dialog level that should have more realistic values. In this paper, we extend the ECPE task with a broader definition and scenario, presenting a new task, Emotion-Cause Quadruple Extraction in Dialogs (ECQED), which requires detecting emotion-cause utterance pairs and emotion and cause types. We present an ECQED model based on a structural and semantic heterogeneous graph as well as a parallel grid tagging scheme, which advances in effectively incorporating the dialog context structure, meanwhile solving the challenging overlapped quadruple issue. Via experiments we show that introducing the fine-grained emotion and caus
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#27169;&#22411;&#22312;&#26631;&#35760;&#25968;&#25454;&#26377;&#38480;&#35821;&#35328;&#20013;&#36827;&#34892;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#26377;&#25928;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;NLI&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#33521;&#35821;&#25968;&#25454;&#21305;&#37197;&#27979;&#35797;&#39046;&#22495;&#26102;&#20165;&#26377;&#33258;&#23450;&#20041;&#30340;NLI&#37197;&#26041;&#33021;&#22815;&#32988;&#36807;&#20013;&#38388;&#33521;&#35821;&#24494;&#35843;&#12290;</title><link>http://arxiv.org/abs/2306.03722</link><description>&lt;p&gt;
&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#22312;&#26631;&#35760;&#25968;&#25454;&#26377;&#38480;&#30340;&#35821;&#35328;&#20013;&#23545;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Effectiveness of Natural Language Inference for Hate Speech Detection in Languages with Limited Labeled Data. (arXiv:2306.03722v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#27169;&#22411;&#22312;&#26631;&#35760;&#25968;&#25454;&#26377;&#38480;&#35821;&#35328;&#20013;&#36827;&#34892;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#26377;&#25928;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;NLI&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#33521;&#35821;&#25968;&#25454;&#21305;&#37197;&#27979;&#35797;&#39046;&#22495;&#26102;&#20165;&#26377;&#33258;&#23450;&#20041;&#30340;NLI&#37197;&#26041;&#33021;&#22815;&#32988;&#36807;&#20013;&#38388;&#33521;&#35821;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#23545;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#30740;&#31350;&#37117;&#20851;&#27880;&#20110;&#33521;&#35821;&#65292;&#22240;&#20026;&#26377;&#22823;&#37327;&#30340;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#21487;&#20379;&#20351;&#29992;&#12290;&#20294;&#26159;&#65292;&#20026;&#20102;&#23558;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#25193;&#23637;&#21040;&#26356;&#22810;&#30340;&#35821;&#35328;&#65292;&#38656;&#35201;&#37319;&#29992;&#38656;&#35201;&#26368;&#23569;&#35757;&#32451;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#27169;&#22411;&#22312;&#20165;&#26377;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#30446;&#26631;&#35821;&#35328;&#20013;&#26159;&#21542;&#33021;&#23545;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#24615;&#33021;&#26377;&#25152;&#25552;&#21319;&#12290;&#25105;&#20204;&#22312;&#20116;&#31181;&#35821;&#35328;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;NLI&#24494;&#35843;&#22312;&#30446;&#26631;&#35821;&#35328;&#20013;&#23545;&#20110;&#30452;&#25509;&#24494;&#35843;&#30456;&#27604;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#25552;&#20986;&#30340;&#22312;&#33521;&#35821;&#25968;&#25454;&#19978;&#36827;&#34892;&#20013;&#38388;&#24494;&#35843;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#24456;&#38590;&#21305;&#37197;&#12290;&#21482;&#26377;&#22312;English&#35757;&#32451;&#25968;&#25454;&#19982;&#27979;&#35797;&#39046;&#22495;&#19981;&#21305;&#37197;&#26102;&#65292;&#25105;&#20204;&#33258;&#23450;&#20041;&#30340;NLI&#37197;&#26041;&#25165;&#33021;&#32988;&#36807;&#20013;&#38388;&#33521;&#35821;&#24494;&#35843;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26377;&#25928;&#21033;&#29992;NLI&#27169;&#22411;&#22312;&#26631;&#35760;&#25968;&#25454;&#26377;&#38480;&#30340;&#35821;&#35328;&#20013;&#36827;&#34892;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most research on hate speech detection has focused on English where a sizeable amount of labeled training data is available. However, to expand hate speech detection into more languages, approaches that require minimal training data are needed. In this paper, we test whether natural language inference (NLI) models which perform well in zero- and few-shot settings can benefit hate speech detection performance in scenarios where only a limited amount of labeled data is available in the target language. Our evaluation on five languages demonstrates large performance improvements of NLI fine-tuning over direct fine-tuning in the target language. However, the effectiveness of previous work that proposed intermediate fine-tuning on English data is hard to match. Only in settings where the English training data does not match the test domain, can our customised NLI-formulation outperform intermediate fine-tuning on English. Based on our extensive experiments, we propose a set of recommendatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#22914;&#20309;&#20026;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#21046;&#23450;&#23433;&#20840;&#20445;&#38556;&#65292;&#20998;&#26512;LLMs&#30340;&#20869;&#23481;&#29983;&#25104;&#26426;&#21046;&#65292;&#30830;&#23450;&#20102;&#22235;&#20010;&#20851;&#38190;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#21457;&#21644;&#38144;&#21806;LLM&#29983;&#25104;&#20869;&#23481;&#30340;&#20225;&#19994;&#30340;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2306.03503</link><description>&lt;p&gt;
&#24212;&#29992;&#26631;&#20934;&#20419;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#19979;&#28216;&#20262;&#29702;
&lt;/p&gt;
&lt;p&gt;
Applying Standards to Advance Upstream &amp; Downstream Ethics in Large Language Models. (arXiv:2306.03503v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03503
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#22914;&#20309;&#20026;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#21046;&#23450;&#23433;&#20840;&#20445;&#38556;&#65292;&#20998;&#26512;LLMs&#30340;&#20869;&#23481;&#29983;&#25104;&#26426;&#21046;&#65292;&#30830;&#23450;&#20102;&#22235;&#20010;&#20851;&#38190;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#21457;&#21644;&#38144;&#21806;LLM&#29983;&#25104;&#20869;&#23481;&#30340;&#20225;&#19994;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;AI&#25152;&#26377;&#32773;&#22914;&#20309;&#20511;&#37492;&#20854;&#20182;&#20869;&#23481;&#21019;&#20316;&#34892;&#19994;&#30340;&#34892;&#20026;&#20934;&#21017;&#21644;&#20262;&#29702;&#26631;&#20934;&#65292;&#20026;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#21046;&#23450;&#23433;&#20840;&#20445;&#38556;&#12290;&#23427;&#28145;&#20837;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20262;&#29702;&#24847;&#35782;&#29616;&#29366;&#12290;&#36890;&#36807;&#20998;&#26512;LLMs&#30340;&#20869;&#23481;&#29983;&#25104;&#26426;&#21046;&#65292;&#30830;&#23450;&#20102;&#22235;&#20010;&#20851;&#38190;&#39046;&#22495;&#65288;&#19978;&#19979;&#28216;&#21644;&#29992;&#25143;&#25552;&#31034;/&#22238;&#31572;&#65289;&#65292;&#22312;&#36825;&#20123;&#39046;&#22495;&#21487;&#20197;&#26377;&#25928;&#22320;&#24212;&#29992;&#20445;&#38556;&#25514;&#26045;&#12290;&#38543;&#21518;&#65292;&#23545;&#36825;&#22235;&#20010;&#39046;&#22495;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#21253;&#25324;&#22312;&#25104;&#26412;&#12289;&#26377;&#25928;&#24615;&#21644;&#19982;&#34892;&#19994;&#24815;&#20363;&#30340;&#19968;&#33268;&#24615;&#26041;&#38754;&#35780;&#20272;&#29616;&#26377;&#30340;&#20262;&#29702;&#20445;&#38556;&#25514;&#26045;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#35266;&#28857;&#26159;&#65292;&#29616;&#26377;&#30340;&#19982;IT&#30456;&#20851;&#30340;&#20262;&#29702;&#20934;&#21017;&#34429;&#28982;&#36866;&#29992;&#20110;&#20256;&#32479;&#30340;IT&#24037;&#31243;&#39046;&#22495;&#65292;&#20294;&#19981;&#36275;&#20197;&#24212;&#23545;&#22522;&#20110;LLMs&#20869;&#23481;&#29983;&#25104;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#20511;&#37492;&#26032;&#38395;&#19994;&#20869;&#24050;&#26377;&#30340;&#23454;&#36341;&#65292;&#20026;&#20998;&#21457;&#21644;&#38144;&#21806;LLM&#29983;&#25104;&#20869;&#23481;&#30340;&#20225;&#19994;&#25552;&#20986;&#20102;&#28508;&#22312;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores how AI-owners can develop safeguards for AI-generated content by drawing from established codes of conduct and ethical standards in other content-creation industries. It delves into the current state of ethical awareness on Large Language Models (LLMs). By dissecting the mechanism of content generation by LLMs, four key areas (upstream/downstream and at user prompt/answer), where safeguards could be effectively applied, are identified. A comparative analysis of these four areas follows and includes an evaluation of the existing ethical safeguards in terms of cost, effectiveness, and alignment with established industry practices. The paper's key argument is that existing IT-related ethical codes, while adequate for traditional IT engineering, are inadequate for the challenges posed by LLM-based content generation. Drawing from established practices within journalism, we propose potential standards for businesses involved in distributing and selling LLM-generated cont
&lt;/p&gt;</description></item><item><title>Video-LLaMA&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#26694;&#26550;&#65292;&#21033;&#29992;&#24050;&#26377;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#35270;&#39057;&#20013;&#30340;&#35270;&#35273;&#21644;&#21548;&#35273;&#30340;&#29702;&#35299;&#38382;&#39064;&#65292;&#20854;&#20013;Video Q-former&#21644;Audio Q-former&#29992;&#20110;&#22788;&#29702;&#35270;&#39057;&#20013;&#30340;&#35270;&#35273;&#19982;&#26102;&#38388;&#21464;&#21270;&#21644;&#38899;&#39057;&#20449;&#21495;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.02858</link><description>&lt;p&gt;
Video-LLaMA&#65306;&#29992;&#20110;&#35270;&#39057;&#29702;&#35299;&#30340;&#25351;&#20196;&#35843;&#25972;&#30340;&#35821;&#38899;-&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding. (arXiv:2306.02858v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02858
&lt;/p&gt;
&lt;p&gt;
Video-LLaMA&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#26694;&#26550;&#65292;&#21033;&#29992;&#24050;&#26377;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#35270;&#39057;&#20013;&#30340;&#35270;&#35273;&#21644;&#21548;&#35273;&#30340;&#29702;&#35299;&#38382;&#39064;&#65292;&#20854;&#20013;Video Q-former&#21644;Audio Q-former&#29992;&#20110;&#22788;&#29702;&#35270;&#39057;&#20013;&#30340;&#35270;&#35273;&#19982;&#26102;&#38388;&#21464;&#21270;&#21644;&#38899;&#39057;&#20449;&#21495;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#26694;&#26550;Video-LLaMA&#65292;&#36171;&#20104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29702;&#35299;&#35270;&#39057;&#20013;&#30340;&#35270;&#35273;&#21644;&#21548;&#35273;&#20869;&#23481;&#30340;&#33021;&#21147;&#12290;Video-LLaMA&#20174;&#24050;&#32463;&#39044;&#35757;&#32451;&#22909;&#30340;&#35270;&#35273;&#21644;&#38899;&#39057;&#32534;&#30721;&#22120;&#20197;&#21450;&#24050;&#32463;&#20923;&#32467;&#30340;LLMs&#36827;&#34892;&#36328;&#27169;&#24577;&#35757;&#32451;&#12290;&#30456;&#27604;&#20110;&#20043;&#21069;&#19987;&#27880;&#20110;&#38745;&#24577;&#22270;&#20687;&#29702;&#35299;&#30340;&#35270;&#35273;-LLMs&#65292;&#22914;MiniGPT-4&#21644;LLaVA&#65292;Video-LLaMA&#20027;&#35201;&#35299;&#20915;&#20004;&#20010;&#35270;&#39057;&#29702;&#35299;&#26041;&#38754;&#30340;&#25361;&#25112;&#65306;&#65288;1&#65289;&#25429;&#25417;&#35270;&#35273;&#22330;&#26223;&#20013;&#30340;&#26102;&#38388;&#21464;&#21270;&#65292;&#65288;2&#65289;&#38598;&#25104;&#38899;&#39057;&#35270;&#35273;&#20449;&#21495;&#12290;&#20026;&#20102;&#20811;&#26381;&#31532;&#19968;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;Video Q-former&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#32452;&#35013;&#21040;&#25105;&#20204;&#30340;&#35270;&#39057;&#32534;&#30721;&#22120;&#20013;&#65292;&#24182;&#24341;&#20837;&#19968;&#20010;&#35270;&#39057;&#21040;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#26469;&#23398;&#20064;&#35270;&#39057;-&#35821;&#35328;&#23545;&#24212;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#31532;&#20108;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#21033;&#29992;ImageBind&#65292;&#19968;&#20010;&#23558;&#22810;&#31181;&#27169;&#24577;&#23545;&#40784;&#30340;&#36890;&#29992;&#23884;&#20837;&#27169;&#22411;&#65292;&#20316;&#20026;&#39044;&#35757;&#32451;&#30340;&#38899;&#39057;&#32534;&#30721;&#22120;&#65292;&#24182;&#22312;ImageBind&#20043;&#19978;&#24341;&#20837;&#19968;&#20010;Audio Q-former&#65292;&#23398;&#20064;&#21512;&#29702;&#30340;&#21548;&#35273;&#26597;&#35810;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Video-LLaMA, a multi-modal framework that empowers Large Language Models (LLMs) with the capability of understanding both visual and auditory content in the video. Video-LLaMA bootstraps cross-modal training from the frozen pre-trained visual &amp; audio encoders and the frozen LLMs. Unlike previous vision-LLMs that focus on static image comprehensions such as MiniGPT-4 and LLaVA, Video-LLaMA mainly tackles two challenges in video understanding: (1) capturing the temporal changes in visual scenes, (2) integrating audio-visual signals. To counter the first challenge, we propose a Video Q-former to assemble the pre-trained image encoder into our video encoder and introduce a video-to-text generation task to learn video-language correspondence. For the second challenge, we leverage ImageBind, a universal embedding model aligning multiple modalities as the pre-trained audio encoder, and introduce an Audio Q-former on top of ImageBind to learn reasonable auditory query embeddings for
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LLM-Blender&#65292;&#23427;&#26159;&#19968;&#20010;&#38598;&#25104;&#26694;&#26550;&#65292;&#26088;&#22312;&#21033;&#29992;&#19981;&#21516;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#31168;&#29305;&#24615;&#65292;&#23454;&#29616;&#22987;&#32456;&#22914;&#19968;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;PairRanker&#21644;GenFuser&#26159;&#35813;&#26694;&#26550;&#30340;&#20004;&#20010;&#27169;&#22359;&#65292;PairRanker&#20351;&#29992;&#25104;&#23545;&#27604;&#36739;&#26041;&#27861;&#26469;&#21306;&#20998;&#20505;&#36873;&#36755;&#20986;&#65292;&#24182;&#19988;GenFuser&#26088;&#22312;&#21512;&#24182;&#25490;&#21517;&#26368;&#39640;&#30340;&#20505;&#36873;&#32773;&#65292;&#20197;&#29983;&#25104;&#25913;&#36827;&#30340;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2306.02561</link><description>&lt;p&gt;
LLM-Blender: &#21033;&#29992;&#25104;&#23545;&#25490;&#21517;&#21644;&#29983;&#25104;&#34701;&#21512;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion. (arXiv:2306.02561v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LLM-Blender&#65292;&#23427;&#26159;&#19968;&#20010;&#38598;&#25104;&#26694;&#26550;&#65292;&#26088;&#22312;&#21033;&#29992;&#19981;&#21516;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#31168;&#29305;&#24615;&#65292;&#23454;&#29616;&#22987;&#32456;&#22914;&#19968;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;PairRanker&#21644;GenFuser&#26159;&#35813;&#26694;&#26550;&#30340;&#20004;&#20010;&#27169;&#22359;&#65292;PairRanker&#20351;&#29992;&#25104;&#23545;&#27604;&#36739;&#26041;&#27861;&#26469;&#21306;&#20998;&#20505;&#36873;&#36755;&#20986;&#65292;&#24182;&#19988;GenFuser&#26088;&#22312;&#21512;&#24182;&#25490;&#21517;&#26368;&#39640;&#30340;&#20505;&#36873;&#32773;&#65292;&#20197;&#29983;&#25104;&#25913;&#36827;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LLM-Blender&#65292;&#19968;&#20010;&#38598;&#25104;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#19981;&#21516;&#20248;&#21183;&#26469;&#36798;&#21040;&#22987;&#32456;&#22914;&#19968;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#30001;&#20004;&#20010;&#27169;&#22359;&#32452;&#25104;&#65306;PairRanker&#21644;GenFuser&#65292;&#20197;&#24212;&#23545;&#19981;&#21516;&#31034;&#20363;&#30340;&#26368;&#20248;LLMs&#21487;&#20197;&#26174;&#30528;&#21464;&#21270;&#30340;&#35266;&#23519;&#12290;PairRanker&#20351;&#29992;&#19987;&#38376;&#30340;&#25104;&#23545;&#27604;&#36739;&#26041;&#27861;&#26469;&#21306;&#20998;&#20505;&#36873;&#36755;&#20986;&#20043;&#38388;&#30340;&#24494;&#23567;&#24046;&#24322;&#12290;&#23427;&#32852;&#21512;&#32534;&#30721;&#36755;&#20837;&#25991;&#26412;&#21644;&#19968;&#23545;&#20505;&#36873;&#32773;&#65292;&#20351;&#29992;&#20132;&#21449;&#27880;&#24847;&#32534;&#30721;&#22120;&#26469;&#30830;&#23450;&#20248;&#36234;&#32773;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;PairRanker&#19982;ChatGPT&#30340;&#25490;&#21517;&#30456;&#20851;&#24615;&#26368;&#39640;&#12290;&#28982;&#21518;&#65292;GenFuser&#26088;&#22312;&#21512;&#24182;&#25490;&#21517;&#26368;&#39640;&#30340;&#20505;&#36873;&#32773;&#65292;&#36890;&#36807;&#21033;&#29992;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#20943;&#23569;&#23427;&#20204;&#30340;&#24369;&#28857;&#26469;&#29983;&#25104;&#25913;&#36827;&#30340;&#36755;&#20986;&#12290;&#20026;&#20102;&#20419;&#36827;&#22823;&#35268;&#27169;&#35780;&#20272;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;MixInstruct&#65292;&#23427;&#26159;&#22810;&#20010;&#25351;&#20196;&#25968;&#25454;&#38598;&#30340;&#28151;&#21512;&#65292;&#20855;&#26377;oracle p&#12290;
&lt;/p&gt;
&lt;p&gt;
We present LLM-Blender, an ensembling framework designed to attain consistently superior performance by leveraging the diverse strengths of multiple open-source large language models (LLMs). Our framework consists of two modules: PairRanker and GenFuser, addressing the observation that optimal LLMs for different examples can significantly vary. PairRanker employs a specialized pairwise comparison method to distinguish subtle differences between candidate outputs. It jointly encodes the input text and a pair of candidates, using cross-attention encoders to determine the superior one. Our results demonstrate that PairRanker exhibits the highest correlation with ChatGPT-based ranking. Then, GenFuser aims to merge the top-ranked candidates, generating an improved output by capitalizing on their strengths and mitigating their weaknesses. To facilitate large-scale evaluation, we introduce a benchmark dataset, MixInstruct, which is a mixture of multiple instruction datasets featuring oracle p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#36890;&#36807;&#21453;&#29109;&#12289;&#21387;&#32553;&#22240;&#23376;&#21644;&#36328;&#20998;&#35010;F1&#20998;&#25968;&#20026;&#30446;&#26631;&#30340;&#20132;&#27969;&#20195;&#30721;&#28436;&#21464;&#20135;&#29289;&#65292;&#21457;&#29616;&#35821;&#35328;&#32467;&#26500;&#24418;&#25104;&#21487;&#20197;&#36890;&#36807;&#36825;&#20123;&#24230;&#37327;&#26469;&#39537;&#21160;&#12290;</title><link>http://arxiv.org/abs/2306.02383</link><description>&lt;p&gt;
&#39640;&#25928;&#35937;&#24449;&#20132;&#27969;&#32534;&#30721;&#30340;&#28436;&#21464;
&lt;/p&gt;
&lt;p&gt;
Evolution of Efficient Symbolic Communication Codes. (arXiv:2306.02383v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02383
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#36890;&#36807;&#21453;&#29109;&#12289;&#21387;&#32553;&#22240;&#23376;&#21644;&#36328;&#20998;&#35010;F1&#20998;&#25968;&#20026;&#30446;&#26631;&#30340;&#20132;&#27969;&#20195;&#30721;&#28436;&#21464;&#20135;&#29289;&#65292;&#21457;&#29616;&#35821;&#35328;&#32467;&#26500;&#24418;&#25104;&#21487;&#20197;&#36890;&#36807;&#36825;&#20123;&#24230;&#37327;&#26469;&#39537;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20154;&#31867;&#33258;&#28982;&#35821;&#35328;&#32467;&#26500;&#22914;&#20309;&#34987;&#30475;&#20316;&#26159;&#20154;&#38469;&#20132;&#27969;&#20195;&#30721;&#30340;&#28436;&#21464;&#20135;&#29289;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#25991;&#21270;&#26080;&#20851;&#21644;&#36328;&#35821;&#35328;&#24230;&#37327;&#26631;&#20934;&#65292;&#22914;&#21453;&#29109;&#12289;&#21387;&#32553;&#22240;&#23376;&#21644;&#36328;&#20998;&#35010;F1&#20998;&#25968;&#12290;&#25506;&#32034;&#26159;&#20316;&#20026;&#26356;&#22823;&#30340;&#26080;&#30417;&#30563;&#35821;&#35328;&#23398;&#20064;&#21162;&#21147;&#30340;&#19968;&#37096;&#20998;&#23436;&#25104;&#30340;&#65292;&#35797;&#22270;&#22312;&#22522;&#20110;&#8220;&#22522;&#26412;&#35821;&#35328;&#32467;&#26500;&#8221;&#30340;&#36229;&#21442;&#25968;&#31354;&#38388;&#20013;&#25191;&#34892;&#20803;&#23398;&#20064;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#19978;&#36848;&#25351;&#26631;&#26469;&#23454;&#29616;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#20420;&#35821;&#12289;&#20013;&#25991;&#21644;&#33521;&#35821;&#30340;&#36328;&#35821;&#35328;&#35789;&#32423;&#20998;&#35789;&#26631;&#35760;&#21270;&#30740;&#31350;&#20197;&#21450;&#38024;&#23545;&#33521;&#35821;&#30340;&#23376;&#35789;&#20998;&#21106;&#25110;&#24418;&#24577;&#20998;&#26512;&#30740;&#31350;&#30340;&#21021;&#27493;&#32467;&#26524;&#12290;&#21457;&#29616;&#35821;&#35328;&#32467;&#26500;&#24418;&#25104;&#35789;&#32423;&#20998;&#21106;&#25110;&#26631;&#35760;&#21270;&#21487;&#20197;&#36890;&#36807;&#25152;&#26377;&#36825;&#20123;&#24230;&#37327;&#26469;&#39537;&#21160;&#65292;&#21453;&#29109;&#23545;&#33521;&#35821;&#21644;&#20420;&#35821;&#26356;&#30456;&#20851;&#65292;&#32780;&#21387;&#32553;&#22240;&#23376;&#23545;&#20013;&#25991;&#26356;&#20855;&#29305;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper explores how the human natural language structure can be seen as a product of evolution of inter-personal communication code, targeting maximisation of such culture-agnostic and cross-lingual metrics such as anti-entropy, compression factor and cross-split F1 score. The exploration is done as part of a larger unsupervised language learning effort, the attempt is made to perform meta-learning in a space of hyper-parameters maximising F1 score based on the "ground truth" language structure, by means of maximising the metrics mentioned above. The paper presents preliminary results of cross-lingual word-level segmentation tokenisation study for Russian, Chinese and English as well as subword segmentation or morphological parsing study for English. It is found that language structure form the word-level segmentation or tokenisation can be found as driven by all of these metrics, anti-entropy being more relevant to English and Russian while compression factor more specific for Chin
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#38024;&#23545;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;11&#31181;&#36866;&#24212;&#26041;&#27861;&#22312;&#19981;&#21516;&#27745;&#26579;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#36866;&#24212;&#26041;&#27861;&#23545;&#25991;&#26412;&#27745;&#26579;&#26356;&#25935;&#24863;&#65292;&#21333;&#29420;&#20351;&#29992;&#23567;&#22411;&#25991;&#26412;&#36866;&#37197;&#22120;&#27604;&#20849;&#20139;&#36866;&#37197;&#22120;&#26356;&#40065;&#26834;&#65292;&#21487;&#33719;&#24471;&#21487;&#27604;&#36739;&#30340;&#24178;&#20928;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.02080</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#22522;&#20934;&#27979;&#35797;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Robustness of Adaptation Methods on Pre-trained Vision-Language Models. (arXiv:2306.02080v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02080
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#38024;&#23545;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;11&#31181;&#36866;&#24212;&#26041;&#27861;&#22312;&#19981;&#21516;&#27745;&#26579;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#36866;&#24212;&#26041;&#27861;&#23545;&#25991;&#26412;&#27745;&#26579;&#26356;&#25935;&#24863;&#65292;&#21333;&#29420;&#20351;&#29992;&#23567;&#22411;&#25991;&#26412;&#36866;&#37197;&#22120;&#27604;&#20849;&#20139;&#36866;&#37197;&#22120;&#26356;&#40065;&#26834;&#65292;&#21487;&#33719;&#24471;&#21487;&#27604;&#36739;&#30340;&#24178;&#20928;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#21319;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#29305;&#23450;&#39046;&#22495;&#34920;&#29616;&#30340;&#21508;&#31181;&#36866;&#24212;&#26041;&#27861;&#65292;&#22914; LoRA&#12289;prompts &#21644; adapters &#31561;&#24050;&#34987;&#25552;&#20986;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#36866;&#24212;&#26041;&#27861;&#23545;&#20110;&#20998;&#24067;&#20301;&#31227;&#30340;&#40065;&#26834;&#24615;&#23578;&#26410;&#24471;&#21040;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;11&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#36866;&#24212;&#26041;&#27861;&#22312;4&#20010;&#35270;&#35273;&#35821;&#35328;&#25968;&#25454;&#38598;&#19978;&#30340;&#40065;&#26834;&#24615;&#65292;&#32771;&#23519;&#20102;&#21487;&#29992;&#36866;&#24212;&#31034;&#20363;&#21644;&#36866;&#24212;&#36807;&#31243;&#20013;&#21487;&#35757;&#32451;&#21442;&#25968;&#22823;&#23567;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#22320;&#65292;&#24341;&#20837;&#20102;7&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;96&#31181;&#35270;&#35273;&#21644;87&#31181;&#25991;&#26412;&#27745;&#25439;&#65292;&#20197;&#30740;&#31350;&#19981;&#21516;&#36866;&#24212;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#65306;1&#65289;&#36866;&#24212;&#26041;&#27861;&#23545;&#25991;&#26412;&#27745;&#26579;&#27604;&#35270;&#35273;&#27745;&#26579;&#26356;&#25935;&#24863;&#12290;2) &#20840;&#37327;&#24494;&#35843;&#24182;&#19981;&#24635;&#33021;&#25552;&#20379;&#26368;&#39640;&#30340;&#40065;&#26834;&#24615;&#65307;&#30456;&#21453;&#65292;&#36866;&#37197;&#22120;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#20855;&#26377;&#21487;&#27604;&#36739;&#30340;&#24178;&#20928;&#24615;&#33021;&#12290;3&#65289;&#19982;&#39044;&#26399;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#21333;&#29420;&#20351;&#29992;&#23567;&#22411;&#25991;&#26412;&#36866;&#37197;&#22120;&#36890;&#24120;&#27604;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#31354;&#38388;&#20013;&#20849;&#20139;&#36866;&#37197;&#22120;&#26356;&#40065;&#26834;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various adaptation methods, such as LoRA, prompts, and adapters, have been proposed to enhance the performance of pre-trained vision-language models in specific domains. The robustness of these adaptation methods against distribution shifts have not been studied. In this study, we assess the robustness of 11 widely-used adaptation methods across 4 vision-language datasets under multimodal corruptions. Concretely, we introduce 7 benchmark datasets, including 96 visual and 87 textual corruptions, to investigate the robustness of different adaptation methods, the impact of available adaptation examples, and the influence of trainable parameter size during adaptation. Our analysis reveals that: 1) Adaptation methods are more sensitive to text corruptions than visual corruptions. 2) Full fine-tuning does not consistently provide the highest robustness; instead, adapters can achieve better robustness with comparable clean performance. 3) Contrary to expectations, our findings indicate that i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23581;&#35797;&#23454;&#29616;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#36816;&#29992;&#21487;&#36870;&#27169;&#22411;&#23454;&#29616;&#39640;&#25928;&#30340;&#24494;&#35843;&#65292;&#24182;&#21457;&#29616;&#22312;&#21021;&#22987;&#21270;&#24494;&#35843;&#26102;&#20445;&#30041;PLM&#30340;&#36215;&#28857;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2306.00477</link><description>&lt;p&gt;
&#20351;&#39044;&#35757;&#32451;&#27169;&#22411;&#20855;&#26377;&#21487;&#36870;&#24615;&#65306;&#20174;&#21442;&#25968;&#21040;&#20869;&#23384;&#39640;&#25928;&#30340;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Make Your Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning. (arXiv:2306.00477v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23581;&#35797;&#23454;&#29616;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#36816;&#29992;&#21487;&#36870;&#27169;&#22411;&#23454;&#29616;&#39640;&#25928;&#30340;&#24494;&#35843;&#65292;&#24182;&#21457;&#29616;&#22312;&#21021;&#22987;&#21270;&#24494;&#35843;&#26102;&#20445;&#30041;PLM&#30340;&#36215;&#28857;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#38750;&#24120;&#25104;&#21151;&#30340;&#26041;&#27861;&#65292;&#21482;&#38656;&#35757;&#32451;&#23569;&#37327;&#21442;&#25968;&#32780;&#19981;&#20250;&#38477;&#20302;&#24615;&#33021;&#65292;&#24182;&#38543;&#30528;PLM&#36234;&#26469;&#36234;&#22823;&#32780;&#25104;&#20026;&#20107;&#23454;&#19978;&#30340;&#23398;&#20064;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;PEFT&#26041;&#27861;&#19981;&#20855;&#22791;&#20869;&#23384;&#25928;&#29575;&#65292;&#22240;&#20026;&#23427;&#20204;&#20173;&#38656;&#35201;&#23384;&#20648;&#22823;&#37096;&#20998;&#20013;&#38388;&#28608;&#27963;&#20540;&#20197;&#20415;&#35745;&#31639;&#26799;&#24230;&#65292;&#31867;&#20284;&#20110;&#24494;&#35843;&#12290;&#19968;&#20010;&#20943;&#23569;&#28608;&#27963;&#20869;&#23384;&#30340;&#26377;&#25928;&#26041;&#27861;&#26159;&#24212;&#29992;&#21487;&#36870;&#27169;&#22411;&#65292;&#36825;&#26679;&#20013;&#38388;&#28608;&#27963;&#20540;&#23601;&#26080;&#38656;&#32531;&#23384;&#65292;&#21487;&#20197;&#37325;&#26032;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#23558;PLM&#20462;&#25913;&#20026;&#23427;&#30340;&#21487;&#36870;&#21464;&#20307;&#24182;&#36827;&#34892;PEFT&#24182;&#19981;&#26159;&#19968;&#20214;&#23481;&#26131;&#30340;&#20107;&#65292;&#22240;&#20026;&#21487;&#36870;&#27169;&#22411;&#20855;&#26377;&#19982;&#24403;&#21069;&#21457;&#24067;&#30340;PLM&#19981;&#21516;&#30340;&#20307;&#31995;&#32467;&#26500;&#12290;&#26412;&#25991;&#39318;&#20808;&#35843;&#26597;&#29616;&#26377;PEFT&#26041;&#27861;&#25104;&#21151;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#35748;&#35782;&#21040;&#22312;&#21021;&#22987;&#21270;PEFT&#26102;&#20445;&#30041;PLM&#30340;&#36215;&#28857;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter-efficient fine-tuning (PEFT) of pre-trained language models (PLMs) has emerged as a highly successful approach, with training only a small number of parameters without sacrificing performance and becoming the de-facto learning paradigm with the increasing size of PLMs. However, existing PEFT methods are not memory-efficient, because they still require caching most of the intermediate activations for the gradient calculation, akin to fine-tuning. One effective way to reduce the activation memory is to apply a reversible model, so the intermediate activations are not necessary to be cached and can be recomputed. Nevertheless, modifying a PLM to its reversible variant with PEFT is not straightforward, since the reversible model has a distinct architecture from the currently released PLMs. In this paper, we first investigate what is a key factor for the success of existing PEFT methods, and realize that it's essential to preserve the PLM's starting point when initializing a PEFT 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#30340;&#28966;&#28857;&#21069;&#32512;&#35843;&#25972;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22312;&#21333;&#23646;&#24615;&#25511;&#21046;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#25511;&#21046;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#27969;&#30021;&#24230;&#65292;&#22312;&#22810;&#23646;&#24615;&#25511;&#21046;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#30340;&#25511;&#21046;&#20934;&#30830;&#24615;&#65292;&#24182;&#20445;&#25345;&#20102;&#25511;&#21046;&#26032;&#23646;&#24615;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#29616;&#26377;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.00369</link><description>&lt;p&gt;
&#38024;&#23545;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#30340;&#28966;&#28857;&#21069;&#32512;&#35843;&#25972;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Focused Prefix Tuning for Controllable Text Generation. (arXiv:2306.00369v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#30340;&#28966;&#28857;&#21069;&#32512;&#35843;&#25972;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22312;&#21333;&#23646;&#24615;&#25511;&#21046;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#25511;&#21046;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#27969;&#30021;&#24230;&#65292;&#22312;&#22810;&#23646;&#24615;&#25511;&#21046;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#30340;&#25511;&#21046;&#20934;&#30830;&#24615;&#65292;&#24182;&#20445;&#25345;&#20102;&#25511;&#21046;&#26032;&#23646;&#24615;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#29616;&#26377;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#25968;&#25454;&#38598;&#20013;&#65292;&#23384;&#22312;&#26410;&#26631;&#27880;&#23646;&#24615;&#65292;&#21487;&#33021;&#20250;&#20026;&#20351;&#29992;&#20854;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#25552;&#20379;&#26080;&#20851;&#30340;&#23398;&#20064;&#20449;&#21495;&#65292;&#20174;&#32780;&#38477;&#20302;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#28966;&#28857;&#21069;&#32512;&#35843;&#25972;&#65288;FPT&#65289;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#20351;&#25511;&#21046;&#33021;&#22815;&#19987;&#27880;&#20110;&#25152;&#38656;&#23646;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#32447;&#27169;&#22411;&#30456;&#27604;&#65292;&#22312;&#21333;&#23646;&#24615;&#25511;&#21046;&#20219;&#21153;&#20013;&#65292;FPT&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#25511;&#21046;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#27969;&#30021;&#24230;&#12290;&#22312;&#22810;&#23646;&#24615;&#25511;&#21046;&#20219;&#21153;&#20013;&#65292;FPT&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#30340;&#25511;&#21046;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#25511;&#21046;&#26032;&#23646;&#24615;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#29616;&#26377;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a controllable text generation dataset, there exist unannotated attributes that could provide irrelevant learning signals to models that use it for training and thus degrade their performance. We propose focused prefix tuning(FPT) to mitigate the problem and to enable the control to focus on the desired attribute. Experimental results show that FPT can achieve better control accuracy and text fluency than baseline models in single-attribute control tasks. In multi-attribute control tasks, FPT achieves comparable control accuracy with the state-of-the-art approach while keeping the flexibility to control new attributes without retraining existing models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#28304;&#20195;&#30721;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#21644;&#32508;&#36848;&#65292;&#20171;&#32461;&#20102;&#23427;&#20204;&#30340;&#20998;&#31867;&#27861;&#12289;&#20248;&#21270;&#31574;&#30053;&#21644;&#24615;&#33021;&#32467;&#26524;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#26041;&#21521;&#21644;&#30740;&#31350;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.19915</link><description>&lt;p&gt;
&#28304;&#20195;&#30721;&#27169;&#22411;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65306;&#19968;&#20221;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation Approaches for Source Code Models: A Survey. (arXiv:2305.19915v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#28304;&#20195;&#30721;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#21644;&#32508;&#36848;&#65292;&#20171;&#32461;&#20102;&#23427;&#20204;&#30340;&#20998;&#31867;&#27861;&#12289;&#20248;&#21270;&#31574;&#30053;&#21644;&#24615;&#33021;&#32467;&#26524;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#26041;&#21521;&#21644;&#30740;&#31350;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28304;&#20195;&#30721;&#22312;&#35768;&#22810;&#20851;&#38190;&#20219;&#21153;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#20419;&#36827;&#20102;&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#20197;&#22686;&#24378;&#35757;&#32451;&#25968;&#25454;&#24182;&#25552;&#39640;&#36825;&#20123;&#27169;&#22411;&#30340;&#21508;&#31181;&#33021;&#21147;&#65288;&#20363;&#22914;&#20581;&#22766;&#24615;&#21644;&#21487;&#27867;&#21270;&#24615;&#65289;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#24182;&#38024;&#23545;&#28304;&#20195;&#30721;&#27169;&#22411;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;DA&#26041;&#27861;&#30340;&#35843;&#25972;&#65292;&#20294;&#32570;&#20047;&#32508;&#21512;&#24615;&#30340;&#35843;&#26597;&#21644;&#23457;&#26597;&#20197;&#29702;&#35299;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#21644;&#21547;&#20041;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#28304;&#20195;&#30721;&#30340;&#25968;&#25454;&#22686;&#24378;&#36827;&#34892;&#20840;&#38754;&#32780;&#32508;&#21512;&#30340;&#35843;&#26597;&#65292;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#25972;&#29702;&#21644;&#27010;&#36848;&#29616;&#26377;&#25991;&#29486;&#65292;&#20197;&#25552;&#20379;&#35813;&#39046;&#22495;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#36866;&#29992;&#20110;&#28304;&#20195;&#30721;&#27169;&#22411;&#30340;&#25968;&#25454;&#22686;&#24378;&#30340;&#20998;&#31867;&#27861;&#65292;&#28982;&#21518;&#35752;&#35770;&#20102;&#33879;&#21517;&#30340;&#12289;&#26041;&#27861;&#19978;&#20855;&#26377;&#35828;&#26126;&#24615;&#30340;&#26041;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#20248;&#21270;DA&#36136;&#37327;&#30340;&#19968;&#33324;&#31574;&#30053;&#21644;&#25216;&#26415;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#22312;&#34987;&#24191;&#27867;&#25509;&#21463;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#21457;&#25381;&#20316;&#29992;&#30340;&#25216;&#26415;&#65292;&#24182;&#21576;&#29616;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;DA&#29992;&#20110;&#28304;&#20195;&#30721;&#27169;&#22411;&#30340;&#28508;&#22312;&#26410;&#26469;&#26041;&#21521;&#21644;&#24320;&#25918;&#30740;&#31350;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasingly popular adoption of source code in many critical tasks motivates the development of data augmentation (DA) techniques to enhance training data and improve various capabilities (e.g., robustness and generalizability) of these models. Although a series of DA methods have been proposed and tailored for source code models, there lacks a comprehensive survey and examination to understand their effectiveness and implications. This paper fills this gap by conducting a comprehensive and integrative survey of data augmentation for source code, wherein we systematically compile and encapsulate existing literature to provide a comprehensive overview of the field. We start by constructing a taxonomy of DA for source code models model approaches, followed by a discussion on prominent, methodologically illustrative approaches. Next, we highlight the general strategies and techniques to optimize the DA quality. Subsequently, we underscore techniques that find utility in widely-accept
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#25955;&#30721;&#21644;&#28151;&#21512;&#22120;&#30456;&#21327;&#20316;&#30340;&#31471;&#21040;&#31471;&#34920;&#29616;&#21147;TTS&#65292;&#23427;&#37319;&#29992;&#26032;&#30340;&#36755;&#20837;&#34920;&#31034;&#21644;&#31616;&#21333;&#30340;&#26550;&#26500;&#26469;&#23454;&#29616;&#25913;&#36827;&#30340;&#38901;&#24459;&#24314;&#27169;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.19567</link><description>&lt;p&gt;
DC CoMix TTS&#65306;&#19968;&#31181;&#19982;&#28151;&#21512;&#22120;&#21327;&#20316;&#30340;&#31471;&#21040;&#31471;&#34920;&#29616;&#21147;TTS&#65292;&#21033;&#29992;&#31163;&#25955;&#30721;&#23454;&#29616;&#25913;&#36827;&#30340;&#38901;&#24459;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
DC CoMix TTS: An End-to-End Expressive TTS with Discrete Code Collaborated with Mixer. (arXiv:2305.19567v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19567
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#25955;&#30721;&#21644;&#28151;&#21512;&#22120;&#30456;&#21327;&#20316;&#30340;&#31471;&#21040;&#31471;&#34920;&#29616;&#21147;TTS&#65292;&#23427;&#37319;&#29992;&#26032;&#30340;&#36755;&#20837;&#34920;&#31034;&#21644;&#31616;&#21333;&#30340;&#26550;&#26500;&#26469;&#23454;&#29616;&#25913;&#36827;&#30340;&#38901;&#24459;&#24314;&#27169;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20013;&#24615;TTS&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#20869;&#23481;&#27844;&#28431;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36755;&#20837;&#34920;&#31034;&#21644;&#31616;&#21333;&#30340;&#26550;&#26500;&#26469;&#23454;&#29616;&#25913;&#36827;&#30340;&#38901;&#24459;&#24314;&#27169;&#12290;&#21463;&#26368;&#36817;&#22312;TTS&#20013;&#20351;&#29992;&#31163;&#25955;&#30721;&#21462;&#24471;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#31163;&#25955;&#30721;&#24341;&#20837;&#21040;&#21442;&#32771;&#32534;&#30721;&#22120;&#30340;&#36755;&#20837;&#20013;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#38899;&#39057;&#21387;&#32553;&#27169;&#22411;&#20013;&#30340;&#21521;&#37327;&#37327;&#21270;&#22120;&#26469;&#21033;&#29992;&#23427;&#24050;&#32463;&#35757;&#32451;&#36807;&#30340;&#22810;&#26679;&#21270;&#30340;&#22768;&#23398;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#20462;&#25913;&#21518;&#30340;MLP-Mixer&#24212;&#29992;&#21040;&#21442;&#32771;&#32534;&#30721;&#22120;&#20013;&#65292;&#20351;&#24471;&#26550;&#26500;&#26356;&#21152;&#36731;&#30408;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#35757;&#32451;&#38901;&#24459;&#36716;&#31227;TTS&#12290;&#25105;&#20204;&#36890;&#36807;&#20027;&#35266;&#21644;&#23458;&#35266;&#35780;&#20272;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#65292;&#24403;&#31163;&#25955;&#30721;&#20316;&#20026;&#36755;&#20837;&#26102;&#65292;&#21442;&#32771;&#32534;&#30721;&#22120;&#21487;&#20197;&#23398;&#20064;&#21040;&#26356;&#22909;&#30340;&#19982;&#35828;&#35805;&#20154;&#26080;&#20851;&#30340;&#38901;&#24459;&#12290;&#21478;&#22806;&#65292;&#21363;&#20351;&#36755;&#20837;&#21442;&#25968;&#26356;&#23569;&#65292;&#25105;&#20204;&#20063;&#21487;&#20197;&#33719;&#24471;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the huge successes made in neutral TTS, content-leakage remains a challenge. In this paper, we propose a new input representation and simple architecture to achieve improved prosody modeling. Inspired by the recent success in the use of discrete code in TTS, we introduce discrete code to the input of the reference encoder. Specifically, we leverage the vector quantizer from the audio compression model to exploit the diverse acoustic information it has already been trained on. In addition, we apply the modified MLP-Mixer to the reference encoder, making the architecture lighter. As a result, we train the prosody transfer TTS in an end-to-end manner. We prove the effectiveness of our method through both subjective and objective evaluations. We demonstrate that the reference encoder learns better speaker-independent prosody when discrete code is utilized as input in the experiments. In addition, we obtain comparable results even when fewer parameters are inputted.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;&#26041;&#27861;&#65292;&#22312;&#19981;&#20381;&#36182;&#22806;&#37096;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#27604;&#20043;&#21069;&#21033;&#29992;&#39044;&#35757;&#32451;&#26435;&#37325;&#12289;&#23884;&#20837;&#21644;&#22806;&#37096;&#35821;&#27861;&#20998;&#26512;&#22120;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#34920;&#26126;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2305.19512</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Fine-grained Text Style Transfer with Diffusion-Based Language Models. (arXiv:2305.19512v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;&#26041;&#27861;&#65292;&#22312;&#19981;&#20381;&#36182;&#22806;&#37096;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#27604;&#20043;&#21069;&#21033;&#29992;&#39044;&#35757;&#32451;&#26435;&#37325;&#12289;&#23884;&#20837;&#21644;&#22806;&#37096;&#35821;&#27861;&#20998;&#26512;&#22120;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#34920;&#26126;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#24335;&#27010;&#29575;&#27169;&#22411;&#24050;&#32463;&#22312;&#21487;&#25511;&#21046;&#22320;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#19978;&#26174;&#31034;&#20986;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#35797;&#22270;&#23558;&#36825;&#31181;&#21487;&#25511;&#24615;&#36816;&#29992;&#21040;&#25991;&#26412;&#29983;&#25104;&#39046;&#22495;&#12290;&#20197;&#21069;&#30340;&#25193;&#25955;&#24335;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#34920;&#26126;&#65292;&#23427;&#20204;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#22806;&#37096;&#30693;&#35782;&#65288;&#22914;&#39044;&#35757;&#32451;&#26435;&#37325;&#65289;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#20173;&#28982;&#21487;&#20197;&#23454;&#29616;&#31283;&#23450;&#30340;&#24615;&#33021;&#21644;&#21487;&#25511;&#24615;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;StylePTB&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#19968;&#20010;&#25193;&#25955;&#24335;&#27169;&#22411;&#65292;&#36825;&#26159;&#32454;&#31890;&#24230;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;&#30340;&#26631;&#20934;&#22522;&#20934;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#35780;&#20272;&#20219;&#21153;&#30456;&#27604;&#65292;StylePTB&#20013;&#30340;&#20219;&#21153;&#38656;&#35201;&#23545;&#36755;&#20986;&#25991;&#26412;&#36827;&#34892;&#26356;&#21152;&#31934;&#32454;&#30340;&#25511;&#21046;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#22312;StylePTB&#19978;&#23454;&#29616;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#20010;&#21035;&#21644;&#32452;&#21512;&#36716;&#25442;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#27809;&#26377;&#22806;&#37096;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;StylePTB&#30340;&#26377;&#38480;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#34920;&#29616;&#20248;&#20110;&#20197;&#21069;&#21033;&#29992;&#39044;&#35757;&#32451;&#26435;&#37325;&#12289;&#23884;&#20837;&#21644;&#22806;&#37096;&#35821;&#27861;&#20998;&#26512;&#22120;&#30340;&#24037;&#20316;&#65292;&#36825;&#21487;&#33021;&#34920;&#26126;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#39046;&#22495;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion probabilistic models have shown great success in generating high-quality images controllably, and researchers have tried to utilize this controllability into text generation domain. Previous works on diffusion-based language models have shown that they can be trained without external knowledge (such as pre-trained weights) and still achieve stable performance and controllability. In this paper, we trained a diffusion-based model on StylePTB dataset, the standard benchmark for fine-grained text style transfers. The tasks in StylePTB requires much more refined control over the output text compared to tasks evaluated in previous works, and our model was able to achieve state-of-the-art performance on StylePTB on both individual and compositional transfers. Moreover, our model, trained on limited data from StylePTB without external knowledge, outperforms previous works that utilized pretrained weights, embeddings, and external grammar parsers, and this may indicate that diffusion
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#25968;&#25454;&#38598;&#29305;&#24449;&#21270;&#26694;&#26550;infoVerse&#65292;&#36890;&#36807;&#32467;&#21512;&#21508;&#31181;&#27169;&#22411;&#39537;&#21160;&#30340;&#20803;&#20449;&#24687;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#25968;&#25454;&#38598;&#30340;&#22810;&#32500;&#29305;&#24449;&#65292;&#26377;&#21161;&#20110;&#29992;&#25143;&#25110;&#27169;&#22411;&#30830;&#23450;&#21738;&#20123;&#26679;&#26412;&#38656;&#35201;&#20851;&#27880;&#12290;</title><link>http://arxiv.org/abs/2305.19344</link><description>&lt;p&gt;
infoVerse&#65306;&#19968;&#31181;&#29992;&#22810;&#32500;&#24230;&#20803;&#20449;&#24687;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#29305;&#24449;&#21270;&#30340;&#36890;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
infoVerse: A Universal Framework for Dataset Characterization with Multidimensional Meta-information. (arXiv:2305.19344v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#25968;&#25454;&#38598;&#29305;&#24449;&#21270;&#26694;&#26550;infoVerse&#65292;&#36890;&#36807;&#32467;&#21512;&#21508;&#31181;&#27169;&#22411;&#39537;&#21160;&#30340;&#20803;&#20449;&#24687;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#25968;&#25454;&#38598;&#30340;&#22810;&#32500;&#29305;&#24449;&#65292;&#26377;&#21161;&#20110;&#29992;&#25143;&#25110;&#27169;&#22411;&#30830;&#23450;&#21738;&#20123;&#26679;&#26412;&#38656;&#35201;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
NLP&#31995;&#32479;&#30340;&#25104;&#21151;&#24448;&#24448;&#20381;&#36182;&#20110;&#22823;&#37327;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20013;&#24182;&#38750;&#25152;&#26377;&#26679;&#26412;&#37117;&#21516;&#26679;&#26377;&#21033;&#20110;&#23398;&#20064;&#65292;&#22240;&#20026;&#26377;&#20123;&#21487;&#33021;&#26159;&#20887;&#20313;&#25110;&#24102;&#26377;&#22122;&#22768;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#20960;&#31181;&#22522;&#20110;&#27169;&#22411;&#39537;&#21160;&#20803;&#20449;&#24687;&#65288;&#20363;&#22914;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#65289;&#30340;&#25968;&#25454;&#38598;&#29305;&#24449;&#21270;&#26041;&#27861;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#20114;&#34917;&#25928;&#24212;&#21463;&#21040;&#30340;&#20851;&#27880;&#36739;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;infoVerse&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#25968;&#25454;&#38598;&#29305;&#24449;&#21270;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#32467;&#21512;&#21508;&#31181;&#27169;&#22411;&#39537;&#21160;&#30340;&#20803;&#20449;&#24687;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#26377;&#25928;&#22320;&#25429;&#25417;&#20102;&#25968;&#25454;&#38598;&#30340;&#22810;&#32500;&#29305;&#24449;&#12290;infoVerse&#25581;&#31034;&#20102;&#25968;&#25454;&#38598;&#20013;&#22312;&#21407;&#22987;&#35821;&#20041;&#31354;&#38388;&#20013;&#19981;&#26126;&#26174;&#30340;&#29420;&#29305;&#21306;&#22495;&#65292;&#20174;&#32780;&#24341;&#23548;&#29992;&#25143;&#65288;&#25110;&#27169;&#22411;&#65289;&#30830;&#23450;&#21738;&#20123;&#26679;&#26412;&#38656;&#35201;&#20851;&#27880;&#20197;&#36827;&#34892;&#25506;&#32034;&#12289;&#35780;&#20272;&#25110;&#27880;&#37322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;infoVerse&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#26041;&#27861;&#26469;&#36873;&#25321;&#19968;&#32452;&#25968;&#25454;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of NLP systems often relies on the availability of large, high-quality datasets. However, not all samples in these datasets are equally valuable for learning, as some may be redundant or noisy. Several methods for characterizing datasets based on model-driven meta-information (e.g., model's confidence) have been developed, but the relationship and complementary effects of these methods have received less attention. In this paper, we introduce infoVerse, a universal framework for dataset characterization, which provides a new feature space that effectively captures multidimensional characteristics of datasets by incorporating various model-driven meta-information. infoVerse reveals distinctive regions of the dataset that are not apparent in the original semantic space, hence guiding users (or models) in identifying which samples to focus on for exploration, assessment, or annotation. Additionally, we propose a novel sampling method on infoVerse to select a set of data points
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#20013;&#30340;&#19977;&#31181;&#26631;&#31614;&#20559;&#24046;&#25552;&#20986;&#20998;&#31867;&#27861;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#30340;&#20559;&#24046;&#26657;&#20934;&#26041;&#27861;&#65292;&#20351;&#29992;&#38543;&#26426;&#30340;&#39046;&#22495;&#35789;&#20272;&#31639;&#35821;&#35328;&#27169;&#22411;&#30340;&#26631;&#31614;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2305.19148</link><description>&lt;p&gt;
&#32531;&#35299;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26631;&#31614;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Mitigating Label Biases for In-context Learning. (arXiv:2305.19148v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19148
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#20013;&#30340;&#19977;&#31181;&#26631;&#31614;&#20559;&#24046;&#25552;&#20986;&#20998;&#31867;&#27861;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#30340;&#20559;&#24046;&#26657;&#20934;&#26041;&#27861;&#65292;&#20351;&#29992;&#38543;&#26426;&#30340;&#39046;&#22495;&#35789;&#20272;&#31639;&#35821;&#35328;&#27169;&#22411;&#30340;&#26631;&#31614;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#21508;&#31181;&#35774;&#35745;&#35774;&#32622;&#65292;&#22914;&#36873;&#25321;&#21644;&#39034;&#24207;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#65292;&#21487;&#33021;&#20351;&#27169;&#22411;&#23545;&#26576;&#31181;&#29305;&#23450;&#39044;&#27979;&#20559;&#35265;&#65292;&#32780;&#36825;&#31181;&#39044;&#27979;&#24182;&#19981;&#21453;&#26144;&#23545;&#20219;&#21153;&#30340;&#29702;&#35299;&#12290;&#34429;&#28982;&#35768;&#22810;&#30740;&#31350;&#35752;&#35770;&#20102;&#36825;&#20123;&#35774;&#35745;&#36873;&#25321;&#65292;&#20294;&#23545;&#23427;&#20204;&#36827;&#34892;&#20998;&#31867;&#21644;&#20943;&#32531;&#20854;&#24433;&#21709;&#30340;&#31995;&#32479;&#35843;&#26597;&#24456;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;&#25991;&#26412;&#20998;&#31867;&#20013;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#20013;&#30340;&#19977;&#31181;&#26631;&#31614;&#20559;&#24046;&#23450;&#20041;&#20102;&#19968;&#20010;&#20998;&#31867;&#27861;&#65306;&#39321;&#33609;&#26631;&#31614;&#20559;&#24046;&#12289;&#19978;&#19979;&#25991;&#26631;&#31614;&#20559;&#24046;&#21644;&#39046;&#22495;&#26631;&#31614;&#20559;&#24046;&#65288;&#25105;&#20204;&#39318;&#27425;&#27010;&#24565;&#21270;&#21644;&#26816;&#27979;&#21040;&#65289;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#20808;&#21069;&#30340;&#26631;&#31614;&#20559;&#24046;&#26657;&#20934;&#26041;&#27861;&#19981;&#33021;&#35299;&#20915;&#25152;&#26377;&#19977;&#31181;&#20559;&#24046;&#12290;&#29305;&#21035;&#26159;&#65292;&#39046;&#22495;&#26631;&#31614;&#20559;&#24046;&#20351;LLM&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#21482;&#33021;&#23454;&#29616;&#38543;&#26426;&#32423;&#21035;&#30340;&#24615;&#33021;&#65292;&#32780;&#19981;&#31649;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#36873;&#25321;&#22914;&#20309;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#31616;&#21333;&#30340;&#20559;&#24046;&#26657;&#20934;&#26041;&#27861;&#65292;&#20351;&#29992;&#38543;&#26426;&#30340;&#39046;&#22495;&#35789;&#20272;&#31639;&#35821;&#35328;&#27169;&#22411;&#30340;&#26631;&#31614;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various design settings for in-context learning (ICL), such as the choice and order of the in-context examples, can bias a model toward a particular prediction without being reflective of an understanding of the task. While many studies discuss these design choices, there have been few systematic investigations into categorizing them and mitigating their impact. In this work, we define a typology for three types of label biases in ICL for text classification: vanilla-label bias, context-label bias, and domain-label bias (which we conceptualize and detect for the first time).  Our analysis demonstrates that prior label bias calibration methods fall short of addressing all three types of biases. Specifically, domain-label bias restricts LLMs to random-level performance on many tasks regardless of the choice of in-context examples. To mitigate the effect of these biases, we propose a simple bias calibration method that estimates a language model's label bias using random in-domain words f
&lt;/p&gt;</description></item><item><title>W-procer&#26159;&#19968;&#31181;&#22522;&#20110;&#21152;&#26435;&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#30340;&#21307;&#23398;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;&#65292;&#22312;&#26500;&#24314;&#22522;&#20110;&#21407;&#22411;&#30340;&#23545;&#27604;&#25439;&#22833;&#21644;&#21152;&#26435;&#32593;&#32476;&#26041;&#38754;&#20855;&#26377;&#21019;&#26032;&#24615;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.18624</link><description>&lt;p&gt;
W-procer: &#22522;&#20110;&#21152;&#26435;&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#30340;&#21307;&#23398;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
W-procer: Weighted Prototypical Contrastive Learning for Medical Few-Shot Named Entity Recognition. (arXiv:2305.18624v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18624
&lt;/p&gt;
&lt;p&gt;
W-procer&#26159;&#19968;&#31181;&#22522;&#20110;&#21152;&#26435;&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#30340;&#21307;&#23398;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;&#65292;&#22312;&#26500;&#24314;&#22522;&#20110;&#21407;&#22411;&#30340;&#23545;&#27604;&#25439;&#22833;&#21644;&#21152;&#26435;&#32593;&#32476;&#26041;&#38754;&#20855;&#26377;&#21019;&#26032;&#24615;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#24050;&#25104;&#20026;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#30340;&#19968;&#31181;&#21463;&#27426;&#36814;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20256;&#32479;&#37197;&#32622;&#21147;&#27714;&#20943;&#23569;&#20855;&#26377;&#30456;&#21516;&#26631;&#31614;&#30340;&#26631;&#35760;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#24182;&#22686;&#21152;&#20855;&#26377;&#19981;&#21516;&#26631;&#31614;&#30340;&#26631;&#35760;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#23384;&#22312;&#22823;&#37327;&#34987;&#27880;&#37322;&#20026;&#8220;O&#8221;&#65288;&#21363;&#8220;OUTSIDE&#8221;&#65289;&#30340;&#23454;&#20307;&#65292;&#24182;&#19988;&#23427;&#20204;&#19981;&#24076;&#26395;&#34987;&#25512;&#31163;&#21040;&#24403;&#21069;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26631;&#35760;&#20026;&#8220;O&#8221;&#20197;&#22806;&#30340;&#20854;&#20182;&#23454;&#20307;&#65292;&#36825;&#31181;&#35774;&#23450;&#25928;&#26524;&#19981;&#20339;&#65292;&#21487;&#33021;&#20250;&#24471;&#20986;&#21547;&#26377;&#22122;&#22768;&#21407;&#22411;&#26631;&#31614;&#30340;&#35821;&#20041;&#34920;&#31034;&#65292;&#23613;&#31649;&#23384;&#22312;&#35768;&#22810;&#8220;O&#8221;&#26631;&#31614;&#23454;&#20307;&#19982;&#26377;&#26631;&#31614;&#23454;&#20307;&#30456;&#20851;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21307;&#23398;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#22522;&#20110;&#21152;&#26435;&#21407;&#22411;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65288;W-PROCER&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20027;&#35201;&#22260;&#32469;&#26500;&#24314;&#22522;&#20110;&#21407;&#22411;&#30340;&#23545;&#27604;&#25439;&#22833;&#21644;&#21152;&#26435;&#32593;&#32476;&#23637;&#24320;&#12290;&#36825;&#20123;&#32452;&#20214;&#22312;&#21327;&#21161;&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#38754;&#21457;&#25381;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23558;W-PROCER&#24212;&#29992;&#20110;&#19968;&#20010;&#20844;&#20849;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#20248;&#24322;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning has become a popular solution for few-shot Name Entity Recognization (NER). The conventional configuration strives to reduce the distance between tokens with the same labels and increase the distance between tokens with different labels. The effect of this setup may, however, in the medical domain, there are a lot of entities annotated as OUTSIDE (O), and they are undesirably pushed apart to other entities that are not labeled as OUTSIDE (O) by the current contrastive learning method end up with a noisy prototype for the semantic representation of the label, though there are many OUTSIDE (O) labeled entities are relevant to the labeled entities. To address this challenge, we propose a novel method named Weighted Prototypical Contrastive Learning for Medical Few Shot Named Entity Recognization (W-PROCER). Our approach primarily revolves around constructing the prototype-based contractive loss and weighting network. These components play a crucial role in assisting t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PROPETL&#26041;&#27861;&#65292;&#36890;&#36807;&#21407;&#22411;&#32593;&#32476;&#21644;&#20108;&#36827;&#21046;&#25513;&#30721;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#21442;&#25968;&#20849;&#29992;&#36801;&#31227;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#22810;&#20219;&#21153;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23384;&#20648;&#31354;&#38388;&#21344;&#29992;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.17682</link><description>&lt;p&gt;
&#19968;&#32593;&#32476;&#65292;&#22810;&#20219;&#21153;&#65306;&#26356;&#39640;&#25928;&#30340;&#21442;&#25968;&#20849;&#29992;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
One Network, Many Masks: Towards More Parameter-Efficient Transfer Learning. (arXiv:2305.17682v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17682
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PROPETL&#26041;&#27861;&#65292;&#36890;&#36807;&#21407;&#22411;&#32593;&#32476;&#21644;&#20108;&#36827;&#21046;&#25513;&#30721;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#21442;&#25968;&#20849;&#29992;&#36801;&#31227;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#22810;&#20219;&#21153;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23384;&#20648;&#31354;&#38388;&#21344;&#29992;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22810;&#20010;&#20219;&#21153;&#26469;&#35828;&#65292;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#24448;&#24448;&#20250;&#21344;&#29992;&#22823;&#37327;&#23384;&#20648;&#31354;&#38388;&#65292;&#21442;&#25968;&#20849;&#29992;&#36801;&#31227;&#23398;&#20064;&#65288;PETL&#65289;&#26041;&#27861;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#22312;&#24212;&#29992;&#20110;&#26356;&#24191;&#27867;&#30340;&#20219;&#21153;&#33539;&#22260;&#26102;&#20173;&#38656;&#35201;&#22823;&#37327;&#30340;&#21442;&#25968;&#21644;&#23384;&#20648;&#31354;&#38388;&#12290;&#20026;&#20102;&#23454;&#29616;&#26356;&#22823;&#30340;&#23384;&#20648;&#31354;&#38388;&#20943;&#23569;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PROPETL&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#23618;&#21644;&#20219;&#21153;&#20043;&#38388;&#20351;&#29992;&#21333;&#20010;PETL&#27169;&#22359;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#21407;&#22411;&#32593;&#32476;&#65288;&#20363;&#22914;&#36866;&#37197;&#22120;&#12289;LoRA&#21644;&#21069;&#32512;&#35843;&#25972;&#65289;&#12290;&#25105;&#20204;&#28982;&#21518;&#23398;&#20064;&#20108;&#36827;&#21046;&#25513;&#30721;&#20197;&#20174;&#20849;&#20139;&#30340;&#21407;&#22411;&#32593;&#32476;&#20013;&#36873;&#25321;&#19981;&#21516;&#30340;&#23376;&#32593;&#32476;&#65292;&#24182;&#23558;&#23427;&#20204;&#20316;&#20026;PETL&#27169;&#22359;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#23618;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20108;&#36827;&#21046;&#25513;&#30721;&#21487;&#20197;&#30830;&#23450;&#32593;&#32476;&#20013;&#20851;&#38190;&#30340;&#20449;&#24687;&#65292;&#36825;&#22312;&#20197;&#21069;&#30340;&#30740;&#31350;&#20013;&#32463;&#24120;&#34987;&#24573;&#30053;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20063;&#21487;&#20197;&#30475;&#20316;&#26159;&#19968;&#31181;&#20462;&#21098;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#21363;&#20351;&#22312;&#30475;&#20284;&#24456;&#23567;&#30340;PETL&#27169;&#22359;&#20013;&#20063;&#23384;&#22312;&#36807;&#24230;&#21442;&#25968;&#21270;&#12290;&#25105;&#20204;&#23545;PROPETL&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning pre-trained language models for multiple tasks tends to be expensive in terms of storage. To mitigate this, parameter-efficient transfer learning (PETL) methods have been proposed to address this issue, but they still require a significant number of parameters and storage when being applied to broader ranges of tasks. To achieve even greater storage reduction, we propose PROPETL, a novel method that enables efficient sharing of a single PETL module which we call prototype network (e.g., adapter, LoRA, and prefix-tuning) across layers and tasks. We then learn binary masks to select different sub-networks from the shared prototype network and apply them as PETL modules into different layers. We find that the binary masks can determine crucial information from the network, which is often ignored in previous studies. Our work can also be seen as a type of pruning method, where we find that overparameterization also exists in the seemingly small PETL modules. We evaluate PROPETL
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#20195;&#30721;&#25628;&#32034;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#25351;&#20986;&#25915;&#20987;&#32773;&#21487;&#20197;&#27880;&#20837;&#21518;&#38376;&#26469;&#36820;&#22238;&#20855;&#26377;&#23433;&#20840;/&#38544;&#31169;&#38382;&#39064;&#30340;&#20195;&#30721;&#65292;&#25552;&#20986;&#20102;&#20960;&#31181;&#38450;&#24481;&#26426;&#21046;&#26469;&#32531;&#35299;&#36825;&#31181;&#23041;&#32961;&#65292;&#35813;&#24037;&#20316;&#31361;&#26174;&#20102;&#30740;&#31350;AI&#31995;&#32479;&#23433;&#20840;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#37096;&#32626;&#20110;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#26102;&#12290;</title><link>http://arxiv.org/abs/2305.17506</link><description>&lt;p&gt;
&#31070;&#32463;&#20195;&#30721;&#25628;&#32034;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Backdooring Neural Code Search. (arXiv:2305.17506v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#20195;&#30721;&#25628;&#32034;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#25351;&#20986;&#25915;&#20987;&#32773;&#21487;&#20197;&#27880;&#20837;&#21518;&#38376;&#26469;&#36820;&#22238;&#20855;&#26377;&#23433;&#20840;/&#38544;&#31169;&#38382;&#39064;&#30340;&#20195;&#30721;&#65292;&#25552;&#20986;&#20102;&#20960;&#31181;&#38450;&#24481;&#26426;&#21046;&#26469;&#32531;&#35299;&#36825;&#31181;&#23041;&#32961;&#65292;&#35813;&#24037;&#20316;&#31361;&#26174;&#20102;&#30740;&#31350;AI&#31995;&#32479;&#23433;&#20840;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#37096;&#32626;&#20110;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#22312;&#32447;&#20195;&#30721;&#24211;&#20013;&#37325;&#22797;&#20351;&#29992;&#29616;&#25104;&#20195;&#30721;&#26159;&#24120;&#35265;&#30340;&#20570;&#27861;&#65292;&#23427;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#36719;&#20214;&#24320;&#21457;&#20154;&#21592;&#30340;&#29983;&#20135;&#21147;&#12290;&#35201;&#26597;&#25214;&#25152;&#38656;&#30340;&#20195;&#30721;&#29255;&#27573;&#65292;&#24320;&#21457;&#20154;&#21592;&#21017;&#35201;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#20351;&#29992;&#20195;&#30721;&#25628;&#32034;&#24341;&#25806;&#12290;&#22240;&#27492;&#65292;&#22312;&#35768;&#22810;&#36825;&#26679;&#30340;&#25628;&#32034;&#24341;&#25806;&#21518;&#38754;&#65292;&#37117;&#26159;&#31070;&#32463;&#20195;&#30721;&#25628;&#32034;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#65292;&#22240;&#20854;&#20986;&#33394;&#30340;&#24615;&#33021;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#24456;&#23569;&#34987;&#30740;&#31350;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25915;&#20987;&#32773;&#21487;&#33021;&#20250;&#22312;&#31070;&#32463;&#20195;&#30721;&#25628;&#32034;&#27169;&#22411;&#20013;&#27880;&#20837;&#21518;&#38376;&#65292;&#20174;&#32780;&#36820;&#22238;&#20855;&#26377;&#23433;&#20840;/&#38544;&#31169;&#38382;&#39064;&#30340;&#38169;&#35823;&#25110;&#29978;&#33267;&#26131;&#21463;&#25915;&#20987;&#30340;&#20195;&#30721;&#12290;&#36825;&#21487;&#33021;&#20250;&#24433;&#21709;&#19979;&#28216;&#36719;&#20214;&#65288;&#20363;&#22914;&#32929;&#31080;&#20132;&#26131;&#31995;&#32479;&#21644;&#33258;&#21160;&#39550;&#39542;&#65289;&#65292;&#24182;&#36896;&#25104;&#36130;&#21153;&#25439;&#22833;&#21644;/&#25110;&#21361;&#21450;&#29983;&#21629;&#30340;&#20107;&#20214;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#26679;&#30340;&#25915;&#20987;&#26159;&#21487;&#34892;&#30340;&#65292;&#24182;&#19988;&#21487;&#33021;&#38750;&#24120;&#38544;&#34109;&#12290;&#36890;&#36807;&#31616;&#21333;&#22320;&#20462;&#25913;&#19968;&#20010;&#21464;&#37327;/&#20989;&#25968;&#21517;&#31216;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#20351;&#20986;&#29616;&#38169;&#35823;/&#26131;&#21463;&#25915;&#20987;&#30340;&#20195;&#30721;&#25490;&#21517;&#21069;11&#65285;&#12290;&#25105;&#20204;&#30340;&#25915;&#20987;&#21033;&#29992;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#20877;&#35757;&#32451;&#24615;&#21644;&#20195;&#30721;&#30456;&#20284;&#24230;&#24230;&#37327;&#30340;&#23485;&#26494;&#24615;&#65292;&#20197;&#27880;&#20837;&#21518;&#38376;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20960;&#31181;&#38450;&#24481;&#26426;&#21046;&#26469;&#32531;&#35299;&#36825;&#31181;&#23041;&#32961;&#65292;&#20363;&#22914;&#28155;&#21152;&#23545;&#25239;&#24615;&#35757;&#32451;&#25968;&#25454;&#21644;&#29305;&#24449;&#21387;&#32553;&#12290;&#25105;&#20204;&#30456;&#20449;&#25105;&#20204;&#30340;&#24037;&#20316;&#31361;&#26174;&#20102;&#30740;&#31350;AI&#31995;&#32479;&#23433;&#20840;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#37096;&#32626;&#20110;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reusing off-the-shelf code snippets from online repositories is a common practice, which significantly enhances the productivity of software developers. To find desired code snippets, developers resort to code search engines through natural language queries. Neural code search models are hence behind many such engines. These models are based on deep learning and gain substantial attention due to their impressive performance. However, the security aspect of these models is rarely studied. Particularly, an adversary can inject a backdoor in neural code search models, which return buggy or even vulnerable code with security/privacy issues. This may impact the downstream software (e.g., stock trading systems and autonomous driving) and cause financial loss and/or life-threatening incidents. In this paper, we demonstrate such attacks are feasible and can be quite stealthy. By simply modifying one variable/function name, the attacker can make buggy/vulnerable code rank in the top 11%. Our at
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#33258;&#21160;&#35780;&#20272;&#24230;&#37327;&#26041;&#27861;&#65288;CMN&#65289;&#65292;&#33021;&#22815;&#36890;&#36807;&#23558;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;CVAEs&#65289;&#19982;&#19979;&#19968;&#21477;&#39044;&#27979;&#65288;NSP&#65289;&#30446;&#26631;&#30456;&#32467;&#21512;&#65292;&#24182;&#21033;&#29992;&#20114;&#20449;&#24687;&#65288;MI&#65289;&#22312;&#28508;&#31354;&#38388;&#20013;&#24314;&#27169;&#25991;&#26412;&#30340;&#35821;&#20041;&#30456;&#20284;&#24230;&#65292;&#26469;&#40065;&#26834;&#22320;&#35780;&#20272;&#24320;&#25918;&#22495;&#23545;&#35805;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.16967</link><description>&lt;p&gt;
&#29992;&#19979;&#19968;&#21477;&#39044;&#27979;&#21644;&#20114;&#20449;&#24687;&#22312;&#28508;&#31354;&#38388;&#20013;&#35780;&#20272;&#24320;&#25918;&#22495;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
Evaluating Open-Domain Dialogues in Latent Space with Next Sentence Prediction and Mutual Information. (arXiv:2305.16967v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#33258;&#21160;&#35780;&#20272;&#24230;&#37327;&#26041;&#27861;&#65288;CMN&#65289;&#65292;&#33021;&#22815;&#36890;&#36807;&#23558;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;CVAEs&#65289;&#19982;&#19979;&#19968;&#21477;&#39044;&#27979;&#65288;NSP&#65289;&#30446;&#26631;&#30456;&#32467;&#21512;&#65292;&#24182;&#21033;&#29992;&#20114;&#20449;&#24687;&#65288;MI&#65289;&#22312;&#28508;&#31354;&#38388;&#20013;&#24314;&#27169;&#25991;&#26412;&#30340;&#35821;&#20041;&#30456;&#20284;&#24230;&#65292;&#26469;&#40065;&#26834;&#22320;&#35780;&#20272;&#24320;&#25918;&#22495;&#23545;&#35805;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#22495;&#23545;&#35805;&#20013;&#30340;&#19968;&#23545;&#22810;&#38382;&#39064;&#20351;&#24471;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#33258;&#21160;&#35780;&#20272;&#24230;&#37327;&#26041;&#27861;&#65288;CMN&#65289;&#65292;&#36890;&#36807;&#23558;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;CVAEs&#65289;&#19982;&#19979;&#19968;&#21477;&#39044;&#27979;&#65288;NSP&#65289;&#30446;&#26631;&#30456;&#32467;&#21512;&#65292;&#24182;&#21033;&#29992;&#20114;&#20449;&#24687;&#65288;MI&#65289;&#22312;&#28508;&#31354;&#38388;&#20013;&#24314;&#27169;&#25991;&#26412;&#30340;&#35821;&#20041;&#30456;&#20284;&#24230;&#65292;&#23454;&#29616;&#20102;&#23545;&#24320;&#25918;&#22495;&#23545;&#35805;&#30340;&#40065;&#26834;&#35780;&#20272;&#12290;&#22312;&#20004;&#20010;&#24320;&#25918;&#22495;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#24191;&#27867;&#30340;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26126;&#26174;&#30340;&#20248;&#36234;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#35821;&#20041;&#19978;&#36828;&#31163;&#40644;&#37329;&#21442;&#32771;&#22238;&#31572;&#30340;&#21709;&#24212;&#26102;&#26356;&#20026;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
The long-standing one-to-many issue of the open-domain dialogues poses significant challenges for automatic evaluation methods, i.e., there may be multiple suitable responses which differ in semantics for a given conversational context. To tackle this challenge, we propose a novel learning-based automatic evaluation metric (CMN), which can robustly evaluate open-domain dialogues by augmenting Conditional Variational Autoencoders (CVAEs) with a Next Sentence Prediction (NSP) objective and employing Mutual Information (MI) to model the semantic similarity of text in the latent space. Experimental results on two open-domain dialogue datasets demonstrate the superiority of our method compared with a wide range of baselines, especially in handling responses which are distant to the golden reference responses in semantics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#29992;&#22522;&#20110;&#34920;&#38754;&#32423;&#21035;&#30340;&#26816;&#32034;&#26426;&#21046;&#21462;&#20195;&#35821;&#20041;&#26816;&#32034;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#22256;&#24785;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.16243</link><description>&lt;p&gt;
&#22522;&#20110;&#34920;&#38754;&#30340;&#26816;&#32034;&#38477;&#20302;&#20102;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#22256;&#24785;&#24230;
&lt;/p&gt;
&lt;p&gt;
Surface-Based Retrieval Reduces Perplexity of Retrieval-Augmented Language Models. (arXiv:2305.16243v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16243
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#29992;&#22522;&#20110;&#34920;&#38754;&#32423;&#21035;&#30340;&#26816;&#32034;&#26426;&#21046;&#21462;&#20195;&#35821;&#20041;&#26816;&#32034;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#22256;&#24785;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#35777;&#26126;&#65292;&#36890;&#36807;&#26816;&#32034;&#26426;&#21046;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#21442;&#25968;&#25968;&#37327;&#36739;&#20302;&#12290;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#36890;&#24120;&#20381;&#38752;&#22522;&#20110;&#26597;&#35810;&#22359;&#21644;&#28508;&#22312;&#37051;&#23621;&#20043;&#38388;&#30340;&#23494;&#38598;&#34920;&#31034;&#30456;&#20284;&#24615;&#30340;&#35821;&#20041;&#26816;&#32034;&#26426;&#21046;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#20808;&#36827;&#30340;Retro&#27169;&#22411;&#65292;&#24182;&#35266;&#23519;&#21040;&#20854;&#24615;&#33021;&#25552;&#21319;&#26356;&#22909;&#22320;&#35299;&#37322;&#20026;&#22522;&#20110;&#34920;&#38754;&#32423;&#21035;&#30340;&#30456;&#20284;&#24615;&#65292;&#20363;&#22914;&#26631;&#35760;&#37325;&#21472;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#29992;BM25&#26367;&#25442;Retro&#20013;&#30340;&#35821;&#20041;&#26816;&#32034;&#65292;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#22256;&#24785;&#24230;&#38477;&#20302;&#12290;&#30001;&#20110;&#23436;&#25972;&#30340;BM25&#26816;&#32034;&#21487;&#33021;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#35745;&#31639;&#25104;&#26412;&#65292;&#22240;&#27492;&#25105;&#20204;&#36824;&#23558;&#20854;&#24212;&#29992;&#20110;&#37325;&#26032;&#25490;&#21517;&#22330;&#26223;&#20013;&#65292;&#20197;&#26368;&#23567;&#30340;&#35745;&#31639;&#24320;&#38144;&#33719;&#24471;&#37096;&#20998;&#22256;&#24785;&#24230;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Augmenting language models with a retrieval mechanism has been shown to significantly improve their performance while keeping the number of parameters low. Retrieval-augmented models commonly rely on a semantic retrieval mechanism based on the similarity between dense representations of the query chunk and potential neighbors. In this paper, we study the state-of-the-art Retro model and observe that its performance gain is better explained by surface-level similarities, such as token overlap. Inspired by this, we replace the semantic retrieval in Retro with a surface-level method based on BM25, obtaining a significant reduction in perplexity. As full BM25 retrieval can be computationally costly for large datasets, we also apply it in a re-ranking scenario, gaining part of the perplexity reduction with minimal computational overhead.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#35299;&#37322;&#25552;&#39640;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#31995;&#32479;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21253;&#21547;&#35777;&#25454;&#21333;&#35789;&#21644;&#35821;&#27861;&#38169;&#35823;&#31867;&#22411;&#27880;&#37322;&#30340;&#22823;&#25968;&#25454;&#38598;&#65292;&#25214;&#21040;&#38169;&#35823;&#30340;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#20010;&#22522;&#32447;&#21644;&#20998;&#26512;&#26041;&#27861;&#26469;&#29702;&#35299;&#36825;&#20010;&#20219;&#21153;&#65292;&#21516;&#26102;&#20063;&#35777;&#26126;&#20102;&#35299;&#37322;&#21487;&#20197;&#24110;&#21161;&#31532;&#20108;&#35821;&#35328;&#23398;&#20064;&#32773;&#26356;&#22909;&#22320;&#29702;&#35299;&#35821;&#27861;&#35268;&#21017;&#12290;</title><link>http://arxiv.org/abs/2305.15676</link><description>&lt;p&gt;
&#29992;&#35299;&#37322;&#25552;&#39640;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#31995;&#32479;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Enhancing Grammatical Error Correction Systems with Explanations. (arXiv:2305.15676v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15676
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#35299;&#37322;&#25552;&#39640;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#31995;&#32479;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21253;&#21547;&#35777;&#25454;&#21333;&#35789;&#21644;&#35821;&#27861;&#38169;&#35823;&#31867;&#22411;&#27880;&#37322;&#30340;&#22823;&#25968;&#25454;&#38598;&#65292;&#25214;&#21040;&#38169;&#35823;&#30340;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#20010;&#22522;&#32447;&#21644;&#20998;&#26512;&#26041;&#27861;&#26469;&#29702;&#35299;&#36825;&#20010;&#20219;&#21153;&#65292;&#21516;&#26102;&#20063;&#35777;&#26126;&#20102;&#35299;&#37322;&#21487;&#20197;&#24110;&#21161;&#31532;&#20108;&#35821;&#35328;&#23398;&#20064;&#32773;&#26356;&#22909;&#22320;&#29702;&#35299;&#35821;&#27861;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#27861;&#26657;&#27491;&#31995;&#32479;&#36890;&#36807;&#26816;&#27979;&#21644;&#26356;&#27491;&#35821;&#35328;&#38169;&#35823;&#26469;&#25552;&#21319;&#20070;&#20889;&#20132;&#27969;&#12290;&#20026;&#20102;&#24110;&#21161;&#35821;&#35328;&#23398;&#20064;&#32773;&#26356;&#22909;&#22320;&#29702;&#35299;GEC&#31995;&#32479;&#20026;&#20160;&#20040;&#20570;&#20986;&#26576;&#31181;&#26356;&#27491;&#65292;&#38169;&#35823;&#30340;&#21407;&#22240;&#65288;&#35777;&#25454;&#21333;&#35789;&#65289;&#21644;&#30456;&#24212;&#30340;&#38169;&#35823;&#31867;&#22411;&#26159;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#12290;&#20026;&#20102;&#29992;&#35299;&#37322;&#22686;&#24378;GEC&#31995;&#32479;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;EXPECT&#65292;&#19968;&#20010;&#22823;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#35777;&#25454;&#21333;&#35789;&#21644;&#35821;&#27861;&#38169;&#35823;&#31867;&#22411;&#30340;&#27880;&#37322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#22522;&#32447;&#21644;&#20998;&#26512;&#26041;&#27861;&#26469;&#29702;&#35299;&#36825;&#20010;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#20154;&#31867;&#35780;&#20272;&#35777;&#26126;&#65292;&#25105;&#20204;&#21487;&#35299;&#37322;&#30340;GEC&#31995;&#32479;&#30340;&#35299;&#37322;&#33021;&#22815;&#24110;&#21161;&#31532;&#20108;&#35821;&#35328;&#23398;&#20064;&#32773;&#30830;&#23450;&#26159;&#21542;&#25509;&#21463;&#26356;&#27491;&#24314;&#35758;&#65292;&#24182;&#29702;&#35299;&#30456;&#20851;&#30340;&#35821;&#27861;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
Grammatical error correction systems improve written communication by detecting and correcting language mistakes. To help language learners better understand why the GEC system makes a certain correction, the causes of errors (evidence words) and the corresponding error types are two key factors. To enhance GEC systems with explanations, we introduce EXPECT, a large dataset annotated with evidence words and grammatical error types. We propose several baselines and anlysis to understand this task. Furthermore, human evaluation verifies our explainable GEC system's explanations can assist second-language learners in determining whether to accept a correction suggestion and in understanding the associated grammar rule.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#27169;&#31946;&#27979;&#35797;&#33719;&#21462;&#20195;&#34920;&#24615;&#36755;&#20837;&#26469;&#24110;&#21161;&#35821;&#20041;&#29702;&#35299;&#31243;&#24207;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.13592</link><description>&lt;p&gt;
&#21033;&#29992;&#65288;&#27169;&#31946;&#27979;&#35797;&#65289;&#27979;&#35797;&#29992;&#20363;&#26469;&#29702;&#35299;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
Understanding Programs by Exploiting (Fuzzing) Test Cases. (arXiv:2305.13592v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#27169;&#31946;&#27979;&#35797;&#33719;&#21462;&#20195;&#34920;&#24615;&#36755;&#20837;&#26469;&#24110;&#21161;&#35821;&#20041;&#29702;&#35299;&#31243;&#24207;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31243;&#24207;&#30340;&#35821;&#20041;&#29702;&#35299;&#24341;&#36215;&#20102;&#31038;&#21306;&#30340;&#26497;&#22823;&#20851;&#27880;&#12290;&#21463;&#21040;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26368;&#36817;&#25104;&#21151;&#21551;&#21457;&#65292;&#36890;&#36807;&#23558;&#32534;&#31243;&#35821;&#35328;&#35270;&#20026;&#21478;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#65292;&#24182;&#22312;&#31243;&#24207;&#20195;&#30721;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;LLM&#65292;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#31243;&#24207;&#27605;&#31455;&#19982;&#25991;&#26412;&#26377;&#26412;&#36136;&#30340;&#21306;&#21035;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#20855;&#26377;&#20005;&#26684;&#30340;&#32467;&#26500;&#21644;&#35821;&#27861;&#12290;&#29305;&#21035;&#26159;&#65292;&#31243;&#24207;&#21450;&#20854;&#22522;&#26412;&#21333;&#20803;&#65288;&#21363;&#20989;&#25968;&#21644;&#23376;&#31243;&#24207;&#65289;&#26088;&#22312;&#23637;&#31034;&#21508;&#31181;&#34892;&#20026;&#21644;/&#25110;&#25552;&#20379;&#21487;&#33021;&#30340;&#36755;&#20986;&#65292;&#32473;&#23450;&#19981;&#21516;&#30340;&#36755;&#20837;&#12290;&#36755;&#20837;&#21644;&#21487;&#33021;&#30340;&#36755;&#20986;/&#34892;&#20026;&#20043;&#38388;&#30340;&#20851;&#31995;&#34920;&#31034;&#20989;&#25968;/&#23376;&#31243;&#24207;&#65292;&#24182;&#27010;&#36848;&#20102;&#25972;&#20010;&#31243;&#24207;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#36825;&#31181;&#20851;&#31995;&#32435;&#20837;&#23398;&#20064;&#20013;&#65292;&#20197;&#23454;&#29616;&#23545;&#31243;&#24207;&#30340;&#26356;&#28145;&#20837;&#35821;&#20041;&#29702;&#35299;&#12290;&#20026;&#20102;&#33719;&#24471;&#36275;&#22815;&#20195;&#34920;&#24615;&#30340;&#36755;&#20837;&#20197;&#35302;&#21457;&#22823;&#37327;&#25191;&#34892;&#65292;&#21487;&#20197;&#20351;&#29992;&#27169;&#31946;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic understanding of programs has attracted great attention in the community. Inspired by recent successes of large language models (LLMs) in natural language understanding, tremendous progress has been made by treating programming language as another sort of natural language and training LLMs on corpora of program code. However, programs are essentially different from texts after all, in a sense that they are normally heavily structured and syntax-strict. In particular, programs and their basic units (i.e., functions and subroutines) are designed to demonstrate a variety of behaviors and/or provide possible outputs, given different inputs. The relationship between inputs and possible outputs/behaviors represents the functions/subroutines and profiles the program as a whole. Therefore, we propose to incorporate such a relationship into learning, for achieving a deeper semantic understanding of programs. To obtain inputs that are representative enough to trigger the execution of mo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#29992;&#20110;&#35789;&#20041;&#28040;&#27495;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#21457;&#29616;&#20256;&#32479;&#39044;&#27979;&#27010;&#29575;&#19981;&#36275;&#20197;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#21516;&#26102;&#21457;&#29616;&#27169;&#22411;&#20196;&#20154;&#28385;&#24847;&#22320;&#21453;&#26144;&#20102;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#20294;&#26159;&#20302;&#20272;&#20102;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13119</link><description>&lt;p&gt;
&#35821;&#20041;&#27495;&#20041;&#36935;&#19978;&#19981;&#30830;&#23450;&#24615;&#65306;&#25506;&#31350;&#29992;&#20110;&#35789;&#20041;&#28040;&#27495;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Ambiguity Meets Uncertainty: Investigating Uncertainty Estimation for Word Sense Disambiguation. (arXiv:2305.13119v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#29992;&#20110;&#35789;&#20041;&#28040;&#27495;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#21457;&#29616;&#20256;&#32479;&#39044;&#27979;&#27010;&#29575;&#19981;&#36275;&#20197;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#21516;&#26102;&#21457;&#29616;&#27169;&#22411;&#20196;&#20154;&#28385;&#24847;&#22320;&#21453;&#26144;&#20102;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#20294;&#26159;&#20302;&#20272;&#20102;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#20041;&#28040;&#27495;(WSD)&#26159;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20013;&#38750;&#24120;&#37325;&#35201;&#30340;&#19968;&#29615;&#65292;&#23427;&#36890;&#36807;&#32473;&#23450;&#19968;&#20010;&#21477;&#23376;&#20013;&#19968;&#20010;&#30446;&#26631;&#35789;&#30340;&#19978;&#19979;&#25991;&#65292;&#30830;&#23450;&#20854;&#24688;&#24403;&#30340;&#35789;&#20041;&#12290;&#29616;&#26377;&#30340;&#30417;&#30563;&#26041;&#27861;&#23558;WSD&#35270;&#20026;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#29615;&#22659;&#20013;&#24573;&#30053;&#20102;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65288;UE&#65289;&#65292;&#22312;&#36825;&#31181;&#29615;&#22659;&#19979;&#65292;&#25968;&#25454;&#24635;&#26159;&#22024;&#26434;&#30340;&#65292;&#36229;&#20986;&#20102;&#20998;&#24067;&#33539;&#22260;&#12290;&#26412;&#25991;&#22312;&#19987;&#38376;&#20026;WSD&#35774;&#35745;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#24191;&#27867;&#30740;&#31350;&#20102;UE&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#27604;&#36739;&#20102;&#22235;&#31181;&#29992;&#20110;&#26368;&#20808;&#36827;WSD&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#24471;&#20998;&#65292;&#39564;&#35777;&#20102;&#27169;&#22411;&#26411;&#31471;&#33719;&#24471;&#30340;&#20256;&#32479;&#39044;&#27979;&#27010;&#29575;&#19981;&#36275;&#20197;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#25317;&#26377;&#25152;&#36873;UE&#24471;&#20998;&#30340;&#27169;&#22411;&#22312;&#35774;&#35745;&#33391;&#22909;&#30340;&#27979;&#35797;&#22330;&#26223;&#20013;&#26816;&#27979;&#25429;&#33719;&#25968;&#25454;&#21644;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#27169;&#22411;&#20196;&#20154;&#28385;&#24847;&#22320;&#21453;&#26144;&#20102;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#65292;&#20294;&#20302;&#20272;&#20102;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#22823;&#37327;&#35789;&#27719;&#23646;&#24615;&#65292;&#20197;&#26816;&#26597;&#36825;&#20123;&#23646;&#24615;&#26159;&#21542;&#26377;&#26395;&#20316;&#20026;UE&#30340;&#25351;&#23548;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Word sense disambiguation (WSD), which aims to determine an appropriate sense for a target word given its context, is crucial for natural language understanding. Existing supervised methods treat WSD as a classification task and have achieved remarkable performance. However, they ignore uncertainty estimation (UE) in the real-world setting, where the data is always noisy and out of distribution. This paper extensively studies UE on the benchmark designed for WSD. Specifically, we first compare four uncertainty scores for a state-of-the-art WSD model and verify that the conventional predictive probabilities obtained at the end of the model are inadequate to quantify uncertainty. Then, we examine the capability of capturing data and model uncertainties by the model with the selected UE score on well-designed test scenarios and discover that the model reflects data uncertainty satisfactorily but underestimates model uncertainty. Furthermore, we explore numerous lexical properties that int
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;&#22522;&#20110;&#25552;&#31034;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#22312;&#23398;&#26415;&#30693;&#35782;&#22270;&#35889;&#23545;&#35937;&#39044;&#27979;&#39046;&#22495;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;transformers&#27169;&#22411;&#30340;&#35780;&#20272;&#21644;&#27979;&#35797;&#65292;&#21457;&#29616;&#25552;&#31034;&#30340;&#20351;&#29992;&#21487;&#20197;&#25913;&#36827;pre-trained transformers&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.12900</link><description>&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#38382;&#39064;&#22238;&#31572;&#24212;&#29992;&#20110;&#24320;&#25918;&#30740;&#31350;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#23545;&#35937;&#39044;&#27979;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluating Prompt-based Question Answering for Object Prediction in the Open Research Knowledge Graph. (arXiv:2305.12900v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#22522;&#20110;&#25552;&#31034;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#22312;&#23398;&#26415;&#30693;&#35782;&#22270;&#35889;&#23545;&#35937;&#39044;&#27979;&#39046;&#22495;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;transformers&#27169;&#22411;&#30340;&#35780;&#20272;&#21644;&#27979;&#35797;&#65292;&#21457;&#29616;&#25552;&#31034;&#30340;&#20351;&#29992;&#21487;&#20197;&#25913;&#36827;pre-trained transformers&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#20110;&#22522;&#20110;&#25552;&#31034;&#30340;&#35757;&#32451;&#26041;&#27861;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#23545;&#36716;&#25442;&#22120;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26032;&#25991;&#26412;&#20307;&#35009;&#35757;&#32451;&#30340;&#35843;&#26597;&#26377;&#24456;&#22810;&#12290;&#21457;&#29616;&#22522;&#20110;&#25552;&#31034;&#30340;&#35757;&#32451;&#26041;&#27861;&#23545;&#20110;&#36890;&#29992;&#39044;&#35757;&#32451;&#25110;&#24494;&#35843;&#27169;&#22411;&#20197;&#36866;&#24212;&#36164;&#28304;&#32570;&#20047;&#30340;&#29615;&#22659;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;&#26412;&#30740;&#31350;&#39318;&#27425;&#25253;&#36947;&#20102;&#37319;&#29992;&#22522;&#20110;&#25552;&#31034;&#35757;&#32451;transformers&#36827;&#34892;&#8220;&#23398;&#26415;&#30693;&#35782;&#22270;&#35889;&#23545;&#35937;&#39044;&#27979;&#8221;&#30340;&#32467;&#26524;&#12290;&#35813;&#30740;&#31350;&#20855;&#26377;&#20197;&#19979;&#20004;&#20010;&#20027;&#35201;&#29305;&#28857;&#12290;1&#65289;&#23427;&#20559;&#31163;&#20102;&#20854;&#20182;&#25552;&#20986;&#29992;&#20110;&#39044;&#27979;&#23398;&#26415;&#30693;&#35782;&#22270;&#35889;&#23545;&#35937;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#25552;&#21462;&#27969;&#31243;&#30340;&#30740;&#31350;&#12290;2&#65289;&#22312;&#20854;&#20182;&#30740;&#31350;&#20013;&#27979;&#35797;&#20102;&#35813;&#26041;&#27861;&#23545;&#20110;&#19982;&#36890;&#29992;&#30693;&#35782;&#39046;&#22495;&#30456;&#23545;&#25509;&#36817;&#30340;&#25991;&#26412;&#20307;&#35009;&#65292;&#32780;&#25105;&#20204;&#27979;&#35797;&#20102;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#26174;&#33879;&#19981;&#21516;&#30340;&#23398;&#26415;&#30693;&#35782;&#39046;&#22495;&#65292;&#20174;&#32780;&#27979;&#35797;&#36825;&#20123;&#22823;&#35268;&#27169;transformers&#27169;&#22411;&#30340;&#35821;&#35328;&#65292;&#27010;&#29575;&#21644;&#20107;&#23454;&#30340;&#26222;&#36866;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65288;i&#65289;&#31526;&#21512;&#39044;&#26399;&#65292;&#20351;&#29992;&#25552;&#31034;&#36827;&#34892;&#24494;&#35843;&#30340;transformers&#20248;&#20110;&#22522;&#32447;&#65307;&#65288;ii&#65289;&#19982;&#20808;&#21069;&#30740;&#31350;&#20013;&#30475;&#21040;&#30340;&#27169;&#24335;&#19981;&#21516;&#65292;&#39044;&#20808;&#35757;&#32451;&#30340;transformers&#24182;&#19981;&#33021;&#22987;&#32456;&#36275;&#20197;&#32988;&#20219;&#23398;&#26415;&#23545;&#35937;&#39044;&#27979;&#30340;&#20219;&#21153;&#65292;&#32467;&#26524;&#34920;&#26126;&#25552;&#31034;&#30830;&#23454;&#26377;&#21161;&#20110;&#25913;&#36827;&#25277;&#21462;&#39044;&#35757;&#32451;&#27169;&#22411;&#25152;&#33719;&#24471;&#30340;&#35821;&#20041;&#20449;&#24687;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
There have been many recent investigations into prompt-based training of transformer language models for new text genres in low-resource settings. The prompt-based training approach has been found to be effective in generalizing pre-trained or fine-tuned models for transfer to resource-scarce settings. This work, for the first time, reports results on adopting prompt-based training of transformers for \textit{scholarly knowledge graph object prediction}. The work is unique in the following two main aspects. 1) It deviates from the other works proposing entity and relation extraction pipelines for predicting objects of a scholarly knowledge graph. 2) While other works have tested the method on text genera relatively close to the general knowledge domain, we test the method for a significantly different domain, i.e. scholarly knowledge, in turn testing the linguistic, probabilistic, and factual generalizability of these large-scale transformer models. We find that (i) per expectations, t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#35268;&#21010;&#65288;NLP&#65289;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#30740;&#31350;LLMs&#22312;&#38656;&#35201;&#29702;&#35299;&#24182;&#22312;&#25991;&#26412;&#20013;&#30456;&#24212;&#36827;&#34892;&#25805;&#20316;&#30340;&#22797;&#26434;&#35268;&#21010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;CoS&#65292;&#20351;&#29992;&#31616;&#21270;&#30340;&#31526;&#21495;&#31354;&#38388;&#34920;&#31034;&#27861;&#26469;&#34920;&#31034;&#22797;&#26434;&#30340;&#29615;&#22659;&#12290;</title><link>http://arxiv.org/abs/2305.10276</link><description>&lt;p&gt;
&#36830;&#38145;&#31526;&#21495;&#25552;&#31034;&#28608;&#21457;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35268;&#21010;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models. (arXiv:2305.10276v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#35268;&#21010;&#65288;NLP&#65289;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#30740;&#31350;LLMs&#22312;&#38656;&#35201;&#29702;&#35299;&#24182;&#22312;&#25991;&#26412;&#20013;&#30456;&#24212;&#36827;&#34892;&#25805;&#20316;&#30340;&#22797;&#26434;&#35268;&#21010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;CoS&#65292;&#20351;&#29992;&#31616;&#21270;&#30340;&#31526;&#21495;&#31354;&#38388;&#34920;&#31034;&#27861;&#26469;&#34920;&#31034;&#22797;&#26434;&#30340;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;LLMs&#22312;&#38656;&#35201;&#29702;&#35299;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#27169;&#25311;&#30340;&#34394;&#25311;&#31354;&#38388;&#29615;&#22659;&#24182;&#22312;&#25991;&#26412;&#20013;&#30456;&#24212;&#36827;&#34892;&#25805;&#20316;&#30340;&#22797;&#26434;&#35268;&#21010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#33258;&#28982;&#35821;&#35328;&#35268;&#21010;&#65288;NLP&#65289;&#30340;&#22522;&#20934;&#65292;&#23427;&#30001;&#19968;&#32452;&#26032;&#39062;&#30340;&#20219;&#21153;&#32452;&#25104;&#65306;Brick World&#12289;&#22522;&#20110;NLVR&#30340;&#25805;&#20316;&#21644;&#33258;&#28982;&#35821;&#35328;&#23548;&#33322;&#12290;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#27969;&#34892;&#30340;LLMs&#65288;&#22914;ChatGPT&#65289;&#20173;&#28982;&#32570;&#20047;&#22797;&#26434;&#35268;&#21010;&#30340;&#33021;&#21147;&#12290;&#36825;&#24341;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#8212;&#8212;LLMs&#26159;&#21542;&#23545;&#33258;&#28982;&#35821;&#35328;&#20013;&#25551;&#36848;&#30340;&#29615;&#22659;&#26377;&#33391;&#22909;&#30340;&#29702;&#35299;&#65292;&#25110;&#32773;&#20854;&#20182;&#26367;&#20195;&#26041;&#27861;&#65288;&#22914;&#31526;&#21495;&#34920;&#31034;&#65289;&#26159;&#21542;&#26356;&#21152;&#31616;&#21333;&#65292;&#22240;&#27492;&#26356;&#23481;&#26131;&#34987;LLMs&#29702;&#35299;&#65311;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CoS&#65288;Chain-of-Symbol Prompting&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#38142;&#24335;&#20013;&#38388;&#24605;&#32771;&#27493;&#39588;&#20013;&#20351;&#29992;&#31616;&#21270;&#30340;&#31526;&#21495;&#31354;&#38388;&#34920;&#31034;&#27861;&#26469;&#34920;&#31034;&#22797;&#26434;&#30340;&#29615;&#22659;&#12290;CoS&#26131;&#20110;&#20351;&#29992;&#65292;&#19981;&#38656;&#35201;&#23545;LLMs&#36827;&#34892;&#39069;&#22806;&#30340;&#22521;&#35757;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we take the initiative to investigate the performance of LLMs on complex planning tasks that require LLMs to understand a virtual spatial environment simulated via natural language and act correspondingly in text. We propose a benchmark named Natural Language Planning (NLP) composed of a set of novel tasks: Brick World, NLVR-based Manipulations, and Natural Language Navigation. We found that current popular LLMs such as ChatGPT still lack abilities in complex planning. This arises a question -- do the LLMs have a good understanding of the environments described in natural language, or maybe other alternatives such as symbolic representations are neater and hence better to be understood by LLMs? To this end, we propose a novel method called CoS (Chain-of-Symbol Prompting) that represents the complex environments with condensed symbolic spatial representations during the chained intermediate thinking steps. CoS is easy to use and does not need additional training on LLMs. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35821;&#35328;&#25552;&#31034;&#19979;&#30340;&#22870;&#21169;&#26102;&#38388;&#20559;&#22909;&#65292;&#24182;&#21457;&#29616;GPT&#22312;&#20855;&#26377;&#36739;&#24369;&#26410;&#26469;&#26102;&#24577;&#30340;&#35821;&#35328;&#19979;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#32784;&#24515;&#65292;&#36825;&#19982;&#20351;&#29992;&#35813;&#35821;&#35328;&#30340;&#20154;&#31867;&#30340;&#20559;&#22909;&#30456;&#20284;&#12290;</title><link>http://arxiv.org/abs/2305.02531</link><description>&lt;p&gt;
&#35821;&#35328;&#12289;&#26102;&#38388;&#20559;&#22909;&#21644;&#28040;&#36153;&#34892;&#20026;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Language, Time Preferences, and Consumer Behavior: Evidence from Large Language Models. (arXiv:2305.02531v1 [econ.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35821;&#35328;&#25552;&#31034;&#19979;&#30340;&#22870;&#21169;&#26102;&#38388;&#20559;&#22909;&#65292;&#24182;&#21457;&#29616;GPT&#22312;&#20855;&#26377;&#36739;&#24369;&#26410;&#26469;&#26102;&#24577;&#30340;&#35821;&#35328;&#19979;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#32784;&#24515;&#65292;&#36825;&#19982;&#20351;&#29992;&#35813;&#35821;&#35328;&#30340;&#20154;&#31867;&#30340;&#20559;&#22909;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#23545;&#25105;&#20204;&#23545;&#26102;&#38388;&#21644;&#22870;&#21169;&#30340;&#24863;&#30693;&#26377;&#24456;&#22823;&#30340;&#24433;&#21709;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#24403;&#20197;&#19981;&#21516;&#30340;&#35821;&#35328;&#35810;&#38382;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#23427;&#20204;&#26159;&#21542;&#26174;&#31034;&#20986;&#19981;&#21516;&#30340;&#22870;&#21169;&#26102;&#38388;&#20559;&#22909;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#36873;&#25321;&#26159;&#21542;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#36873;&#25321;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;GPT-3.5&#65288;&#20197;&#19979;&#31616;&#31216;GPT&#65289;&#22312;&#22810;&#31181;&#35821;&#35328;&#25552;&#31034;&#19979;&#30340;&#21709;&#24212;&#65292;&#25506;&#32034;&#20102;&#36739;&#23567;&#12289;&#36739;&#26089;&#30340;&#22870;&#21169;&#21644;&#36739;&#22823;&#12289;&#36739;&#26202;&#30340;&#22870;&#21169;&#20043;&#38388;&#30340;&#20559;&#22909;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#24403;&#20197;&#35821;&#20041;&#21547;&#20041;&#36739;&#24369;&#30340;&#26410;&#26469;&#26102;&#24577;&#21442;&#32771;&#65288;FTR&#65289;&#65292;&#22914;&#24503;&#35821;&#21644;&#27721;&#35821;&#65292;&#20026;&#25552;&#31034;&#35821;&#26102;&#65292;GPT&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#32784;&#24515;&#65292;&#30456;&#27604;&#33521;&#35821;&#21644;&#27861;&#35821;&#31561;&#20855;&#26377;&#24378;&#22823;FTR&#30340;&#35821;&#35328;&#12290;&#36825;&#20123;&#21457;&#29616;&#19982;&#29616;&#26377;&#25991;&#29486;&#19968;&#33268;&#65292;&#24182;&#34920;&#26126;&#20102;GPT&#30340;&#36873;&#25321;&#19982;&#36825;&#20123;&#35821;&#35328;&#30340;&#20351;&#29992;&#32773;&#30340;&#20559;&#22909;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#28982;&#32780;&#65292;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#36739;&#26089;&#25110;&#36739;&#26202;&#22870;&#21169;&#30340;&#20559;&#22909;&#24182;&#27809;&#26377;&#38543;&#30528;&#22870;&#21169;&#24046;&#24322;&#31995;&#32479;&#22320;&#25913;&#21464;&#65292;&#36825;&#34920;&#26126;&#20102;&#19968;&#31181;&#35789;&#20856;&#24207;&#20248;&#20808;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language has a strong influence on our perceptions of time and rewards. This raises the question of whether large language models, when asked in different languages, show different preferences for rewards over time and if their choices are similar to those of humans. In this study, we analyze the responses of GPT-3.5 (hereafter referred to as GPT) to prompts in multiple languages, exploring preferences between smaller, sooner rewards and larger, later rewards. Our results show that GPT displays greater patience when prompted in languages with weak future tense references (FTR), such as German and Mandarin, compared to languages with strong FTR, like English and French. These findings are consistent with existing literature and suggest a correlation between GPT's choices and the preferences of speakers of these languages. However, further analysis reveals that the preference for earlier or later rewards does not systematically change with reward gaps, indicating a lexicographic preferen
&lt;/p&gt;</description></item><item><title>&#20026;&#20102;&#39564;&#35777;&#25991;&#26412;&#20998;&#32452;&#30340;&#20551;&#35774;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#35745;&#25991;&#26412;&#25506;&#32034;&#30340;&#27969;&#31243;&#65292;&#24182;&#22312;&#22307;&#32463;&#30340;&#21069;&#20004;&#21367;&#20070;&#20013;&#24212;&#29992;&#27492;&#27969;&#31243;&#65292;&#25104;&#21151;&#22320;&#35782;&#21035;&#24182;&#25506;&#32034;&#20102;&#21496;&#31085;&#27966;&#21035;&#21644;&#38750;&#21496;&#31085;&#27966;&#21035;&#20043;&#38388;&#30340;&#32479;&#35745;&#26126;&#26174;&#30340;&#25991;&#20307;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.02170</link><description>&lt;p&gt;
&#19968;&#31181;&#25991;&#26412;&#20998;&#32452;&#30340;&#32479;&#35745;&#25506;&#32034;&#65306;&#12298;&#21019;&#19990;&#35760;&#12299;&#21644;&#12298;&#20986;&#22467;&#21450;&#35760;&#12299;&#20013;&#21496;&#31085;&#27966;&#21035;&#30340;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
A Statistical Exploration of Text Partition Into Constituents: The Case of the Priestly Source in the Books of Genesis and Exodus. (arXiv:2305.02170v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02170
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#39564;&#35777;&#25991;&#26412;&#20998;&#32452;&#30340;&#20551;&#35774;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#35745;&#25991;&#26412;&#25506;&#32034;&#30340;&#27969;&#31243;&#65292;&#24182;&#22312;&#22307;&#32463;&#30340;&#21069;&#20004;&#21367;&#20070;&#20013;&#24212;&#29992;&#27492;&#27969;&#31243;&#65292;&#25104;&#21151;&#22320;&#35782;&#21035;&#24182;&#25506;&#32034;&#20102;&#21496;&#31085;&#27966;&#21035;&#21644;&#38750;&#21496;&#31085;&#27966;&#21035;&#20043;&#38388;&#30340;&#32479;&#35745;&#26126;&#26174;&#30340;&#25991;&#20307;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#35745;&#25991;&#26412;&#25506;&#32034;&#30340;&#27969;&#31243;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#20307;&#23398;&#30340;&#35299;&#37322;&#65292;&#24182;&#23545;&#25991;&#26412;&#30340;&#20551;&#35774;&#20998;&#32452;&#36827;&#34892;&#20102;&#32479;&#35745;&#39564;&#35777;&#12290;&#32473;&#23450;&#25991;&#26412;&#30340;&#21442;&#25968;&#21270;&#65292;&#25105;&#20204;&#30340;&#27969;&#31243;&#65306;&#65288;1&#65289;&#26816;&#27979;&#25991;&#23398;&#29305;&#24449;&#65292;&#20197;&#20135;&#29983;&#20551;&#35774;&#20998;&#32452;&#21644;&#26080;&#30417;&#30563;&#20998;&#32452;&#20043;&#38388;&#30340;&#26368;&#20339;&#37325;&#21472;&#65292;&#65288;2&#65289;&#25191;&#34892;&#20551;&#35774;&#26816;&#39564;&#20998;&#26512;&#65292;&#37327;&#21270;&#26368;&#20339;&#37325;&#21472;&#30340;&#32479;&#35745;&#26174;&#33879;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#26356;&#21487;&#33021;&#34987;&#20998;&#32452;&#30340;&#25991;&#26412;&#21333;&#20301;&#20043;&#38388;&#30340;&#38544;&#24335;&#30456;&#20851;&#24615;&#65292;&#20197;&#21450;&#65288;3&#65289;&#25552;&#21462;&#21644;&#37327;&#21270;&#23545;&#20998;&#31867;&#26368;&#36127;&#36131;&#30340;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20272;&#35745;&#23427;&#20204;&#30340;&#32479;&#35745;&#31283;&#23450;&#24615;&#21644;&#32858;&#31867;-wise&#20016;&#24230;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#27969;&#31243;&#24212;&#29992;&#20110;&#22307;&#32463;&#20013;&#30340;&#21069;&#20004;&#21367;&#20070;&#65292;&#22307;&#32463;&#23398;&#32773;&#20204;&#35748;&#20026;&#65292;&#20854;&#20013;&#19968;&#31181;&#25991;&#20307;&#25104;&#20998;&#29305;&#21035;&#31361;&#20986;&#65292;&#21363;&#21496;&#31085;&#27966;&#21035;&#12290;&#25105;&#20204;&#30830;&#23450;&#24182;&#25506;&#32034;&#20102;&#21496;&#31085;&#27966;&#21035;&#21644;&#38750;&#21496;&#31085;&#27966;&#21035;&#20043;&#38388;&#30340;&#32479;&#35745;&#26126;&#26174;&#30340;&#25991;&#20307;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a pipeline for a statistical textual exploration, offering a stylometry-based explanation and statistical validation of a hypothesized partition of a text. Given a parameterization of the text, our pipeline: (1) detects literary features yielding the optimal overlap between the hypothesized and unsupervised partitions, (2) performs a hypothesis-testing analysis to quantify the statistical significance of the optimal overlap, while conserving implicit correlations between units of text that are more likely to be grouped, and (3) extracts and quantifies the importance of features most responsible for the classification, estimates their statistical stability and cluster-wise abundance.  We apply our pipeline to the first two books in the Bible, where one stylistic component stands out in the eyes of biblical scholars, namely, the Priestly component. We identify and explore statistically significant stylistic differences between the Priestly and non-Priestly components.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#24863;&#30693;&#30340;&#30693;&#35782;&#24341;&#23548;&#25552;&#31034;&#26041;&#27861;&#65292;&#23558;&#20854;&#20316;&#20026;&#24178;&#39044;&#22120;&#35013;&#22791;&#21040;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21477;&#23376;&#25552;&#21462;&#22120;&#20013;&#65292;&#20197;&#32531;&#35299;&#27010;&#24565;&#20559;&#24046;&#12290;&#22312;&#20195;&#34920;&#24615;&#30340;&#22810;&#35821;&#35328;KG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.01876</link><description>&lt;p&gt;
&#22522;&#20110;&#22240;&#26524;&#24863;&#30693;&#30340;&#30693;&#35782;&#24341;&#23548;&#21477;&#23376;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Causality-aware Concept Extraction based on Knowledge-guided Prompting. (arXiv:2305.01876v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01876
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#24863;&#30693;&#30340;&#30693;&#35782;&#24341;&#23548;&#25552;&#31034;&#26041;&#27861;&#65292;&#23558;&#20854;&#20316;&#20026;&#24178;&#39044;&#22120;&#35013;&#22791;&#21040;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21477;&#23376;&#25552;&#21462;&#22120;&#20013;&#65292;&#20197;&#32531;&#35299;&#27010;&#24565;&#20559;&#24046;&#12290;&#22312;&#20195;&#34920;&#24615;&#30340;&#22810;&#35821;&#35328;KG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#26377;&#21161;&#20110;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65292;&#20294;&#29616;&#26377;&#30340;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#20013;&#36828;&#26410;&#23436;&#21892;&#12290;&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#22522;&#20110;&#25991;&#26412;&#30340;&#27010;&#24565;&#25552;&#21462;&#65288;CE&#65289;&#12290;&#28982;&#32780;&#65292;PLM&#24448;&#24448;&#20174;&#22823;&#37327;&#35821;&#26009;&#24211;&#30340;&#20849;&#29616;&#20851;&#32852;&#20013;&#36827;&#34892;&#39044;&#35757;&#32451;&#30693;&#35782;&#25366;&#25496;&#65292;&#32780;&#38750;Token&#20043;&#38388;&#30340;&#30495;&#23454;&#22240;&#26524;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#39044;&#35757;&#32451;&#30693;&#35782;&#28151;&#28102;&#20102;PLM&#65292;&#23548;&#33268;&#25552;&#21462;&#22522;&#20110;&#34394;&#20551;&#20849;&#29616;&#30456;&#20851;&#24615;&#30340;&#26377;&#20559;&#27010;&#24565;&#65292;&#19981;&#21487;&#36991;&#20813;&#22320;&#23548;&#33268;&#20302;&#31934;&#24230;&#12290;&#26412;&#25991;&#36890;&#36807;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24341;&#23548;&#25552;&#31034;&#26041;&#27861;&#65292;&#23558;&#20854;&#20316;&#20026;&#24178;&#39044;&#22120;&#35013;&#22791;&#21040;&#22522;&#20110;PLM&#30340;&#25552;&#21462;&#22120;&#20013;&#65292;&#20197;&#20943;&#36731;&#27010;&#24565;&#20559;&#24046;&#12290;&#25552;&#31034;&#37319;&#29992;&#29616;&#26377;KG&#20013;&#30340;&#32473;&#23450;&#23454;&#20307;&#20027;&#39064;&#26469;&#32531;&#35299;&#23454;&#20307;&#21644;&#26377;&#20559;&#27010;&#24565;&#20043;&#38388;&#30340;&#34394;&#20551;&#20849;&#29616;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#22312;&#20195;&#34920;&#24615;&#30340;&#22810;&#35821;&#35328;KG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#25552;&#31034;&#26174;&#33879;&#25913;&#36827;&#20102;&#25552;&#21462;&#24615;&#33021;&#65292;&#24182;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concepts benefit natural language understanding but are far from complete in existing knowledge graphs (KGs). Recently, pre-trained language models (PLMs) have been widely used in text-based concept extraction (CE). However, PLMs tend to mine the co-occurrence associations from massive corpus as pre-trained knowledge rather than the real causal effect between tokens.As a result, the pre-trained knowledge confounds PLMs to extract biased concepts based on spurious co-occurrence correlations, inevitably resulting in low precision. In this paper, through the lens of a Structural Causal Model (SCM), we propose equipping the PLM-based extractor with a knowledge-guided prompt as an intervention to alleviate concept bias. The prompt adopts the topic of the given entity from the existing knowledge in KGs to mitigate the spurious co-occurrence correlations between entities and biased concepts. Our extensive experiments on representative multilingual KG datasets justify that our proposed prompt 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20005;&#26684;&#30340;&#20195;&#30721;&#32508;&#21512;&#22522;&#20934;&#35780;&#20272;&#26694;&#26550;EvalPlus&#65292;&#29992;&#20110;&#35780;&#20272;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#21151;&#33021;&#27491;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.01210</link><description>&lt;p&gt;
ChatGPT&#29983;&#25104;&#30340;&#20195;&#30721;&#30495;&#30340;&#27491;&#30830;&#21527;&#65311;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#30340;&#20005;&#26684;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation. (arXiv:2305.01210v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20005;&#26684;&#30340;&#20195;&#30721;&#32508;&#21512;&#22522;&#20934;&#35780;&#20272;&#26694;&#26550;EvalPlus&#65292;&#29992;&#20110;&#35780;&#20272;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#21151;&#33021;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31243;&#24207;&#32508;&#21512;&#19968;&#30452;&#20197;&#26469;&#37117;&#26159;&#34987;&#38271;&#26399;&#30740;&#31350;&#30340;&#39046;&#22495;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#38598;&#20013;&#20110;&#30452;&#25509;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#20013;&#29992;&#25143;&#30340;&#24847;&#22270;&#29983;&#25104;&#20195;&#30721;&#12290;&#20195;&#30721;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#31574;&#21010;&#22909;&#30340;&#32508;&#21512;&#38382;&#39064;&#21644;&#21508;&#31181;&#36755;&#20837;/&#36755;&#20986;&#27979;&#35797;&#29992;&#20363;&#65292;&#34987;&#29992;&#26469;&#34913;&#37327;&#21508;&#31181;LLMs&#22312;&#20195;&#30721;&#32508;&#21512;&#19978;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20013;&#30340;&#27979;&#35797;&#29992;&#20363;&#22312;&#23436;&#20840;&#35780;&#20272;&#29983;&#25104;&#20195;&#30721;&#30340;&#21151;&#33021;&#27491;&#30830;&#24615;&#26041;&#38754;&#65292;&#25968;&#37327;&#21644;&#36136;&#37327;&#37117;&#21487;&#33021;&#26377;&#25152;&#38480;&#21046;&#12290;&#36825;&#31181;&#29616;&#26377;&#22522;&#20934;&#20013;&#30340;&#38480;&#21046;&#24341;&#20986;&#20102;&#20197;&#19979;&#38382;&#39064;&#65306;&#22312;LLMs&#26102;&#20195;&#65292;&#29983;&#25104;&#30340;&#20195;&#30721;&#30495;&#30340;&#27491;&#30830;&#21527;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EvalPlus&#8212;&#8212;&#19968;&#20010;&#35780;&#20272;LLM-synthesized&#20195;&#30721;&#21151;&#33021;&#27491;&#30830;&#24615;&#30340;&#20005;&#26684;&#22522;&#20934;&#35780;&#20272;&#26694;&#26550;&#12290;EvalPlus&#25509;&#21463;&#22522;&#30784;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#33258;&#21160;&#36755;&#20837;&#29983;&#25104;&#27493;&#39588;&#65292;&#20351;&#29992;LLM-based&#21644;&#22522;&#20110;&#21464;&#24322;&#30340;&#26041;&#27861;&#29983;&#25104;&#21644;&#22810;&#26679;&#21270;&#22823;&#37327;&#26032;&#30340;&#27979;&#35797;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Program synthesis has been long studied with recent approaches focused on directly using the power of Large Language Models (LLMs) to generate code according to user intent written in natural language. Code evaluation datasets, containing curated synthesis problems with input/output test-cases, are used to measure the performance of various LLMs on code synthesis. However, test-cases in these datasets can be limited in both quantity and quality for fully assessing the functional correctness of the generated code. Such limitation in the existing benchmarks begs the following question: In the era of LLMs, is the code generated really correct? To answer this, we propose EvalPlus -- a code synthesis benchmarking framework to rigorously evaluate the functional correctness of LLM-synthesized code. In short, EvalPlus takes in the base evaluation dataset and uses an automatic input generation step to produce and diversify large amounts of new test inputs using both LLM-based and mutation-based
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ChartSumm&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#38271;&#30701;&#25688;&#35201;&#33258;&#21160;&#29983;&#25104;&#20219;&#21153;&#65292;&#21253;&#25324;84000&#22810;&#20010;&#22270;&#34920;&#21450;&#20854;&#20803;&#25968;&#25454;&#21644;&#25551;&#36848;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;&#33258;&#21160;&#25688;&#35201;&#27169;&#22411;&#34429;&#28982;&#24471;&#20998;&#19981;&#38169;&#65292;&#20294;&#32463;&#24120;&#38754;&#20020;&#38169;&#35273;&#12289;&#28431;&#25481;&#37325;&#35201;&#25968;&#25454;&#28857;&#20197;&#21450;&#19981;&#27491;&#30830;&#35299;&#37322;&#22797;&#26434;&#36235;&#21183;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.13620</link><description>&lt;p&gt;
ChartSumm&#65306;&#38271;&#30701;&#25688;&#35201;&#33258;&#21160;&#29983;&#25104;&#20219;&#21153;&#30340;&#20840;&#38754;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ChartSumm: A Comprehensive Benchmark for Automatic Chart Summarization of Long and Short Summaries. (arXiv:2304.13620v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ChartSumm&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#38271;&#30701;&#25688;&#35201;&#33258;&#21160;&#29983;&#25104;&#20219;&#21153;&#65292;&#21253;&#25324;84000&#22810;&#20010;&#22270;&#34920;&#21450;&#20854;&#20803;&#25968;&#25454;&#21644;&#25551;&#36848;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;&#33258;&#21160;&#25688;&#35201;&#27169;&#22411;&#34429;&#28982;&#24471;&#20998;&#19981;&#38169;&#65292;&#20294;&#32463;&#24120;&#38754;&#20020;&#38169;&#35273;&#12289;&#28431;&#25481;&#37325;&#35201;&#25968;&#25454;&#28857;&#20197;&#21450;&#19981;&#27491;&#30830;&#35299;&#37322;&#22797;&#26434;&#36235;&#21183;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#23558;&#22270;&#34920;&#36716;&#25442;&#20026;&#25991;&#26412;&#25688;&#35201;&#26159;&#35270;&#38556;&#20154;&#22763;&#30340;&#26377;&#25928;&#24037;&#20855;&#65292;&#21516;&#26102;&#20026;&#29992;&#25143;&#25552;&#20379;&#34920;&#26684;&#25968;&#25454;&#30340;&#33258;&#28982;&#35821;&#35328;&#31934;&#30830;&#27934;&#23519;&#21147;&#12290;&#22823;&#22411;&#12289;&#32467;&#26500;&#33391;&#22909;&#30340;&#25968;&#25454;&#38598;&#22987;&#32456;&#26159;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30340;&#20851;&#38190;&#37096;&#20998;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ChartSumm&#65306;&#19968;&#20010;&#22823;&#35268;&#27169;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#20849;84363&#20010;&#22270;&#34920;&#21450;&#20854;&#20803;&#25968;&#25454;&#21644;&#25551;&#36848;&#65292;&#28085;&#30422;&#24191;&#27867;&#30340;&#20027;&#39064;&#21644;&#22270;&#34920;&#31867;&#22411;&#65292;&#21487;&#29983;&#25104;&#38271;&#30701;&#25688;&#35201;&#12290;&#24378;&#22522;&#32447;&#27169;&#22411;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#23454;&#29616;&#21508;&#31181;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#30340;&#24471;&#20998;&#26469;&#29983;&#25104;&#27969;&#30021;&#19988;&#20449;&#24687;&#20016;&#23500;&#30340;&#25688;&#35201;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#36935;&#21040;&#19968;&#20123;&#38382;&#39064;&#65292;&#20363;&#22914;&#20135;&#29983;&#38169;&#35273;&#65292;&#28431;&#25481;&#37325;&#35201;&#30340;&#25968;&#25454;&#28857;&#65292;&#20197;&#21450;&#19981;&#27491;&#30830;&#22320;&#35299;&#37322;&#22270;&#34920;&#20013;&#30340;&#22797;&#26434;&#36235;&#21183;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#33258;&#21160;&#32763;&#35793;&#24037;&#20855;&#25506;&#35752;&#20102;&#23558;ChartSumm&#25193;&#23637;&#21040;&#20854;&#20182;&#35821;&#35328;&#30340;&#28508;&#21147;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#25104;&#20026;&#19968;&#20010;&#26377;&#25361;&#25112;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic chart to text summarization is an effective tool for the visually impaired people along with providing precise insights of tabular data in natural language to the user. A large and well-structured dataset is always a key part for data driven models. In this paper, we propose ChartSumm: a large-scale benchmark dataset consisting of a total of 84,363 charts along with their metadata and descriptions covering a wide range of topics and chart types to generate short and long summaries. Extensive experiments with strong baseline models show that even though these models generate fluent and informative summaries by achieving decent scores in various automatic evaluation metrics, they often face issues like suffering from hallucination, missing out important data points, in addition to incorrect explanation of complex trends in the charts. We also investigated the potential of expanding ChartSumm to other languages using automated translation tools. These make our dataset a challeng
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992; Evol-Instruct &#26041;&#27861;&#21019;&#24314;&#20102;&#22823;&#37327;&#19981;&#21516;&#22797;&#26434;&#24230;&#30340;&#25351;&#20196;&#25968;&#25454;&#29992;&#20110;&#24494;&#35843; LLaMA &#27169;&#22411;&#65292;&#24471;&#21040;&#20102;&#26032;&#27169;&#22411; WizardLM&#12290;&#20154;&#31867;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126; Evol-Instruct &#29983;&#25104;&#30340;&#25351;&#20196;&#20248;&#20110;&#20154;&#24037;&#21019;&#24314;&#30340;&#65292;&#32780; WizardLM &#36755;&#20986;&#30340;&#32467;&#26524;&#20063;&#27604; OpenAI ChatGPT &#26356;&#21463;&#27426;&#36814;&#12290;</title><link>http://arxiv.org/abs/2304.12244</link><description>&lt;p&gt;
WizardLM: &#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36981;&#24490;&#22797;&#26434;&#25351;&#20196;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
WizardLM: Empowering Large Language Models to Follow Complex Instructions. (arXiv:2304.12244v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12244
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992; Evol-Instruct &#26041;&#27861;&#21019;&#24314;&#20102;&#22823;&#37327;&#19981;&#21516;&#22797;&#26434;&#24230;&#30340;&#25351;&#20196;&#25968;&#25454;&#29992;&#20110;&#24494;&#35843; LLaMA &#27169;&#22411;&#65292;&#24471;&#21040;&#20102;&#26032;&#27169;&#22411; WizardLM&#12290;&#20154;&#31867;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126; Evol-Instruct &#29983;&#25104;&#30340;&#25351;&#20196;&#20248;&#20110;&#20154;&#24037;&#21019;&#24314;&#30340;&#65292;&#32780; WizardLM &#36755;&#20986;&#30340;&#32467;&#26524;&#20063;&#27604; OpenAI ChatGPT &#26356;&#21463;&#27426;&#36814;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#24320;&#25918;&#22495;&#25351;&#20196;&#36861;&#36394;&#25968;&#25454;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#21019;&#24314;&#36825;&#26679;&#30340;&#25351;&#20196;&#25968;&#25454;&#38750;&#24120;&#32791;&#26102;&#21644;&#21171;&#21160;&#23494;&#38598;&#65292;&#19988;&#20154;&#31867;&#21487;&#33021;&#38590;&#20197;&#29983;&#25104;&#39640;&#22797;&#26434;&#24230;&#25351;&#20196;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;LLM&#32780;&#19981;&#26159;&#20154;&#31867;&#21019;&#24314;&#22823;&#37327;&#19981;&#21516;&#22797;&#26434;&#24230;&#25351;&#20196;&#25968;&#25454;&#30340;&#36884;&#24452;&#12290;&#25105;&#20204;&#20174;&#19968;&#32452;&#21021;&#22987;&#25351;&#20196;&#24320;&#22987;&#65292;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;Evol-Instruct&#36880;&#27493;&#23558;&#20854;&#37325;&#26032;&#32534;&#20889;&#20026;&#26356;&#22797;&#26434;&#30340;&#25351;&#20196;&#12290;&#28982;&#21518;&#65292;&#23558;&#25152;&#26377;&#29983;&#25104;&#30340;&#25351;&#20196;&#25968;&#25454;&#28151;&#21512;&#20197;&#24494;&#35843;LLaMA&#12290;&#25105;&#20204;&#31216;&#32467;&#26524;&#27169;&#22411;&#20026;WizardLM&#12290;&#38024;&#23545;&#19968;&#20010;&#22797;&#26434;&#24230;&#24179;&#34913;&#30340;&#27979;&#35797;&#38598;&#21644;Vicuna&#30340;&#27979;&#35797;&#38598;&#36827;&#34892;&#30340;&#20154;&#31867;&#35780;&#20272;&#34920;&#26126;&#65292;Evol-Instruct&#29983;&#25104;&#30340;&#25351;&#20196;&#20248;&#20110;&#20154;&#24037;&#21019;&#24314;&#30340;&#25351;&#20196;&#12290;&#36890;&#36807;&#20998;&#26512;&#39640;&#22797;&#26434;&#24615;&#37096;&#20998;&#30340;&#20154;&#31867;&#35780;&#20272;&#32467;&#26524;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20174;&#25105;&#20204;&#30340;WizardLM&#29983;&#25104;&#30340;&#36755;&#20986;&#27604;&#20174;OpenAI ChatGPT&#29983;&#25104;&#30340;&#36755;&#20986;&#26356;&#21463;&#27426;&#36814;&#12290;&#22312;GPT-4&#33258;&#21160;&#35780;&#20272;&#20013;&#65292;WizardLM&#20135;&#29983;&#20102;&#26368;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training large language models (LLMs) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Human evaluations on a complexity-balanced test bed and Vicuna's testset show that instructions from Evol-Instruct are superior to human-created ones. By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4 automatic evaluation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20027;&#24352;&#23558;&#20135;&#21697;&#25628;&#32034;&#30475;&#20316;&#31243;&#24207;&#21512;&#25104;&#65292;&#30456;&#27604;&#21521;&#37327;&#31354;&#38388;&#27169;&#22411;&#26377;&#30528;&#37325;&#22823;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2304.11473</link><description>&lt;p&gt;
(&#21521;&#37327;)&#31354;&#38388;&#19981;&#26159;&#26368;&#21518;&#30340;&#30086;&#22495;&#65306;&#23558;&#20135;&#21697;&#25628;&#32034;&#30475;&#20316;&#31243;&#24207;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
(Vector) Space is Not the Final Frontier: Product Search as Program Synthesis. (arXiv:2304.11473v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#24352;&#23558;&#20135;&#21697;&#25628;&#32034;&#30475;&#20316;&#31243;&#24207;&#21512;&#25104;&#65292;&#30456;&#27604;&#21521;&#37327;&#31354;&#38388;&#27169;&#22411;&#26377;&#30528;&#37325;&#22823;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#30005;&#23376;&#21830;&#21153;&#30340;&#19981;&#26029;&#22686;&#38271;&#65292;&#24040;&#39069;&#25237;&#36164;&#29992;&#20110;&#20449;&#24687;&#26816;&#32034;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20063;&#38543;&#20043;&#32780;&#26469;&#12290;&#34429;&#28982;&#21521;&#37327;&#31354;&#38388;&#27169;&#22411;&#20027;&#23472;&#20102;&#20135;&#21697;&#25628;&#32034;&#20013;&#30340;&#26816;&#32034;&#27169;&#22411;&#65292;&#20294;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#20986;&#29616;&#65292;&#21521;&#37327;&#21270;&#26412;&#36523;&#20063;&#21457;&#29983;&#20102;&#24040;&#22823;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#31435;&#22330;&#35770;&#25991;&#20197;&#30456;&#21453;&#30340;&#26041;&#24335;&#20027;&#24352;&#65292;&#21363;&#31243;&#24207;&#21512;&#25104;&#23545;&#35768;&#22810;&#26597;&#35810;&#21644;&#24066;&#22330;&#20013;&#30340;&#22823;&#37327;&#21442;&#19982;&#32773;&#25552;&#20379;&#20102;&#37325;&#22823;&#20248;&#21183;&#12290;&#25105;&#20204;&#35814;&#32454;&#35828;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#34892;&#19994;&#37325;&#35201;&#24615;&#65292;&#27010;&#36848;&#20102;&#20855;&#20307;&#23454;&#29616;&#32454;&#33410;&#65292;&#24182;&#22522;&#20110;&#25105;&#20204;&#22312;Tooso&#26500;&#24314;&#31867;&#20284;&#31995;&#32479;&#30340;&#32463;&#39564;&#65292;&#22238;&#31572;&#20102;&#19968;&#20123;&#24120;&#35265;&#30340;&#21453;&#23545;&#24847;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
As ecommerce continues growing, huge investments in ML and NLP for Information Retrieval are following. While the vector space model dominated retrieval modelling in product search - even as vectorization itself greatly changed with the advent of deep learning -, our position paper argues in a contrarian fashion that program synthesis provides significant advantages for many queries and a significant number of players in the market. We detail the industry significance of the proposed approach, sketch implementation details, and address common objections drawing from our experience building a similar system at Tooso.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#33258;&#25105;&#20013;&#24515;&#35760;&#24518;&#21644;&#25511;&#21046;&#30340;&#26694;&#26550;LLM-Brain&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#26426;&#22120;&#20154;&#22823;&#33041;&#36827;&#34892;&#38646;-shot&#23398;&#20064;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#23553;&#38381;&#24335;&#22810;&#36718;&#23545;&#35805;&#65292;&#35206;&#30422;&#20102;&#24863;&#30693;&#12289;&#35268;&#21010;&#12289;&#25511;&#21046;&#21644;&#35760;&#24518;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#26426;&#22120;&#20154;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2304.09349</link><description>&lt;p&gt;
LLM&#20316;&#20026;&#26426;&#22120;&#20154;&#30340;&#22823;&#33041;&#65306;&#32479;&#19968;&#33258;&#25105;&#20013;&#24515;&#35760;&#24518;&#19982;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
LLM as A Robotic Brain: Unifying Egocentric Memory and Control. (arXiv:2304.09349v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#33258;&#25105;&#20013;&#24515;&#35760;&#24518;&#21644;&#25511;&#21046;&#30340;&#26694;&#26550;LLM-Brain&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#26426;&#22120;&#20154;&#22823;&#33041;&#36827;&#34892;&#38646;-shot&#23398;&#20064;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#23553;&#38381;&#24335;&#22810;&#36718;&#23545;&#35805;&#65292;&#35206;&#30422;&#20102;&#24863;&#30693;&#12289;&#35268;&#21010;&#12289;&#25511;&#21046;&#21644;&#35760;&#24518;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#26426;&#22120;&#20154;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20307;&#24863;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#21644;&#24320;&#21457;&#20855;&#22791;&#29289;&#29702;&#25110;&#34394;&#25311;&#23454;&#20307;&#65288;&#21363;&#26426;&#22120;&#20154;&#65289;&#24182;&#33021;&#22815;&#19982;&#29615;&#22659;&#21160;&#24577;&#20132;&#20114;&#30340;&#26234;&#33021;&#31995;&#32479;&#12290;&#35760;&#24518;&#21644;&#25511;&#21046;&#26159;&#20307;&#24863;&#31995;&#32479;&#30340;&#20004;&#20010;&#22522;&#26412;&#37096;&#20998;&#65292;&#36890;&#24120;&#38656;&#35201;&#20998;&#21035;&#20351;&#29992;&#26694;&#26550;&#36827;&#34892;&#24314;&#27169;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#12289;&#21487;&#25512;&#24191;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;LLM-Brain&#65306;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#26426;&#22120;&#20154;&#22823;&#33041;&#65292;&#32479;&#19968;&#33258;&#25105;&#20013;&#24515;&#35760;&#24518;&#21644;&#25511;&#21046;&#12290;LLM-Brain&#26694;&#26550;&#38598;&#25104;&#20102;&#22810;&#20010;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#26426;&#22120;&#20154;&#20219;&#21153;&#65292;&#21033;&#29992;&#38646;-shot&#23398;&#20064;&#26041;&#27861;&#12290;LLM-Brain&#20013;&#30340;&#25152;&#26377;&#32452;&#20214;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#23553;&#38381;&#24335;&#22810;&#36718;&#23545;&#35805;&#65292;&#21253;&#25324;&#24863;&#30693;&#12289;&#35268;&#21010;&#12289;&#25511;&#21046;&#21644;&#35760;&#24518;&#12290;&#31995;&#32479;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#20855;&#22791;&#33258;&#25105;&#20013;&#24515;&#35760;&#24518;&#21644;&#25511;&#21046;&#26426;&#22120;&#20154;&#30340;&#23454;&#20307;LLM&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#20004;&#20010;&#19979;&#28216;&#20219;&#21153;&#65306;&#20027;&#21160;&#25506;&#32034;&#21644;&#23454;&#20307;&#38382;&#31572;&#26469;&#28436;&#31034;LLM-Brain&#12290;
&lt;/p&gt;
&lt;p&gt;
Embodied AI focuses on the study and development of intelligent systems that possess a physical or virtual embodiment (i.e. robots) and are able to dynamically interact with their environment. Memory and control are the two essential parts of an embodied system and usually require separate frameworks to model each of them. In this paper, we propose a novel and generalizable framework called LLM-Brain: using Large-scale Language Model as a robotic brain to unify egocentric memory and control. The LLM-Brain framework integrates multiple multimodal language models for robotic tasks, utilizing a zero-shot learning approach. All components within LLM-Brain communicate using natural language in closed-loop multi-round dialogues that encompass perception, planning, control, and memory. The core of the system is an embodied LLM to maintain egocentric memory and control the robot. We demonstrate LLM-Brain by examining two downstream tasks: active exploration and embodied question answering. The
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20043;&#20026;instructRL&#30340;&#26032;&#30340;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#26469;&#25351;&#23450;&#23545;&#20154;&#24037;&#26234;&#33021;&#25645;&#26723;&#30340;&#39044;&#26399;&#31574;&#30053;&#65292;&#35299;&#20915;&#22312;&#32570;&#20047;&#39640;&#36136;&#37327;&#20154;&#31867;&#34892;&#20026;&#25968;&#25454;&#30340;&#39046;&#22495;&#20013;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#25910;&#25947;&#20110;&#20154;&#31867;&#19981;&#20559;&#29233;&#30340;&#31574;&#30053;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.07297</link><description>&lt;p&gt;
&#35821;&#35328;&#25351;&#23548;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#20197;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
Language Instructed Reinforcement Learning for Human-AI Coordination. (arXiv:2304.07297v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07297
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20043;&#20026;instructRL&#30340;&#26032;&#30340;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#26469;&#25351;&#23450;&#23545;&#20154;&#24037;&#26234;&#33021;&#25645;&#26723;&#30340;&#39044;&#26399;&#31574;&#30053;&#65292;&#35299;&#20915;&#22312;&#32570;&#20047;&#39640;&#36136;&#37327;&#20154;&#31867;&#34892;&#20026;&#25968;&#25454;&#30340;&#39046;&#22495;&#20013;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#25910;&#25947;&#20110;&#20154;&#31867;&#19981;&#20559;&#29233;&#30340;&#31574;&#30053;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#22914;&#20309;&#35753;&#26234;&#33021;&#20307;&#33021;&#22815;&#21644;&#20154;&#31867;&#26377;&#25928;&#22320;&#21327;&#20316;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20043;&#20026;instructRL&#30340;&#26032;&#30340;&#26694;&#26550;&#65292;&#35753;&#20154;&#20204;&#21487;&#20197;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#26469;&#25351;&#23450;&#23545;&#20154;&#24037;&#26234;&#33021;&#25645;&#26723;&#30340;&#39044;&#26399;&#31574;&#30053;&#65292;&#20197;&#27492;&#35299;&#20915;&#22312;&#32570;&#20047;&#36739;&#39640;&#36136;&#37327;&#30340;&#20154;&#31867;&#34892;&#20026;&#25968;&#25454;&#30340;&#39046;&#22495;&#20013;&#65292;&#30001;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#24120;&#24120;&#20250;&#25910;&#25947;&#21040;&#20154;&#31867;&#24182;&#19981;&#20559;&#29233;&#30340;&#31574;&#30053;&#30340;&#19981;&#36275;&#12290;&#25105;&#20204;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#19968;&#20010;&#22312;&#20154;&#31867;&#25351;&#20196;&#19979;&#30340;&#20808;&#39564;&#31574;&#30053;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#30446;&#26631;&#12290;&#36825;&#23548;&#33268;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#25910;&#25947;&#21040;&#19982;&#20154;&#31867;&#21916;&#22909;&#19968;&#33268;&#30340;&#22343;&#34913;&#28857;&#12290;&#36890;&#36807;&#27010;&#24565;&#35777;&#26126;&#29615;&#22659;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;Hanabi&#22522;&#20934;&#65292;&#35777;&#26126;&#20102;instructRL&#25910;&#25947;&#20110;&#28385;&#36275;&#32473;&#23450;&#25351;&#20196;&#30340;&#31867;&#20284;&#20154;&#31867;&#26234;&#33021;&#20307;&#30340;&#31574;&#30053;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#30693;&#36947;&#35821;&#35328;&#25351;&#20196;&#26174;&#33879;&#25552;&#39640;&#20102;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the fundamental quests of AI is to produce agents that coordinate well with humans. This problem is challenging, especially in domains that lack high quality human behavioral data, because multi-agent reinforcement learning (RL) often converges to different equilibria from the ones that humans prefer. We propose a novel framework, instructRL, that enables humans to specify what kind of strategies they expect from their AI partners through natural language instructions. We use pretrained large language models to generate a prior policy conditioned on the human instruction and use the prior to regularize the RL objective. This leads to the RL agent converging to equilibria that are aligned with human preferences. We show that instructRL converges to human-like policies that satisfy the given instructions in a proof-of-concept environment as well as the challenging Hanabi benchmark. Finally, we show that knowing the language instruction significantly boosts human-AI coordination pe
&lt;/p&gt;</description></item><item><title>Multimodal C4&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#12289;&#20197;&#22270;&#20687;&#19982;&#25991;&#26412;&#20132;&#26367;&#24418;&#24335;&#23384;&#22312;&#30340;&#25968;&#25454;&#24211;&#65292;&#20854;&#20351;&#29992;&#32447;&#24615;&#20998;&#37197;&#31639;&#27861;&#23558;&#22270;&#20687;&#25918;&#21040;&#38271;&#25991;&#26412;&#27573;&#33853;&#20013;&#65292;&#21487;&#29992;&#20110;&#36890;&#36807;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#21644;&#22797;&#26434;&#30456;&#20851;&#24230;&#25552;&#31034;&#30340;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2304.06939</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;C4&#65306;&#19968;&#31181;&#21253;&#21547;&#22823;&#37327;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#24320;&#25918;&#24335;&#25968;&#25454;&#24211;
&lt;/p&gt;
&lt;p&gt;
Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved With Text. (arXiv:2304.06939v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06939
&lt;/p&gt;
&lt;p&gt;
Multimodal C4&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#12289;&#20197;&#22270;&#20687;&#19982;&#25991;&#26412;&#20132;&#26367;&#24418;&#24335;&#23384;&#22312;&#30340;&#25968;&#25454;&#24211;&#65292;&#20854;&#20351;&#29992;&#32447;&#24615;&#20998;&#37197;&#31639;&#27861;&#23558;&#22270;&#20687;&#25918;&#21040;&#38271;&#25991;&#26412;&#27573;&#33853;&#20013;&#65292;&#21487;&#29992;&#20110;&#36890;&#36807;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#21644;&#22797;&#26434;&#30456;&#20851;&#24230;&#25552;&#31034;&#30340;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#38656;&#35201;&#25903;&#25345;&#20219;&#24847;&#20132;&#26367;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#24207;&#21015;&#20316;&#20026;&#36755;&#20837;, &#36825;&#31181;&#26684;&#24335;&#19981;&#20165;&#21487;&#20197;&#36890;&#36807;&#20132;&#26367;&#29420;&#31435;&#30417;&#30563;&#30340;(&#22270;&#20687;,&#25991;&#26412;)&#31034;&#20363;&#26469;&#36827;&#34892;&#20302;&#27425;&#23398;&#20064;,&#32780;&#19988;&#21487;&#20197;&#24212;&#23545;&#26356;&#22797;&#26434;&#30340;&#25552;&#31034;, &#28041;&#21450;&#22270;&#20687;&#38388;&#20114;&#21160;,&#20363;&#22914;&#8220;&#22270;&#20687;A&#21644;&#22270;&#20687;B&#26377;&#20160;&#20040;&#20849;&#21516;&#20043;&#22788;?&#8221;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20351;&#29992;&#31867;&#20284;&#20110;&#20132;&#26367;&#22270;&#20687;+&#25991;&#26412;&#30340;web&#35821;&#26009;&#24211;&#12290;&#20294;&#26159;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#36825;&#31181;&#24418;&#24335;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#36824;&#27809;&#26377;&#20844;&#24320;&#25552;&#20379;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;Multimodal C4 (mmc4)&#65292;&#36825;&#26159;&#19968;&#20010;&#21152;&#24378;&#29256;&#30340;c4&#25991;&#26412;&#24211;&#65292;&#20854;&#20013;&#25554;&#20837;&#20102;&#22270;&#20687;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#32447;&#24615;&#20998;&#37197;&#31639;&#27861;&#65292;&#20351;&#29992;CLIP&#29305;&#24449;&#23558;&#22270;&#20687;&#25918;&#21040;&#26356;&#38271;&#30340;&#25991;&#26412;&#20307;&#20013;&#65292;&#27492;&#36807;&#31243;&#20248;&#20110;&#20854;&#20182;&#26367;&#20195;&#26041;&#26696;&#12290;mmc4&#28085;&#30422;&#20102;&#35832;&#22914;&#28921;&#39274;&#65292;&#26053;&#28216;&#65292;&#25216;&#26415;&#31561;&#26085;&#24120;&#20027;&#39064;&#12290;&#23545;&#38543;&#26426;&#26679;&#26412;&#30340;&#25163;&#21160;&#26816;&#26597;&#34920;&#26126;&#65292;&#32477;&#22823;&#22810;&#25968;(90%)&#30340;&#22270;&#20687;&#19982;&#20027;&#39064;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context vision and language models like Flamingo support arbitrarily interleaved sequences of images and text as input. This format not only enables few-shot learning via interleaving independent supervised (image, text) examples, but also, more complex prompts involving interaction between images, e.g., "What do image A and image B have in common?" To support this interface, pretraining occurs over web corpora that similarly contain interleaved images+text. To date, however, large-scale data of this form have not been publicly available.  We release Multimodal C4 (mmc4), an augmentation of the popular text-only c4 corpus with images interleaved. We use a linear assignment algorithm to place images into longer bodies of text using CLIP features, a process that we show outperforms alternatives. mmc4 spans everyday topics like cooking, travel, technology, etc. A manual inspection of a random sample of documents shows that a vast majority (90%) of images are topically relevant, and tha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25512;&#29305;&#30123;&#33495;&#25968;&#25454;&#38598;Vax-Culture&#65292;&#23427;&#26088;&#22312;&#25214;&#20986;&#25512;&#24191;&#30123;&#33495;&#38169;&#35823;&#20449;&#24687;&#30340;&#25991;&#21270;&#21644;&#25919;&#27835;&#20449;&#24565;&#30340;&#37325;&#21472;&#37096;&#20998;&#65292;&#24110;&#21161;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20197;&#33258;&#21160;&#26816;&#27979;&#30123;&#33495;&#38169;&#35823;&#20449;&#24687;&#24086;&#23376;&#24182;&#24212;&#23545;&#20854;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.06858</link><description>&lt;p&gt;
Vax-Culture: &#29992;&#20110;&#30740;&#31350;&#25512;&#29305;&#19978;&#30123;&#33495;&#35752;&#35770;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Vax-Culture: A Dataset for Studying Vaccine Discourse on Twitter. (arXiv:2304.06858v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25512;&#29305;&#30123;&#33495;&#25968;&#25454;&#38598;Vax-Culture&#65292;&#23427;&#26088;&#22312;&#25214;&#20986;&#25512;&#24191;&#30123;&#33495;&#38169;&#35823;&#20449;&#24687;&#30340;&#25991;&#21270;&#21644;&#25919;&#27835;&#20449;&#24565;&#30340;&#37325;&#21472;&#37096;&#20998;&#65292;&#24110;&#21161;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20197;&#33258;&#21160;&#26816;&#27979;&#30123;&#33495;&#38169;&#35823;&#20449;&#24687;&#24086;&#23376;&#24182;&#24212;&#23545;&#20854;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19&#30123;&#24773;&#26399;&#38388;&#65292;&#30123;&#33495;&#29369;&#35947;&#32487;&#32493;&#26159;&#20844;&#20849;&#21355;&#29983;&#23448;&#21592;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#30001;&#20110;&#35813;&#29369;&#35947;&#30772;&#22351;&#20102;&#30123;&#33495;&#36816;&#21160;&#65292;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#35797;&#22270;&#30830;&#23450;&#20854;&#26681;&#26412;&#21407;&#22240;&#65292;&#24182;&#21457;&#29616;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#21453;&#30123;&#33495;&#38169;&#35823;&#20449;&#24687;&#30340;&#19981;&#26029;&#22686;&#38271;&#26159;&#35813;&#38382;&#39064;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#25105;&#20204;&#23558;&#25512;&#29305;&#20316;&#20026;&#35823;&#23548;&#20869;&#23481;&#30340;&#26469;&#28304;&#65292;&#24182;&#26088;&#22312;&#25552;&#21462;&#25512;&#24191;&#30123;&#33495;&#38169;&#35823;&#20449;&#24687;&#30340;&#25991;&#21270;&#21644;&#25919;&#27835;&#20449;&#24565;&#30340;&#37325;&#21472;&#37096;&#20998;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#19982;&#30123;&#33495;&#26377;&#20851;&#30340;&#25512;&#25991;&#25968;&#25454;&#38598;&#65292;&#24182;&#20511;&#21161;&#19987;&#19994;&#27807;&#36890;&#21644;&#26032;&#38395;&#32972;&#26223;&#30340;&#27880;&#37322;&#20154;&#21592;&#36827;&#34892;&#27880;&#37322;&#12290;&#25105;&#20204;&#26368;&#32456;&#24076;&#26395;&#36825;&#21487;&#20197;&#24102;&#26469;&#26377;&#25928;&#21644;&#26377;&#38024;&#23545;&#24615;&#30340;&#20844;&#20849;&#21355;&#29983;&#36890;&#20449;&#31574;&#30053;&#65292;&#20197;&#25509;&#35302;&#37027;&#20123;&#25345;&#21453;&#30123;&#33495;&#20449;&#20208;&#32773;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#20449;&#24687;&#26377;&#21161;&#20110;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20197;&#33258;&#21160;&#26816;&#27979;&#30123;&#33495;&#38169;&#35823;&#20449;&#24687;&#24086;&#23376;&#24182;&#24212;&#23545;&#20854;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vaccine hesitancy continues to be a main challenge for public health officials during the COVID-19 pandemic. As this hesitancy undermines vaccine campaigns, many researchers have sought to identify its root causes, finding that the increasing volume of anti-vaccine misinformation on social media platforms is a key element of this problem. We explored Twitter as a source of misleading content with the goal of extracting overlapping cultural and political beliefs that motivate the spread of vaccine misinformation. To do this, we have collected a data set of vaccine-related Tweets and annotated them with the help of a team of annotators with a background in communications and journalism. Ultimately we hope this can lead to effective and targeted public health communication strategies for reaching individuals with anti-vaccine beliefs. Moreover, this information helps with developing Machine Learning models to automatically detect vaccine misinformation posts and combat their negative impa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;SwissBERT&#65292;&#23427;&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#22788;&#29702;&#29790;&#22763;&#30456;&#20851;&#25991;&#26412;&#32780;&#21019;&#24314;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#65292;SwissBERT&#22312;&#19982;&#29790;&#22763;&#30456;&#20851;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#25928;&#26524;&#20248;&#20110;&#20197;&#21069;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.13310</link><description>&lt;p&gt;
SwissBERT&#65306;&#29790;&#22763;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SwissBERT: The Multilingual Language Model for Switzerland. (arXiv:2303.13310v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13310
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;SwissBERT&#65292;&#23427;&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#22788;&#29702;&#29790;&#22763;&#30456;&#20851;&#25991;&#26412;&#32780;&#21019;&#24314;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#65292;SwissBERT&#22312;&#19982;&#29790;&#22763;&#30456;&#20851;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#25928;&#26524;&#20248;&#20110;&#20197;&#21069;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;SwissBERT&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#22788;&#29702;&#19982;&#29790;&#22763;&#30456;&#20851;&#30340;&#25991;&#26412;&#32780;&#21019;&#24314;&#30340;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#12290; SwissBERT&#26159;&#19968;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#25105;&#20204;&#23558;&#20854;&#35843;&#25972;&#20026;&#33021;&#22815;&#22788;&#29702;&#29790;&#22763;&#22269;&#23478;&#35821;&#35328; -&#24503;&#35821;&#12289;&#27861;&#35821;&#12289;&#24847;&#22823;&#21033;&#35821;&#21644;&#32599;&#26364;&#20160;&#35821;&#30340;&#26032;&#38395;&#25991;&#31456;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;SwissBERT&#22312;&#19982;&#29790;&#22763;&#30456;&#20851;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#23427;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#24448;&#24448;&#20248;&#20110;&#20197;&#21069;&#30340;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#24403;&#20195;&#26032;&#38395;&#21644;/&#25110;&#32599;&#26364;&#20160;&#35821;&#26684;&#37324;&#26031;&#26118;&#26102;&#12290;&#30001;&#20110;SwissBERT&#20351;&#29992;&#35821;&#35328;&#36866;&#37197;&#22120;&#65292;&#22240;&#27492;&#26410;&#26469;&#30340;&#24037;&#20316;&#21487;&#33021;&#23558;&#20854;&#25193;&#23637;&#21040;&#29790;&#22763;&#24503;&#35821;&#26041;&#35328;&#20013;&#12290;&#35813;&#27169;&#22411;&#21644;&#25105;&#20204;&#30340;&#24320;&#28304;&#20195;&#30721;&#20844;&#24320;&#21457;&#24067;&#22312;https://github.com/ZurichNLP/swissbert&#12290;
&lt;/p&gt;
&lt;p&gt;
We present SwissBERT, a masked language model created specifically for processing Switzerland-related text. SwissBERT is a pre-trained model that we adapted to news articles written in the national languages of Switzerland -German, French, Italian, and Romansh. We evaluate SwissBERT on natural language understanding tasks related to Switzerland and find that it tends to outperform previous models on these tasks, especially when processing contemporary news and/or Romansh Grischun. Since SwissBERT uses language adapters, it may be extended to Swiss German dialects in future work. The model and our open-source code are publicly released at https://github.com/ZurichNLP/swissbert.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986; Reflexion &#26041;&#27861;&#65292;&#32473;&#26234;&#33021;&#20307;&#36171;&#20104;&#20102;&#21160;&#24577;&#35760;&#24518;&#21644;&#33258;&#25105;&#21453;&#24605;&#33021;&#21147;&#65292;&#20197;&#22686;&#24378;&#20854;&#20219;&#21153;&#29305;&#23450;&#30340;&#34892;&#21160;&#36873;&#25321;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.11366</link><description>&lt;p&gt;
Reflexion&#65306;&#20855;&#26377;&#21160;&#24577;&#35760;&#24518;&#21644;&#33258;&#25105;&#21453;&#24605;&#30340;&#33258;&#20027;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Reflexion: an autonomous agent with dynamic memory and self-reflection. (arXiv:2303.11366v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986; Reflexion &#26041;&#27861;&#65292;&#32473;&#26234;&#33021;&#20307;&#36171;&#20104;&#20102;&#21160;&#24577;&#35760;&#24518;&#21644;&#33258;&#25105;&#21453;&#24605;&#33021;&#21147;&#65292;&#20197;&#22686;&#24378;&#20854;&#20219;&#21153;&#29305;&#23450;&#30340;&#34892;&#21160;&#36873;&#25321;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20915;&#31574;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#30340;&#21457;&#23637;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#20869;&#37096;&#27169;&#22411;&#24494;&#35843;&#12289;&#22806;&#37096;&#27169;&#22411;&#24494;&#35843;&#25110;&#22312;&#23450;&#20041;&#30340;&#29366;&#24577;&#31354;&#38388;&#19978;&#36827;&#34892;&#31574;&#30053;&#20248;&#21270;&#12290;&#30001;&#20110;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#25110;&#32570;&#20047;&#33391;&#22909;&#23450;&#20041;&#30340;&#29366;&#24577;&#31354;&#38388;&#65292;&#23454;&#29616;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#20250;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#20195;&#29702;&#27809;&#26377;&#20154;&#31867;&#20915;&#31574;&#36807;&#31243;&#22266;&#26377;&#30340;&#26576;&#20123;&#21697;&#36136;&#65292;&#29305;&#21035;&#26159;&#20174;&#38169;&#35823;&#20013;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#21453;&#24605;&#65292;&#20154;&#31867;&#21487;&#20197;&#36890;&#36807;&#35797;&#38169;&#36807;&#31243;&#39640;&#25928;&#22320;&#35299;&#20915;&#26032;&#30340;&#38382;&#39064;&#12290;&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986; Reflexion&#65292;&#19968;&#31181;&#23558;&#21160;&#24577;&#35760;&#24518;&#21644;&#33258;&#25105;&#21453;&#24605;&#33021;&#21147;&#36171;&#20104;&#26234;&#33021;&#20307;&#30340;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#20854;&#29616;&#26377;&#30340;&#25512;&#29702;&#36712;&#36857;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#34892;&#21160;&#36873;&#25321;&#33021;&#21147;&#12290;&#20026;&#20102;&#23454;&#29616;&#23436;&#20840;&#33258;&#21160;&#21270;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in decision-making large language model (LLM) agents have demonstrated impressive performance across various benchmarks. However, these state-of-the-art approaches typically necessitate internal model fine-tuning, external model fine-tuning, or policy optimization over a defined state space. Implementing these methods can prove challenging due to the scarcity of high-quality training data or the lack of well-defined state space. Moreover, these agents do not possess certain qualities inherent to human decision-making processes, specifically the ability to learn from mistakes. Self-reflection allows humans to efficiently solve novel problems through a process of trial and error. Building on recent research, we propose Reflexion, an approach that endows an agent with dynamic memory and self-reflection capabilities to enhance its existing reasoning trace and task-specific action choice abilities. To achieve full automation, we introduce a straightforward yet effective 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20572;&#29992;&#35789;&#22312;&#36719;&#20214;&#24037;&#31243;&#25991;&#26723;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#32463;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#30340;&#20572;&#29992;&#35789;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#30740;&#31350;&#24037;&#20855;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;19&#20010;&#35780;&#20272;&#25514;&#26045;&#20013;&#26377;17&#20010;&#35780;&#20272;&#25514;&#26045;&#21463;&#30410;&#20110;&#20572;&#29992;&#35789;&#30340;&#28040;&#38500;&#12290;</title><link>http://arxiv.org/abs/2303.10439</link><description>&lt;p&gt;
&#22788;&#29702;&#36719;&#20214;&#24037;&#31243;&#25991;&#26723;&#30340;&#20572;&#29992;&#35789;&#65306;&#23427;&#20204;&#37325;&#35201;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Stop Words for Processing Software Engineering Documents: Do they Matter?. (arXiv:2303.10439v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20572;&#29992;&#35789;&#22312;&#36719;&#20214;&#24037;&#31243;&#25991;&#26723;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#32463;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#30340;&#20572;&#29992;&#35789;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#30740;&#31350;&#24037;&#20855;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;19&#20010;&#35780;&#20272;&#25514;&#26045;&#20013;&#26377;17&#20010;&#35780;&#20272;&#25514;&#26045;&#21463;&#30410;&#20110;&#20572;&#29992;&#35789;&#30340;&#28040;&#38500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20572;&#29992;&#35789;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#19981;&#20855;&#26377;&#39044;&#27979;&#24615;&#30340;&#65292;&#22240;&#27492;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#36890;&#24120;&#20250;&#34987;&#21435;&#38500;&#12290;&#28982;&#32780;&#65292;&#19981;&#30830;&#23450;&#24615;&#35789;&#27719;&#30340;&#23450;&#20041;&#26159;&#27169;&#31946;&#30340;&#65292;&#22240;&#27492;&#22823;&#22810;&#25968;&#31639;&#27861;&#20351;&#29992;&#22522;&#20110;&#36890;&#29992;&#30693;&#35782;&#30340;&#20572;&#29992;&#35789;&#21015;&#34920;&#26469;&#21435;&#38500;&#20572;&#29992;&#35789;&#12290;&#23398;&#32773;&#20204;&#19968;&#30452;&#22312;&#23601;&#20572;&#29992;&#35789;&#30340;&#20351;&#29992;&#20215;&#20540;&#36827;&#34892;&#35752;&#35770;&#65292;&#29305;&#21035;&#26159;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#35774;&#32622;&#20013;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20572;&#29992;&#35789;&#21435;&#38500;&#22312;&#36719;&#20214;&#24037;&#31243;&#32972;&#26223;&#19979;&#30340;&#23454;&#29992;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22797;&#21046;&#24182;&#23454;&#39564;&#20102;&#19977;&#20010;&#36719;&#20214;&#24037;&#31243;&#30740;&#31350;&#24037;&#20855;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#36719;&#20214;&#24037;&#31243;&#39046;&#22495;&#30456;&#20851;&#25991;&#26412;&#30340;&#35821;&#26009;&#24211;&#65292;&#21253;&#25324;&#26469;&#33258; Stack Overflow &#30340;10,000&#20010;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#20256;&#32479;&#30340;&#20449;&#24687;&#35770;&#26041;&#27861;&#35782;&#21035;&#20102;200&#20010;&#39046;&#22495;&#29305;&#23450;&#30340;&#20572;&#29992;&#35789;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#30340;&#20572;&#29992;&#35789;&#19982;&#20351;&#29992;&#36890;&#29992;&#20572;&#29992;&#21015;&#34920;&#30456;&#27604;&#65292;&#26174;&#30528;&#25552;&#39640;&#20102;&#30740;&#31350;&#24037;&#20855;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;19&#20010;&#35780;&#20272;&#25514;&#26045;&#20013;&#26377;17&#20010;&#35780;&#20272;&#25514;&#26045;&#21463;&#30410;&#20110;&#20572;&#29992;&#35789;&#30340;&#28040;&#38500;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#35777;&#26126;&#20102;&#22312;&#22788;&#29702;&#36719;&#20214;&#24037;&#31243;&#25991;&#26723;&#20013;&#21435;&#38500;&#39046;&#22495;&#29305;&#23450;&#30340;&#20572;&#29992;&#35789;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stop words, which are considered non-predictive, are often eliminated in natural language processing tasks. However, the definition of uninformative vocabulary is vague, so most algorithms use general knowledge-based stop lists to remove stop words. There is an ongoing debate among academics about the usefulness of stop word elimination, especially in domain-specific settings. In this work, we investigate the usefulness of stop word removal in a software engineering context. To do this, we replicate and experiment with three software engineering research tools from related work. Additionally, we construct a corpus of software engineering domain-related text from 10,000 Stack Overflow questions and identify 200 domain-specific stop words using traditional information-theoretic methods. Our results show that the use of domain-specific stop words significantly improved the performance of research tools compared to the use of a general stop list and that 17 out of 19 evaluation measures sh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#21521;&#37327;&#21270;&#25216;&#26415;&#20013;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#27969;&#34892;&#30340;&#23884;&#20837;&#26041;&#26696;&#20855;&#26377;Hamming&#36317;&#31163;&#24847;&#20041;&#19978;&#30340;&#40065;&#26834;&#24615;&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#23450;&#37327;&#36793;&#30028;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20013;&#30340;&#24120;&#25968;&#21463;&#25991;&#26723;&#38271;&#24230;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.07203</link><description>&lt;p&gt;
&#20851;&#20110;&#25991;&#26412;&#21521;&#37327;&#21270;&#25216;&#26415;&#30340;&#40065;&#26834;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Robustness of Text Vectorizers. (arXiv:2303.07203v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#21521;&#37327;&#21270;&#25216;&#26415;&#20013;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#27969;&#34892;&#30340;&#23884;&#20837;&#26041;&#26696;&#20855;&#26377;Hamming&#36317;&#31163;&#24847;&#20041;&#19978;&#30340;&#40065;&#26834;&#24615;&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#23450;&#37327;&#36793;&#30028;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20013;&#30340;&#24120;&#25968;&#21463;&#25991;&#26723;&#38271;&#24230;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#27169;&#22411;&#23545;&#36755;&#20837;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#27169;&#22411;&#36890;&#24120;&#21253;&#21547;&#31532;&#19968;&#23618;&#23884;&#20837;&#65292;&#23558;&#35789;&#27719;&#24207;&#21015;&#36716;&#25442;&#20026;&#21521;&#37327;&#34920;&#31034;&#12290;&#34429;&#28982;&#36830;&#32493;&#36755;&#20837;&#30340;&#31283;&#20581;&#24615;&#24050;&#32463;&#34987;&#24456;&#22909;&#22320;&#29702;&#35299;&#65292;&#20294;&#32771;&#34385;&#21040;&#31163;&#25955;&#21464;&#21270;(&#27604;&#22914;&#26367;&#25442;&#21477;&#23376;&#20013;&#30340;&#19968;&#20010;&#35789;)&#65292;&#24773;&#20917;&#23601;&#19981;&#37027;&#20040;&#26126;&#30830;&#20102;&#12290;&#26412;&#25991;&#27491;&#24335;&#35777;&#26126;&#20102;&#27969;&#34892;&#30340;&#23884;&#20837;&#26041;&#26696;(&#22914;&#25340;&#25509;&#12289;TF-IDF&#12289;&#27573;&#33853;&#21521;&#37327;)&#22312;Hamming&#36317;&#31163;&#24847;&#20041;&#19979;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#20026;&#36825;&#20123;&#26041;&#27861;&#25552;&#20379;&#20102;&#23450;&#37327;&#36793;&#30028;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20013;&#30340;&#24120;&#25968;&#22914;&#20309;&#21463;&#25991;&#26723;&#38271;&#24230;&#24433;&#21709;&#12290;&#36825;&#20123;&#21457;&#29616;&#36890;&#36807;&#19968;&#31995;&#21015;&#25968;&#20540;&#23454;&#20363;&#21152;&#20197;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
A fundamental issue in machine learning is the robustness of the model with respect to changes in the input. In natural language processing, models typically contain a first embedding layer, transforming a sequence of tokens into vector representations. While the robustness with respect to changes of continuous inputs is well-understood, the situation is less clear when considering discrete changes, for instance replacing a word by another in an input sentence. Our work formally proves that popular embedding schemes, such as concatenation, TF-IDF, and Paragraph Vector (a.k.a. doc2vec), exhibit robustness in the H\"older or Lipschitz sense with respect to the Hamming distance. We provide quantitative bounds for these schemes and demonstrate how the constants involved are affected by the length of the document. These findings are exemplified through a series of numerical examples.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26816;&#39564;&#20102; ChatGPT &#22312; 25 &#20010;&#19981;&#21516;&#30340; NLP &#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#23427;&#26159;&#19968;&#20010;&#19975;&#33021;&#30340; AI &#27169;&#22411;&#65292;&#20294;&#26080;&#20851;&#32039;&#35201;&#30340;&#34920;&#29616;&#21487;&#33021;&#20250;&#23545;&#26576;&#20123;&#20219;&#21153;&#30340;&#34920;&#29616;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2302.10724</link><description>&lt;p&gt;
ChatGPT&#65306;&#24212;&#20184;&#21315;&#20107;&#30340;&#19975;&#33021;&#22411; AI&#65292;&#20294;&#26080;&#25152;&#19987;&#31934;
&lt;/p&gt;
&lt;p&gt;
ChatGPT: Jack of all trades, master of none. (arXiv:2302.10724v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26816;&#39564;&#20102; ChatGPT &#22312; 25 &#20010;&#19981;&#21516;&#30340; NLP &#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#23427;&#26159;&#19968;&#20010;&#19975;&#33021;&#30340; AI &#27169;&#22411;&#65292;&#20294;&#26080;&#20851;&#32039;&#35201;&#30340;&#34920;&#29616;&#21487;&#33021;&#20250;&#23545;&#26576;&#20123;&#20219;&#21153;&#30340;&#34920;&#29616;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
OpenAI &#25512;&#20986;&#20102;&#32842;&#22825;&#29983;&#25104;&#39044;&#35757;&#32451; Transformer&#65288;ChatGPT&#65289;&#65292;&#38761;&#26032;&#20102;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20114;&#21160;&#30340;&#26041;&#27861;&#12290;&#35768;&#22810;&#30740;&#31350;&#36890;&#36807;&#27979;&#35797; ChatGPT &#22312;&#20247;&#25152;&#21608;&#30693;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#65292;&#26469;&#35780;&#20272;&#35813;&#27169;&#22411;&#30340;&#25928;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#22823;&#22810;&#38750;&#33258;&#21160;&#21270;&#65292;&#24182;&#19988;&#35268;&#27169;&#38750;&#24120;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#22312; 25 &#20010;&#19981;&#21516;&#30340; NLP &#20219;&#21153;&#19978;&#26816;&#39564;&#20102; ChatGPT &#30340;&#24615;&#33021;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#20219;&#21153;&#29978;&#33267;&#23545;&#20154;&#31867;&#32780;&#35328;&#37117;&#26159;&#20027;&#35266;&#30340;&#65292;&#20363;&#22914;&#24773;&#24863;&#20998;&#26512;&#12289;&#24773;&#32490;&#35782;&#21035;&#12289;&#25915;&#20987;&#24615;&#21644;&#31435;&#22330;&#26816;&#27979;&#12290;&#21478;&#19968;&#20123;&#20219;&#21153;&#21017;&#38656;&#35201;&#26356;&#23458;&#35266;&#30340;&#25512;&#29702;&#65292;&#22914;&#35789;&#20041;&#28040;&#27495;&#12289;&#35821;&#35328;&#21487;&#25509;&#21463;&#24615;&#21644;&#38382;&#31572;&#12290;&#25105;&#20204;&#36824;&#23545; GPT-4 &#27169;&#22411;&#22312;&#20116;&#20010;&#36873;&#23450;&#30340; NLP &#20219;&#21153;&#23376;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#33258;&#21160;&#21270;&#20102; ChatGPT &#21644; GPT-4 &#30340;&#24341;&#23548;&#36807;&#31243;&#65292;&#24182;&#20998;&#26512;&#20102;&#36229;&#36807; 49k &#20010;&#21709;&#24212;&#12290;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#35299;&#20915;&#26041;&#26696;&#65288;SOTA&#65289;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#19968;&#20123;&#20219;&#21153;&#19978; ChatGPT &#30340;&#24615;&#33021;&#23384;&#22312;&#19968;&#23450;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;
OpenAI has released the Chat Generative Pre-trained Transformer (ChatGPT) and revolutionized the approach in artificial intelligence to human-model interaction. Several publications on ChatGPT evaluation test its effectiveness on well-known natural language processing (NLP) tasks. However, the existing studies are mostly non-automated and tested on a very limited scale. In this work, we examined ChatGPT's capabilities on 25 diverse analytical NLP tasks, most of them subjective even to humans, such as sentiment analysis, emotion recognition, offensiveness, and stance detection. In contrast, the other tasks require more objective reasoning like word sense disambiguation, linguistic acceptability, and question answering. We also evaluated GPT-4 model on five selected subsets of NLP tasks. We automated ChatGPT and GPT-4 prompting process and analyzed more than 49k responses. Our comparison of its results with available State-of-the-Art (SOTA) solutions showed that the average loss in quali
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102; MultiInstruct&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#20248;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#25506;&#32034;&#22810;&#31181;&#36801;&#31227;&#23398;&#20064;&#31574;&#30053;&#20174;&#22823;&#35268;&#27169;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#25968;&#25454;&#38598;&#20013;&#25552;&#39640;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#20854;&#22312;&#21508;&#31181;&#26410;&#35265;&#36807;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#20855;&#26377;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#34920;&#29616;&#65292;&#20197;&#21450;&#35774;&#35745;&#30340;&#26032;&#30340;&#20219;&#21153;&#23436;&#25104;&#29575;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2212.10773</link><description>&lt;p&gt;
MultiInstruct: &#36890;&#36807;&#25351;&#20196;&#35843;&#20248;&#26469;&#25913;&#21892;&#22810;&#27169;&#24577;&#38646;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning. (arXiv:2212.10773v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10773
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; MultiInstruct&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#20248;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#25506;&#32034;&#22810;&#31181;&#36801;&#31227;&#23398;&#20064;&#31574;&#30053;&#20174;&#22823;&#35268;&#27169;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#25968;&#25454;&#38598;&#20013;&#25552;&#39640;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#20854;&#22312;&#21508;&#31181;&#26410;&#35265;&#36807;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#20855;&#26377;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#34920;&#29616;&#65292;&#20197;&#21450;&#35774;&#35745;&#30340;&#26032;&#30340;&#20219;&#21153;&#23436;&#25104;&#29575;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#20248;&#26159;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#22312;&#25351;&#20196;&#25351;&#23450;&#30340;&#20219;&#21153;&#19978;&#23545;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#23637;&#29616;&#20102;&#26377;&#24076;&#26395;&#30340;&#38646;&#26679;&#26412;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#23427;&#23578;&#26410;&#34987;&#29992;&#20110;&#35270;&#35273;&#21644;&#22810;&#27169;&#24577;&#20219;&#21153;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102; MultiInstruct&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#20248;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547; 47 &#20010;&#19981;&#21516;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#28085;&#30422;&#20102; 11 &#20010;&#24191;&#27867;&#30340;&#31867;&#21035;&#12290;&#27599;&#20010;&#20219;&#21153;&#33267;&#23569;&#35774;&#35745;&#26377; 5,000 &#20010;&#23454;&#20363;&#65288;&#36755;&#20837;-&#36755;&#20986;&#23545;&#65289;&#26469;&#33258;&#29616;&#26377;&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#21644; 5 &#20010;&#19987;&#23478;&#32534;&#20889;&#30340;&#25351;&#20196;&#12290;&#25105;&#20204;&#36873;&#21462; OFA &#20316;&#20026;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#20248;&#30340;&#22522;&#30784;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#25506;&#32034;&#22810;&#31181;&#36801;&#31227;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#21033;&#29992;&#22823;&#35268;&#27169;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#20854;&#22312;&#21508;&#31181;&#26410;&#35265;&#36807;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#20855;&#26377;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#34920;&#29616;&#65292;&#20197;&#21450;&#20174;&#32431;&#25991;&#26412;&#25351;&#20196;&#20013;&#33719;&#24471;&#36801;&#31227;&#23398;&#20064;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#23436;&#25104;&#29575;&#25351;&#26631;&#26469;&#35780;&#20272;&#38646;&#26679;&#26412;&#24615;&#33021;&#65292;&#24230;&#37327;&#27169;&#22411;&#22312;&#20165;&#26377;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction tuning, a new learning paradigm that fine-tunes pre-trained language models on tasks specified through instructions, has shown promising zero-shot performance on various natural language processing tasks. However, it's still not explored for vision and multimodal tasks. In this work, we introduce MultiInstruct, the first multimodal instruction tuning benchmark dataset that consists of 47 diverse multimodal tasks covering 11 broad categories. Each task is designed at least with 5,000 instances (input-out pairs) from existing open-source datasets and 5 expert-written instructions. We take OFA as the base pre-trained model for multimodal instruction tuning, and to improve its performance, we explore multiple transfer learning strategies to leverage the large-scale Natural Instructions dataset. Experimental results demonstrate its strong zero-shot performance on various unseen multimodal tasks and the benefit of transfer learning from text-only instructions. We also design a ne
&lt;/p&gt;</description></item><item><title>BLIND&#21487;&#20197;&#22312;&#27809;&#26377;&#20808;&#21069;&#20102;&#35299;&#25968;&#25454;&#38598;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#28040;&#38500;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2212.10563</link><description>&lt;p&gt;
BLIND: &#26080;&#20154;&#21475;&#32479;&#35745;&#23398;&#30340;&#20559;&#35265;&#21435;&#38500;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
BLIND: Bias Removal With No Demographics. (arXiv:2212.10563v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10563
&lt;/p&gt;
&lt;p&gt;
BLIND&#21487;&#20197;&#22312;&#27809;&#26377;&#20808;&#21069;&#20102;&#35299;&#25968;&#25454;&#38598;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#28040;&#38500;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#24448;&#24448;&#20250;&#27169;&#20223;&#21644;&#25918;&#22823;&#31038;&#20250;&#20559;&#35265;&#12290;&#24120;&#35265;&#30340;&#20943;&#36731;&#20559;&#35265;&#30340;&#26041;&#27861;&#38656;&#35201;&#20808;&#20102;&#35299;&#21738;&#20123;&#31867;&#22411;&#30340;&#20559;&#35265;&#38656;&#35201;&#34987;&#32416;&#27491;&#65288;&#20363;&#22914;&#24615;&#21035;&#25110;&#31181;&#26063;&#20559;&#35265;&#65289;&#20197;&#21450;&#19982;&#27599;&#20010;&#25968;&#25454;&#26679;&#26412;&#30456;&#20851;&#32852;&#30340;&#31038;&#20250;&#32676;&#20307;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;BLIND&#30340;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#27809;&#26377;&#23545;&#25968;&#25454;&#38598;&#20013;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#30340;&#20808;&#21069;&#20102;&#35299;&#19979;&#36827;&#34892;&#20559;&#35265;&#21435;&#38500;&#12290;&#22312;&#35757;&#32451;&#19979;&#28216;&#20219;&#21153;&#27169;&#22411;&#26102;&#65292;BLIND&#20351;&#29992;&#19968;&#20010;&#36741;&#21161;&#27169;&#22411;&#26469;&#39044;&#27979;&#20027;&#27169;&#22411;&#30340;&#25104;&#21151;&#65292;&#24182;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20943;&#23569;&#36825;&#20123;&#21463;&#21040;&#20559;&#35265;&#26679;&#26412;&#30340;&#26435;&#37325;&#12290;&#22522;&#20110;&#23545;&#24773;&#24863;&#20998;&#31867;&#21644;&#32844;&#19994;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#31181;&#26063;&#21644;&#24615;&#21035;&#20559;&#35265;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;BLIND&#21487;&#20197;&#22312;&#19981;&#20381;&#36182;&#26114;&#36149;&#30340;&#20154;&#21475;&#32479;&#35745;&#27880;&#37322;&#36807;&#31243;&#30340;&#24773;&#20917;&#19979;&#28040;&#38500;&#31038;&#20250;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#38656;&#35201;&#20154;&#21475;&#32479;&#35745;&#23398;&#20449;&#24687;&#30340;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#26377;&#26102;&#29978;&#33267;&#36229;&#36807;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models trained on real-world data tend to imitate and amplify social biases. Common methods to mitigate biases require prior information on the types of biases that should be mitigated (e.g., gender or racial bias) and the social groups associated with each data sample. In this work, we introduce BLIND, a method for bias removal with no prior knowledge of the demographics in the dataset. While training a model on a downstream task, BLIND detects biased samples using an auxiliary model that predicts the main model's success, and down-weights those samples during the training process. Experiments with racial and gender biases in sentiment classification and occupation classification tasks demonstrate that BLIND mitigates social biases without relying on a costly demographic annotation process. Our method is competitive with other methods that require demographic information and sometimes even surpasses them.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#25345;&#32493;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#24050;&#26377;&#30340;&#32763;&#35793;&#27169;&#22411;&#26469;&#25552;&#39640;&#19968;&#20010;&#26032;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.09097</link><description>&lt;p&gt;
&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#25345;&#32493;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Continual Knowledge Distillation for Neural Machine Translation. (arXiv:2212.09097v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09097
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#25345;&#32493;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#24050;&#26377;&#30340;&#32763;&#35793;&#27169;&#22411;&#26469;&#25552;&#39640;&#19968;&#20010;&#26032;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#35768;&#22810;&#24179;&#34892;&#35821;&#26009;&#24211;&#22240;&#20026;&#29256;&#26435;&#12289;&#25968;&#25454;&#38544;&#31169;&#21644;&#31454;&#20105;&#24046;&#24322;&#31561;&#21407;&#22240;&#19981;&#20844;&#24320;&#65292;&#20294;&#35757;&#32451;&#22909;&#30340;&#32763;&#35793;&#27169;&#22411;&#22312;&#24320;&#25918;&#24179;&#21488;&#19978;&#21464;&#24471;&#36234;&#26469;&#36234;&#23481;&#26131;&#24471;&#21040;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#25345;&#32493;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#32763;&#35793;&#27169;&#22411;&#26469;&#25552;&#39640;&#24863;&#20852;&#36259;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22522;&#26412;&#24605;&#36335;&#26159;&#23558;&#27599;&#20010;&#24050;&#35757;&#32451;&#27169;&#22411;&#30340;&#30693;&#35782;&#25353;&#39034;&#24207;&#36716;&#31227;&#32473;&#34987;&#33976;&#39311;&#27169;&#22411;&#12290;&#22312;&#20013;&#33521;&#21644;&#24503;&#33521;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21516;&#36136;&#21644;&#24322;&#36136;&#35757;&#32451;&#27169;&#22411;&#35774;&#32622;&#19979;&#37117;&#23454;&#29616;&#20102;&#26174;&#33879;&#19988;&#19968;&#33268;&#30340;&#25913;&#36827;&#65292;&#24182;&#19988;&#23545;&#24694;&#24847;&#27169;&#22411;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While many parallel corpora are not publicly accessible for data copyright, data privacy and competitive differentiation reasons, trained translation models are increasingly available on open platforms. In this work, we propose a method called continual knowledge distillation to take advantage of existing translation models to improve one model of interest. The basic idea is to sequentially transfer knowledge from each trained model to the distilled model. Extensive experiments on Chinese-English and German-English datasets show that our method achieves significant and consistent improvements over strong baselines under both homogeneous and heterogeneous trained model settings and is robust to malicious models.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;&#23454;&#29616;&#30340;&#32852;&#37030;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#19981;&#20849;&#20139;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20801;&#35768;&#22810;&#20010;&#26041;&#20849;&#21516;&#35757;&#32451;&#20027;&#39064;&#27169;&#22411;&#21644;&#20445;&#25252;&#33410;&#28857;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2212.02269</link><description>&lt;p&gt;
&#32852;&#37030;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Federated Neural Topic Models. (arXiv:2212.02269v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02269
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;&#23454;&#29616;&#30340;&#32852;&#37030;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#19981;&#20849;&#20139;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20801;&#35768;&#22810;&#20010;&#26041;&#20849;&#21516;&#35757;&#32451;&#20027;&#39064;&#27169;&#22411;&#21644;&#20445;&#25252;&#33410;&#28857;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20027;&#39064;&#24314;&#27169;&#24050;&#25104;&#20026;&#32452;&#32455;&#21644;&#24635;&#32467;&#22823;&#22411;&#25991;&#26723;&#38598;&#21512;&#25110;&#22312;&#20854;&#20013;&#25628;&#32034;&#29305;&#23450;&#27169;&#24335;&#30340;&#24378;&#22823;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#24403;&#20174;&#19981;&#21516;&#26469;&#28304;&#20132;&#21449;&#20998;&#26512;&#25968;&#25454;&#26102;&#65292;&#21487;&#33021;&#20250;&#20986;&#29616;&#38544;&#31169;&#38382;&#39064;&#12290;&#32852;&#37030;&#20027;&#39064;&#24314;&#27169;&#36890;&#36807;&#20801;&#35768;&#22810;&#20010;&#26041;&#20849;&#21516;&#35757;&#32451;&#20027;&#39064;&#27169;&#22411;&#32780;&#19981;&#20849;&#20139;&#20854;&#25968;&#25454;&#26469;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;&#23454;&#29616;&#30340;&#32852;&#37030;&#23454;&#29616;&#65292;&#26174;&#31034;&#20854;&#22312;&#33410;&#28857;&#25991;&#26723;&#30340;&#20027;&#39064;&#22810;&#26679;&#24615;&#21644;&#24314;&#31435;&#32852;&#21512;&#27169;&#22411;&#30340;&#38656;&#35201;&#26102;&#30340;&#20248;&#21183;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#24403;&#20110;&#38598;&#20013;&#27169;&#22411;&#35757;&#32451;&#65292;&#20294;&#20445;&#25252;&#33410;&#28857;&#30340;&#38544;&#31169;&#12290;&#20351;&#29992;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#35828;&#26126;&#20102;&#36825;&#31181;&#32852;&#37030;&#22330;&#26223;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the last years, topic modeling has emerged as a powerful technique for organizing and summarizing big collections of documents or searching for particular patterns in them. However, privacy concerns may arise when cross-analyzing data from different sources. Federated topic modeling solves this issue by allowing multiple parties to jointly train a topic model without sharing their data. While several federated approximations of classical topic models do exist, no research has been conducted on their application for neural topic models. To fill this gap, we propose and analyze a federated implementation based on state-of-the-art neural topic modeling implementations, showing its benefits when there is a diversity of topics across the nodes' documents and the need to build a joint model. In practice, our approach is equivalent to a centralized model training, but preserves the privacy of the nodes. Advantages of this federated scenario are illustrated by means of experiments using b
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#26041;&#27861;&#26469;&#36866;&#24212;&#21322;&#21442;&#25968;&#26368;&#36817;&#37051;&#35821;&#35328;&#27169;&#22411;&#65288;$k$NN-LMs&#65289;&#65292;&#24182;&#36816;&#29992;&#28040;&#34701;&#23454;&#39564;&#21644;&#23545;&#22810;&#20010;&#36866;&#24212;&#39046;&#22495;&#30340;&#24191;&#27867;&#35780;&#20272;&#65292;&#21457;&#29616;&#32452;&#21512;&#36866;&#24212;&#26041;&#27861; consistently outperforms&#21333;&#19968;&#36866;&#24212;&#31574;&#30053;&#21644;&#38646;&#26679;&#26412;&#65288;$k$NN-LM&#65289;&#22522;&#32447;&#65292;&#37325;&#35745;&#20998;&#27169;&#22359;&#20351;&#24471;&#24615;&#33021;&#25552;&#39640;&#26368;&#22810;&#12290;</title><link>http://arxiv.org/abs/2211.07828</link><description>&lt;p&gt;
&#26368;&#36817;&#37051;&#35821;&#35328;&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Adaptation Approaches for Nearest Neighbor Language Models. (arXiv:2211.07828v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#26041;&#27861;&#26469;&#36866;&#24212;&#21322;&#21442;&#25968;&#26368;&#36817;&#37051;&#35821;&#35328;&#27169;&#22411;&#65288;$k$NN-LMs&#65289;&#65292;&#24182;&#36816;&#29992;&#28040;&#34701;&#23454;&#39564;&#21644;&#23545;&#22810;&#20010;&#36866;&#24212;&#39046;&#22495;&#30340;&#24191;&#27867;&#35780;&#20272;&#65292;&#21457;&#29616;&#32452;&#21512;&#36866;&#24212;&#26041;&#27861; consistently outperforms&#21333;&#19968;&#36866;&#24212;&#31574;&#30053;&#21644;&#38646;&#26679;&#26412;&#65288;$k$NN-LM&#65289;&#22522;&#32447;&#65292;&#37325;&#35745;&#20998;&#27169;&#22359;&#20351;&#24471;&#24615;&#33021;&#25552;&#39640;&#26368;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#21442;&#25968;&#26368;&#36817;&#37051;&#35821;&#35328;&#27169;&#22411;&#65288;$k$NN-LMs&#65289;&#36890;&#36807;&#21033;&#29992;&#23545;&#22806;&#37096;&#20869;&#23384;&#25968;&#25454;&#23384;&#20648;&#22120;&#30340;&#22823;&#35268;&#27169;&#37051;&#22495;&#26816;&#32034;&#65292;&#27604;&#32431;&#21442;&#25968;&#27169;&#22411;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22914;&#20309;&#36866;&#24212;&#26032;&#39046;&#22495;&#30340;&#27169;&#22411;&#65292;&#30446;&#21069;&#30740;&#31350;&#36824;&#24456;&#23569;&#12290;&#26412;&#25991;&#35797;&#22270;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#24182;&#24314;&#35758;&#20197;&#19979;&#36866;&#24212;$k$NN-LMs&#30340;&#26041;&#27861;&#8212;&#8212;1&#65289;&#36890;&#36807;&#20351;&#29992;&#36716;&#25442;&#22120;&#26469;&#36866;&#24212;&#24213;&#23618;LM&#65292;2&#65289;&#25193;&#23637;&#37051;&#22495;&#26816;&#32034;&#21040;&#21478;&#19968;&#20010;&#36866;&#24212;&#25968;&#25454;&#23384;&#20648;&#65292;3&#65289;&#20351;&#29992;&#23398;&#20064;&#30340;&#37325;&#35745;&#20998;&#27169;&#22359;&#26469;&#36866;&#24212;&#26816;&#32034;&#37051;&#23621;&#30340;&#26435;&#37325;&#65288;&#20998;&#25968;&#65289;&#12290;&#25105;&#20204;&#20998;&#21035;&#30740;&#31350;&#20102;&#27599;&#31181;&#36866;&#24212;&#31574;&#30053;&#65292;&#20197;&#21450;&#36890;&#36807;&#28040;&#34701;&#23454;&#39564;&#21644;&#23545;7&#20010;&#36866;&#24212;&#23383;&#27573;&#36816;&#34892;&#30340;&#24191;&#27867;&#35780;&#20272;&#26469;&#32452;&#21512;&#24615;&#33021;&#30340;&#25552;&#39640;&#12290;&#25105;&#20204;&#30340;&#32452;&#21512;&#36866;&#24212;&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;&#32431;&#21442;&#25968;&#36866;&#24212;&#21644;&#38646;&#26679;&#26412;&#65288;$k$NN-LM&#65289;&#22522;&#32447;&#65292;&#36825;&#20123;&#22522;&#32447;&#20174;&#36866;&#24212;&#25968;&#25454;&#26500;&#24314;&#25968;&#25454;&#23384;&#20648;&#12290;&#24179;&#22343;&#32780;&#35328;&#65292;&#25105;&#20204;&#30475;&#21040;&#30456;&#23545;&#20110;&#22522;&#32447;&#30340;&#22256;&#24785;&#24230;&#38477;&#20302;&#26368;&#22810;20&#65285;&#65292;&#20854;&#20013;&#25105;&#20204;&#30340;&#37325;&#26032;&#35745;&#20998;-$k$NN-LM&#26041;&#27861;&#22312;&#25152;&#26377;&#36866;&#24212;&#22330;&#26223;&#20013;&#25552;&#20379;&#20102;&#26368;&#22823;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-parametric Nearest Neighbor Language Models ($k$NN-LMs) have produced impressive gains over purely parametric LMs, by leveraging large-scale neighborhood retrieval over external memory datastores. However, there has been little investigation into adapting such models for new domains. This work attempts to fill that gap and suggests the following approaches for adapting $k$NN-LMs -- 1) adapting the underlying LM (using Adapters), 2) expanding neighborhood retrieval over an additional adaptation datastore, and 3) adapting the weights (scores) of retrieved neighbors using a learned Rescorer module. We study each adaptation strategy separately, as well as the combined performance improvement through ablation experiments and an extensive set of evaluations run over seven adaptation domains. Our combined adaptation approach consistently outperforms purely parametric adaptation and zero-shot ($k$NN-LM) baselines that construct datastores from the adaptation data. On average, we see perpl
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#38382;&#39064;&#26085;&#30410;&#24341;&#36215;&#20851;&#27880;&#65292;&#20294;&#24403;&#21069;&#21435;&#20559;&#32622;&#26041;&#27861;&#24448;&#24448;&#20250;&#38477;&#20302;&#27169;&#22411;&#22312;&#20854;&#20182;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65307;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#35299;&#37322;&#27169;&#22411;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#26426;&#21046;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#21435;&#20559;&#32622;&#26041;&#27861;&#65292;&#33021;&#23454;&#29616;&#32531;&#35299;&#24615;&#21035;&#20559;&#35265;&#21516;&#26102;&#36991;&#20813;&#24615;&#33021;&#19979;&#38477;&#30340;&#21452;&#37325;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2211.07350</link><description>&lt;p&gt;
&#21435;&#20559;&#32622;&#19968;&#23450;&#20250;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#21527;?
&lt;/p&gt;
&lt;p&gt;
Does Debiasing Inevitably Degrade the Model Performance. (arXiv:2211.07350v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07350
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#38382;&#39064;&#26085;&#30410;&#24341;&#36215;&#20851;&#27880;&#65292;&#20294;&#24403;&#21069;&#21435;&#20559;&#32622;&#26041;&#27861;&#24448;&#24448;&#20250;&#38477;&#20302;&#27169;&#22411;&#22312;&#20854;&#20182;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65307;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#35299;&#37322;&#27169;&#22411;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#26426;&#21046;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#21435;&#20559;&#32622;&#26041;&#27861;&#65292;&#33021;&#23454;&#29616;&#32531;&#35299;&#24615;&#21035;&#20559;&#35265;&#21516;&#26102;&#36991;&#20813;&#24615;&#33021;&#19979;&#38477;&#30340;&#21452;&#37325;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#24050;&#32463;&#24341;&#36215;&#20102;&#36275;&#22815;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#23041;&#32961;&#21040;&#31038;&#20250;&#20844;&#27491;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22823;&#22810;&#25968;&#21435;&#20559;&#32622;&#26041;&#27861;&#22312;&#38477;&#20302;&#27169;&#22411;&#22312;&#20854;&#20182;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#19981;&#31283;&#23450;&#30340;&#24615;&#36136;&#65292;&#32780;&#36825;&#31181;&#38477;&#20302;&#30340;&#26426;&#21046;&#20173;&#28982;&#26159;&#31070;&#31192;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#24615;&#21035;&#20559;&#35265;&#30340;&#19977;&#31181;&#20505;&#36873;&#26426;&#21046;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#37322;&#30446;&#21069;&#30340;&#21435;&#20559;&#32622;&#26041;&#27861;&#22914;&#20309;&#23548;&#33268;&#24615;&#33021;&#38477;&#20302;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#19968;&#31181;&#21435;&#20559;&#32622;&#19981;&#20250;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#30340;&#36884;&#24452;&#65292;&#24182;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#31181;&#22240;&#26524;&#26816;&#27979;&#24494;&#35843;&#26041;&#27861;&#26469;&#32416;&#27491;&#24615;&#21035;&#20559;&#35265;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#21452;&#37325;&#25910;&#30410;&#65306;&#37096;&#20998;&#32531;&#35299;&#24615;&#21035;&#20559;&#35265;&#21516;&#26102;&#36991;&#20813;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gender bias in language models has attracted sufficient attention because it threatens social justice. However, most of the current debiasing methods degraded the model's performance on other tasks while the degradation mechanism is still mysterious. We propose a theoretical framework explaining the three candidate mechanisms of the language model's gender bias. We use our theoretical framework to explain why the current debiasing methods cause performance degradation. We also discover a pathway through which debiasing will not degrade the model performance. We further develop a causality-detection fine-tuning approach to correct gender bias. The numerical experiment demonstrates that our method is able to lead to double dividends: partially mitigating gender bias while avoiding performance degradation.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29983;&#25104;&#22810;&#35821;&#35328;&#30340;&#24615;&#21035;&#19981;&#26126;&#30830;&#30340;TTS&#22768;&#38899;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20986;&#30340;&#24615;&#21035;&#24863;&#30693;&#26041;&#27861;&#20174;&#28508;&#22312;&#35828;&#35805;&#20154;&#20013;&#26377;&#25928;&#22320;&#36827;&#34892;&#37319;&#26679;&#65292;&#25104;&#21151;&#29983;&#25104;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#12289;&#19968;&#33268;&#24615;&#21644;&#24615;&#21035;&#19981;&#26126;&#30830;&#24615;&#26356;&#24378;&#30340;&#22768;&#38899;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#23454;&#39564;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2211.00375</link><description>&lt;p&gt;
&#29983;&#25104;&#22810;&#35821;&#35328;&#30340;&#24615;&#21035;&#19981;&#26126;&#30830;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#22768;&#38899;
&lt;/p&gt;
&lt;p&gt;
Generating Multilingual Gender-Ambiguous Text-to-Speech Voices. (arXiv:2211.00375v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00375
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29983;&#25104;&#22810;&#35821;&#35328;&#30340;&#24615;&#21035;&#19981;&#26126;&#30830;&#30340;TTS&#22768;&#38899;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20986;&#30340;&#24615;&#21035;&#24863;&#30693;&#26041;&#27861;&#20174;&#28508;&#22312;&#35828;&#35805;&#20154;&#20013;&#26377;&#25928;&#22320;&#36827;&#34892;&#37319;&#26679;&#65292;&#25104;&#21151;&#29983;&#25104;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#12289;&#19968;&#33268;&#24615;&#21644;&#24615;&#21035;&#19981;&#26126;&#30830;&#24615;&#26356;&#24378;&#30340;&#22768;&#38899;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#23454;&#39564;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#29992;&#25143;&#30028;&#38754;&#30340;&#24615;&#21035;&#26159;&#20854;&#34987;&#24863;&#30693;&#36523;&#20221;&#30340;&#20851;&#38190;&#20803;&#32032;&#12290;&#26368;&#36817;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#30028;&#38754;&#24320;&#22987;&#37319;&#29992;&#19981;&#26126;&#30830;&#30340;&#24615;&#21035;&#65292;&#32780;&#19981;&#26159;&#26126;&#30830;&#30028;&#23450;&#20026;&#30007;&#24615;&#25110;&#22899;&#24615;&#12290;&#36825;&#39033;&#24037;&#20316;&#35299;&#20915;&#20102;&#22312;&#22810;&#35828;&#35805;&#20154;&#65292;&#22810;&#35821;&#35328;&#29615;&#22659;&#20013;&#29983;&#25104;&#26032;&#30340;&#24615;&#21035;&#19981;&#26126;&#30830;&#30340;TTS&#35821;&#38899;&#30340;&#20219;&#21153;&#12290;&#36825;&#26159;&#36890;&#36807;&#20351;&#29992;&#25552;&#20986;&#30340;&#24615;&#21035;&#24863;&#30693;&#26041;&#27861;&#26377;&#25928;&#22320;&#20174;&#28508;&#22312;&#30340;&#35828;&#35805;&#20154;&#23884;&#20837;&#31354;&#38388;&#20013;&#36827;&#34892;&#37319;&#26679;&#26469;&#23454;&#29616;&#30340;&#12290;&#24191;&#27867;&#30340;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#28165;&#26970;&#22320;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#29983;&#25104;&#19968;&#31995;&#21015;&#26032;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#22768;&#38899;&#65292;&#36825;&#20123;&#22768;&#38899;&#22312;&#25152;&#26377;&#32771;&#23519;&#30340;&#35821;&#35328;&#20013;&#37117;&#34987;&#35748;&#20026;&#27604;&#22522;&#32447;&#22768;&#38899;&#26356;&#20855;&#24615;&#21035;&#19981;&#26126;&#30830;&#24615;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#24615;&#21035;&#35748;&#30693;&#34987;&#21457;&#29616;&#22312;&#21548;&#20247;&#30340;&#20004;&#20010;&#20154;&#21475;&#32479;&#35745;&#22240;&#32032;&#26041;&#38754;&#20855;&#26377;&#40065;&#26834;&#24615;&#65306;&#27597;&#35821;&#21644;&#24615;&#21035;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21487;&#20197;&#21487;&#38752;&#22320;&#29983;&#25104;&#22810;&#31181;&#24615;&#21035;&#19981;&#26126;&#30830;&#22768;&#38899;&#30340;&#31995;&#32479;&#24615;&#21644;&#32463;&#36807;&#39564;&#35777;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The gender of any voice user interface is a key element of its perceived identity. Recently, there has been increasing interest in interfaces where the gender is ambiguous rather than clearly identifying as female or male. This work addresses the task of generating novel gender-ambiguous TTS voices in a multi-speaker, multilingual setting. This is accomplished by efficiently sampling from a latent speaker embedding space using a proposed gender-aware method. Extensive objective and subjective evaluations clearly indicate that this method is able to efficiently generate a range of novel, diverse voices that are consistent and perceived as more gender-ambiguous than a baseline voice across all the languages examined. Interestingly, the gender perception is found to be robust across two demographic factors of the listeners: native language and gender. To our knowledge, this is the first systematic and validated approach that can reliably generate a variety of gender-ambiguous voices.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#28608;&#27963;&#22270;&#30340;&#31232;&#30095;&#29616;&#35937;&#65292;&#21457;&#29616;&#22312;&#19981;&#21516;&#23618;&#25968;&#30340;&#21464;&#21387;&#22120;&#37197;&#32622;&#21644;&#20854;&#20182;&#20307;&#31995;&#32467;&#26500;&#20013;&#37117;&#20986;&#29616;&#20102;&#31232;&#30095;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2210.06313</link><description>&lt;p&gt;
&#24608;&#24816;&#31070;&#32463;&#20803;&#29616;&#35937;&#65306;&#21464;&#21387;&#22120;&#27169;&#22411;&#28608;&#27963;&#31232;&#30095;&#24615;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers. (arXiv:2210.06313v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#28608;&#27963;&#22270;&#30340;&#31232;&#30095;&#29616;&#35937;&#65292;&#21457;&#29616;&#22312;&#19981;&#21516;&#23618;&#25968;&#30340;&#21464;&#21387;&#22120;&#37197;&#32622;&#21644;&#20854;&#20182;&#20307;&#31995;&#32467;&#26500;&#20013;&#37117;&#20986;&#29616;&#20102;&#31232;&#30095;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#28608;&#27963;&#22270;&#31232;&#30095;&#30340;&#22855;&#29305;&#29616;&#35937;&#12290;&#25105;&#20204;&#36890;&#36807;&#20013;&#38388;&#23618;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#20351;&#29992;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#36755;&#20986;&#26469;&#34920;&#31034;&#28608;&#27963;&#22270;&#65292;&#31232;&#30095;&#26159;&#25351;&#24179;&#22343;&#24773;&#20917;&#19979;&#27599;&#20010;&#36755;&#20837;&#21040;MLP&#30340;&#38750;&#38646;&#20803;&#32032;&#38750;&#24120;&#23569;&#65288;&#20363;&#22914;&#65292;T5-Base&#20026;3.0&#65285;&#65292;ViT-B16&#20026;6.3&#65285;&#65289;&#12290;&#27492;&#22806;&#65292;&#36739;&#22823;&#30340;&#21464;&#21387;&#22120;&#21644;&#26356;&#23485;&#30340;MLP&#38544;&#34255;&#23618;&#32500;&#24230;&#20250;&#20135;&#29983;&#26356;&#31232;&#30095;&#30340;&#28608;&#27963;&#22270;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31232;&#30095;&#30340;&#20986;&#29616;&#26159;&#19968;&#31181;&#26222;&#36941;&#29616;&#35937;&#65292;&#23427;&#20986;&#29616;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35270;&#35273;&#20219;&#21153;&#20013;&#65292;&#20986;&#29616;&#22312;&#35757;&#32451;&#21644;&#35780;&#20272;&#25968;&#25454;&#20013;&#65292;&#22312;&#19981;&#21516;&#23618;&#25968;&#30340;&#21464;&#21387;&#22120;&#37197;&#32622;&#21644;&#20854;&#20182;&#20307;&#31995;&#32467;&#26500;&#20013;&#65292;&#20063;&#21253;&#25324;MLP-&#28151;&#21512;&#22120;&#21644;2&#23618;MLP&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#20351;&#29992;&#20855;&#26377;&#38543;&#26426;&#26631;&#31614;&#25110;&#38543;&#26426;&#36755;&#20837;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#20063;&#20250;&#20986;&#29616;&#31232;&#30095;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the curious phenomenon for machine learning models with Transformer architectures that their activation maps are sparse. By activation map we refer to the intermediate output of the multi-layer perceptrons (MLPs) after a ReLU activation function, and by sparse we mean that on average very few entries (e.g., 3.0% for T5-Base and 6.3% for ViT-B16) are nonzero for each input to MLP. Moreover, larger Transformers with more layers and wider MLP hidden dimensions are sparser as measured by the percentage of nonzero entries. Through extensive experiments we demonstrate that the emergence of sparsity is a prevalent phenomenon that occurs for both natural language processing and vision tasks, on both training and evaluation data, for Transformers of various configurations, at layers of all depth levels, as well as for other architectures including MLP-mixers and 2-layer MLPs. We show that sparsity also emerges using training datasets with random labels, or with random inputs,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; AMC &#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#40723;&#21169;&#22522;&#20110;&#26799;&#24230;&#30340;&#35299;&#37322;&#35206;&#30422;&#26377;&#27880;&#37322;&#30340;&#24863;&#20852;&#36259;&#21306;&#22495;&#65292;&#21363;&#32534;&#30721;&#21306;&#22495;&#12290;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#35270;&#35273; grounding &#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#21331;&#36234;&#65292;&#26377;&#26395;&#25104;&#20026;&#35270;&#35273; grounding &#39046;&#22495;&#30340;&#26032;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2206.15462</link><description>&lt;p&gt;
&#36890;&#36807;&#40723;&#21169;&#19968;&#33268;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#35299;&#37322;&#26469;&#25913;&#36827;&#35270;&#35273; grounding
&lt;/p&gt;
&lt;p&gt;
Improving Visual Grounding by Encouraging Consistent Gradient-based Explanations. (arXiv:2206.15462v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.15462
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; AMC &#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#40723;&#21169;&#22522;&#20110;&#26799;&#24230;&#30340;&#35299;&#37322;&#35206;&#30422;&#26377;&#27880;&#37322;&#30340;&#24863;&#20852;&#36259;&#21306;&#22495;&#65292;&#21363;&#32534;&#30721;&#21306;&#22495;&#12290;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#35270;&#35273; grounding &#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#21331;&#36234;&#65292;&#26377;&#26395;&#25104;&#20026;&#35270;&#35273; grounding &#39046;&#22495;&#30340;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#32536;&#30340;&#25439;&#22833;&#65292;&#29992;&#20110;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#40723;&#21169;&#22522;&#20110;&#26799;&#24230;&#30340;&#35299;&#37322;&#19982;&#21306;&#22495;&#32423;&#27880;&#37322;&#20445;&#25345;&#19968;&#33268;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#30446;&#26631;&#31216;&#20026; Attention Mask Consistency&#65288;AMC&#65289;&#65292;&#24182;&#35777;&#26126;&#23427;&#20135;&#29983;&#20102;&#27604;&#20381;&#36182;&#20110;&#21306;&#22495;&#32423;&#27880;&#37322;&#30340;&#27169;&#22411;&#26356;&#20248;&#36234;&#30340;&#35270;&#35273; grounding &#24615;&#33021;&#12290; AMC &#36890;&#36807;&#40723;&#21169;&#22522;&#20110;&#26799;&#24230;&#30340;&#35299;&#37322;&#25513;&#30721;&#65292;&#22312;&#21253;&#21547;&#27492;&#31867;&#27880;&#37322;&#30340;&#22270;&#20687;&#20013;&#65292;&#25226;&#23427;&#20204;&#30340;&#27880;&#24847;&#21147;&#20998;&#25968;&#20027;&#35201;&#38598;&#20013;&#22312;&#27880;&#37322;&#30340;&#24863;&#20852;&#36259;&#21306;&#22495;&#20869;&#12290;&#29305;&#21035;&#22320;&#65292;&#19968;&#20010;&#22312;&#26631;&#20934;&#35270;&#35273;-&#35821;&#35328;&#24314;&#27169;&#30446;&#26631;&#20043;&#19978;&#29992; AMC &#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#22312; Flickr30k &#35270;&#35273; grounding &#22522;&#20934;&#27979;&#35797;&#20013;&#33719;&#24471;&#20102;86.59%&#30340;&#26368;&#26032;&#32467;&#26524;&#65292;&#30456;&#27604;&#26368;&#20339;&#32467;&#26524;&#33719;&#24471;&#20102;5.48%&#30340;&#32477;&#23545;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24050;&#24314;&#31435;&#30340;&#25351;&#20195;&#34920;&#36798;&#29702;&#35299;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#36824;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a margin-based loss for vision-language model pretraining that encourages gradient-based explanations that are consistent with region-level annotations. We refer to this objective as Attention Mask Consistency (AMC) and demonstrate that it produces superior visual grounding performance compared to models that rely instead on region-level annotations for explicitly training an object detector such as Faster R-CNN. AMC works by encouraging gradient-based explanation masks that focus their attention scores mostly within annotated regions of interest for images that contain such annotations. Particularly, a model trained with AMC on top of standard vision-language modeling objectives obtains a state-of-the-art accuracy of 86.59% in the Flickr30k visual grounding benchmark, an absolute improvement of 5.48% when compared to the best previous model. Our approach also performs exceedingly well on established benchmarks for referring expression comprehension and offers the added bene
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;Beyond the Imitation Game&#22522;&#20934;&#27979;&#35797;&#65288;BIG-bench&#65289;&#65292;&#35813;&#27979;&#35797;&#38598;&#21253;&#21547;&#20102;204&#20010;&#21508;&#39046;&#22495;&#30340;&#38590;&#39064;&#65292;&#26088;&#22312;&#35780;&#20272;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20449;&#24687;&#21644;&#20934;&#22791;&#12290;</title><link>http://arxiv.org/abs/2206.04615</link><description>&lt;p&gt;
&#36229;&#36234;&#27169;&#20223;&#28216;&#25103;&#65306;&#37327;&#21270;&#21644;&#25299;&#23637;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models. (arXiv:2206.04615v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.04615
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;Beyond the Imitation Game&#22522;&#20934;&#27979;&#35797;&#65288;BIG-bench&#65289;&#65292;&#35813;&#27979;&#35797;&#38598;&#21253;&#21547;&#20102;204&#20010;&#21508;&#39046;&#22495;&#30340;&#38590;&#39064;&#65292;&#26088;&#22312;&#35780;&#20272;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20449;&#24687;&#21644;&#20934;&#22791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35268;&#27169;&#30340;&#22686;&#22823;&#65292;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#25968;&#37327;&#19978;&#30340;&#25552;&#21319;&#21644;&#26032;&#30340;&#23450;&#24615;&#33021;&#21147;&#12290;&#23613;&#31649;&#20855;&#26377;&#28508;&#22312;&#30340;&#36716;&#21464;&#24615;&#24433;&#21709;&#65292;&#20294;&#36825;&#20123;&#26032;&#30340;&#33021;&#21147;&#30446;&#21069;&#23578;&#26410;&#34987;&#20805;&#20998;&#25551;&#36848;&#12290;&#20026;&#20102;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20449;&#24687;&#65292;&#20026;&#21095;&#21464;&#30340;&#26032;&#22411;&#27169;&#22411;&#33021;&#21147;&#20570;&#20934;&#22791;&#65292;&#24182;&#32531;&#35299;&#31038;&#20250;&#26377;&#23475;&#24433;&#21709;&#65292;&#25105;&#20204;&#24517;&#39035;&#20102;&#35299;&#35821;&#35328;&#27169;&#22411;&#30340;&#29616;&#26377;&#21644;&#36817;&#26399;&#33021;&#21147;&#21644;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Beyond the Imitation Game&#22522;&#20934;&#27979;&#35797;&#65288;BIG-bench&#65289;&#12290;BIG-bench&#30446;&#21069;&#21253;&#25324;204&#20010;&#20219;&#21153;&#65292;&#30001;132&#20010;&#26426;&#26500;&#30340;450&#21517;&#20316;&#32773;&#36129;&#29486;&#12290;&#20219;&#21153;&#20027;&#39064;&#22810;&#26679;&#65292;&#28085;&#30422;&#20102;&#35821;&#35328;&#23398;&#12289;&#20799;&#31461;&#21457;&#23637;&#12289;&#25968;&#23398;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#29983;&#29289;&#23398;&#12289;&#29289;&#29702;&#23398;&#12289;&#31038;&#20250;&#20559;&#35265;&#12289;&#36719;&#20214;&#24320;&#21457;&#31561;&#31561;&#12290;BIG-bench&#19987;&#27880;&#20110;&#37027;&#20123;&#34987;&#35748;&#20026;&#36229;&#20986;&#20102;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;OpenAI&#30340;GPT&#27169;&#22411;&#21644;&#35895;&#27468;&#20869;&#37096;&#30340;&#23494;&#38598;&#36716;&#25442;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transform
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;DiMS&#65292;&#21487;&#20197;&#36890;&#36807;&#21387;&#32553;&#22810;&#27493;&#39588;&#26426;&#21046;&#26469;&#20248;&#21270;&#35299;&#30721;&#36807;&#31243;&#65292;&#25552;&#39640;&#26426;&#22120;&#32763;&#35793;&#30340;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2206.02999</link><description>&lt;p&gt;
DiMS&#65306;&#36845;&#20195;&#38750;&#33258;&#22238;&#24402;Transformer&#30340;&#21387;&#32553;&#22810;&#27493;&#39588;&#26426;&#21046;&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
DiMS: Distilling Multiple Steps of Iterative Non-Autoregressive Transformers for Machine Translation. (arXiv:2206.02999v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02999
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;DiMS&#65292;&#21487;&#20197;&#36890;&#36807;&#21387;&#32553;&#22810;&#27493;&#39588;&#26426;&#21046;&#26469;&#20248;&#21270;&#35299;&#30721;&#36807;&#31243;&#65292;&#25552;&#39640;&#26426;&#22120;&#32763;&#35793;&#30340;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36845;&#20195;&#38750;&#33258;&#22238;&#24402;Transformer&#30340;&#35745;&#31639;&#20248;&#21183;&#22312;&#35299;&#30721;&#27493;&#39588;&#22686;&#21152;&#26102;&#20250;&#20943;&#24369;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Distill Multiple Steps&#65288;DiMS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21387;&#32553;&#25216;&#26415;&#65292;&#21487;&#20943;&#23569;&#36798;&#21040;&#29305;&#23450;&#32763;&#35793;&#36136;&#37327;&#25152;&#38656;&#30340;&#27493;&#39588;&#25968;&#12290;&#21387;&#32553;&#21518;&#30340;&#27169;&#22411;&#26082;&#20139;&#21463;&#20102;&#26089;&#26399;&#36845;&#20195;&#30340;&#35745;&#31639;&#20248;&#21183;&#65292;&#21516;&#26102;&#21448;&#20445;&#30041;&#20102;&#22810;&#20010;&#36845;&#20195;&#27493;&#39588;&#30340;&#22686;&#24378;&#25928;&#26524;&#12290;DiMS&#38656;&#35201;&#20004;&#20010;&#27169;&#22411;&#65292;&#21363;&#23398;&#29983;&#27169;&#22411;&#21644;&#25945;&#24072;&#27169;&#22411;&#12290;&#23398;&#29983;&#27169;&#22411;&#36890;&#36807;&#20248;&#21270;&#20197;&#39044;&#27979;&#22810;&#27425;&#35299;&#30721;&#21518;&#30340;&#25945;&#24072;&#27169;&#22411;&#36755;&#20986;&#65292;&#32780;&#25945;&#24072;&#27169;&#22411;&#21017;&#36890;&#36807;&#32531;&#24930;&#31227;&#21160;&#24179;&#22343;&#36319;&#38543;&#23398;&#29983;&#27169;&#22411;&#65292;&#36825;&#20351;&#24471;&#25945;&#24072;&#27169;&#22411;&#30340;&#30693;&#35782;&#24471;&#21040;&#26356;&#26032;&#65292;&#24182;&#25552;&#39640;&#20102;&#25552;&#20379;&#30340;&#26631;&#31614;&#30340;&#36136;&#37327;&#12290;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#23398;&#29983;&#27169;&#22411;&#29992;&#20110;&#32763;&#35793;&#65292;&#24182;&#19988;&#27809;&#26377;&#39069;&#22806;&#30340;&#35745;&#31639;&#36127;&#25285;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#27169;&#22411;&#19978;&#39564;&#35777;&#20102;DiMS&#30340;&#26377;&#25928;&#24615;&#65292;&#21333;&#27493;&#32763;&#35793;&#20013;&#33719;&#24471;&#20102;7.8&#21644;12.9 BLEU&#20998;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
The computational benefits of iterative non-autoregressive transformers decrease as the number of decoding steps increases. As a remedy, we introduce Distill Multiple Steps (DiMS), a simple yet effective distillation technique to decrease the number of required steps to reach a certain translation quality. The distilled model enjoys the computational benefits of early iterations while preserving the enhancements from several iterative steps. DiMS relies on two models namely student and teacher. The student is optimized to predict the output of the teacher after multiple decoding steps while the teacher follows the student via a slow-moving average. The moving average keeps the teacher's knowledge updated and enhances the quality of the labels provided by the teacher. During inference, the student is used for translation and no additional computation is added. We verify the effectiveness of DiMS on various models obtaining 7.8 and 12.9 BLEU points improvements in single-step translation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36328;&#35270;&#22270;&#35821;&#35328;&#24314;&#27169;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#36328;&#35821;&#35328;&#21644;&#36328;&#27169;&#24577;&#39044;&#35757;&#32451;&#32479;&#19968;&#22312;&#20849;&#20139;&#30340;&#26550;&#26500;&#21644;&#30446;&#26631;&#19979;&#36827;&#34892;&#65292;&#36890;&#36807;&#26377;&#26465;&#20214;&#30340;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#21644;&#23545;&#27604;&#23398;&#20064;&#26469;&#26368;&#22823;&#21270;&#19981;&#21516;&#35270;&#22270;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#20197;&#23454;&#29616;&#20004;&#20010;&#35270;&#22270;&#30340;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2206.00621</link><description>&lt;p&gt;
&#36328;&#35270;&#22270;&#35821;&#35328;&#24314;&#27169;&#65306;&#36808;&#21521;&#32479;&#19968;&#30340;&#36328;&#35821;&#35328;&#36328;&#27169;&#24577;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Cross-View Language Modeling: Towards Unified Cross-Lingual Cross-Modal Pre-training. (arXiv:2206.00621v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36328;&#35270;&#22270;&#35821;&#35328;&#24314;&#27169;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#36328;&#35821;&#35328;&#21644;&#36328;&#27169;&#24577;&#39044;&#35757;&#32451;&#32479;&#19968;&#22312;&#20849;&#20139;&#30340;&#26550;&#26500;&#21644;&#30446;&#26631;&#19979;&#36827;&#34892;&#65292;&#36890;&#36807;&#26377;&#26465;&#20214;&#30340;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#21644;&#23545;&#27604;&#23398;&#20064;&#26469;&#26368;&#22823;&#21270;&#19981;&#21516;&#35270;&#22270;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#20197;&#23454;&#29616;&#20004;&#20010;&#35270;&#22270;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#36328;&#35270;&#22270;&#35821;&#35328;&#24314;&#27169;&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#23558;&#36328;&#35821;&#35328;&#21644;&#36328;&#27169;&#24577;&#39044;&#35757;&#32451;&#19982;&#20849;&#20139;&#26550;&#26500;&#21644;&#30446;&#26631;&#32479;&#19968;&#36215;&#26469;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21463;&#21040;&#19968;&#20010;&#20851;&#38190;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#21363;&#36328;&#35821;&#35328;&#21644;&#36328;&#27169;&#24577;&#39044;&#35757;&#32451;&#20855;&#26377;&#23558;&#21516;&#19968;&#23545;&#35937;&#30340;&#20004;&#20010;&#19981;&#21516;&#35270;&#22270;&#23545;&#40784;&#21040;&#19968;&#20010;&#20849;&#21516;&#35821;&#20041;&#31354;&#38388;&#30340;&#30456;&#21516;&#30446;&#26631;&#12290;&#20026;&#27492;&#65292;&#36328;&#35270;&#22270;&#35821;&#35328;&#24314;&#27169;&#26694;&#26550;&#23558;&#22810;&#27169;&#24577;&#25968;&#25454;&#65288;&#21363;&#22270;&#20687;-&#26631;&#39064;&#23545;&#65289;&#21644;&#22810;&#35821;&#35328;&#25968;&#25454;&#65288;&#21363;&#24179;&#34892;&#21477;&#23545;&#65289;&#35270;&#20026;&#21516;&#19968;&#23545;&#35937;&#30340;&#20004;&#20010;&#19981;&#21516;&#35270;&#22270;&#65292;&#24182;&#36890;&#36807;&#26377;&#26465;&#20214;&#30340;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#21644;&#23545;&#27604;&#23398;&#20064;&#26469;&#26368;&#22823;&#21270;&#23427;&#20204;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#23545;&#40784;&#20004;&#20010;&#35270;&#22270;&#12290;&#25105;&#20204;&#20351;&#29992;&#36328;&#35270;&#22270;&#35821;&#35328;&#24314;&#27169;&#26694;&#26550;&#23545;&#36328;&#35821;&#35328;&#36328;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;CCLM&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#22312;&#22810;&#35821;&#35328;&#22810;&#27169;&#24577;&#22522;&#20934;IGLUE&#21644;&#20004;&#20010;&#22810;&#35821;&#35328;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce Cross-View Language Modeling, a simple and effective pre-training framework that unifies cross-lingual and cross-modal pre-training with shared architectures and objectives. Our approach is motivated by a key observation that cross-lingual and cross-modal pre-training share the same goal of aligning two different views of the same object into a common semantic space. To this end, the cross-view language modeling framework considers both multi-modal data (i.e., image-caption pairs) and multi-lingual data (i.e., parallel sentence pairs) as two different views of the same object, and trains the model to align the two views by maximizing the mutual information between them with conditional masked language modeling and contrastive learning. We pre-train CCLM, a Cross-lingual Cross-modal Language Model, with the cross-view language modeling framework. Empirical results on IGLUE, a multi-lingual multi-modal benchmark, and two multi-lingual image-text retrieval data
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;ClaimDiff&#25968;&#25454;&#38598;&#65292;&#20027;&#35201;&#20851;&#27880;&#22768;&#26126;&#20043;&#38388;&#30340;&#24494;&#22937;&#24046;&#21035;&#65292;&#24182;&#35266;&#23519;&#21040;&#24378;&#26377;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;&#26080;&#27861;&#25506;&#27979;&#36825;&#20123;&#24046;&#21035;&#65292;&#19982;&#20154;&#31867;&#23384;&#22312;&#36229;&#36807;19%&#30340;&#32477;&#23545;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2205.12221</link><description>&lt;p&gt;
ClaimDiff&#65306;&#27604;&#36739;&#21644;&#23545;&#27604;&#26377;&#20105;&#35758;&#38382;&#39064;&#30340;&#22768;&#26126;
&lt;/p&gt;
&lt;p&gt;
ClaimDiff: Comparing and Contrasting Claims on Contentious Issues. (arXiv:2205.12221v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12221
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;ClaimDiff&#25968;&#25454;&#38598;&#65292;&#20027;&#35201;&#20851;&#27880;&#22768;&#26126;&#20043;&#38388;&#30340;&#24494;&#22937;&#24046;&#21035;&#65292;&#24182;&#35266;&#23519;&#21040;&#24378;&#26377;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;&#26080;&#27861;&#25506;&#27979;&#36825;&#20123;&#24046;&#21035;&#65292;&#19982;&#20154;&#31867;&#23384;&#22312;&#36229;&#36807;19%&#30340;&#32477;&#23545;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21457;&#29616;&#34394;&#20551;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#26085;&#30410;&#22686;&#21152;&#65292;&#35768;&#22810;&#30740;&#31350;&#37117;&#19987;&#27880;&#20110;&#26816;&#39564;&#20107;&#23454;&#22768;&#26126;&#26469;&#26816;&#32034;&#35777;&#25454;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#20107;&#23454;&#39564;&#35777;&#20219;&#21153;&#24182;&#19981;&#36866;&#29992;&#20110;&#25429;&#25417;&#20107;&#23454;&#19978;&#19968;&#33268;&#20294;&#21487;&#33021;&#20173;&#20250;&#23545;&#35835;&#32773;&#20135;&#29983;&#20559;&#35265;&#30340;&#24494;&#22937;&#24046;&#24322;&#65292;&#23588;&#20854;&#26159;&#22312;&#26377;&#20105;&#35758;&#30340;&#25919;&#27835;&#25110;&#32463;&#27982;&#38382;&#39064;&#19978;&#12290;&#25105;&#20204;&#30340;&#22522;&#26412;&#20551;&#35774;&#26159;&#65292;&#22312;&#21463;&#20449;&#20219;&#30340;&#20449;&#24687;&#28304;&#20013;&#65292;&#19968;&#20010;&#20154;&#30340;&#35770;&#28857;&#24182;&#19981;&#19968;&#23450;&#27604;&#21478;&#19968;&#20010;&#20154;&#26356;&#27491;&#30830;&#65292;&#38656;&#35201;&#36827;&#34892;&#27604;&#36739;&#32780;&#19981;&#26159;&#31616;&#21333;&#30340;&#39564;&#35777;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ClaimDiff&#65292;&#36825;&#26159;&#19968;&#20010;&#20027;&#35201;&#20851;&#27880;&#22768;&#26126;&#23545;&#20043;&#38388;&#32454;&#24494;&#24046;&#21035;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#22312;ClaimDiff&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26469;&#33258;268&#31687;&#26032;&#38395;&#25991;&#31456;&#30340;2,941&#20010;&#26631;&#27880;&#22768;&#26126;&#23545;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#34429;&#28982;&#20154;&#31867;&#33021;&#22815;&#26816;&#27979;&#21040;&#22768;&#26126;&#20043;&#38388;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;&#20294;&#24378;&#22823;&#30340;&#22522;&#20934;&#27979;&#35797;&#26080;&#27861;&#25506;&#27979;&#21040;&#23427;&#20204;&#65292;&#19982;&#20154;&#31867;&#23384;&#22312;&#36229;&#36807;19%&#30340;&#32477;&#23545;&#24046;&#36317;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#39033;&#21021;&#27493;&#30740;&#31350;&#33021;&#22815;&#36890;&#36807;&#26426;&#22120;&#36741;&#21161;&#24110;&#21161;&#35835;&#32773;&#33719;&#24471;&#26377;&#20105;&#35758;&#38382;&#39064;&#30340;&#26080;&#20559;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the growing importance of detecting misinformation, many studies have focused on verifying factual claims by retrieving evidence. However, canonical fact verification tasks do not apply to catching subtle differences in factually consistent claims, which might still bias the readers, especially on contentious political or economic issues. Our underlying assumption is that among the trusted sources, one's argument is not necessarily more true than the other, requiring comparison rather than verification. In this study, we propose ClaimDiff, a novel dataset that primarily focuses on comparing the nuance between claim pairs. In ClaimDiff, we provide 2,941 annotated claim pairs from 268 news articles. We observe that while humans are capable of detecting the nuances between claims, strong baselines struggle to detect them, showing over a 19% absolute gap with the humans. We hope this initial study could help readers to gain an unbiased grasp of contentious issues through machine-aided
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#26368;&#36817;&#31070;&#32463;&#25991;&#26412;&#29983;&#25104;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#25968;&#25454;&#26500;&#24314;&#12289;&#31070;&#32463;&#26694;&#26550;&#12289;&#35757;&#32451;&#21644;&#25512;&#29702;&#31574;&#30053;&#21644;&#35780;&#20272;&#25351;&#26631;&#31561;&#22235;&#20010;&#26041;&#38754;&#65292;&#24182;&#25506;&#35752;&#20102;&#31070;&#32463;&#31649;&#36947;&#21644;&#32972;&#26223;&#30693;&#35782;&#30340;&#21033;&#29992;&#31561;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2203.03047</link><description>&lt;p&gt;
&#31070;&#32463;&#25991;&#26412;&#29983;&#25104;&#30340;&#26368;&#26032;&#36827;&#23637;&#65306;&#19968;&#39033;&#20219;&#21153;&#26080;&#20851;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Recent Advances in Neural Text Generation: A Task-Agnostic Survey. (arXiv:2203.03047v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.03047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#26368;&#36817;&#31070;&#32463;&#25991;&#26412;&#29983;&#25104;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#25968;&#25454;&#26500;&#24314;&#12289;&#31070;&#32463;&#26694;&#26550;&#12289;&#35757;&#32451;&#21644;&#25512;&#29702;&#31574;&#30053;&#21644;&#35780;&#20272;&#25351;&#26631;&#31561;&#22235;&#20010;&#26041;&#38754;&#65292;&#24182;&#25506;&#35752;&#20102;&#31070;&#32463;&#31649;&#36947;&#21644;&#32972;&#26223;&#30693;&#35782;&#30340;&#21033;&#29992;&#31561;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30456;&#24403;&#22810;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#39046;&#22495;&#20013;&#24212;&#29992;&#31070;&#32463;&#27169;&#22411;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#29983;&#25104;&#26082;&#20855;&#26377;&#35821;&#35328;&#33258;&#28982;&#24615;&#21448;&#20855;&#26377;&#20154;&#31867;&#21270;&#23646;&#24615;&#30340;&#25991;&#26412;&#65292;&#24182;&#21516;&#26102;&#23545;&#29983;&#25104;&#36807;&#31243;&#36827;&#34892;&#25511;&#21046;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20221;&#20840;&#38754;&#30340;&#65292;&#20219;&#21153;&#26080;&#20851;&#30340;&#31070;&#32463;&#25991;&#26412;&#29983;&#25104;&#26368;&#26032;&#36827;&#23637;&#35843;&#26597;&#12290;&#36825;&#20123;&#36827;&#23637;&#36890;&#36807;&#22810;&#31181;&#21457;&#23637;&#24471;&#20197;&#23454;&#29616;&#65292;&#25105;&#20204;&#23558;&#20854;&#20998;&#20026;&#22235;&#20010;&#20027;&#35201;&#26041;&#38754;&#65306;&#25968;&#25454;&#26500;&#24314;&#65292;&#31070;&#32463;&#26694;&#26550;&#65292;&#35757;&#32451;&#21644;&#25512;&#29702;&#31574;&#30053;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#36890;&#36807;&#30740;&#31350;&#36825;&#20123;&#19981;&#21516;&#26041;&#38754;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#23545;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#31070;&#32463;&#25991;&#26412;&#29983;&#25104;&#30340;&#26410;&#26469;&#26041;&#21521;&#65292;&#36825;&#20123;&#26041;&#21521;&#21253;&#25324;&#21033;&#29992;&#31070;&#32463;&#31649;&#36947;&#21644;&#34701;&#21512;&#32972;&#26223;&#30693;&#35782;&#65292;&#36825;&#20123;&#36884;&#24452;&#20026;&#36827;&#19968;&#27493;&#22686;&#24378;&#31070;&#32463;&#25991;&#26412;&#29983;&#25104;&#31995;&#32479;&#30340;&#33021;&#21147;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, considerable research has been dedicated to the application of neural models in the field of natural language generation (NLG). The primary objective is to generate text that is both linguistically natural and human-like, while also exerting control over the generation process. This paper offers a comprehensive and task-agnostic survey of the recent advancements in neural text generation. These advancements have been facilitated through a multitude of developments, which we categorize into four key areas: data construction, neural frameworks, training and inference strategies, and evaluation metrics. By examining these different aspects, we aim to provide a holistic overview of the progress made in the field. Furthermore, we explore the future directions for the advancement of neural text generation, which encompass the utilization of neural pipelines and the incorporation of background knowledge. These avenues present promising opportunities to further enhance the cap
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#39046;&#22495;&#22240;&#32032;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#32467;&#26524;&#34920;&#26126;&#35821;&#38899;&#23398;&#39046;&#22495;&#22240;&#32032;&#22312;&#39044;&#35757;&#32451;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#32780;&#35821;&#27861;&#21644;&#21477;&#27861;&#22240;&#32032;&#21017;&#36828;&#19981;&#21450;&#20854;&#37325;&#35201;&#65292;&#36825;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#20102;&#35299;&#33258;&#25105;&#30417;&#30563;&#35757;&#32451;&#35821;&#38899;&#39044;&#35757;&#32451;&#38598;&#30340;&#39046;&#22495;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2203.00648</link><description>&lt;p&gt;
&#22312;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#20013;&#34913;&#37327;&#20010;&#21035;&#39046;&#22495;&#22240;&#32032;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Measuring the Impact of Individual Domain Factors in Self-Supervised Pre-Training. (arXiv:2203.00648v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.00648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#39046;&#22495;&#22240;&#32032;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#32467;&#26524;&#34920;&#26126;&#35821;&#38899;&#23398;&#39046;&#22495;&#22240;&#32032;&#22312;&#39044;&#35757;&#32451;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#32780;&#35821;&#27861;&#21644;&#21477;&#27861;&#22240;&#32032;&#21017;&#36828;&#19981;&#21450;&#20854;&#37325;&#35201;&#65292;&#36825;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#20102;&#35299;&#33258;&#25105;&#30417;&#30563;&#35757;&#32451;&#35821;&#38899;&#39044;&#35757;&#32451;&#38598;&#30340;&#39046;&#22495;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#35821;&#38899;&#25968;&#25454;&#30001;&#21475;&#38899;&#12289;&#21477;&#27861;&#21644;&#35821;&#20041;&#22810;&#26679;&#24615;&#20197;&#21450;&#22768;&#23398;&#29615;&#22659;&#31561;&#20016;&#23500;&#30340;&#39046;&#22495;&#22240;&#32032;&#26500;&#25104;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20043;&#38388;&#39046;&#22495;&#19981;&#21305;&#37197;&#23545;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#25972;&#20307;&#34920;&#29616;&#30340;&#24433;&#21709;&#65292;&#20294;&#24182;&#26410;&#20998;&#35299;&#20010;&#21035;&#22240;&#32032;&#30340;&#24046;&#24322;&#36129;&#29486;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#39033;&#23545;&#21463;&#25511;&#30740;&#31350;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#20010;&#21035;&#22240;&#32032;&#23545;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#39044;&#35757;&#32451;&#34920;&#31034;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20197;&#21333;&#20010;&#39046;&#22495;&#22240;&#32032;&#36827;&#34892;&#33258;&#28982;&#35821;&#38899;&#25110;&#21512;&#25104;&#38899;&#39057;&#30340;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#24494;&#35843;&#21518;&#27979;&#37327;&#24615;&#33021;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35821;&#38899;&#23398;&#39046;&#22495;&#22240;&#32032;&#22312;&#39044;&#35757;&#32451;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#32780;&#35821;&#27861;&#21644;&#21477;&#27861;&#22240;&#32032;&#21017;&#36828;&#19981;&#21450;&#20854;&#37325;&#35201;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#39033;&#30740;&#31350;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#33258;&#25105;&#30417;&#30563;&#35757;&#32451;&#35821;&#38899;&#39044;&#35757;&#32451;&#38598;&#30340;&#39046;&#22495;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human speech data comprises a rich set of domain factors such as accent, syntactic and semantic variety, or acoustic environment. Previous work explores the effect of domain mismatch in automatic speech recognition between pre-training and fine-tuning as a whole but does not dissect the contribution of individual factors. In this paper, we present a controlled study to better understand the effect of such factors on the performance of pre-trained representations on automatic speech recognition. To do so, we pre-train models either on modified natural speech or synthesized audio, with a single domain factor modified, and then measure performance after fine-tuning. Results show that phonetic domain factors play an important role during pre-training while grammatical and syntactic factors are far less important. To our knowledge, this is the first study to better understand the domain characteristics of pre-trained sets in self-supervised pre-training for speech.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MITQA&#30340;&#22522;&#20110;Transformer&#30340;TextTableQA&#31995;&#32479;&#65292;&#36890;&#36807;&#22810;&#23454;&#20363;&#23398;&#20064;&#20197;&#21450;&#36828;&#31243;&#30417;&#30563;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#34920;&#26684;&#21152;&#25991;&#26412;&#38382;&#39064;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2112.07337</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#34892;&#22810;&#36328;&#24230;&#36828;&#31243;&#30417;&#30563;&#30340;&#34920;&#26684;&#21152;&#25991;&#26412;&#38382;&#39064;&#22810;&#23454;&#20363;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-Row, Multi-Span Distant Supervision For Table+Text Question. (arXiv:2112.07337v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.07337
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MITQA&#30340;&#22522;&#20110;Transformer&#30340;TextTableQA&#31995;&#32479;&#65292;&#36890;&#36807;&#22810;&#23454;&#20363;&#23398;&#20064;&#20197;&#21450;&#36828;&#31243;&#30417;&#30563;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#34920;&#26684;&#21152;&#25991;&#26412;&#38382;&#39064;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20851;&#20110;&#34920;&#26684;&#21644;&#38142;&#25509;&#25991;&#26412;&#30340;&#38382;&#31572;&#65288;TextTableQA&#65289;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#35201;&#30340;&#30740;&#31350;&#25104;&#26524;&#65292;&#22240;&#20026;&#34920;&#26684;&#36890;&#24120;&#19982;&#30456;&#20851;&#30340;&#25991;&#26412;&#23884;&#20837;&#22312;&#25991;&#26723;&#20013;&#12290;HybridQA&#21644;OTT-QA&#26159;&#20004;&#20010;&#26368;&#30693;&#21517;&#30340;TextTableQA&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#30340;&#38382;&#39064;&#26368;&#22909;&#36890;&#36807;&#21516;&#26102;&#20174;&#34920;&#26684;&#21333;&#20803;&#21644;&#38142;&#25509;&#25991;&#26412;&#27573;&#33853;&#20013;&#33719;&#21462;&#20449;&#24687;&#26469;&#22238;&#31572;&#12290;&#36825;&#20004;&#20010;&#25968;&#25454;&#38598;&#30340;&#20849;&#21516;&#25361;&#25112;&#26159;&#65292;&#35757;&#32451;&#23454;&#20363;&#22914;&#38382;&#39064;&#21644;&#31572;&#26696;&#65292;&#20854;&#20013;&#40644;&#37329;&#31572;&#26696;&#21487;&#33021;&#19981;&#20165;&#21305;&#37197;&#36328;&#36234;&#34920;&#26684;&#34892;&#30340;&#22810;&#20010;&#34920;&#26684;&#21333;&#20803;&#65292;&#32780;&#19988;&#36824;&#21253;&#25324;&#34920;&#26684;&#34892;&#21450;&#20854;&#30456;&#20851;&#25991;&#26412;&#33539;&#22260;&#20869;&#30340;&#22810;&#20010;&#25991;&#26412;&#36328;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MITQA&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;TextTableQA&#31995;&#32479;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#36890;&#36807;&#22810;&#23454;&#20363;&#25439;&#22833;&#30446;&#26631;&#21644;&#35880;&#24910;&#30340;&#35838;&#31243;&#35774;&#35745;&#26469;&#24212;&#23545;&#36825;&#20004;&#20010;&#26041;&#38754;&#30340;&#36828;&#31243;&#30417;&#30563;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;MRMS-DS&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;MITQA&#22312;HybridQA&#21644;OTT-QA&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;TextTableQA&#25361;&#25112;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question answering (QA) over tables and linked text, also called TextTableQA, has witnessed significant research in recent years, as tables are often found embedded in documents along with related text. HybridQA and OTT-QA are the two best-known TextTableQA datasets, with questions that are best answered by combining information from both table cells and linked text passages. A common challenge in both datasets, and TextTableQA in general, is that the training instances include just the question and answer, where the gold answer may match not only multiple table cells across table rows but also multiple text spans within the scope of a table row and its associated text. This leads to a noisy multi instance training regime. We present MITQA, a transformer-based TextTableQA system that is explicitly designed to cope with distant supervision along both these axes, through a multi-instance loss objective, together with careful curriculum design. Our experiments show that the proposed multi
&lt;/p&gt;</description></item></channel></rss>