<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25688;&#35201;&#24615;&#24635;&#32467;&#23545;&#35805;&#36827;&#34892;&#32858;&#31867;&#30340; IDAS &#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#26080;&#30417;&#30563;&#29615;&#22659;&#19979;&#32988;&#36807;&#26368;&#20808;&#36827;&#30340;&#31454;&#20105;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312; Banking &#21644; Stack Overflow &#25968;&#25454;&#38598;&#19978;&#21482;&#20351;&#29992; 5% &#30340;&#26631;&#35760;&#25968;&#25454;&#21363;&#21487;&#33719;&#24471;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.19783</link><description>&lt;p&gt;
IDAS: &#24102;&#26377;&#25688;&#35201;&#24615;&#24635;&#32467;&#30340;&#24847;&#22270;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
IDAS: Intent Discovery with Abstractive Summarization. (arXiv:2305.19783v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19783
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25688;&#35201;&#24615;&#24635;&#32467;&#23545;&#35805;&#36827;&#34892;&#32858;&#31867;&#30340; IDAS &#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#26080;&#30417;&#30563;&#29615;&#22659;&#19979;&#32988;&#36807;&#26368;&#20808;&#36827;&#30340;&#31454;&#20105;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312; Banking &#21644; Stack Overflow &#25968;&#25454;&#38598;&#19978;&#21482;&#20351;&#29992; 5% &#30340;&#26631;&#35760;&#25968;&#25454;&#21363;&#21487;&#33719;&#24471;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24847;&#22270;&#21457;&#29616;&#26159;&#20174;&#19968;&#32452;&#26410;&#26631;&#35760;&#30340;&#35805;&#35821;&#20013;&#25512;&#26029;&#28508;&#22312;&#24847;&#22270;&#30340;&#20219;&#21153;&#65292;&#26159;&#21019;&#24314;&#26032;&#23545;&#35805;&#20195;&#29702;&#30340;&#26377;&#25928;&#27493;&#39588;&#12290;&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#25688;&#35201;&#24615;&#24635;&#32467;&#65288;&#21363;&#21024;&#38500;&#38750;&#24517;&#35201;&#20449;&#24687;&#65292;&#20445;&#30041;&#26680;&#24515;&#35201;&#32032;&#30340;&#8220;&#26631;&#31614;&#8221;&#65289;&#23545;&#35805;&#36827;&#34892;&#32858;&#31867;&#21487;&#20197;&#32988;&#36807;&#26368;&#36817;&#22312;&#24847;&#22270;&#21457;&#29616;&#26041;&#38754;&#30340;&#19968;&#20123;&#31454;&#20105;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; IDAS &#26041;&#27861;&#65292;&#36890;&#36807;&#21551;&#21160;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#31934;&#24515;&#36873;&#25321;&#30340;&#21407;&#22411;&#35805;&#35821;&#31181;&#26893;&#24320;&#22987;&#65292;&#37319;&#38598;&#19968;&#32452;&#25551;&#36848;&#24615;&#35805;&#35821;&#26631;&#31614;&#65292;&#20197;&#21551;&#21160;&#19968;&#31181;&#19978;&#19979;&#25991;&#23398;&#20064;&#31243;&#24207;&#26469;&#20026;&#38750;&#20856;&#22411;&#35805;&#35821;&#29983;&#25104;&#26631;&#31614;&#12290;&#28982;&#21518;&#65292;&#23558;&#35805;&#35821;&#21450;&#20854;&#22024;&#26434;&#30340;&#26631;&#31614;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#36890;&#36807;&#32858;&#31867;&#26469;&#24674;&#22797;&#28508;&#22312;&#30340;&#24847;&#22270;&#12290;&#23545;&#20110;&#26080;&#30417;&#30563;&#20219;&#21153;&#65288;&#27809;&#26377;&#20219;&#20309;&#24847;&#22270;&#26631;&#31614;&#65289;&#65292;IDAS &#22312; Banking&#12289;Stack Overflow &#21644; Chatbot &#25968;&#25454;&#38598;&#30340;&#26631;&#20934;&#32858;&#31867;&#24230;&#37327;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#21487;&#20197;&#36229;&#36807;&#26368;&#20808;&#36827;&#26041;&#27861;&#39640;&#36798; +7.42%&#65292;&#22312; Banking &#21644; Stack Overflow &#25968;&#25454;&#38598;&#19978;&#20165;&#20351;&#29992; 5% &#30340;&#26631;&#35760;&#25968;&#25454;&#21363;&#21487;&#33719;&#24471;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#65292;&#36825;&#34920;&#26126;&#23427;&#22312;&#26377;&#25928;&#24847;&#22270;&#21457;&#29616;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intent discovery is the task of inferring latent intents from a set of unlabeled utterances, and is a useful step towards the efficient creation of new conversational agents. We show that recent competitive methods in intent discovery can be outperformed by clustering utterances based on abstractive summaries, i.e., "labels", that retain the core elements while removing non-essential information. We contribute the IDAS approach, which collects a set of descriptive utterance labels by prompting a Large Language Model, starting from a well-chosen seed set of prototypical utterances, to bootstrap an In-Context Learning procedure to generate labels for non-prototypical utterances. The utterances and their resulting noisy labels are then encoded by a frozen pre-trained encoder, and subsequently clustered to recover the latent intents. For the unsupervised task (without any intent labels) IDAS outperforms the state-of-the-art by up to +7.42% in standard cluster metrics for the Banking, Stack
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#25105;&#27880;&#24847;&#21147;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#65292;&#29992;&#20110;&#38899;&#39057;&#38382;&#31572;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#36739;&#20110;&#21442;&#32771;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;Clotho-AQA&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#26377;&#26126;&#26174;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.19769</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#38899;&#39057;&#38382;&#31572;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Attention-Based Methods For Audio Question Answering. (arXiv:2305.19769v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#25105;&#27880;&#24847;&#21147;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#65292;&#29992;&#20110;&#38899;&#39057;&#38382;&#31572;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#36739;&#20110;&#21442;&#32771;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;Clotho-AQA&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#26377;&#26126;&#26174;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#38382;&#31572;(AQA)&#26159;&#23545;&#20110;&#38899;&#39057;&#21644;&#33258;&#28982;&#35821;&#35328;&#25552;&#20986;&#38382;&#39064;&#26102;&#65292;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#22238;&#31572;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#65292;&#29992;&#20110;&#38899;&#39057;&#38382;&#31572;&#20219;&#21153;&#12290;&#33258;&#25105;&#27880;&#24847;&#21147;&#23618;&#21487;&#20197;&#25552;&#21462;&#24378;&#22823;&#30340;&#38899;&#39057;&#21644;&#25991;&#26412;&#34920;&#31034;&#12290;&#20132;&#21449;&#27880;&#24847;&#21147;&#23558;&#19982;&#25991;&#26412;&#29305;&#24449;&#30456;&#20851;&#30340;&#38899;&#39057;&#29305;&#24449;&#26144;&#23556;&#21040;&#31572;&#26696;&#20013;&#12290;&#25105;&#20204;&#30340;&#25152;&#26377;&#27169;&#22411;&#37117;&#22312;&#26368;&#36817;&#25552;&#20986;&#30340;Clotho-AQA&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#29992;&#20110;&#20108;&#36827;&#21046;&#26159;/&#21542;&#38382;&#39064;&#21644;&#21333;&#35789;&#22238;&#31572;&#38382;&#39064;&#12290;&#32467;&#26524;&#28165;&#26970;&#22320;&#26174;&#31034;&#20986;&#30456;&#23545;&#20110;&#21407;&#22987;&#35770;&#25991;&#20013;&#30340;&#21442;&#32771;&#26041;&#27861;&#25913;&#36827;&#12290;&#22312;&#26159;/&#21542;&#20108;&#36827;&#21046;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#30340;&#25552;&#20986;&#30340;&#27169;&#22411;&#30456;&#23545;&#20110;&#21442;&#32771;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#20174;62.7&#65285;&#25552;&#39640;&#21040;&#20102;68.3&#65285;&#12290;&#23545;&#20110;&#21333;&#35789;&#31572;&#26696;&#22810;&#31867;&#20998;&#31867;&#22120;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20998;&#21035;&#20135;&#29983;&#20102;57.9&#65285;&#21644;99.8&#65285;&#30340;top-1&#21644;top-5&#20934;&#30830;&#29575;&#65292;&#30456;&#23545;&#20110;&#21442;&#32771;&#27169;&#22411;&#30340;54.2&#65285;&#21644;93.7&#65285;&#26377;&#20102;&#26126;&#26174;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Audio question answering (AQA) is the task of producing natural language answers when a system is provided with audio and natural language questions. In this paper, we propose neural network architectures based on self-attention and cross-attention for the AQA task. The self-attention layers extract powerful audio and textual representations. The cross-attention maps audio features that are relevant to the textual features to produce answers. All our models are trained on the recently proposed Clotho-AQA dataset for both binary yes/no questions and single-word answer questions. Our results clearly show improvement over the reference method reported in the original paper. On the yes/no binary classification task, our proposed model achieves an accuracy of 68.3% compared to 62.7% in the reference model. For the single-word answers multiclass classifier, our model produces a top-1 and top-5 accuracy of 57.9% and 99.8% compared to 54.2% and 93.7% in the reference model respectively. We fur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36882;&#24402; Metropolis-Hastings &#21629;&#21517;&#28216;&#25103; (RMHNG) &#27169;&#22411;&#65292;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#31526;&#21495;&#20986;&#29616;&#65292;&#20855;&#26377;&#36739;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#22330;&#26223;&#19979;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.19761</link><description>&lt;p&gt;
&#36882;&#24402; Metropolis-Hastings &#21629;&#21517;&#28216;&#25103;&#65306;&#22522;&#20110;&#27010;&#29575;&#29983;&#25104;&#27169;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#31526;&#21495;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
Recursive Metropolis-Hastings Naming Game: Symbol Emergence in a Multi-agent System based on Probabilistic Generative Models. (arXiv:2305.19761v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36882;&#24402; Metropolis-Hastings &#21629;&#21517;&#28216;&#25103; (RMHNG) &#27169;&#22411;&#65292;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#31526;&#21495;&#20986;&#29616;&#65292;&#20855;&#26377;&#36739;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#22330;&#26223;&#19979;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20195;&#29702;&#20154;&#32676;&#20307;&#20013;&#30740;&#31350;&#31526;&#21495;&#30340;&#20986;&#29616;&#21644;&#32039;&#24613;&#36890;&#20449;&#65292;&#20351;&#29992;&#20102;&#19968;&#31181;&#35745;&#31639;&#27169;&#22411;&#65292;&#20854;&#20013;&#20195;&#29702;&#20154;&#21442;&#19982;&#21508;&#31181;&#35821;&#35328;&#28216;&#25103;&#12290;&#20854;&#20013;&#65292;Metropolis-Hastings &#21629;&#21517;&#28216;&#25103; (MHNG) &#20855;&#26377;&#19968;&#20010;&#26174;&#33879;&#30340;&#25968;&#23398;&#23646;&#24615;&#65306;&#36890;&#36807; MHNG &#30340;&#31526;&#21495;&#20986;&#29616;&#34987;&#35777;&#26126;&#26159;&#20998;&#25955;&#24335;&#36125;&#21494;&#26031;&#25512;&#29702;&#65292;&#26159;&#20195;&#29702;&#20154;&#20849;&#20139;&#30340;&#34920;&#24449;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#25552;&#20986;&#30340; MHNG &#20165;&#22312;&#20004;&#20010;&#20195;&#29702;&#20154;&#22330;&#26223;&#20013;&#20351;&#29992;&#12290;&#26412;&#25991;&#23558; MHNG &#25193;&#23637;&#21040; N &#20195;&#29702;&#20154;&#22330;&#26223;&#20013;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26377;&#20004;&#20010;&#65306;(1) &#25105;&#20204;&#23558;&#36882;&#24402; Metropolis-Hastings &#21629;&#21517;&#28216;&#25103; (RMHNG) &#25552;&#20986;&#20026; MHNG &#30340; N &#20195;&#29702;&#20154;&#29256;&#26412;&#65292;&#24182;&#35777;&#26126; RMHNG &#26159;&#19968;&#31181;&#36817;&#20284;&#36125;&#21494;&#26031;&#25512;&#29702;&#26041;&#27861;&#65292;&#31867;&#20284;&#20110; MHNG&#65292;&#29992;&#20110;&#20195;&#29702;&#20154;&#20849;&#20139;&#30340;&#28508;&#22312;&#21464;&#37327;&#30340;&#21518;&#39564;&#20998;&#24067;&#65307;(2) &#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#22270;&#20687;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102; RMHNG &#30340;&#24615;&#33021;&#23454;&#35777;&#35780;&#20272;&#65292;&#20351;&#22810;&#20010;&#20195;&#29702;&#21487;&#20197;&#24320;&#21457;&#21644;&#20849;&#20139;&#31526;&#21495;&#31995;&#32479;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558; RMHNG &#30340;&#24615;&#33021;&#19982;&#20854;&#20182;&#29616;&#26377;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#25910;&#25947;&#36895;&#24230;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the studies on symbol emergence and emergent communication in a population of agents, a computational model was employed in which agents participate in various language games. Among these, the Metropolis-Hastings naming game (MHNG) possesses a notable mathematical property: symbol emergence through MHNG is proven to be a decentralized Bayesian inference of representations shared by the agents. However, the previously proposed MHNG is limited to a two-agent scenario. This paper extends MHNG to an N-agent scenario. The main contributions of this paper are twofold: (1) we propose the recursive Metropolis-Hastings naming game (RMHNG) as an N-agent version of MHNG and demonstrate that RMHNG is an approximate Bayesian inference method for the posterior distribution over a latent variable shared by agents, similar to MHNG; and (2) we empirically evaluate the performance of RMHNG on synthetic and real image data, enabling multiple agents to develop and share a symbol system. Furthermore, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#26041;&#27861;&#26469;&#25552;&#39640;&#22312;&#28151;&#21512;&#35821;&#38899;&#25968;&#25454;&#19978;&#30340;&#35821;&#35328;&#35782;&#21035;&#31934;&#24230;&#65292;&#21253;&#25324;&#20351;&#29992;Residual CNN+GRU&#27169;&#22411;&#21644;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#30001;&#20110;&#35821;&#35328;&#28151;&#21512;&#30340;&#20302;&#36164;&#28304;&#24615;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#20102;&#21333;&#35821;&#35821;&#26009;&#24211;&#36827;&#34892;&#38134;&#25968;&#25454;&#21019;&#24314;&#21644;&#19978;&#37319;&#26679;&#12290;</title><link>http://arxiv.org/abs/2305.19759</link><description>&lt;p&gt;
&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#19982;&#36801;&#31227;&#23398;&#20064;&#35782;&#21035;&#28151;&#21512;&#35821;&#35328;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Simple yet Effective Code-Switching Language Identification with Multitask Pre-Training and Transfer Learning. (arXiv:2305.19759v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#26041;&#27861;&#26469;&#25552;&#39640;&#22312;&#28151;&#21512;&#35821;&#38899;&#25968;&#25454;&#19978;&#30340;&#35821;&#35328;&#35782;&#21035;&#31934;&#24230;&#65292;&#21253;&#25324;&#20351;&#29992;Residual CNN+GRU&#27169;&#22411;&#21644;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#30001;&#20110;&#35821;&#35328;&#28151;&#21512;&#30340;&#20302;&#36164;&#28304;&#24615;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#20102;&#21333;&#35821;&#35821;&#26009;&#24211;&#36827;&#34892;&#38134;&#25968;&#25454;&#21019;&#24314;&#21644;&#19978;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#35821;&#35328;&#28151;&#21512;&#29616;&#35937;&#65288;code-switching&#65289;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#22312;&#33521;&#27721;&#20799;&#31461;&#35821;&#38899;&#25968;&#25454;&#38598;&#19978;&#30340;&#35821;&#35328;&#35782;&#21035;&#31934;&#24230;&#65292;&#21253;&#25324;Residual CNN+GRU&#27169;&#22411;&#21644;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#26041;&#27861;&#20351;&#29992;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20316;&#20026;CSLID&#30340;&#36741;&#21161;&#20219;&#21153;&#12290;&#30001;&#20110;&#35821;&#35328;&#28151;&#21512;&#30340;&#20302;&#36164;&#28304;&#24615;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#20102;&#21333;&#35821;&#35821;&#26009;&#24211;&#36827;&#34892;&#38134;&#25968;&#25454;&#21019;&#24314;&#21644;&#19978;&#37319;&#26679;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code-switching, also called code-mixing, is the linguistics phenomenon where in casual settings, multilingual speakers mix words from different languages in one utterance. Due to its spontaneous nature, code-switching is extremely low-resource, which makes it a challenging problem for language and speech processing tasks. In such contexts, Code-Switching Language Identification (CSLID) becomes a difficult but necessary task if we want to maximally leverage existing monolingual tools for other tasks. In this work, we propose two novel approaches toward improving language identification accuracy on an English-Mandarin child-directed speech dataset. Our methods include a stacked Residual CNN+GRU model and a multitask pre-training approach to use Automatic Speech Recognition (ASR) as an auxiliary task for CSLID. Due to the low-resource nature of code-switching, we also employ careful silver data creation using monolingual corpora in both languages and up-sampling as data augmentation. We f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#65292;&#22914;&#20309;&#33258;&#21160;&#21306;&#20998;&#20154;&#31867;&#32763;&#35793;&#21644;&#26426;&#22120;&#32763;&#35793;&#12290;&#22312;&#21333;&#19968;&#28304;&#35821;&#35328;&#30340;&#24179;&#34892;&#25968;&#25454;&#35757;&#32451;&#20998;&#31867;&#22120;&#26102;&#65292;&#25105;&#20204;&#21487;&#20197;&#24456;&#22909;&#22320;&#25191;&#34892;&#26469;&#33258;&#19981;&#21516;&#28304;&#35821;&#35328;&#30340;&#33521;&#35821;&#32763;&#35793;&#30340;&#21028;&#21035;&#12290;&#23558;&#28304;&#25991;&#26412;&#21152;&#20837;&#22810;&#35821;&#35328;&#20998;&#31867;&#22120;&#30340;&#36755;&#20837;&#21487;&#20197;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#20351;&#29992;&#19981;&#21516;&#28304;&#35821;&#35328;&#30340;&#35757;&#32451;&#25968;&#25454;&#24448;&#24448;&#25552;&#39640;&#21333;&#35821;&#21644;&#22810;&#35821;&#35328;&#20998;&#31867;&#22120;&#30340;&#20934;&#30830;&#24615;&#12290;&#21452;&#35821;&#20998;&#31867;&#22120;&#19981;&#21487;&#34892;&#12290;</title><link>http://arxiv.org/abs/2305.19757</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#22330;&#26223;&#20013;&#33258;&#21160;&#21306;&#20998;&#20154;&#31867;&#32763;&#35793;&#21644;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Automatic Discrimination of Human and Neural Machine Translation in Multilingual Scenarios. (arXiv:2305.19757v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#65292;&#22914;&#20309;&#33258;&#21160;&#21306;&#20998;&#20154;&#31867;&#32763;&#35793;&#21644;&#26426;&#22120;&#32763;&#35793;&#12290;&#22312;&#21333;&#19968;&#28304;&#35821;&#35328;&#30340;&#24179;&#34892;&#25968;&#25454;&#35757;&#32451;&#20998;&#31867;&#22120;&#26102;&#65292;&#25105;&#20204;&#21487;&#20197;&#24456;&#22909;&#22320;&#25191;&#34892;&#26469;&#33258;&#19981;&#21516;&#28304;&#35821;&#35328;&#30340;&#33521;&#35821;&#32763;&#35793;&#30340;&#21028;&#21035;&#12290;&#23558;&#28304;&#25991;&#26412;&#21152;&#20837;&#22810;&#35821;&#35328;&#20998;&#31867;&#22120;&#30340;&#36755;&#20837;&#21487;&#20197;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#20351;&#29992;&#19981;&#21516;&#28304;&#35821;&#35328;&#30340;&#35757;&#32451;&#25968;&#25454;&#24448;&#24448;&#25552;&#39640;&#21333;&#35821;&#21644;&#22810;&#35821;&#35328;&#20998;&#31867;&#22120;&#30340;&#20934;&#30830;&#24615;&#12290;&#21452;&#35821;&#20998;&#31867;&#22120;&#19981;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#35299;&#20915;&#33258;&#21160;&#21306;&#20998;&#20154;&#31867;&#32763;&#35793;&#21644;&#26426;&#22120;&#32763;&#35793;&#30340;&#38382;&#39064;&#12290;&#19982;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#30740;&#31350;&#30456;&#21453;&#65292;&#25105;&#20204;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#36827;&#34892;&#23454;&#39564;&#65292;&#32771;&#34385;&#20102;&#22810;&#31181;&#35821;&#35328;&#21644;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#25105;&#20204;&#30340;&#24773;&#22659;&#20013;&#65292;&#20351;&#29992;&#21333;&#19968;&#28304;&#35821;&#35328;&#65288;&#22312;&#26412;&#20363;&#20013;&#20026;&#24503;&#35821;-&#33521;&#35821;&#65289;&#30340;&#24179;&#34892;&#25968;&#25454;&#35757;&#32451;&#20998;&#31867;&#22120;&#65292;&#20173;&#28982;&#33021;&#22815;&#24456;&#22909;&#22320;&#25191;&#34892;&#26469;&#33258;&#19981;&#21516;&#28304;&#35821;&#35328;&#30340;&#33521;&#35821;&#32763;&#35793;&#30340;&#21028;&#21035;&#65292;&#29978;&#33267;&#22312;&#26426;&#22120;&#32763;&#35793;&#26159;&#30001;&#19981;&#21516;&#20110;&#20854;&#35757;&#32451;&#31995;&#32479;&#30340;&#20854;&#20182;&#31995;&#32479;&#29983;&#25104;&#26102;&#20063;&#26159;&#22914;&#27492;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#19968;&#20010;&#22810;&#35821;&#35328;&#20998;&#31867;&#22120;&#30340;&#36755;&#20837;&#20013;&#21152;&#20837;&#28304;&#25991;&#26412;&#21487;&#20197;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#30456;&#27604;&#20110;&#21333;&#35821;&#20998;&#31867;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#19981;&#21516;&#28304;&#35821;&#35328;&#30340;&#35757;&#32451;&#25968;&#25454;&#65288;&#24503;&#35821;&#12289;&#20420;&#35821;&#21644;&#20013;&#25991;&#65289;&#24448;&#24448;&#21487;&#20197;&#25552;&#39640;&#21333;&#35821;&#21644;&#22810;&#35821;&#35328;&#20998;&#31867;&#22120;&#30340;&#20934;&#30830;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21452;&#35821;&#20998;&#31867;&#22120;&#26159;&#19981;&#29616;&#23454;&#30340;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
We tackle the task of automatically discriminating between human and machine translations. As opposed to most previous work, we perform experiments in a multilingual setting, considering multiple languages and multilingual pretrained language models. We show that a classifier trained on parallel data with a single source language (in our case German-English) can still perform well on English translations that come from different source languages, even when the machine translations were produced by other systems than the one it was trained on. Additionally, we demonstrate that incorporating the source text in the input of a multilingual classifier improves (i) its accuracy and (ii) its robustness on cross-system evaluation, compared to a monolingual classifier. Furthermore, we find that using training data from multiple source languages (German, Russian, and Chinese) tends to improve the accuracy of both monolingual and multilingual classifiers. Finally, we show that bilingual classifie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#20351;&#29992;&#37322;&#20041;&#35821;&#26009;&#24211;&#23545;&#31070;&#32463;&#21477;&#23376;&#31616;&#21270;&#26041;&#27861;&#36827;&#34892;&#21021;&#22987;&#21270;&#30340;&#31574;&#30053;&#65292;&#20174;&#32780;&#38477;&#20302;&#23545;&#24179;&#34892;&#35821;&#26009;&#24211;&#30340;&#20381;&#36182;&#65292;&#24182;&#22312;WikiLarge&#25968;&#25454;&#19978;&#33719;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.19754</link><description>&lt;p&gt;
&#20351;&#29992;&#37322;&#20041;&#35821;&#26009;&#24211;&#36827;&#34892;&#21021;&#22987;&#21270;&#30340;&#21477;&#23376;&#31616;&#21270;
&lt;/p&gt;
&lt;p&gt;
Sentence Simplification Using Paraphrase Corpus for Initialization. (arXiv:2305.19754v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#20351;&#29992;&#37322;&#20041;&#35821;&#26009;&#24211;&#23545;&#31070;&#32463;&#21477;&#23376;&#31616;&#21270;&#26041;&#27861;&#36827;&#34892;&#21021;&#22987;&#21270;&#30340;&#31574;&#30053;&#65292;&#20174;&#32780;&#38477;&#20302;&#23545;&#24179;&#34892;&#35821;&#26009;&#24211;&#30340;&#20381;&#36182;&#65292;&#24182;&#22312;WikiLarge&#25968;&#25454;&#19978;&#33719;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24207;&#21015;&#21040;&#24207;&#21015;&#26694;&#26550;&#30340;&#31070;&#32463;&#21477;&#23376;&#31616;&#21270;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#21477;&#23376;&#31616;&#21270;(SS)&#20219;&#21153;&#30340;&#20027;&#27969;&#26041;&#27861;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#26041;&#27861;&#30446;&#21069;&#21463;&#21040;&#24179;&#34892;SS&#35821;&#26009;&#24211;&#30340;&#31232;&#32570;&#24615;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#22914;&#20309;&#36890;&#36807;&#21033;&#29992;&#37322;&#20041;&#35821;&#26009;&#24211;&#23545;&#31070;&#32463;SS&#26041;&#27861;&#36827;&#34892;&#31934;&#24515;&#21021;&#22987;&#21270;&#65292;&#20174;&#32780;&#38477;&#20302;&#23545;&#24179;&#34892;&#35821;&#26009;&#24211;&#30340;&#20381;&#36182;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24471;&#21040;&#20197;&#19979;&#20004;&#20010;&#21457;&#29616;&#30340;&#21551;&#21457;&#65306;(1)&#37322;&#20041;&#35821;&#26009;&#24211;&#21253;&#25324;&#22823;&#37327;&#23646;&#20110;SS&#35821;&#26009;&#24211;&#30340;&#21477;&#23376;&#23545;&#12290;(2)&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#20445;&#30041;&#36825;&#20123;&#21477;&#23376;&#23545;&#20013;&#26356;&#39640;&#22797;&#26434;&#24230;&#24046;&#24322;&#30340;&#21477;&#23376;&#23545;&#26469;&#26500;&#24314;&#22823;&#35268;&#27169;&#30340;&#20266;&#24182;&#34892;SS&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#20351;&#29992;&#37322;&#20041;&#35821;&#26009;&#24211;&#21021;&#22987;&#21270;&#31070;&#32463;SS&#26041;&#27861;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#29992;&#21021;&#22987;&#21270;&#35757;&#32451;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#31070;&#32463;SS&#26041;&#27861;&#65292;&#22312;&#21487;&#29992;&#30340;WikiLarge&#25968;&#25454;&#19978;&#19982;&#33258;&#36523;&#27809;&#26377;&#21021;&#22987;&#21270;&#30456;&#27604;&#65292;&#21487;&#20197;&#33719;&#24471;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural sentence simplification method based on sequence-to-sequence framework has become the mainstream method for sentence simplification (SS) task. Unfortunately, these methods are currently limited by the scarcity of parallel SS corpus. In this paper, we focus on how to reduce the dependence on parallel corpus by leveraging a careful initialization for neural SS methods from paraphrase corpus. Our work is motivated by the following two findings: (1) Paraphrase corpus includes a large proportion of sentence pairs belonging to SS corpus. (2) We can construct large-scale pseudo parallel SS data by keeping these sentence pairs with a higher complexity difference. Therefore, we propose two strategies to initialize neural SS methods using paraphrase corpus. We train three different neural SS methods with our initialization, which can obtain substantial improvements on the available WikiLarge data compared with themselves without initialization.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#27169;&#22411;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#23545;&#29790;&#22763;&#24503;&#35821;&#26041;&#35328;&#36827;&#34892;&#39640;&#36136;&#37327;&#35821;&#38899;&#21512;&#25104;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#39033;&#26032;&#30340;TTS&#27169;&#22411;&#35780;&#20272;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.19750</link><description>&lt;p&gt;
&#29790;&#22763;&#24503;&#35821;&#35821;&#38899;&#21512;&#25104;&#27969;&#31243;--&#19968;&#39033;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Text-to-Speech Pipeline for Swiss German -- A comparison. (arXiv:2305.19750v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#27169;&#22411;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#23545;&#29790;&#22763;&#24503;&#35821;&#26041;&#35328;&#36827;&#34892;&#39640;&#36136;&#37327;&#35821;&#38899;&#21512;&#25104;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#39033;&#26032;&#30340;TTS&#27169;&#22411;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#20351;&#29992;&#19981;&#21516;&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#27169;&#22411;&#21512;&#25104;&#29790;&#22763;&#24503;&#35821;&#35821;&#38899;&#12290;&#25105;&#20204;&#23545;&#19977;&#20010;&#35821;&#26009;&#24211;&#30340;TTS&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21457;&#29616;VITS&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#65292;&#22240;&#27492;&#23558;&#20854;&#29992;&#20110;&#36827;&#19968;&#27493;&#27979;&#35797;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35753;&#35757;&#32451;&#22909;&#30340;vocoder GAN&#27169;&#22411;&#30340;&#37492;&#21035;&#22120;&#39044;&#27979;&#32473;&#23450;&#27874;&#24418;&#26159;&#20154;&#31867;&#29983;&#25104;&#30340;&#36824;&#26159;&#21512;&#25104;&#30340;&#26469;&#35780;&#20272;TTS&#27169;&#22411;&#12290;&#24635;&#20043;&#65292;&#25105;&#20204;&#30340;&#26368;&#20339;&#27169;&#22411;&#33021;&#22815;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#36136;&#37327;&#20026;&#19981;&#21516;&#30340;&#29790;&#22763;&#24503;&#35821;&#26041;&#35328;&#25552;&#20379;&#35821;&#38899;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we studied the synthesis of Swiss German speech using different Text-to-Speech (TTS) models. We evaluated the TTS models on three corpora, and we found, that VITS models performed best, hence, using them for further testing. We also introduce a new method to evaluate TTS models by letting the discriminator of a trained vocoder GAN model predict whether a given waveform is human or synthesized. In summary, our best model delivers speech synthesis for different Swiss German dialects with previously unachieved quality.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;UKP-SQuARE&#20316;&#20026;&#19968;&#20010;&#29992;&#20110;&#25945;&#25480;&#38382;&#31572;&#25216;&#26415;&#30340;&#20132;&#20114;&#24335;&#24037;&#20855;&#65292;&#23398;&#29983;&#21487;&#20197;&#36890;&#36807;&#20854;&#22312;&#19981;&#21516;&#35282;&#24230;&#20102;&#35299;&#21508;&#31181;QA&#27169;&#22411;&#65292;&#24182;&#20511;&#27492;&#33719;&#24471;&#29702;&#35770;&#27010;&#24565;&#21644;&#38382;&#39064;&#35299;&#20915;&#25216;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.19748</link><description>&lt;p&gt;
UKP-SQuARE: &#19968;&#20010;&#29992;&#20110;&#25945;&#25480;&#38382;&#31572;&#25216;&#26415;&#30340;&#20132;&#20114;&#24335;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
UKP-SQuARE: An Interactive Tool for Teaching Question Answering. (arXiv:2305.19748v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19748
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;UKP-SQuARE&#20316;&#20026;&#19968;&#20010;&#29992;&#20110;&#25945;&#25480;&#38382;&#31572;&#25216;&#26415;&#30340;&#20132;&#20114;&#24335;&#24037;&#20855;&#65292;&#23398;&#29983;&#21487;&#20197;&#36890;&#36807;&#20854;&#22312;&#19981;&#21516;&#35282;&#24230;&#20102;&#35299;&#21508;&#31181;QA&#27169;&#22411;&#65292;&#24182;&#20511;&#27492;&#33719;&#24471;&#29702;&#35770;&#27010;&#24565;&#21644;&#38382;&#39064;&#35299;&#20915;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#31572;&#25216;&#26415;&#30340;&#25351;&#25968;&#32423;&#22686;&#38271;&#20351;&#20854;&#25104;&#20026;&#20219;&#20309;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#35838;&#31243;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#35805;&#39064;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#25351;&#25968;&#32423;&#22686;&#38271;&#25152;&#23548;&#33268;&#30340;&#38382;&#31572;&#24191;&#24230;&#20351;&#20854;&#25104;&#20026;&#25945;&#25480;&#30456;&#20851;NLP&#20027;&#39064;&#65288;&#22914;&#20449;&#24687;&#26816;&#32034;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#23545;&#25239;&#25915;&#20987;&#31561;&#65289;&#30340;&#29702;&#24819;&#22330;&#26223;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;UKP-SQuARE&#20316;&#20026;QA&#25945;&#32946;&#24179;&#21488;&#12290;&#35813;&#24179;&#21488;&#25552;&#20379;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#29615;&#22659;&#65292;&#22312;&#36825;&#37324;&#23398;&#29983;&#21487;&#20197;&#36816;&#34892;&#12289;&#27604;&#36739;&#21644;&#20998;&#26512;&#19981;&#21516;&#35282;&#24230;&#30340;&#21508;&#31181;QA&#27169;&#22411;&#65292;&#22914;&#19968;&#33324;&#34892;&#20026;&#65292;&#21487;&#35299;&#37322;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#22240;&#27492;&#65292;&#23398;&#29983;&#21487;&#20197;&#22312;&#35838;&#22530;&#19978;&#20146;&#36523;&#20307;&#39564;&#19981;&#21516;&#30340;QA&#25216;&#26415;&#12290;&#30001;&#20110;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#23398;&#29983;&#20026;&#20013;&#24515;&#30340;QA&#25945;&#32946;&#26041;&#27861;&#65292;&#23398;&#29983;&#21487;&#20197;&#36890;&#36807;&#20132;&#20114;&#24335;&#30340;&#25506;&#32034;&#12289;&#23454;&#39564;&#21644;&#23454;&#36341;&#20219;&#21153;&#20027;&#21160;&#23398;&#20064;&#29702;&#35770;&#27010;&#24565;&#24182;&#33719;&#24471;&#38382;&#39064;&#35299;&#20915;&#25216;&#33021;&#65292;&#32780;&#19981;&#20165;&#20165;&#20381;&#36182;&#20256;&#32479;&#30340;&#35762;&#25480;&#12290;
&lt;/p&gt;
&lt;p&gt;
The exponential growth of question answering (QA) has made it an indispensable topic in any Natural Language Processing (NLP) course. Additionally, the breadth of QA derived from this exponential growth makes it an ideal scenario for teaching related NLP topics such as information retrieval, explainability, and adversarial attacks among others. In this paper, we introduce UKP-SQuARE as a platform for QA education. This platform provides an interactive environment where students can run, compare, and analyze various QA models from different perspectives, such as general behavior, explainability, and robustness. Therefore, students can get a first-hand experience in different QA techniques during the class. Thanks to this, we propose a learner-centered approach for QA education in which students proactively learn theoretical concepts and acquire problem-solving skills through interactive exploration, experimentation, and practical assignments, rather than solely relying on traditional le
&lt;/p&gt;</description></item><item><title>&#25991;&#31456;&#30740;&#31350;&#20102;&#25991;&#26412;&#34920;&#31034;&#23545;&#20110;&#20998;&#31867;&#20219;&#21153;&#34920;&#29616;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#20219;&#21153;&#23545;&#40784;&#24471;&#20998;&#26041;&#27861;&#26469;&#24230;&#37327;&#23545;&#40784;&#24615;&#65292;&#39564;&#35777;&#20102;&#20219;&#21153;&#23545;&#40784;&#24615;&#23545;&#20110;&#34920;&#31034;&#30340;&#20998;&#31867;&#24615;&#33021;&#26377;&#20915;&#23450;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.19747</link><description>&lt;p&gt;
&#36890;&#36807;&#27979;&#37327;&#20219;&#21153;&#23545;&#40784;&#24615;&#20998;&#26512;&#25991;&#26412;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Analyzing Text Representations by Measuring Task Alignment. (arXiv:2305.19747v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19747
&lt;/p&gt;
&lt;p&gt;
&#25991;&#31456;&#30740;&#31350;&#20102;&#25991;&#26412;&#34920;&#31034;&#23545;&#20110;&#20998;&#31867;&#20219;&#21153;&#34920;&#29616;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#20219;&#21153;&#23545;&#40784;&#24471;&#20998;&#26041;&#27861;&#26469;&#24230;&#37327;&#23545;&#40784;&#24615;&#65292;&#39564;&#35777;&#20102;&#20219;&#21153;&#23545;&#40784;&#24615;&#23545;&#20110;&#34920;&#31034;&#30340;&#20998;&#31867;&#24615;&#33021;&#26377;&#20915;&#23450;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#34920;&#31034;&#23545;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#12290;&#20160;&#20040;&#20351;&#24471;&#25991;&#26412;&#20998;&#31867;&#30340;&#34920;&#31034;&#26356;&#22909;&#65311;&#26159;&#22240;&#20026;&#31354;&#38388;&#30340;&#20960;&#20309;&#23646;&#24615;&#36824;&#26159;&#22240;&#20026;&#23427;&#19982;&#20219;&#21153;&#26377;&#33391;&#22909;&#30340;&#23545;&#40784;&#24615;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#20108;&#31181;&#20551;&#35774;&#12290;&#20026;&#20102;&#27979;&#35797;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#27425;&#32858;&#31867;&#30340;&#20219;&#21153;&#23545;&#40784;&#24471;&#20998;&#26041;&#27861;&#65292;&#22312;&#19981;&#21516;&#31890;&#24230;&#32423;&#21035;&#19978;&#24230;&#37327;&#23545;&#40784;&#24615;&#12290;&#25105;&#20204;&#30340;&#25991;&#26412;&#20998;&#31867;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#20551;&#35774;&#65292;&#35777;&#26126;&#20219;&#21153;&#23545;&#40784;&#21487;&#20197;&#35299;&#37322;&#32473;&#23450;&#34920;&#31034;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Textual representations based on pre-trained language models are key, especially in few-shot learning scenarios. What makes a representation good for text classification? Is it due to the geometric properties of the space or because it is well aligned with the task? We hypothesize the second claim. To test it, we develop a task alignment score based on hierarchical clustering that measures alignment at different levels of granularity. Our experiments on text classification validate our hypothesis by showing that task alignment can explain the classification performance of a given representation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#30693;&#35782;&#24211;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#22238;&#31572;&#22797;&#26434;&#33258;&#28982;&#35821;&#35328;&#30340;&#26597;&#35810;&#65292;&#25903;&#25345;&#24037;&#31243;&#24072;&#35775;&#38382;&#22826;&#31354;&#30862;&#29255;&#29615;&#22659;&#30340;&#30693;&#35782;&#24211;&#20013;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2305.19734</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#24211;&#30340;&#22826;&#31354;&#30862;&#29255;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Knowledge Base Question Answering for Space Debris Queries. (arXiv:2305.19734v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19734
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#30693;&#35782;&#24211;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#22238;&#31572;&#22797;&#26434;&#33258;&#28982;&#35821;&#35328;&#30340;&#26597;&#35810;&#65292;&#25903;&#25345;&#24037;&#31243;&#24072;&#35775;&#38382;&#22826;&#31354;&#30862;&#29255;&#29615;&#22659;&#30340;&#30693;&#35782;&#24211;&#20013;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22826;&#31354;&#26426;&#26500;&#25191;&#34892;&#22797;&#26434;&#30340;&#21355;&#26143;&#25805;&#20316;&#65292;&#38656;&#35201;&#22312;&#20854;&#24191;&#27867;&#30340;&#20449;&#24687;&#31995;&#32479;&#20013;&#23384;&#20648;&#21644;&#35775;&#38382;&#25216;&#26415;&#30693;&#35782;&#12290;&#30693;&#35782;&#24211;&#26159;&#20197;&#35268;&#27169;&#23384;&#20648;&#21644;&#35775;&#38382;&#27492;&#31867;&#20449;&#24687;&#30340;&#26377;&#25928;&#26041;&#24335;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#20026;&#27431;&#27954;&#31354;&#38388;&#23616;&#65288;ESA&#65289;&#24320;&#21457;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#22238;&#31572;&#22797;&#26434;&#30340;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#65292;&#20197;&#25903;&#25345;&#24037;&#31243;&#24072;&#35775;&#38382;&#27169;&#25311;&#36712;&#36947;&#22826;&#31354;&#30862;&#29255;&#29615;&#22659;&#30340;&#30693;&#35782;&#24211;&#20013;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22522;&#20110;&#19968;&#31181;&#31649;&#36947;&#27969;&#31243;&#65292;&#39318;&#20808;&#20174;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#29983;&#25104;&#19968;&#31995;&#21015;&#22522;&#26412;&#25968;&#25454;&#24211;&#25805;&#20316;&#65292;&#31216;&#20026;&#8220;&#31243;&#24207;&#33609;&#22270;&#8221;&#65292;&#28982;&#21518;&#23558;&#33609;&#22270;&#36716;&#25442;&#20026;&#20855;&#20307;&#30340;&#26597;&#35810;&#31243;&#24207;&#65292;&#21253;&#25324;&#23454;&#20307;&#12289;&#23646;&#24615;&#21644;&#20851;&#31995;&#30340;&#25552;&#21450;&#65292;&#26368;&#21518;&#38024;&#23545;&#25968;&#25454;&#24211;&#25191;&#34892;&#31243;&#24207;&#12290;&#36825;&#31181;&#27969;&#31243;&#20998;&#35299;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#21033;&#29992;&#22495;&#22806;&#25968;&#25454;&#21644;&#30001;GPT-3&#29983;&#25104;&#30340;&#21322;&#21512;&#25104;&#25968;&#25454;&#26469;&#35757;&#32451;&#31995;&#32479;&#65292;&#20174;&#32780;&#20943;&#23569;&#36807;&#24230;&#25311;&#21512;&#21644;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Space agencies execute complex satellite operations that need to be supported by the technical knowledge contained in their extensive information systems. Knowledge bases (KB) are an effective way of storing and accessing such information at scale. In this work we present a system, developed for the European Space Agency (ESA), that can answer complex natural language queries, to support engineers in accessing the information contained in a KB that models the orbital space debris environment. Our system is based on a pipeline which first generates a sequence of basic database operations, called a %program sketch, from a natural language question, then specializes the sketch into a concrete query program with mentions of entities, attributes and relations, and finally executes the program against the database. This pipeline decomposition approach enables us to train the system by leveraging out-of-domain data and semi-synthetic data generated by GPT-3, thus reducing overfitting and shor
&lt;/p&gt;</description></item><item><title>XPhoneBERT&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#25991;&#26412;&#36716;&#35821;&#38899;&#30340;&#39044;&#35757;&#32451;&#22810;&#35821;&#35328;&#35821;&#38899;&#21333;&#20301;&#34920;&#31034;&#27169;&#22411;&#65292;&#36890;&#36807;&#20854;&#20316;&#20026;&#36755;&#20837;&#35821;&#38899;&#32534;&#30721;&#22120;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;TTS&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#35821;&#38899;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.19709</link><description>&lt;p&gt;
XPhoneBERT: &#19968;&#31181;&#29992;&#20110;&#25991;&#26412;&#36716;&#35821;&#38899;&#30340;&#39044;&#35757;&#32451;&#22810;&#35821;&#35328;&#35821;&#38899;&#21333;&#20301;&#34920;&#31034;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
XPhoneBERT: A Pre-trained Multilingual Model for Phoneme Representations for Text-to-Speech. (arXiv:2305.19709v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19709
&lt;/p&gt;
&lt;p&gt;
XPhoneBERT&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#25991;&#26412;&#36716;&#35821;&#38899;&#30340;&#39044;&#35757;&#32451;&#22810;&#35821;&#35328;&#35821;&#38899;&#21333;&#20301;&#34920;&#31034;&#27169;&#22411;&#65292;&#36890;&#36807;&#20854;&#20316;&#20026;&#36755;&#20837;&#35821;&#38899;&#32534;&#30721;&#22120;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;TTS&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#35821;&#38899;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;XPhoneBERT&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#19979;&#28216;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#20219;&#21153;&#30340;&#35821;&#38899;&#21333;&#20301;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;XPhoneBERT&#20855;&#26377;&#19982;BERT-base&#30456;&#21516;&#30340;&#27169;&#22411;&#26550;&#26500;&#65292;&#20351;&#29992;RoBERTa&#39044;&#35757;&#32451;&#26041;&#27861;&#22312;&#23558;&#36817;100&#31181;&#35821;&#35328;&#21644;&#35821;&#22659;&#20013;&#30340;330M&#20010;&#35821;&#38899;&#21333;&#20301;&#32423;&#21477;&#23376;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;XPhoneBERT&#20316;&#20026;&#36755;&#20837;&#35821;&#38899;&#32534;&#30721;&#22120;&#26174;&#33879;&#25552;&#21319;&#20102;&#24378;&#31070;&#32463;&#32593;&#32476;TTS&#27169;&#22411;&#30340;&#33258;&#28982;&#24230;&#21644;&#35821;&#35843;&#65292;&#24182;&#19988;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#19979;&#20063;&#26377;&#36739;&#39640;&#30340;&#35821;&#38899;&#36136;&#37327;&#12290;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;XPhoneBERT&#65292;&#24076;&#26395;&#23427;&#33021;&#20419;&#36827;&#26410;&#26469;&#30340;&#22810;&#31181;&#35821;&#35328;TTS&#24212;&#29992;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;XPhoneBERT&#27169;&#22411;&#21487;&#22312;https://github.com/VinAIResearch/XPhoneBERT&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present XPhoneBERT, the first multilingual model pre-trained to learn phoneme representations for the downstream text-to-speech (TTS) task. Our XPhoneBERT has the same model architecture as BERT-base, trained using the RoBERTa pre-training approach on 330M phoneme-level sentences from nearly 100 languages and locales. Experimental results show that employing XPhoneBERT as an input phoneme encoder significantly boosts the performance of a strong neural TTS model in terms of naturalness and prosody and also helps produce fairly high-quality speech with limited training data. We publicly release our pre-trained XPhoneBERT with the hope that it would facilitate future research and downstream TTS applications for multiple languages. Our XPhoneBERT model is available at https://github.com/VinAIResearch/XPhoneBERT
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20154;&#24037;&#26234;&#33021;&#20581;&#24247;&#36741;&#23548;&#27169;&#22411;&#65292;&#21253;&#21547;&#19968;&#20010;&#25552;&#21462;&#24335; QA &#31995;&#32479;&#65292;&#36890;&#36807;&#23545;&#30561;&#30496;&#30456;&#20851;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#36890;&#36807;&#25968;&#25454;&#20013;&#24515;&#21270;&#26694;&#26550;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#12290; &#22312;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#30340;&#20154;&#24037;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#22312;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#30340;&#35797;&#28857;&#27979;&#35797;&#20013;&#39564;&#35777;&#23558;&#20854;&#38598;&#25104;&#21040;&#20154;&#24037;&#26234;&#33021;&#20581;&#24247;&#36741;&#23548;&#27169;&#22411;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.19707</link><description>&lt;p&gt;
&#26500;&#24314;&#25552;&#21462;&#24335;&#38382;&#31572;&#31995;&#32479;&#20197;&#25903;&#25345;&#20154;&#24037;&#26234;&#33021;&#20581;&#24247;&#36741;&#23548;&#27169;&#22411;&#22312;&#30561;&#30496;&#39046;&#22495;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Building Extractive Question Answering System to Support Human-AI Health Coaching Model for Sleep Domain. (arXiv:2305.19707v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19707
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20154;&#24037;&#26234;&#33021;&#20581;&#24247;&#36741;&#23548;&#27169;&#22411;&#65292;&#21253;&#21547;&#19968;&#20010;&#25552;&#21462;&#24335; QA &#31995;&#32479;&#65292;&#36890;&#36807;&#23545;&#30561;&#30496;&#30456;&#20851;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#36890;&#36807;&#25968;&#25454;&#20013;&#24515;&#21270;&#26694;&#26550;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#12290; &#22312;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#30340;&#20154;&#24037;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#22312;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#30340;&#35797;&#28857;&#27979;&#35797;&#20013;&#39564;&#35777;&#23558;&#20854;&#38598;&#25104;&#21040;&#20154;&#24037;&#26234;&#33021;&#20581;&#24247;&#36741;&#23548;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#20256;&#26579;&#24615;&#30142;&#30149; (NCDs) &#26159;&#20840;&#29699;&#27515;&#20129;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#38656;&#35201;&#20851;&#27880;&#21021;&#32423;&#39044;&#38450;&#21644;&#29983;&#27963;&#26041;&#24335;&#34892;&#20026;&#25913;&#21464;&#12290; &#20581;&#24247;&#36741;&#23548;&#21644;&#38382;&#31572; (QA) &#31995;&#32479;&#32467;&#21512;&#36215;&#26469;&#65292;&#26377;&#28508;&#21147;&#25913;&#21464;&#39044;&#38450;&#20445;&#20581;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20154;&#24037;&#26234;&#33021;&#20581;&#24247;&#36741;&#23548;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#20855;&#26377;&#29305;&#23450;&#39046;&#22495;&#30340;&#25552;&#21462;&#24335; QA &#31995;&#32479;&#12290; &#30561;&#30496;&#30456;&#20851;&#25968;&#25454;&#38598; SleepQA &#34987;&#25163;&#21160;&#32452;&#35013;&#24182;&#29992;&#20110;&#24494;&#35843;&#29305;&#23450;&#39046;&#22495;&#30340; BERT &#27169;&#22411;&#12290; &#35813; QA &#31995;&#32479;&#20351;&#29992;&#33258;&#21160;&#21644;&#20154;&#31867;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#12290; &#25968;&#25454;&#20013;&#24515;&#21270;&#26694;&#26550;&#36890;&#36807;&#25913;&#36827;&#27573;&#33853;&#26816;&#32034;&#21644;&#38382;&#39064;&#25913;&#20889;&#25552;&#39640;&#20102;&#31995;&#32479;&#24615;&#33021;&#12290; &#23613;&#31649;&#35813;&#31995;&#32479;&#22312;&#33258;&#21160;&#35780;&#20272;&#20013;&#26410;&#36229;&#36234;&#22522;&#32447;&#65292;&#20294;&#22312;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#30340;&#20154;&#24037;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290; &#22312;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#30340;&#35797;&#28857;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#23558;&#20854;&#38598;&#25104;&#21040;&#20154;&#24037;&#26234;&#33021;&#20581;&#24247;&#36741;&#23548;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Non-communicable diseases (NCDs) are a leading cause of global deaths, necessitating a focus on primary prevention and lifestyle behavior change. Health coaching, coupled with Question Answering (QA) systems, has the potential to transform preventive healthcare. This paper presents a human-Artificial Intelligence (AI) health coaching model incorporating a domain-specific extractive QA system. A sleep-focused dataset, SleepQA, was manually assembled and used to fine-tune domain-specific BERT models. The QA system was evaluated using automatic and human methods. A data-centric framework enhanced the system's performance by improving passage retrieval and question reformulation. Although the system did not outperform the baseline in automatic evaluation, it excelled in the human evaluation of real-world questions. Integration into a Human-AI health coaching model was tested in a pilot Randomized Controlled Trial (RCT).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#35299;&#20915;&#35821;&#20041;&#20219;&#21153;&#30340;&#27169;&#22411;&#20013;&#25512;&#23548;&#20986;&#21333;&#35789;&#37325;&#35201;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20854;&#31283;&#20581;&#24615;&#65292;&#21487;&#20197;&#22312;&#26080;&#38656;&#26174;&#24335;&#21333;&#35789;&#37325;&#35201;&#24615;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#35782;&#21035;&#21477;&#23376;&#20013;&#37325;&#35201;&#30340;&#21333;&#35789;&#12290;</title><link>http://arxiv.org/abs/2305.19689</link><description>&lt;p&gt;
&#21033;&#29992;&#35757;&#32451;&#29992;&#20110;&#35821;&#20041;&#20219;&#21153;&#30340;&#27169;&#22411;&#35780;&#20272;&#35789;&#35821;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Assessing Word Importance Using Models Trained for Semantic Tasks. (arXiv:2305.19689v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#35299;&#20915;&#35821;&#20041;&#20219;&#21153;&#30340;&#27169;&#22411;&#20013;&#25512;&#23548;&#20986;&#21333;&#35789;&#37325;&#35201;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20854;&#31283;&#20581;&#24615;&#65292;&#21487;&#20197;&#22312;&#26080;&#38656;&#26174;&#24335;&#21333;&#35789;&#37325;&#35201;&#24615;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#35782;&#21035;&#21477;&#23376;&#20013;&#37325;&#35201;&#30340;&#21333;&#35789;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#38656;&#35201;&#33258;&#21160;&#35782;&#21035;&#25991;&#26412;&#20013;&#26368;&#37325;&#35201;&#30340;&#21333;&#35789;&#12290;&#26412;&#25991;&#20174;&#35757;&#32451;&#29992;&#20110;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#21644;&#37322;&#20041;&#35782;&#21035;&#30340;&#27169;&#22411;&#20013;&#24471;&#21040;&#21333;&#35789;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#26088;&#22312;&#35299;&#37322;&#36825;&#20123;&#27169;&#22411;&#39044;&#27979;&#30340;&#23646;&#24615;&#26041;&#27861;&#65292;&#20026;&#27599;&#20010;&#36755;&#20837;&#20196;&#29260;&#23548;&#20986;&#37325;&#35201;&#24615;&#24471;&#20998;&#12290;&#25105;&#20204;&#20351;&#29992;&#25152;&#35859;&#30340;&#20132;&#21449;&#20219;&#21153;&#35780;&#20272;&#26469;&#35780;&#20272;&#23427;&#20204;&#30340;&#30456;&#20851;&#24615;&#65306;&#20998;&#26512;&#19968;&#20010;&#27169;&#22411;&#22312;&#25353;&#29031;&#21478;&#19968;&#20010;&#27169;&#22411;&#26435;&#37325;&#23631;&#34109;&#30340;&#36755;&#20837;&#19978;&#30340;&#34920;&#29616;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#21021;&#22987;&#20219;&#21153;&#30340;&#36873;&#25321;&#26159;&#31283;&#20581;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#35821;&#27861;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#20998;&#25968;&#65292;&#24182;&#35266;&#23519;&#21040;&#26377;&#36259;&#30340;&#27169;&#24335;&#65292;&#20363;&#22914;&#26356;&#25509;&#36817;&#35821;&#27861;&#26641;&#26681;&#30340;&#21333;&#35789;&#25509;&#25910;&#36739;&#39640;&#30340;&#37325;&#35201;&#24615;&#24471;&#20998;&#12290;&#24635;&#20043;&#65292;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#22312;&#35757;&#32451;&#20013;&#27809;&#26377;&#20219;&#20309;&#26174;&#24335;&#21333;&#35789;&#37325;&#35201;&#24615;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#35782;&#21035;&#21477;&#23376;&#20013;&#37325;&#35201;&#30340;&#21333;&#35789;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many NLP tasks require to automatically identify the most significant words in a text. In this work, we derive word significance from models trained to solve semantic task: Natural Language Inference and Paraphrase Identification. Using an attribution method aimed to explain the predictions of these models, we derive importance scores for each input token. We evaluate their relevance using a so-called cross-task evaluation: Analyzing the performance of one model on an input masked according to the other model's weight, we show that our method is robust with respect to the choice of the initial task. Additionally, we investigate the scores from the syntax point of view and observe interesting patterns, e.g. words closer to the root of a syntactic tree receive higher importance scores. Altogether, these observations suggest that our method can be used to identify important words in sentences without any explicit word importance labeling in training.
&lt;/p&gt;</description></item><item><title>&#20026;&#20102;&#35299;&#20915;VQA&#31995;&#32479;&#22240;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#28151;&#28102;&#25928;&#24212;&#32780;&#23548;&#33268;&#30340;&#21452;&#37325;&#20559;&#24046;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21453;&#20107;&#23454;&#30340;&#25512;&#29702;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#21516;&#26102;&#24182;&#39640;&#25928;&#22320;&#20943;&#23569;&#35270;&#35273;&#21644;&#35821;&#35328;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2305.19664</link><description>&lt;p&gt;
&#25581;&#31034;&#35270;&#35273;&#38382;&#31572;&#20013;&#30340;&#36328;&#27169;&#24577;&#20559;&#35265;&#65306;&#21487;&#33021;&#19990;&#30028;VQA&#30340;&#22240;&#26524;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Unveiling Cross Modality Bias in Visual Question Answering: A Causal View with Possible Worlds VQA. (arXiv:2305.19664v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19664
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;VQA&#31995;&#32479;&#22240;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#28151;&#28102;&#25928;&#24212;&#32780;&#23548;&#33268;&#30340;&#21452;&#37325;&#20559;&#24046;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21453;&#20107;&#23454;&#30340;&#25512;&#29702;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#21516;&#26102;&#24182;&#39640;&#25928;&#22320;&#20943;&#23569;&#35270;&#35273;&#21644;&#35821;&#35328;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22686;&#24378;VQA&#31995;&#32479;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#35768;&#22810;&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#28040;&#38500;&#25463;&#24452;&#24335;&#30340;&#35821;&#35328;&#25110;&#35270;&#35273;&#20851;&#32852;&#26469;&#22238;&#31572;&#38382;&#39064;&#12290;&#23613;&#31649;&#36825;&#20123;&#30740;&#31350;&#33268;&#21147;&#20110;&#27492;&#65292;&#20294;&#25991;&#29486;&#36824;&#27809;&#26377;&#21516;&#26102;&#35299;&#20915;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#28151;&#28102;&#25928;&#24212;&#12290;&#32467;&#26524;&#65292;&#20182;&#20204;&#20174;&#19968;&#20010;&#27169;&#24577;&#20943;&#23569;&#23398;&#20064;&#20559;&#35265;&#26102;&#65292;&#36890;&#24120;&#20250;&#22686;&#21152;&#21478;&#19968;&#20010;&#27169;&#24577;&#30340;&#20559;&#35265;&#12290;&#26412;&#25991;&#39318;&#20808;&#24314;&#27169;&#23548;&#33268;&#35821;&#35328;&#21644;&#35270;&#35273;&#20559;&#35265;&#30340;&#28151;&#28102;&#25928;&#24212;&#65292;&#28982;&#21518;&#25552;&#20986;&#19968;&#20010;&#21453;&#20107;&#23454;&#30340;&#25512;&#29702;&#26041;&#27861;&#26469;&#28040;&#38500;&#36825;&#31181;&#24433;&#21709;&#12290;&#22312;&#36825;&#31181;&#31574;&#30053;&#19979;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#21516;&#26102;&#19988;&#39640;&#25928;&#22320;&#20943;&#23569;&#35270;&#35273;&#21644;&#35821;&#35328;&#20559;&#35265;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#31687;&#21033;&#29992;&#22240;&#26524;&#20851;&#31995;&#35299;&#37322;&#28040;&#38500;&#35270;&#35273;&#38382;&#31572;&#20013;&#35270;&#35273;&#21644;&#35821;&#35328;&#28151;&#28102;&#25928;&#24212;&#30340;&#22320;&#22336;&#20559;&#35265;&#30340;&#20316;&#21697;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#19968;&#20010;&#35299;&#37322;&#28040;&#38500;&#31574;&#30053;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#25968;&#23383;&#31572;&#26696;&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
To increase the generalization capability of VQA systems, many recent studies have tried to de-bias spurious language or vision associations that shortcut the question or image to the answer. Despite these efforts, the literature fails to address the confounding effect of vision and language simultaneously. As a result, when they reduce bias learned from one modality, they usually increase bias from the other. In this paper, we first model a confounding effect that causes language and vision bias simultaneously, then propose a counterfactual inference to remove the influence of this effect. The model trained in this strategy can concurrently and efficiently reduce vision and language bias. To the best of our knowledge, this is the first work to reduce biases resulting from confounding effects of vision and language in VQA, leveraging causal explain-away relations. We accompany our method with an explain-away strategy, pushing the accuracy of the questions with numerical answers results
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#35745;&#31639;&#35821;&#35328;&#23398;&#20013;&#21103;&#35789;&#30340;&#30456;&#20851;&#20998;&#26512;&#23384;&#22312;&#31995;&#32479;&#24615;&#31354;&#32570;&#65292;&#22312;&#26694;&#26550;&#35821;&#20041;&#23398;&#65288;&#22914;FrameNet&#65289;&#30340;&#24110;&#21161;&#19979;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#21103;&#35789;&#20998;&#26512;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.19650</link><description>&lt;p&gt;
&#24778;&#20154;&#22320;&#65292;&#21103;&#35789;&#22312;&#35745;&#31639;&#35821;&#35328;&#23398;&#20013;&#34987;&#24573;&#35270;&#20102; (arXiv:2305.19650v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
Adverbs, Surprisingly. (arXiv:2305.19650v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#35745;&#31639;&#35821;&#35328;&#23398;&#20013;&#21103;&#35789;&#30340;&#30456;&#20851;&#20998;&#26512;&#23384;&#22312;&#31995;&#32479;&#24615;&#31354;&#32570;&#65292;&#22312;&#26694;&#26550;&#35821;&#20041;&#23398;&#65288;&#22914;FrameNet&#65289;&#30340;&#24110;&#21161;&#19979;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#21103;&#35789;&#20998;&#26512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30340;&#20986;&#21457;&#28857;&#26159;&#21103;&#35789;&#22312;&#35745;&#31639;&#35821;&#35328;&#23398;&#20013;&#34987;&#24573;&#30053;&#20102;&#12290;&#36825;&#20010;&#35266;&#28857;&#26469;&#28304;&#20110;&#20004;&#20010;&#20998;&#26512;&#65306;&#19968;&#26159;&#25991;&#29486;&#32508;&#36848;&#65292;&#20108;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#21103;&#35789;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#39564;&#35777;&#26368;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#25581;&#31034;&#20102;&#26377;&#20851;&#21103;&#35789;&#21547;&#20041;&#30340;&#31995;&#32479;&#24615;&#31354;&#32570;&#12290;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#26694;&#26550;&#35821;&#20041;&#23398;&#26469;&#34920;&#24449;&#21333;&#35789;&#21547;&#20041;&#65292;&#20363;&#22914;FrameNet&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#21103;&#35789;&#20998;&#26512;&#26041;&#27861;&#65292;&#22240;&#20854;&#33021;&#25551;&#36848;&#27169;&#31946;&#24615;&#12289;&#35821;&#20041;&#35282;&#33394;&#21644;&#31354;&#20540;&#23454;&#20363;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper begins with the premise that adverbs are neglected in computational linguistics. This view derives from two analyses: a literature review and a novel adverb dataset to probe a state-of-the-art language model, thereby uncovering systematic gaps in accounts for adverb meaning. We suggest that using Frame Semantics for characterizing word meaning, as in FrameNet, provides a promising approach to adverb analysis, given its ability to describe ambiguity, semantic roles, and null instantiation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#25991;&#26412;&#20998;&#31867;&#31995;&#32479;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#24178;&#20928;&#26631;&#31614;&#25915;&#20987;&#65292;&#24182;&#25552;&#39640;&#20102;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#27604;&#36215;&#26631;&#31614;&#32763;&#36716;&#25915;&#20987;&#26356;&#38590;&#20197;&#38450;&#24481;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#20102;&#22810;&#31181;&#38450;&#24481;&#31574;&#30053;&#23545;&#20004;&#31181;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.19607</link><description>&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#31995;&#32479;&#20013;&#30340;&#23545;&#25239;&#24615;&#24178;&#20928;&#26631;&#31614;&#21518;&#38376;&#25915;&#20987;&#21450;&#20854;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Adversarial Clean Label Backdoor Attacks and Defenses on Text Classification Systems. (arXiv:2305.19607v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19607
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#25991;&#26412;&#20998;&#31867;&#31995;&#32479;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#24178;&#20928;&#26631;&#31614;&#25915;&#20987;&#65292;&#24182;&#25552;&#39640;&#20102;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#27604;&#36215;&#26631;&#31614;&#32763;&#36716;&#25915;&#20987;&#26356;&#38590;&#20197;&#38450;&#24481;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#20102;&#22810;&#31181;&#38450;&#24481;&#31574;&#30053;&#23545;&#20004;&#31181;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24178;&#20928;&#26631;&#31614;&#25915;&#20987;&#26159;&#19968;&#31181;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#20165;&#20462;&#25913;&#35757;&#32451;&#25968;&#25454;&#30340;&#25991;&#26412;&#36755;&#20837;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#26631;&#27880;&#20989;&#25968;&#12290;&#26412;&#25991;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#24178;&#20928;&#26631;&#31614;&#25915;&#20987;&#65292;&#21487;&#20197;&#25925;&#24847;&#25200;&#21160;&#35757;&#32451;&#38598;&#20013;&#30340;&#21516;&#31867;&#26679;&#26412;&#26469;&#27745;&#26579;&#35757;&#32451;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25915;&#20987;&#32773;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#24178;&#20928;&#26631;&#31614;&#25915;&#20987;&#30340;&#25968;&#25454;&#35201;&#27714;&#65292;&#20351;&#29992;&#19978;&#36848;&#26041;&#27861;&#21487;&#20197;&#23558;&#35201;&#27714;&#30340;&#25968;&#25454;&#37327;&#38477;&#20302;&#21040;&#21407;&#26469;&#30340;20%&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#35780;&#20272;&#21644;&#20998;&#26512;&#20102;&#19968;&#20123;&#26082;&#21487;&#38450;&#24481;&#24178;&#20928;&#26631;&#31614;&#25915;&#20987;&#21448;&#21487;&#38450;&#24481;&#26631;&#31614;&#32763;&#36716;&#25915;&#20987;&#30340;&#38450;&#24481;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clean-label (CL) attack is a form of data poisoning attack where an adversary modifies only the textual input of the training data, without requiring access to the labeling function. CL attacks are relatively unexplored in NLP, as compared to label flipping (LF) attacks, where the latter additionally requires access to the labeling function as well. While CL attacks are more resilient to data sanitization and manual relabeling methods than LF attacks, they often demand as high as ten times the poisoning budget than LF attacks. In this work, we first introduce an Adversarial Clean Label attack which can adversarially perturb in-class training examples for poisoning the training set. We then show that an adversary can significantly bring down the data requirements for a CL attack, using the aforementioned approach, to as low as 20% of the data otherwise required. We then systematically benchmark and analyze a number of defense methods, for both LF and CL attacks, some previously employed
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;/&#23569;&#26679;&#26412;&#29615;&#22659;&#19979;&#38590;&#20197;&#29702;&#35299;&#8220;respectively&#8221;&#30340;&#21508;&#31181;&#35835;&#27861;&#65292;&#38656;&#35201;&#26356;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#21644;&#20381;&#36182;&#24120;&#35782;&#25512;&#29702;&#65292;&#20173;&#33853;&#21518;&#20110;&#20154;&#31867;&#12290;</title><link>http://arxiv.org/abs/2305.19597</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;/&#23569;&#26679;&#26412;&#29615;&#22659;&#19979;&#26080;&#27861;&#27491;&#30830;&#29702;&#35299;&#8220;respectively&#8221;&#30340;&#21407;&#22240;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
What does the Failure to Reason with "Respectively" in Zero/Few-Shot Settings Tell Us about Language Models?. (arXiv:2305.19597v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;/&#23569;&#26679;&#26412;&#29615;&#22659;&#19979;&#38590;&#20197;&#29702;&#35299;&#8220;respectively&#8221;&#30340;&#21508;&#31181;&#35835;&#27861;&#65292;&#38656;&#35201;&#26356;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#21644;&#20381;&#36182;&#24120;&#35782;&#25512;&#29702;&#65292;&#20173;&#33853;&#21518;&#20110;&#20154;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21487;&#20197;&#36731;&#26494;&#29702;&#35299;&#8220;Niels Bohr and Kurt Cobain were born in Copenhagen and Seattle, respectively&#8221;&#36825;&#31181;&#21477;&#23376;&#30340;&#21327;&#20316;&#32467;&#26500;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#26029;&#65288;NLI&#65289;&#30340;&#32972;&#26223;&#19979;&#65292;&#26412;&#25991;&#20174;&#21477;&#27861;-&#35821;&#20041;&#21644;&#24120;&#35782;-&#19990;&#30028;&#30693;&#35782;&#30340;&#20004;&#20010;&#35282;&#24230;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22914;&#20309;&#29702;&#35299;&#20004;&#20010;&#35835;&#25968;&#65288;Gawron&#21644;Kehler&#65292;2004&#65289; &#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#25511;&#21512;&#25104;&#25968;&#25454;&#38598;WikiResNLI &#21644;&#19968;&#20010;&#33258;&#28982;&#25968;&#25454;&#38598; NatResNLI&#65292;&#20197;&#21253;&#21547;&#21508;&#31181;&#26174;&#24335;&#21644;&#38544;&#24335;&#23454;&#29616;&#30340;&#8220;respectively&#8221;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24494;&#35843;&#21518;&#30340;NLI&#27169;&#22411;&#22312;&#27809;&#26377;&#26174;&#24335;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#38590;&#20197;&#29702;&#35299;&#36825;&#26679;&#30340;&#35835;&#25968;&#12290;&#24403;&#23384;&#22312;&#26174;&#24335;&#25552;&#31034;&#26102;&#65292;&#38646;/&#23569;&#26679;&#26412;&#23398;&#20064;&#24456;&#23481;&#26131;&#65292;&#32780;&#24403;&#35813;&#35835;&#25968;&#38544;&#21547;&#26102;&#65292;&#21017;&#38656;&#35201;&#26356;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#65292;&#20197;&#20381;&#36182;&#24120;&#35782;&#25512;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32454;&#33268;&#20998;&#26512;&#34920;&#26126;&#65292;&#27169;&#22411;&#26080;&#27861;&#22312;&#19981;&#21516;&#32467;&#26500;&#20043;&#38388;&#36827;&#34892;&#27867;&#21270;&#12290;&#24635;&#20043;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#38646;/&#23569;&#26679;&#26412;&#29615;&#22659;&#19979;&#65292;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#8220;respectively&#8221;&#30340;&#21508;&#31181;&#35835;&#27861;&#26041;&#38754;&#20173;&#33853;&#21518;&#20110;&#20154;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans can effortlessly understand the coordinate structure of sentences such as "Niels Bohr and Kurt Cobain were born in Copenhagen and Seattle, respectively". In the context of natural language inference (NLI), we examine how language models (LMs) reason with respective readings (Gawron and Kehler, 2004) from two perspectives: syntactic-semantic and commonsense-world knowledge. We propose a controlled synthetic dataset WikiResNLI and a naturally occurring dataset NatResNLI to encompass various explicit and implicit realizations of "respectively". We show that fine-tuned NLI models struggle with understanding such readings without explicit supervision. While few-shot learning is easy in the presence of explicit cues, longer training is required when the reading is evoked implicitly, leaving models to rely on common sense inferences. Furthermore, our fine-grained analysis indicates models fail to generalize across different constructions. To conclude, we demonstrate that LMs still lag 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;BERT&#27169;&#22411;&#24314;&#27169;&#20102;&#31532;&#20108;&#35821;&#35328;&#20064;&#24471;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#20197;&#30740;&#31350;&#27597;&#35821;&#20799;&#31461;&#25351;&#23548;&#35821;&#35328;&#65288;CDS&#65289;&#23545;&#33521;&#25991;&#35821;&#35328;&#20064;&#24471;&#30340;&#31215;&#26497;&#21644;&#28040;&#26497;&#36716;&#31227;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2305.19589</link><description>&lt;p&gt;
SLABERT&#65306;&#20351;&#29992;BERT&#27169;&#22411;&#24314;&#27169;&#31532;&#20108;&#35821;&#35328;&#20064;&#24471;
&lt;/p&gt;
&lt;p&gt;
SLABERT Talk Pretty One Day: Modeling Second Language Acquisition with BERT. (arXiv:2305.19589v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19589
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;BERT&#27169;&#22411;&#24314;&#27169;&#20102;&#31532;&#20108;&#35821;&#35328;&#20064;&#24471;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#20197;&#30740;&#31350;&#27597;&#35821;&#20799;&#31461;&#25351;&#23548;&#35821;&#35328;&#65288;CDS&#65289;&#23545;&#33521;&#25991;&#35821;&#35328;&#20064;&#24471;&#30340;&#31215;&#26497;&#21644;&#28040;&#26497;&#36716;&#31227;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31532;&#20108;&#35821;&#35328;&#20064;&#24471;&#65288;SLA&#65289;&#30740;&#31350;&#24191;&#27867;&#30740;&#31350;&#20102;&#36328;&#35821;&#35328;&#36716;&#31227;&#65292;&#21363;&#27597;&#35821;[L1]&#30340;&#35821;&#35328;&#32467;&#26500;&#23545;&#22806;&#35821;[L2]&#20064;&#24471;&#30340;&#24433;&#21709;&#12290;&#36825;&#31181;&#36716;&#31227;&#30340;&#24433;&#21709;&#21487;&#20197;&#26159;&#31215;&#26497;&#30340;(&#20419;&#36827;&#20064;&#24471;)&#25110;&#28040;&#26497;&#30340;(&#38459;&#30861;&#20064;&#24471;)&#12290;&#25105;&#20204;&#21457;&#29616;NLP&#25991;&#29486;&#27809;&#26377;&#36275;&#22815;&#20851;&#27880;&#28040;&#26497;&#36716;&#31227;&#29616;&#35937;&#12290;&#20026;&#20102;&#29702;&#35299;L1&#21644;L2&#20043;&#38388;&#31215;&#26497;&#21644;&#28040;&#26497;&#36716;&#31227;&#30340;&#27169;&#24335;&#65292;&#25105;&#20204;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#23545;&#39034;&#24207;&#31532;&#20108;&#35821;&#35328;&#20064;&#24471;&#36827;&#34892;&#24314;&#27169;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#24180;&#40836;&#25490;&#24207;CHILDES&#65288;MAO-CHILDES&#65289;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;5&#31181;&#31867;&#22411;&#19981;&#21516;&#30340;&#35821;&#35328;&#65292;&#21363;&#24503;&#35821;&#12289;&#27861;&#35821;&#12289;&#27874;&#20848;&#35821;&#12289;&#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#21644;&#26085;&#35821;&#65292;&#20197;&#29702;&#35299;&#27597;&#35821;&#20799;&#31461;&#25351;&#23548;&#35821;&#35328;&#65288;CDS&#65289;[L1]&#23545;&#33521;&#35821;&#35821;&#35328;&#20064;&#24471;[L2]&#26377;&#22810;&#22823;&#24110;&#21161;&#25110;&#20914;&#31361;&#12290;&#20026;&#20102;&#26816;&#26597;&#27597;&#35821;CDS&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#20351;&#29992;TILT-based&#36328;&#35821;&#35328;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Second language acquisition (SLA) research has extensively studied cross-linguistic transfer, the influence of linguistic structure of a speaker's native language [L1] on the successful acquisition of a foreign language [L2]. Effects of such transfer can be positive (facilitating acquisition) or negative (impeding acquisition). We find that NLP literature has not given enough attention to the phenomenon of negative transfer. To understand patterns of both positive and negative transfer between L1 and L2, we model sequential second language acquisition in LMs. Further, we build a Mutlilingual Age Ordered CHILDES (MAO-CHILDES) -- a dataset consisting of 5 typologically diverse languages, i.e., German, French, Polish, Indonesian, and Japanese -- to understand the degree to which native Child-Directed Speech (CDS) [L1] can help or conflict with English language acquisition [L2]. To examine the impact of native CDS, we use the TILT-based cross lingual transfer learning approach established 
&lt;/p&gt;</description></item><item><title>LAIT&#26159;&#19968;&#31181;&#36890;&#36807;&#22810;&#27573;&#32534;&#30721;&#23454;&#29616;&#36328;&#27573;&#27880;&#24847;&#21147;&#30340;&#26032;&#26694;&#26550;&#65292;&#20351;&#29992;Layer-Adjustable Interactions&#25216;&#26415;&#23454;&#29616;&#20998;&#27573;&#32534;&#30721;&#21644;&#36880;&#23618;&#20132;&#20114;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#31934;&#24230;&#65292;&#24182;&#26497;&#22823;&#22320;&#31616;&#21270;&#20102;&#27169;&#22411;&#35774;&#35745;&#30340;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.19585</link><description>&lt;p&gt;
&#22522;&#20110;&#21487;&#35843;&#23618;&#20132;&#20114;&#30340;Transformer&#39640;&#25928;&#22810;&#27573;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
LAIT: Efficient Multi-Segment Encoding in Transformers with Layer-Adjustable Interaction. (arXiv:2305.19585v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19585
&lt;/p&gt;
&lt;p&gt;
LAIT&#26159;&#19968;&#31181;&#36890;&#36807;&#22810;&#27573;&#32534;&#30721;&#23454;&#29616;&#36328;&#27573;&#27880;&#24847;&#21147;&#30340;&#26032;&#26694;&#26550;&#65292;&#20351;&#29992;Layer-Adjustable Interactions&#25216;&#26415;&#23454;&#29616;&#20998;&#27573;&#32534;&#30721;&#21644;&#36880;&#23618;&#20132;&#20114;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#31934;&#24230;&#65292;&#24182;&#26497;&#22823;&#22320;&#31616;&#21270;&#20102;&#27169;&#22411;&#35774;&#35745;&#30340;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#32534;&#30721;&#22120;&#36890;&#36807;&#23545;&#21508;&#20010;&#20196;&#29260;&#30340;&#27880;&#24847;&#21147;&#36827;&#34892;&#32534;&#30721;&#65292;&#20351;&#20854;&#19978;&#19979;&#25991;&#24471;&#20197;&#24314;&#31435;&#65292;&#20294;&#23545;&#20110;&#38271;&#25991;&#26412;&#20108;&#27425;&#35745;&#31639;&#25104;&#26412;&#36739;&#39640;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21517;&#20026;LAIT&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#27573;&#32534;&#30721;&#23454;&#29616;&#36328;&#27573;&#27880;&#24847;&#21147;&#65292;&#26497;&#22823;&#22320;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#31934;&#24230;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;Layer-Adjustable Interactions&#25216;&#26415;&#23454;&#29616;&#20998;&#27573;&#32534;&#30721;&#21644;&#36880;&#23618;&#20132;&#20114;&#65292;&#19981;&#20165;&#26377;&#25928;&#22320;&#21033;&#29992;&#20102;&#39044;&#35757;&#32451;&#30340;Transformers&#27169;&#22411;&#65292;&#32780;&#19988;&#26497;&#22823;&#22320;&#31616;&#21270;&#20102;&#27169;&#22411;&#35774;&#35745;&#30340;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer encoders contextualize token representations by attending to all other tokens at each layer, leading to quadratic increase in compute effort with the input length. In practice, however, the input text of many NLP tasks can be seen as a sequence of related segments (e.g., the sequence of sentences within a passage, or the hypothesis and premise in NLI). While attending across these segments is highly beneficial for many tasks, we hypothesize that this interaction can be delayed until later encoding stages.  To this end, we introduce Layer-Adjustable Interactions in Transformers (LAIT). Within LAIT, segmented inputs are first encoded independently, and then jointly. This partial two-tower architecture bridges the gap between a Dual Encoder's ability to pre-compute representations for segments and a fully self-attentive Transformer's capacity to model cross-segment attention. The LAIT framework effectively leverages existing pretrained Transformers and converts them into the h
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#30456;&#20284;&#30340;&#22768;&#38899;&#26144;&#23556;&#21040;&#24120;&#35265;&#30340;&#26631;&#31614;&#65292;&#21033;&#29992;CLS&#21644;&#35821;&#35328;&#26631;&#27880;&#30456;&#32467;&#21512;&#26469;&#25552;&#39640;&#22810;&#35821;&#35328;ASR&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.19584</link><description>&lt;p&gt;
&#22522;&#20110;CLS&#21644;&#35821;&#35328;&#26631;&#27880;&#30340;&#26631;&#31614;&#32452;&#21512;&#26041;&#27861;&#65306;&#25552;&#39640;&#22810;&#35821;&#35328;ASR&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
The Tag-Team Approach: Leveraging CLS and Language Tagging for Enhancing Multilingual ASR. (arXiv:2305.19584v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19584
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#30456;&#20284;&#30340;&#22768;&#38899;&#26144;&#23556;&#21040;&#24120;&#35265;&#30340;&#26631;&#31614;&#65292;&#21033;&#29992;CLS&#21644;&#35821;&#35328;&#26631;&#27880;&#30456;&#32467;&#21512;&#26469;&#25552;&#39640;&#22810;&#35821;&#35328;ASR&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20010;&#35821;&#35328;&#22810;&#26679;&#30340;&#22269;&#23478;&#65292;&#22914;&#21360;&#24230;&#65292;&#24314;&#31435;&#19968;&#20010;&#22810;&#35821;&#35328;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;(ASR)&#31995;&#32479;&#21487;&#33021;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#30001;&#20110;&#35821;&#38899;&#25968;&#25454;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#21644;&#33050;&#26412;&#30340;&#24046;&#24322;&#12290;&#36825;&#20010;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#35768;&#22810;&#36825;&#20123;&#35821;&#35328;&#22312;&#38899;&#32032;&#19978;&#30340;&#30456;&#20284;&#24615;&#26469;&#35299;&#20915;&#12290;&#23558;&#36825;&#20123;&#35821;&#35328;&#36716;&#25442;&#20026;&#19968;&#20010;&#36890;&#29992;&#26631;&#31614;&#38598;(CLSet)&#65292;&#36890;&#36807;&#23558;&#30456;&#20284;&#30340;&#22768;&#38899;&#26144;&#23556;&#21040;&#24120;&#35265;&#30340;&#26631;&#31614;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25506;&#35752;&#24182;&#27604;&#36739;&#26032;&#30340;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#22522;&#20110;CLS&#30340;&#22810;&#35821;&#35328;ASR&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22312;CLS&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#32473;&#20104;&#35821;&#35328;ID&#25110;&#32773;&#20351;&#29992;CLS&#21040;&#26412;&#22320;&#33050;&#26412;&#36716;&#25442;&#22120;&#26469;&#27880;&#20837;&#29305;&#23450;&#30340;&#35821;&#35328;&#20449;&#24687;&#12290;&#36825;&#20123;&#26041;&#27861;&#19982;CLS&#22522;&#32447;&#30456;&#27604;&#65292;&#22312;&#35789;&#38169;&#35823;&#29575;(WER)&#26041;&#38754;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#20998;&#24067;&#20043;&#22806;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#23581;&#35797;&#20197;&#26816;&#26597;&#20854;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building a multilingual Automated Speech Recognition (ASR) system in a linguistically diverse country like India can be a challenging task due to the differences in scripts and the limited availability of speech data. This problem can be solved by exploiting the fact that many of these languages are phonetically similar. These languages can be converted into a Common Label Set (CLS) by mapping similar sounds to common labels. In this paper, new approaches are explored and compared to improve the performance of CLS based multilingual ASR model. Specific language information is infused in the ASR model by giving Language ID or using CLS to Native script converter on top of the CLS Multilingual model. These methods give a significant improvement in Word Error Rate (WER) compared to the CLS baseline. These methods are further tried on out-of-distribution data to check their robustness.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#25955;&#30721;&#21644;&#28151;&#21512;&#22120;&#30456;&#21327;&#20316;&#30340;&#31471;&#21040;&#31471;&#34920;&#29616;&#21147;TTS&#65292;&#23427;&#37319;&#29992;&#26032;&#30340;&#36755;&#20837;&#34920;&#31034;&#21644;&#31616;&#21333;&#30340;&#26550;&#26500;&#26469;&#23454;&#29616;&#25913;&#36827;&#30340;&#38901;&#24459;&#24314;&#27169;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.19567</link><description>&lt;p&gt;
DC CoMix TTS&#65306;&#19968;&#31181;&#19982;&#28151;&#21512;&#22120;&#21327;&#20316;&#30340;&#31471;&#21040;&#31471;&#34920;&#29616;&#21147;TTS&#65292;&#21033;&#29992;&#31163;&#25955;&#30721;&#23454;&#29616;&#25913;&#36827;&#30340;&#38901;&#24459;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
DC CoMix TTS: An End-to-End Expressive TTS with Discrete Code Collaborated with Mixer. (arXiv:2305.19567v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19567
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#25955;&#30721;&#21644;&#28151;&#21512;&#22120;&#30456;&#21327;&#20316;&#30340;&#31471;&#21040;&#31471;&#34920;&#29616;&#21147;TTS&#65292;&#23427;&#37319;&#29992;&#26032;&#30340;&#36755;&#20837;&#34920;&#31034;&#21644;&#31616;&#21333;&#30340;&#26550;&#26500;&#26469;&#23454;&#29616;&#25913;&#36827;&#30340;&#38901;&#24459;&#24314;&#27169;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20013;&#24615;TTS&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#20869;&#23481;&#27844;&#28431;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36755;&#20837;&#34920;&#31034;&#21644;&#31616;&#21333;&#30340;&#26550;&#26500;&#26469;&#23454;&#29616;&#25913;&#36827;&#30340;&#38901;&#24459;&#24314;&#27169;&#12290;&#21463;&#26368;&#36817;&#22312;TTS&#20013;&#20351;&#29992;&#31163;&#25955;&#30721;&#21462;&#24471;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#31163;&#25955;&#30721;&#24341;&#20837;&#21040;&#21442;&#32771;&#32534;&#30721;&#22120;&#30340;&#36755;&#20837;&#20013;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#38899;&#39057;&#21387;&#32553;&#27169;&#22411;&#20013;&#30340;&#21521;&#37327;&#37327;&#21270;&#22120;&#26469;&#21033;&#29992;&#23427;&#24050;&#32463;&#35757;&#32451;&#36807;&#30340;&#22810;&#26679;&#21270;&#30340;&#22768;&#23398;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#20462;&#25913;&#21518;&#30340;MLP-Mixer&#24212;&#29992;&#21040;&#21442;&#32771;&#32534;&#30721;&#22120;&#20013;&#65292;&#20351;&#24471;&#26550;&#26500;&#26356;&#21152;&#36731;&#30408;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#35757;&#32451;&#38901;&#24459;&#36716;&#31227;TTS&#12290;&#25105;&#20204;&#36890;&#36807;&#20027;&#35266;&#21644;&#23458;&#35266;&#35780;&#20272;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#65292;&#24403;&#31163;&#25955;&#30721;&#20316;&#20026;&#36755;&#20837;&#26102;&#65292;&#21442;&#32771;&#32534;&#30721;&#22120;&#21487;&#20197;&#23398;&#20064;&#21040;&#26356;&#22909;&#30340;&#19982;&#35828;&#35805;&#20154;&#26080;&#20851;&#30340;&#38901;&#24459;&#12290;&#21478;&#22806;&#65292;&#21363;&#20351;&#36755;&#20837;&#21442;&#25968;&#26356;&#23569;&#65292;&#25105;&#20204;&#20063;&#21487;&#20197;&#33719;&#24471;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the huge successes made in neutral TTS, content-leakage remains a challenge. In this paper, we propose a new input representation and simple architecture to achieve improved prosody modeling. Inspired by the recent success in the use of discrete code in TTS, we introduce discrete code to the input of the reference encoder. Specifically, we leverage the vector quantizer from the audio compression model to exploit the diverse acoustic information it has already been trained on. In addition, we apply the modified MLP-Mixer to the reference encoder, making the architecture lighter. As a result, we train the prosody transfer TTS in an end-to-end manner. We prove the effectiveness of our method through both subjective and objective evaluations. We demonstrate that the reference encoder learns better speaker-independent prosody when discrete code is utilized as input in the experiments. In addition, we obtain comparable results even when fewer parameters are inputted.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#22768;&#23398;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#33258;&#21160;&#21457;&#38899;&#35780;&#20272;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#26631;&#27880;&#25968;&#25454;&#65292;&#20351;&#29992;&#25513;&#30721;&#27169;&#22359;&#30772;&#22351;&#35821;&#38899;&#36755;&#20837;&#65292;&#24212;&#29992;k&#22343;&#20540;&#32858;&#31867;&#33719;&#24471;&#26631;&#35760;&#24207;&#21015;&#24182;&#20351;&#29992;&#35780;&#20998;&#27169;&#22359;&#27979;&#37327;&#38169;&#35823;&#24674;&#22797;&#26631;&#35760;&#30340;&#25968;&#37327;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#19982;&#30417;&#30563;&#22238;&#24402;&#22522;&#32447;&#30456;&#24403;&#30340;&#24615;&#33021;&#20197;&#21450;&#20248;&#20110;&#38750;&#22238;&#24402;&#22522;&#32447;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.19563</link><description>&lt;p&gt;
&#26080;&#38656;&#26631;&#27880;&#25968;&#25454;&#30340;&#38646;&#26679;&#26412;&#33258;&#21160;&#21457;&#38899;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Automatic Pronunciation Assessment. (arXiv:2305.19563v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#22768;&#23398;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#33258;&#21160;&#21457;&#38899;&#35780;&#20272;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#26631;&#27880;&#25968;&#25454;&#65292;&#20351;&#29992;&#25513;&#30721;&#27169;&#22359;&#30772;&#22351;&#35821;&#38899;&#36755;&#20837;&#65292;&#24212;&#29992;k&#22343;&#20540;&#32858;&#31867;&#33719;&#24471;&#26631;&#35760;&#24207;&#21015;&#24182;&#20351;&#29992;&#35780;&#20998;&#27169;&#22359;&#27979;&#37327;&#38169;&#35823;&#24674;&#22797;&#26631;&#35760;&#30340;&#25968;&#37327;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#19982;&#30417;&#30563;&#22238;&#24402;&#22522;&#32447;&#30456;&#24403;&#30340;&#24615;&#33021;&#20197;&#21450;&#20248;&#20110;&#38750;&#22238;&#24402;&#22522;&#32447;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21457;&#38899;&#35780;&#20272;&#23545;&#20110;&#35745;&#31639;&#26426;&#36741;&#21161;&#35821;&#35328;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#26041;&#27861;&#20381;&#36182;&#20110;&#24102;&#27880;&#37322;&#30340;&#35821;&#38899;&#25991;&#26412;&#25968;&#25454;&#26469;&#35757;&#32451;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#25110;&#20381;&#36182;&#20110;&#24102;&#20998;&#25968;&#30340;&#35821;&#38899;&#25968;&#25454;&#26469;&#35757;&#32451;&#22238;&#24402;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#22768;&#23398;&#27169;&#22411;HuBERT&#30340;&#20840;&#26032;&#38646;&#26679;&#26412;&#33258;&#21160;&#21457;&#38899;&#35780;&#20272;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#23545;&#35821;&#38899;&#36755;&#20837;&#36827;&#34892;&#32534;&#30721;&#24182;&#36890;&#36807;&#25513;&#30721;&#27169;&#22359;&#36827;&#34892;&#30772;&#22351;&#12290;&#28982;&#21518;&#20351;&#29992;Transformer&#32534;&#30721;&#22120;&#24182;&#24212;&#29992;k&#22343;&#20540;&#32858;&#31867;&#20197;&#33719;&#24471;&#26631;&#35760;&#24207;&#21015;&#12290;&#26368;&#21518;&#65292;&#35774;&#35745;&#20102;&#35780;&#20998;&#27169;&#22359;&#26469;&#27979;&#37327;&#38169;&#35823;&#24674;&#22797;&#26631;&#35760;&#30340;&#25968;&#37327;&#12290;&#22312;speechocean762&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#30382;&#23572;&#36874;&#30456;&#20851;&#31995;&#25968;&#65288;PCC&#65289;&#26041;&#38754;&#19982;&#30417;&#30563;&#22238;&#24402;&#22522;&#32447;&#26041;&#27861;&#20855;&#26377;&#21487;&#27604;&#24615;&#65292;&#24182;&#19988;&#22312;&#38750;&#22238;&#24402;&#22522;&#32447;&#26041;&#27861;&#26041;&#38754;&#34920;&#29616;&#26356;&#20248;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#25513;&#30721;&#31574;&#30053;&#23545;&#33258;&#21160;&#21457;&#38899;&#35780;&#20272;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic Pronunciation Assessment (APA) is vital for computer-assisted language learning. Prior methods rely on annotated speech-text data to train Automatic Speech Recognition (ASR) models or speech-score data to train regression models. In this work, we propose a novel zero-shot APA method based on the pre-trained acoustic model, HuBERT. Our method involves encoding speech input and corrupting them via a masking module. We then employ the Transformer encoder and apply k-means clustering to obtain token sequences. Finally, a scoring module is designed to measure the number of wrongly recovered tokens. Experimental results on speechocean762 demonstrate that the proposed method achieves comparable performance to supervised regression baselines and outperforms non-regression baselines in terms of Pearson Correlation Coefficient (PCC). Additionally, we analyze how masking strategies affect the performance of APA.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#36825;&#26041;&#38754;&#30340;&#34920;&#29616;&#21313;&#20998;&#26377;&#38480;&#65292;&#25581;&#31034;&#20102;&#20854;&#22312;&#25512;&#29702;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.19555</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19981;&#33021;&#20316;&#20026;&#25277;&#35937;&#25512;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Are Not Abstract Reasoners. (arXiv:2305.19555v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#36825;&#26041;&#38754;&#30340;&#34920;&#29616;&#21313;&#20998;&#26377;&#38480;&#65292;&#25581;&#31034;&#20102;&#20854;&#22312;&#25512;&#29702;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26497;&#22909;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#25991;&#26412;&#29702;&#35299;&#21644;&#24120;&#35782;&#25512;&#29702;&#31561;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25104;&#21151;&#30340;&#26426;&#21046;&#23578;&#19981;&#28165;&#26970;&#65292;LLMs&#26159;&#21542;&#33021;&#22815;&#36798;&#21040;&#20154;&#31867;&#30340;&#35748;&#30693;&#33021;&#21147;&#25110;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#36824;&#23384;&#22312;&#26681;&#26412;&#24615;&#30340;&#23616;&#38480;&#24615;&#20063;&#19981;&#30830;&#23450;&#12290;&#25277;&#35937;&#25512;&#29702;&#26159;&#35748;&#30693;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#21253;&#25324;&#20174;&#23569;&#37327;&#25968;&#25454;&#20013;&#25214;&#21040;&#21644;&#24212;&#29992;&#19968;&#33324;&#27169;&#24335;&#12290;&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32467;&#26500;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#21487;&#20197;&#25581;&#31034;&#23427;&#20204;&#22312;&#25512;&#29702;&#26041;&#38754;&#30340;&#28508;&#22312;&#23616;&#38480;&#24615;&#21644;&#24191;&#27867;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36825;&#26159;&#19968;&#20010;&#30446;&#21069;&#26410;&#34987;&#25506;&#32034;&#30340;&#39046;&#22495;&#12290;&#26412;&#25991;&#23545;&#26368;&#20808;&#36827;&#30340;LLMs&#36827;&#34892;&#20102;&#22823;&#37327;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#38750;&#24120;&#26377;&#38480;&#65292;&#24182;&#25506;&#31350;&#20102;&#36896;&#25104;&#36825;&#31181;&#24046;&#24322;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models have shown tremendous performance on a large variety of natural language processing tasks, ranging from text comprehension to common sense reasoning. However, the mechanisms responsible for this success remain unknown, and it is unclear whether LLMs can achieve human-like cognitive capabilities or whether these models are still fundamentally limited. Abstract reasoning is a fundamental task for cognition, consisting of finding and applying a general pattern from few data. Evaluating deep neural architectures on this task could give insight into their potential limitations regarding reasoning and their broad generalisation abilities, yet this is currently an under-explored area. In this paper, we perform extensive evaluations of state-of-the-art LLMs on abstract reasoning tasks, showing that they achieve very limited performance in contrast with other natural language tasks, and we investigate the reasons for this difference. We apply techniques that have been show
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#21387;&#32553;&#31574;&#30053;&#65292;&#32467;&#21512;&#32467;&#26500;&#21270;&#21098;&#26525;&#21644;&#30693;&#35782;&#33976;&#39311;&#65292;&#20197;&#20943;&#23569;Conformer&#27169;&#22411;&#30340;&#27169;&#22411;&#22823;&#23567;&#21644;&#25512;&#29702;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#36739;&#39640;&#30340;&#35782;&#21035;&#24615;&#33021;&#65292;&#24182;&#22312;LibriSpeech&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#25152;&#26377;&#21098;&#26525;&#22522;&#20934;&#32447;&#12290;</title><link>http://arxiv.org/abs/2305.19549</link><description>&lt;p&gt;
&#20934;&#30830;&#21644;&#32467;&#26500;&#21270;&#21098;&#26525;&#29992;&#20110;&#39640;&#25928;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Accurate and Structured Pruning for Efficient Automatic Speech Recognition. (arXiv:2305.19549v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#21387;&#32553;&#31574;&#30053;&#65292;&#32467;&#21512;&#32467;&#26500;&#21270;&#21098;&#26525;&#21644;&#30693;&#35782;&#33976;&#39311;&#65292;&#20197;&#20943;&#23569;Conformer&#27169;&#22411;&#30340;&#27169;&#22411;&#22823;&#23567;&#21644;&#25512;&#29702;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#36739;&#39640;&#30340;&#35782;&#21035;&#24615;&#33021;&#65292;&#24182;&#22312;LibriSpeech&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#25152;&#26377;&#21098;&#26525;&#22522;&#20934;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#22914;Transformer&#21644;Conformer&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#20855;&#26377;&#36739;&#22823;&#30340;&#27169;&#22411;&#22823;&#23567;&#21644;&#39640;&#26114;&#30340;&#25512;&#29702;&#25104;&#26412;&#65292;&#36825;&#23545;&#20110;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#35774;&#22791;&#19978;&#37096;&#32626;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#32467;&#26500;&#21270;&#21098;&#26525;&#21644;&#30693;&#35782;&#33976;&#39311;&#26469;&#20943;&#23569;Conformer&#27169;&#22411;&#30340;&#27169;&#22411;&#22823;&#23567;&#21644;&#25512;&#29702;&#25104;&#26412;&#30340;&#26032;&#22411;&#21387;&#32553;&#31574;&#30053;&#65292;&#21516;&#26102;&#20445;&#25345;&#36739;&#39640;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#19968;&#32452;&#20108;&#36827;&#21046;&#25513;&#30721;&#26469;&#25351;&#31034;&#26159;&#21542;&#20445;&#30041;&#25110;&#20462;&#21098;&#27599;&#20010;Conformer&#27169;&#22359;&#65292;&#24182;&#37319;&#29992;L0&#27491;&#21017;&#21270;&#26469;&#23398;&#20064;&#26368;&#20339;&#25513;&#30721;&#20540;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;&#21098;&#26525;&#24615;&#33021;&#65292;&#25105;&#20204;&#20351;&#29992;&#36880;&#23618;&#33976;&#39311;&#31574;&#30053;&#23558;&#30693;&#35782;&#20174;&#26410;&#20462;&#21098;&#36807;&#30340;&#27169;&#22411;&#20256;&#36755;&#21040;&#20462;&#21098;&#36807;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;LibriSpeech&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#25152;&#26377;&#21098;&#26525;&#22522;&#20934;&#32447;&#65292;&#22312;&#26368;&#23567;&#24615;&#33021;&#25439;&#22833;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;50&#65285;&#30340;&#27169;&#22411;&#22823;&#23567;&#20943;&#23569;&#21644;28&#65285;&#30340;&#25512;&#29702;&#25104;&#26412;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic Speech Recognition (ASR) has seen remarkable advancements with deep neural networks, such as Transformer and Conformer. However, these models typically have large model sizes and high inference costs, posing a challenge to deploy on resource-limited devices. In this paper, we propose a novel compression strategy that leverages structured pruning and knowledge distillation to reduce the model size and inference cost of the Conformer model while preserving high recognition performance. Our approach utilizes a set of binary masks to indicate whether to retain or prune each Conformer module, and employs L0 regularization to learn the optimal mask values. To further enhance pruning performance, we use a layerwise distillation strategy to transfer knowledge from unpruned to pruned models. Our method outperforms all pruning baselines on the widely used LibriSpeech benchmark, achieving a 50% reduction in model size and a 28% reduction in inference cost with minimal performance loss.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;&#26041;&#27861;&#65292;&#22312;&#19981;&#20381;&#36182;&#22806;&#37096;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#27604;&#20043;&#21069;&#21033;&#29992;&#39044;&#35757;&#32451;&#26435;&#37325;&#12289;&#23884;&#20837;&#21644;&#22806;&#37096;&#35821;&#27861;&#20998;&#26512;&#22120;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#34920;&#26126;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2305.19512</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Fine-grained Text Style Transfer with Diffusion-Based Language Models. (arXiv:2305.19512v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;&#26041;&#27861;&#65292;&#22312;&#19981;&#20381;&#36182;&#22806;&#37096;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#27604;&#20043;&#21069;&#21033;&#29992;&#39044;&#35757;&#32451;&#26435;&#37325;&#12289;&#23884;&#20837;&#21644;&#22806;&#37096;&#35821;&#27861;&#20998;&#26512;&#22120;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#34920;&#26126;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#24335;&#27010;&#29575;&#27169;&#22411;&#24050;&#32463;&#22312;&#21487;&#25511;&#21046;&#22320;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#19978;&#26174;&#31034;&#20986;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#35797;&#22270;&#23558;&#36825;&#31181;&#21487;&#25511;&#24615;&#36816;&#29992;&#21040;&#25991;&#26412;&#29983;&#25104;&#39046;&#22495;&#12290;&#20197;&#21069;&#30340;&#25193;&#25955;&#24335;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#34920;&#26126;&#65292;&#23427;&#20204;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#22806;&#37096;&#30693;&#35782;&#65288;&#22914;&#39044;&#35757;&#32451;&#26435;&#37325;&#65289;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#20173;&#28982;&#21487;&#20197;&#23454;&#29616;&#31283;&#23450;&#30340;&#24615;&#33021;&#21644;&#21487;&#25511;&#24615;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;StylePTB&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#19968;&#20010;&#25193;&#25955;&#24335;&#27169;&#22411;&#65292;&#36825;&#26159;&#32454;&#31890;&#24230;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;&#30340;&#26631;&#20934;&#22522;&#20934;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#35780;&#20272;&#20219;&#21153;&#30456;&#27604;&#65292;StylePTB&#20013;&#30340;&#20219;&#21153;&#38656;&#35201;&#23545;&#36755;&#20986;&#25991;&#26412;&#36827;&#34892;&#26356;&#21152;&#31934;&#32454;&#30340;&#25511;&#21046;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#22312;StylePTB&#19978;&#23454;&#29616;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#20010;&#21035;&#21644;&#32452;&#21512;&#36716;&#25442;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#27809;&#26377;&#22806;&#37096;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;StylePTB&#30340;&#26377;&#38480;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#34920;&#29616;&#20248;&#20110;&#20197;&#21069;&#21033;&#29992;&#39044;&#35757;&#32451;&#26435;&#37325;&#12289;&#23884;&#20837;&#21644;&#22806;&#37096;&#35821;&#27861;&#20998;&#26512;&#22120;&#30340;&#24037;&#20316;&#65292;&#36825;&#21487;&#33021;&#34920;&#26126;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#39046;&#22495;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion probabilistic models have shown great success in generating high-quality images controllably, and researchers have tried to utilize this controllability into text generation domain. Previous works on diffusion-based language models have shown that they can be trained without external knowledge (such as pre-trained weights) and still achieve stable performance and controllability. In this paper, we trained a diffusion-based model on StylePTB dataset, the standard benchmark for fine-grained text style transfers. The tasks in StylePTB requires much more refined control over the output text compared to tasks evaluated in previous works, and our model was able to achieve state-of-the-art performance on StylePTB on both individual and compositional transfers. Moreover, our model, trained on limited data from StylePTB without external knowledge, outperforms previous works that utilized pretrained weights, embeddings, and external grammar parsers, and this may indicate that diffusion
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#23454;&#20363;&#32423;&#25552;&#31034;&#21450;&#20854;&#26222;&#36866;&#24615;&#30340;&#25506;&#35752;&#65292;&#21457;&#29616;&#20102;&#19968;&#20123;&#24378;&#22823;&#19988;&#20855;&#22791;&#21487;&#21306;&#20998;&#35821;&#35328;&#29305;&#24449;&#30340;&#25277;&#22870;&#25552;&#31034;&#65292;&#36890;&#36807;&#25552;&#31034;&#38598;&#25104;&#26041;&#27861;&#21487;&#23558;&#20854;&#25512;&#24191;&#21040;&#26410;&#35265;&#25968;&#25454;&#38598;&#19978;&#65292;&#20026;&#26367;&#20195;&#20256;&#32479;&#24494;&#35843;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#26377;&#25928;&#30340;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2305.19500</link><description>&lt;p&gt;
&#25506;&#32034;&#29992;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25277;&#22870;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Exploring Lottery Prompts for Pre-trained Language Models. (arXiv:2305.19500v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#23454;&#20363;&#32423;&#25552;&#31034;&#21450;&#20854;&#26222;&#36866;&#24615;&#30340;&#25506;&#35752;&#65292;&#21457;&#29616;&#20102;&#19968;&#20123;&#24378;&#22823;&#19988;&#20855;&#22791;&#21487;&#21306;&#20998;&#35821;&#35328;&#29305;&#24449;&#30340;&#25277;&#22870;&#25552;&#31034;&#65292;&#36890;&#36807;&#25552;&#31034;&#38598;&#25104;&#26041;&#27861;&#21487;&#23558;&#20854;&#25512;&#24191;&#21040;&#26410;&#35265;&#25968;&#25454;&#38598;&#19978;&#65292;&#20026;&#26367;&#20195;&#20256;&#32479;&#24494;&#35843;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#26377;&#25928;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#30452;&#20197;&#26469;&#65292;&#23545;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#21487;&#25345;&#32493;&#25193;&#23637;&#65292;&#22312;&#27169;&#22411;&#36866;&#24212;&#24615;&#26041;&#38754;&#23384;&#22312;&#37325;&#22823;&#36127;&#25285;&#65292;&#38656;&#35201;&#26356;&#26377;&#25928;&#30340;&#26367;&#20195;&#20256;&#32479;&#30340;&#24494;&#35843;&#26041;&#27861;&#12290; &#37492;&#20110;&#22312;&#38646;-shot&#29615;&#22659;&#20013;&#25552;&#31034;&#30340;&#20248;&#21183;&#21644;&#35266;&#23519;&#21040;&#19981;&#21516;&#25552;&#31034;&#20043;&#38388;&#30340;&#24615;&#33021;&#27874;&#21160;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#23454;&#20363;&#32423;&#25552;&#31034;&#21450;&#20854;&#26222;&#36866;&#24615;&#12290; &#36890;&#36807;&#22312;&#25552;&#31034;&#31354;&#38388;&#20013;&#36827;&#34892;&#25628;&#32034;&#65292;&#25105;&#20204;&#39318;&#20808;&#39564;&#35777;&#20102;&#36825;&#26679;&#19968;&#31181;&#20551;&#35774;&#65306;&#23545;&#20110;&#27599;&#20010;&#23454;&#20363;&#65292;&#20960;&#20046;&#24635;&#26159;&#23384;&#22312;&#19968;&#31181;&#25277;&#22870;&#25552;&#31034;&#65292;&#21487;&#20197;&#20197;&#20302;&#25104;&#26412;&#33719;&#24471;PLM&#30340;&#27491;&#30830;&#39044;&#27979;&#33021;&#21147;&#12290; &#21516;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#19968;&#20123;&#24378;&#22823;&#30340;&#25277;&#22870;&#25552;&#31034;&#22312;&#25972;&#20010;&#35757;&#32451;&#38598;&#19978;&#20855;&#26377;&#39640;&#24615;&#33021;&#65292;&#24182;&#20855;&#22791;&#21487;&#21306;&#20998;&#30340;&#35821;&#35328;&#29305;&#24449;&#12290; &#26368;&#21518;&#65292;&#25105;&#20204;&#35797;&#22270;&#20351;&#29992;&#25552;&#31034;&#38598;&#25104;&#26041;&#27861;&#23558;&#25628;&#32034;&#21040;&#30340;&#24378;&#22823;&#25277;&#22870;&#25552;&#31034;&#25512;&#24191;&#21040;&#26410;&#35265;&#25968;&#25454;&#32780;&#26080;&#38656;&#20219;&#20309;&#21442;&#25968;&#35843;&#25972;&#12290; &#22312;&#21508;&#31181;&#31867;&#22411;&#30340;NLP c&#19978;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consistently scaling pre-trained language models (PLMs) imposes substantial burdens on model adaptation, necessitating more efficient alternatives to conventional fine-tuning. Given the advantage of prompting in the zero-shot setting and the observed performance fluctuation among different prompts, we explore the instance-level prompt and their generalizability. By searching through the prompt space, we first validate the assumption that for every instance, there is almost always a lottery prompt that induces the correct prediction from the PLM, and such prompt can be obtained at a low cost thanks to the inherent ability of PLMs. Meanwhile, we find that some strong lottery prompts have high performance over the whole training set, and they are equipped with distinguishable linguistic features. Lastly, we attempt to generalize the searched strong lottery prompts to unseen data with prompt ensembling method without any parameter tuning. Experiments are conducted on various types of NLP c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#24320;&#25918;&#39046;&#22495;&#25805;&#20316;&#25991;&#26412;&#30340;&#27969;&#31243;&#22270;&#39044;&#27979;&#26694;&#26550;&#65292;&#20351;&#29992;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#27604;&#21333;&#29420;&#22312;&#28921;&#39274;&#25110;&#30446;&#26631;&#39046;&#22495;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.19497</link><description>&lt;p&gt;
&#38754;&#21521;&#24320;&#25918;&#39046;&#22495;&#25805;&#20316;&#25991;&#26412;&#30340;&#27969;&#31243;&#22270;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Towards Flow Graph Prediction of Open-Domain Procedural Texts. (arXiv:2305.19497v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#24320;&#25918;&#39046;&#22495;&#25805;&#20316;&#25991;&#26412;&#30340;&#27969;&#31243;&#22270;&#39044;&#27979;&#26694;&#26550;&#65292;&#20351;&#29992;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#27604;&#21333;&#29420;&#22312;&#28921;&#39274;&#25110;&#30446;&#26631;&#39046;&#22495;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25805;&#20316;&#25991;&#26412;&#30340;&#26426;&#22120;&#29702;&#35299;&#23545;&#20110;&#25512;&#29702;&#27493;&#39588;&#21644;&#33258;&#21160;&#21270;&#25805;&#20316;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36825;&#35201;&#27714;&#22312;&#25991;&#26412;&#20013;&#35782;&#21035;&#23454;&#20307;&#24182;&#35299;&#20915;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#28921;&#39274;&#39046;&#22495;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#33756;&#35889;&#25991;&#26412;&#36716;&#25442;&#20026;&#27969;&#31243;&#22270;(FG)&#34920;&#31034;&#30340;&#26694;&#26550;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33756;&#35889;FG&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;&#24320;&#25918;&#39046;&#22495;&#25805;&#20316;&#25991;&#26412;&#30340;&#27969;&#31243;&#22270;&#12290;&#20026;&#20102;&#30740;&#31350;&#38750;&#28921;&#39274;&#39046;&#22495;&#20013;&#30340;&#27969;&#31243;&#22270;&#39044;&#27979;&#24615;&#33021;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#26469;&#33258;how-to&#25351;&#21335;&#32593;&#31449;wikiHow&#19978;&#25991;&#31456;&#30340;wikiHow-FG&#35821;&#26009;&#24211;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#29616;&#26377;&#30340;&#33756;&#35889;&#35821;&#26009;&#24211;&#65292;&#24182;&#20174;&#28921;&#39274;&#39046;&#22495;&#36827;&#34892;&#22495;&#33258;&#36866;&#24212;&#21040;&#30446;&#26631;&#39046;&#22495;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22495;&#33258;&#36866;&#24212;&#27169;&#22411;&#27604;&#20165;&#22312;&#28921;&#39274;&#25110;&#30446;&#26631;&#39046;&#22495;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine comprehension of procedural texts is essential for reasoning about the steps and automating the procedures. However, this requires identifying entities within a text and resolving the relationships between the entities. Previous work focused on the cooking domain and proposed a framework to convert a recipe text into a flow graph (FG) representation. In this work, we propose a framework based on the recipe FG for flow graph prediction of open-domain procedural texts. To investigate flow graph prediction performance in non-cooking domains, we introduce the wikiHow-FG corpus from articles on wikiHow, a website of how-to instruction articles. In experiments, we consider using the existing recipe corpus and performing domain adaptation from the cooking to the target domain. Experimental results show that the domain adaptation models achieve higher performance than those trained only on the cooking or target domain data.
&lt;/p&gt;</description></item><item><title>PlaSma&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36807;&#31243;&#30693;&#35782;&#21644;&#35745;&#21010;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#65292;</title><link>http://arxiv.org/abs/2305.19472</link><description>&lt;p&gt;
PlaSma: &#20026; (&#21453;&#20107;&#23454;) &#35745;&#21010;&#21046;&#23450;&#22686;&#24378;&#36807;&#31243;&#30693;&#35782;&#27169;&#22411;&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PlaSma: Making Small Language Models Better Procedural Knowledge Models for (Counterfactual) Planning. (arXiv:2305.19472v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19472
&lt;/p&gt;
&lt;p&gt;
PlaSma&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36807;&#31243;&#30693;&#35782;&#21644;&#35745;&#21010;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#65292;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#31243;&#35268;&#21010;&#26159;&#26426;&#22120;&#30340;&#19968;&#39033;&#37325;&#35201;&#32780;&#21448;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#23427;&#23558;&#19968;&#20010;&#39640;&#32423;&#30446;&#26631;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#26102;&#38388;&#39034;&#24207;&#30340;&#27493;&#39588;&#12290;&#23427;&#38656;&#35201;&#25972;&#21512;&#24120;&#35782;&#30693;&#35782;&#20197;&#25512;&#29702;&#20986;&#24120;&#24120;&#26159;&#21453;&#20107;&#23454;&#30340;&#22797;&#26434;&#24773;&#22659;&#65292;&#20363;&#22914; "&#27809;&#26377;&#30005;&#35805;&#26102;&#23433;&#25490;&#21307;&#29983;&#30340;&#32422;&#20250;"&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#20294;&#21463;&#21040;&#26114;&#36149;&#30340; API &#35843;&#29992;&#21644;&#21487;&#22797;&#29616;&#24615;&#38382;&#39064;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#26356;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#36827;&#34892;&#35268;&#21010;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; PlaSma&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#21452;&#37325;&#26041;&#27861;&#65292;&#20351;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#36807;&#31243;&#30693;&#35782;&#21644; (&#21453;&#20107;&#23454;) &#35745;&#21010;&#33021;&#21147;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#31526;&#21495;&#36807;&#31243;&#30693;&#35782;&#33976;&#39311;&#26469;&#22686;&#24378;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38544;&#21547;&#30693;&#35782;&#65292;&#20197;&#21450;&#19968;&#31181;&#25512;&#29702;&#31639;&#27861;&#26469;&#20419;&#36827;&#26356;&#32467;&#26500;&#21270;&#21644;&#20934;&#30830;&#30340;&#25512;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65292;&#21453;&#20107;&#23454;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Procedural planning, which entails decomposing a high-level goal into a sequence of temporally ordered steps, is an important yet intricate task for machines. It involves integrating common-sense knowledge to reason about complex contextualized situations that are often counterfactual, e.g. "scheduling a doctor's appointment without a phone". While current approaches show encouraging results using large language models (LLMs), they are hindered by drawbacks such as costly API calls and reproducibility issues. In this paper, we advocate planning using smaller language models. We present PlaSma, a novel two-pronged approach to endow small language models with procedural knowledge and (counterfactual) planning capabilities. More concretely, we develop symbolic procedural knowledge distillation to enhance the implicit knowledge in small language models and an inference-time algorithm to facilitate more structured and accurate reasoning. In addition, we introduce a novel task, Counterfactua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350; Transformer &#27169;&#22411;&#20013;&#20301;&#32622;&#32534;&#30721;&#23545;&#20110;&#38271;&#24230;&#25512;&#24191;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#24120;&#29992;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#24182;&#19981;&#36866;&#21512;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#38271;&#24230;&#25512;&#24191;&#65292;&#24182;&#19988;&#20351;&#29992;&#20301;&#32622;&#32534;&#30721;&#29978;&#33267;&#21487;&#33021;&#20250;&#25439;&#23475;&#38271;&#24230;&#25512;&#24191;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.19466</link><description>&lt;p&gt;
&#20301;&#32622;&#32534;&#30721;&#23545; Transformer &#27169;&#22411;&#38271;&#24230;&#25512;&#24191;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Impact of Positional Encoding on Length Generalization in Transformers. (arXiv:2305.19466v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350; Transformer &#27169;&#22411;&#20013;&#20301;&#32622;&#32534;&#30721;&#23545;&#20110;&#38271;&#24230;&#25512;&#24191;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#24120;&#29992;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#24182;&#19981;&#36866;&#21512;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#38271;&#24230;&#25512;&#24191;&#65292;&#24182;&#19988;&#20351;&#29992;&#20301;&#32622;&#32534;&#30721;&#29978;&#33267;&#21487;&#33021;&#20250;&#25439;&#23475;&#38271;&#24230;&#25512;&#24191;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer-based &#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#21457;&#20013;&#65292;&#38271;&#24230;&#25512;&#24191;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#25361;&#25112;&#65292;&#23427;&#26159;&#25351;&#20174;&#23567;&#30340;&#35757;&#32451;&#25991;&#26412;&#33539;&#22260;&#21040;&#26356;&#22823;&#33539;&#22260;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20301;&#32622;&#32534;&#30721;&#65288;PE&#65289;&#34987;&#21457;&#29616;&#26159;&#24433;&#21709;&#38271;&#24230;&#25512;&#24191;&#30340;&#20027;&#35201;&#22240;&#32032;&#20043;&#19968;&#65292;&#20294;&#19981;&#21516;&#30340; PE &#26041;&#26696;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#22806;&#25512;&#24433;&#21709;&#36824;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#27604;&#35780;&#20272;&#20116;&#31181;&#19981;&#21516;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#65288;&#21253;&#25324;&#32477;&#23545;&#20301;&#32622;&#23884;&#20837;&#12289;T5 &#30340;&#30456;&#23545; PE&#12289;ALiBi&#12289;Rotary &#21644;&#26080;&#20301;&#32622;&#32534;&#30721;&#65289;&#30340;&#35299;&#30721;&#22120; Transformer &#30340;&#38271;&#24230;&#25512;&#24191;&#33021;&#21147;&#65292;&#23545;&#25512;&#29702;&#21644;&#25968;&#23398;&#20219;&#21153;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24120;&#29992;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#65292;&#22914; ALiBi&#12289;Rotary &#21644; APE&#65292;&#24182;&#19981;&#36866;&#21512;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#38271;&#24230;&#25512;&#24191;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#26080; PE &#30340; Transformer &#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26174;&#24335; PE &#26041;&#27861;&#65292;&#36825;&#24847;&#21619;&#30528;&#20351;&#29992;&#20301;&#32622;&#32534;&#30721;&#23454;&#38469;&#19978;&#21487;&#33021;&#20250;&#25439;&#23475;&#38271;&#24230;&#25512;&#24191;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;&#20301;&#32622;&#32534;&#30721;&#22312;&#26377;&#25928; Transformer &#27169;&#22411;&#24320;&#21457;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Length generalization, the ability to generalize from small training context sizes to larger ones, is a critical challenge in the development of Transformer-based language models. Positional encoding (PE) has been identified as a major factor influencing length generalization, but the exact impact of different PE schemes on extrapolation in downstream tasks remains unclear. In this paper, we conduct a systematic empirical study comparing the length generalization performance of decoder-only Transformers with five different position encoding approaches including Absolute Position Embedding (APE), T5's Relative PE, ALiBi, and Rotary, in addition to Transformers without positional encoding (NoPE). Our evaluation encompasses a battery of reasoning and mathematical tasks. Our findings reveal that the most commonly used positional encoding methods, such as ALiBi, Rotary, and APE, are not well suited for length generalization in downstream tasks. More importantly, NoPE outperforms other expli
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Scoped Negation NLI (ScoNe-NLI)&#22522;&#20934;&#27979;&#35797;&#20197;&#35780;&#20272;&#24494;&#35843;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31574;&#30053;&#23545;&#35821;&#35328;&#27169;&#22411;&#20013;&#21542;&#23450;&#25512;&#29702;&#34920;&#29616;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36827;&#34892;&#35768;&#22810;&#27425;&#24494;&#35843;&#21518;&#65292;RoBERTa&#21644;DeBERTa&#27169;&#22411;&#21487;&#20197;&#25104;&#21151;&#35299;&#20915;ScoNe-NLI&#12290;&#23545;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#65292;&#22823;&#22810;&#25968;&#25552;&#31034;&#31574;&#30053;&#37117;&#26080;&#27861;&#25104;&#21151;&#65292;&#20294;&#22312;&#23884;&#20837;&#21542;&#23450;&#25512;&#29702;&#30340;&#30701;&#25925;&#20107;&#30340;&#21477;&#23376;&#23436;&#25104;&#27979;&#35797;&#20013;&#65292;InstructGPT&#26159;&#25104;&#21151;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.19426</link><description>&lt;p&gt;
ScoNe: &#29992;&#24494;&#35843;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21542;&#23450;&#25512;&#29702;&#34920;&#29616;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
ScoNe: Benchmarking Negation Reasoning in Language Models With Fine-Tuning and In-Context Learning. (arXiv:2305.19426v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Scoped Negation NLI (ScoNe-NLI)&#22522;&#20934;&#27979;&#35797;&#20197;&#35780;&#20272;&#24494;&#35843;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31574;&#30053;&#23545;&#35821;&#35328;&#27169;&#22411;&#20013;&#21542;&#23450;&#25512;&#29702;&#34920;&#29616;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36827;&#34892;&#35768;&#22810;&#27425;&#24494;&#35843;&#21518;&#65292;RoBERTa&#21644;DeBERTa&#27169;&#22411;&#21487;&#20197;&#25104;&#21151;&#35299;&#20915;ScoNe-NLI&#12290;&#23545;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#65292;&#22823;&#22810;&#25968;&#25552;&#31034;&#31574;&#30053;&#37117;&#26080;&#27861;&#25104;&#21151;&#65292;&#20294;&#22312;&#23884;&#20837;&#21542;&#23450;&#25512;&#29702;&#30340;&#30701;&#25925;&#20107;&#30340;&#21477;&#23376;&#23436;&#25104;&#27979;&#35797;&#20013;&#65292;InstructGPT&#26159;&#25104;&#21151;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#19968;&#20123;&#22522;&#20934;&#27979;&#35797;&#35797;&#22270;&#35780;&#20272;&#27169;&#22411;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#21542;&#23450;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#32570;&#20047;&#21463;&#25511;&#30340;&#31034;&#20363;&#33539;&#20363;&#65292;&#26080;&#27861;&#25512;&#26029;&#27169;&#22411;&#26159;&#21542;&#24050;&#32463;&#23398;&#20250;&#20102;&#21542;&#23450;&#35821;&#32032;&#30340;&#35821;&#20041;&#20316;&#29992;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#20998;&#26512;&#19978;&#30340;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Scoped Negation NLI (ScoNe-NLI)&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#21253;&#21547;&#20845;&#20010;&#23545;&#27604;&#31034;&#20363;&#32452;&#25104;&#30340;&#38598;&#21512;&#65292;&#20854;&#20013;&#21253;&#25324;&#22810;&#36798;&#20004;&#20010;&#21542;&#23450;&#35821;&#32032;&#65292;&#20854;&#20013;&#38646;&#20010;&#12289;&#19968;&#20010;&#25110;&#20004;&#20010;&#21542;&#23450;&#35821;&#32032;&#24433;&#21709;NLI&#26631;&#31614;&#12290;&#25105;&#20204;&#20351;&#29992;ScoNe-NLI&#35780;&#20272;&#24494;&#35843;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31574;&#30053;&#12290;&#25105;&#20204;&#21457;&#29616;RoBERTa&#21644;DeBERTa&#27169;&#22411;&#22312;&#36827;&#34892;&#35768;&#22810;&#27425;&#24494;&#35843;&#20043;&#21518;&#21487;&#20197;&#35299;&#20915;ScoNe-NLI&#12290;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;InstructGPT&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#22823;&#22810;&#25968;&#25552;&#31034;&#31574;&#30053;&#37117;&#19981;&#25104;&#21151;&#65292;&#21253;&#25324;&#37027;&#20123;&#20351;&#29992;&#36880;&#27493;&#25512;&#29702;&#30340;&#31574;&#30053;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#20010;&#32467;&#26524;&#65292;&#25105;&#20204;&#23558;ScoNe&#25193;&#23637;&#20026;ScoNe-NLG&#65292;&#36825;&#26159;&#19968;&#20010;&#23884;&#20837;&#20102;&#21542;&#23450;&#25512;&#29702;&#30340;&#30701;&#25925;&#20107;&#30340;&#21477;&#23376;&#23436;&#25104;&#27979;&#35797;&#38598;&#12290;&#22312;&#36825;&#37324;&#65292;InstructGPT&#26159;&#25104;&#21151;&#30340;&#65292;&#25581;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
A number of recent benchmarks seek to assess how well models handle natural language negation. However, these benchmarks lack the controlled example paradigms that would allow us to infer whether a model had learned how negation morphemes semantically scope. To fill these analytical gaps, we present the Scoped Negation NLI (ScoNe-NLI) benchmark, which contains contrast sets of six examples with up to two negations where either zero, one, or both negative morphemes affect the NLI label. We use ScoNe-NLI to assess fine-tuning and in-context learning strategies. We find that RoBERTa and DeBERTa models solve ScoNe-NLI after many shot fine-tuning. For in-context learning, we test InstructGPT models and find that most prompt strategies are not successful, including those using step-by-step reasoning. To better understand this result, we extend ScoNe with ScoNe-NLG, a sentence completion test set that embeds negation reasoning in short narratives. Here, InstructGPT is successful, which reveal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#21516;&#26102;&#23545;&#25152;&#26377;&#27573;&#33853;&#36827;&#34892;&#23459;&#20256;&#25216;&#26415;&#20998;&#31867;&#30340;&#22810;&#23454;&#20363;&#22810;&#26631;&#31614;&#65288;MIML&#65289;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#20026;&#27169;&#22411;&#24341;&#20837;&#20102;&#23618;&#27425;&#24615;&#26631;&#31614;&#20381;&#36182;&#20851;&#31995;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;&#26041;&#27861;&#37117;&#24573;&#30053;&#20102;&#36825;&#19968;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.19419</link><description>&lt;p&gt;
&#38024;&#23545;&#26816;&#27979;&#23459;&#20256;&#25216;&#26415;&#30340;&#20998;&#23618;&#22810;&#23454;&#20363;&#22810;&#26631;&#31614;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Multi-Instance Multi-Label Learning for Detecting Propaganda Techniques. (arXiv:2305.19419v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#21516;&#26102;&#23545;&#25152;&#26377;&#27573;&#33853;&#36827;&#34892;&#23459;&#20256;&#25216;&#26415;&#20998;&#31867;&#30340;&#22810;&#23454;&#20363;&#22810;&#26631;&#31614;&#65288;MIML&#65289;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#20026;&#27169;&#22411;&#24341;&#20837;&#20102;&#23618;&#27425;&#24615;&#26631;&#31614;&#20381;&#36182;&#20851;&#31995;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;&#26041;&#27861;&#37117;&#24573;&#30053;&#20102;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;SemEval 2020&#20219;&#21153;11&#65288;Martino&#31561;&#65292;2020a&#65289;&#25512;&#20986;&#20197;&#26469;&#65292;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#22810;&#31181;&#26041;&#27861;&#20197;&#22522;&#20110;&#20462;&#36766;&#25216;&#24039;&#20998;&#31867;&#23459;&#20256;&#20869;&#23481;&#21435;&#24433;&#21709;&#35835;&#32773;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#19968;&#27425;&#21482;&#33021;&#23545;&#26576;&#20010;&#27573;&#33853;&#36827;&#34892;&#20998;&#31867;&#65292;&#24573;&#30053;&#20102;&#21516;&#19968;&#19978;&#19979;&#25991;&#20854;&#20182;&#27573;&#33853;&#26631;&#31614;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#23459;&#20256;&#25216;&#26415;&#20998;&#31867;&#20316;&#20026;&#19968;&#31181;&#22810;&#23454;&#20363;&#22810;&#26631;&#31614;&#65288;MIML&#65289;&#23398;&#20064;&#38382;&#39064;&#65288;Zhou&#31561;&#65292;2012&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#20110;RoBERTa&#30340;&#27169;&#22411;&#65288;Zhuang&#31561;&#65292;2021&#65289;&#26469;&#21516;&#26102;&#20998;&#31867;&#25991;&#31456;&#20013;&#30340;&#25152;&#26377;&#27573;&#33853;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#27880;&#24847;&#21040;&#65292;&#30001;&#20110;&#27880;&#37322;&#36807;&#31243;&#65292;&#26631;&#27880;&#32773;&#25353;&#29031;&#20915;&#31574;&#26641;&#23545;&#36825;&#20123;&#27573;&#33853;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#20174;&#32780;&#24418;&#25104;&#20102;&#19981;&#21516;&#25216;&#24039;&#20043;&#38388;&#30340;&#23618;&#27425;&#20851;&#31995;&#65292;&#32780;&#29616;&#26377;&#30340;&#26041;&#27861;&#21364;&#24573;&#30053;&#20102;&#36825;&#20123;&#23618;&#27425;&#20851;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#20026;&#35757;&#32451;&#30446;&#26631;&#20013;&#30340;&#27599;&#20010;&#33410;&#28857;&#28155;&#21152;&#36741;&#21161;&#20998;&#31867;&#22120;&#26469;&#24182;&#20837;&#36825;&#20123;&#20998;&#23618;&#26631;&#31614;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#20248;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the introduction of the SemEval 2020 Task 11 (Martino et al., 2020a), several approaches have been proposed in the literature for classifying propaganda based on the rhetorical techniques used to influence readers. These methods, however, classify one span at a time, ignoring dependencies from the labels of other spans within the same context. In this paper, we approach propaganda technique classification as a Multi-Instance Multi-Label (MIML) learning problem (Zhou et al., 2012) and propose a simple RoBERTa-based model (Zhuang et al., 2021) for classifying all spans in an article simultaneously. Further, we note that, due to the annotation process where annotators classified the spans by following a decision tree, there is an inherent hierarchical relationship among the different techniques, which existing approaches ignore. We incorporate these hierarchical label dependencies by adding an auxiliary classifier for each node in the decision tree to the training objective and ense
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20799;&#31461;&#20445;&#25252;&#26381;&#21153;&#20013;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#23384;&#22312;&#30340;&#31181;&#26063;&#20559;&#35265;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#20102;NER&#27169;&#22411;&#19968;&#33268;&#23384;&#22312;&#31639;&#27861;&#19981;&#20844;&#24179;&#30340;&#24773;&#20917;&#65292;&#21516;&#26102;&#21487;&#33021;&#20986;&#29616;&#25351;&#20195;&#28040;&#35299;&#21644;&#39118;&#38505;&#39044;&#27979;&#20013;&#31639;&#27861;&#19981;&#20844;&#24179;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2305.19409</link><description>&lt;p&gt;
&#20998;&#26512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#20799;&#31461;&#20445;&#25252;&#26381;&#21153;&#20013;&#30340;&#31181;&#26063;&#20559;&#35265;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Examining risks of racial biases in NLP tools for child protective services. (arXiv:2305.19409v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19409
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20799;&#31461;&#20445;&#25252;&#26381;&#21153;&#20013;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#23384;&#22312;&#30340;&#31181;&#26063;&#20559;&#35265;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#20102;NER&#27169;&#22411;&#19968;&#33268;&#23384;&#22312;&#31639;&#27861;&#19981;&#20844;&#24179;&#30340;&#24773;&#20917;&#65292;&#21516;&#26102;&#21487;&#33021;&#20986;&#29616;&#25351;&#20195;&#28040;&#35299;&#21644;&#39118;&#38505;&#39044;&#27979;&#20013;&#31639;&#27861;&#19981;&#20844;&#24179;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#24456;&#22810;&#25991;&#29486;&#24050;&#32463;&#30830;&#35748;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20013;&#23384;&#22312;&#20154;&#21475;&#32479;&#35745;&#23398;&#20559;&#35265;&#30340;&#38382;&#39064;&#65292;&#20294;&#26159;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#20381;&#36182;&#20110;&#31574;&#21010;&#30340;&#20559;&#35265;&#24230;&#37327;&#25351;&#26631;&#65292;&#36825;&#21487;&#33021;&#19981;&#33021;&#21453;&#26144;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#12290;&#21516;&#26102;&#65292;&#20174;&#19994;&#20154;&#21592;&#36234;&#26469;&#36234;&#22810;&#22320;&#22312;&#39640;&#39118;&#38505;&#29615;&#22659;&#20013;&#20351;&#29992;&#31639;&#27861;&#24037;&#20855;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#12290;&#26412;&#30740;&#31350;&#32858;&#28966;&#22312;&#20799;&#31461;&#20445;&#25252;&#26381;&#21153;&#20013;&#30340;&#19968;&#31181;&#24773;&#20917;&#12290;&#20799;&#31461;&#20445;&#25252;&#26381;&#21153;&#24037;&#20316;&#32773;&#32463;&#24120;&#20889;&#26377;&#20851;&#20182;&#20204;&#25152;&#26381;&#21153;&#30340;&#23478;&#24237;&#30340;&#35814;&#32454;&#25991;&#26412;&#31508;&#35760;&#65292;&#32780;&#20799;&#31461;&#20445;&#25252;&#26426;&#26500;&#27491;&#22312;&#31215;&#26497;&#23547;&#27714;&#37096;&#32626;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#26469;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#12290;&#37492;&#20110;&#22312;&#36825;&#20010;&#29615;&#22659;&#20013;&#24050;&#32463;&#23384;&#22312;&#30340;&#31181;&#26063;&#20559;&#35265;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#37096;&#32626;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#21487;&#33021;&#22686;&#21152;&#31181;&#26063;&#24046;&#24322;&#30340;&#21487;&#33021;&#36884;&#24452;&#12290;&#25105;&#20204;&#29305;&#21035;&#23457;&#26597;&#20102;&#31508;&#35760;&#20013;&#30340;&#35789;&#27719;&#32479;&#35745;&#21644;&#39118;&#38505;&#39044;&#27979;&#12289;&#25351;&#20195;&#28040;&#35299;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#20013;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#35760;&#24405;&#20102;NER&#27169;&#22411;&#30340;&#19968;&#33268;&#31639;&#27861;&#19981;&#20844;&#24179;&#24615;&#65292;&#24182;&#21487;&#33021;&#20986;&#29616;&#25351;&#20195;&#28040;&#35299;&#21644;&#39118;&#38505;&#39044;&#27979;&#20013;&#31639;&#27861;&#19981;&#20844;&#24179;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although much literature has established the presence of demographic bias in natural language processing (NLP) models, most work relies on curated bias metrics that may not be reflective of real-world applications. At the same time, practitioners are increasingly using algorithmic tools in high-stakes settings, with particular recent interest in NLP. In this work, we focus on one such setting: child protective services (CPS). CPS workers often write copious free-form text notes about families they are working with, and CPS agencies are actively seeking to deploy NLP models to leverage these data. Given well-established racial bias in this setting, we investigate possible ways deployed NLP is liable to increase racial disparities. We specifically examine word statistics within notes and algorithmic fairness in risk prediction, coreference resolution, and named entity recognition (NER). We document consistent algorithmic unfairness in NER models, possible algorithmic unfairness in corefe
&lt;/p&gt;</description></item><item><title>&#19978;&#19979;&#25991;&#35270;&#35273;&#21464;&#25442;&#22120;(ContextViT)&#29992;&#20110;&#29983;&#25104;&#22270;&#20687;&#30340;&#40065;&#26834;&#29305;&#24449;&#34920;&#31034;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#20196;&#29260;&#65292;&#21487;&#20197;&#35299;&#37322;&#25481;&#29305;&#23450;&#20110;&#32452;&#30340;&#21327;&#21464;&#37327;&#32467;&#26500;&#65292;&#21516;&#26102;&#20445;&#25345;&#36328;&#32452;&#20849;&#20139;&#30340;&#26680;&#24515;&#35270;&#35273;&#29305;&#24449;&#65292;&#33021;&#22815;&#22312;&#30417;&#30563;&#24494;&#35843;&#12289;&#21322;&#30417;&#30563;&#23398;&#20064;&#20197;&#21450;&#20027;&#21160;&#23398;&#20064;&#31561;&#26041;&#38754;&#24471;&#21040;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.19402</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#35270;&#35273;&#21464;&#25442;&#22120;&#29992;&#20110;&#24378;&#20581;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Contextual Vision Transformers for Robust Representation Learning. (arXiv:2305.19402v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19402
&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#35270;&#35273;&#21464;&#25442;&#22120;(ContextViT)&#29992;&#20110;&#29983;&#25104;&#22270;&#20687;&#30340;&#40065;&#26834;&#29305;&#24449;&#34920;&#31034;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#20196;&#29260;&#65292;&#21487;&#20197;&#35299;&#37322;&#25481;&#29305;&#23450;&#20110;&#32452;&#30340;&#21327;&#21464;&#37327;&#32467;&#26500;&#65292;&#21516;&#26102;&#20445;&#25345;&#36328;&#32452;&#20849;&#20139;&#30340;&#26680;&#24515;&#35270;&#35273;&#29305;&#24449;&#65292;&#33021;&#22815;&#22312;&#30417;&#30563;&#24494;&#35843;&#12289;&#21322;&#30417;&#30563;&#23398;&#20064;&#20197;&#21450;&#20027;&#21160;&#23398;&#20064;&#31561;&#26041;&#38754;&#24471;&#21040;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#35270;&#35273;&#21464;&#25442;&#22120;(ContextViT)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#22270;&#20687;&#30340;&#40065;&#26834;&#29305;&#24449;&#34920;&#31034;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#34920;&#29616;&#20986;&#20998;&#32452;&#32467;&#26500;&#30340;&#22270;&#20687;&#65292;&#22914;&#21327;&#21464;&#37327;&#12290;ContextViT&#24341;&#20837;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#20196;&#29260;&#26469;&#32534;&#30721;&#29305;&#23450;&#20110;&#32452;&#30340;&#20449;&#24687;&#65292;&#20801;&#35768;&#27169;&#22411;&#35299;&#37322;&#25481;&#29305;&#23450;&#20110;&#32452;&#30340;&#21327;&#21464;&#37327;&#32467;&#26500;&#65292;&#21516;&#26102;&#20445;&#25345;&#36328;&#32452;&#20849;&#20139;&#30340;&#26680;&#24515;&#35270;&#35273;&#29305;&#24449;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#32473;&#23450;&#36755;&#20837;&#22270;&#20687;&#30340;&#24773;&#20917;&#19979;&#65292;Context-ViT&#23558;&#20849;&#20139;&#30456;&#21516;&#21327;&#21464;&#37327;&#30340;&#22270;&#20687;&#26144;&#23556;&#21040;&#35813;&#19978;&#19979;&#25991;&#20196;&#29260;&#65292;&#24182;&#28155;&#21152;&#21040;&#36755;&#20837;&#22270;&#20687;&#20196;&#29260;&#20013;&#65292;&#20197;&#25429;&#33719;&#23558;&#27169;&#22411;&#26465;&#20214;&#21270;&#20026;&#32452;&#25104;&#21592;&#36523;&#20221;&#30340;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#25512;&#26029;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#36816;&#34892;&#26102;&#39044;&#27979;&#36825;&#20123;&#20196;&#29260;&#65292;&#21482;&#38656;&#35201;&#32473;&#20986;&#19968;&#20123;&#26469;&#33258;&#32452;&#20998;&#24067;&#30340;&#26679;&#26412;&#21363;&#21487;&#65292;&#20351;&#24471;ContextViT&#21487;&#20197;&#25512;&#24191;&#21040;&#26032;&#30340;&#27979;&#35797;&#20998;&#24067;&#12290;&#25105;&#20204;&#36890;&#36807;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#35828;&#26126;&#20102;ContextViT&#30340;&#24615;&#33021;&#12290;&#22312;&#30417;&#30563;&#24494;&#35843;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23558;&#39044;&#35757;&#32451;&#30340;ViTs&#19982;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#20196;&#29260;&#30456;&#32467;&#21512;&#21487;&#20197;&#25552;&#39640;&#22270;&#20687;&#20998;&#31867;&#22522;&#20934;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;ContextViT&#21487;&#20197;&#29992;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#65292;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#20165;&#20351;&#29992;10%&#30340;&#26631;&#35760;&#26679;&#26412;&#21363;&#21487;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ContextViT&#21487;&#20197;&#36890;&#36807;&#24341;&#23548;&#36873;&#25321;&#26469;&#33258;&#23569;&#25968;&#32676;&#20307;&#30340;&#26356;&#20855;&#20449;&#24687;&#20215;&#20540;&#30340;&#26679;&#26412;&#65292;&#20174;&#32780;&#25552;&#39640;&#20027;&#21160;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Contextual Vision Transformers (ContextViT), a method for producing robust feature representations for images exhibiting grouped structure such as covariates. ContextViT introduces an extra context token to encode group-specific information, allowing the model to explain away group-specific covariate structures while keeping core visual features shared across groups. Specifically, given an input image, Context-ViT maps images that share the same covariate into this context token appended to the input image tokens to capture the effects of conditioning the model on group membership. We furthermore introduce a context inference network to predict such tokens on the fly given a few samples from a group distribution, enabling ContextViT to generalize to new testing distributions at inference time. We illustrate the performance of ContextViT through a diverse range of applications. In supervised fine-tuning, we demonstrate that augmenting pre-trained ViTs with additional context 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328; West Frisian &#19978;&#30340;&#27979;&#35797;&#21457;&#29616;&#65292;&#22312;&#39044;&#35757;&#32451; BVCC &#25968;&#25454;&#38598;&#21518;&#65292;&#24494;&#35843; SOMOS &#25968;&#25454;&#38598;&#33021;&#22815;&#24471;&#21040;&#26368;&#20339;&#31934;&#24230;&#12290;&#20351;&#29992;&#36229;&#36807;&#24635;&#25968;&#25454;&#30340; 30% &#24182;&#19981;&#33021;&#26174;&#33879;&#25552;&#39640;&#31934;&#24230;&#12290;&#20351;&#29992;&#26469;&#33258;&#21333;&#20010;&#21548;&#20247;&#30340;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#26377;&#26395;&#25552;&#39640;&#31995;&#32479;&#32423;&#31934;&#24230;&#65292;&#25903;&#25345;&#21333;&#21442;&#19982;&#32773;&#35797;&#39564;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.19396</link><description>&lt;p&gt;
&#38024;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#33258;&#21160; MOS &#39044;&#27979;&#30340;&#36164;&#28304;&#26377;&#25928;&#24494;&#35843;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Resource-Efficient Fine-Tuning Strategies for Automatic MOS Prediction in Text-to-Speech for Low-Resource Languages. (arXiv:2305.19396v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328; West Frisian &#19978;&#30340;&#27979;&#35797;&#21457;&#29616;&#65292;&#22312;&#39044;&#35757;&#32451; BVCC &#25968;&#25454;&#38598;&#21518;&#65292;&#24494;&#35843; SOMOS &#25968;&#25454;&#38598;&#33021;&#22815;&#24471;&#21040;&#26368;&#20339;&#31934;&#24230;&#12290;&#20351;&#29992;&#36229;&#36807;&#24635;&#25968;&#25454;&#30340; 30% &#24182;&#19981;&#33021;&#26174;&#33879;&#25552;&#39640;&#31934;&#24230;&#12290;&#20351;&#29992;&#26469;&#33258;&#21333;&#20010;&#21548;&#20247;&#30340;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#26377;&#26395;&#25552;&#39640;&#31995;&#32479;&#32423;&#31934;&#24230;&#65292;&#25903;&#25345;&#21333;&#21442;&#19982;&#32773;&#35797;&#39564;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#24320;&#25918;&#25968;&#25454;&#38598; BVCC &#21644; SOMOS &#22522;&#20110; wav2vec 2.0 &#35757;&#32451;&#20102;&#19968;&#20010; MOS &#39044;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328; West Frisian &#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312; SOMOS &#24494;&#35843;&#21069;&#20808;&#22312; BVCC &#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#21487;&#20197;&#24471;&#21040;&#26368;&#20339;&#30340;&#24494;&#35843;&#21644;&#38646;&#26679;&#26412;&#39044;&#27979;&#31934;&#24230;&#12290;&#36827;&#19968;&#27493;&#30340;&#24494;&#35843;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#36229;&#36807;&#24635;&#25968;&#25454;&#30340; 30% &#24182;&#19981;&#20250;&#24102;&#26469;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#26469;&#33258;&#21333;&#20010;&#21548;&#20247;&#30340;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#26377;&#26395;&#25552;&#39640;&#31995;&#32479;&#32423;&#31934;&#24230;&#65292;&#25903;&#25345;&#21333;&#21442;&#19982;&#32773;&#35797;&#39564;&#30340;&#21487;&#34892;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#21487;&#20197;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340; TTS &#24320;&#21457;&#25552;&#20379;&#24110;&#21161;&#65292;&#36827;&#19968;&#27493;&#25512;&#36827;&#38646;&#26679;&#26412; MOS &#39044;&#27979;&#24182;&#25351;&#23548;&#26089;&#26399;&#35780;&#20272;&#20013;&#30340;&#27979;&#35797;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
We train a MOS prediction model based on wav2vec 2.0 using the open-access data sets BVCC and SOMOS. Our test with neural TTS data in the low-resource language (LRL) West Frisian shows that pre-training on BVCC before fine-tuning on SOMOS leads to the best accuracy for both fine-tuned and zero-shot prediction. Further fine-tuning experiments show that using more than 30 percent of the total data does not lead to significant improvements. In addition, fine-tuning with data from a single listener shows promising system-level accuracy, supporting the viability of one-participant pilot tests. These findings can all assist the resource-conscious development of TTS for LRLs by progressing towards better zero-shot MOS prediction and informing the design of listening tests, especially in early-stage evaluation.
&lt;/p&gt;</description></item><item><title>DyGen&#26159;&#19968;&#20010;&#21160;&#24577;&#22686;&#24378;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20351;&#29992;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#21160;&#24577;&#27169;&#24335;&#21487;&#20197;&#25913;&#21892;&#20174;&#22122;&#22768;&#26631;&#31614;&#20013;&#23398;&#20064;&#30340;&#31934;&#24230;&#65292;&#21516;&#26102;&#20351;&#29992;&#20849;&#35268;&#27491;&#21017;&#21270;&#26426;&#21046;&#26469;&#26368;&#23567;&#21270;&#28508;&#22312;&#22122;&#22768;&#26631;&#31614;&#21644;&#20808;&#39564;&#30340;&#24433;&#21709;&#65292;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.19395</link><description>&lt;p&gt;
DyGen: &#36890;&#36807;&#21160;&#24577;&#22686;&#24378;&#30340;&#29983;&#25104;&#24314;&#27169;&#20174;&#22122;&#22768;&#26631;&#31614;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DyGen: Learning from Noisy Labels via Dynamics-Enhanced Generative Modeling. (arXiv:2305.19395v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19395
&lt;/p&gt;
&lt;p&gt;
DyGen&#26159;&#19968;&#20010;&#21160;&#24577;&#22686;&#24378;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20351;&#29992;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#21160;&#24577;&#27169;&#24335;&#21487;&#20197;&#25913;&#21892;&#20174;&#22122;&#22768;&#26631;&#31614;&#20013;&#23398;&#20064;&#30340;&#31934;&#24230;&#65292;&#21516;&#26102;&#20351;&#29992;&#20849;&#35268;&#27491;&#21017;&#21270;&#26426;&#21046;&#26469;&#26368;&#23567;&#21270;&#28508;&#22312;&#22122;&#22768;&#26631;&#31614;&#21644;&#20808;&#39564;&#30340;&#24433;&#21709;&#65292;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#35757;&#32451;&#25968;&#25454;&#21487;&#33021;&#21253;&#21547;&#19981;&#27491;&#30830;&#25110;&#24050;&#25439;&#22351;&#30340;&#26631;&#31614;&#65292;&#20174;&#22122;&#22768;&#26631;&#31614;&#20013;&#23398;&#20064;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#24403;&#20351;&#29992;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#26102;&#65292;&#27169;&#22411;&#24456;&#23481;&#26131;&#36807;&#24230;&#25311;&#21512;&#26631;&#31614;&#22122;&#22768;&#65292;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#20174;&#22122;&#22768;&#26631;&#31614;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#20351;&#29992;&#38745;&#24577;&#36755;&#20837;&#29305;&#24449;&#36827;&#34892;&#21435;&#22122;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#21463;&#38480;&#20110;&#23427;&#20204;&#22312;&#30495;&#23454;&#26631;&#31614;&#20998;&#24067;&#26041;&#38754;&#25552;&#20379;&#30340;&#20449;&#24687;&#65292;&#21487;&#33021;&#23548;&#33268;&#26377;&#20559;&#30340;&#25110;&#19981;&#27491;&#30830;&#30340;&#39044;&#27979;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DyGen&#30340;&#21160;&#24577;&#22686;&#24378;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#36807;&#31243;&#20013;&#21033;&#29992;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#21160;&#24577;&#27169;&#24335;&#26469;&#25913;&#21892;&#22122;&#22768;&#26631;&#31614;&#39044;&#27979;&#12290;DyGen&#20351;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#26694;&#26550;&#20174;&#22122;&#22768;&#26631;&#31614;&#21644;&#35757;&#32451;&#21160;&#24577;&#20013;&#25512;&#26029;&#30495;&#23454;&#26631;&#31614;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#20849;&#35268;&#27491;&#21017;&#21270;&#26426;&#21046;&#26469;&#26368;&#23567;&#21270;&#28508;&#22312;&#22122;&#22768;&#26631;&#31614;&#21644;&#20808;&#39564;&#30340;&#24433;&#21709;&#12290;&#22312;&#23384;&#22312;&#19981;&#21516;&#32423;&#21035;&#30340;&#26631;&#31614;&#22122;&#22768;&#24773;&#20917;&#19979;&#65292;DyGen&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from noisy labels is a challenge that arises in many real-world applications where training data can contain incorrect or corrupted labels. When fine-tuning language models with noisy labels, models can easily overfit the label noise, leading to decreased performance. Most existing methods for learning from noisy labels use static input features for denoising, but these methods are limited by the information they can provide on true label distributions and can result in biased or incorrect predictions. In this work, we propose the Dynamics-Enhanced Generative Model (DyGen), which uses dynamic patterns in the embedding space during the fine-tuning process of language models to improve noisy label predictions. DyGen uses the variational auto-encoding framework to infer the posterior distributions of true labels from noisy labels and training dynamics. Additionally, a co-regularization mechanism is used to minimize the impact of potentially noisy labels and priors. DyGen demonstr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#24773;&#24863;&#20998;&#26512;&#20013;&#31532;&#19968;&#27425;&#24212;&#29992;&#20102;&#37327;&#23376;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;QNLP&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;lambeq QNLP&#24037;&#20855;&#21253;&#21644;&#21073;&#26725;&#37327;&#23376;&#65288;Quantinuum&#65289;&#30340;$t|ket&gt;$&#65292;&#22312;&#19977;&#31181;&#27169;&#25311;&#21644;&#22122;&#22768;&#37327;&#23376;&#35774;&#22791;&#19978;&#21462;&#24471;&#20102;&#23436;&#32654;&#27979;&#35797;&#38598;&#20934;&#30830;&#24615;&#21644;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.19383</link><description>&lt;p&gt;
&#20351;&#29992;lambeq&#24037;&#20855;&#21253;&#30340;&#37327;&#23376;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Quantum Natural Language Processing based Sentiment Analysis using lambeq Toolkit. (arXiv:2305.19383v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19383
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#24773;&#24863;&#20998;&#26512;&#20013;&#31532;&#19968;&#27425;&#24212;&#29992;&#20102;&#37327;&#23376;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;QNLP&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;lambeq QNLP&#24037;&#20855;&#21253;&#21644;&#21073;&#26725;&#37327;&#23376;&#65288;Quantinuum&#65289;&#30340;$t|ket&gt;$&#65292;&#22312;&#19977;&#31181;&#27169;&#25311;&#21644;&#22122;&#22768;&#37327;&#23376;&#35774;&#22791;&#19978;&#21462;&#24471;&#20102;&#23436;&#32654;&#27979;&#35797;&#38598;&#20934;&#30830;&#24615;&#21644;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#20998;&#31867;&#26159;&#32463;&#20856;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26368;&#22909;&#30340;&#24212;&#29992;&#20043;&#19968;&#65292;&#22312;&#38134;&#34892;&#12289;&#21830;&#19994;&#21644;&#33829;&#38144;&#34892;&#19994;&#31561;&#21508;&#20010;&#26085;&#24120;&#39046;&#22495;&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#35265;&#35777;&#20854;&#21147;&#37327;&#12290;&#25105;&#20204;&#24050;&#32463;&#30693;&#36947;&#20102;&#32463;&#20856;AI&#21644;&#26426;&#22120;&#23398;&#20064;&#22914;&#20309;&#25913;&#21464;&#21644;&#25913;&#36827;&#25216;&#26415;&#12290;&#37327;&#23376;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;QNLP&#65289;&#26159;&#19968;&#31181;&#24180;&#36731;&#32780;&#36880;&#28176;&#20986;&#29616;&#30340;&#25216;&#26415;&#65292;&#23427;&#20855;&#26377;&#20026;NLP&#20219;&#21153;&#25552;&#20379;&#37327;&#23376;&#20248;&#21183;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;QNLP&#22312;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#31532;&#19968;&#20010;&#24212;&#29992;&#65292;&#24182;&#22312;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#27169;&#25311;&#21644;&#22312;&#22122;&#22768;&#37327;&#23376;&#35774;&#22791;&#19978;&#36816;&#34892;&#30340;&#23454;&#39564;&#20013;&#23454;&#29616;&#20102;&#23436;&#32654;&#30340;&#27979;&#35797;&#38598;&#20934;&#30830;&#24615;&#21644;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;lambeq QNLP&#24037;&#20855;&#21253;&#21644;&#21073;&#26725;&#37327;&#23376;&#65288;Quantinuum&#65289;&#30340;$t|ket&gt;$&#26469;&#21576;&#29616;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentiment classification is one the best use case of classical natural language processing (NLP) where we can witness its power in various daily life domains such as banking, business and marketing industry. We already know how classical AI and machine learning can change and improve technology. Quantum natural language processing (QNLP) is a young and gradually emerging technology which has the potential to provide quantum advantage for NLP tasks. In this paper we show the first application of QNLP for sentiment analysis and achieve perfect test set accuracy for three different kinds of simulations and a decent accuracy for experiments ran on a noisy quantum device. We utilize the lambeq QNLP toolkit and $t|ket&gt;$ by Cambridge Quantum (Quantinuum) to bring out the results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37319;&#29992;&#20027;&#39064;&#24314;&#27169;&#25216;&#26415;&#23545;1200&#20363;&#24515;&#21147;&#34928;&#31469;&#24739;&#32773;&#30340;&#35786;&#26029;&#32534;&#30721;&#21644;&#31243;&#24207;&#25253;&#21578;&#20013;&#23384;&#22312;&#30340;&#20027;&#39064;&#36827;&#34892;&#35782;&#21035;&#65292;&#26088;&#22312;&#20174;&#20013;&#35782;&#21035;&#24515;&#21147;&#34928;&#31469;&#30340;&#20020;&#24202;&#34920;&#22411;&#24182;&#39044;&#27979;&#30149;&#20154;&#20303;&#38498;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2305.19373</link><description>&lt;p&gt;
&#36890;&#36807;&#25366;&#25496;&#20020;&#24202;&#31508;&#35760;&#20013;&#30340;&#20027;&#39064;&#35782;&#21035;&#24515;&#21147;&#34928;&#31469;&#24739;&#32773;&#30340;&#34920;&#22411;&#24182;&#39044;&#27979;&#20303;&#38498;&#26102;&#38388;
&lt;/p&gt;
&lt;p&gt;
Mining Themes in Clinical Notes to Identify Phenotypes and to Predict Length of Stay in Patients admitted with Heart Failure. (arXiv:2305.19373v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992;&#20027;&#39064;&#24314;&#27169;&#25216;&#26415;&#23545;1200&#20363;&#24515;&#21147;&#34928;&#31469;&#24739;&#32773;&#30340;&#35786;&#26029;&#32534;&#30721;&#21644;&#31243;&#24207;&#25253;&#21578;&#20013;&#23384;&#22312;&#30340;&#20027;&#39064;&#36827;&#34892;&#35782;&#21035;&#65292;&#26088;&#22312;&#20174;&#20013;&#35782;&#21035;&#24515;&#21147;&#34928;&#31469;&#30340;&#20020;&#24202;&#34920;&#22411;&#24182;&#39044;&#27979;&#30149;&#20154;&#20303;&#38498;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#21147;&#34928;&#31469;&#26159;&#19968;&#31181;&#32508;&#21512;&#24449;&#65292;&#24403;&#24515;&#33039;&#26080;&#27861;&#27893;&#34880;&#21644;&#36755;&#36865;&#27687;&#27668;&#20197;&#25903;&#25345;&#20307;&#20869;&#20854;&#20182;&#22120;&#23448;&#26102;&#20986;&#29616;&#12290;&#35782;&#21035;&#25509;&#21463;&#24515;&#21147;&#34928;&#31469;&#27835;&#30103;&#30340;&#30149;&#20154;&#30340;&#35786;&#26029;&#32534;&#30721;&#21644;&#31243;&#24207;&#25253;&#21578;&#20013;&#30340;&#28508;&#22312;&#20027;&#39064;&#65292;&#21487;&#20197;&#25581;&#31034;&#19982;&#24515;&#21147;&#34928;&#31469;&#30456;&#20851;&#30340;&#20020;&#24202;&#34920;&#22411;&#65292;&#24182;&#26681;&#25454;&#30456;&#20284;&#30340;&#29305;&#24449;&#23545;&#30149;&#20154;&#36827;&#34892;&#20998;&#32452;&#65292;&#36825;&#20063;&#26377;&#21161;&#20110;&#39044;&#27979;&#30149;&#20154;&#30340;&#20303;&#38498;&#26102;&#38388;&#12290;&#30001;&#20110;&#36825;&#20123;&#20020;&#24202;&#34920;&#22411;&#36890;&#24120;&#20855;&#26377;&#27010;&#29575;&#30340;&#28508;&#22312;&#32467;&#26500;&#65292;&#24182;&#19988;&#20043;&#21069;&#27809;&#26377;&#20851;&#20110;&#20351;&#29992;&#27010;&#29575;&#26694;&#26550;&#22312;&#24515;&#21147;&#34928;&#31469;&#24739;&#32773;&#30340;&#20020;&#24202;&#31508;&#35760;&#20013;&#35782;&#21035;&#34920;&#22411;&#21644;&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#26041;&#27861;&#39044;&#27979;&#36825;&#20123;&#24739;&#32773;&#30340;&#20303;&#38498;&#26102;&#38388;&#30340;&#30740;&#31350;&#65292;&#22240;&#27492;&#25105;&#20204;&#37319;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#8212;&#8212;&#20027;&#39064;&#24314;&#27169;&#65292;&#23545;&#20234;&#21033;&#35834;&#20234;&#22823;&#23398;&#21307;&#38498;1200&#20363;&#24515;&#21147;&#34928;&#31469;&#24739;&#32773;&#30340;&#35786;&#26029;&#32534;&#30721;&#21644;&#31243;&#24207;&#25253;&#21578;&#20013;&#23384;&#22312;&#30340;&#20027;&#39064;&#36827;&#34892;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heart failure is a syndrome which occurs when the heart is not able to pump blood and oxygen to support other organs in the body. Identifying the underlying themes in the diagnostic codes and procedure reports of patients admitted for heart failure could reveal the clinical phenotypes associated with heart failure and to group patients based on their similar characteristics which could also help in predicting patient outcomes like length of stay. These clinical phenotypes usually have a probabilistic latent structure and hence, as there has been no previous work on identifying phenotypes in clinical notes of heart failure patients using a probabilistic framework and to predict length of stay of these patients using data-driven artificial intelligence-based methods, we apply natural language processing technique, topic modeling, to identify the themes present in diagnostic codes and in procedure reports of 1,200 patients admitted for heart failure at the University of Illinois Hospital 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22359;&#32423;&#24182;&#34892;Transformer&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#21270;&#20869;&#23384;&#25104;&#26412;&#65292;&#33021;&#22815;&#22788;&#29702;&#38271;&#24207;&#21015;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#27604;&#20808;&#21069;&#30340;&#20869;&#23384;&#39640;&#25928;&#26041;&#27861;&#26356;&#38271;32&#20493;&#30340;&#35757;&#32451;&#24207;&#21015;&#12290;</title><link>http://arxiv.org/abs/2305.19370</link><description>&lt;p&gt;
&#22823;&#22411;&#38271;&#24207;&#21015;&#27169;&#22411;&#30340;&#22359;&#32423;&#24182;&#34892;Transformer
&lt;/p&gt;
&lt;p&gt;
Blockwise Parallel Transformer for Long Context Large Models. (arXiv:2305.19370v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22359;&#32423;&#24182;&#34892;Transformer&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#21270;&#20869;&#23384;&#25104;&#26412;&#65292;&#33021;&#22815;&#22788;&#29702;&#38271;&#24207;&#21015;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#27604;&#20808;&#21069;&#30340;&#20869;&#23384;&#39640;&#25928;&#26041;&#27861;&#26356;&#38271;32&#20493;&#30340;&#35757;&#32451;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#24050;&#32463;&#25104;&#20026;&#26368;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#22522;&#30707;&#65292;&#22312;&#21508;&#31181;AI&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;Transformer&#20013;&#30340;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#21644;&#22823;&#22411;&#21069;&#39304;&#32593;&#32476;&#25152;&#38656;&#30340;&#20869;&#23384;&#23481;&#37327;&#38480;&#21046;&#20102;&#23427;&#20204;&#22788;&#29702;&#38271;&#24207;&#21015;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#20026;&#28041;&#21450;&#22810;&#20010;&#38271;&#24207;&#21015;&#25110;&#38271;&#26399;&#20381;&#36182;&#30340;&#20219;&#21153;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#26041;&#27861;&#65292;&#22359;&#32423;&#24182;&#34892;Transformer&#65288;BPT&#65289;&#65292;&#23427;&#21033;&#29992;&#22359;&#32423;&#35745;&#31639;&#33258;&#25105;&#27880;&#24847;&#21644;&#21069;&#39304;&#32593;&#32476;&#34701;&#21512;&#20197;&#26368;&#23567;&#21270;&#20869;&#23384;&#25104;&#26412;&#12290;&#36890;&#36807;&#22312;&#20445;&#25345;&#20869;&#23384;&#25928;&#29575;&#30340;&#21516;&#26102;&#22788;&#29702;&#26356;&#38271;&#30340;&#36755;&#20837;&#24207;&#21015;&#65292;BPT&#20351;&#35757;&#32451;&#24207;&#21015;&#30340;&#38271;&#24230;&#27604;&#21407;&#22987;&#30340;Transformer&#38271;32&#20493;&#65292;&#27604;&#20808;&#21069;&#30340;&#20869;&#23384;&#39640;&#25928;&#26041;&#27861;&#38271;2&#21040;4&#20493;&#12290;&#23545;&#35821;&#35328;&#24314;&#27169;&#21644;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;BPT&#22312;&#20943;&#23569;&#20869;&#23384;&#38656;&#27714;&#21644;&#25552;&#39640;&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have emerged as the cornerstone of state-of-the-art natural language processing models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands posed by the self-attention mechanism and the large feedforward network in Transformers limit their ability to handle long sequences, thereby creating challenges for tasks involving multiple long sequences or long-term dependencies. We present a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs. By processing longer input sequences while maintaining memory efficiency, BPT enables training sequences up to 32 times longer than vanilla Transformers and 2 to 4 times longer than previous memory-efficient methods. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of BPT in reducing memory requirements and improving perfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;I-STAR&#65292;&#21487;&#20197;&#22686;&#21152;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#65292;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#25913;&#21892;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#32452;&#21512;&#34920;&#31034;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.19358</link><description>&lt;p&gt;
&#31283;&#20581;&#30340;&#21508;&#21521;&#24322;&#24615;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Stable Anisotropic Regularization. (arXiv:2305.19358v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;I-STAR&#65292;&#21487;&#20197;&#22686;&#21152;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#65292;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#25913;&#21892;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#32452;&#21512;&#34920;&#31034;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25104;&#21151;&#65292;&#30740;&#31350;&#27169;&#22411;&#28608;&#27963;&#30340;&#23646;&#24615;&#24050;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20852;&#36259;&#12290;&#25991;&#29486;&#26222;&#36941;&#35748;&#20026;LLMs&#34920;&#31034;&#30001;&#23569;&#25968;&#20855;&#26377;&#26497;&#39640;&#26041;&#24046;&#21644;&#24133;&#24230;&#30340;&#8220;&#24322;&#24120;&#32500;&#24230;&#8221;&#20027;&#23548;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#30340;&#20960;&#39033;&#30740;&#31350;&#35797;&#22270;&#20943;&#36731;&#36825;&#20123;&#24322;&#24120;&#32500;&#24230;&#30340;&#24433;&#21709;&#65292;&#24182;&#36843;&#20351;LLMs&#25104;&#20026;&#21508;&#21521;&#21516;&#24615;&#65288;&#21363;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#25152;&#26377;&#32500;&#24230;&#20855;&#26377;&#22343;&#21248;&#26041;&#24046;&#65289;&#30340;&#12290;&#21508;&#21521;&#21516;&#24615;&#34987;&#35748;&#20026;&#26159;LLMs&#30340;&#19968;&#31181;&#29702;&#24819;&#23646;&#24615;&#65292;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#24182;&#26356;&#21152;&#36148;&#36817;&#20154;&#31867;&#30452;&#35273;&#30340;&#25991;&#26412;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;NLP&#20013;&#21508;&#21521;&#21516;&#24615;&#30340;&#35768;&#22810;&#35266;&#28857;&#37117;&#26159;&#22522;&#20110;&#23884;&#20837;&#30340;&#24179;&#22343;&#20313;&#24358;&#30456;&#20284;&#24230;&#65292;&#26368;&#36817;&#24050;&#32463;&#34920;&#26126;&#36825;&#26159;&#19968;&#31181;&#26377;&#32570;&#38519;&#30340;&#21508;&#21521;&#21516;&#24615;&#24230;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;I-STAR&#65306;&#22522;&#20110;IsoScore$^{\star}$&#30340;&#31283;&#23450;&#21508;&#21521;&#24322;&#24615;&#27491;&#21017;&#21270;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#22686;&#21152;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the success of Large Language Models (LLMs), there has been considerable interest in studying the properties of model activations. The literature overwhelmingly agrees that LLM representations are dominated by a few ``outlier dimensions'' with exceedingly high variance and magnitude. Several studies in Natural Language Processing (NLP) have sought to mitigate the impact of such outlier dimensions and force LLMs to be isotropic (i.e., have uniform variance across all dimensions in embedding space). Isotropy is thought to be a desirable property for LLMs that improves model performance and more closely aligns textual representations with human intuition. However, many of the claims regarding isotropy in NLP have been based on the average cosine similarity of embeddings, which has recently been shown to be a flawed measure of isotropy. In this paper, we propose I-STAR: IsoScore$^{\star}$-based STable Anisotropic Regularization, a novel regularization method that can be used to incre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#25968;&#25454;&#38598;&#29305;&#24449;&#21270;&#26694;&#26550;infoVerse&#65292;&#36890;&#36807;&#32467;&#21512;&#21508;&#31181;&#27169;&#22411;&#39537;&#21160;&#30340;&#20803;&#20449;&#24687;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#25968;&#25454;&#38598;&#30340;&#22810;&#32500;&#29305;&#24449;&#65292;&#26377;&#21161;&#20110;&#29992;&#25143;&#25110;&#27169;&#22411;&#30830;&#23450;&#21738;&#20123;&#26679;&#26412;&#38656;&#35201;&#20851;&#27880;&#12290;</title><link>http://arxiv.org/abs/2305.19344</link><description>&lt;p&gt;
infoVerse&#65306;&#19968;&#31181;&#29992;&#22810;&#32500;&#24230;&#20803;&#20449;&#24687;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#29305;&#24449;&#21270;&#30340;&#36890;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
infoVerse: A Universal Framework for Dataset Characterization with Multidimensional Meta-information. (arXiv:2305.19344v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#25968;&#25454;&#38598;&#29305;&#24449;&#21270;&#26694;&#26550;infoVerse&#65292;&#36890;&#36807;&#32467;&#21512;&#21508;&#31181;&#27169;&#22411;&#39537;&#21160;&#30340;&#20803;&#20449;&#24687;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#25968;&#25454;&#38598;&#30340;&#22810;&#32500;&#29305;&#24449;&#65292;&#26377;&#21161;&#20110;&#29992;&#25143;&#25110;&#27169;&#22411;&#30830;&#23450;&#21738;&#20123;&#26679;&#26412;&#38656;&#35201;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
NLP&#31995;&#32479;&#30340;&#25104;&#21151;&#24448;&#24448;&#20381;&#36182;&#20110;&#22823;&#37327;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20013;&#24182;&#38750;&#25152;&#26377;&#26679;&#26412;&#37117;&#21516;&#26679;&#26377;&#21033;&#20110;&#23398;&#20064;&#65292;&#22240;&#20026;&#26377;&#20123;&#21487;&#33021;&#26159;&#20887;&#20313;&#25110;&#24102;&#26377;&#22122;&#22768;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#20960;&#31181;&#22522;&#20110;&#27169;&#22411;&#39537;&#21160;&#20803;&#20449;&#24687;&#65288;&#20363;&#22914;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#65289;&#30340;&#25968;&#25454;&#38598;&#29305;&#24449;&#21270;&#26041;&#27861;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#20114;&#34917;&#25928;&#24212;&#21463;&#21040;&#30340;&#20851;&#27880;&#36739;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;infoVerse&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#25968;&#25454;&#38598;&#29305;&#24449;&#21270;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#32467;&#21512;&#21508;&#31181;&#27169;&#22411;&#39537;&#21160;&#30340;&#20803;&#20449;&#24687;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#26377;&#25928;&#22320;&#25429;&#25417;&#20102;&#25968;&#25454;&#38598;&#30340;&#22810;&#32500;&#29305;&#24449;&#12290;infoVerse&#25581;&#31034;&#20102;&#25968;&#25454;&#38598;&#20013;&#22312;&#21407;&#22987;&#35821;&#20041;&#31354;&#38388;&#20013;&#19981;&#26126;&#26174;&#30340;&#29420;&#29305;&#21306;&#22495;&#65292;&#20174;&#32780;&#24341;&#23548;&#29992;&#25143;&#65288;&#25110;&#27169;&#22411;&#65289;&#30830;&#23450;&#21738;&#20123;&#26679;&#26412;&#38656;&#35201;&#20851;&#27880;&#20197;&#36827;&#34892;&#25506;&#32034;&#12289;&#35780;&#20272;&#25110;&#27880;&#37322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;infoVerse&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#26041;&#27861;&#26469;&#36873;&#25321;&#19968;&#32452;&#25968;&#25454;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of NLP systems often relies on the availability of large, high-quality datasets. However, not all samples in these datasets are equally valuable for learning, as some may be redundant or noisy. Several methods for characterizing datasets based on model-driven meta-information (e.g., model's confidence) have been developed, but the relationship and complementary effects of these methods have received less attention. In this paper, we introduce infoVerse, a universal framework for dataset characterization, which provides a new feature space that effectively captures multidimensional characteristics of datasets by incorporating various model-driven meta-information. infoVerse reveals distinctive regions of the dataset that are not apparent in the original semantic space, hence guiding users (or models) in identifying which samples to focus on for exploration, assessment, or annotation. Additionally, we propose a novel sampling method on infoVerse to select a set of data points
&lt;/p&gt;</description></item><item><title /><link>http://arxiv.org/abs/2305.19339</link><description>&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses. (arXiv:2305.19339v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19339
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
A human decision-maker benefits the most from an AI assistant that corrects for their biases. For problems such as generating interpretation of a radiology report given findings, a system predicting only highly likely outcomes may be less useful, where such outcomes are already obvious to the user. To alleviate biases in human decision-making, it is worth considering a broad differential diagnosis, going beyond the most likely options. We introduce a new task, "less likely brainstorming," that asks a model to generate outputs that humans think are relevant but less likely to happen. We explore the task in two settings: a brain MRI interpretation generation setting and an everyday commonsense reasoning setting. We found that a baseline approach of training with less likely hypotheses as targets generates outputs that humans evaluate as either likely or irrelevant nearly half of the time; standard MLE training is not effective. To tackle this problem, we propose a controlled text generat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#39640;&#23545;&#38750;&#24433;&#20687;&#25968;&#25454;&#30340;&#24212;&#29992;&#33021;&#21147;&#65292;&#24182;&#22312;ADNI&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;SOTA&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.19280</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25913;&#21892;&#22810;&#27169;&#24577;&#25968;&#25454;&#35786;&#26029;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;
&lt;/p&gt;
&lt;p&gt;
Large language models improve Alzheimer's disease diagnosis using multi-modality data. (arXiv:2305.19280v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#39640;&#23545;&#38750;&#24433;&#20687;&#25968;&#25454;&#30340;&#24212;&#29992;&#33021;&#21147;&#65292;&#24182;&#22312;ADNI&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;SOTA&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35786;&#26029;&#20687;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#36825;&#26679;&#30340;&#33392;&#38590;&#30149;&#30151;&#26102;&#65292;&#24433;&#20687;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#21442;&#32771;&#12290;&#38750;&#24433;&#20687;&#24739;&#32773;&#25968;&#25454;&#65288;&#20363;&#22914;&#24739;&#32773;&#20449;&#24687;&#12289;&#36951;&#20256;&#25968;&#25454;&#12289;&#33647;&#29289;&#20449;&#24687;&#12289;&#35748;&#30693;&#21644;&#35760;&#24518;&#27979;&#35797;&#65289;&#22312;&#35786;&#26029;&#20013;&#20063;&#36215;&#30528;&#38750;&#24120;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#25366;&#25496;&#36825;&#20123;&#20449;&#24687;&#30340;&#33021;&#21147;&#21463;&#38480;&#65292;&#22823;&#37096;&#20998;&#29616;&#26377;&#27169;&#22411;&#21482;&#33021;&#20351;&#29992;&#22810;&#27169;&#24577;&#24433;&#20687;&#25968;&#25454;&#65292;&#32780;&#19981;&#33021;&#20805;&#20998;&#21033;&#29992;&#38750;&#24433;&#20687;&#25968;&#25454;&#12290;&#25105;&#20204;&#20351;&#29992;&#30446;&#21069;&#38750;&#24120;&#27969;&#34892;&#30340;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#20351;&#29992;&#38750;&#24433;&#20687;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;ADNI&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;SOTA&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In diagnosing challenging conditions such as Alzheimer's disease (AD), imaging is an important reference. Non-imaging patient data such as patient information, genetic data, medication information, cognitive and memory tests also play a very important role in diagnosis. Effect. However, limited by the ability of artificial intelligence models to mine such information, most of the existing models only use multi-modal image data, and cannot make full use of non-image data. We use a currently very popular pre-trained large language model (LLM) to enhance the model's ability to utilize non-image data, and achieved SOTA results on the ADNI dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#27861;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#19987;&#29992;&#30340;&#35821;&#27861;&#26469;&#22686;&#24378;&#31034;&#20363;&#65292;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#21644;&#29305;&#23450;&#32422;&#26463;&#26465;&#20214;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.19234</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#29305;&#23450;&#39046;&#22495;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#35821;&#27861;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Grammar Prompting for Domain-Specific Language Generation with Large Language Models. (arXiv:2305.19234v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#27861;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#19987;&#29992;&#30340;&#35821;&#27861;&#26469;&#22686;&#24378;&#31034;&#20363;&#65292;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#21644;&#29305;&#23450;&#32422;&#26463;&#26465;&#20214;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#20197;&#20174;&#20165;&#26377;&#20960;&#20010;&#19978;&#19979;&#25991;&#31034;&#20363;&#20013;&#23398;&#20064;&#25191;&#34892;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20174;&#39640;&#24230;&#32467;&#26500;&#21270;&#30340;&#35821;&#35328;&#65288;&#20363;&#22914;&#65292;&#20174;&#35821;&#20041;&#35299;&#26512;&#21040;&#22797;&#26434;&#30340;&#29305;&#23450;&#39046;&#22495;&#35821;&#35328;&#65289;&#29983;&#25104;&#23383;&#31526;&#20018;&#65292;LLM&#21482;&#20174;&#23569;&#37327;&#31034;&#20363;&#20013;&#36827;&#34892;&#27867;&#21270;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;$\textbf{&#35821;&#27861;&#25552;&#31034;}$&#20316;&#20026;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#32972;&#31185;&#26031;-&#35834;&#23572;&#33539;&#24335;&#65288;BNF&#65289;&#20013;&#34920;&#36798;&#30340;&#35821;&#27861;&#26469;&#21551;&#29992;LLM&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#21644;&#29305;&#23450;&#39046;&#22495;&#30340;&#32422;&#26463;&#26465;&#20214;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#35821;&#27861;&#25552;&#31034;&#20351;&#29992;&#19968;&#20010;&#19987;&#38376;&#30340;&#35821;&#27861;&#26469;&#22686;&#24378;&#27599;&#20010;&#28436;&#31034;&#31034;&#20363;&#65292;&#35813;&#35821;&#27861;&#36275;&#20197;&#29983;&#25104;&#29305;&#23450;&#30340;&#36755;&#20986;&#31034;&#20363;&#65292;&#20854;&#20013;&#35813;&#19987;&#38376;&#30340;&#35821;&#27861;&#26159;&#20840;DSL&#35821;&#27861;&#30340;&#23376;&#38598;&#12290;&#23545;&#20110;&#25512;&#29702;&#65292;LLM&#39318;&#20808;&#39044;&#27979;&#19968;&#20010;&#32473;&#23450;&#27979;&#35797;&#36755;&#20837;&#30340;BNF&#35821;&#27861;&#65292;&#28982;&#21518;&#26681;&#25454;&#35821;&#27861;&#35268;&#21017;&#29983;&#25104;&#36755;&#20986;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35821;&#27861;&#25552;&#31034;&#21487;&#20197;&#20351;LLM&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) can learn to perform a wide range of natural language tasks from just a handful of in-context examples. However, for generating strings from highly structured languages (e.g., semantic parsing to complex domain-specific languages), it is challenging for the LLM to generalize from just a few exemplars. We explore $\textbf{grammar prompting}$ as a simple approach for enabling LLMs to use external knowledge and domain-specific constraints, expressed through a grammar expressed in Backus--Naur Form (BNF), during in-context learning. Grammar prompting augments each demonstration example with a specialized grammar that is minimally sufficient for generating the particular output example, where the specialized grammar is a subset of the full DSL grammar. For inference, the LLM first predicts a BNF grammar given a test input, and then generates the output according to the rules of the grammar. Experiments demonstrate that grammar prompting can enable LLMs to perfor
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;CHRT&#65292;&#23427;&#26159;&#19968;&#31181;&#21487;&#25511;&#35821;&#35328;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#34920;&#31034;&#36716;&#25442;&#26469;&#20462;&#25913;&#22522;&#30784;&#27169;&#22411;&#30340;&#38544;&#34255;&#34920;&#31034;&#20174;&#32780;&#33719;&#24471;&#23646;&#24615;&#25511;&#21046;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CHRT&#22312;&#19977;&#20010;&#23646;&#24615;&#19978;&#34920;&#29616;&#22343;&#20248;&#20110;&#25152;&#26377;&#22522;&#32447;&#27169;&#22411;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20102;&#22312;&#35821;&#35328;&#36136;&#37327;&#19978;&#30340;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2305.19230</link><description>&lt;p&gt;
&#36890;&#36807;&#38544;&#34255;&#34920;&#31034;&#36716;&#25442;&#23454;&#29616;&#30340;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Controlled Text Generation with Hidden Representation Transformations. (arXiv:2305.19230v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19230
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;CHRT&#65292;&#23427;&#26159;&#19968;&#31181;&#21487;&#25511;&#35821;&#35328;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#34920;&#31034;&#36716;&#25442;&#26469;&#20462;&#25913;&#22522;&#30784;&#27169;&#22411;&#30340;&#38544;&#34255;&#34920;&#31034;&#20174;&#32780;&#33719;&#24471;&#23646;&#24615;&#25511;&#21046;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CHRT&#22312;&#19977;&#20010;&#23646;&#24615;&#19978;&#34920;&#29616;&#22343;&#20248;&#20110;&#25152;&#26377;&#22522;&#32447;&#27169;&#22411;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20102;&#22312;&#35821;&#35328;&#36136;&#37327;&#19978;&#30340;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;CHRT(Control Hidden Representation Transformation)&#65292;&#23427;&#26159;&#19968;&#31181;&#21487;&#25511;&#35821;&#35328;&#29983;&#25104;&#26694;&#26550;&#65292;&#21487;&#20197;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#29305;&#23450;&#23646;&#24615;&#30340;&#25991;&#26412;(&#22914;&#26377;&#27602;&#24615;&#25991;&#26412;)&#12290;CHRT&#36890;&#36807;&#23398;&#20064;&#34920;&#31034;&#36716;&#25442;&#20174;&#32780;&#20462;&#25913;&#22522;&#30784;&#27169;&#22411;&#30340;&#38544;&#34255;&#34920;&#31034;&#26469;&#33719;&#24471;&#23646;&#24615;&#25511;&#21046;&#12290;&#25105;&#20204;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#26469;&#23398;&#20064;&#36825;&#20123;&#34920;&#31034;&#36716;&#25442;&#65292;&#21487;&#20197;&#32467;&#21512;&#20351;&#29992;&#20197;&#33719;&#24471;&#22810;&#23646;&#24615;&#25511;&#21046;&#12290;&#36890;&#36807;&#22312;&#19977;&#20010;&#23646;&#24615;&#19978;&#19982;&#19971;&#20010;&#22522;&#32447;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;CHRT&#30340;&#26377;&#25928;&#24615;&#12290;CHRT&#22312;&#35299;&#27602;&#12289;&#27491;&#38754;&#24773;&#24863;&#24341;&#23548;&#21644;&#25991;&#26412;&#31616;&#21270;&#20219;&#21153;&#20013;&#34920;&#29616;&#22343;&#20248;&#20110;&#25152;&#26377;&#22522;&#32447;&#27169;&#22411;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20102;&#22312;&#35821;&#35328;&#36136;&#37327;&#19978;&#30340;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25512;&#26029;&#24310;&#36831;&#20165;&#27604;&#22522;&#30784;&#27169;&#22411;&#22810;0.01&#31186;&#65292;&#26159;&#26368;&#36866;&#21512;&#39640;&#24615;&#33021;&#29983;&#20135;&#29615;&#22659;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24320;&#25918;&#28304;&#20195;&#30721;&#24182;&#21457;&#24067;&#20102;&#20004;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#36827;&#19968;&#27493;&#25512;&#21160;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose CHRT (Control Hidden Representation Transformation) - a controlled language generation framework that steers large language models to generate text pertaining to certain attributes (such as toxicity). CHRT gains attribute control by modifying the hidden representation of the base model through learned transformations. We employ a contrastive-learning framework to learn these transformations that can be combined to gain multi-attribute control. The effectiveness of CHRT is experimentally shown by comparing it with seven baselines over three attributes. CHRT outperforms all the baselines in the task of detoxification, positive sentiment steering, and text simplification while minimizing the loss in linguistic qualities. Further, our approach has the lowest inference latency of only 0.01 seconds more than the base model, making it the most suitable for high-performance production environments. We open-source our code and release two novel datasets to further propel controlled l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#32034;&#26041;&#27861;CEAA&#65292;&#23558;&#20132;&#21449;&#32534;&#30721;&#20316;&#20026;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#24212;&#29992;&#22312;&#25945;&#32946;&#25991;&#26412;&#20998;&#31867;&#20013;&#65292;&#33021;&#22815;&#35299;&#20915;&#25945;&#32946;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#26377;&#25928;&#25552;&#39640;&#20998;&#31867;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.18977</link><description>&lt;p&gt;
&#20132;&#21449;&#32534;&#30721;&#20316;&#20026;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65306;&#23454;&#29616;&#26377;&#25928;&#30340;&#25945;&#32946;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Cross Encoding as Augmentation: Towards Effective Educational Text Classification. (arXiv:2305.18977v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18977
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#32034;&#26041;&#27861;CEAA&#65292;&#23558;&#20132;&#21449;&#32534;&#30721;&#20316;&#20026;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#24212;&#29992;&#22312;&#25945;&#32946;&#25991;&#26412;&#20998;&#31867;&#20013;&#65292;&#33021;&#22815;&#35299;&#20915;&#25945;&#32946;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#26377;&#25928;&#25552;&#39640;&#20998;&#31867;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25945;&#32946;&#25991;&#26412;&#20998;&#31867;&#65292;&#36890;&#24120;&#31216;&#20026;&#33258;&#21160;&#26631;&#35760;&#65292;&#26159;&#23558;&#30456;&#20851;&#26631;&#35760;&#33258;&#21160;&#20998;&#37197;&#32473;&#25945;&#32946;&#20869;&#23481;&#65288;&#22914;&#38382;&#39064;&#21644;&#25945;&#31185;&#20070;&#65289;&#30340;&#33258;&#21160;&#21270;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#33258;&#21160;&#26631;&#35760;&#38754;&#20020;&#30528;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#36825;&#28304;&#20110;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;1&#65289;&#23427;&#20855;&#26377;&#22823;&#30340;&#26631;&#35760;&#31354;&#38388;&#65307;2&#65289;&#23427;&#26159;&#22810;&#26631;&#31614;&#30340;&#12290;&#23613;&#31649;&#26816;&#32034;&#26041;&#27861;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#30452;&#25509;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#26041;&#38754;&#30340;&#21162;&#21147;&#36739;&#23569;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#32034;&#26041;&#27861;CEAA&#65292;&#20026;&#25945;&#32946;&#25991;&#26412;&#20998;&#31867;&#25552;&#20379;&#26377;&#25928;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#22914;&#19979;&#65306;1&#65289;&#25105;&#20204;&#21033;&#29992;&#26469;&#33258;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#36801;&#31227;&#23398;&#20064;&#65307;2&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#23558;&#20132;&#21449;&#32534;&#30721;&#39118;&#26684;&#30340;&#25991;&#26412;&#24341;&#20837;&#21040;&#21452;&#32534;&#30721;&#22120;&#26550;&#26500;&#20013;&#65292;&#20197;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#25512;&#29702;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#26631;&#31614;&#22330;&#26223;&#21644;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#37117;&#21313;&#20998;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text classification in education, usually called auto-tagging, is the automated process of assigning relevant tags to educational content, such as questions and textbooks. However, auto-tagging suffers from a data scarcity problem, which stems from two major challenges: 1) it possesses a large tag space and 2) it is multi-label. Though a retrieval approach is reportedly good at low-resource scenarios, there have been fewer efforts to directly address the data scarcity problem. To mitigate these issues, here we propose a novel retrieval approach CEAA that provides effective learning in educational text classification. Our main contributions are as follows: 1) we leverage transfer learning from question-answering datasets, and 2) we propose a simple but effective data augmentation method introducing cross-encoder style texts to a bi-encoder architecture for more efficient inference. An extensive set of experiments shows that our proposed method is effective in multi-label scenarios and l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#35270;&#35273;&#35821;&#35328;&#22522;&#20934;&#27979;&#35797;&#26041;&#26696;&#65292;&#23427;&#21487;&#20197;&#21033;&#29992;&#24050;&#26377;&#30340;&#27880;&#37322;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#36890;&#36807;&#25552;&#21462;&#22810;&#31181;&#19981;&#21516;&#30340;&#29305;&#24449;&#26469;&#27979;&#37327;&#27169;&#22411;&#36755;&#20986;&#30340;&#30456;&#20851;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#21457;&#29616;&#65292;CLIP&#27169;&#22411;&#31867;&#20284;&#20110;&#35789;&#34955;&#27169;&#22411;&#65292;&#22312;&#21517;&#35789;&#21644;&#21160;&#35789;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23481;&#26131;&#34987;&#20855;&#20307;&#35789;&#27719;&#22256;&#25200;&#12290;</title><link>http://arxiv.org/abs/2305.18786</link><description>&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#33021;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Scalable Performance Analysis for Vision-Language Models. (arXiv:2305.18786v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#35270;&#35273;&#35821;&#35328;&#22522;&#20934;&#27979;&#35797;&#26041;&#26696;&#65292;&#23427;&#21487;&#20197;&#21033;&#29992;&#24050;&#26377;&#30340;&#27880;&#37322;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#36890;&#36807;&#25552;&#21462;&#22810;&#31181;&#19981;&#21516;&#30340;&#29305;&#24449;&#26469;&#27979;&#37327;&#27169;&#22411;&#36755;&#20986;&#30340;&#30456;&#20851;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#21457;&#29616;&#65292;CLIP&#27169;&#22411;&#31867;&#20284;&#20110;&#35789;&#34955;&#27169;&#22411;&#65292;&#22312;&#21517;&#35789;&#21644;&#21160;&#35789;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23481;&#26131;&#34987;&#20855;&#20307;&#35789;&#27719;&#22256;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290; &#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20123;&#27169;&#22411;&#23398;&#20064;&#30340;&#39640;&#32500;&#31354;&#38388;&#20351;&#24471;&#35782;&#21035;&#35821;&#20041;&#38169;&#35823;&#21464;&#24471;&#22256;&#38590;&#65292;&#22240;&#27492;&#25105;&#20204;&#23545;&#23427;&#20204;&#30340;&#38480;&#21046;&#30693;&#20043;&#29978;&#23569;&#12290; &#26368;&#36817;&#30340;&#24037;&#20316;&#36890;&#36807;&#35774;&#35745;&#39640;&#24230;&#21487;&#25511;&#30340;&#25506;&#27979;&#20219;&#21153;&#22522;&#20934;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#20381;&#36182;&#20110;&#24050;&#27880;&#37322;&#30340;&#22522;&#20934;&#12290; &#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20174;&#35270;&#35273;&#35821;&#35328;&#22522;&#20934;&#20013;&#25552;&#21462;&#22823;&#37327;&#19981;&#21516;&#30340;&#29305;&#24449;&#65292;&#24182;&#27979;&#37327;&#23427;&#20204;&#19982;&#30446;&#26631;&#27169;&#22411;&#30340;&#36755;&#20986;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30830;&#35748;&#20808;&#21069;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#21363;CLIP&#31867;&#20284;&#20110;&#35789;&#34955;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#21517;&#35789;&#21644;&#21160;&#35789;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;; &#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#19968;&#20123;&#26032;&#30340;&#35265;&#35299;&#65292;&#20363;&#22914;CLIP&#34987;&#20855;&#20307;&#30340;&#35789;&#27719;&#22256;&#25200;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#22312;https://github.com/MichiganNLP/Scalable-VLM-Probing&#19978;&#33719;&#24471;&#65292;&#22312;&#20854;&#20182;&#22810;&#27169;&#24335;&#27169;&#22411;&#21644;&#22522;&#20934;&#27979;&#35797;&#20013;&#20063;&#21487;&#20197;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Joint vision-language models have shown great performance over a diverse set of tasks. However, little is known about their limitations, as the high dimensional space learned by these models makes it difficult to identify semantic errors. Recent work has addressed this problem by designing highly controlled probing task benchmarks. Our paper introduces a more scalable solution that relies on already annotated benchmarks. Our method consists of extracting a large set of diverse features from a vision-language benchmark and measuring their correlation with the output of the target model. We confirm previous findings that CLIP behaves like a bag of words model and performs better with nouns and verbs; we also uncover novel insights such as CLIP getting confused by concrete words. Our framework is available at https://github.com/MichiganNLP/Scalable-VLM-Probing and can be used with other multimodal models and benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;&#19987;&#38376;&#21270;&#65292;&#21253;&#25324;&#21160;&#26426;&#12289;&#25361;&#25112;&#12289;&#26041;&#27861;&#35770;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#29305;&#23450;&#39046;&#22495;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#27861;&#65292;&#23545;&#29616;&#26377;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#23450;&#21046;&#25216;&#26415;&#36827;&#34892;&#20102;&#35814;&#32454;&#27604;&#36739;&#65292;&#24182;&#24191;&#27867;&#35752;&#35770;&#20102;&#36825;&#19968;&#39046;&#22495;&#20013;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.18703</link><description>&lt;p&gt;
&#36229;&#36234;&#19968;&#20010;&#27169;&#22411;&#36866;&#29992;&#20110;&#25152;&#26377;&#39046;&#22495;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;&#19987;&#38376;&#21270;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Beyond One-Model-Fits-All: A Survey of Domain Specialization for Large Language Models. (arXiv:2305.18703v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;&#19987;&#38376;&#21270;&#65292;&#21253;&#25324;&#21160;&#26426;&#12289;&#25361;&#25112;&#12289;&#26041;&#27861;&#35770;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#29305;&#23450;&#39046;&#22495;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#27861;&#65292;&#23545;&#29616;&#26377;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#23450;&#21046;&#25216;&#26415;&#36827;&#34892;&#20102;&#35814;&#32454;&#27604;&#36739;&#65292;&#24182;&#24191;&#27867;&#35752;&#35770;&#20102;&#36825;&#19968;&#39046;&#22495;&#20013;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#22823;&#22823;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#20026;&#24191;&#27867;&#24212;&#29992;&#25552;&#20379;&#20102;&#39640;&#24230;&#23454;&#29992;&#12289;&#20219;&#21153;&#26080;&#20851;&#30340;&#22522;&#30784;&#12290;LLMs &#20316;&#20026;&#36890;&#29992;&#20219;&#21153;&#27714;&#35299;&#22120;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20419;&#20351;&#20154;&#20204;&#23558;&#20854;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#65292;&#22914;&#21307;&#30103;&#20445;&#20581;&#12289;&#37329;&#34701;&#21644;&#25945;&#32946;&#65292;&#24182;&#23558;&#20854;&#29992;&#20316;&#21161;&#25163;&#29978;&#33267;&#26367;&#20195;&#29305;&#23450;&#39046;&#22495;&#30340;&#19987;&#23478;&#21644;&#24037;&#20855;&#12290;&#20294;&#26159;&#65292;&#23558;LLMs&#30452;&#25509;&#24212;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#20013;&#30340;&#22797;&#26434;&#38382;&#39064;&#20250;&#36935;&#21040;&#35768;&#22810;&#22256;&#38590;&#65292;&#21253;&#25324;&#39046;&#22495;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#12289;&#39046;&#22495;&#30693;&#35782;&#30340;&#22797;&#26434;&#24615;&#12289;&#39046;&#22495;&#30446;&#26631;&#30340;&#29420;&#29305;&#24615;&#20197;&#21450;&#32422;&#26463;&#30340;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#31181;&#24046;&#36317;&#65292;&#26368;&#36817;&#20960;&#24180;&#36827;&#34892;&#20102;&#24613;&#21095;&#22686;&#21152;&#30340;&#30740;&#31350;&#21644;&#23454;&#36341;&#33268;&#21147;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;&#19987;&#38376;&#21270;&#65292;&#28982;&#32780;&#36825;&#26041;&#38754;&#30340;&#30740;&#31350;&#23578;&#26410;&#34987;&#31995;&#32479;&#22320;&#24635;&#32467;&#12290;&#22312;&#36825;&#31687;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#23545;LLMs&#30340;&#39046;&#22495;&#19987;&#38376;&#21270;&#36827;&#34892;&#20102;&#20840;&#38754;&#27010;&#36848;&#65292;&#21253;&#25324;&#21160;&#26426;&#12289;&#25361;&#25112;&#12289;&#26041;&#27861;&#35770;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#29305;&#23450;&#39046;&#22495;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#27861;&#65292;&#23545;&#29616;&#26377;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#23450;&#21046;&#25216;&#26415;&#36827;&#34892;&#20102;&#35814;&#32454;&#27604;&#36739;&#65292;&#24182;&#24191;&#27867;&#35752;&#35770;&#20102;&#36825;&#19968;&#39046;&#22495;&#20013;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have significantly advanced the field of natural language processing (NLP), providing a highly useful, task-agnostic foundation for a wide range of applications. The great promise of LLMs as general task solvers motivated people to extend their functionality largely beyond just a ``chatbot'', and use it as an assistant or even replacement for domain experts and tools in specific domains such as healthcare, finance, and education. However, directly applying LLMs to solve sophisticated problems in specific domains meets many hurdles, caused by the heterogeneity of domain data, the sophistication of domain knowledge, the uniqueness of domain objectives, and the diversity of the constraints (e.g., various social norms, cultural conformity, religious beliefs, and ethical standards in the domain applications). To fill such a gap, explosively-increase research, and practices have been conducted in very recent years on the domain specialization of LLMs, which, howe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#22914;&#20309;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#28216;&#25103;&#25151;&#38388;&#65292;&#22312;&#20165;&#26377;&#23569;&#37327;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#29983;&#25104;&#22810;&#36798;37%&#30340;&#21487;&#29609;&#26032;&#39062;&#20851;&#21345;&#65292;&#35813;&#25216;&#26415;&#26377;&#21161;&#20110;&#35299;&#20915;&#21253;&#21547;&#35768;&#22810;&#23616;&#37096;&#21644;&#20840;&#23616;&#32422;&#26463;&#30340;PCG&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.18243</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#23454;&#29992;&#30340;PCG
&lt;/p&gt;
&lt;p&gt;
Practical PCG Through Large Language Models. (arXiv:2305.18243v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18243
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#22914;&#20309;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#28216;&#25103;&#25151;&#38388;&#65292;&#22312;&#20165;&#26377;&#23569;&#37327;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#29983;&#25104;&#22810;&#36798;37%&#30340;&#21487;&#29609;&#26032;&#39062;&#20851;&#21345;&#65292;&#35813;&#25216;&#26415;&#26377;&#21161;&#20110;&#35299;&#20915;&#21253;&#21547;&#35768;&#22810;&#23616;&#37096;&#21644;&#20840;&#23616;&#32422;&#26463;&#30340;PCG&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20043;&#22806;&#30340;&#21508;&#31181;&#39046;&#22495;&#20013;&#38750;&#24120;&#26377;&#29992;&#30340;&#24037;&#20855;&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#22914;&#20309;&#20351;&#29992;LLMs&#20026;&#27491;&#22312;&#24320;&#21457;&#20013;&#30340;&#28216;&#25103;Metavoidal&#29983;&#25104;2D&#28216;&#25103;&#25151;&#38388;&#30340;&#23454;&#29992;&#26041;&#21521;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#21487;&#20197;&#36890;&#36807;&#20154;&#31867;&#21442;&#19982;&#30340;&#24494;&#35843;&#65292;&#21033;&#29992;GPT-3&#30340;&#33021;&#21147;&#65292;&#20165;&#20351;&#29992;60&#20010;&#25163;&#21160;&#35774;&#35745;&#30340;&#25151;&#38388;&#25968;&#25454;&#65292;&#22312;&#22797;&#26434;&#30340;&#28216;&#25103;&#22330;&#26223;&#19979;&#65292;&#29983;&#25104;37%&#30340;&#21487;&#29609;&#26032;&#39062;&#20851;&#21345;&#65292;&#36825;&#26159;&#38024;&#23545;&#23384;&#22312;&#22823;&#37327;&#23616;&#37096;&#21644;&#20840;&#23616;&#32422;&#26463;&#30340;PCG&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have proven to be useful tools in various domains outside of the field of their inception, which was natural language processing. In this study, we provide practical directions on how to use LLMs to generate 2D-game rooms for an under-development game, named Metavoidal. Our technique can harness the power of GPT-3 by Human-in-the-loop fine-tuning which allows our method to create 37% Playable-Novel levels from as scarce data as only 60 hand-designed rooms under a scenario of the non-trivial game, with respect to (Procedural Content Generation) PCG, that has a good amount of local and global constraints.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#29983;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#20250;&#23548;&#33268;&#27169;&#22411;&#19981;&#21487;&#36870;&#30340;&#32570;&#38519;&#24182;&#19988;&#20351;&#24471;&#21407;&#22987;&#20869;&#23481;&#20998;&#24067;&#30340;&#23614;&#37096;&#28040;&#22833;&#65292;&#36825;&#31181;&#25928;&#24212;&#31216;&#20026;&#27169;&#22411;&#25240;&#21472;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#29616;&#35937;&#22312;&#25152;&#26377;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#20013;&#37117;&#23384;&#22312;&#65292;&#24517;&#39035;&#35748;&#30495;&#23545;&#24453;&#12290;</title><link>http://arxiv.org/abs/2305.17493</link><description>&lt;p&gt;
&#36882;&#24402;&#30340;&#35781;&#21650;&#65306;&#20351;&#29992;&#29983;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#20250;&#35753;&#27169;&#22411;&#24536;&#35760;
&lt;/p&gt;
&lt;p&gt;
The Curse of Recursion: Training on Generated Data Makes Models Forget. (arXiv:2305.17493v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17493
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#20250;&#23548;&#33268;&#27169;&#22411;&#19981;&#21487;&#36870;&#30340;&#32570;&#38519;&#24182;&#19988;&#20351;&#24471;&#21407;&#22987;&#20869;&#23481;&#20998;&#24067;&#30340;&#23614;&#37096;&#28040;&#22833;&#65292;&#36825;&#31181;&#25928;&#24212;&#31216;&#20026;&#27169;&#22411;&#25240;&#21472;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#29616;&#35937;&#22312;&#25152;&#26377;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#20013;&#37117;&#23384;&#22312;&#65292;&#24517;&#39035;&#35748;&#30495;&#23545;&#24453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31283;&#23450;&#25193;&#25955;&#25216;&#26415;&#38761;&#21629;&#24615;&#22320;&#25913;&#21464;&#20102;&#20174;&#25551;&#36848;&#24615;&#25991;&#26412;&#20013;&#29983;&#25104;&#22270;&#20687;&#30340;&#26041;&#27861;&#12290;GPT-2&#12289;GPT-3(.5)&#21644;GPT-4&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#24778;&#20154;&#12290;ChatGPT&#23558;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#24341;&#20837;&#20102;&#22823;&#20247;&#35270;&#37326;&#12290;&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#19981;&#21487;&#36991;&#20813;&#24182;&#23558;&#24443;&#24213;&#25913;&#21464;&#22312;&#32447;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#25972;&#20010;&#29983;&#24577;&#31995;&#32479;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#26410;&#26469;&#21487;&#33021;&#21457;&#29983;&#30340;&#20107;&#24773;&#12290;&#24403;LLMs&#21344;&#25454;&#20102;&#22312;&#32447;&#35821;&#35328;&#30340;&#22823;&#37096;&#20998;&#26102;&#65292;GPT-{n}&#20250;&#21457;&#29983;&#20160;&#20040;&#65311;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#35757;&#32451;&#20013;&#20351;&#29992;&#27169;&#22411;&#29983;&#25104;&#30340;&#20869;&#23481;&#20250;&#23548;&#33268;&#25152;&#24471;&#27169;&#22411;&#20013;&#19981;&#21487;&#36870;&#32570;&#38519;&#65292;&#21407;&#22987;&#20869;&#23481;&#20998;&#24067;&#30340;&#23614;&#37096;&#28040;&#22833;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#25928;&#24212;&#31216;&#20026;&#27169;&#22411;&#25240;&#21472;&#65292;&#24182;&#26174;&#31034;&#23427;&#21487;&#20197;&#21457;&#29983;&#22312;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#12289;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#21644;LLMs&#20013;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#29616;&#35937;&#32972;&#21518;&#30340;&#29702;&#35770;&#30452;&#35273;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;&#29616;&#35937;&#22312;&#25152;&#26377;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#26222;&#36941;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22914;&#26524;&#25105;&#20204;&#35201;&#22312;&#23454;&#36341;&#20013;&#20351;&#29992;&#29983;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#23601;&#24517;&#39035;&#35748;&#30495;&#23545;&#24453;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stable Diffusion revolutionised image creation from descriptive text. GPT-2, GPT-3(.5) and GPT-4 demonstrated astonishing performance across a variety of language tasks. ChatGPT introduced such language models to the general public. It is now clear that large language models (LLMs) are here to stay, and will bring about drastic change in the whole ecosystem of online text and images. In this paper we consider what the future might hold. What will happen to GPT-{n} once LLMs contribute much of the language found online? We find that use of model-generated content in training causes irreversible defects in the resulting models, where tails of the original content distribution disappear. We refer to this effect as Model Collapse and show that it can occur in Variational Autoencoders, Gaussian Mixture Models and LLMs. We build theoretical intuition behind the phenomenon and portray its ubiquity amongst all learned generative models. We demonstrate that it has to be taken seriously if we ar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#20174;&#23398;&#26415;&#35770;&#25991;&#20013;&#25552;&#28860;&#25991;&#26412;&#20998;&#31867;&#21644;&#23545;&#35937;&#35782;&#21035;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.17401</link><description>&lt;p&gt;
&#19968;&#31181;&#20174;&#23398;&#26415;&#35770;&#25991;&#20013;&#25552;&#28860;&#25991;&#26412;&#20998;&#31867;&#21644;&#23545;&#35937;&#35782;&#21035;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework For Refining Text Classification and Object Recognition from Academic Articles. (arXiv:2305.17401v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#20174;&#23398;&#26415;&#35770;&#25991;&#20013;&#25552;&#28860;&#25991;&#26412;&#20998;&#31867;&#21644;&#23545;&#35937;&#35782;&#21035;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20114;&#32852;&#32593;&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#39640;&#25928;&#22320;&#20174;&#22823;&#37327;&#23398;&#26415;&#35770;&#25991;&#20013;&#25552;&#21462;&#29305;&#23450;&#20449;&#24687;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#36890;&#24120;&#29992;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#25366;&#25496;&#23398;&#26415;&#35770;&#25991;&#30340;&#25968;&#25454;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#33258;&#21160;&#20174;&#22797;&#26434;&#30340;&#38750;&#32467;&#26500;&#21270;&#24067;&#23616;&#25991;&#26723;&#20013;&#25552;&#21462;&#29305;&#23450;&#27169;&#24335;&#12290;&#24403;&#21069;&#30340;&#23398;&#26415;&#35770;&#25991;&#25968;&#25454;&#25366;&#25496;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;&#65288;RB&#65289;&#25110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#38656;&#35201;&#32534;&#20889;&#22797;&#26434;&#25490;&#29256;&#35770;&#25991;&#30340;&#39640;&#26114;&#25104;&#26412;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20165;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#23545;&#25991;&#31456;&#20013;&#22797;&#26434;&#20869;&#23481;&#31867;&#22411;&#36827;&#34892;&#27880;&#37322;&#24037;&#20316;&#65292;&#36825;&#21487;&#33021;&#25104;&#26412;&#39640;&#26114;&#12290;&#27492;&#22806;&#65292;&#20165;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21487;&#33021;&#20250;&#23548;&#33268;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#23481;&#26131;&#35782;&#21035;&#30340;&#27169;&#24335;&#34987;&#38169;&#35823;&#25552;&#21462;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#20174;&#20998;&#26512;&#25351;&#23450;&#33879;&#20316;&#20013;&#20351;&#29992;&#30340;&#26631;&#20934;&#24067;&#23616;&#21644;&#25490;&#29256;&#35282;&#24230;&#20986;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the widespread use of the internet, it has become increasingly crucial to extract specific information from vast amounts of academic articles efficiently. Data mining techniques are generally employed to solve this issue. However, data mining for academic articles is challenging since it requires automatically extracting specific patterns in complex and unstructured layout documents. Current data mining methods for academic articles employ rule-based(RB) or machine learning(ML) approaches. However, using rule-based methods incurs a high coding cost for complex typesetting articles. On the other hand, simply using machine learning methods requires annotation work for complex content types within the paper, which can be costly. Furthermore, only using machine learning can lead to cases where patterns easily recognized by rule-based methods are mistakenly extracted. To overcome these issues, from the perspective of analyzing the standard layout and typesetting used in the specified p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23567;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#30446;&#26631;&#30340;&#39044;&#35757;&#32451;&#23545;&#24615;&#33021;&#30340;&#25552;&#39640;&#20316;&#29992;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#36824;&#21457;&#29616;&#20102;&#35745;&#31639;&#25104;&#26412;&#19982;&#27169;&#22411;&#25928;&#26524;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.17266</link><description>&lt;p&gt;
&#32437;&#35272;&#35821;&#35328;&#27169;&#22411;&#65306;&#32553;&#20943;&#35268;&#27169;&#21518;&#30340;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Honey, I Shrunk the Language: Language Model Behavior at Reduced Scale. (arXiv:2305.17266v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17266
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23567;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#30446;&#26631;&#30340;&#39044;&#35757;&#32451;&#23545;&#24615;&#33021;&#30340;&#25552;&#39640;&#20316;&#29992;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#36824;&#21457;&#29616;&#20102;&#35745;&#31639;&#25104;&#26412;&#19982;&#27169;&#22411;&#25928;&#26524;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35821;&#35328;&#27169;&#22411;&#30340;&#35268;&#27169;&#24613;&#21095;&#22686;&#38271;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#33021;&#21147;&#20063;&#38543;&#30528;&#35268;&#27169;&#30340;&#25193;&#22823;&#32780;&#24471;&#21040;&#20102;&#25552;&#39640;&#12290;&#22823;&#37096;&#20998;&#26368;&#36817;&#30340;&#35268;&#27169;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#39640;&#35745;&#31639;&#37327;&#65292;&#39640;&#21442;&#25968;&#30340;&#29615;&#22659;&#20013;&#65292;&#27809;&#26377;&#22238;&#31572;&#36825;&#20123;&#33021;&#21147;&#20309;&#26102;&#24320;&#22987;&#20986;&#29616;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#38382;&#39064;&#35268;&#27169;&#20943;&#23567;&#30340;&#24773;&#20917;&#19979;&#26159;&#21542;&#21487;&#20197;&#35266;&#23519;&#21040;&#39044;&#35757;&#32451;&#30340;&#25928;&#26524;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#36739;&#23567;&#30340;&#12289;&#32553;&#20943;&#20102;&#35789;&#27719;&#37327;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#21442;&#25968;&#20026;125&#19975;&#30340;&#27169;&#22411;&#20013;&#20351;&#29992;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#30446;&#26631;&#39044;&#35757;&#32451;&#30340;&#22909;&#22788;&#65292;&#24182;&#24314;&#31435;&#20102;&#39044;&#35757;&#32451;&#22256;&#24785;&#21644;&#19979;&#28216;&#24615;&#33021;&#65288;GLUE&#22522;&#20934;&#65289;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#32553;&#23567;&#35268;&#27169;&#30340;&#24433;&#21709;&#65292;&#23558;&#32553;&#25918;&#23450;&#24459;&#25193;&#23637;&#21040;&#20102;&#22823;&#32422;100&#19975;&#20010;&#21442;&#25968;&#30340;&#27169;&#22411;&#20013;&#12290;&#22312;&#36825;&#20010;&#35268;&#27169;&#19979;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#35745;&#31639;-&#26368;&#20248;&#27169;&#22411;&#30340;&#24130;&#24459;&#30772;&#35010;&#65292;&#24182;&#23637;&#31034;&#20102;MLM&#25439;&#22833;&#22312;&#20302;&#20110;22&#19975;&#20159;FLOPs&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#24182;&#19981;&#24179;&#28369;&#22320;&#32553;&#25918;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, language models have drastically grown in size, and the abilities of these models have been shown to improve with scale. The majority of recent scaling laws studies focused on high-compute high-parameter count settings, leaving the question of when these abilities begin to emerge largely unanswered. In this paper, we investigate whether the effects of pre-training can be observed when the problem size is reduced, modeling a smaller, reduced-vocabulary language. We show the benefits of pre-training with masked language modeling (MLM) objective in models as small as 1.25M parameters, and establish a strong correlation between pre-training perplexity and downstream performance (GLUE benchmark). We examine downscaling effects, extending scaling laws to models as small as ~1M parameters. At this scale, we observe a break of the power law for compute-optimal models and show that the MLM loss does not scale smoothly with compute-cost (FLOPs) below $2.2 \times 10^{15}$ FLOPs. 
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#35299;&#30721;&#22120;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#26222;&#36866;&#24615;&#65292;&#34920;&#26126;&#21363;&#20351;&#21482;&#26377;&#21333;&#23618;&#21644;&#21333;&#27880;&#24847;&#21147;&#22836;&#65292;&#20173;&#28982;&#20855;&#26377;&#22270;&#28789;&#23436;&#22791;&#24615;&#65292;&#20854;&#20013;&#21333;&#35789;&#23884;&#20837;&#30340;&#31232;&#30095;&#24615;/&#21487;&#21387;&#32553;&#24615;&#26159;&#24517;&#35201;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2305.17026</link><description>&lt;p&gt;
&#35770;&#35299;&#30721;&#22120;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
On the Computational Power of Decoder-Only Transformer Language Models. (arXiv:2305.17026v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#35299;&#30721;&#22120;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#26222;&#36866;&#24615;&#65292;&#34920;&#26126;&#21363;&#20351;&#21482;&#26377;&#21333;&#23618;&#21644;&#21333;&#27880;&#24847;&#21147;&#22836;&#65292;&#20173;&#28982;&#20855;&#26377;&#22270;&#28789;&#23436;&#22791;&#24615;&#65292;&#20854;&#20013;&#21333;&#35789;&#23884;&#20837;&#30340;&#31232;&#30095;&#24615;/&#21487;&#21387;&#32553;&#24615;&#26159;&#24517;&#35201;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31456;&#23545;&#35299;&#30721;&#22120;Transformer&#27169;&#22411;&#30340;&#35745;&#31639;&#26222;&#36866;&#24615;&#36827;&#34892;&#20102;&#29702;&#35770;&#35780;&#20272;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;Transformer&#27169;&#22411;&#30340;&#29702;&#35770;&#25991;&#29486;&#65292;&#24182;&#34920;&#26126;&#20165;&#20351;&#29992;&#21333;&#23618;&#21644;&#21333;&#27880;&#24847;&#21147;&#22836;&#30340;&#35299;&#30721;&#22120;Transformer&#32467;&#26500;&#65292;&#22312;&#21512;&#29702;&#20551;&#35774;&#19979;&#20855;&#22791;&#22270;&#28789;&#23436;&#22791;&#24615;&#12290;&#20174;&#29702;&#35770;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21333;&#35789;&#23884;&#20837;&#30340;&#31232;&#30095;&#24615;/&#21487;&#21387;&#32553;&#24615;&#26159;&#22270;&#28789;&#23436;&#22791;&#24615;&#25104;&#31435;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article presents a theoretical evaluation of the computational universality of decoder-only transformer models. We extend the theoretical literature on transformer models and show that decoder-only transformer architectures (even with only a single layer and single attention head) are Turing complete under reasonable assumptions. From the theoretical analysis, we show sparsity/compressibility of the word embedding to be a necessary condition for Turing completeness to hold.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;GPT&#21644;NMT&#29983;&#25104;&#32763;&#35793;&#30340;&#25991;&#23383;&#31215;&#26497;&#24230;&#24046;&#24322;&#65292;&#21457;&#29616;GPT&#32763;&#35793;&#26356;&#19981;&#20934;&#30830;&#65292;&#20294;&#22312;MT&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#30456;&#20284;&#25110;&#26356;&#22909;&#30340;&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.16806</link><description>&lt;p&gt;
GPT&#26159;&#21542;&#20250;&#20135;&#29983;&#26356;&#19981;&#20934;&#30830;&#30340;&#32763;&#35793;?
&lt;/p&gt;
&lt;p&gt;
Do GPTs Produce Less Literal Translations?. (arXiv:2305.16806v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;GPT&#21644;NMT&#29983;&#25104;&#32763;&#35793;&#30340;&#25991;&#23383;&#31215;&#26497;&#24230;&#24046;&#24322;&#65292;&#21457;&#29616;GPT&#32763;&#35793;&#26356;&#19981;&#20934;&#30830;&#65292;&#20294;&#22312;MT&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#30456;&#20284;&#25110;&#26356;&#22909;&#30340;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT-3&#65292;&#24050;&#32463;&#25104;&#20026;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#22788;&#29702;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#25110;&#29702;&#35299;&#20219;&#21153;&#12290;&#22312;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#20219;&#21153;&#20013;&#65292;&#24050;&#26377;&#22810;&#39033;&#30740;&#31350;&#25506;&#32034;&#21033;&#29992;few-shot&#25552;&#31034;&#26426;&#21046;&#20174;LLMs&#20013;&#24341;&#20986;&#26356;&#22909;&#30340;&#32763;&#35793;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#30456;&#23545;&#36739;&#23569;&#22320;&#20851;&#27880;&#36825;&#31181;&#32763;&#35793;&#19982;&#26631;&#20934;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#27169;&#22411;&#29983;&#25104;&#32763;&#35793;&#30340;&#36136;&#37327;&#24046;&#24322;&#12290;&#26412;&#30740;&#31350;&#20174;&#25991;&#23383;&#23545;&#40784;&#21644;&#21333;&#35843;&#24615;&#31561;&#26041;&#38754;&#65292;&#27604;&#36739;&#20102;GPT&#21644;NMT&#29983;&#25104;&#32763;&#35793;&#30340;&#25991;&#26412;&#25991;&#23383;&#31215;&#26497;&#24230;&#65292;&#21457;&#29616;GPT&#20174;&#33521;&#35821;&#65288;E-X&#65289;&#32763;&#35793;&#30340;&#25991;&#26412;&#26356;&#19981;&#20934;&#30830;&#65292;&#20294;&#22312;MT&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#30456;&#20284;&#25110;&#26356;&#22909;&#30340;&#20998;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#19968;&#32467;&#26524;&#22312;&#20154;&#24037;&#35780;&#20272;&#20013;&#20063;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;&#21516;&#26102;&#65292;&#24403;&#32763;&#35793;&#21477;&#23376;&#38271;&#24230;&#22686;&#21152;&#26102;&#65292;&#36825;&#31181;&#24046;&#21035;&#23601;&#23588;&#20026;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) such as GPT-3 have emerged as general-purpose language models capable of addressing many natural language generation or understanding tasks. On the task of Machine Translation (MT), multiple works have investigated few-shot prompting mechanisms to elicit better translations from LLMs. However, there has been relatively little investigation on how such translations differ qualitatively from the translations generated by standard Neural Machine Translation (NMT) models. In this work, we investigate these differences in terms of the literalness of translations produced by the two systems. Using literalness measures involving word alignment and monotonicity, we find that translations out of English (E-X) from GPTs tend to be less literal, while exhibiting similar or better scores on MT quality metrics. We demonstrate that this finding is borne out in human evaluations as well. We then show that these differences are especially pronounced when translating senten
&lt;/p&gt;</description></item><item><title>LLM&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;Adaplanner&#33258;&#36866;&#24212;&#25913;&#36827;&#33258;&#24049;&#30340;&#35745;&#21010;&#20197;&#24212;&#23545;&#29615;&#22659;&#21453;&#39304;&#65292;&#20026;&#27492;&#25552;&#20986;&#35745;&#21010;&#20869;&#22806;&#30340;&#25913;&#36827;&#31574;&#30053;&#20197;&#21450;&#20195;&#30721;&#39118;&#26684;&#30340;LLM&#25552;&#31034;&#32467;&#26500;&#21644;&#25216;&#33021;&#21457;&#29616;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.16653</link><description>&lt;p&gt;
AdaPlanner:&#33258;&#36866;&#24212;&#35268;&#21010;&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39304;&#12290; &#65288;arXiv&#65306;2305.16653v1 [cs.CL]&#65289;
&lt;/p&gt;
&lt;p&gt;
AdaPlanner: Adaptive Planning from Feedback with Language Models. (arXiv:2305.16653v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16653
&lt;/p&gt;
&lt;p&gt;
LLM&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;Adaplanner&#33258;&#36866;&#24212;&#25913;&#36827;&#33258;&#24049;&#30340;&#35745;&#21010;&#20197;&#24212;&#23545;&#29615;&#22659;&#21453;&#39304;&#65292;&#20026;&#27492;&#25552;&#20986;&#35745;&#21010;&#20869;&#22806;&#30340;&#25913;&#36827;&#31574;&#30053;&#20197;&#21450;&#20195;&#30721;&#39118;&#26684;&#30340;LLM&#25552;&#31034;&#32467;&#26500;&#21644;&#25216;&#33021;&#21457;&#29616;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#22312;&#24207;&#21015;&#20915;&#31574;&#20219;&#21153;&#20013;&#20316;&#20026;&#33258;&#20027;&#20195;&#29702;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#36138;&#23146;&#22320;&#37319;&#21462;&#34892;&#21160;&#32780;&#27809;&#26377;&#35745;&#21010;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#19981;&#21487;&#36866;&#24212;&#29615;&#22659;&#21453;&#39304;&#30340;&#38745;&#24577;&#35745;&#21010;&#12290;&#22240;&#27492;&#65292;&#38543;&#30528;&#38382;&#39064;&#22797;&#26434;&#24615;&#21644;&#35745;&#21010;&#27700;&#24179;&#30340;&#22686;&#21152;&#65292;LLM&#20195;&#29702;&#30340;&#39034;&#24207;&#20915;&#31574;&#24615;&#33021;&#20250;&#36864;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38381;&#29615;&#26041;&#27861;AdaPlanner&#65292;&#23427;&#20801;&#35768;LLM&#20195;&#29702;&#26681;&#25454;&#29615;&#22659;&#21453;&#39304;&#33258;&#36866;&#24212;&#22320;&#25913;&#36827;&#20854;&#33258;&#21160;&#29983;&#25104;&#30340;&#35745;&#21010;&#12290;&#22312;AdaPlanner&#20013;&#65292;LLM&#20195;&#29702;&#36890;&#36807;&#35745;&#21010;&#20869;&#21644;&#35745;&#21010;&#22806;&#30340;&#25913;&#36827;&#31574;&#30053;&#33258;&#36866;&#24212;&#22320;&#25913;&#36827;&#20854;&#35745;&#21010;&#12290;&#20026;&#20102;&#20943;&#36731;&#24187;&#35273;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20195;&#30721;&#39118;&#26684;&#30340;LLM&#25552;&#31034;&#32467;&#26500;&#65292;&#20419;&#36827;&#20102;&#36328;&#21508;&#31181;&#20219;&#21153;&#65292;&#29615;&#22659;&#21644;&#20195;&#29702;&#33021;&#21147;&#30340;&#35745;&#21010;&#29983;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25216;&#33021;&#21457;&#29616;&#26426;&#21046;&#65292;&#21033;&#29992;&#25104;&#21151;&#30340;&#35745;&#21010;&#20316;&#20026;&#23569;&#37327;&#31034;&#20363;&#65292;&#20351;&#35745;&#21010;&#26356;&#20855;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently demonstrated the potential in acting as autonomous agents for sequential decision-making tasks. However, most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase. We propose a closed-loop approach, AdaPlanner, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback. In AdaPlanner, the LLM agent adaptively refines its plan from feedback with both in-plan and out-of-plan refinement strategies. To mitigate hallucination, we develop a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities. Furthermore, we propose a skill discovery mechanism that leverages successful plans as few-shot exemplars, enabling the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21435;&#22122;&#29942;&#39048;&#21644;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#30340;&#35270;&#39057;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;&#65288;DBF&#65289;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#32454;&#31890;&#24230;&#22320;&#36807;&#28388;&#25481;&#20887;&#20313;&#21644;&#22122;&#22768;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#30041;&#19981;&#21516;&#27169;&#24577;&#20013;&#30340;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#22312;&#22810;&#35821;&#35328;&#35270;&#39057;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14652</link><description>&lt;p&gt;
&#22522;&#20110;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#30340;&#35270;&#39057;&#22810;&#27169;&#24577;&#34701;&#21512;&#21435;&#22122;&#29942;&#39048;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Denoising Bottleneck with Mutual Information Maximization for Video Multimodal Fusion. (arXiv:2305.14652v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21435;&#22122;&#29942;&#39048;&#21644;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#30340;&#35270;&#39057;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;&#65288;DBF&#65289;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#32454;&#31890;&#24230;&#22320;&#36807;&#28388;&#25481;&#20887;&#20313;&#21644;&#22122;&#22768;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#30041;&#19981;&#21516;&#27169;&#24577;&#20013;&#30340;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#22312;&#22810;&#35821;&#35328;&#35270;&#39057;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#22810;&#27169;&#24577;&#34701;&#21512;&#26088;&#22312;&#23558;&#35270;&#39057;&#20013;&#30340;&#22810;&#27169;&#24577;&#20449;&#21495;&#65288;&#22914;&#35270;&#35273;&#12289;&#38899;&#39057;&#21644;&#25991;&#26412;&#65289;&#25972;&#21512;&#65292;&#20197;&#20351;&#29992;&#22810;&#27169;&#24577;&#20869;&#23481;&#36827;&#34892;&#34917;&#20805;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#19982;&#20854;&#20182;&#22270;&#20687;-&#25991;&#26412;&#22810;&#27169;&#24577;&#20219;&#21153;&#19981;&#21516;&#65292;&#35270;&#39057;&#20855;&#26377;&#26356;&#38271;&#30340;&#22810;&#27169;&#24577;&#24207;&#21015;&#65292;&#22312;&#35270;&#35273;&#21644;&#38899;&#39057;&#27169;&#24577;&#20013;&#23384;&#22312;&#26356;&#22810;&#30340;&#20887;&#20313;&#21644;&#22122;&#22768;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32454;&#31890;&#24230;&#35270;&#39057;&#22810;&#27169;&#24577;&#34701;&#21512;&#30340;&#21435;&#22122;&#29942;&#39048;&#34701;&#21512;&#65288;DBF&#65289;&#27169;&#22411;&#12290;&#25105;&#20204;&#19968;&#26041;&#38754;&#37319;&#29992;&#29942;&#39048;&#26426;&#21046;&#65292;&#20197;&#38480;&#21046;&#30340;&#24863;&#21463;&#37326;&#36807;&#28388;&#22122;&#22768;&#21644;&#20887;&#20313;&#20449;&#24687;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#20351;&#29992;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#27169;&#22359;&#26469;&#35843;&#33410;&#36807;&#28388;&#27169;&#22359;&#65292;&#20197;&#20445;&#30041;&#19981;&#21516;&#27169;&#24577;&#20013;&#30340;&#20851;&#38190;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;DBF&#27169;&#22411;&#22312;&#22810;&#35821;&#35328;&#35270;&#39057;&#20998;&#31867;&#20219;&#21153;&#20013;&#26174;&#33879;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video multimodal fusion aims to integrate multimodal signals in videos, such as visual, audio and text, to make a complementary prediction with multiple modalities contents. However, unlike other image-text multimodal tasks, video has longer multimodal sequences with more redundancy and noise in both visual and audio modalities. Prior denoising methods like forget gate are coarse in the granularity of noise filtering. They often suppress the redundant and noisy information at the risk of losing critical information. Therefore, we propose a denoising bottleneck fusion (DBF) model for fine-grained video multimodal fusion. On the one hand, we employ a bottleneck mechanism to filter out noise and redundancy with a restrained receptive field. On the other hand, we use a mutual information maximization module to regulate the filter-out module to preserve key information within different modalities. Our DBF model achieves significant improvement over current state-of-the-art baselines on mult
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#19978;&#28216;&#20559;&#35265;&#12289;&#26679;&#26412;&#20559;&#35265;&#21644;&#36807;&#24230;&#25918;&#22823;&#20559;&#35265;&#19977;&#26041;&#38754;&#20998;&#26512;&#20102;NLP&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#22914;&#20309;&#24433;&#21709;&#25991;&#26412;&#20998;&#31867;&#30340;&#20844;&#24179;&#24615;&#65292;&#24182;&#38024;&#23545;&#36807;&#24230;&#25918;&#22823;&#20559;&#35265;&#36890;&#36807;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#36798;&#21040;&#20844;&#24179;&#20998;&#31867;&#25928;&#26524;&#12290;&#25552;&#20986;&#20102;&#26500;&#24314;&#20844;&#27491;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#30340;&#23454;&#29992;&#25351;&#21335;&#12290;</title><link>http://arxiv.org/abs/2305.12829</link><description>&lt;p&gt;
&#35770;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#20559;&#35265;&#21644;&#20844;&#24179;&#65306;&#22914;&#20309;&#26500;&#24314;&#26356;&#20844;&#27491;&#30340;&#25991;&#26412;&#20998;&#31867;&#65311;
&lt;/p&gt;
&lt;p&gt;
On Bias and Fairness in NLP: How to have a fairer text classification?. (arXiv:2305.12829v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12829
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#19978;&#28216;&#20559;&#35265;&#12289;&#26679;&#26412;&#20559;&#35265;&#21644;&#36807;&#24230;&#25918;&#22823;&#20559;&#35265;&#19977;&#26041;&#38754;&#20998;&#26512;&#20102;NLP&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#22914;&#20309;&#24433;&#21709;&#25991;&#26412;&#20998;&#31867;&#30340;&#20844;&#24179;&#24615;&#65292;&#24182;&#38024;&#23545;&#36807;&#24230;&#25918;&#22823;&#20559;&#35265;&#36890;&#36807;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#36798;&#21040;&#20844;&#24179;&#20998;&#31867;&#25928;&#26524;&#12290;&#25552;&#20986;&#20102;&#26500;&#24314;&#20844;&#27491;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#30340;&#23454;&#29992;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#20998;&#26512;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20013;&#19981;&#21516;&#26469;&#28304;&#30340;&#20559;&#35265;&#65292;&#21363;&#19978;&#28216;&#20559;&#35265;&#12289;&#26679;&#26412;&#20559;&#35265;&#21644;&#36807;&#24230;&#25918;&#22823;&#20559;&#35265;&#65292;&#20197;&#21450;&#23427;&#20204;&#23545;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20351;&#29992;&#19981;&#21516;&#21435;&#20559;&#26041;&#27861;&#28040;&#38500;&#36825;&#20123;&#20559;&#35265;&#23545;&#25991;&#26412;&#20998;&#31867;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#36807;&#24230;&#25918;&#22823;&#20559;&#35265;&#23545;&#25991;&#26412;&#20998;&#31867;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#26368;&#22823;&#12290;&#23558;&#35821;&#35328;&#27169;&#22411;&#22312;&#24179;&#34913;&#19981;&#21516;&#31867;&#21035;&#36523;&#20221;&#32676;&#20307;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#21487;&#20197;&#21435;&#38500;&#36807;&#24230;&#25918;&#22823;&#20559;&#35265;&#65292;&#36827;&#32780;&#26500;&#24314;&#26356;&#20844;&#27491;&#30340;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;&#30740;&#31350;&#21457;&#29616;&#25552;&#20986;&#20102;&#26500;&#24314;&#26356;&#20844;&#27491;&#30340;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#30340;&#23454;&#29992;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we provide a holistic analysis of the different sources of bias, Upstream, Sample and Overampflication biases, in NLP models. We investigate how they impact the fairness of the task of text classification. We also investigate the impact of removing these biases using different debiasing techniques on the fairness of text classification. We found that overamplification bias is the most impactful bias on the fairness of text classification. And that removing overamplification bias by fine-tuning the LM models on a dataset with balanced representations of the different identity groups leads to fairer text classification models. Finally, we build on our findings and introduce practical guidelines on how to have a fairer text classification model.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#33258;&#30417;&#30563;&#21644;&#24369;&#30417;&#30563;&#22810;&#35821;&#35328;&#35821;&#38899;&#39044;&#35757;&#32451;&#22312;&#36866;&#24212;&#26410;&#30693;&#35821;&#35328;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#39044;&#35757;&#32451;&#26399;&#38388;&#27599;&#31181;&#35821;&#35328;&#21450;&#20854;&#35821;&#31995;&#20986;&#29616;&#30340;&#23567;&#26102;&#25968;&#21487;&#20197;&#39044;&#27979;&#27169;&#22411;&#30340;&#27604;&#36739;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.12606</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#21644;&#24369;&#30417;&#30563;&#22810;&#35821;&#35328;&#35821;&#38899;&#39044;&#35757;&#32451;&#22312;&#36866;&#24212;&#26410;&#30693;&#35821;&#35328;&#26041;&#38754;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Comparison of Multilingual Self-Supervised and Weakly-Supervised Speech Pre-Training for Adaptation to Unseen Languages. (arXiv:2305.12606v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12606
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#33258;&#30417;&#30563;&#21644;&#24369;&#30417;&#30563;&#22810;&#35821;&#35328;&#35821;&#38899;&#39044;&#35757;&#32451;&#22312;&#36866;&#24212;&#26410;&#30693;&#35821;&#35328;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#39044;&#35757;&#32451;&#26399;&#38388;&#27599;&#31181;&#35821;&#35328;&#21450;&#20854;&#35821;&#31995;&#20986;&#29616;&#30340;&#23567;&#26102;&#25968;&#21487;&#20197;&#39044;&#27979;&#27169;&#22411;&#30340;&#27604;&#36739;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#19968;&#20123;&#27169;&#22411;(&#22914;XLS-R&#21644;Whisper)&#36890;&#36807;&#23545;&#26469;&#33258;&#22823;&#32422;100&#31181;&#35821;&#35328;&#30340;&#38899;&#39057;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20351;&#24471;&#22810;&#35821;&#35328;&#35821;&#38899;&#25216;&#26415;&#26356;&#21152;&#26131;&#29992;&#12290;&#28982;&#32780;&#65292;&#19990;&#30028;&#19978;&#26377;&#25104;&#21315;&#19978;&#19975;&#31181;&#35821;&#35328;&#65292;&#36866;&#24212;&#26032;&#35821;&#35328;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#20102;&#35299;&#21738;&#31181;&#27169;&#22411;&#26356;&#22909;&#22320;&#36866;&#24212;&#39044;&#35757;&#32451;&#26102;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#12290;&#25105;&#20204;&#22312;13&#31181;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#21644;18&#31181;&#24050;&#35265;&#36807;&#30340;&#35821;&#35328;&#19978;&#24494;&#35843;&#20102;&#20004;&#31181;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#26399;&#38388;&#27599;&#31181;&#35821;&#35328;&#21450;&#20854;&#35821;&#31995;&#20986;&#29616;&#30340;&#23567;&#26102;&#25968;&#21487;&#20197;&#39044;&#27979;&#27169;&#22411;&#30340;&#27604;&#36739;&#32467;&#26524;&#65292;&#23613;&#31649;&#39044;&#35757;&#32451;&#26041;&#27861;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent models such as XLS-R and Whisper have made multilingual speech technologies more accessible by pre-training on audio from around 100 spoken languages each. However, there are thousands of spoken languages worldwide, and adapting to new languages is an important problem. In this work, we aim to understand which model adapts better to languages unseen during pre-training. We fine-tune both models on 13 unseen languages and 18 seen languages. Our results show that the number of hours seen per language and language family during pre-training is predictive of how the models compare, despite the significant differences in the pre-training methods.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;QUEST&#30340;&#26816;&#32034;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21547;&#26377;3357&#20010;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#65292;&#36825;&#20123;&#26597;&#35810;&#20351;&#29992;&#38544;&#24335;&#38598;&#21512;&#25805;&#20316;&#26469;&#28385;&#36275;&#26377;&#36873;&#25321;&#24615;&#30340;&#20449;&#24687;&#38656;&#27714;&#65292;&#36825;&#20010;&#25968;&#25454;&#38598;&#35201;&#27714;&#27169;&#22411;&#21305;&#37197;&#26597;&#35810;&#20013;&#25552;&#21040;&#30340;&#26465;&#20214;&#65292;&#24182;&#27491;&#30830;&#25191;&#34892;&#38598;&#21512;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2305.11694</link><description>&lt;p&gt;
QUEST:&#19968;&#31181;&#21033;&#29992;&#38544;&#24335;&#38598;&#21512;&#25805;&#20316;&#30340;&#23454;&#20307;&#25628;&#32034;&#26597;&#35810;&#26816;&#32034;&#25968;&#25454;&#38598;( arXiv:2305.11694v1[cs.CL])
&lt;/p&gt;
&lt;p&gt;
QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations. (arXiv:2305.11694v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11694
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;QUEST&#30340;&#26816;&#32034;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21547;&#26377;3357&#20010;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#65292;&#36825;&#20123;&#26597;&#35810;&#20351;&#29992;&#38544;&#24335;&#38598;&#21512;&#25805;&#20316;&#26469;&#28385;&#36275;&#26377;&#36873;&#25321;&#24615;&#30340;&#20449;&#24687;&#38656;&#27714;&#65292;&#36825;&#20010;&#25968;&#25454;&#38598;&#35201;&#27714;&#27169;&#22411;&#21305;&#37197;&#26597;&#35810;&#20013;&#25552;&#21040;&#30340;&#26465;&#20214;&#65292;&#24182;&#27491;&#30830;&#25191;&#34892;&#38598;&#21512;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#28385;&#36275;&#26377;&#36873;&#25321;&#24615;&#30340;&#20449;&#24687;&#38656;&#27714;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#38544;&#24335;&#38598;&#21512;&#25805;&#20316;&#65292;&#20363;&#22914;&#20132;&#38598;&#12289;&#24182;&#38598;&#21644;&#24046;&#38598;&#12290;&#30740;&#31350;&#26816;&#32034;&#31995;&#32479;&#28385;&#36275;&#36825;&#31181;&#20449;&#24687;&#38656;&#27714;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;QUEST&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;3357&#20010;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#65292;&#36825;&#20123;&#26597;&#35810;&#20855;&#26377;&#38544;&#24335;&#38598;&#21512;&#25805;&#20316;&#65292;&#21516;&#26102;&#23545;&#24212;&#20110;&#32500;&#22522;&#30334;&#31185;&#25991;&#26723;&#20013;&#30340;&#23454;&#20307;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#35201;&#27714;&#27169;&#22411;&#23558;&#26597;&#35810;&#20013;&#25552;&#21040;&#30340;&#22810;&#20010;&#32422;&#26463;&#26465;&#20214;&#19982;&#25991;&#26723;&#20013;&#30456;&#24212;&#30340;&#35777;&#25454;&#30456;&#21305;&#37197;&#65292;&#24182;&#27491;&#30830;&#25191;&#34892;&#21508;&#31181;&#38598;&#21512;&#25805;&#20316;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#21322;&#33258;&#21160;&#26500;&#24314;&#30340;&#65292;&#20351;&#29992;&#32500;&#22522;&#30334;&#31185;&#31867;&#21035;&#21517;&#31216;&#33258;&#21160;&#32452;&#21512;&#26597;&#35810;&#65292;&#28982;&#21518;&#30001;&#20247;&#21253;&#24037;&#20316;&#32773;&#23545;&#20854;&#36827;&#34892;&#37322;&#20041;&#21644;&#33258;&#28982;&#24230;&#39564;&#35777;&#12290;&#20247;&#21253;&#24037;&#20316;&#32773;&#36824;&#26681;&#25454;&#25991;&#26723;&#35780;&#20272;&#23454;&#20307;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#31361;&#20986;&#26174;&#31034;&#26597;&#35810;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Formulating selective information needs results in queries that implicitly specify set operations, such as intersection, union, and difference. For instance, one might search for "shorebirds that are not sandpipers" or "science-fiction films shot in England". To study the ability of retrieval systems to meet such information needs, we construct QUEST, a dataset of 3357 natural language queries with implicit set operations, that map to a set of entities corresponding to Wikipedia documents. The dataset challenges models to match multiple constraints mentioned in queries with corresponding evidence in documents and correctly perform various set operations. The dataset is constructed semi-automatically using Wikipedia category names. Queries are automatically composed from individual categories, then paraphrased and further validated for naturalness and fluency by crowdworkers. Crowdworkers also assess the relevance of entities based on their documents and highlight attribution of query c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;&#29616;&#26377;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#25991;&#26412;&#30456;&#20851;&#30340;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#36825;&#20123;&#27169;&#22411;&#34429;&#28982;&#22312;&#35821;&#20041;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#20294;&#23545;&#21333;&#20010;&#23383;&#31526;&#24418;&#29366;&#30340;&#24863;&#30693;&#26377;&#38480;&#65292;&#23545;&#22270;&#20687;&#30340;&#32454;&#31890;&#24230;&#29305;&#24449;&#26816;&#27979;&#33021;&#21147;&#20063;&#19981;&#36275;&#65292;&#19981;&#33021;&#19982;&#20256;&#32479;&#39046;&#22495;&#29305;&#23450;&#26041;&#27861;&#30456;&#21305;&#37197;&#65292;&#24182;&#20173;&#38656;&#36827;&#19968;&#27493;&#25506;&#32034;&#23427;&#20204;&#22312;OCR&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.07895</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;OCR&#30340;&#38544;&#31192;&#20043;&#35868;
&lt;/p&gt;
&lt;p&gt;
On the Hidden Mystery of OCR in Large Multimodal Models. (arXiv:2305.07895v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;&#29616;&#26377;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#25991;&#26412;&#30456;&#20851;&#30340;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#36825;&#20123;&#27169;&#22411;&#34429;&#28982;&#22312;&#35821;&#20041;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#20294;&#23545;&#21333;&#20010;&#23383;&#31526;&#24418;&#29366;&#30340;&#24863;&#30693;&#26377;&#38480;&#65292;&#23545;&#22270;&#20687;&#30340;&#32454;&#31890;&#24230;&#29305;&#24449;&#26816;&#27979;&#33021;&#21147;&#20063;&#19981;&#36275;&#65292;&#19981;&#33021;&#19982;&#20256;&#32479;&#39046;&#22495;&#29305;&#23450;&#26041;&#27861;&#30456;&#21305;&#37197;&#65292;&#24182;&#20173;&#38656;&#36827;&#19968;&#27493;&#25506;&#32034;&#23427;&#20204;&#22312;OCR&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#22823;&#22411;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22810;&#27169;&#24577;&#35270;&#35273;&#35821;&#35328;&#23398;&#20064;&#20013;&#25198;&#28436;&#30528;&#25903;&#37197;&#24615;&#30340;&#35282;&#33394;&#12290;&#20851;&#20110;&#23427;&#20204;&#22312;&#25991;&#26412;&#30456;&#20851;&#30340;&#35270;&#35273;&#20219;&#21153;&#20013;&#26377;&#25928;&#24615;&#30340;&#25506;&#32034;&#20173;&#19981;&#22815;&#12290;&#25105;&#20204;&#23545;&#29616;&#26377;&#20844;&#24320;&#21487;&#29992;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#25991;&#26412;&#35782;&#21035;&#12289;&#22522;&#20110;&#25991;&#26412;&#30340;&#35270;&#35273;&#38382;&#31572;&#21644;&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#20248;&#21155;&#21183;&#65292;&#23427;&#20204;&#20027;&#35201;&#20381;&#36182;&#20110;&#35821;&#20041;&#29702;&#35299;&#26469;&#35782;&#21035;&#21333;&#35789;&#65292;&#24182;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#23545;&#21333;&#20010;&#23383;&#31526;&#24418;&#29366;&#30340;&#24863;&#30693;&#12290;&#23427;&#20204;&#23545;&#25991;&#26412;&#38271;&#24230;&#28448;&#19981;&#20851;&#24515;&#65292;&#22312;&#26816;&#27979;&#22270;&#20687;&#30340;&#32454;&#31890;&#24230;&#29305;&#24449;&#26041;&#38754;&#20855;&#26377;&#26377;&#38480;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#24403;&#21069;&#26368;&#24378;&#22823;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20063;&#26080;&#27861;&#19982;&#20256;&#32479;&#25991;&#26412;&#20219;&#21153;&#30340;&#39046;&#22495;&#29305;&#23450;&#26041;&#27861;&#30456;&#21305;&#37197;&#65292;&#24182;&#22312;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#20013;&#38754;&#20020;&#26356;&#22823;&#30340;&#25361;&#25112;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#26412;&#30740;&#31350;&#23637;&#31034;&#30340;&#22522;&#32447;&#32467;&#26524;&#25581;&#31034;&#20102;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;OCR&#30340;&#38544;&#31192;&#20043;&#35868;&#65292;&#20173;&#38656;&#35201;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large models have recently played a dominant role in natural language processing and multimodal vision-language learning. It remains less explored about their efficacy in text-related visual tasks. We conducted a comprehensive study of existing publicly available multimodal models, evaluating their performance in text recognition, text-based visual question answering, and key information extraction. Our findings reveal strengths and weaknesses in these models, which primarily rely on semantic understanding for word recognition and exhibit inferior perception of individual character shapes. They also display indifference towards text length and have limited capabilities in detecting fine-grained features in images. Consequently, these results demonstrate that even the current most powerful large multimodal models cannot match domain-specific methods in traditional text tasks and face greater challenges in more complex tasks. Most importantly, the baseline results showcased in this study
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#25506;&#35752;&#20102;&#20840;&#23616;&#25991;&#26723;&#19978;&#19979;&#25991;&#19982;&#23616;&#37096;&#19978;&#19979;&#25991;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#27491;&#30830;&#26816;&#32034;&#20840;&#23616;&#25991;&#26723;&#19978;&#19979;&#25991;&#23545;&#25552;&#39640;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2305.03132</link><description>&lt;p&gt;
&#20840;&#23616;&#21644;&#23616;&#37096;&#25991;&#33033;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Role of Global and Local Context in Named Entity Recognition. (arXiv:2305.03132v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03132
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#25506;&#35752;&#20102;&#20840;&#23616;&#25991;&#26723;&#19978;&#19979;&#25991;&#19982;&#23616;&#37096;&#19978;&#19979;&#25991;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#27491;&#30830;&#26816;&#32034;&#20840;&#23616;&#25991;&#26723;&#19978;&#19979;&#25991;&#23545;&#25552;&#39640;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#30340;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#30001;&#20110;&#20854;&#33258;&#25105;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22797;&#26434;&#24615;&#65292;&#23427;&#20204;&#19981;&#33021;&#19968;&#27425;&#22788;&#29702;&#38271;&#25991;&#26723;&#65292;&#22240;&#27492;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#26159;&#25353;&#39034;&#24207;&#24212;&#29992;&#30340;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#24184;&#22320;&#21482;&#21253;&#21547;&#23616;&#37096;&#19978;&#19979;&#25991;&#65292;&#24182;&#38459;&#30861;&#20102;&#21033;&#29992;&#20840;&#23616;&#25991;&#26723;&#19978;&#19979;&#25991;&#30340;&#21487;&#33021;&#24615;&#65292;&#36825;&#21487;&#33021;&#20250;&#22952;&#30861;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20840;&#23616;&#25991;&#26723;&#19978;&#19979;&#25991;&#30340;&#24433;&#21709;&#21450;&#20854;&#19982;&#23616;&#37096;&#19978;&#19979;&#25991;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#27491;&#30830;&#26816;&#32034;&#20840;&#23616;&#25991;&#26723;&#19978;&#19979;&#25991;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#22823;&#20110;&#20165;&#21033;&#29992;&#23616;&#37096;&#25991;&#26412;&#12290;&#36825;&#20419;&#20351;&#36827;&#19968;&#27493;&#30740;&#31350;&#22914;&#20309;&#26356;&#22909;&#22320;&#26816;&#32034;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained transformer-based models have recently shown great performance when applied to Named Entity Recognition (NER). As the complexity of their self-attention mechanism prevents them from processing long documents at once, these models are usually applied in a sequential fashion. Such an approach unfortunately only incorporates local context and prevents leveraging global document context in long documents such as novels, which might hinder performance. In this article, we explore the impact of global document context, and its relationships with local context. We find that correctly retrieving global document context has a greater impact on performance than only leveraging local context, prompting for further research on how to better retrieve that context.
&lt;/p&gt;</description></item><item><title>&#22312;&#19981;&#21516;&#25968;&#25454;&#23376;&#32676;&#20307;&#38388;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20869;&#37096;&#20934;&#30830;&#24615;&#21644;&#22806;&#37096;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26159;&#38750;&#32447;&#24615;&#30340;&#65292;&#21576;&#29616;&#20986;&#8220;&#26376;&#20142;&#24418;&#8221;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.02995</link><description>&lt;p&gt;
&#20851;&#20110;&#25968;&#25454;&#23376;&#32676;&#20307;&#38388;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#38750;&#32447;&#24615;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the nonlinear correlation of ML performance between data subpopulations. (arXiv:2305.02995v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02995
&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#21516;&#25968;&#25454;&#23376;&#32676;&#20307;&#38388;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20869;&#37096;&#20934;&#30830;&#24615;&#21644;&#22806;&#37096;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26159;&#38750;&#32447;&#24615;&#30340;&#65292;&#21576;&#29616;&#20986;&#8220;&#26376;&#20142;&#24418;&#8221;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#19979;&#30340;&#24615;&#33021;&#23545;&#20110;&#21487;&#38752;&#30340;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#26368;&#26032;&#30340;&#32463;&#39564;&#30740;&#31350;&#35748;&#20026;&#35757;&#32451;&#25968;&#25454;&#20869;&#37096;&#30340;&#20934;&#30830;&#24615;&#21644;&#26032;&#25968;&#25454;&#22806;&#37096;&#30340;&#20934;&#30830;&#24615;&#20043;&#38388;&#23384;&#22312;&#36817;&#20046;&#23436;&#32654;&#30340;&#32447;&#24615;&#30456;&#20851;&#24615;&#65292;&#20294;&#25105;&#20204;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#35757;&#32451;&#26102;&#26399;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#23376;&#32676;&#20307;&#36716;&#31227;&#19979;&#65292;&#20869;&#37096;&#20934;&#30830;&#24615;&#21644;&#22806;&#37096;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26356;&#20026;&#24494;&#22937;&#65292;&#24182;&#19988;&#22312;&#19978;&#21319;&#38454;&#27573;&#23384;&#22312;&#8220;&#26376;&#20142;&#24418;&#8221;&#30340;&#30456;&#20851;&#24615;&#65288;&#25243;&#29289;&#32447;&#19978;&#21319;&#26354;&#32447;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the performance of machine learning (ML) models across diverse data distributions is critically important for reliable applications. Despite recent empirical studies positing a near-perfect linear correlation between in-distribution (ID) and out-of-distribution (OOD) accuracies, we empirically demonstrate that this correlation is more nuanced under subpopulation shifts. Through rigorous experimentation and analysis across a variety of datasets, models, and training epochs, we demonstrate that OOD performance often has a nonlinear correlation with ID performance in subpopulation shifts. Our findings, which contrast previous studies that have posited a linear correlation in model performance during distribution shifts, reveal a "moon shape" correlation (parabolic uptrend curve) between the test performance on the majority subpopulation and the minority subpopulation. This non-trivial nonlinear correlation holds across model architectures, hyperparameters, training durations
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#32593;&#32476;&#30340;&#22266;&#23450;&#23618;&#21518;&#28155;&#21152;&#23569;&#37327;&#21442;&#25968;&#30340;&#20219;&#21153;&#20248;&#21270;&#36866;&#37197;&#22120;&#26469;&#29420;&#31435;&#22320;&#23398;&#20064;&#27599;&#20010;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#25552;&#39640;DST&#21644;NLG&#27169;&#22359;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.02468</link><description>&lt;p&gt;
&#38754;&#21521;&#20219;&#21153;&#30340;&#31471;&#21040;&#31471;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#20219;&#21153;&#20248;&#21270;&#36866;&#37197;&#22120;
&lt;/p&gt;
&lt;p&gt;
Task-Optimized Adapters for an End-to-End Task-Oriented Dialogue System. (arXiv:2305.02468v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#32593;&#32476;&#30340;&#22266;&#23450;&#23618;&#21518;&#28155;&#21152;&#23569;&#37327;&#21442;&#25968;&#30340;&#20219;&#21153;&#20248;&#21270;&#36866;&#37197;&#22120;&#26469;&#29420;&#31435;&#22320;&#23398;&#20064;&#27599;&#20010;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#25552;&#39640;DST&#21644;NLG&#27169;&#22359;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#26088;&#22312;&#36890;&#36807;&#36319;&#36394;&#23545;&#35805;&#29366;&#24577;&#21644;&#29983;&#25104;&#36866;&#24403;&#30340;&#21709;&#24212;&#26469;&#25191;&#34892;&#29305;&#23450;&#20219;&#21153;&#65292;&#24110;&#21161;&#29992;&#25143;&#23454;&#29616;&#23450;&#20041;&#30340;&#30446;&#26631;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#39044;&#35757;&#32451;&#30340;&#31471;&#21040;&#31471;&#23545;&#35805;&#27169;&#22411;&#22312;&#23545;&#35805;&#31995;&#32479;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20849;&#20139;&#30456;&#21516;&#30340;&#21442;&#25968;&#20197;&#35757;&#32451;&#23545;&#35805;&#31995;&#32479;&#30340;&#20219;&#21153;(NLU&#65292;DST&#65292;NLG)&#65292;&#22240;&#27492;&#27599;&#20010;&#20219;&#21153;&#30340;&#35843;&#35797;&#37117;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#30456;&#36739;&#20110;PLM&#65292;&#23558;&#22823;&#37327;&#21442;&#25968;&#24494;&#35843;&#26469;&#21019;&#24314;&#38754;&#21521;&#20219;&#21153;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#38656;&#35201;&#22823;&#37327;&#30340;&#21162;&#21147;&#65292;&#36825;&#20351;&#24471;&#38750;&#19987;&#23478;&#38590;&#20197;&#22788;&#29702;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25171;&#31639;&#35757;&#32451;&#30456;&#23545;&#36731;&#37327;&#32423;&#21644;&#24555;&#36895;&#30340;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20219;&#21153;&#20248;&#21270;&#36866;&#37197;&#22120;&#30340;&#31471;&#21040;&#31471;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#65292;&#27599;&#20010;&#20219;&#21153;&#29420;&#31435;&#23398;&#20064;&#65292;&#22312;&#39044;&#35757;&#32451;&#32593;&#32476;&#30340;&#22266;&#23450;&#23618;&#20043;&#21518;&#20165;&#28155;&#21152;&#23569;&#37327;&#21442;&#25968;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#25552;&#39640;&#20102;DST&#21644;NLG&#27169;&#22359;&#30340;&#24615;&#33021;&#65292;&#20811;&#26381;&#20102;&#23398;&#20064;&#26354;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Task-Oriented Dialogue (TOD) systems are designed to carry out specific tasks by tracking dialogue states and generating appropriate responses to help users achieve defined goals. Recently, end-to-end dialogue models pre-trained based on large datasets have shown promising performance in the conversational system. However, they share the same parameters to train tasks of the dialogue system (NLU, DST, NLG), so debugging each task is challenging. Also, they require a lot of effort to fine-tune large parameters to create a task-oriented chatbot, making it difficult for non-experts to handle. Therefore, we intend to train relatively lightweight and fast models compared to PLM. In this paper, we propose an End-to-end TOD system with Task-Optimized Adapters which learn independently per task, adding only small number of parameters after fixed layers of pre-trained network. We also enhance the performance of the DST and NLG modules through reinforcement learning, overcoming the learning curv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#22871;&#21517;&#20026; Pythia &#30340;&#24037;&#20855;&#22871;&#20214;&#65292;&#21253;&#21547; 16 &#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#22823;&#23567;&#20174; 70M &#21040; 12B &#21442;&#25968;&#19981;&#31561;&#12290;Pythia &#21487;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#22312;&#22810;&#20010;&#39046;&#22495;&#24320;&#23637;&#30740;&#31350;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#20960;&#20010;&#26032;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#22312;&#35760;&#24518;&#12289;&#24212;&#29992;&#23569;&#37327;&#25968;&#25454;&#26102;&#30340;&#25928;&#26524;&#20197;&#21450;&#20943;&#23569;&#24615;&#21035;&#20559;&#35265;&#31561;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2304.01373</link><description>&lt;p&gt;
Pythia&#65306;&#19968;&#22871;&#29992;&#20110;&#36328;&#35757;&#32451;&#21644;&#25193;&#23637;&#20998;&#26512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling. (arXiv:2304.01373v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#22871;&#21517;&#20026; Pythia &#30340;&#24037;&#20855;&#22871;&#20214;&#65292;&#21253;&#21547; 16 &#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#22823;&#23567;&#20174; 70M &#21040; 12B &#21442;&#25968;&#19981;&#31561;&#12290;Pythia &#21487;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#22312;&#22810;&#20010;&#39046;&#22495;&#24320;&#23637;&#30740;&#31350;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#20960;&#20010;&#26032;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#22312;&#35760;&#24518;&#12289;&#24212;&#29992;&#23569;&#37327;&#25968;&#25454;&#26102;&#30340;&#25928;&#26524;&#20197;&#21450;&#20943;&#23569;&#24615;&#21035;&#20559;&#35265;&#31561;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#22871;&#21517;&#20026;Pythia&#30340;&#24037;&#20855;&#22871;&#20214;&#65292;&#20854;&#20013;&#21253;&#25324;16&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#37117;&#26159;&#22312;&#23436;&#20840;&#30456;&#21516;&#30340;&#39034;&#24207;&#19979;&#20174;&#20844;&#20849;&#25968;&#25454;&#20013;&#35757;&#32451;&#32780;&#26469;&#30340;&#65292;&#22823;&#23567;&#20174;70M&#21040;12B&#21442;&#25968;&#19981;&#31561;&#12290;&#20316;&#32773;&#20844;&#24320;&#20102;&#36825;16&#20010;&#27169;&#22411;&#30340;154&#20010;&#26816;&#26597;&#28857;&#65292;&#24182;&#25552;&#20379;&#20102;&#24037;&#20855;&#20197;&#19979;&#36733;&#21644;&#37325;&#26500;&#27169;&#22411;&#30340;exact training dataloaders&#20197;&#36827;&#34892;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Pythia&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#23545;&#35760;&#24518;&#12289;&#20943;&#23569;&#24615;&#21035;&#20559;&#35265;&#31561;&#26041;&#38754;&#30340;&#26032;&#39062;&#30740;&#31350;&#32467;&#26524;&#65292;&#24182;&#28436;&#31034;&#20102;&#36825;&#31181;&#39640;&#24230;&#25511;&#21046;&#30340;&#35774;&#32622;&#22914;&#20309;&#29992;&#20110;&#33719;&#24471;&#26377;&#20851;&#35821;&#35328;&#27169;&#22411;&#21450;&#20854;&#35757;&#32451;&#21160;&#24577;&#30340;&#26032;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce \textit{Pythia}, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend \textit{Pythia} to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics. Trained models, analysis code, training code, and training data can be found at https://github.com/EleutherAI/pythia.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#33521;&#25991;GPT&#27169;&#22411;&#23545;&#40784;&#21040;&#35199;&#29677;&#29273;&#35821;&#30340;&#23567;&#23553;&#38381;&#39046;&#22495;&#20013;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#22870;&#21169;&#27169;&#22411;&#26469;&#25913;&#36827;&#31572;&#26696;&#30340;&#35299;&#30721;&#21644;&#29983;&#25104;&#65292;&#22312;&#38382;&#31572;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.17649</link><description>&lt;p&gt;
&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#23558;&#19968;&#20010;&#20013;&#31561;&#22823;&#23567;&#30340;&#33521;&#25991;GPT&#27169;&#22411;&#23545;&#40784;&#21040;&#35199;&#29677;&#29273;&#35821;&#30340;&#23567;&#23553;&#38381;&#39046;&#22495;&#20013;
&lt;/p&gt;
&lt;p&gt;
Aligning a medium-size GPT model in English to a small closed domain in Spanish using reinforcement learning. (arXiv:2303.17649v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#33521;&#25991;GPT&#27169;&#22411;&#23545;&#40784;&#21040;&#35199;&#29677;&#29273;&#35821;&#30340;&#23567;&#23553;&#38381;&#39046;&#22495;&#20013;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#22870;&#21169;&#27169;&#22411;&#26469;&#25913;&#36827;&#31572;&#26696;&#30340;&#35299;&#30721;&#21644;&#29983;&#25104;&#65292;&#22312;&#38382;&#31572;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#21407;&#26412;&#29992;&#20110;&#24320;&#25918;&#39046;&#22495;&#30340;&#20013;&#31561;&#22823;&#23567;&#33521;&#25991;GPT&#27169;&#22411;&#65292;&#23545;&#40784;&#21040;&#35199;&#29677;&#29273;&#35821;&#30340;&#23567;&#23553;&#38381;&#39046;&#22495;&#12290;&#35813;&#27169;&#22411;&#34987;&#31934;&#32454;&#35843;&#25972;&#29992;&#20110;&#38382;&#31572;&#20219;&#21153;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#36824;&#38656;&#35201;&#35757;&#32451;&#21644;&#23454;&#29616;&#21478;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#22870;&#21169;&#27169;&#22411;&#65289;&#65292;&#20197;&#35780;&#20998;&#24182;&#30830;&#23450;&#31572;&#26696;&#26159;&#21542;&#36866;&#29992;&#20110;&#32473;&#23450;&#30340;&#38382;&#39064;&#12290;&#35813;&#32452;&#20214;&#26377;&#21161;&#20110;&#25913;&#36827;&#31995;&#32479;&#22238;&#31572;&#30340;&#35299;&#30721;&#21644;&#29983;&#25104;&#12290; BLEU&#21644;perplexity&#31561;&#25968;&#23383;&#24230;&#37327;&#26631;&#20934;&#34987;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#65292;&#21516;&#26102;&#20063;&#20351;&#29992;&#20154;&#31867;&#21028;&#26029;&#26469;&#27604;&#36739;&#35299;&#30721;&#25216;&#26415;&#19982;&#20854;&#20182;&#25216;&#26415;&#12290;&#26368;&#32456;&#65292;&#32467;&#26524;&#25903;&#25345;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#24182;&#30830;&#23450;&#20351;&#29992;&#22870;&#21169;&#27169;&#22411;&#26469;&#23545;&#40784;&#29983;&#25104;&#22238;&#31572;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a methodology to align a medium-sized GPT model, originally trained in English for an open domain, to a small closed domain in Spanish. The application for which the model is finely tuned is the question answering task. To achieve this we also needed to train and implement another neural network (which we called the reward model) that could score and determine whether an answer is appropriate for a given question. This component served to improve the decoding and generation of the answers of the system. Numerical metrics such as BLEU and perplexity were used to evaluate the model, and human judgment was also used to compare the decoding technique with others. Finally, the results favored the proposed method, and it was determined that it is feasible to use a reward model to align the generation of responses.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;cTBL&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#34920;&#26684;&#20013;&#26816;&#32034;&#20449;&#24687;&#65292;&#24182;&#29983;&#25104;&#20855;&#26377;&#26816;&#32034;&#20449;&#24687;&#25903;&#25745;&#30340;&#23545;&#35805;&#21709;&#24212;&#65292;&#20854;&#20013;&#20351;&#29992;&#20102;&#36716;&#25442;&#22120;&#32534;&#30721;&#22120;&#23884;&#20837;&#36827;&#34892;&#27987;&#23494;&#34920;&#26816;&#32034;&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.12024</link><description>&lt;p&gt;
cTBL&#65306;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#23545;&#35805;&#34920;&#26684;
&lt;/p&gt;
&lt;p&gt;
cTBL: Augmenting Large Language Models for Conversational Tables. (arXiv:2303.12024v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12024
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;cTBL&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#34920;&#26684;&#20013;&#26816;&#32034;&#20449;&#24687;&#65292;&#24182;&#29983;&#25104;&#20855;&#26377;&#26816;&#32034;&#20449;&#24687;&#25903;&#25745;&#30340;&#23545;&#35805;&#21709;&#24212;&#65292;&#20854;&#20013;&#20351;&#29992;&#20102;&#36716;&#25442;&#22120;&#32534;&#30721;&#22120;&#23884;&#20837;&#36827;&#34892;&#27987;&#23494;&#34920;&#26816;&#32034;&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23545;&#35805;&#20154;&#24037;&#26234;&#33021;&#20013;&#19968;&#20010;&#24320;&#25918;&#30340;&#25361;&#25112;&#26159;&#22914;&#20309;&#20174;&#25991;&#26412;&#21644;&#38750;&#25991;&#26412;&#26469;&#28304;&#20013;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#36827;&#34892;&#22810;&#36718;&#23545;&#35805;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;Conversation Table (cTBL)&#65292;&#36825;&#26159;&#19968;&#31181;&#19977;&#27493;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#32034;&#34920;&#26684;&#20449;&#24687;&#24182;&#29983;&#25104;&#22522;&#20110;&#26816;&#32034;&#20449;&#24687;&#30340;&#23545;&#35805;&#21709;&#24212;&#12290;cTBL&#20351;&#29992;&#36716;&#25442;&#22120;&#32534;&#30721;&#22120;&#23884;&#20837;&#36827;&#34892;&#27987;&#23494;&#34920;&#26816;&#32034;&#65292;&#24182;&#22312;HyrbiDialogue&#25968;&#25454;&#38598;Top-1&#21644;Top-3&#20934;&#30830;&#24615;&#19978;&#30456;&#23545;&#20110;&#31232;&#30095;&#26816;&#32034;&#25552;&#39640;&#20102;&#26368;&#22810;5%&#12290;&#27492;&#22806;&#65292;cTBL&#20351;&#29992;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#27169;&#22411;&#36827;&#34892;&#34920;&#26684;&#30693;&#35782;&#26816;&#32034;&#65292;&#22312;HyrbiDialogue&#19978;&#20135;&#29983;&#20102;&#26368;&#39640;46%&#30340;ROUGE&#20998;&#25968;&#30456;&#23545;&#25913;&#36827;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#20154;&#24037;&#35780;&#20272;&#21709;&#24212;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
An open challenge in multimodal conversational AI requires augmenting large language models with information from textual and non-textual sources for multi-turn dialogue. To address this problem, this paper introduces Conversational Tables (cTBL), a three-step encoder-decoder approach to retrieve tabular information and generate dialogue responses grounded on the retrieved information. cTBL uses Transformer encoder embeddings for Dense Table Retrieval and obtains up to 5% relative improvement in Top-1 and Top-3 accuracy over sparse retrieval on the HyrbiDialogue dataset. Additionally, cTBL performs tabular knowledge retrieval using both encoder and decoder models, resulting in up to 46% relative improvement in ROUGE scores and better human evaluation for response generation on HyrbiDialogue.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#20013;&#30340;&#37096;&#20998;&#30693;&#35782;&#24211;&#25512;&#29702;&#38382;&#39064;&#65292;&#21457;&#29616;&#30001;&#20110;&#31934;&#24230;&#19979;&#38477;&#23548;&#33268;EL&#24615;&#33021;&#20986;&#29616;&#28798;&#38590;&#24615;&#19979;&#38477;&#65292;&#32780;&#19988;EL&#33539;&#20363;&#26080;&#27861;&#22788;&#29702;&#26080;&#27861;&#38142;&#25509;&#30340;&#25552;&#21450;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#36174;&#22238;&#26041;&#27861;&#26469;&#35299;&#20915;NIL&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.10330</link><description>&lt;p&gt;
&#25506;&#32034;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#20013;&#30340;&#37096;&#20998;&#30693;&#35782;&#24211;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Exploring Partial Knowledge Base Inference in Biomedical Entity Linking. (arXiv:2303.10330v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#20013;&#30340;&#37096;&#20998;&#30693;&#35782;&#24211;&#25512;&#29702;&#38382;&#39064;&#65292;&#21457;&#29616;&#30001;&#20110;&#31934;&#24230;&#19979;&#38477;&#23548;&#33268;EL&#24615;&#33021;&#20986;&#29616;&#28798;&#38590;&#24615;&#19979;&#38477;&#65292;&#32780;&#19988;EL&#33539;&#20363;&#26080;&#27861;&#22788;&#29702;&#26080;&#27861;&#38142;&#25509;&#30340;&#25552;&#21450;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#36174;&#22238;&#26041;&#27861;&#26469;&#35299;&#20915;NIL&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#65288;EL&#65289;&#21253;&#25324;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#21644;&#21629;&#21517;&#23454;&#20307;&#28040;&#27495;&#65288;NED&#65289;&#12290;EL&#27169;&#22411;&#22312;&#30001;&#39044;&#23450;&#20041;&#30340;&#30693;&#35782;&#24211;&#26631;&#35760;&#30340;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#24120;&#35265;&#30340;&#24773;&#20917;&#26159;&#21482;&#26377;&#30693;&#35782;&#24211;&#30340;&#23376;&#38598;&#20013;&#30340;&#23454;&#20307;&#23545;&#21033;&#30410;&#30456;&#20851;&#32773;&#26377;&#20215;&#20540;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#24773;&#20917;&#20026;&#37096;&#20998;&#30693;&#35782;&#24211;&#25512;&#29702;&#65306;&#20351;&#29992;&#19968;&#20010;&#30693;&#35782;&#24211;&#23545;EL&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#27809;&#26377;&#36827;&#19968;&#27493;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#23545;&#20854;&#37096;&#20998;&#36827;&#34892;&#25512;&#29702;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#36825;&#31181;&#23454;&#38469;&#19978;&#38750;&#24120;&#26377;&#20215;&#20540;&#20294;&#26126;&#26174;&#19981;&#22815;&#30740;&#31350;&#30340;&#24773;&#20917;&#30340;&#35814;&#32454;&#23450;&#20041;&#21644;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#19977;&#20010;&#20195;&#34920;&#24615;&#30340;EL&#33539;&#20363;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#37096;&#20998;&#30693;&#35782;&#24211;&#25512;&#29702;&#22522;&#20934;&#65292;&#24182;&#21457;&#29616;&#30001;&#20110;&#22823;&#37327;&#31934;&#24230;&#19979;&#38477;&#23548;&#33268;EL&#24615;&#33021;&#20986;&#29616;&#28798;&#38590;&#24615;&#19979;&#38477;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#36825;&#20123;EL&#33539;&#20363;&#26080;&#27861;&#27491;&#30830;&#22788;&#29702;&#26080;&#27861;&#38142;&#25509;&#30340;&#25552;&#21450;&#65288;NIL&#65289;&#65292;&#22240;&#27492;&#23427;&#20204;&#23545;&#37096;&#20998;&#30693;&#35782;&#24211;&#25512;&#29702;&#19981;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20004;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#36174;&#22238;&#26041;&#27861;&#26469;&#35299;&#20915;NIL&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biomedical entity linking (EL) consists of named entity recognition (NER) and named entity disambiguation (NED). EL models are trained on corpora labeled by a predefined KB. However, it is a common scenario that only entities within a subset of the KB are precious to stakeholders. We name this scenario partial knowledge base inference: training an EL model with one KB and inferring on the part of it without further training. In this work, we give a detailed definition and evaluation procedures for this practically valuable but significantly understudied scenario and evaluate methods from three representative EL paradigms. We construct partial KB inference benchmarks and witness a catastrophic degradation in EL performance due to dramatically precision drop. Our findings reveal these EL paradigms can not correctly handle unlinkable mentions (NIL), so they are not robust to partial KB inference. We also propose two simple-and-effective redemption methods to combat the NIL issue with litt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26597;&#35810;-&#35805;&#35821;&#27880;&#24847;&#21147;&#21644;&#32852;&#21512;&#24314;&#27169;&#30340;&#26597;&#35810;&#24863;&#30693;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#23494;&#38598;&#26816;&#32034;&#27169;&#22359;&#35745;&#31639;&#35805;&#35821;&#32423;&#21035;&#19982;&#26597;&#35810;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#23558;&#26631;&#35760;&#32423;&#21035;&#30340;&#26597;&#35810;&#20851;&#32852;&#24615;&#21644;&#35805;&#35821;&#32423;&#21035;&#30340;&#26597;&#35810;&#20851;&#32852;&#24615;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#29983;&#25104;&#19968;&#20010;&#26356;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#25688;&#35201;&#12290;&#32463;&#36807;&#23545;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#27979;&#35797;&#65292;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;QFMS&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.04487</link><description>&lt;p&gt;
&#22522;&#20110;&#26597;&#35810;-&#35805;&#35821;&#27880;&#24847;&#21147;&#21644;&#32852;&#21512;&#24314;&#27169;&#30340;&#26597;&#35810;&#28966;&#28857;&#20250;&#35758;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Query-Utterance Attention with Joint modeling for Query-Focused Meeting Summarization. (arXiv:2303.04487v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26597;&#35810;-&#35805;&#35821;&#27880;&#24847;&#21147;&#21644;&#32852;&#21512;&#24314;&#27169;&#30340;&#26597;&#35810;&#24863;&#30693;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#23494;&#38598;&#26816;&#32034;&#27169;&#22359;&#35745;&#31639;&#35805;&#35821;&#32423;&#21035;&#19982;&#26597;&#35810;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#23558;&#26631;&#35760;&#32423;&#21035;&#30340;&#26597;&#35810;&#20851;&#32852;&#24615;&#21644;&#35805;&#35821;&#32423;&#21035;&#30340;&#26597;&#35810;&#20851;&#32852;&#24615;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#29983;&#25104;&#19968;&#20010;&#26356;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#25688;&#35201;&#12290;&#32463;&#36807;&#23545;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#27979;&#35797;&#65292;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;QFMS&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26597;&#35810;&#28966;&#28857;&#20250;&#35758;&#25688;&#35201;&#65288;QFMS&#65289;&#26088;&#22312;&#26681;&#25454;&#32473;&#23450;&#30340;&#26597;&#35810;&#65292;&#20174;&#20250;&#35758;&#35760;&#24405;&#20013;&#29983;&#25104;&#25688;&#35201;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#36890;&#24120;&#23558;&#26597;&#35810;&#19982;&#20250;&#35758;&#35760;&#24405;&#25340;&#25509;&#36215;&#26469;&#65292;&#24182;&#20351;&#29992;&#27880;&#24847;&#26426;&#21046;&#38544;&#24335;&#22320;&#23545;&#26631;&#35760;&#32423;&#21035;&#30340;&#26597;&#35810;&#30456;&#20851;&#24615;&#36827;&#34892;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38271;&#26102;&#38388;&#30340;&#20250;&#35758;&#35760;&#24405;&#23548;&#33268;&#20851;&#38190;&#30340;&#26597;&#35810;&#30456;&#20851;&#20449;&#24687;&#34987;&#31232;&#37322;&#65292;&#22240;&#27492;&#21407;&#22987;&#30340;&#22522;&#20110;&#36716;&#25442;&#30340;&#27169;&#22411;&#19981;&#36275;&#20197;&#31361;&#20986;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#20851;&#38190;&#37096;&#20998;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26597;&#35810;-&#35805;&#35821;&#27880;&#24847;&#21147;&#21644;&#32852;&#21512;&#24314;&#27169;&#30340;&#26597;&#35810;&#24863;&#30693;&#26694;&#26550;&#12290;&#23427;&#20351;&#29992;&#23494;&#38598;&#26816;&#32034;&#27169;&#22359;&#35745;&#31639;&#35805;&#35821;&#32423;&#21035;&#19982;&#26597;&#35810;&#30340;&#30456;&#20851;&#24615;&#12290;&#28982;&#21518;&#65292;&#23558;&#26631;&#35760;&#32423;&#21035;&#30340;&#26597;&#35810;&#20851;&#32852;&#24615;&#21644;&#35805;&#35821;&#32423;&#21035;&#30340;&#26597;&#35810;&#20851;&#32852;&#24615;&#32467;&#21512;&#36215;&#26469;&#65292;&#24182;&#36890;&#36807;&#26126;&#30830;&#30340;&#27880;&#24847;&#26426;&#21046;&#25972;&#21512;&#21040;&#29983;&#25104;&#36807;&#31243;&#20013;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#19981;&#21516;&#39063;&#31890;&#24230;&#30340;&#26597;&#35810;&#30456;&#20851;&#24615;&#26377;&#21161;&#20110;&#29983;&#25104;&#19968;&#20010;&#26356;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#25688;&#35201;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;QFMS&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Query-focused meeting summarization (QFMS) aims to generate summaries from meeting transcripts in response to a given query. Previous works typically concatenate the query with meeting transcripts and implicitly model the query relevance only at the token level with attention mechanism. However, due to the dilution of key query-relevant information caused by long meeting transcripts, the original transformer-based model is insufficient to highlight the key parts related to the query. In this paper, we propose a query-aware framework with joint modeling token and utterance based on Query-Utterance Attention. It calculates the utterance-level relevance to the query with a dense retrieval module. Then both token-level query relevance and utterance-level query relevance are combined and incorporated into the generation process with attention mechanism explicitly. We show that the query relevance of different granularities contributes to generating a summary more related to the query. Exper
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#24341;&#23548;&#26041;&#27861;&#65292;&#21517;&#20026;&#21442;&#19982;&#20852;&#22859;&#65292;&#22312;&#25512;&#29702;&#26102;&#38388;&#20869;&#24178;&#39044;&#29983;&#25104;&#36807;&#31243;&#20197;&#25913;&#21892;&#29983;&#25104;&#22270;&#20687;&#30340;&#20449;&#23454;&#24615;&#21644;&#23436;&#25972;&#24615;&#65292;&#24182;&#35299;&#20915;&#20102;&#20256;&#32479;&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#35821;&#20041;&#29983;&#25104;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#22833;&#28789;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2301.13826</link><description>&lt;p&gt;
&#21442;&#19982;&#20852;&#22859;&#65306;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#24341;&#23548;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models. (arXiv:2301.13826v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13826
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#24341;&#23548;&#26041;&#27861;&#65292;&#21517;&#20026;&#21442;&#19982;&#20852;&#22859;&#65292;&#22312;&#25512;&#29702;&#26102;&#38388;&#20869;&#24178;&#39044;&#29983;&#25104;&#36807;&#31243;&#20197;&#25913;&#21892;&#29983;&#25104;&#22270;&#20687;&#30340;&#20449;&#23454;&#24615;&#21644;&#23436;&#25972;&#24615;&#65292;&#24182;&#35299;&#20915;&#20102;&#20256;&#32479;&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#35821;&#20041;&#29983;&#25104;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#22833;&#28789;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#23637;&#31034;&#20102;&#19968;&#31181;&#26080;&#19982;&#20262;&#27604;&#30340;&#36890;&#36807;&#30446;&#26631;&#25991;&#26412;&#25552;&#31034;&#36827;&#34892;&#25351;&#23548;&#29983;&#25104;&#22810;&#31181;&#22810;&#26679;&#21644;&#23500;&#26377;&#21019;&#36896;&#24615;&#30340;&#24418;&#35937;&#30340;&#33021;&#21147;&#12290;&#34429;&#28982;&#20855;&#26377;&#38761;&#21629;&#24615;&#65292;&#20294;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#25193;&#25955;&#27169;&#22411;&#20173;&#21487;&#33021;&#22312;&#29983;&#25104;&#23436;&#20840;&#20256;&#36798;&#32473;&#23450;&#25991;&#26412;&#25552;&#31034;&#20013;&#30340;&#35821;&#20041;&#30340;&#22270;&#20687;&#26041;&#38754;&#22833;&#36133;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20844;&#24320;&#30340;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#35780;&#20272;&#20102;&#28798;&#38590;&#24615;&#24573;&#35270;&#30340;&#23384;&#22312;&#65292;&#21363;&#27169;&#22411;&#26080;&#27861;&#29983;&#25104;&#36755;&#20837;&#25552;&#31034;&#20013;&#30340;&#19968;&#20010;&#25110;&#22810;&#20010;&#20027;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#36824;&#26410;&#33021;&#23558;&#23646;&#24615;&#65288;&#20363;&#22914;&#39068;&#33394;&#65289;&#27491;&#30830;&#32465;&#23450;&#21040;&#20854;&#30456;&#24212;&#30340;&#20027;&#39064;&#19978;&#12290;&#20026;&#20102;&#24110;&#21161;&#20943;&#36731;&#36825;&#20123;&#22833;&#36133;&#24773;&#20917;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20135;&#29983;&#24335;&#35821;&#20041;&#25252;&#29702;&#65288;GSN&#65289;&#30340;&#27010;&#24565;&#65292;&#22312;&#25512;&#29702;&#26102;&#38388;&#20869;&#23547;&#27714;&#24178;&#39044;&#29983;&#25104;&#36807;&#31243;&#20197;&#25913;&#21892;&#25152;&#29983;&#25104;&#22270;&#20687;&#30340;&#20449;&#23454;&#24615;&#12290;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340; GSN &#20844;&#24335;&#65292;&#34987;&#31216;&#20026;&#21442;&#19982;&#20852;&#22859;&#65292;&#25105;&#20204;&#24341;&#23548;&#27169;&#22411;&#25913;&#36827;&#36328;&#27880;&#24847;&#21147;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent text-to-image generative models have demonstrated an unparalleled ability to generate diverse and creative imagery guided by a target text prompt. While revolutionary, current state-of-the-art diffusion models may still fail in generating images that fully convey the semantics in the given text prompt. We analyze the publicly available Stable Diffusion model and assess the existence of catastrophic neglect, where the model fails to generate one or more of the subjects from the input prompt. Moreover, we find that in some cases the model also fails to correctly bind attributes (e.g., colors) to their corresponding subjects. To help mitigate these failure cases, we introduce the concept of Generative Semantic Nursing (GSN), where we seek to intervene in the generative process on the fly during inference time to improve the faithfulness of the generated images. Using an attention-based formulation of GSN, dubbed Attend-and-Excite, we guide the model to refine the cross-attention un
&lt;/p&gt;</description></item><item><title>UPop&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#35270;&#35273;&#35821;&#35328;Transformer&#21387;&#32553;&#26694;&#26550;&#65292;&#37319;&#29992;&#32479;&#19968;&#21644;&#28176;&#36827;&#24335;&#21098;&#26525;&#26041;&#27861;&#65292;&#21487;&#33258;&#21160;&#20998;&#37197;&#21098;&#26525;&#27604;&#29575;&#65292;&#23454;&#29616;&#26356;&#39640;&#30340;&#21387;&#32553;&#27604;&#29575;&#12290;</title><link>http://arxiv.org/abs/2301.13741</link><description>&lt;p&gt;
UPop&#65306;&#29992;&#20110;&#21387;&#32553;&#35270;&#35273;&#35821;&#35328;Transformer&#27169;&#22411;&#30340;&#32479;&#19968;&#21644;&#28176;&#36827;&#24335;&#21098;&#26525;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
UPop: Unified and Progressive Pruning for Compressing Vision-Language Transformers. (arXiv:2301.13741v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13741
&lt;/p&gt;
&lt;p&gt;
UPop&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#35270;&#35273;&#35821;&#35328;Transformer&#21387;&#32553;&#26694;&#26550;&#65292;&#37319;&#29992;&#32479;&#19968;&#21644;&#28176;&#36827;&#24335;&#21098;&#26525;&#26041;&#27861;&#65292;&#21487;&#33258;&#21160;&#20998;&#37197;&#21098;&#26525;&#27604;&#29575;&#65292;&#23454;&#29616;&#26356;&#39640;&#30340;&#21387;&#32553;&#27604;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#21253;&#21547;&#22823;&#37327;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#20854;&#20013;&#35270;&#35273;&#21644;&#35821;&#35328;&#26159;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#20004;&#31181;&#27169;&#24577;&#12290;&#27492;&#22806;&#65292;&#36234;&#26469;&#36234;&#37325;&#30340;&#27169;&#22411;&#65292;&#20363;&#22914;Transformer&#65292;&#24050;&#32463;&#24341;&#36215;&#20102;&#30740;&#31350;&#20154;&#21592;&#23545;&#27169;&#22411;&#21387;&#32553;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#21387;&#32553;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#35270;&#35273;&#35821;&#35328;Transformer&#65292;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UPop&#30340;&#36890;&#29992;&#35270;&#35273;&#35821;&#35328;Transformer&#21387;&#32553;&#26694;&#26550;&#65292;&#23427;&#21253;&#25324;1&#65289;&#22312;&#21407;&#22987;&#27169;&#22411;&#20013;&#22312;&#36830;&#32493;&#20248;&#21270;&#31354;&#38388;&#20013;&#32479;&#19968;&#25628;&#32034;&#22810;&#27169;&#24577;&#23376;&#32593;&#65292;&#20174;&#32780;&#23454;&#29616;&#21487;&#21387;&#32553;&#27169;&#24577;&#21644;&#32467;&#26500;&#20043;&#38388;&#33258;&#21160;&#20998;&#37197;&#21098;&#26525;&#27604;&#29575;&#65307;2&#65289;&#28176;&#36827;&#24335;&#25628;&#32034;&#21644;&#24494;&#35843;&#23376;&#32593;&#65292;&#20174;&#32780;&#20445;&#25345;&#25628;&#32034;&#21644;&#24494;&#35843;&#20043;&#38388;&#30340;&#25910;&#25947;&#65292;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#21387;&#32553;&#27604;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world data contains a vast amount of multimodal information, among which vision and language are the two most representative modalities. Moreover, increasingly heavier models, \textit{e}.\textit{g}., Transformers, have attracted the attention of researchers to model compression. However, how to compress multimodal models, especially vison-language Transformers, is still under-explored. This paper proposes the \textbf{U}nified and \textbf{P}r\textbf{o}gressive \textbf{P}runing (\textbf{\emph{UPop}}) as a universal vison-language Transformer compression framework, which incorporates 1) unifiedly searching multimodal subnets in a continuous optimization space from the original model, which enables automatic assignment of pruning ratios among compressible modalities and structures; 2) progressively searching and retraining the subnet, which maintains convergence between the search and retrain to attain higher compression ratios. Experiments on various tasks, datasets, and model archit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#37319;&#29992;INT4&#26435;&#37325;&#21644;&#28608;&#27963;&#37327;&#21270;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#39640;&#24230;&#20248;&#21270;&#30340;W4A4&#32534;&#30721;&#22120;&#25512;&#26029;&#31649;&#36947;&#65292;&#25903;&#25345;&#19981;&#21516;&#30340;&#37327;&#21270;&#31574;&#30053;&#12290;&#20351;&#29992;W4A4&#21487;&#20197;&#23454;&#29616;&#27169;&#22411;&#22312;&#24310;&#36831;&#26041;&#38754;&#30340;&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2301.12017</link><description>&lt;p&gt;
&#29702;&#35299;Transformer&#27169;&#22411;&#30340;INT4&#37327;&#21270;&#65306;&#24310;&#36831;&#36895;&#24230;&#25552;&#21319;&#12289;&#21487;&#32452;&#21512;&#24615;&#21644;&#25925;&#38556;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
Understanding INT4 Quantization for Transformer Models: Latency Speedup, Composability, and Failure Cases. (arXiv:2301.12017v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#37319;&#29992;INT4&#26435;&#37325;&#21644;&#28608;&#27963;&#37327;&#21270;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#39640;&#24230;&#20248;&#21270;&#30340;W4A4&#32534;&#30721;&#22120;&#25512;&#26029;&#31649;&#36947;&#65292;&#25903;&#25345;&#19981;&#21516;&#30340;&#37327;&#21270;&#31574;&#30053;&#12290;&#20351;&#29992;W4A4&#21487;&#20197;&#23454;&#29616;&#27169;&#22411;&#22312;&#24310;&#36831;&#26041;&#38754;&#30340;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;Transformer&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#65292;&#25552;&#39640;&#20854;&#37096;&#32626;&#25928;&#29575;&#19968;&#30452;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#23613;&#31649;&#26368;&#36817;&#24050;&#32463;&#35777;&#26126;&#20102;INT8&#37327;&#21270;&#22312;&#20943;&#23569;&#20869;&#23384;&#25104;&#26412;&#21644;&#24310;&#36831;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#21516;&#26102;&#36824;&#20445;&#25345;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#25105;&#20204;&#26159;&#21542;&#21487;&#20197;&#21033;&#29992;INT4&#65288;&#21487;&#20197;&#20351;&#30828;&#20214;&#23792;&#20540;&#21534;&#21520;&#37327;&#22686;&#21152;&#19968;&#20493;&#65289;&#26469;&#23454;&#29616;&#36827;&#19968;&#27493;&#30340;&#24310;&#36831;&#25913;&#36827;&#36824;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#37319;&#29992;INT4&#26435;&#37325;&#21644;&#28608;&#27963;&#65288;W4A4&#65289;&#37327;&#21270;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#23545;&#20110;&#20165;&#32534;&#30721;&#22120;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;W4A4&#37327;&#21270;&#24341;&#20837;&#30340;&#20934;&#30830;&#24615;&#38477;&#20302;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#65292;&#20294;&#23545;&#20110;&#20165;&#35299;&#30721;&#22120;&#27169;&#22411;&#32780;&#35328;&#65292;&#20250;&#23548;&#33268;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#19979;&#38477;&#12290;&#20026;&#20102;&#23454;&#29616;&#20351;&#29992;W4A4&#30340;&#24615;&#33021;&#22686;&#30410;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#39640;&#24230;&#20248;&#21270;&#30340;&#31471;&#21040;&#31471;W4A4&#32534;&#30721;&#22120;&#25512;&#26029;&#31649;&#36947;&#65292;&#25903;&#25345;&#19981;&#21516;&#30340;&#37327;&#21270;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;INT4&#31649;&#36947;&#22312;&#38754;&#21521;&#24310;&#36831;&#30340;&#22330;&#26223;&#19979;&#30340;&#36895;&#24230;&#21487;&#20197;&#25552;&#39640;8.5&#20493;&#65292;&#22312;&#20854;&#20182;&#22330;&#26223;&#19979;&#26368;&#22810;&#21487;&#20197;&#25552;&#39640;3&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Improving the deployment efficiency of transformer-based language models has been challenging given their high computation and memory cost. While INT8 quantization has recently been shown to be effective in reducing both the memory cost and latency while preserving model accuracy, it remains unclear whether we can leverage INT4 (which doubles peak hardware throughput) to achieve further latency improvement. In this study, we explore the feasibility of employing INT4 weight and activation (W4A4) quantization for language models. Our findings indicate that W4A4 quantization introduces no to negligible accuracy degradation for encoder-only and encoder-decoder models, but causes a significant accuracy drop for decoder-only models. To materialize the performance gain using W4A4, we develop a highly optimized end-to-end W4A4 encoder inference pipeline supporting different quantization strategies. Our INT4 pipeline is $8.5\times$ faster for latency-oriented scenarios and up to $3\times$ for t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CTC&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#35821;&#38899;&#32763;&#35793;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#23567;&#35821;&#38899;&#21644;&#25991;&#26412;&#27169;&#24577;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25552;&#39640;&#26368;&#32456;&#30340;ST&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.11716</link><description>&lt;p&gt;
&#22522;&#20110;CTC&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#35821;&#38899;&#32763;&#35793;&#39044;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Pre-training for Speech Translation: CTC Meets Optimal Transport. (arXiv:2301.11716v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CTC&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#35821;&#38899;&#32763;&#35793;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#23567;&#35821;&#38899;&#21644;&#25991;&#26412;&#27169;&#24577;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25552;&#39640;&#26368;&#32456;&#30340;ST&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#21040;&#25991;&#26412;&#32763;&#35793;(ST)&#20013;&#30340;&#27169;&#24577;&#24046;&#36317;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#35813;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#35757;&#32451;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#65292;&#26080;&#38656;&#25913;&#21464;ST&#27169;&#22411;&#30340;&#26550;&#26500;&#12290;&#39318;&#20808;&#65292;&#26412;&#25991;&#34920;&#26126;&#36830;&#25509;&#26102;&#24207;&#20998;&#31867;(CTC)&#25439;&#22833;&#21487;&#20197;&#36890;&#36807;&#35774;&#35745;&#26469;&#20943;&#23567;&#27169;&#24577;&#24046;&#36317;&#12290;&#36890;&#36807;&#19982;&#26356;&#24120;&#35265;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#30340;&#23450;&#37327;&#27604;&#36739;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;CTC&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#22987;&#32456;&#23454;&#29616;&#26356;&#22909;&#30340;&#26368;&#32456;ST&#20934;&#30830;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;CTC&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#26032;&#22411;&#39044;&#35757;&#32451;&#26041;&#27861;&#20197;&#36827;&#19968;&#27493;&#20943;&#23567;&#36825;&#31181;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20351;&#29992;CTC&#21644;&#26368;&#20248;&#20256;&#36755;&#36827;&#34892;&#39044;&#35757;&#32451;&#30456;&#23545;&#20110;&#20165;&#20351;&#29992;CTC&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#27809;&#26377;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#22522;&#32447;&#27169;&#22411;&#22343;&#33021;&#22815;&#25552;&#20379;&#25345;&#32493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
The gap between speech and text modalities is a major challenge in speech-to-text translation (ST). Different methods have been proposed to reduce this gap, but most of them require architectural changes in ST training. In this work, we propose to mitigate this issue at the pre-training stage, requiring no change in the ST model. First, we show that the connectionist temporal classification (CTC) loss can reduce the modality gap by design. We provide a quantitative comparison with the more common cross-entropy loss, showing that pre-training with CTC consistently achieves better final ST accuracy. Nevertheless, CTC is only a partial solution and thus, in our second contribution, we propose a novel pre-training method combining CTC and optimal transport to further reduce this gap. Our method pre-trains a Siamese-like model composed of two encoders, one for acoustic inputs and the other for textual inputs, such that they produce representations that are close to each other in the Wassers
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#23398;&#20064;&#33258;&#21160;&#35780;&#20998;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#37197;&#21512;&#21305;&#37197;&#26631;&#26412;&#20316;&#20026;&#19979;&#19968;&#21477;&#39044;&#27979;&#25216;&#26415;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#31185;&#23398;&#25945;&#32946;&#39046;&#22495;&#30340;&#35770;&#35777;&#20219;&#21153;&#65292;&#26497;&#22823;&#22320;&#20943;&#23569;&#20102;&#35757;&#32451;&#25104;&#26412;&#21644;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2301.08771</link><description>&lt;p&gt;
&#21305;&#37197;&#26631;&#26412;&#20316;&#20026;&#19979;&#19968;&#21477;&#39044;&#27979;&#65306;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31185;&#23398;&#25945;&#32946;&#20013;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#33258;&#21160;&#35780;&#20998;
&lt;/p&gt;
&lt;p&gt;
Matching Exemplar as Next Sentence Prediction (MeNSP): Zero-shot Prompt Learning for Automatic Scoring in Science Education. (arXiv:2301.08771v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#23398;&#20064;&#33258;&#21160;&#35780;&#20998;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#37197;&#21512;&#21305;&#37197;&#26631;&#26412;&#20316;&#20026;&#19979;&#19968;&#21477;&#39044;&#27979;&#25216;&#26415;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#31185;&#23398;&#25945;&#32946;&#39046;&#22495;&#30340;&#35770;&#35777;&#20219;&#21153;&#65292;&#26497;&#22823;&#22320;&#20943;&#23569;&#20102;&#35757;&#32451;&#25104;&#26412;&#21644;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#33021;&#22815;&#33258;&#21160;&#35780;&#20998;&#31185;&#23398;&#38382;&#39064;&#30340;&#23398;&#29983;&#20070;&#38754;&#31572;&#26696;&#30340;&#27169;&#22411;&#23545;&#20110;&#31185;&#23398;&#25945;&#32946;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#21644;&#26631;&#35760;&#36275;&#22815;&#30340;&#23398;&#29983;&#31572;&#26696;&#20197;&#35757;&#32451;&#27169;&#22411;&#26159;&#32791;&#26102;&#21644;&#36153;&#29992;&#39640;&#26114;&#30340;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;prompt&#35843;&#25972;&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#31185;&#23398;&#25945;&#32946;&#20013;&#36824;&#27809;&#26377;&#20351;&#29992;&#36807;&#36825;&#31181;&#25552;&#31034;&#26041;&#27861;&#30340;&#30740;&#31350;&#12290;&#30001;&#20110;&#23398;&#29983;&#30340;&#31572;&#26696;&#26159;&#29992;&#33258;&#28982;&#35821;&#35328;&#21576;&#29616;&#30340;&#65292;&#22240;&#27492;&#20351;&#29992;&#25552;&#31034;&#23558;&#35780;&#20998;&#36807;&#31243;&#23545;&#40784;&#20026;&#19979;&#19968;&#21477;&#39044;&#27979;&#20219;&#21153;&#21487;&#20197;&#36339;&#36807;&#26114;&#36149;&#30340;&#35843;&#25972;&#38454;&#27573;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21305;&#37197;&#26631;&#26412;&#20316;&#20026;&#19979;&#19968;&#21477;&#39044;&#27979;&#65288;MeNSP&#65289;&#24320;&#21457;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#33258;&#21160;&#35780;&#20998;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#35757;&#32451;&#26679;&#26412;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#35780;&#20998;&#19977;&#20010;&#31185;&#23398;&#35770;&#35777;&#20219;&#21153;&#20013;&#24212;&#29992;MeNSP&#65292;&#24182;&#21457;&#29616;&#26426;&#22120;-&#20154;&#35780;&#20998;&#30340;&#19968;&#33268;&#24615;&#65292;Cohen&#30340;Kappa&#31995;&#25968;&#22312;0.30&#21040;0.57&#20043;&#38388;&#65292;F1&#20998;&#25968;
&lt;/p&gt;
&lt;p&gt;
Developing models to automatically score students' written responses to science problems is critical for science education. However, collecting and labeling sufficient student responses for training models is time and cost-consuming. Recent studies suggest that pre-trained language models (PLMs) can be adapted to downstream tasks without fine-tuning with prompts. However, no research has employed such a prompt approach in science education. As student responses are presented with natural language, aligning the scoring procedure as the next sentence prediction task using prompts can skip the costly fine-tuning stage. In this study, we developed a zero-shot approach to automatically score student responses via Matching Exemplars as Next Sentence Prediction (MeNSP). This approach employs no training samples. We first apply MeNSP in scoring three assessment tasks of scientific argumentation and found machine-human scoring agreements, Cohen's Kappa ranges from 0.30 to 0.57, and F1 score ran
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36830;&#32493;&#23545;&#27604;&#24494;&#35843;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#20302;&#36164;&#28304;&#20851;&#31995;&#25552;&#21462;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#33268;&#30340;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;RE&#27169;&#22411;&#65292;&#20197;&#21450;&#22810;&#20013;&#24515;&#23545;&#27604;&#25439;&#22833;&#26469;&#20801;&#35768;&#19968;&#20010;&#20851;&#31995;&#24418;&#25104;&#22810;&#20010;&#32858;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#20302;&#36164;&#28304;&#24773;&#20917;&#21644;&#39046;&#22495;&#20013;&#30340;&#20851;&#31995;&#25552;&#21462;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.10823</link><description>&lt;p&gt;
&#36830;&#32493;&#23545;&#27604;&#24494;&#35843;&#25913;&#36827;&#20302;&#36164;&#28304;&#20851;&#31995;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Continual Contrastive Finetuning Improves Low-Resource Relation Extraction. (arXiv:2212.10823v1 [cs.CL] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36830;&#32493;&#23545;&#27604;&#24494;&#35843;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#20302;&#36164;&#28304;&#20851;&#31995;&#25552;&#21462;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#33268;&#30340;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;RE&#27169;&#22411;&#65292;&#20197;&#21450;&#22810;&#20013;&#24515;&#23545;&#27604;&#25439;&#22833;&#26469;&#20801;&#35768;&#19968;&#20010;&#20851;&#31995;&#24418;&#25104;&#22810;&#20010;&#32858;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#20302;&#36164;&#28304;&#24773;&#20917;&#21644;&#39046;&#22495;&#20013;&#30340;&#20851;&#31995;&#25552;&#21462;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25552;&#21462;&#65288;RE&#65289;&#20381;&#36182;&#32467;&#26500;&#21270;&#27880;&#37322;&#35821;&#26009;&#24211;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#23588;&#20854;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#21644;&#39046;&#22495;&#20013;&#65292;&#35813;&#20219;&#21153;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36817;&#26399;&#30740;&#31350;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#26469;&#35299;&#20915;&#20302;&#36164;&#28304;&#30340;RE&#65292;&#20854;&#20013;&#35299;&#20915;&#26041;&#26696;&#21253;&#25324;&#36890;&#36807;RE&#30446;&#26631;&#39044;&#35757;&#32451;&#20851;&#31995;&#23884;&#20837;&#65292;&#24182;&#36890;&#36807;&#20998;&#31867;&#20026;&#22522;&#30784;&#30340;&#30446;&#26631;&#23545;&#26377;&#26631;&#31614;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#30446;&#26631;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#23427;&#38459;&#27490;RE&#27169;&#22411;&#20805;&#20998;&#21033;&#29992;&#39044;&#35757;&#32451;&#34920;&#31034;&#20013;&#30340;&#30693;&#35782;&#12290;&#26412;&#25991;&#26088;&#22312;&#24357;&#21512;&#24046;&#36317;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#19968;&#33268;&#30340;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;RE&#27169;&#22411;&#12290;&#30001;&#20110;&#22312;&#36825;&#31181;&#34920;&#31034;&#23398;&#20064;&#33539;&#24335;&#20013;&#65292;&#19968;&#20010;&#20851;&#31995;&#21487;&#33021;&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#36731;&#26494;&#24418;&#25104;&#22810;&#20010;&#32858;&#31867;&#65292;&#22240;&#27492;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#22810;&#20013;&#24515;&#23545;&#27604;&#25439;&#22833;&#65292;&#20801;&#35768;&#19968;&#20010;&#20851;&#31995;&#24418;&#25104;&#22810;&#20010;&#32858;&#31867;&#20197;&#26356;&#22909;&#22320;&#23545;&#40784;&#39044;&#35757;&#32451;&#12290;&#22312;&#20004;&#20010;&#25991;&#26723;&#20013;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#21644;&#39046;&#22495;&#20013;&#26174;&#30528;&#25552;&#39640;&#20851;&#31995;&#25552;&#21462;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation extraction (RE), which has relied on structurally annotated corpora for model training, has been particularly challenging in low-resource scenarios and domains. Recent literature has tackled low-resource RE by self-supervised learning, where the solution involves pretraining the relation embedding by RE-based objective and finetuning on labeled data by classification-based objective. However, a critical challenge to this approach is the gap in objectives, which prevents the RE model from fully utilizing the knowledge in pretrained representations. In this paper, we aim at bridging the gap and propose to pretrain and finetune the RE model using consistent objectives of contrastive learning. Since in this kind of representation learning paradigm, one relation may easily form multiple clusters in the representation space, we further propose a multi-center contrastive loss that allows one relation to form multiple clusters to better align with pretraining. Experiments on two docum
&lt;/p&gt;</description></item><item><title>ClarifyDelphi&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#31995;&#32479;&#65292;&#33021;&#22815;&#38024;&#23545;&#31038;&#20250;&#25110;&#36947;&#24503;&#24773;&#22659;&#25552;&#20986;&#26368;&#26377;&#20449;&#24687;&#20215;&#20540;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22870;&#21169;&#26426;&#21046;&#26368;&#22823;&#21270;&#22238;&#31572;&#38382;&#39064;&#26102;&#30340;&#36947;&#24503;&#21028;&#26029;&#20998;&#27495;&#12290;</title><link>http://arxiv.org/abs/2212.10409</link><description>&lt;p&gt;
ClarifyDelphi&#65306;&#38024;&#23545;&#31038;&#20250;&#21644;&#36947;&#24503;&#24773;&#22659;&#30340;&#24378;&#21270;&#28548;&#28165;&#38382;&#39064;&#19982;&#20248;&#20808;&#32771;&#34385;&#23545;&#25239;&#30340;&#22870;&#21169;
&lt;/p&gt;
&lt;p&gt;
ClarifyDelphi: Reinforced Clarification Questions with Defeasibility Rewards for Social and Moral Situations. (arXiv:2212.10409v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10409
&lt;/p&gt;
&lt;p&gt;
ClarifyDelphi&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#31995;&#32479;&#65292;&#33021;&#22815;&#38024;&#23545;&#31038;&#20250;&#25110;&#36947;&#24503;&#24773;&#22659;&#25552;&#20986;&#26368;&#26377;&#20449;&#24687;&#20215;&#20540;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22870;&#21169;&#26426;&#21046;&#26368;&#22823;&#21270;&#22238;&#31572;&#38382;&#39064;&#26102;&#30340;&#36947;&#24503;&#21028;&#26029;&#20998;&#27495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#30340;&#37325;&#35201;&#24615;&#19981;&#35328;&#32780;&#21947;&#65292;&#29978;&#33267;&#22312;&#24120;&#35782;&#36947;&#24503;&#25512;&#29702;&#20013;&#20063;&#26159;&#22914;&#27492;&#12290;&#25913;&#21464;&#19978;&#19979;&#25991;&#21487;&#33021;&#20250;&#39072;&#20498;&#19968;&#39033;&#34892;&#20026;&#30340;&#36947;&#24503;&#21028;&#26029;;&#8220;&#23545;&#26379;&#21451;&#25746;&#35854;&#8221;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#26159;&#19981;&#23545;&#30340;&#65292;&#20294;&#22914;&#26524;&#26088;&#22312;&#20445;&#25252;&#20182;&#20204;&#30340;&#29983;&#21629;&#65292;&#23601;&#21487;&#33021;&#26159;&#36947;&#24503;&#19978;&#21487;&#25509;&#21463;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ClarifyDelphi&#65292;&#19968;&#20010;&#20132;&#20114;&#24335;&#31995;&#32479;&#65292;&#23427;&#23398;&#20064;&#25552;&#20986;&#28548;&#28165;&#38382;&#39064;&#65288;&#20363;&#22914;&#65292;&#20320;&#20026;&#20160;&#20040;&#35201;&#23545;&#20320;&#30340;&#26379;&#21451;&#25746;&#35854;&#65311;&#65289;&#20197;&#33719;&#21462;&#31038;&#20250;&#25110;&#36947;&#24503;&#24773;&#22659;&#30340;&#20854;&#20182;&#37325;&#35201;&#20449;&#24687;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20854;&#28508;&#22312;&#31572;&#26696;&#23548;&#33268;&#36947;&#24503;&#21028;&#26029;&#26377;&#25152;&#20998;&#27495;&#30340;&#38382;&#39064;&#26159;&#26368;&#26377;&#20449;&#24687;&#20215;&#20540;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20855;&#26377;&#23545;&#25239;&#24615;&#22870;&#21169;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#22238;&#31572;&#38382;&#39064;&#26102;&#30340;&#36947;&#24503;&#21028;&#26029;&#20998;&#27495;&#12290;&#20154;&#31867;&#35780;&#20272;&#34920;&#26126;&#65292;&#19982;&#31454;&#20105;&#22522;&#32447;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#29983;&#25104;&#30340;&#38382;&#39064;&#26356;&#30456;&#20851;&#12289;&#26356;&#26377;&#20449;&#24687;&#20215;&#20540;&#21644;&#26356;&#20855;&#20248;&#32988;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26368;&#32456;&#21463;&#21040;&#35748;&#30693;&#31185;&#23398;&#30740;&#31350;&#30340;&#21551;&#21457;&#65292;&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#36947;&#24503;&#35748;&#30693;&#30340;&#28789;&#27963;&#24615;&#65288;&#21363;&#33021;&#22815;&#32435;&#20837;&#26032;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#24182;&#30456;&#24212;&#22320;&#20462;&#25913;&#36947;&#24503;&#21028;&#26029;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Context is everything, even in commonsense moral reasoning. Changing contexts can flip the moral judgment of an action; "Lying to a friend" is wrong in general, but may be morally acceptable if it is intended to protect their life.  We present ClarifyDelphi, an interactive system that learns to ask clarification questions (e.g., why did you lie to your friend?) in order to elicit additional salient contexts of a social or moral situation. We posit that questions whose potential answers lead to diverging moral judgments are the most informative. Thus, we propose a reinforcement learning framework with a defeasibility reward that aims to maximize the divergence between moral judgments of hypothetical answers to a question. Human evaluation demonstrates that our system generates more relevant, informative and defeasible questions compared to competitive baselines. Our work is ultimately inspired by studies in cognitive science that have investigated the flexibility in moral cognition (i.e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21512;&#25104;&#20219;&#21153;&#21644;&#25968;&#25454;&#39044;&#35757;&#32451;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20854;&#21487;&#20197;&#32531;&#35299;&#22823;&#35268;&#27169;&#25235;&#21462;&#30340;&#35821;&#26009;&#24211;&#25152;&#23548;&#33268;&#30340;&#27602;&#24615;&#12289;&#20559;&#35265;&#21644;&#27861;&#24459;&#38544;&#24739;&#65292;&#24182;&#35777;&#26126;&#20102;&#21363;&#20351;&#37319;&#29992;&#39640;&#24230;&#28151;&#28102;&#25110;&#32431;&#21512;&#25104;&#25968;&#25454;&#65292;&#39044;&#35757;&#32451;&#20381;&#28982;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2212.09864</link><description>&lt;p&gt;
&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#21512;&#25104;&#39044;&#35757;&#32451;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Synthetic Pre-Training Tasks for Neural Machine Translation. (arXiv:2212.09864v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21512;&#25104;&#20219;&#21153;&#21644;&#25968;&#25454;&#39044;&#35757;&#32451;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20854;&#21487;&#20197;&#32531;&#35299;&#22823;&#35268;&#27169;&#25235;&#21462;&#30340;&#35821;&#26009;&#24211;&#25152;&#23548;&#33268;&#30340;&#27602;&#24615;&#12289;&#20559;&#35265;&#21644;&#27861;&#24459;&#38544;&#24739;&#65292;&#24182;&#35777;&#26126;&#20102;&#21363;&#20351;&#37319;&#29992;&#39640;&#24230;&#28151;&#28102;&#25110;&#32431;&#21512;&#25104;&#25968;&#25454;&#65292;&#39044;&#35757;&#32451;&#20381;&#28982;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#35268;&#27169;&#25235;&#21462;&#30340;&#35821;&#26009;&#24211;&#36827;&#34892;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#33021;&#20250;&#23548;&#33268;&#27602;&#24615;&#21644;&#20559;&#35265;&#31561;&#38382;&#39064;&#65292;&#20197;&#21450;&#29256;&#26435;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;&#37319;&#29992;&#21512;&#25104;&#20219;&#21153;&#21644;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#26159;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#24335;&#65292;&#22240;&#20026;&#27169;&#22411;&#19981;&#20250;&#21560;&#25910;&#20219;&#20309;&#30495;&#23454;&#19990;&#30028;&#20449;&#24687;&#12290;&#26412;&#25991;&#26088;&#22312;&#20102;&#35299;&#20351;&#29992;&#21512;&#25104;&#36164;&#28304;&#26102;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#26377;&#25928;&#24615;&#30340;&#24433;&#21709;&#22240;&#32032;&#65292;&#29305;&#21035;&#26159;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#32972;&#26223;&#19979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#32763;&#35793;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#19981;&#21516;&#27700;&#24179;&#30340;&#35789;&#27719;&#21644;&#32467;&#26500;&#30693;&#35782;&#65292;&#20363;&#22914;&#65306;1&#65289;&#20174;&#22823;&#22411;&#24179;&#34892;&#35821;&#26009;&#24211;&#29983;&#25104;&#28151;&#28102;&#25968;&#25454;&#65292;2&#65289;&#36830;&#25509;&#20174;&#23567;&#22411;&#35789;&#23545;&#40784;&#35821;&#26009;&#24211;&#25552;&#21462;&#30340;&#30701;&#35821;&#23545;&#65292;&#20197;&#21450;3&#65289;&#29983;&#25104;&#19981;&#24102;&#30495;&#23454;&#20154;&#31867;&#35821;&#26009;&#24211;&#30340;&#21512;&#25104;&#24179;&#34892;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#22810;&#31181;&#35821;&#35328;&#23545;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#21363;&#20351;&#23384;&#22312;&#39640;&#27700;&#24179;&#30340;&#28151;&#28102;&#25110;&#32431;&#21512;&#25104;&#25968;&#25454;&#65292;&#20063;&#21487;&#20197;&#23454;&#29616;&#39044;&#35757;&#32451;&#30340;&#25928;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-training models with large crawled corpora can lead to issues such as toxicity and bias, as well as copyright and privacy concerns. A promising way of alleviating such concerns is to conduct pre-training with synthetic tasks and data, since no real-world information is ingested by the model. Our goal in this paper is to understand the factors that contribute to the effectiveness of pre-training models when using synthetic resources, particularly in the context of neural machine translation. We propose several novel approaches to pre-training translation models that involve different levels of lexical and structural knowledge, including: 1) generating obfuscated data from a large parallel corpus 2) concatenating phrase pairs extracted from a small word-aligned corpus, and 3) generating synthetic parallel data without real human language corpora. Our experiments on multiple language pairs reveal that pre-training benefits can be realized even with high levels of obfuscation or purely
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36328;&#35821;&#35328;&#26816;&#32034;&#22686;&#24378;&#25552;&#31034;(PARC)&#31649;&#36947;&#65292;&#22312;&#38646;-shot&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#36890;&#36807;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#20013;&#26816;&#32034;&#20986;&#30340;&#35821;&#20041;&#19978;&#31867;&#20284;&#30340;&#21477;&#23376;&#26469;&#25913;&#21892;&#24615;&#33021;&#65292;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110; fine-tuning &#22522;&#32447;&#65292;&#21516;&#26102;&#19982;&#39640;&#20302;&#36164;&#28304;&#35821;&#35328;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#20197;&#21450;&#20302;&#36164;&#28304;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#25968;&#37327;&#23384;&#22312;&#26174;&#33879;&#27491;&#30456;&#20851;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2212.09651</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#36328;&#35821;&#35328;&#26816;&#32034;&#22686;&#24378;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Cross-Lingual Retrieval Augmented Prompt for Low-Resource Languages. (arXiv:2212.09651v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36328;&#35821;&#35328;&#26816;&#32034;&#22686;&#24378;&#25552;&#31034;(PARC)&#31649;&#36947;&#65292;&#22312;&#38646;-shot&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#36890;&#36807;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#20013;&#26816;&#32034;&#20986;&#30340;&#35821;&#20041;&#19978;&#31867;&#20284;&#30340;&#21477;&#23376;&#26469;&#25913;&#21892;&#24615;&#33021;&#65292;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110; fine-tuning &#22522;&#32447;&#65292;&#21516;&#26102;&#19982;&#39640;&#20302;&#36164;&#28304;&#35821;&#35328;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#20197;&#21450;&#20302;&#36164;&#28304;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#25968;&#37327;&#23384;&#22312;&#26174;&#33879;&#27491;&#30456;&#20851;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(MPLMs)&#22312;&#26368;&#36817;&#30340;&#32463;&#39564;&#36328;&#35821;&#35328;&#36716;&#31227;&#30740;&#31350;&#20013;&#23637;&#29616;&#20102;&#20854;&#24378;&#22823;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36328;&#35821;&#35328;&#26816;&#32034;&#22686;&#24378;&#30340;&#25552;&#31034;(PARC)&#31649;&#36947;&#65292;&#36890;&#36807;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;(HRL)&#20013;&#26816;&#32034;&#20986;&#30340;&#35821;&#20041;&#19978;&#31867;&#20284;&#30340;&#21477;&#23376;&#20316;&#20026;&#25552;&#31034;&#26469;&#25913;&#21892;&#38646;-shot&#20302;&#36164;&#28304;&#35821;&#35328;(LRLs)&#30340;&#24615;&#33021;&#12290;PARC&#36890;&#36807;&#22810;&#35821;&#35328;&#24182;&#34892;&#27979;&#35797;&#38598;&#22312;&#19977;&#20010;&#19979;&#28216;&#20219;&#21153;(&#20108;&#20803;&#24773;&#24863;&#20998;&#31867;&#12289;&#20027;&#39064;&#20998;&#31867;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#26029;)&#19978;&#25552;&#39640;&#20102;&#38646;-shot&#30340;&#24615;&#33021;&#65292;&#35206;&#30422;&#20102;10&#20010;LRLs&#65292;&#28085;&#30422;&#20102;6&#31181;&#35821;&#35328;&#23478;&#26063;&#65292;&#22312;&#26410;&#26631;&#35760;&#30340;&#35774;&#32622;&#20013;&#25552;&#39640;&#20102;(+5.1%)&#65292;&#22312;&#26631;&#35760;&#30340;&#35774;&#32622;&#20013;&#25552;&#39640;&#20102;(+16.3%)&#12290;PARC&#26631;&#35760;&#36824;&#36229;&#36234;&#20102; fine-tuning &#22522;&#32447;3.7%&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36328;&#35821;&#35328;&#36716;&#31227;&#24615;&#33021;&#22312;&#19968;&#26041;&#38754;&#19982;&#39640;&#20302;&#36164;&#28304;&#35821;&#35328;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#20197;&#21450;&#20302;&#36164;&#28304;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#25968;&#37327;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#27491;&#30456;&#20851;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual Pretrained Language Models (MPLMs) have shown their strong multilinguality in recent empirical cross-lingual transfer studies. In this paper, we propose the Prompts Augmented by Retrieval Crosslingually (PARC) pipeline to improve the zero-shot performance on low-resource languages (LRLs) by augmenting the context with semantically similar sentences retrieved from a high-resource language (HRL) as prompts. PARC improves the zero-shot performance on three downstream tasks (binary sentiment classification, topic categorization and natural language inference) with multilingual parallel test sets across 10 LRLs covering 6 language families in both unlabeled settings (+5.1%) and labeled settings (+16.3%). PARC-labeled also outperforms the finetuning baseline by 3.7%. We find a significant positive correlation between cross-lingual transfer performance on one side, and the similarity between the high- and low-resource languages as well as the amount of low-resource pretraining da
&lt;/p&gt;</description></item><item><title>DuNST&#26159;&#19968;&#31181;&#21452;&#37325;&#22122;&#22768;&#33258;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#21322;&#30417;&#30563;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25200;&#21160;&#29983;&#25104;&#30340;&#20266;&#25991;&#26412;&#65292;&#23558;&#20266;&#25991;&#26412;&#26631;&#35760;&#21644;&#26080;&#26631;&#31614;&#30340;&#20266;&#26631;&#31614;&#32467;&#21512;&#20351;&#29992;&#65292;&#24182;&#19988;&#21487;&#20197;&#32531;&#35299;&#20808;&#21069;&#23398;&#20064;&#21040;&#30340;&#31354;&#38388;&#30340;&#38480;&#21046;&#24615;&#27867;&#21270;&#36793;&#30028;&#12290;</title><link>http://arxiv.org/abs/2212.08724</link><description>&lt;p&gt;
DuNST&#65306;&#21452;&#37325;&#22122;&#22768;&#33258;&#35757;&#32451;&#29992;&#20110;&#21322;&#30417;&#30563;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
DuNST: Dual Noisy Self Training for Semi-Supervised Controllable Text Generation. (arXiv:2212.08724v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08724
&lt;/p&gt;
&lt;p&gt;
DuNST&#26159;&#19968;&#31181;&#21452;&#37325;&#22122;&#22768;&#33258;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#21322;&#30417;&#30563;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25200;&#21160;&#29983;&#25104;&#30340;&#20266;&#25991;&#26412;&#65292;&#23558;&#20266;&#25991;&#26412;&#26631;&#35760;&#21644;&#26080;&#26631;&#31614;&#30340;&#20266;&#26631;&#31614;&#32467;&#21512;&#20351;&#29992;&#65292;&#24182;&#19988;&#21487;&#20197;&#32531;&#35299;&#20808;&#21069;&#23398;&#20064;&#21040;&#30340;&#31354;&#38388;&#30340;&#38480;&#21046;&#24615;&#27867;&#21270;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35821;&#35328;&#29702;&#35299;&#65292;&#33258;&#35757;&#32451;&#65288;ST&#65289;&#36890;&#36807;&#22686;&#21152;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#27425;&#25968;&#26469;&#25193;&#20805;&#26631;&#35760;&#25968;&#25454;&#19981;&#36275;&#30340;&#24773;&#20917;&#65292;&#26377;&#20102;&#36739;&#22823;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#24102;&#23646;&#24615;&#25511;&#21046;&#30340;&#35821;&#35328;&#29983;&#25104;&#20013;&#65292;&#23558;ST&#32435;&#20837;&#20854;&#20013;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#21482;&#33021;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#30340;&#20266;&#25991;&#26412;&#36827;&#34892;&#22686;&#24378;&#30340;&#29983;&#25104;&#27169;&#22411;&#20250;&#36807;&#24230;&#24378;&#35843;&#20808;&#21069;&#23398;&#20064;&#21040;&#30340;&#31354;&#38388;&#65292;&#21463;&#21040;&#21463;&#38480;&#30340;&#27867;&#21270;&#36793;&#30028;&#25152;&#22256;&#25200;&#12290;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;ST&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;DuNST&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;DuNST&#36890;&#36807;&#19968;&#20010;&#20849;&#20139;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26469;&#32852;&#21512;&#29983;&#25104;&#25991;&#26412;&#21644;&#23545;&#24212;&#30340;&#20998;&#31867;&#26631;&#31614;&#65292;&#24182;&#20351;&#29992;&#20004;&#31181;&#28789;&#27963;&#30340;&#22122;&#22768;&#26469;&#25200;&#20081;&#29983;&#25104;&#30340;&#20266;&#25991;&#26412;&#12290;&#36825;&#26679;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#26500;&#24314;&#24182;&#21033;&#29992;&#26469;&#33258;&#32473;&#23450;&#26631;&#31614;&#30340;&#20266;&#25991;&#26412;&#20197;&#21450;&#26469;&#33258;&#21487;&#29992;&#26080;&#26631;&#31614;&#25991;&#26412;&#30340;&#20266;&#26631;&#31614;&#65292;&#22312;ST&#36807;&#31243;&#20013;&#36880;&#28176;&#25913;&#36827;&#12290;&#29702;&#35770;&#19978;&#35777;&#26126;DuNST&#21487;&#20197;&#34987;&#35270;&#20026;&#21521;&#28508;&#22312;&#30495;&#23454;&#25991;&#26412;&#30340;&#25506;&#32034;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-training (ST) has prospered again in language understanding by augmenting the fine-tuning of pre-trained language models when labeled data is insufficient. However, it remains challenging to incorporate ST into attribute-controllable language generation. Augmented by only self-generated pseudo text, generation models over-emphasize exploitation of the previously learned space, suffering from a constrained generalization boundary. We revisit ST and propose a novel method, DuNST to alleviate this problem. DuNST jointly models text generation and classification with a shared Variational AutoEncoder and corrupts the generated pseudo text by two kinds of flexible noise to disturb the space. In this way, our model could construct and utilize both pseudo text from given labels and pseudo labels from available unlabeled text, which are gradually refined during the ST process. We theoretically demonstrate that DuNST can be regarded as enhancing exploration towards the potential real text s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#65292;&#35757;&#32451;Transformer&#27169;&#22411;&#24212;&#29992;&#20110;&#33258;&#22238;&#24402;&#30446;&#26631;&#38382;&#39064;&#26102;&#65292;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#20803;&#23398;&#20064;&#30340;&#24418;&#24335;&#23494;&#20999;&#30456;&#20851;&#65292;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#27169;&#22411;&#30340;&#8220;&#24213;&#23618;&#20248;&#21270;&#31243;&#24207;&#8221;&#30340;&#26426;&#21046;&#65292;&#22312;&#22238;&#24402;&#38382;&#39064;&#30340;&#39046;&#22495;&#20013;&#20174;&#26426;&#26800;&#30340;&#35282;&#24230;&#29702;&#35299;&#20102;Transformers&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#20869;&#37096;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2212.07677</link><description>&lt;p&gt;
Transformer&#27169;&#22411;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23454;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Transformers learn in-context by gradient descent. (arXiv:2212.07677v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#65292;&#35757;&#32451;Transformer&#27169;&#22411;&#24212;&#29992;&#20110;&#33258;&#22238;&#24402;&#30446;&#26631;&#38382;&#39064;&#26102;&#65292;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#20803;&#23398;&#20064;&#30340;&#24418;&#24335;&#23494;&#20999;&#30456;&#20851;&#65292;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#27169;&#22411;&#30340;&#8220;&#24213;&#23618;&#20248;&#21270;&#31243;&#24207;&#8221;&#30340;&#26426;&#21046;&#65292;&#22312;&#22238;&#24402;&#38382;&#39064;&#30340;&#39046;&#22495;&#20013;&#20174;&#26426;&#26800;&#30340;&#35282;&#24230;&#29702;&#35299;&#20102;Transformers&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#20869;&#37096;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;Transformers&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26426;&#21046;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#29702;&#35299;&#65292;&#22823;&#22810;&#21482;&#20572;&#30041;&#22312;&#30452;&#35273;&#19978;&#12290;&#26412;&#25991;&#25552;&#20986;&#65292;&#35757;&#32451;Transformer&#27169;&#22411;&#24212;&#29992;&#20110;&#33258;&#22238;&#24402;&#30446;&#26631;&#38382;&#39064;&#26102;&#65292;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#20803;&#23398;&#20064;&#30340;&#24418;&#24335;&#23494;&#20999;&#30456;&#20851;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#19968;&#20010;&#31616;&#21333;&#30340;&#26435;&#37325;&#26500;&#36896;&#65292;&#35777;&#26126;&#20102;&#30001;&#21333;&#20010;&#32447;&#24615;&#33258;&#27880;&#24847;&#21147;&#23618;&#24341;&#21457;&#30340;&#25968;&#25454;&#36716;&#25442;&#19982;&#30001;&#20855;&#26377;&#22238;&#24402;&#25439;&#22833;&#30340;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#33719;&#24471;&#30340;&#36716;&#25442;&#20855;&#26377;&#31561;&#20215;&#24615;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#24403;&#20165;&#35757;&#32451;&#33258;&#27880;&#24847;&#21147;Transformer&#27169;&#22411;&#36827;&#34892;&#31616;&#21333;&#30340;&#22238;&#24402;&#20219;&#21153;&#26102;&#65292;&#36890;&#36807;GD&#20248;&#21270;&#24471;&#21040;&#30340;&#27169;&#22411;&#19982;&#27169;&#22411;&#26435;&#37325;&#21313;&#20998;&#30456;&#20284;&#65292;&#25110;&#32773;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;GD&#20248;&#21270;&#30340;&#26435;&#37325;&#19982;&#26500;&#36896;&#30340;&#26435;&#37325;&#30456;&#21516;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32463;&#36807;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#26159;&#22914;&#20309;&#22312;&#21069;&#21521;&#20256;&#36882;&#20013;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#27169;&#22411;&#30340;&#8220;&#24213;&#23618;&#20248;&#21270;&#31243;&#24207;&#8221;&#30340;&#12290;&#22312;&#22238;&#24402;&#38382;&#39064;&#30340;&#39046;&#22495;&#20013;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20174;&#26426;&#26800;&#30340;&#35282;&#24230;&#29702;&#35299;Transformers&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#20869;&#37096;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
At present, the mechanisms of in-context learning in Transformers are not well understood and remain mostly an intuition. In this paper, we suggest that training Transformers on auto-regressive objectives is closely related to gradient-based meta-learning formulations. We start by providing a simple weight construction that shows the equivalence of data transformations induced by 1) a single linear self-attention layer and by 2) gradient-descent (GD) on a regression loss. Motivated by that construction, we show empirically that when training self-attention-only Transformers on simple regression tasks either the models learned by GD and Transformers show great similarity or, remarkably, the weights found by optimization match the construction. Thus we show how trained Transformers become mesa-optimizers i.e. learn models by gradient descent in their forward pass. This allows us, at least in the domain of regression problems, to mechanistically understand the inner workings of in-context
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#33258;&#30417;&#30563;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;MT4SSL&#65292;&#36890;&#36807;&#21516;&#26102;&#20351;&#29992;K&#22343;&#20540;&#31639;&#27861;&#20316;&#20026;&#31163;&#32447;&#30446;&#26631;&#25552;&#21462;&#22120;&#21644;&#27809;&#26377;&#26799;&#24230;&#30340;&#25945;&#24072;&#32593;&#32476;&#20316;&#20026;&#22312;&#32447;&#30446;&#26631;&#25552;&#21462;&#22120;&#65292;&#21462;&#24471;&#20102;&#27604;&#20197;&#21069;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;&#21516;&#26102;&#65292;&#20351;&#29992;&#31163;&#32447;&#21644;&#22312;&#32447;&#30446;&#26631;&#25552;&#21462;&#22120;&#21487;&#20197;&#24471;&#21040;&#26356;&#22909;&#30340;&#25910;&#25947;&#24615;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#19978;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26377;&#21069;&#36884;&#30340;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2211.07321</link><description>&lt;p&gt;
MT4SSL&#65306;&#36890;&#36807;&#38598;&#25104;&#22810;&#20010;&#30446;&#26631;&#26469;&#25552;&#21319;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MT4SSL: Boosting Self-Supervised Speech Representation Learning by Integrating Multiple Targets. (arXiv:2211.07321v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#33258;&#30417;&#30563;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;MT4SSL&#65292;&#36890;&#36807;&#21516;&#26102;&#20351;&#29992;K&#22343;&#20540;&#31639;&#27861;&#20316;&#20026;&#31163;&#32447;&#30446;&#26631;&#25552;&#21462;&#22120;&#21644;&#27809;&#26377;&#26799;&#24230;&#30340;&#25945;&#24072;&#32593;&#32476;&#20316;&#20026;&#22312;&#32447;&#30446;&#26631;&#25552;&#21462;&#22120;&#65292;&#21462;&#24471;&#20102;&#27604;&#20197;&#21069;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;&#21516;&#26102;&#65292;&#20351;&#29992;&#31163;&#32447;&#21644;&#22312;&#32447;&#30446;&#26631;&#25552;&#21462;&#22120;&#21487;&#20197;&#24471;&#21040;&#26356;&#22909;&#30340;&#25910;&#25947;&#24615;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#19978;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26377;&#21069;&#36884;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#35757;&#32451;&#30446;&#26631;&#30340;&#33719;&#21462;&#26041;&#24335;&#25552;&#20986;&#20102;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#30340;&#26032;&#35270;&#35282;&#65292;&#24182;&#23558;&#30446;&#26631;&#25552;&#21462;&#22120;&#27010;&#25324;&#20026;&#31163;&#32447;&#30446;&#26631;&#25552;&#21462;&#22120;&#21644;&#22312;&#32447;&#30446;&#26631;&#25552;&#21462;&#22120;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;MT4SSL&#65292;&#23427;&#20351;&#29992;K&#22343;&#20540;&#31639;&#27861;&#20316;&#20026;&#31163;&#32447;&#30446;&#26631;&#25552;&#21462;&#22120;&#65292;&#20351;&#29992;&#27809;&#26377;&#26799;&#24230;&#30340;&#25945;&#24072;&#32593;&#32476;&#20316;&#20026;&#22312;&#32447;&#30446;&#26631;&#25552;&#21462;&#22120;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;LibriSpeech&#22522;&#20934;&#27979;&#35797;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#19982;&#20351;&#29992;&#26356;&#23569;&#25968;&#25454;&#30340;&#26368;&#20339;&#27169;&#22411;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#21516;&#26102;&#20351;&#29992;&#31163;&#32447;&#21644;&#22312;&#32447;&#30446;&#26631;&#25552;&#21462;&#22120;&#21487;&#20197;&#24471;&#21040;&#26356;&#22909;&#30340;&#25910;&#25947;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35748;&#20026;&#20174;&#25105;&#20204;&#30340;&#35282;&#24230;&#36827;&#34892;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#19978;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we provide a new perspective on self-supervised speech models from how the training targets are obtained. We generalize the targets extractor into Offline Targets Extractor (Off-TE) and Online Targets Extractor (On-TE). Based on this, we propose a new multi-tasking learning framework for self-supervised learning, MT4SSL, which stands for Boosting Self-Supervised Speech Representation Learning by Integrating Multiple Targets. MT4SSL uses the K-means algorithm as an Off-TE and a teacher network without gradients as an On-TE, respectively. Our model outperforms previous SSL methods by nontrivial margins on the LibriSpeech benchmark, and is comparable to or even better than the best-performing models with fewer data. Furthermore, we find that using both Off-TE and On-TE results in better convergence in the pre-training phase. With both effectiveness and efficiency, we think doing multi-task learning on self-supervised speech models from our perspective is a promising trend.
&lt;/p&gt;</description></item><item><title>RARR&#26159;&#19968;&#20010;&#21487;&#20197;&#23545;&#19981;&#30830;&#23450;&#20449;&#24687;&#36827;&#34892;&#30740;&#31350;&#21644;&#20462;&#35746;&#30340;&#31995;&#32479;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#25214;&#21040;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#36755;&#20986;&#30340;&#24402;&#22240;&#24182;&#20462;&#27491;&#19981;&#25903;&#25345;&#30340;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2210.08726</link><description>&lt;p&gt;
RARR: &#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#21644;&#20462;&#27491;&#20854;&#36755;&#20986;&#32467;&#26524;&#20013;&#30340;&#19981;&#30830;&#23450;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
RARR: Researching and Revising What Language Models Say, Using Language Models. (arXiv:2210.08726v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08726
&lt;/p&gt;
&lt;p&gt;
RARR&#26159;&#19968;&#20010;&#21487;&#20197;&#23545;&#19981;&#30830;&#23450;&#20449;&#24687;&#36827;&#34892;&#30740;&#31350;&#21644;&#20462;&#35746;&#30340;&#31995;&#32479;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#25214;&#21040;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#36755;&#20986;&#30340;&#24402;&#22240;&#24182;&#20462;&#27491;&#19981;&#25903;&#25345;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#35832;&#22914;&#23569;&#26679;&#26412;&#23398;&#20064;&#12289;&#38382;&#31572;&#12289;&#25512;&#29702;&#21644;&#23545;&#35805;&#31561;&#35768;&#22810;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26377;&#26102;&#20250;&#29983;&#25104;&#26080;&#25903;&#25345;&#25110;&#35823;&#23548;&#24615;&#30340;&#20869;&#23481;&#12290;&#30001;&#20110;&#22823;&#22810;&#25968;&#35821;&#35328;&#27169;&#22411;&#27809;&#26377;&#20219;&#20309;&#20869;&#32622;&#30340;&#24402;&#22240;&#22806;&#37096;&#35777;&#25454;&#30340;&#26426;&#21046;&#65292;&#29992;&#25143;&#24456;&#38590;&#30830;&#23450;&#23427;&#20204;&#30340;&#36755;&#20986;&#26159;&#21542;&#21487;&#38752;&#12290;&#20026;&#20102;&#22312;&#20445;&#30041;&#26368;&#26032;&#19968;&#20195;&#27169;&#22411;&#30340;&#25152;&#26377;&#24378;&#22823;&#20248;&#21183;&#30340;&#21516;&#26102;&#23454;&#29616;&#24402;&#22240;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; RARR (&#20351;&#29992;&#30740;&#31350;&#21644;&#20462;&#35746;&#36827;&#34892;&#25913;&#36827;&#24402;&#22240;)&#31995;&#32479;&#65292;&#23427; 1) &#33258;&#21160;&#25214;&#21040;&#20219;&#20309;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#36755;&#20986;&#30340;&#24402;&#22240;&#24182; 2) &#22312;&#23613;&#21487;&#33021;&#20445;&#30041;&#21407;&#22987;&#36755;&#20986;&#30340;&#21516;&#26102;&#65292;&#20462;&#27491;&#19981;&#25903;&#25345;&#30340;&#20869;&#23481;&#12290;&#24403;&#24212;&#29992;&#20110;&#20960;&#20010;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#36755;&#20986;&#20219;&#21153;&#19978;&#30340;&#32467;&#26524;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;RARR&#22312;&#26174;&#33879;&#25552;&#39640;&#24402;&#22240;&#29575;&#30340;&#21516;&#26102;&#65292;&#27604;&#20197;&#21069;&#25506;&#32034;&#30340;&#32534;&#36753;&#27169;&#22411;&#26356;&#33021;&#20445;&#30041;&#21407;&#22987;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) now excel at many tasks such as few-shot learning, question answering, reasoning, and dialog. However, they sometimes generate unsupported or misleading content. A user cannot easily determine whether their outputs are trustworthy or not, because most LMs do not have any built-in mechanism for attribution to external evidence. To enable attribution while still preserving all the powerful advantages of recent generation models, we propose RARR (Retrofit Attribution using Research and Revision), a system that 1) automatically finds attribution for the output of any text generation model and 2) post-edits the output to fix unsupported content while preserving the original output as much as possible. When applied to the output of several state-of-the-art LMs on a diverse set of generation tasks, we find that RARR significantly improves attribution while otherwise preserving the original input to a much greater degree than previously explored edit models. Furthermore, 
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#20165;&#20351;&#29992;&#25552;&#21450;&#27880;&#37322;&#30340;&#20849;&#25351;&#28040;&#35299;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#26377;&#25928;&#25552;&#39640;&#27169;&#22411;&#25928;&#26524;&#32780;&#19981;&#22686;&#21152;&#26102;&#38388;&#19982;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2210.07602</link><description>&lt;p&gt;
&#20165;&#25552;&#21450;&#27880;&#37322;&#21363;&#21487;&#26377;&#25928;&#36827;&#34892;&#20849;&#25351;&#28040;&#35299;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Mention Annotations Alone Enable Efficient Domain Adaptation for Coreference Resolution. (arXiv:2210.07602v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07602
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#20165;&#20351;&#29992;&#25552;&#21450;&#27880;&#37322;&#30340;&#20849;&#25351;&#28040;&#35299;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#26377;&#25928;&#25552;&#39640;&#27169;&#22411;&#25928;&#26524;&#32780;&#19981;&#22686;&#21152;&#26102;&#38388;&#19982;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#20849;&#25351;&#28040;&#35299;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23558;&#36825;&#20123;&#27169;&#22411;&#36716;&#31227;&#21040;&#21253;&#21547;&#26032;&#30340;&#36229;&#20986;&#35789;&#27719;&#34920;&#33539;&#22260;&#21450;&#38656;&#35201;&#19981;&#21516;&#27880;&#37322;&#26041;&#26696;&#30340;&#26032;&#30446;&#26631;&#22495;&#20013;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20856;&#22411;&#26041;&#27861;&#28041;&#21450;&#22312;&#30446;&#26631;&#22495;&#25968;&#25454;&#19978;&#36827;&#34892;&#25345;&#32493;&#35757;&#32451;&#65292;&#20294;&#33719;&#21462;&#27880;&#37322;&#26159;&#26114;&#36149;&#19988;&#32791;&#26102;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#26469;&#26377;&#25928;&#36866;&#24212;&#20849;&#25351;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#39640;&#31934;&#24230;&#25552;&#21450;&#26816;&#27979;&#30446;&#26631;&#24182;&#20165;&#23545;&#30446;&#26631;&#22495;&#20013;&#30340;&#25552;&#21450;&#36827;&#34892;&#27880;&#37322;&#12290;&#22312;&#19977;&#20010;&#33521;&#35821;&#20849;&#25351;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65306;CoNLL-2012&#65288;&#26032;&#38395;/&#20250;&#35805;&#65289;&#65292;i2b2 / VA&#65288;&#21307;&#23398;&#35760;&#24405;&#65289;&#21644;&#20197;&#21069;&#26410;&#30740;&#31350;&#30340;&#20799;&#31461;&#31119;&#21033;&#31508;&#35760;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#21161;&#20110;&#26377;&#25928;&#27880;&#37322;&#36716;&#31227;&#65292;&#32467;&#26524;&#24179;&#22343;F1&#20540;&#25552;&#39640;&#20102;7-14&#65285;&#65292;&#32780;&#19981;&#22686;&#21152;&#26102;&#38388;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although recent neural models for coreference resolution have led to substantial improvements on benchmark datasets, transferring these models to new target domains containing out-of-vocabulary spans and requiring differing annotation schemes remains challenging. Typical approaches involve continued training on annotated target-domain data, but obtaining annotations is costly and time-consuming. We show that annotating mentions alone is nearly twice as fast as annotating full coreference chains. Accordingly, we propose a method for efficiently adapting coreference models, which includes a high-precision mention detection objective and requires annotating only mentions in the target domain. Extensive evaluation across three English coreference datasets: CoNLL-2012 (news/conversation), i2b2/VA (medical notes), and previously unstudied child welfare notes, reveals that our approach facilitates annotation-efficient transfer and results in a 7-14% improvement in average F1 without increasin
&lt;/p&gt;</description></item><item><title>&#36739;&#20302;&#37319;&#26679;&#25928;&#29575;&#30340;NLP&#27169;&#22411;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#21487;&#33021;&#27604;&#36739;&#39640;&#37319;&#26679;&#25928;&#29575;&#30340;&#27169;&#22411;&#26356;&#20026;&#40065;&#26834;&#65292;&#34920;&#26126;&#36890;&#29992;&#30340;&#25552;&#39640;&#37319;&#26679;&#25928;&#29575;&#26041;&#27861;&#19981;&#22826;&#21487;&#33021;&#25913;&#21892;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;OOD&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.06456</link><description>&lt;p&gt;
&#37319;&#26679;&#25928;&#29575;&#26356;&#39640;&#30340;NLP&#27169;&#22411;&#26356;&#21152;&#40065;&#26834;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Sample-Efficient NLP Models More Robust?. (arXiv:2210.06456v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06456
&lt;/p&gt;
&lt;p&gt;
&#36739;&#20302;&#37319;&#26679;&#25928;&#29575;&#30340;NLP&#27169;&#22411;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#21487;&#33021;&#27604;&#36739;&#39640;&#37319;&#26679;&#25928;&#29575;&#30340;&#27169;&#22411;&#26356;&#20026;&#40065;&#26834;&#65292;&#34920;&#26126;&#36890;&#29992;&#30340;&#25552;&#39640;&#37319;&#26679;&#25928;&#29575;&#26041;&#27861;&#19981;&#22826;&#21487;&#33021;&#25913;&#21892;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;OOD&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#25277;&#21462;&#24335;&#38382;&#31572;&#20013;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#26356;&#23569;&#30340;&#20869;&#37096;&#25968;&#25454;&#19978;&#35757;&#32451;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#22806;&#37096;&#35780;&#27979;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#36235;&#21183;&#30340;&#26222;&#36866;&#24615;&#36824;&#19981;&#28165;&#26970;&#12290;&#22312;&#19977;&#20010;&#20219;&#21153;&#12289;&#19977;&#20010;&#24191;&#27867;&#36866;&#29992;&#30340;&#24314;&#27169;&#24178;&#39044;&#65288;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#12289;&#20351;&#29992;&#19981;&#21516;&#30340;&#36866;&#24212;&#26041;&#27861;&#21644;&#22312;&#26356;&#22810;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65289;&#21644;14&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#20197;&#30740;&#31350;&#26679;&#26412;&#25928;&#29575;&#65288;&#36798;&#21040;&#32473;&#23450;ID&#20934;&#30830;&#24230;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#65289;&#21644;&#40065;&#26834;&#24615;&#65288;&#27169;&#22411;&#22312;OOD&#35780;&#20272;&#20013;&#30340;&#34920;&#29616;&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36739;&#39640;&#30340;&#26679;&#26412;&#25928;&#29575;&#20165;&#22312;&#26576;&#20123;&#24314;&#27169;&#24178;&#39044;&#21644;&#20219;&#21153;&#19978;&#19982;&#26356;&#22909;&#30340;&#24179;&#22343;OOD&#40065;&#26834;&#24615;&#30456;&#20851;&#65292;&#32780;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#21017;&#19981;&#28982;&#12290;&#22312;&#20010;&#21035;&#25968;&#25454;&#38598;&#19978;&#65292;&#26679;&#26412;&#25928;&#29575;&#36739;&#20302;&#30340;&#27169;&#22411;&#29978;&#33267;&#26356;&#20026;&#20581;&#22766;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#30340;&#36890;&#29992;&#26041;&#27861;&#19981;&#22826;&#21487;&#33021;&#25913;&#21892;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#26222;&#36941;OOD&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent results in image classification and extractive question answering have observed that pre-trained models trained on less in-distribution data have better out-of-distribution performance. However, it is unclear how broadly these trends hold. We conduct a large empirical study across three tasks, three broadly-applicable modeling interventions (increasing model size, using a different adaptation method, and pre-training on more data), and 14 diverse datasets to investigate the relationship between sample efficiency (amount of data needed to reach a given ID accuracy) and robustness (how models fare on OOD evaluation). We find that higher sample efficiency is only correlated with better average OOD robustness on some modeling interventions and tasks, but not others. On individual datasets, models with lower sample efficiency can even be more robust. These results suggest that general-purpose methods for improving sample efficiency are unlikely to yield universal OOD robustness impro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21464;&#20998;&#24320;&#25918;&#39046;&#22495;&#65288;VOD&#65289;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#24402;&#19968;&#21270;&#30340;R&#233;nyi&#21464;&#20998;&#30028;&#30340;&#20272;&#35745;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#35757;&#32451;&#20855;&#26377;&#26816;&#32034;&#22686;&#24378;&#21151;&#33021;&#30340;&#27169;&#22411;&#65292;&#20363;&#22914;&#38405;&#35835;&#22120;-&#26816;&#32034;&#22120;BERT-sized&#27169;&#22411;&#65292;&#24182;&#23454;&#29616;&#20102;&#22312;&#22810;&#39033;&#36873;&#25321;&#21307;&#23398;&#32771;&#35797;&#38382;&#39064;&#19978;&#30340;&#20248;&#24322;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2210.06345</link><description>&lt;p&gt;
&#21464;&#20998;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Variational Open-Domain Question Answering. (arXiv:2210.06345v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21464;&#20998;&#24320;&#25918;&#39046;&#22495;&#65288;VOD&#65289;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#24402;&#19968;&#21270;&#30340;R&#233;nyi&#21464;&#20998;&#30028;&#30340;&#20272;&#35745;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#35757;&#32451;&#20855;&#26377;&#26816;&#32034;&#22686;&#24378;&#21151;&#33021;&#30340;&#27169;&#22411;&#65292;&#20363;&#22914;&#38405;&#35835;&#22120;-&#26816;&#32034;&#22120;BERT-sized&#27169;&#22411;&#65292;&#24182;&#23454;&#29616;&#20102;&#22312;&#22810;&#39033;&#36873;&#25321;&#21307;&#23398;&#32771;&#35797;&#38382;&#39064;&#19978;&#30340;&#20248;&#24322;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#26159;&#23545;&#23427;&#20204;&#36827;&#34892;&#21464;&#20998;&#25512;&#26029;&#30340;&#20248;&#21270;&#30740;&#31350;&#20173;&#28982;&#19981;&#36275;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21464;&#20998;&#24320;&#25918;&#39046;&#22495;&#65288;VOD&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#37325;&#28857;&#25918;&#22312;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#21644;&#35821;&#35328;&#24314;&#27169;&#26041;&#38754;&#12290;VOD&#30446;&#26631;&#26159;&#19968;&#31181;&#33258;&#24402;&#19968;&#21270;&#30340;R&#233;nyi&#21464;&#20998;&#30028;&#30340;&#20272;&#35745;&#65292;&#36817;&#20284;&#20110;&#20219;&#21153;&#36793;&#32536;&#20284;&#28982;&#65292;&#24182;&#22312;&#19968;&#20010;&#36741;&#21161;&#37319;&#26679;&#20998;&#24067;&#65288;&#32531;&#23384;&#30340;&#26816;&#32034;&#22120;&#21644;/&#25110;&#36817;&#20284;&#21518;&#39564;&#65289;&#20013;&#36827;&#34892;&#26679;&#26412;&#25277;&#21462;&#35780;&#20272;&#12290;&#23427;&#20173;&#28982;&#26159;&#21487;&#34892;&#30340;&#65292;&#21363;&#20351;&#26159;&#22312;&#23545;&#22823;&#37327;&#35821;&#26009;&#24211;&#23450;&#20041;&#30340;&#26816;&#32034;&#22120;&#20998;&#24067;&#19979;&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#38024;&#23545;&#22810;&#39033;&#36873;&#25321;&#21307;&#23398;&#32771;&#35797;&#38382;&#39064;&#30340;&#38405;&#35835;&#22120;-&#26816;&#32034;&#22120;BERT-sized&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;VOD&#30340;&#22810;&#21151;&#33021;&#24615;&#12290;&#22312;MedMCQA&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#36229;&#36807;&#20102;&#39046;&#22495;&#24494;&#35843;&#30340;Med-PaLM 5.3&#65285;&#65292;&#23613;&#31649;&#20351;&#29992;&#30340;&#21442;&#25968;&#23569;&#20102;2500&#20493;&#12290;&#25105;&#20204;&#30340;&#26816;&#32034;&#22686;&#24378;BioLinkBERT&#27169;&#22411;&#24471;&#20998;&#20026;62.9&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval-augmented models have proven to be effective in natural language processing tasks, yet there remains a lack of research on their optimization using variational inference. We introduce the Variational Open-Domain (VOD) framework for end-to-end training and evaluation of retrieval-augmented models, focusing on open-domain question answering and language modelling. The VOD objective, a self-normalized estimate of the R\'enyi variational bound, approximates the task marginal likelihood and is evaluated under samples drawn from an auxiliary sampling distribution (cached retriever and/or approximate posterior). It remains tractable, even for retriever distributions defined on large corpora. We demonstrate VOD's versatility by training reader-retriever BERT-sized models on multiple-choice medical exam questions. On the MedMCQA dataset, we outperform the domain-tuned Med-PaLM by +5.3% despite using 2.500$\times$ fewer parameters. Our retrieval-augmented BioLinkBERT model scored 62.9%
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CONE&#65292;&#19968;&#20010;&#39640;&#25928;&#30340;&#31895;-&#32454;&#23545;&#40784;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#38271;&#35270;&#39057;&#26102;&#38388;&#23450;&#20301;&#12290;CONE&#36890;&#36807;&#22522;&#20110;&#26597;&#35810;&#30340;&#31383;&#21475;&#36873;&#25321;&#31574;&#30053;&#21644;&#23545;&#27604;&#23398;&#20064;&#26426;&#21046;&#25552;&#21319;&#20102;&#22810;&#27169;&#24577;&#23545;&#40784;&#65292;&#24182;&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;&#38271;&#35270;&#39057;&#26102;&#38388;&#23450;&#20301;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2209.10918</link><description>&lt;p&gt;
CONE&#65306;&#29992;&#20110;&#38271;&#35270;&#39057;&#26102;&#38388;&#23450;&#20301;&#30340;&#39640;&#25928;&#31895;-&#32454;&#23545;&#40784;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CONE: An Efficient COarse-to-fiNE Alignment Framework for Long Video Temporal Grounding. (arXiv:2209.10918v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.10918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CONE&#65292;&#19968;&#20010;&#39640;&#25928;&#30340;&#31895;-&#32454;&#23545;&#40784;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#38271;&#35270;&#39057;&#26102;&#38388;&#23450;&#20301;&#12290;CONE&#36890;&#36807;&#22522;&#20110;&#26597;&#35810;&#30340;&#31383;&#21475;&#36873;&#25321;&#31574;&#30053;&#21644;&#23545;&#27604;&#23398;&#20064;&#26426;&#21046;&#25552;&#21319;&#20102;&#22810;&#27169;&#24577;&#23545;&#40784;&#65292;&#24182;&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;&#38271;&#35270;&#39057;&#26102;&#38388;&#23450;&#20301;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#19968;&#20010;&#26032;&#20852;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#8212;&#8212;&#38271;&#35270;&#39057;&#26102;&#38388;&#23450;&#20301;&#65288;VTG&#65289;&#65292;&#21363;&#23450;&#20301;&#19982;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#30456;&#20851;&#30340;&#35270;&#39057;&#29255;&#27573;&#12290;&#30456;&#27604;&#20110;&#30701;&#35270;&#39057;&#65292;&#38271;&#35270;&#39057;&#21516;&#26679;&#38750;&#24120;&#21463;&#27426;&#36814;&#65292;&#20294;&#26159;&#25506;&#32034;&#36739;&#23569;&#65292;&#36825;&#24102;&#26469;&#20102;&#22810;&#20010;&#25361;&#25112;&#65292;&#20363;&#22914;&#26356;&#39640;&#30340;&#25512;&#29702;&#35745;&#31639;&#25104;&#26412;&#21644;&#24369;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CONE&#65292;&#19968;&#20010;&#39640;&#25928;&#30340;&#31895;-&#32454;&#23545;&#40784;&#26694;&#26550;&#12290;CONE&#26159;&#19968;&#20010;&#25554;&#25300;&#24335;&#30340;&#26694;&#26550;&#65292;&#21487;&#22312;&#29616;&#26377;&#30340;VTG&#27169;&#22411;&#19978;&#22788;&#29702;&#38271;&#35270;&#39057;&#65292;&#36890;&#36807;&#28369;&#21160;&#31383;&#21475;&#26426;&#21046;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;CONE&#65288;1&#65289;&#24341;&#20837;&#20102;&#22522;&#20110;&#26597;&#35810;&#30340;&#31383;&#21475;&#36873;&#25321;&#31574;&#30053;&#20197;&#21152;&#24555;&#25512;&#29702;&#36895;&#24230;&#65292;&#65288;2&#65289;&#25552;&#35758;&#20102;&#36890;&#36807;&#26032;&#22686;&#23545;&#27604;&#23398;&#20064;&#26469;&#22686;&#24378;&#38271;&#35270;&#39057;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#30340;&#31895;&#32454;&#26426;&#21046;&#12290;&#23545;&#20004;&#20010;&#22823;&#35268;&#27169;&#38271;VTG&#22522;&#20934;&#27979;&#35797;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#22343;&#34920;&#26126;&#65292;CONE&#22312;&#24615;&#33021;&#19978;&#37117;&#26377;&#24456;&#22823;&#25552;&#21319;&#65288;&#20363;&#22914;&#22312;MAD&#19978;&#20174;3.13&#65285;&#21040;6.87&#65285;&#65289;&#65292;&#24182;&#19988;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#20998;&#26512;&#20063;&#35777;&#26126;&#20102;CONE&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper tackles an emerging and challenging problem of long video temporal grounding~(VTG) that localizes video moments related to a natural language (NL) query. Compared with short videos, long videos are also highly demanded but less explored, which brings new challenges in higher inference computation cost and weaker multi-modal alignment. To address these challenges, we propose CONE, an efficient COarse-to-fiNE alignment framework. CONE is a plug-and-play framework on top of existing VTG models to handle long videos through a sliding window mechanism. Specifically, CONE (1) introduces a query-guided window selection strategy to speed up inference, and (2) proposes a coarse-to-fine mechanism via a novel incorporation of contrastive learning to enhance multi-modal alignment for long videos. Extensive experiments on two large-scale long VTG benchmarks consistently show both substantial performance gains (e.g., from 3.13% to 6.87% on MAD) and state-of-the-art results. Analyses also 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35843;&#25972;&#33539;&#20363;&#65292;&#21517;&#20026;ILLUME&#65292;&#36890;&#36807;&#20154;&#26426;&#20132;&#20114;&#26469;&#21512;&#29702;&#21270;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#30340;&#36755;&#20986;&#26356;&#31526;&#21512;&#20154;&#30340;&#24605;&#32500;&#26041;&#24335;&#12290;&#22312;&#20351;&#29992;&#30456;&#23545;&#36739;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#26368;&#23569;&#30340;&#20154;&#31867;&#21453;&#39304;&#19979;&#65292;ILLUME&#34920;&#29616;&#20986;&#19982;&#26631;&#20934;&#30417;&#30563;&#24494;&#35843;&#30456;&#24403;&#30340;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2208.08241</link><description>&lt;p&gt;
ILLUME&#65306;&#36890;&#36807;&#20154;&#26426;&#20132;&#20114;&#26469;&#21512;&#29702;&#21270;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ILLUME: Rationalizing Vision-Language Models through Human Interactions. (arXiv:2208.08241v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.08241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35843;&#25972;&#33539;&#20363;&#65292;&#21517;&#20026;ILLUME&#65292;&#36890;&#36807;&#20154;&#26426;&#20132;&#20114;&#26469;&#21512;&#29702;&#21270;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#30340;&#36755;&#20986;&#26356;&#31526;&#21512;&#20154;&#30340;&#24605;&#32500;&#26041;&#24335;&#12290;&#22312;&#20351;&#29992;&#30456;&#23545;&#36739;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#26368;&#23569;&#30340;&#20154;&#31867;&#21453;&#39304;&#19979;&#65292;ILLUME&#34920;&#29616;&#20986;&#19982;&#26631;&#20934;&#30417;&#30563;&#24494;&#35843;&#30456;&#24403;&#30340;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24341;&#23548;&#24050;&#34987;&#35777;&#26126;&#26159;&#26500;&#24314;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#22270;&#20687;&#23383;&#24149;&#25110;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#31561;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#36755;&#20986;&#24456;&#23569;&#19982;&#29992;&#25143;&#23545;&#29305;&#23450;&#31572;&#26696;&#30340;&#29702;&#24615;&#30456;&#19968;&#33268;&#12290;&#20026;&#20102;&#25913;&#21892;&#36825;&#31181;&#23545;&#40784;&#24182;&#21152;&#24378;&#24120;&#35782;&#21407;&#22240;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#26426;&#29983;&#25104;&#25968;&#25454;&#30340;&#35843;&#25972;&#33539;&#20363;&#12290;&#25105;&#20204;&#30340;ILLUME&#25191;&#34892;&#20197;&#19979;&#24490;&#29615;&#65306;&#32473;&#23450;&#19968;&#20010;&#22270;&#20687;-&#38382;&#39064;-&#31572;&#26696;&#25552;&#31034;&#65292;VLM&#26679;&#26412;&#22810;&#20010;&#20505;&#36873;&#21407;&#29702;&#65292;&#20154;&#31867;&#35780;&#35770;&#23478;&#36890;&#36807;&#20559;&#22909;&#36873;&#25321;&#25552;&#20379;&#21453;&#39304;&#65292;&#29992;&#20110;&#24494;&#35843;&#12290;&#36825;&#20010;&#24490;&#29615;&#22686;&#21152;&#20102;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#36880;&#28176;&#38613;&#21051;&#20986;&#19982;&#20154;&#31867;&#24847;&#22270;&#30456;&#19968;&#33268;&#30340;VLM&#30340;&#29702;&#24615;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#35814;&#23613;&#23454;&#39564;&#34920;&#26126;&#65292;ILLUME&#22312;&#20351;&#29992; significantly &#26356;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#20165;&#38656;&#35201; minimal &#21453;&#39304;&#30340;&#21516;&#26102;&#65292;&#19982;&#26631;&#20934;&#30417;&#30563;&#24494;&#35843;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bootstrapping from pre-trained language models has been proven to be an efficient approach for building vision-language models (VLM) for tasks such as image captioning or visual question answering. However, outputs of these models rarely align with user's rationales for specific answers. In order to improve this alignment and reinforce commonsense reasons, we propose a tuning paradigm based on human interactions with machine-generated data. Our ILLUME executes the following loop: Given an image-question-answer prompt, the VLM samples multiple candidate rationales, and a human critic provides feedback via preference selection, used for fine-tuning. This loop increases the training data and gradually carves out the VLM's rationalization capabilities that are aligned with human intent. Our exhaustive experiments demonstrate that ILLUME is competitive with standard supervised finetuning while using significantly fewer training data and only requiring minimal feedback.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;ROUGE&#30340;&#36136;&#24515;&#32858;&#31867;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#22810;&#25991;&#26723;&#25688;&#35201;&#20013;&#65292;&#19981;&#38656;&#35201;&#20154;&#24037;&#32534;&#20889;&#30340;&#25688;&#35201;&#65292;&#27169;&#22411;Centrum&#27604;&#29616;&#26377;&#20808;&#36827;&#27169;&#22411;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2208.01006</link><description>&lt;p&gt;
&#22522;&#20110;&#36136;&#24515;&#39044;&#35757;&#32451;&#30340;&#22810;&#25991;&#26723;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Multi-Document Summarization with Centroid-Based Pretraining. (arXiv:2208.01006v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.01006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;ROUGE&#30340;&#36136;&#24515;&#32858;&#31867;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#22810;&#25991;&#26723;&#25688;&#35201;&#20013;&#65292;&#19981;&#38656;&#35201;&#20154;&#24037;&#32534;&#20889;&#30340;&#25688;&#35201;&#65292;&#27169;&#22411;Centrum&#27604;&#29616;&#26377;&#20808;&#36827;&#27169;&#22411;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#25991;&#26723;&#25688;&#35201;(MDS)&#20013;&#65292;&#36755;&#20837;&#21487;&#20197;&#34987;&#24314;&#27169;&#20026;&#19968;&#32452;&#25991;&#26723;&#65292;&#36755;&#20986;&#26159;&#23427;&#20204;&#30340;&#25688;&#35201;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;MDS&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#23427;&#28041;&#21450;&#36873;&#25321;&#27599;&#20010;&#25991;&#26723;&#32858;&#31867;&#30340;&#22522;&#20110;ROUGE&#30340;&#36136;&#24515;&#20316;&#20026;&#20854;&#25688;&#35201;&#30340;&#20195;&#29702;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#19981;&#38656;&#35201;&#20154;&#24037;&#32534;&#20889;&#30340;&#25688;&#35201;&#65292;&#21487;&#20197;&#29992;&#20110;&#20165;&#30001;&#25991;&#26723;&#38598;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#30340;&#39044;&#35757;&#32451;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;MDS&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#38646;&#26679;&#26412;&#12289;&#23569;&#26679;&#26412;&#21644;&#23436;&#20840;&#30417;&#30563;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;Centrum&#27604;&#29616;&#26377;&#30340;&#20808;&#36827;&#27169;&#22411;&#26356;&#22909;&#25110;&#21487;&#27604;&#12290;&#25105;&#20204;&#23558;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#30340;&#27169;&#22411;&#20813;&#36153;&#25552;&#20379;&#32473;&#30740;&#31350;&#31038;&#21306;https://github.com/ratishsp/centrum&#12290;
&lt;/p&gt;
&lt;p&gt;
In Multi-Document Summarization (MDS), the input can be modeled as a set of documents, and the output is its summary. In this paper, we focus on pretraining objectives for MDS. Specifically, we introduce a novel pretraining objective, which involves selecting the ROUGE-based centroid of each document cluster as a proxy for its summary. Our objective thus does not require human written summaries and can be utilized for pretraining on a dataset consisting solely of document sets. Through zero-shot, few-shot, and fully supervised experiments on multiple MDS datasets, we show that our model Centrum is better or comparable to a state-of-the-art model. We make the pretrained and fine-tuned models freely available to the research community https://github.com/ratishsp/centrum.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#25991;&#26412;&#30340;&#38544;&#31169;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#37096;&#32626;&#24694;&#24847;&#21442;&#25968;&#21521;&#37327;&#26469;&#25581;&#31034;&#31169;&#20154;&#29992;&#25143;&#25991;&#26412;&#65292;&#24182;&#25104;&#21151;&#22320;&#36827;&#34892;mini-batches&#35757;&#32451;&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#29992;&#25143;&#21644;&#38271;&#24207;&#21015;&#65292;&#25552;&#31034;&#20102;&#25991;&#26412;&#39046;&#22495;&#30340;FL&#27604;&#20808;&#21069;&#35748;&#20026;&#30340;&#26356;&#33030;&#24369;&#12290;</title><link>http://arxiv.org/abs/2201.12675</link><description>&lt;p&gt;
&#30772;&#22351;&#32773;&#65306;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#21464;&#24418;&#37329;&#21018;&#38544;&#31169;&#27844;&#38706;
&lt;/p&gt;
&lt;p&gt;
Decepticons: Corrupted Transformers Breach Privacy in Federated Learning for Language Models. (arXiv:2201.12675v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.12675
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#25991;&#26412;&#30340;&#38544;&#31169;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#37096;&#32626;&#24694;&#24847;&#21442;&#25968;&#21521;&#37327;&#26469;&#25581;&#31034;&#31169;&#20154;&#29992;&#25143;&#25991;&#26412;&#65292;&#24182;&#25104;&#21151;&#22320;&#36827;&#34892;mini-batches&#35757;&#32451;&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#29992;&#25143;&#21644;&#38271;&#24207;&#21015;&#65292;&#25552;&#31034;&#20102;&#25991;&#26412;&#39046;&#22495;&#30340;FL&#27604;&#20808;&#21069;&#35748;&#20026;&#30340;&#26356;&#33030;&#24369;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;(Federated learning, FL)&#30340;&#26680;&#24515;&#21407;&#21017;&#26159;&#22312;&#19981;&#38598;&#20013;&#29992;&#25143;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#27169;&#22411;&#65292;&#36825;&#31181;&#26041;&#27861;&#24378;&#35843;&#38544;&#31169;&#20445;&#25252;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;FL&#20013;&#20351;&#29992;&#30340;&#26799;&#24230;&#26356;&#26032;&#21487;&#33021;&#27844;&#38706;&#29992;&#25143;&#20449;&#24687;&#12290;&#23613;&#31649;FL&#22312;&#25991;&#26412;&#24212;&#29992;&#39046;&#22495;&#65288;&#20363;&#22914;&#20987;&#38190;&#39044;&#27979;&#65289;&#20013;&#24456;&#24120;&#35265;&#65292;&#20294;&#23545;&#20110;FL&#38544;&#31169;&#30340;&#20960;&#20046;&#25152;&#26377;&#25915;&#20987;&#37117;&#38598;&#20013;&#22312;&#31616;&#21333;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#19978;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#37096;&#32626;&#24694;&#24847;&#21442;&#25968;&#21521;&#37327;&#26469;&#25581;&#31034;&#31169;&#20154;&#29992;&#25143;&#25991;&#26412;&#65292;&#36825;&#31181;&#25915;&#20987;&#21487;&#20197;&#25104;&#21151;&#22320;&#36827;&#34892;mini-batches&#35757;&#32451;&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#29992;&#25143;&#21644;&#38271;&#24207;&#21015;&#12290;&#19982;&#20197;&#24448;&#38024;&#23545;FL&#30340;&#25915;&#20987;&#19981;&#21516;&#30340;&#26159;&#65292;&#36825;&#31181;&#25915;&#20987;&#21033;&#29992;&#20102;Transformer&#26550;&#26500;&#21644;&#26631;&#35760;&#23884;&#20837;(token embedding)&#30340;&#29305;&#24615;&#65292;&#20998;&#21035;&#25552;&#21462;&#26631;&#35760;&#21644;&#20301;&#32622;&#23884;&#20837;&#20197;&#26816;&#32034;&#39640;&#20445;&#30495;&#24230;&#25991;&#26412;&#12290;&#36825;&#39033;&#24037;&#20316;&#34920;&#26126;&#65292;&#25991;&#26412;&#39046;&#22495;&#30340;FL&#22312;&#21382;&#21490;&#19978;&#19968;&#30452;&#33021;&#22815;&#25269;&#24481;&#38544;&#31169;&#25915;&#20987;&#65292;&#20294;&#27604;&#20808;&#21069;&#35748;&#20026;&#30340;&#26356;&#21152;&#33030;&#24369;&#12290;
&lt;/p&gt;
&lt;p&gt;
A central tenet of Federated learning (FL), which trains models without centralizing user data, is privacy. However, previous work has shown that the gradient updates used in FL can leak user information. While the most industrial uses of FL are for text applications (e.g. keystroke prediction), nearly all attacks on FL privacy have focused on simple image classifiers. We propose a novel attack that reveals private user text by deploying malicious parameter vectors, and which succeeds even with mini-batches, multiple users, and long sequences. Unlike previous attacks on FL, the attack exploits characteristics of both the Transformer architecture and the token embedding, separately extracting tokens and positional embeddings to retrieve high-fidelity text. This work suggests that FL on text, which has historically been resistant to privacy attacks, is far more vulnerable than previously thought.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20107;&#23454;&#26680;&#26597;&#25968;&#25454;&#38598;AmbiFC&#65292;&#29992;&#20110;&#22788;&#29702;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#21547;&#31946;&#24615;&#22768;&#26126;&#26680;&#26597;&#38382;&#39064;&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#30340;&#35777;&#25454;&#27880;&#37322;&#21644;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21547;&#31946;&#24615;&#22768;&#26126;&#30340;&#36719;&#26631;&#31614;&#35777;&#25454;&#26680;&#26597;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#27880;&#37322;&#20154;&#21592;&#20105;&#35758;&#20998;&#26512;&#20013;&#21457;&#29616;&#20102;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2104.00640</link><description>&lt;p&gt;
AmbiFC: &#29992;&#35777;&#25454;&#26816;&#39564;&#21547;&#31946;&#24615;&#22768;&#26126;&#30340;&#30495;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
AmbiFC: Fact-Checking Ambiguous Claims with Evidence. (arXiv:2104.00640v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.00640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20107;&#23454;&#26680;&#26597;&#25968;&#25454;&#38598;AmbiFC&#65292;&#29992;&#20110;&#22788;&#29702;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#21547;&#31946;&#24615;&#22768;&#26126;&#26680;&#26597;&#38382;&#39064;&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#30340;&#35777;&#25454;&#27880;&#37322;&#21644;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21547;&#31946;&#24615;&#22768;&#26126;&#30340;&#36719;&#26631;&#31614;&#35777;&#25454;&#26680;&#26597;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#27880;&#37322;&#20154;&#21592;&#20105;&#35758;&#20998;&#26512;&#20013;&#21457;&#29616;&#20102;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#33258;&#21160;&#21270;&#20107;&#23454;&#26680;&#26597;&#31995;&#32479;&#24517;&#39035;&#23558;&#22768;&#26126;&#19982;&#26816;&#32034;&#21040;&#30340;&#35777;&#25454;&#36827;&#34892;&#27604;&#36739;&#20197;&#39044;&#27979;&#30495;&#23454;&#24615;&#12290;&#26816;&#32034;&#21040;&#30340;&#35777;&#25454;&#21487;&#33021;&#26080;&#27861;&#26126;&#30830;&#25903;&#25345;&#25110;&#21453;&#39539;&#22768;&#26126;&#65292;&#24182;&#20135;&#29983;&#21508;&#31181;&#26377;&#25928;&#35299;&#37322;&#12290;&#29616;&#26377;&#30340;&#20107;&#23454;&#26680;&#26597;&#25968;&#25454;&#38598;&#38656;&#35201;&#27169;&#22411;&#20026;&#27599;&#20010;&#22768;&#26126;&#39044;&#27979;&#21333;&#20010;&#30495;&#23454;&#24615;&#26631;&#31614;&#65292;&#24182;&#19988;&#32570;&#20047;&#31649;&#29702;&#27492;&#31867;&#27169;&#31946;&#24615;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20107;&#23454;&#26680;&#26597;&#25968;&#25454;&#38598;AmbiFC&#65292;&#20854;&#20013;&#21253;&#21547;&#20174;&#23436;&#25972;&#32500;&#22522;&#30334;&#31185;&#39029;&#38754;&#20013;&#33719;&#21462;&#30340;&#32463;&#36807;&#32454;&#31890;&#24230;&#35777;&#25454;&#27880;&#37322;&#30340;&#20449;&#24687;&#38656;&#27714;&#30340;&#29616;&#23454;&#22768;&#26126;&#12290;&#25105;&#20204;&#24443;&#24213;&#20998;&#26512;&#20102;AmbiFC&#20013;&#28041;&#21450;&#21547;&#31946;&#22768;&#26126;&#24341;&#36215;&#30340;&#20105;&#35758;&#65292;&#35266;&#23519;&#21040;&#19982;&#27880;&#37322;&#20154;&#21592;&#30340;&#33258;&#25105;&#35780;&#20272;&#21644;&#19987;&#23478;&#27880;&#37322;&#30340;&#35821;&#35328;&#29616;&#35937;&#24378;&#28872;&#30456;&#20851;&#30340;&#27880;&#37322;&#20154;&#21592;&#20105;&#35758;&#12290;&#25105;&#20204;&#24341;&#20837;&#22522;&#20110;&#35777;&#25454;&#30340;&#21547;&#31946;&#22768;&#26126;&#30340;&#30495;&#23454;&#24615;&#26680;&#26597;&#20219;&#21153;&#65292;&#27604;&#36739;&#20102;&#19977;&#31181;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#21547;&#27880;&#37322;&#20449;&#21495;&#21644;&#21333;&#26631;&#31614;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated fact-checking systems in real-world scenarios must compare claims with retrieved evidence to predict the veracity. The retrieved evidence may not unambiguously support or refute the claim and yield diverse valid interpretations. Existing fact-checking datasets necessitate that models predict a single veracity label for each claim and lack the ability to manage such ambiguity. We present AmbiFC, a large-scale fact-checking dataset with realistic claims derived from real-world information needs. Our dataset contains fine-grained evidence annotations of passages from complete Wikipedia pages. We thoroughly analyze disagreements arising from ambiguous claims in AmbiFC, observing a strong correlation of annotator disagreement with their self-assessment and expert-annotated linguistic phenomena. We introduce the task of evidence-based fact-checking for ambiguous claims with soft labels, and compare three methodologies incorporating annotation signals with a single-label classificat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#27979;&#37327;32&#20010;&#38382;&#39064;&#22238;&#31572;&#22522;&#20934;&#27979;&#35797;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#21457;&#29616;&#20154;&#24037;&#26500;&#24314;&#30340;&#22522;&#20934;&#27979;&#35797;&#20855;&#26377;&#39640;&#24230;&#30340;&#19968;&#33268;&#24615;&#65292;&#21363;&#20351;&#23427;&#20204;&#30340;&#27573;&#33853;&#21644;&#38382;&#39064;&#20998;&#24067;&#26159;&#38750;&#24120;&#19981;&#21516;&#30340;&#65292;&#36825;&#34920;&#26126;&#23613;&#31649;&#20154;&#20204;&#38271;&#26102;&#38388;&#20851;&#27880;&#23569;&#25968;&#22522;&#20934;&#27979;&#35797;&#65292;&#20294;&#25152;&#30740;&#31350;&#30340;&#24314;&#27169;&#25913;&#36827;&#20173;&#28982;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2102.01065</link><description>&lt;p&gt;
&#38382;&#39064;&#22238;&#31572;&#24314;&#27169;&#30340;&#25913;&#36827;&#26159;&#21542;&#36328;&#36234;&#22522;&#20934;&#27979;&#35797;&#25345;&#32493;&#23384;&#22312;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Question Answering Modeling Improvements Hold Across Benchmarks?. (arXiv:2102.01065v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.01065
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#27979;&#37327;32&#20010;&#38382;&#39064;&#22238;&#31572;&#22522;&#20934;&#27979;&#35797;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#21457;&#29616;&#20154;&#24037;&#26500;&#24314;&#30340;&#22522;&#20934;&#27979;&#35797;&#20855;&#26377;&#39640;&#24230;&#30340;&#19968;&#33268;&#24615;&#65292;&#21363;&#20351;&#23427;&#20204;&#30340;&#27573;&#33853;&#21644;&#38382;&#39064;&#20998;&#24067;&#26159;&#38750;&#24120;&#19981;&#21516;&#30340;&#65292;&#36825;&#34920;&#26126;&#23613;&#31649;&#20154;&#20204;&#38271;&#26102;&#38388;&#20851;&#27880;&#23569;&#25968;&#22522;&#20934;&#27979;&#35797;&#65292;&#20294;&#25152;&#30740;&#31350;&#30340;&#24314;&#27169;&#25913;&#36827;&#20173;&#28982;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#39064;&#22238;&#31572; (QA) &#27169;&#22411;&#30340;&#25913;&#36827;&#65288;&#20363;&#22914;&#65292;&#26550;&#26500;&#21644;&#35757;&#32451;&#36807;&#31243;&#30340;&#36873;&#25321;&#65289;&#26159;&#21542;&#22312;&#21508;&#31181;QA&#22522;&#20934;&#27979;&#35797;&#20013;&#37117;&#33021;&#25345;&#32493;&#23384;&#22312;&#65311;&#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#19968;&#33268;&#24615;&#8221;&#30340;&#27010;&#24565;--&#22914;&#26524;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#22312;&#19968;&#32452;&#24314;&#27169;&#26041;&#27861;&#19978;&#25490;&#21517;&#30456;&#20284;&#65292;&#37027;&#20040;&#23427;&#20204;&#20043;&#38388;&#26377;&#39640;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#22312;&#19968;&#32452;20&#20010;&#19981;&#21516;&#30340;&#24314;&#27169;&#26041;&#27861;&#19978;&#27979;&#37327;&#20102;32&#20010;&#38382;&#39064;&#22238;&#31572;&#22522;&#20934;&#27979;&#35797;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#21457;&#29616;&#20154;&#24037;&#26500;&#24314;&#30340;&#22522;&#20934;&#27979;&#35797;&#22312;&#30456;&#20114;&#20043;&#38388;&#20855;&#26377;&#39640;&#24230;&#19968;&#33268;&#24615;&#65292;&#21363;&#20351;&#23427;&#20204;&#30340;&#27573;&#33853;&#21644;&#38382;&#39064;&#20998;&#24067;&#26159;&#38750;&#24120;&#19981;&#21516;&#30340;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#21363;&#20351;&#26159;&#32553;&#20943;&#20102;&#25968;&#25454;&#37327;&#30340;&#20154;&#24037;&#26500;&#24314;&#22522;&#20934;&#27979;&#35797;&#65288;&#21363;&#37319;&#38598;&#36739;&#23569;&#25968;&#25454;&#37327;&#65289;&#21644;&#31243;&#24207;&#29983;&#25104;&#30340;&#22522;&#20934;&#27979;&#35797;&#65288;&#20363;&#22914;&#65292;&#22635;&#31354;&#26684;&#24335;&#30340;&#31034;&#20363;&#65289;&#20063;&#19982;&#20154;&#24037;&#26500;&#24314;&#30340;&#22522;&#20934;&#27979;&#35797;&#20855;&#26377;&#39640;&#24230;&#19968;&#33268;&#24615;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#31038;&#21306;&#38271;&#26399;&#20851;&#27880;&#23569;&#25968;&#22522;&#20934;&#27979;&#35797;&#65292;&#20294;&#25152;&#30740;&#31350;&#30340;&#24314;&#27169;&#25913;&#36827;&#20173;&#28982;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Do question answering (QA) modeling improvements (e.g., choice of architecture and training procedure) hold consistently across the diverse landscape of QA benchmarks? To study this question, we introduce the notion of concurrence -- two benchmarks have high concurrence on a set of modeling approaches if they rank the modeling approaches similarly. We measure the concurrence between 32 QA benchmarks on a set of 20 diverse modeling approaches and find that human-constructed benchmarks have high concurrence amongst themselves, even if their passage and question distributions are very different. Surprisingly, even downsampled human-constructed benchmarks (i.e., collecting less data) and programmatically-generated benchmarks (e.g., cloze-formatted examples) have high concurrence with human-constructed benchmarks. These results indicate that, despite years of intense community focus on a small number of benchmarks, the modeling improvements studied hold broadly.
&lt;/p&gt;</description></item></channel></rss>