<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#25552;&#20986;UniMax&#65292;&#19968;&#31181;&#23545;&#22810;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26356;&#20844;&#24179;&#21644;&#26356;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#37319;&#26679;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26126;&#30830;&#38480;&#21046;&#27599;&#31181;&#35821;&#35328;&#35821;&#26009;&#24211;&#19978;&#30340;&#37325;&#22797;&#27425;&#25968;&#26469;&#25552;&#20379;&#26356;&#22343;&#21248;&#30340;&#26680;&#24515;&#35821;&#35328;&#35206;&#30422;&#29575;&#65292;&#24182;&#20943;&#36731;&#20102;&#23545;&#23614;&#37096;&#35821;&#35328;&#30340;&#36807;&#24230;&#25311;&#21512;&#12290;UniMax&#20248;&#20110;&#26631;&#20934;&#30340;&#22522;&#20110;&#28201;&#24230;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#32780;&#19988;&#36825;&#20123;&#22909;&#22788;&#38543;&#30528;&#35268;&#27169;&#30340;&#22686;&#21152;&#32780;&#25345;&#32493;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2304.09151</link><description>&lt;p&gt;
UniMax: &#26356;&#20844;&#24179;&#21644;&#26356;&#26377;&#25928;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#37319;&#26679;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
UniMax: Fairer and more Effective Language Sampling for Large-Scale Multilingual Pretraining. (arXiv:2304.09151v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;UniMax&#65292;&#19968;&#31181;&#23545;&#22810;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26356;&#20844;&#24179;&#21644;&#26356;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#37319;&#26679;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26126;&#30830;&#38480;&#21046;&#27599;&#31181;&#35821;&#35328;&#35821;&#26009;&#24211;&#19978;&#30340;&#37325;&#22797;&#27425;&#25968;&#26469;&#25552;&#20379;&#26356;&#22343;&#21248;&#30340;&#26680;&#24515;&#35821;&#35328;&#35206;&#30422;&#29575;&#65292;&#24182;&#20943;&#36731;&#20102;&#23545;&#23614;&#37096;&#35821;&#35328;&#30340;&#36807;&#24230;&#25311;&#21512;&#12290;UniMax&#20248;&#20110;&#26631;&#20934;&#30340;&#22522;&#20110;&#28201;&#24230;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#32780;&#19988;&#36825;&#20123;&#22909;&#22788;&#38543;&#30528;&#35268;&#27169;&#30340;&#22686;&#21152;&#32780;&#25345;&#32493;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#20351;&#29992;&#22522;&#20110;&#28201;&#24230;&#30340;&#21551;&#21457;&#24335;&#37319;&#26679;&#26469;&#24179;&#34913;&#19981;&#21516;&#35821;&#35328;&#65292;&#20294;&#20808;&#21069;&#30340;&#30740;&#31350;&#27809;&#26377;&#31995;&#32479;&#22320;&#35780;&#20272;&#19981;&#21516;&#39044;&#35757;&#32451;&#35821;&#35328;&#20998;&#24067;&#22312;&#27169;&#22411;&#35268;&#27169;&#19978;&#30340;&#21151;&#25928;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#26041;&#27861;UniMax&#65292;&#36890;&#36807;&#26126;&#30830;&#22320;&#38480;&#21046;&#27599;&#31181;&#35821;&#35328;&#35821;&#26009;&#24211;&#19978;&#30340;&#37325;&#22797;&#27425;&#25968;&#65292;&#25552;&#20379;&#26356;&#22343;&#21248;&#30340;&#26680;&#24515;&#35821;&#35328;&#35206;&#30422;&#29575;&#65292;&#21516;&#26102;&#20943;&#36731;&#20102;&#23545;&#23614;&#37096;&#35821;&#35328;&#30340;&#36807;&#24230;&#25311;&#21512;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#22810;&#35821;&#35328;&#22522;&#20934;&#27979;&#35797;&#20013;&#25191;&#34892;&#20102;&#24191;&#27867;&#30340;&#28040;&#34701;&#27979;&#35797;&#65292;&#27979;&#35797;&#20102;&#19968;&#31995;&#21015;&#37319;&#26679;&#31574;&#30053;&#65292;&#21516;&#26102;&#21464;&#21270;&#27169;&#22411;&#35268;&#27169;&#12290;&#25105;&#20204;&#21457;&#29616;UniMax&#20248;&#20110;&#26631;&#20934;&#30340;&#22522;&#20110;&#28201;&#24230;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#24182;&#19988;&#36825;&#20123;&#22909;&#22788;&#38543;&#30528;&#35268;&#27169;&#30340;&#22686;&#21152;&#32780;&#25345;&#32493;&#23384;&#22312;&#12290;&#20316;&#20026;&#25105;&#20204;&#30340;&#36129;&#29486;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#65306;&#65288;i&#65289;29&#19975;&#20159;&#20010;&#23383;&#31526;&#36328;107&#31181;&#35821;&#35328;&#30340;&#25913;&#36827;&#21644;&#26356;&#26032;&#30340;mC4&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#20351;&#29992;UniMax&#35757;&#32451;&#30340;&#39044;&#35757;&#32451;umT5&#27169;&#22411;&#26816;&#26597;&#28857;&#22871;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained multilingual large language models have typically used heuristic temperature-based sampling to balance between different languages. However previous work has not systematically evaluated the efficacy of different pretraining language distributions across model scales. In this paper, we propose a new sampling method, UniMax, that delivers more uniform coverage of head languages while mitigating overfitting on tail languages by explicitly capping the number of repeats over each language's corpus. We perform an extensive series of ablations testing a range of sampling strategies on a suite of multilingual benchmarks, while varying model scale. We find that UniMax outperforms standard temperature-based sampling, and the benefits persist as scale increases. As part of our contribution, we release: (i) an improved and refreshed mC4 multilingual corpus consisting of 29 trillion characters across 107 languages, and (ii) a suite of pretrained umT5 model checkpoints trained with UniMa
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Outlier Suppression+&#26694;&#26550;&#30340;&#36890;&#36947;&#32423;&#31227;&#20301;&#21644;&#32553;&#25918;&#25805;&#20316;&#65292;&#20998;&#26512;&#24471;&#21040;&#26368;&#20248;&#31227;&#20301;&#21644;&#32553;&#25918;&#20540;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#37327;&#21270;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#19981;&#23545;&#31216;&#31163;&#32676;&#20540;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#25509;&#36817;&#28014;&#28857;&#24615;&#33021;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.09145</link><description>&lt;p&gt;
Outlier Suppression+&#65306;&#36890;&#36807;&#31561;&#25928;&#21644;&#26368;&#20248;&#31227;&#20301;&#21644;&#32553;&#25918;&#26469;&#20934;&#30830;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Outlier Suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling. (arXiv:2304.09145v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09145
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Outlier Suppression+&#26694;&#26550;&#30340;&#36890;&#36947;&#32423;&#31227;&#20301;&#21644;&#32553;&#25918;&#25805;&#20316;&#65292;&#20998;&#26512;&#24471;&#21040;&#26368;&#20248;&#31227;&#20301;&#21644;&#32553;&#25918;&#20540;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#37327;&#21270;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#19981;&#23545;&#31216;&#31163;&#32676;&#20540;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#25509;&#36817;&#28014;&#28857;&#24615;&#33021;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;Transformer&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#37327;&#21270;&#38754;&#20020;&#30528;&#23384;&#22312;&#25439;&#23475;&#24615;&#31163;&#32676;&#20540;&#30340;&#37325;&#35201;&#38590;&#39064;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20123;&#31163;&#32676;&#20540;&#26159;&#19981;&#23545;&#31216;&#30340;&#24182;&#19988;&#38598;&#20013;&#22312;&#29305;&#23450;&#36890;&#36947;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Outlier Suppression+&#26694;&#26550;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36890;&#36947;&#32423;&#21035;&#30340;&#31227;&#20301;&#21644;&#32553;&#25918;&#25805;&#20316;&#26469;&#28040;&#38500;&#19981;&#23545;&#31216;&#34920;&#31034;&#24182;&#32553;&#23567;&#26377;&#38382;&#39064;&#30340;&#36890;&#36947;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#25805;&#20316;&#21487;&#20197;&#26080;&#32541;&#22320;&#36801;&#31227;&#33267;&#21518;&#32493;&#27169;&#22359;&#32780;&#20445;&#25345;&#31561;&#25928;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#37327;&#21270;&#20998;&#26512;&#20102;&#26368;&#20248;&#30340;&#31227;&#20301;&#21644;&#32553;&#25918;&#20540;&#65292;&#32771;&#34385;&#21040;&#19979;&#19968;&#23618;&#26435;&#37325;&#30340;&#19981;&#23545;&#31216;&#29305;&#24615;&#21644;&#37327;&#21270;&#35823;&#24046;&#12290;&#25105;&#20204;&#36731;&#37327;&#32423;&#30340;&#26694;&#26550;&#21487;&#20197;&#22312;&#38745;&#24577;&#21644;&#26631;&#20934;&#30340;&#35757;&#32451;&#21518;&#37327;&#21270;&#35774;&#32622;&#19979;&#36896;&#25104;&#26368;&#23567;&#30340;&#24615;&#33021;&#38477;&#20302;&#12290;&#21508;&#31181;&#20219;&#21153;&#21644;&#27169;&#22411;&#30340;&#20840;&#38754;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23567;&#22411;&#27169;&#22411;&#21644;&#22823;&#22411;&#27169;&#22411;&#22914;GPT-2&#26041;&#38754;&#23454;&#29616;&#20102;&#25509;&#36817;&#28014;&#28857;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantization of transformer language models faces significant challenges due to the existence of detrimental outliers in activations. We observe that these outliers are asymmetric and concentrated in specific channels. To address this issue, we propose the Outlier Suppression+ framework. First, we introduce channel-wise shifting and scaling operations to eliminate asymmetric presentation and scale down problematic channels. We demonstrate that these operations can be seamlessly migrated into subsequent modules while maintaining equivalence. Second, we quantitatively analyze the optimal values for shifting and scaling, taking into account both the asymmetric property and quantization errors of weights in the next layer. Our lightweight framework can incur minimal performance degradation under static and standard post-training quantization settings. Comprehensive results across various tasks and models reveal that our approach achieves near-floating-point performance on both small models
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#19982;&#19987;&#20026;&#39640;&#24230;&#29305;&#23450;&#30340;&#25918;&#23556;&#23398;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#24494;&#35843;&#30340;&#26412;&#22320;&#27169;&#22411;&#20043;&#38388;&#30340;&#34920;&#29616;&#24046;&#24322;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#30340;&#39046;&#22495;&#29305;&#23450;&#27169;&#22411;&#35774;&#35745;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2304.09138</link><description>&lt;p&gt;
&#25506;&#32034;&#26435;&#34913;&#65306;&#32479;&#19968;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411; vs &#19987;&#20026;&#39640;&#24230;&#29305;&#23450;&#30340;&#25918;&#23556;&#23398; NLI &#20219;&#21153;&#24494;&#35843;&#30340;&#26412;&#22320;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Exploring the Trade-Offs: Unified Large Language Models vs Local Fine-Tuned Models for Highly-Specific Radiology NLI Task. (arXiv:2304.09138v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09138
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#19982;&#19987;&#20026;&#39640;&#24230;&#29305;&#23450;&#30340;&#25918;&#23556;&#23398;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#24494;&#35843;&#30340;&#26412;&#22320;&#27169;&#22411;&#20043;&#38388;&#30340;&#34920;&#29616;&#24046;&#24322;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#30340;&#39046;&#22495;&#29305;&#23450;&#27169;&#22411;&#35774;&#35745;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;ChatGPT &#21644; GPT-4 &#20973;&#20511;&#20854;&#22312;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#26080;&#19982;&#20262;&#27604;&#30340;&#24615;&#33021;&#32780;&#23853;&#38706;&#22836;&#35282;&#65292;&#24182;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#23613;&#31649;&#22312;&#21508;&#31181;&#24320;&#25918;&#39046;&#22495;&#30340;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#25918;&#23556;&#23398;&#31561;&#39640;&#24230;&#29305;&#23450;&#39046;&#22495;&#30340;&#36275;&#22815;&#24615;&#23578;&#26410;&#24471;&#21040;&#27979;&#35797;&#12290;&#25918;&#23556;&#23398;&#30001;&#20110;&#20854;&#29305;&#24322;&#24615;&#21644;&#22797;&#26434;&#24615;&#21576;&#29616;&#20986;&#19982;&#24320;&#25918;&#39046;&#22495;&#25968;&#25454;&#19981;&#21516;&#30340;&#29420;&#29305;&#35821;&#35328;&#29616;&#35937;&#12290;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36825;&#26679;&#30340;&#29305;&#23450;&#39046;&#22495;&#20013;&#30340;&#24615;&#33021;&#23545;&#20110;&#20840;&#38754;&#35780;&#20272;&#23427;&#20204;&#30340;&#25972;&#20307;&#24615;&#33021;&#20197;&#21450;&#20026;&#26410;&#26469;&#30340;&#27169;&#22411;&#35774;&#35745;&#26041;&#21521;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65306;&#26080;&#35770;&#27169;&#22411;&#35774;&#35745;&#26159;&#21542;&#24212;&#26159;&#36890;&#29992;&#30340;&#36824;&#26159;&#39046;&#22495;&#29305;&#23450;&#30340;&#12290;&#20026;&#27492;&#65292;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102; ChatGPT/GPT-4 &#22312;&#25918;&#23556;&#23398; NLI &#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#23558;&#20854;&#19982;&#19987;&#20026;&#20219;&#21153;&#30456;&#20851;&#25968;&#25454;&#26679;&#26412;&#24494;&#35843;&#30340;&#20854;&#20182;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#24341;&#20837;&#19981;&#21516;&#32423;&#21035;&#30340;&#25512;&#29702;&#23545; ChatGPT/GPT-4 &#30340;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, ChatGPT and GPT-4 have emerged and gained immense global attention due to their unparalleled performance in language processing. Despite demonstrating impressive capability in various open-domain tasks, their adequacy in highly specific fields like radiology remains untested. Radiology presents unique linguistic phenomena distinct from open-domain data due to its specificity and complexity. Assessing the performance of large language models (LLMs) in such specific domains is crucial not only for a thorough evaluation of their overall performance but also for providing valuable insights into future model design directions: whether model design should be generic or domain-specific. To this end, in this study, we evaluate the performance of ChatGPT/GPT-4 on a radiology NLI task and compare it to other models fine-tuned specifically on task-related data samples. We also conduct a comprehensive investigation on ChatGPT/GPT-4's reasoning ability by introducing varying levels of inf
&lt;/p&gt;</description></item><item><title>ChatGPT&#26159;OpenAI&#24320;&#21457;&#30340;&#19968;&#31181;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#21487;&#20197;&#20840;&#33258;&#21160;&#22320;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#23545;&#35805;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;ChatGPT&#22312;10&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#12289;&#26426;&#20250;&#21644;&#23041;&#32961;&#65292;&#21516;&#26102;&#23637;&#31034;&#20102;GPT-4&#30340;&#24040;&#22823;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2304.09103</link><description>&lt;p&gt;
ChatGPT: &#24212;&#29992;&#12289;&#26426;&#20250;&#21644;&#23041;&#32961;
&lt;/p&gt;
&lt;p&gt;
ChatGPT: Applications, Opportunities, and Threats. (arXiv:2304.09103v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09103
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;OpenAI&#24320;&#21457;&#30340;&#19968;&#31181;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#21487;&#20197;&#20840;&#33258;&#21160;&#22320;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#23545;&#35805;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;ChatGPT&#22312;10&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#12289;&#26426;&#20250;&#21644;&#23041;&#32961;&#65292;&#21516;&#26102;&#23637;&#31034;&#20102;GPT-4&#30340;&#24040;&#22823;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT(&#26465;&#20214;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;Transformer)&#26159;&#30001;OpenAI&#24320;&#21457;&#30340;&#19968;&#31181;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#20351;&#29992;&#20102;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#21644;&#22686;&#24378;&#24335;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#24494;&#35843;&#65292;&#20351;&#24471;&#35745;&#31639;&#26426;&#21487;&#20197;&#20840;&#33258;&#21160;&#22320;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#23545;&#35805;&#12290;ChatGPT&#22522;&#20110;Transformer&#26550;&#26500;&#65292;&#24182;&#22312;&#21508;&#31181;&#26469;&#28304;&#30340;&#25968;&#30334;&#19975;&#20010;&#23545;&#35805;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#35813;&#31995;&#32479;&#32467;&#21512;&#20102;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#33021;&#21147;&#19982;&#21487;&#32534;&#31243;&#23618;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#23545;&#35805;&#30340;&#24378;&#22823;&#22522;&#30784;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#35780;&#20272;&#29616;&#26377;&#25991;&#29486;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;ChatGPT&#22312;10&#20010;&#20027;&#35201;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#12289;&#26426;&#20250;&#21644;&#23041;&#32961;&#65292;&#24182;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#20363;&#23376;&#65292;&#21253;&#25324;&#21830;&#19994;&#21644;&#24037;&#19994;&#65292;&#20197;&#21450;&#25945;&#32946;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#23454;&#39564;&#30740;&#31350;&#65292;&#26816;&#26597;&#20102;GPT-3.5&#21644;GPT-4&#30340;&#26377;&#25928;&#24615;&#21644;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#21518;&#32773;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developed by OpenAI, ChatGPT (Conditional Generative Pre-trained Transformer) is an artificial intelligence technology that is fine-tuned using supervised machine learning and reinforcement learning techniques, allowing a computer to generate natural language conversation fully autonomously. ChatGPT is built on the transformer architecture and trained on millions of conversations from various sources. The system combines the power of pre-trained deep learning models with a programmability layer to provide a strong base for generating natural language conversations. In this study, after reviewing the existing literature, we examine the applications, opportunities, and threats of ChatGPT in 10 main domains, providing detailed examples for the business and industry as well as education. We also conducted an experimental study, checking the effectiveness and comparing the performances of GPT-3.5 and GPT-4, and found that the latter performs significantly better. Despite its exceptional abi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#31526;&#21495;&#27714;&#35299;&#22120;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#27492;&#36807;&#31243;&#20013;&#31361;&#20986;&#20102;&#20351;&#29992;&#22768;&#26126;&#24615;&#21644;&#36880;&#27493;&#34920;&#31034;&#30340;&#22909;&#22788;&#12290;</title><link>http://arxiv.org/abs/2304.09102</link><description>&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#31526;&#21495;&#27714;&#35299;&#22120;&#30456;&#32467;&#21512;&#27714;&#35299;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving Math Word Problems by Combining Language Models With Symbolic Solvers. (arXiv:2304.09102v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09102
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#31526;&#21495;&#27714;&#35299;&#22120;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#27492;&#36807;&#31243;&#20013;&#31361;&#20986;&#20102;&#20351;&#29992;&#22768;&#26126;&#24615;&#21644;&#36880;&#27493;&#34920;&#31034;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#36880;&#27493;&#35299;&#20915;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#25945;&#32946;&#20013;&#26377;&#35768;&#22810;&#24212;&#29992;&#12290;&#26368;&#36817;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#22806;&#37096;&#24037;&#20855;&#32467;&#21512;&#20351;&#29992;&#20197;&#25191;&#34892;&#22797;&#26434;&#30340;&#25512;&#29702;&#21644;&#35745;&#31639;&#24050;&#25104;&#20026;&#35299;&#20915;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65292;&#20294;&#20808;&#21069;&#30340;&#26041;&#27861;&#65288;&#22914;&#31243;&#24207;&#36741;&#21161;&#35821;&#35328;&#27169;&#22411;&#65288;PAL&#65289;&#65289;&#23545;&#20110;&#38656;&#35201;&#38472;&#36848;&#24615;&#25512;&#29702;&#30340;&#38382;&#39064;&#20855;&#26377;&#20559;&#35265;&#65292;&#32780;&#23545;&#20110;&#31616;&#21333;&#30340;&#36807;&#31243;&#38382;&#39064;&#21017;&#19981;&#22826;&#26377;&#25928;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#19968;&#31181;&#21487;&#20197;&#23558;&#21333;&#35789;&#38382;&#39064;&#36880;&#27493;&#27491;&#24335;&#21270;&#20026;&#19968;&#32452;&#21464;&#37327;&#21644;&#26041;&#31243;&#24335;&#30340;LLM&#19982;&#22806;&#37096;&#31526;&#21495;&#27714;&#35299;&#22120;&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#26041;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;GSM8K&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#20102;&#19982;&#21407;&#22987;PAL&#30456;&#24403;&#30340;&#20934;&#30830;&#24230;&#65292;&#32780;&#22312;ALGEBRA&#19978;&#21017;&#26126;&#26174;&#20248;&#20110;PAL&#65292;&#35813;&#25968;&#25454;&#38598;&#20174;&#20195;&#25968;&#25945;&#31185;&#20070;&#20013;&#25552;&#21462;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#21333;&#35789;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#31361;&#20986;&#20102;&#22312;&#35299;&#20915;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#26102;&#20351;&#29992;&#22768;&#26126;&#24615;&#21644;&#36880;&#27493;&#34920;&#31034;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatically generating high-quality step-by-step solutions to math word problems has many applications in education. Recently, combining large language models (LLMs) with external tools to perform complex reasoning and calculation has emerged as a promising direction for solving math word problems, but prior approaches such as Program-Aided Language model (PAL) are biased towards simple procedural problems and less effective for problems that require declarative reasoning. We propose an approach that combines an LLM that can incrementally formalize word problems as a set of variables and equations with an external symbolic solver that can solve the equations. Our approach achieves comparable accuracy to the original PAL on the GSM8K benchmark of math word problems and outperforms PAL by an absolute 20% on ALGEBRA, a new dataset of more challenging word problems extracted from Algebra textbooks. Our work highlights the benefits of using declarative and incremental representations when
&lt;/p&gt;</description></item><item><title>KLEVER&#26159;&#19968;&#20010;&#26032;&#30340;CRS&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#29289;&#21697;&#21644;&#23427;&#20204;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#21333;&#35789;&#32852;&#21512;&#24314;&#27169;&#22312;&#21516;&#19968;&#35821;&#20041;&#31354;&#38388;&#20013;&#65292;&#35299;&#20915;&#20102;&#20197;&#21069;&#24037;&#20316;&#20013;&#30340;&#29289;&#21697;&#21644;&#21333;&#35789;&#35821;&#20041;&#31354;&#38388;&#19981;&#23545;&#40784;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.09093</link><description>&lt;p&gt;
&#22522;&#20110;&#25551;&#36848;&#24615;&#22270;&#30340;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#29289;&#21697;&#21644;&#19978;&#19979;&#25991;&#29702;&#35299;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Improving Items and Contexts Understanding with Descriptive Graph for Conversational Recommendation. (arXiv:2304.09093v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09093
&lt;/p&gt;
&lt;p&gt;
KLEVER&#26159;&#19968;&#20010;&#26032;&#30340;CRS&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#29289;&#21697;&#21644;&#23427;&#20204;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#21333;&#35789;&#32852;&#21512;&#24314;&#27169;&#22312;&#21516;&#19968;&#35821;&#20041;&#31354;&#38388;&#20013;&#65292;&#35299;&#20915;&#20102;&#20197;&#21069;&#24037;&#20316;&#20013;&#30340;&#29289;&#21697;&#21644;&#21333;&#35789;&#35821;&#20041;&#31354;&#38388;&#19981;&#23545;&#40784;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#65288;CRS&#65289;&#22312;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#25552;&#39640;&#29289;&#21697;&#21644;&#19978;&#19979;&#25991;&#21333;&#35789;&#34920;&#31034;&#26041;&#38754;&#22788;&#20110;&#26368;&#21069;&#27839;&#27700;&#24179;&#65292;&#20197;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#25512;&#33616;&#21644;&#21709;&#24212;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#29289;&#21697;&#21644;&#21333;&#35789;&#30340;&#34920;&#31034;&#36890;&#24120;&#22312;&#20004;&#20010;&#29420;&#31435;&#30340;&#35821;&#20041;&#31354;&#38388;&#20013;&#24314;&#27169;&#65292;&#36825;&#20250;&#23548;&#33268;&#23427;&#20204;&#20043;&#38388;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#24403;&#29992;&#25143;&#36755;&#20837;&#20449;&#24687;&#19981;&#36275;&#26102;&#65292;&#36825;&#23558;&#23548;&#33268;CRS&#20165;&#23454;&#29616;&#27425;&#20248;&#25490;&#21517;&#34920;&#29616;&#12290;&#20026;&#20102;&#35299;&#20915;&#20197;&#21069;&#24037;&#20316;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;CRS&#26694;&#26550;KLEVER&#65292;&#23427;&#21487;&#20197;&#32852;&#21512;&#24314;&#27169;&#22312;&#30456;&#21516;&#35821;&#20041;&#31354;&#38388;&#20013;&#30340;&#29289;&#21697;&#21644;&#23427;&#20204;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#21333;&#35789;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#20174;&#20016;&#23500;&#30340;&#29289;&#21697;&#25991;&#26412;&#29305;&#24449;&#65288;&#22914;&#29289;&#21697;&#25551;&#36848;&#21644;&#31867;&#21035;&#65289;&#20013;&#26500;&#24314;&#19968;&#20010;&#29289;&#21697;&#25551;&#36848;&#24615;&#22270;&#12290;&#22522;&#20110;&#26500;&#24314;&#30340;&#25551;&#36848;&#24615;&#22270;&#65292;KLEVER&#20849;&#21516;&#23398;&#20064;&#21333;&#35789;&#21644;&#29289;&#21697;&#30340;&#23884;&#20837;&#65292;&#20197;&#22686;&#24378;&#25512;&#33616;&#21644;&#23545;&#35805;&#29983;&#25104;&#30340;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art methods on conversational recommender systems (CRS) leverage external knowledge to enhance both items' and contextual words' representations to achieve high quality recommendations and responses generation. However, the representations of the items and words are usually modeled in two separated semantic spaces, which leads to misalignment issue between them. Consequently, this will cause the CRS to only achieve a sub-optimal ranking performance, especially when there is a lack of sufficient information from the user's input. To address limitations of previous works, we propose a new CRS framework KLEVER, which jointly models items and their associated contextual words in the same semantic space. Particularly, we construct an item descriptive graph from the rich items' textual features, such as item description and categories. Based on the constructed descriptive graph, KLEVER jointly learns the embeddings of the words and items, towards enhancing both recommender and d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#20998;&#31867;&#26041;&#27861;&#65292;&#21363;&#23558;&#19968;&#32452;&#25991;&#20214;&#20998;&#25104;&#19981;&#21516;&#30340;&#26041;&#35328;&#65292;&#20854;&#20013;&#26041;&#35328;&#30001;&#20854;&#34892;&#20026;&#27169;&#24335;&#32452;&#25104;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#36138;&#24515;&#31639;&#27861;&#20174;&#25991;&#20214;-&#28040;&#24687;&#25968;&#25454;&#30697;&#38453;&#30340;&#25968;&#25454;&#38598;&#20013;&#25512;&#26029;&#20986;&#20505;&#36873;&#26041;&#35328;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#26159;&#26368;&#20248;&#26102;&#30340;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2304.09082</link><description>&lt;p&gt;
&#25353;&#29031;&#28151;&#21512;&#30340;&#21333;&#35843;&#20998;&#35299;&#26080;&#30417;&#30563;&#32858;&#31867;&#25991;&#20214;&#26041;&#35328;
&lt;/p&gt;
&lt;p&gt;
Unsupervised clustering of file dialects according to monotonic decompositions of mixtures. (arXiv:2304.09082v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#20998;&#31867;&#26041;&#27861;&#65292;&#21363;&#23558;&#19968;&#32452;&#25991;&#20214;&#20998;&#25104;&#19981;&#21516;&#30340;&#26041;&#35328;&#65292;&#20854;&#20013;&#26041;&#35328;&#30001;&#20854;&#34892;&#20026;&#27169;&#24335;&#32452;&#25104;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#36138;&#24515;&#31639;&#27861;&#20174;&#25991;&#20214;-&#28040;&#24687;&#25968;&#25454;&#30697;&#38453;&#30340;&#25968;&#25454;&#38598;&#20013;&#25512;&#26029;&#20986;&#20505;&#36873;&#26041;&#35328;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#26159;&#26368;&#20248;&#26102;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#20998;&#31867;&#26041;&#27861;&#65292;&#26681;&#25454;&#19968;&#32452;&#31243;&#24207;&#28040;&#32791;&#30340;&#28040;&#24687;&#65292;&#23558;&#19968;&#32452;&#25991;&#20214;&#20998;&#21106;&#20026;&#19981;&#37325;&#21472;&#30340;&#26041;&#35328;&#12290;&#28040;&#24687;&#30340;&#27169;&#24335;&#21487;&#20197;&#34987;&#29992;&#20316;&#29305;&#23450;&#34892;&#20026;&#30340;&#26631;&#24535;&#65292;&#26377;&#20123;&#28040;&#24687;&#21487;&#33021;&#20250;&#21516;&#26102;&#20986;&#29616;&#65292;&#32780;&#20854;&#20182;&#28040;&#24687;&#19981;&#20250;&#12290;&#28040;&#24687;&#27169;&#24335;&#21487;&#20197;&#29992;&#26469;&#23558;&#25991;&#20214;&#20998;&#31867;&#20026;&#19981;&#21516;&#26041;&#35328;&#12290;&#19968;&#20010;&#26041;&#35328;&#30001;&#23376;&#38598;&#28040;&#24687;&#20316;&#20026;&#24517;&#38656;&#28040;&#24687;&#26469;&#23450;&#20041;&#12290;&#19968;&#26086;&#23558;&#25991;&#20214;&#32622;&#20110;&#26041;&#35328;&#21450;&#20854;&#24517;&#38656;&#30340;&#28040;&#24687;&#20043;&#19979;&#65292;&#21097;&#19979;&#30340;&#28040;&#24687;&#21017;&#26159;&#32479;&#35745;&#29420;&#31435;&#30340;&#12290;&#26377;&#20102;&#36825;&#20010;&#26041;&#35328;&#23450;&#20041;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36138;&#24515;&#31639;&#27861;&#65292;&#20174;&#25991;&#20214;-&#28040;&#24687;&#25968;&#25454;&#30697;&#38453;&#30340;&#25968;&#25454;&#38598;&#20013;&#25512;&#26029;&#20986;&#20505;&#36873;&#26041;&#35328;&#12290;&#25991;&#31456;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#22312;&#22810;&#31181;&#25991;&#20214;&#26684;&#24335;&#19978;&#30340;&#25928;&#26524;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#26159;&#26368;&#20248;&#26102;&#30340;&#26465;&#20214;&#12290;&#25991;&#31456;&#34920;&#26126;&#65292;&#20998;&#26512;&#21592;&#38656;&#35201;&#32771;&#34385;&#30340;&#26041;&#35328;&#27604;&#19981;&#21516;&#25991;&#20214;&#25152;&#38656;&#30340;&#32452;&#25968;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an unsupervised classification method that partitions a set of files into non-overlapping dialects based upon their behaviors, determined by messages produced by a collection of programs that consume them. The pattern of messages can be used as the signature of a particular kind of behavior, with the understanding that some messages are likely to co-occur, while others are not. Patterns of messages can be used to classify files into dialects. A dialect is defined by a subset of messages, called the required messages. Once files are conditioned upon dialect and its required messages, the remaining messages are statistically independent.  With this definition of dialect in hand, we present a greedy algorithm that deduces candidate dialects from a dataset consisting of a matrix of file-message data, demonstrate its performance on several file formats, and prove conditions under which it is optimal. We show that an analyst needs to consider fewer dialects than distinct 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#32467;&#21512;k-NN&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#33021;&#22815;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.09058</link><description>&lt;p&gt;
&#37325;&#35775;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;k-NN
&lt;/p&gt;
&lt;p&gt;
Revisiting k-NN for Pre-trained Language Models. (arXiv:2304.09058v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09058
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#32467;&#21512;k-NN&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#33021;&#22815;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#20316;&#20026;&#21442;&#25968;&#21270;&#30340;&#24613;&#20999;&#23398;&#20064;&#22120;&#65292;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24403;&#21069;&#33539;&#24335;&#30340;&#23454;&#38469;&#36873;&#25321;&#12290;&#19982;&#27492;&#24418;&#25104;&#23545;&#27604;&#30340;&#26159;&#65292;k-&#26368;&#36817;&#37051;&#65288;k-NN&#65289;&#20998;&#31867;&#22120;&#20316;&#20026;&#24310;&#36831;&#23398;&#20064;&#27169;&#22411;&#65292;&#20542;&#21521;&#20110;&#20943;&#36731;&#36807;&#25311;&#21512;&#21644;&#23396;&#31435;&#22122;&#22768;&#12290;&#26412;&#25991;&#20013;&#25105;&#20204;&#37325;&#35775;&#20102;k-NN&#20998;&#31867;&#22120;&#65292;&#20197;&#22686;&#24378;&#22522;&#20110;PLMs&#30340;&#20998;&#31867;&#22120;&#12290;&#20174;&#26041;&#27861;&#23618;&#38754;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#37319;&#29992;&#25991;&#26412;&#34920;&#31034;&#30340;PLMs&#22312;&#20004;&#20010;&#27493;&#39588;&#20013;&#37319;&#29992;k-NN&#65306;&#65288;1&#65289;&#21033;&#29992;k-NN&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#26469;&#26657;&#20934;&#35757;&#32451;&#36807;&#31243;&#65288;2&#65289;&#32447;&#24615;&#25554;&#20540;k-NN&#39044;&#27979;&#30340;&#27010;&#29575;&#20998;&#24067;&#21644;PLMs&#20998;&#31867;&#22120;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26680;&#24515;&#26159;&#23454;&#29616;&#20102;k-NN&#26657;&#20934;&#35757;&#32451;&#65292;&#23558;&#39044;&#27979;&#32467;&#26524;&#20316;&#20026;&#35757;&#32451;&#36807;&#31243;&#20013;&#26131;&#20110;&#21644;&#38590;&#20197;&#23398;&#20064;&#30340;&#31034;&#20363;&#30340;&#25351;&#26631;&#12290;&#20174;&#24212;&#29992;&#22330;&#26223;&#22810;&#26679;&#24615;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#25105;&#20204;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#24494;&#35843;&#12289;&#25552;&#31034;&#24494;&#35843;&#33539;&#24335;&#21644;&#38646;&#26679;&#26412;&#20219;&#21153;&#35774;&#32622;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#32467;&#21512;k-NN&#21487;&#20197;&#22312;&#25152;&#26377;&#21463;&#21040;&#26816;&#26597;&#30340;&#35774;&#32622;&#20013;&#25345;&#32493;&#25552;&#39640;PLMs&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#25152;&#26377;&#21463;&#21040;&#32771;&#34385;&#30340;&#35774;&#32622;&#20013;&#36305;&#36194;&#20102;&#22522;&#20110;&#26222;&#36890;PLMs&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Language Models (PLMs), as parametric-based eager learners, have become the de-facto choice for current paradigms of Natural Language Processing (NLP). In contrast, k-Nearest-Neighbor (k-NN) classifiers, as the lazy learning paradigm, tend to mitigate over-fitting and isolated noise. In this paper, we revisit k-NN classifiers for augmenting the PLMs-based classifiers. From the methodological level, we propose to adopt k-NN with textual representations of PLMs in two steps: (1) Utilize k-NN as prior knowledge to calibrate the training process. (2) Linearly interpolate the probability distribution predicted by k-NN with that of the PLMs' classifier. At the heart of our approach is the implementation of k-NN-calibrated training, which treats predicted results as indicators for easy versus hard examples during the training process. From the perspective of the diversity of application scenarios, we conduct extensive experiments on fine-tuning, prompt-tuning paradigms and zero-sh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#29983;&#25104;&#24335;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#20869;&#30340;&#35821;&#20041;&#32467;&#26500;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.09048</link><description>&lt;p&gt;
CodeKGC&#65306;&#29992;&#20110;&#29983;&#25104;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CodeKGC: Code Language Model for Generative Knowledge Graph Construction. (arXiv:2304.09048v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#29983;&#25104;&#24335;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#20869;&#30340;&#35821;&#20041;&#32467;&#26500;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#29983;&#25104;&#24335;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#26041;&#27861;&#36890;&#24120;&#26080;&#27861;&#25429;&#25417;&#32467;&#26500;&#24615;&#30693;&#35782;&#65292;&#32780;&#21482;&#26159;&#23558;&#33258;&#28982;&#35821;&#35328;&#36716;&#21270;&#20026;&#24207;&#21015;&#21270;&#25991;&#26412;&#25110;&#35268;&#33539;&#35821;&#35328;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20687;&#20195;&#30721;&#36825;&#26679;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#22823;&#22411;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#23637;&#29616;&#20102;&#22312;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#20197;&#36827;&#34892;&#32467;&#26500;&#24615;&#39044;&#27979;&#21644;&#25512;&#29702;&#20219;&#21153;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#29983;&#25104;&#24335;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#32473;&#23450;&#20195;&#30721;&#26684;&#24335;&#30340;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#65292;&#30446;&#26631;&#26159;&#29983;&#25104;&#21487;&#20197;&#34920;&#31034;&#20026;&#20195;&#30721;&#34917;&#20840;&#20219;&#21153;&#30340;&#19977;&#20803;&#32452;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20855;&#26377;&#27169;&#24335;&#24863;&#30693;&#22411;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#20869;&#30340;&#35821;&#20041;&#32467;&#26500;&#12290;&#30001;&#20110;&#20195;&#30721;&#26412;&#36136;&#19978;&#20855;&#26377;&#32467;&#26500;&#65292;&#22914;&#31867;&#21644;&#20989;&#25968;&#23450;&#20041;&#65292;&#22240;&#27492;&#23427;&#20316;&#20026;&#20808;&#39564;&#30340;&#35821;&#20041;&#32467;&#26500;&#30693;&#35782;&#27169;&#22411;&#38750;&#24120;&#26377;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#22522;&#20110;&#21407;&#29702;&#30340;&#29983;&#25104;&#26041;&#27861;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#21407;&#29702;&#25552;&#20379;&#20102;&#27169;&#22411;&#29983;&#25104;&#32467;&#26524;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current generative knowledge graph construction approaches usually fail to capture structural knowledge by simply flattening natural language into serialized texts or a specification language. However, large generative language model trained on structured data such as code has demonstrated impressive capability in understanding natural language for structural prediction and reasoning tasks. Intuitively, we address the task of generative knowledge graph construction with code language model: given a code-format natural language input, the target is to generate triples which can be represented as code completion tasks. Specifically, we develop schema-aware prompts that effectively utilize the semantic structure within the knowledge graph. As code inherently possesses structure, such as class and function definitions, it serves as a useful model for prior semantic structural knowledge. Furthermore, we employ a rationale-enhanced generation method to boost the performance. Rationales provi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#25928;&#25552;&#21462;&#33889;&#33796;&#29273;&#35821;&#32959;&#30244;&#20581;&#24247;&#35760;&#24405;&#20013;&#36807;&#31243;&#12289;&#33647;&#29289;&#21644;&#30142;&#30149;&#30340;&#26041;&#27861;&#65292;&#24110;&#21161;&#21307;&#25252;&#20154;&#21592;&#26356;&#39640;&#25928;&#22320;&#33719;&#21462;&#24739;&#32773;&#27835;&#30103;&#29366;&#20917;&#30340;&#23436;&#25972;&#27010;&#36848;&#65292;&#26377;&#21161;&#20110;&#25552;&#21319;&#32959;&#30244;&#27835;&#30103;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.08999</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#25552;&#21462;&#33889;&#33796;&#29273;&#35821;&#32959;&#30244;&#20581;&#24247;&#35760;&#24405;&#30340;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#25552;&#21462;&#27969;&#27700;&#32447;
&lt;/p&gt;
&lt;p&gt;
A Biomedical Entity Extraction Pipeline for Oncology Health Records in Portuguese. (arXiv:2304.08999v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08999
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#25928;&#25552;&#21462;&#33889;&#33796;&#29273;&#35821;&#32959;&#30244;&#20581;&#24247;&#35760;&#24405;&#20013;&#36807;&#31243;&#12289;&#33647;&#29289;&#21644;&#30142;&#30149;&#30340;&#26041;&#27861;&#65292;&#24110;&#21161;&#21307;&#25252;&#20154;&#21592;&#26356;&#39640;&#25928;&#22320;&#33719;&#21462;&#24739;&#32773;&#27835;&#30103;&#29366;&#20917;&#30340;&#23436;&#25972;&#27010;&#36848;&#65292;&#26377;&#21161;&#20110;&#25552;&#21319;&#32959;&#30244;&#27835;&#30103;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30284;&#30151;&#24739;&#32773;&#30340;&#25991;&#26412;&#20581;&#24247;&#35760;&#24405;&#36890;&#24120;&#24456;&#20887;&#38271;&#19988;&#39640;&#24230;&#19981;&#32467;&#26500;&#21270;&#65292;&#20351;&#24471;&#21307;&#25252;&#20154;&#21592;&#33719;&#21462;&#23436;&#25972;&#24739;&#32773;&#27835;&#30103;&#29366;&#20917;&#30340;&#23436;&#25972;&#27010;&#36848;&#38750;&#24120;&#32791;&#26102;&#12290;&#30001;&#20110;&#36825;&#20123;&#38480;&#21046;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#21644;/&#25110;&#20302;&#25928;&#30340;&#27835;&#30103;&#31243;&#24207;&#65292;&#22240;&#27492;&#21307;&#30103;&#26381;&#21153;&#25552;&#20379;&#32773;&#23558;&#26497;&#22823;&#22320;&#21463;&#30410;&#20110;&#26377;&#25928;&#22320;&#27010;&#25324;&#36825;&#20123;&#35760;&#24405;&#30340;&#31995;&#32479;&#12290;&#38543;&#30528;&#28145;&#24230;&#31070;&#32463;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#23545;&#20110;&#33521;&#35821;&#20020;&#24202;&#25991;&#26412;&#65292;&#36825;&#20010;&#30446;&#26631;&#24050;&#32463;&#37096;&#20998;&#23454;&#29616;&#65292;&#28982;&#32780;&#65292;&#30740;&#31350;&#31038;&#21306;&#20173;&#32570;&#20047;&#38024;&#23545;&#36164;&#28304;&#26377;&#38480;&#35821;&#35328;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#25105;&#20204;&#24320;&#21457;&#30340;&#26041;&#27861;&#65292;&#20174;&#27431;&#27954;&#33889;&#33796;&#29273;&#35821;&#32959;&#30244;&#20581;&#24247;&#35760;&#24405;&#20013;&#25552;&#21462;&#36807;&#31243;&#12289;&#33647;&#29289;&#21644;&#30142;&#30149;&#12290;&#36825;&#20010;&#39033;&#30446;&#19982;&#33889;&#33796;&#29273;&#32959;&#30244;&#30740;&#31350;&#25152;&#21512;&#20316;&#23436;&#25104;&#65292;&#35813;&#25152;&#38500;&#20102;&#25317;&#26377;&#21313;&#22810;&#24180;&#30340;&#21463;&#20445;&#25252;&#30340;&#21307;&#30103;&#35760;&#24405;&#22806;&#65292;&#22312;&#25972;&#20010;&#24320;&#21457;&#36807;&#31243;&#20013;&#36824;&#25552;&#20379;&#20102;&#32959;&#30244;&#23398;&#19987;&#23478;&#30340;&#19987;&#19994;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Textual health records of cancer patients are usually protracted and highly unstructured, making it very time-consuming for health professionals to get a complete overview of the patient's therapeutic course. As such limitations can lead to suboptimal and/or inefficient treatment procedures, healthcare providers would greatly benefit from a system that effectively summarizes the information of those records. With the advent of deep neural models, this objective has been partially attained for English clinical texts, however, the research community still lacks an effective solution for languages with limited resources. In this paper, we present the approach we developed to extract procedures, drugs, and diseases from oncology health records written in European Portuguese. This project was conducted in collaboration with the Portuguese Institute for Oncology which, besides holding over $10$ years of duly protected medical records, also provided oncologist expertise throughout the develop
&lt;/p&gt;</description></item><item><title>D2CSE&#26159;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#21477;&#23376;&#23884;&#20837;&#30340;&#26032;&#27169;&#22411;&#65292;&#37319;&#29992;&#22522;&#20110;&#24046;&#24322;&#24863;&#30693;&#30340;&#28145;&#24230;&#36830;&#32493;&#25552;&#31034;&#26469;&#35745;&#31639;&#20855;&#26377;&#21306;&#20998;&#24494;&#22937;&#24046;&#24322;&#33021;&#21147;&#30340;&#21477;&#23376;&#21521;&#37327;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;D2CSE&#21482;&#20351;&#29992;&#19968;&#20010;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#36991;&#20813;&#20102;&#32321;&#29712;&#30340;&#24494;&#35843;&#65292;&#24182;&#22823;&#22823;&#20943;&#23569;&#20102;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#65292;&#21516;&#26102;&#26174;&#33879;&#25552;&#39640;&#20102;&#21477;&#23376;&#23884;&#20837;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2304.08991</link><description>&lt;p&gt;
D2CSE: &#22522;&#20110;&#24046;&#24322;&#24863;&#30693;&#30340;&#28145;&#24230;&#36830;&#32493;&#25552;&#31034;&#29992;&#20110;&#23545;&#27604;&#21477;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
D2CSE: Difference-aware Deep continuous prompts for Contrastive Sentence Embeddings. (arXiv:2304.08991v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08991
&lt;/p&gt;
&lt;p&gt;
D2CSE&#26159;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#21477;&#23376;&#23884;&#20837;&#30340;&#26032;&#27169;&#22411;&#65292;&#37319;&#29992;&#22522;&#20110;&#24046;&#24322;&#24863;&#30693;&#30340;&#28145;&#24230;&#36830;&#32493;&#25552;&#31034;&#26469;&#35745;&#31639;&#20855;&#26377;&#21306;&#20998;&#24494;&#22937;&#24046;&#24322;&#33021;&#21147;&#30340;&#21477;&#23376;&#21521;&#37327;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;D2CSE&#21482;&#20351;&#29992;&#19968;&#20010;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#36991;&#20813;&#20102;&#32321;&#29712;&#30340;&#24494;&#35843;&#65292;&#24182;&#22823;&#22823;&#20943;&#23569;&#20102;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#65292;&#21516;&#26102;&#26174;&#33879;&#25552;&#39640;&#20102;&#21477;&#23376;&#23884;&#20837;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;D2CSE&#30340;&#22522;&#20110;&#24046;&#24322;&#24863;&#30693;&#30340;&#28145;&#24230;&#36830;&#32493;&#25552;&#31034;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#21477;&#23376;&#23884;&#20837;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;D2CSE&#37319;&#29992;&#20102;&#31616;&#21333;&#30340;&#31070;&#32463;&#26550;&#26500;&#26469;&#35745;&#31639;&#21477;&#23376;&#21521;&#37327;&#65292;&#20351;&#24471;&#20854;&#23545;&#20110;&#22312;&#31867;&#20284;&#21477;&#23376;&#20013;&#21306;&#20998;&#24494;&#22937;&#24046;&#24322;&#26377;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;&#19982;&#38656;&#35201;&#22810;&#20010;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22788;&#29702;&#21407;&#22987;&#21644;&#25439;&#22351;&#65288;&#24494;&#22937;&#20462;&#25913;&#65289;&#21477;&#23376;&#23545;&#30340;&#29616;&#26377;&#31070;&#32463;&#32593;&#32476;&#19981;&#21516;&#65292;D2CSE&#20165;&#36890;&#36807;&#25191;&#34892;&#22810;&#20010;&#20219;&#21153;&#65288;&#21363;&#65292;&#23545;&#27604;&#23398;&#20064;&#21644;&#26465;&#20214;&#26367;&#25442;&#26631;&#35760;&#26816;&#27979;&#65289;&#33258;&#20027;&#24341;&#23548;&#22320;&#20248;&#21270;&#20102;&#36830;&#32493;&#25552;&#31034;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#32321;&#29712;&#30340;&#22810;&#20010;PLMs&#30340;&#24494;&#35843;&#12290;D2CSE&#23558;&#21333;&#20010;PLM&#37325;&#36733;&#21040;&#36830;&#32493;&#25552;&#31034;&#19978;&#65292;&#22823;&#22823;&#33410;&#30465;&#20102;&#23384;&#20648;&#31354;&#38388;&#12290; D2CSE&#30340;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#32422;&#20026;&#29616;&#26377;&#26041;&#27861;&#30340;1&#65285;&#65292;&#21516;&#26102;&#26174;&#33879;&#25552;&#39640;&#20102;&#21477;&#23376;&#23884;&#20837;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes Difference-aware Deep continuous prompt for Contrastive Sentence Embeddings (D2CSE) that learns sentence embeddings. Compared to state-of-the-art approaches, D2CSE computes sentence vectors that are exceptional to distinguish a subtle difference in similar sentences by employing a simple neural architecture for continuous prompts. Unlike existing architectures that require multiple pretrained language models (PLMs) to process a pair of the original and corrupted (subtly modified) sentences, D2CSE avoids cumbersome fine-tuning of multiple PLMs by only optimizing continuous prompts by performing multiple tasks -- i.e., contrastive learning and conditional replaced token detection all done in a self-guided manner. D2CSE overloads a single PLM on continuous prompts and greatly saves memory consumption as a result. The number of training parameters in D2CSE is reduced to about 1\% of existing approaches while substantially improving the quality of sentence embeddings. W
&lt;/p&gt;</description></item><item><title>&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#25361;&#25112;&#36187;&#65288;MER 2023&#65289;&#25552;&#20986;&#20102;&#19977;&#20010;&#23376;&#25361;&#25112;&#65306;MER-MULTI&#12289;MER-NOISE&#21644;MER-SEMI&#65292;&#20026;&#20840;&#29699;&#30740;&#31350;&#20154;&#21592;&#26500;&#24314;&#21019;&#26032;&#25216;&#26415;&#25552;&#20379;&#20102;&#28608;&#21169;&#65292;&#24182;&#27979;&#35797;&#20102;&#21508;&#31181;&#22810;&#27169;&#24577;&#29305;&#24449;&#65292;&#25552;&#20379;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#22522;&#32447;&#65292;&#20197;&#20419;&#36827;&#40065;&#26834;&#32780;&#26377;&#25928;&#30340;&#31639;&#27861;&#30340;&#21457;&#23637;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.08981</link><description>&lt;p&gt;
MER 2023: &#22810;&#26631;&#31614;&#23398;&#20064;&#65292;&#27169;&#24577;&#40065;&#26834;&#24615;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MER 2023: Multi-label Learning, Modality Robustness, and Semi-Supervised Learning. (arXiv:2304.08981v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08981
&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#25361;&#25112;&#36187;&#65288;MER 2023&#65289;&#25552;&#20986;&#20102;&#19977;&#20010;&#23376;&#25361;&#25112;&#65306;MER-MULTI&#12289;MER-NOISE&#21644;MER-SEMI&#65292;&#20026;&#20840;&#29699;&#30740;&#31350;&#20154;&#21592;&#26500;&#24314;&#21019;&#26032;&#25216;&#26415;&#25552;&#20379;&#20102;&#28608;&#21169;&#65292;&#24182;&#27979;&#35797;&#20102;&#21508;&#31181;&#22810;&#27169;&#24577;&#29305;&#24449;&#65292;&#25552;&#20379;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#22522;&#32447;&#65292;&#20197;&#20419;&#36827;&#40065;&#26834;&#32780;&#26377;&#25928;&#30340;&#31639;&#27861;&#30340;&#21457;&#23637;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#21313;&#24180;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#30340;&#21457;&#23637;&#20351;&#24471;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#21462;&#24471;&#20102;&#26174;&#30528;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25216;&#26415;&#38590;&#20197;&#28385;&#36275;&#23454;&#38469;&#24212;&#29992;&#30340;&#38656;&#27714;&#12290;&#20026;&#20102;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#21457;&#36215;&#20102;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#25361;&#25112;&#36187;&#65288;MER 2023&#65289;&#65292;&#20197;&#28608;&#21169;&#20840;&#29699;&#30740;&#31350;&#20154;&#21592;&#26500;&#24314;&#21019;&#26032;&#25216;&#26415;&#65292;&#36827;&#19968;&#27493;&#21152;&#36895;&#21644;&#20419;&#36827;&#30740;&#31350;&#12290;&#38024;&#23545;&#20170;&#24180;&#30340;&#25361;&#25112;&#36187;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#19981;&#21516;&#30340;&#23376;&#25361;&#25112;&#65306;&#65288;1&#65289;MER-MULTI&#65292;&#21442;&#36187;&#32773;&#38656;&#35201;&#35782;&#21035;&#31163;&#25955;&#21644;&#32500;&#24230;&#24773;&#24863;&#65307;&#65288;2&#65289;MER-NOISE&#65292;&#22312;&#27979;&#35797;&#35270;&#39057;&#20013;&#28155;&#21152;&#22122;&#22768;&#65292;&#20197;&#35780;&#20272;&#27169;&#24577;&#40065;&#26834;&#24615;&#65307;&#65288;3&#65289;MER-SEMI&#65292;&#25552;&#20379;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#26679;&#26412;&#65292;&#29992;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#21508;&#31181;&#22810;&#27169;&#24577;&#29305;&#24449;&#65292;&#24182;&#20026;&#27599;&#20010;&#23376;&#25361;&#25112;&#25552;&#20379;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#22522;&#32447;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;MER-MULTI&#19978;&#33719;&#24471;&#20102;77.57&#65285;&#30340;F1&#20998;&#25968;&#21644;0.82&#30340;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#65292;&#22312;MER-NOISE&#19978;&#33719;&#24471;&#20102;69.82&#65285;&#30340;F1&#20998;&#25968;&#21644;0.75&#30340;MSE&#65292;&#22312;MER-SEMI&#19978;&#33719;&#24471;&#20102;69.39&#65285;&#30340;F1&#20998;&#25968;&#21644;0.80&#30340;MSE&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#20010;&#25361;&#25112;&#36187;&#33021;&#22815;&#28608;&#21457;&#26356;&#22810;&#30340;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#65292;&#24182;&#20419;&#36827;&#40065;&#26834;&#32780;&#26377;&#25928;&#30340;&#31639;&#27861;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past few decades, multimodal emotion recognition has made remarkable progress with the development of deep learning. However, existing technologies are difficult to meet the demand for practical applications. To improve the robustness, we launch a Multimodal Emotion Recognition Challenge (MER 2023) to motivate global researchers to build innovative technologies that can further accelerate and foster research. For this year's challenge, we present three distinct sub-challenges: (1) MER-MULTI, in which participants recognize both discrete and dimensional emotions; (2) MER-NOISE, in which noise is added to test videos for modality robustness evaluation; (3) MER-SEMI, which provides large amounts of unlabeled samples for semi-supervised learning. In this paper, we test a variety of multimodal features and provide a competitive baseline for each sub-challenge. Our system achieves 77.57% on the F1 score and 0.82 on the mean squared error (MSE) for MER-MULTI, 69.82% on the F1 score a
&lt;/p&gt;</description></item><item><title>LLMs&#22312;&#20844;&#20247;&#20013;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#26159;&#30446;&#21069;&#22823;&#37096;&#20998;&#26816;&#27979;&#24037;&#20855;&#23384;&#22312;&#20005;&#37325;&#32570;&#38519;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;LLMs&#23481;&#26131;&#24494;&#35843;&#19988;&#38590;&#20197;&#34987;&#20854;&#20182;LLMs&#26816;&#27979;&#21040;&#12290;</title><link>http://arxiv.org/abs/2304.08968</link><description>&lt;p&gt;
&#38543;&#26426;&#40550;&#40521;&#23547;&#25214;&#38543;&#26426;&#40550;&#40521;&#65306;LLMs&#26131;&#20110;&#24494;&#35843;&#19988;&#38590;&#20197;&#34987;&#20854;&#20182;LLMs&#26816;&#27979;&#21040;
&lt;/p&gt;
&lt;p&gt;
Stochastic Parrots Looking for Stochastic Parrots: LLMs are Easy to Fine-Tune and Hard to Detect with other LLMs. (arXiv:2304.08968v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08968
&lt;/p&gt;
&lt;p&gt;
LLMs&#22312;&#20844;&#20247;&#20013;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#26159;&#30446;&#21069;&#22823;&#37096;&#20998;&#26816;&#27979;&#24037;&#20855;&#23384;&#22312;&#20005;&#37325;&#32570;&#38519;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;LLMs&#23481;&#26131;&#24494;&#35843;&#19988;&#38590;&#20197;&#34987;&#20854;&#20182;LLMs&#26816;&#27979;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#27880;&#24847;&#21147;&#38761;&#21629;&#20351;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#24471;&#20197;&#25193;&#23637;&#24182;&#23454;&#29616;&#36234;&#26469;&#36234;&#24778;&#20154;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#31216;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#26368;&#36817;&#30001;&#20110;&#23545;&#35805;&#24494;&#35843;&#32780;&#22312;&#20844;&#20247;&#20013;&#33719;&#24471;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20174;&#32780;&#20351;&#20854;&#34892;&#20026;&#31526;&#21512;&#20844;&#20247;&#23545;&#20110;AI&#30340;&#26399;&#26395;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31361;&#20986;&#20063;&#21152;&#22823;&#20102;&#20851;&#27880;LLMs&#35823;&#29992;&#30340;&#20808;&#21069;&#25285;&#24551;&#65292;&#24182;&#23548;&#33268;&#20986;&#29616;&#35768;&#22810;&#22312;&#37326;&#22806;&#26816;&#27979;LLMs&#30340;&#24037;&#20855;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22823;&#22810;&#25968;&#36825;&#26679;&#30340;&#24037;&#20855;&#37117;&#23384;&#22312;&#20005;&#37325;&#32570;&#38519;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#23637;&#31034;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22823;&#22823;&#38477;&#20302;&#22522;&#20110;&#24494;&#35843;&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#26816;&#27979;LLMs&#30340;&#25104;&#21151;&#29575;&#65292;&#24182;&#35828;&#26126;&#25105;&#20204;&#30340;&#24037;&#20316;&#28041;&#21450;&#30340;&#37325;&#35201;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;
The self-attention revolution allowed generative language models to scale and achieve increasingly impressive abilities. Such models - commonly referred to as Large Language Models (LLMs) - have recently gained prominence with the general public, thanks to conversational fine-tuning, putting their behavior in line with public expectations regarding AI. This prominence amplified prior concerns regarding the misuse of LLMs and led to the emergence of numerous tools to detect LLMs in the wild.  Unfortunately, most such tools are critically flawed. While major publications in the LLM detectability field suggested that LLMs were easy to detect with fine-tuned autoencoders, the limitations of their results are easy to overlook. Specifically, they assumed publicly available generative models without fine-tunes or non-trivial prompts. While the importance of these assumptions has been demonstrated, until now, it remained unclear how well such detection could be countered.  Here, we show that a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#34917;&#20805;&#25945;&#26448;&#30340;&#26377;&#36259;&#35270;&#35273;&#25903;&#25345;&#65292;&#20197;&#20419;&#36827;&#23398;&#29983;&#30340;&#23398;&#20064;&#12290;&#36890;&#36807;&#20248;&#21270;&#38382;&#39064;&#35299;&#20915;&#29616;&#26377;&#25945;&#26448;&#20013;&#32570;&#20047;&#30340;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#30340;&#25928;&#26524;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.08931</link><description>&lt;p&gt;
&#21033;&#29992;&#32593;&#32476;&#22270;&#29255;&#25552;&#21319;&#25945;&#26448;&#30340;&#21487;&#35270;&#21270;&#25928;&#26524;&#20197;&#20419;&#36827;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Enhancing Textbooks with Visuals from the Web for Improved Learning. (arXiv:2304.08931v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08931
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#34917;&#20805;&#25945;&#26448;&#30340;&#26377;&#36259;&#35270;&#35273;&#25903;&#25345;&#65292;&#20197;&#20419;&#36827;&#23398;&#29983;&#30340;&#23398;&#20064;&#12290;&#36890;&#36807;&#20248;&#21270;&#38382;&#39064;&#35299;&#20915;&#29616;&#26377;&#25945;&#26448;&#20013;&#32570;&#20047;&#30340;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#30340;&#25928;&#26524;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25945;&#26448;&#26159;&#21521;&#23398;&#29983;&#20256;&#36798;&#20248;&#36136;&#25945;&#32946;&#30340;&#20027;&#35201;&#26041;&#24335;&#12290;&#24050;&#32463;&#35777;&#26126;&#65292;&#35299;&#37322;&#24615;&#25110;&#20855;&#35937;&#21270;&#30340;&#21487;&#35270;&#21270;&#20869;&#23481;&#22312;&#30693;&#35782;&#30340;&#35760;&#24518;&#12289;&#29702;&#35299;&#21644;&#20256;&#36882;&#26041;&#38754;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#21457;&#23637;&#20013;&#22269;&#23478;&#65292;&#35768;&#22810;&#25945;&#31185;&#20070;&#21697;&#36136;&#36739;&#20302;&#19988;&#32570;&#20047;&#26377;&#36259;&#30340;&#35270;&#35273;&#25903;&#25345;&#20197;&#21161;&#21147;&#20110;&#23398;&#29983;&#30340;&#23398;&#20064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#33258;&#21160;&#20351;&#29992;&#26469;&#33258;&#32593;&#32476;&#30340;&#22270;&#20687;&#20026;&#25945;&#26448;&#22686;&#28155;&#22270;&#29255;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#26469;&#33258;&#19990;&#30028;&#19978;&#26368;&#22823;&#30340;&#20813;&#36153;&#22312;&#32447;&#20986;&#29256;&#21830;&#20043;&#19968;&#30340;&#30005;&#23376;&#25945;&#26448;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20005;&#23494;&#30340;&#25968;&#25454;&#38598;&#20998;&#26512;&#65292;&#24182;&#21033;&#29992;&#25152;&#24471;&#30340;&#20998;&#26512;&#32467;&#26524;&#28608;&#21457;&#20102;&#19968;&#39033;&#20219;&#21153;&#65292;&#26088;&#22312;&#26816;&#32034;&#24182;&#36866;&#24403;&#20998;&#37197;&#32593;&#32476;&#22270;&#29255;&#21040;&#25945;&#26448;&#20013;&#65292;&#25105;&#20204;&#23558;&#20854;&#20316;&#20026;&#19968;&#39033;&#26032;&#39062;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#20247;&#21253;&#35780;&#20272;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#65306;(1)&#34429;&#28982;&#21407;&#22987;&#25945;&#26448;&#22270;&#20687;&#35780;&#32423;&#26356;&#39640;&#65292;&#20294;&#33258;&#21160;&#20998;&#37197;&#22270;&#20687;&#24182;&#19981;&#30456;&#36317;&#22826;&#36828;&#65292;&#24182;&#19988;(2)&#36873;&#25321;&#30340;&#22270;&#20687;&#26469;&#28304;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Textbooks are the primary vehicle for delivering quality education to students. It has been shown that explanatory or illustrative visuals play a key role in the retention, comprehension and the general transfer of knowledge. However, many textbooks, especially in the developing world, are low quality and lack interesting visuals to support student learning. In this paper, we investigate the effectiveness of vision-language models to automatically enhance textbooks with images from the web. Specifically, we collect a dataset of e-textbooks from one of the largest free online publishers in the world. We rigorously analyse the dataset, and use the resulting analysis to motivate a task that involves retrieving and appropriately assigning web images to textbooks, which we frame as a novel optimization problem. Through a crowd-sourced evaluation, we verify that (1) while the original textbook images are rated higher, automatically assigned ones are not far behind, and (2) the choice of the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#32467;&#21512;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#36136;&#37327;&#35780;&#20272;&#31995;&#32479;&#65292;&#38024;&#23545;&#25968;&#25454;&#32570;&#20047;&#21644;&#39046;&#22495;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#22312;&#36890;&#29992;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#32467;&#26524;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2304.08891</link><description>&lt;p&gt;
&#20026;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#35780;&#20272;&#37327;&#36523;&#23450;&#21046;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Tailoring Domain Adaptation for Machine Translation Quality Estimation. (arXiv:2304.08891v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#32467;&#21512;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#36136;&#37327;&#35780;&#20272;&#31995;&#32479;&#65292;&#38024;&#23545;&#25968;&#25454;&#32570;&#20047;&#21644;&#39046;&#22495;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#22312;&#36890;&#29992;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#32467;&#26524;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36136;&#37327;&#35780;&#20272;&#23545;&#32763;&#35793;&#27969;&#31243;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20854;&#26377;&#25928;&#24615;&#21462;&#20915;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#21644;&#36136;&#37327;&#12290;&#23545;&#20110;&#29305;&#23450;&#30340;&#36136;&#37327;&#35780;&#20272;&#32780;&#35328;&#65292;&#30001;&#20110;&#26631;&#35760;&#36825;&#26679;&#30340;&#25968;&#25454;&#30340;&#25104;&#26412;&#21644;&#24037;&#20316;&#37327;&#39640;&#26114;&#65292;&#22240;&#27492;&#39640;&#36136;&#37327;&#30340;&#26631;&#35760;&#25968;&#25454;&#32463;&#24120;&#32570;&#20047;&#12290;&#38500;&#20102;&#25968;&#25454;&#32570;&#20047;&#26041;&#38754;&#30340;&#25361;&#25112;&#22806;&#65292;&#36136;&#37327;&#35780;&#20272;&#27169;&#22411;&#36824;&#24212;&#20855;&#26377;&#27867;&#21270;&#24615;&#65292;&#21363;&#23427;&#20204;&#24212;&#35813;&#33021;&#22815;&#22788;&#29702;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#65292;&#21253;&#25324;&#36890;&#29992;&#39046;&#22495;&#21644;&#29305;&#23450;&#39046;&#22495;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#23558;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#25968;&#25454;&#22686;&#24378;&#36827;&#34892;&#20102;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#36136;&#37327;&#35780;&#20272;&#31995;&#32479;&#12290;&#26041;&#27861;&#26159;&#20808;&#35757;&#32451;&#19968;&#20010;&#36890;&#29992;&#36136;&#37327;&#35780;&#20272;&#27169;&#22411;&#65292;&#28982;&#21518;&#22312;&#20445;&#30041;&#36890;&#29992;&#30693;&#35782;&#30340;&#21516;&#26102;&#23545;&#29305;&#23450;&#39046;&#22495;&#36827;&#34892;&#24494;&#35843;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25152;&#26377;&#30740;&#31350;&#30340;&#35821;&#35328;&#23545;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22343;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#65292;&#24182;&#20855;&#26377;&#26356;&#22909;&#30340;&#36328;&#35821;&#35328;&#25512;&#26029;&#65292;&#30456;&#27604;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#22312;&#38646;-shot&#23398;&#20064;&#26041;&#26696;&#20013;&#20855;&#26377;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While quality estimation (QE) can play an important role in the translation process, its effectiveness relies on the availability and quality of training data. For QE in particular, high-quality labeled data is often lacking due to the high-cost and effort associated with labeling such data. Aside from the data scarcity challenge, QE models should also be generalizable, i.e., they should be able to handle data from different domains, both generic and specific. To alleviate these two main issues -- data scarcity and domain mismatch -- this paper combines domain adaptation and data augmentation within a robust QE system. Our method is to first train a generic QE model and then fine-tune it on a specific domain while retaining generic knowledge. Our results show a significant improvement for all the language pairs investigated, better cross-lingual inference, and a superior performance in zero-shot learning scenarios as compared to state-of-the-art baselines.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#32034;&#21033;&#29992;&#22823;&#35268;&#27169;&#36716;&#20889;&#26469;&#25552;&#21319;&#22823;&#22411;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#22788;&#29702;&#20302;&#36164;&#28304;&#21644;&#26410;&#30693;&#35821;&#35328;&#30340;&#33021;&#21147;&#65292;&#21033;&#29992;UROMAN&#36716;&#20889;&#24037;&#20855;&#30340;&#28508;&#21147;&#65292;&#24182;&#30740;&#31350;&#20102;&#19968;&#31995;&#21015;&#39640;&#25928;&#30340;&#31574;&#30053;&#65292;&#20197;&#36866;&#24212;&#21508;&#31181;&#35821;&#35328;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2304.08865</link><description>&lt;p&gt;
&#22522;&#20110;&#32599;&#39532;&#21270;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#22823;&#35268;&#27169;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Romanization-based Large-scale Adaptation of Multilingual Language Models. (arXiv:2304.08865v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08865
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#32034;&#21033;&#29992;&#22823;&#35268;&#27169;&#36716;&#20889;&#26469;&#25552;&#21319;&#22823;&#22411;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#22788;&#29702;&#20302;&#36164;&#28304;&#21644;&#26410;&#30693;&#35821;&#35328;&#30340;&#33021;&#21147;&#65292;&#21033;&#29992;UROMAN&#36716;&#20889;&#24037;&#20855;&#30340;&#28508;&#21147;&#65292;&#24182;&#30740;&#31350;&#20102;&#19968;&#31995;&#21015;&#39640;&#25928;&#30340;&#31574;&#30053;&#65292;&#20197;&#36866;&#24212;&#21508;&#31181;&#35821;&#35328;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;mPLMs&#65289;&#24050;&#25104;&#20026;&#36328;&#35821;&#35328;NLP&#20013;&#30340;&#20107;&#23454;&#26631;&#20934;&#65292;&#20294;&#26159;&#65292;&#23427;&#20204;&#22312;&#35768;&#22810;&#35821;&#35328;&#30340;&#22823;&#35268;&#27169;&#37096;&#32626;&#26041;&#38754;&#21463;&#21040;&#35832;&#22810;&#38480;&#21046;&#65292;&#21253;&#25324;&#39044;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#12289;&#35789;&#27719;&#37327;&#22686;&#21152;&#21644;&#21442;&#25968;&#39044;&#31639;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#22686;&#24378;mPLMs&#22788;&#29702;&#20302;&#36164;&#28304;&#21644;&#26410;&#30693;&#35821;&#35328;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25506;&#32034;&#22823;&#35268;&#27169;&#21033;&#29992;&#36716;&#20889;&#30340;&#28508;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25506;&#32034;UROMAN&#36716;&#20889;&#24037;&#20855;&#30340;&#28508;&#21147;&#65292;&#35813;&#24037;&#20855;&#20026;&#25152;&#26377;&#20070;&#20889;&#31995;&#32479;&#25552;&#20379;&#20102;&#20174;UTF-8&#21040;&#25289;&#19969;&#23383;&#31526;&#30340;&#26144;&#23556;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#20960;&#20046;&#20219;&#20309;&#35821;&#35328;&#30340;&#24265;&#20215;&#32599;&#39532;&#21270;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#20102;UROMAN&#30456;&#23545;&#20110;&#20854;&#20182;&#35821;&#35328;&#29305;&#23450;&#21644;&#25163;&#21160;&#31574;&#21010;&#30340;&#36716;&#20889;&#24037;&#20855;&#22312;&#36866;&#24212;&#22810;&#35821;&#35328;PLMs&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#24182;&#27604;&#36739;&#20102;&#19968;&#31995;&#21015;&#25968;&#25454;&#21644;&#21442;&#25968;&#39640;&#25928;&#30340;&#31574;&#30053;&#65292;&#20197;&#36866;&#24212;&#32599;&#39532;&#21270;&#21644;&#38750;&#32599;&#39532;&#21270;&#30340;14&#31181;&#19981;&#21516;&#20197;&#19978;&#35821;&#35328;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large multilingual pretrained language models (mPLMs) have become the de facto state of the art for cross-lingual transfer in NLP. However, their large-scale deployment to many languages, besides pretraining data scarcity, is also hindered by the increase in vocabulary size and limitations in their parameter budget. In order to boost the capacity of mPLMs to deal with low-resource and unseen languages, we explore the potential of leveraging transliteration on a massive scale. In particular, we explore the UROMAN transliteration tool, which provides mappings from UTF-8 to Latin characters for all the writing systems, enabling inexpensive romanization for virtually any language. We first focus on establishing how UROMAN compares against other language-specific and manually curated transliterators for adapting multilingual PLMs. We then study and compare a plethora of data- and parameter-efficient strategies for adapting the mPLMs to romanized and non-romanized corpora of 14 diverse low-r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36817;&#20284;&#26368;&#36817;&#37051;&#30701;&#35821;&#25366;&#25496;&#30340;&#26041;&#27861;&#26469;&#35757;&#32451;&#19978;&#19979;&#25991;&#24863;&#30693;Transformer&#36716;&#24405;&#22120;(CATT)&#27169;&#22411;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.08862</link><description>&lt;p&gt;
&#36817;&#20284;&#26368;&#36817;&#37051;&#30701;&#35821;&#25366;&#25496;&#22312;&#19978;&#19979;&#25991;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Approximate Nearest Neighbour Phrase Mining for Contextual Speech Recognition. (arXiv:2304.08862v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36817;&#20284;&#26368;&#36817;&#37051;&#30701;&#35821;&#25366;&#25496;&#30340;&#26041;&#27861;&#26469;&#35757;&#32451;&#19978;&#19979;&#25991;&#24863;&#30693;Transformer&#36716;&#24405;&#22120;(CATT)&#27169;&#22411;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36817;&#20284;&#26368;&#36817;&#37051;&#30701;&#35821;&#25366;&#25496;&#30340;&#26041;&#27861;&#26469;&#35757;&#32451;&#31471;&#21040;&#31471;&#19978;&#19979;&#25991;&#24863;&#30693;Transformer&#36716;&#24405;&#22120;(CATT)&#27169;&#22411;&#30340;&#25193;&#23637;&#26041;&#27861;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#32473;&#23450;&#19968;&#20010;&#21442;&#32771;&#26597;&#35810;&#65292;&#25105;&#20204;&#20351;&#29992;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#25366;&#25496;&#20102;&#33509;&#24178;&#30456;&#20284;&#30340;&#30701;&#35821;&#20316;&#20026;&#36127;&#20363;&#65292;&#24182;&#23558;&#36825;&#20123;&#30701;&#35821;&#19982;&#38543;&#26426;&#21644;&#30495;&#23454;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#19968;&#36215;&#29992;&#20316;&#19978;&#19979;&#25991;&#21015;&#34920;&#20013;&#30340;&#36127;&#20363;&#12290;&#36890;&#36807;&#23558;&#36817;&#20284;&#26368;&#36817;&#37051;&#30701;&#35821;&#65288;ANN-P&#65289;&#21253;&#21547;&#22312;&#19978;&#19979;&#25991;&#21015;&#34920;&#20013;&#65292;&#25105;&#20204;&#40723;&#21169;&#23398;&#20064;&#34920;&#31034;&#26469;&#21306;&#20998;&#30456;&#20284;&#20294;&#19981;&#23436;&#20840;&#30456;&#21516;&#30340;&#20559;&#35265;&#30701;&#35821;&#65292;&#20174;&#32780;&#22312;&#20559;&#35265;&#28165;&#21333;&#20013;&#23384;&#22312;&#20960;&#20010;&#30456;&#20284;&#30340;&#30701;&#35821;&#26102;&#25552;&#39640;&#20559;&#35265;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#24773;&#20917;&#19979;&#36827;&#34892;&#23454;&#39564;&#65292;&#33719;&#24471;&#20102;&#30456;&#23545;&#23383;&#35823;&#29575;&#36798;7&#65285;&#30340;&#19978;&#19979;&#25991;&#37096;&#20998;&#30340;&#23454;&#39564;&#25928;&#26524;&#12290;&#25105;&#20204;&#36824;&#25193;&#23637;&#24182;&#35780;&#20272;&#20102;CATT&#26041;&#27861;&#22312;&#20018;&#27969;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an extension to train end-to-end Context-Aware Transformer Transducer ( CATT ) models by using a simple, yet efficient method of mining hard negative phrases from the latent space of the context encoder. During training, given a reference query, we mine a number of similar phrases using approximate nearest neighbour search. These sampled phrases are then used as negative examples in the context list alongside random and ground truth contextual information. By including approximate nearest neighbour phrases (ANN-P) in the context list, we encourage the learned representation to disambiguate between similar, but not identical, biasing phrases. This improves biasing accuracy when there are several similar phrases in the biasing inventory. We carry out experiments in a large-scale data regime obtaining up to 7% relative word error rate reductions for the contextual portion of test data. We also extend and evaluate CATT approach in streaming applications.
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#19982;&#20854;&#20851;&#31995;&#32039;&#23494;&#30340;&#35821;&#35328;&#65292;&#21487;&#25913;&#21892;&#21040;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#65292;&#20197;&#26031;&#22570;&#30340;&#32435;&#32500;&#20122;&#35821;&#35328;&#23478;&#26063;&#20013;&#30340;&#20854;&#20182;&#35821;&#35328;&#36164;&#28304;&#25968;&#25454;&#20248;&#21270;&#27861;&#32599;&#35821;&#30340;NLP&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.08823</link><description>&lt;p&gt;
&#36890;&#36807;&#25509;&#36817;&#30340;&#35821;&#35328;&#23454;&#29616;&#21040;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#36716;&#31227;: &#20197;&#27861;&#32599;&#35821;&#20026;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Transfer to a Low-Resource Language via Close Relatives: The Case Study on Faroese. (arXiv:2304.08823v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08823
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#19982;&#20854;&#20851;&#31995;&#32039;&#23494;&#30340;&#35821;&#35328;&#65292;&#21487;&#25913;&#21892;&#21040;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#65292;&#20197;&#26031;&#22570;&#30340;&#32435;&#32500;&#20122;&#35821;&#35328;&#23478;&#26063;&#20013;&#30340;&#20854;&#20182;&#35821;&#35328;&#36164;&#28304;&#25968;&#25454;&#20248;&#21270;&#27861;&#32599;&#35821;&#30340;NLP&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#31181;&#35821;&#35328;&#27169;&#22411;&#25512;&#21160;&#20102;&#36328;&#35821;&#35328;NLP&#36716;&#31227;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#38646;-shot&#36328;&#35821;&#35328;&#20256;&#36755;&#20351;&#29992;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#35821;&#31181;&#21464;&#21387;&#22120;&#65288;&#20363;&#22914;mBERT&#25110;XLM-R&#65289;&#20256;&#36755;&#21040;&#25152;&#26377;&#30446;&#26631;&#35821;&#35328;&#65292;&#32780;&#19981;&#32771;&#34385;&#23427;&#20204;&#19982;&#20854;&#20182;&#35821;&#35328;&#30340;&#35821;&#35328;&#23398;&#12289;&#35789;&#28304;&#21644;&#31995;&#32479;&#21457;&#32946;&#20851;&#31995;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#27861;&#32599;&#35821;&#8212;&#8212;&#19968;&#31181;&#26469;&#33258;&#39640;&#36164;&#28304;&#35821;&#35328;&#23478;&#26063;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#23454;&#39564;&#35777;&#26126;&#36890;&#36807;&#21033;&#29992;&#31995;&#32479;&#21457;&#32946;&#20449;&#24687;&#21644;&#31163;&#24320;&#8220;&#19968;&#20992;&#20999;&#8221;&#30340;&#33539;&#24335;&#65292;&#21487;&#20197;&#25913;&#21892;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21033;&#29992;&#20854;&#20182;&#26031;&#22570;&#30340;&#32435;&#32500;&#20122;&#35821;&#35328;&#30340;&#20016;&#23500;&#36164;&#28304;&#65288;&#22914;&#20025;&#40614;&#35821;&#12289;&#25386;&#23041;&#35821;&#12289;&#29790;&#20856;&#35821;&#21644;&#20912;&#23707;&#35821;&#65289;&#26469;&#22686;&#24378;&#27861;&#32599;&#35821;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#36328;&#35821;&#35328;&#36716;&#31227;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual language models have pushed state-of-the-art in cross-lingual NLP transfer. The majority of zero-shot cross-lingual transfer, however, use one and the same massively multilingual transformer (e.g., mBERT or XLM-R) to transfer to all target languages, irrespective of their typological, etymological, and phylogenetic relations to other languages. In particular, readily available data and models of resource-rich sibling languages are often ignored. In this work, we empirically show, in a case study for Faroese -- a low-resource language from a high-resource language family -- that by leveraging the phylogenetic information and departing from the 'one-size-fits-all' paradigm, one can improve cross-lingual transfer to low-resource languages. In particular, we leverage abundant resources of other Scandinavian languages (i.e., Danish, Norwegian, Swedish, and Icelandic) for the benefit of Faroese. Our evaluation results show that we can substantially improve the transfer performan
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TTIDA&#30340;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21033;&#29992;&#25991;&#26412;&#21040;&#25991;&#26412;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#29983;&#25104;&#21487;&#25511;&#30340;&#36924;&#30495;&#26631;&#35760;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2304.08821</link><description>&lt;p&gt;
TTIDA: &#36890;&#36807;&#25991;&#26412;&#21040;&#25991;&#26412;&#27169;&#22411;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#36827;&#34892;&#21487;&#25511;&#29983;&#25104;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
TTIDA: Controllable Generative Data Augmentation via Text-to-Text and Text-to-Image Models. (arXiv:2304.08821v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TTIDA&#30340;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21033;&#29992;&#25991;&#26412;&#21040;&#25991;&#26412;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#29983;&#25104;&#21487;&#25511;&#30340;&#36924;&#30495;&#26631;&#35760;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#22686;&#34917;&#20302;&#36164;&#28304;&#25968;&#25454;&#38598;&#26377;&#29992;&#20449;&#24687;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#20256;&#32479;&#30340;&#22686;&#24378;&#25216;&#26415;&#65292;&#22914;&#22122;&#22768;&#27880;&#20837;&#21644;&#22270;&#20687;&#21464;&#25442;&#65292;&#24050;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#27492;&#22806;&#65292;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#65288;GDA&#65289;&#24050;&#34987;&#35777;&#26126;&#33021;&#22815;&#20135;&#29983;&#26356;&#22810;&#26679;&#21270;&#21644;&#28789;&#27963;&#30340;&#25968;&#25454;&#12290;&#34429;&#28982;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#32463;&#24120;&#29992;&#20110;GDA&#65292;&#20294;&#19982;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30456;&#27604;&#65292;&#23427;&#20204;&#32570;&#20047;&#22810;&#26679;&#24615;&#21644;&#21487;&#25511;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TTIDA&#65288;&#25991;&#26412;&#21040;&#25991;&#26412;&#21040;&#22270;&#20687;&#25968;&#25454;&#22686;&#24378;&#65289;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#25991;&#26412;&#65288;T2T&#65289;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#12290;&#36890;&#36807;&#23558;T2I&#27169;&#22411;&#30340;&#26465;&#20214;&#35774;&#32622;&#20026;T2T&#27169;&#22411;&#29983;&#25104;&#30340;&#35814;&#32454;&#25551;&#36848;&#65292;&#25105;&#20204;&#33021;&#22815;&#20197;&#28789;&#27963;&#21644;&#21487;&#25511;&#30340;&#26041;&#24335;&#29983;&#25104;&#36924;&#30495;&#30340;&#26631;&#35760;&#22270;&#20687;&#12290;&#22312;&#39046;&#22495;&#20869;&#20998;&#31867;&#12289;&#36328;&#39046;&#22495;&#20998;&#31867;&#21644;&#22270;&#20687;&#23383;&#24149;&#20219;&#21153;&#30340;&#23454;&#39564;&#20013;&#65292;&#23637;&#31034;&#20102;&#19968;&#33268;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data augmentation has been established as an efficacious approach to supplement useful information for low-resource datasets. Traditional augmentation techniques such as noise injection and image transformations have been widely used. In addition, generative data augmentation (GDA) has been shown to produce more diverse and flexible data. While generative adversarial networks (GANs) have been frequently used for GDA, they lack diversity and controllability compared to text-to-image diffusion models. In this paper, we propose TTIDA (Text-to-Text-to-Image Data Augmentation) to leverage the capabilities of large-scale pre-trained Text-to-Text (T2T) and Text-to-Image (T2I) generative models for data augmentation. By conditioning the T2I model on detailed descriptions produced by T2T models, we are able to generate photo-realistic labeled images in a flexible and controllable manner. Experiments on in-domain classification, cross-domain classification, and image captioning tasks show consis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#20339;&#21453;&#39539;&#26816;&#32034;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20998;&#27169;&#22411;&#65292;&#20351;&#29992;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#24615;&#24230;&#37327;&#65292;&#36798;&#21040;&#20102;88.9&#65285;&#30340;accuracy@1&#65292;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.08807</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#24615;&#22312;&#26368;&#20339;&#21453;&#39539;&#26816;&#32034;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Revisiting the Role of Similarity and Dissimilarity inBest Counter Argument Retrieval. (arXiv:2304.08807v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#20339;&#21453;&#39539;&#26816;&#32034;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20998;&#27169;&#22411;&#65292;&#20351;&#29992;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#24615;&#24230;&#37327;&#65292;&#36798;&#21040;&#20102;88.9&#65285;&#30340;accuracy@1&#65292;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32473;&#23450;&#36755;&#20837;&#35770;&#28857;&#24773;&#20917;&#19979;&#30340;&#26368;&#20339;&#21453;&#39539;&#26816;&#32034;&#20219;&#21153;&#12290;&#26681;&#25454;&#26368;&#20339;&#21453;&#39539;&#23450;&#20041;&#65292;&#26368;&#20339;&#21453;&#39539;&#24212;&#19982;&#36755;&#20837;&#35770;&#28857;&#22312;&#32454;&#33410;&#26041;&#38754;&#30456;&#20284;&#65292;&#20294;&#31435;&#22330;&#30456;&#21453;&#12290;&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#22522;&#20110;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#24615;&#24230;&#37327;&#30340;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#27169;&#22411;&#26469;&#23545;&#21453;&#39539;&#36827;&#34892;&#35780;&#20998;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#29616;&#26377;&#30340;&#35780;&#20998;&#26041;&#27861;&#65288;&#21253;&#25324;&#20256;&#32479;&#30340;&#23398;&#20064;&#25490;&#24207;&#65288;LTR&#65289;&#21644;&#26368;&#36817;&#30340;&#31070;&#32463;&#35780;&#20998;&#27169;&#22411;&#65289;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Bipolar-encoder&#65292;&#19968;&#31181;&#22522;&#20110;BERT&#30340;&#26032;&#22411;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#21516;&#26102;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#24615;&#30340;&#26368;&#20248;&#34920;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#36798;&#21040;88.9&#65285;&#30340;accuracy@1&#65292;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#12290;&#24403;&#19982;&#36866;&#24403;&#30340;&#32531;&#23384;&#25216;&#26415;&#32467;&#21512;&#20351;&#29992;&#26102;&#65292;Bipolar-encoder&#22312;&#39044;&#27979;&#26102;&#38388;&#19978;&#20063;&#20855;&#26377;&#21487;&#27604;&#24615;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the task of best counter-argument retrieval given an input argument. Following the definition that the best counter-argument addresses the same aspects as the input argument while having the opposite stance, we aim to develop an efficient and effective model for scoring counter-arguments based on similarity and dissimilarity metrics. We first conduct an experimental study on the effectiveness of available scoring methods, including traditional Learning-To-Rank (LTR) and recent neural scoring models. We then propose Bipolar-encoder, a novel BERT-based model to learn an optimal representation for simultaneous similarity and dissimilarity. Experimental results show that our proposed method can achieve the accuracy@1 of 88.9\%, which significantly outperforms other baselines by a large margin. When combined with an appropriate caching technique, Bipolar-encoder is comparably efficient at prediction time.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SPC&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#20026;&#23545;&#35805;&#20013;&#27599;&#20010;&#21457;&#35328;&#32773;&#29983;&#25104;&#20010;&#20154;&#29305;&#24449;&#25688;&#35201;&#12290;&#20219;&#21153;&#34987;&#20998;&#20026;&#19977;&#20010;&#23376;&#20219;&#21153;&#65306;&#20010;&#20154;&#29305;&#24449;&#21457;&#29616;&#12289;&#20010;&#20154;&#29305;&#24449;&#31867;&#22411;&#35782;&#21035;&#21644;&#20010;&#20154;&#29305;&#24449;&#20215;&#20540;&#25552;&#21462;&#12290;&#20219;&#21153;&#23545;&#20110;&#38134;&#34892;&#12289;&#37202;&#24215;&#39044;&#35746;&#21644;&#33322;&#31354;&#39044;&#35746;&#31561;&#34892;&#19994;&#20013;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#38750;&#24120;&#37325;&#35201;&#65292;&#21487;&#20197;&#20351;&#32842;&#22825;&#26426;&#22120;&#20154;&#26356;&#22909;&#22320;&#20102;&#35299;&#21644;&#22238;&#24212;&#27599;&#20010;&#21457;&#35328;&#20154;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2304.08801</link><description>&lt;p&gt;
&#22810;&#26041;&#20250;&#35805;&#20013;&#30340;&#21457;&#35328;&#20154;&#20010;&#20154;&#29305;&#24449;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Speaker Profiling in Multiparty Conversations. (arXiv:2304.08801v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08801
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SPC&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#20026;&#23545;&#35805;&#20013;&#27599;&#20010;&#21457;&#35328;&#32773;&#29983;&#25104;&#20010;&#20154;&#29305;&#24449;&#25688;&#35201;&#12290;&#20219;&#21153;&#34987;&#20998;&#20026;&#19977;&#20010;&#23376;&#20219;&#21153;&#65306;&#20010;&#20154;&#29305;&#24449;&#21457;&#29616;&#12289;&#20010;&#20154;&#29305;&#24449;&#31867;&#22411;&#35782;&#21035;&#21644;&#20010;&#20154;&#29305;&#24449;&#20215;&#20540;&#25552;&#21462;&#12290;&#20219;&#21153;&#23545;&#20110;&#38134;&#34892;&#12289;&#37202;&#24215;&#39044;&#35746;&#21644;&#33322;&#31354;&#39044;&#35746;&#31561;&#34892;&#19994;&#20013;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#38750;&#24120;&#37325;&#35201;&#65292;&#21487;&#20197;&#20351;&#32842;&#22825;&#26426;&#22120;&#20154;&#26356;&#22909;&#22320;&#20102;&#35299;&#21644;&#22238;&#24212;&#27599;&#20010;&#21457;&#35328;&#20154;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#35805;&#29615;&#22659;&#20013;&#65292;&#20010;&#20307;&#23637;&#29616;&#20986;&#29420;&#29305;&#30340;&#34892;&#20026;&#65292;&#20351;&#24471;&#8220;&#19968;&#20992;&#20999;&#8221;&#30340;&#26041;&#27861;&#19981;&#36275;&#20197;&#20026;&#23545;&#35805;&#20195;&#29702;&#29983;&#25104;&#22238;&#24212;&#12290;&#34429;&#28982;&#36807;&#21435;&#30340;&#30740;&#31350;&#26088;&#22312;&#20351;&#29992;&#21457;&#35328;&#20154;&#20010;&#20154;&#20449;&#24687;&#21019;&#24314;&#20010;&#24615;&#21270;&#23545;&#35805;&#20195;&#29702;&#65292;&#20294;&#23427;&#20204;&#20381;&#36182;&#20110;&#21069;&#25552;&#65292;&#21363;&#21457;&#35328;&#20154;&#20010;&#20154;&#29305;&#24449;&#24050;&#32463;&#34987;&#25552;&#20379;&#12290;&#28982;&#32780;&#65292;&#22312;&#20687;&#38134;&#34892;&#12289;&#37202;&#24215;&#39044;&#35746;&#21644;&#33322;&#31354;&#39044;&#35746;&#31561;&#34892;&#19994;&#20013;&#20351;&#29992;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#26041;&#38754;&#65292;&#36825;&#19968;&#20551;&#35774;&#24182;&#19981;&#24635;&#26159;&#27491;&#30830;&#30340;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25506;&#32034;&#23545;&#35805;&#20013;&#30340;&#21457;&#35328;&#20154;&#20010;&#20154;&#29305;&#24449;&#20998;&#26512; (SPC)&#20219;&#21153;&#26469;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#12290;SPC&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#20026;&#23545;&#35805;&#20013;&#27599;&#20010;&#21457;&#35328;&#20154;&#20135;&#29983;&#20010;&#20154;&#29305;&#24449;&#25688;&#35201;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#23558;&#20219;&#21153;&#20998;&#20026;&#19977;&#20010;&#23376;&#20219;&#21153;&#65306;&#20010;&#20154;&#29305;&#24449;&#21457;&#29616;&#12289;&#20010;&#20154;&#29305;&#24449;&#31867;&#22411;&#35782;&#21035;&#21644;&#20010;&#20154;&#29305;&#24449;&#20215;&#20540;&#25552;&#21462;&#12290;&#22312;&#32473;&#23450;&#23545;&#35805;&#30340;&#24773;&#20917;&#19979;&#65292;&#31532;&#19968;&#20010;&#23376;&#20219;&#21153;&#26088;&#22312;&#35782;&#21035;&#21253;&#21547;&#20010;&#20154;&#20449;&#24687;&#30340;&#25152;&#26377;&#35805;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;
In conversational settings, individuals exhibit unique behaviors, rendering a one-size-fits-all approach insufficient for generating responses by dialogue agents. Although past studies have aimed to create personalized dialogue agents using speaker persona information, they have relied on the assumption that the speaker's persona is already provided. However, this assumption is not always valid, especially when it comes to chatbots utilized in industries like banking, hotel reservations, and airline bookings. This research paper aims to fill this gap by exploring the task of Speaker Profiling in Conversations (SPC). The primary objective of SPC is to produce a summary of persona characteristics for each individual speaker present in a dialogue. To accomplish this, we have divided the task into three subtasks: persona discovery, persona-type identification, and persona-value extraction. Given a dialogue, the first subtask aims to identify all utterances that contain persona information.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25688;&#35201;&#26041;&#27861;&#65292;&#25552;&#28860;&#20851;&#38190;&#20449;&#24687;&#29983;&#25104;&#31616;&#27905;&#30340;&#25688;&#35201;&#65292;&#20998;&#20026;&#24494;&#35843;&#12289;&#22522;&#20110;&#29305;&#24449;&#21644;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#21253;&#25324;&#34701;&#21512;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#21644;&#24320;&#21457;&#26356;&#36866;&#21512;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2304.08763</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25688;&#35201;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Biomedical Text Summarization with Pre-trained Language Model. (arXiv:2304.08763v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25688;&#35201;&#26041;&#27861;&#65292;&#25552;&#28860;&#20851;&#38190;&#20449;&#24687;&#29983;&#25104;&#31616;&#27905;&#30340;&#25688;&#35201;&#65292;&#20998;&#20026;&#24494;&#35843;&#12289;&#22522;&#20110;&#29305;&#24449;&#21644;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#21253;&#25324;&#34701;&#21512;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#21644;&#24320;&#21457;&#26356;&#36866;&#21512;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#21644;&#30005;&#23376;&#30149;&#21382;&#31561;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#30340;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#32473;&#20020;&#24202;&#21307;&#29983;&#21644;&#30740;&#31350;&#20154;&#21592;&#39640;&#25928;&#33719;&#21462;&#20020;&#24202;&#20449;&#24687;&#24102;&#26469;&#24040;&#22823;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25688;&#35201;&#26041;&#27861;&#65292;&#26088;&#22312;&#20174;&#21333;&#20010;&#25110;&#22810;&#20010;&#29983;&#29289;&#21307;&#23398;&#25991;&#26723;&#20013;&#25552;&#28860;&#20851;&#38190;&#20449;&#24687;&#29983;&#25104;&#31616;&#27905;&#30340;&#25688;&#35201;&#12290;&#36817;&#24180;&#26469;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#24050;&#25104;&#20026;&#22810;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#20107;&#23454;&#26631;&#20934;&#65292;PLMs&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#20063;&#20026;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25688;&#35201;&#20219;&#21153;&#24102;&#26469;&#26032;&#30340;&#21551;&#31034;&#12290;&#26412;&#25991;&#31995;&#32479;&#22320;&#24635;&#32467;&#20102;&#36817;&#26399;&#22522;&#20110;PLMs&#25506;&#32034;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25688;&#35201;&#30340;&#36827;&#23637;&#65292;&#24110;&#21161;&#29702;&#35299;&#26368;&#26032;&#30340;&#36827;&#23637;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;&#25105;&#20204;&#26681;&#25454;&#20351;&#29992;PLMs&#30340;&#26041;&#24335;&#23545;&#22522;&#20110;PLMs&#30340;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#65292;&#21253;&#25324;&#24494;&#35843;&#12289;&#22522;&#20110;&#29305;&#24449;&#21644;&#26080;&#30417;&#30563;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#28508;&#22312;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#65292;&#22914;&#34701;&#21512;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#21644;&#24320;&#21457;&#26356;&#36866;&#21512;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
The exponential growth of biomedical texts such as biomedical literature and electronic health records (EHRs), provides a big challenge for clinicians and researchers to access clinical information efficiently. To address the problem, biomedical text summarization has been proposed to support clinical information retrieval and management, aiming at generating concise summaries that distill key information from single or multiple biomedical documents. In recent years, pre-trained language models (PLMs) have been the de facto standard of various natural language processing tasks in the general domain. Most recently, PLMs have been further investigated in the biomedical field and brought new insights into the biomedical text summarization task. In this paper, we systematically summarize recent advances that explore PLMs for biomedical text summarization, to help understand recent progress, challenges, and future directions. We categorize PLMs-based approaches according to how they utilize
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#19981;&#21516;&#26368;&#20808;&#36827;&#30340;&#27010;&#29575;&#23398;&#20064;&#26041;&#27861;&#22312;&#25552;&#39640;&#31070;&#32463;&#25688;&#35201;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#36136;&#37327;&#21644;&#29983;&#25104;&#25928;&#26524;&#26041;&#38754;&#36827;&#34892;&#20102;&#35843;&#26597;&#21644;&#23545;&#27604;&#65292;&#32467;&#26524;&#34920;&#26126;&#27010;&#29575;&#26041;&#27861;&#33021;&#22815;&#25345;&#32493;&#25552;&#39640;&#29983;&#25104;&#21644;&#19981;&#30830;&#23450;&#24615;&#36136;&#37327;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#29983;&#25104;&#21644;&#25918;&#24323;&#20302;&#36136;&#37327;&#25688;&#35201;&#65292;&#19988;&#25581;&#31034;&#20102;&#26174;&#33879;&#30340;&#22833;&#25928;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2304.08653</link><description>&lt;p&gt;
&#20851;&#20110;&#27010;&#29575;&#31070;&#32463;&#25688;&#35201;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#21644;&#36873;&#25321;&#24615;&#29983;&#25104;&#30340;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Uncertainty Calibration and Selective Generation in Probabilistic Neural Summarization: A Benchmark Study. (arXiv:2304.08653v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08653
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#19981;&#21516;&#26368;&#20808;&#36827;&#30340;&#27010;&#29575;&#23398;&#20064;&#26041;&#27861;&#22312;&#25552;&#39640;&#31070;&#32463;&#25688;&#35201;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#36136;&#37327;&#21644;&#29983;&#25104;&#25928;&#26524;&#26041;&#38754;&#36827;&#34892;&#20102;&#35843;&#26597;&#21644;&#23545;&#27604;&#65292;&#32467;&#26524;&#34920;&#26126;&#27010;&#29575;&#26041;&#27861;&#33021;&#22815;&#25345;&#32493;&#25552;&#39640;&#29983;&#25104;&#21644;&#19981;&#30830;&#23450;&#24615;&#36136;&#37327;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#29983;&#25104;&#21644;&#25918;&#24323;&#20302;&#36136;&#37327;&#25688;&#35201;&#65292;&#19988;&#25581;&#31034;&#20102;&#26174;&#33879;&#30340;&#22833;&#25928;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#25688;&#35201;&#27169;&#22411;&#22312;&#22522;&#20934;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#26524;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#20250;&#29983;&#25104;&#38169;&#35823;&#26657;&#20934;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#23545;&#36136;&#37327;&#36739;&#20302;&#30340;&#39044;&#27979;&#36171;&#20104;&#20102;&#39640;&#20449;&#24515;&#24230;&#65292;&#20174;&#32780;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#23548;&#33268;&#21487;&#38752;&#24615;&#21644;&#20449;&#20219;&#24230;&#30340;&#38477;&#20302;&#12290;&#27010;&#29575;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26159;&#35299;&#20915;&#35823;&#26657;&#20934;&#38382;&#39064;&#30340;&#24120;&#35265;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#22797;&#26434;&#33258;&#22238;&#24402;&#25688;&#35201;&#20219;&#21153;&#20013;&#30340;&#30456;&#23545;&#26377;&#25928;&#24615;&#23578;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24443;&#24213;&#35843;&#26597;&#20102;&#19981;&#21516;&#26368;&#20808;&#36827;&#30340;&#27010;&#29575;&#26041;&#27861;&#22312;&#25552;&#39640;&#31070;&#32463;&#25688;&#35201;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#36136;&#37327;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#36328;&#36234;&#20102;&#19977;&#20010;&#38590;&#24230;&#19981;&#21516;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#27010;&#29575;&#26041;&#27861;&#22987;&#32456;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#30340;&#29983;&#25104;&#21644;&#19981;&#30830;&#23450;&#24615;&#36136;&#37327;&#65292;&#20174;&#32780;&#22312;&#23454;&#36341;&#20013;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#29983;&#25104;&#65288;&#21363;&#25918;&#24323;&#20302;&#36136;&#37327;&#25688;&#35201;&#65289;&#12290;&#25105;&#20204;&#36824;&#25581;&#31034;&#20102;&#26174;&#33879;&#30340;&#22833;&#25928;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern deep models for summarization attains impressive benchmark performance, but they are prone to generating miscalibrated predictive uncertainty. This means that they assign high confidence to low-quality predictions, leading to compromised reliability and trustworthiness in real-world applications. Probabilistic deep learning methods are common solutions to the miscalibration problem. However, their relative effectiveness in complex autoregressive summarization tasks are not well-understood. In this work, we thoroughly investigate different state-of-the-art probabilistic methods' effectiveness in improving the uncertainty quality of the neural summarization models, across three large-scale benchmarks with varying difficulty. We show that the probabilistic methods consistently improve the model's generation and uncertainty quality, leading to improved selective generation performance (i.e., abstaining from low-quality summaries) in practice. We also reveal notable failure patterns 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;BERT&#25216;&#26415;&#25506;&#31350;&#20102;&#23545;&#32654;&#22269;&#26368;&#39640;&#27861;&#38498;&#26696;&#20363;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#27604;&#36739;&#20102;&#20351;&#29992;BERT&#27169;&#22411;&#19982;&#20854;&#20182;&#20808;&#36827;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#26368;&#32456;&#22312;15&#20010;&#24191;&#27867;&#31867;&#21035;&#19978;&#21462;&#24471;&#20102;80%&#30340;&#20934;&#30830;&#24230;&#65292;&#22312;279&#20010;&#32454;&#31890;&#24230;&#31867;&#21035;&#19978;&#21462;&#24471;&#20102;60%&#30340;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.08649</link><description>&lt;p&gt;
&#22522;&#20110;BERT&#30340;&#25216;&#26415;&#23545;&#32654;&#22269;&#26368;&#39640;&#27861;&#38498;&#26696;&#20363;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classification of US Supreme Court Cases using BERT-Based Techniques. (arXiv:2304.08649v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;BERT&#25216;&#26415;&#25506;&#31350;&#20102;&#23545;&#32654;&#22269;&#26368;&#39640;&#27861;&#38498;&#26696;&#20363;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#27604;&#36739;&#20102;&#20351;&#29992;BERT&#27169;&#22411;&#19982;&#20854;&#20182;&#20808;&#36827;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#26368;&#32456;&#22312;15&#20010;&#24191;&#27867;&#31867;&#21035;&#19978;&#21462;&#24471;&#20102;80%&#30340;&#20934;&#30830;&#24230;&#65292;&#22312;279&#20010;&#32454;&#31890;&#24230;&#31867;&#21035;&#19978;&#21462;&#24471;&#20102;60%&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#26469;&#33258;&#21464;&#21387;&#22120;&#30340;&#27169;&#22411;&#65288;BERT&#65289;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#65288;&#22914;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#65292;&#35789;&#24615;&#65288;POS&#65289;&#26631;&#35760;&#31561;&#65289;&#19978;&#20135;&#29983;&#20102;&#26368;&#26032;&#25216;&#26415;&#65288;SOTA&#65289;&#32467;&#26524;&#12290;&#24403;&#20998;&#31867;&#38271;&#25991;&#26723;&#65288;&#20363;&#22914;&#26469;&#33258;&#32654;&#22269;&#26368;&#39640;&#27861;&#38498;&#30340;&#25991;&#26723;&#65289;&#26102;&#65292;&#20351;&#29992;BERT&#27169;&#22411;&#21487;&#33021;&#27604;&#36739;&#22256;&#38590;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#20960;&#31181;&#22522;&#20110;BERT&#30340;&#20998;&#31867;&#25216;&#26415;&#65292;&#29992;&#20110;&#23545;&#32654;&#22269;&#26368;&#39640;&#27861;&#38498;&#20915;&#23450;&#25110;&#26368;&#39640;&#27861;&#38498;&#25968;&#25454;&#24211;&#65288;SCDB&#65289;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#23558;&#20854;&#19982;&#20808;&#21069;&#30340;SOTA&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#32467;&#26524;&#19982;&#38024;&#23545;&#38271;&#25991;&#26723;&#30340;SOTA&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#23545;&#20004;&#20010;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#20102;&#27604;&#36739;&#65306;&#65288;1&#65289;&#24191;&#27867;&#30340;&#20998;&#31867;&#20219;&#21153;&#65292;&#20855;&#26377;15&#20010;&#31867;&#21035;&#65307;&#65288;2&#65289;&#32454;&#31890;&#24230;&#30340;&#20998;&#31867;&#20219;&#21153;&#65292;&#20855;&#26377;279&#20010;&#31867;&#21035;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#32467;&#26524;&#22312;15&#20010;&#24191;&#27867;&#31867;&#21035;&#19978;&#20135;&#29983;80&#65285;&#30340;&#20934;&#30830;&#24230;&#65292;&#22312;279&#20010;&#32454;&#31890;&#24230;&#31867;&#21035;&#19978;&#20135;&#29983;60&#65285;&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models based on bidirectional encoder representations from transformers (BERT) produce state of the art (SOTA) results on many natural language processing (NLP) tasks such as named entity recognition (NER), part-of-speech (POS) tagging etc. An interesting phenomenon occurs when classifying long documents such as those from the US supreme court where BERT-based models can be considered difficult to use on a first-pass or out-of-the-box basis. In this paper, we experiment with several BERT-based classification techniques for US supreme court decisions or supreme court database (SCDB) and compare them with the previous SOTA results. We then compare our results specifically with SOTA models for long documents. We compare our results for two classification tasks: (1) a broad classification task with 15 categories and (2) a fine-grained classification task with 279 categories. Our best result produces an accuracy of 80\% on the 15 broad categories and 60\% on the fine-grained 279 categories 
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#20102;&#20061;&#20010;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#21457;&#29616;&#20854;&#20013;80&#65285;&#21253;&#21547;&#35760;&#24518;&#25968;&#25454;&#65292;&#20294;&#21253;&#21547;&#26368;&#22810;&#35760;&#24518;&#20869;&#23481;&#30340;&#36755;&#20986;&#26356;&#21487;&#33021;&#26159;&#39640;&#36136;&#37327;&#30340;&#12290;&#25552;&#20986;&#20102;&#32531;&#35299;&#31574;&#30053;&#20197;&#38477;&#20302;&#35760;&#24518;&#25991;&#26412;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.08637</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#30340;&#35780;&#20272;&#65306;&#35805;&#35821;&#21644;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
An Evaluation on Large Language Model Outputs: Discourse and Memorization. (arXiv:2304.08637v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08637
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20102;&#20061;&#20010;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#21457;&#29616;&#20854;&#20013;80&#65285;&#21253;&#21547;&#35760;&#24518;&#25968;&#25454;&#65292;&#20294;&#21253;&#21547;&#26368;&#22810;&#35760;&#24518;&#20869;&#23481;&#30340;&#36755;&#20986;&#26356;&#21487;&#33021;&#26159;&#39640;&#36136;&#37327;&#30340;&#12290;&#25552;&#20986;&#20102;&#32531;&#35299;&#31574;&#30053;&#20197;&#38477;&#20302;&#35760;&#24518;&#25991;&#26412;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#20061;&#20010;&#26368;&#24191;&#27867;&#21487;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#21508;&#31181;&#36755;&#20986;&#36827;&#34892;&#20102;&#32463;&#39564;&#24615;&#35780;&#20272;&#12290;&#25105;&#20204;&#20351;&#29992;&#29616;&#25104;&#30340;&#24037;&#20855;&#36827;&#34892;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#19982;&#36755;&#20986;&#30149;&#24577;&#65288;&#20363;&#22914;&#65292;&#21453;&#20107;&#23454;&#21644;&#36923;&#36753;&#19978;&#30340;&#38169;&#35823;&#38472;&#36848;&#65289;&#20197;&#21450;&#19981;&#20445;&#25345;&#20027;&#39064;&#31561;&#26041;&#38754;&#30340;&#20851;&#31995;&#20013;&#65292;&#35760;&#24518;&#25991;&#26412;&#30334;&#20998;&#27604;&#12289;&#29420;&#29305;&#25991;&#26412;&#30334;&#20998;&#27604;&#21644;&#25972;&#20307;&#36755;&#20986;&#36136;&#37327;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#24615;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;80.0&#65285;&#30340;&#36755;&#20986;&#21253;&#21547;&#35760;&#24518;&#25968;&#25454;&#65292;&#20294;&#21253;&#21547;&#26368;&#22810;&#35760;&#24518;&#20869;&#23481;&#30340;&#36755;&#20986;&#20063;&#26356;&#26377;&#21487;&#33021;&#34987;&#35748;&#20026;&#20855;&#26377;&#39640;&#36136;&#37327;&#12290;&#25105;&#20204;&#35752;&#35770;&#21644;&#35780;&#20272;&#20102;&#32531;&#35299;&#31574;&#30053;&#65292;&#24182;&#26174;&#31034;&#65292;&#22312;&#35780;&#20272;&#30340;&#27169;&#22411;&#20013;&#65292;&#36755;&#20986;&#30340;&#35760;&#24518;&#25991;&#26412;&#29575;&#26377;&#25152;&#38477;&#20302;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23601;&#23398;&#20064;&#12289;&#35760;&#24518;&#21644;&#35780;&#20272;&#20248;&#36136;&#25991;&#26412;&#30340;&#28508;&#22312;&#24433;&#21709;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an empirical evaluation of various outputs generated by nine of the most widely-available large language models (LLMs). Our analysis is done with off-the-shelf, readily-available tools. We find a correlation between percentage of memorized text, percentage of unique text, and overall output quality, when measured with respect to output pathologies such as counterfactual and logically-flawed statements, and general failures like not staying on topic. Overall, 80.0% of the outputs evaluated contained memorized data, but outputs containing the most memorized content were also more likely to be considered of high quality. We discuss and evaluate mitigation strategies, showing that, in the models evaluated, the rate of memorized text being output is reduced. We conclude with a discussion on potential implications around what it means to learn, to memorize, and to evaluate quality text.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#36924;&#36817;&#29983;&#25104;&#31163;&#25955;&#28508;&#21464;&#37327;&#30340;&#21442;&#25968;&#30340;&#26799;&#24230;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#19968;&#20123;&#25968;&#20540;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20108;&#38454;&#31934;&#24230;&#65292;&#21462;&#24471;&#20102;&#23454;&#39564;&#19978;&#30340;&#25345;&#32493;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2304.08612</link><description>&lt;p&gt;
&#31163;&#25955;&#19982;&#21453;&#21521;&#20256;&#25773;&#30340;&#26725;&#26753;&#65306;&#30452;&#36890;&#27861;&#19982;&#20854;&#23427;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bridging Discrete and Backpropagation: Straight-Through and Beyond. (arXiv:2304.08612v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#36924;&#36817;&#29983;&#25104;&#31163;&#25955;&#28508;&#21464;&#37327;&#30340;&#21442;&#25968;&#30340;&#26799;&#24230;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#19968;&#20123;&#25968;&#20540;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20108;&#38454;&#31934;&#24230;&#65292;&#21462;&#24471;&#20102;&#23454;&#39564;&#19978;&#30340;&#25345;&#32493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21521;&#20256;&#25773;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#22522;&#30707;&#65292;&#20294;&#20854;&#20165;&#38480;&#20110;&#35745;&#31639;&#36830;&#32493;&#21464;&#37327;&#30340;&#26799;&#24230;&#65292;&#38480;&#21046;&#20102;&#28041;&#21450;&#31163;&#25955;&#28508;&#21464;&#37327;&#30340;&#38382;&#39064;&#30340;&#30740;&#31350;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#36817;&#20284;&#29983;&#25104;&#31163;&#25955;&#28508;&#21464;&#37327;&#30340;&#21442;&#25968;&#30340;&#26799;&#24230;&#12290;&#25105;&#20204;&#39318;&#20808;&#32771;&#23519;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340; Straight-Through&#65288;ST&#65289;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#23427;&#20316;&#20026;&#26799;&#24230;&#30340;&#19968;&#38454;&#36817;&#20284;&#20540;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026; ReinMax&#65292;&#23427;&#38598;&#25104;&#20102; Heun's Method&#65292;&#19968;&#31181;&#35299;ODE&#30340;&#20108;&#38454;&#25968;&#20540;&#26041;&#27861;&#65292;&#20197;&#36817;&#20284;&#26799;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#20108;&#38454;&#31934;&#24230;&#65292;&#32780;&#19981;&#38656;&#35201; Hessian &#25110;&#20854;&#20182;&#20108;&#38454;&#23548;&#25968;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#32467;&#26500;&#21270;&#36755;&#20986;&#39044;&#27979;&#21644;&#26080;&#30417;&#30563;&#29983;&#25104;&#24314;&#27169;&#20219;&#21153;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;\ours &#22312;&#29616;&#26377;&#25216;&#26415;&#20013;&#24102;&#26469;&#20102;&#25345;&#32493;&#30340;&#25913;&#36827;&#65292;&#21253;&#25324; ST &#21644; Straight-Through Gum&#12290;
&lt;/p&gt;
&lt;p&gt;
Backpropagation, the cornerstone of deep learning, is limited to computing gradients solely for continuous variables. This limitation hinders various research on problems involving discrete latent variables. To address this issue, we propose a novel approach for approximating the gradient of parameters involved in generating discrete latent variables. First, we examine the widely used Straight-Through (ST) heuristic and demonstrate that it works as a first-order approximation of the gradient. Guided by our findings, we propose a novel method called ReinMax, which integrates Heun's Method, a second-order numerical method for solving ODEs, to approximate the gradient. Our method achieves second-order accuracy without requiring Hessian or other second-order derivatives. We conduct experiments on structured output prediction and unsupervised generative modeling tasks. Our results show that \ours brings consistent improvements over the state of the art, including ST and Straight-Through Gum
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#24179;&#34913;&#21644;&#38271;&#23614;&#20998;&#24067;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#25991;&#26412;&#35782;&#21035;&#35757;&#32451;&#65292;&#21516;&#26102;&#20445;&#30041;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20197;&#25552;&#39640;&#23383;&#31526;&#32423;&#38271;&#23614;&#20998;&#24067;&#19978;&#30340;STR&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.08592</link><description>&lt;p&gt;
&#38754;&#21521;&#23383;&#31526;&#32423;&#38271;&#23614;&#20998;&#24067;&#30340;&#22330;&#26223;&#25991;&#26412;&#35782;&#21035;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Improving Scene Text Recognition for Character-Level Long-Tailed Distribution. (arXiv:2304.08592v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08592
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#24179;&#34913;&#21644;&#38271;&#23614;&#20998;&#24067;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#25991;&#26412;&#35782;&#21035;&#35757;&#32451;&#65292;&#21516;&#26102;&#20445;&#30041;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20197;&#25552;&#39640;&#23383;&#31526;&#32423;&#38271;&#23614;&#20998;&#24067;&#19978;&#30340;STR&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22330;&#26223;&#25991;&#26412;&#35782;&#21035;&#65288;STR&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#22823;&#22810;&#25968;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20165;&#21253;&#21547;&#23569;&#37327;&#23383;&#31526;&#30340;&#33521;&#35821;&#19978;&#12290;&#28982;&#32780;&#65292;STR&#27169;&#22411;&#22312;&#35832;&#22914;&#20013;&#25991;&#21644;&#38889;&#25991;&#31561;&#23383;&#31526;&#25968;&#37327;&#20247;&#22810;&#30340;&#35821;&#35328;&#20013;&#65288;&#23588;&#20854;&#26159;&#22240;&#23383;&#31526;&#38271;&#23614;&#20998;&#24067;&#32780;&#24456;&#23569;&#20986;&#29616;&#30340;&#23383;&#31526;&#65289;&#34920;&#29616;&#20986;&#36739;&#22823;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#20855;&#26377;&#19981;&#21516;&#23383;&#31526;&#32423;&#20998;&#24067;&#65288;&#20363;&#22914;&#24179;&#34913;&#21644;&#38271;&#23614;&#20998;&#24067;&#65289;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#12290;&#22686;&#21152;&#22823;&#37327;&#23614;&#37096;&#31867;&#21035;&#21487;&#20197;&#24110;&#21161;&#27169;&#22411;&#27491;&#30830;&#22320;&#21333;&#29420;&#35782;&#21035;&#23383;&#31526;&#65292;&#20294;&#26159;&#65292;&#20351;&#29992;&#36825;&#26679;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#20250;&#24433;&#21709;&#27169;&#22411;&#23398;&#20064;&#35789;&#27719;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65288;&#21363;&#23383;&#31526;&#20043;&#38388;&#30340;&#20851;&#31995;&#65289;&#65292;&#36825;&#23545;&#20110;&#27491;&#30830;&#35782;&#21035;&#25972;&#20010;&#35789;&#27719;&#21516;&#26679;&#37325;&#35201;&#12290;&#22522;&#20110;&#27492;&#21160;&#26426;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21033;&#29992;&#24179;&#34913;&#21644;&#38271;&#23614;&#20998;&#24067;&#65292;&#21516;&#26102;&#20445;&#30041;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20197;&#25552;&#39640;&#23383;&#31526;&#32423;&#38271;&#23614;&#20998;&#24067;&#19978;&#30340;STR&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the recent remarkable improvements in scene text recognition (STR), the majority of the studies focused mainly on the English language, which only includes few number of characters. However, STR models show a large performance degradation on languages with a numerous number of characters (e.g., Chinese and Korean), especially on characters that rarely appear due to the long-tailed distribution of characters in such languages. To address such an issue, we conducted an empirical analysis using synthetic datasets with different character-level distributions (e.g., balanced and long-tailed distributions). While increasing a substantial number of tail classes without considering the context helps the model to correctly recognize characters individually, training with such a synthetic dataset interferes the model with learning the contextual information (i.e., relation among characters), which is also important for predicting the whole word. Based on this motivation, we propose a nov
&lt;/p&gt;</description></item><item><title>&#31038;&#20132;&#23186;&#20307;&#20013;&#35773;&#21050;&#25991;&#26412;&#20351;&#29992;&#26222;&#36941;&#65292;&#20294;&#35773;&#21050;&#26816;&#27979;&#36739;&#20026;&#22256;&#38590;&#12290;&#25991;&#31456;&#35752;&#35770;&#20102;&#35773;&#21050;&#26816;&#27979;&#30340;&#25216;&#26415;&#21644;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#38656;&#35201;&#37325;&#28857;&#20851;&#27880;&#20048;&#35266;&#21644;&#21069;&#30651;&#24615;&#30340;&#35773;&#21050;&#26816;&#27979;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.08582</link><description>&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#20013;&#35773;&#21050;&#26816;&#27979;&#30340;&#30740;&#31350;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Researchers eye-view of sarcasm detection in social media textual content. (arXiv:2304.08582v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08582
&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#20013;&#35773;&#21050;&#25991;&#26412;&#20351;&#29992;&#26222;&#36941;&#65292;&#20294;&#35773;&#21050;&#26816;&#27979;&#36739;&#20026;&#22256;&#38590;&#12290;&#25991;&#31456;&#35752;&#35770;&#20102;&#35773;&#21050;&#26816;&#27979;&#30340;&#25216;&#26415;&#21644;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#38656;&#35201;&#37325;&#28857;&#20851;&#27880;&#20048;&#35266;&#21644;&#21069;&#30651;&#24615;&#30340;&#35773;&#21050;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#20013;&#35773;&#21050;&#25991;&#26412;&#30340;&#24191;&#27867;&#20351;&#29992;&#23545;&#30446;&#26631;&#29992;&#25143;&#20135;&#29983;&#29983;&#29702;&#20316;&#29992;&#12290;&#27599;&#20010;&#29992;&#25143;&#23545;&#35823;&#29992;&#21644;&#35782;&#21035;&#35773;&#21050;&#30340;&#26041;&#27861;&#19981;&#21516;&#12290;&#35773;&#21050;&#26816;&#27979;&#21363;&#20351;&#23545;&#20110;&#29992;&#25143;&#26469;&#35828;&#20063;&#26159;&#22256;&#38590;&#30340;&#65292;&#36825;&#23558;&#21462;&#20915;&#20110;&#24456;&#22810;&#22240;&#32032;&#65292;&#22914;&#35270;&#35282;&#12289;&#35821;&#22659;&#12289;&#29305;&#27530;&#31526;&#21495;&#31561;&#12290;&#22240;&#27492;&#65292;&#35753;&#26426;&#22120;&#20174;&#20247;&#22810;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#21306;&#20998;&#35773;&#21050;&#21477;&#23376;&#21644;&#38750;&#35773;&#21050;&#21477;&#23376;&#23558;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#30446;&#21069;&#30340;&#24773;&#20917;&#19979;&#65292;&#27809;&#26377;&#30830;&#20999;&#30340;&#35268;&#21017;&#21487;&#20197;&#20351;&#27169;&#22411;&#20934;&#30830;&#22320;&#26816;&#27979;&#20986;&#35773;&#21050;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#37325;&#28857;&#20851;&#27880;&#20048;&#35266;&#21644;&#21069;&#30651;&#24615;&#30340;&#35773;&#21050;&#26816;&#27979;&#26041;&#27861;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#21508;&#31181;&#35773;&#21050;&#26816;&#27979;&#25216;&#26415;&#65292;&#24182;&#20197;&#19968;&#20123;&#26041;&#27861;&#12289;&#30456;&#20851;&#25968;&#25454;&#38598;&#20197;&#21450;&#30740;&#31350;&#20154;&#21592;&#38754;&#20020;&#30340;&#25361;&#25112;&#20316;&#20026;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
The enormous use of sarcastic text in all forms of communication in social media will have a physiological effect on target users. Each user has a different approach to misusing and recognising sarcasm. Sarcasm detection is difficult even for users, and this will depend on many things such as perspective, context, special symbols. So, that will be a challenging task for machines to differentiate sarcastic sentences from non-sarcastic sentences. There are no exact rules based on which model will accurately detect sarcasm from many text corpus in the current situation. So, one needs to focus on optimistic and forthcoming approaches in the sarcasm detection domain. This paper discusses various sarcasm detection techniques and concludes with some approaches, related datasets with optimal features, and the researcher's challenges.
&lt;/p&gt;</description></item><item><title>&#37319;&#29992;&#21442;&#25968;&#25928;&#29575;&#39640;&#30340;LoRA&#35843;&#21442;&#25216;&#26415;&#21487;&#20197;&#22312;&#25351;&#20196;&#35843;&#21442;&#20013;&#21462;&#24471;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#20294;&#20840;&#21442;&#25968;&#35843;&#21442;&#26041;&#27861;&#22312;&#22522;&#30784;&#27169;&#22411;&#36873;&#25321;&#12289;&#35757;&#32451;&#25968;&#25454;&#38598;&#35268;&#27169;&#12289;&#21487;&#23398;&#21442;&#25968;&#37327;&#31561;&#26041;&#38754;&#20063;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.08109</link><description>&lt;p&gt;
&#22522;&#20110;&#20013;&#25991;&#25351;&#20196;&#25968;&#25454;&#30340;&#20840;&#21442;&#25968;&#21644;LoRA&#35843;&#21442;&#26041;&#27861;&#22312;&#25351;&#20196;&#36981;&#24490;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comparative Study between Full-Parameter and LoRA-based Fine-Tuning on Chinese Instruction Data for Instruction Following Large Language Model. (arXiv:2304.08109v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08109
&lt;/p&gt;
&lt;p&gt;
&#37319;&#29992;&#21442;&#25968;&#25928;&#29575;&#39640;&#30340;LoRA&#35843;&#21442;&#25216;&#26415;&#21487;&#20197;&#22312;&#25351;&#20196;&#35843;&#21442;&#20013;&#21462;&#24471;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#20294;&#20840;&#21442;&#25968;&#35843;&#21442;&#26041;&#27861;&#22312;&#22522;&#30784;&#27169;&#22411;&#36873;&#25321;&#12289;&#35757;&#32451;&#25968;&#25454;&#38598;&#35268;&#27169;&#12289;&#21487;&#23398;&#21442;&#25968;&#37327;&#31561;&#26041;&#38754;&#20063;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#35843;&#21442;&#30740;&#31350;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#19968;&#20010;&#20851;&#38190;&#30740;&#31350;&#26041;&#21521;&#12290;&#30001;&#20110;&#36164;&#28304;&#19982;&#25104;&#26412;&#38480;&#21046;&#65292;&#19968;&#20123;&#30740;&#31350;&#32773;&#37319;&#29992;&#21442;&#25968;&#25928;&#29575;&#39640;&#30340;&#35843;&#21442;&#25216;&#26415;&#65292;&#20363;&#22914;LoRA&#65292;&#22312;&#25351;&#20196;&#35843;&#21442;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#20197;LLaMA&#20026;&#22522;&#30784;&#27169;&#22411;&#65292;&#23545;&#20840;&#21442;&#25968;&#35843;&#21442;&#21644;LoRA&#35843;&#21442;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#39564;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#30784;&#27169;&#22411;&#30340;&#36873;&#25321;&#12289;&#35757;&#32451;&#25968;&#25454;&#38598;&#35268;&#27169;&#12289;&#21487;&#23398;&#21442;&#25968;&#37327;&#20197;&#21450;&#27169;&#22411;&#35757;&#32451;&#25104;&#26412;&#37117;&#26159;&#37325;&#35201;&#22240;&#32032;&#12290;&#24076;&#26395;&#26412;&#25991;&#30340;&#23454;&#39564;&#32467;&#35770;&#33021;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#25552;&#20379;&#21551;&#31034;&#65292;&#29305;&#21035;&#26159;&#22312;&#20013;&#25991;&#39046;&#22495;&#65292;&#24182;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#25214;&#21040;&#26356;&#22909;&#30340;&#35757;&#32451;&#25104;&#26412;&#19982;&#24615;&#33021;&#30340;&#24179;&#34913;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the instruction-tuning of large language models is a crucial area of research in the field of natural language processing. Due to resource and cost limitations, several researchers have employed parameter-efficient tuning techniques, such as LoRA, for instruction tuning, and have obtained encouraging results In comparison to full-parameter fine-tuning, LoRA-based tuning demonstrates salient benefits in terms of training costs. In this study, we undertook experimental comparisons between full-parameter fine-tuning and LoRA-based tuning methods, utilizing LLaMA as the base model. The experimental results show that the selection of the foundational model, training dataset scale, learnable parameter quantity, and model training cost are all important factors. We hope that the experimental conclusions of this paper can provide inspiration for training large language models, especially in the field of Chinese, and help researchers find a better trade-off strategy between training c
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#36866;&#24212;&#19981;&#21516;&#23376;&#20219;&#21153;&#30340;&#22266;&#26377;&#29305;&#24615;&#65292;&#21019;&#24314;&#19968;&#20010;&#20013;&#25991;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#20197;&#22635;&#34917;&#25351;&#20196;&#35843;&#25972;&#25216;&#26415;&#22312;&#20013;&#25991;&#35821;&#35328;&#39046;&#22495;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2304.07987</link><description>&lt;p&gt;
&#20013;&#25991;&#24320;&#25918;&#24335;&#25351;&#20196;&#24191;&#20041;&#35821;&#35328;&#27169;&#22411;&#65306;&#21021;&#27493;&#21457;&#24067;
&lt;/p&gt;
&lt;p&gt;
Chinese Open Instruction Generalist: A Preliminary Release. (arXiv:2304.07987v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#36866;&#24212;&#19981;&#21516;&#23376;&#20219;&#21153;&#30340;&#22266;&#26377;&#29305;&#24615;&#65292;&#21019;&#24314;&#19968;&#20010;&#20013;&#25991;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#20197;&#22635;&#34917;&#25351;&#20196;&#35843;&#25972;&#25216;&#26415;&#22312;&#20013;&#25991;&#35821;&#35328;&#39046;&#22495;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#26500;&#24314;&#24191;&#20041;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#38543;&#30528;InstructGPT&#21644;ChatGPT&#30340;&#21457;&#24067;&#65292;&#23427;&#24050;&#32463;&#24341;&#36215;&#20102;&#30740;&#31350;&#20154;&#21592;&#21644;&#20844;&#20247;&#30340;&#20851;&#27880;&#12290;&#23613;&#31649;&#33521;&#35821;&#20026;&#22522;&#30784;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#65292;&#20294;&#26159;&#36824;&#26410;&#25506;&#32034;&#33521;&#35821;&#20026;&#22522;&#30784;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#35821;&#20219;&#21153;&#19978;&#26159;&#21542;&#21487;&#20197;&#20687;&#33521;&#35821;&#20219;&#21153;&#37027;&#26679;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25351;&#20196;&#35843;&#25972;&#26469;&#25191;&#34892;&#65292;&#20197;&#21450;&#25105;&#20204;&#22914;&#20309;&#26500;&#24314;&#25152;&#38656;&#30340;&#35821;&#26009;&#24211;&#36827;&#34892;&#35843;&#25972;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39033;&#30446;&#65292;&#35797;&#22270;&#36890;&#36807;&#36866;&#24212;4&#20010;&#23376;&#20219;&#21153;&#30340;&#22266;&#26377;&#29305;&#24615;&#65292;&#37319;&#29992;&#21508;&#31181;&#26041;&#27861;&#21019;&#24314;&#19968;&#20010;&#20013;&#25991;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#32422;20&#19975;&#20010;&#20013;&#25991;&#25351;&#20196;&#35843;&#25972;&#26679;&#26412;&#65292;&#24182;&#36827;&#34892;&#20102;&#20154;&#24037;&#26816;&#26597;&#20197;&#30830;&#20445;&#39640;&#36136;&#37327;&#12290;&#25105;&#20204;&#36824;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#33521;&#25991;&#21644;&#20013;&#25991;&#25351;&#20196;&#35821;&#26009;&#24211;&#65292;&#24182;&#23545;&#19968;&#20123;&#28508;&#22312;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#31616;&#35201;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction tuning is widely recognized as a key technique for building generalist language models, which has attracted the attention of researchers and the public with the release of InstructGPT~\citep{ouyang2022training} and ChatGPT\footnote{\url{https://chat.openai.com/}}. Despite impressive progress in English-oriented large-scale language models (LLMs), it is still under-explored whether English-based foundation LLMs can perform similarly on multilingual tasks compared to English tasks with well-designed instruction tuning and how we can construct the corpora needed for the tuning.  To remedy this gap, we propose the project as an attempt to create a Chinese instruction dataset by various methods adapted to the intrinsic characteristics of 4 sub-tasks. We collect around 200k Chinese instruction tuning samples, which have been manually checked to guarantee high quality. We also summarize the existing English and Chinese instruction corpora and briefly describe some potential applic
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110; \texttt{mBART.CC25} &#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#21518;&#21521;&#32763;&#35793;&#21644;&#36801;&#31227;&#23398;&#20064;&#31561; NLP &#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#22686;&#24378;&#65292;&#20197;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.07869</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Neural Machine Translation For Low Resource Languages. (arXiv:2304.07869v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07869
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110; \texttt{mBART.CC25} &#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#21518;&#21521;&#32763;&#35793;&#21644;&#36801;&#31227;&#23398;&#20064;&#31561; NLP &#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#22686;&#24378;&#65292;&#20197;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#21644;&#27969;&#21160;&#24615;&#65292;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#22312;&#20960;&#31181;&#35821;&#35328;&#23545;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#65292;&#20294;&#22312;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793; (MNMT) &#39046;&#22495;&#30475;&#21040;&#20102;&#24456;&#22810;&#20851;&#27880;&#65292;&#21364;&#27809;&#26377;&#36827;&#34892;&#20840;&#38754;&#35843;&#26597;&#20197;&#30830;&#23450;&#21738;&#20123;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#12290;&#35813;&#39033;&#30446;&#30340;&#30446;&#26631;&#26159;&#30740;&#31350;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#39046;&#22495;&#65292;&#24182;&#26500;&#24314;&#19968;&#20010;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#35813;&#39033;&#30446;&#26088;&#22312;&#24314;&#31435;&#22312; \texttt{mBART.CC25} &#35821;&#35328;&#27169;&#22411;&#22522;&#30784;&#19978;&#65292;&#24182;&#25506;&#32034;&#21033;&#29992;&#21508;&#31181; NLP &#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65288;&#22914;&#21518;&#21521;&#32763;&#35793;&#21644;&#36801;&#31227;&#23398;&#20064;&#65289;&#26469;&#22686;&#24378;&#23427;&#30340;&#31574;&#30053;&#12290;&#35813;&#23454;&#29616;&#35797;&#22270;&#35299;&#24320; NMT &#24212;&#29992;&#31243;&#24207;&#30340;&#26550;&#26500;&#65292;&#24182;&#30830;&#23450;&#19981;&#21516;&#30340;&#32452;&#20214;&#65292;&#36825;&#20123;&#32452;&#20214;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#20462;&#25913;&#25152;&#36848;&#24212;&#29992;&#31243;&#24207;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Machine translation is a challenging task due to the inherent complex nature and the fluidity that natural languages bring. Nonetheless, in recent years, it has achieved state-of-the-art performance in several language pairs. Although, a lot of traction can be seen in the areas of multilingual neural machine translation (MNMT) in the recent years, there are no comprehensive survey done to identify what approaches work well. The goal of this project is to investigate the realm of low resource languages and build a Neural Machine Translation model to achieve state-of-the-art results. The project looks to build upon the \texttt{mBART.CC25} \cite{liu2020multilingual} language model and explore strategies to augment it with various NLP and Deep Learning techniques like back translation and transfer learning. This implementation tries to unpack the architecture of the NMT application and determine the different components which offers us opportunities to amend the said application wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33258;&#22238;&#24402;&#25991;&#26412;&#29983;&#25104;&#20013;&#20351;&#29992;&#21487;&#25805;&#20316;&#27010;&#29575;&#27169;&#22411;&#26469;&#24378;&#21046;&#23454;&#26045;&#38480;&#21046;&#30340;&#25511;&#21046;&#26041;&#27861;GeLaTo&#65292;&#24182;&#21462;&#24471;&#20102;&#22312;&#24120;&#35265;&#30340;&#32422;&#26463;&#25991;&#26412;&#29983;&#25104;&#27979;&#35797;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.07438</link><description>&lt;p&gt;
&#21487;&#25805;&#20316;&#30340;&#33258;&#22238;&#24402;&#35821;&#35328;&#29983;&#25104;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Tractable Control for Autoregressive Language Generation. (arXiv:2304.07438v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33258;&#22238;&#24402;&#25991;&#26412;&#29983;&#25104;&#20013;&#20351;&#29992;&#21487;&#25805;&#20316;&#27010;&#29575;&#27169;&#22411;&#26469;&#24378;&#21046;&#23454;&#26045;&#38480;&#21046;&#30340;&#25511;&#21046;&#26041;&#27861;GeLaTo&#65292;&#24182;&#21462;&#24471;&#20102;&#22312;&#24120;&#35265;&#30340;&#32422;&#26463;&#25991;&#26412;&#29983;&#25104;&#27979;&#35797;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#22238;&#24402;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#29983;&#25104;&#28385;&#36275;&#22797;&#26434;&#38480;&#21046;&#30340;&#25991;&#26412;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65306;&#21363;&#20351;&#26159;&#26368;&#31616;&#21333;&#30340;&#35789;&#27719;&#38480;&#21046;&#20063;&#20351;&#26465;&#20214;&#20998;&#24067;$\Pr(\text{text} | \alpha)$&#30340;&#37319;&#26679;&#21464;&#24471;&#19981;&#21487;&#35745;&#31639;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#21487;&#25805;&#20316;&#30340;&#27010;&#29575;&#27169;&#22411;&#23558;&#35789;&#27719;&#38480;&#21046;&#24378;&#21152;&#20110;&#33258;&#22238;&#24402;&#25991;&#26412;&#29983;&#25104;&#20013;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026; GeLaTo&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#20010;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#31934;&#31616;&#30340;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#26469;&#25511;&#21046;&#20174;GPT2&#21040;&#33258;&#22238;&#24402;&#30340;&#29983;&#25104;&#12290;GeLaTo&#22312;&#32422;&#26463;&#25991;&#26412;&#29983;&#25104;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;CommonGen&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#22823;&#24133;&#20987;&#36133;&#20102;&#21508;&#31181;&#24378;&#22522;&#32447;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#19981;&#20165;&#20026;&#25511;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#65292;&#36824;&#28608;&#21169;&#20154;&#20204;&#24320;&#21457;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#21487;&#25805;&#20316;&#27010;&#29575;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of autoregressive large language models in text generation, it remains a major challenge to generate text that satisfies complex constraints: sampling from the conditional distribution $\Pr(\text{text} | \alpha)$ is intractable for even the simplest lexical constraints $\alpha$. To overcome this challenge, we propose to use tractable probabilistic models to impose lexical constraints in autoregressive text generation, which we refer to as GeLaTo. To demonstrate the effectiveness of this framework, we use distilled hidden Markov models to control autoregressive generation from GPT2. GeLaTo achieves state-of-the-art performance on CommonGen, a challenging benchmark for constrained text generation, beating a wide range of strong baselines by a large margin. Our work not only opens up new avenues for controlling large language models but also motivates the development of more expressive tractable probabilistic models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32467;&#26500;&#21387;&#32553;&#21644;&#27169;&#22411;&#23610;&#23544;&#19981;&#23545;&#31216;&#30340;&#21452;&#32534;&#30721;&#22120;&#27169;&#22411; KALE&#65292;&#26377;&#25928;&#25552;&#39640;&#23494;&#38598;&#20449;&#24687;&#26816;&#32034;&#30340;&#25512;&#29702;&#25928;&#29575;&#65292;&#21516;&#26102;&#20801;&#35768;&#26597;&#35810;&#32534;&#30721;&#22120;&#30340;&#26377;&#25928;&#21387;&#32553;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#20840;&#37096;&#30340;&#20877;&#35757;&#32451;&#25110;&#32034;&#24341;&#29983;&#25104;&#65292;&#27492;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#36229;&#36807;DistilBERT&#24615;&#33021;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.01016</link><description>&lt;p&gt;
&#24555;&#36895;&#23494;&#38598;&#20449;&#24687;&#26816;&#32034;&#22120;&#21033;&#29992;KALE&#36827;&#34892;&#21518;&#32622;KL&#23545;&#40784;&#30340;&#24322;&#24418;&#21452;&#32534;&#30721;&#22120;&#27169;&#22411;&#35757;&#32451; (arXiv:2304.01016v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
Quick Dense Retrievers Consume KALE: Post Training Kullback Leibler Alignment of Embeddings for Asymmetrical dual encoders. (arXiv:2304.01016v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32467;&#26500;&#21387;&#32553;&#21644;&#27169;&#22411;&#23610;&#23544;&#19981;&#23545;&#31216;&#30340;&#21452;&#32534;&#30721;&#22120;&#27169;&#22411; KALE&#65292;&#26377;&#25928;&#25552;&#39640;&#23494;&#38598;&#20449;&#24687;&#26816;&#32034;&#30340;&#25512;&#29702;&#25928;&#29575;&#65292;&#21516;&#26102;&#20801;&#35768;&#26597;&#35810;&#32534;&#30721;&#22120;&#30340;&#26377;&#25928;&#21387;&#32553;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#20840;&#37096;&#30340;&#20877;&#35757;&#32451;&#25110;&#32034;&#24341;&#29983;&#25104;&#65292;&#27492;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#36229;&#36807;DistilBERT&#24615;&#33021;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#32467;&#26500;&#21387;&#32553;&#21644;&#27169;&#22411;&#23610;&#23544;&#19981;&#23545;&#31216;&#30340;&#21452;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#39640;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#23494;&#38598;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;&#36890;&#36807;&#23545;MSMARCO&#12289;&#33258;&#28982;&#38382;&#31572;&#12289;&#38382;&#31572;&#28216;&#25103;&#31561;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#21069;&#21518;&#35757;&#32451;&#21387;&#32553;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;&#21387;&#32553;&#23545;&#31995;&#32479;&#25512;&#29702;&#25928;&#29575;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#23494;&#38598;&#20449;&#24687;&#26816;&#32034;&#22120;&#30340;&#21452;&#32534;&#30721;&#22120;&#32467;&#26500;&#24322;&#24418;&#21270;&#26377;&#21161;&#20110;&#25552;&#39640;&#20854;&#25512;&#29702;&#25928;&#29575;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;Kullback Leibler Alignment of Embeddings (KALE)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35009;&#21098;&#21644;&#23545;&#40784;&#26597;&#35810;&#32534;&#30721;&#22120;&#65292;&#25552;&#39640;&#20102;&#23494;&#38598;&#20449;&#24687;&#26816;&#32034;&#30340;&#25512;&#29702;&#25928;&#29575;&#12290;KALE&#25193;&#23637;&#20102;&#20256;&#32479;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#20351;&#24471;&#22312;&#21452;&#32534;&#30721;&#22120;&#35757;&#32451;&#21518;&#21487;&#20197;&#26377;&#25928;&#22320;&#23545;&#26597;&#35810;&#32534;&#30721;&#22120;&#36827;&#34892;&#21387;&#32553;&#32780;&#26080;&#38656;&#36827;&#34892;&#23436;&#25972;&#30340;&#20877;&#35757;&#32451;&#25110;&#32034;&#24341;&#29983;&#25104;&#12290;&#20351;&#29992;KALE&#21644;&#19981;&#23545;&#31216;&#35757;&#32451;&#65292;&#25105;&#20204;&#21487;&#20197;&#29983;&#25104;&#36229;&#36807;DistilBERT&#24615;&#33021;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#27169;&#22411;&#23610;&#23544;&#26356;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider the problem of improving the inference latency of language model-based dense retrieval systems by introducing structural compression and model size asymmetry between the context and query encoders. First, we investigate the impact of pre and post-training compression on the MSMARCO, Natural Questions, TriviaQA, SQUAD, and SCIFACT, finding that asymmetry in the dual encoders in dense retrieval can lead to improved inference efficiency. Knowing this, we introduce Kullback Leibler Alignment of Embeddings (KALE), an efficient and accurate method for increasing the inference efficiency of dense retrieval methods by pruning and aligning the query encoder after training. Specifically, KALE extends traditional Knowledge Distillation after bi-encoder training, allowing for effective query encoder compression without full retraining or index generation. Using KALE and asymmetric training, we can generate models which exceed the performance of DistilBERT despite having 
&lt;/p&gt;</description></item><item><title>GrapeQA&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#8220;&#37325;&#35201;&#23454;&#20307;&#22270;&#24418;&#22686;&#24378;&#8221;&#21644;&#8220;&#19978;&#19979;&#25991;&#24863;&#30693;&#33410;&#28857;&#21098;&#26525;&#8221;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#38382;&#31572;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.12320</link><description>&lt;p&gt;
GrapeQA&#65306;&#22686;&#24378;&#38382;&#31572;&#21151;&#33021;&#30340;&#22270;&#24418;&#22686;&#24378;&#21644;&#21098;&#26525;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GrapeQA: GRaph Augmentation and Pruning to Enhance Question-Answering. (arXiv:2303.12320v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12320
&lt;/p&gt;
&lt;p&gt;
GrapeQA&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#8220;&#37325;&#35201;&#23454;&#20307;&#22270;&#24418;&#22686;&#24378;&#8221;&#21644;&#8220;&#19978;&#19979;&#25991;&#24863;&#30693;&#33410;&#28857;&#21098;&#26525;&#8221;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#38382;&#31572;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24120;&#35782;&#38382;&#31572;&#26041;&#27861;&#32467;&#21512;&#20102;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#30340;&#33021;&#21147;&#21644;&#30693;&#35782;&#22270;&#65288;KG&#65289;&#25552;&#20379;&#30340;&#25512;&#29702;&#12290; &#20856;&#22411;&#26041;&#27861;&#20174;KG&#20013;&#25910;&#38598;&#19982;QA&#21305;&#37197;&#30340;&#33410;&#28857;&#20197;&#24418;&#25104;&#24037;&#20316;&#22270;&#65288;WG&#65289;&#65292;&#28982;&#21518;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#36827;&#34892;&#25512;&#29702;&#12290;&#36825;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#65288;i&#65289;&#24456;&#38590;&#20174;WG&#20013;&#25429;&#33719;QA&#20013;&#30340;&#25152;&#26377;&#20449;&#24687;&#65292;&#65288;ii&#65289;WG&#21253;&#21547;&#19968;&#20123;&#26469;&#33258;KG&#30340;&#19981;&#30456;&#20851;&#33410;&#28857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GrapeQA&#30340;&#31639;&#27861;&#20197;&#23545;WG&#36827;&#34892;&#20004;&#20010;&#31616;&#21333;&#30340;&#25913;&#36827;&#65306;&#65288;i&#65289;&#29992;&#20110;&#22270;&#24418;&#22686;&#24378;&#30340;&#37325;&#35201;&#23454;&#20307;&#65288;Prominent Entities&#65289;&#35782;&#21035;QA&#23545;&#24403;&#20013;&#30456;&#20851;&#25991;&#26412;&#22359;&#65292;&#24182;&#20351;&#29992;&#30456;&#24212;&#30340;&#28508;&#22312;&#34920;&#31034;&#20174;LM&#36827;&#34892;&#22686;&#24378;&#65307;&#65288;ii&#65289;&#23558;&#19981;&#30456;&#20851;&#30340;&#33410;&#28857;&#21098;&#26525;&#12290;&#25105;&#20204;&#22312;OpenBookQA&#65292;CommonsenseQA&#21644;MedQA-USMLE&#19978;&#35780;&#20272;&#20102;&#32467;&#26524;&#65292;&#24182;&#21457;&#29616;GrapeQA&#26174;&#31034;&#20986;&#25345;&#32493;&#30340;&#25913;&#36827;&#65292;&#36229;&#36807;&#20102;&#20854;LM + KG&#21069;&#36523;&#65288;&#29305;&#21035;&#26159;QA-GNN&#65289;&#24182;&#33719;&#24471;&#20102;&#24040;&#22823;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Commonsense question-answering (QA) methods combine the power of pre-trained Language Models (LM) with the reasoning provided by Knowledge Graphs (KG). A typical approach collects nodes relevant to the QA pair from a KG to form a Working Graph (WG) followed by reasoning using Graph Neural Networks(GNNs). This faces two major challenges: (i) it is difficult to capture all the information from the QA in the WG, and (ii) the WG contains some irrelevant nodes from the KG. To address these, we propose GrapeQA with two simple improvements on the WG: (i) Prominent Entities for Graph Augmentation identifies relevant text chunks from the QA pair and augments the WG with corresponding latent representations from the LM, and (ii) Context-Aware Node Pruning removes nodes that are less relevant to the QA pair. We evaluate our results on OpenBookQA, CommonsenseQA and MedQA-USMLE and see that GrapeQA shows consistent improvements over its LM + KG predecessor (QA-GNN in particular) and large improveme
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#24037;&#20316;&#22330;&#25152;&#20013;&#30340;&#32844;&#19994;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;&#20219;&#21153;&#25552;&#31034;&#24037;&#31243;&#35774;&#35745;&#20102;&#33391;&#22909;&#30340;&#25552;&#31034;&#65292;&#25104;&#21151;&#22320;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#29992;&#20110;&#35813;&#20219;&#21153;&#20013;&#65292;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#34920;&#29616;&#65292;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.07142</link><description>&lt;p&gt;
&#24037;&#20316;&#22330;&#25152;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#20010;&#20851;&#20110;&#20219;&#21153;&#25552;&#31034;&#24037;&#31243;&#22312;&#32844;&#19994;&#31867;&#22411;&#20998;&#31867;&#20013;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Large Language Models in the Workplace: A Case Study on Prompt Engineering for Job Type Classification. (arXiv:2303.07142v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07142
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#24037;&#20316;&#22330;&#25152;&#20013;&#30340;&#32844;&#19994;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;&#20219;&#21153;&#25552;&#31034;&#24037;&#31243;&#35774;&#35745;&#20102;&#33391;&#22909;&#30340;&#25552;&#31034;&#65292;&#25104;&#21151;&#22320;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#29992;&#20110;&#35813;&#20219;&#21153;&#20013;&#65292;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#34920;&#29616;&#65292;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25506;&#32034;&#22810;&#31181;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;&#30417;&#30563;&#23398;&#20064;&#30340;&#20256;&#32479;&#27169;&#22411;&#22914;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVMs&#65289;&#20197;&#21450;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;DeBERTa&#65292;&#20197;&#21450;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#20998;&#31867;&#24773;&#20917;&#19979;&#30340;&#24212;&#29992;&#65292;&#26469;&#30740;&#31350;&#23454;&#38469;&#24037;&#20316;&#22330;&#25152;&#20013;&#30340;&#32844;&#19994;&#20998;&#31867;&#20219;&#21153;&#12290;&#20026;&#20102;&#23436;&#25104;&#27492;&#20219;&#21153;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20219;&#21153;&#25552;&#31034;&#24037;&#31243;&#30340;&#25216;&#26415;&#65292;&#21363;&#35774;&#35745;&#25552;&#31034;&#20197;&#24341;&#23548;LLMs&#36798;&#21040;&#25152;&#38656;&#30340;&#36755;&#20986;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20004;&#31181;&#21830;&#19994;&#21487;&#29992;&#30340;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;GPT-3.5&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;text-davinci-003&#21644;gpt-3.5-turbo&#12290;&#25105;&#20204;&#36824;&#23545;&#25552;&#31034;&#24037;&#31243;&#30340;&#19981;&#21516;&#26041;&#38754;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#33391;&#22909;&#35774;&#35745;&#30340;&#25552;&#31034;&#30340;&#24110;&#21161;&#19979;&#65292;LLMs&#22312;&#32844;&#19994;&#31867;&#22411;&#20998;&#31867;&#20219;&#21153;&#19978;&#21487;&#20197;&#36798;&#21040;&#20986;&#33394;&#30340;&#34920;&#29616;&#65292;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#22914;SVMs&#65292;&#29978;&#33267;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22914;DeBERTa&#12290;
&lt;/p&gt;
&lt;p&gt;
This case study investigates the task of job classification in a real-world setting, where the goal is to determine whether an English-language job posting is appropriate for a graduate or entry-level position. We explore multiple approaches to text classification, including supervised approaches such as traditional models like Support Vector Machines (SVMs) and state-of-the-art deep learning methods such as DeBERTa. We compare them with Large Language Models (LLMs) used in both few-shot and zero-shot classification settings. To accomplish this task, we employ prompt engineering, a technique that involves designing prompts to guide the LLMs towards the desired output. Specifically, we evaluate the performance of two commercially available state-of-the-art GPT-3.5-based language models, text-davinci-003 and gpt-3.5-turbo. We also conduct a detailed analysis of the impact of different aspects of prompt engineering on the model's performance. Our results show that, with a well-designed pr
&lt;/p&gt;</description></item><item><title>&#12298;Reception Reader&#12299;&#26159;&#19968;&#20010;web&#24037;&#20855;&#65292;&#29992;&#20110;&#25506;&#32034;&#26089;&#26399;&#33521;&#22269;&#20986;&#29256;&#29289;&#20013;&#30340;&#25991;&#26412;&#37325;&#29992;&#24773;&#20917;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#20849;&#20139;&#25991;&#26412;&#29255;&#27573;&#25506;&#32034;&#26576;&#19968;&#20316;&#21697;&#30340;&#25509;&#21463;&#24773;&#20917;&#25110;&#20854;&#27969;&#20837;&#36830;&#25509;&#21382;&#21490;&#65292;&#24182;&#19988;&#21487;&#20197;&#20132;&#20114;&#24335;&#22320;&#27983;&#35272;&#36830;&#25509;&#25991;&#26723;&#30340;&#35814;&#32454;&#20449;&#24687;&#65292;&#20197;&#21450;&#26816;&#26597;&#37325;&#29992;&#25991;&#26412;&#30340;&#19978;&#19979;&#25991;&#20197;&#36827;&#34892;&#8220;&#36817;&#36317;&#31163;&#38405;&#35835;&#8221;&#12290;</title><link>http://arxiv.org/abs/2302.04084</link><description>&lt;p&gt;
&#25506;&#32034;&#26089;&#26399;&#33521;&#22269;&#20986;&#29256;&#29289;&#20013;&#30340;&#25991;&#26412;&#37325;&#29992;&#24773;&#20917;&#65306;&#12298;Reception Reader&#12299;
&lt;/p&gt;
&lt;p&gt;
Reception Reader: Exploring Text Reuse in Early Modern British Publications. (arXiv:2302.04084v2 [cs.DL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04084
&lt;/p&gt;
&lt;p&gt;
&#12298;Reception Reader&#12299;&#26159;&#19968;&#20010;web&#24037;&#20855;&#65292;&#29992;&#20110;&#25506;&#32034;&#26089;&#26399;&#33521;&#22269;&#20986;&#29256;&#29289;&#20013;&#30340;&#25991;&#26412;&#37325;&#29992;&#24773;&#20917;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#20849;&#20139;&#25991;&#26412;&#29255;&#27573;&#25506;&#32034;&#26576;&#19968;&#20316;&#21697;&#30340;&#25509;&#21463;&#24773;&#20917;&#25110;&#20854;&#27969;&#20837;&#36830;&#25509;&#21382;&#21490;&#65292;&#24182;&#19988;&#21487;&#20197;&#20132;&#20114;&#24335;&#22320;&#27983;&#35272;&#36830;&#25509;&#25991;&#26723;&#30340;&#35814;&#32454;&#20449;&#24687;&#65292;&#20197;&#21450;&#26816;&#26597;&#37325;&#29992;&#25991;&#26412;&#30340;&#19978;&#19979;&#25991;&#20197;&#36827;&#34892;&#8220;&#36817;&#36317;&#31163;&#38405;&#35835;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
The Reception Reader&#26159;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;Early English Books Online&#65288;EEBO-TCP&#65289;&#21644;Eighteenth Century Collections Online&#65288;ECCO&#65289;&#25968;&#25454;&#20013;&#25991;&#26412;&#37325;&#29992;&#30340;web&#24037;&#20855;&#12290;&#29992;&#25143;&#21487;&#20197;&#65306;1&#65289;&#22522;&#20110;&#20849;&#20139;&#25991;&#26412;&#29255;&#27573;&#25506;&#32034;&#26576;&#19968;&#20316;&#21697;&#30340;&#25509;&#21463;&#24773;&#20917;&#25110;&#20854;&#27969;&#20837;&#36830;&#25509;&#21382;&#21490;&#65307;2&#65289;&#20132;&#20114;&#24335;&#22320;&#27983;&#35272;&#36830;&#25509;&#25991;&#26723;&#30340;&#35814;&#32454;&#20449;&#24687;&#65307;&#20197;&#21450;3&#65289;&#26816;&#26597;&#37325;&#29992;&#25991;&#26412;&#30340;&#19978;&#19979;&#25991;&#20197;&#36827;&#34892;&#8220;&#36817;&#36317;&#31163;&#38405;&#35835;&#8221;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#24037;&#20855;&#22914;&#20309;&#31616;&#21270;&#30740;&#31350;&#21644;&#25506;&#32034;&#20219;&#21153;&#65292;&#24182;&#35752;&#35770;&#20102;&#29992;&#25143;&#30028;&#38754;&#30340;&#23454;&#29992;&#24615;&#21644;&#23616;&#38480;&#24615;&#20197;&#21450;&#24403;&#21069;&#25968;&#25454;&#26469;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Reception Reader is a web tool for studying text reuse in the Early English Books Online (EEBO-TCP) and Eighteenth Century Collections Online (ECCO) data. Users can: 1) explore a visual overview of the reception of a work, or its incoming connections, across time based on shared text segments, 2) interactively survey the details of connected documents, and 3) examine the context of reused text for "close reading". We show examples of how the tool streamlines research and exploration tasks, and discuss the utility and limitations of the user interface along with its current data sources.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;AMSAL&#31639;&#27861;&#65292;&#21487;&#21435;&#38500;&#31070;&#32463;&#34920;&#24449;&#20013;&#30340;&#23545;&#40784;&#23646;&#24615;&#38544;&#21547;&#20449;&#24687;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#26377;&#25928;&#28040;&#38500;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2302.02997</link><description>&lt;p&gt;
&#31070;&#32463;&#34920;&#24449;&#30340;&#38750;&#23545;&#40784;&#23646;&#24615;&#28040;&#38500;
&lt;/p&gt;
&lt;p&gt;
Erasure of Unaligned Attributes from Neural Representations. (arXiv:2302.02997v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02997
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;AMSAL&#31639;&#27861;&#65292;&#21487;&#21435;&#38500;&#31070;&#32463;&#34920;&#24449;&#20013;&#30340;&#23545;&#40784;&#23646;&#24615;&#38544;&#21547;&#20449;&#24687;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#26377;&#25928;&#28040;&#38500;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#37197;-&#26368;&#22823;&#21270;&#35889;&#23646;&#24615;&#21435;&#38500;&#65288;AMSAL&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#22312;&#23545;&#40784;&#23646;&#24615;&#38544;&#21547;&#20110;&#36755;&#20837;&#26679;&#20363;&#20013;&#26102;&#28040;&#38500;&#31070;&#32463;&#34920;&#24449;&#20013;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#36890;&#36807;&#20132;&#26367;&#20004;&#27493;&#25805;&#20316;&#26469;&#23454;&#29616;&#12290;&#31532;&#19968;&#27493;&#65292;&#25214;&#21040;&#36755;&#20837;&#34920;&#24449;&#19982;&#35201;&#21024;&#38500;&#30340;&#20449;&#24687;&#20043;&#38388;&#30340;&#21305;&#37197;&#65292;&#31532;&#20108;&#27493;&#65292;&#23558;&#36755;&#20837;&#34920;&#24449;&#21644;&#35201;&#21024;&#38500;&#30340;&#20449;&#24687;&#25237;&#24433;&#21040;&#30456;&#21516;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#24102;&#26377;&#22810;&#20010;&#21463;&#20445;&#25252;&#23646;&#24615;&#30340;Twitter&#25968;&#25454;&#38598;&#65292;BiasBios&#25968;&#25454;&#38598;&#21644;BiasBench&#22522;&#20934;&#27979;&#35797;&#12290; BiasBench &#22522;&#20934;&#27979;&#35797;&#21253;&#25324;&#22235;&#20010;&#20855;&#26377;&#21508;&#31181;&#31867;&#22411;&#21463;&#20445;&#25252;&#23646;&#24615;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65306;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#26377;&#25928;&#28040;&#38500;&#20559;&#35265;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#24403;&#20027;&#20219;&#21153;&#19982;&#35201;&#21024;&#38500;&#30340;&#20449;&#24687;&#20043;&#38388;&#23384;&#22312;&#24378;&#32806;&#21512;&#26102;&#65292;&#25105;&#20204;&#31639;&#27861;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the Assignment-Maximization Spectral Attribute removaL (AMSAL) algorithm, which erases information from neural representations when the information to be erased is implicit rather than directly being aligned to each input example. Our algorithm works by alternating between two steps. In one, it finds an assignment of the input representations to the information to be erased, and in the other, it creates projections of both the input representations and the information to be erased into a joint latent space. We test our algorithm on an extensive array of datasets, including a Twitter dataset with multiple guarded attributes, the BiasBios dataset and the BiasBench benchmark. The last benchmark includes four datasets with various types of protected attributes. Our results demonstrate that bias can often be removed in our setup. We also discuss the limitations of our approach when there is a strong entanglement between the main task and the information to be erased.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23398;&#20064;&#22270;&#20687;&#21644;&#30456;&#26426;&#20803;&#25968;&#25454;&#20043;&#38388;&#30340;&#20132;&#21449;&#27169;&#24577;&#20851;&#32852;&#25552;&#21462;&#30456;&#26426;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#24471;&#21040;&#30340;&#29305;&#24449;&#25104;&#21151;&#23454;&#29616;&#25340;&#25509;&#22270;&#20687;&#21306;&#22495;&#30340;"&#38646;&#26679;&#26412;"&#23450;&#20301;&#12290;</title><link>http://arxiv.org/abs/2301.04647</link><description>&lt;p&gt;
EXIF&#20316;&#20026;&#19968;&#31181;&#35821;&#35328;&#65306;&#23398;&#20064;&#22270;&#20687;&#19982;&#30456;&#26426;&#20803;&#25968;&#25454;&#20043;&#38388;&#30340;&#20132;&#21449;&#27169;&#24577;&#20851;&#32852;
&lt;/p&gt;
&lt;p&gt;
EXIF as Language: Learning Cross-Modal Associations Between Images and Camera Metadata. (arXiv:2301.04647v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23398;&#20064;&#22270;&#20687;&#21644;&#30456;&#26426;&#20803;&#25968;&#25454;&#20043;&#38388;&#30340;&#20132;&#21449;&#27169;&#24577;&#20851;&#32852;&#25552;&#21462;&#30456;&#26426;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#24471;&#21040;&#30340;&#29305;&#24449;&#25104;&#21151;&#23454;&#29616;&#25340;&#25509;&#22270;&#20687;&#21306;&#22495;&#30340;"&#38646;&#26679;&#26412;"&#23450;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#35270;&#35273;&#34920;&#31034;&#65292;&#20174;&#32780;&#25552;&#21462;&#19982;&#25152;&#35760;&#24405;&#30340;&#29031;&#29255;&#30456;&#20851;&#30340;&#30456;&#26426;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#22270;&#20687;&#22359;&#21644;&#33258;&#21160;&#25554;&#20837;&#21040;&#22270;&#20687;&#25991;&#20214;&#20013;&#30340;EXIF&#20803;&#25968;&#25454;&#20043;&#38388;&#35757;&#32451;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#23884;&#20837;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#36890;&#36807;&#23558;&#20803;&#25968;&#25454;&#36716;&#25442;&#20026;&#25991;&#26412;&#65292;&#28982;&#21518;&#20351;&#29992;transformer&#36827;&#34892;&#22788;&#29702;&#26469;&#34920;&#31034;&#27492;&#20803;&#25968;&#25454;&#12290;&#25105;&#20204;&#23398;&#20064;&#30340;&#29305;&#24449;&#22312;&#19979;&#28216;&#22270;&#20687;&#21462;&#35777;&#21644;&#26657;&#20934;&#20219;&#21153;&#19978;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#33258;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#29305;&#24449;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#36890;&#36807;&#23545;&#22270;&#20687;&#20869;&#25152;&#26377;&#22359;&#30340;&#35270;&#35273;&#23884;&#20837;&#36827;&#34892;&#32858;&#31867;&#26469;&#23454;&#29616;"&#38646;&#26679;&#26412;"&#30340;&#25340;&#25509;&#22270;&#20687;&#21306;&#22495;&#23450;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;
We learn a visual representation that captures information about the camera that recorded a given photo. To do this, we train a multimodal embedding between image patches and the EXIF metadata that cameras automatically insert into image files. Our model represents this metadata by simply converting it to text and then processing it with a transformer. The features that we learn significantly outperform other self-supervised and supervised features on downstream image forensics and calibration tasks. In particular, we successfully localize spliced image regions "zero shot" by clustering the visual embeddings for all of the patches within an image.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25512;&#26029;&#23545;&#35805;&#20013;&#26368;&#21518;&#19968;&#27425;&#21457;&#35328;&#26469;&#25429;&#25417;&#35828;&#35805;&#32773;&#30340;&#24847;&#22270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#24847;&#22270;&#34701;&#21512;&#27169;&#22359;&#30340;&#20849;&#24773;&#23545;&#35805;&#29983;&#25104;&#27169;&#22411;InferEM&#12290;&#27169;&#22411;&#21516;&#26102;&#21033;&#29992;&#21069;&#20960;&#27425;&#21457;&#35328;&#39044;&#27979;&#26368;&#21518;&#19968;&#27425;&#21457;&#35328;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.06373</link><description>&lt;p&gt;
InferEM: &#25512;&#26029;&#35828;&#35805;&#32773;&#24847;&#22270;&#30340;&#20849;&#24773;&#23545;&#35805;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
InferEM: Inferring the Speaker's Intention for Empathetic Dialogue Generation. (arXiv:2212.06373v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06373
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25512;&#26029;&#23545;&#35805;&#20013;&#26368;&#21518;&#19968;&#27425;&#21457;&#35328;&#26469;&#25429;&#25417;&#35828;&#35805;&#32773;&#30340;&#24847;&#22270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#24847;&#22270;&#34701;&#21512;&#27169;&#22359;&#30340;&#20849;&#24773;&#23545;&#35805;&#29983;&#25104;&#27169;&#22411;InferEM&#12290;&#27169;&#22411;&#21516;&#26102;&#21033;&#29992;&#21069;&#20960;&#27425;&#21457;&#35328;&#39044;&#27979;&#26368;&#21518;&#19968;&#27425;&#21457;&#35328;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#20849;&#24773;&#22238;&#22797;&#29983;&#25104;&#30340;&#26041;&#27861;&#19968;&#33324;&#30452;&#25509;&#32534;&#30721;&#25972;&#20010;&#23545;&#35805;&#21382;&#21490;&#65292;&#28982;&#21518;&#36890;&#36807;&#35299;&#30721;&#22120;&#29983;&#25104;&#21451;&#22909;&#30340;&#21453;&#39304;&#12290;&#36825;&#20123;&#26041;&#27861;&#24378;&#35843;&#24314;&#27169;&#24773;&#22659;&#20449;&#24687;&#65292;&#20294;&#24573;&#35270;&#20102;&#25429;&#25417;&#35828;&#35805;&#32773;&#30340;&#30452;&#25509;&#24847;&#22270;&#12290;&#25105;&#20204;&#35748;&#20026;&#23545;&#35805;&#20013;&#26368;&#21518;&#19968;&#27425;&#21457;&#35328;&#34920;&#36798;&#20102;&#35828;&#35805;&#32773;&#30340;&#24847;&#22270;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InferEM&#30340;&#26032;&#27169;&#22411;&#29992;&#20110;&#20849;&#24773;&#22238;&#22797;&#29983;&#25104;&#12290;&#25105;&#20204;&#23558;&#26368;&#21518;&#19968;&#27425;&#21457;&#35328;&#21333;&#29420;&#32534;&#30721;&#65292;&#36890;&#36807;&#22522;&#20110;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#24847;&#22270;&#34701;&#21512;&#27169;&#22359;&#19982;&#25972;&#20010;&#23545;&#35805;&#34701;&#21512;&#20197;&#25429;&#25417;&#35828;&#35805;&#32773;&#30340;&#24847;&#22270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#21069;&#20960;&#27425;&#21457;&#35328;&#39044;&#27979;&#26368;&#21518;&#19968;&#27425;&#21457;&#35328;&#65292;&#20197;&#27169;&#25311;&#20154;&#31867;&#30340;&#24515;&#29702;&#65292;&#29468;&#27979;&#23545;&#35805;&#32773;&#21487;&#33021;&#25552;&#21069;&#35828;&#20123;&#20160;&#20040;&#12290;&#20026;&#24179;&#34913;&#21457;&#35328;&#39044;&#27979;&#21644;&#22238;&#22797;&#29983;&#25104;&#30340;&#20248;&#21270;&#36895;&#29575;&#65292;InferEM&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#23398;&#20064;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current approaches to empathetic response generation typically encode the entire dialogue history directly and put the output into a decoder to generate friendly feedback. These methods focus on modelling contextual information but neglect capturing the direct intention of the speaker. We argue that the last utterance in the dialogue empirically conveys the intention of the speaker. Consequently, we propose a novel model named InferEM for empathetic response generation. We separately encode the last utterance and fuse it with the entire dialogue through the multi-head attention based intention fusion module to capture the speaker's intention. Besides, we utilize previous utterances to predict the last utterance, which simulates human's psychology to guess what the interlocutor may speak in advance. To balance the optimizing rates of the utterance prediction and response generation, a multi-task learning strategy is designed for InferEM. Experimental results demonstrate the plausibility
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22240;&#26524;&#32467;&#26500;&#27169;&#22411;&#20998;&#26512;&#20102;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#27169;&#22411;&#23398;&#20064;&#20915;&#31574;&#30340;&#21407;&#29702;&#65292;&#21457;&#29616;&#29616;&#26377;&#26368;&#20808;&#36827;&#27169;&#22411;&#21033;&#29992;&#38750;&#22240;&#26524;&#20449;&#24687;&#36827;&#34892;&#21028;&#20915;&#39044;&#27979;&#65292;&#36829;&#21453;&#27861;&#24459;&#35268;&#21017;&#20250;&#21066;&#24369;&#27169;&#22411;&#40065;&#26834;&#24615;&#21644;&#26222;&#36866;&#24615;&#24182;&#23548;&#33268;&#27495;&#35270;&#38382;&#39064;&#65292;&#25552;&#20986;&#22522;&#20110;&#22240;&#26524;&#24178;&#39044;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2211.03046</link><description>&lt;p&gt;
&#30693;&#35782;&#23601;&#26159;&#21147;&#37327;&#65306;&#29702;&#35299;&#22240;&#26524;&#20851;&#31995;&#20351;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#27169;&#22411;&#26356;&#20855;&#26222;&#36866;&#24615;&#21644;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Knowledge is Power: Understanding Causality Makes Legal judgment Prediction Models More Generalizable and Robust. (arXiv:2211.03046v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22240;&#26524;&#32467;&#26500;&#27169;&#22411;&#20998;&#26512;&#20102;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#27169;&#22411;&#23398;&#20064;&#20915;&#31574;&#30340;&#21407;&#29702;&#65292;&#21457;&#29616;&#29616;&#26377;&#26368;&#20808;&#36827;&#27169;&#22411;&#21033;&#29992;&#38750;&#22240;&#26524;&#20449;&#24687;&#36827;&#34892;&#21028;&#20915;&#39044;&#27979;&#65292;&#36829;&#21453;&#27861;&#24459;&#35268;&#21017;&#20250;&#21066;&#24369;&#27169;&#22411;&#40065;&#26834;&#24615;&#21644;&#26222;&#36866;&#24615;&#24182;&#23548;&#33268;&#27495;&#35270;&#38382;&#39064;&#65292;&#25552;&#20986;&#22522;&#20110;&#22240;&#26524;&#24178;&#39044;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#26159;&#19968;&#31181;&#22522;&#20110;&#27861;&#24459;&#35268;&#21017;&#26681;&#25454;&#20107;&#23454;&#25551;&#36848;&#39044;&#27979;&#21028;&#20915;&#30340;&#27861;&#24459;&#36741;&#21161;&#24037;&#20855;&#65292;&#26088;&#22312;&#32531;&#35299;&#26377;&#38480;&#27861;&#24459;&#20174;&#19994;&#20154;&#21592;&#30340;&#24040;&#22823;&#24037;&#20316;&#36127;&#25285;&#12290;&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#37319;&#29992;&#21508;&#31181;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;LJP&#20219;&#21153;&#20013;&#36827;&#34892;&#24494;&#35843;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#26681;&#25454;&#26080;&#20851;&#65288;&#25110;&#38750;&#22240;&#26524;&#65289;&#20449;&#24687;&#36827;&#34892;&#21028;&#20915;&#39044;&#27979;&#12290;&#36829;&#21453;&#27861;&#24459;&#35268;&#21017;&#19981;&#20165;&#21066;&#24369;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#26222;&#36866;&#24615;&#65292;&#36824;&#20250;&#23548;&#33268;&#20005;&#37325;&#30340;&#31038;&#20250;&#38382;&#39064;&#65292;&#22914;&#27495;&#35270;&#12290;&#26412;&#25991;&#20351;&#29992;&#22240;&#26524;&#32467;&#26500;&#27169;&#22411;&#65288;SCMs&#65289;&#29702;&#35770;&#22320;&#20998;&#26512;&#20102;LJP&#27169;&#22411;&#22914;&#20309;&#23398;&#20064;&#20570;&#20986;&#20915;&#31574;&#20197;&#21450;&#20026;&#20160;&#20040;&#23427;&#20204;&#21487;&#20197;&#22312;&#19981;&#23398;&#20064;&#22240;&#26524;&#20851;&#31995;&#30340;&#24773;&#20917;&#19979;&#25104;&#21151;&#36890;&#36807;&#20256;&#32479;&#30340;&#27979;&#35797;&#33539;&#24335;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20379;&#20004;&#31181;&#20998;&#21035;&#22522;&#20110;&#25968;&#25454;&#21644;&#27169;&#22411;&#30340;&#22240;&#26524;&#24178;&#39044;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Legal Judgment Prediction (LJP), aiming to predict a judgment based on fact descriptions according to rule of law, serves as legal assistance to mitigate the great work burden of limited legal practitioners. Most existing methods apply various large-scale pre-trained language models (PLMs) finetuned in LJP tasks to obtain consistent improvements. However, we discover the fact that the state-of-the-art (SOTA) model makes judgment predictions according to irrelevant (or non-casual) information. The violation of rule of law not only weakens the robustness and generalization ability of models but also results in severe social problems like discrimination. In this paper, we use causal structural models (SCMs) to theoretically analyze how LJP models learn to make decisions and why they can succeed in passing the traditional testing paradigm without learning causality. According to our analysis, we provide two solutions intervening on data and model by causality, respectively. In detail, we f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#24490;&#29615;&#27169;&#22411;&#30340;&#20248;&#36234;&#24615;&#24182;&#25552;&#20986;&#20102;&#32467;&#21512;&#26356;&#22909;&#30340;&#24490;&#29615;&#21333;&#20803;&#12289;&#26550;&#26500;&#12289;&#30446;&#26631;&#20989;&#25968;&#20197;&#21450;&#20248;&#21270;&#31639;&#27861;&#30340;&#25913;&#36827;&#26041;&#27861;&#65292;&#22312;&#23567;&#25968;&#25454;&#38598;&#21644;Enwik8&#21160;&#24577;&#35780;&#20272;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20248;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.01848</link><description>&lt;p&gt;
&#20877;&#22238;&#24402;&#21040;&#24490;&#29615;&#27169;&#22411;&#30340;&#35821;&#35328;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Circling Back to Recurrent Models of Language. (arXiv:2211.01848v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#24490;&#29615;&#27169;&#22411;&#30340;&#20248;&#36234;&#24615;&#24182;&#25552;&#20986;&#20102;&#32467;&#21512;&#26356;&#22909;&#30340;&#24490;&#29615;&#21333;&#20803;&#12289;&#26550;&#26500;&#12289;&#30446;&#26631;&#20989;&#25968;&#20197;&#21450;&#20248;&#21270;&#31639;&#27861;&#30340;&#25913;&#36827;&#26041;&#27861;&#65292;&#22312;&#23567;&#25968;&#25454;&#38598;&#21644;Enwik8&#21160;&#24577;&#35780;&#20272;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20248;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#20165;&#22240;&#20026;&#19968;&#20123;&#32431;&#24490;&#29615;&#27169;&#22411;&#20855;&#26377;&#38590;&#20197;&#20248;&#21270;&#21644;&#22312;&#29616;&#20170;&#30828;&#20214;&#19978;&#25928;&#29575;&#20302;&#31561;&#32570;&#28857;&#65292;&#24182;&#19981;&#24847;&#21619;&#30528;&#23427;&#20204;&#19981;&#26159;&#22909;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#20316;&#32773;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#32467;&#21512;&#26356;&#22909;&#30340;&#24490;&#29615;&#21333;&#20803;&#65292;&#26550;&#26500;&#65292;&#30446;&#26631;&#20989;&#25968;&#21644;&#20248;&#21270;&#31639;&#27861;&#26469;&#25913;&#21892;&#36825;&#20123;&#27169;&#22411;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#20316;&#32773;&#20204;&#22312;&#23567;&#25968;&#25454;&#38598;&#21644;Enwik8&#21160;&#24577;&#35780;&#20272;&#19978;&#24314;&#31435;&#20102;&#26032;&#30340;&#26368;&#20248;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Just because some purely recurrent models suffer from being hard to optimize and inefficient on today's hardware, they are not necessarily bad models of language. We demonstrate this by the extent to which these models can still be improved by a combination of a slightly better recurrent cell, architecture, objective, as well as optimization. In the process, we establish a new state of the art for language modelling on small datasets and on Enwik8 with dynamic evaluation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;Transformer Transducer&#35821;&#38899;&#35782;&#21035;&#20013;&#20351;&#29992;&#21487;&#21464;&#27880;&#24847;&#21147;&#25513;&#27169;&#26469;&#26500;&#24314;&#21487;&#36866;&#24212;&#19981;&#21516;&#22330;&#26223;&#30340;&#27169;&#22411;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20998;&#22359;&#25513;&#27169;&#34920;&#29616;&#26356;&#20248;&#65292;&#20351;&#29992;&#21487;&#21464;&#25513;&#27169;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.01438</link><description>&lt;p&gt;
&#21487;&#37197;&#32622;Transformer Transducer&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#21487;&#21464;&#27880;&#24847;&#21147;&#25513;&#27169;
&lt;/p&gt;
&lt;p&gt;
Variable Attention Masking for Configurable Transformer Transducer Speech Recognition. (arXiv:2211.01438v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;Transformer Transducer&#35821;&#38899;&#35782;&#21035;&#20013;&#20351;&#29992;&#21487;&#21464;&#27880;&#24847;&#21147;&#25513;&#27169;&#26469;&#26500;&#24314;&#21487;&#36866;&#24212;&#19981;&#21516;&#22330;&#26223;&#30340;&#27169;&#22411;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20998;&#22359;&#25513;&#27169;&#34920;&#29616;&#26356;&#20248;&#65292;&#20351;&#29992;&#21487;&#21464;&#25513;&#27169;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22522;&#20110;Transformer Transducer&#30340;&#35821;&#38899;&#35782;&#21035;&#20013;&#20351;&#29992;&#27880;&#24847;&#21147;&#25513;&#27169;&#20197;&#26500;&#24314;&#36866;&#24212;&#19981;&#21516;&#37096;&#32626;&#22330;&#26223;&#30340;&#21333;&#20010;&#21487;&#37197;&#32622;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#22266;&#23450;&#25513;&#27169;(&#27599;&#20010;&#24103;&#24212;&#29992;&#30456;&#21516;&#30340;&#27880;&#24847;&#21147;&#25513;&#27169;)&#21644;&#20998;&#22359;&#25513;&#27169;(&#27599;&#20010;&#24103;&#30340;&#27880;&#24847;&#21147;&#25513;&#27169;&#30001;&#22359;&#36793;&#30028;&#30830;&#23450;)&#22312;&#35782;&#21035;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#22312;&#35757;&#32451;&#26102;&#20174;&#30446;&#26631;&#20998;&#24067;&#20013;&#23545;&#27880;&#24847;&#21147;&#25513;&#27169;&#36827;&#34892;&#37319;&#26679;&#20197;&#26500;&#24314;&#33021;&#22815;&#22312;&#19981;&#21516;&#37197;&#32622;&#19979;&#24037;&#20316;&#30340;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#21333;&#20010;&#21487;&#37197;&#32622;&#27169;&#22411;&#26469;&#25191;&#34892;&#31532;&#19968;&#38454;&#27573;&#27969;&#35782;&#21035;&#21644;&#31532;&#20108;&#38454;&#27573;&#22768;&#23398;&#37325;&#25171;&#20998;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22266;&#23450;&#25513;&#27169;&#30456;&#27604;&#65292;&#20998;&#22359;&#25513;&#27169;&#22312;&#20934;&#30830;&#24615;&#19982;&#24310;&#36831;&#30340;&#25240;&#34935;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#26080;&#35770;&#26159;&#21542;&#20351;&#29992;FastEmit&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#21487;&#21464;&#25513;&#27169;&#21487;&#20351;&#20934;&#30830;&#24615;&#25552;&#39640;&#26368;&#22810;8&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work studies the use of attention masking in transformer transducer based speech recognition for building a single configurable model for different deployment scenarios. We present a comprehensive set of experiments comparing fixed masking, where the same attention mask is applied at every frame, with chunked masking, where the attention mask for each frame is determined by chunk boundaries, in terms of recognition accuracy and latency. We then explore the use of variable masking, where the attention masks are sampled from a target distribution at training time, to build models that can work in different configurations. Finally, we investigate how a single configurable model can be used to perform both first pass streaming recognition and second pass acoustic rescoring. Experiments show that chunked masking achieves a better accuracy vs latency trade-off compared to fixed masking, both with and without FastEmit. We also show that variable masking improves the accuracy by up to 8% 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#25311;&#40657;&#30418;&#31070;&#32463;&#25490;&#21517;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#21487;&#34987;&#40657;&#24125;SEO&#29992;&#26469;&#25171;&#36133;&#26356;&#22909;&#38450;&#25252;&#30340;&#25628;&#32034;&#24341;&#25806;&#12290;</title><link>http://arxiv.org/abs/2209.06506</link><description>&lt;p&gt;
&#38454;-&#26080;&#24207;&#65306;&#40657;&#30418;&#31070;&#32463;&#25490;&#21517;&#27169;&#22411;&#30340;&#27169;&#25311;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Order-Disorder: Imitation Adversarial Attacks for Black-box Neural Ranking Models. (arXiv:2209.06506v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#25311;&#40657;&#30418;&#31070;&#32463;&#25490;&#21517;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#21487;&#34987;&#40657;&#24125;SEO&#29992;&#26469;&#25171;&#36133;&#26356;&#22909;&#38450;&#25252;&#30340;&#25628;&#32034;&#24341;&#25806;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#25991;&#26412;&#25490;&#21517;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#30528;&#36827;&#23637;&#65292;&#24182;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#24212;&#29992;&#20110;&#23454;&#36341;&#20013;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20063;&#32487;&#25215;&#20102;&#19968;&#33324;&#31070;&#32463;&#27169;&#22411;&#30340;&#23545;&#25239;&#24615;&#24369;&#28857;&#65292;&#34429;&#28982;&#24050;&#34987;&#21457;&#29616;&#65292;&#20294;&#20173;&#26410;&#34987;&#20808;&#21069;&#30340;&#30740;&#31350;&#20805;&#20998;&#25506;&#35752;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#23545;&#25239;&#24615;&#24369;&#28857;&#21487;&#33021;&#34987;&#40657;&#24125;SEO&#29992;&#26469;&#25171;&#36133;&#26356;&#22909;&#38450;&#25252;&#30340;&#25628;&#32034;&#24341;&#25806;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#25311;&#40657;&#30418;&#31070;&#32463;&#27573;&#33853;&#25490;&#21517;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#30446;&#26631;&#25490;&#21517;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#26522;&#20030;&#20851;&#38190;&#26597;&#35810;/&#20505;&#36873;&#39033;&#36879;&#26126;&#21270;&#24182;&#27169;&#20223;&#65292;&#28982;&#21518;&#35757;&#32451;&#19968;&#20010;&#25490;&#21517;&#27169;&#20223;&#27169;&#22411;&#12290;&#21033;&#29992;&#25490;&#21517;&#27169;&#20223;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#31934;&#24515;&#25805;&#32437;&#25490;&#21517;&#32467;&#26524;&#24182;&#23558;&#25805;&#32437;&#25915;&#20987;&#36716;&#31227;&#21040;&#30446;&#26631;&#25490;&#21517;&#27169;&#22411;&#19978;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#20351;&#29992;&#37197;&#23545;&#30446;&#26631;&#20989;&#25968;&#24378;&#21270;&#65292;&#20197;&#29983;&#25104;&#23545;&#25239;&#35302;&#21457;&#22120;&#65292;&#20174;&#32780;&#23548;&#33268;PremE.
&lt;/p&gt;
&lt;p&gt;
Neural text ranking models have witnessed significant advancement and are increasingly being deployed in practice. Unfortunately, they also inherit adversarial vulnerabilities of general neural models, which have been detected but remain underexplored by prior studies. Moreover, the inherit adversarial vulnerabilities might be leveraged by blackhat SEO to defeat better-protected search engines. In this study, we propose an imitation adversarial attack on black-box neural passage ranking models. We first show that the target passage ranking model can be transparentized and imitated by enumerating critical queries/candidates and then train a ranking imitation model. Leveraging the ranking imitation model, we can elaborately manipulate the ranking results and transfer the manipulation attack to the target ranking model. For this purpose, we propose an innovative gradient-based attack method, empowered by the pairwise objective function, to generate adversarial triggers, which causes preme
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#24494;&#21338;&#25968;&#25454;&#38598;&#33258;&#21160;&#20998;&#26512;&#25991;&#21270;&#22320;&#22495;&#65292;&#21019;&#26032;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#20551;&#35774;&#12289;&#20559;&#35265;&#25110;&#25104;&#35265;&#26469;&#35782;&#21035;&#25991;&#21270;&#22320;&#21306;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#20154;&#20204;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#35752;&#35770;&#20027;&#39064;&#26469;&#25512;&#26029;&#25991;&#21270;&#24402;&#23646;&#65292;&#25581;&#31034;&#20102;&#32654;&#22269;&#22810;&#20010;&#20855;&#26377;&#26126;&#26174;&#36523;&#20221;&#30340;&#22320;&#21306;&#21644;&#20854;&#20182;&#22797;&#26434;&#22810;&#38754;&#30340;&#22320;&#21306;&#12290;</title><link>http://arxiv.org/abs/2208.07649</link><description>&lt;p&gt;
&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#30340;&#35789;&#27719;&#20998;&#26512;&#32472;&#21046;&#32654;&#22269;&#25991;&#21270;&#22320;&#22495;&#22270;
&lt;/p&gt;
&lt;p&gt;
American cultural regions mapped through the lexical analysis of social media. (arXiv:2208.07649v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.07649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#24494;&#21338;&#25968;&#25454;&#38598;&#33258;&#21160;&#20998;&#26512;&#25991;&#21270;&#22320;&#22495;&#65292;&#21019;&#26032;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#20551;&#35774;&#12289;&#20559;&#35265;&#25110;&#25104;&#35265;&#26469;&#35782;&#21035;&#25991;&#21270;&#22320;&#21306;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#20154;&#20204;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#35752;&#35770;&#20027;&#39064;&#26469;&#25512;&#26029;&#25991;&#21270;&#24402;&#23646;&#65292;&#25581;&#31034;&#20102;&#32654;&#22269;&#22810;&#20010;&#20855;&#26377;&#26126;&#26174;&#36523;&#20221;&#30340;&#22320;&#21306;&#21644;&#20854;&#20182;&#22797;&#26434;&#22810;&#38754;&#30340;&#22320;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#21270;&#39046;&#22495;&#20195;&#34920;&#20102;&#19968;&#20010;&#26377;&#29992;&#30340;&#27010;&#24565;&#65292;&#21487;&#20197;&#20419;&#36827;&#31038;&#20250;&#31185;&#23398;&#30340;&#19981;&#21516;&#39046;&#22495;&#30340;&#20132;&#27969;&#12290;&#20102;&#35299;&#20154;&#20204;&#22312;&#31038;&#20250;&#20013;&#22914;&#20309;&#32452;&#32455;&#21644;&#20851;&#32852;&#20182;&#20204;&#30340;&#24605;&#24819;&#21644;&#34892;&#20026;&#65292;&#26377;&#21161;&#20110;&#29702;&#35299;&#20182;&#20204;&#23545;&#19981;&#21516;&#38382;&#39064;&#30340;&#34892;&#21160;&#21644;&#24577;&#24230;&#12290;&#28982;&#32780;&#65292;&#22609;&#36896;&#25991;&#21270;&#22320;&#21306;&#30340;&#20849;&#21516;&#29305;&#24449;&#30340;&#36873;&#25321;&#26377;&#20123;&#38543;&#24847;&#12290;&#38656;&#35201;&#30340;&#26159;&#19968;&#31181;&#21487;&#20197;&#21033;&#29992;&#22823;&#37327;&#22312;&#32447;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#65292;&#35782;&#21035;&#25991;&#21270;&#22320;&#21306;&#32780;&#27809;&#26377;&#29305;&#23450;&#30340;&#20551;&#35774;&#12289;&#20559;&#35265;&#25110;&#25104;&#35265;&#30340;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20171;&#32461;&#19968;&#31181;&#22522;&#20110;&#33258;&#21160;&#20998;&#26512;&#24494;&#21338;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#26469;&#25512;&#26029;&#25991;&#21270;&#22320;&#21306;&#65292;&#20174;&#32780;&#26397;&#30528;&#36825;&#20010;&#26041;&#21521;&#36808;&#20986;&#20102;&#37325;&#35201;&#30340;&#19968;&#27493;&#12290;&#26412;&#25991;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#36825;&#26679;&#19968;&#20010;&#21407;&#21017;&#65306;&#25991;&#21270;&#24402;&#23646;&#21487;&#20197;&#36890;&#36807;&#20154;&#20204;&#22312;&#20182;&#20204;&#20043;&#38388;&#35752;&#35770;&#30340;&#20027;&#39064;&#26469;&#25512;&#26029;&#20986;&#26469;&#12290;&#20855;&#20307;&#22320;&#65292;&#22312;&#32654;&#22269;&#31038;&#20132;&#23186;&#20307;&#19978;&#27979;&#37327;&#20102;&#20070;&#38754;&#35805;&#35821;&#30340;&#22320;&#21306;&#21464;&#21270;&#65292;&#20174;&#26631;&#35760;&#22320;&#28857;&#20026;&#32654;&#22269;&#30340;&#25512;&#25991;&#20013;&#20351;&#29992;&#30340;&#21333;&#35789;&#21644;&#34920;&#36798;&#24335;&#30340;&#39057;&#29575;&#20998;&#24067;&#65292;&#26500;&#24314;&#20102;&#25991;&#21270;&#22320;&#21306;&#36718;&#24275;&#65292;&#20195;&#34920;&#20154;&#20204;&#20849;&#20139;&#24120;&#35265;&#30340;&#27807;&#36890;&#27169;&#24335;&#65292;&#24182;&#21487;&#33021;&#21463;&#21040;&#20849;&#21516;&#25991;&#21270;&#26681;&#28304;&#30340;&#24433;&#21709;&#12290;&#32654;&#22269;&#30340;&#25991;&#21270;&#22320;&#22270;&#26174;&#31034;&#20986;&#20960;&#20010;&#20855;&#26377;&#26126;&#26174;&#36523;&#20221;&#30340;&#22320;&#21306;&#21644;&#20854;&#20182;&#20855;&#26377;&#22797;&#26434;&#21644;&#22810;&#26041;&#38754;&#32452;&#25104;&#30340;&#22320;&#21306;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#19968;&#31181;&#23458;&#35266;&#12289;&#26174;&#33879;&#30340;&#26041;&#27861;&#65292;&#20197;&#35782;&#21035;&#19968;&#20010;&#22269;&#23478;&#20869;&#30340;&#25991;&#21270;&#22320;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cultural areas represent a useful concept that cross-fertilizes diverse fields in social sciences. Knowledge of how humans organize and relate their ideas and behavior within a society helps to understand their actions and attitudes towards different issues. However, the selection of common traits that shape a cultural area is somewhat arbitrary. What is needed is a method that can leverage the massive amounts of data coming online, especially through social media, to identify cultural regions without ad-hoc assumptions, biases or prejudices. This work takes a crucial step in this direction by introducing a method to infer cultural regions based on the automatic analysis of large datasets from microblogging posts. The approach presented here is based on the principle that cultural affiliation can be inferred from the topics that people discuss among themselves. Specifically, regional variations in written discourse are measured in American social media. From the frequency distributions
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#23545;&#25239;&#38450;&#24481;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#38450;&#24481;&#31070;&#32463;&#32593;&#32476;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20316;&#20026;&#27491;&#21017;&#21270;&#26426;&#21046;&#65292;&#38450;&#27490;&#27169;&#22411;&#36807;&#24230;&#25311;&#21512;&#12290;</title><link>http://arxiv.org/abs/2203.06414</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#23545;&#25239;&#38450;&#24481;&#21644;&#40065;&#26834;&#24615;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Adversarial Defences and Robustness in NLP. (arXiv:2203.06414v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.06414
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#23545;&#25239;&#38450;&#24481;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#38450;&#24481;&#31070;&#32463;&#32593;&#32476;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20316;&#20026;&#27491;&#21017;&#21270;&#26426;&#21046;&#65292;&#38450;&#27490;&#27169;&#22411;&#36807;&#24230;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#36234;&#26469;&#36234;&#26126;&#26174;&#30340;&#26159;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26080;&#27861;&#25269;&#24481;&#36755;&#20837;&#25968;&#25454;&#20013;&#30340;&#23545;&#25239;&#25200;&#21160;&#65292;&#20351;&#20854;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#21508;&#31181;&#20316;&#32773;&#24050;&#32463;&#38024;&#23545;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#20316;&#20026;&#22238;&#24212;&#65292;&#20063;&#25552;&#20986;&#20102;&#35768;&#22810;&#38450;&#24481;&#26426;&#21046;&#65292;&#20197;&#38450;&#27490;&#36825;&#20123;&#32593;&#32476;&#22833;&#36133;&#12290;&#20445;&#25252;&#31070;&#32463;&#32593;&#32476;&#20813;&#21463;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#37325;&#35201;&#24615;&#22312;&#20110;&#30830;&#20445;&#27169;&#22411;&#30340;&#39044;&#27979;&#21363;&#20351;&#36755;&#20837;&#25968;&#25454;&#21457;&#29983;&#25200;&#21160;&#20063;&#33021;&#20445;&#25345;&#19981;&#21464;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#22810;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#23545;&#25239;&#38450;&#24481;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#22914;&#25991;&#26412;&#20998;&#31867;&#65292;&#23454;&#20307;&#35782;&#21035;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#12290;&#20854;&#20013;&#19968;&#20123;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#38450;&#24481;&#31070;&#32463;&#32593;&#32476;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20316;&#20026;&#27491;&#21017;&#21270;&#26426;&#21046;&#65292;&#38450;&#27490;&#27169;&#22411;&#36807;&#24230;&#25311;&#21512;&#12290;&#26412;&#32508;&#36848;&#26088;&#22312;&#22238;&#39038;&#21508;&#31181;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#23545;&#25239;&#38450;&#24481;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the past few years, it has become increasingly evident that deep neural networks are not resilient enough to withstand adversarial perturbations in input data, leaving them vulnerable to attack. Various authors have proposed strong adversarial attacks for computer vision and Natural Language Processing (NLP) tasks. As a response, many defense mechanisms have also been proposed to prevent these networks from failing. The significance of defending neural networks against adversarial attacks lies in ensuring that the model's predictions remain unchanged even if the input data is perturbed. Several methods for adversarial defense in NLP have been proposed, catering to different NLP tasks such as text classification, named entity recognition, and natural language inference. Some of these methods not only defend neural networks against adversarial attacks but also act as a regularization mechanism during training, saving the model from overfitting. This survey aims to review the various m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;PMC-Patients&#8221;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#23450;&#20041;&#21644;&#27979;&#35797;&#30149;&#24739;&#21040;&#25991;&#31456;&#30340;&#26816;&#32034;&#65288;ReCDS-PAR&#65289;&#21644;&#30149;&#24739;&#21040;&#30149;&#24739;&#30340;&#26816;&#32034;&#65288;ReCDS-PPR&#65289;&#65292;&#20197;&#35780;&#20272;&#22522;&#20110;&#21484;&#22238;&#30340;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65288;ReCDS&#65289;&#30340;&#24615;&#33021;&#12290;PMC-Patients&#25968;&#25454;&#38598;&#28085;&#30422;&#36926;10,000&#21517;&#30149;&#24739;&#20449;&#24687;&#21644;27,000&#31687;PubMed Central&#25991;&#31456;&#65292;&#24182;&#23637;&#31034;&#20102;&#22810;&#31181;ReCDS&#31995;&#32479;&#30340;&#25928;&#26524;&#20998;&#26512;&#21644;20&#20010;&#26696;&#20363;&#30340;&#23454;&#29992;&#24615;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2202.13876</link><description>&lt;p&gt;
PMC-Patients: &#29992;&#20110;&#35780;&#20272;&#22522;&#20110;&#21484;&#22238;&#30340;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#30340;&#22823;&#35268;&#27169;&#30149;&#24739;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
PMC-Patients: A Large-scale Dataset of Patient Summaries and Relations for Benchmarking Retrieval-based Clinical Decision Support Systems. (arXiv:2202.13876v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.13876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;PMC-Patients&#8221;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#23450;&#20041;&#21644;&#27979;&#35797;&#30149;&#24739;&#21040;&#25991;&#31456;&#30340;&#26816;&#32034;&#65288;ReCDS-PAR&#65289;&#21644;&#30149;&#24739;&#21040;&#30149;&#24739;&#30340;&#26816;&#32034;&#65288;ReCDS-PPR&#65289;&#65292;&#20197;&#35780;&#20272;&#22522;&#20110;&#21484;&#22238;&#30340;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65288;ReCDS&#65289;&#30340;&#24615;&#33021;&#12290;PMC-Patients&#25968;&#25454;&#38598;&#28085;&#30422;&#36926;10,000&#21517;&#30149;&#24739;&#20449;&#24687;&#21644;27,000&#31687;PubMed Central&#25991;&#31456;&#65292;&#24182;&#23637;&#31034;&#20102;&#22810;&#31181;ReCDS&#31995;&#32479;&#30340;&#25928;&#26524;&#20998;&#26512;&#21644;20&#20010;&#26696;&#20363;&#30340;&#23454;&#29992;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#22522;&#20110;&#21484;&#22238;&#30340;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65288;ReCDS&#65289;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#30456;&#20851;&#25991;&#29486;&#21644;&#31867;&#20284;&#30149;&#24739;&#30340;&#20449;&#24687;&#26469;&#24110;&#21161;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#22810;&#26679;&#30340;&#30149;&#24739;&#25910;&#38598;&#21644;&#20844;&#24320;&#30340;&#22823;&#35268;&#27169;&#30149;&#24739;&#23618;&#38754;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;ReCDS&#31995;&#32479;&#30340;&#21457;&#23637;&#21463;&#21040;&#20102;&#20005;&#37325;&#38459;&#30861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#20351;&#29992;&#21517;&#20026;PMC-Patients&#30340;&#26032;&#25968;&#25454;&#38598;&#23450;&#20041;&#21644;&#27979;&#35797;&#20004;&#20010;ReCDS&#20219;&#21153;&#65306;&#30149;&#24739;&#21040;&#25991;&#31456;&#30340;&#26816;&#32034;&#65288;ReCDS-PAR&#65289;&#21644;&#30149;&#24739;&#21040;&#30149;&#24739;&#30340;&#26816;&#32034;&#65288;ReCDS-PPR&#65289;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#20351;&#29992;&#31616;&#21333;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#20174;PubMed Central&#25991;&#31456;&#20013;&#25552;&#21462;&#30149;&#24739;&#24635;&#32467;&#65292;&#24182;&#21033;&#29992;PubMed&#24341;&#25991;&#20851;&#31995;&#22270;&#26469;&#23450;&#20041;&#30149;&#24739;-&#25991;&#31456;&#30456;&#20851;&#24615;&#21644;&#30149;&#24739;-&#30149;&#24739;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#36824;&#22312;PMC-Patients&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#26045;&#21644;&#35780;&#20272;&#20102;&#20960;&#31181;ReCDS&#31995;&#32479;&#65292;&#21253;&#25324;&#31232;&#30095;&#26816;&#32034;&#22120;&#12289;&#23494;&#38598;&#26816;&#32034;&#22120;&#21644;&#26368;&#36817;&#37051;&#26816;&#32034;&#22120;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20960;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#20197;&#23637;&#31034;PMC-Patients&#30340;&#20020;&#24202;&#25928;&#29992;&#12290;&#32467;&#26524;&#65306;PMC-Patients&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;10,186&#21517;&#30149;&#24739;&#30340;&#20449;&#24687;&#65292;&#28085;&#30422;&#20102;&#36926;27,000&#31687;PubMed Central&#25991;&#31456;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#20010;ReCDS&#20219;&#21153;&#30340;&#22522;&#20934;&#35780;&#20272;&#32467;&#26524;&#21644;&#22810;&#31181;&#31995;&#32479;&#30340;&#25928;&#26524;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#20004;&#20010;&#20219;&#21153;&#30340;20&#20010;&#26696;&#20363;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;PMC-Patients&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: Retrieval-based Clinical Decision Support (ReCDS) can aid clinical workflow by providing relevant literature and similar patients for a given patient. However, the development of ReCDS systems has been severely obstructed by the lack of diverse patient collections and publicly available large-scale patient-level annotation datasets. In this paper, we aim to define and benchmark two ReCDS tasks: Patient-to-Article Retrieval (ReCDS-PAR) and Patient-to-Patient Retrieval (ReCDS-PPR) using a novel dataset called PMC-Patients.  Methods: We extract patient summaries from PubMed Central articles using simple heuristics and utilize the PubMed citation graph to define patient-article relevance and patient-patient similarity. We also implement and evaluate several ReCDS systems on the PMC-Patients benchmarks, including sparse retrievers, dense retrievers, and nearest neighbor retrievers. We conduct several case studies to show the clinical utility of PMC-Patients.  Results: PMC-Patient
&lt;/p&gt;</description></item><item><title>&#20302;&#36164;&#28304;&#24773;&#22659;&#19979;&#65292;&#22914;&#20309;&#35753;&#30693;&#35782;&#25277;&#21462;&#26356;&#22909;&#22320;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#25552;&#21462;&#20449;&#24687;&#65311;&#26412;&#25991;&#35843;&#30740;&#20102;&#19977;&#31181;&#35299;&#20915;&#33539;&#24335;&#65306;&#39640;&#36164;&#28304;&#25968;&#25454;&#12289;&#26356;&#24378;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#19982;&#27169;&#22411;&#30340;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2202.08063</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#24773;&#22659;&#19979;&#30340;&#30693;&#35782;&#25277;&#21462;&#65306;&#35843;&#30740;&#19982;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Knowledge Extraction in Low-Resource Scenarios: Survey and Perspective. (arXiv:2202.08063v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.08063
&lt;/p&gt;
&lt;p&gt;
&#20302;&#36164;&#28304;&#24773;&#22659;&#19979;&#65292;&#22914;&#20309;&#35753;&#30693;&#35782;&#25277;&#21462;&#26356;&#22909;&#22320;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#25552;&#21462;&#20449;&#24687;&#65311;&#26412;&#25991;&#35843;&#30740;&#20102;&#19977;&#31181;&#35299;&#20915;&#33539;&#24335;&#65306;&#39640;&#36164;&#28304;&#25968;&#25454;&#12289;&#26356;&#24378;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#19982;&#27169;&#22411;&#30340;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#25277;&#21462;&#65288;KE&#65289;&#26088;&#22312;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#25552;&#21462;&#32467;&#26500;&#20449;&#24687;&#65292;&#36890;&#24120;&#36973;&#21463;&#25968;&#25454;&#21294;&#20047;&#21644;&#20986;&#29616;&#26410;&#35265;&#31867;&#22411;&#65288;&#20302;&#36164;&#28304;&#24773;&#22659;&#65289;&#30340;&#22256;&#25200;&#12290;&#35768;&#22810;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#24050;&#24191;&#27867;&#30740;&#31350;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#23545;&#20302;&#36164;&#28304;&#24773;&#22659;&#19979;KE&#36827;&#34892;&#25991;&#29486;&#32508;&#36848;&#65292;&#24182;&#23558;&#29616;&#26377;&#30340;&#24037;&#20316;&#31995;&#32479;&#24615;&#22320;&#20998;&#20026;&#19977;&#31181;&#33539;&#24335;&#65306;&#65288;1&#65289;&#21033;&#29992;&#39640;&#36164;&#28304;&#25968;&#25454;&#65292;&#65288;2&#65289;&#21033;&#29992;&#26356;&#24378;&#30340;&#27169;&#22411;&#65292;&#65288;3&#65289;&#21516;&#26102;&#21033;&#29992;&#25968;&#25454;&#21644;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#25552;&#20986;&#26377;&#21069;&#36884;&#30340;&#24212;&#29992;&#65292;&#24182;&#27010;&#36848;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#19968;&#20123;&#28508;&#22312;&#26041;&#21521;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#35843;&#30740;&#21487;&#20197;&#24110;&#21161;&#23398;&#26415;&#21644;&#24037;&#19994;&#30028;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#19968;&#39046;&#22495;&#65292;&#28608;&#21457;&#26356;&#22810;&#30340;&#21019;&#24847;&#65292;&#25552;&#21319;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Extraction (KE), aiming to extract structural information from unstructured texts, often suffers from data scarcity and emerging unseen types, i.e., low-resource scenarios. Many neural approaches to low-resource KE have been widely investigated and achieved impressive performance. In this paper, we present a literature review towards KE in low-resource scenarios, and systematically categorize existing works into three paradigms: (1) exploiting higher-resource data, (2) exploiting stronger models, and (3) exploiting data and models together. In addition, we highlight promising applications and outline some potential directions for future research. We hope that our survey can help both the academic and industrial communities to better understand this field, inspire more ideas, and boost broader applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22269;&#23478;&#26377;&#32447;&#30005;&#35270;&#26032;&#38395;&#23545;&#32654;&#22269;&#26412;&#22303;&#25253;&#32440;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24403;&#22320;&#25253;&#32440;&#30340;&#20869;&#23481;&#20250;&#22240;&#20026;&#24403;&#22320; FNC &#35266;&#20247;&#25968;&#37327;&#30340;&#22686;&#21152;&#32780;&#36235;&#21521;&#20110; FNC &#30340;&#20542;&#21521;&#65292;&#24182;&#19988;&#26377;&#32447;&#30005;&#35270;&#20542;&#21521;&#20250;&#26497;&#21270;&#22320;&#26041;&#26032;&#38395;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2202.07269</link><description>&lt;p&gt;
&#23186;&#20307;&#20542;&#21521;&#26159;&#20855;&#26377;&#20256;&#26579;&#24615;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Media Slant is Contagious. (arXiv:2202.07269v2 [econ.GN] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.07269
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22269;&#23478;&#26377;&#32447;&#30005;&#35270;&#26032;&#38395;&#23545;&#32654;&#22269;&#26412;&#22303;&#25253;&#32440;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24403;&#22320;&#25253;&#32440;&#30340;&#20869;&#23481;&#20250;&#22240;&#20026;&#24403;&#22320; FNC &#35266;&#20247;&#25968;&#37327;&#30340;&#22686;&#21152;&#32780;&#36235;&#21521;&#20110; FNC &#30340;&#20542;&#21521;&#65292;&#24182;&#19988;&#26377;&#32447;&#30005;&#35270;&#20542;&#21521;&#20250;&#26497;&#21270;&#22320;&#26041;&#26032;&#38395;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#23519;&#20102;&#23186;&#20307;&#20542;&#21521;&#30340;&#20256;&#25773;&#65292;&#20855;&#20307;&#26469;&#35828;&#26159;&#22269;&#23478;&#26377;&#32447;&#30005;&#35270;&#26032;&#38395;&#23545;&#32654;&#22269;&#26412;&#22303;&#25253;&#32440;&#65288;2005-2008&#65289;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#22522;&#20110; Fox News Channel&#65288;FNC&#65289;&#12289;CNN &#21644; MSNBC &#20869;&#23481;&#30340;&#26377;&#32447;&#30005;&#35270;&#20542;&#21521;&#25991;&#26412;&#24230;&#37327;&#26041;&#27861;&#65292;&#20998;&#26512;&#22320;&#26041;&#25253;&#32440;&#22914;&#20309;&#37319;&#29992; FNC &#30340;&#20542;&#21521;&#32780;&#19981;&#26159; CNN/MSNBC &#30340;&#20542;&#21521;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#22320;&#26041;&#26032;&#38395;&#38543;&#30528;&#24403;&#22320; FNC &#35266;&#20247;&#20154;&#25968;&#30340;&#22806;&#37096;&#22686;&#38271;&#32780;&#21464;&#24471;&#26356;&#21152;&#31867;&#20284;&#20110; FNC &#30340;&#20869;&#23481;&#12290;&#36825;&#31181;&#36716;&#21464;&#19981;&#20165;&#38480;&#20110;&#20174;&#26377;&#32447;&#30005;&#35270;&#20511;&#37492;&#65292;&#32780;&#26159;&#22320;&#26041;&#25253;&#32440;&#33258;&#36523;&#20869;&#23481;&#30340;&#25913;&#21464;&#12290;&#27492;&#22806;&#65292;&#26377;&#32447;&#30005;&#35270;&#20542;&#21521;&#26497;&#21270;&#20102;&#22320;&#26041;&#26032;&#38395;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
We examine the diffusion of media slant, specifically how partisan content from national cable news affects local newspapers in the U.S., 2005-2008. We use a text-based measure of cable news slant trained on content from Fox News Channel (FNC), CNN, and MSNBC to analyze how local newspapers adopt FNC's slant over CNN/MSNBC's. Our findings show that local news becomes more similar to FNC content in response to an exogenous increase in local FNC viewership. This shift is not limited to borrowing from cable news, but rather, local newspapers' own content changes. Further, cable TV slant polarizes local news content.
&lt;/p&gt;</description></item></channel></rss>