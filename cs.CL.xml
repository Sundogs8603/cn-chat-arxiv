<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#20351;&#29992;&#26368;&#26032;&#21457;&#24067;&#30340;Gemma&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;LLaVA&#26694;&#26550;&#20013;&#35757;&#32451;&#20102;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#36830;&#25509;&#22120;&#12289;&#26356;&#24378;&#22823;&#30340;&#22270;&#20687;&#20027;&#24178;&#21644;&#22686;&#21152;&#35821;&#35328;&#20027;&#24178;&#22823;&#23567;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2404.01331</link><description>&lt;p&gt;
LLaVA-Gemma&#65306;&#21033;&#29992;&#32039;&#20945;&#30340;&#35821;&#35328;&#27169;&#22411;&#21152;&#36895;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LLaVA-Gemma: Accelerating Multimodal Foundation Models with a Compact Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01331
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26368;&#26032;&#21457;&#24067;&#30340;Gemma&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;LLaVA&#26694;&#26550;&#20013;&#35757;&#32451;&#20102;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#36830;&#25509;&#22120;&#12289;&#26356;&#24378;&#22823;&#30340;&#22270;&#20687;&#20027;&#24178;&#21644;&#22686;&#21152;&#35821;&#35328;&#20027;&#24178;&#22823;&#23567;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#26368;&#26032;&#21457;&#24067;&#30340;Gemma&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#27969;&#34892;&#30340;LLaVA&#26694;&#26550;&#20013;&#35757;&#32451;&#19968;&#31995;&#21015;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65288;MMFM&#65289;&#12290;&#29305;&#21035;&#20540;&#24471;&#20851;&#27880;&#30340;&#26159;2B&#21442;&#25968;&#30340;Gemma&#27169;&#22411;&#65292;&#23427;&#25552;&#20379;&#20102;&#26500;&#24314;&#21151;&#33021;&#24378;&#22823;&#30340;&#23567;&#35268;&#27169;MMFM&#30340;&#26426;&#20250;&#12290;&#19982;&#35813;&#39046;&#22495;&#20854;&#20182;&#35770;&#25991;&#30340;&#21457;&#29616;&#19968;&#33268;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#21435;&#38500;&#19977;&#31181;&#35774;&#35745;&#29305;&#24615;&#30340;&#24433;&#21709;&#65306;&#39044;&#35757;&#32451;&#36830;&#25509;&#22120;&#65292;&#21033;&#29992;&#26356;&#24378;&#22823;&#30340;&#22270;&#20687;&#20027;&#24178;&#65292;&#22686;&#21152;&#35821;&#35328;&#20027;&#24178;&#30340;&#22823;&#23567;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;LLaVA-Gemma&#30340;&#32467;&#26524;&#27169;&#22411;&#22312;&#19968;&#31995;&#21015;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#20013;&#31561;&#24615;&#33021;&#65292;&#20294;&#26410;&#33021;&#36229;&#36234;&#24403;&#21069;&#30456;&#23545;&#22823;&#23567;&#30340;SOTA&#27169;&#22411;&#12290;&#24615;&#33021;&#30340;&#26356;&#35814;&#32454;&#20998;&#26512;&#26174;&#31034;&#20986;&#19981;&#21516;&#30340;&#25928;&#26524;&#65306;&#36339;&#36807;&#39044;&#35757;&#32451;&#24448;&#24448;&#20250;&#38477;&#20302;&#24615;&#33021;&#65292;&#26356;&#22823;&#30340;&#35270;&#35273;&#27169;&#22411;&#26377;&#26102;&#20250;&#25552;&#39640;&#24615;&#33021;&#65292;&#22686;&#21152;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#23567;&#25928;&#26524;&#19981;&#19968;&#33268;&#12290;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#20102;&#35757;&#32451;&#37197;&#26041;&#65292;&#20195;&#30721;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01331v1 Announce Type: cross  Abstract: We train a suite of multimodal foundation models (MMFM) using the popular LLaVA framework with the recently released Gemma family of large language models (LLMs). Of particular interest is the 2B parameter Gemma model, which provides opportunities to construct capable small-scale MMFMs. In line with findings from other papers in this space, we test the effect of ablating three design features: pretraining the connector, utilizing a more powerful image backbone, and increasing the size of the language backbone. The resulting models, which we call LLaVA-Gemma, exhibit moderate performance on an array of evaluations, but fail to improve past the current comparably sized SOTA models. Closer analysis of performance shows mixed effects; skipping pretraining tends to reduce performance, larger vision models sometimes improve performance, and increasing language model size has inconsistent effects. We publicly release training recipes, code an
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19987;&#23478;&#20195;&#24065;&#36335;&#30001;&#23558;&#22810;&#20010;&#19987;&#23478;LLM&#21327;&#21516;&#20316;&#20026;&#36890;&#29992;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#22810;&#20010;&#19987;&#23478;LLMs&#30340;&#26080;&#32541;&#38598;&#25104;&#65292;&#25903;&#25345;&#38544;&#24335;&#19987;&#19994;&#30693;&#35782;&#30340;&#23398;&#20064;&#21644;&#21160;&#24577;&#25193;&#23637;&#26032;&#30340;&#19987;&#23478;LLMs&#65292;&#21516;&#26102;&#26356;&#22909;&#22320;&#38544;&#34255;&#21327;&#20316;&#32454;&#33410;&#65292;&#23637;&#29616;&#20986;&#27604;&#29616;&#26377;&#22810;LLM&#21327;&#20316;&#33539;&#24335;&#26356;&#22909;&#30340;&#25928;&#26524;&#21644;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.16854</link><description>&lt;p&gt;
&#19968;&#20010;&#19987;&#23478;&#20215;&#20540;&#19968;&#20010;&#20195;&#24065;&#65306;&#36890;&#36807;&#19987;&#23478;&#20195;&#24065;&#36335;&#30001;&#23558;&#22810;&#20010;&#19987;&#23478;LLM&#21327;&#21516;&#20316;&#20026;&#36890;&#29992;&#22411;
&lt;/p&gt;
&lt;p&gt;
An Expert is Worth One Token: Synergizing Multiple Expert LLMs as Generalist via Expert Token Routing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16854
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19987;&#23478;&#20195;&#24065;&#36335;&#30001;&#23558;&#22810;&#20010;&#19987;&#23478;LLM&#21327;&#21516;&#20316;&#20026;&#36890;&#29992;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#22810;&#20010;&#19987;&#23478;LLMs&#30340;&#26080;&#32541;&#38598;&#25104;&#65292;&#25903;&#25345;&#38544;&#24335;&#19987;&#19994;&#30693;&#35782;&#30340;&#23398;&#20064;&#21644;&#21160;&#24577;&#25193;&#23637;&#26032;&#30340;&#19987;&#23478;LLMs&#65292;&#21516;&#26102;&#26356;&#22909;&#22320;&#38544;&#34255;&#21327;&#20316;&#32454;&#33410;&#65292;&#23637;&#29616;&#20986;&#27604;&#29616;&#26377;&#22810;LLM&#21327;&#20316;&#33539;&#24335;&#26356;&#22909;&#30340;&#25928;&#26524;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19987;&#23478;&#20195;&#24065;&#36335;&#30001;&#65288;Expert-Token-Routing&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#36890;&#29992;&#22411;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#22810;&#20010;&#19987;&#23478;LLM&#30340;&#26080;&#32541;&#38598;&#25104;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#19987;&#23478;LLMs&#34920;&#31034;&#20026;&#20803;LLM&#35789;&#27719;&#20013;&#30340;&#29305;&#27530;&#19987;&#23478;&#20195;&#24065;&#12290;&#20803;LLM&#21487;&#20197;&#36335;&#30001;&#21040;&#19987;&#23478;LLM&#65292;&#23601;&#20687;&#29983;&#25104;&#26032;&#20195;&#24065;&#19968;&#26679;&#12290;&#19987;&#23478;&#20195;&#24065;&#36335;&#30001;&#19981;&#20165;&#21487;&#20197;&#20174;&#29616;&#26377;&#30340;&#25351;&#23548;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#19987;&#23478;LLMs&#30340;&#38544;&#24335;&#19987;&#19994;&#30693;&#35782;&#65292;&#36824;&#21487;&#20197;&#20197;&#21363;&#25554;&#21363;&#29992;&#30340;&#26041;&#24335;&#21160;&#24577;&#25193;&#23637;&#26032;&#30340;&#19987;&#23478;LLMs&#12290;&#23427;&#36824;&#21487;&#20197;&#38544;&#34255;&#29992;&#25143;&#35270;&#35282;&#20013;&#30340;&#35814;&#32454;&#21327;&#20316;&#36807;&#31243;&#65292;&#20419;&#36827;&#20132;&#20114;&#23601;&#20687;&#26159;&#19968;&#20010;&#21333;&#19968;&#30340;LLM&#19968;&#26679;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#28085;&#30422;&#20845;&#20010;&#19981;&#21516;&#19987;&#23478;&#39046;&#22495;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#32988;&#36807;&#20102;&#21508;&#31181;&#29616;&#26377;&#30340;&#22810;LLM&#21327;&#20316;&#33539;&#24335;&#65292;&#23637;&#29616;&#20102;&#36890;&#36807;&#21327;&#21516;&#22810;&#20010;&#19987;&#23478;LLM&#26469;&#26500;&#24314;&#36890;&#29992;&#22411;LLM&#31995;&#32479;&#30340;&#25928;&#26524;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16854v1 Announce Type: cross  Abstract: We present Expert-Token-Routing, a unified generalist framework that facilitates seamless integration of multiple expert LLMs. Our framework represents expert LLMs as special expert tokens within the vocabulary of a meta LLM. The meta LLM can route to an expert LLM like generating new tokens. Expert-Token-Routing not only supports learning the implicit expertise of expert LLMs from existing instruction dataset but also allows for dynamic extension of new expert LLMs in a plug-and-play manner. It also conceals the detailed collaboration process from the user's perspective, facilitating interaction as though it were a singular LLM. Our framework outperforms various existing multi-LLM collaboration paradigms across benchmarks that incorporate six diverse expert domains, demonstrating effectiveness and robustness in building generalist LLM system via synergizing multiple expert LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ProCoder&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#32534;&#35793;&#22120;&#21453;&#39304;&#24341;&#23548;&#65292;&#36845;&#20195;&#22320;&#25913;&#36827;&#39033;&#30446;&#32423;&#20195;&#30721;&#19978;&#19979;&#25991;&#65292;&#20197;&#33719;&#24471;&#31934;&#30830;&#30340;&#20195;&#30721;&#29983;&#25104;</title><link>https://arxiv.org/abs/2403.16792</link><description>&lt;p&gt;
&#36890;&#36807;&#32534;&#35793;&#22120;&#21453;&#39304;&#36845;&#20195;&#25913;&#36827;&#39033;&#30446;&#32423;&#20195;&#30721;&#19978;&#19979;&#25991;&#65292;&#20197;&#33719;&#24471;&#31934;&#30830;&#30340;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Iterative Refinement of Project-Level Code Context for Precise Code Generation with Compiler Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ProCoder&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#32534;&#35793;&#22120;&#21453;&#39304;&#24341;&#23548;&#65292;&#36845;&#20195;&#22320;&#25913;&#36827;&#39033;&#30446;&#32423;&#20195;&#30721;&#19978;&#19979;&#25991;&#65292;&#20197;&#33719;&#24471;&#31934;&#30830;&#30340;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#33258;&#21160;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23558;&#22522;&#20110;LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#24212;&#29992;&#21040;&#29616;&#23454;&#39033;&#30446;&#20013;&#20250;&#38754;&#20020;&#25361;&#25112;&#65292;&#22240;&#20026;&#29983;&#25104;&#30340;&#20195;&#30721;&#21487;&#33021;&#23384;&#22312;API&#20351;&#29992;&#12289;&#31867;&#12289;&#25968;&#25454;&#32467;&#26500;&#38169;&#35823;&#25110;&#32570;&#23569;&#39033;&#30446;&#29305;&#23450;&#20449;&#24687;&#12290;&#37492;&#20110;&#22823;&#37096;&#20998;&#39033;&#30446;&#29305;&#23450;&#19978;&#19979;&#25991;&#26080;&#27861;&#36866;&#24212;LLMs&#30340;&#25552;&#31034;&#65292;&#25105;&#20204;&#24517;&#39035;&#25214;&#21040;&#35753;&#27169;&#22411;&#33021;&#22815;&#25506;&#32034;&#39033;&#30446;&#32423;&#20195;&#30721;&#19978;&#19979;&#25991;&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ProCoder&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#32534;&#35793;&#22120;&#21453;&#39304;&#24341;&#23548;&#65292;&#36845;&#20195;&#22320;&#25913;&#36827;&#39033;&#30446;&#32423;&#20195;&#30721;&#19978;&#19979;&#25991;&#65292;&#20197;&#33719;&#24471;&#31934;&#30830;&#30340;&#20195;&#30721;&#29983;&#25104;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ProCoder&#39318;&#20808;&#21033;&#29992;&#32534;&#35793;&#22120;&#25216;&#26415;&#35782;&#21035;&#29983;&#25104;&#30340;&#20195;&#30721;&#19982;&#39033;&#30446;&#19978;&#19979;&#25991;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#20043;&#22788;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#20174;&#20195;&#30721;&#24211;&#20013;&#25552;&#21462;&#30340;&#20449;&#24687;&#36845;&#20195;&#22320;&#23545;&#40784;&#21644;&#20462;&#22797;&#35782;&#21035;&#20986;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#23558;ProCoder&#19982;&#20004;&#20010;&#20195;&#34920;&#24615;&#30340;LLM&#38598;&#25104;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16792v1 Announce Type: new  Abstract: Large language models (LLMs) have shown remarkable progress in automated code generation. Yet, incorporating LLM-based code generation into real-life software projects poses challenges, as the generated code may contain errors in API usage, class, data structure, or missing project-specific information. As much of this project-specific context cannot fit into the prompts of LLMs, we must find ways to allow the model to explore the project-level code context. To this end, this paper puts forward a novel approach, termed ProCoder, which iteratively refines the project-level code context for precise code generation, guided by the compiler feedback. In particular, ProCoder first leverages compiler techniques to identify a mismatch between the generated code and the project's context. It then iteratively aligns and fixes the identified errors using information extracted from the code repository. We integrate ProCoder with two representative L
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22270;&#31639;&#27861;&#21152;&#36895;&#31185;&#23398;&#21457;&#29616;&#65292;&#25581;&#31034;&#35770;&#25991;&#20043;&#38388;&#30340;&#28145;&#20837;&#36328;&#23398;&#31185;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#26448;&#26009;&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2403.11996</link><description>&lt;p&gt;
&#21033;&#29992;&#29983;&#25104;&#24335;&#30693;&#35782;&#25552;&#21462;&#12289;&#22522;&#20110;&#22270;&#30340;&#34920;&#31034;&#21644;&#22810;&#27169;&#24577;&#26234;&#33021;&#22270;&#25512;&#29702;&#21152;&#36895;&#31185;&#23398;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Accelerating Scientific Discovery with Generative Knowledge Extraction, Graph-Based Representation, and Multimodal Intelligent Graph Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11996
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22270;&#31639;&#27861;&#21152;&#36895;&#31185;&#23398;&#21457;&#29616;&#65292;&#25581;&#31034;&#35770;&#25991;&#20043;&#38388;&#30340;&#28145;&#20837;&#36328;&#23398;&#31185;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#26448;&#26009;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65292;&#25105;&#20204;&#23558;&#19968;&#32452;&#28041;&#21450;&#29983;&#29289;&#26448;&#26009;&#39046;&#22495;&#30340;1,000&#31687;&#31185;&#23398;&#35770;&#25991;&#36716;&#21270;&#20026;&#35814;&#32454;&#30340;&#26412;&#20307;&#30693;&#35782;&#22270;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22266;&#26377;&#30340;&#26080;&#26631;&#24230;&#29305;&#24615;&#12290;&#36890;&#36807;&#22522;&#20110;&#33410;&#28857;&#30456;&#20284;&#24615;&#21644;&#20171;&#25968;&#20013;&#24515;&#24615;&#30340;&#32452;&#21512;&#25490;&#21517;&#65292;&#25506;&#27979;&#19981;&#21516;&#27010;&#24565;&#20043;&#38388;&#30340;&#22270;&#36941;&#21382;&#36335;&#24452;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#28145;&#20837;&#30340;&#36328;&#23398;&#31185;&#20851;&#31995;&#65292;&#21487;&#29992;&#20110;&#22238;&#31572;&#26597;&#35810;&#65292;&#35782;&#21035;&#30693;&#35782;&#20013;&#30340;&#31354;&#30333;&#65292;&#24182;&#25552;&#20986;&#21069;&#25152;&#26410;&#35265;&#30340;&#26448;&#26009;&#35774;&#35745;&#21450;&#20854;&#34892;&#20026;&#12290;&#19968;&#39033;&#27604;&#36739;&#25581;&#31034;&#20102;&#29983;&#29289;&#26448;&#26009;&#21644;&#36125;&#22810;&#33452;&#31532;&#20061;&#20132;&#21709;&#26354;&#20043;&#38388;&#30340;&#35814;&#32454;&#32467;&#26500;&#30456;&#20284;&#20043;&#22788;&#65292;&#31361;&#26174;&#20102;&#36890;&#36807;&#21516;&#26500;&#26144;&#23556;&#20849;&#20139;&#22797;&#26434;&#24615;&#27169;&#24335;&#12290;&#35813;&#31639;&#27861;&#36827;&#19968;&#27493;&#21019;&#24314;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#20998;&#32423;&#33740;&#19997;&#20307;&#30340;&#22797;&#21512;&#26448;&#26009;&#65292;&#23558;&#22270;&#37319;&#26679;&#30340;&#32852;&#21512;&#21512;&#25104;&#21407;&#29702;&#19982;&#24247;&#23450;&#26031;&#22522;&#12298;&#31532;&#19971;&#32452;&#25104;&#12299;&#20013;&#25552;&#21462;&#30340;&#21407;&#21017;&#30456;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11996v1 Announce Type: cross  Abstract: Using generative Artificial Intelligence (AI), we transformed a set of 1,000 scientific papers in the area of biological materials into detailed ontological knowledge graphs, revealing their inherently scale-free nature. Using graph traversal path detection between dissimilar concepts based on combinatorial ranking of node similarity and betweenness centrality, we reveal deep insights into unprecedented interdisciplinary relationships that can be used to answer queries, identify gaps in knowledge, and propose never-before-seen material designs and their behaviors. One comparison revealed detailed structural parallels between biological materials and Beethoven's 9th Symphony, highlighting shared patterns of complexity through isomorphic mapping. The algorithm further created an innovative hierarchical mycelium-based composite that incorporates joint synthesis of graph sampling with principles extracted from Kandinsky's Composition VII p
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30495;&#30456;&#24863;&#30693;&#30340;&#19978;&#19979;&#25991;&#36873;&#25321;&#65288;TACS&#65289;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#23545;&#36755;&#20837;&#19978;&#19979;&#25991;&#36827;&#34892;&#30495;&#30456;&#26816;&#27979;&#24182;&#26500;&#24314;&#30456;&#24212;&#30340;&#27880;&#24847;&#21147;&#33945;&#29256;&#26469;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#19981;&#30495;&#23454;&#19978;&#19979;&#25991;&#35823;&#23548;&#20135;&#29983;&#24187;&#35273;</title><link>https://arxiv.org/abs/2403.07556</link><description>&lt;p&gt;
&#30495;&#30456;&#24863;&#30693;&#30340;&#19978;&#19979;&#25991;&#36873;&#25321;&#65306;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#19981;&#30495;&#23454;&#19978;&#19979;&#25991;&#35823;&#23548;&#20135;&#29983;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Truth-Aware Context Selection: Mitigating the Hallucinations of Large Language Models Being Misled by Untruthful Contexts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07556
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30495;&#30456;&#24863;&#30693;&#30340;&#19978;&#19979;&#25991;&#36873;&#25321;&#65288;TACS&#65289;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#23545;&#36755;&#20837;&#19978;&#19979;&#25991;&#36827;&#34892;&#30495;&#30456;&#26816;&#27979;&#24182;&#26500;&#24314;&#30456;&#24212;&#30340;&#27880;&#24847;&#21147;&#33945;&#29256;&#26469;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#19981;&#30495;&#23454;&#19978;&#19979;&#25991;&#35823;&#23548;&#20135;&#29983;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#24456;&#23481;&#26131;&#34987;&#29992;&#25143;&#25110;&#30693;&#35782;&#35770;&#35777;&#24037;&#20855;&#25552;&#20379;&#30340;&#19981;&#30495;&#23454;&#19978;&#19979;&#25991;&#35823;&#23548;&#65292;&#20174;&#32780;&#20135;&#29983;&#24187;&#35273;&#12290;&#20026;&#20102;&#20943;&#36731;LLMs&#34987;&#19981;&#30495;&#23454;&#20449;&#24687;&#35823;&#23548;&#24182;&#21033;&#29992;&#30693;&#35782;&#35770;&#35777;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30495;&#30456;&#24863;&#30693;&#30340;&#19978;&#19979;&#25991;&#36873;&#25321;&#65288;TACS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#36755;&#20837;&#20013;&#23631;&#34109;&#19981;&#30495;&#23454;&#30340;&#19978;&#19979;&#25991;&#12290;TACS&#39318;&#20808;&#23545;&#36755;&#20837;&#19978;&#19979;&#25991;&#36827;&#34892;&#30495;&#30456;&#26816;&#27979;&#65292;&#21033;&#29992;LLM&#20869;&#30340;&#21442;&#25968;&#21270;&#30693;&#35782;&#12290;&#38543;&#21518;&#65292;&#26681;&#25454;&#27599;&#20010;&#20301;&#32622;&#30340;&#30495;&#23454;&#24615;&#26500;&#24314;&#30456;&#24212;&#30340;&#27880;&#24847;&#21147;&#33945;&#29256;&#65292;&#36873;&#25321;&#30495;&#23454;&#30340;&#19978;&#19979;&#25991;&#24182;&#20002;&#24323;&#19981;&#30495;&#23454;&#30340;&#19978;&#19979;&#25991;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#25200;&#21160;&#36866;&#24212;&#29575;&#65292;&#20197;&#36827;&#19968;&#27493;&#30740;&#31350;LLMs&#25509;&#21463;&#30495;&#23454;&#20449;&#24687;&#21644;&#25269;&#21046;&#19981;&#30495;&#23454;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07556v1 Announce Type: new  Abstract: Although large language models (LLMs) have demonstrated impressive text generation capabilities, they are easily misled by the untruthful context provided by users or knowledge argumentation tools, thereby producing hallucinations. To alleviate the LLMs from being misled by untruthful information and take advantage of knowledge argumentation, we propose Truth-Aware Context Selection (TACS), a lightweight method to shield untruthful context from the inputs. TACS begins by performing truth detection on the input context, leveraging the parameterized knowledge within the LLM. Subsequently, it constructs a corresponding attention mask based on the truthfulness of each position, selecting the truthful context and discarding the untruthful context. Additionally, we introduce a new evaluation metric, Disturbance Adaption Rate, to further study the LLMs' ability to accept truthful information and resist untruthful information. Experimental resul
&lt;/p&gt;</description></item><item><title>&#31616;&#21333;&#30340;&#22522;&#20110;guardrail&#30340;&#26041;&#27861;&#22914;&#25552;&#31034;&#21644;&#36807;&#28388;&#21487;&#20197;&#23454;&#29616;&#19982;fine-tuning&#30456;&#23218;&#32654;&#30340;unlearning&#32467;&#26524;&#65292;&#24314;&#35758;&#30740;&#31350;&#20154;&#21592;&#22312;&#35780;&#20272;&#26356;&#28040;&#32791;&#35745;&#31639;&#36164;&#28304;&#30340;fine-tuning&#26041;&#27861;&#26102;&#32771;&#34385;&#36825;&#20123;&#36731;&#37327;&#32423;&#22522;&#32447;&#12290;</title><link>https://arxiv.org/abs/2403.03329</link><description>&lt;p&gt;
Guardrail Baselines for Unlearning in LLMs
&lt;/p&gt;
&lt;p&gt;
Guardrail Baselines for Unlearning in LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03329
&lt;/p&gt;
&lt;p&gt;
&#31616;&#21333;&#30340;&#22522;&#20110;guardrail&#30340;&#26041;&#27861;&#22914;&#25552;&#31034;&#21644;&#36807;&#28388;&#21487;&#20197;&#23454;&#29616;&#19982;fine-tuning&#30456;&#23218;&#32654;&#30340;unlearning&#32467;&#26524;&#65292;&#24314;&#35758;&#30740;&#31350;&#20154;&#21592;&#22312;&#35780;&#20272;&#26356;&#28040;&#32791;&#35745;&#31639;&#36164;&#28304;&#30340;fine-tuning&#26041;&#27861;&#26102;&#32771;&#34385;&#36825;&#20123;&#36731;&#37327;&#32423;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;fine-tuning&#26159;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#8220;unlearn&#8221;&#27010;&#24565;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;fine-tuning&#21487;&#33021;&#24456;&#26114;&#36149;&#65292;&#22240;&#20026;&#23427;&#26082;&#38656;&#35201;&#29983;&#25104;&#19968;&#32452;&#31034;&#20363;&#65292;&#21448;&#38656;&#35201;&#36816;&#34892;&#22810;&#27425;&#36845;&#20195;&#30340;fine-tuning&#26469;&#26356;&#26032;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31616;&#21333;&#30340;&#22522;&#20110;guardrail&#30340;&#26041;&#27861;&#65292;&#22914;&#25552;&#31034;&#21644;&#36807;&#28388;&#65292;&#21487;&#20197;&#23454;&#29616;&#19982;fine-tuning&#30456;&#23218;&#32654;&#30340;unlearning&#32467;&#26524;&#12290;&#25105;&#20204;&#24314;&#35758;&#30740;&#31350;&#20154;&#21592;&#22312;&#35780;&#20272;&#26356;&#28040;&#32791;&#35745;&#31639;&#36164;&#28304;&#30340;fine-tuning&#26041;&#27861;&#30340;&#24615;&#33021;&#26102;&#65292;&#35843;&#26597;&#36825;&#20123;&#36731;&#37327;&#32423;&#22522;&#32447;&#12290;&#34429;&#28982;&#25105;&#20204;&#24182;&#19981;&#22768;&#31216;&#25552;&#31034;&#25110;&#36807;&#28388;&#31561;&#26041;&#27861;&#26159;unlearning&#38382;&#39064;&#30340;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#38656;&#35201;&#26356;&#22909;&#22320;&#21306;&#20998;guardrails&#19982;fine-tuning&#30340;&#24378;&#22823;&#20043;&#22788;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#24378;&#35843;guardrails&#26412;&#36523;&#21487;&#33021;&#20026;unlearning&#20855;&#26377;&#20248;&#21183;&#30340;&#22330;&#26223;&#65292;&#20363;&#22914;&#29983;&#25104;&#31034;&#20363;&#29992;&#20110;fine-tuning&#25110;u
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03329v1 Announce Type: new  Abstract: Recent work has demonstrated that fine-tuning is a promising approach to `unlearn' concepts from large language models. However, fine-tuning can be expensive, as it requires both generating a set of examples and running iterations of fine-tuning to update the model. In this work, we show that simple guardrail-based approaches such as prompting and filtering can achieve unlearning results comparable to fine-tuning. We recommend that researchers investigate these lightweight baselines when evaluating the performance of more computationally intensive fine-tuning methods. While we do not claim that methods such as prompting or filtering are universal solutions to the problem of unlearning, our work suggests the need for evaluation metrics that can better separate the power of guardrails vs. fine-tuning, and highlights scenarios where guardrails themselves may be advantageous for unlearning, such as in generating examples for fine-tuning or u
&lt;/p&gt;</description></item><item><title>BlendSQL&#26159;&#19968;&#20010;&#36229;&#38598;&#30340;SQLite&#65292;&#29992;&#20110;&#32479;&#19968;&#28151;&#21512;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#38750;&#32467;&#26500;&#21270;&#21644;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#21253;&#21547;&#22810;&#36339;&#25512;&#29702;&#30340;&#20219;&#21153;&#65292;&#24182;&#19988;&#22312;&#20351;&#29992;&#26356;&#23569;&#20196;&#29260;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.17882</link><description>&lt;p&gt;
BlendSQL&#65306;&#29992;&#20110;&#32479;&#19968;&#28151;&#21512;&#38382;&#39064;&#22238;&#31572;&#30340;&#21487;&#25193;&#23637;&#26041;&#35328;&#22312;&#20851;&#31995;&#20195;&#25968;&#20013;
&lt;/p&gt;
&lt;p&gt;
BlendSQL: A Scalable Dialect for Unifying Hybrid Question Answering in Relational Algebra
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17882
&lt;/p&gt;
&lt;p&gt;
BlendSQL&#26159;&#19968;&#20010;&#36229;&#38598;&#30340;SQLite&#65292;&#29992;&#20110;&#32479;&#19968;&#28151;&#21512;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#38750;&#32467;&#26500;&#21270;&#21644;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#21253;&#21547;&#22810;&#36339;&#25512;&#29702;&#30340;&#20219;&#21153;&#65292;&#24182;&#19988;&#22312;&#20351;&#29992;&#26356;&#23569;&#20196;&#29260;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#26377;&#31471;&#21040;&#31471;&#31995;&#32479;&#29992;&#20110;&#28151;&#21512;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#65292;&#24448;&#24448;&#21487;&#20197;&#24402;&#32467;&#20026;&#8220;&#25552;&#31034;-&#31048;&#31095;&#8221;&#33539;&#24335;&#65292;&#29992;&#25143;&#23545;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#30340;&#25511;&#21046;&#21644;&#27934;&#23519;&#21463;&#38480;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#35768;&#22810;&#22522;&#20110;Transformer&#30340;LLM&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#22823;&#23567;&#38480;&#21046;&#65292;&#24448;&#24448;&#19981;&#21512;&#29702;&#26399;&#26395;&#23436;&#25972;&#30340;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#19978;&#19979;&#25991;&#36866;&#21512;&#20110;&#32473;&#23450;&#25552;&#31034;&#22312;&#38646;&#27425;&#31034;&#33539;&#29615;&#22659;&#20013;&#65292;&#26356;&#19981;&#29992;&#35828;&#20960;&#27425;&#25552;&#31034;&#29615;&#22659;&#20013;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;BlendSQL&#65292;&#23427;&#26159;SQLite&#30340;&#19968;&#20010;&#36229;&#38598;&#65292;&#29992;&#20316;&#32479;&#19968;&#30340;&#26041;&#35328;&#65292;&#29992;&#20110;&#22312;&#38750;&#32467;&#26500;&#21270;&#21644;&#32467;&#26500;&#21270;&#25968;&#25454;&#20043;&#38388;&#32534;&#25490;&#25512;&#29702;&#12290;&#23545;&#20110;&#28041;&#21450;&#22810;&#36339;&#25512;&#29702;&#30340;&#28151;&#21512;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#65292;&#25105;&#20204;&#23558;&#23436;&#25972;&#30340;&#20998;&#35299;&#25512;&#29702;&#36335;&#32447;&#22270;&#32534;&#30721;&#20026;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;BlendSQL&#26597;&#35810;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;BlendSQL&#21487;&#20197;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#20351;&#29992;35%&#26356;&#23569;&#20196;&#29260;&#30340;&#24773;&#20917;&#19979;&#25913;&#21892;&#31471;&#21040;&#31471;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17882v1 Announce Type: new  Abstract: Many existing end-to-end systems for hybrid question answering tasks can often be boiled down to a "prompt-and-pray" paradigm, where the user has limited control and insight into the intermediate reasoning steps used to achieve the final result. Additionally, due to the context size limitation of many transformer-based LLMs, it is often not reasonable to expect that the full structured and unstructured context will fit into a given prompt in a zero-shot setting, let alone a few-shot setting. We introduce BlendSQL, a superset of SQLite to act as a unified dialect for orchestrating reasoning across both unstructured and structured data. For hybrid question answering tasks involving multi-hop reasoning, we encode the full decomposed reasoning roadmap into a single interpretable BlendSQL query. Notably, we show that BlendSQL can scale to massive datasets and improve the performance of end-to-end systems while using 35% fewer tokens. Our code
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#39318;&#27425;&#28145;&#20837;&#30740;&#31350;&#20102;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#23450;&#20301;&#65292;&#38024;&#23545;&#25991;&#26723;&#20013;&#26426;&#22120;&#29983;&#25104;&#37096;&#20998;&#30340;&#23450;&#20301;&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#30340;&#26041;&#27861;&#25552;&#20986;&#20102;&#35299;&#20915;&#25972;&#20010;&#25991;&#26723;MGT&#26816;&#27979;&#22833;&#36133;&#24773;&#20917;&#30340;&#26032;&#36884;&#24452;&#12290;</title><link>https://arxiv.org/abs/2402.11744</link><description>&lt;p&gt;
&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Machine-generated Text Localization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11744
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#39318;&#27425;&#28145;&#20837;&#30740;&#31350;&#20102;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#23450;&#20301;&#65292;&#38024;&#23545;&#25991;&#26723;&#20013;&#26426;&#22120;&#29983;&#25104;&#37096;&#20998;&#30340;&#23450;&#20301;&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#30340;&#26041;&#27861;&#25552;&#20986;&#20102;&#35299;&#20915;&#25972;&#20010;&#25991;&#26723;MGT&#26816;&#27979;&#22833;&#36133;&#24773;&#20917;&#30340;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#65288;MGT&#65289;&#26816;&#27979;&#26088;&#22312;&#35782;&#21035;&#19968;&#27573;&#25991;&#26412;&#26159;&#26426;&#22120;&#20889;&#20316;&#36824;&#26159;&#20154;&#31867;&#20889;&#20316;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#23558;MGT&#26500;&#24314;&#20026;&#23545;&#25972;&#20010;&#25991;&#26723;&#30340;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#65292;&#23545;&#25991;&#26723;&#20013;&#20165;&#37096;&#20998;&#20869;&#23481;&#20026;&#26426;&#22120;&#29983;&#25104;&#30340;&#24773;&#20917;&#36827;&#34892;&#30340;&#30740;&#31350;&#26377;&#38480;&#12290;&#26412;&#25991;&#39318;&#27425;&#28145;&#20837;&#30740;&#31350;&#20102;&#23450;&#20301;&#25991;&#26723;&#20013;&#26426;&#22120;&#29983;&#25104;&#37096;&#20998;&#30340;MGT&#12290;&#22240;&#27492;&#65292;&#22914;&#26524;&#24694;&#24847;&#34892;&#20026;&#32773;&#26356;&#25913;&#26032;&#38395;&#25991;&#31456;&#30340;&#20851;&#38190;&#37096;&#20998;&#20197;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#65292;&#25972;&#20010;&#25991;&#26723;&#30340;MGT&#26816;&#27979;&#21487;&#33021;&#20250;&#22833;&#36133;&#65292;&#22240;&#20026;&#32477;&#22823;&#37096;&#20998;&#26159;&#20154;&#31867;&#20889;&#20316;&#65292;&#20294;&#25105;&#20204;&#30340;&#26041;&#27861;&#30001;&#20110;&#20854;&#32454;&#31890;&#24230;&#30340;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#12290;&#25105;&#20204;&#30340;MGT&#23450;&#20301;&#20219;&#21153;&#38754;&#20020;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#65292;&#30701;&#36328;&#24230;&#30340;&#25991;&#26412;&#65292;&#20363;&#22914;&#19968;&#20010;&#21477;&#23376;&#65292;&#30001;&#20110;&#38271;&#24230;&#36739;&#30701;&#20960;&#20046;&#19981;&#25552;&#20379;&#25351;&#31034;&#20854;&#26159;&#21542;&#26426;&#22120;&#29983;&#25104;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#39044;&#27979;&#22810;&#20010;&#21477;&#23376;&#26159;&#26426;&#22120;&#29983;&#25104;&#36824;&#26159;&#20154;&#31867;&#20889;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11744v1 Announce Type: new  Abstract: Machine-Generated Text (MGT) detection aims to identify a piece of text as machine or human written. Prior work has primarily formulated MGT as a binary classification task over an entire document, with limited work exploring cases where only part of a document is machine generated. This paper provides the first in-depth study of MGT that localizes the portions of a document that were machine generated. Thus, if a bad actor were to change a key portion of a news article to spread misinformation, whole document MGT detection may fail since the vast majority is human written, but our approach can succeed due to its granular approach. A key challenge in our MGT localization task is that short spans of text, e.g., a single sentence, provides little information indicating if it is machine generated due to its short length. To address this, we leverage contextual information, where we predict whether multiple sentences are machine or human wri
&lt;/p&gt;</description></item><item><title>LLM&#20309;&#26102;&#38656;&#35201;&#26816;&#32034;&#22686;&#24378;&#65311;&#20943;&#36731;LLM&#30340;&#36807;&#24230;&#33258;&#20449;&#26377;&#21161;&#20110;&#26816;&#32034;&#22686;&#24378;</title><link>https://arxiv.org/abs/2402.11457</link><description>&lt;p&gt;
LLM&#20309;&#26102;&#38656;&#35201;&#26816;&#32034;&#22686;&#24378;&#65311;&#20943;&#36731;LLM&#30340;&#36807;&#24230;&#33258;&#20449;&#26377;&#21161;&#20110;&#26816;&#32034;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
When Do LLMs Need Retrieval Augmentation? Mitigating LLMs' Overconfidence Helps Retrieval Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11457
&lt;/p&gt;
&lt;p&gt;
LLM&#20309;&#26102;&#38656;&#35201;&#26816;&#32034;&#22686;&#24378;&#65311;&#20943;&#36731;LLM&#30340;&#36807;&#24230;&#33258;&#20449;&#26377;&#21161;&#20110;&#26816;&#32034;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#21457;&#29616;&#24456;&#38590;&#30693;&#36947;&#33258;&#24049;&#19981;&#20855;&#22791;&#26576;&#20123;&#30693;&#35782;&#65292;&#24182;&#19988;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#24448;&#24448;&#20250;&#25552;&#20379;&#34394;&#20551;&#31572;&#26696;&#12290;&#26816;&#32034;&#22686;&#24378;&#65288;RA&#65289;&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#20197;&#20943;&#36731;LLMs&#30340;&#24187;&#35273;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39069;&#22806;&#30340;&#24320;&#38144;&#21644;&#26816;&#32034;&#36136;&#37327;&#19981;&#30830;&#23450;&#65292;&#22987;&#32456;&#36827;&#34892;RA&#21487;&#33021;&#24182;&#19981;&#26159;&#26368;&#20339;&#36873;&#25321;&#12290;&#19968;&#20010;&#30452;&#35266;&#30340;&#24819;&#27861;&#26159;&#21482;&#26377;&#22312;LLMs&#23545;&#38382;&#39064;&#19981;&#30830;&#23450;&#26102;&#25165;&#36827;&#34892;&#26816;&#32034;&#12290;&#36825;&#28608;&#21457;&#25105;&#20204;&#22686;&#24378;LLMs&#24863;&#30693;&#30693;&#35782;&#36793;&#30028;&#30340;&#33021;&#21147;&#20197;&#24110;&#21161;RA&#12290;&#26412;&#25991;&#39318;&#20808;&#23450;&#37327;&#34913;&#37327;LLMs&#30340;&#36825;&#31181;&#33021;&#21147;&#24182;&#30830;&#35748;&#23427;&#20204;&#30340;&#36807;&#24230;&#33258;&#20449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;LLMs&#23545;&#38382;&#39064;&#30340;&#30830;&#23450;&#24615;&#22914;&#20309;&#19982;&#20182;&#20204;&#20381;&#36182;&#22806;&#37096;&#26816;&#32034;&#20449;&#24687;&#30456;&#20851;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#26469;&#22686;&#24378;LLMs&#23545;&#30693;&#35782;&#36793;&#30028;&#30340;&#24863;&#30693;&#65292;&#24182;&#26174;&#31034;&#23427;&#20204;&#22312;&#20943;&#23569;&#36807;&#24230;&#33258;&#20449;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11457v1 Announce Type: new  Abstract: Large Language Models (LLMs) have been found to have difficulty knowing they do not possess certain knowledge and tend to provide specious answers in such cases. Retrieval Augmentation (RA) has been extensively studied to mitigate LLMs' hallucinations. However, due to the extra overhead and unassured quality of retrieval, it may not be optimal to conduct RA all the time. A straightforward idea is to only conduct retrieval when LLMs are uncertain about a question. This motivates us to enhance the LLMs' ability to perceive their knowledge boundaries to help RA. In this paper, we first quantitatively measure LLMs' such ability and confirm their overconfidence. Then, we study how LLMs' certainty about a question correlates with their dependence on external retrieved information. We propose several methods to enhance LLMs' perception of knowledge boundaries and show that they are effective in reducing overconfidence. Additionally, equipped wi
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#36136;&#24515;&#30340;MBR&#35299;&#30721;&#26041;&#27861;&#25552;&#39640;&#20102;&#35299;&#30721;&#36895;&#24230;&#21644;&#32763;&#35793;&#36136;&#37327;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.11197</link><description>&lt;p&gt;
&#22522;&#20110;&#36136;&#24515;&#30340;&#39640;&#25928;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Centroid-Based Efficient Minimum Bayes Risk Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11197
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36136;&#24515;&#30340;MBR&#35299;&#30721;&#26041;&#27861;&#25552;&#39640;&#20102;&#35299;&#30721;&#36895;&#24230;&#21644;&#32763;&#35793;&#36136;&#37327;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#65288;MBR&#65289;&#35299;&#30721;&#36890;&#36807;&#20351;&#29992;COMET&#23454;&#29616;&#20102;&#19968;&#27969;&#30340;&#32763;&#35793;&#24615;&#33021;&#65292;&#35813;&#31070;&#32463;&#24230;&#37327;&#19982;&#20154;&#31867;&#35780;&#20272;&#20855;&#26377;&#24456;&#39640;&#30340;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;MBR&#35299;&#30721;&#38656;&#35201;&#20108;&#27425;&#26102;&#38388;&#65292;&#22240;&#20026;&#23427;&#35745;&#31639;&#32763;&#35793;&#20551;&#35774;&#19982;&#25152;&#26377;&#21442;&#32771;&#32763;&#35793;&#20043;&#38388;&#30340;&#26399;&#26395;&#20998;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#36136;&#24515;&#30340;MBR&#65288;CBMBR&#65289;&#35299;&#30721;&#20197;&#25552;&#39640;MBR&#35299;&#30721;&#30340;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#23545;&#21442;&#32771;&#32763;&#35793;&#36827;&#34892;&#32858;&#31867;&#65292;&#28982;&#21518;&#20351;&#29992;&#27599;&#20010;&#31751;&#30340;&#36136;&#24515;&#35745;&#31639;&#20998;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;CBMBR&#19981;&#20165;&#23558;&#26399;&#26395;&#20998;&#25968;&#35745;&#31639;&#30340;&#35299;&#30721;&#36895;&#24230;&#25552;&#39640;&#20102;6.9&#20493;&#65292;&#32780;&#19988;&#22312;WMT'22 En$\leftrightarrow$Ja&#12289;En$\leftrightarrow$De&#12289;En$\leftrightarrow$Zh&#20197;&#21450;WMT'23 En$\leftrightarrow$Ja&#32763;&#35793;&#20219;&#21153;&#20013;&#65292;&#22312;&#32763;&#35793;&#36136;&#37327;&#19978;&#36229;&#36807;&#20102;&#39321;&#33609;MBR&#35299;&#30721;&#65292;&#26368;&#39640;&#25552;&#39640;&#20102;0.5 COMET&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11197v1 Announce Type: new  Abstract: Minimum Bayes risk (MBR) decoding achieved state-of-the-art translation performance by using COMET, a neural metric that has a high correlation with human evaluation. However, MBR decoding requires quadratic time since it computes the expected score between a translation hypothesis and all reference translations. We propose centroid-based MBR (CBMBR) decoding to improve the speed of MBR decoding. Our method clusters the reference translations in the feature space, and then calculates the score using the centroids of each cluster. The experimental results show that our CBMBR not only improved the decoding speed of the expected score calculation 6.9 times, but also outperformed vanilla MBR decoding in translation quality by up to 0.5 COMET in the WMT'22 En$\leftrightarrow$Ja, En$\leftrightarrow$De, En$\leftrightarrow$Zh, and WMT'23 En$\leftrightarrow$Ja translation tasks.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#25968;&#25454;&#27700;&#21360;&#22312;LLM&#39044;&#35757;&#32451;&#20013;&#26816;&#27979;&#29256;&#26435;&#25345;&#26377;&#20154;&#20316;&#21697;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36827;&#34892;&#21512;&#29702;&#26816;&#27979;&#19988;&#25552;&#20379;&#35823;&#26816;&#29575;&#20445;&#35777;&#65292;&#30740;&#31350;&#20102;&#27700;&#21360;&#35774;&#35745;&#23545;&#20551;&#35774;&#26816;&#39564;&#33021;&#21147;&#30340;&#24433;&#21709;&#20197;&#21450;&#22312;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#32553;&#25918;&#19979;&#30340;&#26816;&#27979;&#24378;&#24230;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.10892</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#27700;&#21360;&#35777;&#26126;LLM&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#25104;&#21592;&#36164;&#26684;
&lt;/p&gt;
&lt;p&gt;
Proving membership in LLM pretraining data via data watermarks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10892
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#25968;&#25454;&#27700;&#21360;&#22312;LLM&#39044;&#35757;&#32451;&#20013;&#26816;&#27979;&#29256;&#26435;&#25345;&#26377;&#20154;&#20316;&#21697;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36827;&#34892;&#21512;&#29702;&#26816;&#27979;&#19988;&#25552;&#20379;&#35823;&#26816;&#29575;&#20445;&#35777;&#65292;&#30740;&#31350;&#20102;&#27700;&#21360;&#35774;&#35745;&#23545;&#20551;&#35774;&#26816;&#39564;&#33021;&#21147;&#30340;&#24433;&#21709;&#20197;&#21450;&#22312;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#32553;&#25918;&#19979;&#30340;&#26816;&#27979;&#24378;&#24230;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#29256;&#26435;&#25345;&#26377;&#20154;&#30340;&#20316;&#21697;&#26159;&#21542;&#22312;LLM&#39044;&#35757;&#32451;&#20013;&#20351;&#29992;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#25968;&#25454;&#27700;&#21360;&#23454;&#29616;&#22522;&#20110;&#40657;&#30418;&#27169;&#22411;&#35775;&#38382;&#30340;&#21512;&#29702;&#26816;&#27979;&#65292;&#21069;&#25552;&#26159;&#29256;&#26435;&#25345;&#26377;&#20154;&#22312;&#20844;&#24320;&#21457;&#24067;&#20043;&#21069;&#36129;&#29486;&#20102;&#22810;&#20010;&#35757;&#32451;&#25991;&#26723;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#27700;&#21360;&#22788;&#29702;&#12290;&#36890;&#36807;&#24212;&#29992;&#38543;&#26426;&#37319;&#26679;&#30340;&#25968;&#25454;&#27700;&#21360;&#65292;&#26816;&#27979;&#21487;&#20197;&#34987;&#26500;&#36896;&#20026;&#20551;&#35774;&#26816;&#39564;&#65292;&#20174;&#32780;&#25552;&#20379;&#23545;&#35823;&#26816;&#29575;&#30340;&#20445;&#35777;&#12290;&#30740;&#31350;&#20102;&#20004;&#31181;&#27700;&#21360;&#65306;&#19968;&#31181;&#25554;&#20837;&#38543;&#26426;&#24207;&#21015;&#65292;&#21478;&#19968;&#31181;&#38543;&#26426;&#29992;Unicode&#31867;&#20284;&#23383;&#31526;&#26367;&#25442;&#23383;&#31526;&#12290;&#39318;&#20808;&#23637;&#31034;&#20102;&#27700;&#21360;&#35774;&#35745;&#30340;&#19977;&#20010;&#26041;&#38754;--&#27700;&#21360;&#38271;&#24230;&#12289;&#22797;&#21046;&#27425;&#25968;&#21644;&#24178;&#25200;--&#22914;&#20309;&#24433;&#21709;&#20551;&#35774;&#26816;&#39564;&#30340;&#33021;&#21147;&#12290;&#25509;&#30528;&#30740;&#31350;&#20102;&#27700;&#21360;&#22312;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#32553;&#25918;&#19979;&#30340;&#26816;&#27979;&#24378;&#24230;&#22914;&#20309;&#21464;&#21270;&#65306;&#22686;&#21152;&#25968;&#25454;&#38598;&#22823;&#23567;&#20250;&#38477;&#20302;&#27700;&#21360;&#30340;&#24378;&#24230;&#65292;&#27700;&#21360;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10892v1 Announce Type: cross  Abstract: Detecting whether copyright holders' works were used in LLM pretraining is poised to be an important problem. This work proposes using data watermarks to enable principled detection with only black-box model access, provided that the rightholder contributed multiple training documents and watermarked them before public release. By applying a randomly sampled data watermark, detection can be framed as hypothesis testing, which provides guarantees on the false detection rate. We study two watermarks: one that inserts random sequences, and another that randomly substitutes characters with Unicode lookalikes. We first show how three aspects of watermark design -- watermark length, number of duplications, and interference -- affect the power of the hypothesis test. Next, we study how a watermark's detection strength changes under model and dataset scaling: while increasing the dataset size decreases the strength of the watermark, watermarks
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;Llama-2&#31995;&#21015;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#33521;&#35821;&#20316;&#20026;&#20869;&#37096;&#26530;&#32445;&#35821;&#35328;&#30340;&#29616;&#35937;&#65292;&#36825;&#26377;&#21161;&#20110;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#30340;&#21151;&#33021;&#26041;&#24335;&#20197;&#21450;&#35821;&#35328;&#20559;&#35265;&#30340;&#36215;&#28304;&#12290;</title><link>https://arxiv.org/abs/2402.10588</link><description>&lt;p&gt;
&#25289;&#39532;&#22312;&#33521;&#35821;&#20013;&#26377;&#25928;&#21527;&#65311;&#20851;&#20110;&#22810;&#35821;&#35328;&#21464;&#21387;&#22120;&#30340;&#28508;&#22312;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Do Llamas Work in English? On the Latent Language of Multilingual Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;Llama-2&#31995;&#21015;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#33521;&#35821;&#20316;&#20026;&#20869;&#37096;&#26530;&#32445;&#35821;&#35328;&#30340;&#29616;&#35937;&#65292;&#36825;&#26377;&#21161;&#20110;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#30340;&#21151;&#33021;&#26041;&#24335;&#20197;&#21450;&#35821;&#35328;&#20559;&#35265;&#30340;&#36215;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#26159;&#21542;&#22312;&#19981;&#24179;&#34913;&#12289;&#33521;&#35821;&#20027;&#23548;&#30340;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#33521;&#35821;&#20316;&#20026;&#20869;&#37096;&#26530;&#32445;&#35821;&#35328;&#30340;&#38382;&#39064;&#8212;&#8212;&#36825;&#23545;&#20110;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#30340;&#21151;&#33021;&#26041;&#24335;&#20197;&#21450;&#35821;&#35328;&#20559;&#35265;&#30340;&#36215;&#28304;&#33267;&#20851;&#37325;&#35201;&#12290; &#25105;&#20204;&#20851;&#27880;Llama-2&#31995;&#21015;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#31934;&#24515;&#26500;&#24314;&#30340;&#38750;&#33521;&#35821;&#25552;&#31034;&#21644;&#21807;&#19968;&#27491;&#30830;&#30340;&#21333;&#35789;&#24310;&#32493;&#26469;&#36827;&#34892;&#30740;&#31350;&#12290; &#20174;&#19968;&#23618;&#21040;&#21478;&#19968;&#23618;&#65292;&#21464;&#21387;&#22120;&#36880;&#28176;&#23558;&#26368;&#32456;&#25552;&#31034;&#20196;&#29260;&#30340;&#36755;&#20837;&#23884;&#20837;&#26144;&#23556;&#21040;&#36755;&#20986;&#23884;&#20837;&#65292;&#20174;&#20013;&#35745;&#31639;&#19979;&#19968;&#20010;&#20196;&#29260;&#30340;&#27010;&#29575;&#12290; &#36890;&#36807;&#36319;&#36394;&#20854;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#20013;&#38388;&#23884;&#20837;&#65292;&#25581;&#31034;&#20102;&#19977;&#20010;&#19981;&#21516;&#30340;&#38454;&#27573;&#65292;&#21363;&#20013;&#38388;&#23884;&#20837;&#65288;1&#65289;&#24320;&#22987;&#36828;&#31163;&#36755;&#20986;&#20196;&#29260;&#23884;&#20837;&#65307;&#65288;2&#65289;&#22312;&#20013;&#38388;&#23618;&#24050;&#32463;&#20801;&#35768;&#35299;&#30721;&#19968;&#20010;&#35821;&#20041;&#27491;&#30830;&#30340;&#19979;&#19968;&#20010;&#20196;&#29260;&#65292;&#20294;&#26356;&#20542;&#21521;&#20110;&#33521;&#35821;&#29256;&#26412;&#32780;&#19981;&#26159;&#36755;&#20837;&#35821;&#35328;&#30340;&#29256;&#26412;&#65307;&#65288;3&#65289;&#26368;&#32456;&#31227;&#21160;&#21040;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10588v1 Announce Type: new  Abstract: We ask whether multilingual language models trained on unbalanced, English-dominated corpora use English as an internal pivot language -- a question of key importance for understanding how language models function and the origins of linguistic bias. Focusing on the Llama-2 family of transformer models, our study uses carefully constructed non-English prompts with a unique correct single-token continuation. From layer to layer, transformers gradually map an input embedding of the final prompt token to an output embedding from which next-token probabilities are computed. Tracking intermediate embeddings through their high-dimensional space reveals three distinct phases, whereby intermediate embeddings (1) start far away from output token embeddings; (2) already allow for decoding a semantically correct next token in the middle layers, but give higher probability to its version in English than in the input language; (3) finally move into an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#33258;&#21160;&#26657;&#20934;&#23454;&#20107;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#35780;&#20272;&#33021;&#21147;&#65292;&#24341;&#23548;&#27169;&#22411;&#21521;&#23454;&#20107;&#24615;&#38752;&#36817;&#65292;&#24182;&#25913;&#21892;&#27169;&#22411;&#30340;&#32622;&#20449;&#20272;&#35745;&#21644;&#26657;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.09267</link><description>&lt;p&gt;
&#33258;&#21160;&#26657;&#20934;&#23454;&#20107;&#24615;&#65306;&#36890;&#36807;&#33258;&#35780;&#20943;&#32531;LLMs&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Self-Alignment for Factuality: Mitigating Hallucinations in LLMs via Self-Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#33258;&#21160;&#26657;&#20934;&#23454;&#20107;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#35780;&#20272;&#33021;&#21147;&#65292;&#24341;&#23548;&#27169;&#22411;&#21521;&#23454;&#20107;&#24615;&#38752;&#36817;&#65292;&#24182;&#25913;&#21892;&#27169;&#22411;&#30340;&#32622;&#20449;&#20272;&#35745;&#21644;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26174;&#31034;&#20986;&#36234;&#26469;&#36234;&#25509;&#36817;&#20154;&#31867;&#30340;&#33021;&#21147;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20107;&#23454;&#20934;&#30830;&#24615;&#26041;&#38754;&#65288;&#21363;&#8220;&#24187;&#35273;&#8221;&#65289;&#24448;&#24448;&#23384;&#22312;&#22256;&#38590;&#65292;&#21363;&#20351;&#23427;&#20204;&#20855;&#26377;&#30456;&#20851;&#30340;&#30693;&#35782;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#24187;&#35273;&#38382;&#39064;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#39640;&#36136;&#37327;&#30340;&#20154;&#24037;&#20107;&#23454;&#24615;&#27880;&#37322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#33258;&#21160;&#26657;&#20934;&#23454;&#20107;&#24615;&#65292;&#21363;&#21033;&#29992;LLM&#30340;&#33258;&#25105;&#35780;&#20272;&#33021;&#21147;&#25552;&#20379;&#35757;&#32451;&#20449;&#21495;&#65292;&#23558;&#27169;&#22411;&#24341;&#23548;&#21521;&#23454;&#20107;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#33258;&#25105;&#35780;&#20272;&#32452;&#20214;Self-Eval&#32435;&#20837;&#21040;LLM&#20013;&#65292;&#20197;&#20165;&#22522;&#20110;&#20854;&#20869;&#37096;&#30693;&#35782;&#39564;&#35777;&#20854;&#33258;&#24049;&#29983;&#25104;&#30340;&#22238;&#22797;&#30340;&#23454;&#20107;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#33258;&#25105;&#30693;&#35782;&#35843;&#25972;&#65288;SK-Tuning&#65289;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#33258;&#25105;&#35780;&#20272;&#33021;&#21147;&#65292;&#25913;&#21892;&#27169;&#22411;&#30340;&#32622;&#20449;&#20272;&#35745;&#21644;&#26657;&#20934;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#33258;&#25105;&#27880;&#37322;&#30340;&#22238;&#22797;&#36890;&#36807;&#30452;&#25509;&#20248;&#21270;&#20559;&#22909;&#31639;&#27861;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09267v1 Announce Type: cross Abstract: Despite showing increasingly human-like abilities, large language models (LLMs) often struggle with factual inaccuracies, i.e. "hallucinations", even when they hold relevant knowledge. To address these hallucinations, current approaches typically necessitate high-quality human factuality annotations. In this work, we explore Self-Alignment for Factuality, where we leverage the self-evaluation capability of an LLM to provide training signals that steer the model towards factuality. Specifically, we incorporate Self-Eval, a self-evaluation component, to prompt an LLM to validate the factuality of its own generated responses solely based on its internal knowledge. Additionally, we design Self-Knowledge Tuning (SK-Tuning) to augment the LLM's self-evaluation ability by improving the model's confidence estimation and calibration. We then utilize these self-annotated responses to fine-tune the model via Direct Preference Optimization algorith
&lt;/p&gt;</description></item><item><title>Mercury&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;LLM&#20195;&#30721;&#32508;&#21512;&#20219;&#21153;&#30340;&#25928;&#29575;&#35780;&#20272;&#22522;&#20934;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;Beyond@K&#26469;&#34913;&#37327;&#24402;&#19968;&#21270;&#30340;&#20195;&#30721;&#25928;&#29575;&#65292;&#20174;&#32780;&#40723;&#21169;&#29983;&#25104;&#21151;&#33021;&#27491;&#30830;&#19988;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#20195;&#30721;&#12290;</title><link>https://arxiv.org/abs/2402.07844</link><description>&lt;p&gt;
Mercury: &#19968;&#31181;&#29992;&#20110;LLM&#20195;&#30721;&#32508;&#21512;&#25928;&#29575;&#35780;&#20272;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Mercury: An Efficiency Benchmark for LLM Code Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07844
&lt;/p&gt;
&lt;p&gt;
Mercury&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;LLM&#20195;&#30721;&#32508;&#21512;&#20219;&#21153;&#30340;&#25928;&#29575;&#35780;&#20272;&#22522;&#20934;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;Beyond@K&#26469;&#34913;&#37327;&#24402;&#19968;&#21270;&#30340;&#20195;&#30721;&#25928;&#29575;&#65292;&#20174;&#32780;&#40723;&#21169;&#29983;&#25104;&#21151;&#33021;&#27491;&#30830;&#19988;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#20195;&#30721;&#32508;&#21512;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#22522;&#20934;&#20027;&#35201;&#38598;&#20013;&#22312;&#21151;&#33021;&#27491;&#30830;&#24615;&#19978;&#65292;&#24573;&#35270;&#20102;&#20195;&#30721;&#25928;&#29575;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Mercury&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#29992;&#20110;&#35780;&#20272;LLM&#20195;&#30721;&#32508;&#21512;&#20219;&#21153;&#30340;&#20195;&#30721;&#25928;&#29575;&#30340;&#22522;&#20934;&#12290;Mercury&#30001;1,889&#20010;&#28085;&#30422;&#19981;&#21516;&#38590;&#24230;&#32423;&#21035;&#30340;&#32534;&#31243;&#20219;&#21153;&#32452;&#25104;&#65292;&#36824;&#21253;&#25324;&#29983;&#25104;&#26080;&#38480;&#26696;&#20363;&#30340;&#27979;&#35797;&#29992;&#20363;&#29983;&#25104;&#22120;&#65292;&#20197;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20934;&#19981;&#21516;&#65292;Mercury&#38598;&#25104;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;Beyond@K&#65292;&#20197;&#22522;&#20110;&#21382;&#21490;&#25552;&#20132;&#26469;&#34913;&#37327;&#24402;&#19968;&#21270;&#30340;&#20195;&#30721;&#25928;&#29575;&#65292;&#20174;&#32780;&#20026;&#20195;&#30721;&#32508;&#21512;&#25552;&#20379;&#20102;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#40723;&#21169;&#29983;&#25104;&#21151;&#33021;&#27491;&#30830;&#19988;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#20195;&#30721;&#65292;&#20307;&#29616;&#20102;&#29616;&#23454;&#19990;&#30028;&#36719;&#20214;&#24320;&#21457;&#30340;&#26631;&#20934;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;LLM&#34920;&#29616;&#20986;&#29983;&#25104;&#21151;&#33021;&#27491;&#30830;&#20195;&#30721;&#30340;&#26174;&#33879;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#25928;&#29575;&#36755;&#20986;&#26041;&#38754;&#20173;&#23384;&#22312;&#24456;&#22823;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite advancements in evaluating Large Language Models (LLMs) for code synthesis, benchmarks have predominantly focused on functional correctness, overlooking the importance of code efficiency. We present Mercury, the first benchmark designated for assessing the code efficiency of LLM code synthesis tasks. Mercury consists of 1,889 programming tasks covering diverse difficulty levels alongside test case generators generating unlimited cases for comprehensive evaluation. Unlike existing benchmarks, Mercury integrates a novel metric Beyond@K to measure normalized code efficiency based on historical submissions, leading to a new evaluation indicator for code synthesis, which encourages generating functionally correct and computationally efficient code, mirroring the real-world software development standard. Our findings reveal that while LLMs demonstrate the remarkable capability to generate functionally correct code, there still exists a substantial gap in their efficiency output, unde
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;MLLM&#20316;&#20026;&#27861;&#23448;&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;MLLM&#22312;&#36741;&#21161;&#27861;&#23448;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;MLLM&#22312;&#23545;&#27604;&#35780;&#20272;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#38750;&#24120;&#31867;&#20154;&#30340;&#36776;&#21035;&#33021;&#21147;&#65292;&#20294;&#22312;&#35780;&#20998;&#35780;&#20272;&#21644;&#25209;&#37327;&#25490;&#24207;&#20219;&#21153;&#20013;&#19982;&#20154;&#31867;&#20559;&#22909;&#23384;&#22312;&#26174;&#33879;&#30340;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#23545;&#20110;&#20808;&#36827;&#27169;&#22411;&#22914;GPT-4V&#65292;MLLM&#20173;&#28982;&#38754;&#20020;&#30528;&#20559;&#35265;&#12289;&#24187;&#35273;&#24335;&#22238;&#31572;&#21644;&#19981;&#19968;&#33268;&#24615;&#31561;&#21028;&#26029;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.04788</link><description>&lt;p&gt;
MLLM&#20316;&#20026;&#27861;&#23448;&#65306;&#20351;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#20934;&#35780;&#20272;&#22810;&#27169;&#24577;MLLM&#20316;&#20026;&#27861;&#23448;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;MLLM&#20316;&#20026;&#27861;&#23448;&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;MLLM&#22312;&#36741;&#21161;&#27861;&#23448;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;MLLM&#22312;&#23545;&#27604;&#35780;&#20272;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#38750;&#24120;&#31867;&#20154;&#30340;&#36776;&#21035;&#33021;&#21147;&#65292;&#20294;&#22312;&#35780;&#20998;&#35780;&#20272;&#21644;&#25209;&#37327;&#25490;&#24207;&#20219;&#21153;&#20013;&#19982;&#20154;&#31867;&#20559;&#22909;&#23384;&#22312;&#26174;&#33879;&#30340;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#23545;&#20110;&#20808;&#36827;&#27169;&#22411;&#22914;GPT-4V&#65292;MLLM&#20173;&#28982;&#38754;&#20020;&#30528;&#20559;&#35265;&#12289;&#24187;&#35273;&#24335;&#22238;&#31572;&#21644;&#19981;&#19968;&#33268;&#24615;&#31561;&#21028;&#26029;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#36817;&#26469;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#23637;&#29616;&#20986;&#20154;&#24037;&#26234;&#33021;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;MLLM&#30340;&#23454;&#29992;&#24615;&#23384;&#22312;&#30528;&#30456;&#24403;&#22823;&#30340;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#32570;&#20047;&#19982;&#20154;&#31867;&#20559;&#22909;&#30456;&#31526;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#12290;&#26412;&#25991;&#21463;&#21040;LLM&#27169;&#22411;&#20013;LLM&#20316;&#20026;&#27861;&#23448;&#30340;&#21551;&#21457;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#34987;&#31216;&#20026;MLLM&#20316;&#20026;&#27861;&#23448;&#65292;&#29992;&#20110;&#35780;&#20272;MLLM&#22312;&#21327;&#21161;&#27861;&#23448;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#19977;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#65306;&#35780;&#20998;&#35780;&#20272;&#12289;&#23545;&#27604;&#35780;&#20272;&#21644;&#25209;&#37327;&#25490;&#24207;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#34429;&#28982;MLLM&#22312;&#23545;&#27604;&#35780;&#20272;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#31867;&#20154;&#36776;&#21035;&#33021;&#21147;&#65292;&#20294;&#22312;&#35780;&#20998;&#35780;&#20272;&#21644;&#25209;&#37327;&#25490;&#24207;&#20219;&#21153;&#20013;&#19982;&#20154;&#31867;&#20559;&#22909;&#23384;&#22312;&#26174;&#33879;&#30340;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#23545;&#20110;&#20687;GPT-4V&#36825;&#26679;&#30340;&#20808;&#36827;&#27169;&#22411;&#65292;MLLM&#20173;&#28982;&#38754;&#20020;&#30528;&#21028;&#26029;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#22810;&#26679;&#30340;&#20559;&#35265;&#12289;&#24187;&#35273;&#24335;&#30340;&#22238;&#31572;&#21644;&#19981;&#19968;&#33268;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#23545;MLLM&#30340;&#25913;&#36827;&#21644;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#36843;&#20999;&#38656;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Large Language Models (MLLMs) have gained significant attention recently, showing remarkable potential in artificial general intelligence. However, assessing the utility of MLLMs presents considerable challenges, primarily due to the absence multimodal benchmarks that align with human preferences. Inspired by LLM-as-a-Judge in LLMs, this paper introduces a novel benchmark, termed MLLM-as-a-Judge, to assess the ability of MLLMs in assisting judges including three distinct tasks: Scoring Evaluation, Pair Comparison, and Batch Ranking. Our study reveals that, while MLLMs demonstrate remarkable human-like discernment in Pair Comparisons, there is a significant divergence from human preferences in Scoring Evaluation and Batch Ranking tasks. Furthermore, MLLMs still face challenges in judgment, including diverse biases, hallucinatory responses, and inconsistencies, even for advanced models such as GPT-4V. These findings emphasize the pressing need for enhancements and further rese
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#23450;&#20301;&#21644;&#25237;&#24433;&#26041;&#27861;&#30340;&#20551;&#35774;&#12290;&#36890;&#36807;&#26597;&#35810;&#21644;&#38190;&#30697;&#38453;&#26469;&#35745;&#31639;&#36755;&#20837;&#25991;&#26412;&#19982;&#27599;&#20010;&#28436;&#31034;&#20043;&#38388;&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;&#65292;&#20197;&#23398;&#20064;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.02872</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65311;&#26597;&#35810;&#21644;&#38190;&#30697;&#38453;&#26159;&#19978;&#19979;&#25991;&#22836;&#37096;&#36827;&#34892;&#24230;&#37327;&#23398;&#20064;&#30340;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;
&lt;/p&gt;
&lt;p&gt;
How do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#23450;&#20301;&#21644;&#25237;&#24433;&#26041;&#27861;&#30340;&#20551;&#35774;&#12290;&#36890;&#36807;&#26597;&#35810;&#21644;&#38190;&#30697;&#38453;&#26469;&#35745;&#31639;&#36755;&#20837;&#25991;&#26412;&#19982;&#27599;&#20010;&#28436;&#31034;&#20043;&#38388;&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;&#65292;&#20197;&#23398;&#20064;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#32034;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26426;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#23450;&#20301;&#21644;&#25237;&#24433;&#26041;&#27861;&#30340;&#20551;&#35774;&#12290;&#22312;&#27973;&#23618;&#20013;&#65292;&#28436;&#31034;&#30340;&#29305;&#24449;&#34987;&#21512;&#24182;&#21040;&#30456;&#24212;&#30340;&#26631;&#31614;&#20013;&#65292;&#36755;&#20837;&#25991;&#26412;&#30340;&#29305;&#24449;&#34987;&#32858;&#21512;&#21040;&#26368;&#21518;&#19968;&#20010;&#26631;&#35760;&#20013;&#12290;&#22312;&#28145;&#23618;&#20013;&#65292;&#19978;&#19979;&#25991;&#22836;&#37096;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#27599;&#20010;&#19978;&#19979;&#25991;&#22836;&#37096;&#20013;&#65292;&#20540;-&#36755;&#20986;&#30697;&#38453;&#25552;&#21462;&#20102;&#26631;&#31614;&#30340;&#29305;&#24449;&#12290;&#26597;&#35810;&#21644;&#38190;&#30697;&#38453;&#35745;&#31639;&#20102;&#36755;&#20837;&#25991;&#26412;&#19982;&#27599;&#20010;&#28436;&#31034;&#20043;&#38388;&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;&#12290;&#27880;&#24847;&#21147;&#26435;&#37325;&#36234;&#22823;&#65292;&#36234;&#22810;&#30340;&#26631;&#31614;&#20449;&#24687;&#34987;&#20256;&#36755;&#21040;&#26368;&#21518;&#19968;&#20010;&#26631;&#35760;&#20013;&#65292;&#29992;&#20110;&#39044;&#27979;&#19979;&#19968;&#20010;&#21333;&#35789;&#12290;&#26597;&#35810;&#21644;&#38190;&#30697;&#38453;&#21487;&#20197;&#34987;&#35270;&#20026;&#23398;&#20064;&#36755;&#20837;&#25991;&#26412;&#19982;&#27599;&#20010;&#28436;&#31034;&#20043;&#38388;&#30456;&#20284;&#24230;&#24230;&#37327;&#30340;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#22522;&#20110;&#36825;&#20010;&#20551;&#35774;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#19981;&#24179;&#34913;&#30340;&#26631;&#31614;&#21644;&#28436;&#31034;&#39034;&#24207;&#20250;&#24433;&#21709;&#39044;&#27979;&#12290;&#25105;&#20204;&#22312;GPT2&#22823;&#22411;&#12289;Llama 7B&#12289;13B&#21644;30B&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#32467;&#26524;&#25903;&#25345;&#25105;&#20204;&#30340;&#20998;&#26512;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#29702;&#35770;&#35299;&#37322;&#21644;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the mechanism of in-context learning and propose a hypothesis using locate-and-project method. In shallow layers, the features of demonstrations are merged into their corresponding labels, and the features of the input text are aggregated into the last token. In deep layers, in-context heads make great contributions. In each in-context head, the value-output matrix extracts the labels' features. Query and key matrices compute the attention weights between the input text and each demonstration. The larger the attention weight is, the more label information is transferred into the last token for predicting the next word. Query and key matrices can be regarded as two towers for learning the similarity metric between the input text and each demonstration. Based on this hypothesis, we explain why imbalanced labels and demonstration order affect predictions. We conduct experiments on GPT2 large, Llama 7B, 13B and 30B. The results can support our analysis. Overall, our study provid
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;TG-LLM&#26694;&#26550;&#65292;&#20197;&#35821;&#35328;&#20026;&#22522;&#30784;&#36827;&#34892;&#26102;&#38388;&#25512;&#29702;&#65292;&#36890;&#36807;&#25945;&#23548;LLM&#23558;&#19978;&#19979;&#25991;&#32763;&#35793;&#25104;&#26102;&#38388;&#22270;&#65292;&#24182;&#20351;&#29992;CoTs&#25351;&#23548;LLM&#36827;&#34892;&#31526;&#21495;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2401.06853</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#26102;&#38388;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Can Learn Temporal Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.06853
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;TG-LLM&#26694;&#26550;&#65292;&#20197;&#35821;&#35328;&#20026;&#22522;&#30784;&#36827;&#34892;&#26102;&#38388;&#25512;&#29702;&#65292;&#36890;&#36807;&#25945;&#23548;LLM&#23558;&#19978;&#19979;&#25991;&#32763;&#35793;&#25104;&#26102;&#38388;&#22270;&#65292;&#24182;&#20351;&#29992;CoTs&#25351;&#23548;LLM&#36827;&#34892;&#31526;&#21495;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#24182;&#38750;&#27809;&#26377;&#32570;&#38519;&#21644;&#19981;&#20934;&#30830;&#20043;&#22788;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20171;&#32461;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#20123;&#23616;&#38480;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#26102;&#38388;&#25512;&#29702;&#65288;TR&#65289;&#23545;LLMs&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20381;&#36182;&#20110;&#22810;&#26679;&#30340;&#26102;&#38388;&#34920;&#36798;&#21644;&#22797;&#26434;&#30340;&#19978;&#19979;&#25991;&#32454;&#33410;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TG-LLM&#65292;&#19968;&#20010;&#33268;&#21147;&#20110;&#22522;&#20110;&#35821;&#35328;&#30340;&#26102;&#38388;&#25512;&#29702;&#30340;&#26032;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25945;&#23548;LLM&#23558;&#19978;&#19979;&#25991;&#32763;&#35793;&#25104;&#26102;&#38388;&#22270;&#65288;TG&#65289;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#20840;&#21487;&#25511;&#19988;&#38656;&#35201;&#26368;&#23569;&#30417;&#30563;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22312;&#36825;&#20010;&#22270;&#32763;&#35793;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#22312;&#23454;&#39564;&#35777;&#23454;&#65292;&#23398;&#20064;&#22312;&#25105;&#20204;&#25968;&#25454;&#38598;&#19978;&#30340;TG&#25552;&#21462;&#33021;&#21147;&#21487;&#20197;&#36716;&#31227;&#21040;&#20854;&#20182;TR&#20219;&#21153;&#21644;&#22522;&#20934;&#27979;&#35797;&#19978;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;CoTs&#24341;&#23548;LLM&#36890;&#36807;TG&#36827;&#34892;&#31526;&#21495;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.06853v2 Announce Type: replace  Abstract: While large language models (LLMs) have demonstrated remarkable reasoning capabilities, they are not without their flaws and inaccuracies. Recent studies have introduced various methods to mitigate these limitations. Temporal reasoning (TR), in particular, presents a significant challenge for LLMs due to its reliance on diverse temporal expressions and intricate contextual details. In this paper, we propose TG-LLM, a new framework towards language-based TR. To be specific, we first teach LLM to translate the context into a temporal graph (TG). A synthetic dataset, which is fully controllable and requires minimal supervision, is constructed for fine-tuning on this graph translation task. We confirm in experiments that the capability of TG extraction learned on our dataset can be transferred to other TR tasks and benchmarks. On top of that, we guide LLM to perform symbolic reasoning over the TG via Chain of Thoughts (CoTs) bootstrappin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;diff&#21382;&#21490;&#30340;&#31616;&#21333;&#19988;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#23558;&#29615;&#22659;&#20013;&#30340;&#35266;&#27979;&#36716;&#25442;&#20026;&#25991;&#26412;&#25552;&#31034;&#65292;&#20197;&#20415;&#23545;&#20110;&#38271;&#26399;&#25512;&#29702;&#20915;&#31574;&#30340;&#20219;&#21153;&#20013;&#30340;Neural Language Models&#36827;&#34892;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2312.07540</link><description>&lt;p&gt;
Neural Language Agents&#30340;diff&#21382;&#21490;
&lt;/p&gt;
&lt;p&gt;
diff History for Neural Language Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.07540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;diff&#21382;&#21490;&#30340;&#31616;&#21333;&#19988;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#23558;&#29615;&#22659;&#20013;&#30340;&#35266;&#27979;&#36716;&#25442;&#20026;&#25991;&#26412;&#25552;&#31034;&#65292;&#20197;&#20415;&#23545;&#20110;&#38271;&#26399;&#25512;&#29702;&#20915;&#31574;&#30340;&#20219;&#21153;&#20013;&#30340;Neural Language Models&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Neural Language Models (LMs)&#20026;&#36890;&#29992;&#30340;&#20855;&#20307;&#25511;&#21046;&#25552;&#20379;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#24403;&#20351;&#29992;&#22522;&#20110;LM&#30340;&#25511;&#21046;&#22120;&#26102;&#65292;&#20250;&#20986;&#29616;&#19968;&#20010;&#20851;&#38190;&#30340;&#25216;&#26415;&#38382;&#39064;&#65306;&#29615;&#22659;&#35266;&#27979;&#24517;&#39035;&#36716;&#25442;&#20026;&#25991;&#26412;&#65292;&#36825;&#19982;&#21382;&#21490;&#32806;&#21512;&#22312;&#19968;&#36215;&#65292;&#23548;&#33268;&#20887;&#38271;&#32780;&#20887;&#20313;&#30340;&#25991;&#26412;&#25552;&#31034;&#12290;&#22240;&#27492;&#65292;LM&#20195;&#29702;&#30340;&#20808;&#21069;&#24037;&#20316;&#23616;&#38480;&#20110;&#20855;&#26377;&#23567;&#35266;&#27979;&#22823;&#23567;&#20197;&#21450;&#23545;&#20132;&#20114;&#21382;&#21490;&#25110;&#25351;&#31034;&#35843;&#20248;&#38656;&#27714;&#36739;&#23567;&#30340;&#38480;&#21046;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;diff&#21382;&#21490;&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#19988;&#38750;&#24120;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#22312;&#29992;&#20110;&#25552;&#31034;LM&#31574;&#30053;&#30340;&#20132;&#20114;&#21382;&#21490;&#20013;&#30340;&#36830;&#32493;&#25991;&#26412;&#35266;&#27979;&#19978;&#24212;&#29992;Unix diff&#21629;&#20196;&#65292;&#25105;&#20204;&#26082;&#21487;&#20197;&#25688;&#38500;&#20887;&#20313;&#20449;&#24687;&#65292;&#21448;&#21487;&#20197;&#23558;&#25991;&#26412;&#36755;&#20837;&#30340;&#20869;&#23481;&#38598;&#20013;&#22312;&#29615;&#22659;&#20013;&#26174;&#33879;&#21464;&#21270;&#30340;&#26041;&#38754;&#12290;&#22312;&#38656;&#35201;&#38271;&#26399;&#25512;&#29702;&#36827;&#34892;&#20915;&#31574;&#30340;&#26410;&#35299;&#20915;&#30340;&#35270;&#39057;&#28216;&#25103;NetHack&#20013;&#65292;&#20351;&#29992;diff&#21382;&#21490;&#35843;&#20248;&#30340;LM&#19982;&#29366;&#24577;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.07540v2 Announce Type: replace Abstract: Neural Language Models (LMs) offer an exciting solution for general-purpose embodied control. However, a key technical issue arises when using an LM-based controller: environment observations must be converted to text, which coupled with history, results in long and verbose textual prompts. As a result, prior work in LM agents is limited to restricted domains with small observation size as well as minimal needs for interaction history or instruction tuning. In this paper, we introduce diff history, a simple and highly effective solution to these issues. By applying the Unix diff command on consecutive text observations in the interaction histories used to prompt LM policies, we can both abstract away redundant information and focus the content of textual inputs on the salient changes in the environment. On NetHack, an unsolved video game that requires long-horizon reasoning for decision-making, LMs tuned with diff history match state-
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Fakepedia&#25968;&#25454;&#38598;&#30740;&#31350;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#33021;&#21147;&#21644;&#36827;&#34892;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#65292;&#20197;&#35299;&#20915;&#19978;&#19979;&#25991;&#20449;&#24687;&#19982;&#23384;&#20648;&#30693;&#35782;&#30456;&#30683;&#30462;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2312.02073</link><description>&lt;p&gt;
&#19968;&#21058;&#30149;&#27602;&#65311;&#20351;&#29992;Fakepedia&#23450;&#20301;&#21644;&#26816;&#27979;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
A Glitch in the Matrix? Locating and Detecting Language Model Grounding with Fakepedia
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02073
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Fakepedia&#25968;&#25454;&#38598;&#30740;&#31350;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#33021;&#21147;&#21644;&#36827;&#34892;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#65292;&#20197;&#35299;&#20915;&#19978;&#19979;&#25991;&#20449;&#24687;&#19982;&#23384;&#20648;&#30693;&#35782;&#30456;&#30683;&#30462;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#20174;&#20854;&#19978;&#19979;&#25991;&#20013;&#25552;&#20379;&#30340;&#26032;&#39062;&#20449;&#24687;&#20013;&#33719;&#24471;&#30340;&#20986;&#33394;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23578;&#19981;&#28165;&#26970;&#22312;&#19978;&#19979;&#25991;&#20449;&#24687;&#19982;&#21442;&#25968;&#20013;&#23384;&#20648;&#30340;&#20107;&#23454;&#30693;&#35782;&#30456;&#30683;&#30462;&#30340;&#24773;&#20917;&#19979;&#65292;&#25903;&#25745;&#36825;&#31181;&#19978;&#19979;&#25991;&#22522;&#30784;&#30340;&#26426;&#21046;&#65292;LLMs&#22312;&#22238;&#24518;&#26041;&#38754;&#20063;&#34920;&#29616;&#20986;&#33394;&#12290;&#20559;&#22909;&#19978;&#19979;&#25991;&#20449;&#24687;&#23545;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26041;&#27861;&#33267;&#20851;&#37325;&#35201;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#36807;&#23558;&#19978;&#19979;&#25991;&#19982;&#26368;&#26032;&#20449;&#24687;&#20016;&#23500;&#65292;&#24076;&#26395;&#22522;&#30784;&#21487;&#20197;&#32416;&#27491;&#36807;&#26102;&#25110;&#26377;&#22122;&#22768;&#30340;&#23384;&#20648;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Fakepedia&#30340;&#26032;&#39062;&#26041;&#27861;&#26469;&#30740;&#31350;&#22522;&#30784;&#33021;&#21147;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#19982;&#27169;&#22411;&#20869;&#37096;&#21442;&#25968;&#30693;&#35782;&#20914;&#31361;&#30340;&#21453;&#20107;&#23454;&#25991;&#26412;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;Fakepedia&#23545;&#21508;&#31181;LLMs&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#28982;&#21518;&#25105;&#20204;&#36827;&#34892;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#65292;&#22522;&#20110;&#25105;&#20204;&#30340;&#36974;&#34109;&#20998;&#32452;&#22240;&#26524;&#36861;&#36394;&#65288;MGCT&#65289;&#65292;&#23545;&#22238;&#31572;Fakepedia&#26597;&#35810;&#26102;&#30340;LLM&#32452;&#20214;&#36827;&#34892;&#20998;&#26512;&#12290;&#22312;&#36825;&#20010;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#37492;&#21035;&#20986;d
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02073v2 Announce Type: replace  Abstract: Large language models (LLMs) have an impressive ability to draw on novel information supplied in their context. Yet the mechanisms underlying this contextual grounding remain unknown, especially in situations where contextual information contradicts factual knowledge stored in the parameters, which LLMs also excel at recalling. Favoring the contextual information is critical for retrieval-augmented generation methods, which enrich the context with up-to-date information, hoping that grounding can rectify outdated or noisy stored knowledge. We present a novel method to study grounding abilities using Fakepedia, a dataset of counterfactual texts constructed to clash with a model's internal parametric knowledge. We benchmark various LLMs with Fakepedia and then we conduct a causal mediation analysis, based on our Masked Grouped Causal Tracing (MGCT), on LLM components when answering Fakepedia queries. Within this analysis, we identify d
&lt;/p&gt;</description></item><item><title>&#30446;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#21518;&#32493;&#38382;&#39064;&#26102;&#24120;&#24120;&#25671;&#25670;&#19981;&#23450;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#21518;&#32493;&#38382;&#39064;&#26426;&#21046;&#21644;&#20004;&#20010;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#24320;&#21457;&#20986;Unwavering-FQ&#26694;&#26550;&#26469;&#25945;&#23548;&#27169;&#22411;&#20445;&#25345;&#26368;&#21021;&#30340;&#27491;&#30830;&#21028;&#26029;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2310.02174</link><description>&lt;p&gt;
&#35753;&#24490;&#29615;&#30340;&#35810;&#38382;: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21028;&#26029;&#20013;&#30340;&#25671;&#25670;
&lt;/p&gt;
&lt;p&gt;
Ask Again, Then Fail: Large Language Models' Vacillations in Judgement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.02174
&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#21518;&#32493;&#38382;&#39064;&#26102;&#24120;&#24120;&#25671;&#25670;&#19981;&#23450;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#21518;&#32493;&#38382;&#39064;&#26426;&#21046;&#21644;&#20004;&#20010;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#24320;&#21457;&#20986;Unwavering-FQ&#26694;&#26550;&#26469;&#25945;&#23548;&#27169;&#22411;&#20445;&#25345;&#26368;&#21021;&#30340;&#27491;&#30830;&#21028;&#26029;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35266;&#23519;&#21040;&#30446;&#21069;&#30340;&#20250;&#35805;&#24335;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#21518;&#32493;&#38382;&#39064;&#26102;&#24448;&#24448;&#22312;&#20854;&#21028;&#26029;&#19978;&#25671;&#25670;&#19981;&#23450;&#65292;&#21363;&#20351;&#21407;&#22987;&#21028;&#26029;&#26159;&#27491;&#30830;&#30340;&#12290;&#36825;&#31181;&#25671;&#25670;&#23545;&#20110;&#29983;&#25104;&#21487;&#38752;&#22238;&#22797;&#21644;&#24314;&#31435;&#29992;&#25143;&#20449;&#20219;&#26500;&#25104;&#20102;&#37325;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#20840;&#38754;&#35780;&#20272;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21518;&#32493;&#38382;&#39064;&#26426;&#21046;&#20197;&#21450;&#20004;&#20010;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#30830;&#35748;&#20102;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#26222;&#36941;&#23384;&#22312;&#36825;&#31181;&#24773;&#20917;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#21508;&#31181;&#25552;&#31034;&#31574;&#30053;&#29992;&#20110;&#38381;&#28304;&#27169;&#22411;&#65307;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#35757;&#32451;&#30340;&#26694;&#26550;Unwavering-FQ&#65292;&#36890;&#36807;&#21512;&#25104;&#39640;&#36136;&#37327;&#30340;&#20559;&#22909;&#25968;&#25454;&#26469;&#25945;&#23548;&#35821;&#35328;&#27169;&#22411;&#20445;&#25345;&#20854;&#26368;&#21021;&#30340;&#27491;&#30830;&#21028;&#26029;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#20854;&#22686;&#24378;&#27169;&#22411;&#36890;&#29992;&#33021;&#21147;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.02174v2 Announce Type: replace-cross  Abstract: We observe that current conversational language models often waver in their judgements when faced with follow-up questions, even if the original judgement was correct. This wavering presents a significant challenge for generating reliable responses and building user trust. To comprehensively assess this issue, we introduce a Follow-up Questioning Mechanism along with two metrics to quantify this inconsistency, confirming its widespread presence in current language models. To mitigate this issue, we explore various prompting strategies for closed-source models; moreover, we develop a training-based framework Unwavering-FQ that teaches language models to maintain their originally correct judgements through synthesized high-quality preference data. Our experimental results confirm the effectiveness of our framework and its ability to enhance the general capabilities of models (https://github.com/NUSTM/LLMs-Waver-In-Judgements).
&lt;/p&gt;</description></item><item><title>SciMMIR&#26159;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#31185;&#23398;&#39046;&#22495;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#26816;&#32034;&#22522;&#20934;&#65292;&#36890;&#36807;&#24320;&#25918;&#33719;&#21462;&#30340;&#35770;&#25991;&#38598;&#21512;&#25552;&#21462;&#19982;&#31185;&#23398;&#39046;&#22495;&#30456;&#20851;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#65292;&#20174;&#32780;&#24357;&#34917;&#20102;&#29616;&#26377;&#22522;&#20934;&#22312;&#27492;&#39046;&#22495;&#20013;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2401.13478</link><description>&lt;p&gt;
SciMMIR:&#31185;&#23398;&#22810;&#27169;&#24577;&#20449;&#24687;&#26816;&#32034;&#30340;&#22522;&#20934;&#35780;&#27979;
&lt;/p&gt;
&lt;p&gt;
SciMMIR: Benchmarking Scientific Multi-modal Information Retrieval. (arXiv:2401.13478v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13478
&lt;/p&gt;
&lt;p&gt;
SciMMIR&#26159;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#31185;&#23398;&#39046;&#22495;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#26816;&#32034;&#22522;&#20934;&#65292;&#36890;&#36807;&#24320;&#25918;&#33719;&#21462;&#30340;&#35770;&#25991;&#38598;&#21512;&#25552;&#21462;&#19982;&#31185;&#23398;&#39046;&#22495;&#30456;&#20851;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#65292;&#20174;&#32780;&#24357;&#34917;&#20102;&#29616;&#26377;&#22522;&#20934;&#22312;&#27492;&#39046;&#22495;&#20013;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#20449;&#24687;&#26816;&#32034;&#65288;MMIR&#65289;&#26159;&#19968;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#36890;&#36807;&#20808;&#36827;&#30340;&#34920;&#31034;&#23398;&#20064;&#21644;&#36328;&#27169;&#24577;&#23545;&#40784;&#30740;&#31350;&#65292;&#22312;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#31185;&#23398;&#39046;&#22495;&#20869;&#35780;&#20272;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#30340;MMIR&#24615;&#33021;&#30340;&#24403;&#21069;&#22522;&#20934;&#23384;&#22312;&#26126;&#26174;&#24046;&#36317;&#65292;&#23398;&#26415;&#35821;&#35328;&#20013;&#25551;&#36848;&#30340;&#22270;&#34920;&#21644;&#34920;&#26684;&#22270;&#20687;&#36890;&#24120;&#19981;&#36215;&#37325;&#35201;&#20316;&#29992;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#21033;&#29992;&#24320;&#25918;&#33719;&#21462;&#30340;&#35770;&#25991;&#38598;&#21512;&#26500;&#24314;&#20102;&#19968;&#20010;&#19987;&#38376;&#30340;&#31185;&#23398;MMIR&#65288;SciMMIR&#65289;&#22522;&#20934;&#65292;&#20197;&#25552;&#21462;&#19982;&#31185;&#23398;&#39046;&#22495;&#30456;&#20851;&#30340;&#25968;&#25454;&#12290;&#35813;&#22522;&#20934;&#21253;&#21547;&#20102;530K&#20010;&#31934;&#24515;&#31574;&#21010;&#30340;&#20174;&#31185;&#23398;&#25991;&#26723;&#20013;&#25552;&#21462;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#65292;&#36825;&#20123;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#26469;&#33258;&#20110;&#20855;&#26377;&#35814;&#32454;&#26631;&#39064;&#30340;&#31185;&#23398;&#25991;&#26723;&#20013;&#30340;&#22270;&#34920;&#21644;&#34920;&#26684;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#20004;&#32423;&#23376;&#38598;-&#23376;&#31867;&#21035;&#23618;&#27425;&#27880;&#37322;&#23545;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#36827;&#34892;&#20102;&#27880;&#37322;&#65292;&#20197;&#20419;&#36827;&#23545;&#22522;&#20934;&#27169;&#22411;&#30340;&#26356;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#23545;&#38646;&#26679;&#26412;&#21644;&#24494;&#35843;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal information retrieval (MMIR) is a rapidly evolving field, where significant progress, particularly in image-text pairing, has been made through advanced representation learning and cross-modality alignment research. However, current benchmarks for evaluating MMIR performance in image-text pairing within the scientific domain show a notable gap, where chart and table images described in scholarly language usually do not play a significant role. To bridge this gap, we develop a specialised scientific MMIR (SciMMIR) benchmark by leveraging open-access paper collections to extract data relevant to the scientific domain. This benchmark comprises 530K meticulously curated image-text pairs, extracted from figures and tables with detailed captions in scientific documents. We further annotate the image-text pairs with two-level subset-subcategory hierarchy annotations to facilitate a more comprehensive evaluation of the baselines. We conducted zero-shot and fine-tuning evaluations o
&lt;/p&gt;</description></item><item><title>LinguAlchemy&#26159;&#19968;&#31181;&#23558;&#35821;&#35328;&#31867;&#22411;&#23398;&#21644;&#22320;&#29702;&#20803;&#32032;&#34701;&#21512;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#26410;&#35265;&#35821;&#35328;&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.06034</link><description>&lt;p&gt;
LinguAlchemy: &#23558;&#35821;&#35328;&#31867;&#22411;&#23398;&#21644;&#22320;&#29702;&#20803;&#32032;&#34701;&#21512;&#20197;&#23454;&#29616;&#23545;&#26410;&#35265;&#35821;&#35328;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
LinguAlchemy: Fusing Typological and Geographical Elements for Unseen Language Generalization. (arXiv:2401.06034v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06034
&lt;/p&gt;
&lt;p&gt;
LinguAlchemy&#26159;&#19968;&#31181;&#23558;&#35821;&#35328;&#31867;&#22411;&#23398;&#21644;&#22320;&#29702;&#20803;&#32032;&#34701;&#21512;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#26410;&#35265;&#35821;&#35328;&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#22810;&#20010;&#20219;&#21153;&#21644;&#35821;&#35328;&#19978;&#23637;&#31034;&#20986;&#20102;&#38750;&#20961;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#65292;PLMs&#30340;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#65292;&#23548;&#33268;&#35821;&#35328;&#24615;&#33021;&#26126;&#26174;&#19979;&#38477;&#65292;&#29978;&#33267;&#29983;&#25104;&#30340;&#22238;&#24212;&#19982;&#38543;&#26426;&#22522;&#20934;&#30456;&#24403;&#33618;&#21776;&#12290;&#36825;&#19968;&#38480;&#21046;&#19968;&#30452;&#20197;&#26469;&#37117;&#26159;PLMs&#30340;&#19968;&#20010;&#38271;&#26399;&#38382;&#39064;&#65292;&#28041;&#21450;&#21040;&#35821;&#35328;&#24314;&#27169;&#25216;&#26415;&#30340;&#22810;&#26679;&#24615;&#21644;&#24179;&#31561;&#33719;&#21462;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;LinguAlchemy&#26469;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#36825;&#26159;&#19968;&#31181;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#23558;&#35821;&#35328;&#30340;&#21508;&#20010;&#26041;&#38754;&#65288;&#21253;&#25324;&#31867;&#22411;&#23398;&#12289;&#22320;&#29702;&#21644;&#35821;&#31995;&#65289;&#32435;&#20837;PLMs&#30340;&#34920;&#31034;&#20013;&#65292;&#20197;&#26356;&#22909;&#22320;&#34920;&#24449;&#30456;&#24212;&#30340;&#35821;&#35328;&#32422;&#26463;&#12290;&#19982;&#23436;&#20840;&#24494;&#35843;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;LinguAlchemy&#26174;&#33879;&#25552;&#39640;&#20102;mBERT&#21644;XLM-R&#23545;&#26410;&#35265;&#35821;&#35328;&#30340;&#20934;&#30830;&#24615;&#32489;&#25928;&#65292;&#20998;&#21035;&#25552;&#39640;&#20102;&#32422;18%&#21644;&#32422;2%&#65292;&#23637;&#29616;&#20986;&#36739;&#39640;&#30340;&#26410;&#35265;&#35821;&#35328;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models (PLMs) have shown remarkable generalization toward multiple tasks and languages. Nonetheless, the generalization of PLMs towards unseen languages is poor, resulting in significantly worse language performance, or even generating nonsensical responses that are comparable to a random baseline. This limitation has been a longstanding problem of PLMs raising the problem of diversity and equal access to language modeling technology. In this work, we solve this limitation by introducing LinguAlchemy, a regularization technique that incorporates various aspects of languages covering typological, geographical, and phylogenetic constraining the resulting representation of PLMs to better characterize the corresponding linguistics constraints. LinguAlchemy significantly improves the accuracy performance of mBERT and XLM-R on unseen languages by ~18% and ~2%, respectively compared to fully finetuned models and displaying a high degree of unseen language generalization. W
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;OrthogonalE&#65292;&#21033;&#29992;&#30697;&#38453;&#34920;&#31034;&#23454;&#20307;&#21644;&#22359;&#23545;&#35282;&#27491;&#20132;&#30697;&#38453;&#34920;&#31034;&#20851;&#31995;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#21644;&#24191;&#27867;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#27604;&#26368;&#20808;&#36827;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#36739;&#23569;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.05967</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#20998;&#22359;&#23545;&#35282;&#27491;&#20132;&#20851;&#31995;&#21644;&#30697;&#38453;&#23454;&#20307;
&lt;/p&gt;
&lt;p&gt;
Block-Diagonal Orthogonal Relation and Matrix Entity for Knowledge Graph Embedding. (arXiv:2401.05967v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05967
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;OrthogonalE&#65292;&#21033;&#29992;&#30697;&#38453;&#34920;&#31034;&#23454;&#20307;&#21644;&#22359;&#23545;&#35282;&#27491;&#20132;&#30697;&#38453;&#34920;&#31034;&#20851;&#31995;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#21644;&#24191;&#27867;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#27604;&#26368;&#20808;&#36827;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#36739;&#23569;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#23398;&#20064;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#20302;&#32500;&#34920;&#31034;&#20197;&#39044;&#27979;&#32570;&#22833;&#30340;&#20107;&#23454;&#12290;&#26059;&#36716;-based&#26041;&#27861;&#22914;RotatE&#21644;QuatE&#22312;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;&#65306;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#26377;&#38480;&#65292;&#38656;&#35201;&#19982;&#23454;&#20307;&#32500;&#24230;&#25104;&#27604;&#20363;&#22320;&#22686;&#21152;&#20851;&#31995;&#22823;&#23567;&#65292;&#24182;&#19988;&#38590;&#20197;&#25512;&#24191;&#21040;&#26356;&#39640;&#32500;&#24230;&#30340;&#26059;&#36716;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;OrthogonalE&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#30697;&#38453;&#34920;&#31034;&#23454;&#20307;&#21644;&#22359;&#23545;&#35282;&#27491;&#20132;&#30697;&#38453;&#19982;Riemannian&#20248;&#21270;&#34920;&#31034;&#20851;&#31995;&#12290;&#36825;&#31181;&#26041;&#27861;&#22686;&#24378;&#20102;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#30340;&#24191;&#27867;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26032;&#22411;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;OrthogonalE&#26082;&#20855;&#26377;&#24191;&#27867;&#24615;&#21448;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#65292;&#24182;&#26174;&#33879;&#20943;&#23569;&#20102;&#20851;&#31995;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The primary aim of Knowledge Graph embeddings (KGE) is to learn low-dimensional representations of entities and relations for predicting missing facts. While rotation-based methods like RotatE and QuatE perform well in KGE, they face two challenges: limited model flexibility requiring proportional increases in relation size with entity dimension, and difficulties in generalizing the model for higher-dimensional rotations. To address these issues, we introduce OrthogonalE, a novel KGE model employing matrices for entities and block-diagonal orthogonal matrices with Riemannian optimization for relations. This approach enhances the generality and flexibility of KGE models. The experimental results indicate that our new KGE model, OrthogonalE, is both general and flexible, significantly outperforming state-of-the-art KGE models while substantially reducing the number of relation parameters.
&lt;/p&gt;</description></item><item><title>Whisper-MCE&#26159;&#20351;&#29992;&#33258;&#24049;&#25910;&#38598;&#30340;&#28151;&#21512;&#31908;&#35821;&#21644;&#33521;&#35821;&#38899;&#39057;&#25968;&#25454;&#38598;&#65288;MCE&#65289;&#36827;&#34892;&#35757;&#32451;&#30340;Whisper&#27169;&#22411;&#24494;&#35843;&#65292;&#30456;&#36739;&#20110;&#22522;&#20934;&#27169;&#22411;&#65292;&#20854;&#22312;&#20934;&#30830;&#25429;&#25417;&#21407;&#22987;&#38899;&#39057;&#20869;&#23481;&#12289;&#25552;&#39640;&#35782;&#21035;&#20934;&#30830;&#24615;&#21644;&#21152;&#24555;&#35782;&#21035;&#36895;&#24230;&#26041;&#38754;&#20855;&#26377;&#26356;&#20248;&#36234;&#30340;&#33021;&#21147;&#65292;&#23588;&#20854;&#22312;&#28151;&#21512;&#35821;&#35328;&#35782;&#21035;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2310.17953</link><description>&lt;p&gt;
Whisper-MCE: &#38024;&#23545;&#28151;&#21512;&#35821;&#35328;&#23454;&#29616;&#26356;&#22909;&#24615;&#33021;&#30340;Whisper&#27169;&#22411;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Whisper-MCE: Whisper Model Finetuned for Better Performance with Mixed Languages. (arXiv:2310.17953v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17953
&lt;/p&gt;
&lt;p&gt;
Whisper-MCE&#26159;&#20351;&#29992;&#33258;&#24049;&#25910;&#38598;&#30340;&#28151;&#21512;&#31908;&#35821;&#21644;&#33521;&#35821;&#38899;&#39057;&#25968;&#25454;&#38598;&#65288;MCE&#65289;&#36827;&#34892;&#35757;&#32451;&#30340;Whisper&#27169;&#22411;&#24494;&#35843;&#65292;&#30456;&#36739;&#20110;&#22522;&#20934;&#27169;&#22411;&#65292;&#20854;&#22312;&#20934;&#30830;&#25429;&#25417;&#21407;&#22987;&#38899;&#39057;&#20869;&#23481;&#12289;&#25552;&#39640;&#35782;&#21035;&#20934;&#30830;&#24615;&#21644;&#21152;&#24555;&#35782;&#21035;&#36895;&#24230;&#26041;&#38754;&#20855;&#26377;&#26356;&#20248;&#36234;&#30340;&#33021;&#21147;&#65292;&#23588;&#20854;&#22312;&#28151;&#21512;&#35821;&#35328;&#35782;&#21035;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;Whisper&#22312;&#33521;&#35821;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#39046;&#22495;&#24050;&#32463;&#25509;&#36817;&#20110;&#20154;&#31867;&#32423;&#21035;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#20294;&#22312;&#36739;&#23567;&#35821;&#31181;&#21644;&#28151;&#21512;&#35821;&#35328;&#30340;&#35821;&#38899;&#35782;&#21035;&#20013;&#65292;&#20173;&#28982;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#32454;&#35843;&#30340;Whisper&#27169;&#22411;Whisper-MCE&#30340;&#20196;&#20154;&#30633;&#30446;&#30340;&#32467;&#26524;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#20102;&#25105;&#20204;&#33258;&#24049;&#25910;&#38598;&#30340;&#28151;&#21512;&#31908;&#35821;&#21644;&#33521;&#35821;&#38899;&#39057;&#25968;&#25454;&#38598;&#65288;MCE&#65289;&#36827;&#34892;&#35757;&#32451;&#12290;&#21516;&#26102;&#65292;&#32771;&#34385;&#21040;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#22312;&#36739;&#23567;&#35821;&#31181;&#21644;&#28151;&#21512;&#35821;&#35328;&#29615;&#22659;&#20013;&#35780;&#20272;&#20854;&#26377;&#25928;&#24615;&#26102;&#23384;&#22312;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#26426;&#21046;&#12290;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;&#22522;&#20934;&#30340;whisper-large-v2&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#20934;&#30830;&#25429;&#25417;&#21407;&#22987;&#38899;&#39057;&#20869;&#23481;&#30340;&#33021;&#21147;&#26356;&#24378;&#12289;&#35782;&#21035;&#20934;&#30830;&#24615;&#26356;&#39640;&#12289;&#35782;&#21035;&#36895;&#24230;&#26356;&#24555;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#35782;&#21035;&#28151;&#21512;&#35821;&#35328;&#30340;&#29305;&#23450;&#20219;&#21153;&#20013;&#32988;&#36807;&#20854;&#20182;&#29616;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently Whisper has approached human-level robustness and accuracy in English automatic speech recognition (ASR), while in minor language and mixed language speech recognition, there remains a compelling need for further improvement. In this work, we present the impressive results of Whisper-MCE, our finetuned Whisper model, which was trained using our self-collected dataset, Mixed Cantonese and English audio dataset (MCE). Meanwhile, considering word error rate (WER) poses challenges when it comes to evaluating its effectiveness in minor language and mixed-language contexts, we present a novel rating mechanism. By comparing our model to the baseline whisper-large-v2 model, we demonstrate its superior ability to accurately capture the content of the original audio, achieve higher recognition accuracy, and exhibit faster recognition speed. Notably, our model outperforms other existing models in the specific task of recognizing mixed language.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20174;&#26410;&#32467;&#26500;&#21270;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20013;&#26816;&#32034;&#21644;&#24635;&#32467;&#30456;&#20851;&#35777;&#25454;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#38646;&#26679;&#26412;&#26465;&#20214;&#19979;&#35757;&#32451;LLM&#26469;&#25512;&#26029;&#24739;&#32773;&#26159;&#21542;&#24739;&#26377;&#29305;&#23450;&#30142;&#30149;&#65292;&#24182;&#19988;&#27169;&#22411;&#21487;&#20197;&#24635;&#32467;&#25903;&#25345;&#30340;&#35777;&#25454;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#34987;&#35777;&#26126;&#20248;&#20110;&#20256;&#32479;&#30340;&#20449;&#24687;&#26816;&#32034;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.04550</link><description>&lt;p&gt;
&#20351;&#29992;LLMs&#20174;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#26816;&#32034;&#35777;&#25454;&#65306;&#21487;&#33021;&#24615;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Retrieving Evidence from EHRs with LLMs: Possibilities and Challenges. (arXiv:2309.04550v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20174;&#26410;&#32467;&#26500;&#21270;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20013;&#26816;&#32034;&#21644;&#24635;&#32467;&#30456;&#20851;&#35777;&#25454;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#38646;&#26679;&#26412;&#26465;&#20214;&#19979;&#35757;&#32451;LLM&#26469;&#25512;&#26029;&#24739;&#32773;&#26159;&#21542;&#24739;&#26377;&#29305;&#23450;&#30142;&#30149;&#65292;&#24182;&#19988;&#27169;&#22411;&#21487;&#20197;&#24635;&#32467;&#25903;&#25345;&#30340;&#35777;&#25454;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#34987;&#35777;&#26126;&#20248;&#20110;&#20256;&#32479;&#30340;&#20449;&#24687;&#26816;&#32034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26410;&#32467;&#26500;&#21270;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#36890;&#24120;&#21253;&#21547;&#19982;&#24433;&#20687;&#25968;&#25454;&#20114;&#34917;&#30340;&#20851;&#38190;&#20449;&#24687;&#65292;&#21487;&#20197;&#20026;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#35786;&#26029;&#25552;&#20379;&#24110;&#21161;&#12290;&#28982;&#32780;&#65292;&#26102;&#38388;&#38480;&#21046;&#21644;&#19982;&#27599;&#20010;&#24739;&#32773;&#30456;&#20851;&#30340;&#22823;&#37327;&#31508;&#35760;&#20351;&#24471;&#25163;&#21160;&#27983;&#35272;&#27492;&#31867;&#25968;&#25454;&#20197;&#35782;&#21035;&#30456;&#20851;&#35777;&#25454;&#22312;&#23454;&#36341;&#20013;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#29616;&#20195;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#26041;&#24335;&#26469;&#22788;&#29702;&#26410;&#32467;&#26500;&#21270;&#30340;EHR&#25968;&#25454;&#65292;&#24182;&#21487;&#20197;&#25552;&#20379;&#19968;&#31181;&#26426;&#21046;&#26469;&#39640;&#25928;&#22320;&#26816;&#32034;&#21644;&#24635;&#32467;&#19982;&#32473;&#23450;&#26597;&#35810;&#30456;&#20851;&#30340;&#26410;&#32467;&#26500;&#21270;&#35777;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19968;&#20010;LLM&#65288;Flan-T5 XXL&#65289;&#26469;&#23454;&#29616;&#36825;&#20010;&#30446;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#38646;&#26679;&#26412;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#35201;&#27714;LLM&#25512;&#26029;&#19968;&#20010;&#24739;&#32773;&#26159;&#21542;&#26377;&#25110;&#22788;&#20110;&#26576;&#31181;&#29305;&#23450;&#30142;&#30149;&#30340;&#39118;&#38505;&#65292;&#24182;&#22312;&#26159;&#30340;&#24773;&#20917;&#19979;&#25552;&#31034;&#27169;&#22411;&#24635;&#32467;&#25903;&#25345;&#30340;&#35777;&#25454;&#12290;&#36890;&#36807;&#24341;&#20837;&#25918;&#23556;&#31185;&#21307;&#29983;&#36827;&#34892;&#25163;&#21160;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#22522;&#20110;LLM&#30340;&#26041;&#27861;&#25552;&#20379;&#30340;&#36755;&#20986;&#22987;&#32456;&#20248;&#20110;&#26631;&#20934;&#30340;&#20449;&#24687;&#26816;&#32034;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unstructured Electronic Health Record (EHR) data often contains critical information complementary to imaging data that would inform radiologists' diagnoses. However, time constraints and the large volume of notes frequently associated with individual patients renders manual perusal of such data to identify relevant evidence infeasible in practice. Modern Large Language Models (LLMs) provide a flexible means of interacting with unstructured EHR data, and may provide a mechanism to efficiently retrieve and summarize unstructured evidence relevant to a given query. In this work, we propose and evaluate an LLM (Flan-T5 XXL) for this purpose. Specifically, in a zero-shot setting we task the LLM to infer whether a patient has or is at risk of a particular condition; if so, we prompt the model to summarize the supporting evidence. Enlisting radiologists for manual evaluation, we find that this LLM-based approach provides outputs consistently preferred to a standard information retrieval base
&lt;/p&gt;</description></item><item><title>LLaMA-E&#26159;&#19968;&#31181;&#32479;&#19968;&#19988;&#23450;&#21046;&#30340;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#30005;&#23376;&#21830;&#21153;&#21019;&#20316;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#24191;&#21578;&#29983;&#25104;&#12289;&#26597;&#35810;&#22686;&#24378;&#30340;&#20135;&#21697;&#26631;&#39064;&#25913;&#20889;&#12289;&#20135;&#21697;&#20998;&#31867;&#12289;&#36141;&#20080;&#24847;&#22270;&#25512;&#27979;&#21644;&#24120;&#35268;&#38382;&#31572;&#12290;</title><link>http://arxiv.org/abs/2308.04913</link><description>&lt;p&gt;
LLaMA-E&#65306;&#22810;&#26041;&#38754;&#25351;&#23548;&#19979;&#30340;&#30005;&#23376;&#21830;&#21153;&#21019;&#20316;&#22686;&#24378;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
LLaMA-E: Empowering E-commerce Authoring with Multi-Aspect Instruction Following. (arXiv:2308.04913v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04913
&lt;/p&gt;
&lt;p&gt;
LLaMA-E&#26159;&#19968;&#31181;&#32479;&#19968;&#19988;&#23450;&#21046;&#30340;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#30005;&#23376;&#21830;&#21153;&#21019;&#20316;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#24191;&#21578;&#29983;&#25104;&#12289;&#26597;&#35810;&#22686;&#24378;&#30340;&#20135;&#21697;&#26631;&#39064;&#25913;&#20889;&#12289;&#20135;&#21697;&#20998;&#31867;&#12289;&#36141;&#20080;&#24847;&#22270;&#25512;&#27979;&#21644;&#24120;&#35268;&#38382;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#21830;&#21153;&#21019;&#20316;&#28041;&#21450;&#21019;&#24314;&#21560;&#24341;&#20154;&#12289;&#20016;&#23500;&#19988;&#26377;&#38024;&#23545;&#24615;&#30340;&#20419;&#38144;&#20869;&#23481;&#65292;&#20197;&#25512;&#21160;&#20135;&#21697;&#38144;&#21806;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#33539;&#20363;&#65292;&#20026;&#35299;&#20915;&#36825;&#31181;&#24773;&#26223;&#20013;&#30340;&#21508;&#31181;&#21019;&#20316;&#20219;&#21153;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#36890;&#29992;&#35821;&#26009;&#24211;&#21644;&#24120;&#35782;&#30693;&#35782;&#35757;&#32451;&#30340;&#20027;&#27969;LLM&#22312;&#36866;&#24212;&#30005;&#23376;&#21830;&#21153;&#20135;&#21697;&#21644;&#23458;&#25143;&#29420;&#29305;&#30340;&#22797;&#26434;&#21644;&#20010;&#24615;&#21270;&#29305;&#24449;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#27492;&#22806;&#65292;&#20687;GPT-3.5&#36825;&#26679;&#30340;LLM&#38656;&#35201;&#36827;&#34892;&#36828;&#31243;&#35775;&#38382;&#65292;&#24341;&#21457;&#20102;&#22312;&#20256;&#36755;&#36807;&#31243;&#20013;&#20445;&#25252;&#22823;&#37327;&#23458;&#25143;&#38544;&#31169;&#25968;&#25454;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;LLaMA-E&#65292;&#38024;&#23545;&#22810;&#26679;&#21270;&#30340;&#30005;&#23376;&#21830;&#21153;&#21019;&#20316;&#20219;&#21153;&#30340;&#32479;&#19968;&#19988;&#23450;&#21046;&#30340;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#39046;&#22495;&#19987;&#23478;&#20174;&#24191;&#21578;&#29983;&#25104;&#12289;&#26597;&#35810;&#22686;&#24378;&#30340;&#20135;&#21697;&#26631;&#39064;&#25913;&#20889;&#12289;&#20135;&#21697;&#20998;&#31867;&#12289;&#36141;&#20080;&#24847;&#22270;&#25512;&#27979;&#21644;&#24120;&#35268;&#38382;&#31572;&#31561;&#20219;&#21153;&#20013;&#21019;&#24314;&#20102;&#31181;&#23376;&#25351;&#23548;&#38598;&#21512;&#12290;&#36825;&#20123;&#20219;&#21153;&#33021;&#22815;...
&lt;/p&gt;
&lt;p&gt;
E-commerce authoring involves creating attractive, abundant, and targeted promotional content to drive product sales. The emergence of large language models (LLMs) introduces an innovative paradigm, offering a unified solution to address various authoring tasks within this scenario. However, mainstream LLMs trained on general corpora with common sense knowledge reveal limitations in fitting complex and personalized features unique to e-commerce products and customers. Furthermore, LLMs like GPT-3.5 necessitate remote accessibility, raising concerns about safeguarding voluminous customer privacy data during transmission. This paper proposes the LLaMA-E, the unified and customized instruction-following language models focusing on diverse e-commerce authoring tasks. Specifically, the domain experts create the seed instruction set from the tasks of ads generation, query-enhanced product title rewriting, product classification, purchase intent speculation, and general Q&amp;A. These tasks enabl
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#35270;&#39057;&#26816;&#32034;&#27169;&#22411;&#20013;&#30340;&#32452;&#21512;&#21644;&#35821;&#20041;&#29702;&#35299;&#65292;&#24182;&#36890;&#36807;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#23545;&#35270;&#39057;&#26816;&#32034;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.16533</link><description>&lt;p&gt;
ICSVR: &#22312;&#35270;&#39057;&#26816;&#32034;&#27169;&#22411;&#20013;&#30740;&#31350;&#32452;&#21512;&#21644;&#35821;&#20041;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
ICSVR: Investigating Compositional and Semantic Understanding in Video Retrieval Models. (arXiv:2306.16533v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16533
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#35270;&#39057;&#26816;&#32034;&#27169;&#22411;&#20013;&#30340;&#32452;&#21512;&#21644;&#35821;&#20041;&#29702;&#35299;&#65292;&#24182;&#36890;&#36807;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#23545;&#35270;&#39057;&#26816;&#32034;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#26816;&#32034;&#65288;VR&#65289;&#28041;&#21450;&#26681;&#25454;&#25991;&#26412;&#26631;&#39064;&#26816;&#32034;&#35270;&#39057;&#25968;&#25454;&#24211;&#20013;&#30340;&#30495;&#23454;&#35270;&#39057;&#65292;&#25110;&#21453;&#20043;&#20134;&#28982;&#12290;&#21512;&#25104;&#24615;&#30340;&#20004;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65306;&#23545;&#35937;&#21644;&#23646;&#24615;&#20197;&#21450;&#21160;&#20316;&#65292;&#20351;&#29992;&#27491;&#30830;&#30340;&#35821;&#20041;&#32852;&#32467;&#20197;&#24418;&#25104;&#27491;&#30830;&#30340;&#25991;&#26412;&#26597;&#35810;&#12290;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#65288;&#23545;&#35937;&#21644;&#23646;&#24615;&#12289;&#21160;&#20316;&#21644;&#35821;&#20041;&#65289;&#21508;&#33258;&#22312;&#24110;&#21161;&#21306;&#20998;&#35270;&#39057;&#21644;&#26816;&#32034;&#27491;&#30830;&#30340;&#30495;&#23454;&#35270;&#39057;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#23545;&#35270;&#39057;&#26816;&#32034;&#24615;&#33021;&#30340;&#24433;&#21709;&#23578;&#19981;&#28165;&#26970;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#31995;&#32479;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#35270;&#39057;&#26816;&#32034;&#27169;&#22411;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#19978;&#23545;&#32452;&#21512;&#21644;&#35821;&#20041;&#29702;&#35299;&#30340;&#33021;&#21147;&#65292;&#22914;MSRVTT&#12289;MSVD&#21644;DIDEMO&#12290;&#35813;&#30740;&#31350;&#38024;&#23545;&#20004;&#31867;&#35270;&#39057;&#26816;&#32034;&#27169;&#22411;&#36827;&#34892;&#20102;&#65292;&#19968;&#31867;&#26159;&#22312;&#35270;&#39057;&#25991;&#26412;&#23545;&#19978;&#39044;&#35757;&#32451;&#24182;&#22312;&#19979;&#28216;&#35270;&#39057;&#26816;&#32034;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#65288;&#20363;&#22914;&#65292;Frozen-in-Time&#12289;Violet&#12289;MCQ&#31561;&#65289;&#65292;&#21478;&#19968;&#31867;&#26159;&#36866;&#24212;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#25991;&#26412;&#34920;&#31034;&#65288;&#22914;CLIP&#65289;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video retrieval (VR) involves retrieving the ground truth video from the video database given a text caption or vice-versa. The two important components of compositionality: objects \&amp; attributes and actions are joined using correct semantics to form a proper text query. These components (objects \&amp; attributes, actions and semantics) each play an important role to help distinguish among videos and retrieve the correct ground truth video. However, it is unclear what is the effect of these components on the video retrieval performance. We therefore, conduct a systematic study to evaluate the compositional and semantic understanding of video retrieval models on standard benchmarks such as MSRVTT, MSVD and DIDEMO. The study is performed on two categories of video retrieval models: (i) which are pre-trained on video-text pairs and fine-tuned on downstream video retrieval datasets (Eg. Frozen-in-Time, Violet, MCQ etc.) (ii) which adapt pre-trained image-text representations like CLIP for vid
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#23558;BERT-GPT2&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#38544;&#34255;&#31354;&#38388;&#36716;&#25442;&#20026;&#26356;&#21487;&#20998;&#31163;&#30340;&#35821;&#20041;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#27492;&#26041;&#27861;&#21487;&#20197;&#25913;&#36827;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25511;&#24615;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#26368;&#20808;&#36827;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.01713</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#35299;&#37322;&#30340;&#38750;&#20132;&#20114;&#35821;&#20041;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Learning Disentangled Semantic Spaces of Explanations via Invertible Neural Networks. (arXiv:2305.01713v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#23558;BERT-GPT2&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#38544;&#34255;&#31354;&#38388;&#36716;&#25442;&#20026;&#26356;&#21487;&#20998;&#31163;&#30340;&#35821;&#20041;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#27492;&#26041;&#27861;&#21487;&#20197;&#25913;&#36827;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25511;&#24615;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#26368;&#20808;&#36827;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32454;&#21270;&#36830;&#32493;&#31354;&#38388;&#30340;&#21477;&#23376;&#34920;&#24449;&#19978;&#36827;&#34892;&#35299;&#32806;&#21487;&#20197;&#22312;&#23450;&#20301;&#26126;&#30830;&#21457;&#29983;&#30340;&#29983;&#25104;&#22240;&#32032;&#30340;&#21516;&#26102;&#65292;&#25913;&#36827;&#21487;&#35299;&#37322;&#24615;&#21644;&#35821;&#20041;&#25511;&#21046;&#65292;&#36825;&#20026;&#22522;&#20110;&#31070;&#32463;&#30340;&#35821;&#35328;&#27169;&#22411;&#36171;&#20104;&#20102;&#19968;&#20123;&#31526;&#21495;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#28789;&#27963;&#24615;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#65288;INN&#65289;&#23558;BERT-GPT2&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#38544;&#34255;&#31354;&#38388;&#36716;&#25442;&#20026;&#26356;&#21487;&#20998;&#31163;&#30340;&#35821;&#20041;&#31354;&#38388;&#26469;&#35299;&#38500;&#32534;&#30721;&#30340;&#38544;&#34255;&#31354;&#38388;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#26032;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#27604;&#65292;INN&#33021;&#22815;&#23558;&#20998;&#24067;&#24335;&#38544;&#34255;&#31354;&#38388;&#36716;&#25442;&#20026;&#26356;&#22909;&#30340;&#35821;&#20041;&#19978;&#35299;&#32806;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#20174;&#32780;&#20135;&#29983;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25511;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Disentangling sentence representations over continuous spaces can be a critical process in improving interpretability and semantic control by localising explicit generative factors. Such process confers to neural-based language models some of the advantages that are characteristic of symbolic models, while keeping their flexibility. This work presents a methodology for disentangling the hidden space of a BERT-GPT2 autoencoder by transforming it into a more separable semantic space with the support of a flow-based invertible neural network (INN). Experimental results indicate that the INN can transform the distributed hidden space into a better semantically disentangled latent space, resulting in better interpretability and controllability, when compared to recent state-of-the-art models.
&lt;/p&gt;</description></item><item><title>&#19978;&#19979;&#25991;&#35821;&#22659;&#19979;&#35821;&#20041;&#36716;&#21464;&#26816;&#27979;&#30340;&#35745;&#31639;&#26426;&#26041;&#27861;&#19981;&#26029;&#36827;&#27493;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#21333;&#35789;&#30340;&#22810;&#37325;&#29992;&#27861;/&#24847;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21547;&#20041;&#34920;&#31034;&#12289;&#26102;&#38388;&#24863;&#30693;&#21644;&#23398;&#20064;&#27169;&#24577;&#30340;&#20998;&#31867;&#26694;&#26550;&#12290;&#35813;&#32508;&#36848;&#25506;&#35752;&#20102;&#35813;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;</title><link>http://arxiv.org/abs/2304.01666</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#35821;&#22659;&#19979;&#35821;&#20041;&#36716;&#21464;&#26816;&#27979;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Contextualised Semantic Shift Detection. (arXiv:2304.01666v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01666
&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#35821;&#22659;&#19979;&#35821;&#20041;&#36716;&#21464;&#26816;&#27979;&#30340;&#35745;&#31639;&#26426;&#26041;&#27861;&#19981;&#26029;&#36827;&#27493;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#21333;&#35789;&#30340;&#22810;&#37325;&#29992;&#27861;/&#24847;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21547;&#20041;&#34920;&#31034;&#12289;&#26102;&#38388;&#24863;&#30693;&#21644;&#23398;&#20064;&#27169;&#24577;&#30340;&#20998;&#31867;&#26694;&#26550;&#12290;&#35813;&#32508;&#36848;&#25506;&#35752;&#20102;&#35813;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#36716;&#21464;&#26816;&#27979; (SSD) &#26159;&#25351;&#35782;&#21035;&#12289;&#35299;&#37322;&#21644;&#35780;&#20272;&#30446;&#26631;&#35789;&#21487;&#33021;&#38543;&#30528;&#26102;&#38388;&#21457;&#29983;&#30340;&#24847;&#20041;&#21464;&#21270;&#30340;&#20219;&#21153;&#12290;&#20256;&#32479;&#19978;&#65292;SSD &#26159;&#30001;&#35821;&#35328;&#23398;&#23478;&#21644;&#31038;&#20250;&#31185;&#23398;&#23478;&#36890;&#36807;&#25163;&#21160;&#21644;&#32791;&#26102;&#30340;&#27963;&#21160;&#26469;&#22788;&#29702;&#30340;&#12290;&#36817;&#24180;&#26469;&#65292;&#35745;&#31639;&#26426;&#26041;&#27861;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35789;&#23884;&#20837;&#25216;&#26415;&#24471;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20197;&#23613;&#21487;&#33021;&#22320;&#33258;&#21160;&#21270; SSD&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#36807;&#21435;&#30340;&#19977;&#24180;&#20013;&#65292;&#20960;&#20046;&#23436;&#20840;&#22522;&#20110;&#35789;&#27719;&#30340;&#19978;&#19979;&#25991;&#23884;&#20837;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#36827;&#23637;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#22788;&#29702;&#21333;&#35789;&#30340;&#22810;&#31181;&#29992;&#27861;/&#24847;&#20041;&#65292;&#24182;&#26356;&#22909;&#22320;&#25429;&#25417;&#30456;&#20851;&#30340;&#35821;&#20041;&#36716;&#21464;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#19978;&#19979;&#25991;&#23884;&#20837;&#30340; SSD &#26041;&#27861;&#65288;&#21363; CSSDetection&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#21547;&#20041;&#34920;&#31034;&#12289;&#26102;&#38388;&#24863;&#30693;&#21644;&#23398;&#20064;&#27169;&#24577;&#32500;&#24230;&#20026;&#29305;&#24449;&#30340;&#20998;&#31867;&#26694;&#26550;&#12290;&#21033;&#29992;&#35813;&#26694;&#26550;&#65292;&#25105;&#20204;&#23545;&#36716;&#21464;&#35780;&#20272;&#25514;&#26045;&#36827;&#34892;&#20102;&#22238;&#39038;&#65292;&#24182;&#30830;&#23450;&#20102;&#19978;&#19979;&#25991;&#21270; SSD &#39046;&#22495;&#26410;&#26469;&#30740;&#31350;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic Shift Detection (SSD) is the task of identifying, interpreting, and assessing the possible change over time in the meanings of a target word. Traditionally, SSD has been addressed by linguists and social scientists through manual and time-consuming activities. In the recent years, computational approaches based on Natural Language Processing and word embeddings gained increasing attention to automate SSD as much as possible. In particular, over the past three years, significant advancements have been made almost exclusively based on word contextualised embedding models, which can handle the multiple usages/meanings of the words and better capture the related semantic shifts. In this paper, we survey the approaches based on contextualised embeddings for SSD (i.e., CSSDetection) and we propose a classification framework characterised by meaning representation, time-awareness, and learning modality dimensions. The framework is exploited i) to review the measures for shift assessm
&lt;/p&gt;</description></item></channel></rss>