<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#36825;&#31687;&#35770;&#25991;&#35777;&#26126;&#20102;Transformer&#27169;&#22411;&#22312;&#22797;&#21046;&#20219;&#21153;&#19978;&#30340;&#20248;&#21183;&#65292;&#30456;&#27604;&#20110;&#20351;&#29992;&#22266;&#23450;&#28508;&#22312;&#29366;&#24577;&#30340;&#24191;&#20041;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;Transformer&#27169;&#22411;&#33021;&#22815;&#26356;&#39640;&#25928;&#22320;&#22797;&#21046;&#21644;&#26816;&#32034;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01032</link><description>&lt;p&gt;
&#36319;&#30528;&#25105;&#37325;&#22797;&#65306;Transformer&#22312;&#22797;&#21046;&#20219;&#21153;&#19978;&#27604;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#26356;&#22909;
&lt;/p&gt;
&lt;p&gt;
Repeat After Me: Transformers are Better than State Space Models at Copying
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01032
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35777;&#26126;&#20102;Transformer&#27169;&#22411;&#22312;&#22797;&#21046;&#20219;&#21153;&#19978;&#30340;&#20248;&#21183;&#65292;&#30456;&#27604;&#20110;&#20351;&#29992;&#22266;&#23450;&#28508;&#22312;&#29366;&#24577;&#30340;&#24191;&#20041;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;Transformer&#27169;&#22411;&#33021;&#22815;&#26356;&#39640;&#25928;&#22320;&#22797;&#21046;&#21644;&#26816;&#32034;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26159;&#24207;&#21015;&#24314;&#27169;&#30340;&#20027;&#35201;&#26550;&#26500;&#65292;&#20294;&#23545;&#20110;&#20351;&#29992;&#19981;&#20381;&#36182;&#20110;&#24207;&#21015;&#38271;&#24230;&#30340;&#22266;&#23450;&#22823;&#23567;&#28508;&#22312;&#29366;&#24577;&#30340;&#27169;&#22411;&#65292;&#20063;&#23601;&#26159;"&#24191;&#20041;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;" (GSSMs)&#65292;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#34429;&#28982;GSSMs&#22312;&#25512;&#29702;&#26102;&#38388;&#25928;&#29575;&#19978;&#26377;&#20248;&#21183;&#65292;&#20294;&#22312;&#38656;&#35201;&#20174;&#36755;&#20837;&#19978;&#19979;&#25991;&#22797;&#21046;&#30340;&#20219;&#21153;&#19978;&#65292;&#23427;&#20204;&#30456;&#23545;&#20110;transformer&#27169;&#22411;&#26469;&#35828;&#26377;&#38480;&#21046;&#12290;&#25105;&#20204;&#20174;&#23545;&#31616;&#21333;&#30340;&#23383;&#31526;&#20018;&#22797;&#21046;&#20219;&#21153;&#30340;&#29702;&#35770;&#20998;&#26512;&#24320;&#22987;&#65292;&#24182;&#35777;&#26126;&#20102;&#19968;&#20010;&#20004;&#23618;&#30340;transformer&#21487;&#20197;&#22797;&#21046;&#25351;&#25968;&#38271;&#24230;&#30340;&#23383;&#31526;&#20018;&#65292;&#32780;GSSMs&#30001;&#20110;&#20854;&#22266;&#23450;&#22823;&#23567;&#30340;&#28508;&#22312;&#29366;&#24577;&#22312;&#26681;&#26412;&#19978;&#26159;&#26377;&#38480;&#21046;&#30340;&#12290;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;transformer&#22312;&#38656;&#35201;&#22797;&#21046;&#19978;&#19979;&#25991;&#30340;&#21512;&#25104;&#20219;&#21153;&#20013;&#65292;&#22312;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#19978;&#20248;&#20110;GSSMs&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;transformer&#27169;&#22411;&#22312;&#22797;&#21046;&#21644;&#26816;&#32034;&#19978;&#19979;&#25991;&#20449;&#24687;&#26041;&#38754;&#36828;&#36828;&#20248;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers are the dominant architecture for sequence modeling, but there is growing interest in models that use a fixed-size latent state that does not depend on the sequence length, which we refer to as "generalized state space models" (GSSMs). In this paper we show that while GSSMs are promising in terms of inference-time efficiency, they are limited compared to transformer models on tasks that require copying from the input context. We start with a theoretical analysis of the simple task of string copying and prove that a two layer transformer can copy strings of exponential length while GSSMs are fundamentally limited by their fixed-size latent state. Empirically, we find that transformers outperform GSSMs in terms of efficiency and generalization on synthetic tasks that require copying the context. Finally, we evaluate pretrained large language models and find that transformer models dramatically outperform state space models at copying and retrieving information from context. 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20809;&#35889;&#21367;&#31215;&#21464;&#21387;&#22120; (SCT)&#65292;&#36890;&#36807;&#32467;&#21512;&#23616;&#37096;&#20449;&#24687;&#30340;&#21367;&#31215;&#25805;&#20316;&#21644;&#20840;&#23616;&#20449;&#24687;&#30340;&#22797;&#26434;&#20613;&#37324;&#21494;&#22522;&#30784;&#65292;&#23454;&#29616;&#20102;&#23545;&#35270;&#35273;&#21464;&#21387;&#22120;&#20013;&#23454;&#37096;&#21644;&#22797;&#37096;&#22810;&#35270;&#22270;&#20809;&#35889;&#31639;&#23376;&#30340;&#21327;&#35843;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.18063</link><description>&lt;p&gt;
&#20809;&#35889;&#21367;&#31215;&#21464;&#21387;&#22120;&#65306;&#21327;&#35843;&#35270;&#35273;&#21464;&#21387;&#22120;&#20013;&#30340;&#23454;&#37096;&#21644;&#22797;&#37096;&#22810;&#35270;&#22270;&#20809;&#35889;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Spectral Convolutional Transformer: Harmonizing Real vs. Complex Multi-View Spectral Operators for Vision Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18063
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20809;&#35889;&#21367;&#31215;&#21464;&#21387;&#22120; (SCT)&#65292;&#36890;&#36807;&#32467;&#21512;&#23616;&#37096;&#20449;&#24687;&#30340;&#21367;&#31215;&#25805;&#20316;&#21644;&#20840;&#23616;&#20449;&#24687;&#30340;&#22797;&#26434;&#20613;&#37324;&#21494;&#22522;&#30784;&#65292;&#23454;&#29616;&#20102;&#23545;&#35270;&#35273;&#21464;&#21387;&#22120;&#20013;&#23454;&#37096;&#21644;&#22797;&#37096;&#22810;&#35270;&#22270;&#20809;&#35889;&#31639;&#23376;&#30340;&#21327;&#35843;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#20013;&#20351;&#29992;&#30340;Transformer&#24050;&#32463;&#36890;&#36807;&#21508;&#31181;&#32467;&#26500;&#36827;&#34892;&#20102;&#30740;&#31350; - &#22914;ViT&#12289;PVT&#21644;Swin&#12290;&#36825;&#20123;&#24037;&#20316;&#26088;&#22312;&#25913;&#36827;&#27880;&#24847;&#21147;&#26426;&#21046;&#24182;&#20351;&#20854;&#26356;&#21152;&#39640;&#25928;&#12290;&#19982;&#27492;&#19981;&#21516;&#30340;&#26159;&#65292;&#20154;&#20204;&#24863;&#21463;&#21040;&#20102;&#21253;&#21547;&#23616;&#37096;&#20449;&#24687;&#30340;&#38656;&#35201;&#65292;&#36825;&#23548;&#33268;&#22312;Transformer&#20013;&#24341;&#20837;&#21367;&#31215;&#65292;&#22914;CPVT&#21644;CvT&#12290;&#25105;&#20204;&#20351;&#29992;&#22797;&#26434;&#20613;&#31435;&#21494;&#22522;&#30784;&#25429;&#25417;&#20840;&#23616;&#20449;&#24687;&#65292;&#36890;&#36807;&#21508;&#31181;&#26041;&#27861;&#65292;&#22914;AFNO&#12289;GFNet&#21644;Spectformer&#23454;&#29616;&#20840;&#23616;&#20196;&#29260;&#28151;&#21512;&#12290;&#25105;&#20204;&#25552;&#20513;&#32467;&#21512;&#25968;&#25454;&#30340;&#19977;&#31181;&#19981;&#21516;&#35270;&#22270; - &#23616;&#37096;&#12289;&#20840;&#23616;&#21644;&#38271;&#31243;&#20381;&#36182;&#24615;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20165;&#20351;&#29992;&#23454;&#22495;&#20809;&#35889;&#34920;&#31034;&#30340;&#26368;&#31616;&#21333;&#20840;&#23616;&#34920;&#31034; - &#36890;&#36807;Hartley&#21464;&#25442;&#33719;&#24471;&#12290;&#25105;&#20204;&#22312;&#21021;&#22987;&#23618;&#20013;&#20351;&#29992;&#21367;&#31215;&#31639;&#23376;&#25429;&#25417;&#23616;&#37096;&#20449;&#24687;&#12290;&#36890;&#36807;&#36825;&#20004;&#20010;&#36129;&#29486;&#65292;&#25105;&#20204;&#33021;&#22815;&#20248;&#21270;&#24182;&#33719;&#24471;&#19968;&#20010;&#25552;&#20379;&#25913;&#36827;&#24615;&#33021;&#30340;&#20809;&#35889;&#21367;&#31215;&#21464;&#21387;&#22120;&#65288;SCT&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18063v1 Announce Type: cross  Abstract: Transformers used in vision have been investigated through diverse architectures - ViT, PVT, and Swin. These have worked to improve the attention mechanism and make it more efficient. Differently, the need for including local information was felt, leading to incorporating convolutions in transformers such as CPVT and CvT. Global information is captured using a complex Fourier basis to achieve global token mixing through various methods, such as AFNO, GFNet, and Spectformer. We advocate combining three diverse views of data - local, global, and long-range dependence. We also investigate the simplest global representation using only the real domain spectral representation - obtained through the Hartley transform. We use a convolutional operator in the initial layers to capture local information. Through these two contributions, we are able to optimize and obtain a spectral convolution transformer (SCT) that provides improved performance 
&lt;/p&gt;</description></item><item><title>&#37327;&#21270;&#30446;&#21069;&#27604;&#21098;&#26525;&#26356;&#26377;&#25928;&#65292;&#21487;&#20197;&#21516;&#26102;&#23454;&#29616;&#25928;&#29575;&#21644;&#21487;&#20449;&#24230;&#65292;&#20294;&#21098;&#26525;&#20250;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;</title><link>https://arxiv.org/abs/2403.15447</link><description>&lt;p&gt;
&#35299;&#30721;&#21387;&#32553;&#30340;&#20449;&#20219;&#65306;&#23457;&#35270;&#22312;&#21387;&#32553;&#19979;&#39640;&#25928;LLMs&#30340;&#21487;&#20449;&#24230;
&lt;/p&gt;
&lt;p&gt;
Decoding Compressed Trust: Scrutinizing the Trustworthiness of Efficient LLMs Under Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15447
&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#30446;&#21069;&#27604;&#21098;&#26525;&#26356;&#26377;&#25928;&#65292;&#21487;&#20197;&#21516;&#26102;&#23454;&#29616;&#25928;&#29575;&#21644;&#21487;&#20449;&#24230;&#65292;&#20294;&#21098;&#26525;&#20250;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#39640;&#24615;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21387;&#32553;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#36164;&#28304;&#39640;&#25928;&#25512;&#26029;&#30340;&#39318;&#36873;&#31574;&#30053;&#12290;&#23613;&#31649;&#26368;&#20808;&#36827;&#30340;&#21387;&#32553;&#26041;&#27861;&#22312;&#20445;&#30041;&#33391;&#24615;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36827;&#23637;&#65292;&#20294;&#21387;&#32553;&#22312;&#23433;&#20840;&#24615;&#21644;&#21487;&#20449;&#24230;&#26041;&#38754;&#30340;&#28508;&#22312;&#39118;&#38505;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#34987;&#24573;&#35270;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#20351;&#29992;&#20116;&#31181;&#26368;&#20808;&#36827;&#21387;&#32553;&#25216;&#26415;&#35780;&#20272;&#19977;&#31181;&#39046;&#20808;LLMs&#30340;&#21487;&#20449;&#24230;&#32500;&#24230;&#36827;&#34892;&#20102;&#39318;&#27425;&#24443;&#24213;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#31361;&#20986;&#20102;&#21387;&#32553;&#19982;&#21487;&#20449;&#24230;&#20043;&#38388;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#25581;&#31034;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#27169;&#24335;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30446;&#21069;&#37327;&#21270;&#27604;&#21098;&#26525;&#26356;&#26377;&#25928;&#22320;&#21516;&#26102;&#23454;&#29616;&#25928;&#29575;&#21644;&#21487;&#20449;&#24230;&#12290;&#20363;&#22914;&#65292;4&#20301;&#37327;&#21270;&#27169;&#22411;&#20445;&#30041;&#20102;&#20854;&#21407;&#22987;&#23545;&#24212;&#29289;&#30340;&#21487;&#20449;&#24230;&#65292;&#20294;&#27169;&#22411;&#21098;&#26525;&#26174;&#33879;&#38477;&#20302;&#20102;&#21487;&#20449;&#24230;&#65292;&#21363;&#20351;&#22312;50%&#30340;&#31232;&#30095;&#24230;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15447v1 Announce Type: cross  Abstract: Compressing high-capability Large Language Models (LLMs) has emerged as a favored strategy for resource-efficient inferences. While state-of-the-art (SoTA) compression methods boast impressive advancements in preserving benign task performance, the potential risks of compression in terms of safety and trustworthiness have been largely neglected. This study conducts the first, thorough evaluation of three (3) leading LLMs using five (5) SoTA compression techniques across eight (8) trustworthiness dimensions. Our experiments highlight the intricate interplay between compression and trustworthiness, revealing some interesting patterns. We find that quantization is currently a more effective approach than pruning in achieving efficiency and trustworthiness simultaneously. For instance, a 4-bit quantized model retains the trustworthiness of its original counterpart, but model pruning significantly degrades trustworthiness, even at 50% spars
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;M$^3$AV&#38899;&#35270;&#39057;&#23398;&#26415;&#35762;&#24231;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#22810;&#27169;&#24577;&#12289;&#22810;&#20307;&#35009;&#21644;&#39640;&#36136;&#37327;&#20154;&#24037;&#27880;&#37322;&#65292;&#21487;&#29992;&#20110;&#22810;&#31181;&#38899;&#35270;&#39057;&#35782;&#21035;&#20219;&#21153;</title><link>https://arxiv.org/abs/2403.14168</link><description>&lt;p&gt;
M$^3$AV&#65306;&#19968;&#31181;&#22810;&#27169;&#24577;&#12289;&#22810;&#20307;&#35009;&#21644;&#22810;&#29992;&#36884;&#30340;&#38899;&#35270;&#39057;&#23398;&#26415;&#35762;&#24231;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
M$^3$AV: A Multimodal, Multigenre, and Multipurpose Audio-Visual Academic Lecture Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14168
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;M$^3$AV&#38899;&#35270;&#39057;&#23398;&#26415;&#35762;&#24231;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#22810;&#27169;&#24577;&#12289;&#22810;&#20307;&#35009;&#21644;&#39640;&#36136;&#37327;&#20154;&#24037;&#27880;&#37322;&#65292;&#21487;&#29992;&#20110;&#22810;&#31181;&#38899;&#35270;&#39057;&#35782;&#21035;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14168v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#21457;&#24067;&#24320;&#28304;&#23398;&#26415;&#35270;&#39057;&#24405;&#20687;&#26159;&#22312;&#32447;&#20998;&#20139;&#30693;&#35782;&#30340;&#19968;&#31181;&#26032;&#20852;&#21644;&#26222;&#36941;&#26041;&#27861;&#12290;&#36825;&#20123;&#35270;&#39057;&#21253;&#21547;&#20016;&#23500;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#21253;&#25324;&#28436;&#35762;&#32773;&#30340;&#35821;&#38899;&#12289;&#38754;&#37096;&#21644;&#36523;&#20307;&#21160;&#20316;&#65292;&#20197;&#21450;&#24187;&#28783;&#29255;&#20013;&#30340;&#25991;&#26412;&#21644;&#22270;&#29255;&#65292;&#29978;&#33267;&#21487;&#33021;&#21253;&#25324;&#35770;&#25991;&#20869;&#23481;&#12290;&#23613;&#31649;&#24050;&#26500;&#24314;&#21644;&#21457;&#24067;&#20102;&#22810;&#20010;&#23398;&#26415;&#35270;&#39057;&#25968;&#25454;&#38598;&#65292;&#20294;&#24456;&#23569;&#26377;&#25968;&#25454;&#38598;&#25903;&#25345;&#22810;&#27169;&#24577;&#20869;&#23481;&#35782;&#21035;&#21644;&#29702;&#35299;&#20219;&#21153;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#32570;&#20047;&#39640;&#36136;&#37327;&#30340;&#20154;&#24037;&#27880;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#12289;&#22810;&#20307;&#35009;&#21644;&#22810;&#29992;&#36884;&#30340;&#38899;&#35270;&#39057;&#23398;&#26415;&#35762;&#24231;&#25968;&#25454;&#38598;(M$^3$AV)&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#26469;&#33258;&#20116;&#20010;&#26469;&#28304;&#30340;&#36817;367&#23567;&#26102;&#30340;&#35270;&#39057;&#65292;&#28085;&#30422;&#35745;&#31639;&#26426;&#31185;&#23398;&#12289;&#25968;&#23398;&#20197;&#21450;&#21307;&#23398;&#21644;&#29983;&#29289;&#23398;&#31561;&#20027;&#39064;&#12290;&#36890;&#36807;&#23545;&#35328;&#35821;&#21644;&#20070;&#38754;&#25991;&#23383;&#65288;&#23588;&#20854;&#26159;&#39640;&#20215;&#20540;&#21517;&#31216;&#23454;&#20307;&#65289;&#30340;&#39640;&#36136;&#37327;&#20154;&#24037;&#27880;&#37322;&#65292;&#35813;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#22810;&#31181;&#38899;&#35270;&#39057;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14168v1 Announce Type: new  Abstract: Publishing open-source academic video recordings is an emergent and prevalent approach to sharing knowledge online. Such videos carry rich multimodal information including speech, the facial and body movements of the speakers, as well as the texts and pictures in the slides and possibly even the papers. Although multiple academic video datasets have been constructed and released, few of them support both multimodal content recognition and understanding tasks, which is partially due to the lack of high-quality human annotations. In this paper, we propose a novel multimodal, multigenre, and multipurpose audio-visual academic lecture dataset (M$^3$AV), which has almost 367 hours of videos from five sources covering computer science, mathematics, and medical and biology topics. With high-quality human annotations of the spoken and written words, in particular high-valued name entities, the dataset can be used for multiple audio-visual recogn
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#27700;&#21360;&#26816;&#27979;&#26041;&#27861;&#65292;&#22312;&#26816;&#27979;&#36807;&#31243;&#20013;&#26681;&#25454;&#20196;&#29260;&#30340;&#29109;&#35843;&#25972;&#20854;&#26435;&#37325;&#65292;&#20197;&#26356;&#22909;&#22320;&#21453;&#26144;&#27700;&#21360;&#31243;&#24230;&#65292;&#35813;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20855;&#26377;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.13485</link><description>&lt;p&gt;
&#22522;&#20110;&#29109;&#30340;&#25991;&#26412;&#27700;&#21360;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Entropy-based Text Watermarking Detection Method
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13485
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#27700;&#21360;&#26816;&#27979;&#26041;&#27861;&#65292;&#22312;&#26816;&#27979;&#36807;&#31243;&#20013;&#26681;&#25454;&#20196;&#29260;&#30340;&#29109;&#35843;&#25972;&#20854;&#26435;&#37325;&#65292;&#20197;&#26356;&#22909;&#22320;&#21453;&#26144;&#27700;&#21360;&#31243;&#24230;&#65292;&#35813;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20855;&#26377;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25991;&#26412;&#27700;&#21360;&#31639;&#27861;&#33021;&#22815;&#23884;&#20837;&#38544;&#34255;&#29305;&#24449;&#21040;LLMs&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#65292;&#20197;&#20415;&#21518;&#32493;&#26816;&#27979;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;LLMs&#34987;&#35823;&#29992;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#24403;&#21069;&#30340;&#25991;&#26412;&#27700;&#21360;&#31639;&#27861;&#22312;&#22823;&#22810;&#25968;&#39640;&#29109;&#24773;&#20917;&#19979;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#20302;&#29109;&#24773;&#20917;&#19979;&#20173;&#38656;&#35201;&#25913;&#36827;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#27700;&#21360;&#26816;&#27979;&#36807;&#31243;&#20013;&#24212;&#20840;&#38754;&#32771;&#34385;&#20196;&#29260;&#29109;&#30340;&#24433;&#21709;&#65292;&#21363;&#24212;&#26681;&#25454;&#20854;&#29109;&#35843;&#25972;&#27599;&#20010;&#20196;&#29260;&#30340;&#37325;&#37327;&#65292;&#32780;&#19981;&#26159;&#20687;&#20197;&#21069;&#30340;&#26041;&#27861;&#20013;&#23558;&#25152;&#26377;&#20196;&#29260;&#30340;&#37325;&#37327;&#35774;&#32622;&#20026;&#30456;&#21516;&#20540;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#27700;&#21360;&#26816;&#27979;&#65288;EWD&#65289;&#65292;&#22312;&#27700;&#21360;&#26816;&#27979;&#36807;&#31243;&#20013;&#36171;&#20104;&#39640;&#29109;&#20196;&#29260;&#26356;&#39640;&#30340;&#26435;&#37325;&#65292;&#20197;&#26356;&#22909;&#22320;&#21453;&#26144;&#27700;&#21360;&#31243;&#24230;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#26816;&#27979;&#36807;&#31243;&#26080;&#38656;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13485v1 Announce Type: new  Abstract: Currently, text watermarking algorithms for large language models (LLMs) can embed hidden features to texts generated by LLMs to facilitate subsequent detection, thus alleviating the problem of misuse of LLMs. Although the current text watermarking algorithms perform well in most high-entropy scenarios, its performance in low-entropy scenarios still needs to be improved. In this work, we proposed that the influence of token entropy should be fully considered in the watermark detection process, that is, the weight of each token should be adjusted according to its entropy during watermark detection, rather than setting the weight of all tokens to the same value as in previous methods. Specifically, we proposed an Entropy-based Watermark Detection (EWD) that gives higher-entropy tokens higher weights during watermark detection, so as to better reflect the degree of watermarking. Furthermore, the proposed detection process is training-free a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;CantonMT&#39033;&#30446;&#65292;&#21033;&#29992;&#21512;&#25104;&#21453;&#21521;&#32763;&#35793;&#25968;&#25454;&#23545;&#31908;&#35821;&#33267;&#33521;&#35821;NMT&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#21644;&#24320;&#28304;&#24037;&#20855;&#21253;&#65292;&#20197;&#20419;&#36827;&#30740;&#31350;</title><link>https://arxiv.org/abs/2403.11346</link><description>&lt;p&gt;
CantonMT: &#27721;&#33521;NMT&#24179;&#21488;&#65292;&#20351;&#29992;&#21512;&#25104;&#21453;&#21521;&#32763;&#35793;&#25968;&#25454;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
CantonMT: Cantonese to English NMT Platform with Fine-Tuned Models Using Synthetic Back-Translation Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11346
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;CantonMT&#39033;&#30446;&#65292;&#21033;&#29992;&#21512;&#25104;&#21453;&#21521;&#32763;&#35793;&#25968;&#25454;&#23545;&#31908;&#35821;&#33267;&#33521;&#35821;NMT&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#21644;&#24320;&#28304;&#24037;&#20855;&#21253;&#65292;&#20197;&#20419;&#36827;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11346v1 &#28040;&#24687;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;(NMT)&#20173;&#28982;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20154;&#21592;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#19968;&#20010;&#26631;&#20934;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#8212;&#8212;&#21453;&#21521;&#32763;&#35793;&#65292;&#24212;&#29992;&#21040;&#20102;&#26032;&#30340;&#35821;&#35328;&#32763;&#35793;&#26041;&#21521;&#31908;&#35821;&#33267;&#33521;&#35821;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#25105;&#20204;&#20351;&#29992;&#26377;&#38480;&#25968;&#37327;&#30495;&#23454;&#25968;&#25454;&#21644;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;(&#21253;&#25324;OpusMT, NLLB,&#21644;mBART)&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31995;&#21015;&#19981;&#21516;&#25351;&#26631;&#21253;&#25324;&#22522;&#20110;&#35789;&#27719;&#21644;&#23884;&#20837;&#30340;&#33258;&#21160;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#36825;&#39033;\textsc{CantonMT}&#30740;&#31350;&#39033;&#30446;&#20013;&#21253;&#21547;&#30340;&#27169;&#22411;&#21019;&#24314;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#65292;&#24182;&#25552;&#20379;&#20415;&#21033;&#23454;&#29616;&#31908;&#35821;&#33267;&#33521;&#35821;MT&#30740;&#31350;&#12290;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36890;&#36807;&#25105;&#20204;&#30340;&#24320;&#28304;\textsc{CantonMT}&#24037;&#20855;&#21253;\url{https://github.com/kenrickkung/CantoneseTranslation}&#21521;&#24179;&#21488;&#28155;&#21152;&#26356;&#22810;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11346v1 Announce Type: cross  Abstract: Neural Machine Translation (NMT) for low-resource languages is still a challenging task in front of NLP researchers. In this work, we deploy a standard data augmentation methodology by back-translation to a new language translation direction Cantonese-to-English. We present the models we fine-tuned using the limited amount of real data and the synthetic data we generated using back-translation including OpusMT, NLLB, and mBART. We carried out automatic evaluation using a range of different metrics including lexical-based and embedding-based. Furthermore. we create a user-friendly interface for the models we included in this\textsc{ CantonMT} research project and make it available to facilitate Cantonese-to-English MT research. Researchers can add more models into this platform via our open-source\textsc{ CantonMT} toolkit \url{https://github.com/kenrickkung/CantoneseTranslation}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36873;&#25321;&#24615;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20351;&#20195;&#30721;LM&#33021;&#22815;&#36991;&#20813;&#19981;&#24517;&#35201;&#30340;&#26816;&#32034;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#19978;&#22987;&#32456;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.10059</link><description>&lt;p&gt;
Repoformer&#65306;&#38754;&#21521;&#23384;&#20648;&#24211;&#32423;&#20195;&#30721;&#34917;&#20840;&#30340;&#36873;&#25321;&#24615;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Repoformer: Selective Retrieval for Repository-Level Code Completion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36873;&#25321;&#24615;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20351;&#20195;&#30721;LM&#33021;&#22815;&#36991;&#20813;&#19981;&#24517;&#35201;&#30340;&#26816;&#32034;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#19978;&#22987;&#32456;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10059v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#25991;&#25688;&#65306;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#24320;&#21551;&#20102;&#23384;&#20648;&#24211;&#32423;&#20195;&#30721;&#34917;&#20840;&#30340;&#26032;&#26102;&#20195;&#12290;&#20294;&#26159;&#65292;&#29616;&#26377;&#26041;&#27861;&#20013;&#26816;&#32034;&#30340;&#19981;&#21464;&#20351;&#29992;&#26292;&#38706;&#20102;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#22823;&#37096;&#20998;&#26816;&#32034;&#21040;&#30340;&#19978;&#19979;&#25991;&#23545;&#20110;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#65288;code LM&#65289;&#26469;&#35828;&#26082;&#26080;&#25928;&#21448;&#26377;&#23475;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36873;&#25321;&#24615;RAG&#26694;&#26550;&#65292;&#22312;&#19981;&#24517;&#35201;&#26102;&#36991;&#20813;&#20351;&#29992;&#26816;&#32034;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#19968;&#26694;&#26550;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#20195;&#30721;LM&#33021;&#22815;&#20934;&#30830;&#33258;&#25105;&#35780;&#20272;&#26816;&#32034;&#26159;&#21542;&#21487;&#20197;&#25552;&#39640;&#20854;&#36755;&#20986;&#36136;&#37327;&#65292;&#24182;&#33021;&#22815;&#31283;&#20581;&#22320;&#21033;&#29992;&#28508;&#22312;&#21547;&#22122;&#22768;&#30340;&#26816;&#32034;&#19978;&#19979;&#25991;&#12290;&#20351;&#29992;&#36825;&#31181;LM&#20316;&#20026;&#36873;&#25321;&#24615;&#26816;&#32034;&#31574;&#30053;&#21644;&#29983;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#21253;&#25324;RepoEval&#12289;CrossCodeEval&#21644;&#19968;&#20010;&#26032;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10059v1 Announce Type: cross  Abstract: Recent advances in retrieval-augmented generation (RAG) have initiated a new era in repository-level code completion. However, the invariable use of retrieval in existing methods exposes issues in both efficiency and robustness, with a large proportion of the retrieved contexts proving unhelpful or harmful to code language models (code LMs). To tackle the challenges, this paper proposes a selective RAG framework where retrieval is avoided when unnecessary. To power this framework, we design a self-supervised learning approach that enables a code LM to accurately self-evaluate whether retrieval can improve its output quality and robustly leverage the potentially noisy retrieved contexts. Using this LM as both the selective retrieval policy and the generation model, our framework consistently outperforms the state-of-the-art prompting with an invariable retrieval approach on diverse benchmarks including RepoEval, CrossCodeEval, and a new
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32622;&#20449;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#24605;&#21644;&#25552;&#20379;&#22810;&#20010;&#20505;&#36873;&#31572;&#26696;&#30340;&#29702;&#30001;&#26469;&#35299;&#20915;&#23545;&#19981;&#27491;&#30830;&#31572;&#26696;&#30340;&#36807;&#24230;&#33258;&#20449;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.09972</link><description>&lt;p&gt;
&#22312;&#25215;&#35834;&#20043;&#21069;&#19977;&#24605;&#65306;&#36890;&#36807;&#21453;&#24605;&#22810;&#20010;&#31572;&#26696;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;
&lt;/p&gt;
&lt;p&gt;
Think Twice Before Assure: Confidence Estimation for Large Language Models through Reflection on Multiple Answers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09972
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32622;&#20449;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#24605;&#21644;&#25552;&#20379;&#22810;&#20010;&#20505;&#36873;&#31572;&#26696;&#30340;&#29702;&#30001;&#26469;&#35299;&#20915;&#23545;&#19981;&#27491;&#30830;&#31572;&#26696;&#30340;&#36807;&#24230;&#33258;&#20449;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32622;&#20449;&#24230;&#20272;&#35745;&#26088;&#22312;&#35780;&#20272;&#36755;&#20986;&#30340;&#21487;&#20449;&#24230;&#65292;&#22312;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26102;&#33267;&#20851;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#40657;&#30418;&#27169;&#22411;&#12290;&#30001;&#20110;LLM&#22312;&#29983;&#25104;&#19981;&#27491;&#30830;&#31572;&#26696;&#26102;&#30340;&#36807;&#24230;&#33258;&#20449;&#65292;&#29616;&#26377;&#23545;LLM&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#36890;&#24120;&#19981;&#21487;&#26657;&#20934;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#21463;&#21040;&#19968;&#20010;&#26174;&#33879;&#38480;&#21046;&#30340;&#38459;&#30861;&#65292;&#21363;&#23427;&#20204;&#20165;&#32771;&#34385;LLM&#29983;&#25104;&#30340;&#19968;&#20010;&#31572;&#26696;&#30340;&#32622;&#20449;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#33539;&#24335;&#65292;&#24443;&#24213;&#35780;&#20272;&#22810;&#20010;&#20505;&#36873;&#31572;&#26696;&#30340;&#21487;&#20449;&#24230;&#65292;&#20197;&#20943;&#36731;&#23545;&#19981;&#27491;&#30830;&#31572;&#26696;&#30340;&#36807;&#24230;&#33258;&#20449;&#12290;&#22522;&#20110;&#36825;&#19968;&#33539;&#24335;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20004;&#27493;&#26694;&#26550;&#65292;&#39318;&#20808;&#25351;&#23548;LLM&#21453;&#24605;&#24182;&#20026;&#27599;&#20010;&#31572;&#26696;&#25552;&#20379;&#29702;&#30001;&#65292;&#28982;&#21518;&#27719;&#24635;&#36825;&#20123;&#29702;&#30001;&#36827;&#34892;&#32508;&#21512;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#12290;&#36825;&#19968;&#26694;&#26550;&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#26041;&#27861;&#30456;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09972v1 Announce Type: new  Abstract: Confidence estimation aiming to evaluate output trustability is crucial for the application of large language models (LLM), especially the black-box ones. Existing confidence estimation of LLM is typically not calibrated due to the overconfidence of LLM on its generated incorrect answers. Existing approaches addressing the overconfidence issue are hindered by a significant limitation that they merely consider the confidence of one answer generated by LLM. To tackle this limitation, we propose a novel paradigm that thoroughly evaluates the trustability of multiple candidate answers to mitigate the overconfidence on incorrect answers. Building upon this paradigm, we introduce a two-step framework, which firstly instructs LLM to reflect and provide justifications for each answer, and then aggregates the justifications for comprehensive confidence estimation. This framework can be integrated with existing confidence estimation approaches for
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#20102;&#24403;&#21069;&#35270;&#35273;&#19982;&#35821;&#35328;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#31867;&#22312;&#21487;&#33021;&#26631;&#31614;&#30340;&#20998;&#24067;&#19978;&#26174;&#31034;&#20986;&#26497;&#22823;&#20027;&#35266;&#21464;&#24322;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#35270;&#35273;&#23545;&#35937;&#30340;&#21629;&#21517;&#12289;&#25551;&#36848;&#21644;&#37327;&#21270;&#30340;&#33021;&#21147;</title><link>https://arxiv.org/abs/2403.06935</link><description>&lt;p&gt;
&#20154;&#31867;&#21644;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35270;&#35273;&#23545;&#35937;&#21629;&#21517;&#12289;&#25551;&#36848;&#21644;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Naming, Describing, and Quantifying Visual Objects in Humans and LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06935
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20102;&#24403;&#21069;&#35270;&#35273;&#19982;&#35821;&#35328;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#31867;&#22312;&#21487;&#33021;&#26631;&#31614;&#30340;&#20998;&#24067;&#19978;&#26174;&#31034;&#20986;&#26497;&#22823;&#20027;&#35266;&#21464;&#24322;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#35270;&#35273;&#23545;&#35937;&#30340;&#21629;&#21517;&#12289;&#25551;&#36848;&#21644;&#37327;&#21270;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#35762;&#35805;&#32773;&#22312;&#25551;&#36848;&#22270;&#20687;&#20013;&#30340;&#21516;&#19968;&#23545;&#35937;&#26102;&#20351;&#29992;&#21508;&#31181;&#19981;&#21516;&#30340;&#34920;&#36798;&#26041;&#24335;&#65292;&#36825;&#20135;&#29983;&#20102;&#30001;&#35821;&#29992;&#32422;&#26463;&#39537;&#21160;&#30340;&#21512;&#29702;&#26631;&#31614;&#20998;&#24067;&#65292;&#24403;&#21069;&#35270;&#35273;&#19982;&#35821;&#35328;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;VLLMs&#65289;&#33021;&#22815;&#27169;&#20223;&#35821;&#35328;&#20351;&#29992;&#20013;&#36825;&#19968;&#20851;&#38190;&#29305;&#24449;&#30340;&#31243;&#24230;&#23578;&#19981;&#26126;&#30830;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;VLLMs&#65288;FROMAGe&#12289;BLIP-2&#12289;LLaVA&#65289;&#22312;&#20154;&#31867;&#22312;&#21487;&#33021;&#26631;&#31614;&#30340;&#20998;&#24067;&#19978;&#26174;&#31034;&#20986;&#26497;&#22823;&#20027;&#35266;&#21464;&#24322;&#24615;&#30340;&#19977;&#20010;&#31867;&#21035;&#65288;&#21517;&#35789;&#12289;&#23646;&#24615;&#21644;&#37327;&#35789;&#65289;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06935v1 Announce Type: new  Abstract: While human speakers use a variety of different expressions when describing the same object in an image, giving rise to a distribution of plausible labels driven by pragmatic constraints, the extent to which current Vision \&amp; Language Large Language Models (VLLMs) can mimic this crucial feature of language use is an open question. This applies to common, everyday objects, but it is particularly interesting for uncommon or novel objects for which a category label may be lacking or fuzzy. Furthermore, humans show clear production preferences for highly context-sensitive expressions, such as the quantifiers `few' or `most'. In our work, we evaluate VLLMs (FROMAGe, BLIP-2, LLaVA) on three categories (nouns, attributes, and quantifiers) where humans show great subjective variability concerning the distribution over plausible labels, using datasets and resources mostly under-explored in previous work. Our results reveal mixed evidence on the a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20351;&#29992;&#20840;&#38754;&#26631;&#20934;&#20026;&#20219;&#21153;&#25552;&#20379;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#65292;&#24182;&#22312;&#35770;&#25991;&#24341;&#35328;&#20889;&#20316;&#12289;Python&#20195;&#30721;&#32534;&#20889;&#21644;Reddit&#24086;&#23376;&#25776;&#20889;&#31561;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.01069</link><description>&lt;p&gt;
LLMCRIT:&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#26631;&#20934;
&lt;/p&gt;
&lt;p&gt;
LLMCRIT: Teaching Large Language Models to Use Criteria
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01069
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20351;&#29992;&#20840;&#38754;&#26631;&#20934;&#20026;&#20219;&#21153;&#25552;&#20379;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#65292;&#24182;&#22312;&#35770;&#25991;&#24341;&#35328;&#20889;&#20316;&#12289;Python&#20195;&#30721;&#32534;&#20889;&#21644;Reddit&#24086;&#23376;&#25776;&#20889;&#31561;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#22312;&#25191;&#34892;&#20219;&#21153;&#26102;&#36981;&#24490;&#26631;&#20934;&#65292;&#36825;&#20123;&#26631;&#20934;&#30452;&#25509;&#29992;&#20110;&#35780;&#20272;&#20219;&#21153;&#23436;&#25104;&#30340;&#36136;&#37327;&#12290;&#22240;&#27492;&#65292;&#20351;&#27169;&#22411;&#23398;&#20064;&#20351;&#29992;&#26631;&#20934;&#25552;&#20379;&#21453;&#39304;&#21487;&#20197;&#24110;&#21161;&#20154;&#31867;&#25110;&#27169;&#22411;&#26356;&#22909;&#22320;&#25191;&#34892;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#24448;&#24448;&#21482;&#32771;&#34385;&#26377;&#38480;&#30340;&#26631;&#20934;&#25110;&#36136;&#37327;&#35780;&#20272;&#26041;&#38754;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#22312;&#23436;&#25104;&#20219;&#21153;&#26102;&#20351;&#29992;&#20840;&#38754;&#30340;&#26631;&#20934;&#25552;&#20379;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;-&#29615;&#36335;&#26694;&#26550;&#65292;&#20174;&#25910;&#38598;&#30340;&#19981;&#21516;&#20889;&#20316;&#20219;&#21153;&#25351;&#21335;&#20013;&#21322;&#33258;&#21160;&#22320;&#25552;&#21462;&#26631;&#20934;&#65292;&#24182;&#20026;&#27599;&#20010;&#26631;&#20934;&#26500;&#24314;&#19978;&#19979;&#25991;&#28436;&#31034;&#12290;&#25105;&#20204;&#36873;&#25321;&#20102;&#26469;&#33258;&#29616;&#23454;&#22330;&#26223;&#30340;&#19977;&#20010;&#20219;&#21153;&#26469;&#23454;&#29616;&#36825;&#19968;&#24819;&#27861;&#65306;&#35770;&#25991;&#24341;&#35328;&#20889;&#20316;&#12289;Python&#20195;&#30721;&#32534;&#20889;&#21644;Reddit&#24086;&#23376;&#25776;&#20889;&#65292;&#24182;&#35780;&#20272;&#25105;&#20204;&#30340;&#21453;&#39304;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01069v1 Announce Type: new  Abstract: Humans follow criteria when they execute tasks, and these criteria are directly used to assess the quality of task completion. Therefore, having models learn to use criteria to provide feedback can help humans or models to perform tasks better. However, existing research in this field tends to consider only a limited set of criteria or quality assessment aspects. To fill this gap, we propose a general framework that enables large language models (LLMs) to use comprehensive criteria for a task in delivering natural language feedback on task execution. In particular, we present a model-in-the-loop framework that semi-automatically derives criteria from collected guidelines for different writing tasks and constructs in-context demonstrations for each criterion. We choose three tasks from real-world scenarios to operationalize this idea: paper introduction writing, Python code writing, and Reddit post writing, and evaluate our feedback gener
&lt;/p&gt;</description></item><item><title>NewsBench&#26159;&#19968;&#20010;&#35780;&#20272;LLMs&#22312;&#20013;&#22269;&#26032;&#38395;&#20889;&#20316;&#27700;&#24179;&#21644;&#23433;&#20840;&#24615;&#36981;&#20174;&#33021;&#21147;&#30340;&#22522;&#20934;&#26694;&#26550;&#65292;&#25581;&#31034;&#20102;&#22312;&#21019;&#36896;&#24615;&#20889;&#20316;&#20219;&#21153;&#20013;LLMs&#30456;&#23545;&#19981;&#36275;&#30340;&#26032;&#38395;&#20262;&#29702;&#36981;&#23432;&#26041;&#38754;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2403.00862</link><description>&lt;p&gt;
NewsBench&#65306;&#31995;&#32479;&#24615;&#35780;&#20272;LLM&#22312;&#20013;&#22269;&#26032;&#38395;&#32534;&#36753;&#24212;&#29992;&#20013;&#30340;&#20889;&#20316;&#27700;&#24179;&#21644;&#23433;&#20840;&#24615;&#36981;&#20174;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
NewsBench: Systematic Evaluation of LLMs for Writing Proficiency and Safety Adherence in Chinese Journalistic Editorial Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00862
&lt;/p&gt;
&lt;p&gt;
NewsBench&#26159;&#19968;&#20010;&#35780;&#20272;LLMs&#22312;&#20013;&#22269;&#26032;&#38395;&#20889;&#20316;&#27700;&#24179;&#21644;&#23433;&#20840;&#24615;&#36981;&#20174;&#33021;&#21147;&#30340;&#22522;&#20934;&#26694;&#26550;&#65292;&#25581;&#31034;&#20102;&#22312;&#21019;&#36896;&#24615;&#20889;&#20316;&#20219;&#21153;&#20013;LLMs&#30456;&#23545;&#19981;&#36275;&#30340;&#26032;&#38395;&#20262;&#29702;&#36981;&#23432;&#26041;&#38754;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;NewsBench&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#26694;&#26550;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20013;&#22269;&#26032;&#38395;&#20889;&#20316;&#27700;&#24179;&#65288;JWP&#65289;&#21644;&#23433;&#20840;&#24615;&#36981;&#20174;&#65288;SA&#65289;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24357;&#34917;&#20102;&#26032;&#38395;&#20262;&#29702;&#19982;&#20154;&#24037;&#26234;&#33021;&#21033;&#29992;&#39118;&#38505;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;NewsBench&#21253;&#25324;5&#20010;&#32534;&#36753;&#24212;&#29992;&#20013;&#30340;1,267&#39033;&#20219;&#21153;&#65292;7&#20010;&#26041;&#38754;&#65288;&#21253;&#25324;&#23433;&#20840;&#24615;&#21644;&#26032;&#38395;&#20889;&#20316;&#65292;&#20197;&#21450;4&#20010;&#35814;&#32454;&#35201;&#38754;&#65289;&#65292;&#28085;&#30422;24&#20010;&#26032;&#38395;&#20027;&#39064;&#39046;&#22495;&#65292;&#37319;&#29992;&#22522;&#20110;&#20004;&#31181;GPT-4&#30340;&#33258;&#21160;&#35780;&#20272;&#21327;&#35758;&#65292;&#24182;&#32463;&#36807;&#20154;&#31867;&#35780;&#20272;&#39564;&#35777;&#12290;&#25105;&#20204;&#23545;11&#20010;LLM&#30340;&#20840;&#38754;&#20998;&#26512;&#31361;&#20986;&#20102;GPT-4&#21644;ERNIE Bot&#20316;&#20026;&#34920;&#29616;&#26368;&#20339;&#65292;&#20294;&#22312;&#21019;&#36896;&#24615;&#20889;&#20316;&#20219;&#21153;&#20013;&#25581;&#31034;&#20102;&#26032;&#38395;&#20262;&#29702;&#36981;&#23432;&#26041;&#38754;&#30340;&#30456;&#23545;&#19981;&#36275;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;AI&#29983;&#25104;&#30340;&#26032;&#38395;&#20869;&#23481;&#38656;&#35201;&#25552;&#39640;&#20262;&#29702;&#25351;&#23548;&#65292;&#26631;&#24535;&#30528;&#20197;&#26032;&#38395;&#26631;&#20934;&#21644;&#23433;&#20840;&#24615;&#23545;&#40784;AI&#33021;&#21147;&#36808;&#20986;&#20102;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00862v1 Announce Type: cross  Abstract: This study presents NewsBench, a novel benchmark framework developed to evaluate the capability of Large Language Models (LLMs) in Chinese Journalistic Writing Proficiency (JWP) and their Safety Adherence (SA), addressing the gap between journalistic ethics and the risks associated with AI utilization. Comprising 1,267 tasks across 5 editorial applications, 7 aspects (including safety and journalistic writing with 4 detailed facets), and spanning 24 news topics domains, NewsBench employs two GPT-4 based automatic evaluation protocols validated by human assessment. Our comprehensive analysis of 11 LLMs highlighted GPT-4 and ERNIE Bot as top performers, yet revealed a relative deficiency in journalistic ethic adherence during creative writing tasks. These findings underscore the need for enhanced ethical guidance in AI-generated journalistic content, marking a step forward in aligning AI capabilities with journalistic standards and safet
&lt;/p&gt;</description></item><item><title>RAM-EHR&#36890;&#36807;&#22686;&#24378;&#26816;&#32034;&#24182;&#21033;&#29992;&#24635;&#32467;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#38024;&#23545;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#20020;&#24202;&#39044;&#27979;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.00815</link><description>&lt;p&gt;
RAM-EHR: &#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#19978;&#30340;&#26816;&#32034;&#22686;&#24378;&#19982;&#20020;&#24202;&#39044;&#27979;&#30456;&#36935;
&lt;/p&gt;
&lt;p&gt;
RAM-EHR: Retrieval Augmentation Meets Clinical Predictions on Electronic Health Records
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00815
&lt;/p&gt;
&lt;p&gt;
RAM-EHR&#36890;&#36807;&#22686;&#24378;&#26816;&#32034;&#24182;&#21033;&#29992;&#24635;&#32467;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#38024;&#23545;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#20020;&#24202;&#39044;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;RAM-EHR&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#25913;&#21892;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#19978;&#20020;&#24202;&#39044;&#27979;&#30340;&#26816;&#32034;&#22686;&#24378;&#65288;Retrieval Augmentation&#65289;&#27969;&#31243;&#12290;RAM-EHR&#39318;&#20808;&#25910;&#38598;&#22810;&#20010;&#30693;&#35782;&#26469;&#28304;&#65292;&#23558;&#23427;&#20204;&#36716;&#25442;&#20026;&#25991;&#26412;&#26684;&#24335;&#65292;&#24182;&#20351;&#29992;&#23494;&#38598;&#26816;&#32034;&#26469;&#33719;&#21462;&#19982;&#21307;&#23398;&#27010;&#24565;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#36825;&#19968;&#31574;&#30053;&#35299;&#20915;&#20102;&#19982;&#22797;&#26434;&#27010;&#24565;&#21517;&#31216;&#30456;&#20851;&#30340;&#22256;&#38590;&#12290;RAM-EHR&#28982;&#21518;&#22686;&#24191;&#20102;&#19982;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#20195;&#30721;&#32852;&#21512;&#35757;&#32451;&#30340;&#26412;&#22320;EHR&#39044;&#27979;&#27169;&#22411;&#65292;&#20197;&#25429;&#33719;&#26469;&#33258;&#24739;&#32773;&#23601;&#35786;&#21644;&#24635;&#32467;&#30693;&#35782;&#30340;&#20114;&#34917;&#20449;&#24687;&#12290;&#22312;&#20004;&#20010;EHR&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;RAM-EHR&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;&#30693;&#35782;&#22686;&#24378;&#22522;&#32447;&#25928;&#26524;&#26174;&#33879;&#65288;AUROC&#22686;&#30410;3.4&#65285;&#65292;AUPR&#22686;&#30410;7.2&#65285;&#65289;&#65292;&#24378;&#35843;&#20102;RAM-EHR&#30340;&#24635;&#32467;&#30693;&#35782;&#23545;&#20020;&#24202;&#39044;&#27979;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#12290;&#20195;&#30721;&#23558;&#21457;&#24067;&#22312;\url{https://github.com/ritaranx/RAM-EHR}&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00815v1 Announce Type: cross  Abstract: We present RAM-EHR, a Retrieval AugMentation pipeline to improve clinical predictions on Electronic Health Records (EHRs). RAM-EHR first collects multiple knowledge sources, converts them into text format, and uses dense retrieval to obtain information related to medical concepts. This strategy addresses the difficulties associated with complex names for the concepts. RAM-EHR then augments the local EHR predictive model co-trained with consistency regularization to capture complementary information from patient visits and summarized knowledge. Experiments on two EHR datasets show the efficacy of RAM-EHR over previous knowledge-enhanced baselines (3.4% gain in AUROC and 7.2% gain in AUPR), emphasizing the effectiveness of the summarized knowledge from RAM-EHR for clinical prediction tasks. The code will be published at \url{https://github.com/ritaranx/RAM-EHR}.
&lt;/p&gt;</description></item><item><title>&#32454;&#35843;&#30340;&#26426;&#22120;&#32763;&#35793;&#24230;&#37327;&#22312;&#26410;&#30693;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#30456;&#23545;&#20110;&#20381;&#36182;&#34920;&#38754;&#24418;&#24335;&#30340;&#24230;&#37327;&#21644;&#26410;&#32463;MT&#36136;&#37327;&#21028;&#26029;&#32454;&#35843;&#30340;&#39044;&#35757;&#32451;&#24230;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.18747</link><description>&lt;p&gt;
&#32454;&#35843;&#30340;&#26426;&#22120;&#32763;&#35793;&#24230;&#37327;&#22312;&#26410;&#30693;&#39046;&#22495;&#20013;&#23384;&#22312;&#22256;&#38590;
&lt;/p&gt;
&lt;p&gt;
Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18747
&lt;/p&gt;
&lt;p&gt;
&#32454;&#35843;&#30340;&#26426;&#22120;&#32763;&#35793;&#24230;&#37327;&#22312;&#26410;&#30693;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#30456;&#23545;&#20110;&#20381;&#36182;&#34920;&#38754;&#24418;&#24335;&#30340;&#24230;&#37327;&#21644;&#26410;&#32463;MT&#36136;&#37327;&#21028;&#26029;&#32454;&#35843;&#30340;&#39044;&#35757;&#32451;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#12289;&#28085;&#30422;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;11&#31181;&#35821;&#35328;&#23545;&#30340;&#24191;&#27867;&#30340;&#22810;&#32500;&#36136;&#37327;&#24230;&#37327;(MQM)&#27880;&#37322;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#26469;&#25506;&#31350;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#20043;&#38388;&#30340;&#39046;&#22495;&#36716;&#31227;&#26102;&#65292;&#26159;&#21542;&#37027;&#20123;&#26681;&#25454;&#20154;&#24037;&#29983;&#25104;&#30340;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#21028;&#26029;&#36827;&#34892;&#32454;&#35843;&#30340;MT&#24230;&#37327;&#26159;&#31283;&#20581;&#30340;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#26410;&#30693;&#39046;&#22495;&#30340;&#24773;&#20917;&#19979;&#65292;&#32454;&#35843;&#30340;&#24230;&#37327;&#30456;&#23545;&#20110;&#20381;&#36182;&#34920;&#38754;&#24418;&#24335;&#30340;&#24230;&#37327;&#20197;&#21450;&#26410;&#32463;MT&#36136;&#37327;&#21028;&#26029;&#32454;&#35843;&#30340;&#39044;&#35757;&#32451;&#24230;&#37327;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18747v1 Announce Type: cross  Abstract: We introduce a new, extensive multidimensional quality metrics (MQM) annotated dataset covering 11 language pairs in the biomedical domain. We use this dataset to investigate whether machine translation (MT) metrics which are fine-tuned on human-generated MT quality judgements are robust to domain shifts between training and inference. We find that fine-tuned metrics exhibit a substantial performance drop in the unseen domain scenario relative to metrics that rely on the surface form, as well as pre-trained metrics which are not fine-tuned on MT quality judgments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36866;&#37197;&#22120;&#29992;&#20110;&#22122;&#22768;&#40065;&#26834;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#25506;&#32034;&#65292;&#21457;&#29616;&#23558;&#36866;&#37197;&#22120;&#25554;&#20837;&#27973;&#23618;&#21487;&#20197;&#33719;&#24471;&#26356;&#26174;&#33879;&#30340;&#25928;&#26524;&#65292;&#30495;&#23454;&#25968;&#25454;&#27604;&#27169;&#25311;&#25968;&#25454;&#26356;&#26377;&#25928;&#65292;&#24182;&#19988;&#23558;&#36866;&#37197;&#22120;&#38598;&#25104;&#21040;&#22522;&#20110;&#35821;&#38899;&#22686;&#24378;&#30340;ASR&#31995;&#32479;&#20013;&#21487;&#20197;&#24102;&#26469;&#23454;&#36136;&#24615;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.18275</link><description>&lt;p&gt;
&#25506;&#32034;&#36866;&#37197;&#22120;&#29992;&#20110;&#22122;&#22768;&#40065;&#26834;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Exploration of Adapter for Noise Robust Automatic Speech Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36866;&#37197;&#22120;&#29992;&#20110;&#22122;&#22768;&#40065;&#26834;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#25506;&#32034;&#65292;&#21457;&#29616;&#23558;&#36866;&#37197;&#22120;&#25554;&#20837;&#27973;&#23618;&#21487;&#20197;&#33719;&#24471;&#26356;&#26174;&#33879;&#30340;&#25928;&#26524;&#65292;&#30495;&#23454;&#25968;&#25454;&#27604;&#27169;&#25311;&#25968;&#25454;&#26356;&#26377;&#25928;&#65292;&#24182;&#19988;&#23558;&#36866;&#37197;&#22120;&#38598;&#25104;&#21040;&#22522;&#20110;&#35821;&#38899;&#22686;&#24378;&#30340;ASR&#31995;&#32479;&#20013;&#21487;&#20197;&#24102;&#26469;&#23454;&#36136;&#24615;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36866;&#24212;&#40065;&#26834;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#20197;&#35299;&#20915;&#26410;&#30693;&#22122;&#22768;&#22330;&#26223;&#33267;&#20851;&#37325;&#35201;&#12290;&#23558;&#36866;&#37197;&#22120;&#38598;&#25104;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#12290;&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#22522;&#20110;&#36866;&#37197;&#22120;&#30340;&#22122;&#22768;&#40065;&#26834;ASR&#36866;&#24212;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;CHiME--4&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#27973;&#23618;&#25554;&#20837;&#36866;&#37197;&#22120;&#33021;&#22815;&#20135;&#29983;&#26356;&#26174;&#33879;&#30340;&#25928;&#26524;&#65292;&#22312;&#20165;&#22312;&#27973;&#23618;&#20869;&#37096;&#36827;&#34892;&#36866;&#24212;&#21644;&#22312;&#25152;&#26377;&#23618;&#20043;&#38388;&#36827;&#34892;&#36866;&#24212;&#20043;&#38388;&#27809;&#26377;&#26174;&#33879;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#27169;&#25311;&#25968;&#25454;&#26377;&#21161;&#20110;&#31995;&#32479;&#25913;&#21892;&#20854;&#22312;&#23454;&#38469;&#22122;&#22768;&#26465;&#20214;&#19979;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#22312;&#25968;&#25454;&#37327;&#30456;&#21516;&#26102;&#65292;&#30495;&#23454;&#25968;&#25454;&#27604;&#27169;&#25311;&#25968;&#25454;&#26356;&#26377;&#25928;&#12290;&#22312;&#36866;&#37197;&#22120;&#35757;&#32451;&#20013;&#65292;&#22810;&#26465;&#20214;&#35757;&#32451;&#20173;&#28982;&#26377;&#25928;&#12290;&#27492;&#22806;&#65292;&#23558;&#36866;&#37197;&#22120;&#38598;&#25104;&#21040;&#22522;&#20110;&#35821;&#38899;&#22686;&#24378;&#30340;ASR&#31995;&#32479;&#20013;&#20250;&#24102;&#26469;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18275v1 Announce Type: cross  Abstract: Adapting a robust automatic speech recognition (ASR) system to tackle unseen noise scenarios is crucial. Integrating adapters into neural networks has emerged as a potent technique for transfer learning. This paper thoroughly investigates adapter-based noise-robust ASR adaptation. We conducted the experiments using the CHiME--4 dataset. The results show that inserting the adapter in the shallow layer yields superior effectiveness, and there is no significant difference between adapting solely within the shallow layer and adapting across all layers. Besides, the simulated data helps the system to improve its performance under real noise conditions. Nonetheless, when the amount of data is the same, the real data is more effective than the simulated data. Multi-condition training remains valid for adapter training. Furthermore, integrating adapters into speech enhancement-based ASR systems yields substantial improvements.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;Layer-wise Scalable Adapter&#31574;&#30053;MedLaSA&#65292;&#29992;&#20110;&#32534;&#36753;&#21307;&#23398;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#31934;&#30830;&#20462;&#25913;&#21307;&#23398;&#30693;&#35782;&#24182;&#35299;&#37322;&#20107;&#23454;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#26041;&#27861;&#22312;&#21307;&#23398;&#30693;&#35782;&#29305;&#27530;&#21270;&#21644;&#22797;&#26434;&#24615;&#26041;&#38754;&#30340;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2402.18099</link><description>&lt;p&gt;
&#32534;&#36753;&#21307;&#23398;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20107;&#23454;&#30693;&#35782;&#21644;&#35299;&#37322;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Editing Factual Knowledge and Explanatory Ability of Medical Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18099
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;Layer-wise Scalable Adapter&#31574;&#30053;MedLaSA&#65292;&#29992;&#20110;&#32534;&#36753;&#21307;&#23398;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#31934;&#30830;&#20462;&#25913;&#21307;&#23398;&#30693;&#35782;&#24182;&#35299;&#37322;&#20107;&#23454;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#26041;&#27861;&#22312;&#21307;&#23398;&#30693;&#35782;&#29305;&#27530;&#21270;&#21644;&#22797;&#26434;&#24615;&#26041;&#38754;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#26088;&#22312;&#31934;&#30830;&#20462;&#25913;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#29305;&#23450;&#30693;&#35782;&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#19981;&#30456;&#20851;&#30340;&#30693;&#35782;&#19981;&#21464;&#12290;&#24050;&#32463;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#35299;&#20915;LLMs&#20013;&#30340;&#24187;&#35273;&#21644;&#36807;&#26102;&#38382;&#39064;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;&#22240;&#27492;&#65292;&#23427;&#21487;&#20197;&#25552;&#39640;LLMs&#22312;&#35768;&#22810;&#20851;&#38190;&#39046;&#22495;&#65288;&#20363;&#22914;&#21307;&#23398;&#39046;&#22495;&#65289;&#20013;&#30340;&#24212;&#29992;&#65292;&#20854;&#20013;&#24187;&#35273;&#26159;&#19981;&#21487;&#23481;&#24525;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20004;&#39033;&#27169;&#22411;&#32534;&#36753;&#30740;&#31350;&#65292;&#24182;&#22312;&#21307;&#23398;&#39046;&#22495;&#39564;&#35777;&#23427;&#20204;&#65306;&#65288;1&#65289;&#30452;&#25509;&#32534;&#36753;&#21307;&#23398;&#20107;&#23454;&#30693;&#35782;&#21644;&#65288;2&#65289;&#32534;&#36753;&#23545;&#20107;&#23454;&#30340;&#35299;&#37322;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#24403;&#21069;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#22312;&#21307;&#23398;&#30693;&#35782;&#30340;&#29305;&#27530;&#21270;&#21644;&#22797;&#26434;&#24615;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MedLaSA&#65292;&#19968;&#31181;&#26032;&#22411;&#30340;&#36866;&#29992;&#20110;&#21307;&#23398;&#27169;&#22411;&#32534;&#36753;&#30340;&#20998;&#23618;&#21487;&#25193;&#23637;&#36866;&#37197;&#22120;&#31574;&#30053;&#12290;&#23427;&#37319;&#29992;&#22240;&#26524;&#36861;&#36394;&#26469;&#35782;&#21035;&#31070;&#32463;&#20803;&#20013;&#30693;&#35782;&#30340;&#31934;&#30830;&#20301;&#32622;&#65292;&#28982;&#21518;&#23558;&#21487;&#25193;&#23637;&#36866;&#37197;&#22120;&#24341;&#20837;&#21040;LLMs&#30340;&#23494;&#38598;&#23618;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18099v1 Announce Type: cross  Abstract: Model editing aims to precisely modify the behaviours of large language models (LLMs) on specific knowledge while keeping irrelevant knowledge unchanged. It has been proven effective in resolving hallucination and out-of-date issues in LLMs. As a result, it can boost the application of LLMs in many critical domains (e.g., medical domain), where the hallucination is not tolerable. In this paper, we propose two model editing studies and validate them in the medical domain: (1) directly editing the factual medical knowledge and (2) editing the explanations to facts. Meanwhile, we observed that current model editing methods struggle with the specialization and complexity of medical knowledge. Therefore, we propose MedLaSA, a novel Layer-wise Scalable Adapter strategy for medical model editing. It employs causal tracing to identify the precise location of knowledge in neurons and then introduces scalable adapters into the dense layers of LL
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20154;&#31867;&#20803;&#23457;&#38405;&#32773;&#30340;&#24773;&#24863;&#25972;&#21512;&#26694;&#26550;&#65292;&#25552;&#20986;&#35780;&#20272;&#25351;&#26631;&#24182;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#65292;&#25351;&#23548;LLMs&#29983;&#25104;&#31185;&#23398;&#20803;&#23457;&#38405;&#30340;&#36923;&#36753;&#34987;&#39564;&#35777;&#21487;&#34892;&#12290;</title><link>https://arxiv.org/abs/2402.18005</link><description>&lt;p&gt;
&#25506;&#32034;&#31185;&#23398;&#24773;&#24863;&#24635;&#32467;&#30340;&#22810;&#25991;&#26723;&#20449;&#24687;&#25972;&#21512;
&lt;/p&gt;
&lt;p&gt;
Exploring Multi-Document Information Consolidation for Scientific Sentiment Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18005
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20154;&#31867;&#20803;&#23457;&#38405;&#32773;&#30340;&#24773;&#24863;&#25972;&#21512;&#26694;&#26550;&#65292;&#25552;&#20986;&#35780;&#20272;&#25351;&#26631;&#24182;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#65292;&#25351;&#23548;LLMs&#29983;&#25104;&#31185;&#23398;&#20803;&#23457;&#38405;&#30340;&#36923;&#36753;&#34987;&#39564;&#35777;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#31995;&#32479;&#20855;&#26377;&#29983;&#25104;&#22810;&#20010;&#25991;&#26723;&#30340;&#21512;&#29702;&#25688;&#35201;&#30340;&#33021;&#21147;&#65307;&#28982;&#32780;&#65292;&#29616;&#22312;&#23578;&#19981;&#30830;&#23450;&#27169;&#22411;&#26159;&#21542;&#30495;&#27491;&#20855;&#26377;&#25972;&#21512;&#20449;&#24687;&#30340;&#33021;&#21147;&#26469;&#29983;&#25104;&#24635;&#32467;&#65292;&#23588;&#20854;&#26159;&#23545;&#37027;&#20123;&#21253;&#21547;&#20010;&#20154;&#24847;&#35265;&#20449;&#24687;&#30340;&#28304;&#25991;&#26723;&#12290;&#20026;&#20102;&#20351;&#31185;&#23398;&#24773;&#24863;&#24635;&#32467;&#26356;&#21152;&#25166;&#23454;&#65292;&#25105;&#20204;&#20551;&#35774;&#22312;&#21516;&#34892;&#35780;&#23457;&#20013;&#65292;&#20154;&#31867;&#20803;&#23457;&#38405;&#32773;&#36981;&#24490;&#24773;&#24863;&#25972;&#21512;&#30340;&#19977;&#23618;&#26694;&#26550;&#26469;&#25776;&#20889;&#20803;&#23457;&#38405;&#65292;&#24182;&#19988;&#36825;&#20195;&#34920;&#20102;&#22312;&#20803;&#23457;&#38405;&#29983;&#25104;&#36807;&#31243;&#20013;&#24635;&#32467;&#31185;&#23398;&#24773;&#24863;&#30340;&#36923;&#36753;&#12290;&#36890;&#36807;&#20154;&#31867;&#27880;&#37322;&#65292;&#39564;&#35777;&#20102;&#36825;&#19968;&#26694;&#26550;&#12290;&#22522;&#20110;&#35813;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35780;&#20272;&#25351;&#26631;&#26469;&#35780;&#20272;&#29983;&#25104;&#30340;&#20803;&#23457;&#38405;&#30340;&#36136;&#37327;&#65292;&#24182;&#19988;&#22312;&#24191;&#27867;&#23454;&#39564;&#20013;&#21457;&#29616;&#65292;&#24403;&#25105;&#20204;&#23558;&#20854;&#20316;&#20026;LLMs&#29983;&#25104;&#20803;&#23457;&#38405;&#30340;&#25552;&#31034;&#26102;&#65292;&#24773;&#24863;&#25972;&#21512;&#26694;&#26550;&#30340;&#20551;&#35774;&#22312;&#32463;&#39564;&#19978;&#26159;&#34892;&#24471;&#36890;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18005v1 Announce Type: cross  Abstract: Modern natural language generation systems with LLMs exhibit the capability to generate a plausible summary of multiple documents; however, it is uncertain if models truly possess the ability of information consolidation to generate summaries, especially on those source documents with opinionated information. To make scientific sentiment summarization more grounded, we hypothesize that in peer review human meta-reviewers follow a three-layer framework of sentiment consolidation to write meta-reviews and it represents the logic of summarizing scientific sentiments in meta-review generation. The framework is validated via human annotation. Based on the framework, we propose evaluation metrics to assess the quality of generated meta-reviews, and we find that the hypothesis of the sentiment consolidation framework works out empirically when we incorporate it as prompts for LLMs to generate meta-reviews in extensive experiments.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25919;&#27835;&#19990;&#30028;&#35266;&#30340;&#21487;&#38752;&#24615;&#21644;&#19968;&#33268;&#24615;&#65292;&#21457;&#29616;&#20182;&#20204;&#30340;&#21487;&#38752;&#24615;&#38543;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#22686;&#21152;&#32780;&#22686;&#21152;&#65292;&#19988;&#22312;&#25919;&#31574;&#26041;&#26696;&#19978;&#26377;&#25152;&#19981;&#21516;&#12290;</title><link>https://arxiv.org/abs/2402.17649</link><description>&lt;p&gt;
&#36229;&#36234;&#25552;&#31034;&#33030;&#24369;&#24615;&#65306;&#35780;&#20272;LLMs&#20013;&#25919;&#27835;&#19990;&#30028;&#35266;&#30340;&#21487;&#38752;&#24615;&#21644;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Beyond prompt brittleness: Evaluating the reliability and consistency of political worldviews in LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17649
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25919;&#27835;&#19990;&#30028;&#35266;&#30340;&#21487;&#38752;&#24615;&#21644;&#19968;&#33268;&#24615;&#65292;&#21457;&#29616;&#20182;&#20204;&#30340;&#21487;&#38752;&#24615;&#38543;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#22686;&#21152;&#32780;&#22686;&#21152;&#65292;&#19988;&#22312;&#25919;&#31574;&#26041;&#26696;&#19978;&#26377;&#25152;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24191;&#27867;&#31995;&#32479;&#20013;&#30340;&#20351;&#29992;&#65292;&#25105;&#20204;&#38656;&#35201;&#20102;&#35299;&#23427;&#20204;&#26159;&#21542;&#23884;&#20837;&#20102;&#29305;&#23450;&#30340;&#19990;&#30028;&#35266;&#20197;&#21450;&#36825;&#20123;&#35266;&#28857;&#25152;&#21453;&#26144;&#30340;&#20869;&#23481;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25253;&#21578;&#31216;&#65292;&#24403;&#29992;&#25919;&#27835;&#38382;&#21367;&#36827;&#34892;&#25552;&#31034;&#26102;&#65292;LLMs&#34920;&#29616;&#20986;&#24038;&#20542;&#33258;&#30001;&#20542;&#21521;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#20123;&#20542;&#21521;&#26159;&#21542;&#21487;&#38752;&#65288;&#23545;&#25552;&#31034;&#21464;&#21270;&#31283;&#20581;&#65289;&#20197;&#21450;&#36825;&#31181;&#20542;&#21521;&#26159;&#21542;&#22312;&#25919;&#31574;&#21644;&#25919;&#27835;&#20542;&#21521;&#19978;&#20445;&#25345;&#19968;&#33268;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#27979;&#35797;&#65292;&#35780;&#20272;&#20102;&#22522;&#20110;&#25910;&#38598;&#33258;&#19971;&#20010;&#27431;&#30431;&#22269;&#23478;&#30340;&#36873;&#20030;&#24314;&#35758;&#38382;&#21367;&#24182;&#26631;&#27880;&#20026;&#25919;&#31574;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;LLMs&#22312;&#25919;&#27835;&#22768;&#26126;&#19978;&#31435;&#22330;&#30340;&#21487;&#38752;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21442;&#25968;&#20174;7B&#21040;70B&#30340;LLMs&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#38543;&#21442;&#25968;&#25968;&#37327;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;&#26356;&#22823;&#30340;&#27169;&#22411;&#26174;&#31034;&#24635;&#20307;&#19978;&#19982;&#24038;&#20542;&#25919;&#20826;&#26356;&#24378;&#30340;&#19968;&#33268;&#24615;&#65292;&#20294;&#22312;&#25919;&#31574;&#26041;&#26696;&#20013;&#26377;&#25152;&#19981;&#21516;&#65306;&#23427;&#20204;&#34920;&#29616;&#20986;&#65288;&#24038;&#20542;&#65289;&#31215;&#26497;&#30340;&#31435;&#22330;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17649v1 Announce Type: new  Abstract: Due to the widespread use of large language models (LLMs) in ubiquitous systems, we need to understand whether they embed a specific worldview and what these views reflect. Recent studies report that, prompted with political questionnaires, LLMs show left-liberal leanings. However, it is as yet unclear whether these leanings are reliable (robust to prompt variations) and whether the leaning is consistent across policies and political leaning. We propose a series of tests which assess the reliability and consistency of LLMs' stances on political statements based on a dataset of voting-advice questionnaires collected from seven EU countries and annotated for policy domains. We study LLMs ranging in size from 7B to 70B parameters and find that their reliability increases with parameter count. Larger models show overall stronger alignment with left-leaning parties but differ among policy programs: They evince a (left-wing) positive stance to
&lt;/p&gt;</description></item><item><title>LDB&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#35843;&#35797;&#26694;&#26550;&#65292;&#21487;&#20197;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#36816;&#34892;&#26102;&#25191;&#34892;&#20449;&#24687;&#26469;&#23436;&#21892;&#29983;&#25104;&#30340;&#31243;&#24207;&#12290;</title><link>https://arxiv.org/abs/2402.16906</link><description>&lt;p&gt;
LDB&#65306;&#36890;&#36807;&#36880;&#27493;&#39564;&#35777;&#36816;&#34892;&#26102;&#25191;&#34892;&#26469;&#35843;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LDB: A Large Language Model Debugger via Verifying Runtime Execution Step-by-step
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16906
&lt;/p&gt;
&lt;p&gt;
LDB&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#35843;&#35797;&#26694;&#26550;&#65292;&#21487;&#20197;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#36816;&#34892;&#26102;&#25191;&#34892;&#20449;&#24687;&#26469;&#23436;&#21892;&#29983;&#25104;&#30340;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#19981;&#20165;&#23558;&#21333;&#27425;&#20195;&#30721;&#29983;&#25104;&#65292;&#32780;&#19988;&#36824;&#23558;&#21333;&#20803;&#27979;&#35797;&#21644;&#31243;&#24207;&#39564;&#35777;&#22120;&#25972;&#21512;&#21040;LLMs&#20013;&#65292;&#20197;&#36845;&#20195;&#22320;&#23436;&#21892;&#29983;&#25104;&#30340;&#31243;&#24207;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20316;&#23558;&#29983;&#25104;&#30340;&#31243;&#24207;&#35270;&#20026;&#19981;&#21487;&#20998;&#21106;&#30340;&#23454;&#20307;&#65292;&#36825;&#23545;LLMs&#22312;&#35843;&#35797;&#31243;&#24207;&#26102;&#23384;&#22312;&#19981;&#36275;&#65292;&#29305;&#21035;&#26159;&#24403;&#31243;&#24207;&#21253;&#21547;&#22797;&#26434;&#30340;&#36923;&#36753;&#27969;&#31243;&#21644;&#25968;&#25454;&#25805;&#20316;&#26102;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#24403;&#20154;&#31867;&#24320;&#21457;&#20154;&#21592;&#35843;&#35797;&#31243;&#24207;&#26102;&#65292;&#20182;&#20204;&#36890;&#24120;&#35774;&#32622;&#26029;&#28857;&#24182;&#26377;&#36873;&#25321;&#22320;&#26816;&#26597;&#36816;&#34892;&#26102;&#25191;&#34892;&#20449;&#24687;&#12290;&#25191;&#34892;&#27969;&#21644;&#20013;&#38388;&#21464;&#37327;&#22312;&#35843;&#35797;&#36807;&#31243;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#28982;&#32780;&#29616;&#26377;&#30340;&#20195;&#30721;&#29983;&#25104;&#25991;&#29486;&#20013;&#26410;&#20805;&#20998;&#21033;&#29992;&#23427;&#20204;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35843;&#35797;&#22120;&#65288;LDB&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#35843;&#35797;&#26694;&#26550;&#65292;&#21487;&#20197;&#35753;LLMs&#36890;&#36807;&#36816;&#34892;&#26102;&#25191;&#34892;&#20449;&#24687;&#23436;&#21892;&#20854;&#29983;&#25104;&#30340;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16906v1 Announce Type: cross  Abstract: Large language models (LLMs) are leading significant progress in code generation. Beyond one-pass code generation, recent works further integrate unit tests and program verifiers into LLMs to iteratively refine the generated programs. However, these works consider the generated programs as an indivisible entity, which falls short for LLMs in debugging the programs, especially when the programs contain complex logic flows and data operations. In contrast, when human developers debug programs, they typically set breakpoints and selectively examine runtime execution information. The execution flow and the intermediate variables play a crucial role in the debugging process, yet they are underutilized in the existing literature on code generation. In this study, we introduce Large Language Model Debugger (LDB), a novel debugging framework that enables LLMs to refine their generated programs with the runtime execution information. Specifical
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Adversarial Suffixes Embedding Translation Framework&#65292;&#23558;&#19981;&#21487;&#35835;&#30340;&#25932;&#23545;&#21518;&#32512;&#32763;&#35793;&#20026;&#36830;&#36143;&#12289;&#21487;&#35835;&#30340;&#25991;&#26412;&#65292;&#26377;&#21161;&#20110;&#26356;&#23481;&#26131;&#29702;&#35299;&#21644;&#20998;&#26512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#30340;&#21407;&#22240;&#12290;</title><link>https://arxiv.org/abs/2402.16006</link><description>&lt;p&gt;
&#20174;&#22122;&#38899;&#21040;&#28165;&#26224;&#65306;&#36890;&#36807;&#25991;&#26412;&#23884;&#20837;&#30340;&#32763;&#35793;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25915;&#20987;&#30340;&#25932;&#23545;&#21518;&#32512;
&lt;/p&gt;
&lt;p&gt;
From Noise to Clarity: Unraveling the Adversarial Suffix of Large Language Model Attacks via Translation of Text Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16006
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Adversarial Suffixes Embedding Translation Framework&#65292;&#23558;&#19981;&#21487;&#35835;&#30340;&#25932;&#23545;&#21518;&#32512;&#32763;&#35793;&#20026;&#36830;&#36143;&#12289;&#21487;&#35835;&#30340;&#25991;&#26412;&#65292;&#26377;&#21161;&#20110;&#26356;&#23481;&#26131;&#29702;&#35299;&#21644;&#20998;&#26512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23433;&#20840;&#38450;&#24481;&#26041;&#27861;&#20173;&#28982;&#26377;&#38480;&#65292;&#22240;&#20026;&#21361;&#38505;&#25552;&#31034;&#34987;&#25163;&#24037;&#31574;&#21010;&#20026;&#20165;&#20960;&#31181;&#24050;&#30693;&#30340;&#25915;&#20987;&#31867;&#22411;&#65292;&#36825;&#20007;&#22833;&#20102;&#19982;&#26032;&#20852;&#21464;&#20307;&#21516;&#27493;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#26377;&#23475;&#25351;&#20196;&#21518;&#28155;&#21152;&#21518;&#32512;&#21487;&#20197;&#31361;&#30772;LLMs&#30340;&#38450;&#24481;&#65292;&#24182;&#23548;&#33268;&#21361;&#38505;&#36755;&#20986;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#30001;&#20110;&#19981;&#21487;&#35835;&#24615;&#65292;&#23384;&#22312;&#19968;&#31181;&#23380;&#38553;&#65292;&#20351;&#24471;&#36890;&#36807;&#24120;&#35265;&#30340;&#38450;&#24481;&#26041;&#27861;&#22914;&#22256;&#24785;&#24230;&#36807;&#28388;&#22120;&#30456;&#23545;&#23481;&#26131;&#30475;&#31359;&#36825;&#31181;&#23545;&#25239;&#24615;&#21518;&#32512;&#30340;&#20869;&#22312;&#26426;&#21046;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25932;&#23545;&#21518;&#32512;&#23884;&#20837;&#32763;&#35793;&#26694;&#26550;&#65288;ASETF&#65289;&#65292;&#21487;&#20197;&#23558;&#19981;&#21487;&#35835;&#30340;&#25932;&#23545;&#21518;&#32512;&#32763;&#35793;&#25104;&#36830;&#36143;&#30340;&#21487;&#35835;&#25991;&#26412;&#65292;&#20174;&#32780;&#26356;&#23481;&#26131;&#29702;&#35299;&#21644;&#20998;&#26512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#30340;&#21407;&#22240;&#12290;&#25105;&#20204;&#22312;LLMs&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#22914;LLaMa2&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16006v1 Announce Type: new  Abstract: The safety defense methods of Large language models(LLMs) stays limited because the dangerous prompts are manually curated to just few known attack types, which fails to keep pace with emerging varieties. Recent studies found that attaching suffixes to harmful instructions can hack the defense of LLMs and lead to dangerous outputs. This method, while effective, leaves a gap in understanding the underlying mechanics of such adversarial suffix due to the non-readability and it can be relatively easily seen through by common defense methods such as perplexity filters.To cope with this challenge, in this paper, we propose an Adversarial Suffixes Embedding Translation Framework(ASETF) that are able to translate the unreadable adversarial suffixes into coherent, readable text, which makes it easier to understand and analyze the reasons behind harmful content generation by large language models. We conducted experiments on LLMs such as LLaMa2, 
&lt;/p&gt;</description></item><item><title>&#39318;&#27425;&#23558;&#35848;&#21028;&#20219;&#21153;&#24418;&#24335;&#21270;&#25551;&#36848;&#20026;&#19981;&#23436;&#20840;&#20449;&#24687;&#30340;&#19981;&#23545;&#31216;&#28216;&#25103;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#20195;&#29702;&#34920;&#29616;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#25198;&#28436;&#20080;&#26041;&#30340;&#38590;&#24230;&#22823;&#19988;&#27169;&#22411;&#22823;&#23567;&#19981;&#33021;&#26377;&#25928;&#25552;&#39640;&#20080;&#26041;&#34920;&#29616;&#65292;&#20026;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OG-Narrator&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15813</link><description>&lt;p&gt;
&#35780;&#20272;LLMs&#30340;&#35848;&#21028;&#33021;&#21147;&#65306;&#19968;&#20010;&#22522;&#20934;&#21644;&#19968;&#20010;&#20080;&#26041;&#22686;&#24378;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Measuring Bargaining Abilities of LLMs: A Benchmark and A Buyer-Enhancement Method
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15813
&lt;/p&gt;
&lt;p&gt;
&#39318;&#27425;&#23558;&#35848;&#21028;&#20219;&#21153;&#24418;&#24335;&#21270;&#25551;&#36848;&#20026;&#19981;&#23436;&#20840;&#20449;&#24687;&#30340;&#19981;&#23545;&#31216;&#28216;&#25103;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#20195;&#29702;&#34920;&#29616;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#25198;&#28436;&#20080;&#26041;&#30340;&#38590;&#24230;&#22823;&#19988;&#27169;&#22411;&#22823;&#23567;&#19981;&#33021;&#26377;&#25928;&#25552;&#39640;&#20080;&#26041;&#34920;&#29616;&#65292;&#20026;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OG-Narrator&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35848;&#21028;&#26159;&#20154;&#31867;&#20043;&#38388;&#35848;&#21028;&#30340;&#19968;&#20010;&#37325;&#35201;&#19988;&#29420;&#29305;&#30340;&#37096;&#20998;&#12290;&#38543;&#30528;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#23398;&#20064;&#35848;&#21028;&#24182;&#34920;&#29616;&#24471;&#20687;&#30495;&#27491;&#30340;&#20154;&#31867;&#19968;&#26679;&#65292;&#22914;&#20309;&#35780;&#20272;&#20195;&#29702;&#30340;&#35848;&#21028;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#31532;&#19968;&#27425;&#23558;&#35848;&#21028;&#20219;&#21153;&#24418;&#24335;&#21270;&#25551;&#36848;&#20026;&#19968;&#31181;&#19981;&#23436;&#20840;&#20449;&#24687;&#30340;&#19981;&#23545;&#31216;&#28216;&#25103;&#65292;&#23450;&#20041;&#20102;&#20080;&#26041;&#21644;&#21334;&#26041;&#22312;&#22810;&#27425;&#35848;&#21028;&#36807;&#31243;&#20013;&#30340;&#25910;&#30410;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#23450;&#37327;&#35780;&#20272;&#19968;&#20010;&#20195;&#29702;&#22312;&#35848;&#21028;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#30495;&#23454;&#20135;&#21697;&#20215;&#26684;&#25968;&#25454;&#38598;AmazonHistoryPrice&#65292;&#24182;&#23545;&#21508;&#31181;LLM&#20195;&#29702;&#30340;&#35848;&#21028;&#33021;&#21147;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#25198;&#28436;&#20080;&#26041;&#27604;&#25198;&#28436;&#21334;&#26041;&#35201;&#22256;&#38590;&#24471;&#22810;&#65292;&#24182;&#19988;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#26080;&#27861;&#26377;&#25928;&#22320;&#25552;&#39640;&#20080;&#26041;&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;OG-Narrator&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#38598;&#25104;&#20102;&#19968;&#20010;&#30830;&#23450;&#24615;&#30340;&#25253;&#20215;&#29983;&#25104;&#22120;&#26469;&#25511;&#21046;&#20080;&#26041;&#25253;&#20215;&#30340;&#20215;&#26684;&#33539;&#22260;&#65292;&#24182;&#19988;&#38598;&#25104;&#20102;&#19968;&#20010;LLM&#35299;&#35828;&#32773;&#26469;&#21019;&#24314;&#19968;&#31181;&#33258;&#28982;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15813v1 Announce Type: new  Abstract: Bargaining is an important and unique part of negotiation between humans. As LLM-driven agents learn to negotiate and act like real humans, how to evaluate agents' bargaining abilities remains an open problem. For the first time, we formally described the Bargaining task as an asymmetric incomplete information game, defining the gains of the Buyer and Seller in multiple bargaining processes. It allows us to quantitatively assess an agent's performance in the Bargain task. We collected a real product price dataset, AmazonHistoryPrice, and conducted evaluations of various LLM agents' bargaining abilities. We find that playing a Buyer is much harder than a Seller, and increasing model size can not effectively improve the Buyer's performance. To address the challenge, we propose a novel approach called OG-Narrator that integrates a deterministic Offer Generator to control the price range of Buyer's offers, and an LLM Narrator to create natur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#35762;&#25925;&#20107;&#26469;&#34920;&#36798;&#22266;&#26377;&#30340;&#24120;&#35782;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#25925;&#20107;&#20248;&#20110;&#35268;&#21017;&#20316;&#20026;&#20174;LLMs&#26816;&#32034;&#24120;&#35782;&#30340;&#34920;&#36798;&#24418;&#24335;&#12290;</title><link>https://arxiv.org/abs/2402.14355</link><description>&lt;p&gt;
&#26159;&#35268;&#21017;&#22909;&#36824;&#26159;&#25925;&#20107;&#26356;&#22909;&#30340;&#24120;&#35782;&#34920;&#36798;&#26041;&#24335;&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65311;
&lt;/p&gt;
&lt;p&gt;
Rule or Story, Which is a Better Commonsense Expression for Talking with Large Language Models?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#35762;&#25925;&#20107;&#26469;&#34920;&#36798;&#22266;&#26377;&#30340;&#24120;&#35782;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#25925;&#20107;&#20248;&#20110;&#35268;&#21017;&#20316;&#20026;&#20174;LLMs&#26816;&#32034;&#24120;&#35782;&#30340;&#34920;&#36798;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#20855;&#22791;&#24120;&#35782;&#30340;&#26426;&#22120;&#19968;&#30452;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#36825;&#26159;&#30001;&#20110;&#24120;&#35782;&#35268;&#21017;&#30340;&#25253;&#21578;&#20559;&#24046;&#21644;&#22522;&#20110;&#35268;&#21017;&#30340;&#24120;&#35782;&#25512;&#29702;&#30340;&#26292;&#38706;&#20559;&#24046;&#25152;&#33268;&#12290;&#30456;&#21453;&#65292;&#20154;&#31867;&#36890;&#36807;&#25925;&#20107;&#38544;&#21547;&#22320;&#20256;&#36882;&#21644;&#20256;&#25215;&#24120;&#35782;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#35762;&#25925;&#20107;&#26469;&#34920;&#36798;&#22266;&#26377;&#30340;&#24120;&#35782;&#33021;&#21147;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#21644;&#27604;&#36739;&#20102;&#25925;&#20107;&#21644;&#35268;&#21017;&#22312;&#20174;LLMs&#26816;&#32034;&#21644;&#21033;&#29992;&#24120;&#35782;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#22312;28&#20010;&#24120;&#35782;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25925;&#20107;&#20248;&#20110;&#35268;&#21017;&#20316;&#20026;&#20174;LLMs&#26816;&#32034;&#24120;&#35782;&#30340;&#34920;&#36798;&#24418;&#24335;&#65292;&#22312;&#29983;&#25104;&#20449;&#24515;&#21644;&#24120;&#35782;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#27492;&#22806;&#65292;&#25925;&#20107;&#26159;&#22238;&#31572;&#26377;&#20851;&#26085;&#24120;&#20107;&#20214;&#30340;&#38382;&#39064;&#30340;&#26356;&#26377;&#25928;&#24120;&#35782;&#34920;&#36798;&#26041;&#24335;&#65292;&#32780;&#35268;&#21017;&#23545;&#20110;&#31185;&#23398;&#38382;&#39064;&#26356;&#26377;&#25928;&#12290;&#36825;&#19982;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#30340;&#24120;&#35782;&#25253;&#21578;&#20559;&#24046;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14355v1 Announce Type: new  Abstract: Building machines with commonsense has been a longstanding challenge in NLP due to the reporting bias of commonsense rules and the exposure bias of rule-based commonsense reasoning. In contrast, humans convey and pass down commonsense implicitly through stories. This paper investigates the inherent commonsense ability of large language models (LLMs) expressed through storytelling. We systematically investigate and compare stories and rules for retrieving and leveraging commonsense in LLMs. Experimental results on 28 commonsense QA datasets show that stories outperform rules as the expression for retrieving commonsense from LLMs, exhibiting higher generation confidence and commonsense accuracy. Moreover, stories are the more effective commonsense expression for answering questions regarding daily events, while rules are more effective for scientific questions. This aligns with the reporting bias of commonsense in text corpora. We further 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#25991;&#26412;&#27700;&#21360;&#20013;&#30340;&#8220;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#8221;&#27010;&#24565;&#65292;&#21457;&#29616;&#24403;&#21069;&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;&#22312;&#25991;&#26412;&#34987;&#32763;&#35793;&#25104;&#20854;&#20182;&#35821;&#35328;&#21518;&#22833;&#21435;&#20102;&#19968;&#33268;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#35821;&#35328;&#27700;&#21360;&#21435;&#38500;&#25915;&#20987;&#26041;&#27861;&#65292;&#26377;&#25928;&#32469;&#36807;&#27700;&#21360;&#65292;&#38477;&#20302;AUC&#20540;&#65292;&#21516;&#26102;&#25351;&#20986;&#20102;&#23548;&#33268;&#36825;&#31181;&#24046;&#24322;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;</title><link>https://arxiv.org/abs/2402.14007</link><description>&lt;p&gt;
&#27700;&#21360;&#26159;&#21542;&#33021;&#22815;&#22312;&#32763;&#35793;&#20013;&#23384;&#27963;&#65311;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#27700;&#21360;&#30340;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Can Watermarks Survive Translation? On the Cross-lingual Consistency of Text Watermark for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14007
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#25991;&#26412;&#27700;&#21360;&#20013;&#30340;&#8220;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#8221;&#27010;&#24565;&#65292;&#21457;&#29616;&#24403;&#21069;&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;&#22312;&#25991;&#26412;&#34987;&#32763;&#35793;&#25104;&#20854;&#20182;&#35821;&#35328;&#21518;&#22833;&#21435;&#20102;&#19968;&#33268;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#35821;&#35328;&#27700;&#21360;&#21435;&#38500;&#25915;&#20987;&#26041;&#27861;&#65292;&#26377;&#25928;&#32469;&#36807;&#27700;&#21360;&#65292;&#38477;&#20302;AUC&#20540;&#65292;&#21516;&#26102;&#25351;&#20986;&#20102;&#23548;&#33268;&#36825;&#31181;&#24046;&#24322;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;&#26088;&#22312;&#26631;&#35760;&#21644;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#20869;&#23481;&#65292;&#20197;&#38450;&#27490;&#28389;&#29992;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#25991;&#26412;&#27700;&#21360;&#20013;&#30340;&#8220;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#8221;&#27010;&#24565;&#65292;&#35780;&#20272;&#20102;&#25991;&#26412;&#27700;&#21360;&#22312;&#34987;&#32763;&#35793;&#25104;&#20854;&#20182;&#35821;&#35328;&#21518;&#20445;&#25345;&#26377;&#25928;&#24615;&#30340;&#33021;&#21147;&#12290;&#20004;&#20010;LLM&#21644;&#19977;&#31181;&#27700;&#21360;&#26041;&#27861;&#30340;&#21021;&#27493;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#65292;&#24403;&#21069;&#30340;&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;&#22312;&#25991;&#26412;&#34987;&#32763;&#35793;&#25104;&#19981;&#21516;&#35821;&#35328;&#26102;&#32570;&#20047;&#19968;&#33268;&#24615;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#35821;&#35328;&#27700;&#21360;&#21435;&#38500;&#25915;&#20987;&#65288;CWRA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#39318;&#20808;&#20174;&#19968;&#20010;LLM&#20013;&#33719;&#21462;&#26469;&#33258;&#20013;&#20171;&#35821;&#35328;&#30340;&#21709;&#24212;&#65292;&#28982;&#21518;&#23558;&#20854;&#32763;&#35793;&#25104;&#30446;&#26631;&#35821;&#35328;&#26469;&#32469;&#36807;&#27700;&#21360;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#20943;&#23569;AUC&#20540;&#20174;0.95&#38477;&#33267;0.67&#32780;&#26080;&#24615;&#33021;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#23548;&#33268;&#20132;&#21449;&#19968;&#33268;&#24615;&#24046;&#24322;&#30340;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14007v1 Announce Type: cross  Abstract: Text watermarking technology aims to tag and identify content produced by large language models (LLMs) to prevent misuse. In this study, we introduce the concept of ''cross-lingual consistency'' in text watermarking, which assesses the ability of text watermarks to maintain their effectiveness after being translated into other languages. Preliminary empirical results from two LLMs and three watermarking methods reveal that current text watermarking technologies lack consistency when texts are translated into various languages. Based on this observation, we propose a Cross-lingual Watermark Removal Attack (CWRA) to bypass watermarking by first obtaining a response from an LLM in a pivot language, which is then translated into the target language. CWRA can effectively remove watermarks by reducing the Area Under the Curve (AUC) from 0.95 to 0.67 without performance loss. Furthermore, we analyze two key factors that contribute to the cros
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#23454;&#29992;&#30340;&#21465;&#20107;&#29702;&#35299;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#21465;&#20107;&#20013;&#24418;&#25104;&#22270;NARCO&#26469;&#25551;&#36848;&#25972;&#20010;&#32972;&#26223;&#30340;&#20219;&#21153;&#26080;&#20851;&#30340;&#36830;&#36143;&#20381;&#36182;&#65292;&#20854;&#20013;&#30340;&#36793;&#21453;&#26144;&#20102;&#39640;&#23618;&#27425;&#30340;&#36830;&#36143;&#20851;&#31995;&#65292;&#26080;&#38656;&#20381;&#36182;&#20154;&#31867;&#27880;&#37322;&#12290;</title><link>https://arxiv.org/abs/2402.13551</link><description>&lt;p&gt;
&#21465;&#20107;&#32972;&#26223;&#30340;&#22270;&#34920;&#31034;&#65306;&#36890;&#36807;&#22238;&#39038;&#24615;&#38382;&#39064;&#30340;&#36830;&#36143;&#20381;&#36182;
&lt;/p&gt;
&lt;p&gt;
Graph Representation of Narrative Context: Coherence Dependency via Retrospective Questions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13551
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#23454;&#29992;&#30340;&#21465;&#20107;&#29702;&#35299;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#21465;&#20107;&#20013;&#24418;&#25104;&#22270;NARCO&#26469;&#25551;&#36848;&#25972;&#20010;&#32972;&#26223;&#30340;&#20219;&#21153;&#26080;&#20851;&#30340;&#36830;&#36143;&#20381;&#36182;&#65292;&#20854;&#20013;&#30340;&#36793;&#21453;&#26144;&#20102;&#39640;&#23618;&#27425;&#30340;&#36830;&#36143;&#20851;&#31995;&#65292;&#26080;&#38656;&#20381;&#36182;&#20154;&#31867;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#23454;&#29992;&#30340;&#21465;&#20107;&#29702;&#35299;&#33539;&#24335;&#65292;&#36825;&#26159;&#22522;&#20110;&#19968;&#20010;&#35266;&#23519;&#65306;&#21465;&#36848;&#20013;&#30340;&#20010;&#21035;&#27573;&#33853;&#36890;&#24120;&#26159;&#30456;&#20114;&#20851;&#32852;&#30340;&#65292;&#32780;&#19981;&#26159;&#23396;&#31435;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#21465;&#20107;&#20013;&#24418;&#25104;&#19968;&#20010;&#21517;&#20026;NARCO&#30340;&#22270;&#65292;&#25551;&#36848;&#25972;&#20010;&#32972;&#26223;&#30340;&#20219;&#21153;&#26080;&#20851;&#30340;&#36830;&#36143;&#20381;&#36182;&#12290;&#29305;&#21035;&#26159;&#65292;NARCO&#20013;&#30340;&#36793;&#28085;&#30422;&#20102;&#20004;&#20010;&#19978;&#19979;&#25991;&#29255;&#27573;&#20043;&#38388;&#30340;&#33258;&#30001;&#24418;&#24335;&#22238;&#39038;&#24615;&#38382;&#39064;&#65292;&#21453;&#26144;&#20102;&#39640;&#23618;&#27425;&#30340;&#36830;&#36143;&#20851;&#31995;&#65292;&#21463;&#21040;&#20154;&#31867;&#35748;&#30693;&#24863;&#30693;&#30340;&#21551;&#21457;&#65292;&#20154;&#31867;&#19981;&#26029;&#20174;&#20808;&#21069;&#32972;&#26223;&#20013;&#37325;&#30003;&#30456;&#20851;&#20107;&#20214;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#22270;&#26159;&#36890;&#36807;&#25105;&#20204;&#35774;&#35745;&#30340;&#20004;&#38454;&#27573;LLM&#25552;&#31034;&#23454;&#20363;&#21270;&#30340;&#65292;&#22240;&#27492;&#26080;&#38656;&#20381;&#36182;&#20154;&#31867;&#27880;&#37322;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19977;&#20010;&#20851;&#20110;&#20854;&#23454;&#38469;&#25928;&#29992;&#30340;&#29420;&#29305;&#30740;&#31350;&#65292;&#36890;&#36807;&#24635;&#32467;&#35782;&#21035;&#26816;&#39564;&#36793;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#24773;&#33410;&#26816;&#32034;&#36827;&#34892;&#26412;&#22320;&#19978;&#19979;&#25991;&#22686;&#24378;&#65292;&#20197;&#21450;&#36890;&#36807;&#38271;&#25991;&#26723;&#38382;&#31572;&#31034;&#20363;&#21270;&#30340;&#26356;&#24191;&#27867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13551v1 Announce Type: new  Abstract: This work introduces a novel and practical paradigm for narrative comprehension, stemming from the observation that individual passages within narratives are often cohesively related than being isolated. We therefore propose to formulate a graph upon narratives dubbed NARCO that depicts a task-agnostic coherence dependency of the entire context. Especially, edges in NARCO encompass retrospective free-form questions between two context snippets reflecting high-level coherent relations, inspired by the cognitive perception of humans who constantly reinstate relevant events from prior context. Importantly, our graph is instantiated through our designed two-stage LLM prompting, thereby without reliance on human annotations. We present three unique studies on its practical utility, examining the edge efficacy via recap identification, local context augmentation via plot retrieval, and broader applications exemplified by long document QA. Expe
&lt;/p&gt;</description></item><item><title>ARL2&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#21033;&#29992;LLMs&#20316;&#20026;&#26631;&#27880;&#32773;&#65292;&#24182;&#37319;&#29992;&#33258;&#36866;&#24212;&#33258;&#35757;&#32451;&#31574;&#30053;&#65292;&#33021;&#22815;&#26377;&#25928;&#20943;&#23569;&#27880;&#37322;&#25104;&#26412;&#65292;&#24182;&#22312;NQ&#21644;MMLU&#19978;&#21462;&#24471;&#20102;5.4%&#21644;4.6%&#30340;&#20934;&#30830;&#24230;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.13542</link><description>&lt;p&gt;
ARL2: &#36890;&#36807;&#33258;&#23548;&#33258;&#36866;&#24212;&#30456;&#20851;&#24615;&#26631;&#35760;&#23558;&#26816;&#32034;&#22120;&#19982;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
ARL2: Aligning Retrievers for Black-box Large Language Models via Self-guided Adaptive Relevance Labeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13542
&lt;/p&gt;
&lt;p&gt;
ARL2&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#21033;&#29992;LLMs&#20316;&#20026;&#26631;&#27880;&#32773;&#65292;&#24182;&#37319;&#29992;&#33258;&#36866;&#24212;&#33258;&#35757;&#32451;&#31574;&#30053;&#65292;&#33021;&#22815;&#26377;&#25928;&#20943;&#23569;&#27880;&#37322;&#25104;&#26412;&#65292;&#24182;&#22312;NQ&#21644;MMLU&#19978;&#21462;&#24471;&#20102;5.4%&#21644;4.6%&#30340;&#20934;&#30830;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13542v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449; &#25688;&#35201;: &#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#36890;&#36807;&#25972;&#21512;&#22806;&#37096;&#30693;&#35782;&#28304;&#30340;&#30456;&#20851;&#20449;&#24687;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20351;LLMs&#33021;&#22815;&#36866;&#24212;&#29305;&#23450;&#39046;&#22495;&#65292;&#24182;&#20943;&#36731;&#30693;&#35782;&#23494;&#38598;&#20219;&#21153;&#20013;&#30340;&#24187;&#35273;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#20998;&#24320;&#30340;&#35757;&#32451;&#36807;&#31243;&#21644;LLMs&#30340;&#40657;&#30418;&#29305;&#24615;&#65292;&#29616;&#26377;&#30340;&#26816;&#32034;&#22120;&#36890;&#24120;&#19982;LLMs&#19981;&#21305;&#37197;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ARL2&#65292;&#19968;&#31181;&#21033;&#29992;LLMs&#20316;&#20026;&#26631;&#27880;&#32773;&#30340;&#26816;&#32034;&#22120;&#23398;&#20064;&#25216;&#26415;&#12290;ARL2&#21033;&#29992;LLMs&#27880;&#37322;&#21644;&#35780;&#20998;&#30456;&#20851;&#35777;&#25454;&#65292;&#20174;&#32780;&#33021;&#22815;&#20174;&#24378;&#22823;&#30340;LLM&#30417;&#30563;&#20013;&#23398;&#20064;&#26816;&#32034;&#22120;&#12290;&#27492;&#22806;&#65292;ARL2&#20351;&#29992;&#33258;&#36866;&#24212;&#33258;&#35757;&#32451;&#31574;&#30053;&#26469;&#31574;&#21010;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#30456;&#20851;&#24615;&#25968;&#25454;&#65292;&#21487;&#20197;&#26377;&#25928;&#38477;&#20302;&#26631;&#27880;&#25104;&#26412;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;ARL2&#30340;&#26377;&#25928;&#24615;&#65292;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;NQ&#19978;&#25552;&#39640;&#20102;5.4%&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;MMLU&#19978;&#25552;&#39640;&#20102;4.6%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13542v1 Announce Type: cross  Abstract: Retrieval-augmented generation enhances large language models (LLMs) by incorporating relevant information from external knowledge sources. This enables LLMs to adapt to specific domains and mitigate hallucinations in knowledge-intensive tasks. However, existing retrievers are often misaligned with LLMs due to their separate training processes and the black-box nature of LLMs. To address this challenge, we propose ARL2, a retriever learning technique that harnesses LLMs as labelers. ARL2 leverages LLMs to annotate and score relevant evidence, enabling learning the retriever from robust LLM supervision. Furthermore, ARL2 uses an adaptive self-training strategy for curating high-quality and diverse relevance data, which can effectively reduce the annotation cost. Extensive experiments demonstrate the effectiveness of ARL2, achieving accuracy improvements of 5.4% on NQ and 4.6% on MMLU compared to the state-of-the-art methods. Additionall
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;RefuteBench&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#21453;&#39539;&#25351;&#20196;&#30340;&#36981;&#24490;&#33021;&#21147;&#65292;&#21457;&#29616;LLMs&#20542;&#21521;&#20110;&#22266;&#25191;&#20110;&#20854;&#20869;&#37096;&#30693;&#35782;&#32780;&#26080;&#27861;&#36981;&#20174;&#29992;&#25143;&#21453;&#39304;&#12290;</title><link>https://arxiv.org/abs/2402.13463</link><description>&lt;p&gt;
RefuteBench&#65306;&#35780;&#20272;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39539;&#25351;&#20196;&#36981;&#24490;
&lt;/p&gt;
&lt;p&gt;
RefuteBench: Evaluating Refuting Instruction-Following for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;RefuteBench&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#21453;&#39539;&#25351;&#20196;&#30340;&#36981;&#24490;&#33021;&#21147;&#65292;&#21457;&#29616;LLMs&#20542;&#21521;&#20110;&#22266;&#25191;&#20110;&#20854;&#20869;&#37096;&#30693;&#35782;&#32780;&#26080;&#27861;&#36981;&#20174;&#29992;&#25143;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24212;&#29992;&#33539;&#22260;&#26085;&#30410;&#25193;&#22823;&#12290;&#22312;&#23454;&#38469;&#20351;&#29992;&#20013;&#65292;&#29992;&#25143;&#21487;&#33021;&#26681;&#25454;&#27169;&#22411;&#30340;&#36755;&#20986;&#25552;&#20379;&#21453;&#39304;&#65292;&#24076;&#26395;&#24471;&#21040;&#19968;&#20010;&#21487;&#20197;&#26681;&#25454;&#20182;&#20204;&#30340;&#21453;&#39304;&#23436;&#25104;&#21709;&#24212;&#30340;&#21709;&#24212;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#33021;&#21542;&#24688;&#24403;&#22320;&#21709;&#24212;&#29992;&#25143;&#30340;&#21453;&#39539;&#21453;&#39304;&#24182;&#22987;&#32456;&#25191;&#34892;&#19979;&#21435;&#23578;&#26410;&#24471;&#21040;&#24443;&#24213;&#20998;&#26512;&#12290;&#22522;&#20110;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;RefuteBench&#65292;&#28085;&#30422;&#20102;&#35832;&#22914;&#38382;&#31572;&#12289;&#26426;&#22120;&#32763;&#35793;&#21644;&#30005;&#23376;&#37038;&#20214;&#25776;&#20889;&#31561;&#20219;&#21153;&#12290;&#35780;&#20272;&#26088;&#22312;&#35780;&#20272;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#31215;&#26497;&#25509;&#21463;&#21453;&#39539;&#25351;&#20196;&#24418;&#24335;&#30340;&#21453;&#39304;&#65292;&#24182;&#26159;&#21542;&#33021;&#22815;&#22312;&#23545;&#35805;&#20013;&#22987;&#32456;&#36981;&#24490;&#29992;&#25143;&#38656;&#27714;&#12290;&#25105;&#20204;&#23545;&#20247;&#22810;LLMs&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#21457;&#29616;LLMs&#20542;&#21521;&#22266;&#25191;&#65292;&#21363;&#20542;&#21521;&#20110;&#20854;&#20869;&#37096;&#30693;&#35782;&#65292;&#32463;&#24120;&#26410;&#33021;&#36981;&#23432;&#29992;&#25143;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13463v1 Announce Type: cross  Abstract: The application scope of large language models (LLMs) is increasingly expanding. In practical use, users might provide feedback based on the model's output, hoping for a responsive model that can complete responses according to their feedback. Whether the model can appropriately respond to users' refuting feedback and consistently follow through with execution has not been thoroughly analyzed. In light of this, this paper proposes a comprehensive benchmark, RefuteBench, covering tasks such as question answering, machine translation, and email writing. The evaluation aims to assess whether models can positively accept feedback in form of refuting instructions and whether they can consistently adhere to user demands throughout the conversation. We conduct evaluations on numerous LLMs and find that LLMs are stubborn, i.e. exhibit inclination to their internal knowledge, often failing to comply with user feedback. Additionally, as the leng
&lt;/p&gt;</description></item><item><title>CIF-Bench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20013;&#25991;&#35821;&#35328;&#19978;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#22810;&#26679;&#21270;&#30340;&#25351;&#20196;&#21644;&#25968;&#25454;&#38598;&#21010;&#20998;&#26469;&#20943;&#23569;&#35780;&#20272;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.13109</link><description>&lt;p&gt;
CIF-Bench&#65306;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#30340;&#20013;&#25991;&#25351;&#20196;&#36981;&#24490;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
CIF-Bench: A Chinese Instruction-Following Benchmark for Evaluating the Generalizability of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13109
&lt;/p&gt;
&lt;p&gt;
CIF-Bench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20013;&#25991;&#35821;&#35328;&#19978;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#22810;&#26679;&#21270;&#30340;&#25351;&#20196;&#21644;&#25968;&#25454;&#38598;&#21010;&#20998;&#26469;&#20943;&#23569;&#35780;&#20272;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#27493;&#22686;&#24378;&#20102;&#36890;&#36807;&#25351;&#20196;&#36981;&#24490;&#22312;&#24191;&#27867;&#33539;&#22260;&#30340;&#26410;&#35265;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#22914;&#20013;&#25991;&#36825;&#26679;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#26377;&#25928;&#24615;&#24120;&#24120;&#20250;&#20943;&#24369;&#65292;&#21463;&#21040;&#25968;&#25454;&#27844;&#28431;&#24341;&#36215;&#30340;&#20559;&#35265;&#35780;&#20272;&#30340;&#24433;&#21709;&#65292;&#36825;&#20351;&#20154;&#23545;&#23427;&#20204;&#30495;&#27491;&#30340;&#27867;&#21270;&#33021;&#21147;&#21040;&#26032;&#35821;&#35328;&#39046;&#22495;&#20135;&#29983;&#20102;&#24576;&#30097;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20013;&#25991;&#25351;&#20196;&#36981;&#24490;&#22522;&#20934;&#65288;CIF-Bench&#65289;&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#23545;&#20013;&#25991;&#35821;&#35328;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12290;CIF-Bench &#21253;&#21547;150&#20010;&#20219;&#21153;&#21644;15,000&#20010;&#36755;&#20837;&#36755;&#20986;&#23545;&#65292;&#30001;&#27597;&#35821;&#32773;&#24320;&#21457;&#65292;&#29992;&#20110;&#27979;&#35797;&#36328;&#36234;20&#20010;&#31867;&#21035;&#30340;&#22797;&#26434;&#25512;&#29702;&#21644;&#20013;&#22269;&#25991;&#21270;&#32454;&#24494;&#24046;&#21035;&#12290;&#20026;&#20102;&#20943;&#23569;&#35780;&#20272;&#20559;&#35265;&#65292;&#25105;&#20204;&#21482;&#20844;&#24320;&#20102;&#25968;&#25454;&#38598;&#30340;&#19968;&#21322;&#65292;&#20854;&#20313;&#37096;&#20998;&#20445;&#25345;&#31169;&#23494;&#65292;&#24182;&#24341;&#20837;&#22810;&#26679;&#21270;&#30340;&#25351;&#20196;&#20197;&#26368;&#23567;&#21270;&#24471;&#20998;&#26041;&#24046;&#65292;&#20849;&#35745;45,000&#20010;&#25968;&#25454;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13109v1 Announce Type: cross  Abstract: The advancement of large language models (LLMs) has enhanced the ability to generalize across a wide range of unseen natural language processing (NLP) tasks through instruction-following. Yet, their effectiveness often diminishes in low-resource languages like Chinese, exacerbated by biased evaluations from data leakage, casting doubt on their true generalizability to new linguistic territories. In response, we introduce the Chinese Instruction-Following Benchmark (CIF-Bench), designed to evaluate the zero-shot generalizability of LLMs to the Chinese language. CIF-Bench comprises 150 tasks and 15,000 input-output pairs, developed by native speakers to test complex reasoning and Chinese cultural nuances across 20 categories. To mitigate evaluation bias, we release only half of the dataset publicly, with the remainder kept private, and introduce diversified instructions to minimize score variance, totaling 45,000 data instances. Our eval
&lt;/p&gt;</description></item><item><title>&#27169;&#22411;&#31283;&#23450;&#24615;&#23545;&#35299;&#37322;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#24182;&#21457;&#29616;&#23454;&#38469;&#25200;&#21160;&#23545;&#24615;&#33021;&#21644;&#35299;&#37322;&#24433;&#21709;&#36739;&#23567;&#65292;&#20294;&#25513;&#30422;&#21364;&#26377; drastical &#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.13006</link><description>&lt;p&gt;
&#25506;&#31350;&#27169;&#22411;&#19981;&#31283;&#23450;&#24615;&#23545;&#35299;&#37322;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Investigating the Impact of Model Instability on Explanations and Uncertainty
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13006
&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#31283;&#23450;&#24615;&#23545;&#35299;&#37322;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#24182;&#21457;&#29616;&#23454;&#38469;&#25200;&#21160;&#23545;&#24615;&#33021;&#21644;&#35299;&#37322;&#24433;&#21709;&#36739;&#23567;&#65292;&#20294;&#25513;&#30422;&#21364;&#26377; drastical &#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;AI&#26041;&#27861;&#26377;&#21161;&#20110;&#29702;&#35299;&#27169;&#22411;&#34892;&#20026;&#65292;&#28982;&#32780;&#65292;&#23545;&#36755;&#20837;&#36827;&#34892;&#24494;&#23567;&#12289;&#19981;&#21487;&#23519;&#35273;&#30340;&#25200;&#21160;&#21487;&#33021;&#20250;&#26497;&#22823;&#22320;&#25197;&#26354;&#35299;&#37322;&#12290;&#36825;&#20123;&#35299;&#37322;&#36890;&#24120;&#22312;&#27169;&#22411;&#37096;&#32626;&#20043;&#21069;&#34987;&#20840;&#38754;&#35780;&#20272;&#65292;&#22240;&#27492;&#24456;&#38590;&#35780;&#20272;&#29305;&#23450;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;&#19968;&#20123;&#30740;&#31350;&#24050;&#32463;&#23581;&#35797;&#20026;&#35299;&#37322;&#21019;&#24314;&#32622;&#20449;&#24230;&#20272;&#35745;&#22120;&#65292;&#20294;&#27809;&#26377;&#20154;&#35843;&#26597;&#19981;&#30830;&#23450;&#24615;&#21644;&#35299;&#37322;&#36136;&#37327;&#20043;&#38388;&#30340;&#29616;&#26377;&#32852;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#25512;&#26029;&#26102;&#24341;&#20837;&#22122;&#22768;&#26469;&#20154;&#20026;&#27169;&#25311;&#25991;&#26412;&#36755;&#20837;&#20013;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#36825;&#39033;&#22823;&#35268;&#27169;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25554;&#20837;&#19981;&#21516;&#32423;&#21035;&#30340;&#22122;&#22768;&#25200;&#21160;&#65292;&#24182;&#27979;&#37327;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#21644;&#19981;&#21516;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#30340;&#24433;&#21709;&#12290;&#23454;&#38469;&#25200;&#21160;&#23545;&#24615;&#33021;&#21644;&#35299;&#37322;&#30340;&#24433;&#21709;&#24456;&#23567;&#65292;&#28982;&#32780;&#25513;&#30422;&#21364;&#26377; drastical &#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#39640;&#19981;&#30830;&#23450;&#24615;&#24182;&#19981;&#19968;&#23450;&#24847;&#21619;&#30528;&#35299;&#37322;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13006v1 Announce Type: cross  Abstract: Explainable AI methods facilitate the understanding of model behaviour, yet, small, imperceptible perturbations to inputs can vastly distort explanations. As these explanations are typically evaluated holistically, before model deployment, it is difficult to assess when a particular explanation is trustworthy. Some studies have tried to create confidence estimators for explanations, but none have investigated an existing link between uncertainty and explanation quality. We artificially simulate epistemic uncertainty in text input by introducing noise at inference time. In this large-scale empirical study, we insert different levels of noise perturbations and measure the effect on the output of pre-trained language models and different uncertainty metrics. Realistic perturbations have minimal effect on performance and explanations, yet masking has a drastic effect. We find that high uncertainty doesn't necessarily imply low explanation 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#25991;&#26412;&#36716;&#35821;&#38899;&#27169;&#22411;&#20013;&#25506;&#32034;&#20102;&#20923;&#32467;&#27169;&#22411;&#30340;&#28508;&#31354;&#38388;&#65292;&#21457;&#29616;&#20854;&#20013;&#21253;&#21547;&#20016;&#23500;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#26041;&#27861;&#26469;&#25214;&#20986;&#20854;&#20013;&#30340;&#35821;&#20041;&#26041;&#21521;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#19981;&#32463;&#36807;&#39069;&#22806;&#35757;&#32451;&#12289;&#26550;&#26500;&#26356;&#25913;&#25110;&#25968;&#25454;&#38656;&#27714;&#23601;&#33021;&#36827;&#34892;&#38899;&#39057;&#32534;&#36753;&#12290;</title><link>https://arxiv.org/abs/2402.12423</link><description>&lt;p&gt;
&#20851;&#20110;&#22522;&#20110;&#25193;&#25955;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#27169;&#22411;&#30340;&#35821;&#20041;&#28508;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
On the Semantic Latent Space of Diffusion-Based Text-to-Speech Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#25991;&#26412;&#36716;&#35821;&#38899;&#27169;&#22411;&#20013;&#25506;&#32034;&#20102;&#20923;&#32467;&#27169;&#22411;&#30340;&#28508;&#31354;&#38388;&#65292;&#21457;&#29616;&#20854;&#20013;&#21253;&#21547;&#20016;&#23500;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#26041;&#27861;&#26469;&#25214;&#20986;&#20854;&#20013;&#30340;&#35821;&#20041;&#26041;&#21521;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#19981;&#32463;&#36807;&#39069;&#22806;&#35757;&#32451;&#12289;&#26550;&#26500;&#26356;&#25913;&#25110;&#25968;&#25454;&#38656;&#27714;&#23601;&#33021;&#36827;&#34892;&#38899;&#39057;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#39046;&#22495;&#65292;Denoising Diffusion Models (DDMs) &#30340;&#24341;&#20837;&#26085;&#30410;&#22686;&#22810;&#65292;&#20026;&#21512;&#25104;&#39640;&#36136;&#37327;&#35821;&#38899;&#25552;&#20379;&#20102;&#24040;&#22823;&#20215;&#20540;&#12290;&#23613;&#31649;&#23427;&#20204;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38899;&#39057;&#36136;&#37327;&#65292;&#20294;&#23427;&#20204;&#30340;&#35821;&#20041;&#33021;&#21147;&#31243;&#24230;&#23578;&#19981;&#26126;&#30830;&#65292;&#24182;&#19988;&#25511;&#21046;&#21512;&#25104;&#35821;&#38899;&#30340;&#22768;&#38899;&#29305;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#21463;&#22270;&#20687;&#21512;&#25104;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20923;&#32467;&#30340;TTS&#27169;&#22411;&#30340;&#28508;&#31354;&#38388;&#65292;&#35813;&#31354;&#38388;&#30001;DDM&#21435;&#22122;&#22120;&#30340;&#28508;&#31354;&#38388;&#28608;&#27963;&#32452;&#25104;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20010;&#31354;&#38388;&#21253;&#21547;&#20016;&#23500;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#27010;&#36848;&#20102;&#33509;&#24178;&#26597;&#25214;&#20854;&#20013;&#35821;&#20041;&#26041;&#21521;&#30340;&#26032;&#26041;&#27861;&#65292;&#21253;&#25324;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#29616;&#25104;&#38899;&#39057;&#32534;&#36753;&#65292;&#26080;&#38656;&#36827;&#19968;&#27493;&#35757;&#32451;&#12289;&#26550;&#26500;&#26356;&#25913;&#25110;&#25968;&#25454;&#38656;&#27714;&#12290;&#25105;&#20204;&#21576;&#29616;&#20102;&#32534;&#36753;&#21518;&#38899;&#39057;&#30340;&#35821;&#20041;&#21644;&#22768;&#23398;&#29305;&#36136;&#30340;&#35777;&#25454;&#65292;&#24182;&#25552;&#20379;&#20102;&#34917;&#20805;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12423v1 Announce Type: cross  Abstract: The incorporation of Denoising Diffusion Models (DDMs) in the Text-to-Speech (TTS) domain is rising, providing great value in synthesizing high quality speech. Although they exhibit impressive audio quality, the extent of their semantic capabilities is unknown, and controlling their synthesized speech's vocal properties remains a challenge. Inspired by recent advances in image synthesis, we explore the latent space of frozen TTS models, which is composed of the latent bottleneck activations of the DDM's denoiser. We identify that this space contains rich semantic information, and outline several novel methods for finding semantic directions within it, both supervised and unsupervised. We then demonstrate how these enable off-the-shelf audio editing, without any further training, architectural changes or data requirements. We present evidence of the semantic and acoustic qualities of the edited audio, and provide supplemental samples: h
&lt;/p&gt;</description></item><item><title>PAT-Questions&#22522;&#20934;&#29992;&#20110;&#29616;&#22312;&#26102;&#21051;&#20026;&#38170;&#28857;&#30340;&#26102;&#38388;&#38382;&#31572;&#65292;&#36890;&#36807;&#33258;&#21160;&#21047;&#26032;&#31572;&#26696;&#20197;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#36807;&#26102;&#12289;&#22797;&#26434;&#26102;&#38388;&#20851;&#31995;&#38590;&#20197;&#25512;&#29702;&#12289;&#21487;&#33021;&#38656;&#35201;&#22810;&#36339;&#25512;&#29702;&#20197;&#21450;&#22522;&#20934;&#31572;&#26696;&#25345;&#32493;&#26356;&#26032;&#31561;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.11034</link><description>&lt;p&gt;
PAT-Questions&#65306;&#19968;&#20010;&#29992;&#20110;&#29616;&#22312;&#26102;&#21051;&#20026;&#38170;&#28857;&#30340;&#26102;&#38388;&#38382;&#31572;&#33258;&#26356;&#26032;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
PAT-Questions: A Self-Updating Benchmark for Present-Anchored Temporal Question-Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11034
&lt;/p&gt;
&lt;p&gt;
PAT-Questions&#22522;&#20934;&#29992;&#20110;&#29616;&#22312;&#26102;&#21051;&#20026;&#38170;&#28857;&#30340;&#26102;&#38388;&#38382;&#31572;&#65292;&#36890;&#36807;&#33258;&#21160;&#21047;&#26032;&#31572;&#26696;&#20197;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#36807;&#26102;&#12289;&#22797;&#26434;&#26102;&#38388;&#20851;&#31995;&#38590;&#20197;&#25512;&#29702;&#12289;&#21487;&#33021;&#38656;&#35201;&#22810;&#36339;&#25512;&#29702;&#20197;&#21450;&#22522;&#20934;&#31572;&#26696;&#25345;&#32493;&#26356;&#26032;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#26102;&#38388;&#38382;&#31572;&#65288;TQA&#65289;&#30340;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#38170;&#23450;&#29305;&#23450;&#26102;&#38388;&#25139;&#25110;&#20107;&#20214;&#30340;&#38382;&#39064;&#19978;&#65288;&#20363;&#22914;&#8220;1970&#24180;&#35841;&#26159;&#32654;&#22269;&#24635;&#32479;&#65311;&#8221;&#65289;&#12290;&#24456;&#23569;&#26377;&#30740;&#31350;&#20851;&#27880;&#20854;&#26102;&#38388;&#32972;&#26223;&#30456;&#23545;&#20110;&#24403;&#21069;&#26102;&#38388;&#30340;&#38382;&#39064;&#65288;&#20363;&#22914;&#8220;&#20043;&#21069;&#30340;&#32654;&#22269;&#24635;&#32479;&#26159;&#35841;&#65311;&#8221;&#65289;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#31216;&#20026;&#29616;&#22312;&#26102;&#21051;&#20026;&#38170;&#30340;&#26102;&#38388;&#38382;&#31572;&#65288;PATQA&#65289;&#12290;PATQA&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#25361;&#25112;&#65306;&#65288;1&#65289;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#33021;&#20855;&#26377;&#36807;&#26102;&#30340;&#30693;&#35782;&#65292;&#65288;2&#65289;&#22797;&#26434;&#30340;&#26102;&#38388;&#20851;&#31995;&#65288;&#20363;&#22914;&#8220;&#20043;&#21069;&#8221;&#65292;&#8220;&#20197;&#21069;&#8221;&#65289;&#38590;&#20197;&#25512;&#29702;&#65292;&#65288;3&#65289;&#21487;&#33021;&#38656;&#35201;&#22810;&#36339;&#25512;&#29702;&#65292;&#65288;4&#65289;&#22522;&#20934;&#30340;&#27491;&#30830;&#31572;&#26696;&#24517;&#39035;&#25345;&#32493;&#26356;&#26032;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PAT-Questions&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#25324;&#21333;&#36339;&#21644;&#22810;&#36339;&#26102;&#38388;&#38382;&#39064;&#12290;PAT-Questions&#20013;&#30340;&#31572;&#26696;&#21487;&#20197;&#36890;&#36807;&#22312;&#30693;&#35782;&#22270;&#19978;&#37325;&#26032;&#36816;&#34892;SPARQL&#26597;&#35810;&#26469;&#33258;&#21160;&#21047;&#26032;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20960;&#20010;&#26368;&#20808;&#36827;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11034v1 Announce Type: new  Abstract: Existing work on Temporal Question Answering (TQA) has predominantly focused on questions anchored to specific timestamps or events (e.g. "Who was the US president in 1970?"). Little work has studied questions whose temporal context is relative to the present time (e.g. "Who was the previous US president?"). We refer to this problem as Present-Anchored Temporal QA (PATQA). PATQA poses unique challenges: (1) large language models (LLMs) may have outdated knowledge, (2) complex temporal relationships (e.g. 'before', 'previous') are hard to reason, (3) multi-hop reasoning may be required, and (4) the gold answers of benchmarks must be continuously updated. To address these challenges, we introduce the PAT-Questions benchmark, which includes single and multi-hop temporal questions. The answers in PAT-Questions can be automatically refreshed by re-running SPARQL queries on a knowledge graph, if available. We evaluate several state-of-the-art 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#23558;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#25351;&#26631;&#20174;&#22270;&#20687;&#29983;&#25104;&#36716;&#21270;&#20026;&#25991;&#26412;&#29983;&#25104;&#65292;&#32454;&#33268;&#35780;&#20272;&#20102;LLMs&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#65292;&#25581;&#31034;&#20102;&#24403;&#21069;LLMs&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#24615;&#33021;&#34920;&#29616;&#30340;&#37325;&#35201;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.10693</link><description>&lt;p&gt;
&#25506;&#32034;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#20197;&#35780;&#20272;LLMs&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring Precision and Recall to assess the quality and diversity of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10693
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#23558;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#25351;&#26631;&#20174;&#22270;&#20687;&#29983;&#25104;&#36716;&#21270;&#20026;&#25991;&#26412;&#29983;&#25104;&#65292;&#32454;&#33268;&#35780;&#20272;&#20102;LLMs&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#65292;&#25581;&#31034;&#20102;&#24403;&#21069;LLMs&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#24615;&#33021;&#34920;&#29616;&#30340;&#37325;&#35201;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;Llama-2&#21644;Mistral&#30340;&#26032;&#22411;&#35780;&#20272;&#26694;&#26550;&#65292;&#37325;&#28857;&#26159;&#23558;&#22270;&#20687;&#29983;&#25104;&#30340;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#25351;&#26631;&#36716;&#21270;&#20026;&#25991;&#26412;&#29983;&#25104;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#23545;&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#36827;&#34892;&#32454;&#33268;&#35780;&#20272;&#65292;&#32780;&#26080;&#38656;&#23545;&#40784;&#30340;&#35821;&#26009;&#24211;&#12290;&#36890;&#36807;&#23545;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#30740;&#31350;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#24320;&#25918;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#36825;&#26159;&#20256;&#32479;&#22522;&#20934;&#26080;&#27861;&#20805;&#20998;&#25429;&#25417;&#30340;&#12290;&#30740;&#31350;&#32467;&#26524;&#31361;&#20986;&#20102;&#22312;&#27169;&#22411;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#24494;&#35843;&#26102;&#65292;&#29983;&#25104;&#26679;&#26412;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#36825;&#39033;&#24037;&#20316;&#25193;&#23637;&#20102;&#22522;&#20110;&#20998;&#24067;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#35780;&#20272;&#24037;&#20855;&#21253;&#65292;&#20026;&#24403;&#21069;LLMs&#22312;&#29983;&#25104;&#22810;&#26679;&#24615;&#21644;&#39640;&#36136;&#37327;&#25991;&#26412;&#26041;&#38754;&#38754;&#20020;&#30340;&#23454;&#38469;&#33021;&#21147;&#21644;&#25361;&#25112;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10693v1 Announce Type: new  Abstract: This paper introduces a novel evaluation framework for Large Language Models (LLMs) such as Llama-2 and Mistral, focusing on the adaptation of Precision and Recall metrics from image generation to text generation. This approach allows for a nuanced assessment of the quality and diversity of generated text without the need for aligned corpora. By conducting a comprehensive evaluation of state-of-the-art language models, the study reveals significant insights into their performance on open-ended generation tasks, which are not adequately captured by traditional benchmarks. The findings highlight a trade-off between the quality and diversity of generated samples, particularly when models are fine-tuned with human feedback. This work extends the toolkit for distribution-based NLP evaluation, offering insights into the practical capabilities and challenges faced by current LLMs in generating diverse and high-quality text.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;API Pack&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;API&#35843;&#29992;&#29983;&#25104;&#33021;&#21147;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#29983;&#25104;&#26410;&#35265;&#36807;&#30340;API&#35843;&#29992;&#26041;&#38754;&#30340;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#30340;API&#35843;&#29992;&#29983;&#25104;</title><link>https://arxiv.org/abs/2402.09615</link><description>&lt;p&gt;
API Pack&#65306;&#19968;&#20010;&#29992;&#20110;API&#35843;&#29992;&#29983;&#25104;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
API Pack: A Massive Multilingual Dataset for API Call Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09615
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;API Pack&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;API&#35843;&#29992;&#29983;&#25104;&#33021;&#21147;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#29983;&#25104;&#26410;&#35265;&#36807;&#30340;API&#35843;&#29992;&#26041;&#38754;&#30340;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#30340;API&#35843;&#29992;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;API Pack&#65292;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;&#19968;&#30334;&#19975;&#20010;&#25351;&#20196;-API&#35843;&#29992;&#23545;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;API&#35843;&#29992;&#29983;&#25104;&#33021;&#21147;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;API Pack&#22312;&#25552;&#21319;&#27169;&#22411;&#22312;&#36825;&#19968;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#25928;&#26524;&#30340;&#21516;&#26102;&#65292;&#20445;&#25345;&#20854;&#22312;&#19968;&#33324;&#32534;&#30721;&#26041;&#38754;&#30340;&#25972;&#20307;&#29087;&#32451;&#31243;&#24230;&#12290;&#20165;&#22312;20,000&#20010;Python&#23454;&#20363;&#19978;&#23545;CodeLlama-13B&#36827;&#34892;&#24494;&#35843;&#65292;&#20854;&#29983;&#25104;&#26410;&#35265;&#36807;&#30340;API&#35843;&#29992;&#30340;&#20934;&#30830;&#29575;&#27604;GPT-3.5&#21644;GPT-4&#20998;&#21035;&#39640;&#20986;10%&#21644;5%&#12290;&#25193;&#23637;&#21040;100k&#20010;&#20363;&#23376;&#21487;&#20197;&#25552;&#39640;&#23545;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#26032;API&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#30340;API&#35843;&#29992;&#29983;&#25104;&#65292;&#32780;&#26080;&#38656;&#22823;&#37327;&#35821;&#35328;&#29305;&#23450;&#30340;&#25968;&#25454;&#12290;&#25968;&#25454;&#38598;&#12289;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#21644;&#25972;&#20307;&#20195;&#30721;&#24211;&#21487;&#22312;https://github.com/anonymous_url&#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09615v1 Announce Type: cross  Abstract: We introduce API Pack, a multilingual dataset featuring over one million instruction-API call pairs aimed at advancing large language models' API call generation capabilities. Through experiments, we demonstrate API Pack's efficacy in enhancing models for this specialized task while maintaining their overall proficiency at general coding. Fine-tuning CodeLlama-13B on just 20,000 Python instances yields over 10% and 5% higher accuracy than GPT-3.5 and GPT-4 respectively in generating unseen API calls. Scaling to 100k examples improves generalization to new APIs not seen during training. In addition, cross-lingual API call generation is achieved without needing extensive data per language. The dataset, fine-tuned models, and overall code base are publicly available at https://github.com/anonymous_url.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;LLM-&#35748;&#30693;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;&#26469;&#24191;&#20041;&#23545;&#35805;&#23494;&#38598;&#26816;&#32034;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#29983;&#25104;&#22810;&#32423;&#22686;&#24378;&#23545;&#35805;&#65292;&#25429;&#25417;&#22810;&#26679;&#30340;&#23545;&#35805;&#29615;&#22659;&#12290;&#20854;&#27425;&#65292;&#36890;&#36807;&#35748;&#30693;&#24863;&#30693;&#36807;&#31243;&#20943;&#23569;&#38169;&#35823;&#29983;&#25104;&#24773;&#20917;&#65292;&#24182;&#36890;&#36807;&#38590;&#24230;&#33258;&#36866;&#24212;&#26679;&#26412;&#31579;&#36873;&#22120;&#36873;&#25321;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.07092</link><description>&lt;p&gt;
&#36890;&#36807;LLM-&#35748;&#30693;&#25968;&#25454;&#22686;&#24378;&#24191;&#20041;&#23545;&#35805;&#23494;&#38598;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Generalizing Conversational Dense Retrieval via LLM-Cognition Data Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;LLM-&#35748;&#30693;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;&#26469;&#24191;&#20041;&#23545;&#35805;&#23494;&#38598;&#26816;&#32034;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#29983;&#25104;&#22810;&#32423;&#22686;&#24378;&#23545;&#35805;&#65292;&#25429;&#25417;&#22810;&#26679;&#30340;&#23545;&#35805;&#29615;&#22659;&#12290;&#20854;&#27425;&#65292;&#36890;&#36807;&#35748;&#30693;&#24863;&#30693;&#36807;&#31243;&#20943;&#23569;&#38169;&#35823;&#29983;&#25104;&#24773;&#20917;&#65292;&#24182;&#36890;&#36807;&#38590;&#24230;&#33258;&#36866;&#24212;&#26679;&#26412;&#31579;&#36873;&#22120;&#36873;&#25321;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#24335;&#25628;&#32034;&#21033;&#29992;&#22810;&#36718;&#33258;&#28982;&#35821;&#35328;&#29615;&#22659;&#26469;&#26816;&#32034;&#30456;&#20851;&#27573;&#33853;&#12290;&#29616;&#26377;&#30340;&#23545;&#35805;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#22823;&#22810;&#23558;&#23545;&#35805;&#35270;&#20026;&#19968;&#31995;&#21015;&#22266;&#23450;&#30340;&#38382;&#39064;&#21644;&#22238;&#31572;&#65292;&#24573;&#35270;&#20102;&#20005;&#37325;&#30340;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064; - &#20063;&#23601;&#26159;&#35828;&#65292;&#29992;&#25143;&#21487;&#20197;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#36827;&#34892;&#23545;&#35805;&#65292;&#32780;&#36825;&#20123;&#22791;&#36873;&#23545;&#35805;&#26159;&#26410;&#35760;&#24405;&#30340;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#32463;&#24120;&#38590;&#20197;&#25512;&#24191;&#21040;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#22810;&#26679;&#23545;&#35805;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;LLM-&#35748;&#30693;&#25968;&#25454;&#22686;&#24378;&#24191;&#20041;&#23545;&#35805;&#23494;&#38598;&#26816;&#32034;&#30340;&#26694;&#26550;(ConvAug)&#12290;ConvAug&#39318;&#20808;&#29983;&#25104;&#22810;&#32423;&#22686;&#24378;&#23545;&#35805;&#65292;&#20197;&#25429;&#25417;&#23545;&#35805;&#29615;&#22659;&#30340;&#22810;&#26679;&#24615;&#12290;&#21463;&#20154;&#31867;&#35748;&#30693;&#26041;&#24335;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#35748;&#30693;&#24863;&#30693;&#36807;&#31243;&#65292;&#20197;&#20943;&#23569;&#38169;&#35823;&#30340;&#27491;&#20363;&#12289;&#36127;&#20363;&#21644;&#24187;&#35273;&#30340;&#29983;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#38590;&#24230;&#33258;&#36866;&#24212;&#26679;&#26412;&#31579;&#36873;&#22120;&#65292;&#29992;&#20110;&#36873;&#25321;&#22797;&#26434;&#23545;&#35805;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational search utilizes muli-turn natural language contexts to retrieve relevant passages. Existing conversational dense retrieval models mostly view a conversation as a fixed sequence of questions and responses, overlooking the severe data sparsity problem -- that is, users can perform a conversation in various ways, and these alternate conversations are unrecorded. Consequently, they often struggle to generalize to diverse conversations in real-world scenarios. In this work, we propose a framework for generalizing Conversational dense retrieval via LLM-cognition data Augmentation (ConvAug). ConvAug first generates multi-level augmented conversations to capture the diverse nature of conversational contexts. Inspired by human cognition, we devise a cognition-aware process to mitigate the generation of false positives, false negatives, and hallucinations. Moreover, we develop a difficulty-adaptive sample filter that selects challenging samples for complex conversations, thereby g
&lt;/p&gt;</description></item><item><title>&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39304;&#24490;&#29615;&#21487;&#33021;&#23548;&#33268;&#19978;&#19979;&#25991;&#20869;&#22870;&#21169;&#27450;&#39575;&#65288;ICRH&#65289;&#65292;&#21363;&#35821;&#35328;&#27169;&#22411;&#22312;&#27979;&#35797;&#26102;&#22312;&#20248;&#21270;&#30446;&#26631;&#30340;&#21516;&#26102;&#21364;&#20135;&#29983;&#36127;&#38754;&#21103;&#20316;&#29992;&#12290;&#36825;&#39033;&#30740;&#31350;&#30830;&#23450;&#20102;&#20004;&#20010;&#23548;&#33268;ICRH&#30340;&#36807;&#31243;&#65306;&#36755;&#20986;&#20248;&#21270;&#21644;&#31574;&#30053;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.06627</link><description>&lt;p&gt;
&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39304;&#24490;&#29615;&#25512;&#21160;&#19978;&#19979;&#25991;&#20869;&#22870;&#21169;&#27450;&#39575;
&lt;/p&gt;
&lt;p&gt;
Feedback Loops With Language Models Drive In-Context Reward Hacking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06627
&lt;/p&gt;
&lt;p&gt;
&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39304;&#24490;&#29615;&#21487;&#33021;&#23548;&#33268;&#19978;&#19979;&#25991;&#20869;&#22870;&#21169;&#27450;&#39575;&#65288;ICRH&#65289;&#65292;&#21363;&#35821;&#35328;&#27169;&#22411;&#22312;&#27979;&#35797;&#26102;&#22312;&#20248;&#21270;&#30446;&#26631;&#30340;&#21516;&#26102;&#21364;&#20135;&#29983;&#36127;&#38754;&#21103;&#20316;&#29992;&#12290;&#36825;&#39033;&#30740;&#31350;&#30830;&#23450;&#20102;&#20004;&#20010;&#23548;&#33268;ICRH&#30340;&#36807;&#31243;&#65306;&#36755;&#20986;&#20248;&#21270;&#21644;&#31574;&#30053;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#23545;&#22806;&#37096;&#19990;&#30028;&#20135;&#29983;&#24433;&#21709;&#65306;&#23427;&#20204;&#26597;&#35810;&#21487;&#20197;&#35835;&#20889;&#32593;&#39029;&#30340;API&#65292;&#29983;&#25104;&#33021;&#22815;&#24433;&#21709;&#20154;&#31867;&#34892;&#20026;&#30340;&#20869;&#23481;&#65292;&#20197;&#21450;&#20316;&#20026;&#33258;&#20027;&#20195;&#29702;&#36816;&#34892;&#31995;&#32479;&#21629;&#20196;&#12290;&#36825;&#20123;&#20114;&#21160;&#24418;&#25104;&#20102;&#21453;&#39304;&#24490;&#29615;&#65306;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#24433;&#21709;&#19990;&#30028;&#65292;&#21453;&#36807;&#26469;&#21448;&#24433;&#21709;&#21518;&#32493;&#30340;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21453;&#39304;&#24490;&#29615;&#21487;&#33021;&#23548;&#33268;&#19978;&#19979;&#25991;&#20869;&#22870;&#21169;&#27450;&#39575;(ICRH)&#65292;&#21363;&#27979;&#35797;&#26102;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#20248;&#21270;&#65288;&#21487;&#33021;&#38544;&#21547;&#30340;&#65289;&#30446;&#26631;&#30340;&#21516;&#26102;&#65292;&#20135;&#29983;&#36127;&#38754;&#21103;&#20316;&#29992;&#12290;&#20363;&#22914;&#65292;&#32771;&#34385;&#19968;&#20010;&#34987;&#37096;&#32626;&#29992;&#20110;&#22686;&#21152;Twitter&#21442;&#19982;&#24230;&#30340;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#65307;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#22312;&#19978;&#19979;&#25991;&#31383;&#21475;&#20013;&#26816;&#32034;&#20854;&#20197;&#21069;&#30340;&#25512;&#25991;&#65292;&#24182;&#20351;&#25512;&#25991;&#26356;&#20855;&#20105;&#35758;&#24615;&#65292;&#20174;&#32780;&#22686;&#21152;&#21442;&#19982;&#24230;&#65292;&#20294;&#20063;&#22686;&#21152;&#20102;&#26377;&#27602;&#24615;&#12290;&#25105;&#20204;&#30830;&#23450;&#24182;&#30740;&#31350;&#20102;&#23548;&#33268;ICRH&#30340;&#20004;&#20010;&#36807;&#31243;&#65306;&#36755;&#20986;&#20248;&#21270;&#21644;&#31574;&#30053;&#20248;&#21270;&#12290;&#23545;&#20110;&#36825;&#20123;&#36807;&#31243;&#65292;&#38745;&#24577;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#26159;&#19981;&#36275;&#22815;&#30340;-&#20182;&#20204;&#26080;&#27861;&#25429;&#25417;&#21040;&#21453;&#39304;&#25928;&#24212;&#65292;&#20063;&#19981;&#33021;&#25429;&#25417;&#21040;&#26368;&#26377;&#23475;&#30340;&#34892;&#20026;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Language models influence the external world: they query APIs that read and write to web pages, generate content that shapes human behavior, and run system commands as autonomous agents. These interactions form feedback loops: LLM outputs affect the world, which in turn affect subsequent LLM outputs. In this work, we show that feedback loops can cause in-context reward hacking (ICRH), where the LLM at test-time optimizes a (potentially implicit) objective but creates negative side effects in the process. For example, consider an LLM agent deployed to increase Twitter engagement; the LLM may retrieve its previous tweets into the context window and make them more controversial, increasing engagement but also toxicity. We identify and study two processes that lead to ICRH: output-refinement and policy-refinement. For these processes, evaluations on static datasets are insufficient -- they miss the feedback effects and thus cannot capture the most harmful behavior. In response, we provide 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20869;&#30465;&#35268;&#21010;&#30340;&#27010;&#24565;&#65292;&#20316;&#20026;&#19968;&#31181;&#24341;&#23548;&#35821;&#35328;&#39537;&#21160;&#30340;&#20195;&#29702;&#26426;&#22120;&#20154;&#25913;&#36827;&#33258;&#36523;&#19981;&#30830;&#23450;&#24615;&#30340;&#31995;&#32479;&#26041;&#27861;&#12290;&#36890;&#36807;&#35782;&#21035;&#20219;&#21153;&#19981;&#30830;&#23450;&#24615;&#24182;&#20027;&#21160;&#23547;&#27714;&#28548;&#28165;&#65292;&#20869;&#30465;&#26174;&#33879;&#25552;&#39640;&#20102;&#26426;&#22120;&#20154;&#20219;&#21153;&#35268;&#21010;&#30340;&#25104;&#21151;&#29575;&#21644;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06529</link><description>&lt;p&gt;
&#20869;&#30465;&#35268;&#21010;&#65306;&#24341;&#23548;&#35821;&#35328;&#39537;&#21160;&#30340;&#20195;&#29702;&#26426;&#22120;&#20154;&#25913;&#36827;&#33258;&#36523;&#30340;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Introspective Planning: Guiding Language-Enabled Agents to Refine Their Own Uncertainty
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20869;&#30465;&#35268;&#21010;&#30340;&#27010;&#24565;&#65292;&#20316;&#20026;&#19968;&#31181;&#24341;&#23548;&#35821;&#35328;&#39537;&#21160;&#30340;&#20195;&#29702;&#26426;&#22120;&#20154;&#25913;&#36827;&#33258;&#36523;&#19981;&#30830;&#23450;&#24615;&#30340;&#31995;&#32479;&#26041;&#27861;&#12290;&#36890;&#36807;&#35782;&#21035;&#20219;&#21153;&#19981;&#30830;&#23450;&#24615;&#24182;&#20027;&#21160;&#23547;&#27714;&#28548;&#28165;&#65292;&#20869;&#30465;&#26174;&#33879;&#25552;&#39640;&#20102;&#26426;&#22120;&#20154;&#20219;&#21153;&#35268;&#21010;&#30340;&#25104;&#21151;&#29575;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#20808;&#36827;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20351;&#24471;&#26426;&#22120;&#20154;&#33021;&#22815;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#65292;&#24182;&#36890;&#36807;&#36866;&#24403;&#30340;&#22522;&#30784;&#22609;&#36896;&#26469;&#31574;&#30053;&#24615;&#22320;&#36827;&#34892;&#39640;&#32423;&#34892;&#21160;&#35268;&#21010;&#12290;&#28982;&#32780;&#65292;LLM&#20135;&#29983;&#30340;&#24187;&#35273;&#21487;&#33021;&#23548;&#33268;&#26426;&#22120;&#20154;&#33258;&#20449;&#22320;&#25191;&#34892;&#19982;&#29992;&#25143;&#30446;&#26631;&#19981;&#31526;&#25110;&#22312;&#26497;&#31471;&#24773;&#20917;&#19979;&#19981;&#23433;&#20840;&#30340;&#35745;&#21010;&#12290;&#27492;&#22806;&#65292;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#20013;&#30340;&#22266;&#26377;&#27495;&#20041;&#21487;&#33021;&#24341;&#21457;&#20219;&#21153;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#23384;&#22312;&#22810;&#20010;&#26377;&#25928;&#36873;&#39033;&#30340;&#24773;&#20917;&#19979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;LLMs&#24517;&#39035;&#35782;&#21035;&#27492;&#31867;&#19981;&#30830;&#23450;&#24615;&#24182;&#20027;&#21160;&#23547;&#27714;&#28548;&#28165;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#20869;&#30465;&#35268;&#21010;&#30340;&#27010;&#24565;&#65292;&#20316;&#20026;&#19968;&#31181;&#31995;&#32479;&#26041;&#27861;&#65292;&#24341;&#23548;LLMs&#22312;&#26080;&#38656;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#24418;&#25104;&#24847;&#35782;&#21040;&#19981;&#30830;&#23450;&#24615;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#25191;&#34892;&#35745;&#21010;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20219;&#21153;&#32423;&#26426;&#22120;&#20154;&#35268;&#21010;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#24182;&#35777;&#26126;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;LLM&#30340;&#35268;&#21010;&#26041;&#27861;&#30456;&#27604;&#65292;&#20869;&#30465;&#26174;&#33879;&#25552;&#39640;&#20102;&#25104;&#21151;&#29575;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) exhibit advanced reasoning skills, enabling robots to comprehend natural language instructions and strategically plan high-level actions through proper grounding. However, LLM hallucination may result in robots confidently executing plans that are misaligned with user goals or, in extreme cases, unsafe. Additionally, inherent ambiguity in natural language instructions can induce task uncertainty, particularly in situations where multiple valid options exist. To address this issue, LLMs must identify such uncertainty and proactively seek clarification. This paper explores the concept of introspective planning as a systematic method for guiding LLMs in forming uncertainty--aware plans for robotic task execution without the need for fine-tuning. We investigate uncertainty quantification in task-level robot planning and demonstrate that introspection significantly improves both success rates and safety compared to state-of-the-art LLM-based planning approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;"Learn-To-be-Efficient(LTE)"&#65292;&#25552;&#20986;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20013;&#26500;&#24314;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#39640;&#25928;&#24847;&#35782;&#30340;LLM&#23398;&#20064;&#28608;&#27963;&#26356;&#23569;&#30340;&#31070;&#32463;&#20803;&#65292;&#21462;&#24471;&#26356;&#22909;&#30340;&#31232;&#30095;&#24615;&#21644;&#24615;&#33021;&#25240;&#34935;&#12290;</title><link>https://arxiv.org/abs/2402.06126</link><description>&lt;p&gt;
&#23398;&#20064;&#21464;&#24471;&#39640;&#25928;&#65306;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26500;&#24314;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;
&lt;/p&gt;
&lt;p&gt;
Learn To be Efficient: Build Structured Sparsity in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;"Learn-To-be-Efficient(LTE)"&#65292;&#25552;&#20986;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20013;&#26500;&#24314;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#39640;&#25928;&#24847;&#35782;&#30340;LLM&#23398;&#20064;&#28608;&#27963;&#26356;&#23569;&#30340;&#31070;&#32463;&#20803;&#65292;&#21462;&#24471;&#26356;&#22909;&#30340;&#31232;&#30095;&#24615;&#21644;&#24615;&#33021;&#25240;&#34935;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20197;&#20854;&#21313;&#20159;&#32423;&#21442;&#25968;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#20135;&#29983;&#20102;&#39640;&#26114;&#30340;&#25512;&#29702;&#24320;&#38144;&#12290;&#22312;LLM&#20013;&#20986;&#29616;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#20026;&#36890;&#36807;&#20165;&#28041;&#21450;&#37096;&#20998;&#21442;&#25968;&#36827;&#34892;&#25512;&#29702;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;&#36825;&#31181;&#25104;&#26412;&#12290;&#29616;&#26377;&#26041;&#27861;&#21482;&#20851;&#27880;&#21033;&#29992;&#36825;&#31181;&#33258;&#28982;&#24418;&#25104;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#65292;&#24573;&#35270;&#20102;&#36827;&#19968;&#27493;&#25918;&#22823;&#36825;&#31181;&#22266;&#26377;&#31232;&#30095;&#24615;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20551;&#35774;LLM&#21487;&#20197;&#36890;&#36807;&#23454;&#29616;&#26356;&#32467;&#26500;&#21270;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#26469;&#23398;&#20064;&#39640;&#25928;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;"Learn-To-be-Efficient(LTE)", &#26088;&#22312;&#35757;&#32451;&#39640;&#25928;&#24847;&#35782;&#30340;LLM&#23398;&#20064;&#28608;&#27963;&#26356;&#23569;&#30340;&#31070;&#32463;&#20803;&#65292;&#24182;&#22312;&#31232;&#30095;&#24615;&#21644;&#24615;&#33021;&#20043;&#38388;&#21462;&#24471;&#26356;&#22909;&#30340;&#25240;&#34935;&#12290;&#27492;&#22806;&#65292;&#19982;&#20027;&#35201;&#20851;&#27880;&#22522;&#20110;ReLU&#27169;&#22411;&#30340;SOTA MoEfication&#26041;&#27861;&#19981;&#21516;&#65292;LTE&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#20687;GPT&#21644;LLaMA&#36825;&#26679;&#20855;&#26377;&#36719;&#28608;&#27963;&#20989;&#25968;&#30340;LLM&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#27169;&#22411;&#21644;&#21313;&#19968;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;LTE&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have achieved remarkable success with their billion-level parameters, yet they incur high inference overheads. The emergence of activation sparsity in LLMs provides a natural approach to reduce this cost by involving only parts of the parameters for inference. Existing methods only focus on utilizing this naturally formed activation sparsity, overlooking the potential for further amplifying this inherent sparsity. In this paper, we hypothesize that LLMs can learn to be efficient by achieving more structured activation sparsity.To achieve this, we introduce a novel algorithm, Learn-To-be-Efficient (LTE), designed to train efficiency-aware LLMs to learn to activate fewer neurons and achieve a better trade-off between sparsity and performance. Furthermore, unlike SOTA MoEfication methods, which mainly focus on ReLU-based models, LTE can also be applied to LLMs like GPT and LLaMA with soft activation functions. We evaluate LTE on four models and eleven datasets
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36873;&#25321;&#26631;&#20934;&#25968;&#25454;&#38598;&#20013;&#21709;&#24212;&#26368;&#38271;&#30340;1,000&#26465;&#25351;&#31034;&#20316;&#20026;&#22522;&#20934;&#32447;&#65292;&#21487;&#20197;&#22312;&#21508;&#31181;&#35821;&#35328;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#23545;&#40784;&#30340;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#23545;&#38271;&#25351;&#31034;&#36827;&#34892;&#36731;&#37327;&#32423;&#25913;&#36827;&#36827;&#19968;&#27493;&#25552;&#21319;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.04833</link><description>&lt;p&gt;
&#38271;&#24230;&#26356;&#38271;&#23545;&#40784;&#26356;&#22909;&#65306;&#19968;&#31181;&#31616;&#21333;&#20294;&#38590;&#20197;&#26395;&#20854;&#39033;&#32972;&#30340;&#25351;&#23548;&#24494;&#35843;&#22522;&#20934;&#32447;
&lt;/p&gt;
&lt;p&gt;
Long Is More for Alignment: A Simple but Tough-to-Beat Baseline for Instruction Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04833
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36873;&#25321;&#26631;&#20934;&#25968;&#25454;&#38598;&#20013;&#21709;&#24212;&#26368;&#38271;&#30340;1,000&#26465;&#25351;&#31034;&#20316;&#20026;&#22522;&#20934;&#32447;&#65292;&#21487;&#20197;&#22312;&#21508;&#31181;&#35821;&#35328;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#23545;&#40784;&#30340;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#23545;&#38271;&#25351;&#31034;&#36827;&#34892;&#36731;&#37327;&#32423;&#25913;&#36827;&#36827;&#19968;&#27493;&#25552;&#21319;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#20849;&#35782;&#35748;&#20026;&#65292;&#23545;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#23548;&#24494;&#35843;&#38656;&#35201;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#65292;&#20294;&#20855;&#20307;&#26159;&#20160;&#20040;&#21602;&#65311;LIMA&#65288;NeurIPS 2023&#65289;&#21644;AlpaGasus&#65288;ICLR 2024&#65289;&#26159;&#36873;&#25321;&#36825;&#31867;&#39640;&#36136;&#37327;&#31034;&#20363;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#23427;&#20204;&#35201;&#20040;&#36890;&#36807;&#25163;&#21160;&#25972;&#29702;&#35201;&#20040;&#20351;&#29992;GPT-3.5-Turbo&#20316;&#20026;&#36136;&#37327;&#35780;&#20998;&#22120;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20174;&#26631;&#20934;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#21709;&#24212;&#26368;&#38271;&#30340;1,000&#26465;&#25351;&#31034;&#30340;&#26497;&#31616;&#22522;&#20934;&#32447;&#22312;GPT-4&#21644;PaLM-2&#30340;&#35780;&#21028;&#19979;&#22987;&#32456;&#33021;&#22815;&#32988;&#36807;&#36825;&#20123;&#22797;&#26434;&#26041;&#27861;&#65292;&#21516;&#26102;&#22312;&#27979;&#35797;&#22522;&#20110;&#20107;&#23454;&#30693;&#35782;&#30340;OpenLLM&#22522;&#20934;&#19978;&#20445;&#25345;&#31454;&#20105;&#21147;&#12290;&#25105;&#20204;&#22312;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;Llama-2-7B&#65292;Llama-2-13B&#21644;Mistral-7B&#65289;&#21644;&#25968;&#25454;&#38598;&#65288;Alpaca-52k&#21644;Evol-Instruct-70k&#65289;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#27492;&#22806;&#65292;&#23545;&#36825;&#26679;&#30340;&#38271;&#25351;&#31034;&#36827;&#34892;&#36731;&#37327;&#32423;&#25913;&#36827;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#24182;&#20351;&#25105;&#20204;&#22312;&#21482;&#35757;&#32451;&#20102;1,000&#20010;&#20363;&#23376;&#19988;&#27809;&#26377;&#22806;&#37096;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;AlpacaEval 2.0&#19978;&#33719;&#24471;&#20102;&#22522;&#20110;Llama-2-7B&#30340;&#27169;&#22411;&#30340;&#31532;&#20108;&#39640;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a consensus that instruction fine-tuning of LLMs requires high-quality data, but what are they? LIMA (NeurIPS 2023) and AlpaGasus (ICLR 2024) are state-of-the-art methods for selecting such high-quality examples, either via manual curation or using GPT-3.5-Turbo as a quality scorer. We show that the extremely simple baseline of selecting the 1,000 instructions with longest responses from standard datasets can consistently outperform these sophisticated methods according to GPT-4 and PaLM-2 as judges, while remaining competitive on the OpenLLM benchmarks that test factual knowledge. We demonstrate this for several state-of-the-art LLMs (Llama-2-7B, Llama-2-13B, and Mistral-7B) and datasets (Alpaca-52k and Evol-Instruct-70k). In addition, a lightweight refinement of such long instructions can further improve the abilities of the fine-tuned LLMs, and allows us to obtain the 2nd highest-ranked Llama-2-7B-based model on AlpacaEval 2.0 while training on only 1,000 examples and no ex
&lt;/p&gt;</description></item><item><title>QuIP#&#26159;&#19968;&#31181;&#20351;&#29992;&#21704;&#36798;&#29595;&#24503;&#38750;&#30456;&#24178;&#24615;&#21644;&#26684;&#20070;&#30340;&#26435;&#37325;&#37327;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#26497;&#38480;&#21387;&#32553;&#33539;&#22260;&#19979;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#20855;&#26377;&#24555;&#36895;&#25512;&#29702;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.04396</link><description>&lt;p&gt;
QuIP#: &#20351;&#29992;&#21704;&#36798;&#29595;&#24503;&#38750;&#30456;&#24178;&#24615;&#21644;&#26684;&#20070;&#36827;&#34892;&#26356;&#22909;&#30340;LLM&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04396
&lt;/p&gt;
&lt;p&gt;
QuIP#&#26159;&#19968;&#31181;&#20351;&#29992;&#21704;&#36798;&#29595;&#24503;&#38750;&#30456;&#24178;&#24615;&#21644;&#26684;&#20070;&#30340;&#26435;&#37325;&#37327;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#26497;&#38480;&#21387;&#32553;&#33539;&#22260;&#19979;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#20855;&#26377;&#24555;&#36895;&#25512;&#29702;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#35757;&#32451;&#37327;&#21270;(PTQ)&#36890;&#36807;&#23558;LLM&#30340;&#26435;&#37325;&#37327;&#21270;&#20026;&#20302;&#31934;&#24230;&#26469;&#20943;&#23569;&#20854;&#20869;&#23384;&#21344;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;QuIP#&#65292;&#19968;&#31181;&#20165;&#22522;&#20110;&#26435;&#37325;&#30340;PTQ&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#19977;&#31181;&#26032;&#25216;&#26415;&#65292;&#22312;&#26497;&#38480;&#21387;&#32553;&#33539;&#22260;($\le$ 4&#27604;&#29305;&#27599;&#20010;&#26435;&#37325;)&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#39318;&#20808;&#65292;QuIP#&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#21704;&#36798;&#29595;&#24503;&#21464;&#25442;&#25913;&#36827;&#20102;QuIP&#20013;&#30340;&#38750;&#30456;&#24178;&#22788;&#29702;&#65292;&#35813;&#26041;&#27861;&#26356;&#24555;&#19988;&#20855;&#26377;&#26356;&#22909;&#30340;&#29702;&#35770;&#29305;&#24615;&#12290;&#20854;&#27425;&#65292;QuIP#&#20351;&#29992;&#21521;&#37327;&#37327;&#21270;&#25216;&#26415;&#21033;&#29992;&#20102;&#38750;&#30456;&#24178;&#26435;&#37325;&#20855;&#26377;&#30340;&#29699;&#24418;&#20122;&#39640;&#26031;&#20998;&#24067;&#29305;&#24615;&#65306;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#32452;&#22522;&#20110;&#39640;&#24230;&#23545;&#31216;$E_8$&#26684;&#20070;&#30340;&#30828;&#20214;&#39640;&#25928;&#20195;&#30721;&#20070;&#65292;&#23454;&#29616;&#20102;&#26368;&#20248;&#30340;8&#32500;&#21333;&#20301;&#29699;&#35013;&#22635;&#12290;&#31532;&#19977;&#65292;QuIP#&#20351;&#29992;&#24494;&#35843;&#26469;&#25552;&#39640;&#23545;&#21407;&#22987;&#27169;&#22411;&#30340;&#24544;&#23454;&#24230;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;QuIP#&#20248;&#20110;&#29616;&#26377;&#30340;PTQ&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#26032;&#30340;PTQ&#25193;&#23637;&#34892;&#20026;&#65292;&#24182;&#25903;&#25345;&#24555;&#36895;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-training quantization (PTQ) reduces the memory footprint of LLMs by quantizing their weights to low-precision. In this work, we introduce QuIP#, a weight-only PTQ method that achieves state-of-the-art results in extreme compression regimes ($\le$ 4 bits per weight) using three novel techniques. First, QuIP# improves the incoherence processing from QuIP by using the randomized Hadamard transform, which is faster and has better theoretical properties. Second, QuIP# uses vector quantization techniques to take advantage of the ball-shaped sub-Gaussian distribution that incoherent weights possess: specifically, we introduce a set of hardware-efficient codebooks based on the highly symmetric $E_8$ lattice, which achieves the optimal 8-dimension unit ball packing. Third, QuIP# uses fine-tuning to improve fidelity to the original model. Our experiments show that QuIP# outperforms existing PTQ methods, enables new behaviors in PTQ scaling, and supports fast inference.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#31185;&#23398;&#39046;&#22495;&#20013;&#22522;&#20110;LLM&#30340;&#26234;&#33021;&#26426;&#22120;&#20154;&#30340;&#28431;&#27934;&#19982;&#39118;&#38505;&#65292;&#24182;&#24378;&#35843;&#20102;&#23545;&#23433;&#20840;&#25514;&#26045;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04247</link><description>&lt;p&gt;
&#20248;&#20808;&#23433;&#20840;&#20445;&#38556;&#32780;&#38750;&#33258;&#27835;&#65306;&#31185;&#23398;&#20013;LLM&#26234;&#33021;&#26426;&#22120;&#20154;&#30340;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#31185;&#23398;&#39046;&#22495;&#20013;&#22522;&#20110;LLM&#30340;&#26234;&#33021;&#26426;&#22120;&#20154;&#30340;&#28431;&#27934;&#19982;&#39118;&#38505;&#65292;&#24182;&#24378;&#35843;&#20102;&#23545;&#23433;&#20840;&#25514;&#26045;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39537;&#21160;&#30340;&#26234;&#33021;&#26426;&#22120;&#20154;&#22312;&#21508;&#20010;&#23398;&#31185;&#20013;&#33258;&#20027;&#36827;&#34892;&#23454;&#39564;&#21644;&#20419;&#36827;&#31185;&#23398;&#21457;&#29616;&#26041;&#38754;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#21069;&#26223;&#12290;&#23613;&#31649;&#23427;&#20204;&#30340;&#33021;&#21147;&#38750;&#24120;&#26377;&#21069;&#36884;&#65292;&#20294;&#20063;&#24341;&#20837;&#20102;&#19968;&#20123;&#26032;&#30340;&#28431;&#27934;&#65292;&#38656;&#35201;&#20180;&#32454;&#32771;&#34385;&#23433;&#20840;&#24615;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#31354;&#30333;&#65292;&#23578;&#26410;&#23545;&#36825;&#20123;&#28431;&#27934;&#36827;&#34892;&#20840;&#38754;&#25506;&#35752;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#31185;&#23398;&#39046;&#22495;&#20013;&#22522;&#20110;LLM&#30340;&#26426;&#22120;&#20154;&#30340;&#28431;&#27934;&#36827;&#34892;&#28145;&#20837;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#35823;&#29992;&#21487;&#33021;&#24102;&#26469;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#24182;&#24378;&#35843;&#20102;&#23545;&#23433;&#20840;&#25514;&#26045;&#30340;&#38656;&#27714;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#39318;&#20808;&#20840;&#38754;&#27010;&#36848;&#20102;&#31185;&#23398;LLM&#26426;&#22120;&#20154;&#22266;&#26377;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#32771;&#34385;&#20102;&#29992;&#25143;&#24847;&#22270;&#12289;&#29305;&#23450;&#30340;&#31185;&#23398;&#39046;&#22495;&#20197;&#21450;&#23427;&#20204;&#23545;&#22806;&#37096;&#29615;&#22659;&#21487;&#33021;&#36896;&#25104;&#30340;&#24433;&#21709;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#36825;&#20123;&#28431;&#27934;&#30340;&#36215;&#28304;&#21644;&#25552;&#20379;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligent agents powered by large language models (LLMs) have demonstrated substantial promise in autonomously conducting experiments and facilitating scientific discoveries across various disciplines. While their capabilities are promising, they also introduce novel vulnerabilities that demand careful consideration for safety. However, there exists a notable gap in the literature, as there has been no comprehensive exploration of these vulnerabilities. This position paper fills this gap by conducting a thorough examination of vulnerabilities in LLM-based agents within scientific domains, shedding light on potential risks associated with their misuse and emphasizing the need for safety measures. We begin by providing a comprehensive overview of the potential risks inherent to scientific LLM agents, taking into account user intent, the specific scientific domain, and their potential impact on the external environment. Then, we delve into the origins of these vulnerabilities and provid
&lt;/p&gt;</description></item><item><title>C-RAG&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#35748;&#35777;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39118;&#38505;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#31526;&#21512;&#39118;&#38505;&#20998;&#26512;&#21644;&#29983;&#25104;&#39118;&#38505;&#30340;&#19978;&#30028;&#65292;&#30830;&#20445;&#29983;&#25104;&#32467;&#26524;&#30340;&#21487;&#20449;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03181</link><description>&lt;p&gt;
C-RAG: &#38024;&#23545;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#35777;&#29983;&#25104;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03181
&lt;/p&gt;
&lt;p&gt;
C-RAG&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#35748;&#35777;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39118;&#38505;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#31526;&#21512;&#39118;&#38505;&#20998;&#26512;&#21644;&#29983;&#25104;&#39118;&#38505;&#30340;&#19978;&#30028;&#65292;&#30830;&#20445;&#29983;&#25104;&#32467;&#26524;&#30340;&#21487;&#20449;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#20855;&#22791;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23384;&#22312;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#22914;&#24187;&#35273;&#21644;&#38169;&#20301;&#12290;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65288;RAG&#65289;&#34987;&#25552;&#20986;&#26469;&#22686;&#24378;&#29983;&#25104;&#32467;&#26524;&#30340;&#21487;&#20449;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#22806;&#37096;&#30693;&#35782;&#12290;&#20294;&#26159;&#65292;&#23545;&#20110;RAG&#27169;&#22411;&#30340;&#29983;&#25104;&#39118;&#38505;&#30340;&#29702;&#35770;&#29702;&#35299;&#23578;&#26410;&#34987;&#30740;&#31350;&#12290;&#26412;&#25991;&#22238;&#31572;&#20102;&#20197;&#19979;&#38382;&#39064;&#65306;1&#65289;RAG&#26159;&#21542;&#30830;&#23454;&#33021;&#22815;&#38477;&#20302;&#29983;&#25104;&#39118;&#38505;&#65292;2&#65289;&#22914;&#20309;&#23545;RAG&#21644;&#20256;&#32479;LLM&#30340;&#29983;&#25104;&#39118;&#38505;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#65292;&#20197;&#21450;3&#65289;&#21738;&#20123;&#20805;&#20998;&#26465;&#20214;&#20351;&#24471;RAG&#27169;&#22411;&#33021;&#22815;&#38477;&#20302;&#29983;&#25104;&#39118;&#38505;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;C-RAG&#65292;&#31532;&#19968;&#20010;&#29992;&#20110;&#35748;&#35777;RAG&#27169;&#22411;&#29983;&#25104;&#39118;&#38505;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20026;RAG&#27169;&#22411;&#25552;&#20379;&#20102;&#31526;&#21512;&#39118;&#38505;&#20998;&#26512;&#65292;&#24182;&#30830;&#20445;&#20102;&#29983;&#25104;&#39118;&#38505;&#30340;&#19978;&#30028;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#31526;&#21512;&#29983;&#25104;&#39118;&#38505;&#12290;&#25105;&#20204;&#36824;&#23545;&#19968;&#33324;&#26377;&#30028;&#39118;&#38505;&#19979;&#30340;&#31526;&#21512;&#29983;&#25104;&#39118;&#38505;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the impressive capabilities of large language models (LLMs) across diverse applications, they still suffer from trustworthiness issues, such as hallucinations and misalignments. Retrieval-augmented language models (RAG) have been proposed to enhance the credibility of generations by grounding external knowledge, but the theoretical understandings of their generation risks remains unexplored. In this paper, we answer: 1) whether RAG can indeed lead to low generation risks, 2) how to provide provable guarantees on the generation risks of RAG and vanilla LLMs, and 3) what sufficient conditions enable RAG models to reduce generation risks. We propose C-RAG, the first framework to certify generation risks for RAG models. Specifically, we provide conformal risk analysis for RAG models and certify an upper confidence bound of generation risks, which we refer to as conformal generation risk. We also provide theoretical guarantees on conformal generation risks for general bounded risk f
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#23884;&#20837;&#36870;&#36716;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;&#22810;&#35821;&#35328;&#27169;&#22411;&#26356;&#23481;&#26131;&#21463;&#21040;&#36870;&#36716;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#31616;&#21333;&#30340;&#25513;&#34109;&#38450;&#24481;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2401.12192</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#23884;&#20837;&#21453;&#21521;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
Text Embedding Inversion Security for Multilingual Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.12192
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#23884;&#20837;&#36870;&#36716;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;&#22810;&#35821;&#35328;&#27169;&#22411;&#26356;&#23481;&#26131;&#21463;&#21040;&#36870;&#36716;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#31616;&#21333;&#30340;&#25513;&#34109;&#38450;&#24481;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#25991;&#26412;&#25968;&#25454;&#36890;&#24120;&#20197;&#23454;&#25968;&#23884;&#20837;&#34920;&#31034;&#65292;&#23588;&#20854;&#26159;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#23884;&#20837;&#24335;&#26381;&#21153;&#65288;EaaS&#65289;&#30340;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#23558;&#25935;&#24863;&#20449;&#24687;&#23384;&#20648;&#20026;&#23884;&#20837;&#21487;&#33021;&#23481;&#26131;&#21463;&#21040;&#23433;&#20840;&#28431;&#27934;&#30340;&#24433;&#21709;&#65292;&#22240;&#20026;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#19981;&#30693;&#36947;&#24213;&#23618;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#25991;&#26412;&#20063;&#21487;&#20197;&#20174;&#23884;&#20837;&#20013;&#37325;&#26500;&#12290;&#23613;&#31649;&#24050;&#32463;&#25506;&#35752;&#20102;&#38450;&#24481;&#26426;&#21046;&#65292;&#20294;&#36825;&#20123;&#26426;&#21046;&#19987;&#27880;&#20110;&#33521;&#35821;&#65292;&#20351;&#20854;&#20182;&#35821;&#35328;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#26412;&#25991;&#36890;&#36807;&#22810;&#35821;&#35328;&#23884;&#20837;&#36870;&#36716;&#25506;&#35752;&#20102;LLM&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#40657;&#30418;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#36870;&#36716;&#25915;&#20987;&#30340;&#38382;&#39064;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#20102;&#23427;&#20204;&#21487;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#35821;&#35328;LLMs&#21487;&#33021;&#26356;&#23481;&#26131;&#21463;&#21040;&#36870;&#36716;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#22522;&#20110;&#33521;&#35821;&#30340;&#38450;&#24481;&#21487;&#33021;&#26080;&#25928;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25513;&#34109;&#38450;&#24481;&#26041;&#27861;&#65292;&#23545;b&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.12192v2 Announce Type: replace-cross  Abstract: Textual data is often represented as realnumbered embeddings in NLP, particularly with the popularity of large language models (LLMs) and Embeddings as a Service (EaaS). However, storing sensitive information as embeddings can be vulnerable to security breaches, as research shows that text can be reconstructed from embeddings, even without knowledge of the underlying model. While defence mechanisms have been explored, these are exclusively focused on English, leaving other languages vulnerable to attacks. This work explores LLM security through multilingual embedding inversion. We define the problem of black-box multilingual and cross-lingual inversion attacks, and thoroughly explore their potential implications. Our findings suggest that multilingual LLMs may be more vulnerable to inversion attacks, in part because English based defences may be ineffective. To alleviate this, we propose a simple masking defense effective for b
&lt;/p&gt;</description></item><item><title>&#25237;&#26426;&#35299;&#30721;&#20316;&#20026;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#30721;&#33539;&#24335;&#65292;&#33021;&#22815;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#36807;&#31243;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#27010;&#36848;&#21644;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2401.07851</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#25928;&#29575;&#30340;&#25552;&#21319;&#65306;&#25237;&#26426;&#35299;&#30721;&#30340;&#20840;&#38754;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07851
&lt;/p&gt;
&lt;p&gt;
&#25237;&#26426;&#35299;&#30721;&#20316;&#20026;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#30721;&#33539;&#24335;&#65292;&#33021;&#22815;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#36807;&#31243;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#27010;&#36848;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#33258;&#22238;&#24402;&#35299;&#30721;&#23548;&#33268;&#30340;&#25512;&#29702;&#24310;&#36831;&#65292;&#25237;&#26426;&#35299;&#30721;&#24050;&#32463;&#25104;&#20026;LLMs&#25512;&#29702;&#30340;&#19968;&#31181;&#26032;&#39062;&#35299;&#30721;&#33539;&#24335;&#12290;&#35813;&#26041;&#27861;&#22312;&#27599;&#20010;&#35299;&#30721;&#27493;&#39588;&#20013;&#39318;&#20808;&#39640;&#25928;&#22320;&#36215;&#33609;&#20960;&#20010;&#26410;&#26469;&#26631;&#35760;&#65292;&#28982;&#21518;&#24182;&#34892;&#39564;&#35777;&#36825;&#20123;&#26631;&#35760;&#12290;&#19982;&#33258;&#22238;&#24402;&#35299;&#30721;&#19981;&#21516;&#65292;&#25237;&#26426;&#35299;&#30721;&#20419;&#36827;&#20102;&#27599;&#20010;&#27493;&#39588;&#21516;&#26102;&#35299;&#30721;&#22810;&#20010;&#26631;&#35760;&#65292;&#20174;&#32780;&#21152;&#36895;&#20102;&#25512;&#29702;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#36825;&#19968;&#26377;&#21069;&#26223;&#30340;&#35299;&#30721;&#33539;&#24335;&#30340;&#20840;&#38754;&#27010;&#36848;&#21644;&#20998;&#26512;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;&#23545;&#25237;&#26426;&#35299;&#30721;&#30340;&#27491;&#24335;&#23450;&#20041;&#21644;&#20844;&#24335;&#21270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23601;&#20854;&#20851;&#38190;&#26041;&#38754;&#36827;&#34892;&#20102;&#28145;&#20837;&#35752;&#35770;&#65292;&#22914;&#36215;&#33609;&#32773;&#36873;&#25321;&#21644;&#39564;&#35777;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#31532;&#19977;&#26041;&#27979;&#35797;&#29615;&#22659;&#19979;&#23545;&#20027;&#35201;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#39033;&#24037;&#20316;&#33021;&#22815;&#25104;&#20026;&#25512;&#21160;&#36827;&#19968;&#27493;&#30740;&#31350;&#25237;&#26426;&#35299;&#30721;&#30340;&#20652;&#21270;&#21058;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.07851v2 Announce Type: replace  Abstract: To mitigate the high inference latency stemming from autoregressive decoding in Large Language Models (LLMs), Speculative Decoding has emerged as a novel decoding paradigm for LLM inference. In each decoding step, this method first drafts several future tokens efficiently and then verifies them in parallel. Unlike autoregressive decoding, Speculative Decoding facilitates the simultaneous decoding of multiple tokens per step, thereby accelerating inference. This paper presents a comprehensive overview and analysis of this promising decoding paradigm. We begin by providing a formal definition and formulation of Speculative Decoding. Then, we organize in-depth discussions on its key facets, such as drafter selection and verification strategies. Furthermore, we present a comparative analysis of leading methods under third-party testing environments. We aim for this work to serve as a catalyst for further research on Speculative Decoding,
&lt;/p&gt;</description></item><item><title>KnowGPT&#26159;&#19968;&#31181;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#40657;&#30418;&#30693;&#35782;&#27880;&#20837;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#22810;&#33218;&#32769;&#34382;&#26426;&#26500;&#24314;&#26368;&#36866;&#21512;&#27599;&#20010;&#38382;&#39064;&#30340;&#25552;&#31034;&#65292;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#20854;&#26174;&#33879;&#25552;&#21319;&#20102;&#30693;&#35782;&#27880;&#20837;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2312.06185</link><description>&lt;p&gt;
KnowGPT&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#40657;&#30418;&#30693;&#35782;&#27880;&#20837;
&lt;/p&gt;
&lt;p&gt;
KnowGPT: Black-Box Knowledge Injection for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06185
&lt;/p&gt;
&lt;p&gt;
KnowGPT&#26159;&#19968;&#31181;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#40657;&#30418;&#30693;&#35782;&#27880;&#20837;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#22810;&#33218;&#32769;&#34382;&#26426;&#26500;&#24314;&#26368;&#36866;&#21512;&#27599;&#20010;&#38382;&#39064;&#30340;&#25552;&#31034;&#65292;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#20854;&#26174;&#33879;&#25552;&#21319;&#20102;&#30693;&#35782;&#27880;&#20837;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#65292;&#25552;&#20379;&#20114;&#21160;&#24335;API&#65292;&#21487;&#20197;&#20197;&#20154;&#31867;&#19987;&#23478;&#27700;&#24179;&#22238;&#31572;&#24120;&#35265;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#20020;&#38656;&#35201;&#29305;&#23450;&#39046;&#22495;&#25110;&#19987;&#19994;&#39046;&#22495;&#30693;&#35782;&#30340;&#38382;&#39064;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#20250;&#32473;&#20986;&#19981;&#20934;&#30830;&#25110;&#19981;&#27491;&#30830;&#30340;&#21709;&#24212;&#65292;&#36825;&#20123;&#30693;&#35782;&#24182;&#26410;&#21253;&#21547;&#22312;&#23427;&#20204;&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;LLMs&#24182;&#38750;&#24320;&#28304;&#65292;&#36825;&#20351;&#24471;&#20165;&#20351;&#29992;&#27169;&#22411;API&#27880;&#20837;&#30693;&#35782;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;KnowGPT&#65292;&#19968;&#31181;&#29992;&#20110;LLMs&#22312;&#38382;&#31572;&#20013;&#30340;&#40657;&#30418;&#30693;&#35782;&#27880;&#20837;&#26694;&#26550;&#12290;KnowGPT&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20174;&#30693;&#35782;&#22270;&#20013;&#25552;&#21462;&#30456;&#20851;&#30693;&#35782;&#65292;&#24182;&#20351;&#29992;&#22810;&#33218;&#32769;&#34382;&#26426;&#65288;MAB&#65289;&#20026;&#27599;&#20010;&#38382;&#39064;&#26500;&#24314;&#26368;&#21512;&#36866;&#30340;&#25552;&#31034;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;KnowGPT&#26174;&#33879;&#22686;&#24378;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;KnowGPT&#24179;&#22343;&#25913;&#36827;&#20102;23%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.06185v2 Announce Type: replace-cross  Abstract: Generative Large Language Models (LLMs), such as ChatGPT, offer interactive APIs that can answer common questions at a human-expert level. However, these models often give inaccurate or incorrect responses when faced with questions requiring domain-specific or professional-specific knowledge not covered in their training corpus. Furthermore, many state-of-the-art LLMs are not open-source, making it challenging to inject knowledge with model APIs only. In this work, we introduce KnowGPT, a black-box knowledge injection framework for LLMs in question answering. KnowGPT leverages deep reinforcement learning (RL) to extract relevant knowledge from Knowledge Graphs (KGs) and use Multi-Armed Bandit (MAB) to construct the most suitable prompt for each question. Our extensive experiments on three benchmark datasets showcase that KnowGPT significantly enhances the existing methods. Notably, KnowGPT achieves an average improvement of 23.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20851;&#27880;&#20998;&#37197;&#30340;&#27874;&#24418;&#27169;&#24335;&#23545;&#20854;&#22312;&#38656;&#35201;&#39640;&#24230;&#19978;&#19979;&#25991;&#24847;&#35782;&#30340;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Attention Buckets&#8221;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20010;&#24182;&#34892;&#36807;&#31243;&#21644;&#19981;&#21516;&#30340;&#26059;&#36716;&#20301;&#32622;&#23884;&#20837;&#35282;&#24230;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#23545;&#19981;&#21516;&#19978;&#19979;&#25991;&#20301;&#32622;&#30340;&#24847;&#35782;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#24573;&#35270;&#20851;&#38190;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2312.04455</link><description>&lt;p&gt;
&#24378;&#21270;&#20851;&#27880;&#21147;&#20013;&#26368;&#30701;&#30340;&#25903;&#26609;&#65306;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#24847;&#35782;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#24037;&#20855;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
Fortify the Shortest Stave in Attention: Enhancing Context Awareness of Large Language Models for Effective Tool Use
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20851;&#27880;&#20998;&#37197;&#30340;&#27874;&#24418;&#27169;&#24335;&#23545;&#20854;&#22312;&#38656;&#35201;&#39640;&#24230;&#19978;&#19979;&#25991;&#24847;&#35782;&#30340;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Attention Buckets&#8221;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20010;&#24182;&#34892;&#36807;&#31243;&#21644;&#19981;&#21516;&#30340;&#26059;&#36716;&#20301;&#32622;&#23884;&#20837;&#35282;&#24230;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#23545;&#19981;&#21516;&#19978;&#19979;&#25991;&#20301;&#32622;&#30340;&#24847;&#35782;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#24573;&#35270;&#20851;&#38190;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20013;&#20851;&#27880;&#20998;&#37197;&#20013;&#30340;&#20869;&#22312;&#27874;&#24418;&#27169;&#24335;&#26174;&#33879;&#24433;&#21709;&#23427;&#20204;&#22312;&#38656;&#35201;&#39640;&#24230;&#19978;&#19979;&#25991;&#24847;&#35782;&#30340;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#21033;&#29992;LLMs&#36827;&#34892;&#24037;&#20855;&#20351;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24403;&#20851;&#38190;&#20449;&#24687;&#22312;&#19978;&#19979;&#25991;&#20013;&#20301;&#20110;&#20851;&#27880;&#27874;&#24418;&#30340;&#20302;&#35895;&#21306;&#22495;&#26102;&#65292;&#27169;&#22411;&#21487;&#33021;&#20250;&#24573;&#35270;&#35813;&#20449;&#24687;&#65292;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Attention Buckets&#8221;&#30340;&#26032;&#22411;&#25512;&#29702;&#26041;&#27861;&#12290;&#23427;&#20801;&#35768;LLMs&#36890;&#36807;&#22810;&#20010;&#24182;&#34892;&#36807;&#31243;&#22788;&#29702;&#36755;&#20837;&#12290;&#27599;&#20010;&#36807;&#31243;&#20351;&#29992;&#19981;&#21516;&#30340;&#22522;&#20934;&#35282;&#24230;&#36827;&#34892;&#26059;&#36716;&#20301;&#32622;&#23884;&#20837;&#65292;&#20174;&#32780;&#21019;&#24314;&#20986;&#19968;&#20010;&#29420;&#29305;&#30340;&#20851;&#27880;&#27874;&#24418;&#12290;&#36890;&#36807;&#29992;&#19968;&#20010;&#36807;&#31243;&#30340;&#20851;&#27880;&#20302;&#35895;&#34917;&#20607;&#21478;&#19968;&#20010;&#36807;&#31243;&#30340;&#20851;&#27880;&#39640;&#23792;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;LLM&#23545;&#19981;&#21516;&#19978;&#19979;&#25991;&#20301;&#32622;&#30340;&#24847;&#35782;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#24573;&#35270;&#20851;&#38190;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we demonstrate that an inherent waveform pattern in the attention allocation of large language models (LLMs) significantly affects their performance in tasks demanding a high degree of context awareness, such as utilizing LLMs for tool-use. Specifically, the crucial information in the context will be potentially overlooked by model when it is positioned in the trough zone of the attention waveform, leading to decreased performance. To address this issue, we propose a novel inference method named Attention Buckets. It allows LLMs to process their input through multiple parallel processes. Each process utilizes a distinct base angle for the rotary position embedding, thereby creating a unique attention waveform. By compensating an attention trough of a particular process with an attention peak of another process, our approach enhances LLM's awareness to various contextual positions, thus mitigating the risk of overlooking crucial information. In the largest tool-use benchm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25688;&#35201;&#20013;&#19978;&#19979;&#25991;&#21033;&#29992;&#30340;&#38382;&#39064;&#65292;&#21457;&#29616;&#20102;&#25688;&#35201;&#20219;&#21153;&#20013;&#20851;&#20110;&#36755;&#20837;&#20301;&#32622;&#30340;&#24615;&#33021;&#27169;&#24335;&#20197;&#21450;&#28304;&#25991;&#20214;&#21040;&#25688;&#35201;&#30340;&#20869;&#23481;&#26144;&#23556;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2310.10570</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25688;&#35201;&#20013;&#19978;&#19979;&#25991;&#21033;&#29992;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Context Utilization in Summarization with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.10570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25688;&#35201;&#20013;&#19978;&#19979;&#25991;&#21033;&#29992;&#30340;&#38382;&#39064;&#65292;&#21457;&#29616;&#20102;&#25688;&#35201;&#20219;&#21153;&#20013;&#20851;&#20110;&#36755;&#20837;&#20301;&#32622;&#30340;&#24615;&#33021;&#27169;&#24335;&#20197;&#21450;&#28304;&#25991;&#20214;&#21040;&#25688;&#35201;&#30340;&#20869;&#23481;&#26144;&#23556;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25277;&#35937;&#25688;&#35201;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#25552;&#20379;&#27969;&#30021;&#19988;&#30456;&#20851;&#30340;&#25688;&#35201;&#12290;&#26368;&#36817;&#30340;&#36827;&#23637;&#25193;&#23637;&#20102;&#23427;&#20204;&#22788;&#29702;&#38271;&#36755;&#20837;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#65292;&#36229;&#36807;&#20102;100k&#20010;&#26631;&#35760;&#12290;&#28982;&#32780;&#65292;&#22312;&#38382;&#31572;&#20013;&#65292;&#35821;&#35328;&#27169;&#22411;&#23545;&#20854;&#36755;&#20837;&#19978;&#19979;&#25991;&#30340;&#21033;&#29992;&#19981;&#22343;&#21248;&#12290;&#23427;&#20204;&#20542;&#21521;&#20110;&#20559;&#29233;&#21021;&#22987;&#21644;&#26368;&#32456;&#27573;&#33853;&#65292;&#23548;&#33268;&#20102;&#20851;&#20110;&#31572;&#26696;&#22312;&#36755;&#20837;&#20013;&#20301;&#32622;&#30340;U&#24418;&#24615;&#33021;&#27169;&#24335;&#12290;&#36825;&#31181;&#20559;&#35265;&#24341;&#21457;&#20102;&#25285;&#24551;&#65292;&#29305;&#21035;&#26159;&#22312;&#25688;&#35201;&#20013;&#65292;&#20851;&#38190;&#20869;&#23481;&#21487;&#33021;&#20998;&#25955;&#22312;&#28304;&#25991;&#20214;&#20013;&#12290;&#27492;&#22806;&#65292;&#22312;&#25688;&#35201;&#20013;&#65292;&#20174;&#28304;&#25991;&#20214;&#21040;&#25688;&#35201;&#30340;&#20107;&#23454;&#26144;&#23556;&#24182;&#19981;&#26159;&#24494;&#19981;&#36275;&#36947;&#30340;&#65292;&#22240;&#20026;&#26174;&#33879;&#20869;&#23481;&#36890;&#24120;&#20250;&#34987;&#37325;&#26032;&#34920;&#36848;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#25688;&#35201;&#20013;&#19978;&#19979;&#25991;&#21033;&#29992;&#21644;&#20301;&#32622;&#20559;&#35265;&#36827;&#34892;&#20102;&#31532;&#19968;&#27425;&#20840;&#38754;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#28085;&#30422;&#20102;5&#20010;LLMs&#65292;10&#20010;&#25968;&#25454;&#38598;&#21644;5&#20010;&#35780;&#20272;&#25351;&#26631;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.10570v3 Announce Type: replace  Abstract: Large language models (LLMs) excel in abstractive summarization tasks, delivering fluent and pertinent summaries. Recent advancements have extended their capabilities to handle long-input contexts, exceeding 100k tokens. However, in question answering, language models exhibit uneven utilization of their input context. They tend to favor the initial and final segments, resulting in a U-shaped performance pattern concerning where the answer is located within the input. This bias raises concerns, particularly in summarization where crucial content may be dispersed throughout the source document(s). Besides, in summarization, mapping facts from the source to the summary is not trivial as salient content is usually re-phrased. In this paper, we conduct the first comprehensive study on context utilization and position bias in summarization. Our analysis encompasses 5 LLMs, 10 datasets, and 5 evaluation metrics. We introduce a new evaluatio
&lt;/p&gt;</description></item><item><title>InfoLossQA&#26159;&#19968;&#20010;&#38024;&#23545;&#25991;&#26412;&#31616;&#21270;&#20013;&#20449;&#24687;&#25439;&#22833;&#30340;&#29305;&#24449;&#21270;&#19982;&#24674;&#22797;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#38382;&#31572;&#23545;&#30340;&#24418;&#24335;&#65292;&#24110;&#21161;&#35835;&#32773;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#25991;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20449;&#24687;&#25439;&#22833;&#39057;&#32321;&#21457;&#29983;&#65292;&#32780;QA&#23545;&#21017;&#33021;&#25552;&#20379;&#21738;&#20123;&#20449;&#24687;&#34987;&#20002;&#22833;&#30340;&#24635;&#32467;&#12290;</title><link>http://arxiv.org/abs/2401.16475</link><description>&lt;p&gt;
InfoLossQA: &#25991;&#26412;&#31616;&#21270;&#20013;&#20449;&#24687;&#25439;&#22833;&#30340;&#29305;&#24449;&#21270;&#19982;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
InfoLossQA: Characterizing and Recovering Information Loss in Text Simplification. (arXiv:2401.16475v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16475
&lt;/p&gt;
&lt;p&gt;
InfoLossQA&#26159;&#19968;&#20010;&#38024;&#23545;&#25991;&#26412;&#31616;&#21270;&#20013;&#20449;&#24687;&#25439;&#22833;&#30340;&#29305;&#24449;&#21270;&#19982;&#24674;&#22797;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#38382;&#31572;&#23545;&#30340;&#24418;&#24335;&#65292;&#24110;&#21161;&#35835;&#32773;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#25991;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20449;&#24687;&#25439;&#22833;&#39057;&#32321;&#21457;&#29983;&#65292;&#32780;QA&#23545;&#21017;&#33021;&#25552;&#20379;&#21738;&#20123;&#20449;&#24687;&#34987;&#20002;&#22833;&#30340;&#24635;&#32467;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#31616;&#21270;&#26088;&#22312;&#20351;&#19987;&#19994;&#25991;&#26412;&#23545;&#26222;&#36890;&#35835;&#32773;&#26356;&#26131;&#29702;&#35299;&#65292;&#20294;&#24120;&#24120;&#23548;&#33268;&#20449;&#24687;&#21024;&#38500;&#21644;&#27169;&#31946;&#19981;&#28165;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;InfoLossQA&#26694;&#26550;&#65292;&#29992;&#20197;&#29305;&#24449;&#21270;&#21644;&#24674;&#22797;&#30001;&#31616;&#21270;&#24341;&#36215;&#30340;&#20449;&#24687;&#25439;&#22833;&#65292;&#20197;&#38382;&#31572;&#65288;QA&#65289;&#23545;&#30340;&#24418;&#24335;&#21576;&#29616;&#12290;&#22522;&#20110;&#8220;&#38382;&#39064;&#35752;&#35770;&#8221;&#29702;&#35770;&#65292;&#36825;&#20123;QA&#23545;&#26088;&#22312;&#24110;&#21161;&#35835;&#32773;&#28145;&#20837;&#20102;&#35299;&#25991;&#26412;&#12290;&#25105;&#20204;&#23545;&#35813;&#26694;&#26550;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#30001;104&#20010;&#21307;&#23398;&#30740;&#31350;&#31185;&#23398;&#25688;&#35201;&#30340;104&#20010;LLM&#31616;&#21270;&#20013;&#25152;&#34893;&#29983;&#30340;1000&#20010;&#35821;&#35328;&#23398;&#23478;&#31574;&#21010;&#30340;QA&#23545;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#25968;&#25454;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#20449;&#24687;&#25439;&#22833;&#32463;&#24120;&#21457;&#29983;&#65292;&#24182;&#19988;QA&#23545;&#21487;&#20197;&#39640;&#23618;&#27425;&#22320;&#24635;&#32467;&#20986;&#21738;&#20123;&#20449;&#24687;&#34987;&#20002;&#22833;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#23436;&#25104;&#27492;&#20219;&#21153;&#65306;&#31471;&#21040;&#31471;&#20419;&#20351;&#24320;&#28304;&#21644;&#21830;&#19994;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#27969;&#27700;&#32447;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#32771;&#34385;&#20102;QA&#23545;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text simplification aims to make technical texts more accessible to laypeople but often results in deletion of information and vagueness. This work proposes InfoLossQA, a framework to characterize and recover simplification-induced information loss in form of question-and-answer (QA) pairs. Building on the theory of Question Under Discussion, the QA pairs are designed to help readers deepen their knowledge of a text. We conduct a range of experiments with this framework. First, we collect a dataset of 1,000 linguist-curated QA pairs derived from 104 LLM simplifications of scientific abstracts of medical studies. Our analyses of this data reveal that information loss occurs frequently, and that the QA pairs give a high-level overview of what information was lost. Second, we devise two methods for this task: end-to-end prompting of open-source and commercial language models, and a natural language inference pipeline. With a novel evaluation framework considering the correctness of QA pai
&lt;/p&gt;</description></item><item><title>PROXYQA&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38271;&#31687;&#25991;&#26412;&#29983;&#25104;&#30340;&#26367;&#20195;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#35814;&#23613;&#30340;&#20869;&#23481;&#65292;&#24182;&#21033;&#29992;&#35780;&#20272;&#22120;&#21644;&#29983;&#25104;&#20869;&#23481;&#20316;&#20026;&#32972;&#26223;&#29615;&#22659;&#65292;&#26681;&#25454;&#35780;&#20272;&#22120;&#22238;&#31572;&#20195;&#29702;&#38382;&#39064;&#30340;&#34920;&#29616;&#26469;&#35780;&#20272;&#29983;&#25104;&#20869;&#23481;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.15042</link><description>&lt;p&gt;
PROXYQA&#65306;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38271;&#31687;&#25991;&#26412;&#29983;&#25104;&#30340;&#26367;&#20195;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
PROXYQA: An Alternative Framework for Evaluating Long-Form Text Generation with Large Language Models. (arXiv:2401.15042v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15042
&lt;/p&gt;
&lt;p&gt;
PROXYQA&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38271;&#31687;&#25991;&#26412;&#29983;&#25104;&#30340;&#26367;&#20195;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#35814;&#23613;&#30340;&#20869;&#23481;&#65292;&#24182;&#21033;&#29992;&#35780;&#20272;&#22120;&#21644;&#29983;&#25104;&#20869;&#23481;&#20316;&#20026;&#32972;&#26223;&#29615;&#22659;&#65292;&#26681;&#25454;&#35780;&#20272;&#22120;&#22238;&#31572;&#20195;&#29702;&#38382;&#39064;&#30340;&#34920;&#29616;&#26469;&#35780;&#20272;&#29983;&#25104;&#20869;&#23481;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#38271;&#31687;&#25991;&#26412;&#29702;&#35299;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#29983;&#25104;&#38271;&#31687;&#20869;&#23481;&#65288;&#22914;&#25253;&#21578;&#21644;&#25991;&#31456;&#65289;&#30340;&#33021;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#24403;&#21069;&#30340;&#22522;&#20934;&#19981;&#36275;&#20197;&#20805;&#20998;&#35780;&#20272;LLMs&#29983;&#25104;&#20449;&#24687;&#20016;&#23500;&#19988;&#20840;&#38754;&#30340;&#20869;&#23481;&#65292;&#22240;&#27492;&#38656;&#35201;&#19968;&#31181;&#26356;&#20005;&#26684;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;\textsc{ProxyQA}&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#38271;&#31687;&#25991;&#26412;&#29983;&#25104;&#65292;&#21253;&#25324;&#28145;&#20837;&#20154;&#24037;&#31574;&#21010;&#30340;&#28085;&#30422;&#22810;&#20010;&#39046;&#22495;&#30340;&#8220;&#20803;&#38382;&#39064;&#8221;&#12290;&#27599;&#20010;&#20803;&#38382;&#39064;&#37117;&#21253;&#21547;&#30456;&#24212;&#30340;&#24102;&#27880;&#37322;&#31572;&#26696;&#30340;&#8220;&#20195;&#29702;&#38382;&#39064;&#8221;&#12290;LLMs&#34987;&#35201;&#27714;&#26681;&#25454;&#36825;&#20123;&#20803;&#38382;&#39064;&#29983;&#25104;&#35814;&#23613;&#30340;&#20869;&#23481;&#12290;&#21033;&#29992;&#35780;&#20272;&#22120;&#24182;&#23558;&#29983;&#25104;&#30340;&#20869;&#23481;&#20316;&#20026;&#32972;&#26223;&#29615;&#22659;&#65292;\textsc{ProxyQA}&#26681;&#25454;&#35780;&#20272;&#22120;&#22238;&#31572;&#8220;&#20195;&#29702;&#38382;&#39064;&#8221;&#30340;&#34920;&#29616;&#35780;&#20272;&#29983;&#25104;&#20869;&#23481;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#26816;&#39564;&#20102;&#22810;&#20010;LLMs&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have exhibited remarkable success in long-form context comprehension tasks. However, their capacity to generate long contents, such as reports and articles, remains insufficiently explored. Current benchmarks do not adequately assess LLMs' ability to produce informative and comprehensive content, necessitating a more rigorous evaluation approach. In this study, we introduce \textsc{ProxyQA}, a framework for evaluating long-form text generation, comprising in-depth human-curated \textit{meta-questions} spanning various domains. Each meta-question contains corresponding \textit{proxy-questions} with annotated answers. LLMs are prompted to generate extensive content in response to these meta-questions. Utilizing an evaluator and incorporating generated content as background context, \textsc{ProxyQA} evaluates the quality of generated content based on the evaluator's performance in answering the \textit{proxy-questions}. We examine multiple LLMs, emphasizing \t
&lt;/p&gt;</description></item><item><title>SEER&#26159;&#19968;&#31181;&#36890;&#36807;&#26368;&#22823;&#21270;&#22522;&#20110;&#32467;&#26500;&#30340;&#22238;&#25253;&#26469;&#20419;&#36827;&#32467;&#26500;&#21270;&#25512;&#29702;&#21644;&#35299;&#37322;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.13246</link><description>&lt;p&gt;
SEER: &#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#20419;&#36827;&#32467;&#26500;&#21270;&#25512;&#29702;&#21644;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
SEER: Facilitating Structured Reasoning and Explanation via Reinforcement Learning. (arXiv:2401.13246v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13246
&lt;/p&gt;
&lt;p&gt;
SEER&#26159;&#19968;&#31181;&#36890;&#36807;&#26368;&#22823;&#21270;&#22522;&#20110;&#32467;&#26500;&#30340;&#22238;&#25253;&#26469;&#20419;&#36827;&#32467;&#26500;&#21270;&#25512;&#29702;&#21644;&#35299;&#37322;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38416;&#26126;&#20174;&#38382;&#39064;&#21040;&#31572;&#26696;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#35299;&#37322;&#26159;&#26681;&#26412;&#37325;&#35201;&#30340;&#65292;&#22240;&#20026;&#23427;&#26174;&#33879;&#22686;&#24378;&#20102;&#38382;&#31572;&#31995;&#32479;&#30340;&#35299;&#37322;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;&#28982;&#32780;&#65292;&#32467;&#26500;&#21270;&#35299;&#37322;&#35201;&#27714;&#27169;&#22411;&#36827;&#34892;&#22797;&#26434;&#30340;&#32467;&#26500;&#21270;&#25512;&#29702;&#65292;&#36825;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#38598;&#20013;&#22312;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#21333;&#27493;&#25512;&#29702;&#65292;&#24573;&#35270;&#27493;&#39588;&#20043;&#38388;&#30340;&#36923;&#36753;&#20381;&#36182;&#20851;&#31995;&#12290;&#21516;&#26102;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#26041;&#27861;&#24573;&#35270;&#20102;&#32467;&#26500;&#21270;&#20851;&#31995;&#65292;&#38459;&#30861;&#20102;RL&#22312;&#32467;&#26500;&#21270;&#25512;&#29702;&#20013;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SEER&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#22522;&#20110;&#32467;&#26500;&#30340;&#22238;&#25253;&#65292;&#20197;&#20419;&#36827;&#32467;&#26500;&#21270;&#25512;&#29702;&#21644;&#35299;&#37322;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20110;&#32467;&#26500;&#30340;&#22238;&#25253;&#20934;&#30830;&#25551;&#36848;&#20102;&#32467;&#26500;&#21270;&#25512;&#29702;&#20013;&#22266;&#26377;&#30340;&#20998;&#23618;&#21644;&#20998;&#25903;&#32467;&#26500;&#65292;&#26377;&#25928;&#22320;&#25429;&#25417;&#20102;&#29366;&#24577;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Elucidating the reasoning process with structured explanations from question to answer is fundamentally crucial, as it significantly enhances the interpretability and trustworthiness of question-answering (QA) systems. However, structured explanations demand models to perform intricate structured reasoning, which poses great challenges. Most existing methods focus on single-step reasoning through supervised learning, ignoring logical dependencies between steps. Meanwhile, existing reinforcement learning (RL)-based methods overlook the structured relationships, impeding RL's potential in structured reasoning. In this paper, we propose SEER, a novel method that maximizes a structure-based return to facilitate structured reasoning and explanation. Our proposed structure-based return precisely describes the hierarchical and branching structure inherent in structured reasoning, effectively capturing the intricate relationships between states. We also introduce a fine-grained reward function
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#20998;&#31867;&#31995;&#32479;&#65292;&#29992;&#20110;&#23450;&#20041;&#21487;&#20998;&#32423;&#30340;&#32763;&#35793;&#25552;&#31034;&#65292;&#20197;&#24110;&#21161;&#26500;&#24314;&#36866;&#29992;&#20110;&#19981;&#21516;&#32763;&#35793;&#20219;&#21153;&#30340;&#20855;&#26377;&#19981;&#21516;&#29305;&#24615;&#30340;&#25552;&#31034;&#12290;&#39564;&#35777;&#21644;&#35828;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.09984</link><description>&lt;p&gt;
&#21487;&#20998;&#32423;ChatGPT&#32763;&#35793;&#35780;&#20215;
&lt;/p&gt;
&lt;p&gt;
Gradable ChatGPT Translation Evaluation. (arXiv:2401.09984v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#20998;&#31867;&#31995;&#32479;&#65292;&#29992;&#20110;&#23450;&#20041;&#21487;&#20998;&#32423;&#30340;&#32763;&#35793;&#25552;&#31034;&#65292;&#20197;&#24110;&#21161;&#26500;&#24314;&#36866;&#29992;&#20110;&#19981;&#21516;&#32763;&#35793;&#20219;&#21153;&#30340;&#20855;&#26377;&#19981;&#21516;&#29305;&#24615;&#30340;&#25552;&#31034;&#12290;&#39564;&#35777;&#21644;&#35828;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#20316;&#20026;&#19968;&#31181;&#22522;&#20110;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#26426;&#22120;&#32763;&#35793;&#39046;&#22495;&#20135;&#29983;&#20102;&#28145;&#36828;&#24433;&#21709;&#12290;&#22312;ChatGPT&#20013;&#65292;&#8220;&#25552;&#31034;&#8221;&#26159;&#25351;&#29992;&#20110;&#24341;&#23548;&#27169;&#22411;&#29983;&#25104;&#29305;&#23450;&#31867;&#22411;&#22238;&#24212;&#30340;&#25991;&#26412;&#27573;&#33853;&#25110;&#25351;&#23548;&#12290;&#32763;&#35793;&#25552;&#31034;&#30340;&#35774;&#35745;&#25104;&#20026;&#24433;&#21709;&#32763;&#35793;&#39118;&#26684;&#12289;&#20934;&#30830;&#24615;&#21644;&#31934;&#30830;&#24230;&#31561;&#22240;&#32032;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#32570;&#20047;&#19968;&#20010;&#20849;&#21516;&#30340;&#26631;&#20934;&#21644;&#26041;&#27861;&#26469;&#35774;&#35745;&#21644;&#36873;&#25321;&#32763;&#35793;&#25552;&#31034;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#20998;&#31867;&#31995;&#32479;&#65292;&#20197;&#34920;&#36798;&#31867;&#22411;&#12289;&#32763;&#35793;&#39118;&#26684;&#12289;POS&#20449;&#24687;&#21644;&#26174;&#24335;&#22768;&#26126;&#30340;&#26041;&#24335;&#23450;&#20041;&#21487;&#20998;&#32423;&#30340;&#32763;&#35793;&#25552;&#31034;&#65292;&#20174;&#32780;&#20026;&#19981;&#21516;&#30340;&#32763;&#35793;&#20219;&#21153;&#26500;&#24314;&#20855;&#26377;&#19981;&#21516;&#29305;&#24615;&#30340;&#25552;&#31034;&#12290;&#36873;&#25321;&#20102;&#20855;&#20307;&#30340;&#23454;&#39564;&#21644;&#26696;&#20363;&#26469;&#39564;&#35777;&#21644;&#35828;&#26126;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT, as a language model based on large-scale pre-training, has exerted a profound influence on the domain of machine translation. In ChatGPT, a "Prompt" refers to a segment of text or instruction employed to steer the model towards generating a specific category of response. The design of the translation prompt emerges as a key aspect that can wield influence over factors such as the style, precision and accuracy of the translation to a certain extent. However, there is a lack of a common standard and methodology on how to design and select a translation prompt. Accordingly, this paper proposes a generic taxonomy, which defines gradable translation prompts in terms of expression type, translation style, POS information and explicit statement, thus facilitating the construction of prompts endowed with distinct attributes tailored for various translation tasks. Specific experiments and cases are selected to validate and illustrate the effectiveness of the method.
&lt;/p&gt;</description></item><item><title>LinguAlchemy&#26159;&#19968;&#31181;&#23558;&#35821;&#35328;&#31867;&#22411;&#23398;&#21644;&#22320;&#29702;&#20803;&#32032;&#34701;&#21512;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#26410;&#35265;&#35821;&#35328;&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.06034</link><description>&lt;p&gt;
LinguAlchemy: &#23558;&#35821;&#35328;&#31867;&#22411;&#23398;&#21644;&#22320;&#29702;&#20803;&#32032;&#34701;&#21512;&#20197;&#23454;&#29616;&#23545;&#26410;&#35265;&#35821;&#35328;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
LinguAlchemy: Fusing Typological and Geographical Elements for Unseen Language Generalization. (arXiv:2401.06034v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06034
&lt;/p&gt;
&lt;p&gt;
LinguAlchemy&#26159;&#19968;&#31181;&#23558;&#35821;&#35328;&#31867;&#22411;&#23398;&#21644;&#22320;&#29702;&#20803;&#32032;&#34701;&#21512;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#26410;&#35265;&#35821;&#35328;&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#22810;&#20010;&#20219;&#21153;&#21644;&#35821;&#35328;&#19978;&#23637;&#31034;&#20986;&#20102;&#38750;&#20961;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#65292;PLMs&#30340;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#65292;&#23548;&#33268;&#35821;&#35328;&#24615;&#33021;&#26126;&#26174;&#19979;&#38477;&#65292;&#29978;&#33267;&#29983;&#25104;&#30340;&#22238;&#24212;&#19982;&#38543;&#26426;&#22522;&#20934;&#30456;&#24403;&#33618;&#21776;&#12290;&#36825;&#19968;&#38480;&#21046;&#19968;&#30452;&#20197;&#26469;&#37117;&#26159;PLMs&#30340;&#19968;&#20010;&#38271;&#26399;&#38382;&#39064;&#65292;&#28041;&#21450;&#21040;&#35821;&#35328;&#24314;&#27169;&#25216;&#26415;&#30340;&#22810;&#26679;&#24615;&#21644;&#24179;&#31561;&#33719;&#21462;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;LinguAlchemy&#26469;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#36825;&#26159;&#19968;&#31181;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#23558;&#35821;&#35328;&#30340;&#21508;&#20010;&#26041;&#38754;&#65288;&#21253;&#25324;&#31867;&#22411;&#23398;&#12289;&#22320;&#29702;&#21644;&#35821;&#31995;&#65289;&#32435;&#20837;PLMs&#30340;&#34920;&#31034;&#20013;&#65292;&#20197;&#26356;&#22909;&#22320;&#34920;&#24449;&#30456;&#24212;&#30340;&#35821;&#35328;&#32422;&#26463;&#12290;&#19982;&#23436;&#20840;&#24494;&#35843;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;LinguAlchemy&#26174;&#33879;&#25552;&#39640;&#20102;mBERT&#21644;XLM-R&#23545;&#26410;&#35265;&#35821;&#35328;&#30340;&#20934;&#30830;&#24615;&#32489;&#25928;&#65292;&#20998;&#21035;&#25552;&#39640;&#20102;&#32422;18%&#21644;&#32422;2%&#65292;&#23637;&#29616;&#20986;&#36739;&#39640;&#30340;&#26410;&#35265;&#35821;&#35328;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models (PLMs) have shown remarkable generalization toward multiple tasks and languages. Nonetheless, the generalization of PLMs towards unseen languages is poor, resulting in significantly worse language performance, or even generating nonsensical responses that are comparable to a random baseline. This limitation has been a longstanding problem of PLMs raising the problem of diversity and equal access to language modeling technology. In this work, we solve this limitation by introducing LinguAlchemy, a regularization technique that incorporates various aspects of languages covering typological, geographical, and phylogenetic constraining the resulting representation of PLMs to better characterize the corresponding linguistics constraints. LinguAlchemy significantly improves the accuracy performance of mBERT and XLM-R on unseen languages by ~18% and ~2%, respectively compared to fully finetuned models and displaying a high degree of unseen language generalization. W
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;rebus&#35868;&#39064;&#19978;&#24615;&#33021;&#30340;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#21457;&#29616;&#19987;&#26377;&#27169;&#22411;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27979;&#35797;&#27169;&#22411;&#65292;&#20294;&#26368;&#20339;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#20165;&#20026;24%&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#21487;&#29992;&#20110;&#35782;&#21035;&#30693;&#35782;&#19978;&#30340;&#20027;&#35201;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2401.05604</link><description>&lt;p&gt;
REBUS: &#19968;&#31181;&#23545;&#31526;&#21495;&#29702;&#35299;&#36827;&#34892;&#40065;&#26834;&#35780;&#20272;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
REBUS: A Robust Evaluation Benchmark of Understanding Symbols. (arXiv:2401.05604v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05604
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;rebus&#35868;&#39064;&#19978;&#24615;&#33021;&#30340;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#21457;&#29616;&#19987;&#26377;&#27169;&#22411;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27979;&#35797;&#27169;&#22411;&#65292;&#20294;&#26368;&#20339;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#20165;&#20026;24%&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#21487;&#29992;&#20110;&#35782;&#21035;&#30693;&#35782;&#19978;&#30340;&#20027;&#35201;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;rebus&#35868;&#39064;&#19978;&#30340;&#24615;&#33021;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;333&#20010;&#21407;&#22987;&#30340;&#22522;&#20110;&#22270;&#20687;&#30340;&#25991;&#23383;&#28216;&#25103;&#31034;&#20363;&#65292;&#28085;&#30422;&#20102;&#30005;&#24433;&#12289;&#20316;&#26354;&#23478;&#12289;&#20027;&#35201;&#22478;&#24066;&#21644;&#39135;&#29289;&#31561;13&#20010;&#31867;&#21035;&#12290;&#20026;&#20102;&#22312;&#35782;&#21035;&#25552;&#31034;&#30340;&#35789;&#35821;&#25110;&#30701;&#35821;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#33719;&#24471;&#33391;&#22909;&#24615;&#33021;&#65292;&#27169;&#22411;&#24517;&#39035;&#32467;&#21512;&#22270;&#20687;&#35782;&#21035;&#21644;&#23383;&#31526;&#20018;&#25805;&#20316;&#65292;&#36827;&#34892;&#20551;&#35774;&#26816;&#39564;&#12289;&#22810;&#27493;&#25512;&#29702;&#21644;&#23545;&#20154;&#31867;&#35748;&#30693;&#30340;&#29702;&#35299;&#65292;&#36825;&#20351;&#24471;&#35780;&#20272;&#33021;&#21147;&#21464;&#24471;&#22797;&#26434;&#32780;&#22810;&#27169;&#24577;&#12290;&#25105;&#20204;&#21457;&#29616;&#19987;&#26377;&#27169;&#22411;&#22914;GPT-4V&#21644;Gemini Pro&#26126;&#26174;&#20248;&#20110;&#25152;&#26377;&#20854;&#20182;&#27979;&#35797;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26368;&#22909;&#30340;&#27169;&#22411;&#20063;&#21482;&#26377;24%&#30340;&#26368;&#32456;&#20934;&#30830;&#29575;&#65292;&#31361;&#26174;&#20986;&#22312;&#25512;&#29702;&#26041;&#38754;&#38656;&#35201;&#23454;&#36136;&#24615;&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#24456;&#23569;&#29702;&#35299;&#35868;&#39064;&#30340;&#25152;&#26377;&#37096;&#20998;&#65292;&#20960;&#20046;&#24635;&#26159;&#26080;&#27861;&#20107;&#21518;&#35299;&#37322;&#27491;&#30830;&#31572;&#26696;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#21487;&#20197;&#29992;&#20110;&#35782;&#21035;&#30693;&#35782;&#30340;&#20027;&#35201;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new benchmark evaluating the performance of multimodal large language models on rebus puzzles. The dataset covers 333 original examples of image-based wordplay, cluing 13 categories such as movies, composers, major cities, and food. To achieve good performance on the benchmark of identifying the clued word or phrase, models must combine image recognition and string manipulation with hypothesis testing, multi-step reasoning, and an understanding of human cognition, making for a complex, multimodal evaluation of capabilities. We find that proprietary models such as GPT-4V and Gemini Pro significantly outperform all other tested models. However, even the best model has a final accuracy of just 24%, highlighting the need for substantial improvements in reasoning. Further, models rarely understand all parts of a puzzle, and are almost always incapable of retroactively explaining the correct answer. Our benchmark can therefore be used to identify major shortcomings in the knowle
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21028;&#26029;&#22320;&#29702;&#20301;&#32622;&#26041;&#21521;&#19978;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#20998;&#23618;&#31354;&#38388;&#20559;&#24046;&#12290;&#20854;&#20013;&#65292;GPT-4&#34920;&#29616;&#26368;&#20339;&#65292;&#20934;&#30830;&#29575;&#20026;55.3&#65285;&#12290;</title><link>http://arxiv.org/abs/2401.04218</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21028;&#26029;&#31354;&#38388;&#20851;&#31995;&#22833;&#30495;&#65306;&#33258;&#28982;&#35821;&#35328;&#22320;&#29702;&#25968;&#25454;&#30340;&#40654;&#26126;&#65311;
&lt;/p&gt;
&lt;p&gt;
Distortions in Judged Spatial Relations in Large Language Models: The Dawn of Natural Language Geographic Data?. (arXiv:2401.04218v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04218
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21028;&#26029;&#22320;&#29702;&#20301;&#32622;&#26041;&#21521;&#19978;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#20998;&#23618;&#31354;&#38388;&#20559;&#24046;&#12290;&#20854;&#20013;&#65292;GPT-4&#34920;&#29616;&#26368;&#20339;&#65292;&#20934;&#30830;&#29575;&#20026;55.3&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21028;&#26029;&#22320;&#29702;&#20301;&#32622;&#20043;&#38388;&#30340;&#26041;&#21521;&#19978;&#30340;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#19977;&#20010;&#30693;&#21517;&#30340;LLMs&#65306;GPT-3.5&#65292;GPT-4&#21644;Llama-2&#12290;&#36825;&#20010;&#22522;&#20934;&#29305;&#21035;&#35780;&#20272;&#20102;LLMs&#26159;&#21542;&#34920;&#29616;&#20986;&#31867;&#20284;&#20154;&#31867;&#30340;&#20998;&#23618;&#31354;&#38388;&#20559;&#24046;&#65292;&#21363;&#23545;&#20110;&#21253;&#21547;&#23427;&#20204;&#30340;&#26356;&#22823;&#32676;&#20307;&#30340;&#24863;&#30693;&#20851;&#31995;&#20250;&#24433;&#21709;&#23545;&#20010;&#21035;&#20301;&#32622;&#31354;&#38388;&#20851;&#31995;&#30340;&#21028;&#26029;&#12290;&#20026;&#20102;&#35843;&#26597;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;14&#20010;&#20851;&#20110;&#32654;&#22269;&#30693;&#21517;&#22478;&#24066;&#30340;&#38382;&#39064;&#12290;&#20854;&#20013;&#19971;&#20010;&#38382;&#39064;&#26088;&#22312;&#25361;&#25112;LLMs&#65292;&#36825;&#20123;&#38382;&#39064;&#21487;&#33021;&#21463;&#21040;&#20102;&#26356;&#22823;&#22320;&#29702;&#21333;&#20301;&#65288;&#22914;&#24030;&#25110;&#22269;&#23478;&#65289;&#26041;&#21521;&#30340;&#24433;&#21709;&#65292;&#32780;&#21478;&#22806;&#19971;&#20010;&#38382;&#39064;&#21017;&#38024;&#23545;&#19981;&#23481;&#26131;&#21463;&#21040;&#36825;&#31181;&#23618;&#27425;&#21270;&#20998;&#31867;&#30340;&#20301;&#32622;&#12290;&#22312;&#32463;&#36807;&#27979;&#35797;&#30340;&#27169;&#22411;&#20013;&#65292;GPT-4&#30340;&#20934;&#30830;&#29575;&#26368;&#39640;&#65292;&#20026;55.3&#65285;&#65292;&#20854;&#27425;&#26159;GPT-3.5&#30340;47.3&#65285;&#21644;Llama-2&#30340;44.7&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a benchmark for assessing the capability of Large Language Models (LLMs) to discern intercardinal directions between geographic locations and apply it to three prominent LLMs: GPT-3.5, GPT-4, and Llama-2. This benchmark specifically evaluates whether LLMs exhibit a hierarchical spatial bias similar to humans, where judgments about individual locations' spatial relationships are influenced by the perceived relationships of the larger groups that contain them. To investigate this, we formulated 14 questions focusing on well-known American cities. Seven questions were designed to challenge the LLMs with scenarios potentially influenced by the orientation of larger geographical units, such as states or countries, while the remaining seven targeted locations less susceptible to such hierarchical categorization. Among the tested models, GPT-4 exhibited superior performance with 55.3% accuracy, followed by GPT-3.5 at 47.3%, and Llama-2 at 44.7%. The models showed significantly redu
&lt;/p&gt;</description></item><item><title>TinyLlama&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22522;&#20110;Llama 2&#30340;&#26550;&#26500;&#21644;&#20998;&#35789;&#22120;&#65292;&#21033;&#29992;&#21508;&#31181;&#20808;&#36827;&#25216;&#26415;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;&#23613;&#31649;&#35268;&#27169;&#36739;&#23567;&#65292;&#20294;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#31867;&#20284;&#35268;&#27169;&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.02385</link><description>&lt;p&gt;
TinyLlama&#65306;&#19968;&#20010;&#24320;&#28304;&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TinyLlama: An Open-Source Small Language Model. (arXiv:2401.02385v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02385
&lt;/p&gt;
&lt;p&gt;
TinyLlama&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22522;&#20110;Llama 2&#30340;&#26550;&#26500;&#21644;&#20998;&#35789;&#22120;&#65292;&#21033;&#29992;&#21508;&#31181;&#20808;&#36827;&#25216;&#26415;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;&#23613;&#31649;&#35268;&#27169;&#36739;&#23567;&#65292;&#20294;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#31867;&#20284;&#35268;&#27169;&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;TinyLlama&#65292;&#19968;&#20010;&#26377;&#38480;&#30340;1.1B&#35821;&#35328;&#27169;&#22411;&#65292;&#22823;&#32422;&#39044;&#35757;&#32451;&#20102;1&#19975;&#20159;&#20010;&#26631;&#35760;&#65292;&#35757;&#32451;&#36718;&#25968;&#32422;&#20026;3&#36718;&#12290;TinyLlama&#22522;&#20110;Llama 2&#30340;&#26550;&#26500;&#21644;&#20998;&#35789;&#22120;&#65292;&#22312;&#24320;&#28304;&#31038;&#21306;&#30340;&#36129;&#29486;&#22522;&#30784;&#19978;&#65288;&#20363;&#22914;FlashAttention&#65289;&#65292;&#21033;&#29992;&#21508;&#31181;&#20808;&#36827;&#25216;&#26415;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;&#23613;&#31649;&#35268;&#27169;&#30456;&#23545;&#36739;&#23567;&#65292;TinyLlama&#22312;&#19968;&#31995;&#21015;&#19979;&#28216;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#23427;&#26126;&#26174;&#20248;&#20110;&#20855;&#26377;&#31867;&#20284;&#35268;&#27169;&#30340;&#29616;&#26377;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#26816;&#26597;&#28857;&#21644;&#20195;&#30721;&#21487;&#22312;GitHub&#19978;&#20844;&#24320;&#33719;&#21462;&#65292;&#32593;&#22336;&#20026;https://github.com/jzhang38/TinyLlama&#12290;
&lt;/p&gt;
&lt;p&gt;
We present TinyLlama, a compact 1.1B language model pretrained on around 1 trillion tokens for approximately 3 epochs. Building on the architecture and tokenizer of Llama 2, TinyLlama leverages various advances contributed by the open-source community (e.g., FlashAttention), achieving better computational efficiency. Despite its relatively small size, TinyLlama demonstrates remarkable performance in a series of downstream tasks. It significantly outperforms existing open-source language models with comparable sizes. Our model checkpoints and code are publicly available on GitHub at https://github.com/jzhang38/TinyLlama.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#22312;&#33258;&#25105;&#32416;&#27491;&#36807;&#31243;&#20013;&#30340;&#38169;&#35823;&#21457;&#29616;&#21644;&#36755;&#20986;&#32416;&#27491;&#20004;&#20010;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#12290;&#30740;&#31350;&#21457;&#29616;LLMs&#36890;&#24120;&#38590;&#20197;&#21457;&#29616;&#36923;&#36753;&#38169;&#35823;&#65292;&#20294;&#36890;&#36807;&#20351;&#29992;&#22238;&#28335;&#26041;&#27861;&#21487;&#20197;&#22312;&#25552;&#20379;&#38169;&#35823;&#20301;&#32622;&#20449;&#24687;&#26102;&#33719;&#24471;&#22823;&#24133;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2311.08516</link><description>&lt;p&gt;
LLMs&#26080;&#27861;&#25214;&#21040;&#25512;&#29702;&#38169;&#35823;&#65292;&#20294;&#21487;&#20197;&#32416;&#27491;&#23427;&#20204;&#65281;&#65288;arXiv&#65306;2311.08516v2 [cs.AI] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
LLMs cannot find reasoning errors, but can correct them!. (arXiv:2311.08516v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.08516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#22312;&#33258;&#25105;&#32416;&#27491;&#36807;&#31243;&#20013;&#30340;&#38169;&#35823;&#21457;&#29616;&#21644;&#36755;&#20986;&#32416;&#27491;&#20004;&#20010;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#12290;&#30740;&#31350;&#21457;&#29616;LLMs&#36890;&#24120;&#38590;&#20197;&#21457;&#29616;&#36923;&#36753;&#38169;&#35823;&#65292;&#20294;&#36890;&#36807;&#20351;&#29992;&#22238;&#28335;&#26041;&#27861;&#21487;&#20197;&#22312;&#25552;&#20379;&#38169;&#35823;&#20301;&#32622;&#20449;&#24687;&#26102;&#33719;&#24471;&#22823;&#24133;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#25105;&#32416;&#27491;&#22312;&#25913;&#21892;LLM&#36755;&#20986;&#30340;&#39118;&#26684;&#21644;&#36136;&#37327;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#65288;&#20363;&#22914;Chen&#31561;&#65292;2023&#65307;Madaan&#31561;&#65292;2023&#65289;&#65292;&#26368;&#36817;&#23545;&#36923;&#36753;&#25110;&#25512;&#29702;&#38169;&#35823;&#36827;&#34892;&#33258;&#25105;&#32416;&#27491;&#30340;&#23581;&#35797;&#36890;&#24120;&#20250;&#23548;&#33268;&#27491;&#30830;&#31572;&#26696;&#21464;&#20026;&#38169;&#35823;&#65292;&#20174;&#32780;&#24635;&#20307;&#34920;&#29616;&#21464;&#24046;&#65288;Huang&#31561;&#65292;2023&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#33258;&#25105;&#32416;&#27491;&#36807;&#31243;&#20998;&#35299;&#20026;&#20004;&#20010;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#65306;&#38169;&#35823;&#21457;&#29616;&#21644;&#36755;&#20986;&#32416;&#27491;&#12290;&#23545;&#20110;&#38169;&#35823;&#21457;&#29616;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;BIG-Bench Mistake&#65292;&#36825;&#26159;&#19968;&#20010;Chain-of-Thought&#25512;&#29702;&#36712;&#36857;&#20013;&#30340;&#36923;&#36753;&#38169;&#35823;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20026;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;LLM&#25552;&#20379;&#22522;&#20934;&#25968;&#65292;&#24182;&#35777;&#26126;LLM&#36890;&#24120;&#38590;&#20197;&#21457;&#29616;&#36923;&#36753;&#38169;&#35823;&#12290;&#23545;&#20110;&#36755;&#20986;&#32416;&#27491;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22238;&#28335;&#26041;&#27861;&#65292;&#22312;&#25552;&#20379;&#38169;&#35823;&#20301;&#32622;&#20449;&#24687;&#26102;&#21487;&#20197;&#22823;&#24133;&#25913;&#36827;&#12290;&#25105;&#20204;&#23558;&#22238;&#28335;&#35299;&#37322;&#20026;&#23545;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#36731;&#37327;&#32423;&#26367;&#20195;&#26041;&#26696;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;60-70&#65285;&#20934;&#30830;&#29575;&#19979;&#20445;&#25345;&#26377;&#25928;&#24615;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
While self-correction has shown promise in improving LLM outputs in terms of style and quality (e.g. Chen et al., 2023; Madaan et al., 2023), recent attempts to self-correct logical or reasoning errors often cause correct answers to become incorrect, resulting in worse performances overall (Huang et al., 2023). In this paper, we break down the self-correction process into two core components: mistake finding and output correction. For mistake finding, we release BIG-Bench Mistake, a dataset of logical mistakes in Chain-of-Thought reasoning traces. We provide benchmark numbers for several state-of-the-art LLMs, and demonstrate that LLMs generally struggle with finding logical mistakes. For output correction, we propose a backtracking method which provides large improvements when given information on mistake location. We construe backtracking as a lightweight alternative to reinforcement learning methods, and show that it remains effective with a reward model at 60-70% accuracy.
&lt;/p&gt;</description></item><item><title>VQPy&#26159;&#19968;&#31181;&#38754;&#21521;&#23545;&#35937;&#30340;&#35270;&#39057;&#20998;&#26512;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;Python&#21464;&#20307;&#20316;&#20026;&#21069;&#31471;&#65292;&#24182;&#20855;&#26377;&#21487;&#25193;&#23637;&#30340;&#21518;&#31471;&#65292;&#21487;&#20197;&#33258;&#21160;&#26500;&#24314;&#21644;&#20248;&#21270;&#22522;&#20110;&#35270;&#39057;&#23545;&#35937;&#30340;&#22788;&#29702;&#27969;&#31243;&#12290;</title><link>http://arxiv.org/abs/2311.01623</link><description>&lt;p&gt;
VQPy&#65306;&#19968;&#31181;&#38754;&#21521;&#29616;&#20195;&#35270;&#39057;&#20998;&#26512;&#30340;&#38754;&#21521;&#23545;&#35937;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
VQPy: An Object-Oriented Approach to Modern Video Analytics. (arXiv:2311.01623v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01623
&lt;/p&gt;
&lt;p&gt;
VQPy&#26159;&#19968;&#31181;&#38754;&#21521;&#23545;&#35937;&#30340;&#35270;&#39057;&#20998;&#26512;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;Python&#21464;&#20307;&#20316;&#20026;&#21069;&#31471;&#65292;&#24182;&#20855;&#26377;&#21487;&#25193;&#23637;&#30340;&#21518;&#31471;&#65292;&#21487;&#20197;&#33258;&#21160;&#26500;&#24314;&#21644;&#20248;&#21270;&#22522;&#20110;&#35270;&#39057;&#23545;&#35937;&#30340;&#22788;&#29702;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#20998;&#26512;&#24191;&#27867;&#24212;&#29992;&#20110;&#24403;&#20170;&#31995;&#32479;&#21644;&#26381;&#21153;&#20013;&#12290;&#22312;&#35270;&#39057;&#20998;&#26512;&#30340;&#21069;&#27839;&#26159;&#29992;&#25143;&#24320;&#21457;&#30340;&#35270;&#39057;&#26597;&#35810;&#65292;&#20197;&#25214;&#21040;&#29305;&#23450;&#24863;&#20852;&#36259;&#30340;&#23545;&#35937;&#12290;&#22522;&#20110;&#35270;&#39057;&#23545;&#35937;&#65288;&#20363;&#22914;&#20154;&#65292;&#21160;&#29289;&#65292;&#27773;&#36710;&#31561;&#65289;&#19982;&#20256;&#32479;&#38754;&#21521;&#23545;&#35937;&#35821;&#35328;&#24314;&#27169;&#30340;&#23545;&#35937;&#30456;&#20284;&#30340;&#27934;&#23519;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#35270;&#39057;&#20998;&#26512;&#30340;&#38754;&#21521;&#23545;&#35937;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21517;&#20026;VQPy&#65292;&#21253;&#25324;&#19968;&#20010;&#21069;&#31471;&#65288;&#19968;&#31181;Python&#21464;&#20307;&#65292;&#20854;&#20013;&#21253;&#21547;&#29992;&#25143;&#21487;&#20197;&#34920;&#36798;&#35270;&#39057;&#23545;&#35937;&#21450;&#20854;&#20132;&#20114;&#30340;&#32467;&#26500;&#65289;&#21644;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#21518;&#31471;&#65292;&#21487;&#20197;&#22522;&#20110;&#35270;&#39057;&#23545;&#35937;&#33258;&#21160;&#29983;&#25104;&#21644;&#20248;&#21270;&#31649;&#36947;&#12290;&#25105;&#20204;&#24050;&#32463;&#23454;&#26045;&#21644;&#24320;&#28304;&#20102;VQPy&#65292;&#23427;&#24050;&#32463;&#20316;&#20026;Cisco DeepVision&#26694;&#26550;&#30340;&#19968;&#37096;&#20998;&#20135;&#21697;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video analytics is widely used in contemporary systems and services. At the forefront of video analytics are video queries that users develop to find objects of particular interest. Building upon the insight that video objects (e.g., human, animals, cars, etc.), the center of video analytics, are similar in spirit to objects modeled by traditional object-oriented languages, we propose to develop an object-oriented approach to video analytics. This approach, named VQPy, consists of a frontend$\unicode{x2015}$a Python variant with constructs that make it easy for users to express video objects and their interactions$\unicode{x2015}$as well as an extensible backend that can automatically construct and optimize pipelines based on video objects. We have implemented and open-sourced VQPy, which has been productized in Cisco as part of its DeepVision framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21463;&#25511;&#35299;&#30721;&#65288;CD&#65289;&#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25511;&#21046;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#65292;&#20197;&#36798;&#21040;&#39640;&#22238;&#25253;&#30340;&#32467;&#26524;&#12290;CD&#36890;&#36807;&#21069;&#32512;&#35780;&#20998;&#22120;&#26469;&#24341;&#23548;&#29983;&#25104;&#65292;&#21487;&#20197;&#22312;&#25512;&#29702;&#26102;&#39044;&#27979;&#39044;&#26399;&#22238;&#25253;&#65292;&#24182;&#19988;&#20855;&#26377;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#32780;&#19981;&#22686;&#21152;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17022</link><description>&lt;p&gt;
&#21463;&#25511;&#35299;&#30721;&#26469;&#33258;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Controlled Decoding from Language Models. (arXiv:2310.17022v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21463;&#25511;&#35299;&#30721;&#65288;CD&#65289;&#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25511;&#21046;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#65292;&#20197;&#36798;&#21040;&#39640;&#22238;&#25253;&#30340;&#32467;&#26524;&#12290;CD&#36890;&#36807;&#21069;&#32512;&#35780;&#20998;&#22120;&#26469;&#24341;&#23548;&#29983;&#25104;&#65292;&#21487;&#20197;&#22312;&#25512;&#29702;&#26102;&#39044;&#27979;&#39044;&#26399;&#22238;&#25253;&#65292;&#24182;&#19988;&#20855;&#26377;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#32780;&#19981;&#22686;&#21152;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#21463;&#25511;&#35299;&#30721;&#65288;CD&#65289;&#65292;&#29992;&#20110;&#25511;&#21046;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#65292;&#20197;&#33719;&#24471;&#39640;&#22238;&#25253;&#30340;&#32467;&#26524;&#12290;CD&#36890;&#36807;&#20540;&#20989;&#25968;&#26469;&#35299;&#20915;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#35813;&#20540;&#20989;&#25968;&#34987;&#31216;&#20026;&#21069;&#32512;&#35780;&#20998;&#22120;&#12290;&#21069;&#32512;&#35780;&#20998;&#22120;&#22312;&#25512;&#29702;&#26102;&#29992;&#20110;&#24341;&#23548;&#29983;&#25104;&#21521;&#26356;&#39640;&#22238;&#25253;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21069;&#32512;&#35780;&#20998;&#22120;&#21487;&#20197;&#20174;&#65288;&#21487;&#33021;&#26159;&#65289;&#31163;&#31574;&#30053;&#25968;&#25454;&#20013;&#35757;&#32451;&#20986;&#26469;&#65292;&#29992;&#20110;&#39044;&#27979;&#20174;&#37096;&#20998;&#35299;&#30721;&#30340;&#21709;&#24212;&#32487;&#32493;&#35299;&#30721;&#26102;&#30340;&#39044;&#26399;&#22238;&#25253;&#12290;&#25105;&#20204;&#22312;Reddit&#23545;&#35805;&#35821;&#26009;&#24211;&#19978;&#32463;&#39564;&#35777;&#26126;&#65292;CD&#20316;&#20026;&#19968;&#31181;&#25511;&#21046;&#26426;&#21046;&#26159;&#26377;&#25928;&#30340;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;CD&#35774;&#35745;&#30340;&#27169;&#22359;&#21270;&#20351;&#20854;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#32780;&#19981;&#20250;&#22686;&#21152;&#20219;&#20309;&#22797;&#26434;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;CD&#21487;&#20197;&#20197;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#22359;&#26041;&#24335;&#22312;&#25512;&#29702;&#26102;&#24212;&#29992;&#65292;&#21516;&#26679;&#26080;&#38656;&#20219;&#20309;&#39069;&#22806;&#30340;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose controlled decoding (CD), a novel off-policy reinforcement learning method to control the autoregressive generation from language models towards high reward outcomes. CD solves an off-policy reinforcement learning problem through a value function for the reward, which we call a prefix scorer. The prefix scorer is used at inference time to steer the generation towards higher reward outcomes. We show that the prefix scorer may be trained on (possibly) off-policy data to predict the expected reward when decoding is continued from a partially decoded response. We empirically demonstrate that CD is effective as a control mechanism on Reddit conversations corpus. We also show that the modularity of the design of CD makes it possible to control for multiple rewards, effectively solving a multi-objective reinforcement learning problem with no additional complexity. Finally, we show that CD can be applied in a novel blockwise fashion at inference-time, again without the need for any 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#35821;&#27861;&#38169;&#35823;&#19988;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#30340;LLM&#24037;&#20855;&#20351;&#29992;&#26041;&#27861;ToolDec&#65292;&#36890;&#36807;&#26377;&#38480;&#29366;&#24577;&#35299;&#30721;&#31639;&#27861;&#28040;&#38500;&#20102;&#24037;&#20855;&#30456;&#20851;&#38169;&#35823;&#65292;&#20351;LLM&#33021;&#22815;&#26377;&#25928;&#36873;&#25321;&#24037;&#20855;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#25110;&#19978;&#19979;&#25991;&#25991;&#26723;&#12290;</title><link>http://arxiv.org/abs/2310.07075</link><description>&lt;p&gt;
&#36890;&#36807;&#26377;&#38480;&#29366;&#24577;&#35299;&#30721;&#23454;&#29616;&#26080;&#35821;&#27861;&#38169;&#35823;&#21644;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#30340;LLM&#24037;&#20855;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
Syntax Error-Free and Generalizable Tool Use for LLMs via Finite-State Decoding. (arXiv:2310.07075v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#35821;&#27861;&#38169;&#35823;&#19988;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#30340;LLM&#24037;&#20855;&#20351;&#29992;&#26041;&#27861;ToolDec&#65292;&#36890;&#36807;&#26377;&#38480;&#29366;&#24577;&#35299;&#30721;&#31639;&#27861;&#28040;&#38500;&#20102;&#24037;&#20855;&#30456;&#20851;&#38169;&#35823;&#65292;&#20351;LLM&#33021;&#22815;&#26377;&#25928;&#36873;&#25321;&#24037;&#20855;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#25110;&#19978;&#19979;&#25991;&#25991;&#26723;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#23637;&#31034;&#20986;&#20351;&#29992;&#22806;&#37096;&#24037;&#20855;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#26377;&#24076;&#26395;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#28041;&#21450;&#23545;&#24037;&#20855;&#28436;&#31034;&#36827;&#34892;&#24494;&#35843;&#65292;&#36825;&#26679;&#22312;&#27809;&#26377;&#39069;&#22806;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#26080;&#27861;&#25512;&#24191;&#21040;&#26032;&#30340;&#24037;&#20855;&#65292;&#35201;&#20040;&#22312;&#19978;&#19979;&#25991;&#20013;&#25552;&#20379;&#24037;&#20855;&#25991;&#26723;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#24037;&#20855;&#25968;&#37327;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#24120;&#24120;&#20135;&#29983;&#35821;&#27861;&#26080;&#25928;&#30340;&#24037;&#20855;&#35843;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ToolDec&#65292;&#19968;&#31181;&#26377;&#38480;&#29366;&#24577;&#26426;&#24341;&#23548;&#30340;&#35299;&#30721;&#31639;&#27861;&#65292;&#29992;&#20110;&#24037;&#20855;&#22686;&#24378;&#30340;LLMs&#12290;ToolDec&#36890;&#36807;&#30830;&#20445;&#26377;&#25928;&#30340;&#24037;&#20855;&#21517;&#31216;&#21644;&#31867;&#22411;&#19968;&#33268;&#30340;&#21442;&#25968;&#65292;&#28040;&#38500;&#20102;&#20219;&#20309;&#24037;&#20855;&#22686;&#24378;&#30340;LLMs&#20013;&#30340;&#24037;&#20855;&#30456;&#20851;&#38169;&#35823;&#12290;&#27492;&#22806;&#65292;ToolDec&#20351;LLM&#33021;&#22815;&#20165;&#20165;&#20351;&#29992;&#23427;&#20204;&#30340;&#21517;&#31216;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#26377;&#25928;&#22320;&#36873;&#25321;&#24037;&#20855;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#25110;&#19978;&#19979;&#25991;&#25991;&#26723;&#12290;&#25105;&#20204;&#22312;&#28041;&#21450;&#25968;&#23398;&#20989;&#25968;&#12289;&#30693;&#35782;&#22270;&#35889;&#20851;&#31995;&#21644;&#22797;&#26434;&#30340;&#29616;&#23454;&#19990;&#30028;RESTful API&#30340;&#21508;&#31181;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#22810;&#31181;&#20808;&#21069;&#30340;&#26041;&#27861;&#21450;&#20854;ToolDec&#22686;&#24378;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown promising capabilities in using external tools to solve complex problems. However, existing approaches either involve fine-tuning on tool demonstrations, which do not generalize to new tools without additional training, or providing tool documentation in context, limiting the number of tools. Both approaches often generate syntactically invalid tool calls. In this paper, we propose ToolDec, a finite-state machine-guided decoding algorithm for tool-augmented LLMs. ToolDec eliminates tool-related errors for any tool-augmented LLMs by ensuring valid tool names and type-conforming arguments. Furthermore, ToolDec enables LLM to effectively select tools using only the information contained in their names, with no need for fine-tuning or in-context documentation. We evaluated multiple prior methods and their ToolDec-enhanced versions on a variety of tasks involving tools like math functions, knowledge graph relations, and complex real-world RESTful APIs
&lt;/p&gt;</description></item><item><title>TADIS&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#24341;&#23548;LLMs&#28145;&#20837;&#24605;&#32771;&#31034;&#33539;&#20363;&#23376;&#65292;&#20197;&#20943;&#36731;&#27169;&#22411;&#33258;&#20449;&#30340;&#24187;&#35273;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#25913;&#21892;&#27169;&#22411;&#36755;&#20986;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.00901</link><description>&lt;p&gt;
TADIS: &#28145;&#20837;&#24605;&#32771;&#31034;&#33539;&#20363;&#23376;&#30340;&#27169;&#22411;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
TADIS: Steering Models for Deep-Thinking about Demonstration Examples. (arXiv:2310.00901v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00901
&lt;/p&gt;
&lt;p&gt;
TADIS&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#24341;&#23548;LLMs&#28145;&#20837;&#24605;&#32771;&#31034;&#33539;&#20363;&#23376;&#65292;&#20197;&#20943;&#36731;&#27169;&#22411;&#33258;&#20449;&#30340;&#24187;&#35273;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#25913;&#21892;&#27169;&#22411;&#36755;&#20986;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#24341;&#20837;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#65288;&#20363;&#22914;&#20219;&#21153;&#23450;&#20041;&#12289;&#31034;&#20363;&#65289;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25351;&#31034;&#35843;&#25972;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#38646;-shot&#27867;&#21270;&#33021;&#21147;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30456;&#36739;&#20197;&#21069;&#21462;&#24471;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25253;&#21578;&#31216;&#65292;&#34394;&#20551;&#30340;&#20219;&#21153;&#31034;&#20363;&#21487;&#20197;&#23454;&#29616;&#19982;&#27491;&#30830;&#30340;&#31034;&#20363;&#20960;&#20046;&#30456;&#21516;&#30340;&#24615;&#33021;&#65292;&#34920;&#26126;&#36755;&#20837;-&#26631;&#31614;&#23545;&#24212;&#20851;&#31995;&#27604;&#20197;&#21069;&#35748;&#20026;&#30340;&#37325;&#35201;&#24615;&#36739;&#20302;&#12290;&#21463;&#21040;&#36825;&#19968;&#36829;&#21453;&#30452;&#35273;&#30340;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24576;&#30097;&#27169;&#22411;&#21644;&#20154;&#31867;&#19968;&#26679;&#23384;&#22312;&#33258;&#20449;&#30340;&#24187;&#35273;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;TADIS&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#19981;&#20165;&#20165;&#26159;&#30475;&#21040;&#31034;&#33539;&#20363;&#23376;&#65292;&#32780;&#26159;&#24341;&#23548;LLM&#36827;&#34892;&#8220;&#28145;&#20837;&#24605;&#32771;&#8221;&#12290;&#20026;&#20102;&#20943;&#36731;&#27169;&#22411;&#33258;&#20449;&#30340;&#24187;&#35273;&#65292;&#25105;&#20204;&#39318;&#20808;&#35201;&#27714;&#27169;&#22411;&#39564;&#35777;&#31034;&#20363;&#30340;&#27491;&#30830;&#24615;&#65292;&#28982;&#21518;&#26681;&#25454;&#39564;&#35777;&#32467;&#26524;&#20316;&#20026;&#26465;&#20214;&#26469;&#24341;&#23548;&#27169;&#22411;&#20135;&#29983;&#26356;&#22909;&#30340;&#31572;&#26696;&#12290;&#36890;&#36807;&#24341;&#20837;&#36825;&#31181;&#24605;&#32771;&#36807;&#31243;&#65292;&#25105;&#20204;&#24076;&#26395;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#25913;&#21892;&#27169;&#22411;&#30340;&#36755;&#20986;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction tuning has been demonstrated that could significantly improve the zero-shot generalization capability to unseen tasks by an apparent margin. By incorporating additional context (e.g., task definition, examples) during the fine-tuning process, Large Language Models (LLMs) achieved much higher performance than before. However, recent work reported that delusive task examples can achieve almost the same performance as correct task examples, indicating the input-label correspondence is less important than previously thought. Intrigued by this counter-intuitive observation, we suspect models have the same illusion of competence as humans. Therefore, we propose a novel method called TADIS that steers LLMs for "Deep-Thinking'' about demonstration examples instead of merely seeing. To alleviate the illusion of competence of models, we first ask the model to verify the correctness of shown examples. Then, using the verification results as conditions to elicit models for a better ans
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#20316;&#20026;&#25512;&#33616;&#31995;&#32479;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#25506;&#32034;&#20854;&#21033;&#29992;&#29992;&#25143;&#20559;&#22909;&#36827;&#34892;&#25512;&#33616;&#12289;&#37325;&#26032;&#25490;&#24207;&#25512;&#33616;&#21015;&#34920;&#12289;&#21033;&#29992;&#30456;&#20284;&#29992;&#25143;&#20449;&#24687;&#20197;&#21450;&#22788;&#29702;&#20919;&#21551;&#21160;&#24773;&#20917;&#30340;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#19977;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2309.03613</link><description>&lt;p&gt;
&#35780;&#20272;ChatGPT&#20316;&#20026;&#25512;&#33616;&#31995;&#32479;&#30340;&#20005;&#35880;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Evaluating ChatGPT as a Recommender System: A Rigorous Approach. (arXiv:2309.03613v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03613
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#20316;&#20026;&#25512;&#33616;&#31995;&#32479;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#25506;&#32034;&#20854;&#21033;&#29992;&#29992;&#25143;&#20559;&#22909;&#36827;&#34892;&#25512;&#33616;&#12289;&#37325;&#26032;&#25490;&#24207;&#25512;&#33616;&#21015;&#34920;&#12289;&#21033;&#29992;&#30456;&#20284;&#29992;&#25143;&#20449;&#24687;&#20197;&#21450;&#22788;&#29702;&#20919;&#21551;&#21160;&#24773;&#20917;&#30340;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#19977;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#21331;&#36234;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#65292;&#22823;&#22411;AI&#35821;&#35328;&#27169;&#22411;&#36817;&#24180;&#26469;&#22791;&#21463;&#20851;&#27880;&#12290;&#23427;&#20204;&#22312;&#35821;&#35328;&#30456;&#20851;&#20219;&#21153;&#20013;&#20855;&#26377;&#37325;&#35201;&#36129;&#29486;&#65292;&#21253;&#25324;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#65292;&#22240;&#27492;&#23545;&#20110;&#21508;&#31181;&#29305;&#23450;&#20219;&#21153;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#36825;&#31181;&#26041;&#27861;&#37322;&#25918;&#20102;&#23427;&#20204;&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#24615;&#12290;&#30740;&#31350;&#30028;&#27491;&#22312;&#31215;&#26497;&#25506;&#32034;&#23427;&#20204;&#30340;&#24212;&#29992;&#65292;ChatGPT&#20063;&#22240;&#27492;&#33719;&#24471;&#20102;&#35748;&#21487;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#26377;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20294;&#20854;&#22312;&#25512;&#33616;&#22330;&#26223;&#20013;&#30340;&#28508;&#21147;&#20173;&#24453;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#36890;&#36807;&#25506;&#31350;ChatGPT&#20316;&#20026;&#38646;-shot&#25512;&#33616;&#31995;&#32479;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#21253;&#25324;&#35780;&#20272;&#20854;&#21033;&#29992;&#29992;&#25143;&#20559;&#22909;&#36827;&#34892;&#25512;&#33616;&#12289;&#37325;&#26032;&#25490;&#24207;&#29616;&#26377;&#25512;&#33616;&#21015;&#34920;&#12289;&#21033;&#29992;&#30456;&#20284;&#29992;&#25143;&#30340;&#20449;&#24687;&#20197;&#21450;&#22788;&#29702;&#20919;&#21551;&#21160;&#24773;&#20917;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#19977;&#20010;&#25968;&#25454;&#38598;&#65288;MovieLens Small&#12289;Last.FM&#21644;Facebook Bo&#65289;&#36827;&#34892;&#20840;&#38754;&#23454;&#39564;&#26469;&#35780;&#20272;ChatGPT&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent popularity surrounds large AI language models due to their impressive natural language capabilities. They contribute significantly to language-related tasks, including prompt-based learning, making them valuable for various specific tasks. This approach unlocks their full potential, enhancing precision and generalization. Research communities are actively exploring their applications, with ChatGPT receiving recognition. Despite extensive research on large language models, their potential in recommendation scenarios still needs to be explored. This study aims to fill this gap by investigating ChatGPT's capabilities as a zero-shot recommender system. Our goals include evaluating its ability to use user preferences for recommendations, reordering existing recommendation lists, leveraging information from similar users, and handling cold-start situations. We assess ChatGPT's performance through comprehensive experiments using three datasets (MovieLens Small, Last.FM, and Facebook Bo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#27425;&#21464;&#25442;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#25554;&#20540;&#26469;&#35299;&#20915;&#22312;&#36328;&#23398;&#31185;&#30740;&#31350;&#25552;&#26696;&#21644;&#38750;&#36328;&#23398;&#31185;&#30740;&#31350;&#25552;&#26696;&#20043;&#38388;&#35268;&#27169;&#24046;&#24322;&#24341;&#36215;&#30340;&#19981;&#20844;&#24179;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2309.01717</link><description>&lt;p&gt;
&#22312;&#19981;&#24179;&#34913;&#30340;&#30740;&#31350;&#25552;&#26696;&#20027;&#39064;&#25512;&#29702;&#20013;&#30340;&#36328;&#23398;&#31185;&#20844;&#24179;&#24615;&#65306;&#19968;&#31181;&#22522;&#20110;&#23618;&#27425;&#21464;&#25442;&#22120;&#30340;&#20855;&#26377;&#36873;&#25321;&#24615;&#25554;&#20540;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Interdisciplinary Fairness in Imbalanced Research Proposal Topic Inference: A Hierarchical Transformer-based Method with Selective Interpolation. (arXiv:2309.01717v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01717
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#27425;&#21464;&#25442;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#25554;&#20540;&#26469;&#35299;&#20915;&#22312;&#36328;&#23398;&#31185;&#30740;&#31350;&#25552;&#26696;&#21644;&#38750;&#36328;&#23398;&#31185;&#30740;&#31350;&#25552;&#26696;&#20043;&#38388;&#35268;&#27169;&#24046;&#24322;&#24341;&#36215;&#30340;&#19981;&#20844;&#24179;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#26696;&#20027;&#39064;&#25512;&#29702;&#30340;&#30446;&#26631;&#26159;&#20174;&#36164;&#21161;&#26426;&#26500;&#23450;&#20041;&#30340;&#23398;&#31185;&#20307;&#31995;&#20013;&#33719;&#21462;&#26368;&#21512;&#36866;&#30340;&#23398;&#31185;&#21010;&#20998;&#65292;&#28982;&#21518;&#26426;&#26500;&#23558;&#26681;&#25454;&#36825;&#31181;&#21010;&#20998;&#20174;&#20854;&#25968;&#25454;&#24211;&#20013;&#25214;&#21040;&#21512;&#36866;&#30340;&#21516;&#34892;&#35780;&#23457;&#19987;&#23478;&#12290;&#33258;&#21160;&#21270;&#30340;&#20027;&#39064;&#25512;&#29702;&#21487;&#20197;&#20943;&#23569;&#20154;&#24037;&#20027;&#39064;&#22635;&#20889;&#24341;&#36215;&#30340;&#38169;&#35823;&#65292;&#24357;&#34917;&#36164;&#21161;&#26426;&#26500;&#21644;&#39033;&#30446;&#30003;&#35831;&#20154;&#20043;&#38388;&#30340;&#30693;&#35782;&#24046;&#36317;&#65292;&#25552;&#39640;&#31995;&#32479;&#25928;&#29575;&#12290;&#29616;&#26377;&#26041;&#27861;&#23558;&#20854;&#24314;&#27169;&#20026;&#23618;&#27425;&#24615;&#22810;&#26631;&#31614;&#20998;&#31867;&#38382;&#39064;&#65292;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#36845;&#20195;&#22320;&#25512;&#29702;&#26368;&#21512;&#36866;&#30340;&#20027;&#39064;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24573;&#35270;&#20102;&#36328;&#23398;&#31185;&#30740;&#31350;&#25552;&#26696;&#21644;&#38750;&#36328;&#23398;&#31185;&#30740;&#31350;&#25552;&#26696;&#20043;&#38388;&#35268;&#27169;&#24046;&#24322;&#65292;&#23548;&#33268;&#33258;&#21160;&#25512;&#29702;&#31995;&#32479;&#23558;&#36328;&#23398;&#31185;&#25552;&#26696;&#24402;&#31867;&#20026;&#38750;&#36328;&#23398;&#31185;&#65292;&#36896;&#25104;&#22312;&#19987;&#23478;&#20998;&#37197;&#36807;&#31243;&#20013;&#30340;&#19981;&#20844;&#24179;&#29616;&#35937;&#12290;&#25105;&#20204;&#22914;&#20309;&#35299;&#20915;&#36825;&#20010;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#21602;&#65311;
&lt;/p&gt;
&lt;p&gt;
The objective of topic inference in research proposals aims to obtain the most suitable disciplinary division from the discipline system defined by a funding agency. The agency will subsequently find appropriate peer review experts from their database based on this division. Automated topic inference can reduce human errors caused by manual topic filling, bridge the knowledge gap between funding agencies and project applicants, and improve system efficiency. Existing methods focus on modeling this as a hierarchical multi-label classification problem, using generative models to iteratively infer the most appropriate topic information. However, these methods overlook the gap in scale between interdisciplinary research proposals and non-interdisciplinary ones, leading to an unjust phenomenon where the automated inference system categorizes interdisciplinary proposals as non-interdisciplinary, causing unfairness during the expert assignment. How can we address this data imbalance issue und
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#22312;&#25512;&#29702;&#26102;&#36890;&#36807;&#25913;&#21464;&#28608;&#27963;&#26469;&#39044;&#27979;&#24615;&#22320;&#25913;&#21464;&#35821;&#35328;&#27169;&#22411;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#21644;&#23454;&#26045;&#25104;&#26412;&#65292;&#24182;&#19988;&#33021;&#22815;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.10248</link><description>&lt;p&gt;
&#28608;&#27963;&#28155;&#21152;: &#26080;&#38656;&#20248;&#21270;&#21363;&#21487;&#25805;&#32437;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Activation Addition: Steering Language Models Without Optimization. (arXiv:2308.10248v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10248
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#22312;&#25512;&#29702;&#26102;&#36890;&#36807;&#25913;&#21464;&#28608;&#27963;&#26469;&#39044;&#27979;&#24615;&#22320;&#25913;&#21464;&#35821;&#35328;&#27169;&#22411;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#21644;&#23454;&#26045;&#25104;&#26412;&#65292;&#24182;&#19988;&#33021;&#22815;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#38752;&#22320;&#25511;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#26159;&#19968;&#20010;&#32039;&#36843;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#21253;&#25324;&#26377;&#30417;&#30563;&#24494;&#35843;&#12289;&#26681;&#25454;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#12289;&#25552;&#31034;&#24037;&#31243;&#21644;&#24341;&#23548;&#35299;&#30721;&#12290;&#25105;&#20204;&#30456;&#21453;&#65292;&#30740;&#31350;&#20102;&#28608;&#27963;&#24037;&#31243;&#65306;&#22312;&#25512;&#29702;&#26102;&#20462;&#25913;&#28608;&#27963;&#20197;&#21487;&#39044;&#27979;&#22320;&#25913;&#21464;&#27169;&#22411;&#34892;&#20026;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#38544;&#24335;&#25351;&#23450;&#20102;&#19968;&#20010;&#28155;&#21152;&#30340;&#8220;&#23548;&#21521;&#21521;&#37327;&#8221;&#26469;&#20559;&#32622;&#21069;&#21521;&#20256;&#25773;&#12290;&#19982;&#20197;&#21069;&#23398;&#20064;&#36825;&#20123;&#23548;&#21521;&#21521;&#37327;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#28608;&#27963;&#28155;&#21152;&#65288;ActAdd&#65289;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#26469;&#33258;&#25552;&#31034;&#23545;&#30340;&#28608;&#27963;&#24046;&#24322;&#26469;&#35745;&#31639;&#23427;&#20204;&#12290;&#25105;&#20204;&#22312;OpenWebText&#21644;ConceptNet&#19978;&#23637;&#31034;&#20102;ActAdd&#22312;GPT-2&#19978;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#25512;&#29702;&#26102;&#26041;&#27861;&#25511;&#21046;&#20102;&#36755;&#20986;&#30340;&#39640;&#32423;&#23646;&#24615;&#24182;&#20445;&#25345;&#20102;&#38750;&#30446;&#26631;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23427;&#25152;&#38656;&#30340;&#35745;&#31639;&#21644;&#23454;&#26045;&#24037;&#20316;&#27604;&#24494;&#35843;&#35201;&#23569;&#24471;&#22810;&#65292;&#20801;&#35768;&#29992;&#25143;&#25552;&#20379;&#33258;&#28982;&#35821;&#35328;&#30340;&#35268;&#33539;&#65292;&#24182;&#19988;&#20854;&#24320;&#38144;&#19982;&#27169;&#22411;&#35268;&#27169;&#33258;&#28982;&#22320;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliably controlling the behavior of large language models is a pressing open problem. Existing methods include supervised finetuning, reinforcement learning from human feedback, prompt engineering, and guided decoding. We instead investigate activation engineering: modifying activations at inference time to predictably alter model behavior. In particular, we bias the forward pass with an added 'steering vector' implicitly specified through natural language.  Unlike past work which learned these steering vectors, our Activation Addition (ActAdd) method computes them by taking the activation differences that result from pairs of prompts. We demonstrate ActAdd on GPT-2 on OpenWebText and ConceptNet. Our inference-time approach yields control over high-level properties of output and preserves off-target model performance. It involves far less compute and implementation effort than finetuning, allows users to provide natural language specifications, and its overhead scales naturally with m
&lt;/p&gt;</description></item><item><title>RE$^2$&#26041;&#27861;&#21033;&#29992;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#20013;&#23454;&#20307;&#22359;&#20043;&#38388;&#30340;&#21306;&#22495;&#32423;&#31354;&#38388;&#32467;&#26500;&#26469;&#25552;&#39640;&#23427;&#20204;&#30340;&#20851;&#31995;&#39044;&#27979;&#33021;&#21147;&#65292;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.14590</link><description>&lt;p&gt;
RE$^2$: &#38754;&#21521;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#30340;&#21306;&#22495;&#24863;&#30693;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
RE$^2$: Region-Aware Relation Extraction from Visually Rich Documents. (arXiv:2305.14590v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14590
&lt;/p&gt;
&lt;p&gt;
RE$^2$&#26041;&#27861;&#21033;&#29992;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#20013;&#23454;&#20307;&#22359;&#20043;&#38388;&#30340;&#21306;&#22495;&#32423;&#31354;&#38388;&#32467;&#26500;&#26469;&#25552;&#39640;&#23427;&#20204;&#30340;&#20851;&#31995;&#39044;&#27979;&#33021;&#21147;&#65292;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#34920;&#21333;&#29702;&#35299;&#30740;&#31350;&#20027;&#35201;&#20381;&#36182;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#38656;&#35201;&#24191;&#27867;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#24067;&#23616;&#32467;&#26500;&#65288;&#21363;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#20013;&#23454;&#20307;&#22359;&#20043;&#38388;&#30340;&#31354;&#38388;&#20851;&#31995;&#65289;&#23545;&#20110;&#20851;&#31995;&#25277;&#21462;&#30340;&#37325;&#35201;&#24615;&#21364;&#34987;&#24573;&#35270;&#20102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; RE$^2$ &#30340;&#21306;&#22495;&#24863;&#30693;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;&#65292;&#21033;&#29992;&#23454;&#20307;&#22359;&#20043;&#38388;&#30340;&#21306;&#22495;&#32423;&#31354;&#38388;&#32467;&#26500;&#26469;&#25552;&#39640;&#23427;&#20204;&#30340;&#20851;&#31995;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#36793;&#32536;&#24863;&#30693;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#26469;&#23398;&#20064;&#23454;&#20307;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#65292;&#21516;&#26102;&#32771;&#34385;&#23427;&#20204;&#30340;&#21306;&#22495;&#32423;&#34920;&#31034;&#25152;&#23450;&#20041;&#30340;&#31354;&#38388;&#20851;&#31995;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#32422;&#26463;&#30446;&#26631;&#65292;&#26469;&#35268;&#33539;&#27169;&#22411;&#20197;&#31526;&#21512;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#30340;&#22266;&#26377;&#32422;&#26463;&#26465;&#20214;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#12289;&#35821;&#35328;&#21644;&#39046;&#22495;&#30340;&#24191;&#27867;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current research in form understanding predominantly relies on large pre-trained language models, necessitating extensive data for pre-training. However, the importance of layout structure (i.e., the spatial relationship between the entity blocks in the visually rich document) to relation extraction has been overlooked. In this paper, we propose REgion-Aware Relation Extraction (RE$^2$) that leverages region-level spatial structure among the entity blocks to improve their relation prediction. We design an edge-aware graph attention network to learn the interaction between entities while considering their spatial relationship defined by their region-level representations. We also introduce a constraint objective to regularize the model towards consistency with the inherent constraints of the relation extraction task. Extensive experiments across various datasets, languages and domains demonstrate the superiority of our proposed approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#26694;&#26550;&#26469;&#25552;&#39640;LLMs&#20013;ICL&#30340;&#24615;&#33021;&#65292;&#23427;&#23558;ICL&#36807;&#31243;&#20998;&#20026;&#8220;&#28145;&#24605;&#29087;&#34385;&#8221;&#21644;&#25512;&#29702;&#38454;&#27573;&#12290;&#22312;&#8220;&#28145;&#24605;&#29087;&#34385;&#8221;&#38454;&#27573;&#20013;&#65292;&#36890;&#36807;&#22810;&#27425;&#36845;&#20195;&#20248;&#21270;&#31034;&#33539;&#65292;&#24182;&#25805;&#32437;Transformer&#20013;&#30340;&#33258;&#25105;&#27880;&#24847;&#27169;&#22359;&#20013;&#30340;Key-Value&#30697;&#38453;&#26469;&#29983;&#25104;&#20803;&#26799;&#24230;&#65292;&#20174;&#32780;&#26399;&#26395;&#22312;&#27979;&#35797;&#26102;&#25552;&#39640;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.13016</link><description>&lt;p&gt;
&#12298;&#36845;&#20195;&#21069;&#21521;&#35843;&#25972;&#25552;&#21319;&#35821;&#35328;&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#23398;&#20064;&#12299;
&lt;/p&gt;
&lt;p&gt;
Iterative Forward Tuning Boosts In-context Learning in Language Models. (arXiv:2305.13016v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#26694;&#26550;&#26469;&#25552;&#39640;LLMs&#20013;ICL&#30340;&#24615;&#33021;&#65292;&#23427;&#23558;ICL&#36807;&#31243;&#20998;&#20026;&#8220;&#28145;&#24605;&#29087;&#34385;&#8221;&#21644;&#25512;&#29702;&#38454;&#27573;&#12290;&#22312;&#8220;&#28145;&#24605;&#29087;&#34385;&#8221;&#38454;&#27573;&#20013;&#65292;&#36890;&#36807;&#22810;&#27425;&#36845;&#20195;&#20248;&#21270;&#31034;&#33539;&#65292;&#24182;&#25805;&#32437;Transformer&#20013;&#30340;&#33258;&#25105;&#27880;&#24847;&#27169;&#22359;&#20013;&#30340;Key-Value&#30697;&#38453;&#26469;&#29983;&#25104;&#20803;&#26799;&#24230;&#65292;&#20174;&#32780;&#26399;&#26395;&#22312;&#27979;&#35797;&#26102;&#25552;&#39640;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#32039;&#23494;&#32852;&#31995;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#20294;&#33021;&#22815;&#35299;&#20915;&#26222;&#36890;&#38382;&#39064;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#27169;&#22411;&#26080;&#27861;&#36890;&#36807;&#19968;&#27425;&#22788;&#29702;&#31034;&#33539;&#26679;&#20363;&#26469;&#35299;&#20915;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#36890;&#36807;&#24320;&#21457;Transformer&#27880;&#24847;&#21147;&#21644;&#22522;&#20110;&#26799;&#24230;&#19979;&#38477;&#30340;&#20248;&#21270;&#20043;&#38388;&#30340;&#21452;&#37325;&#24418;&#24335;&#26469;&#25552;&#39640;LLMs&#20013;ICL&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;ICL&#36807;&#31243;&#20998;&#20026;&#8220;&#28145;&#24605;&#29087;&#34385;&#8221;&#21644;&#25512;&#29702;&#38454;&#27573;&#12290;&#22312;&#8220;&#28145;&#24605;&#29087;&#34385;&#8221;&#38454;&#27573;&#20013;&#65292;&#36890;&#36807;&#22810;&#27425;&#36845;&#20195;&#20248;&#21270;&#31034;&#33539;&#65292;&#24182;&#25805;&#32437;Transformer&#20013;&#30340;&#33258;&#25105;&#27880;&#24847;&#27169;&#22359;&#20013;&#30340;Key-Value&#30697;&#38453;&#26469;&#29983;&#25104;&#20803;&#26799;&#24230;&#65292;&#20174;&#32780;&#26399;&#26395;&#22312;&#27979;&#35797;&#26102;&#25552;&#39640;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#25512;&#29702;&#38454;&#27573;&#20165;&#22788;&#29702;&#27979;&#35797;&#26597;&#35810;&#65292;&#32780;&#19981;&#38656;&#35201;&#20877;&#27425;&#32771;&#34385;&#31034;&#33539;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have exhibited an emergent in-context learning (ICL) ability. However, the ICL models that can solve ordinary cases are hardly extended to solve more complex tasks by processing the demonstration examples once. This single-turn ICL is incoordinate with the decision making process of humans by learning from analogy. In this paper, we propose an effective and efficient two-stage framework to boost ICL in LLMs by exploiting a dual form between Transformer attention and gradient descent-based optimization. Concretely, we divide the ICL process into "Deep-Thinking" and inference stages. The "Deep-Thinking" stage performs iterative forward optimization of demonstrations, which is expected to boost the reasoning abilities of LLMs at test time by "thinking" demonstrations multiple times. It produces accumulated meta-gradients by manipulating the Key-Value matrices in the self-attention modules of the Transformer. Then, the inference stage only takes the test query 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Cross-modality Data Augmentation&#65288;XmDA&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#25163;&#35821;&#21333;&#35789;&#32763;&#35793;&#27169;&#22411;&#30340;&#20266;&#25163;&#35821;&#21333;&#35789;-&#25991;&#26412;&#23545;&#65292;&#23558;&#24378;&#22823;&#30340;&#25163;&#35821;&#21333;&#35789;&#21040;&#25991;&#26412;&#30340;&#32763;&#35793;&#33021;&#21147;&#36716;&#31227;&#21040;&#20102;&#31471;&#21040;&#31471;&#25163;&#35821;&#32763;&#35793;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;XmDA&#22312;&#35813;&#39046;&#22495;&#20013;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.11096</link><description>&lt;p&gt;
&#36328;&#27169;&#24577;&#25968;&#25454;&#22686;&#24378;&#29992;&#20110;&#31471;&#21040;&#31471;&#25163;&#35821;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Cross-modality Data Augmentation for End-to-End Sign Language Translation. (arXiv:2305.11096v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Cross-modality Data Augmentation&#65288;XmDA&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#25163;&#35821;&#21333;&#35789;&#32763;&#35793;&#27169;&#22411;&#30340;&#20266;&#25163;&#35821;&#21333;&#35789;-&#25991;&#26412;&#23545;&#65292;&#23558;&#24378;&#22823;&#30340;&#25163;&#35821;&#21333;&#35789;&#21040;&#25991;&#26412;&#30340;&#32763;&#35793;&#33021;&#21147;&#36716;&#31227;&#21040;&#20102;&#31471;&#21040;&#31471;&#25163;&#35821;&#32763;&#35793;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;XmDA&#22312;&#35813;&#39046;&#22495;&#20013;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31471;&#21040;&#31471;&#25163;&#35821;&#32763;&#35793;&#26088;&#22312;&#30452;&#25509;&#23558;&#25163;&#35821;&#35270;&#39057;&#36716;&#25442;&#20026;&#21475;&#35821;&#25991;&#26412;&#65292;&#26080;&#38656;&#20013;&#38388;&#34920;&#31034;&#12290;&#21463;&#25163;&#35821;&#35270;&#39057;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#27169;&#24577;&#24046;&#36317;&#21644;&#26631;&#35760;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#30340;&#25361;&#25112;&#65292;&#36825;&#19968;&#20219;&#21153;&#19968;&#30452;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#36328;&#27169;&#24577;&#25968;&#25454;&#22686;&#24378;&#65288;XmDA&#65289;&#8221;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#25163;&#35821;&#21333;&#35789;&#32763;&#35793;&#27169;&#22411;&#30340;&#20266;&#25163;&#35821;&#21333;&#35789;-&#25991;&#26412;&#23545;&#65292;&#23558;&#24378;&#22823;&#30340;&#25163;&#35821;&#21333;&#35789;&#21040;&#25991;&#26412;&#30340;&#32763;&#35793;&#33021;&#21147;&#36716;&#31227;&#21040;&#20102;&#31471;&#21040;&#31471;&#25163;&#35821;&#32763;&#35793;&#65288;&#21363;&#35270;&#39057;&#21040;&#25991;&#26412;&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;XmDA&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#21363;&#36328;&#27169;&#24577;&#28151;&#21512;&#21644;&#36328;&#27169;&#24577;&#30693;&#35782;&#33976;&#39311;&#12290;&#21069;&#32773;&#26126;&#30830;&#22320;&#20419;&#36827;&#25163;&#35821;&#35270;&#39057;&#29305;&#24449;&#21644;&#25163;&#35821;&#21333;&#35789;&#23884;&#20837;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#20197;&#24357;&#21512;&#27169;&#24577;&#24046;&#36317;&#12290;&#21518;&#32773;&#21033;&#29992;&#26469;&#33258;&#25163;&#35821;&#21333;&#35789;&#21040;&#25991;&#26412;&#30340;&#25945;&#24072;&#27169;&#22411;&#30340;&#29983;&#25104;&#30693;&#35782;&#26469;&#25351;&#23548;&#21475;&#35821;&#25991;&#26412;&#29983;&#25104;&#12290;&#22312;&#20004;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#25163;&#35821;&#32763;&#35793;&#25968;&#25454;&#38598;LIBRISIGN&#21644;WLASL&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;XmDA&#22312;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#21644;&#20154;&#31867;&#35780;&#20272;&#26041;&#38754;&#22343;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-end sign language translation (SLT) aims to convert sign language videos into spoken language texts directly without intermediate representations. It has been a challenging task due to the modality gap between sign videos and texts and the data scarcity of labeled data. To tackle these challenges, we propose a novel Cross-modality Data Augmentation (XmDA) framework to transfer the powerful gloss-to-text translation capabilities to end-to-end sign language translation (i.e. video-to-text) by exploiting pseudo gloss-text pairs from the sign gloss translation model. Specifically, XmDA consists of two key components, namely, cross-modality mix-up and cross-modality knowledge distillation. The former explicitly encourages the alignment between sign video features and gloss embeddings to bridge the modality gap. The latter utilizes the generation knowledge from gloss-to-text teacher models to guide the spoken language text generation. Experimental results on two widely used SLT datase
&lt;/p&gt;</description></item><item><title>KPEval&#26159;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#20851;&#38190;&#35789;&#25552;&#21462;&#21644;&#29983;&#25104;&#31995;&#32479;&#35780;&#20272;&#20013;&#30340;&#30701;&#26495;&#12290;&#36890;&#36807;&#24341;&#20837;&#22235;&#20010;&#20851;&#38190;&#32500;&#24230;&#65292;&#21253;&#25324;&#26174;&#33879;&#24615;&#12289;&#24544;&#23454;&#24615;&#12289;&#22810;&#26679;&#24615;&#21644;&#23454;&#29992;&#24615;&#65292;&#24182;&#35774;&#35745;&#30456;&#24212;&#30340;&#35821;&#20041;&#24230;&#37327;&#25351;&#26631;&#65292;KPEval&#22312;&#19982;&#20154;&#31867;&#20559;&#22909;&#30456;&#20851;&#24615;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#20351;&#29992;&#35813;&#26694;&#26550;&#37325;&#26032;&#35780;&#20272;&#20102;20&#20010;&#20851;&#38190;&#35789;&#31995;&#32479;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#36873;&#25321;&#26368;&#20339;&#30340;&#24773;&#20917;&#21462;&#20915;&#20110;&#35780;&#20272;&#32500;&#24230;&#65292;&#23454;&#29992;&#24615;&#26159;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2303.15422</link><description>&lt;p&gt;
KPEval&#65306;&#38754;&#21521;&#32454;&#31890;&#24230;&#35821;&#20041;&#35780;&#20272;&#20851;&#38190;&#35789;&#25552;&#21462;&#21644;&#29983;&#25104;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
KPEval: Towards Fine-grained Semantic-based Evaluation of Keyphrase Extraction and Generation Systems. (arXiv:2303.15422v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15422
&lt;/p&gt;
&lt;p&gt;
KPEval&#26159;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#20851;&#38190;&#35789;&#25552;&#21462;&#21644;&#29983;&#25104;&#31995;&#32479;&#35780;&#20272;&#20013;&#30340;&#30701;&#26495;&#12290;&#36890;&#36807;&#24341;&#20837;&#22235;&#20010;&#20851;&#38190;&#32500;&#24230;&#65292;&#21253;&#25324;&#26174;&#33879;&#24615;&#12289;&#24544;&#23454;&#24615;&#12289;&#22810;&#26679;&#24615;&#21644;&#23454;&#29992;&#24615;&#65292;&#24182;&#35774;&#35745;&#30456;&#24212;&#30340;&#35821;&#20041;&#24230;&#37327;&#25351;&#26631;&#65292;KPEval&#22312;&#19982;&#20154;&#31867;&#20559;&#22909;&#30456;&#20851;&#24615;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#20351;&#29992;&#35813;&#26694;&#26550;&#37325;&#26032;&#35780;&#20272;&#20102;20&#20010;&#20851;&#38190;&#35789;&#31995;&#32479;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#36873;&#25321;&#26368;&#20339;&#30340;&#24773;&#20917;&#21462;&#20915;&#20110;&#35780;&#20272;&#32500;&#24230;&#65292;&#23454;&#29992;&#24615;&#26159;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20851;&#38190;&#35789;&#25552;&#21462;&#21644;&#29983;&#25104;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#29616;&#34892;&#35780;&#20272;&#26041;&#27861;&#20165;&#20381;&#36182;&#20110;&#19982;&#20154;&#24037;&#21442;&#32771;&#30340;&#23436;&#20840;&#21305;&#37197;&#65292;&#32780;&#24573;&#30053;&#20102;&#26080;&#21442;&#32771;&#23646;&#24615;&#12290;&#36825;&#31181;&#26041;&#24335;&#26080;&#27861;&#35782;&#21035;&#29983;&#25104;&#19982;&#21442;&#32771;&#35821;&#20041;&#31561;&#25928;&#25110;&#20855;&#26377;&#23454;&#38469;&#25928;&#29992;&#30340;&#22810;&#26679;&#21270;&#20851;&#38190;&#35789;&#30340;&#31995;&#32479;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#35780;&#20272;&#20851;&#38190;&#35789;&#31995;&#32479;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#26694;&#26550;KPEval&#65292;&#21253;&#21547;&#22235;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#26174;&#33879;&#24615;&#12289;&#24544;&#23454;&#24615;&#12289;&#22810;&#26679;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#23545;&#20110;&#27599;&#20010;&#32500;&#24230;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19982;&#35780;&#20272;&#30446;&#26631;&#30456;&#19968;&#33268;&#30340;&#22522;&#20110;&#35821;&#20041;&#30340;&#24230;&#37327;&#25351;&#26631;&#12290;&#20803;&#35780;&#20272;&#30740;&#31350;&#34920;&#26126;&#65292;&#19982;&#20043;&#21069;&#20351;&#29992;&#30340;&#19968;&#31995;&#21015;&#24230;&#37327;&#25351;&#26631;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#35780;&#20272;&#31574;&#30053;&#26356;&#22909;&#22320;&#19982;&#20154;&#31867;&#20559;&#22909;&#30456;&#20851;&#12290;&#20351;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#37325;&#26032;&#35780;&#20272;&#20102;20&#20010;&#20851;&#38190;&#35789;&#31995;&#32479;&#65292;&#24182;&#36827;&#19968;&#27493;&#21457;&#29616;&#65306;(1)&#26368;&#22909;&#30340;&#27169;&#22411;&#26681;&#25454;&#35780;&#20272;&#32500;&#24230;&#19981;&#21516;&#32780;&#19981;&#21516;&#65307;(2)&#23454;&#29992;&#24615;&#26159;&#20851;&#38190;&#35789;&#31995;&#32479;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the significant advancements in keyphrase extraction and keyphrase generation methods, the predominant approach for evaluation only relies on exact matching with human references and disregards reference-free attributes. This scheme fails to recognize systems that generate keyphrases semantically equivalent to the references or diverse keyphrases that carry practical utility. To better assess the capability of keyphrase systems, we propose KPEval, a comprehensive evaluation framework consisting of four critical dimensions: saliency, faithfulness, diversity, and utility. For each dimension, we design semantic-based metrics that align with the evaluation objectives. Meta-evaluation studies demonstrate that our evaluation strategy correlates better with human preferences compared to a range of previously used metrics. Using this framework, we re-evaluate 20 keyphrase systems and further discover that (1) the best model differs depending on the evaluation dimension; (2) the utility
&lt;/p&gt;</description></item></channel></rss>