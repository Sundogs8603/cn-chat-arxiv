<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#25552;&#20986;&#20102;&#26041;&#21521;&#20559;&#22909;&#23545;&#40784;&#65288;DPA&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#30446;&#26631;&#22870;&#21169;&#27169;&#25311;&#19981;&#21516;&#20559;&#22909;&#37197;&#32622;&#65292;&#20197;&#23454;&#29616;&#29992;&#25143;&#30456;&#20851;&#30340;&#20559;&#22909;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.18571</link><description>&lt;p&gt;
&#29992;&#20110;&#28385;&#36275;&#22810;&#26679;&#29992;&#25143;&#20559;&#22909;&#30340;&#31639;&#26415;&#25511;&#21046;LLMs&#65306;&#20855;&#26377;&#22810;&#30446;&#26631;&#22870;&#21169;&#30340;&#26041;&#21521;&#20559;&#22909;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18571
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#26041;&#21521;&#20559;&#22909;&#23545;&#40784;&#65288;DPA&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#30446;&#26631;&#22870;&#21169;&#27169;&#25311;&#19981;&#21516;&#20559;&#22909;&#37197;&#32622;&#65292;&#20197;&#23454;&#29616;&#29992;&#25143;&#30456;&#20851;&#30340;&#20559;&#22909;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#31934;&#32454;&#25511;&#21046;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#36866;&#24212;&#21508;&#31181;&#29992;&#25143;&#38656;&#27714;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#26041;&#21521;&#20559;&#22909;&#23545;&#40784;&#65288;DPA&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#30446;&#26631;&#22870;&#21169;&#24314;&#27169;&#26469;&#34920;&#31034;&#22810;&#26679;&#21270;&#30340;&#20559;&#22909;&#37197;&#32622;&#65292;&#23558;&#29992;&#25143;&#20559;&#22909;&#24314;&#27169;&#20026;&#22870;&#21169;&#31354;&#38388;&#20013;&#30340;&#26041;&#21521;&#65288;&#21363;&#21333;&#20301;&#21521;&#37327;&#65289;&#20197;&#23454;&#29616;&#29992;&#25143;&#30456;&#20851;&#30340;&#20559;&#22909;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18571v1 Announce Type: cross  Abstract: Fine-grained control over large language models (LLMs) remains a significant challenge, hindering their adaptability to diverse user needs. While Reinforcement Learning from Human Feedback (RLHF) shows promise in aligning LLMs, its reliance on scalar rewards often limits its ability to capture diverse user preferences in real-world applications. To address this limitation, we introduce the Directional Preference Alignment (DPA) framework. Unlike the scalar-reward RLHF, DPA incorporates multi-objective reward modeling to represent diverse preference profiles. Additionally, DPA models user preferences as directions (i.e., unit vectors) in the reward space to achieve user-dependent preference control. Our method involves training a multi-objective reward model and then fine-tuning the LLM with a preference-conditioned variant of Rejection Sampling Finetuning (RSF), an RLHF method adopted by Llama 2. This method enjoys a better performance
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#36827;&#34892;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#30340;&#33021;&#21147;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#22411;LM&#31995;&#32479;&#65292;&#36890;&#36807;&#22312;&#31454;&#20105;&#24615;&#39044;&#27979;&#24179;&#21488;&#25910;&#38598;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#30693;&#35782;&#25130;&#27490;&#26085;&#26399;&#21518;&#35780;&#20272;&#31995;&#32479;&#24615;&#33021;&#65292;&#21457;&#29616;&#35813;&#31995;&#32479;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20154;&#31867;&#39044;&#27979;&#32773;&#12290;</title><link>https://arxiv.org/abs/2402.18563</link><description>&lt;p&gt;
&#29992;&#35821;&#35328;&#27169;&#22411;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#30340;&#39044;&#27979;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Approaching Human-Level Forecasting with Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18563
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#36827;&#34892;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#30340;&#33021;&#21147;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#22411;LM&#31995;&#32479;&#65292;&#36890;&#36807;&#22312;&#31454;&#20105;&#24615;&#39044;&#27979;&#24179;&#21488;&#25910;&#38598;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#30693;&#35782;&#25130;&#27490;&#26085;&#26399;&#21518;&#35780;&#20272;&#31995;&#32479;&#24615;&#33021;&#65292;&#21457;&#29616;&#35813;&#31995;&#32479;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20154;&#31867;&#39044;&#27979;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#23545;&#25919;&#31574;&#21644;&#20915;&#31574;&#21046;&#23450;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;(LMs)&#26159;&#21542;&#33021;&#22815;&#22312;&#31454;&#20105;&#24615;&#20154;&#31867;&#39044;&#27979;&#32773;&#30340;&#27700;&#24179;&#19978;&#36827;&#34892;&#39044;&#27979;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#22411;LM&#31995;&#32479;&#65292;&#26088;&#22312;&#33258;&#21160;&#25628;&#32034;&#30456;&#20851;&#20449;&#24687;&#12289;&#29983;&#25104;&#39044;&#27979;&#21644;&#32858;&#21512;&#39044;&#27979;&#12290;&#20026;&#20102;&#20419;&#36827;&#30740;&#31350;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#26469;&#33258;&#31454;&#20105;&#24615;&#39044;&#27979;&#24179;&#21488;&#30340;&#22823;&#37327;&#38382;&#39064;&#25968;&#25454;&#38598;&#12290;&#22312;LM&#30340;&#30693;&#35782;&#25130;&#27490;&#26085;&#26399;&#20043;&#21518;&#21457;&#24067;&#30340;&#27979;&#35797;&#38598;&#19979;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#31995;&#32479;&#30340;&#31471;&#21040;&#31471;&#24615;&#33021;&#19982;&#20154;&#31867;&#39044;&#27979;&#30340;&#32858;&#21512;&#20043;&#38388;&#30340;&#27604;&#36739;&#12290;&#24179;&#22343;&#32780;&#35328;&#65292;&#35813;&#31995;&#32479;&#25509;&#36817;&#20110;&#31454;&#20105;&#39044;&#27979;&#32773;&#30340;&#32858;&#21512;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#23427;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#21033;&#29992;LM&#26469;&#39044;&#27979;&#26410;&#26469;&#21487;&#33021;&#20250;&#25552;&#20379;&#20934;&#30830;&#30340;&#22823;&#35268;&#27169;&#39044;&#27979;&#65292;&#24182;&#26377;&#21161;&#20110;&#20026;&#26426;&#26500;&#20915;&#31574;&#25552;&#20379;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18563v1 Announce Type: cross  Abstract: Forecasting future events is important for policy and decision making. In this work, we study whether language models (LMs) can forecast at the level of competitive human forecasters. Towards this goal, we develop a retrieval-augmented LM system designed to automatically search for relevant information, generate forecasts, and aggregate predictions. To facilitate our study, we collect a large dataset of questions from competitive forecasting platforms. Under a test set published after the knowledge cut-offs of our LMs, we evaluate the end-to-end performance of our system against the aggregates of human forecasts. On average, the system nears the crowd aggregate of competitive forecasters, and in some settings surpasses it. Our work suggests that using LMs to forecast the future could provide accurate predictions at scale and help to inform institutional decision making.
&lt;/p&gt;</description></item><item><title>&#22312;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#22120;&#35757;&#32451;&#19979;&#30340;&#32447;&#24615;NTP&#27169;&#22411;&#20013;&#65292;&#30830;&#23450;&#20102;NTP&#21487;&#20998;&#31163;&#26465;&#20214;&#65292;&#24182;&#35777;&#26126;&#26799;&#24230;&#19979;&#38477;&#33021;&#22815;&#23454;&#29616;&#20854;&#19979;&#30028;&#65307;&#21516;&#26102;&#35777;&#26126;&#20102;&#36825;&#20123;&#26465;&#20214;&#22312;&#36807;&#21442;&#25968;&#21270;&#26102;&#20173;&#28982;&#25104;&#31435;&#12290;</title><link>https://arxiv.org/abs/2402.18551</link><description>&lt;p&gt;
&#38544;&#24615;&#20559;&#35265;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Implicit Bias of Next-Token Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18551
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#22120;&#35757;&#32451;&#19979;&#30340;&#32447;&#24615;NTP&#27169;&#22411;&#20013;&#65292;&#30830;&#23450;&#20102;NTP&#21487;&#20998;&#31163;&#26465;&#20214;&#65292;&#24182;&#35777;&#26126;&#26799;&#24230;&#19979;&#38477;&#33021;&#22815;&#23454;&#29616;&#20854;&#19979;&#30028;&#65307;&#21516;&#26102;&#35777;&#26126;&#20102;&#36825;&#20123;&#26465;&#20214;&#22312;&#36807;&#21442;&#25968;&#21270;&#26102;&#20173;&#28982;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#65288;NTP&#65289;&#26159;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39318;&#36873;&#33539;&#24335;&#65292;&#23427;&#28041;&#21450;&#39044;&#27979;&#24207;&#21015;&#20013;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#12290;&#19982;&#20256;&#32479;&#30340;&#29420;&#28909;&#20998;&#31867;&#19981;&#21516;&#65292;&#22312;NTP&#20013;&#65292;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#39057;&#29575;&#30340;&#26631;&#35760;&#22312;&#32473;&#23450;&#19978;&#19979;&#25991;&#21518;&#32487;&#12290;&#26412;&#25991;&#23558;NTP&#35757;&#32451;&#26694;&#26550;&#21270;&#20026;&#36328;&#19981;&#21516;&#19978;&#19979;&#25991;&#30340;&#20132;&#21449;&#29109;&#26368;&#23567;&#21270;&#65292;&#27599;&#20010;&#19978;&#19979;&#25991;&#37117;&#19982;&#26377;&#38480;&#35789;&#27719;&#34920;&#20013;&#30340;&#31232;&#30095;&#32463;&#39564;&#27010;&#29575;&#21521;&#37327;&#30456;&#20851;&#32852;&#12290;&#28982;&#21518;&#65292;&#23427;&#25506;&#35752;&#20102;&#20197;&#19979;&#38382;&#39064;&#65306;&#24403;NTP&#35757;&#32451;&#25439;&#22833;&#36798;&#21040;&#20854;&#19979;&#30028;&#65288;&#29109;&#65289;&#26102;&#65292;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#22120;&#26159;&#21542;&#20250;&#23545;&#20855;&#26377;&#29305;&#23450;&#32467;&#26500;&#30340;&#35299;&#20915;&#26041;&#26696;&#23384;&#22312;&#20559;&#35265;&#65311;&#20855;&#20307;&#22320;&#65292;&#23545;&#20110;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#35757;&#32451;&#30340;&#32447;&#24615;NTP&#27169;&#22411;&#65292;&#25105;&#20204;&#20570;&#20986;&#20197;&#19979;&#36129;&#29486;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#25968;&#25454;&#19978;&#30340;NTP&#21487;&#20998;&#31163;&#26465;&#20214;&#65292;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#65292;GD&#33021;&#22815;&#36798;&#21040;&#20854;&#19979;&#30028;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#36825;&#20123;&#26465;&#20214;&#22312;&#36807;&#21442;&#25968;&#21270;&#26102;&#20173;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18551v1 Announce Type: cross  Abstract: Next-token prediction (NTP), the go-to training paradigm in training large language models, involves predicting the next token in a sequence. Departing from traditional one-hot classification, in NTP, multiple tokens with varying frequencies follow each given context. This work frames NTP training as cross-entropy minimization over distinct contexts, each associated with a sparse empirical probability vector across a finite vocabulary. It then addresses the following question: do gradient-based optimizers exhibit a bias towards solutions with specific structure as the NTP training loss reaches its lower bound (entropy)? Specifically, for linear NTP models trained using gradient descent (GD), we make the following contributions: Firstly, we determine NTP-separability conditions on the data, under which GD can attain its lower bound. We also demonstrate that these conditions hold under overparameterization. Secondly, we establish that th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#8220;&#32431;&#31929;&#35843;&#20248;&#65292;&#23433;&#20840;&#27979;&#35797;&#8221;&#65288;PTST&#65289;&#21407;&#21017;&#65292;&#21363;&#22312;&#24494;&#35843;&#26102;&#19981;&#21253;&#21547;&#23433;&#20840;&#25552;&#31034;&#65292;&#20294;&#22312;&#27979;&#35797;&#26102;&#21152;&#20837;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;LLMs&#20013;&#19981;&#23433;&#20840;&#34892;&#20026;&#30340;&#20986;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.18540</link><description>&lt;p&gt;
&#22312;&#24494;&#35843;&#21518;&#20445;&#25345;LLMs&#30340;&#23545;&#40784;&#24615;:&#25552;&#31034;&#27169;&#26495;&#30340;&#20851;&#38190;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt Templates
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18540
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#8220;&#32431;&#31929;&#35843;&#20248;&#65292;&#23433;&#20840;&#27979;&#35797;&#8221;&#65288;PTST&#65289;&#21407;&#21017;&#65292;&#21363;&#22312;&#24494;&#35843;&#26102;&#19981;&#21253;&#21547;&#23433;&#20840;&#25552;&#31034;&#65292;&#20294;&#22312;&#27979;&#35797;&#26102;&#21152;&#20837;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;LLMs&#20013;&#19981;&#23433;&#20840;&#34892;&#20026;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#20849;LLMs&#65292;&#22914;Llama 2-Chat&#65292;&#25512;&#21160;&#20102;LLM&#30740;&#31350;&#30340;&#24040;&#22823;&#27963;&#21160;&#12290;&#36825;&#20123;&#27169;&#22411;&#32463;&#21382;&#20102;&#23545;&#40784;&#24615;&#35757;&#32451;&#65292;&#34987;&#35748;&#20026;&#26159;&#23433;&#20840;&#30340;&#12290;&#26368;&#36817;&#65292;&#40784;&#31561;&#20154;&#65288;2023&#24180;&#65289;&#25253;&#21578;&#31216;&#65292;&#21363;&#20351;&#26159;&#33391;&#24615;&#30340;&#24494;&#35843;&#65288;&#20363;&#22914;&#65292;&#22312;&#30475;&#20284;&#23433;&#20840;&#30340;&#25968;&#25454;&#38598;&#19978;&#65289;&#20063;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#20135;&#29983;&#19981;&#23433;&#20840;&#30340;&#34892;&#20026;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20943;&#36731;&#36825;&#31181;&#23545;&#40784;&#24615;&#20002;&#22833;&#30340;&#26041;&#27861;&#21644;&#26368;&#20339;&#23454;&#36341;&#12290;&#36890;&#36807;&#23545;&#20960;&#20010;&#32842;&#22825;&#27169;&#22411;&#65288;Meta&#30340;Llama 2-Chat&#65292;Mistral AI&#30340;Mistral 7B Instruct v0.2&#21644;OpenAI&#30340;GPT-3.5 Turbo&#65289;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#26412;&#25991;&#21457;&#29616;&#24494;&#35843;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;&#25552;&#31034;&#27169;&#26495;&#22312;&#20445;&#25345;&#23433;&#20840;&#23545;&#40784;&#24615;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#8220;&#32431;&#31929;&#35843;&#20248;&#65292;&#23433;&#20840;&#27979;&#35797;&#8221;&#65288;PTST&#65289;&#21407;&#21017; - &#22312;&#27979;&#35797;&#26102;&#19981;&#20351;&#29992;&#23433;&#20840;&#25552;&#31034;&#36827;&#34892;&#27169;&#22411;&#24494;&#35843;&#65292;&#20294;&#22312;&#27979;&#35797;&#26102;&#21253;&#21547;&#23427;&#12290;&#23545;GSM8K&#65292;ChatDoctor&#21644;OpenOrca&#36827;&#34892;&#30340;&#24494;&#35843;&#23454;&#39564;&#34920;&#26126;&#65292;PTST&#26174;&#30528;&#20943;&#23569;&#20102;&#19981;&#23433;&#20840;&#34892;&#20026;&#30340;&#22686;&#21152;&#65292;&#29978;&#33267;&#20960;&#20046;&#28040;&#38500;&#20102;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18540v1 Announce Type: cross  Abstract: Public LLMs such as the Llama 2-Chat have driven huge activity in LLM research. These models underwent alignment training and were considered safe. Recently Qi et al. (2023) reported that even benign fine-tuning (e.g., on seemingly safe datasets) can give rise to unsafe behaviors in the models. The current paper is about methods and best practices to mitigate such loss of alignment. Through extensive experiments on several chat models (Meta's Llama 2-Chat, Mistral AI's Mistral 7B Instruct v0.2, and OpenAI's GPT-3.5 Turbo), this paper uncovers that the prompt templates used during fine-tuning and inference play a crucial role in preserving safety alignment, and proposes the "Pure Tuning, Safe Testing" (PTST) principle -- fine-tune models without a safety prompt, but include it at test time. Fine-tuning experiments on GSM8K, ChatDoctor, and OpenOrca show that PTST significantly reduces the rise of unsafe behaviors, and even almost elimin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;RNNs&#21644;Transformer&#22312;&#22788;&#29702;&#31639;&#27861;&#38382;&#39064;&#26102;&#30340;&#34920;&#29616;&#33021;&#21147;&#24046;&#36317;&#65292;&#21457;&#29616;RNNs&#23384;&#22312;&#20851;&#38190;&#29942;&#39048;&#65292;&#21363;&#26080;&#27861;&#23436;&#32654;&#22320;&#20174;&#19978;&#19979;&#25991;&#20013;&#26816;&#32034;&#20449;&#24687;&#65292;&#23548;&#33268;&#26080;&#27861;&#20687;Transformer&#37027;&#26679;&#36731;&#26494;&#35299;&#20915;&#38656;&#35201;&#36825;&#31181;&#33021;&#21147;&#30340;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.18510</link><description>&lt;p&gt;
RNNs&#36824;&#19981;&#26159;Transformer&#65306;&#22312;&#19978;&#19979;&#25991;&#26816;&#32034;&#20013;&#30340;&#20851;&#38190;&#29942;&#39048;
&lt;/p&gt;
&lt;p&gt;
RNNs are not Transformers (Yet): The Key Bottleneck on In-context Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;RNNs&#21644;Transformer&#22312;&#22788;&#29702;&#31639;&#27861;&#38382;&#39064;&#26102;&#30340;&#34920;&#29616;&#33021;&#21147;&#24046;&#36317;&#65292;&#21457;&#29616;RNNs&#23384;&#22312;&#20851;&#38190;&#29942;&#39048;&#65292;&#21363;&#26080;&#27861;&#23436;&#32654;&#22320;&#20174;&#19978;&#19979;&#25991;&#20013;&#26816;&#32034;&#20449;&#24687;&#65292;&#23548;&#33268;&#26080;&#27861;&#20687;Transformer&#37027;&#26679;&#36731;&#26494;&#35299;&#20915;&#38656;&#35201;&#36825;&#31181;&#33021;&#21147;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#21644;Transformer&#22312;&#35299;&#20915;&#31639;&#27861;&#38382;&#39064;&#26102;&#30340;&#34920;&#31034;&#33021;&#21147;&#24046;&#36317;&#12290;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;RNNs&#26159;&#21542;&#33021;&#22312;&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#65292;&#36890;&#36807;Chain-of-Thought (CoT)&#25552;&#31034;&#65292;&#19982;Transformer&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#26174;&#31034;CoT&#21487;&#20197;&#25913;&#36827;RNNs&#65292;&#20294;&#26080;&#27861;&#24357;&#34917;&#19982;Transformer&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#20851;&#38190;&#29942;&#39048;&#22312;&#20110;RNNs&#26080;&#27861;&#23436;&#20840;&#20174;&#19978;&#19979;&#25991;&#20013;&#26816;&#32034;&#20449;&#24687;&#65292;&#21363;&#20351;&#32463;&#36807;CoT&#30340;&#22686;&#24378;&#65306;&#23545;&#20110;&#20960;&#20010;&#26126;&#30830;&#25110;&#38544;&#24335;&#38656;&#35201;&#36825;&#31181;&#33021;&#21147;&#30340;&#20219;&#21153;&#65292;&#22914;&#32852;&#24819;&#21484;&#22238;&#21644;&#30830;&#23450;&#22270;&#26159;&#21542;&#20026;&#26641;&#65292;&#25105;&#20204;&#35777;&#26126;RNNs&#34920;&#36798;&#33021;&#21147;&#19981;&#36275;&#20197;&#35299;&#20915;&#36825;&#20123;&#20219;&#21153;&#65292;&#32780;Transformer&#21487;&#20197;&#36731;&#26494;&#35299;&#20915;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#35777;&#26126;&#37319;&#29992;&#22686;&#24378;RNNs&#19978;&#19979;&#25991;&#26816;&#32034;&#33021;&#21147;&#30340;&#25216;&#26415;&#65292;&#21253;&#25324;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18510v1 Announce Type: cross  Abstract: This paper investigates the gap in representation powers of Recurrent Neural Networks (RNNs) and Transformers in the context of solving algorithmic problems. We focus on understanding whether RNNs, known for their memory efficiency in handling long sequences, can match the performance of Transformers, particularly when enhanced with Chain-of-Thought (CoT) prompting. Our theoretical analysis reveals that CoT improves RNNs but is insufficient to close the gap with Transformers. A key bottleneck lies in the inability of RNNs to perfectly retrieve information from the context, even with CoT: for several tasks that explicitly or implicitly require this capability, such as associative recall and determining if a graph is a tree, we prove that RNNs are not expressive enough to solve the tasks while Transformers can solve them with ease. Conversely, we prove that adopting techniques to enhance the in-context retrieval capability of RNNs, inclu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#20844;&#24179;&#24615;&#23450;&#20041;&#19978;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#25506;&#35752;&#20102;LLM&#22312;&#20844;&#24179;&#24615;&#32771;&#34385;&#19979;&#30340;&#29983;&#25104;&#20844;&#24179;&#32467;&#26524;&#30340;&#26041;&#27861;&#65292;&#21253;&#21547;&#23545;&#19978;&#19979;&#25991;&#23398;&#20064;&#37197;&#32622;&#21644;&#28436;&#31034;&#36873;&#25321;&#30340;&#25506;&#32034;&#12290;</title><link>https://arxiv.org/abs/2402.18502</link><description>&lt;p&gt;
&#23569;&#26679;&#26412;&#20844;&#24179;&#24615;&#65306;&#25581;&#31034;LLM&#22312;&#20844;&#24179;&#24847;&#35782;&#20998;&#31867;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Fairness: Unveiling LLM's Potential for Fairness-Aware Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#20844;&#24179;&#24615;&#23450;&#20041;&#19978;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#25506;&#35752;&#20102;LLM&#22312;&#20844;&#24179;&#24615;&#32771;&#34385;&#19979;&#30340;&#29983;&#25104;&#20844;&#24179;&#32467;&#26524;&#30340;&#26041;&#27861;&#65292;&#21253;&#21547;&#23545;&#19978;&#19979;&#25991;&#23398;&#20064;&#37197;&#32622;&#21644;&#28436;&#31034;&#36873;&#25321;&#30340;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#19979;&#28216;&#24212;&#29992;&#20013;&#36827;&#34892;&#20998;&#31867;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#23588;&#20854;&#23545;&#20110;&#32570;&#20047;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#25152;&#38656;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#36164;&#28304;&#30340;&#23567;&#22411;&#20844;&#21496;&#32780;&#35328;&#12290; LLM&#20013;&#30340;&#20844;&#24179;&#24615;&#26377;&#21161;&#20110;&#30830;&#20445;&#21253;&#23481;&#24615;&#65292;&#22522;&#20110;&#31181;&#26063;&#12289;&#24615;&#21035;&#31561;&#22240;&#32032;&#23454;&#29616;&#24179;&#31561;&#20195;&#34920;&#65292;&#24182;&#20419;&#36827;&#36127;&#36131;&#20219;&#30340;AI&#37096;&#32626;&#12290;&#38543;&#30528;LLM&#30340;&#20351;&#29992;&#26085;&#30410;&#26222;&#21450;&#65292;&#35780;&#20272;LLM&#22312;&#32771;&#34385;&#20844;&#24179;&#24615;&#26102;&#33021;&#21542;&#20135;&#29983;&#20844;&#24179;&#30340;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290; &#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#27010;&#36848;&#20102;&#19982;&#21508;&#31181;&#20844;&#24179;&#23450;&#20041;&#23545;&#40784;&#30340;&#20844;&#24179;&#27861;&#35268;&#65292;&#27599;&#20010;&#23450;&#20041;&#37117;&#30001;&#19981;&#21516;&#31243;&#24230;&#30340;&#25277;&#35937;&#35843;&#33410;&#12290; &#25105;&#20204;&#25506;&#35752;&#20102;&#22522;&#20110;RAG&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#37197;&#32622;&#21644;&#36873;&#25321;&#19978;&#19979;&#25991;&#28436;&#31034;&#30340;&#31243;&#24207;&#65292;&#24182;&#23558;&#20844;&#24179;&#35268;&#21017;&#32435;&#20837;&#20854;&#20013;&#12290;&#23545;&#19981;&#21516;LLM&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;GPT-4&#25552;&#20379;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18502v1 Announce Type: new  Abstract: Employing Large Language Models (LLM) in various downstream applications such as classification is crucial, especially for smaller companies lacking the expertise and resources required for fine-tuning a model. Fairness in LLMs helps ensure inclusivity, equal representation based on factors such as race, gender and promotes responsible AI deployment. As the use of LLMs has become increasingly prevalent, it is essential to assess whether LLMs can generate fair outcomes when subjected to considerations of fairness. In this study, we introduce a framework outlining fairness regulations aligned with various fairness definitions, with each definition being modulated by varying degrees of abstraction. We explore the configuration for in-context learning and the procedure for selecting in-context demonstrations using RAG, while incorporating fairness rules into the process. Experiments conducted with different LLMs indicate that GPT-4 delivers 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31070;&#32463;&#28608;&#27963;&#32447;&#24615;&#35299;&#26512;&#35821;&#35328;&#27169;&#22411;&#20013;&#20195;&#29702;&#20154;&#35266;&#28857;&#19979;&#30340;&#20449;&#24565;&#29366;&#24577;&#65292;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#34920;&#36848;&#33258;&#25105;&#21644;&#20182;&#20154;&#20449;&#24565;&#65292;&#36825;&#23545;&#31038;&#20250;&#25512;&#29702;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#22312;&#22810;&#26679;&#31038;&#20250;&#25512;&#29702;&#20219;&#21153;&#20013;&#20855;&#26377;&#28508;&#22312;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.18496</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#34920;&#36798;&#33258;&#25105;&#21644;&#20182;&#20154;&#20449;&#24565;
&lt;/p&gt;
&lt;p&gt;
Language Models Represent Beliefs of Self and Others
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18496
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#28608;&#27963;&#32447;&#24615;&#35299;&#26512;&#35821;&#35328;&#27169;&#22411;&#20013;&#20195;&#29702;&#20154;&#35266;&#28857;&#19979;&#30340;&#20449;&#24565;&#29366;&#24577;&#65292;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#34920;&#36848;&#33258;&#25105;&#21644;&#20182;&#20154;&#20449;&#24565;&#65292;&#36825;&#23545;&#31038;&#20250;&#25512;&#29702;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#22312;&#22810;&#26679;&#31038;&#20250;&#25512;&#29702;&#20219;&#21153;&#20013;&#20855;&#26377;&#28508;&#22312;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#21644;&#24402;&#22240;&#24515;&#29702;&#29366;&#24577;&#65292;&#21363;&#24515;&#28789;&#29702;&#35770;&#65288;ToM&#65289;&#65292;&#34987;&#35270;&#20026;&#20154;&#31867;&#31038;&#20250;&#25512;&#29702;&#30340;&#22522;&#26412;&#33021;&#21147;&#12290;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20284;&#20046;&#20855;&#26377;&#26576;&#20123;ToM&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#33021;&#21147;&#32972;&#21518;&#30340;&#26426;&#21046;&#20173;&#28982;&#20196;&#20154;&#36153;&#35299;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#30340;&#31070;&#32463;&#28608;&#27963;&#32447;&#24615;&#35299;&#30721;&#21508;&#20010;&#20195;&#29702;&#20154;&#35266;&#28857;&#19979;&#30340;&#20449;&#24565;&#29366;&#24577;&#26159;&#21487;&#33021;&#30340;&#65292;&#36825;&#34920;&#26126;&#23384;&#22312;&#33258;&#25105;&#30340;&#20869;&#37096;&#34920;&#36848;&#21644;&#20182;&#20154;&#20449;&#24565;&#30340;&#34920;&#31034;&#12290;&#36890;&#36807;&#25805;&#32437;&#36825;&#20123;&#34920;&#24449;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#27169;&#22411;&#30340;ToM&#24615;&#33021;&#21457;&#29983;&#26174;&#33879;&#21464;&#21270;&#65292;&#31361;&#26174;&#20102;&#20854;&#22312;&#31038;&#20250;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#36824;&#24310;&#20280;&#21040;&#28041;&#21450;&#19981;&#21516;&#22240;&#26524;&#25512;&#29702;&#27169;&#24335;&#30340;&#22810;&#26679;&#31038;&#20250;&#25512;&#29702;&#20219;&#21153;&#65292;&#26263;&#31034;&#20102;&#36825;&#20123;&#34920;&#24449;&#30340;&#28508;&#22312;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18496v1 Announce Type: new  Abstract: Understanding and attributing mental states, known as Theory of Mind (ToM), emerges as a fundamental capability for human social reasoning. While Large Language Models (LLMs) appear to possess certain ToM abilities, the mechanisms underlying these capabilities remain elusive. In this study, we discover that it is possible to linearly decode the belief status from the perspectives of various agents through neural activations of language models, indicating the existence of internal representations of self and others' beliefs. By manipulating these representations, we observe dramatic changes in the models' ToM performance, underscoring their pivotal role in the social reasoning process. Additionally, our findings extend to diverse social reasoning tasks that involve different causal inference patterns, suggesting the potential generalizability of these representations.
&lt;/p&gt;</description></item><item><title>NewsQs&#26159;&#19968;&#20010;&#20026;&#22810;&#20010;&#26032;&#38395;&#25991;&#26723;&#25552;&#20379;&#38382;&#39064;-&#31572;&#26696;&#23545;&#30340;&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;T5-Large&#27169;&#22411;fine-tune&#33258;&#21160;&#29983;&#25104;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#27169;&#22411;&#36827;&#34892;&#25511;&#21046;&#30721;&#24494;&#35843;&#65292;&#21487;&#20197;&#20135;&#29983;&#26356;&#22810;&#21487;&#25509;&#21463;&#30340;&#38382;&#39064;&#65292;&#37322;&#25918;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#20379;&#26410;&#26469;&#22522;&#20110;&#26597;&#35810;&#30340;&#22810;&#25991;&#26723;&#24635;&#32467;&#30740;&#31350;&#20351;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.18479</link><description>&lt;p&gt;
NewsQs: &#22810;&#28304;&#38382;&#39064;&#29983;&#25104;&#21161;&#21147;&#25506;&#30693;&#24515;&#28789;
&lt;/p&gt;
&lt;p&gt;
NewsQs: Multi-Source Question Generation for the Inquiring Mind
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18479
&lt;/p&gt;
&lt;p&gt;
NewsQs&#26159;&#19968;&#20010;&#20026;&#22810;&#20010;&#26032;&#38395;&#25991;&#26723;&#25552;&#20379;&#38382;&#39064;-&#31572;&#26696;&#23545;&#30340;&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;T5-Large&#27169;&#22411;fine-tune&#33258;&#21160;&#29983;&#25104;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#27169;&#22411;&#36827;&#34892;&#25511;&#21046;&#30721;&#24494;&#35843;&#65292;&#21487;&#20197;&#20135;&#29983;&#26356;&#22810;&#21487;&#25509;&#21463;&#30340;&#38382;&#39064;&#65292;&#37322;&#25918;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#20379;&#26410;&#26469;&#22522;&#20110;&#26597;&#35810;&#30340;&#22810;&#25991;&#26723;&#24635;&#32467;&#30740;&#31350;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102; NewsQs&#65288;news-cues&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#20026;&#22810;&#20010;&#26032;&#38395;&#25991;&#26723;&#25552;&#20379;&#38382;&#39064;-&#31572;&#26696;&#23545;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#21019;&#24314; NewsQs&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#32463;&#36807; T5-Large &#27169;&#22411;&#22312; News On the Web &#35821;&#26009;&#24211;&#20013;&#30340; FAQ &#26679;&#24335;&#26032;&#38395;&#25991;&#31456;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#65292;&#33258;&#21160;&#29983;&#25104;&#38382;&#39064;&#26469;&#25193;&#20805;&#20256;&#32479;&#30340;&#22810;&#25991;&#26723;&#24635;&#32467;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#25511;&#21046;&#30721;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#20135;&#29983;&#26356;&#22810;&#34987;&#20154;&#31867;&#35780;&#20215;&#20026;&#21487;&#25509;&#21463;&#30340;&#38382;&#39064;&#65292;&#36825;&#26159;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#25152;&#27979;&#24471;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;&#19982;&#20154;&#31867;&#27880;&#37322;&#39640;&#24230;&#30456;&#20851;&#30340; QNLI &#27169;&#22411;&#26469;&#36807;&#28388;&#25105;&#20204;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#20316;&#20026;&#26410;&#26469;&#22522;&#20110;&#26597;&#35810;&#30340;&#22810;&#25991;&#26723;&#24635;&#32467;&#24037;&#20316;&#36164;&#28304;&#30340;&#39640;&#36136;&#37327;&#38382;&#39064;&#12289;&#31572;&#26696;&#21644;&#25991;&#26723;&#32858;&#31867;&#30340;&#26368;&#32456;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18479v1 Announce Type: new  Abstract: We present NewsQs (news-cues), a dataset that provides question-answer pairs for multiple news documents. To create NewsQs, we augment a traditional multi-document summarization dataset with questions automatically generated by a T5-Large model fine-tuned on FAQ-style news articles from the News On the Web corpus. We show that fine-tuning a model with control codes produces questions that are judged acceptable more often than the same model without them as measured through human evaluation. We use a QNLI model with high correlation with human annotations to filter our data. We release our final dataset of high-quality questions, answers, and document clusters as a resource for future work in query-based multi-document summarization.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#23884;&#20837;&#26041;&#27861;MetaEOL&#65292;&#36890;&#36807;&#20803;&#20219;&#21153;&#25552;&#31034;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#21477;&#23376;&#23884;&#20837;&#65292;&#26080;&#38656;&#27169;&#22411;&#24494;&#35843;&#25110;&#29305;&#23450;&#20219;&#21153;&#24037;&#31243;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#22312;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#27979;&#35797;&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;</title><link>https://arxiv.org/abs/2402.18458</link><description>&lt;p&gt;
&#20351;&#29992;&#20803;&#20219;&#21153;&#25552;&#31034;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20986;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Meta-Task Prompting Elicits Embedding from Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18458
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#23884;&#20837;&#26041;&#27861;MetaEOL&#65292;&#36890;&#36807;&#20803;&#20219;&#21153;&#25552;&#31034;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#21477;&#23376;&#23884;&#20837;&#65292;&#26080;&#38656;&#27169;&#22411;&#24494;&#35843;&#25110;&#29305;&#23450;&#20219;&#21153;&#24037;&#31243;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#22312;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#27979;&#35797;&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#23884;&#20837;&#26041;&#27861;&#65292;&#21363;&#24102;&#26174;&#24335;&#21333;&#35789;&#38480;&#21046;&#30340;&#20803;&#20219;&#21153;&#25552;&#31034;&#65288;MetaEOL&#65289;&#65292;&#29992;&#20110;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21477;&#23376;&#23884;&#20837;&#65292;&#26080;&#38656;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#25110;&#29305;&#23450;&#20219;&#21153;&#30340;&#24037;&#31243;&#12290;&#36890;&#36807;&#21033;&#29992;&#20803;&#20219;&#21153;&#25552;&#31034;&#65292;MetaEOL&#24341;&#23548;LLMs&#36890;&#36807;&#19968;&#31995;&#21015;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#29983;&#25104;&#23884;&#20837;&#65292;&#36825;&#20123;&#25552;&#31034;&#28085;&#30422;&#20102;&#22810;&#20010;&#34920;&#31034;&#26041;&#38754;&#12290;&#25105;&#20204;&#20840;&#38754;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20174;&#21508;&#31181;&#20803;&#20219;&#21153;&#24179;&#22343;&#24471;&#21040;&#30340;&#23884;&#20837;&#22312;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#65288;STS&#65289;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#65292;&#24182;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#21331;&#36234;&#65292;&#36229;&#36234;&#20102;&#23545;&#27604;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25552;&#20986;&#20102;&#19968;&#31181;&#23884;&#20837;&#29983;&#25104;&#30340;&#26032;&#30340;&#25193;&#23637;&#23450;&#24459;&#65292;&#20026;&#36328;&#22810;&#31181;&#20197;&#21477;&#23376;&#20026;&#20013;&#24515;&#30340;&#22330;&#26223;&#20013;&#30340;&#23884;&#20837;&#25552;&#21462;&#25552;&#20379;&#20102;&#19968;&#31181;&#22810;&#25165;&#22810;&#33402;&#12289;&#36164;&#28304;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18458v1 Announce Type: new  Abstract: In this work, we introduce a new unsupervised embedding method, Meta-Task Prompting with Explicit One-Word Limitation (MetaEOL), for generating high-quality sentence embeddings from Large Language Models (LLMs) without the need for model fine-tuning or task-specific engineering. Leveraging meta-task prompting, MetaEOL guides LLMs to produce embeddings through a series of carefully designed prompts that address multiple representational aspects. Our comprehensive experiments demonstrate that embeddings averaged from various meta-tasks yield competitive performance on Semantic Textual Similarity (STS) benchmarks and excel in downstream tasks, surpassing contrastive-trained models. Our findings suggest a new scaling law for embedding generation, offering a versatile, resource-efficient approach for embedding extraction across diverse sentence-centric scenarios.
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;HOP&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#24341;&#20837;&#20102;&#19977;&#20010;&#26041;&#21521;&#20197;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#36328;&#20219;&#21153;&#21644;&#39046;&#22495;&#36827;&#34892;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.18449</link><description>&lt;p&gt;
HOP&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#36830;&#32493;&#23398;&#20064;&#30340;&#19979;&#19968;&#20010;&#20219;&#21153;&#21644;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
HOP to the Next Tasks and Domains for Continual Learning in NLP
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18449
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;HOP&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#24341;&#20837;&#20102;&#19977;&#20010;&#26041;&#21521;&#20197;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#36328;&#20219;&#21153;&#21644;&#39046;&#22495;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#26088;&#22312;&#36890;&#36807;&#36716;&#31227;&#20808;&#21069;&#38382;&#39064;&#20013;&#33719;&#24471;&#30340;&#30693;&#35782;&#26469;&#23398;&#20064;&#19968;&#31995;&#21015;&#38382;&#39064;&#65288;&#21363;&#20219;&#21153;&#21644;&#39046;&#22495;&#65289;&#65292;&#21516;&#26102;&#36991;&#20813;&#36951;&#24536;&#36807;&#21435;&#30340;&#38382;&#39064;&#12290;&#19982;&#20808;&#21069;&#19987;&#27880;&#20110;&#29305;&#23450;&#29992;&#20363;&#20013;&#19968;&#20010;NLP&#20219;&#21153;&#25110;&#39046;&#22495;&#30340;CL&#26041;&#27861;&#19981;&#21516;&#65292;&#26412;&#25991;&#38024;&#23545;&#19968;&#20010;&#26356;&#36890;&#29992;&#30340;CL&#35774;&#32622;&#65292;&#20174;&#19968;&#20010;&#21807;&#19968;&#30340;&#26694;&#26550;&#20013;&#23398;&#20064;&#19968;&#31995;&#21015;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;HOP&#36890;&#36807;&#27839;&#19977;&#20010;&#26041;&#21521;&#35299;&#20915;CL&#38382;&#39064;&#26469;&#20801;&#35768;&#22312;&#20219;&#21153;&#21644;&#39046;&#22495;&#20043;&#38388;&#36339;&#36291;&#65306;&#65288;i&#65289;&#25105;&#20204;&#20351;&#29992;&#19968;&#32452;&#36866;&#37197;&#22120;&#23558;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#25512;&#24191;&#21040;&#26410;&#35265;&#38382;&#39064;&#65292;&#65288;ii&#65289;&#25105;&#20204;&#35745;&#31639;&#23884;&#20837;&#34920;&#31034;&#20998;&#24067;&#19978;&#30340;&#39640;&#38454;&#30697;&#20197;&#21306;&#20998;&#19981;&#21516;&#20219;&#21153;&#21644;&#39046;&#22495;&#20043;&#38388;&#30340;&#29420;&#31435;&#21644;&#30456;&#20851;&#32479;&#35745;&#25968;&#25454;&#65292;&#65288;iii&#65289;&#25105;&#20204;&#36890;&#36807;&#20026;&#27599;&#20010;&#26368;&#32456;&#38382;&#39064;&#19987;&#38376;&#35774;&#35745;&#30340;&#36741;&#21161;&#22836;&#22788;&#29702;&#36825;&#20123;&#20016;&#23500;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;4&#20010;NLP&#24212;&#29992;&#31243;&#24207;&#65292;5&#20010;&#22522;&#20934;&#27979;&#35797;&#21644;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18449v1 Announce Type: cross  Abstract: Continual Learning (CL) aims to learn a sequence of problems (i.e., tasks and domains) by transferring knowledge acquired on previous problems, whilst avoiding forgetting of past ones. Different from previous approaches which focused on CL for one NLP task or domain in a specific use-case, in this paper, we address a more general CL setting to learn from a sequence of problems in a unique framework. Our method, HOP, permits to hop across tasks and domains by addressing the CL problem along three directions: (i) we employ a set of adapters to generalize a large pre-trained model to unseen problems, (ii) we compute high-order moments over the distribution of embedded representations to distinguish independent and correlated statistics across different tasks and domains, (iii) we process this enriched information with auxiliary heads specialized for each end problem. Extensive experimental campaign on 4 NLP applications, 5 benchmarks and 
&lt;/p&gt;</description></item><item><title>&#25361;&#25112;&#20102;&#40664;&#35748;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#30340;&#20570;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;LLMs&#22312;&#25512;&#29702;&#21644;&#27807;&#36890;&#20013;&#20351;&#29992;&#26367;&#20195;&#26684;&#24335;&#30340;&#23454;&#29992;&#24615;&#65292;&#23454;&#29616;&#20102;&#25512;&#29702;&#25928;&#29575;&#30340;&#25552;&#21319;&#21644;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#26102;&#26631;&#35760;&#20351;&#29992;&#37327;&#30340;&#26174;&#33879;&#20943;&#23569;&#12290;</title><link>https://arxiv.org/abs/2402.18439</link><description>&lt;p&gt;
&#36229;&#36234;&#33258;&#28982;&#35821;&#35328;&#65306;LLM&#21033;&#29992;&#26367;&#20195;&#26684;&#24335;&#36827;&#34892;&#22686;&#24378;&#25512;&#29702;&#21644;&#27807;&#36890;
&lt;/p&gt;
&lt;p&gt;
Beyond Natural Language: LLMs Leveraging Alternative Formats for Enhanced Reasoning and Communication
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18439
&lt;/p&gt;
&lt;p&gt;
&#25361;&#25112;&#20102;&#40664;&#35748;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#30340;&#20570;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;LLMs&#22312;&#25512;&#29702;&#21644;&#27807;&#36890;&#20013;&#20351;&#29992;&#26367;&#20195;&#26684;&#24335;&#30340;&#23454;&#29992;&#24615;&#65292;&#23454;&#29616;&#20102;&#25512;&#29702;&#25928;&#29575;&#30340;&#25552;&#21319;&#21644;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#26102;&#26631;&#35760;&#20351;&#29992;&#37327;&#30340;&#26174;&#33879;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#65288;NL&#65289;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#20154;&#31867;&#35748;&#30693;&#21644;&#27807;&#36890;&#30340;&#20027;&#35201;&#26684;&#24335;&#65292;&#22240;&#27492;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#21644;&#24212;&#29992;&#20013;&#21516;&#26679;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;NL&#20043;&#22806;&#65292;LLMs&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#30475;&#21040;&#20102;&#21508;&#31181;&#38750;NL&#26684;&#24335;&#65292;&#22914;&#20195;&#30721;&#21644;&#36923;&#36753;&#34920;&#36798;&#24335;&#12290;NL&#20316;&#20026;LLMs&#30340;&#26368;&#20339;&#26684;&#24335;&#65292;&#22312;&#21333;&#19968;LLM&#25512;&#29702;&#21644;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#20013;&#30340;&#22320;&#20301;&#23578;&#26410;&#24471;&#21040;&#24443;&#24213;&#23457;&#26597;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;&#40664;&#35748;&#20351;&#29992;NL&#65292;&#36890;&#36807;&#25506;&#32034;&#22312;&#36825;&#20123;&#19978;&#19979;&#25991;&#20013;&#20351;&#29992;&#38750;NL&#26684;&#24335;&#30340;&#25928;&#29992;&#12290;&#25105;&#20204;&#23637;&#31034;&#65292;&#20801;&#35768;LLMs&#22312;&#25512;&#29702;&#25110;&#27807;&#36890;&#20043;&#21069;&#33258;&#20027;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#26684;&#24335;&#65292;&#21487;&#23548;&#33268;&#19981;&#21516;LLMs&#25512;&#29702;&#25928;&#29575;&#25552;&#39640;3.3&#33267;5.7&#65285;&#65292;&#24182;&#19988;&#22312;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#20013;&#26368;&#22810;&#21487;&#20943;&#23569;72.7&#65285;&#30340;&#26631;&#35760;&#20351;&#29992;&#65292;&#21516;&#26102;&#20445;&#25345;&#27807;&#36890;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#32508;&#21512;&#20998;&#26512;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;LLMs&#21487;&#20197;......
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18439v1 Announce Type: cross  Abstract: Natural language (NL) has long been the predominant format for human cognition and communication, and by extension, has been similarly pivotal in the development and application of Large Language Models (LLMs). Yet, besides NL, LLMs have seen various non-NL formats during pre-training, such as code and logical expression. NL's status as the optimal format for LLMs, particularly in single-LLM reasoning and multi-agent communication, has not been thoroughly examined. In this work, we challenge the default use of NL by exploring the utility of non-NL formats in these contexts. We show that allowing LLMs to autonomously select the most suitable format before reasoning or communicating leads to a 3.3 to 5.7\% improvement in reasoning efficiency for different LLMs, and up to a 72.7\% reduction in token usage in multi-agent communication, all while maintaining communicative effectiveness. Our comprehensive analysis further reveals that LLMs c
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36890;&#29992;&#21327;&#20316;&#23398;&#20064;&#26041;&#27861;DCMCL&#65292;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#21033;&#29992;&#22810;&#26679;&#21270;&#24314;&#27169;&#19978;&#19979;&#25991;&#65292;&#23558;&#33258;&#22238;&#24402;&#65288;AR&#65289;&#21644;&#38750;&#33258;&#22238;&#24402;&#65288;NAR&#65289;&#27169;&#22411;&#20316;&#20026;&#21512;&#20316;&#32773;&#12290;</title><link>https://arxiv.org/abs/2402.18428</link><description>&lt;p&gt;
&#21033;&#29992;&#21327;&#21516;&#23398;&#20064;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#21033;&#29992;&#22810;&#26679;&#21270;&#24314;&#27169;&#19978;&#19979;&#25991;
&lt;/p&gt;
&lt;p&gt;
Leveraging Diverse Modeling Contexts with Collaborating Learning for Neural Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18428
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36890;&#29992;&#21327;&#20316;&#23398;&#20064;&#26041;&#27861;DCMCL&#65292;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#21033;&#29992;&#22810;&#26679;&#21270;&#24314;&#27169;&#19978;&#19979;&#25991;&#65292;&#23558;&#33258;&#22238;&#24402;&#65288;AR&#65289;&#21644;&#38750;&#33258;&#22238;&#24402;&#65288;NAR&#65289;&#27169;&#22411;&#20316;&#20026;&#21512;&#20316;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#22238;&#24402;&#65288;AR&#65289;&#21644;&#38750;&#33258;&#22238;&#24402;&#65288;NAR&#65289;&#27169;&#22411;&#26159;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#30340;&#20004;&#31181;&#29983;&#25104;&#27169;&#22411;&#12290;AR&#27169;&#22411;&#20197;&#36880;&#35789;&#26041;&#24335;&#39044;&#27979;&#20196;&#29260;&#65292;&#21487;&#20197;&#26377;&#25928;&#25429;&#25417;&#30495;&#23454;&#32763;&#35793;&#30340;&#20998;&#24067;&#12290;NAR&#27169;&#22411;&#36890;&#36807;&#25552;&#21462;&#21452;&#21521;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#39044;&#27979;&#20196;&#29260;&#65292;&#21487;&#20197;&#25552;&#39640;&#25512;&#29702;&#36895;&#24230;&#65292;&#20294;&#23427;&#20204;&#20250;&#36973;&#21463;&#24615;&#33021;&#19979;&#38477;&#12290; &#20197;&#24448;&#30340;&#30740;&#31350;&#21033;&#29992;AR&#27169;&#22411;&#26469;&#25913;&#36827;NAR&#27169;&#22411;&#65292;&#36890;&#36807;&#20943;&#23569;&#35757;&#32451;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#25110;&#36890;&#36807;NAR&#27169;&#22411;&#23558;&#20840;&#23616;&#20449;&#24687;&#34701;&#20837;AR&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20165;&#21033;&#29992;&#21333;&#19968;&#31867;&#22411;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24573;&#35270;&#20102;&#19981;&#21516;&#31867;&#22411;&#27169;&#22411;&#21487;&#20197;&#25552;&#20379;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#22810;&#26679;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36890;&#29992;&#21327;&#20316;&#23398;&#20064;&#26041;&#27861;&#65292;DCMCL&#65292;&#20854;&#20013;&#23558;AR&#21644;NAR&#27169;&#22411;&#35270;&#20026;&#21512;&#20316;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18428v1 Announce Type: new  Abstract: Autoregressive (AR) and Non-autoregressive (NAR) models are two types of generative models for Neural Machine Translation (NMT). AR models predict tokens in a word-by-word manner and can effectively capture the distribution of real translations. NAR models predict tokens by extracting bidirectional contextual information which can improve the inference speed but they suffer from performance degradation. Previous works utilized AR models to enhance NAR models by reducing the training data's complexity or incorporating the global information into AR models by virtue of NAR models. However, those investigated methods only take advantage of the contextual information of a single type of model while neglecting the diversity in the contextual information that can be provided by different types of models. In this paper, we propose a novel generic collaborative learning method, DCMCL, where AR and NAR models are treated as collaborators instead 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36328;&#35821;&#35328;&#24773;&#24863;&#20998;&#31867;&#22120;&#65292;&#22312;&#20302;&#21644;&#20013;&#31561;&#36164;&#28304;&#35821;&#35328;&#20013;&#23454;&#29616;&#24773;&#24863;&#20998;&#31867;&#65292;&#23637;&#31034;&#20102;&#20004;&#31181;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.18424</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#21644;&#20013;&#31561;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#24773;&#24863;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Emotion Classification in Low and Moderate Resource Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18424
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36328;&#35821;&#35328;&#24773;&#24863;&#20998;&#31867;&#22120;&#65292;&#22312;&#20302;&#21644;&#20013;&#31561;&#36164;&#28304;&#35821;&#35328;&#20013;&#23454;&#29616;&#24773;&#24863;&#20998;&#31867;&#65292;&#23637;&#31034;&#20102;&#20004;&#31181;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#20998;&#26512;&#20840;&#29699;&#33539;&#22260;&#20869;&#20154;&#20204;&#24773;&#32490;&#29366;&#24577;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;&#20840;&#29699;&#26377;7100&#22810;&#31181;&#27963;&#36291;&#35821;&#35328;&#65292;&#20026;&#27599;&#31181;&#35821;&#35328;&#26500;&#24314;&#24773;&#24863;&#20998;&#31867;&#26159;&#19968;&#39033;&#21171;&#21160;&#23494;&#38598;&#22411;&#24037;&#20316;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#20302;&#36164;&#28304;&#21644;&#28626;&#21361;&#35821;&#35328;&#65292;&#24314;&#31435;&#24773;&#24863;&#20998;&#31867;&#21487;&#33021;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#35821;&#35328;&#24773;&#24863;&#20998;&#31867;&#22120;&#65292;&#25105;&#20204;&#22312;&#36164;&#28304;&#20016;&#23500;&#30340;&#35821;&#35328;&#65288;&#20363;&#22914;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#30340;&#33521;&#35821;&#65289;&#19978;&#35757;&#32451;&#24773;&#24863;&#20998;&#31867;&#22120;&#65292;&#24182;&#23558;&#23398;&#20064;&#36801;&#31227;&#21040;&#20302;&#36164;&#28304;&#21644;&#20013;&#31561;&#36164;&#28304;&#30340;&#35821;&#35328;&#12290;&#25105;&#20204;&#27604;&#36739;&#24182;&#23545;&#27604;&#20102;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#21040;&#20302;&#36164;&#28304;&#25110;&#20013;&#31561;&#36164;&#28304;&#35821;&#35328;&#30340;&#20004;&#31181;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#12290;&#19968;&#31181;&#26041;&#27861;&#23558;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#26631;&#27880;&#25237;&#24433;&#21040;&#20302;&#36164;&#28304;&#21644;&#20013;&#31561;&#36164;&#28304;&#35821;&#35328;&#30340;&#24179;&#34892;&#35821;&#26009;&#24211;&#20013;&#65292;&#21478;&#19968;&#31181;&#26041;&#27861;&#30452;&#25509;&#23558;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#23398;&#20064;&#36801;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;6&#31181;&#35821;&#35328;&#19978;&#30340;&#26377;&#25928;&#24615;&#65306;Fa
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18424v1 Announce Type: cross  Abstract: It is important to be able to analyze the emotional state of people around the globe. There are 7100+ active languages spoken around the world and building emotion classification for each language is labor intensive. Particularly for low-resource and endangered languages, building emotion classification can be quite challenging. We present a cross-lingual emotion classifier, where we train an emotion classifier with resource-rich languages (i.e. \textit{English} in our work) and transfer the learning to low and moderate resource languages. We compare and contrast two approaches of transfer learning from a high-resource language to a low or moderate-resource language. One approach projects the annotation from a high-resource language to low and moderate-resource language in parallel corpora and the other one uses direct transfer from high-resource language to the other languages. We show the efficacy of our approaches on 6 languages: Fa
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#38382;&#31572;&#20219;&#21153;&#65292;GPT&#33021;&#22815;&#39564;&#35777;&#21307;&#30103;&#39046;&#22495;&#24739;&#32773;&#30340;PA&#35831;&#27714;&#65292;&#24110;&#21161;&#21355;&#29983;&#35745;&#21010;&#26356;&#24555;&#22320;&#20570;&#20986;&#20915;&#31574;&#12290;</title><link>https://arxiv.org/abs/2402.18419</link><description>&lt;p&gt;
&#33021;&#21542;&#36890;&#36807;&#22522;&#20110;&#25351;&#21335;&#30340;&#33258;&#21160;&#38382;&#31572;&#26469;&#25913;&#21892;GPT&#30340;&#20808;&#21069;&#25480;&#26435;&#29366;&#24577;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can GPT Improve the State of Prior Authorization via Guideline Based Automated Question Answering?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18419
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38382;&#31572;&#20219;&#21153;&#65292;GPT&#33021;&#22815;&#39564;&#35777;&#21307;&#30103;&#39046;&#22495;&#24739;&#32773;&#30340;PA&#35831;&#27714;&#65292;&#24110;&#21161;&#21355;&#29983;&#35745;&#21010;&#26356;&#24555;&#22320;&#20570;&#20986;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21355;&#29983;&#20445;&#38505;&#20844;&#21496;&#26377;&#19968;&#20010;&#34987;&#31216;&#20026;&#20808;&#21069;&#25480;&#26435;&#65288;PA&#65289;&#30340;&#27969;&#31243;&#65292;&#36825;&#26159;&#19968;&#31181;&#21355;&#29983;&#35745;&#21010;&#25104;&#26412;&#25511;&#21046;&#27969;&#31243;&#65292;&#35201;&#27714;&#21307;&#29983;&#21644;&#20854;&#20182;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#22312;&#23545;&#24739;&#32773;&#25191;&#34892;&#29305;&#23450;&#31243;&#24207;&#20043;&#21069;&#24517;&#39035;&#20107;&#20808;&#33719;&#24471;&#21355;&#29983;&#35745;&#21010;&#30340;&#25209;&#20934;&#65292;&#20197;&#20415;&#26377;&#36164;&#26684;&#33719;&#24471;&#25903;&#20184;&#35206;&#30422;&#12290;&#23545;&#21355;&#29983;&#20445;&#38505;&#20844;&#21496;&#26469;&#35828;&#65292;&#25209;&#20934;&#21307;&#30103;&#39046;&#22495;&#24739;&#32773;&#30340;PA&#35831;&#27714;&#26159;&#19968;&#39033;&#32791;&#26102;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20854;&#20013;&#30340;&#19968;&#39033;&#20851;&#38190;&#25361;&#25112;&#26159;&#39564;&#35777;&#35831;&#27714;&#26159;&#21542;&#31526;&#21512;&#26576;&#20123;&#26631;&#20934;&#65292;&#22914;&#24180;&#40836;&#12289;&#24615;&#21035;&#31561;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;GPT&#26159;&#21542;&#33021;&#39564;&#35777;&#22823;&#37327;&#20851;&#38190;&#22240;&#32032;&#65292;&#20174;&#32780;&#24110;&#21161;&#21355;&#29983;&#35745;&#21010;&#26356;&#24555;&#22320;&#20570;&#20986;&#20915;&#31574;&#12290;&#25105;&#20204;&#23558;&#20854;&#26500;&#24314;&#20026;&#19968;&#20010;&#38382;&#31572;&#20219;&#21153;&#65292;&#20419;&#20351;GPT&#20174;&#24739;&#32773;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#22238;&#31572;&#38382;&#39064;&#12290;&#25105;&#20204;&#23581;&#35797;&#20102;&#19981;&#21516;&#30340;&#20256;&#32479;&#25552;&#31034;&#25216;&#26415;&#65292;&#21516;&#26102;&#36824;&#24341;&#20837;&#20102;&#25105;&#20204;&#33258;&#24049;&#30340;&#26032;&#39062;&#25552;&#31034;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18419v1 Announce Type: cross  Abstract: Health insurance companies have a defined process called prior authorization (PA) which is a health plan cost-control process that requires doctors and other healthcare professionals to get clearance in advance from a health plan before performing a particular procedure on a patient in order to be eligible for payment coverage. For health insurance companies, approving PA requests for patients in the medical domain is a time-consuming and challenging task. One of those key challenges is validating if a request matches up to certain criteria such as age, gender, etc. In this work, we evaluate whether GPT can validate numerous key factors, in turn helping health plans reach a decision drastically faster. We frame it as a question answering task, prompting GPT to answer a question from patient electronic health record. We experiment with different conventional prompting techniques as well as introduce our own novel prompting technique. Mo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#30693;&#33021;&#21147;&#65292;&#21457;&#29616;LVLMs&#19982;&#20154;&#31867;&#20043;&#38388;&#23384;&#22312;&#36739;&#22823;&#30340;&#35748;&#30693;&#33021;&#21147;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.18409</link><description>&lt;p&gt;
&#19968;&#20010;&#38024;&#23545;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22270;&#20687;&#25512;&#29702;&#21644;&#25551;&#36848;&#30340;&#35748;&#30693;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18409
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#30693;&#33021;&#21147;&#65292;&#21457;&#29616;LVLMs&#19982;&#20154;&#31867;&#20043;&#38388;&#23384;&#22312;&#36739;&#22823;&#30340;&#35748;&#30693;&#33021;&#21147;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;(LVLMs)&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#24456;&#23569;&#21463;&#21040;&#20840;&#38754;&#30340;&#35748;&#30693;&#33021;&#21147;&#27979;&#35797;&#12290;&#21463;&#21040;&#20154;&#31867;&#35748;&#30693;&#27979;&#35797;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#8220;&#20599;&#39292;&#24178;&#8221;&#20219;&#21153;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#21033;&#29992;&#20855;&#26377;&#20016;&#23500;&#35821;&#20041;&#30340;&#22270;&#20687;&#35780;&#20272;LVLMs&#30340;&#39640;&#32423;&#35748;&#30693;&#33021;&#21147;&#12290;&#23427;&#23450;&#20041;&#20102;&#20843;&#31181;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21253;&#25324;&#22270;&#20687;&#25551;&#36848;&#20219;&#21153;&#21644;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#12290;&#25105;&#20204;&#23545;&#30693;&#21517;LVLMs&#36827;&#34892;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#22312;LVLMs&#21644;&#20154;&#31867;&#20043;&#38388;&#20173;&#23384;&#22312;&#36739;&#22823;&#30340;&#35748;&#30693;&#33021;&#21147;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18409v1 Announce Type: new  Abstract: Large Vision Language Models (LVLMs), despite their recent success, are hardly comprehensively tested for their cognitive abilities. Inspired by the prevalent use of the "Cookie Theft" task in human cognition test, we propose a novel evaluation benchmark to evaluate high-level cognitive ability of LVLMs using images with rich semantics. It defines eight reasoning capabilities and consists of an image description task and a visual question answering task. Our evaluation on well-known LVLMs shows that there is still a large gap in cognitive ability between LVLMs and humans.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#35299;&#25552;&#31034;&#26041;&#27861;&#65292;&#36825;&#39033;&#30740;&#31350;&#25581;&#31034;&#20102;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#35821;&#35328;&#20219;&#21153;&#19978;&#30340;&#35821;&#35328;&#32467;&#26500;&#29702;&#35299;&#33021;&#21147;&#65292;&#35777;&#23454;&#20854;&#22312;&#38646;&#27425;&#21644;&#23569;&#27425;&#36845;&#20195;&#35774;&#32622;&#20013;&#30340;&#39640;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.18397</link><description>&lt;p&gt;
&#20998;&#35299;&#25552;&#31034;&#65306;&#25581;&#31034;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#32467;&#26500;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Decomposed Prompting: Unveiling Multilingual Linguistic Structure Knowledge in English-Centric Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18397
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#35299;&#25552;&#31034;&#26041;&#27861;&#65292;&#36825;&#39033;&#30740;&#31350;&#25581;&#31034;&#20102;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#35821;&#35328;&#20219;&#21153;&#19978;&#30340;&#35821;&#35328;&#32467;&#26500;&#29702;&#35299;&#33021;&#21147;&#65292;&#35777;&#23454;&#20854;&#22312;&#38646;&#27425;&#21644;&#23569;&#27425;&#36845;&#20195;&#35774;&#32622;&#20013;&#30340;&#39640;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33521;&#35821;&#22312;&#23427;&#20204;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#21344;&#20027;&#23548;&#22320;&#20301;&#65292;&#31867;&#20284;GPT-3&#21644;LLaMA&#36825;&#26679;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#22810;&#35821;&#35328;&#20219;&#21153;&#33021;&#21147;&#65292;&#36825;&#24341;&#21457;&#20102;&#20851;&#20110;&#20854;&#36328;&#35821;&#35328;&#33021;&#21147;&#28145;&#24230;&#21644;&#24615;&#36136;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#20998;&#35299;&#25552;&#31034;&#26041;&#27861;&#65292;&#29992;&#20197;&#25506;&#31350;&#36825;&#20123;LLMs&#22312;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#20013;&#30340;&#35821;&#35328;&#32467;&#26500;&#29702;&#35299;&#33021;&#21147;&#12290;&#19982;&#21333;&#19968;&#25991;&#26412;&#21040;&#25991;&#26412;&#25552;&#31034;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#36755;&#20837;&#21477;&#23376;&#30340;&#27599;&#20010;&#20196;&#29260;&#29983;&#25104;&#19968;&#20010;&#21333;&#29420;&#30340;&#25552;&#31034;&#65292;&#35810;&#38382;&#20854;&#35821;&#35328;&#26631;&#31614;&#12290;&#25105;&#20204;&#22312;38&#31181;&#35821;&#35328;&#30340;&#36890;&#29992;&#20381;&#36182;&#35789;&#24615;&#26631;&#27880;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#21644;&#22810;&#35821;&#35328;LLMs&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20998;&#35299;&#25552;&#31034;&#22312;&#38646;&#27425;&#21644;&#23569;&#27425;&#36845;&#20195;&#35774;&#32622;&#19979;&#30340;&#25928;&#21147;&#21644;&#25928;&#29575;&#22343;&#36229;&#36807;&#20102;&#36845;&#20195;&#25552;&#31034;&#22522;&#32447;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#35780;&#20272;&#26041;&#27861;&#21644;&#20182;&#20204;&#20043;&#38388;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18397v1 Announce Type: new  Abstract: Despite the predominance of English in their training data, English-centric Large Language Models (LLMs) like GPT-3 and LLaMA display a remarkable ability to perform multilingual tasks, raising questions about the depth and nature of their cross-lingual capabilities. This paper introduces the decomposed prompting approach to probe the linguistic structure understanding of these LLMs in sequence labeling tasks. Diverging from the single text-to-text prompt, our method generates for each token of the input sentence an individual prompt which asks for its linguistic label. We assess our method on the Universal Dependencies part-of-speech tagging dataset for 38 languages, utilizing both English-centric and multilingual LLMs. Our findings show that decomposed prompting surpasses the iterative prompting baseline in efficacy and efficiency under zero- and few-shot settings. Further analysis reveals the influence of evaluation methods and the us
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;WSDM Cup 2024&#20013;&#33719;&#32988;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21331;&#36234;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#36890;&#36807;&#25913;&#32534;LLMs&#12289;&#35774;&#35745;&#28151;&#21512;&#35757;&#32451;&#31574;&#30053;&#12289;&#37319;&#29992;&#20808;&#36827;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#12289;&#20197;&#21450;&#27169;&#22411;&#38598;&#25104;&#31561;&#22810;&#31181;&#25216;&#26415;&#65292;&#26368;&#32456;&#22312;&#20250;&#35805;&#24335;&#22810;&#25991;&#26723;&#38382;&#31572;&#26041;&#38754;&#21462;&#24471;&#20102;&#31532;&#19968;&#21517;&#12290;</title><link>https://arxiv.org/abs/2402.18385</link><description>&lt;p&gt;
WSDM Cup 2024&#30340;&#31532;&#19968;&#21517;&#35299;&#20915;&#26041;&#26696;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20250;&#35805;&#24335;&#22810;&#25991;&#26723;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
The First Place Solution of WSDM Cup 2024: Leveraging Large Language Models for Conversational Multi-Doc QA
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;WSDM Cup 2024&#20013;&#33719;&#32988;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21331;&#36234;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#36890;&#36807;&#25913;&#32534;LLMs&#12289;&#35774;&#35745;&#28151;&#21512;&#35757;&#32451;&#31574;&#30053;&#12289;&#37319;&#29992;&#20808;&#36827;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#12289;&#20197;&#21450;&#27169;&#22411;&#38598;&#25104;&#31561;&#22810;&#31181;&#25216;&#26415;&#65292;&#26368;&#32456;&#22312;&#20250;&#35805;&#24335;&#22810;&#25991;&#26723;&#38382;&#31572;&#26041;&#38754;&#21462;&#24471;&#20102;&#31532;&#19968;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35805;&#24335;&#22810;&#25991;&#26723;&#38382;&#31572;&#26088;&#22312;&#26681;&#25454;&#26816;&#32034;&#21040;&#30340;&#25991;&#26723;&#20197;&#21450;&#19978;&#19979;&#25991;&#23545;&#35805;&#26469;&#22238;&#31572;&#29305;&#23450;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;WSDM Cup 2024&#8220;&#20250;&#35805;&#24335;&#22810;&#25991;&#26723;&#38382;&#31572;&#8221;&#25361;&#25112;&#20013;&#30340;&#33719;&#32988;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21331;&#36234;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;LLMs&#25913;&#32534;&#20026;&#27492;&#20219;&#21153;&#65292;&#28982;&#21518;&#35774;&#35745;&#20102;&#28151;&#21512;&#35757;&#32451;&#31574;&#30053;&#65292;&#20805;&#20998;&#21033;&#29992;&#39046;&#22495;&#20869;&#26080;&#26631;&#31614;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#37319;&#29992;&#20102;&#20808;&#36827;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#26469;&#36807;&#28388;&#25481;&#28508;&#22312;&#30340;&#19981;&#30456;&#20851;&#25991;&#26723;&#65292;&#24182;&#20026;&#27169;&#22411;&#38598;&#25104;&#35774;&#35745;&#21644;&#27604;&#36739;&#20102;&#20960;&#31181;&#26041;&#27861;&#12290;&#20973;&#20511;&#25152;&#26377;&#36825;&#20123;&#25216;&#26415;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#26368;&#32456;&#22312;WSDM Cup 2024&#20013;&#25490;&#21517;&#31532;&#19968;&#65292;&#22823;&#22823;&#36229;&#36234;&#20102;&#31454;&#20105;&#23545;&#25163;&#12290;&#28304;&#20195;&#30721;&#24050;&#21457;&#24067;&#22312;https://github.com/zhangzhao219/WSDM-Cup-2024&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18385v1 Announce Type: new  Abstract: Conversational multi-doc question answering aims to answer specific questions based on the retrieved documents as well as the contextual conversations. In this paper, we introduce our winning approach for the "Conversational Multi-Doc QA" challenge in WSDM Cup 2024, which exploits the superior natural language understanding and generation capability of Large Language Models (LLMs). We first adapt LLMs to the task, then devise a hybrid training strategy to make the most of in-domain unlabeled data. Moreover, an advanced text embedding model is adopted to filter out potentially irrelevant documents and several approaches are designed and compared for the model ensemble. Equipped with all these techniques, our solution finally ranked 1st place in WSDM Cup 2024, surpassing its rivals to a large extent. The source codes have been released at https://github.com/zhangzhao219/WSDM-Cup-2024.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#20998;&#35789;&#22120;PathPiece&#65292;&#30740;&#31350;&#32773;&#21457;&#29616;&#23569;&#37327;&#26631;&#35760;&#24182;&#19981;&#33021;&#23548;&#33268;&#26356;&#22909;&#30340;&#19979;&#28216;&#24615;&#33021;&#65292;&#36825;&#19968;&#32467;&#26524;&#23545;&#20110; Tokenization &#30340;&#26377;&#25928;&#24615;&#29702;&#35299;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;</title><link>https://arxiv.org/abs/2402.18376</link><description>&lt;p&gt;
Tokenization&#36229;&#36234;&#20102;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Tokenization Is More Than Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18376
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#20998;&#35789;&#22120;PathPiece&#65292;&#30740;&#31350;&#32773;&#21457;&#29616;&#23569;&#37327;&#26631;&#35760;&#24182;&#19981;&#33021;&#23548;&#33268;&#26356;&#22909;&#30340;&#19979;&#28216;&#24615;&#33021;&#65292;&#36825;&#19968;&#32467;&#26524;&#23545;&#20110; Tokenization &#30340;&#26377;&#25928;&#24615;&#29702;&#35299;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Tokenization&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#30340;&#22522;&#30784;&#27493;&#39588;&#65292;&#23427;&#36830;&#25509;&#20102;&#21407;&#22987;&#25991;&#26412;&#21644;&#35821;&#35328;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;Tokenization&#26041;&#27861;&#65292;&#22914;&#23383;&#33410;&#23545;&#32534;&#30721;&#65288;Byte-Pair Encoding&#65292;BPE&#65289;&#65292;&#28304;&#33258;&#25968;&#25454;&#21387;&#32553;&#39046;&#22495;&#65292;&#24182;&#26377;&#20154;&#35748;&#20026;BPE&#30340;&#26377;&#25928;&#24615;&#28304;&#20110;&#20854;&#23558;&#25991;&#26412;&#21387;&#32553;&#20026;&#30456;&#23545;&#36739;&#23569;&#30340;&#26631;&#35760;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;PathPiece&#26469;&#27979;&#35797;&#8220;&#26356;&#23569;&#30340;&#26631;&#35760;&#26159;&#21542;&#20250;&#23548;&#33268;&#26356;&#22909;&#30340;&#19979;&#28216;&#24615;&#33021;&#8221;&#36825;&#19968;&#20551;&#35774;&#65292;PathPiece&#26159;&#19968;&#31181;&#26032;&#30340;&#20998;&#35789;&#22120;&#65292;&#26681;&#25454;&#32473;&#23450;&#35789;&#27719;&#23558;&#25991;&#26723;&#25991;&#26412;&#21010;&#20998;&#20026;&#26368;&#23569;&#25968;&#37327;&#30340;&#26631;&#35760;&#12290;&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#19968;&#20551;&#35774;&#24182;&#38750;&#25104;&#31435;&#65292;&#23545;&#26377;&#25928;Tokenization&#21407;&#22240;&#30340;&#29702;&#35299;&#20135;&#29983;&#20102;&#30097;&#38382;&#12290;&#20026;&#20102;&#26816;&#26597;&#21738;&#20123;&#20854;&#20182;&#22240;&#32032;&#36215;&#21040;&#20316;&#29992;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;Tokenization&#30340;&#25152;&#26377;&#19977;&#20010;&#38454;&#27573;&#65288;&#39044;&#20998;&#35789;&#12289;&#35789;&#27719;&#26500;&#36896;&#21644;&#20998;&#21106;&#65289;&#30340;&#35774;&#35745;&#20915;&#31574;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#35774;&#35745;&#30340;&#26032;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18376v1 Announce Type: cross  Abstract: Tokenization is a foundational step in Natural Language Processing (NLP) tasks, bridging raw text and language models. Existing tokenization approaches like Byte-Pair Encoding (BPE) originate from the field of data compression, and it has been suggested that the effectiveness of BPE stems from its ability to condense text into a relatively small number of tokens. We test the hypothesis that fewer tokens lead to better downstream performance by introducing PathPiece, a new tokenizer that segments a document's text into the minimum number of tokens for a given vocabulary. Through extensive experimentation we find this hypothesis not to be the case, casting doubt on the understanding of the reasons for effective tokenization. To examine which other factors play a role, we evaluate design decisions across all three phases of tokenization: pre-tokenization, vocabulary construction, and segmentation, offering new insights into the design of 
&lt;/p&gt;</description></item><item><title>VerifiNER&#26159;&#19968;&#20010;&#21518;&#32493;&#39564;&#35777;&#26694;&#26550;&#65292;&#36890;&#36807;&#30693;&#35782;&#35782;&#21035;&#24182;&#20462;&#27491;&#29616;&#26377;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;&#30340;&#38169;&#35823;&#65292;&#20197;&#23454;&#29616;&#26356;&#24544;&#23454;&#30340;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.18374</link><description>&lt;p&gt;
VerifiNER: &#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#20110;&#30693;&#35782;&#39537;&#21160;&#30340;&#25512;&#29702;&#22686;&#24378;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
VerifiNER: Verification-augmented NER via Knowledge-grounded Reasoning with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18374
&lt;/p&gt;
&lt;p&gt;
VerifiNER&#26159;&#19968;&#20010;&#21518;&#32493;&#39564;&#35777;&#26694;&#26550;&#65292;&#36890;&#36807;&#30693;&#35782;&#35782;&#21035;&#24182;&#20462;&#27491;&#29616;&#26377;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;&#30340;&#38169;&#35823;&#65292;&#20197;&#23454;&#29616;&#26356;&#24544;&#23454;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#29305;&#23450;&#39046;&#22495;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#20013;&#30340;&#26041;&#27861;&#65292;&#22914;&#29983;&#29289;&#21307;&#23398;NER&#65292;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20173;&#28982;&#32570;&#20047;&#24544;&#23454;&#24615;&#65292;&#20250;&#20135;&#29983;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#35748;&#20026;&#23454;&#20307;&#30340;&#30693;&#35782;&#21487;&#20197;&#22312;&#39564;&#35777;&#39044;&#27979;&#30340;&#27491;&#30830;&#24615;&#26041;&#38754;&#21457;&#25381;&#20316;&#29992;&#12290;&#23613;&#31649;&#30693;&#35782;&#30340;&#26377;&#29992;&#24615;&#65292;&#20351;&#29992;&#30693;&#35782;&#26469;&#32416;&#27491;&#36825;&#20123;&#38169;&#35823;&#24182;&#19981;&#23481;&#26131;&#65292;&#22240;&#20026;&#30693;&#35782;&#26412;&#36523;&#24182;&#19981;&#30452;&#25509;&#25351;&#31034;&#20986;&#30495;&#23454;&#26631;&#31614;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VerifiNER&#65292;&#19968;&#20010;&#21033;&#29992;&#30693;&#35782;&#20174;&#29616;&#26377;NER&#26041;&#27861;&#20013;&#35782;&#21035;&#38169;&#35823;&#24182;&#23558;&#20854;&#20462;&#27491;&#20026;&#26356;&#24544;&#23454;&#39044;&#27979;&#30340;&#21518;&#32493;&#39564;&#35777;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#39564;&#35777;&#36807;&#31243;&#20013;&#20805;&#20998;&#22522;&#20110;&#30693;&#35782;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#39564;&#35777;&#20102;VerifiNER&#30340;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;VerifiNER&#21487;&#20197;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18374v1 Announce Type: new  Abstract: Recent approaches in domain-specific named entity recognition (NER), such as biomedical NER, have shown remarkable advances. However, they still lack of faithfulness, producing erroneous predictions. We assume that knowledge of entities can be useful in verifying the correctness of the predictions. Despite the usefulness of knowledge, resolving such errors with knowledge is nontrivial, since the knowledge itself does not directly indicate the ground-truth label. To this end, we propose VerifiNER, a post-hoc verification framework that identifies errors from existing NER methods using knowledge and revises them into more faithful predictions. Our framework leverages the reasoning abilities of large language models to adequately ground on knowledge and the contextual information in the verification process. We validate effectiveness of VerifiNER through extensive experiments on biomedical datasets. The results suggest that VerifiNER can su
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24120;&#35782;&#25512;&#29702;&#20013;&#34920;&#29616;&#20986;&#39640;&#27700;&#24179;&#30340;&#33021;&#21147;&#65292;&#20294;&#30001;&#20110;&#20449;&#24687;&#20002;&#22833;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26032;&#26041;&#27861;RIDERS&#26469;&#35299;&#37322;&#21644;&#20943;&#36731;&#26377;&#23475;CoT&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.18344</link><description>&lt;p&gt;
&#19987;&#27880;&#20110;&#20320;&#30340;&#38382;&#39064;&#65281;&#35299;&#37322;&#21644;&#20943;&#36731;&#24120;&#35782;&#25512;&#29702;&#20013;&#30340;&#26377;&#23475;CoT&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Focus on Your Question! Interpreting and Mitigating Toxic CoT Problems in Commonsense Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18344
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24120;&#35782;&#25512;&#29702;&#20013;&#34920;&#29616;&#20986;&#39640;&#27700;&#24179;&#30340;&#33021;&#21147;&#65292;&#20294;&#30001;&#20110;&#20449;&#24687;&#20002;&#22833;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26032;&#26041;&#27861;RIDERS&#26469;&#35299;&#37322;&#21644;&#20943;&#36731;&#26377;&#23475;CoT&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#39640;&#27700;&#24179;&#30340;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#36890;&#36807;Chain-of-Thought&#65288;CoT&#65289;&#31561;&#22686;&#24378;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#31867;&#20284;CoT&#30340;&#26041;&#27861;&#23548;&#33268;&#20102;&#21407;&#26412;&#27491;&#30830;&#30340;&#31572;&#26696;&#21464;&#24471;&#38169;&#35823;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#20854;&#23450;&#20041;&#20026;&#26377;&#23475;&#30340;CoT&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#37322;&#21644;&#20943;&#36731;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#23646;&#24615;&#36319;&#36394;&#21644;&#22240;&#26524;&#36319;&#36394;&#26041;&#27861;&#26469;&#25506;&#31350;LLM&#22312;CoT&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#20869;&#37096;&#24037;&#20316;&#26426;&#21046;&#12290;&#36890;&#36807;&#27604;&#36739;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#27169;&#22411;&#22312;&#29983;&#25104;&#25512;&#29702;&#25110;&#31572;&#26696;&#26102;&#23384;&#22312;&#26469;&#33258;&#38382;&#39064;&#30340;&#20449;&#24687;&#20002;&#22833;&#29616;&#35937;&#22312;&#27973;&#23618;&#27880;&#24847;&#21147;&#23618;&#20013;&#12290;&#22522;&#20110;&#25506;&#31350;&#32467;&#26524;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;RIDERS&#65288;Residual decodIng and sERial-position Swap&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#20174;&#35299;&#30721;&#21644;&#24207;&#21015;&#20301;&#32622;&#30340;&#35282;&#24230;&#34917;&#20607;&#27169;&#22411;&#20013;&#30340;&#20449;&#24687;&#20111;&#32570;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#24120;&#35782;&#25512;&#29702;&#22522;&#20934;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18344v1 Announce Type: new  Abstract: Large language models exhibit high-level commonsense reasoning abilities, especially with enhancement methods like Chain-of-Thought (CoT). However, we find these CoT-like methods lead to a considerable number of originally correct answers turning wrong, which we define as the Toxic CoT problem. To interpret and mitigate this problem, we first utilize attribution tracing and causal tracing methods to probe the internal working mechanism of the LLM during CoT reasoning. Through comparisons, we prove that the model exhibits information loss from the question over the shallow attention layers when generating rationales or answers. Based on the probing findings, we design a novel method called RIDERS (Residual decodIng and sERial-position Swap), which compensates for the information deficit in the model from both decoding and serial-position perspectives. Through extensive experiments on multiple commonsense reasoning benchmarks, we validate 
&lt;/p&gt;</description></item><item><title>Bonito&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#25351;&#20196;&#35843;&#20248;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#26410;&#27880;&#37322;&#30340;&#25991;&#26412;&#36716;&#25442;&#20026;&#29305;&#23450;&#20219;&#21153;&#35757;&#32451;&#25968;&#25454;&#65292;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#29992;&#25143;&#19987;&#23646;&#25968;&#25454;&#30340;&#38646;shot&#20219;&#21153;&#36866;&#24212;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#21644;&#25351;&#20196;&#35843;&#25972;&#27169;&#22411;&#30340;&#24179;&#22343;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.18334</link><description>&lt;p&gt;
&#23398;&#20064;&#29983;&#25104;&#29992;&#20110;&#38646;shot&#20219;&#21153;&#36866;&#24212;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Learning to Generate Instruction Tuning Datasets for Zero-Shot Task Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18334
&lt;/p&gt;
&lt;p&gt;
Bonito&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#25351;&#20196;&#35843;&#20248;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#26410;&#27880;&#37322;&#30340;&#25991;&#26412;&#36716;&#25442;&#20026;&#29305;&#23450;&#20219;&#21153;&#35757;&#32451;&#25968;&#25454;&#65292;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#29992;&#25143;&#19987;&#23646;&#25968;&#25454;&#30340;&#38646;shot&#20219;&#21153;&#36866;&#24212;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#21644;&#25351;&#20196;&#35843;&#25972;&#27169;&#22411;&#30340;&#24179;&#22343;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Bonito&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#27169;&#22411;&#65292;&#29992;&#20110;&#26465;&#20214;&#20219;&#21153;&#29983;&#25104;&#65306;&#23558;&#26410;&#27880;&#37322;&#30340;&#25991;&#26412;&#36716;&#25442;&#20026;&#29992;&#20110;&#25351;&#20196;&#35843;&#20248;&#30340;&#29305;&#23450;&#20219;&#21153;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#29992;&#25143;&#19987;&#38376;&#30340;&#31169;&#20154;&#25968;&#25454;&#19978;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;shot&#20219;&#21153;&#36866;&#24212;&#12290;&#25105;&#20204;&#20351;&#29992;1.65M&#20010;&#31034;&#20363;&#30340;&#26032;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#35757;&#32451;Bonito&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#23558;&#29616;&#26377;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#37325;&#26032;&#28151;&#21512;&#25104;&#20803;&#27169;&#26495;&#32780;&#21019;&#24314;&#30340;&#12290;&#25968;&#25454;&#38598;&#30340;&#20803;&#27169;&#26495;&#20135;&#29983;&#35757;&#32451;&#31034;&#20363;&#65292;&#20854;&#20013;&#36755;&#20837;&#26159;&#26410;&#27880;&#37322;&#30340;&#25991;&#26412;&#21644;&#20219;&#21153;&#23646;&#24615;&#65292;&#36755;&#20986;&#21253;&#25324;&#25351;&#20196;&#21644;&#21709;&#24212;&#12290;&#25105;&#20204;&#20351;&#29992;Bonito&#20026;&#19971;&#20010;&#19987;&#19994;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#29983;&#25104;&#21512;&#25104;&#20219;&#21153;&#65292;&#36328;&#19977;&#31181;&#20219;&#21153;&#31867;&#22411; -- &#26159;&#38750;&#38382;&#31572;&#12289;&#25277;&#21462;&#24335;&#38382;&#31572;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702; -- &#24182;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Bonito&#26174;&#33879;&#25913;&#21892;&#20102;&#39044;&#35757;&#32451;&#21644;&#25351;&#20196;&#35843;&#25972;&#27169;&#22411;&#30340;&#24179;&#22343;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18334v1 Announce Type: new  Abstract: We introduce Bonito, an open-source model for conditional task generation: the task of converting unannotated text into task-specific training datasets for instruction tuning. Our goal is to enable zero-shot task adaptation of large language models on users' specialized, private data. We train Bonito on a new large-scale dataset with 1.65M examples created by remixing existing instruction tuning datasets into meta-templates. The meta-templates for a dataset produce training examples where the input is the unannotated text and the task attribute and the output consists of the instruction and the response. We use Bonito to generate synthetic tasks for seven datasets from specialized domains across three task types -- yes-no question answering, extractive question answering, and natural language inference -- and adapt language models. We show that Bonito significantly improves the average performance of pretrained and instruction tuned mode
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;LLMs&#36827;&#34892;&#26426;&#26800;&#24615;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#22312;&#36827;&#34892;&#36880;&#27493;&#25512;&#29702;&#26102;&#20351;&#29992;&#22810;&#20010;&#24182;&#34892;&#36335;&#24452;&#29983;&#25104;&#31572;&#26696;&#65292;&#21516;&#26102;&#23384;&#22312;&#21151;&#33021;&#24615;&#20998;&#27495;&#12290;</title><link>https://arxiv.org/abs/2402.18312</link><description>&lt;p&gt;
&#22914;&#20309;&#36880;&#27493;&#24605;&#32771;&#65306;&#23545;&#24605;&#32500;&#38142;&#25512;&#29702;&#30340;&#26426;&#26800;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18312
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;LLMs&#36827;&#34892;&#26426;&#26800;&#24615;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#22312;&#36827;&#34892;&#36880;&#27493;&#25512;&#29702;&#26102;&#20351;&#29992;&#22810;&#20010;&#24182;&#34892;&#36335;&#24452;&#29983;&#25104;&#31572;&#26696;&#65292;&#21516;&#26102;&#23384;&#22312;&#21151;&#33021;&#24615;&#20998;&#27495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#36890;&#36807;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#25552;&#31034;&#65292;&#20294;&#23545;&#20110;&#20419;&#36827;CoT&#29983;&#25104;&#30340;&#27169;&#22411;&#20869;&#37096;&#26426;&#21046;&#20173;&#23384;&#22312;&#32570;&#20047;&#29702;&#35299;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#20174;&#26426;&#26800;&#24615;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;LLMs&#20013;&#34920;&#29616;&#20986;CoT&#25512;&#29702;&#30340;&#31070;&#32463;&#23376;&#32467;&#26500;&#12290;&#36890;&#36807;&#23545;LLaMA-2 7B&#24212;&#29992;&#20110;&#34394;&#26500;&#26412;&#20307;&#35770;&#30340;&#22810;&#27493;&#25512;&#29702;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LLMs&#20026;&#36880;&#27493;&#25512;&#29702;&#37096;&#32626;&#20102;&#22810;&#20010;&#24182;&#34892;&#31572;&#26696;&#29983;&#25104;&#36335;&#24452;&#12290;&#36825;&#20123;&#24182;&#34892;&#36335;&#24452;&#25552;&#20379;&#20102;&#26469;&#33258;&#36755;&#20837;&#38382;&#39064;&#19978;&#19979;&#25991;&#20197;&#21450;&#29983;&#25104;&#30340;CoT&#30340;&#24207;&#36143;&#31572;&#26696;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;LLMs&#20013;&#38388;&#23618;&#23384;&#22312;&#24341;&#20154;&#30633;&#30446;&#30340;&#21151;&#33021;&#20998;&#27495;&#12290;&#21021;&#22987;&#19968;&#21322;&#30340;&#20196;&#29260;&#34920;&#31034;&#20173;&#28982;&#24378;&#28872;&#20559;&#21521;&#39044;&#35757;&#32451;&#20808;&#39564;&#65292;&#32780;&#21518;&#21322;&#37096;&#20998;&#31361;&#28982;&#34987;&#19978;&#19979;&#25991;&#25152;&#21462;&#20195;&#12290;&#36825;&#31181;&#20869;&#37096;&#30456;&#20301;&#36716;&#21464;&#22312;&#19981;&#21516;&#30340;&#21151;&#33021;&#21327;&#21516;&#20013;&#20307;&#29616;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18312v1 Announce Type: new  Abstract: Despite superior reasoning prowess demonstrated by Large Language Models (LLMs) with Chain-of-Thought (CoT) prompting, a lack of understanding prevails around the internal mechanisms of the models that facilitate CoT generation. This work investigates the neural sub-structures within LLMs that manifest CoT reasoning from a mechanistic point of view. From an analysis of LLaMA-2 7B applied to multistep reasoning over fictional ontologies, we demonstrate that LLMs deploy multiple parallel pathways of answer generation for step-by-step reasoning. These parallel pathways provide sequential answers from the input question context as well as the generated CoT. We observe a striking functional rift in the middle layers of the LLM. Token representations in the initial half remain strongly biased towards the pretraining prior, with the in-context taking over abruptly in the later half. This internal phase shift manifests in different functional co
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#25991;&#26412;&#25490;&#24207;&#26041;&#27861;&#65292;&#21033;&#29992;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#28040;&#38500;&#20102;&#23545;&#20154;&#24037;&#27880;&#37322;&#21592;&#30340;&#38656;&#27714;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#21508;&#39033;&#24471;&#20998;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#22522;&#32447;</title><link>https://arxiv.org/abs/2402.18284</link><description>&lt;p&gt;
&#20247;&#21253;&#26159;&#21542;&#35753;&#24744;&#30772;&#20135;&#20102;&#65311;&#20351;&#29992;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25104;&#26412;&#25928;&#30410;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Is Crowdsourcing Breaking Your Bank? Cost-Effective Fine-Tuning of Pre-trained Language Models with Proximal Policy Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18284
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#25991;&#26412;&#25490;&#24207;&#26041;&#27861;&#65292;&#21033;&#29992;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#28040;&#38500;&#20102;&#23545;&#20154;&#24037;&#27880;&#37322;&#21592;&#30340;&#38656;&#27714;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#21508;&#39033;&#24471;&#20998;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#30340;&#24191;&#27867;&#20351;&#29992;&#20984;&#26174;&#20102;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#20854;&#35757;&#32451;&#27969;&#31243;&#20381;&#36182;&#20110;&#20154;&#24037;&#25490;&#24207;&#65292;&#36825;&#26159;&#19968;&#20010;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#36807;&#31243;&#12290;&#20026;&#20102;&#38477;&#20302;&#21171;&#21160;&#25104;&#26412;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#25991;&#26412;&#25490;&#24207;&#26041;&#27861;&#65292;&#29992;&#20110;&#24212;&#29992;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#26469;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#21516;&#26102;&#28040;&#38500;&#20102;&#23545;&#20154;&#24037;&#27880;&#37322;&#21592;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#27010;&#29575;&#25277;&#26679;&#24320;&#22987;&#65292;&#40723;&#21169;&#35821;&#35328;&#27169;&#22411;&#20026;&#27599;&#20010;&#36755;&#20837;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#21709;&#24212;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;TextRank&#21644;ISODATA&#31639;&#27861;&#65292;&#22522;&#20110;&#35821;&#20041;&#23545;&#36825;&#20123;&#21709;&#24212;&#36827;&#34892;&#25490;&#24207;&#21644;&#32858;&#31867;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22870;&#21169;&#27169;&#22411;&#26469;&#23398;&#20064;&#25490;&#21517;&#24182;&#20248;&#21270;&#25105;&#20204;&#30340;&#29983;&#25104;&#31574;&#30053;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#20219;&#21153;&#19978;&#20351;&#29992;&#20004;&#20010;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;BLEU&#12289;GLEU&#21644;METEOR&#24471;&#20998;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#22522;&#32447;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#25163;&#21160;&#35780;&#20272;&#26174;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18284v1 Announce Type: cross  Abstract: Wide usage of ChatGPT has highlighted the potential of reinforcement learning from human feedback. However, its training pipeline relies on manual ranking, a resource-intensive process. To reduce labor costs, we propose a self-supervised text ranking approach for applying Proximal-Policy-Optimization to fine-tune language models while eliminating the need for human annotators. Our method begins with probabilistic sampling to encourage a language model to generate diverse responses for each input. We then employ TextRank and ISODATA algorithms to rank and cluster these responses based on their semantics. Subsequently, we construct a reward model to learn the rank and optimize our generative policy. Our experimental results, conducted using two language models on three tasks, demonstrate that the models trained by our method considerably outperform baselines regarding BLEU, GLEU, and METEOR scores. Furthermore, our manual evaluation show
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#26799;&#24230;&#65292;&#23558;&#22235;&#31181;&#26377;&#25928;&#30340;&#23545;&#27604;&#25439;&#22833;&#38598;&#25104;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#33539;&#24335;&#20013;&#65292;&#20197;&#25506;&#31350;&#23545;&#27604;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#20013;&#21508;&#31181;&#23545;&#27604;&#25439;&#22833;&#36798;&#21040;&#21331;&#36234;&#24615;&#33021;&#30340;&#20849;&#21516;&#29305;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.18281</link><description>&lt;p&gt;
&#26356;&#22909;&#29702;&#35299;&#23545;&#27604;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#65306;&#26799;&#24230;&#30340;&#32479;&#19968;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
Towards Better Understanding of Contrastive Sentence Representation Learning: A Unified Paradigm for Gradient
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18281
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#26799;&#24230;&#65292;&#23558;&#22235;&#31181;&#26377;&#25928;&#30340;&#23545;&#27604;&#25439;&#22833;&#38598;&#25104;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#33539;&#24335;&#20013;&#65292;&#20197;&#25506;&#31350;&#23545;&#27604;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#20013;&#21508;&#31181;&#23545;&#27604;&#25439;&#22833;&#36798;&#21040;&#21331;&#36234;&#24615;&#33021;&#30340;&#20849;&#21516;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#65288;SRL&#65289;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#23545;&#29031;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26159;&#30446;&#21069;&#20027;&#27969;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20854;&#26174;&#33879;&#26377;&#25928;&#24615;&#32972;&#21518;&#30340;&#21407;&#22240;&#20173;&#19981;&#28165;&#26970;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#20854;&#20182;&#30740;&#31350;&#39046;&#22495;&#20013;&#65292;&#23545;&#27604;SSL&#22312;&#29702;&#35770;&#21644;&#23454;&#38469;&#34920;&#29616;&#19978;&#19982;&#38750;&#23545;&#27604;SSL&#65288;&#20363;&#22914;&#65292;&#23545;&#40784;&#21644;&#19968;&#33268;&#24615;&#12289;Barlow Twins&#21644;VICReg&#65289;&#26377;&#30456;&#20284;&#20043;&#22788;&#12290;&#28982;&#32780;&#65292;&#22312;SRL&#20013;&#65292;&#23545;&#27604;SSL&#26126;&#26174;&#20248;&#20110;&#38750;&#23545;&#27604;SSL&#12290;&#22240;&#27492;&#65292;&#20986;&#29616;&#20102;&#20004;&#20010;&#38382;&#39064;&#65306;&#39318;&#20808;&#65292;&#26159;&#20160;&#20040;&#20849;&#21516;&#28857;&#20351;&#21508;&#31181;&#23545;&#27604;&#25439;&#22833;&#22312;SRL&#20013;&#21462;&#24471;&#20102;&#20248;&#36234;&#24615;&#33021;&#65311;&#20854;&#27425;&#65292;&#25105;&#20204;&#22914;&#20309;&#20351;&#38750;&#23545;&#27604;SSL&#65288;&#19982;&#23545;&#27604;SSL&#30456;&#20284;&#20294;&#22312;SRL&#20013;&#26080;&#25928;&#65289;&#21464;&#24471;&#26377;&#25928;&#65311;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#26799;&#24230;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#21457;&#29616;&#22235;&#31181;&#26377;&#25928;&#30340;&#23545;&#27604;&#25439;&#22833;&#21487;&#20197;&#38598;&#25104;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#33539;&#24335;&#20013;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18281v1 Announce Type: new  Abstract: Sentence Representation Learning (SRL) is a crucial task in Natural Language Processing (NLP), where contrastive Self-Supervised Learning (SSL) is currently a mainstream approach. However, the reasons behind its remarkable effectiveness remain unclear. Specifically, in other research fields, contrastive SSL shares similarities in both theory and practical performance with non-contrastive SSL (e.g., alignment &amp; uniformity, Barlow Twins, and VICReg). However, in SRL, contrastive SSL outperforms non-contrastive SSL significantly. Therefore, two questions arise: First, what commonalities enable various contrastive losses to achieve superior performance in SRL? Second, how can we make non-contrastive SSL, which is similar to contrastive SSL but ineffective in SRL, effective? To address these questions, we start from the perspective of gradients and discover that four effective contrastive losses can be integrated into a unified paradigm, whic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36866;&#37197;&#22120;&#29992;&#20110;&#22122;&#22768;&#40065;&#26834;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#25506;&#32034;&#65292;&#21457;&#29616;&#23558;&#36866;&#37197;&#22120;&#25554;&#20837;&#27973;&#23618;&#21487;&#20197;&#33719;&#24471;&#26356;&#26174;&#33879;&#30340;&#25928;&#26524;&#65292;&#30495;&#23454;&#25968;&#25454;&#27604;&#27169;&#25311;&#25968;&#25454;&#26356;&#26377;&#25928;&#65292;&#24182;&#19988;&#23558;&#36866;&#37197;&#22120;&#38598;&#25104;&#21040;&#22522;&#20110;&#35821;&#38899;&#22686;&#24378;&#30340;ASR&#31995;&#32479;&#20013;&#21487;&#20197;&#24102;&#26469;&#23454;&#36136;&#24615;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.18275</link><description>&lt;p&gt;
&#25506;&#32034;&#36866;&#37197;&#22120;&#29992;&#20110;&#22122;&#22768;&#40065;&#26834;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Exploration of Adapter for Noise Robust Automatic Speech Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36866;&#37197;&#22120;&#29992;&#20110;&#22122;&#22768;&#40065;&#26834;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#25506;&#32034;&#65292;&#21457;&#29616;&#23558;&#36866;&#37197;&#22120;&#25554;&#20837;&#27973;&#23618;&#21487;&#20197;&#33719;&#24471;&#26356;&#26174;&#33879;&#30340;&#25928;&#26524;&#65292;&#30495;&#23454;&#25968;&#25454;&#27604;&#27169;&#25311;&#25968;&#25454;&#26356;&#26377;&#25928;&#65292;&#24182;&#19988;&#23558;&#36866;&#37197;&#22120;&#38598;&#25104;&#21040;&#22522;&#20110;&#35821;&#38899;&#22686;&#24378;&#30340;ASR&#31995;&#32479;&#20013;&#21487;&#20197;&#24102;&#26469;&#23454;&#36136;&#24615;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36866;&#24212;&#40065;&#26834;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#20197;&#35299;&#20915;&#26410;&#30693;&#22122;&#22768;&#22330;&#26223;&#33267;&#20851;&#37325;&#35201;&#12290;&#23558;&#36866;&#37197;&#22120;&#38598;&#25104;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#12290;&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#22522;&#20110;&#36866;&#37197;&#22120;&#30340;&#22122;&#22768;&#40065;&#26834;ASR&#36866;&#24212;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;CHiME--4&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#27973;&#23618;&#25554;&#20837;&#36866;&#37197;&#22120;&#33021;&#22815;&#20135;&#29983;&#26356;&#26174;&#33879;&#30340;&#25928;&#26524;&#65292;&#22312;&#20165;&#22312;&#27973;&#23618;&#20869;&#37096;&#36827;&#34892;&#36866;&#24212;&#21644;&#22312;&#25152;&#26377;&#23618;&#20043;&#38388;&#36827;&#34892;&#36866;&#24212;&#20043;&#38388;&#27809;&#26377;&#26174;&#33879;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#27169;&#25311;&#25968;&#25454;&#26377;&#21161;&#20110;&#31995;&#32479;&#25913;&#21892;&#20854;&#22312;&#23454;&#38469;&#22122;&#22768;&#26465;&#20214;&#19979;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#22312;&#25968;&#25454;&#37327;&#30456;&#21516;&#26102;&#65292;&#30495;&#23454;&#25968;&#25454;&#27604;&#27169;&#25311;&#25968;&#25454;&#26356;&#26377;&#25928;&#12290;&#22312;&#36866;&#37197;&#22120;&#35757;&#32451;&#20013;&#65292;&#22810;&#26465;&#20214;&#35757;&#32451;&#20173;&#28982;&#26377;&#25928;&#12290;&#27492;&#22806;&#65292;&#23558;&#36866;&#37197;&#22120;&#38598;&#25104;&#21040;&#22522;&#20110;&#35821;&#38899;&#22686;&#24378;&#30340;ASR&#31995;&#32479;&#20013;&#20250;&#24102;&#26469;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18275v1 Announce Type: cross  Abstract: Adapting a robust automatic speech recognition (ASR) system to tackle unseen noise scenarios is crucial. Integrating adapters into neural networks has emerged as a potent technique for transfer learning. This paper thoroughly investigates adapter-based noise-robust ASR adaptation. We conducted the experiments using the CHiME--4 dataset. The results show that inserting the adapter in the shallow layer yields superior effectiveness, and there is no significant difference between adapting solely within the shallow layer and adapting across all layers. Besides, the simulated data helps the system to improve its performance under real noise conditions. Nonetheless, when the amount of data is the same, the real data is more effective than the simulated data. Multi-condition training remains valid for adapter training. Furthermore, integrating adapters into speech enhancement-based ASR systems yields substantial improvements.
&lt;/p&gt;</description></item><item><title>&#22810;&#26234;&#33021;&#20307;&#35752;&#35770;&#25552;&#21319;&#20102;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#22312;&#26377;&#28436;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#21333;&#26234;&#33021;&#20307;LLM&#36890;&#36807;&#24378;&#25552;&#31034;&#20960;&#20046;&#33021;&#36798;&#21040;&#19982;&#26368;&#20339;&#35752;&#35770;&#26041;&#27861;&#30456;&#21516;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.18272</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;LLM&#25512;&#29702;&#30340;&#30028;&#38480;&#65306;&#22810;&#26234;&#33021;&#20307;&#35752;&#35770;&#26159;&#20851;&#38190;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18272
&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#35752;&#35770;&#25552;&#21319;&#20102;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#22312;&#26377;&#28436;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#21333;&#26234;&#33021;&#20307;LLM&#36890;&#36807;&#24378;&#25552;&#31034;&#20960;&#20046;&#33021;&#36798;&#21040;&#19982;&#26368;&#20339;&#35752;&#35770;&#26041;&#27861;&#30456;&#21516;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#35752;&#35770;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#34920;&#26126;&#65292;&#22810;&#26234;&#33021;&#20307;&#35752;&#35770;&#25552;&#21319;&#20102;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#31995;&#32479;&#23454;&#39564;&#23545;&#36825;&#19968;&#35828;&#27861;&#36827;&#34892;&#37325;&#26032;&#35780;&#20272;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23567;&#32452;&#35752;&#35770;&#26694;&#26550;&#65292;&#20016;&#23500;&#20102;&#35752;&#35770;&#26426;&#21046;&#38598;&#21512;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#19968;&#20010;&#24102;&#26377;&#24378;&#25552;&#31034;&#30340;&#21333;&#26234;&#33021;&#20307;LLM&#22312;&#24191;&#27867;&#30340;&#25512;&#29702;&#20219;&#21153;&#21644;&#22522;&#26412;LLM&#20013;&#20960;&#20046;&#21487;&#20197;&#36798;&#21040;&#19982;&#26368;&#20339;&#29616;&#26377;&#35752;&#35770;&#26041;&#27861;&#30456;&#21516;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22810;&#26234;&#33021;&#20307;&#35752;&#35770;&#20165;&#22312;&#25552;&#31034;&#20013;&#27809;&#26377;&#28436;&#31034;&#26102;&#25165;&#20248;&#20110;&#21333;&#20010;&#26234;&#33021;&#20307;&#12290;&#36827;&#19968;&#27493;&#30740;&#31350;&#25581;&#31034;&#20102;LLM&#22312;&#35752;&#35770;&#36807;&#31243;&#20013;&#30340;&#24120;&#35265;&#20132;&#20114;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18272v1 Announce Type: cross  Abstract: Recent progress in LLMs discussion suggests that multi-agent discussion improves the reasoning abilities of LLMs. In this work, we reevaluate this claim through systematic experiments, where we propose a novel group discussion framework to enrich the set of discussion mechanisms. Interestingly, our results show that a single-agent LLM with strong prompts can achieve almost the same performance as the best existing discussion approach on a wide range of reasoning tasks and backbone LLMs. We observe that the multi-agent discussion performs better than a single agent only when there is no demonstration in the prompt. Further study reveals the common interaction mechanisms of LLMs during the discussion.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#31995;&#32479;&#30740;&#31350;&#20102;&#31070;&#32463;&#38382;&#31572;&#29983;&#25104;&#65288;NQG&#65289;&#39046;&#22495;&#30340;&#36827;&#23637;&#65292;&#21253;&#25324;&#20102;&#32972;&#26223;&#27010;&#36848;&#12289;&#19981;&#21516;&#31867;&#21035;&#30340;&#26041;&#27861;&#12289;&#20197;&#21450;&#26410;&#26469;&#23637;&#26395;</title><link>https://arxiv.org/abs/2402.18267</link><description>&lt;p&gt;
&#20851;&#20110;&#31070;&#32463;&#38382;&#31572;&#29983;&#25104;&#30340;&#35843;&#26597;&#65306;&#26041;&#27861;&#12289;&#24212;&#29992;&#21644;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
A Survey on Neural Question Generation: Methods, Applications, and Prospects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18267
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#31995;&#32479;&#30740;&#31350;&#20102;&#31070;&#32463;&#38382;&#31572;&#29983;&#25104;&#65288;NQG&#65289;&#39046;&#22495;&#30340;&#36827;&#23637;&#65292;&#21253;&#25324;&#20102;&#32972;&#26223;&#27010;&#36848;&#12289;&#19981;&#21516;&#31867;&#21035;&#30340;&#26041;&#27861;&#12289;&#20197;&#21450;&#26410;&#26469;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#23545;&#31070;&#32463;&#38382;&#31572;&#29983;&#25104;&#65288;NQG&#65289;&#39046;&#22495;&#30340;&#36827;&#23637;&#36827;&#34892;&#20102;&#35814;&#32454;&#26816;&#26597;&#65292;&#36825;&#19968;&#39046;&#22495;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#20174;&#21508;&#31181;&#26469;&#28304;&#65292;&#22914;&#30693;&#35782;&#24211;&#12289;&#25991;&#26412;&#21644;&#22270;&#20687;&#20013;&#29983;&#25104;&#30456;&#20851;&#38382;&#39064;&#12290;&#35843;&#26597;&#20174;NQG&#32972;&#26223;&#27010;&#36848;&#24320;&#22987;&#65292;&#21253;&#25324;&#20219;&#21153;&#30340;&#38382;&#39064;&#21046;&#23450;&#12289;&#27969;&#34892;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12289;&#24050;&#24314;&#31435;&#30340;&#35780;&#20272;&#25351;&#26631;&#21644;&#26174;&#33879;&#24212;&#29992;&#12290;&#28982;&#21518;&#65292;&#31995;&#32479;&#22320;&#23558;NQG&#26041;&#27861;&#20998;&#20026;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#65306;&#32467;&#26500;&#21270;NQG&#65292;&#21033;&#29992;&#26377;&#32452;&#32455;&#30340;&#25968;&#25454;&#28304;&#65292;&#38750;&#32467;&#26500;&#21270;NQG&#65292;&#19987;&#27880;&#20110;&#26356;&#26494;&#25955;&#32467;&#26500;&#30340;&#36755;&#20837;&#65292;&#22914;&#25991;&#26412;&#25110;&#35270;&#35273;&#20869;&#23481;&#65292;&#20197;&#21450;&#28151;&#21512;NQG&#65292;&#21033;&#29992;&#22810;&#26679;&#30340;&#36755;&#20837;&#27169;&#24335;&#12290;&#36825;&#19968;&#20998;&#31867;&#21518;&#26159;&#23545;&#20026;&#27599;&#20010;&#31867;&#21035;&#37327;&#36523;&#23450;&#21046;&#30340;&#19981;&#21516;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#35752;&#35770;&#23427;&#20204;&#22266;&#26377;&#30340;&#20248;&#21183;&#21644;&#28508;&#22312;&#23616;&#38480;&#24615;&#12290;&#35843;&#26597;&#20197;&#23637;&#26395;&#26410;&#26469;&#32467;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18267v1 Announce Type: cross  Abstract: In this survey, we present a detailed examination of the advancements in Neural Question Generation (NQG), a field leveraging neural network techniques to generate relevant questions from diverse inputs like knowledge bases, texts, and images. The survey begins with an overview of NQG's background, encompassing the task's problem formulation, prevalent benchmark datasets, established evaluation metrics, and notable applications. It then methodically classifies NQG approaches into three predominant categories: structured NQG, which utilizes organized data sources, unstructured NQG, focusing on more loosely structured inputs like texts or visual content, and hybrid NQG, drawing on diverse input modalities. This classification is followed by an in-depth analysis of the distinct neural network models tailored for each category, discussing their inherent strengths and potential limitations. The survey culminates with a forward-looking persp
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26816;&#32034;&#33719;&#21462;&#30340;Web&#26469;&#28304;&#20449;&#24687;&#65292;&#20026;&#26032;&#20852;&#20107;&#20214;&#29983;&#25104;&#32467;&#26500;&#21270;&#30340;&#20840;&#38271;&#32500;&#22522;&#30334;&#31185;&#25991;&#26723;&#65292;&#36991;&#20813;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19982;&#26368;&#36817;&#21457;&#29983;&#20107;&#20214;&#30456;&#20851;&#30340;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2402.18264</link><description>&lt;p&gt;
&#22522;&#20110;&#26816;&#32034;&#30340;&#24212;&#24613;&#20107;&#20214;&#20840;&#38271;&#32500;&#22522;&#30334;&#31185;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Retrieval-based Full-length Wikipedia Generation for Emergent Events
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18264
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26816;&#32034;&#33719;&#21462;&#30340;Web&#26469;&#28304;&#20449;&#24687;&#65292;&#20026;&#26032;&#20852;&#20107;&#20214;&#29983;&#25104;&#32467;&#26500;&#21270;&#30340;&#20840;&#38271;&#32500;&#22522;&#30334;&#31185;&#25991;&#26723;&#65292;&#36991;&#20813;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19982;&#26368;&#36817;&#21457;&#29983;&#20107;&#20214;&#30456;&#20851;&#30340;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#24555;&#33410;&#22863;&#30340;&#19990;&#30028;&#20013;&#65292;&#36805;&#36895;&#29983;&#25104;&#26032;&#20852;&#20107;&#20214;&#20840;&#38754;&#20934;&#30830;&#30340;&#32500;&#22522;&#30334;&#31185;&#25991;&#26723;&#30340;&#38656;&#27714;&#26085;&#30410;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#32500;&#22522;&#30334;&#31185;&#29983;&#25104;&#24037;&#20316;&#24448;&#24448;&#26410;&#33021;&#28385;&#36275;&#29616;&#23454;&#38656;&#27714;&#12290;&#19968;&#20123;&#26041;&#27861;&#20165;&#19987;&#27880;&#20110;&#29983;&#25104;&#23436;&#25972;&#32500;&#22522;&#30334;&#31185;&#25991;&#26723;&#30340;&#37096;&#20998;&#20869;&#23481;&#65292;&#32780;&#21478;&#19968;&#20123;&#21017;&#24573;&#35270;&#20102;&#29983;&#25104;&#36807;&#31243;&#20013;&#24544;&#23454;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#25110;&#26410;&#32771;&#34385;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27169;&#25311;&#20102;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#65292;&#20351;&#29992;&#20174;&#32593;&#39029;&#26469;&#28304;&#26816;&#32034;&#30340;&#20869;&#23481;&#20026;&#26032;&#20852;&#20107;&#20214;&#29983;&#25104;&#32467;&#26500;&#21270;&#30340;&#20840;&#38271;&#32500;&#22522;&#30334;&#31185;&#25991;&#26723;&#12290;&#20026;&#30830;&#20445;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26410;&#32463;&#36807;&#22522;&#20110;&#26368;&#36817;&#21457;&#29983;&#20107;&#20214;&#30340;&#35821;&#26009;&#24211;&#35757;&#32451;&#65292;&#25105;&#20204;&#36873;&#25321;&#26368;&#36817;&#21457;&#29983;&#30340;&#20107;&#20214;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934; Wiki-GenBen&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;309&#20010;&#20107;&#20214;&#21450;&#20854;&#23545;&#24212;&#30340;&#26816;&#32034;&#21040;&#30340;&#32593;&#39029;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18264v1 Announce Type: new  Abstract: In today's fast-paced world, the growing demand to quickly generate comprehensive and accurate Wikipedia documents for emerging events is both crucial and challenging. However, previous efforts in Wikipedia generation have often fallen short of meeting real-world requirements. Some approaches focus solely on generating segments of a complete Wikipedia document, while others overlook the importance of faithfulness in generation or fail to consider the influence of the pre-training corpus. In this paper, we simulate a real-world scenario where structured full-length Wikipedia documents are generated for emergent events using input retrieved from web sources. To ensure that Large Language Models (LLMs) are not trained on corpora related to recently occurred events, we select events that have taken place recently and introduce a new benchmark Wiki-GenBen, which consists of 309 events paired with their corresponding retrieved web pages for ge
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#29992;&#20110;&#35270;&#35273;&#20016;&#23500;&#32593;&#39029;&#29702;&#35299;&#30340;&#20998;&#23618;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#32467;&#21512;&#25991;&#26412;&#12289;&#32467;&#26500;&#21644;&#22270;&#20687;&#27169;&#24577;&#30340;&#20132;&#20114;&#26469;&#22686;&#24378;&#23545;&#32593;&#39029;&#30340;&#29702;&#35299;</title><link>https://arxiv.org/abs/2402.18262</link><description>&lt;p&gt;
&#29992;&#20110;&#35270;&#35273;&#20016;&#23500;&#32593;&#39029;&#29702;&#35299;&#30340;&#20998;&#23618;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Multimodal Pre-training for Visually Rich Webpage Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18262
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#29992;&#20110;&#35270;&#35273;&#20016;&#23500;&#32593;&#39029;&#29702;&#35299;&#30340;&#20998;&#23618;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#32467;&#21512;&#25991;&#26412;&#12289;&#32467;&#26500;&#21644;&#22270;&#20687;&#27169;&#24577;&#30340;&#20132;&#20114;&#26469;&#22686;&#24378;&#23545;&#32593;&#39029;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#20016;&#23500;&#35270;&#35273;&#20869;&#23481;&#30340;&#25991;&#26723;&#65288;&#22914;&#32593;&#39029;&#21644;&#25195;&#25551;/&#25968;&#23383;&#21270;&#25991;&#26723;&#65288;&#22270;&#20687;&#65292;PDF&#31561;&#65289;&#65289;&#30340;&#26222;&#21450;&#20351;&#24471;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#23545;&#33258;&#21160;&#25991;&#26723;&#29702;&#35299;&#21644;&#20449;&#24687;&#25552;&#21462;&#20135;&#29983;&#20102;&#26356;&#22823;&#20852;&#36259;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102; WebLM&#65292;&#19968;&#20010;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#32593;&#32476;&#65292;&#26088;&#22312;&#35299;&#20915;&#20165;&#23545;&#32593;&#39029;&#20013;&#30340;&#25991;&#26412;&#21644;&#32467;&#26500;&#27169;&#24577;&#36827;&#34892;&#24314;&#27169;&#30340;&#23616;&#38480;&#24615;&#12290;WebLM&#23558;&#25991;&#26723;&#22270;&#20687;&#30340;&#20998;&#23618;&#32467;&#26500;&#38598;&#25104;&#21040;&#27169;&#24577;&#20013;&#65292;&#20197;&#22686;&#24378;&#23545;&#22522;&#20110;&#26631;&#35760;&#35821;&#35328;&#30340;&#25991;&#26723;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18262v1 Announce Type: new  Abstract: The growing prevalence of visually rich documents, such as webpages and scanned/digital-born documents (images, PDFs, etc.), has led to increased interest in automatic document understanding and information extraction across academia and industry. Although various document modalities, including image, text, layout, and structure, facilitate human information retrieval, the interconnected nature of these modalities presents challenges for neural networks. In this paper, we introduce WebLM, a multimodal pre-training network designed to address the limitations of solely modeling text and structure modalities of HTML in webpages. Instead of processing document images as unified natural images, WebLM integrates the hierarchical structure of document images to enhance the understanding of markup-language-based documents. Additionally, we propose several pre-training tasks to model the interaction among text, structure, and image modalities eff
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#23618;&#27425;&#35821;&#20041;&#26694;&#26550;&#30340;&#22810;&#24847;&#22270;&#21475;&#35821;&#29702;&#35299;BiRGAT&#27169;&#22411;&#65292;&#22312;&#23454;&#39564;&#20013;&#36890;&#36807;&#22810;&#24847;&#22270;&#25968;&#25454;&#38598;MIVS&#65292;&#37319;&#29992;3&#23618;&#23618;&#27425;&#32467;&#26500;&#26469;&#35299;&#20915;&#22810;&#24847;&#22270;&#24773;&#20917;&#19979;&#30340;&#23545;&#40784;&#21644;&#20998;&#37197;&#38382;&#39064;&#65292;&#24182;&#32467;&#21512;3&#36335;&#25351;&#38024;-&#29983;&#25104;&#22120;&#35299;&#30721;&#22120;&#65292;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.18258</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#23618;&#27425;&#35821;&#20041;&#26694;&#26550;&#30340;&#22810;&#24847;&#22270;&#21475;&#35821;&#29702;&#35299;BiRGAT&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A BiRGAT Model for Multi-intent Spoken Language Understanding with Hierarchical Semantic Frames
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18258
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#23618;&#27425;&#35821;&#20041;&#26694;&#26550;&#30340;&#22810;&#24847;&#22270;&#21475;&#35821;&#29702;&#35299;BiRGAT&#27169;&#22411;&#65292;&#22312;&#23454;&#39564;&#20013;&#36890;&#36807;&#22810;&#24847;&#22270;&#25968;&#25454;&#38598;MIVS&#65292;&#37319;&#29992;3&#23618;&#23618;&#27425;&#32467;&#26500;&#26469;&#35299;&#20915;&#22810;&#24847;&#22270;&#24773;&#20917;&#19979;&#30340;&#23545;&#40784;&#21644;&#20998;&#37197;&#38382;&#39064;&#65292;&#24182;&#32467;&#21512;3&#36335;&#25351;&#38024;-&#29983;&#25104;&#22120;&#35299;&#30721;&#22120;&#65292;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#23545;&#21475;&#35821;&#29702;&#35299;&#65288;SLU&#65289;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#24847;&#22270;&#35774;&#32622;&#19978;&#65292;&#21363;&#27599;&#20010;&#36755;&#20837;&#35805;&#35821;&#20165;&#21253;&#21547;&#19968;&#20010;&#29992;&#25143;&#24847;&#22270;&#12290;&#36825;&#31181;&#37197;&#32622;&#26497;&#22823;&#22320;&#38480;&#21046;&#20102;&#29992;&#25143;&#35805;&#35821;&#30340;&#34920;&#38754;&#24418;&#24335;&#21644;&#36755;&#20986;&#35821;&#20041;&#30340;&#23481;&#37327;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#23454;&#38469;&#36710;&#36733;&#23545;&#35805;&#31995;&#32479;MIVS&#20013;&#25910;&#38598;&#30340;&#22810;&#24847;&#22270;&#25968;&#25454;&#38598;&#12290;&#30446;&#26631;&#35821;&#20041;&#26694;&#26550;&#32452;&#32455;&#25104;3&#23618;&#23618;&#27425;&#32467;&#26500;&#65292;&#20197;&#24212;&#23545;&#22810;&#24847;&#22270;&#24773;&#20917;&#19979;&#30340;&#23545;&#40784;&#21644;&#20998;&#37197;&#38382;&#39064;&#12290;&#30456;&#24212;&#22320;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;BiRGAT&#27169;&#22411;&#26469;&#32534;&#30721;&#26412;&#20307;&#39033;&#30446;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#20854;&#20027;&#24178;&#26159;&#21452;&#37325;&#20851;&#31995;&#22270;&#27880;&#24847;&#32593;&#32476;&#12290;&#32467;&#21512;3&#36335;&#25351;&#38024;-&#29983;&#25104;&#22120;&#35299;&#30721;&#22120;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#22823;&#22823;&#20248;&#20110;&#20256;&#32479;&#30340;&#24207;&#21015;&#26631;&#27880;&#21644;&#22522;&#20110;&#20998;&#31867;&#30340;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18258v1 Announce Type: new  Abstract: Previous work on spoken language understanding (SLU) mainly focuses on single-intent settings, where each input utterance merely contains one user intent. This configuration significantly limits the surface form of user utterances and the capacity of output semantics. In this work, we first propose a Multi-Intent dataset which is collected from a realistic in-Vehicle dialogue System, called MIVS. The target semantic frame is organized in a 3-layer hierarchical structure to tackle the alignment and assignment problems in multi-intent cases. Accordingly, we devise a BiRGAT model to encode the hierarchy of ontology items, the backbone of which is a dual relational graph attention network. Coupled with the 3-way pointer-generator decoder, our method outperforms traditional sequence labeling and classification-based schemes by a large margin.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#36890;&#29992;&#25552;&#31034;&#27010;&#24565;&#21644;&#21019;&#26032;&#30340;MeMo&#65288;&#24515;&#26234;&#27169;&#22411;&#65289;&#26041;&#27861;&#65292;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24191;&#27867;&#20219;&#21153;&#33539;&#22260;&#20869;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#65292;&#28040;&#38500;&#20102;&#25163;&#21160;&#36873;&#25321;&#21644;&#23450;&#21046;&#25552;&#31034;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2402.18252</link><description>&lt;p&gt;
&#36890;&#36807;&#24515;&#26234;&#27169;&#22411;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#25552;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Generalist Prompting for Large Language Models by Mental Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18252
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#36890;&#29992;&#25552;&#31034;&#27010;&#24565;&#21644;&#21019;&#26032;&#30340;MeMo&#65288;&#24515;&#26234;&#27169;&#22411;&#65289;&#26041;&#27861;&#65292;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24191;&#27867;&#20219;&#21153;&#33539;&#22260;&#20869;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#65292;&#28040;&#38500;&#20102;&#25163;&#21160;&#36873;&#25321;&#21644;&#23450;&#21046;&#25552;&#31034;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#65292;&#20173;&#28982;&#38656;&#35201;&#29305;&#21035;&#35774;&#35745;&#30340;&#25552;&#31034;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#20381;&#36182;&#20110;&#38656;&#35201;&#19968;&#23450;&#39046;&#22495;&#30693;&#35782;&#30340;&#29305;&#23450;&#20219;&#21153;&#23569;&#37327;&#31034;&#20363;&#65292;&#35201;&#20040;&#34987;&#35774;&#35745;&#20026;&#31616;&#21333;&#65292;&#20294;&#21482;&#23545;&#23569;&#25968;&#31867;&#22411;&#20219;&#21153;&#34920;&#29616;&#33391;&#22909;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#24341;&#20837;&#36890;&#29992;&#25552;&#31034;&#30340;&#27010;&#24565;&#65292;&#23427;&#30340;&#35774;&#35745;&#21407;&#21017;&#26159;&#22312;&#24191;&#27867;&#20219;&#21153;&#33539;&#22260;&#20869;&#23454;&#29616;&#26368;&#20339;&#25110;&#25509;&#36817;&#26368;&#20339;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#28040;&#38500;&#20102;&#38656;&#35201;&#38024;&#23545;&#29305;&#23450;&#38382;&#39064;&#25163;&#21160;&#36873;&#25321;&#21644;&#23450;&#21046;&#25552;&#31034;&#30340;&#38656;&#27714;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MeMo&#65288;&#24515;&#26234;&#27169;&#22411;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#35774;&#35745;&#30340;&#21019;&#26032;&#25552;&#31034;&#26041;&#27861;&#65292;&#33021;&#26377;&#25928;&#22320;&#23454;&#29616;&#36890;&#29992;&#25552;&#31034;&#30340;&#26631;&#20934;&#12290;MeMo&#23558;&#21508;&#31181;&#25552;&#31034;&#26041;&#27861;&#30340;&#26680;&#24515;&#31934;&#39635;&#25552;&#28860;&#20026;&#21333;&#20010;&#24515;&#26234;&#27169;&#22411;&#65292;&#24182;&#20801;&#35768;LLM&#33258;&#20027;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18252v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated impressive performance on many tasks. However, to achieve optimal performance, specially designed prompting methods are still needed. These methods either rely on task-specific few-shot examples that require a certain level of domain knowledge, or are designed to be simple but only perform well on a few types of tasks. In this work, we attempt to introduce the concept of generalist prompting, which operates on the design principle of achieving optimal or near-optimal performance on a wide range of tasks while eliminating the need for manual selection and customization of prompts tailored to specific problems. Furthermore, we propose MeMo (Mental Models), an innovative prompting method that is simple-designed yet effectively fulfills the criteria of generalist prompting. MeMo distills the cores of various prompting methods into individual mental models and allows LLMs to autonomously select
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#25351;&#23548;&#24494;&#35843;&#30340;&#28508;&#22312;&#26426;&#21046;&#65292;&#21457;&#29616;&#23581;&#35797;&#36890;&#36807;&#25351;&#23548;&#24494;&#35843;&#23398;&#20064;&#39069;&#22806;&#19990;&#30028;&#30693;&#35782;&#24448;&#24448;&#38590;&#20197;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#65292;&#37325;&#28857;&#22312;&#20110;&#20445;&#25345;&#20869;&#37096;&#30693;&#35782;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.18243</link><description>&lt;p&gt;
&#23398;&#20064;&#36824;&#26159;&#33258;&#25105;&#35843;&#25972;&#65311;&#37325;&#26032;&#24605;&#32771;&#25351;&#23548;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Learning or Self-aligning? Rethinking Instruction Fine-tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18243
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#25351;&#23548;&#24494;&#35843;&#30340;&#28508;&#22312;&#26426;&#21046;&#65292;&#21457;&#29616;&#23581;&#35797;&#36890;&#36807;&#25351;&#23548;&#24494;&#35843;&#23398;&#20064;&#39069;&#22806;&#19990;&#30028;&#30693;&#35782;&#24448;&#24448;&#38590;&#20197;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#65292;&#37325;&#28857;&#22312;&#20110;&#20445;&#25345;&#20869;&#37096;&#30693;&#35782;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#23548;&#24494;&#35843;&#65288;IFT&#65289;&#26159;&#26500;&#24314;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#38454;&#27573;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;IFT&#22312;&#34892;&#20026;&#35268;&#33539;&#20256;&#36882;&#21644;&#39069;&#22806;&#19990;&#30028;&#30693;&#35782;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;IFT&#28508;&#22312;&#26426;&#21046;&#30340;&#29702;&#35299;&#20173;&#28982;&#30456;&#24403;&#26377;&#38480;&#12290;&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#30693;&#35782;&#24178;&#39044;&#26694;&#26550;&#65292;&#20197;&#35299;&#32806;IFT&#30340;&#28508;&#22312;&#22240;&#32032;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#19981;&#21516;&#22240;&#32032;&#30340;&#20010;&#20307;&#20998;&#26512;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#65292;&#36890;&#36807;IFT&#35797;&#22270;&#23398;&#20064;&#39069;&#22806;&#30340;&#19990;&#30028;&#30693;&#35782;&#24448;&#24448;&#38590;&#20197;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#65292;&#29978;&#33267;&#21487;&#33021;&#23548;&#33268;&#26126;&#26174;&#36127;&#38754;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;IFT&#20043;&#21069;&#21644;&#20043;&#21518;&#20445;&#25345;&#20869;&#37096;&#30693;&#35782;&#19968;&#33268;&#24615;&#26159;&#23454;&#29616;&#25104;&#21151;IFT&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;IFT&#30340;&#28508;&#22312;&#26426;&#21046;&#65292;&#24182;&#20026;&#26368;&#26032;&#21644;&#28508;&#22312;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#21147;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18243v1 Announce Type: new  Abstract: Instruction Fine-tuning~(IFT) is a critical phase in building large language models~(LLMs). Previous works mainly focus on the IFT's role in the transfer of behavioral norms and the learning of additional world knowledge. However, the understanding of the underlying mechanisms of IFT remains significantly limited. In this paper, we design a knowledge intervention framework to decouple the potential underlying factors of IFT, thereby enabling individual analysis of different factors. Surprisingly, our experiments reveal that attempting to learn additional world knowledge through IFT often struggles to yield positive impacts and can even lead to markedly negative effects. Further, we discover that maintaining internal knowledge consistency before and after IFT is a critical factor for achieving successful IFT. Our findings reveal the underlying mechanisms of IFT and provide robust support for some very recent and potential future works.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#24179;&#21488;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;Rec4Agentverse&#65292;&#24378;&#35843;&#20195;&#29702;&#39033;&#21644;&#20195;&#29702;&#25512;&#33616;&#22120;&#20043;&#38388;&#30340;&#21512;&#20316;&#65292;&#20419;&#36827;&#20010;&#24615;&#21270;&#20449;&#24687;&#26381;&#21153;&#65292;&#25552;&#21319;&#20449;&#24687;&#20132;&#25442;&#65292;&#24182;&#23637;&#26395;&#20102;&#20854;&#28436;&#36827;&#20026;&#25903;&#25345;&#20114;&#21160;&#21644;&#20449;&#24687;&#20132;&#25442;&#30340;&#19977;&#20010;&#38454;&#27573;</title><link>https://arxiv.org/abs/2402.18240</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#24179;&#21488;&#19978;&#30340;&#21069;&#26223;&#20010;&#24615;&#21270;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Prospect Personalized Recommendation on Large Language Model-based Agent Platform
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18240
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#24179;&#21488;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;Rec4Agentverse&#65292;&#24378;&#35843;&#20195;&#29702;&#39033;&#21644;&#20195;&#29702;&#25512;&#33616;&#22120;&#20043;&#38388;&#30340;&#21512;&#20316;&#65292;&#20419;&#36827;&#20010;&#24615;&#21270;&#20449;&#24687;&#26381;&#21153;&#65292;&#25552;&#21319;&#20449;&#24687;&#20132;&#25442;&#65292;&#24182;&#23637;&#26395;&#20102;&#20854;&#28436;&#36827;&#20026;&#25903;&#25345;&#20114;&#21160;&#21644;&#20449;&#24687;&#20132;&#25442;&#30340;&#19977;&#20010;&#38454;&#27573;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#22411;&#20195;&#29702;&#23548;&#21521;&#20449;&#24687;&#31995;&#32479;&#65292;&#20197;GPT&#20026;&#20363;&#65292;&#20419;&#20351;&#25105;&#20204;&#23457;&#35270;&#20449;&#24687;&#31995;&#32479;&#22522;&#30784;&#35774;&#26045;&#65292;&#20197;&#25903;&#25345;&#20195;&#29702;&#32423;&#20449;&#24687;&#22788;&#29702;&#24182;&#36866;&#24212;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20195;&#29702;&#30340;&#29305;&#24449;&#65292;&#22914;&#20114;&#21160;&#24615;&#12290;&#26412;&#30740;&#31350;&#23637;&#26395;&#20102;&#22522;&#20110;LLM&#20195;&#29702;&#24179;&#21488;&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#21069;&#26223;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;Rec4Agentverse&#30340;&#26032;&#22411;&#25512;&#33616;&#33539;&#24335;&#65292;&#21253;&#25324;&#20195;&#29702;&#39033;&#21644;&#20195;&#29702;&#25512;&#33616;&#22120;&#12290;Rec4Agentverse&#24378;&#35843;&#20195;&#29702;&#39033;&#21644;&#20195;&#29702;&#25512;&#33616;&#22120;&#20043;&#38388;&#30340;&#21512;&#20316;&#65292;&#20174;&#32780;&#20419;&#36827;&#20010;&#24615;&#21270;&#20449;&#24687;&#26381;&#21153;&#65292;&#24182;&#22686;&#24378;&#20449;&#24687;&#20132;&#25442;&#65292;&#36229;&#36234;&#20256;&#32479;&#30340;&#29992;&#25143;-&#25512;&#33616;&#22120;&#21453;&#39304;&#24490;&#29615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#26395;&#20102;Rec4Agentverse&#30340;&#28436;&#36827;&#65292;&#24182;&#23558;&#20854;&#27010;&#24565;&#21270;&#20026;&#22522;&#20110;&#20195;&#29702;&#39033;&#12289;&#20195;&#29702;&#25512;&#33616;&#22120;&#21644;&#29992;&#25143;&#20043;&#38388;&#20114;&#21160;&#21644;&#20449;&#24687;&#20132;&#25442;&#22686;&#24378;&#30340;&#19977;&#20010;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18240v1 Announce Type: cross  Abstract: The new kind of Agent-oriented information system, exemplified by GPTs, urges us to inspect the information system infrastructure to support Agent-level information processing and to adapt to the characteristics of Large Language Model (LLM)-based Agents, such as interactivity. In this work, we envisage the prospect of the recommender system on LLM-based Agent platforms and introduce a novel recommendation paradigm called Rec4Agentverse, comprised of Agent Items and Agent Recommender. Rec4Agentverse emphasizes the collaboration between Agent Items and Agent Recommender, thereby promoting personalized information services and enhancing the exchange of information beyond the traditional user-recommender feedback loop. Additionally, we prospect the evolution of Rec4Agentverse and conceptualize it into three stages based on the enhancement of the interaction and information exchange among Agent Items, Agent Recommender, and the user. A pre
&lt;/p&gt;</description></item><item><title>CogBench&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#19971;&#20010;&#35748;&#30693;&#24515;&#29702;&#23398;&#23454;&#39564;&#20013;&#34893;&#29983;&#20986;&#21313;&#20010;&#34892;&#20026;&#25351;&#26631;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20026;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#25552;&#20379;&#20102;&#24037;&#20855;&#65292;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#21644;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#24378;&#21270;&#23398;&#20064;&#23545;&#24615;&#33021;&#25913;&#21892;&#21644;&#19982;&#20154;&#31867;&#34892;&#20026;&#19968;&#33268;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.18225</link><description>&lt;p&gt;
CogBench: &#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27493;&#20837;&#24515;&#29702;&#23454;&#39564;&#23460;
&lt;/p&gt;
&lt;p&gt;
CogBench: a large language model walks into a psychology lab
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18225
&lt;/p&gt;
&lt;p&gt;
CogBench&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#19971;&#20010;&#35748;&#30693;&#24515;&#29702;&#23398;&#23454;&#39564;&#20013;&#34893;&#29983;&#20986;&#21313;&#20010;&#34892;&#20026;&#25351;&#26631;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20026;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#25552;&#20379;&#20102;&#24037;&#20855;&#65292;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#21644;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#24378;&#21270;&#23398;&#20064;&#23545;&#24615;&#33021;&#25913;&#21892;&#21644;&#19982;&#20154;&#31867;&#34892;&#20026;&#19968;&#33268;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26174;&#33879;&#25512;&#21160;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#23545;&#23427;&#20204;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#37096;&#20998;&#26159;&#30001;&#20110;&#22823;&#22810;&#25968;&#22522;&#20934;&#27979;&#35797;&#20013;&#23545;&#24615;&#33021;&#25351;&#26631;&#30340;&#20027;&#35201;&#20851;&#27880;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;CogBench&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#20174;&#19971;&#20010;&#35748;&#30693;&#24515;&#29702;&#23398;&#23454;&#39564;&#20013;&#34893;&#29983;&#30340;&#21313;&#20010;&#34892;&#20026;&#25351;&#26631;&#12290;&#36825;&#31181;&#26032;&#39062;&#26041;&#27861;&#20026;&#34920;&#22411;&#21270;LLMs&#30340;&#34892;&#20026;&#25552;&#20379;&#20102;&#19968;&#20010;&#24037;&#20855;&#21253;&#12290;&#25105;&#20204;&#23558;CogBench&#24212;&#29992;&#20110;35&#20010;LLMs&#65292;&#24471;&#21040;&#20016;&#23500;&#22810;&#26679;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;&#32479;&#35745;&#22810;&#23618;&#24314;&#27169;&#25216;&#26415;&#20998;&#26512;&#36825;&#20123;&#25968;&#25454;&#65292;&#32771;&#34385;&#21040;&#29305;&#23450;LLMs&#30340;&#24494;&#35843;&#29256;&#26412;&#20043;&#38388;&#30340;&#23884;&#22871;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#31361;&#20986;&#20102;&#27169;&#22411;&#22823;&#23567;&#21644;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#24378;&#21270;&#23398;&#20064;&#22312;&#25913;&#21892;&#24615;&#33021;&#24182;&#19982;&#20154;&#31867;&#34892;&#20026;&#20445;&#25345;&#19968;&#33268;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#24320;&#28304;&#27169;&#22411;&#27604;&#19987;&#26377;&#27169;&#22411;&#26356;&#23569;&#39118;&#38505;&#65292;&#24182;&#19988;&#31934;&#32454;&#35843;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18225v1 Announce Type: cross  Abstract: Large language models (LLMs) have significantly advanced the field of artificial intelligence. Yet, evaluating them comprehensively remains challenging. We argue that this is partly due to the predominant focus on performance metrics in most benchmarks. This paper introduces CogBench, a benchmark that includes ten behavioral metrics derived from seven cognitive psychology experiments. This novel approach offers a toolkit for phenotyping LLMs' behavior. We apply CogBench to 35 LLMs, yielding a rich and diverse dataset. We analyze this data using statistical multilevel modeling techniques, accounting for the nested dependencies among fine-tuned versions of specific LLMs. Our study highlights the crucial role of model size and reinforcement learning from human feedback (RLHF) in improving performance and aligning with human behavior. Interestingly, we find that open-source models are less risk-prone than proprietary models and that fine-t
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#33258;&#36866;&#24212;&#35299;&#30721;&#26426;&#21046;&#65292;&#36890;&#36807;&#32622;&#20449;&#24230;&#21160;&#24577;&#30830;&#23450;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#20505;&#36873;&#38598;&#65292;&#22312;&#25925;&#20107;&#29983;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;MAUVE&#21644;&#22810;&#26679;&#24615;&#65292;&#20445;&#25345;&#19968;&#23450;&#30340;&#36830;&#36143;&#24615;&#65292;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.18223</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#35299;&#30721;&#25913;&#36827;&#24320;&#25918;&#24335;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Improving Open-Ended Text Generation via Adaptive Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18223
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#33258;&#36866;&#24212;&#35299;&#30721;&#26426;&#21046;&#65292;&#36890;&#36807;&#32622;&#20449;&#24230;&#21160;&#24577;&#30830;&#23450;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#20505;&#36873;&#38598;&#65292;&#22312;&#25925;&#20107;&#29983;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;MAUVE&#21644;&#22810;&#26679;&#24615;&#65292;&#20445;&#25345;&#19968;&#23450;&#30340;&#36830;&#36143;&#24615;&#65292;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#26681;&#25454;&#27010;&#29575;&#20998;&#24067;&#36880;&#26631;&#35760;&#35299;&#30721;&#25991;&#26412;&#65292;&#30830;&#23450;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#24688;&#24403;&#20505;&#36873;&#32773;&#23545;&#20110;&#20445;&#35777;&#29983;&#25104;&#36136;&#37327;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#35299;&#30721;&#65292;&#19968;&#31181;&#26426;&#21046;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#21160;&#24577;&#30830;&#23450;&#19968;&#20010;&#21512;&#29702;&#30340;&#20505;&#36873;&#38598;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#31216;&#20043;&#20026;&#32622;&#20449;&#24230;&#65292;&#24182;&#23558;&#30830;&#23450;&#26368;&#20339;&#20505;&#36873;&#38598;&#35270;&#20026;&#19968;&#20010;&#22686;&#21152;&#32622;&#20449;&#24230;&#30340;&#36807;&#31243;&#12290;&#36890;&#36807;&#21033;&#29992;&#32622;&#20449;&#24230;&#22686;&#21152;&#26469;&#35780;&#20272;&#23558;&#26631;&#35760;&#21253;&#21547;&#22312;&#20505;&#36873;&#38598;&#20013;&#30340;&#21512;&#29702;&#24615;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#30830;&#23450;&#26368;&#21512;&#36866;&#30340;&#20505;&#36873;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25925;&#20107;&#29983;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;MAUVE&#21644;&#22810;&#26679;&#24615;&#65292;&#24182;&#20445;&#25345;&#20102;&#19968;&#23450;&#30340;&#36830;&#36143;&#24615;&#65292;&#31361;&#26174;&#20102;&#20854;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18223v1 Announce Type: new  Abstract: Current language models decode text token by token according to probabilistic distribution, and determining the appropriate candidates for the next token is crucial to ensure generation quality. This study introduces adaptive decoding, a mechanism that empowers the language models to ascertain a sensible candidate set during the generation process dynamically. Specifically, we introduce an entropy-based metric called confidence and conceptualize determining the optimal candidate set as a confidence-increasing process. The rationality of including a token in the candidate set is assessed by leveraging the increment of confidence, enabling the model to determine the most suitable candidate set adaptively. The experimental results reveal that our method achieves higher MAUVE and diversity in story generation tasks and maintains certain coherence, underscoring its superiority over existing algorithms. The code is available at https://github.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21021;&#27493;&#25506;&#35752;&#20102;&#23545;&#35805;&#20013;&#20219;&#21153;&#20999;&#25442;&#23545;LLM&#27169;&#22411;&#24178;&#25200;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20219;&#21153;&#20999;&#25442;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2402.18216</link><description>&lt;p&gt;
LLM&#20219;&#21153;&#24178;&#25200;&#65306;&#20851;&#20110;&#23545;&#35805;&#21382;&#21490;&#20013;&#20219;&#21153;&#20999;&#25442;&#24433;&#21709;&#30340;&#21021;&#27493;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
LLM Task Interference: An Initial Study on the Impact of Task-Switch in Conversational History
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21021;&#27493;&#25506;&#35752;&#20102;&#23545;&#35805;&#20013;&#20219;&#21153;&#20999;&#25442;&#23545;LLM&#27169;&#22411;&#24178;&#25200;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20219;&#21153;&#20999;&#25442;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24378;&#22823;&#30340;&#25351;&#20196;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#20986;&#29616;&#65292;&#20351;&#24471;&#21508;&#31181;&#26377;&#29992;&#30340;&#23545;&#35805;&#20154;&#24037;&#26234;&#33021;(AI)&#31995;&#32479;&#24050;&#32463;&#37096;&#32626;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#12290;&#24403;&#29992;&#25143;&#25552;&#20986;&#38382;&#39064;&#26102;&#65292;&#36825;&#20123;AI&#31995;&#32479;&#25104;&#21151;&#22320;&#20316;&#20026;&#23545;&#35805;&#30340;&#19968;&#37096;&#20998;&#25191;&#34892;&#21508;&#31181;&#20219;&#21153;&#12290;&#20026;&#20102;&#25552;&#20379;&#26576;&#31181;&#35760;&#24518;&#21644;&#19978;&#19979;&#25991;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#23558;&#36755;&#20986;&#26465;&#20214;&#38480;&#21046;&#22312;&#25972;&#20010;&#23545;&#35805;&#21382;&#21490;&#19978;&#12290;&#23613;&#31649;&#23545;&#23545;&#35805;&#21382;&#21490;&#30340;&#25935;&#24863;&#24615;&#32463;&#24120;&#20250;&#23548;&#33268;&#22312;&#38543;&#21518;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#25552;&#39640;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#65292;&#23454;&#38469;&#19978;&#22914;&#26524;&#26377;&#20219;&#21153;&#20999;&#25442;&#65292;&#34920;&#29616;&#20063;&#21487;&#33021;&#21463;&#21040;&#36127;&#38754;&#24433;&#21709;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#39318;&#27425;&#23581;&#35797;&#27491;&#24335;&#30740;&#31350;&#23545;&#35805;LLMs&#20013;&#30001;&#20110;&#23545;&#35805;&#21382;&#21490;&#20013;&#30340;&#20219;&#21153;&#20999;&#25442;&#32780;&#24341;&#36215;&#30340;&#20219;&#21153;&#24178;&#25200;&#21644;&#24178;&#25200;&#30340;&#33030;&#24369;&#24615;&#12290;&#25105;&#20204;&#22312;5&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20351;&#29992;&#20102;&#27969;&#34892;&#30340;LLMs&#36827;&#34892;&#20102;15&#27425;&#20219;&#21153;&#20999;&#25442;&#65292;&#32467;&#26524;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18216v1 Announce Type: new  Abstract: With the recent emergence of powerful instruction-tuned large language models (LLMs), various helpful conversational Artificial Intelligence (AI) systems have been deployed across many applications. When prompted by users, these AI systems successfully perform a wide range of tasks as part of a conversation. To provide some sort of memory and context, such approaches typically condition their output on the entire conversational history. Although this sensitivity to the conversational history can often lead to improved performance on subsequent tasks, we find that performance can in fact also be negatively impacted, if there is a task-switch. To the best of our knowledge, our work makes the first attempt to formalize the study of such vulnerabilities and interference of tasks in conversational LLMs caused by task-switches in the conversational history. Our experiments across 5 datasets with 15 task switches using popular LLMs reveal that 
&lt;/p&gt;</description></item><item><title>&#20025;&#40614;NLP&#20013;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#39046;&#22495;&#30340;&#19968;&#20010;&#37325;&#35201;&#36129;&#29486;&#26159;&#24341;&#20837;&#20102;DANSK&#25968;&#25454;&#38598;&#21644;DaCy 2.6.0&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#27169;&#22411;&#22312;&#32454;&#31890;&#24230;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#36328;&#39046;&#22495;&#27867;&#21270;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.18209</link><description>&lt;p&gt;
DANSK&#21644;DaCy 2.6.0&#65306;&#20025;&#40614;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#39046;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
DANSK and DaCy 2.6.0: Domain Generalization of Danish Named Entity Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18209
&lt;/p&gt;
&lt;p&gt;
&#20025;&#40614;NLP&#20013;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#39046;&#22495;&#30340;&#19968;&#20010;&#37325;&#35201;&#36129;&#29486;&#26159;&#24341;&#20837;&#20102;DANSK&#25968;&#25454;&#38598;&#21644;DaCy 2.6.0&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#27169;&#22411;&#22312;&#32454;&#31890;&#24230;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#36328;&#39046;&#22495;&#27867;&#21270;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26159;&#20025;&#40614;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#22522;&#30707;&#20043;&#19968;&#65292;&#23545;&#20110;&#24037;&#19994;&#21644;&#30740;&#31350;&#20013;&#30340;&#35821;&#35328;&#25216;&#26415;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20025;&#40614;&#30340;NER&#21463;&#21040;&#21487;&#29992;&#25968;&#25454;&#38598;&#30340;&#19981;&#36275;&#25152;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#30446;&#21069;&#27809;&#26377;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#32454;&#31890;&#24230;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65292;&#20063;&#27809;&#26377;&#23545;&#36328;&#25968;&#25454;&#38598;&#21644;&#39046;&#22495;&#30340;&#28508;&#22312;&#27867;&#21270;&#38382;&#39064;&#36827;&#34892;&#35780;&#20272;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#65306;1&#65289;DANSK&#65306;&#19968;&#20010;&#25552;&#20379;&#39640;&#31890;&#24230;&#26631;&#35760;&#20197;&#21450;&#22312;&#21508;&#31181;&#39046;&#22495;&#20869;&#27169;&#22411;&#35780;&#20272;&#30340;&#21629;&#21517;&#23454;&#20307;&#25968;&#25454;&#38598;&#65307;2&#65289;DaCy 2.6.0&#65292;&#20854;&#20013;&#21253;&#25324;&#19977;&#20010;&#21487;&#27867;&#21270;&#30340;&#27169;&#22411;&#65292;&#24182;&#37197;&#26377;&#32454;&#31890;&#24230;&#26631;&#27880;&#65307;&#20197;&#21450;3&#65289;&#23545;&#24403;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#22312;&#36328;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30340;&#35780;&#20272;&#12290;&#23545;&#29616;&#26377;&#21644;&#26032;&#27169;&#22411;&#30340;&#35780;&#20272;&#25581;&#31034;&#20102;&#20540;&#24471;&#22312;&#39046;&#22495;&#20869;&#35299;&#20915;&#30340;&#26174;&#30528;&#24615;&#33021;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18209v1 Announce Type: new  Abstract: Named entity recognition is one of the cornerstones of Danish NLP, essential for language technology applications within both industry and research. However, Danish NER is inhibited by a lack of available datasets. As a consequence, no current models are capable of fine-grained named entity recognition, nor have they been evaluated for potential generalizability issues across datasets and domains. To alleviate these limitations, this paper introduces: 1) DANSK: a named entity dataset providing for high-granularity tagging as well as within-domain evaluation of models across a diverse set of domains; 2) DaCy 2.6.0 that includes three generalizable models with fine-grained annotation; and 3) an evaluation of current state-of-the-art models' ability to generalize across domains. The evaluation of existing and new models revealed notable performance discrepancies across domains, which should be addressed within the field. Shortcomings of the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32858;&#31867;&#19982;&#25490;&#24207;&#26041;&#27861;&#65288;CaR&#65289;&#65292;&#36890;&#36807;&#19982;&#19987;&#23478;&#20559;&#22909;&#30456;&#19968;&#33268;&#30340;&#35780;&#20998;&#27169;&#22411;&#25490;&#21517;&#25351;&#20196;&#23545;&#65292;&#20445;&#30041;&#20102;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.18191</link><description>&lt;p&gt;
&#32858;&#31867;&#19982;&#25490;&#24207;&#65306;&#36890;&#36807;&#19987;&#23478;&#23450;&#20301;&#36136;&#37327;&#20272;&#35745;&#23454;&#29616;&#20445;&#30041;&#22810;&#26679;&#24615;&#30340;&#25351;&#20196;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Clustering and Ranking: Diversity-preserved Instruction Selection through Expert-aligned Quality Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18191
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32858;&#31867;&#19982;&#25490;&#24207;&#26041;&#27861;&#65288;CaR&#65289;&#65292;&#36890;&#36807;&#19982;&#19987;&#23478;&#20559;&#22909;&#30456;&#19968;&#33268;&#30340;&#35780;&#20998;&#27169;&#22411;&#25490;&#21517;&#25351;&#20196;&#23545;&#65292;&#20445;&#30041;&#20102;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24320;&#28304;&#31038;&#21306;&#30340;&#36129;&#29486;&#65292;&#28044;&#29616;&#20102;&#22823;&#37327;&#25351;&#20196;&#35843;&#20248;&#65288;IT&#65289;&#25968;&#25454;&#12290;&#37492;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#36164;&#28304;&#20998;&#37197;&#65292;&#22240;&#27492;&#26377;&#24517;&#35201;&#37319;&#29992;&#39640;&#25928;&#30340;&#26041;&#27861;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;IT&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25351;&#20196;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#27604;&#22914;&#20381;&#36182;&#33030;&#24369;&#30340;&#22806;&#37096;API&#12289;&#21463;GPT&#27169;&#22411;&#20559;&#35265;&#24433;&#21709;&#65292;&#25110;&#20943;&#23569;&#25152;&#36873;&#25351;&#20196;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#24037;&#19994;&#30340;&#12289;&#19982;&#19987;&#23478;&#23450;&#20301;&#30456;&#21563;&#21512;&#24182;&#20445;&#30041;&#22810;&#26679;&#24615;&#30340;&#25351;&#20196;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#65306;&#32858;&#31867;&#19982;&#25490;&#24207;&#65288;CaR&#65289;&#12290;CaR&#20998;&#20026;&#20004;&#20010;&#27493;&#39588;&#12290;&#31532;&#19968;&#27493;&#28041;&#21450;&#20351;&#29992;&#19982;&#19987;&#23478;&#20559;&#22909;&#24456;&#22909;&#23545;&#40784;&#30340;&#35780;&#20998;&#27169;&#22411;&#23545;&#25351;&#20196;&#23545;&#36827;&#34892;&#25490;&#21517;&#65288;&#20934;&#30830;&#29575;&#36798;&#21040;84.25%&#65289;&#12290;&#31532;&#20108;&#27493;&#36890;&#36807;&#32858;&#31867;&#36807;&#31243;&#20445;&#30041;&#25968;&#25454;&#38598;&#22810;&#26679;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;CaR&#36873;&#25321;&#20102;&#19968;&#20010;&#23376;&#38598;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18191v1 Announce Type: new  Abstract: With contributions from the open-source community, a vast amount of instruction tuning (IT) data has emerged. Given the significant resource allocation required by training and evaluating models, it is advantageous to have an efficient method for selecting high-quality IT data. However, existing methods for instruction data selection have limitations such as relying on fragile external APIs, being affected by biases in GPT models, or reducing the diversity of the selected instruction dataset. In this paper, we propose an industrial-friendly, expert-aligned and diversity-preserved instruction data selection method: Clustering and Ranking (CaR). CaR consists of two steps. The first step involves ranking instruction pairs using a scoring model that is well aligned with expert preferences (achieving an accuracy of 84.25%). The second step involves preserving dataset diversity through a clustering process.In our experiment, CaR selected a sub
&lt;/p&gt;</description></item><item><title>&#23558;&#31070;&#32463;&#32593;&#32476;&#39044;&#35757;&#32451;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#39046;&#22495;&#30456;&#32467;&#21512;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#24182;&#25351;&#20986;&#30446;&#21069;&#36716;&#31227;&#23398;&#20064;&#24182;&#26410;&#26174;&#33879;&#25913;&#36827;&#27169;&#22411;&#24615;&#33021;&#65292;&#20027;&#35201;&#38382;&#39064;&#22312;&#20110;&#32570;&#20047;&#22823;&#35268;&#27169;&#36164;&#28304;&#12290;</title><link>https://arxiv.org/abs/2402.18179</link><description>&lt;p&gt;
&#38024;&#23545;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#39044;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25361;&#25112;&#65306;&#23545;&#24403;&#21069;&#31574;&#30053;&#21644;&#36164;&#28304;&#38480;&#21046;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Challenges in Pre-Training Graph Neural Networks for Context-Based Fake News Detection: An Evaluation of Current Strategies and Resource Limitations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18179
&lt;/p&gt;
&lt;p&gt;
&#23558;&#31070;&#32463;&#32593;&#32476;&#39044;&#35757;&#32451;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#39046;&#22495;&#30456;&#32467;&#21512;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#24182;&#25351;&#20986;&#30446;&#21069;&#36716;&#31227;&#23398;&#20064;&#24182;&#26410;&#26174;&#33879;&#25913;&#36827;&#27169;&#22411;&#24615;&#33021;&#65292;&#20027;&#35201;&#38382;&#39064;&#22312;&#20110;&#32570;&#20047;&#22823;&#35268;&#27169;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#35757;&#32451;&#26368;&#36817;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#24443;&#24213;&#25913;&#21464;&#20102;&#23616;&#38754;&#65292;&#24182;&#19988;&#24050;&#32463;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;&#20851;&#20110;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#36827;&#23637;&#20027;&#35201;&#21463;&#21040;&#22522;&#20110;&#19978;&#19979;&#25991;&#33539;&#24335;&#30340;&#39537;&#21160;&#65292;&#20854;&#20013;&#26469;&#33258;&#31038;&#20132;&#23186;&#20307;&#31561;&#19981;&#21516;&#31867;&#22411;&#20449;&#21495;&#24418;&#25104;&#31867;&#20284;&#22270;&#24418;&#32467;&#26500;&#65292;&#20854;&#20013;&#21253;&#21547;&#38500;&#26032;&#38395;&#25991;&#31456;&#20197;&#22806;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#20197;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#24314;&#35758;&#23558;&#36825;&#20004;&#20010;&#21457;&#23637;&#21512;&#24182;&#65292;&#36890;&#36807;&#22312;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#39046;&#22495;&#24212;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#20102;&#22522;&#20110;&#22270;&#30340;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#30340;&#19981;&#21516;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#24182;&#35777;&#26126;&#20102;&#30446;&#21069;&#36716;&#31227;&#23398;&#20064;&#24182;&#26410;&#23548;&#33268;&#26174;&#30528;&#25913;&#36827;&#65292;&#20248;&#20110;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#35748;&#20026;&#30446;&#21069;&#30340;&#20027;&#35201;&#38382;&#39064;&#26159;&#32570;&#20047;&#36866;&#29992;&#30340;&#22823;&#35268;&#27169;&#36164;&#28304;&#21487;&#20197;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18179v1 Announce Type: new  Abstract: Pre-training of neural networks has recently revolutionized the field of Natural Language Processing (NLP) and has before demonstrated its effectiveness in computer vision. At the same time, advances around the detection of fake news were mainly driven by the context-based paradigm, where different types of signals (e.g. from social media) form graph-like structures that hold contextual information apart from the news article to classify. We propose to merge these two developments by applying pre-training of Graph Neural Networks (GNNs) in the domain of context-based fake news detection. Our experiments provide an evaluation of different pre-training strategies for graph-based misinformation detection and demonstrate that transfer learning does currently not lead to significant improvements over training a model from scratch in the domain. We argue that a major current issue is the lack of suitable large-scale resources that can be used 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;MIKO&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#27169;&#24577;&#21644;&#25991;&#26412;&#30340;&#21327;&#21516;&#20316;&#29992;&#25581;&#31034;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#30340;&#24847;&#22270;&#12290;</title><link>https://arxiv.org/abs/2402.18169</link><description>&lt;p&gt;
MIKO&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31038;&#20132;&#23186;&#20307;&#24120;&#35782;&#21457;&#29616;&#30340;&#22810;&#27169;&#24577;&#24847;&#22270;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
MIKO: Multimodal Intention Knowledge Distillation from Large Language Models for Social-Media Commonsense Discovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18169
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;MIKO&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#27169;&#24577;&#21644;&#25991;&#26412;&#30340;&#21327;&#21516;&#20316;&#29992;&#25581;&#31034;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#30340;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24050;&#32463;&#25104;&#20026;&#19982;&#20182;&#20154;&#32852;&#31995;&#12289;&#20102;&#35299;&#26032;&#38395;&#12289;&#34920;&#36798;&#35266;&#28857;&#20197;&#21450;&#25214;&#21040;&#23089;&#20048;&#30340;&#26080;&#22788;&#19981;&#22312;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#24847;&#22270;&#30340;&#38544;&#21547;&#24615;&#12289;&#38656;&#35201;&#36328;&#27169;&#24577;&#29702;&#35299;&#25991;&#26412;&#21644;&#22270;&#20687;&#12289;&#20197;&#21450;&#23384;&#22312;&#26631;&#31614;&#12289;&#25340;&#20889;&#38169;&#35823;&#21644;&#22797;&#26434;&#32553;&#20889;&#31561;&#22024;&#26434;&#20449;&#24687;&#65292;&#29702;&#35299;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#32972;&#21518;&#30340;&#24847;&#22270;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MIKO&#65292;&#19968;&#20010;&#22810;&#27169;&#24577;&#24847;&#22270;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#23427;&#20849;&#21516;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#26469;&#25581;&#31034;&#29992;&#25143;&#30340;&#24847;&#22270;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;MLLM&#26469;&#35299;&#37322;&#22270;&#20687;&#65292;&#24182;&#20351;&#29992;LLM&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#65292;&#26368;&#21518;&#20877;&#27425;&#25351;&#23548;LLM&#29983;&#25104;&#24847;&#22270;&#12290;&#36890;&#36807;&#23558;MIKO&#24212;&#29992;&#20110;&#20844;&#24320;&#21487;&#29992;&#30340;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#24847;&#22270;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18169v1 Announce Type: new  Abstract: Social media has become a ubiquitous tool for connecting with others, staying updated with news, expressing opinions, and finding entertainment. However, understanding the intention behind social media posts remains challenging due to the implicitness of intentions in social media posts, the need for cross-modality understanding of both text and images, and the presence of noisy information such as hashtags, misspelled words, and complicated abbreviations. To address these challenges, we present MIKO, a Multimodal Intention Kowledge DistillatiOn framework that collaboratively leverages a Large Language Model (LLM) and a Multimodal Large Language Model (MLLM) to uncover users' intentions. Specifically, we use an MLLM to interpret the image and an LLM to extract key information from the text and finally instruct the LLM again to generate intentions. By applying MIKO to publicly available social media datasets, we construct an intention kno
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20840;&#38754;&#35780;&#20272;&#21518;&#35757;&#32451;&#37327;&#21270;&#23545;&#26435;&#37325;&#12289;&#28608;&#27963;&#21644;KV&#32531;&#23384;&#30340;&#24433;&#21709;&#65292;&#20197;&#25351;&#23548;&#36873;&#25321;&#37327;&#21270;&#26041;&#27861;&#65292;&#24182;&#23545;11&#31181;&#27169;&#22411;&#31995;&#21015;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#35206;&#30422;&#20102;&#22810;&#31181;&#20219;&#21153;&#31867;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.18158</link><description>&lt;p&gt;
&#35780;&#20272;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Evaluating Quantized Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18158
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20840;&#38754;&#35780;&#20272;&#21518;&#35757;&#32451;&#37327;&#21270;&#23545;&#26435;&#37325;&#12289;&#28608;&#27963;&#21644;KV&#32531;&#23384;&#30340;&#24433;&#21709;&#65292;&#20197;&#25351;&#23548;&#36873;&#25321;&#37327;&#21270;&#26041;&#27861;&#65292;&#24182;&#23545;11&#31181;&#27169;&#22411;&#31995;&#21015;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#35206;&#30422;&#20102;&#22810;&#31181;&#20219;&#21153;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#24050;&#32463;&#25104;&#20026;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25104;&#26412;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#25216;&#26415;&#65292;&#20855;&#20307;&#22320;&#65292;PTQ&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#36731;LLMs&#20013;&#30340;&#20869;&#23384;&#28040;&#32791;&#24182;&#38477;&#20302;&#35745;&#31639;&#24320;&#38144;&#12290;&#20026;&#20102;&#28385;&#36275;&#21508;&#31181;&#22330;&#26223;&#19979;&#39640;&#25928;&#29575;&#21644;&#24615;&#33021;&#30340;&#35201;&#27714;&#65292;&#23545;&#37327;&#21270;LLMs&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#26159;&#24517;&#35201;&#30340;&#65292;&#20197;&#25351;&#23548;&#37327;&#21270;&#26041;&#27861;&#30340;&#36873;&#25321;&#12290;&#26412;&#25991;&#36890;&#36807;&#35780;&#20272;PTQ&#23545;11&#20010;&#27169;&#22411;&#31995;&#21015;&#65288;&#21253;&#25324;OPT&#12289;LLaMA2&#12289;Falcon&#12289;Bloomz&#12289;Mistral&#12289;ChatGLM&#12289;Vicuna&#12289;LongChat&#12289;StableLM&#12289;Gemma&#21644;Mamba&#65289;&#30340;&#26435;&#37325;&#12289;&#28608;&#27963;&#21644;KV&#32531;&#23384;&#30340;&#24433;&#21709;&#65292;&#33539;&#22260;&#20174;125M&#21040;180B&#65292;&#20840;&#38754;&#35780;&#20272;&#20102;&#36825;&#20123;&#22240;&#32032;&#12290;&#35780;&#20272;&#28085;&#30422;&#20102;&#20116;&#31181;&#31867;&#22411;&#30340;&#20219;&#21153;&#65306;&#22522;&#30784;NLP&#12289;&#31361;&#28982;&#20986;&#29616;&#30340;&#33021;&#21147;&#12289;&#21487;&#38752;&#24615;&#12289;&#23545;&#35805;&#21644;&#38271;&#19978;&#19979;&#25991;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#20197;&#23637;&#31034;&#23427;&#20204;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18158v1 Announce Type: cross  Abstract: Post-training quantization (PTQ) has emerged as a promising technique to reduce the cost of large language models (LLMs). Specifically, PTQ can effectively mitigate memory consumption and reduce computational overhead in LLMs. To meet the requirements of both high efficiency and performance across diverse scenarios, a comprehensive evaluation of quantized LLMs is essential to guide the selection of quantization methods. This paper presents a thorough evaluation of these factors by evaluating the effect of PTQ on Weight, Activation, and KV Cache on 11 model families, including OPT, LLaMA2, Falcon, Bloomz, Mistral, ChatGLM, Vicuna, LongChat, StableLM, Gemma, and Mamba, with parameters ranging from 125M to 180B. The evaluation encompasses five types of tasks: basic NLP, emergent ability, trustworthiness, dialogue, and long-context tasks. Moreover, we also evaluate the state-of-the-art (SOTA) quantization methods to demonstrate their appli
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#24037;&#20855;&#35843;&#29992;&#31649;&#36947;&#65292;&#26088;&#22312;&#25511;&#21046;&#22823;&#35268;&#27169;&#29616;&#23454;&#19990;&#30028;API&#65292;&#20174;&#32780;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.18157</link><description>&lt;p&gt;
&#20174;&#24635;&#32467;&#21040;&#34892;&#21160;&#65306;&#21033;&#29992;&#24320;&#25918;&#19990;&#30028;API&#22686;&#24378;&#22797;&#26434;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
From Summary to Action: Enhancing Large Language Models for Complex Tasks with Open World APIs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#24037;&#20855;&#35843;&#29992;&#31649;&#36947;&#65292;&#26088;&#22312;&#25511;&#21046;&#22823;&#35268;&#27169;&#29616;&#23454;&#19990;&#30028;API&#65292;&#20174;&#32780;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#19982;&#21160;&#29289;&#20043;&#38388;&#30340;&#21306;&#21035;&#22312;&#20110;&#20154;&#31867;&#20855;&#26377;&#20351;&#29992;&#21644;&#21019;&#36896;&#24037;&#20855;&#30340;&#29420;&#29305;&#33021;&#21147;&#12290;&#24037;&#20855;&#20351;&#20154;&#31867;&#33021;&#22815;&#20811;&#26381;&#29983;&#29702;&#38480;&#21046;&#65292;&#20419;&#36827;&#20102;&#23439;&#20255;&#25991;&#26126;&#30340;&#21019;&#36896;&#12290;&#31867;&#20284;&#22320;&#65292;&#23558;&#20687;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36825;&#26679;&#30340;&#22522;&#30784;&#27169;&#22411;&#36171;&#20104;&#23398;&#20064;&#22806;&#37096;&#24037;&#20855;&#20351;&#29992;&#33021;&#21147;&#21487;&#33021;&#26159;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;&#26412;&#39046;&#22495;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#36861;&#27714;&#20004;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;LLMs&#30340;&#24037;&#20855;&#35843;&#29992;&#33021;&#21147;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#24378;&#35843;&#26500;&#24314;&#29992;&#20110;&#27169;&#22411;&#24494;&#35843;&#30340;&#30456;&#20851;&#25968;&#25454;&#38598;&#12290;&#30456;&#21453;&#65292;&#31532;&#20108;&#31181;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#31574;&#30053;&#20805;&#20998;&#21033;&#29992;LLMs&#22266;&#26377;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26088;&#22312;&#25511;&#21046;&#22823;&#22411;&#29616;&#23454;&#19990;&#30028;API&#30340;&#21019;&#26032;&#24037;&#20855;&#35843;&#29992;&#31649;&#36947;&#12290;&#36825;&#19968;&#31649;&#36947;&#21453;&#26144;&#20102;&#20154;&#31867;&#35299;&#20915;&#20219;&#21153;&#30340;&#36807;&#31243;&#65292;&#35299;&#20915;&#20102;c
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18157v1 Announce Type: new  Abstract: The distinction between humans and animals lies in the unique ability of humans to use and create tools. Tools empower humans to overcome physiological limitations, fostering the creation of magnificent civilizations. Similarly, enabling foundational models like Large Language Models (LLMs) with the capacity to learn external tool usage may serve as a pivotal step toward realizing artificial general intelligence. Previous studies in this field have predominantly pursued two distinct approaches to augment the tool invocation capabilities of LLMs. The first approach emphasizes the construction of relevant datasets for model fine-tuning. The second approach, in contrast, aims to fully exploit the inherent reasoning abilities of LLMs through in-context learning strategies. In this work, we introduce a novel tool invocation pipeline designed to control massive real-world APIs. This pipeline mirrors the human task-solving process, addressing c
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20449;&#24687;&#27969;&#30340;&#35270;&#35282;&#35299;&#37322;&#24182;&#24178;&#39044;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#20914;&#31361;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Pruning Head via PatH&#30340;&#26041;&#27861;&#26469;&#32531;&#35299;&#20914;&#31361;</title><link>https://arxiv.org/abs/2402.18154</link><description>&lt;p&gt;
&#20999;&#26029;&#22836;&#37096;&#32456;&#32467;&#20914;&#31361;&#65306;&#35299;&#37322;&#21644;&#32531;&#35299;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#20914;&#31361;&#30340;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Cutting Off the Head Ends the Conflict: A Mechanism for Interpreting and Mitigating Knowledge Conflicts in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18154
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20449;&#24687;&#27969;&#30340;&#35270;&#35282;&#35299;&#37322;&#24182;&#24178;&#39044;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#20914;&#31361;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Pruning Head via PatH&#30340;&#26041;&#27861;&#26469;&#32531;&#35299;&#20914;&#31361;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26816;&#32034;&#22686;&#24378;&#21644;&#24037;&#20855;&#22686;&#24378;&#23637;&#31034;&#20102;&#36890;&#36807;&#25552;&#20379;&#22806;&#37096;&#19978;&#19979;&#25991;&#26469;&#25193;&#23637;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#20869;&#37096;&#35760;&#24518;&#36793;&#30028;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20869;&#37096;&#35760;&#24518;&#21644;&#22806;&#37096;&#19978;&#19979;&#25991;&#19981;&#21487;&#36991;&#20813;&#22320;&#21457;&#29983;&#20914;&#31361;&#65292;&#23548;&#33268;LMs&#20869;&#37096;&#20986;&#29616;&#30693;&#35782;&#20914;&#31361;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#20449;&#24687;&#27969;&#30340;&#35270;&#35282;&#35299;&#37322;&#30693;&#35782;&#20914;&#31361;&#30340;&#26426;&#21046;&#65292;&#28982;&#21518;&#36890;&#36807;&#22312;&#20851;&#38190;&#28857;&#36827;&#34892;&#31934;&#30830;&#24178;&#39044;&#26469;&#32531;&#35299;&#20914;&#31361;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#21518;&#32493;&#23618;&#20013;&#26377;&#19968;&#20123;&#20855;&#26377;&#30456;&#21453;&#25928;&#26524;&#30340;&#27880;&#24847;&#21147;&#22836;&#65292;&#20854;&#20013;&#20869;&#23384;&#22836;&#21487;&#20197;&#20174;&#20869;&#37096;&#35760;&#24518;&#20013;&#21484;&#22238;&#30693;&#35782;&#65292;&#32780;&#19978;&#19979;&#25991;&#22836;&#21487;&#20197;&#20174;&#22806;&#37096;&#19978;&#19979;&#25991;&#20013;&#26816;&#32034;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;LMs&#20013;&#30693;&#35782;&#20914;&#31361;&#21457;&#29983;&#30340;&#20851;&#38190;&#28857;&#26159;&#20869;&#23384;&#22836;&#21644;&#19978;&#19979;&#25991;&#22836;&#25972;&#21512;&#19981;&#19968;&#33268;&#20449;&#24687;&#27969;&#30340;&#22320;&#26041;&#12290;&#21463;&#21040;&#36825;&#20123;&#35265;&#35299;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Pruning Head via PatH&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18154v1 Announce Type: cross  Abstract: Recently, retrieval augmentation and tool augmentation have demonstrated a remarkable capability to expand the internal memory boundaries of language models (LMs) by providing external context. However, internal memory and external context inevitably clash, leading to knowledge conflicts within LMs. In this paper, we aim to interpret the mechanism of knowledge conflicts through the lens of information flow, and then mitigate conflicts by precise interventions at the pivotal point. We find there are some attention heads with opposite effects in the later layers, where memory heads can recall knowledge from internal memory, and context heads can retrieve knowledge from external context. Moreover, we reveal that the pivotal point at which knowledge conflicts emerge in LMs is the integration of inconsistent information flows by memory heads and context heads. Inspired by the insights, we propose a novel method called Pruning Head via PatH 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InFO-RAG&#30340;&#26080;&#30417;&#30563;&#20449;&#24687;&#32454;&#21270;&#35757;&#32451;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#20013;&#30340;&#35282;&#33394;&#23450;&#20041;&#20026;&#8220;&#20449;&#24687;&#32454;&#21270;&#32773;&#8221;&#65292;&#24110;&#21161;&#27169;&#22411;&#26356;&#22909;&#22320;&#25972;&#21512;&#26816;&#32034;&#20449;&#24687;&#20197;&#29983;&#25104;&#26356;&#21152;&#31616;&#27905;&#12289;&#20934;&#30830;&#21644;&#23436;&#25972;&#30340;&#25991;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.18150</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#30417;&#30563;&#20449;&#24687;&#32454;&#21270;&#35757;&#32451;&#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Information Refinement Training of Large Language Models for Retrieval-Augmented Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InFO-RAG&#30340;&#26080;&#30417;&#30563;&#20449;&#24687;&#32454;&#21270;&#35757;&#32451;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#20013;&#30340;&#35282;&#33394;&#23450;&#20041;&#20026;&#8220;&#20449;&#24687;&#32454;&#21270;&#32773;&#8221;&#65292;&#24110;&#21161;&#27169;&#22411;&#26356;&#22909;&#22320;&#25972;&#21512;&#26816;&#32034;&#20449;&#24687;&#20197;&#29983;&#25104;&#26356;&#21152;&#31616;&#27905;&#12289;&#20934;&#30830;&#21644;&#23436;&#25972;&#30340;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#36890;&#36807;&#23558;&#26469;&#33258;&#26816;&#32034;&#30340;&#39069;&#22806;&#20449;&#24687;&#25972;&#21512;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#34920;&#26126;&#65292;LLMs&#22312;&#26377;&#25928;&#21033;&#29992;&#26816;&#32034;&#20449;&#24687;&#26041;&#38754;&#20173;&#28982;&#38754;&#20020;&#25361;&#25112;&#65292;&#26377;&#26102;&#20250;&#24573;&#35270;&#25110;&#34987;&#38169;&#35823;&#24341;&#23548;&#12290;&#20854;&#20851;&#38190;&#21407;&#22240;&#22312;&#20110;LLMs&#30340;&#35757;&#32451;&#27809;&#26377;&#28165;&#26224;&#22320;&#35753;LLMs&#23398;&#20250;&#22914;&#20309;&#21033;&#29992;&#20855;&#26377;&#19981;&#21516;&#36136;&#37327;&#30340;&#26816;&#32034;&#25991;&#26412;&#36755;&#20837;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35270;&#35282;&#65292;&#23558;LLMs&#22312;RAG&#20013;&#30340;&#35282;&#33394;&#35270;&#20026;&#8220;&#20449;&#24687;&#32454;&#21270;&#32773;&#8221;&#65292;&#36825;&#24847;&#21619;&#30528;&#26080;&#35770;&#26816;&#32034;&#25991;&#26412;&#30340;&#27491;&#30830;&#24615;&#12289;&#23436;&#25972;&#24615;&#25110;&#26377;&#29992;&#24615;&#22914;&#20309;&#65292;LLMs&#37117;&#33021;&#19968;&#33268;&#22320;&#25972;&#21512;&#26816;&#32034;&#25991;&#26412;&#20013;&#30340;&#30693;&#35782;&#21644;&#27169;&#22411;&#21442;&#25968;&#65292;&#29983;&#25104;&#27604;&#26816;&#32034;&#25991;&#26412;&#26356;&#31616;&#27905;&#12289;&#20934;&#30830;&#21644;&#23436;&#25972;&#30340;&#25991;&#26412;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InFO-RAG&#30340;&#20449;&#24687;&#32454;&#21270;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#20248;&#21270;LLMs&#29992;&#20110;RAG&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18150v1 Announce Type: cross  Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating additional information from retrieval. However, studies have shown that LLMs still face challenges in effectively using the retrieved information, even ignoring it or being misled by it. The key reason is that the training of LLMs does not clearly make LLMs learn how to utilize input retrieved texts with varied quality. In this paper, we propose a novel perspective that considers the role of LLMs in RAG as ``Information Refiner'', which means that regardless of correctness, completeness, or usefulness of retrieved texts, LLMs can consistently integrate knowledge within the retrieved texts and model parameters to generate the texts that are more concise, accurate, and complete than the retrieved texts. To this end, we propose an information refinement training method named InFO-RAG that optimizes LLMs for RAG in an unsupervised manner. InFO-RAG i
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#20449;&#24687;&#29942;&#39048;&#26799;&#24230;&#65288;IBG&#65289;&#35299;&#37322;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#21333;&#35789;&#23884;&#20837;&#35843;&#25972;&#20026;&#31616;&#26126;&#30340;&#20869;&#22312;&#32500;&#24230;&#26469;&#25552;&#39640;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.18145</link><description>&lt;p&gt;
&#36890;&#36807;&#20449;&#24687;&#29942;&#39048;&#23398;&#20064;&#20869;&#22312;&#32500;&#24230;&#65292;&#29992;&#20110;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Learning Intrinsic Dimension via Information Bottleneck for Explainable Aspect-based Sentiment Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18145
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#20449;&#24687;&#29942;&#39048;&#26799;&#24230;&#65288;IBG&#65289;&#35299;&#37322;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#21333;&#35789;&#23884;&#20837;&#35843;&#25972;&#20026;&#31616;&#26126;&#30340;&#20869;&#22312;&#32500;&#24230;&#26469;&#25552;&#39640;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26799;&#24230;&#30340;&#35299;&#37322;&#26041;&#27861;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#35299;&#37322;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#30340;&#31070;&#32463;&#27169;&#22411;&#65292;&#22240;&#20026;&#20854;&#39640;&#20445;&#30495;&#24230;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#36807;&#19968;&#31181;&#35268;&#33539;&#20989;&#25968;&#20351;&#29992;&#32500;&#24230;&#32423;&#26799;&#24230;&#20540;&#26469;&#30830;&#23450;&#21333;&#35789;&#32423;&#37325;&#35201;&#24615;&#65292;&#36890;&#24120;&#20551;&#23450;&#25152;&#26377;&#26799;&#24230;&#32500;&#24230;&#20855;&#26377;&#30456;&#31561;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#65288;ABSA&#65289;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#30340;&#21021;&#27493;&#30740;&#31350;&#34920;&#26126;&#21482;&#26377;&#29305;&#23450;&#30340;&#32500;&#24230;&#26159;&#30456;&#20851;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20449;&#24687;&#29942;&#39048;&#26799;&#24230;&#65288;IBG&#65289;&#35299;&#37322;&#26694;&#26550;&#29992;&#20110;ABSA&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#20449;&#24687;&#29942;&#39048;&#23558;&#21333;&#35789;&#23884;&#20837;&#35843;&#25972;&#20026;&#31616;&#26126;&#30340;&#20869;&#22312;&#32500;&#24230;&#65292;&#20445;&#30041;&#20851;&#38190;&#29305;&#24449;&#24182;&#30465;&#30053;&#26080;&#20851;&#20449;&#24687;&#12290;&#20840;&#38754;&#30340;&#27979;&#35797;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;IBG&#26041;&#27861;&#36890;&#36807;&#35782;&#21035;&#19982;&#24773;&#24863;&#30456;&#20851;&#30340;&#29305;&#24449;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18145v1 Announce Type: new  Abstract: Gradient-based explanation methods are increasingly used to interpret neural models in natural language processing (NLP) due to their high fidelity. Such methods determine word-level importance using dimension-level gradient values through a norm function, often presuming equal significance for all gradient dimensions. However, in the context of Aspect-based Sentiment Analysis (ABSA), our preliminary research suggests that only specific dimensions are pertinent. To address this, we propose the Information Bottleneck-based Gradient (\texttt{IBG}) explanation framework for ABSA. This framework leverages an information bottleneck to refine word embeddings into a concise intrinsic dimension, maintaining essential features and omitting unrelated information. Comprehensive tests show that our \texttt{IBG} approach considerably improves both the models' performance and interpretability by identifying sentiment-aware features.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CARE CA&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#36890;&#36807;&#32467;&#21512;&#26174;&#24335;&#22240;&#26524;&#26816;&#27979;&#27169;&#22359;&#21644;&#21453;&#20107;&#23454;&#38472;&#36848;&#12289;&#20197;&#21450;&#38544;&#21547;&#22240;&#26524;&#26816;&#27979;&#27169;&#22359;&#65292;&#26088;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.18139</link><description>&lt;p&gt;
&#22240;&#26524;&#20851;&#31995;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30495;&#27491;&#29702;&#35299;&#22240;&#26524;&#20851;&#31995;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Cause and Effect: Can Large Language Models Truly Understand Causality?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CARE CA&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#36890;&#36807;&#32467;&#21512;&#26174;&#24335;&#22240;&#26524;&#26816;&#27979;&#27169;&#22359;&#21644;&#21453;&#20107;&#23454;&#38472;&#36848;&#12289;&#20197;&#21450;&#38544;&#21547;&#22240;&#26524;&#26816;&#27979;&#27169;&#22359;&#65292;&#26088;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20852;&#36215;&#65292;&#29702;&#35299;&#23427;&#20204;&#22312;&#35299;&#35835;&#21644;&#35299;&#37322;&#35821;&#35328;&#25152;&#28041;&#21450;&#30340;&#22797;&#26434;&#22240;&#26524;&#20851;&#31995;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#20351;&#29992;&#26126;&#30830;&#25110;&#38544;&#21547;&#30340;&#22240;&#26524;&#25512;&#29702;&#65292;&#28982;&#32780;&#36843;&#20999;&#38656;&#35201;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#65292;&#23558;&#20004;&#32773;&#32467;&#21512;&#36215;&#26469;&#26356;&#26377;&#25928;&#22320;&#22788;&#29702;&#21508;&#31181;&#22240;&#26524;&#20851;&#31995;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;&#20855;&#26377;&#21453;&#20107;&#23454;&#20998;&#26512;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#25512;&#29702;&#22686;&#24378;&#65288;CARE CA&#65289;&#26694;&#26550;&#65292;&#20197;&#22686;&#24378;&#22240;&#26524;&#25512;&#29702;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#23558; ConceptNet &#21644;&#21453;&#20107;&#23454;&#38472;&#36848;&#20013;&#30340;&#26126;&#30830;&#22240;&#26524;&#26816;&#27979;&#27169;&#22359;&#20197;&#21450;&#36890;&#36807;LLMs&#36827;&#34892;&#30340;&#38544;&#21547;&#22240;&#26524;&#26816;&#27979;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#19968;&#23618;&#21453;&#20107;&#23454;&#35299;&#37322;&#36827;&#19968;&#27493;&#31361;&#20986;LLMs&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#29702;&#35299;&#12290;ConceptNet &#20013;&#30340;&#30693;&#35782;&#25552;&#39640;&#20102;&#22810;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18139v1 Announce Type: cross  Abstract: With the rise of Large Language Models(LLMs), it has become crucial to understand their capabilities and limitations in deciphering and explaining the complex web of causal relationships that language entails. Current methods use either explicit or implicit causal reasoning, yet there is a strong need for a unified approach combining both to tackle a wide array of causal relationships more effectively. This research proposes a novel architecture called Context Aware Reasoning Enhancement with Counterfactual Analysis(CARE CA) framework to enhance causal reasoning and explainability. The proposed framework incorporates an explicit causal detection module with ConceptNet and counterfactual statements, as well as implicit causal detection through LLMs. Our framework goes one step further with a layer of counterfactual explanations to accentuate LLMs understanding of causality. The knowledge from ConceptNet enhances the performance of multi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; DecisionNCE &#26694;&#26550;&#65292;&#36890;&#36807;&#38544;&#24335;&#20559;&#22909;&#23398;&#20064;&#23454;&#20307;&#22810;&#27169;&#24577;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#25552;&#21462;&#20219;&#21153;&#36827;&#23637;&#20449;&#24687;&#21644;&#19982;&#35821;&#35328;&#25351;&#20196;&#23545;&#40784;&#30340;&#26377;&#25928;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.18137</link><description>&lt;p&gt;
DecisionNCE: &#36890;&#36807;&#38544;&#24335;&#20559;&#22909;&#23398;&#20064;&#23454;&#20307;&#22810;&#27169;&#24577;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
DecisionNCE: Embodied Multimodal Representations via Implicit Preference Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18137
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; DecisionNCE &#26694;&#26550;&#65292;&#36890;&#36807;&#38544;&#24335;&#20559;&#22909;&#23398;&#20064;&#23454;&#20307;&#22810;&#27169;&#24577;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#25552;&#21462;&#20219;&#21153;&#36827;&#23637;&#20449;&#24687;&#21644;&#19982;&#35821;&#35328;&#25351;&#20196;&#23545;&#40784;&#30340;&#26377;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#24050;&#34987;&#35777;&#26126;&#26159;&#33258;&#20027;&#26426;&#22120;&#20154;&#20013;&#34920;&#31034;&#23398;&#20064;&#30340;&#19977;&#22823;&#30446;&#26631;&#65306;1&#65289;&#25552;&#21462;&#23616;&#37096;&#21644;&#20840;&#23616;&#20219;&#21153;&#36827;&#23637;&#20449;&#24687;&#65307;2&#65289;&#24378;&#21270;&#35270;&#35273;&#34920;&#31034;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#65307;3&#65289;&#25429;&#33719;&#36712;&#36857;&#32423;&#35821;&#35328;&#22522;&#30784;&#30340;&#26377;&#25928;&#31574;&#30053;&#12290;&#22823;&#37096;&#20998;&#24050;&#26377;&#26041;&#27861;&#36890;&#36807;&#19981;&#21516;&#30340;&#30446;&#26631;&#26469;&#22788;&#29702;&#36825;&#20123;&#38382;&#39064;&#65292;&#24448;&#24448;&#23548;&#33268;&#27425;&#20248;&#35299;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#32479;&#19968;&#30446;&#26631;&#65292;&#21487;&#20197;&#21516;&#26102;&#20174;&#22270;&#20687;&#24207;&#21015;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#20219;&#21153;&#36827;&#23637;&#20449;&#24687;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#35821;&#35328;&#25351;&#20196;&#26080;&#32541;&#23545;&#40784;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#38544;&#24335;&#20559;&#22909;&#65292;&#22312;&#35270;&#35273;&#36712;&#36857;&#19982;&#20854;&#23545;&#24212;&#30340;&#35821;&#35328;&#25351;&#20196;&#30456;&#27604;&#19981;&#21305;&#37197;&#23545;&#26356;&#22909;&#22320;&#23545;&#40784;&#26102;&#65292;&#27969;&#34892;&#30340; Bradley-Terry &#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#36866;&#24403;&#30340;&#22870;&#21169;&#37325;&#26032;&#21442;&#25968;&#21270;&#32780;&#21464;&#20026;&#34920;&#31034;&#23398;&#20064;&#12290;&#32467;&#26524;&#20135;&#29983;&#30340; DecisionNCE &#26694;&#26550;&#65292;&#31867;&#20284;&#20110; InfoNC
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18137v1 Announce Type: cross  Abstract: Multimodal pretraining has emerged as an effective strategy for the trinity of goals of representation learning in autonomous robots: 1) extracting both local and global task progression information; 2) enforcing temporal consistency of visual representation; 3) capturing trajectory-level language grounding. Most existing methods approach these via separate objectives, which often reach sub-optimal solutions. In this paper, we propose a universal unified objective that can simultaneously extract meaningful task progression information from image sequences and seamlessly align them with language instructions. We discover that via implicit preferences, where a visual trajectory inherently aligns better with its corresponding language instruction than mismatched pairs, the popular Bradley-Terry model can transform into representation learning through proper reward reparameterizations. The resulted framework, DecisionNCE, mirrors an InfoNC
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22235;&#31181;&#35821;&#35328;&#27169;&#22411;&#22312;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#27688;&#22522;&#37240;&#35821;&#35328;&#20013;&#30340;&#36866;&#24212;&#24615;&#12289;&#26377;&#25928;&#24615;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#20026;&#26410;&#26469;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#36827;&#23637;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;</title><link>https://arxiv.org/abs/2402.18121</link><description>&lt;p&gt;
&#25327;&#25937;&#33521;&#38596;&#20234;&#24052;&#20160;&#30340;&#36951;&#20135;&#65306;&#35780;&#20272;&#22235;&#31181;&#35821;&#35328;&#27169;&#22411;&#22312;&#27688;&#22522;&#37240;&#35821;&#35328;&#20013;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Saving the legacy of Hero Ibash: Evaluating Four Language Models for Aminoacian
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22235;&#31181;&#35821;&#35328;&#27169;&#22411;&#22312;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#27688;&#22522;&#37240;&#35821;&#35328;&#20013;&#30340;&#36866;&#24212;&#24615;&#12289;&#26377;&#25928;&#24615;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#20026;&#26410;&#26469;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#36827;&#23637;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22235;&#31181;&#21069;&#27839;&#35821;&#35328;&#27169;&#22411;&#22312;&#26410;&#24320;&#21457;&#30340;&#27688;&#22522;&#37240;&#35821;&#35328;&#20013;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#35780;&#20272;&#65292;&#26412;&#30740;&#31350;&#23457;&#26597;&#20102;&#23427;&#20204;&#22312;&#25991;&#26412;&#29983;&#25104;&#12289;&#35821;&#20041;&#36830;&#36143;&#24615;&#21644;&#19978;&#19979;&#25991;&#29702;&#35299;&#26041;&#38754;&#30340;&#36866;&#24212;&#24615;&#65292;&#26377;&#25928;&#24615;&#21644;&#23616;&#38480;&#24615;&#12290;&#25581;&#31034;&#36825;&#20123;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#34920;&#29616;&#65292;&#20026;&#24357;&#21512;&#35821;&#35328;&#24046;&#36317;&#24320;&#36767;&#20102;&#36947;&#36335;&#12290;&#36890;&#36807;&#25552;&#20379;&#22522;&#20934;&#21644;&#29702;&#35299;&#25361;&#25112;&#65292;&#20026;&#26410;&#26469;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#36827;&#23637;&#22880;&#23450;&#20102;&#22522;&#30784;&#65292;&#26088;&#22312;&#25552;&#21319;&#35821;&#35328;&#27169;&#22411;&#22312;&#31867;&#20284;&#35821;&#35328;&#29615;&#22659;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#26631;&#24535;&#30528;&#35821;&#35328;&#25216;&#26415;&#21253;&#23481;&#24615;&#21644;&#36827;&#27493;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18121v1 Announce Type: cross  Abstract: This study assesses four cutting-edge language models in the underexplored Aminoacian language. Through evaluation, it scrutinizes their adaptability, effectiveness, and limitations in text generation, semantic coherence, and contextual understanding. Uncovering insights into these models' performance in a low-resourced language, this research pioneers pathways to bridge linguistic gaps. By offering benchmarks and understanding challenges, it lays groundwork for future advancements in natural language processing, aiming to elevate the applicability of language models in similar linguistic landscapes, marking a significant step toward inclusivity and progress in language technology.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25506;&#32034;&#22810;&#35821;&#35328;&#20154;&#31867;&#20215;&#20540;&#35266;&#24565;&#65292;&#25105;&#20204;&#35777;&#23454;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#22810;&#35821;&#35328;&#20154;&#31867;&#20215;&#20540;&#65292;&#24182;&#25581;&#31034;&#20102;&#20215;&#20540;&#23545;&#40784;&#33021;&#21147;&#21487;&#20197;&#34987;&#36328;&#35821;&#35328;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.18120</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25506;&#32034;&#22810;&#35821;&#35328;&#20154;&#31867;&#20215;&#20540;&#35266;&#24565;&#65306;&#20215;&#20540;&#35266;&#40784;&#25972;&#24615;&#12289;&#36328;&#35821;&#35328;&#36716;&#31227;&#24615;&#21644;&#21487;&#25511;&#24615;&#26159;&#21542;&#20855;&#26377;&#19968;&#33268;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
Exploring Multilingual Human Value Concepts in Large Language Models: Is Value Alignment Consistent, Transferable and Controllable across Languages?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18120
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25506;&#32034;&#22810;&#35821;&#35328;&#20154;&#31867;&#20215;&#20540;&#35266;&#24565;&#65292;&#25105;&#20204;&#35777;&#23454;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#22810;&#35821;&#35328;&#20154;&#31867;&#20215;&#20540;&#65292;&#24182;&#25581;&#31034;&#20102;&#20215;&#20540;&#23545;&#40784;&#33021;&#21147;&#21487;&#20197;&#34987;&#36328;&#35821;&#35328;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#22312;&#34920;&#31034;&#24037;&#31243;&#39046;&#22495;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;LLMs&#22312;&#20854;&#34920;&#31034;&#31354;&#38388;&#20013;&#32534;&#30721;&#27010;&#24565;&#65292;&#20027;&#35201;&#22260;&#32469;&#33521;&#35821;&#23637;&#24320;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#19968;&#29702;&#24565;&#25193;&#23637;&#21040;&#22810;&#35821;&#22659;&#22330;&#26223;&#65292;&#28145;&#20837;&#25506;&#35752;LLMs&#20013;&#30340;&#22810;&#35821;&#35328;&#20154;&#31867;&#20215;&#20540;&#27010;&#24565;&#12290;&#36890;&#36807;&#25105;&#20204;&#23545;7&#31181;&#20154;&#31867;&#20215;&#20540;&#12289;16&#31181;&#35821;&#35328;&#20197;&#21450;3&#20010;&#20855;&#26377;&#26126;&#26174;&#22810;&#35821;&#29305;&#24615;&#30340;LLM&#31995;&#21015;&#36827;&#34892;&#30340;&#20840;&#38754;&#25506;&#32034;&#65292;&#25105;&#20204;&#20174;&#23454;&#35777;&#35282;&#24230;&#35777;&#23454;&#20102;LLMs&#20013;&#23384;&#22312;&#22810;&#35821;&#35328;&#20154;&#31867;&#20215;&#20540;&#35266;&#24565;&#12290;&#23545;&#36825;&#20123;&#27010;&#24565;&#30340;&#36328;&#35821;&#35328;&#20998;&#26512;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#30001;&#20110;&#35821;&#35328;&#36164;&#28304;&#24046;&#24322;&#32780;&#20135;&#29983;&#30340;3&#20010;&#29305;&#24449;&#65306;&#36328;&#35821;&#35328;&#19981;&#19968;&#33268;&#24615;&#12289;&#25197;&#26354;&#30340;&#35821;&#35328;&#20851;&#31995;&#20197;&#21450;&#39640;&#36164;&#28304;&#35821;&#35328;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20043;&#38388;&#22312;&#20154;&#31867;&#20215;&#20540;&#27010;&#24565;&#26041;&#38754;&#30340;&#21333;&#21521;&#36328;&#35821;&#35328;&#36716;&#31227;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#36890;&#36807;&#21033;&#29992;&#20027;&#23548;&#35821;&#35328;&#20316;&#20026;&#20449;&#24687;&#28304;&#23454;&#29616;&#23545;LLMs&#20215;&#20540;&#35266;&#40784;&#25972;&#24615;&#30340;&#36328;&#35821;&#35328;&#25511;&#21046;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18120v1 Announce Type: new  Abstract: Prior research in representation engineering has revealed that LLMs encode concepts within their representation spaces, predominantly centered around English. In this study, we extend this philosophy to a multilingual scenario, delving into multilingual human value concepts in LLMs. Through our comprehensive exploration covering 7 types of human values, 16 languages and 3 LLM series with distinct multilinguality, we empirically substantiate the existence of multilingual human values in LLMs. Further cross-lingual analysis on these concepts discloses 3 traits arising from language resource disparities: cross-lingual inconsistency, distorted linguistic relationships, and unidirectional cross-lingual transfer between high- and low-resource languages, all in terms of human value concepts. Additionally, we validate the feasibility of cross-lingual control over value alignment capabilities of LLMs, leveraging the dominant language as a source 
&lt;/p&gt;</description></item><item><title>UniVS&#25552;&#20986;&#20102;&#20351;&#29992;&#25552;&#31034;&#20316;&#20026;&#26597;&#35810;&#30340;&#32479;&#19968;&#35270;&#39057;&#20998;&#21106;&#26550;&#26500;&#65292;&#36890;&#36807;&#24179;&#22343;&#21270;&#21069;&#19968;&#24103;&#20013;&#30446;&#26631;&#30340;&#25552;&#31034;&#29305;&#24449;&#26469;&#35299;&#30721;&#25513;&#30721;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#30446;&#26631;&#30340;&#25552;&#31034;&#20132;&#21449;&#27880;&#24847;&#23618;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#32479;&#19968;&#35270;&#39057;&#20998;&#21106;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.18115</link><description>&lt;p&gt;
UniVS&#65306;&#20351;&#29992;&#25552;&#31034;&#20316;&#20026;&#26597;&#35810;&#30340;&#32479;&#19968;&#21644;&#36890;&#29992;&#35270;&#39057;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
UniVS: Unified and Universal Video Segmentation with Prompts as Queries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18115
&lt;/p&gt;
&lt;p&gt;
UniVS&#25552;&#20986;&#20102;&#20351;&#29992;&#25552;&#31034;&#20316;&#20026;&#26597;&#35810;&#30340;&#32479;&#19968;&#35270;&#39057;&#20998;&#21106;&#26550;&#26500;&#65292;&#36890;&#36807;&#24179;&#22343;&#21270;&#21069;&#19968;&#24103;&#20013;&#30446;&#26631;&#30340;&#25552;&#31034;&#29305;&#24449;&#26469;&#35299;&#30721;&#25513;&#30721;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#30446;&#26631;&#30340;&#25552;&#31034;&#20132;&#21449;&#27880;&#24847;&#23618;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#32479;&#19968;&#35270;&#39057;&#20998;&#21106;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#32479;&#19968;&#22270;&#20687;&#20998;&#21106;&#65288;IS&#65289;&#21462;&#24471;&#20102;&#26368;&#26032;&#36827;&#23637;&#65292;&#20294;&#24320;&#21457;&#32479;&#19968;&#30340;&#35270;&#39057;&#20998;&#21106;&#65288;VS&#65289;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#39033;&#25361;&#25112;&#12290;&#36825;&#20027;&#35201;&#26159;&#22240;&#20026;&#36890;&#29992;&#31867;&#21035;&#25351;&#23450;&#30340;VS&#20219;&#21153;&#38656;&#35201;&#26816;&#27979;&#25152;&#26377;&#23545;&#35937;&#24182;&#36319;&#36394;&#23427;&#20204;&#36328;&#36830;&#32493;&#24103;&#65292;&#32780;&#30001;&#25552;&#31034;&#24341;&#23548;&#30340;VS&#20219;&#21153;&#38656;&#35201;&#22312;&#25972;&#20010;&#35270;&#39057;&#20013;&#37325;&#26032;&#35782;&#21035;&#30446;&#26631;&#24182;&#20351;&#29992;&#35270;&#35273;/&#25991;&#26412;&#25552;&#31034;&#65292;&#20351;&#24471;&#29992;&#30456;&#21516;&#26550;&#26500;&#22788;&#29702;&#19981;&#21516;&#20219;&#21153;&#21464;&#24471;&#22256;&#38590;&#12290;&#25105;&#20204;&#35797;&#22270;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#19968;VS&#26550;&#26500;&#65292;&#21363;UniVS&#65292;&#36890;&#36807;&#20351;&#29992;&#25552;&#31034;&#20316;&#20026;&#26597;&#35810;&#12290;UniVS&#23558;&#21069;&#19968;&#24103;&#20013;&#30446;&#26631;&#30340;&#25552;&#31034;&#29305;&#24449;&#20316;&#20026;&#20854;&#21021;&#22987;&#26597;&#35810;&#24179;&#22343;&#21270;&#65292;&#20197;&#26126;&#30830;&#35299;&#30721;&#25513;&#30721;&#65292;&#24182;&#22312;&#25513;&#30721;&#35299;&#30721;&#22120;&#20013;&#24341;&#20837;&#22522;&#20110;&#30446;&#26631;&#30340;&#25552;&#31034;&#20132;&#21449;&#27880;&#24847;&#23618;&#65292;&#20197;&#22312;&#20869;&#23384;&#27744;&#20013;&#25972;&#21512;&#25552;&#31034;&#29305;&#24449;&#12290;&#36890;&#36807;&#23558;&#20808;&#21069;&#24103;&#20013;&#30340;&#23454;&#20307;&#30340;&#39044;&#27979;&#25513;&#30721;&#20316;&#20026;&#23427;&#20204;&#30340;&#35270;&#35273;&#25552;&#31034;&#65292;UniVS&#23558;&#19981;&#21516;&#30340;VS&#20219;&#21153;&#36716;&#25442;&#25104;&#36890;&#29992;&#30340;VS&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18115v1 Announce Type: cross  Abstract: Despite the recent advances in unified image segmentation (IS), developing a unified video segmentation (VS) model remains a challenge. This is mainly because generic category-specified VS tasks need to detect all objects and track them across consecutive frames, while prompt-guided VS tasks require re-identifying the target with visual/text prompts throughout the entire video, making it hard to handle the different tasks with the same architecture. We make an attempt to address these issues and present a novel unified VS architecture, namely UniVS, by using prompts as queries. UniVS averages the prompt features of the target from previous frames as its initial query to explicitly decode masks, and introduces a target-wise prompt cross-attention layer in the mask decoder to integrate prompt features in the memory pool. By taking the predicted masks of entities from previous frames as their visual prompts, UniVS converts different VS ta
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20998;&#37197;&#20026;&#8220;&#25945;&#24072;&#8221;&#29983;&#25104;&#25968;&#25454;&#21644;&#8220;&#35780;&#35770;&#23478;&#8221;&#35780;&#20272;&#23398;&#29983;&#34920;&#29616;&#30340;&#21452;&#37325;&#35282;&#33394;&#65292;&#30740;&#31350;&#34920;&#26126;&#36825;&#31181;&#22522;&#20110;&#21453;&#39304;&#30340;&#26041;&#27861;&#22312;&#24189;&#40664;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.18113</link><description>&lt;p&gt;
&#23567;&#32780;&#26377;&#36259;&#65306;&#19968;&#31181;&#22522;&#20110;&#21453;&#39304;&#30340;&#24189;&#40664;&#31934;&#39311;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Small But Funny: A Feedback-Driven Approach to Humor Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18113
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20998;&#37197;&#20026;&#8220;&#25945;&#24072;&#8221;&#29983;&#25104;&#25968;&#25454;&#21644;&#8220;&#35780;&#35770;&#23478;&#8221;&#35780;&#20272;&#23398;&#29983;&#34920;&#29616;&#30340;&#21452;&#37325;&#35282;&#33394;&#65292;&#30740;&#31350;&#34920;&#26126;&#36825;&#31181;&#22522;&#20110;&#21453;&#39304;&#30340;&#26041;&#27861;&#22312;&#24189;&#40664;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#25581;&#31034;&#20102;&#26377;&#28508;&#21147;&#30340;&#35821;&#35328;&#29983;&#25104;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#25191;&#34892;&#35832;&#22914;&#22797;&#26434;&#25512;&#29702;&#21644;&#21019;&#24847;&#20889;&#20316;&#20043;&#31867;&#30340;&#20219;&#21153;&#26041;&#38754;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#27169;&#20223;&#25945;&#24072;&#22238;&#31572;&#30340;&#26041;&#24335;&#36827;&#34892;&#31934;&#39311;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#23558;LLMs&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#26356;&#26131;&#35775;&#38382;&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#20013;&#12290;&#34429;&#28982;&#36825;&#23545;&#20110;&#31616;&#21333;&#20219;&#21153;&#25928;&#26524;&#24456;&#22909;&#65292;&#20294;&#22312;&#38656;&#35201;&#22797;&#26434;&#35821;&#35328;&#29702;&#35299;&#21644;&#21019;&#36896;&#21147;&#30340;&#20219;&#21153;&#19978;&#23384;&#22312;&#23454;&#36136;&#24615;&#30340;&#34920;&#29616;&#24046;&#36317;&#65292;&#27604;&#22914;&#24189;&#40664;&#29983;&#25104;&#12290;&#25105;&#20204;&#20551;&#35774;&#36825;&#31181;&#24046;&#36317;&#21487;&#33021;&#28304;&#33258;&#20110;&#21019;&#36896;&#24615;&#20219;&#21153;&#21487;&#33021;&#21333;&#20973;&#27169;&#20223;&#23398;&#20064;&#26159;&#24456;&#38590;&#30340;&#65292;&#24182;&#25506;&#35752;&#19968;&#31181;&#28041;&#21450;&#21040;&#25945;&#24072;&#39069;&#22806;&#25351;&#23548;&#30340;&#26041;&#27861;&#65292;&#33021;&#21542;&#20135;&#29983;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;LLM&#20998;&#37197;&#21452;&#37325;&#35282;&#33394;&#30340;&#25928;&#26524;-&#20316;&#20026;&#29983;&#25104;&#25968;&#25454;&#30340;&#8220;&#25945;&#24072;&#8221;&#65292;&#20197;&#21450;&#20316;&#20026;&#35780;&#20272;&#23398;&#29983;&#34920;&#29616;&#30340;&#8220;&#35780;&#35770;&#23478;&#8221;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#24189;&#40664;&#29983;&#25104;&#20219;&#21153;&#19978;&#30830;&#23454;&#20135;&#29983;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18113v1 Announce Type: cross  Abstract: The emergence of Large Language Models (LLMs) has brought to light promising language generation capabilities, particularly in performing tasks like complex reasoning and creative writing. Consequently, distillation through imitation of teacher responses has emerged as a popular technique to transfer knowledge from LLMs to more accessible, Small Language Models (SLMs). While this works well for simpler tasks, there is a substantial performance gap on tasks requiring intricate language comprehension and creativity, such as humor generation. We hypothesize that this gap may stem from the fact that creative tasks might be hard to learn by imitation alone and explore whether an approach, involving supplementary guidance from the teacher, could yield higher performance. To address this, we study the effect of assigning a dual role to the LLM - as a "teacher" generating data, as well as a "critic" evaluating the student's performance. Our ex
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#20102;&#19968;&#31181;&#26085;&#26412;&#32972;&#26223;&#19979;&#20154;&#31867;&#35780;&#20272;&#26041;&#27861;&#20013;&#26368;&#20808;&#36827;&#35821;&#27861;&#38169;&#35823;&#26816;&#27979;&#21644;&#26657;&#27491;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#22312;&#38169;&#35823;&#26657;&#27491;&#26041;&#38754;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#20445;&#23432;&#24615;&#65292;&#22312;&#38169;&#35823;&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#20986;&#39640;&#31934;&#30830;&#24230;&#21644;&#35843;&#25972;&#21484;&#22238;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.18101</link><description>&lt;p&gt;
&#35780;&#20272;&#35821;&#27861;&#38169;&#35823;&#26657;&#27491;&#30340;&#26377;&#25928;&#24615;&#65306;&#22312;&#26085;&#26412;&#32972;&#26223;&#19979;&#30340;&#20154;&#31867;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Assessing the Efficacy of Grammar Error Correction: A Human Evaluation Approach in the Japanese Context
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18101
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20102;&#19968;&#31181;&#26085;&#26412;&#32972;&#26223;&#19979;&#20154;&#31867;&#35780;&#20272;&#26041;&#27861;&#20013;&#26368;&#20808;&#36827;&#35821;&#27861;&#38169;&#35823;&#26816;&#27979;&#21644;&#26657;&#27491;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#22312;&#38169;&#35823;&#26657;&#27491;&#26041;&#38754;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#20445;&#23432;&#24615;&#65292;&#22312;&#38169;&#35823;&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#20986;&#39640;&#31934;&#30830;&#24230;&#21644;&#35843;&#25972;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#33258;&#21160;&#27880;&#37322;&#24037;&#20855;&#21253;ERRANT&#65292;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;&#24207;&#21015;&#26631;&#35760;&#35821;&#27861;&#38169;&#35823;&#26816;&#27979;&#21644;&#26657;&#27491;&#27169;&#22411;&#65288;SeqTagger&#65289;&#22312;&#26085;&#26412;&#22823;&#23398;&#29983;&#20889;&#20316;&#26679;&#26412;&#19978;&#30340;&#34920;&#29616;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;SeqTagger&#30340;&#24615;&#33021;&#19982;&#20154;&#31867;&#19987;&#23478;&#26657;&#27491;&#20316;&#20026;&#22522;&#20934;&#26469;&#35780;&#20272;&#38169;&#35823;&#26657;&#27491;&#30340;&#24615;&#33021;&#12290;&#28982;&#21518;&#37319;&#29992;&#20154;&#24037;&#26631;&#27880;&#26041;&#27861;&#26469;&#35780;&#20272;Seqtagger&#22312;&#38169;&#35823;&#26816;&#27979;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#20351;&#29992;&#20889;&#20316;&#25968;&#25454;&#38598;&#30340;&#19968;&#20010;&#23376;&#38598;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#25972;&#20010;&#25968;&#25454;&#38598;&#19978;&#65292;&#38169;&#35823;&#26657;&#27491;&#30340;&#31934;&#30830;&#24230;&#20026;63.66%&#65292;&#21484;&#22238;&#29575;&#20026;20.19%&#12290;&#23545;&#20110;&#23376;&#38598;&#65292;&#22312;&#25163;&#21160;&#25490;&#38500;&#20102;&#35821;&#20041;&#21644;&#26426;&#26800;&#38169;&#35823;&#31561;&#19981;&#30456;&#20851;&#38169;&#35823;&#21518;&#65292;&#27169;&#22411;&#22312;&#38169;&#35823;&#26816;&#27979;&#26041;&#38754;&#26174;&#31034;&#20986;97.98%&#30340;&#35843;&#25972;&#31934;&#30830;&#24230;&#21644;42.98%&#30340;&#35843;&#25972;&#21484;&#22238;&#29575;&#65292;&#34920;&#26126;&#27169;&#22411;&#20855;&#26377;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#20063;&#34920;&#29616;&#20986;&#20445;&#23432;&#24615;&#12290;&#23545;&#27169;&#22411;&#26410;&#26816;&#27979;&#21040;&#30340;&#38169;&#35823;&#36827;&#34892;&#30340;&#20027;&#39064;&#20998;&#26512;&#25581;&#31034;&#20102;&#38480;&#23450;&#35789;&#21644;&#20896;&#35789;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18101v1 Announce Type: new  Abstract: In this study, we evaluated the performance of the state-of-the-art sequence tagging grammar error detection and correction model (SeqTagger) using Japanese university students' writing samples. With an automatic annotation toolkit, ERRANT, we first evaluated SeqTagger's performance on error correction with human expert correction as the benchmark. Then a human-annotated approach was adopted to evaluate Seqtagger's performance in error detection using a subset of the writing dataset. Results indicated a precision of 63.66% and a recall of 20.19% for error correction in the full dataset. For the subset, after manual exclusion of irrelevant errors such as semantic and mechanical ones, the model shows an adjusted precision of 97.98% and an adjusted recall of 42.98% for error detection, indicating the model's high accuracy but also its conservativeness. Thematic analysis on errors undetected by the model revealed that determiners and article
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;Layer-wise Scalable Adapter&#31574;&#30053;MedLaSA&#65292;&#29992;&#20110;&#32534;&#36753;&#21307;&#23398;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#31934;&#30830;&#20462;&#25913;&#21307;&#23398;&#30693;&#35782;&#24182;&#35299;&#37322;&#20107;&#23454;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#26041;&#27861;&#22312;&#21307;&#23398;&#30693;&#35782;&#29305;&#27530;&#21270;&#21644;&#22797;&#26434;&#24615;&#26041;&#38754;&#30340;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2402.18099</link><description>&lt;p&gt;
&#32534;&#36753;&#21307;&#23398;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20107;&#23454;&#30693;&#35782;&#21644;&#35299;&#37322;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Editing Factual Knowledge and Explanatory Ability of Medical Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18099
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;Layer-wise Scalable Adapter&#31574;&#30053;MedLaSA&#65292;&#29992;&#20110;&#32534;&#36753;&#21307;&#23398;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#31934;&#30830;&#20462;&#25913;&#21307;&#23398;&#30693;&#35782;&#24182;&#35299;&#37322;&#20107;&#23454;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#26041;&#27861;&#22312;&#21307;&#23398;&#30693;&#35782;&#29305;&#27530;&#21270;&#21644;&#22797;&#26434;&#24615;&#26041;&#38754;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#26088;&#22312;&#31934;&#30830;&#20462;&#25913;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#29305;&#23450;&#30693;&#35782;&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#19981;&#30456;&#20851;&#30340;&#30693;&#35782;&#19981;&#21464;&#12290;&#24050;&#32463;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#35299;&#20915;LLMs&#20013;&#30340;&#24187;&#35273;&#21644;&#36807;&#26102;&#38382;&#39064;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;&#22240;&#27492;&#65292;&#23427;&#21487;&#20197;&#25552;&#39640;LLMs&#22312;&#35768;&#22810;&#20851;&#38190;&#39046;&#22495;&#65288;&#20363;&#22914;&#21307;&#23398;&#39046;&#22495;&#65289;&#20013;&#30340;&#24212;&#29992;&#65292;&#20854;&#20013;&#24187;&#35273;&#26159;&#19981;&#21487;&#23481;&#24525;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20004;&#39033;&#27169;&#22411;&#32534;&#36753;&#30740;&#31350;&#65292;&#24182;&#22312;&#21307;&#23398;&#39046;&#22495;&#39564;&#35777;&#23427;&#20204;&#65306;&#65288;1&#65289;&#30452;&#25509;&#32534;&#36753;&#21307;&#23398;&#20107;&#23454;&#30693;&#35782;&#21644;&#65288;2&#65289;&#32534;&#36753;&#23545;&#20107;&#23454;&#30340;&#35299;&#37322;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#24403;&#21069;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#22312;&#21307;&#23398;&#30693;&#35782;&#30340;&#29305;&#27530;&#21270;&#21644;&#22797;&#26434;&#24615;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MedLaSA&#65292;&#19968;&#31181;&#26032;&#22411;&#30340;&#36866;&#29992;&#20110;&#21307;&#23398;&#27169;&#22411;&#32534;&#36753;&#30340;&#20998;&#23618;&#21487;&#25193;&#23637;&#36866;&#37197;&#22120;&#31574;&#30053;&#12290;&#23427;&#37319;&#29992;&#22240;&#26524;&#36861;&#36394;&#26469;&#35782;&#21035;&#31070;&#32463;&#20803;&#20013;&#30693;&#35782;&#30340;&#31934;&#30830;&#20301;&#32622;&#65292;&#28982;&#21518;&#23558;&#21487;&#25193;&#23637;&#36866;&#37197;&#22120;&#24341;&#20837;&#21040;LLMs&#30340;&#23494;&#38598;&#23618;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18099v1 Announce Type: cross  Abstract: Model editing aims to precisely modify the behaviours of large language models (LLMs) on specific knowledge while keeping irrelevant knowledge unchanged. It has been proven effective in resolving hallucination and out-of-date issues in LLMs. As a result, it can boost the application of LLMs in many critical domains (e.g., medical domain), where the hallucination is not tolerable. In this paper, we propose two model editing studies and validate them in the medical domain: (1) directly editing the factual medical knowledge and (2) editing the explanations to facts. Meanwhile, we observed that current model editing methods struggle with the specialization and complexity of medical knowledge. Therefore, we propose MedLaSA, a novel Layer-wise Scalable Adapter strategy for medical model editing. It employs causal tracing to identify the precise location of knowledge in neurons and then introduces scalable adapters into the dense layers of LL
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27169;&#24577;&#36755;&#20837;&#21644;&#22522;&#20110;&#20154;&#31867;&#21453;&#39304;&#30340;&#26694;&#26550;&#35757;&#32451;&#30340;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;Polos&#65292;&#26088;&#22312;&#26377;&#25928;&#24320;&#21457;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.18091</link><description>&lt;p&gt;
Polos&#65306;&#20174;&#20154;&#31867;&#21453;&#39304;&#23398;&#20064;&#30340;&#22810;&#27169;&#24335;&#24230;&#37327;&#29992;&#20110;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Polos: Multimodal Metric Learning from Human Feedback for Image Captioning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18091
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27169;&#24577;&#36755;&#20837;&#21644;&#22522;&#20110;&#20154;&#31867;&#21453;&#39304;&#30340;&#26694;&#26550;&#35757;&#32451;&#30340;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;Polos&#65292;&#26088;&#22312;&#26377;&#25928;&#24320;&#21457;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#19968;&#20010;&#19982;&#20154;&#31867;&#21028;&#26029;&#32039;&#23494;&#23545;&#40784;&#30340;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#23545;&#20110;&#26377;&#25928;&#24320;&#21457;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#25968;&#25454;&#39537;&#21160;&#25351;&#26631;&#34920;&#29616;&#20986;&#27604;&#20256;&#32479;&#25351;&#26631;&#22914;CIDEr&#26356;&#24378;&#30340;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#65307;&#28982;&#32780;&#65292;&#23427;&#20204;&#32570;&#20047;&#36275;&#22815;&#30340;&#33021;&#21147;&#26469;&#22788;&#29702;&#24187;&#35273;&#65292;&#24182;&#19988;&#36328;&#21508;&#31181;&#22270;&#20687;&#21644;&#25991;&#26412;&#27867;&#21270;&#37096;&#20998;&#26159;&#22240;&#20026;&#23427;&#20204;&#20165;&#20165;&#20351;&#29992;&#20174;&#19982;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#35780;&#20272;&#26080;&#20851;&#30340;&#20219;&#21153;&#23398;&#20064;&#30340;&#23884;&#20837;&#35745;&#31639;&#26631;&#37327;&#30456;&#20284;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Polos&#65292;&#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#27169;&#22411;&#30340;&#30417;&#30563;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#12290;Polos&#20174;&#22810;&#27169;&#24335;&#36755;&#20837;&#20013;&#35745;&#31639;&#24471;&#20998;&#65292;&#20351;&#29992;&#19968;&#20010;&#24182;&#34892;&#29305;&#24449;&#25552;&#21462;&#26426;&#21046;&#65292;&#21033;&#29992;&#36890;&#36807;&#22823;&#35268;&#27169;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#30340;&#23884;&#20837;&#12290;&#20026;&#20102;&#35757;&#32451;Polos&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#20154;&#31867;&#21453;&#39304;&#30340;&#22810;&#27169;&#24577;&#24230;&#37327;&#23398;&#20064;&#65288;M$^2$LHF&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#24320;&#21457;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18091v1 Announce Type: cross  Abstract: Establishing an automatic evaluation metric that closely aligns with human judgments is essential for effectively developing image captioning models. Recent data-driven metrics have demonstrated a stronger correlation with human judgments than classic metrics such as CIDEr; however they lack sufficient capabilities to handle hallucinations and generalize across diverse images and texts partially because they compute scalar similarities merely using embeddings learned from tasks unrelated to image captioning evaluation. In this study, we propose Polos, a supervised automatic evaluation metric for image captioning models. Polos computes scores from multimodal inputs, using a parallel feature extraction mechanism that leverages embeddings trained through large-scale contrastive learning. To train Polos, we introduce Multimodal Metric Learning from Human Feedback (M$^2$LHF), a framework for developing metrics based on human feedback. We co
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;Clean-LaVe&#65292;&#26088;&#22312;&#21033;&#29992;&#38134;&#26631;&#20934;&#25968;&#25454;&#26469;&#22686;&#24378;&#38646;&#26679;&#26412;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.18061</link><description>&lt;p&gt;
&#20851;&#20110;&#22312;&#20449;&#24687;&#25552;&#21462;&#20013;&#21033;&#29992;&#38134;&#26631;&#20934;&#25968;&#25454;&#36827;&#34892;&#38646;&#26679;&#26412;&#20998;&#31867;&#20219;&#21153;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the use of Silver Standard Data for Zero-shot Classification Tasks in Information Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18061
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;Clean-LaVe&#65292;&#26088;&#22312;&#21033;&#29992;&#38134;&#26631;&#20934;&#25968;&#25454;&#26469;&#22686;&#24378;&#38646;&#26679;&#26412;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20449;&#24687;&#25552;&#21462;&#65288;IE&#65289;&#39046;&#22495;&#65292;&#30417;&#30563;&#20998;&#31867;&#26041;&#27861;&#30340;&#21331;&#36234;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#40644;&#37329;&#26631;&#20934;&#25968;&#25454;&#12290;&#26368;&#36817;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#26041;&#27861;&#23558;&#20219;&#21153;&#36716;&#21270;&#20026;&#20854;&#20182;NLP&#20219;&#21153;&#65288;&#20363;&#22914;&#65292;&#25991;&#26412;&#34164;&#28085;&#65289;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;NLP&#20219;&#21153;&#30340;&#29616;&#25104;&#27169;&#22411;&#30452;&#25509;&#23545;&#27979;&#35797;&#25968;&#25454;&#36827;&#34892;&#25512;&#29702;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#22823;&#37327;&#30340;IE&#27880;&#37322;&#25968;&#25454;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#19968;&#20010;&#28508;&#22312;&#26377;&#20215;&#20540;&#30340;&#21103;&#20135;&#21697;&#26159;&#22823;&#35268;&#27169;&#30340;&#38134;&#26631;&#20934;&#25968;&#25454;&#65292;&#21363;&#20854;&#20182;NLP&#20219;&#21153;&#30340;&#29616;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;&#20266;&#26631;&#35760;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36825;&#20123;&#25968;&#25454;&#30340;&#21033;&#29992;&#24182;&#27809;&#26377;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;Clean-LaVe&#65292;&#26088;&#22312;&#21033;&#29992;&#38134;&#26631;&#20934;&#25968;&#25454;&#26469;&#22686;&#24378;&#38646;&#26679;&#26412;&#20998;&#31867;&#24615;&#33021;&#12290;Clean-LaVe&#21253;&#25324;&#22235;&#20010;&#38454;&#27573;&#65306;&#65288;1&#65289;&#33719;&#21462;&#38134;&#26631;&#20934;&#25968;&#25454;&#65307;&#65288;2&#65289;&#20174;&#38134;&#26631;&#20934;&#25968;&#25454;&#20013;&#35782;&#21035;&#30456;&#23545;&#24178;&#20928;&#30340;&#25968;&#25454;&#65307;&#65288;3&#65289;&#20351;&#29992;&#24178;&#20928;&#25968;&#25454;&#24494;&#35843;&#29616;&#25104;&#27169;&#22411;&#65307;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18061v1 Announce Type: cross  Abstract: The superior performance of supervised classification methods in the information extraction (IE) area heavily relies on a large amount of gold standard data. Recent zero-shot classification methods converted the task to other NLP tasks (e.g., textual entailment) and used off-the-shelf models of these NLP tasks to directly perform inference on the test data without using a large amount of IE annotation data. A potentially valuable by-product of these methods is the large-scale silver standard data, i.e., pseudo-labeled data by the off-the-shelf models of other NLP tasks. However, there is no further investigation into the use of these data. In this paper, we propose a new framework, Clean-LaVe, which aims to utilize silver standard data to enhance the zero-shot performance. Clean-LaVe includes four phases: (1) Obtaining silver data; (2) Identifying relatively clean data from silver data; (3) Finetuning the off-the-shelf model using clea
&lt;/p&gt;</description></item><item><title>&#22312;&#22238;&#31572;&#21307;&#23398;&#38382;&#39064;&#26041;&#38754;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23454;&#38469;&#20020;&#24202;&#26696;&#20363;&#19978;&#30340;&#34920;&#29616;&#26159;&#20851;&#38190;&#65292;&#22240;&#27492;&#26500;&#24314;&#20102;&#20004;&#20010;&#32467;&#26500;&#21270;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.18060</link><description>&lt;p&gt;
&#22312;&#22238;&#31572;&#21644;&#35299;&#37322;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21307;&#23398;&#38382;&#39064;&#19978;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Large Language Models on Answering and Explaining Challenging Medical Questions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18060
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22238;&#31572;&#21307;&#23398;&#38382;&#39064;&#26041;&#38754;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23454;&#38469;&#20020;&#24202;&#26696;&#20363;&#19978;&#30340;&#34920;&#29616;&#26159;&#20851;&#38190;&#65292;&#22240;&#27492;&#26500;&#24314;&#20102;&#20004;&#20010;&#32467;&#26500;&#21270;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLMs&#22312;&#22238;&#31572;&#21307;&#23398;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20363;&#22914;&#36890;&#36807;&#21307;&#23398;&#25191;&#29031;&#32771;&#35797;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#20381;&#36182;&#20110;&#22996;&#21592;&#20250;&#32771;&#35797;&#38382;&#39064;&#25110;&#19968;&#33324;&#21307;&#23398;&#38382;&#39064;&#65292;&#26080;&#27861;&#25429;&#25417;&#30495;&#23454;&#20020;&#24202;&#26696;&#20363;&#30340;&#22797;&#26434;&#24615;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#31572;&#26696;&#30340;&#21442;&#32771;&#35299;&#37322;&#38459;&#30861;&#20102;&#23545;&#27169;&#22411;&#35299;&#37322;&#30340;&#35780;&#20272;&#65292;&#36825;&#23545;&#25903;&#25345;&#21307;&#29983;&#20570;&#20986;&#22797;&#26434;&#30340;&#21307;&#30103;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#20004;&#20010;&#26032;&#25968;&#25454;&#38598;&#65306;JAMA&#20020;&#24202;&#25361;&#25112;&#21644;Medbullets&#12290;JAMA&#20020;&#24202;&#25361;&#25112;&#21253;&#21547;&#22522;&#20110;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20020;&#24202;&#26696;&#20363;&#30340;&#38382;&#39064;&#65292;&#32780;Medbullets&#21253;&#21547;&#31867;&#20284;USMLE Step 2&amp;3&#39118;&#26684;&#30340;&#20020;&#24202;&#38382;&#39064;&#12290;&#20004;&#20010;&#25968;&#25454;&#38598;&#22343;&#20197;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;-&#22238;&#31572;&#20219;&#21153;&#30340;&#32467;&#26500;&#21270;&#24418;&#24335;&#21576;&#29616;&#65292;&#27599;&#20010;&#38382;&#39064;&#37117;&#38468;&#26377;&#19987;&#23478;&#25776;&#20889;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#25552;&#31034;&#22312;&#36825;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#22235;&#20010;LLMs&#12290;&#23454;&#39564;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18060v1 Announce Type: new  Abstract: LLMs have demonstrated impressive performance in answering medical questions, such as passing medical licensing examinations. However, most existing benchmarks rely on board exam questions or general medical questions, falling short in capturing the complexity of realistic clinical cases. Moreover, the lack of reference explanations for answers hampers the evaluation of model explanations, which are crucial to supporting doctors in making complex medical decisions. To address these challenges, we construct two new datasets: JAMA Clinical Challenge and Medbullets. JAMA Clinical Challenge consists of questions based on challenging clinical cases, while Medbullets comprises USMLE Step 2&amp;3 style clinical questions. Both datasets are structured as multiple-choice question-answering tasks, where each question is accompanied by an expert-written explanation. We evaluate four LLMs on the two datasets using various prompts. Experiments demonstrat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#22810;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;&#30340;&#27700;&#21360;&#25216;&#26415;&#65292;&#36890;&#36807;&#36731;&#37327;&#32423;&#32593;&#32476;&#29983;&#25104;&#29305;&#23450;&#20196;&#29260;&#27700;&#21360;logits&#21644;&#20998;&#21106;&#27604;&#29575;&#65292;&#22312;&#20445;&#35777;&#26816;&#27979;&#24615;&#30340;&#21516;&#26102;&#25552;&#21319;&#20102;&#25991;&#26412;&#30340;&#35821;&#20041;&#23436;&#25972;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.18059</link><description>&lt;p&gt;
&#20855;&#26377;&#22686;&#24378;&#21487;&#26816;&#27979;&#24615;&#21644;&#35821;&#20041;&#36830;&#36143;&#24615;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29305;&#23450;&#20196;&#29260;&#27700;&#21360;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Token-Specific Watermarking with Enhanced Detectability and Semantic Coherence for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18059
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#22810;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;&#30340;&#27700;&#21360;&#25216;&#26415;&#65292;&#36890;&#36807;&#36731;&#37327;&#32423;&#32593;&#32476;&#29983;&#25104;&#29305;&#23450;&#20196;&#29260;&#27700;&#21360;logits&#21644;&#20998;&#21106;&#27604;&#29575;&#65292;&#22312;&#20445;&#35777;&#26816;&#27979;&#24615;&#30340;&#21516;&#26102;&#25552;&#21319;&#20102;&#25991;&#26412;&#30340;&#35821;&#20041;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21709;&#24212;&#65292;&#28508;&#22312;&#22320;&#23384;&#22312;&#35823;&#23548;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#36890;&#36807;&#21306;&#20998;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#21644;&#20154;&#31867;&#25776;&#20889;&#30340;&#25991;&#26412;&#26469;&#21152;&#20197;&#35268;&#33539;&#30340;&#24517;&#35201;&#24615;&#12290;&#27700;&#21360;&#25216;&#26415;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#33267;&#20851;&#37325;&#35201;&#65292;&#23427;&#28041;&#21450;&#22312;LLM&#25512;&#29702;&#38454;&#27573;&#21521;&#25991;&#26412;&#20013;&#23884;&#20837;&#38544;&#34255;&#26631;&#35760;&#65292;&#32780;&#36825;&#23545;&#20154;&#31867;&#26469;&#35828;&#26159;&#19981;&#21487;&#24863;&#30693;&#30340;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#27700;&#21360;&#31639;&#27861;&#38754;&#20020;&#30528;&#23454;&#29616;&#25554;&#20837;&#27700;&#21360;&#30340;&#21487;&#26816;&#27979;&#24615;&#21644;&#29983;&#25104;&#25991;&#26412;&#30340;&#35821;&#20041;&#23436;&#25972;&#24615;&#20004;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#22686;&#24378;&#20854;&#20013;&#19968;&#20010;&#26041;&#38754;&#24120;&#24120;&#20250;&#25439;&#23475;&#21478;&#19968;&#20010;&#26041;&#38754;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#65288;MOO&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#27700;&#21360;&#25216;&#26415;&#65292;&#21033;&#29992;&#36731;&#37327;&#32423;&#32593;&#32476;&#29983;&#25104;&#29305;&#23450;&#20196;&#29260;&#27700;&#21360;logits&#21644;&#20998;&#21106;&#27604;&#29575;&#12290;&#36890;&#36807;&#21033;&#29992;MOO&#26469;&#20248;&#21270;&#26816;&#27979;&#21644;&#35821;&#20041;&#30446;&#26631;&#20989;&#25968;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21516;&#26102;&#23454;&#29616;&#20102;&#21487;&#26816;&#27979;&#24615;&#21644;&#35821;&#20041;&#23436;&#25972;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18059v1 Announce Type: cross  Abstract: Large language models generate high-quality responses with potential misinformation, underscoring the need for regulation by distinguishing AI-generated and human-written texts. Watermarking is pivotal in this context, which involves embedding hidden markers in texts during the LLM inference phase, which is imperceptible to humans. Current watermarking algorithms, however, face the challenge of achieving both the detectability of inserted watermarks and the semantic integrity of generated texts, where enhancing one aspect often undermines the other. To overcome this, we introduce a novel multi-objective optimization (MOO) approach for watermarking that utilizes lightweight networks to generate token-specific watermarking logits and splitting ratios. By leveraging MOO to optimize for both detection and semantic objective functions, our method simultaneously achieves detectability and semantic integrity. Experimental results show that ou
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#24341;&#25991;&#25991;&#26412;&#26102;&#65292;&#19981;&#20165;&#32771;&#34385;&#24341;&#25991;&#26412;&#36523;&#65292;&#36824;&#35201;&#32771;&#34385;&#25972;&#20010;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#21253;&#25324;&#30446;&#26631;&#24341;&#25991;&#65292;&#20197;&#35299;&#20915;&#29983;&#25104;&#30340;&#24341;&#25991;&#24573;&#35270;&#24341;&#25991;&#19978;&#19979;&#25991;&#28966;&#28857;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.18054</link><description>&lt;p&gt;
&#29983;&#25104;&#24341;&#25991;&#25991;&#26412;&#30340;&#24773;&#22659;&#21270;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Contextualizing Generated Citation Texts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18054
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24341;&#25991;&#25991;&#26412;&#26102;&#65292;&#19981;&#20165;&#32771;&#34385;&#24341;&#25991;&#26412;&#36523;&#65292;&#36824;&#35201;&#32771;&#34385;&#25972;&#20010;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#21253;&#25324;&#30446;&#26631;&#24341;&#25991;&#65292;&#20197;&#35299;&#20915;&#29983;&#25104;&#30340;&#24341;&#25991;&#24573;&#35270;&#24341;&#25991;&#19978;&#19979;&#25991;&#28966;&#28857;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18054v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#25277;&#35937;&#24341;&#25991;&#25991;&#26412;&#29983;&#25104;&#36890;&#24120;&#34987;&#26500;&#24314;&#20026;&#19968;&#20010;&#22635;&#20805;&#20219;&#21153;&#65292;&#21363;&#20351;&#29992;&#19968;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#26469;&#35757;&#32451;&#29983;&#25104;&#19968;&#20010;&#24341;&#25991;&#65292;&#32473;&#20986;&#19968;&#20010;&#21442;&#32771;&#35770;&#25991;&#21644;&#30446;&#26631;&#21608;&#22260;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65307;&#29983;&#25104;&#30340;&#24341;&#25991;&#24212;&#35813;&#26159;&#26377;&#20851;&#21442;&#32771;&#35770;&#25991;&#30340;&#31616;&#35201;&#35752;&#35770;&#65292;&#19982;&#24341;&#25991;&#19978;&#19979;&#25991;&#30456;&#20851;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#26816;&#26597;&#26368;&#36817;&#22522;&#20110;LED&#30340;&#24341;&#25991;&#29983;&#25104;&#31995;&#32479;&#65292;&#25105;&#20204;&#21457;&#29616;&#35768;&#22810;&#29983;&#25104;&#30340;&#24341;&#25991;&#26159;&#21442;&#32771;&#35770;&#25991;&#20027;&#35201;&#36129;&#29486;&#30340;&#36890;&#29992;&#25688;&#35201;&#65292;&#24573;&#35270;&#20102;&#24341;&#25991;&#35821;&#22659;&#20851;&#27880;&#30340;&#19981;&#21516;&#20027;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#23545;&#24341;&#25991;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#36827;&#34892;&#31616;&#21333;&#20462;&#25913;&#65306;&#29983;&#25104;&#30446;&#26631;&#19981;&#20165;&#26159;&#24341;&#25991;&#26412;&#36523;&#65292;&#36824;&#21253;&#25324;&#25972;&#20010;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#21253;&#25324;&#30446;&#26631;&#24341;&#25991;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#20219;&#20309;&#25277;&#35937;&#30340;&#24341;&#25991;&#29983;&#25104;&#31995;&#32479;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20197;&#36825;&#31181;&#26041;&#24335;&#35757;&#32451;&#26356;&#21463;&#20154;&#31867;&#35835;&#32773;&#21644;&#27426;&#36814;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18054v1 Announce Type: new  Abstract: Abstractive citation text generation is usually framed as an infilling task, where a sequence-to-sequence model is trained to generate a citation given a reference paper and the context window around the target; the generated citation should be a brief discussion of the reference paper as it relates to the citing context. However, examining a recent LED-based citation generation system, we find that many of the generated citations are generic summaries of the reference papers main contribution, ignoring the citation contexts focus on a different topic. To address this problem, we propose a simple modification to the citation text generation task: the generation target is not only the citation itself, but the entire context window, including the target citation. This approach can be easily applied to any abstractive citation generation system, and our experimental results show that training in this way is preferred by human readers and al
&lt;/p&gt;</description></item><item><title>MEGAnno+&#25552;&#20379;&#20102;&#19968;&#20010;&#20154;&#31867;-LLM&#21327;&#20316;&#26631;&#27880;&#31995;&#32479;&#65292;&#26088;&#22312;&#36890;&#36807;&#20154;&#31867;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#32852;&#21512;&#24037;&#20316;&#20135;&#29983;&#21487;&#38752;&#19988;&#39640;&#36136;&#37327;&#30340;&#26631;&#31614;&#12290;</title><link>https://arxiv.org/abs/2402.18050</link><description>&lt;p&gt;
MEGAnno+: &#19968;&#20010;&#20154;&#31867;-LLM&#21327;&#20316;&#26631;&#27880;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
MEGAnno+: A Human-LLM Collaborative Annotation System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18050
&lt;/p&gt;
&lt;p&gt;
MEGAnno+&#25552;&#20379;&#20102;&#19968;&#20010;&#20154;&#31867;-LLM&#21327;&#20316;&#26631;&#27880;&#31995;&#32479;&#65292;&#26088;&#22312;&#36890;&#36807;&#20154;&#31867;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#32852;&#21512;&#24037;&#20316;&#20135;&#29983;&#21487;&#38752;&#19988;&#39640;&#36136;&#37327;&#30340;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#27604;&#20154;&#31867;&#26356;&#24555;&#36895;&#12289;&#26356;&#20415;&#23452;&#22320;&#20026;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#26631;&#35760;&#25968;&#25454;&#12290;&#23613;&#31649;&#23427;&#20204;&#24456;&#24378;&#22823;&#65292;&#20294;LLMs&#21487;&#33021;&#22312;&#29702;&#35299;&#22797;&#26434;&#12289;&#31038;&#20250;&#25991;&#21270;&#25110;&#39046;&#22495;&#29305;&#23450;&#19978;&#19979;&#25991;&#26041;&#38754;&#23384;&#22312;&#32570;&#38519;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#30340;&#27880;&#37322;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20513;&#19968;&#31181;&#20154;&#31867;&#21644;LLMs&#20849;&#21516;&#21512;&#20316;&#30340;&#26041;&#27861;&#65292;&#20197;&#20135;&#29983;&#21487;&#38752;&#19988;&#39640;&#36136;&#37327;&#30340;&#26631;&#31614;&#12290;&#25105;&#20204;&#25552;&#20986;MEGAnno+&#65292;&#36825;&#26159;&#19968;&#20010;&#20154;&#31867;-LLM&#21327;&#20316;&#30340;&#26631;&#27880;&#31995;&#32479;&#65292;&#25552;&#20379;&#26377;&#25928;&#30340;LLM&#20195;&#29702;&#21644;&#27880;&#37322;&#31649;&#29702;&#65292;&#20415;&#25463;&#19988;&#31283;&#22266;&#30340;LLM&#26631;&#27880;&#65292;&#20197;&#21450;&#20154;&#31867;&#23545;LLM&#26631;&#31614;&#30340;&#25506;&#32034;&#24615;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18050v1 Announce Type: new  Abstract: Large language models (LLMs) can label data faster and cheaper than humans for various NLP tasks. Despite their prowess, LLMs may fall short in understanding of complex, sociocultural, or domain-specific context, potentially leading to incorrect annotations. Therefore, we advocate a collaborative approach where humans and LLMs work together to produce reliable and high-quality labels. We present MEGAnno+, a human-LLM collaborative annotation system that offers effective LLM agent and annotation management, convenient and robust LLM annotation, and exploratory verification of LLM labels by humans.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#23616;&#37096;&#20869;&#22312;&#32500;&#24230;&#26469;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#30340;&#30495;&#23454;&#24615;&#65292;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.18048</link><description>&lt;p&gt;
&#29992;&#23616;&#37096;&#20869;&#22312;&#32500;&#24230;&#34920;&#24449;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#30495;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
Characterizing Truthfulness in Large Language Model Generations with Local Intrinsic Dimension
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#23616;&#37096;&#20869;&#22312;&#32500;&#24230;&#26469;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#30340;&#30495;&#23454;&#24615;&#65292;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#22914;&#20309;&#34920;&#24449;&#21644;&#39044;&#27979;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#30495;&#23454;&#24615;&#65292;&#36825;&#22312;&#24314;&#31435;&#20154;&#31867;&#19982;LLMs&#20043;&#38388;&#30340;&#20449;&#20219;&#20851;&#31995;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#30740;&#31350;&#20869;&#37096;&#28608;&#27963;&#24182;&#21033;&#29992;&#27169;&#22411;&#28608;&#27963;&#30340;&#23616;&#37096;&#20869;&#22312;&#32500;&#24230;&#65288;LID&#65289;&#26469;&#37327;&#21270;LLM&#30340;&#30495;&#23454;&#24615;&#12290;&#36890;&#36807;&#23545;&#22235;&#20010;&#38382;&#31572;&#65288;QA&#65289;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#20013;&#30340;&#20869;&#22312;&#32500;&#24230;&#21450;&#20854;&#19982;&#27169;&#22411;&#23618;&#12289;&#33258;&#22238;&#24402;&#35821;&#35328;&#24314;&#27169;&#20197;&#21450;LLMs&#30340;&#35757;&#32451;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#20869;&#22312;&#32500;&#24230;&#21487;&#20197;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18048v1 Announce Type: new  Abstract: We study how to characterize and predict the truthfulness of texts generated from large language models (LLMs), which serves as a crucial step in building trust between humans and LLMs. Although several approaches based on entropy or verbalized uncertainty have been proposed to calibrate model predictions, these methods are often intractable, sensitive to hyperparameters, and less reliable when applied in generative tasks with LLMs. In this paper, we suggest investigating internal activations and quantifying LLM's truthfulness using the local intrinsic dimension (LID) of model activations. Through experiments on four question answering (QA) datasets, we demonstrate the effectiveness ohttps://info.arxiv.org/help/prep#abstractsf our proposed method. Additionally, we study intrinsic dimensions in LLMs and their relations with model layers, autoregressive language modeling, and the training of LLMs, revealing that intrinsic dimensions can be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#35780;&#20272;&#20102;&#22810;&#35821;&#35328;LLMs&#36328;&#35821;&#35328;&#21644;&#22320;&#29702;&#21306;&#22495;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#65292;&#24182;&#21457;&#29616;&#33521;&#35821;&#22312;&#20107;&#23454;&#20934;&#30830;&#24615;&#21644;&#29983;&#25104;&#20107;&#23454;&#25968;&#37327;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#35821;&#35328;&#65292;&#21516;&#26102;&#22810;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#23545;&#26469;&#33258;&#35199;&#26041;&#22823;&#38470;&#20107;&#23454;&#20449;&#24687;&#30340;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.18045</link><description>&lt;p&gt;
Multi-FAct: &#20351;&#29992;FActScore&#35780;&#20272;&#22810;&#35821;&#35328;LLM&#30340;&#22810;&#21306;&#22495;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Multi-FAct: Assessing Multilingual LLMs' Multi-Regional Knowledge using FActScore
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#35780;&#20272;&#20102;&#22810;&#35821;&#35328;LLMs&#36328;&#35821;&#35328;&#21644;&#22320;&#29702;&#21306;&#22495;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#65292;&#24182;&#21457;&#29616;&#33521;&#35821;&#22312;&#20107;&#23454;&#20934;&#30830;&#24615;&#21644;&#29983;&#25104;&#20107;&#23454;&#25968;&#37327;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#35821;&#35328;&#65292;&#21516;&#26102;&#22810;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#23545;&#26469;&#33258;&#35199;&#26041;&#22823;&#38470;&#20107;&#23454;&#20449;&#24687;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23481;&#26131;&#20986;&#29616;&#20107;&#23454;&#19978;&#30340;&#24187;&#35273;&#65292;&#29983;&#25104;&#19982;&#24050;&#30693;&#30693;&#35782;&#30456;&#30683;&#30462;&#30340;&#25991;&#26412;&#12290;&#23613;&#31649;&#24191;&#27867;&#30740;&#31350;&#20102;&#33521;&#35821;&#20013;&#30340;&#36825;&#19968;&#38382;&#39064;&#65292;&#20294;&#23545;&#20110;&#22810;&#35821;&#35328;LLMs&#30693;&#20043;&#29978;&#23569;&#12290;&#26412;&#25991;&#31995;&#32479;&#35780;&#20272;&#20102;&#22810;&#35821;&#35328;LLMs&#36328;&#35821;&#35328;&#21644;&#22320;&#29702;&#21306;&#22495;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#35821;&#35328;&#20107;&#23454;&#35780;&#20272;&#27969;&#31243;&#65292;&#23558;FActScore&#65288;Min&#31561;&#65292;2023&#65289;&#25913;&#32534;&#20026;&#22810;&#26679;&#21270;&#35821;&#35328;&#12290;&#25105;&#20204;&#22312;&#20061;&#31181;&#35821;&#35328;&#19978;&#30340;&#20998;&#26512;&#26174;&#31034;&#65292;&#33521;&#35821;&#22312;&#20107;&#23454;&#20934;&#30830;&#24615;&#21644;&#29983;&#25104;&#20107;&#23454;&#25968;&#37327;&#26041;&#38754;&#22987;&#32456;&#34920;&#29616;&#20248;&#24322;&#12290;&#27492;&#22806;&#65292;&#22810;&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20986;&#23545;&#26469;&#33258;&#35199;&#26041;&#22823;&#38470;&#30340;&#20107;&#23454;&#20449;&#24687;&#30340;&#20559;&#35265;&#12290;&#36825;&#20123;&#21457;&#29616;&#31361;&#26174;&#20102;&#23545;&#25913;&#36827;&#22810;&#35821;&#35328;&#20107;&#23454;&#24615;&#35780;&#20272;&#30340;&#38656;&#27714;&#65292;&#24182;&#24378;&#35843;&#20102;LLMs&#30340;&#20107;&#23454;&#29983;&#25104;&#20013;&#30340;&#22320;&#29702;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18045v1 Announce Type: new  Abstract: Large Language Models (LLMs) are prone to factuality hallucination, generating text that contradicts established knowledge. While extensive research has addressed this in English, little is known about multilingual LLMs. This paper systematically evaluates multilingual LLMs' factual accuracy across languages and geographic regions. We introduce a novel pipeline for multilingual factuality evaluation, adapting FActScore(Min et al., 2023) for diverse languages. Our analysis across nine languages reveals that English consistently outperforms others in factual accuracy and quantity of generated facts. Furthermore, multilingual models demonstrate a bias towards factual information from Western continents. These findings highlight the need for improved multilingual factuality assessment and underscore geographical biases in LLMs' fact generation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#33021;&#28304;&#21361;&#26426;&#21644;&#29983;&#27963;&#25104;&#26412;&#30340;&#20844;&#20849;&#36777;&#35770;&#65292;&#35782;&#21035;&#20102;&#20851;&#38190;&#35758;&#39064;&#12289;&#26032;&#36235;&#21183;&#12289;&#20851;&#38190;&#31038;&#20250;&#34892;&#20026;&#32773;&#20197;&#21450;&#20182;&#20204;&#22312;&#36777;&#35770;&#20013;&#30340;&#35282;&#33394;&#65292;&#20197;&#21450;&#19982;&#36825;&#20123;&#35758;&#39064;&#21644;&#34892;&#20026;&#32773;&#30456;&#20851;&#30340;&#24773;&#32490;&#12290;</title><link>https://arxiv.org/abs/2402.18043</link><description>&lt;p&gt;
&#21361;&#26426;&#23545;&#35805;: &#20851;&#20110;&#33021;&#28304;&#21361;&#26426;&#21644;&#29983;&#27963;&#25104;&#26412;&#30340;&#20844;&#20849;&#36777;&#35770;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Crisis talk: analysis of the public debate around the energy crisis and cost of living
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#33021;&#28304;&#21361;&#26426;&#21644;&#29983;&#27963;&#25104;&#26412;&#30340;&#20844;&#20849;&#36777;&#35770;&#65292;&#35782;&#21035;&#20102;&#20851;&#38190;&#35758;&#39064;&#12289;&#26032;&#36235;&#21183;&#12289;&#20851;&#38190;&#31038;&#20250;&#34892;&#20026;&#32773;&#20197;&#21450;&#20182;&#20204;&#22312;&#36777;&#35770;&#20013;&#30340;&#35282;&#33394;&#65292;&#20197;&#21450;&#19982;&#36825;&#20123;&#35758;&#39064;&#21644;&#34892;&#20026;&#32773;&#30456;&#20851;&#30340;&#24773;&#32490;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#22312;2020&#24180;&#20195;&#21021;&#26399;&#65292;&#33521;&#22269;&#21644;&#22823;&#37096;&#20998;&#27431;&#27954;&#21463;&#21040;&#33021;&#28304;&#21361;&#26426;&#30340;&#24433;&#21709;&#65292;&#36825;&#25104;&#20026;&#19968;&#20010;&#31361;&#20986;&#30340;&#23186;&#20307;&#35805;&#39064;&#12290;&#23427;&#23558;&#33021;&#28304;&#20381;&#36182;&#24615;&#21644;&#21487;&#25345;&#32493;&#21457;&#23637;&#12289;&#32463;&#27982;&#36127;&#25285;&#30340;&#20844;&#24179;&#20998;&#37197;&#20197;&#21450;&#29983;&#27963;&#25104;&#26412;&#12289;&#27668;&#20505;&#21464;&#21270;&#12289;&#39118;&#38505;&#21644;&#21487;&#25345;&#32493;&#24615;&#31561;&#38382;&#39064;&#27719;&#38598;&#21040;&#19968;&#20010;&#20844;&#20849;&#36777;&#35770;&#20013;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22260;&#32469;&#33021;&#28304;&#21361;&#26426;&#21644;&#29983;&#27963;&#25104;&#26412;&#30340;&#20844;&#20849;&#35805;&#35821;&#65292;&#20197;&#30830;&#23450;&#22312;&#36825;&#22330;&#36777;&#35770;&#20013;&#22914;&#20309;&#35843;&#21644;&#36825;&#20123;&#20851;&#38190;&#19988;&#30683;&#30462;&#30340;&#38382;&#39064;&#65292;&#35782;&#21035;&#21442;&#19982;&#20854;&#20013;&#30340;&#31038;&#20250;&#34892;&#20026;&#32773;&#20197;&#21450;&#20182;&#20204;&#30340;&#35282;&#33394;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20174;2014&#24180;1&#26376;&#33267;2023&#24180;3&#26376;&#26816;&#32034;&#30340;&#33521;&#22269;&#25253;&#32440;&#25991;&#29486;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#24212;&#29992;&#20102;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#25968;&#25454;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#20197;&#30830;&#23450;&#20851;&#38190;&#20027;&#39064;&#12289;&#26032;&#36235;&#21183;&#12289;&#20851;&#38190;&#31038;&#20250;&#34892;&#20026;&#32773;&#21450;&#20854;&#22312;&#36777;&#35770;&#20013;&#30340;&#35282;&#33394;&#65292;&#20197;&#21450;&#19982;&#36825;&#20123;&#34892;&#20026;&#32773;&#21644;&#20027;&#39064;&#30456;&#20851;&#30340;&#24773;&#32490;&#12290;&#25105;&#20204;&#23558;&#33258;&#21160;&#21270;&#25216;&#26415;&#19982;&#25163;&#21160;&#25216;&#26415;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18043v1 Announce Type: new  Abstract: A prominent media topic in the UK in the early 2020s is the energy crisis affecting the UK and most of Europe. It brings into a single public debate issues of energy dependency and sustainability, fair distribution of economic burdens and cost of living, as well as climate change, risk, and sustainability. In this paper, we investigate the public discourse around the energy crisis and cost of living to identify how these pivotal and contradictory issues are reconciled in this debate and to identify which social actors are involved and the role they play. We analyse a document corpus retrieved from UK newspapers from January 2014 to March 2023. We apply a variety of natural language processing and data visualisation techniques to identify key topics, novel trends, critical social actors, and the role they play in the debate, along with the sentiment associated with those actors and topics. We combine automated techniques with manual disco
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#38598;&#30340;&#19981;&#21516;&#31867;&#22411;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.18041</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#38598;&#65306;&#19968;&#39033;&#20840;&#38754;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Datasets for Large Language Models: A Comprehensive Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#38598;&#30340;&#19981;&#21516;&#31867;&#22411;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#25506;&#32034;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#22312;LLMs&#30340;&#26174;&#30528;&#36827;&#23637;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#25968;&#25454;&#38598;&#31867;&#20284;&#20110;&#32500;&#25345;&#21644;&#22521;&#32946;LLMs&#21457;&#23637;&#30340;&#26681;&#31995;&#22522;&#30784;&#26550;&#26500;&#12290;&#22240;&#27492;&#65292;&#26816;&#26597;&#36825;&#20123;&#25968;&#25454;&#38598;&#25104;&#20026;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#20027;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#24403;&#21069;&#32570;&#20047;&#23545;LLM&#25968;&#25454;&#38598;&#20840;&#38754;&#27010;&#36848;&#21644;&#24443;&#24213;&#20998;&#26512;&#30340;&#38382;&#39064;&#65292;&#24182;&#33719;&#24471;&#26377;&#20851;&#23427;&#20204;&#24403;&#21069;&#29366;&#24577;&#21644;&#26410;&#26469;&#36235;&#21183;&#30340;&#35265;&#35299;&#65292;&#26412;&#35843;&#26597;&#20174;&#20116;&#20010;&#35282;&#24230;&#25972;&#21512;&#21644;&#20998;&#31867;LLM&#25968;&#25454;&#38598;&#30340;&#22522;&#26412;&#26041;&#38754;&#65306;&#65288;1&#65289;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#65307;&#65288;2&#65289;&#25351;&#23548;&#24494;&#35843;&#25968;&#25454;&#38598;&#65307;&#65288;3&#65289;&#20559;&#22909;&#25968;&#25454;&#38598;&#65307;&#65288;4&#65289;&#35780;&#20272;&#25968;&#25454;&#38598;&#65307;&#65288;5&#65289;&#20256;&#32479;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25968;&#25454;&#38598;&#12290;&#35843;&#26597;&#38416;&#26126;&#20102;&#24403;&#21069;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18041v1 Announce Type: cross  Abstract: This paper embarks on an exploration into the Large Language Model (LLM) datasets, which play a crucial role in the remarkable advancements of LLMs. The datasets serve as the foundational infrastructure analogous to a root system that sustains and nurtures the development of LLMs. Consequently, examination of these datasets emerges as a critical topic in research. In order to address the current lack of a comprehensive overview and thorough analysis of LLM datasets, and to gain insights into their current status and future trends, this survey consolidates and categorizes the fundamental aspects of LLM datasets from five perspectives: (1) Pre-training Corpora; (2) Instruction Fine-tuning Datasets; (3) Preference Datasets; (4) Evaluation Datasets; (5) Traditional Natural Language Processing (NLP) Datasets. The survey sheds light on the prevailing challenges and points out potential avenues for future investigation. Additionally, a compre
&lt;/p&gt;</description></item><item><title>ResLoRA&#25552;&#20986;&#20102;&#22312;&#35757;&#32451;&#20013;&#28155;&#21152;&#27531;&#20313;&#36335;&#24452;&#24182;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#28040;&#38500;&#36825;&#20123;&#39069;&#22806;&#36335;&#24452;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#27604;LoRA&#26356;&#21152;&#39640;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.18039</link><description>&lt;p&gt;
ResLoRA&#65306;&#20302;&#31209;&#36866;&#24212;&#20013;&#30340;&#36523;&#20221;&#27531;&#24046;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
ResLoRA: Identity Residual Mapping in Low-Rank Adaption
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18039
&lt;/p&gt;
&lt;p&gt;
ResLoRA&#25552;&#20986;&#20102;&#22312;&#35757;&#32451;&#20013;&#28155;&#21152;&#27531;&#20313;&#36335;&#24452;&#24182;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#28040;&#38500;&#36825;&#20123;&#39069;&#22806;&#36335;&#24452;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#27604;LoRA&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#26368;&#27969;&#34892;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#20043;&#19968;&#65292;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#36890;&#24120;&#24212;&#29992;&#20110;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#28982;&#32780;&#65292;&#22312;&#21407;&#22987;&#27169;&#22411;&#20013;&#30001;&#20110;&#38271;&#35745;&#31639;&#36335;&#24452;&#65292;&#22312;&#26377;&#25928;&#32780;&#36805;&#36895;&#22320;&#26356;&#26032;LoRA&#22359;&#30340;&#26435;&#37325;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ResLoRA&#65292;&#36825;&#26159;LoRA&#30340;&#25913;&#36827;&#26694;&#26550;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#28155;&#21152;&#27531;&#20313;&#36335;&#24452;&#65292;&#24182;&#20351;&#29992;&#21512;&#24182;&#26041;&#27861;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#28040;&#38500;&#36825;&#20123;&#39069;&#22806;&#36335;&#24452;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#36739;&#23569;&#30340;&#35757;&#32451;&#27493;&#39588;&#20869;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#32780;&#19982;LoRA&#30456;&#27604;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#25110;&#25512;&#26029;&#25104;&#26412;&#12290;&#23545; NLG&#12289;NLU &#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;ResLoRA&#26159;&#39318;&#20010;&#23558;&#27531;&#20313;&#36335;&#24452;&#19982;LoRA&#32467;&#21512;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20195;&#30721;&#21487;&#22312; https://github.com/microsoft/LMOps/tree/main/reslora &#19978;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18039v1 Announce Type: cross  Abstract: As one of the most popular parameter-efficient fine-tuning (PEFT) methods, low-rank adaptation (LoRA) is commonly applied to fine-tune large language models (LLMs). However, updating the weights of LoRA blocks effectively and expeditiously is challenging due to the long calculation path in the original model. To address this, we propose ResLoRA, an improved framework of LoRA. By adding residual paths during training and using merging approaches to eliminate these extra paths during inference, our method can achieve better results in fewer training steps without any extra trainable parameters or inference cost compared to LoRA. The experiments on NLG, NLU, and text-to-image tasks demonstrate the effectiveness of our method. To the best of our knowledge, ResLoRA is the first work that combines the residual path with LoRA. The code of our method is available at https://github.com/microsoft/LMOps/tree/main/reslora .
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#35821;&#26009;&#24211;&#24341;&#23548;&#30340;&#26597;&#35810;&#25193;&#23637;&#65288;CSQE&#65289;&#65292;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22686;&#24378;&#25193;&#23637;&#65292;&#25913;&#21892;&#20102;&#26597;&#35810;&#19982;&#30446;&#26631;&#25991;&#26723;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.18031</link><description>&lt;p&gt;
&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#30340;&#35821;&#26009;&#24211;&#26597;&#35810;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Corpus-Steered Query Expansion with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18031
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#35821;&#26009;&#24211;&#24341;&#23548;&#30340;&#26597;&#35810;&#25193;&#23637;&#65288;CSQE&#65289;&#65292;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22686;&#24378;&#25193;&#23637;&#65292;&#25913;&#21892;&#20102;&#26597;&#35810;&#19982;&#30446;&#26631;&#25991;&#26723;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#26597;&#35810;&#25193;&#23637;&#21487;&#20197;&#36890;&#36807;&#29983;&#25104;&#33021;&#22815;&#22238;&#31572;&#26597;&#35810;&#30340;&#20551;&#35774;&#25991;&#26723;&#32780;&#26174;&#30528;&#22686;&#24378;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;LLMs&#30340;&#26377;&#38480;&#20869;&#22312;&#30693;&#35782;&#65292;&#25193;&#23637;&#19982;&#26816;&#32034;&#35821;&#26009;&#24211;&#20043;&#38388;&#30340;&#19981;&#23545;&#40784;&#23548;&#33268;&#38382;&#39064;&#65292;&#22914;&#24187;&#35273;&#21644;&#36807;&#26102;&#20449;&#24687;&#12290;&#21463;&#20266;&#30456;&#20851;&#21453;&#39304;&#65288;PRF&#65289;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#35821;&#26009;&#24211;&#24341;&#23548;&#30340;&#26597;&#35810;&#25193;&#23637;&#65288;CSQE&#65289;&#26469;&#20419;&#36827;&#23884;&#20837;&#22312;&#35821;&#26009;&#24211;&#20013;&#30340;&#30693;&#35782;&#30340;&#25972;&#21512;&#12290;CSQE&#21033;&#29992;LLMs&#30340;&#30456;&#20851;&#24615;&#35780;&#20272;&#33021;&#21147;&#31995;&#32479;&#22320;&#35782;&#21035;&#26368;&#21021;&#26816;&#32034;&#21040;&#30340;&#25991;&#26723;&#20013;&#30340;&#20851;&#38190;&#21477;&#12290;&#36825;&#20123;&#30001;&#35821;&#26009;&#24211;&#20135;&#29983;&#30340;&#25991;&#26412;&#38543;&#21518;&#19982;LLM&#30693;&#35782;&#22686;&#24378;&#25193;&#23637;&#19968;&#36215;&#29992;&#20110;&#25193;&#23637;&#26597;&#35810;&#65292;&#25913;&#21892;&#26597;&#35810;&#19982;&#30446;&#26631;&#25991;&#26723;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#39044;&#27979;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;CSQE&#26126;&#26174;&#25552;&#39640;&#20102;&#20449;&#24687;&#26816;&#32034;&#32467;&#26524;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18031v1 Announce Type: cross  Abstract: Recent studies demonstrate that query expansions generated by large language models (LLMs) can considerably enhance information retrieval systems by generating hypothetical documents that answer the queries as expansions. However, challenges arise from misalignments between the expansions and the retrieval corpus, resulting in issues like hallucinations and outdated information due to the limited intrinsic knowledge of LLMs. Inspired by Pseudo Relevance Feedback (PRF), we introduce Corpus-Steered Query Expansion (CSQE) to promote the incorporation of knowledge embedded within the corpus. CSQE utilizes the relevance assessing capability of LLMs to systematically identify pivotal sentences in the initially-retrieved documents. These corpus-originated texts are subsequently used to expand the query together with LLM-knowledge empowered expansions, improving the relevance prediction between the query and the target documents. Extensive exp
&lt;/p&gt;</description></item><item><title>LINGOLLM&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#25552;&#31034;&#20013;&#23637;&#31034;&#23545;&#30475;&#19981;&#35265;&#35821;&#35328;&#30340;&#35821;&#35328;&#30693;&#35782;&#65292;&#21253;&#25324;&#35789;&#20856;&#12289;&#35821;&#27861;&#20070;&#21644;&#24418;&#24577;&#20998;&#26512;&#30340;&#36755;&#20837;&#25991;&#26412;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#28626;&#21361;&#35821;&#35328;&#26041;&#38754;&#30340;&#32763;&#35793;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.18025</link><description>&lt;p&gt;
&#38599;&#20323;&#19968;&#21517;&#35821;&#35328;&#23398;&#23478;&#65281;&#65306;&#36890;&#36807;&#19978;&#19979;&#25991;&#35821;&#35328;&#25551;&#36848;&#23398;&#20064;&#28626;&#21361;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Hire a Linguist!: Learning Endangered Languages with In-Context Linguistic Descriptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18025
&lt;/p&gt;
&lt;p&gt;
LINGOLLM&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#25552;&#31034;&#20013;&#23637;&#31034;&#23545;&#30475;&#19981;&#35265;&#35821;&#35328;&#30340;&#35821;&#35328;&#30693;&#35782;&#65292;&#21253;&#25324;&#35789;&#20856;&#12289;&#35821;&#27861;&#20070;&#21644;&#24418;&#24577;&#20998;&#26512;&#30340;&#36755;&#20837;&#25991;&#26412;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#28626;&#21361;&#35821;&#35328;&#26041;&#38754;&#30340;&#32763;&#35793;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv&#65306;2402.18025v1
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18025v1 Announce Type: new  Abstract: How can large language models (LLMs) process and translate endangered languages? Many languages lack a large corpus to train a decent LLM; therefore existing LLMs rarely perform well in unseen, endangered languages. On the contrary, we observe that 2000 endangered languages, though without a large corpus, have a grammar book or a dictionary. We propose LINGOLLM, a training-free approach to enable an LLM to process unseen languages that hardly occur in its pre-training. Our key insight is to demonstrate linguistic knowledge of an unseen language in an LLM's prompt, including a dictionary, a grammar book, and morphologically analyzed input text. We implement LINGOLLM on top of two models, GPT-4 and Mixtral, and evaluate their performance on 5 tasks across 8 endangered or low-resource languages. Our results show that LINGOLLM elevates translation capability from GPT-4's 0 to 10.5 BLEU for 10 language directions. Our findings demonstrate the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34920;&#31034;&#19982;&#20154;&#31867;&#35748;&#30693;&#20449;&#21495;&#32852;&#31995;&#36215;&#26469;&#65292;&#35780;&#20272;LLMs&#27169;&#25311;&#35748;&#30693;&#35821;&#35328;&#22788;&#29702;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.18023</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#21453;&#26144;&#35748;&#30693;&#35821;&#35328;&#22788;&#29702;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Large Language Models Mirror Cognitive Language Processing?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34920;&#31034;&#19982;&#20154;&#31867;&#35748;&#30693;&#20449;&#21495;&#32852;&#31995;&#36215;&#26469;&#65292;&#35780;&#20272;LLMs&#27169;&#25311;&#35748;&#30693;&#35821;&#35328;&#22788;&#29702;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25991;&#26412;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#23637;&#29616;&#20986;&#21331;&#36234;&#33021;&#21147;&#65292;&#29978;&#33267;&#22312;&#35768;&#22810;&#35748;&#30693;&#20219;&#21153;&#20013;&#23454;&#29616;&#29978;&#33267;&#36229;&#36234;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#12290;&#30001;&#20110;LLMs&#26159;&#20174;&#20154;&#31867;&#35821;&#35328;&#35748;&#30693;&#30340;&#22823;&#37327;&#25991;&#26412;&#20135;&#20986;&#20013;&#35757;&#32451;&#20986;&#26469;&#30340;&#65292;&#33258;&#28982;&#32780;&#28982;&#22320;&#20250;&#38382;LLMs&#26159;&#21542;&#21453;&#26144;&#35748;&#30693;&#35821;&#35328;&#22788;&#29702;&#65292;&#25110;LLMs&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#31867;&#20284;&#20110;&#35748;&#30693;&#35821;&#35328;&#22788;&#29702;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36830;&#25509;LLMs&#34920;&#24449;&#21644;&#20154;&#31867;&#35748;&#30693;&#20449;&#21495;&#65292;&#20197;&#35780;&#20272;LLMs&#22914;&#20309;&#26377;&#25928;&#22320;&#27169;&#25311;&#35748;&#30693;&#35821;&#35328;&#22788;&#29702;&#12290;&#25105;&#20204;&#37319;&#29992;&#34920;&#24449;&#30456;&#20284;&#24615;&#20998;&#26512;&#65288;RSA&#65289;&#26469;&#34913;&#37327;16&#31181;&#20027;&#27969;LLMs&#19982;&#22823;&#33041;fMRI&#20449;&#21495;&#20043;&#38388;&#30340;&#23545;&#40784;&#31243;&#24230;&#12290;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#25506;&#35752;&#20102;&#21508;&#31181;&#22240;&#32032;&#65288;&#20363;&#22914;&#27169;&#22411;&#35268;&#27169;&#12289;&#23545;&#40784;&#35757;&#32451;&#12289;&#25351;&#23548;&#38468;&#21152;&#65289;&#23545;LLM-&#22823;&#33041;&#23545;&#40784;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#35268;&#27169;&#19982;&#27491;&#30456;&#20851;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18023v1 Announce Type: new  Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in text comprehension and logical reasoning, achiving or even surpassing human-level performance in numerous cognition tasks. As LLMs are trained from massive textual outputs of human language cognition, it is natural to ask whether LLMs mirror cognitive language processing. Or to what extend LLMs resemble cognitive language processing? In this paper, we propose a novel method that bridge between LLM representations and human cognition signals to evaluate how effectively LLMs simulate cognitive language processing. We employ Representational Similarity Analysis (RSA) to mearsure the alignment between 16 mainstream LLMs and fMRI signals of the brain. We empirically investigate the impact of a variety of factors (e.g., model scaling, alignment training, instruction appending) on such LLM-brain alignment. Experimental results indicate that model scaling is positively cor
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#32508;&#36848;&#20102;&#22522;&#20110;LLM&#30340;&#22810;&#36718;&#23545;&#35805;&#31995;&#32479;&#30340;&#30740;&#31350;&#65292;&#24182;&#37325;&#28857;&#20171;&#32461;&#20102;LLMs&#30340;&#24212;&#29992;&#21644;&#26368;&#26032;&#36827;&#23637;&#65292;&#23545;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#21644;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#36827;&#34892;&#20102;&#28085;&#30422;&#65292;&#24182;&#35752;&#35770;&#20102;&#30456;&#20851;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#21450;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#21644;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.18013</link><description>&lt;p&gt;
&#22522;&#20110;LLM&#30340;&#22810;&#36718;&#23545;&#35805;&#31995;&#32479;&#26368;&#26032;&#36827;&#23637;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18013
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#32508;&#36848;&#20102;&#22522;&#20110;LLM&#30340;&#22810;&#36718;&#23545;&#35805;&#31995;&#32479;&#30340;&#30740;&#31350;&#65292;&#24182;&#37325;&#28857;&#20171;&#32461;&#20102;LLMs&#30340;&#24212;&#29992;&#21644;&#26368;&#26032;&#36827;&#23637;&#65292;&#23545;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#21644;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#36827;&#34892;&#20102;&#28085;&#30422;&#65292;&#24182;&#35752;&#35770;&#20102;&#30456;&#20851;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#21450;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#21644;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#20840;&#38754;&#22238;&#39038;&#20102;&#20851;&#20110;&#22810;&#36718;&#23545;&#35805;&#31995;&#32479;&#30340;&#30740;&#31350;&#65292;&#29305;&#21035;&#20391;&#37325;&#20110;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22810;&#36718;&#23545;&#35805;&#31995;&#32479;&#12290;&#26412;&#25991;&#26088;&#22312;&#65288;a&#65289;&#24635;&#32467;&#29616;&#26377;&#30340;LLMs&#21644;&#36866;&#24212;LLMs&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#30340;&#26041;&#27861;&#65307;&#65288;b&#65289;&#35814;&#32454;&#38416;&#36848;&#22810;&#36718;&#23545;&#35805;&#31995;&#32479;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#28085;&#30422;&#22522;&#20110;LLM&#30340;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#65288;ODD&#65289;&#21644;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#65288;TOD&#65289;&#31995;&#32479;&#65292;&#20197;&#21450;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65307;&#65288;c&#65289;&#35752;&#35770;&#30001;&#20110;LLMs&#30340;&#21457;&#23637;&#21644;&#23545;&#22810;&#36718;&#23545;&#35805;&#31995;&#32479;&#19981;&#26029;&#22686;&#21152;&#30340;&#38656;&#27714;&#32780;&#20135;&#29983;&#30340;&#26410;&#26469;&#37325;&#28857;&#21644;&#26368;&#26032;&#30740;&#31350;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18013v1 Announce Type: cross  Abstract: This survey provides a comprehensive review of research on multi-turn dialogue systems, with a particular focus on multi-turn dialogue systems based on large language models (LLMs). This paper aims to (a) give a summary of existing LLMs and approaches for adapting LLMs to downstream tasks; (b) elaborate recent advances in multi-turn dialogue systems, covering both LLM-based open-domain dialogue (ODD) and task-oriented dialogue (TOD) systems, along with datasets and evaluation metrics; (c) discuss some future emphasis and recent research problems arising from the development of LLMs and the increasing demands on multi-turn dialogue systems.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20154;&#31867;&#20803;&#23457;&#38405;&#32773;&#30340;&#24773;&#24863;&#25972;&#21512;&#26694;&#26550;&#65292;&#25552;&#20986;&#35780;&#20272;&#25351;&#26631;&#24182;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#65292;&#25351;&#23548;LLMs&#29983;&#25104;&#31185;&#23398;&#20803;&#23457;&#38405;&#30340;&#36923;&#36753;&#34987;&#39564;&#35777;&#21487;&#34892;&#12290;</title><link>https://arxiv.org/abs/2402.18005</link><description>&lt;p&gt;
&#25506;&#32034;&#31185;&#23398;&#24773;&#24863;&#24635;&#32467;&#30340;&#22810;&#25991;&#26723;&#20449;&#24687;&#25972;&#21512;
&lt;/p&gt;
&lt;p&gt;
Exploring Multi-Document Information Consolidation for Scientific Sentiment Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18005
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20154;&#31867;&#20803;&#23457;&#38405;&#32773;&#30340;&#24773;&#24863;&#25972;&#21512;&#26694;&#26550;&#65292;&#25552;&#20986;&#35780;&#20272;&#25351;&#26631;&#24182;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#65292;&#25351;&#23548;LLMs&#29983;&#25104;&#31185;&#23398;&#20803;&#23457;&#38405;&#30340;&#36923;&#36753;&#34987;&#39564;&#35777;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#31995;&#32479;&#20855;&#26377;&#29983;&#25104;&#22810;&#20010;&#25991;&#26723;&#30340;&#21512;&#29702;&#25688;&#35201;&#30340;&#33021;&#21147;&#65307;&#28982;&#32780;&#65292;&#29616;&#22312;&#23578;&#19981;&#30830;&#23450;&#27169;&#22411;&#26159;&#21542;&#30495;&#27491;&#20855;&#26377;&#25972;&#21512;&#20449;&#24687;&#30340;&#33021;&#21147;&#26469;&#29983;&#25104;&#24635;&#32467;&#65292;&#23588;&#20854;&#26159;&#23545;&#37027;&#20123;&#21253;&#21547;&#20010;&#20154;&#24847;&#35265;&#20449;&#24687;&#30340;&#28304;&#25991;&#26723;&#12290;&#20026;&#20102;&#20351;&#31185;&#23398;&#24773;&#24863;&#24635;&#32467;&#26356;&#21152;&#25166;&#23454;&#65292;&#25105;&#20204;&#20551;&#35774;&#22312;&#21516;&#34892;&#35780;&#23457;&#20013;&#65292;&#20154;&#31867;&#20803;&#23457;&#38405;&#32773;&#36981;&#24490;&#24773;&#24863;&#25972;&#21512;&#30340;&#19977;&#23618;&#26694;&#26550;&#26469;&#25776;&#20889;&#20803;&#23457;&#38405;&#65292;&#24182;&#19988;&#36825;&#20195;&#34920;&#20102;&#22312;&#20803;&#23457;&#38405;&#29983;&#25104;&#36807;&#31243;&#20013;&#24635;&#32467;&#31185;&#23398;&#24773;&#24863;&#30340;&#36923;&#36753;&#12290;&#36890;&#36807;&#20154;&#31867;&#27880;&#37322;&#65292;&#39564;&#35777;&#20102;&#36825;&#19968;&#26694;&#26550;&#12290;&#22522;&#20110;&#35813;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35780;&#20272;&#25351;&#26631;&#26469;&#35780;&#20272;&#29983;&#25104;&#30340;&#20803;&#23457;&#38405;&#30340;&#36136;&#37327;&#65292;&#24182;&#19988;&#22312;&#24191;&#27867;&#23454;&#39564;&#20013;&#21457;&#29616;&#65292;&#24403;&#25105;&#20204;&#23558;&#20854;&#20316;&#20026;LLMs&#29983;&#25104;&#20803;&#23457;&#38405;&#30340;&#25552;&#31034;&#26102;&#65292;&#24773;&#24863;&#25972;&#21512;&#26694;&#26550;&#30340;&#20551;&#35774;&#22312;&#32463;&#39564;&#19978;&#26159;&#34892;&#24471;&#36890;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18005v1 Announce Type: cross  Abstract: Modern natural language generation systems with LLMs exhibit the capability to generate a plausible summary of multiple documents; however, it is uncertain if models truly possess the ability of information consolidation to generate summaries, especially on those source documents with opinionated information. To make scientific sentiment summarization more grounded, we hypothesize that in peer review human meta-reviewers follow a three-layer framework of sentiment consolidation to write meta-reviews and it represents the logic of summarizing scientific sentiments in meta-review generation. The framework is validated via human annotation. Based on the framework, we propose evaluation metrics to assess the quality of generated meta-reviews, and we find that the hypothesis of the sentiment consolidation framework works out empirically when we incorporate it as prompts for LLMs to generate meta-reviews in extensive experiments.
&lt;/p&gt;</description></item><item><title>FlattenQuant&#26041;&#27861;&#36890;&#36807;&#23637;&#24179;&#24352;&#37327;&#20013;&#30340;&#22823;&#36890;&#36947;&#65292;&#23454;&#29616;&#20102;&#20302;&#27604;&#29305;&#27599;&#24352;&#37327;&#37327;&#21270;&#65292;&#38477;&#20302;&#20102;&#20934;&#30830;&#24615;&#25439;&#22833;</title><link>https://arxiv.org/abs/2402.17985</link><description>&lt;p&gt;
FlattenQuant: &#20351;&#29992;&#20998;&#24352;&#37327;&#37327;&#21270;&#25171;&#30772;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#35745;&#31639;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
FlattenQuant: Breaking Through the Inference Compute-bound for Large Language Models with Per-tensor Quantization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17985
&lt;/p&gt;
&lt;p&gt;
FlattenQuant&#26041;&#27861;&#36890;&#36807;&#23637;&#24179;&#24352;&#37327;&#20013;&#30340;&#22823;&#36890;&#36947;&#65292;&#23454;&#29616;&#20102;&#20302;&#27604;&#29305;&#27599;&#24352;&#37327;&#37327;&#21270;&#65292;&#38477;&#20302;&#20102;&#20934;&#30830;&#24615;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#39046;&#20808;&#30340;&#24615;&#33021;&#65292;&#28982;&#32780;&#65292;&#25512;&#26029;&#30340;&#24310;&#36831;&#21644;LLMs&#30340;&#22823;GPU&#20869;&#23384;&#28040;&#32791;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#37096;&#32626;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FlattenQuant&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#24352;&#37327;&#20013;&#30340;&#22823;&#36890;&#36947;&#36827;&#34892;&#23637;&#24179;&#26469;&#26174;&#33879;&#38477;&#20302;&#24352;&#37327;&#30340;&#26368;&#22823;&#20540;&#65292;&#23454;&#29616;&#20102;&#20302;&#27604;&#29305;&#27599;&#24352;&#37327;&#37327;&#21270;&#65292;&#20943;&#23567;&#20102;&#20934;&#30830;&#24615;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17985v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated state-of-the-art performance across various tasks. However, the latency of inference and the large GPU memory consumption of LLMs restrict their deployment performance. Recently, there have been some efficient attempts to quantize LLMs, yet inference with large batch size or long sequence still has the issue of being compute-bound. Fine-grained quantization methods have showcased their proficiency in achieving low-bit quantization for LLMs, while requiring FP16 data type for linear layer computations, which is time-consuming when dealing with large batch size or long sequence. In this paper, we introduce a method called FlattenQuant, which significantly reduces the maximum value of the tensor by flattening the large channels in the tensor, to achieve low bit per-tensor quantization with minimal accuracy loss. Our experiments show that FlattenQuant can directly use 4 bits to achieve 48.29% 
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#27169;&#22411;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#12289;&#22810;&#20219;&#21153;&#12289;&#22810;&#25945;&#24072;&#30340;&#32852;&#21512;&#32454;&#31890;&#24230;&#30693;&#35782;&#33976;&#39311;&#27169;&#22411;&#65292;&#36890;&#36807;&#24494;&#22937;&#21327;&#20316;&#20196;&#29260;&#21644;&#23454;&#20307;&#34920;&#31034;&#65292;&#22788;&#29702;&#22797;&#26434;&#30340;&#34920;&#21333;&#25991;&#26723;&#65292;&#24341;&#20837;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#25913;&#36827;&#30693;&#35782;&#33976;&#39311;&#36807;&#31243;&#65292;&#22312;&#22788;&#29702;&#35270;&#35273;&#22797;&#26434;&#34920;&#21333;&#25991;&#26723;&#30340;&#32467;&#26500;&#21644;&#20869;&#23481;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.17983</link><description>&lt;p&gt;
M3-VRD: &#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#22810;&#25945;&#24072;&#35270;&#35273;&#20016;&#23500;&#34920;&#21333;&#25991;&#26723;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
M3-VRD: Multimodal Multi-task Multi-teacher Visually-Rich Form Document Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17983
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#27169;&#22411;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#12289;&#22810;&#20219;&#21153;&#12289;&#22810;&#25945;&#24072;&#30340;&#32852;&#21512;&#32454;&#31890;&#24230;&#30693;&#35782;&#33976;&#39311;&#27169;&#22411;&#65292;&#36890;&#36807;&#24494;&#22937;&#21327;&#20316;&#20196;&#29260;&#21644;&#23454;&#20307;&#34920;&#31034;&#65292;&#22788;&#29702;&#22797;&#26434;&#30340;&#34920;&#21333;&#25991;&#26723;&#65292;&#24341;&#20837;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#25913;&#36827;&#30693;&#35782;&#33976;&#39311;&#36807;&#31243;&#65292;&#22312;&#22788;&#29702;&#35270;&#35273;&#22797;&#26434;&#34920;&#21333;&#25991;&#26723;&#30340;&#32467;&#26500;&#21644;&#20869;&#23481;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31361;&#30772;&#24615;&#30340;&#22810;&#27169;&#24577;&#12289;&#22810;&#20219;&#21153;&#12289;&#22810;&#25945;&#24072;&#32852;&#21512;&#32454;&#31890;&#24230;&#30693;&#35782;&#33976;&#39311;&#27169;&#22411;&#65292;&#29992;&#20110;&#35270;&#35273;&#20016;&#23500;&#30340;&#34920;&#21333;&#25991;&#26723;&#29702;&#35299;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#36890;&#36807;&#20419;&#36827;&#20196;&#29260;&#21644;&#23454;&#20307;&#34920;&#31034;&#20043;&#38388;&#30340;&#24494;&#22937;&#30456;&#20851;&#24615;&#26469;&#21033;&#29992;&#32454;&#31890;&#24230;&#21644;&#31895;&#31890;&#24230;&#32423;&#21035;&#30340;&#35265;&#35299;&#65292;&#35299;&#20915;&#34920;&#21333;&#25991;&#26723;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#36328;&#32454;&#31890;&#24230;&#21644;&#36328;&#31895;&#31890;&#24230;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#36827;&#19968;&#27493;&#25913;&#36827;&#22810;&#25945;&#24072;&#30693;&#35782;&#33976;&#39311;&#20256;&#36882;&#36807;&#31243;&#65292;&#21576;&#29616;&#20998;&#24067;&#24046;&#36317;&#21644;&#23545;&#34920;&#21333;&#25991;&#26723;&#30340;&#32479;&#19968;&#29702;&#35299;&#12290;&#36890;&#36807;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#34920;&#21333;&#25991;&#26723;&#29702;&#35299;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22987;&#32456;&#34920;&#29616;&#20986;&#33394;&#22320;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#22788;&#29702;&#22797;&#26434;&#35270;&#35273;&#34920;&#21333;&#25991;&#26723;&#30340;&#22797;&#26434;&#32467;&#26500;&#21644;&#20869;&#23481;&#26041;&#38754;&#30340;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17983v1 Announce Type: new  Abstract: This paper presents a groundbreaking multimodal, multi-task, multi-teacher joint-grained knowledge distillation model for visually-rich form document understanding. The model is designed to leverage insights from both fine-grained and coarse-grained levels by facilitating a nuanced correlation between token and entity representations, addressing the complexities inherent in form documents. Additionally, we introduce new inter-grained and cross-grained loss functions to further refine diverse multi-teacher knowledge distillation transfer process, presenting distribution gaps and a harmonised understanding of form documents. Through a comprehensive evaluation across publicly available form document understanding datasets, our proposed model consistently outperforms existing baselines, showcasing its efficacy in handling the intricate structures and content of visually complex form documents.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#21327;&#21516;&#35299;&#30721;&#26694;&#26550;&#36890;&#36807;&#20851;&#38190;&#26631;&#35760;&#27010;&#24565;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#39640;&#20107;&#23454;&#24615;&#65292;&#35774;&#35745;&#20851;&#38190;&#26631;&#35760;&#20998;&#31867;&#22120;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#65292;&#26377;&#25928;&#38477;&#20302;&#24187;&#35273;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2402.17982</link><description>&lt;p&gt;
&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20107;&#23454;&#24615;&#30340;&#20851;&#38190;&#26631;&#35760;&#21327;&#21516;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Collaborative decoding of critical tokens for boosting factuality of large language models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17982
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#21327;&#21516;&#35299;&#30721;&#26694;&#26550;&#36890;&#36807;&#20851;&#38190;&#26631;&#35760;&#27010;&#24565;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#39640;&#20107;&#23454;&#24615;&#65292;&#35774;&#35745;&#20851;&#38190;&#26631;&#35760;&#20998;&#31867;&#22120;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#65292;&#26377;&#25928;&#38477;&#20302;&#24187;&#35273;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24120;&#35265;&#35757;&#32451;&#27969;&#31243;&#21253;&#25324;&#39044;&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#23545;&#40784;&#38454;&#27573;&#65292;&#20135;&#29983;&#30456;&#24212;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#24494;&#35843;&#27169;&#22411;&#12290;&#24494;&#35843;&#21644;&#23545;&#40784;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#25913;&#36827;&#30340;&#25351;&#20196;&#36981;&#24490;&#21644;&#23433;&#20840;&#29983;&#25104;&#33021;&#21147;&#65292;&#28982;&#32780;&#23427;&#20204;&#22312;&#19990;&#30028;&#20107;&#23454;&#26041;&#38754;&#30340;&#33021;&#21147;&#21463;&#24494;&#35843;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#24120;&#35265;&#30340;&#20351;&#29992;&#37319;&#26679;&#30340;&#20570;&#27861;&#20063;&#22686;&#21152;&#20102;&#24187;&#35273;&#30340;&#21487;&#33021;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21327;&#21516;&#35299;&#30721;&#26694;&#26550;&#65292;&#36890;&#36807;&#20851;&#38190;&#26631;&#35760;&#30340;&#27010;&#24565;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#39640;&#20107;&#23454;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;&#20851;&#38190;&#26631;&#35760;&#20998;&#31867;&#22120;&#26469;&#20915;&#23450;&#19979;&#19968;&#20010;&#26631;&#35760;&#20351;&#29992;&#21738;&#20010;&#27169;&#22411;&#65292;&#28982;&#21518;&#20351;&#29992;&#19981;&#21516;&#30340;&#35299;&#30721;&#31574;&#30053;&#29983;&#25104;&#19979;&#19968;&#20010;&#26631;&#35760;&#12290;&#23545;&#19981;&#21516;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#35299;&#30721;&#26694;&#26550;&#33021;&#22815;&#20943;&#23569;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17982v1 Announce Type: new  Abstract: The most common training pipeline for large language models includes pretraining, finetuning and aligning phases, with their respective resulting models, such as the pretrained model and the finetuned model. Finetuned and aligned models show improved abilities of instruction following and safe generation, however their abilities to stay factual about the world are impacted by the finetuning process. Furthermore, the common practice of using sampling during generation also increases chances of hallucination. In this work, we introduce a collaborative decoding framework to harness the high factuality within pretrained models through the concept of critical tokens. We first design a critical token classifier to decide which model to use for the next token, and subsequently generates the next token using different decoding strategies. Experiments with different models and datasets show that our decoding framework is able to reduce model hall
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#29255;&#20869;&#23398;&#20064;&#65288;I$^2$L&#65289;&#30340;&#26032;&#22411;&#19978;&#19979;&#25991;&#23398;&#20064;&#26426;&#21046;&#65292;&#23558;&#28436;&#31034;&#31034;&#20363;&#12289;&#35270;&#35273;&#32447;&#32034;&#21644;&#25351;&#20196;&#21512;&#24182;&#21040;&#19968;&#20010;&#22270;&#29255;&#20013;&#65292;&#20197;&#25552;&#21319;GPT-4V&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#25972;&#21512;&#22270;&#20687;&#22788;&#29702;&#12289;&#29702;&#35299;&#21644;&#25512;&#29702;&#30340;&#33021;&#21147;&#26469;&#21462;&#24471;&#22810;&#20010;&#20248;&#28857;</title><link>https://arxiv.org/abs/2402.17971</link><description>&lt;p&gt;
&#19968;&#24352;&#22270;&#29255;&#25630;&#23450;&#65306;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#26159;&#22270;&#29255;&#20869;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
All in a Single Image: Large Multimodal Models are In-Image Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17971
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#29255;&#20869;&#23398;&#20064;&#65288;I$^2$L&#65289;&#30340;&#26032;&#22411;&#19978;&#19979;&#25991;&#23398;&#20064;&#26426;&#21046;&#65292;&#23558;&#28436;&#31034;&#31034;&#20363;&#12289;&#35270;&#35273;&#32447;&#32034;&#21644;&#25351;&#20196;&#21512;&#24182;&#21040;&#19968;&#20010;&#22270;&#29255;&#20013;&#65292;&#20197;&#25552;&#21319;GPT-4V&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#25972;&#21512;&#22270;&#20687;&#22788;&#29702;&#12289;&#29702;&#35299;&#21644;&#25512;&#29702;&#30340;&#33021;&#21147;&#26469;&#21462;&#24471;&#22810;&#20010;&#20248;&#28857;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#29255;&#20869;&#23398;&#20064;&#65288;I$^2$L&#65289;&#30340;&#26032;&#22411;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#26426;&#21046;&#65292;&#23558;&#28436;&#31034;&#31034;&#20363;&#12289;&#35270;&#35273;&#32447;&#32034;&#21644;&#25351;&#20196;&#21512;&#24182;&#21040;&#19968;&#24352;&#22270;&#29255;&#20013;&#65292;&#20197;&#22686;&#24378;GPT-4V&#30340;&#33021;&#21147;&#12290;&#19982;&#20197;&#24448;&#20381;&#36182;&#23558;&#22270;&#20687;&#36716;&#25442;&#20026;&#25991;&#26412;&#25110;&#23558;&#35270;&#35273;&#36755;&#20837;&#34701;&#20837;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;I$^2$L&#23558;&#25152;&#26377;&#20449;&#24687;&#25972;&#21512;&#21040;&#19968;&#24352;&#22270;&#29255;&#20013;&#65292;&#20027;&#35201;&#21033;&#29992;&#22270;&#20687;&#22788;&#29702;&#12289;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#26377;&#20960;&#20010;&#20248;&#28857;&#65306;&#36991;&#20813;&#20102;&#23545;&#22797;&#26434;&#22270;&#20687;&#30340;&#19981;&#20934;&#30830;&#25991;&#26412;&#25551;&#36848;&#65292;&#25552;&#20379;&#20102;&#22312;&#23450;&#20301;&#28436;&#31034;&#31034;&#20363;&#26102;&#30340;&#28789;&#27963;&#24615;&#65292;&#20943;&#23569;&#20102;&#36755;&#20837;&#36127;&#25285;&#65292;&#24182;&#36890;&#36807;&#28040;&#38500;&#23545;&#22810;&#20010;&#22270;&#29255;&#21644;&#20887;&#38271;&#25991;&#26412;&#30340;&#38656;&#27714;&#26469;&#36991;&#20813;&#36229;&#36807;&#36755;&#20837;&#38480;&#21046;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#32467;&#21512;&#19981;&#21516;ICL&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#21160;&#31574;&#30053;&#65292;&#29992;&#20110;&#36873;&#25321;&#32473;&#23450;&#20219;&#21153;&#20013;&#25968;&#25454;&#31034;&#20363;&#30340;&#36866;&#24403;ICL&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;MathVi&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17971v1 Announce Type: cross  Abstract: This paper introduces a new in-context learning (ICL) mechanism called In-Image Learning (I$^2$L) that combines demonstration examples, visual cues, and instructions into a single image to enhance the capabilities of GPT-4V. Unlike previous approaches that rely on converting images to text or incorporating visual input into language models, I$^2$L consolidates all information into one image and primarily leverages image processing, understanding, and reasoning abilities. This has several advantages: it avoids inaccurate textual descriptions of complex images, provides flexibility in positioning demonstration examples, reduces the input burden, and avoids exceeding input limits by eliminating the need for multiple images and lengthy text. To further combine the strengths of different ICL methods, we introduce an automatic strategy to select the appropriate ICL method for a data example in a given task. We conducted experiments on MathVi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20849;&#24773;&#21709;&#24212;&#29983;&#25104;&#30340;&#36845;&#20195;&#20851;&#32852;&#35760;&#24518;&#27169;&#22411;&#65292;&#37319;&#29992;&#20108;&#38454;&#20132;&#20114;&#27880;&#24847;&#26426;&#21046;&#36845;&#20195;&#22320;&#25429;&#25417;&#30456;&#20851;&#35789;&#35821;&#65292;&#23454;&#29616;&#20934;&#30830;&#12289;&#32454;&#33268;&#22320;&#29702;&#35299;&#35805;&#35821;&#12290;</title><link>https://arxiv.org/abs/2402.17959</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#20849;&#24773;&#21709;&#24212;&#29983;&#25104;&#30340;&#36845;&#20195;&#20851;&#32852;&#35760;&#24518;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
An Iterative Associative Memory Model for Empathetic Response Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17959
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20849;&#24773;&#21709;&#24212;&#29983;&#25104;&#30340;&#36845;&#20195;&#20851;&#32852;&#35760;&#24518;&#27169;&#22411;&#65292;&#37319;&#29992;&#20108;&#38454;&#20132;&#20114;&#27880;&#24847;&#26426;&#21046;&#36845;&#20195;&#22320;&#25429;&#25417;&#30456;&#20851;&#35789;&#35821;&#65292;&#23454;&#29616;&#20934;&#30830;&#12289;&#32454;&#33268;&#22320;&#29702;&#35299;&#35805;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20849;&#24773;&#21709;&#24212;&#29983;&#25104;&#26159;&#29702;&#35299;&#23545;&#35805;&#35805;&#35821;&#20013;&#30340;&#35748;&#30693;&#21644;&#24773;&#24863;&#29366;&#24577;&#65292;&#24182;&#29983;&#25104;&#24688;&#24403;&#22238;&#24212;&#12290;&#24515;&#29702;&#23398;&#29702;&#35770;&#35748;&#20026;&#65292;&#29702;&#35299;&#24773;&#24863;&#21644;&#35748;&#30693;&#29366;&#24577;&#38656;&#35201;&#36845;&#20195;&#22320;&#25429;&#25417;&#21644;&#29702;&#35299;&#23545;&#35805;&#35805;&#35821;&#20043;&#38388;&#30340;&#30456;&#20851;&#35789;&#35821;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#23558;&#23545;&#35805;&#35805;&#35821;&#35270;&#20026;&#38271;&#24207;&#21015;&#25110;&#29420;&#31435;&#35805;&#35821;&#26469;&#29702;&#35299;&#65292;&#24448;&#24448;&#20250;&#24573;&#35270;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#32852;&#35789;&#35821;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20849;&#24773;&#21709;&#24212;&#29983;&#25104;&#30340;&#36845;&#20195;&#20851;&#32852;&#35760;&#24518;&#27169;&#22411;&#65288;IAMM&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#20108;&#38454;&#20132;&#20114;&#27880;&#24847;&#26426;&#21046;&#65292;&#36845;&#20195;&#22320;&#25429;&#25417;&#23545;&#35805;&#35805;&#35821;&#12289;&#24773;&#22659;&#12289;&#23545;&#35805;&#21382;&#21490;&#21644;&#35760;&#24518;&#27169;&#22359;&#65288;&#29992;&#20110;&#23384;&#20648;&#30456;&#20851;&#35789;&#35821;&#65289;&#20043;&#38388;&#30340;&#37325;&#35201;&#20851;&#32852;&#35789;&#35821;&#65292;&#20174;&#32780;&#20934;&#30830;&#32780;&#32454;&#33268;&#22320;&#29702;&#35299;&#35805;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17959v1 Announce Type: new  Abstract: Empathetic response generation is to comprehend the cognitive and emotional states in dialogue utterances and generate proper responses. Psychological theories posit that comprehending emotional and cognitive states necessitates iteratively capturing and understanding associated words across dialogue utterances. However, existing approaches regard dialogue utterances as either a long sequence or independent utterances for comprehension, which are prone to overlook the associated words between them. To address this issue, we propose an Iterative Associative Memory Model (IAMM) for empathetic response generation. Specifically, we employ a novel second-order interaction attention mechanism to iteratively capture vital associated words between dialogue utterances and situations, dialogue history, and a memory module (for storing associated words), thereby accurately and nuancedly comprehending the utterances. We conduct experiments on the Em
&lt;/p&gt;</description></item><item><title>&#22810;&#35821;&#35328;&#35821;&#38899;&#27169;&#22411;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#34920;&#29616;&#20986;&#24615;&#21035;&#24046;&#36317;&#65292;&#19988;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#21463;&#30410;&#30340;&#24615;&#21035;&#32676;&#20307;&#21508;&#19981;&#30456;&#21516;&#12290;</title><link>https://arxiv.org/abs/2402.17954</link><description>&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#22810;&#35821;&#35328;&#35821;&#38899;&#27169;&#22411;&#23384;&#22312;&#24615;&#21035;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Multilingual Speech Models for Automatic Speech Recognition Exhibit Gender Performance Gaps
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17954
&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#35821;&#38899;&#27169;&#22411;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#34920;&#29616;&#20986;&#24615;&#21035;&#24046;&#36317;&#65292;&#19988;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#21463;&#30410;&#30340;&#24615;&#21035;&#32676;&#20307;&#21508;&#19981;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#35821;&#38899;&#35782;&#21035;&#26041;&#27861;&#20351;&#29992;&#22810;&#20219;&#21153;&#12289;&#22810;&#35821;&#35328;&#27169;&#22411;&#26469;&#36827;&#34892;&#35832;&#22914;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#30340;&#35821;&#38899;&#20219;&#21153;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#35768;&#22810;&#35821;&#35328;&#32780;&#19981;&#38656;&#35201;&#23454;&#36136;&#24615;&#30340;&#26356;&#25913;&#12290;&#28982;&#32780;&#65292;&#24191;&#27867;&#30340;&#35821;&#35328;&#35206;&#30422;&#20173;&#28982;&#21487;&#33021;&#25513;&#30422;&#35821;&#35328;&#20869;&#37096;&#23384;&#22312;&#30340;&#24615;&#21035;&#24046;&#36317;&#12290;&#26412;&#25991;&#31995;&#32479;&#35780;&#20272;&#22810;&#35821;&#35328;ASR&#31995;&#32479;&#22312;&#24615;&#21035;&#34920;&#29616;&#24046;&#36317;&#19978;&#30340;&#24773;&#20917;&#12290;&#22312;19&#31181;&#35821;&#35328;&#30340;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#20004;&#31181;&#27969;&#34892;&#27169;&#22411;&#65292;&#36328;&#36234;&#19971;&#20010;&#35821;&#35328;&#23478;&#26063;&#65292;&#25105;&#20204;&#21457;&#29616;&#26126;&#26174;&#30340;&#24615;&#21035;&#24046;&#24322;&#12290;&#19981;&#36807;&#65292;&#19981;&#21516;&#35821;&#35328;&#20013;&#21463;&#30410;&#30340;&#32676;&#20307;&#21508;&#19981;&#30456;&#21516;&#12290;&#23613;&#31649;&#22312;&#35821;&#38899;&#23398;&#21464;&#37327;&#65288;&#38899;&#39640;&#12289;&#35828;&#35805;&#36895;&#24230;&#31561;&#65289;&#19978;&#21508;&#32676;&#20307;&#38388;&#27809;&#26377;&#26174;&#33879;&#24046;&#24322;&#65292;&#20294;&#25506;&#32034;&#27169;&#22411;&#20869;&#37096;&#29366;&#24577;&#21364;&#25581;&#31034;&#20102;&#25506;&#26597;&#24615;&#33021;&#21644;&#24615;&#21035;&#34920;&#29616;&#24046;&#36317;&#20043;&#38388;&#30340;&#36127;&#30456;&#20851;&#20851;&#31995;&#12290;&#21363;&#65292;&#22312;&#26576;&#31181;&#35821;&#35328;&#20013;&#26356;&#23481;&#26131;&#21306;&#20998;&#35828;&#35805;&#32773;&#24615;&#21035;&#65292;&#27169;&#22411;&#23601;&#26356;&#20559;&#21521;&#20110;&#22899;&#24615;&#35828;&#35805;&#32773;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#32452;&#21035;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17954v1 Announce Type: new  Abstract: Current voice recognition approaches use multi-task, multilingual models for speech tasks like Automatic Speech Recognition (ASR) to make them applicable to many languages without substantial changes. However, broad language coverage can still mask performance gaps within languages, for example, across genders. We systematically evaluate multilingual ASR systems on gendered performance gaps. Using two popular models on three datasets in 19 languages across seven language families, we find clear gender disparities. However, the advantaged group varies between languages. While there are no significant differences across groups in phonetic variables (pitch, speaking rate, etc.), probing the model's internal states reveals a negative correlation between probe performance and the gendered performance gap. I.e., the easier to distinguish speaker gender in a language, the more the models favor female speakers. Our results show that group dispar
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#20840;&#23616;&#21098;&#26525;&#65288;AdaGP&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#37325;&#26032;&#23450;&#20041;&#20840;&#23616;&#21098;&#26525;&#36807;&#31243;&#20026;&#21487;&#31649;&#29702;&#30340;&#21327;&#35843;&#23376;&#38382;&#39064;&#65292;&#23454;&#29616;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36164;&#28304;&#39640;&#25928;&#20248;&#21270;&#65292;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.17946</link><description>&lt;p&gt;
&#26080;&#26799;&#24230;&#33258;&#36866;&#24212;&#20840;&#23616;&#21098;&#26525;&#29992;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Gradient-Free Adaptive Global Pruning for Pre-trained Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17946
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#20840;&#23616;&#21098;&#26525;&#65288;AdaGP&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#37325;&#26032;&#23450;&#20041;&#20840;&#23616;&#21098;&#26525;&#36807;&#31243;&#20026;&#21487;&#31649;&#29702;&#30340;&#21327;&#35843;&#23376;&#38382;&#39064;&#65292;&#23454;&#29616;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36164;&#28304;&#39640;&#25928;&#20248;&#21270;&#65292;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;LLaMA&#21644;GPT&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#36716;&#21464;&#24615;&#24433;&#21709;&#21463;&#21040;&#23427;&#20204;&#35745;&#31639;&#38656;&#27714;&#36807;&#39640;&#30340;&#38480;&#21046;&#12290;&#21098;&#26525;&#20316;&#20026;&#19968;&#31181;&#20851;&#38190;&#30340;&#21387;&#32553;&#31574;&#30053;&#20986;&#29616;&#65292;&#24341;&#20837;&#31232;&#30095;&#24615;&#20197;&#22686;&#24378;&#20869;&#23384;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#20840;&#23616;&#21098;&#26525;&#23545;LLMs&#26469;&#35828;&#30001;&#20110;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#32780;&#19981;&#23454;&#29992;&#65292;&#32780;&#26412;&#22320;&#21098;&#26525;&#65292;&#23613;&#31649;&#25928;&#29575;&#39640;&#65292;&#21364;&#23548;&#33268;&#27425;&#20248;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#20840;&#23616;&#21098;&#26525;&#65288;AdaGP&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#37325;&#26032;&#23450;&#20041;&#20840;&#23616;&#21098;&#26525;&#22788;&#29702;&#20026;&#21487;&#31649;&#29702;&#30340;&#21327;&#35843;&#23376;&#38382;&#39064;&#30340;&#26032;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#36164;&#28304;&#26377;&#25928;&#30340;&#20840;&#23616;&#26368;&#20248;&#21270;&#20248;&#21270;&#12290;AdaGP&#30340;&#26041;&#27861;&#23558;LLMs&#27010;&#24565;&#21270;&#20026;&#19968;&#31995;&#21015;&#27169;&#22359;&#21270;&#20989;&#25968;&#65292;&#24182;&#21033;&#29992;&#36741;&#21161;&#21464;&#37327;&#36827;&#34892;&#38382;&#39064;&#20998;&#35299;&#65292;&#19981;&#20165;&#20415;&#20110;&#22312;LLMs&#19978;&#23454;&#29616;&#23454;&#38469;&#24212;&#29992;&#65292;&#32780;&#19988;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17946v1 Announce Type: new  Abstract: The transformative impact of large language models (LLMs) like LLaMA and GPT on natural language processing is countered by their prohibitive computational demands. Pruning has emerged as a pivotal compression strategy, introducing sparsity to enhance both memory and computational efficiency. Yet, traditional global pruning is impractical for LLMs due to scalability issues, while local pruning, despite its efficiency, leads to suboptimal solutions. Addressing these challenges, we propose Adaptive Global Pruning (AdaGP), a novel framework that redefines the global pruning process into manageable, coordinated subproblems, allowing for resource-efficient optimization with global optimality. AdaGP's approach, which conceptualizes LLMs as a chain of modular functions and leverages auxiliary variables for problem decomposition, not only facilitates a pragmatic application on LLMs but also demonstrates significant performance improvements, part
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#20851;&#38190;&#25216;&#26415;&#12289;&#25351;&#26631;&#12289;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.17944</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;--&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Large Language Models on Tabular Data -- A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17944
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#20851;&#38190;&#25216;&#26415;&#12289;&#25351;&#26631;&#12289;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#26684;&#25968;&#25454;&#24314;&#27169;&#26041;&#38754;&#30340;&#24212;&#29992;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#36827;&#23637;&#65292;&#21253;&#25324;&#39044;&#27979;&#12289;&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;&#12289;&#38382;&#31572;&#21644;&#34920;&#26684;&#29702;&#35299;&#31561;&#22810;&#31181;&#20219;&#21153;&#12290;&#27599;&#20010;&#20219;&#21153;&#37117;&#24102;&#26469;&#29420;&#29305;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#32570;&#20047;&#23545;&#35813;&#30740;&#31350;&#39046;&#22495;&#20013;&#20851;&#38190;&#25216;&#26415;&#12289;&#25351;&#26631;&#12289;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#20248;&#21270;&#26041;&#27861;&#30340;&#20840;&#38754;&#23457;&#26597;&#12290;&#26412;&#35843;&#26597;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#24635;&#32467;&#24182;&#27604;&#36739;&#36825;&#20123;&#39046;&#22495;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25552;&#20379;&#23545;&#25968;&#25454;&#38598;&#12289;&#25351;&#26631;&#21644;&#26041;&#27861;&#35770;&#30340;&#20840;&#38754;&#35843;&#26597;&#21644;&#20998;&#31867;&#12290;&#23427;&#35782;&#21035;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#20248;&#21183;&#12289;&#23616;&#38480;&#24615;&#12289;&#26410;&#24320;&#21457;&#39046;&#22495;&#21644;&#31354;&#30333;&#65292;&#21516;&#26102;&#20026;&#36825;&#19968;&#37325;&#35201;&#19988;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#20102;&#19968;&#20123;&#35265;&#35299;&#12290;&#23427;&#36824;&#25552;&#20379;&#20102;&#30456;&#20851;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#24341;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17944v1 Announce Type: new  Abstract: Recent breakthroughs in large language modeling have facilitated rigorous exploration of their application in diverse tasks related to tabular data modeling, such as prediction, tabular data synthesis, question answering, and table understanding. Each task presents unique challenges and opportunities. However, there is currently a lack of comprehensive review that summarizes and compares the key techniques, metrics, datasets, models, and optimization approaches in this research domain. This survey aims to address this gap by consolidating recent progress in these areas, offering a thorough survey and taxonomy of the datasets, metrics, and methodologies utilized. It identifies strengths, limitations, unexplored territories, and gaps in the existing literature, while providing some insights for future research directions in this vital and rapidly evolving field. It also provides relevant code and datasets references. Through this comprehen
&lt;/p&gt;</description></item><item><title>EmMark&#26159;&#19968;&#31181;&#26032;&#22411;&#27700;&#21360;&#26694;&#26550;&#65292;&#29992;&#20110;&#20445;&#25252;&#23884;&#20837;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#20135;&#26435;&#65292;&#22312;&#40065;&#26834;&#24615;&#21644;&#27169;&#22411;&#36136;&#37327;&#32500;&#25345;&#20043;&#38388;&#23454;&#29616;&#24179;&#34913;&#65292;&#25104;&#21151;&#24212;&#23545;&#20102;IP&#30423;&#31363;&#39118;&#38505;&#65292;&#20855;&#26377;100%&#30340;&#27700;&#21360;&#25552;&#21462;&#25104;&#21151;&#29575;&#65292;&#24182;&#25269;&#24481;&#20102;&#27700;&#21360;&#21024;&#38500;&#21644;&#20266;&#36896;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2402.17938</link><description>&lt;p&gt;
EmMark: &#38024;&#23545;&#23884;&#20837;&#24335;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#30340;&#40065;&#26834;&#24615;&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
EmMark: Robust Watermarks for IP Protection of Embedded Quantized Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17938
&lt;/p&gt;
&lt;p&gt;
EmMark&#26159;&#19968;&#31181;&#26032;&#22411;&#27700;&#21360;&#26694;&#26550;&#65292;&#29992;&#20110;&#20445;&#25252;&#23884;&#20837;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#20135;&#26435;&#65292;&#22312;&#40065;&#26834;&#24615;&#21644;&#27169;&#22411;&#36136;&#37327;&#32500;&#25345;&#20043;&#38388;&#23454;&#29616;&#24179;&#34913;&#65292;&#25104;&#21151;&#24212;&#23545;&#20102;IP&#30423;&#31363;&#39118;&#38505;&#65292;&#20855;&#26377;100%&#30340;&#27700;&#21360;&#25552;&#21462;&#25104;&#21151;&#29575;&#65292;&#24182;&#25269;&#24481;&#20102;&#27700;&#21360;&#21024;&#38500;&#21644;&#20266;&#36896;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;EmMark&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#27700;&#21360;&#26694;&#26550;&#65292;&#29992;&#20110;&#20445;&#25252;&#37096;&#32626;&#22312;&#36164;&#28304;&#21463;&#38480;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#23884;&#20837;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#20135;&#26435;&#65288;IP&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#24694;&#24847;&#26368;&#32456;&#29992;&#25143;&#24102;&#26469;&#30340;IP&#30423;&#31363;&#39118;&#38505;&#65292;EmMark&#20351;&#25152;&#26377;&#32773;&#33021;&#22815;&#36890;&#36807;&#26597;&#35810;&#24102;&#26377;&#27700;&#21360;&#30340;&#27169;&#22411;&#26435;&#37325;&#24182;&#21305;&#37197;&#25554;&#20837;&#30340;&#31614;&#21517;&#26469;&#39564;&#35777;&#25152;&#26377;&#26435;&#12290;EmMark&#30340;&#21019;&#26032;&#22312;&#20110;&#20854;&#25112;&#30053;&#24615;&#27700;&#21360;&#26435;&#37325;&#21442;&#25968;&#36873;&#25321;&#65292;&#30830;&#20445;&#20102;&#40065;&#26834;&#24615;&#24182;&#20445;&#25345;&#27169;&#22411;&#36136;&#37327;&#12290;OPT&#21644;LLaMA-2&#31995;&#21015;&#27169;&#22411;&#30340;&#22823;&#37327;&#27010;&#24565;&#39564;&#35777;&#35780;&#20272;&#23637;&#31034;&#20102;EmMark&#30340;&#20934;&#30830;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#20445;&#30041;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#27700;&#21360;&#25552;&#21462;&#25104;&#21151;&#29575;&#36798;&#21040;100%&#12290;EmMark&#36824;&#23637;&#31034;&#20102;&#20854;&#23545;&#25239;&#27700;&#21360;&#21435;&#38500;&#21644;&#31713;&#25913;&#25915;&#20987;&#30340;&#38887;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17938v1 Announce Type: cross  Abstract: This paper introduces EmMark,a novel watermarking framework for protecting the intellectual property (IP) of embedded large language models deployed on resource-constrained edge devices. To address the IP theft risks posed by malicious end-users, EmMark enables proprietors to authenticate ownership by querying the watermarked model weights and matching the inserted signatures. EmMark's novelty lies in its strategic watermark weight parameters selection, nsuring robustness and maintaining model quality. Extensive proof-of-concept evaluations of models from OPT and LLaMA-2 families demonstrate EmMark's fidelity, achieving 100% success in watermark extraction with model performance preservation. EmMark also showcased its resilience against watermark removal and forging attacks.
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#27169;&#22411;&#22312;&#33719;&#21462;&#35821;&#35328;&#30693;&#35782;&#26102;&#34920;&#29616;&#20986;&#36739;&#20302;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#30740;&#31350;&#21457;&#29616;&#36825;&#37096;&#20998;&#21407;&#22240;&#21487;&#33021;&#26159;&#30001;&#20110;&#32570;&#20047;&#22810;&#27169;&#24577;&#36755;&#20837;&#21644;&#25509;&#22320;&#12290;&#20182;&#20204;&#36890;&#36807;&#23545;FLAVA&#27169;&#22411;&#36827;&#34892;&#28040;&#34701;&#30740;&#31350;&#26469;&#39564;&#35777;&#20551;&#35774;&#65292;&#24182;&#35797;&#22270;&#36890;&#36807;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#26469;&#20943;&#23569;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;</title><link>https://arxiv.org/abs/2402.17936</link><description>&lt;p&gt;
&#20174;&#22810;&#27169;&#24577;&#36755;&#20837;&#33719;&#21462;&#35821;&#35328;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Acquiring Linguistic Knowledge from Multimodal Input
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17936
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#33719;&#21462;&#35821;&#35328;&#30693;&#35782;&#26102;&#34920;&#29616;&#20986;&#36739;&#20302;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#30740;&#31350;&#21457;&#29616;&#36825;&#37096;&#20998;&#21407;&#22240;&#21487;&#33021;&#26159;&#30001;&#20110;&#32570;&#20047;&#22810;&#27169;&#24577;&#36755;&#20837;&#21644;&#25509;&#22320;&#12290;&#20182;&#20204;&#36890;&#36807;&#23545;FLAVA&#27169;&#22411;&#36827;&#34892;&#28040;&#34701;&#30740;&#31350;&#26469;&#39564;&#35777;&#20551;&#35774;&#65292;&#24182;&#35797;&#22270;&#36890;&#36807;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#26469;&#20943;&#23569;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20799;&#31461;&#30456;&#27604;&#65292;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#33719;&#21462;&#35821;&#35328;&#26102;&#26174;&#31034;&#20986;&#26126;&#26174;&#36739;&#20302;&#30340;&#25968;&#25454;&#25928;&#29575;&#12290;&#26412;&#25991;&#25552;&#20132;&#32473;BabyLM&#25361;&#25112;&#65288;Warstadt &#31561;&#20154;&#65292;2023&#65289;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#36825;&#19968;&#25968;&#25454;&#25928;&#29575;&#24046;&#36317;&#37096;&#20998;&#26159;&#30001;&#20110;&#20856;&#22411;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#29615;&#22659;&#20013;&#32570;&#20047;&#22810;&#27169;&#24577;&#36755;&#20837;&#21644;&#25509;&#22320;&#24341;&#36215;&#30340;&#20551;&#35774;&#12290;&#23613;&#31649;&#20808;&#21069;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22810;&#27169;&#24577;&#35757;&#32451;&#29978;&#33267;&#21487;&#33021;&#23545;&#20165;&#35821;&#35328;&#34920;&#29616;&#36896;&#25104;&#20260;&#23475;&#65292;&#25105;&#20204;&#25512;&#27979;&#36825;&#20123;&#21457;&#29616;&#21487;&#20197;&#24402;&#22240;&#20110;&#23545;&#26631;&#39064;&#25968;&#25454;&#24494;&#35843;&#23548;&#33268;&#22797;&#26434;&#35821;&#35328;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#20026;&#39564;&#35777;&#25105;&#20204;&#30340;&#20551;&#35774;&#65292;&#25105;&#20204;&#22312;FLAVA&#65288;Singh &#31561;&#20154;&#65292;2022&#65289;&#19978;&#36827;&#34892;&#28040;&#34701;&#30740;&#31350;&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#29420;&#31435;&#22320;&#21464;&#21270;&#25991;&#26412;&#21644;&#35270;&#35273;&#36755;&#20837;&#30340;&#25968;&#37327;&#65292;&#20197;&#37327;&#21270;&#22312;&#19981;&#21516;&#25968;&#25454;&#33539;&#22260;&#19979;&#35270;&#35273;&#36755;&#20837;&#21487;&#20197;&#24357;&#34917;&#22810;&#23569;&#25991;&#26412;&#25968;&#25454;&#65288;&#22914;&#26524;&#26377;&#30340;&#35805;&#65289;&#12290;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#26469;&#38480;&#21046;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17936v1 Announce Type: new  Abstract: In contrast to children, language models (LMs) exhibit considerably inferior data efficiency when acquiring language. In this submission to the BabyLM Challenge (Warstadt et al., 2023), we test the hypothesis that this data efficiency gap is partly caused by a lack of multimodal input and grounding in the learning environment of typical language models. Although previous work looking into this question found that multimodal training can even harm language-only performance, we speculate that these findings can be attributed to catastrophic forgetting of complex language due to fine-tuning on captions data. To test our hypothesis, we perform an ablation study on FLAVA (Singh et al., 2022), a multimodal vision-and-language model, independently varying the volume of text and vision input to quantify how much text data (if any) can be offset by vision at different data scales. We aim to limit catastrophic forgetting through a multitask pretra
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FLix&#30340;&#26032;&#22411;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22810;&#20219;&#21153;&#22810;&#35821;&#35328;&#35843;&#25972;&#65292;&#36890;&#36807;&#20851;&#32852;&#27599;&#20010;&#29420;&#29305;&#25968;&#25454;&#38598;&#29305;&#24449;&#19982;&#20854;&#20302;&#31209;&#26435;&#37325;&#26356;&#26032;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.17934</link><description>&lt;p&gt;
&#29992;&#29305;&#24449;&#21270;&#20302;&#31209;&#28151;&#21512;&#36827;&#34892;&#22810;&#20219;&#21153;&#22810;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Multitask Multilingual Model Adaptation with Featurized Low-Rank Mixtures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17934
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FLix&#30340;&#26032;&#22411;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22810;&#20219;&#21153;&#22810;&#35821;&#35328;&#35843;&#25972;&#65292;&#36890;&#36807;&#20851;&#32852;&#27599;&#20010;&#29420;&#29305;&#25968;&#25454;&#38598;&#29305;&#24449;&#19982;&#20854;&#20302;&#31209;&#26435;&#37325;&#26356;&#26032;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36866;&#24212;&#25968;&#21313;&#29978;&#33267;&#25968;&#30334;&#31181;&#20154;&#31867;&#35821;&#35328;&#30340;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#22312;&#35745;&#31639;&#19978;&#26159;&#26114;&#36149;&#30340;&#12290;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#36890;&#36807;&#21482;&#35843;&#25972;&#23569;&#37327;&#21442;&#25968;&#26174;&#33879;&#20943;&#23569;&#20102;&#36866;&#24212;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#23558;&#20687; LoRA&#65288;Hu &#31561;&#20154;&#65292;2022&#65289;&#36825;&#26679;&#30340; PEFT &#26041;&#27861;&#24212;&#29992;&#20110;&#19981;&#21516;&#25968;&#25454;&#38598;&#28151;&#21512;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#27425;&#20248;&#65292;&#21407;&#22240;&#22312;&#20110;&#26377;&#38480;&#30340;&#21442;&#25968;&#23481;&#37327;&#21644;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#36127;&#38754;&#20114;&#30456;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29305;&#24449;&#21270;&#20302;&#31209;&#28151;&#21512;&#65288;FLix&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#26377;&#25928;&#30340;&#22810;&#20219;&#21153;&#22810;&#35821;&#35328;&#35843;&#25972;&#30340;&#26032;&#22411; PEFT &#26041;&#27861;&#12290;FLix&#23558;&#27599;&#20010;&#29420;&#29305;&#25968;&#25454;&#38598;&#29305;&#24449;&#65288;&#20363;&#22914;&#25968;&#25454;&#38598;&#30340;&#35821;&#35328;&#25110;&#20219;&#21153;&#65289;&#19982;&#20854;&#33258;&#24049;&#30340;&#20302;&#31209;&#26435;&#37325;&#26356;&#26032;&#21442;&#25968;&#30456;&#20851;&#32852;&#12290;&#36890;&#36807;&#20026;&#27599;&#20010;&#25968;&#25454;&#38598;&#32452;&#21512;&#29305;&#23450;&#20110;&#29305;&#24449;&#30340;&#21442;&#25968;&#65292;FLix&#33021;&#22815;&#36866;&#24212;&#22810;&#31181;&#25968;&#25454;&#38598;&#28151;&#21512;&#65292;&#24182;&#26356;&#22909;&#22320;&#27867;&#21270;&#21040;&#26410;&#35265;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;FLix &#21487;&#20197;&#22312;&#25552;&#20379;&#26356;&#22909;&#24615;&#33021;&#30340;&#21516;&#26102;&#26174;&#33879;&#20943;&#23569;&#36866;&#24212;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17934v1 Announce Type: cross  Abstract: Adapting pretrained large language models (LLMs) to various downstream tasks in tens or hundreds of human languages is computationally expensive. Parameter-efficient fine-tuning (PEFT) significantly reduces the adaptation cost, by tuning only a small amount of parameters. However, directly applying PEFT methods such as LoRA (Hu et al., 2022) on diverse dataset mixtures could lead to suboptimal performance due to limited parameter capacity and negative interference among different datasets. In this work, we propose Featurized Low-rank Mixtures (FLix), a novel PEFT method designed for effective multitask multilingual tuning. FLix associates each unique dataset feature, such as the dataset's language or task, with its own low-rank weight update parameters. By composing feature-specific parameters for each dataset, FLix can accommodate diverse dataset mixtures and generalize better to unseen datasets. Our experiments show that FLix leads t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#21512;&#20316;&#35821;&#35328;&#24341;&#23548;&#36870;&#21521;&#35745;&#21010;&#25628;&#32034;&#65288;CLIPS&#65289;&#30340;&#36125;&#21494;&#26031;&#20195;&#29702;&#26550;&#26500;&#65292;&#29992;&#20110;&#23454;&#29616;&#23454;&#29992;&#25351;&#20196;&#36319;&#38543;&#21644;&#30446;&#26631;&#36741;&#21161;&#65292;&#33021;&#22815;&#36890;&#36807;&#22810;&#27169;&#24577;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#25351;&#20196;&#30340;&#21487;&#33021;&#24615;&#20197;&#23454;&#29616;&#23454;&#29992;&#30446;&#26631;&#36798;&#25104;&#25104;&#26412;&#26368;&#23567;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.17930</link><description>&lt;p&gt;
&#36890;&#36807;&#21512;&#20316;&#35821;&#35328;&#24341;&#23548;&#36870;&#21521;&#35268;&#21010;&#23454;&#29616;&#23454;&#29992;&#25351;&#20196;&#36319;&#38543;&#21644;&#30446;&#26631;&#36741;&#21161;
&lt;/p&gt;
&lt;p&gt;
Pragmatic Instruction Following and Goal Assistance via Cooperative Language-Guided Inverse Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#21512;&#20316;&#35821;&#35328;&#24341;&#23548;&#36870;&#21521;&#35745;&#21010;&#25628;&#32034;&#65288;CLIPS&#65289;&#30340;&#36125;&#21494;&#26031;&#20195;&#29702;&#26550;&#26500;&#65292;&#29992;&#20110;&#23454;&#29616;&#23454;&#29992;&#25351;&#20196;&#36319;&#38543;&#21644;&#30446;&#26631;&#36741;&#21161;&#65292;&#33021;&#22815;&#36890;&#36807;&#22810;&#27169;&#24577;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#25351;&#20196;&#30340;&#21487;&#33021;&#24615;&#20197;&#23454;&#29616;&#23454;&#29992;&#30446;&#26631;&#36798;&#25104;&#25104;&#26412;&#26368;&#23567;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#32463;&#24120;&#32473;&#20986;&#22312;&#32570;&#20047;&#36827;&#19968;&#27493;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#24847;&#20041;&#27169;&#31946;&#30340;&#25351;&#20196;&#65292;&#26399;&#26395;&#20182;&#20204;&#30340;&#34892;&#21160;&#25110;&#30446;&#26631;&#33021;&#28040;&#38500;&#19981;&#26126;&#30830;&#30340;&#24847;&#22270;&#12290;&#25105;&#20204;&#22914;&#20309;&#26500;&#24314;&#33021;&#22815;&#20197;&#28789;&#27963;&#12289;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#26041;&#24335;&#36981;&#24490;&#36825;&#31867;&#25351;&#20196;&#30340;&#36741;&#21161;&#20195;&#29702;&#21602;&#65311;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#21512;&#20316;&#35821;&#35328;&#24341;&#23548;&#36870;&#21521;&#35745;&#21010;&#25628;&#32034;&#65288;CLIPS&#65289;&#30340;&#36125;&#21494;&#26031;&#20195;&#29702;&#26550;&#26500;&#65292;&#29992;&#20110;&#23454;&#29616;&#23454;&#29992;&#25351;&#20196;&#36319;&#38543;&#21644;&#30446;&#26631;&#36741;&#21161;&#12290;&#25105;&#20204;&#30340;&#20195;&#29702;&#36890;&#36807;&#23558;&#20154;&#31867;&#24314;&#27169;&#20026;&#19968;&#20010;&#21512;&#20316;&#35268;&#21010;&#32773;&#65292;&#23558;&#20849;&#21516;&#35745;&#21010;&#19982;&#21161;&#25163;&#36827;&#34892;&#36890;&#20449;&#65292;&#28982;&#21518;&#36890;&#36807;&#21160;&#20316;&#21644;&#35821;&#35328;&#25191;&#34892;&#22810;&#27169;&#24577;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35780;&#20272;&#22312;&#20551;&#35774;&#35745;&#21010;&#19979;&#32473;&#20986;&#30340;&#25351;&#20196;&#30340;&#21487;&#33021;&#24615;&#12290;&#22312;&#33719;&#24471;&#36825;&#19968;&#21518;&#39564;&#20998;&#24067;&#21518;&#65292;&#25105;&#20204;&#30340;&#21161;&#25163;&#36890;&#36807;&#34892;&#21160;&#26469;&#26368;&#23567;&#21270;&#26399;&#26395;&#30446;&#26631;&#23454;&#29616;&#25104;&#26412;&#65292;&#20351;&#20854;&#33021;&#22815;&#23454;&#29992;&#22320;&#36981;&#24490;&#21547;&#31946;&#30340;&#25351;&#20196;&#65292;&#24182;&#21363;&#20351;&#22312;&#23545;&#25351;&#20196;&#19981;&#30830;&#23450;&#26102;&#20063;&#33021;&#25552;&#20379;&#26377;&#25928;&#30340;&#36741;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17930v1 Announce Type: new  Abstract: People often give instructions whose meaning is ambiguous without further context, expecting that their actions or goals will disambiguate their intentions. How can we build assistive agents that follow such instructions in a flexible, context-sensitive manner? This paper introduces cooperative language-guided inverse plan search (CLIPS), a Bayesian agent architecture for pragmatic instruction following and goal assistance. Our agent assists a human by modeling them as a cooperative planner who communicates joint plans to the assistant, then performs multimodal Bayesian inference over the human's goal from actions and language, using large language models (LLMs) to evaluate the likelihood of an instruction given a hypothesized plan. Given this posterior, our assistant acts to minimize expected goal achievement cost, enabling it to pragmatically follow ambiguous instructions and provide effective assistance even when uncertain about the g
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#20445;&#30041;&#21407;&#38382;&#39064;&#32467;&#26500;&#38590;&#24230;&#20294;&#38024;&#23545;LLMs&#26080;&#35299;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#26377;&#25928;&#22320;&#38477;&#20302;&#20102;LLMs&#30340;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.17916</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#25915;&#20987;&#23454;&#29616;&#25239;LLM&#30340;&#25968;&#23398;&#38382;&#39064;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
LLM-Resistant Math Word Problem Generation via Adversarial Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#20445;&#30041;&#21407;&#38382;&#39064;&#32467;&#26500;&#38590;&#24230;&#20294;&#38024;&#23545;LLMs&#26080;&#35299;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#26377;&#25928;&#22320;&#38477;&#20302;&#20102;LLMs&#30340;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26174;&#33879;&#25913;&#21464;&#20102;&#25945;&#32946;&#39046;&#22495;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#20363;&#65292;&#29983;&#25104;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#20197;&#30830;&#20445;&#20844;&#24179;&#35780;&#20272;&#65292;&#36825;&#20123;&#31034;&#20363;&#20445;&#30041;&#20102;&#21407;&#22987;&#38382;&#39064;&#30340;&#32467;&#26500;&#21644;&#38590;&#24230;&#65292;&#20294;LLMs&#26080;&#27861;&#35299;&#20915;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#25968;&#23398;&#24212;&#29992;&#39046;&#22495;&#30340;&#35789;&#38382;&#39064;&#65292;&#21033;&#29992;&#25277;&#35937;&#35821;&#27861;&#26641;&#32467;&#26500;&#29983;&#25104;&#23545;&#25239;&#31034;&#20363;&#65292;&#36890;&#36807;&#31616;&#21333;&#32534;&#36753;&#38382;&#39064;&#20013;&#30340;&#25968;&#23383;&#20540;&#65292;&#23548;&#33268;LLMs&#20135;&#29983;&#38169;&#35823;&#31572;&#26696;&#12290;&#25105;&#20204;&#23545;&#21508;&#31181;&#24320;&#28304;&#21644;&#38381;&#28304;LLMs&#36827;&#34892;&#23454;&#39564;&#65292;&#23450;&#37327;&#21644;&#23450;&#24615;&#22320;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#38477;&#20302;&#20102;&#23427;&#20204;&#30340;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17916v1 Announce Type: cross  Abstract: Large language models (LLMs) have significantly transformed the educational landscape. As current plagiarism detection tools struggle to keep pace with LLMs' rapid advancements, the educational community faces the challenge of assessing students' true problem-solving abilities in the presence of LLMs. In this work, we explore a new paradigm for ensuring fair evaluation -- generating adversarial examples which preserve the structure and difficulty of the original questions aimed for assessment, but are unsolvable by LLMs. Focusing on the domain of math word problems, we leverage abstract syntax trees to structurally generate adversarial examples that cause LLMs to produce incorrect answers by simply editing the numeric values in the problems. We conduct experiments on various open- and closed-source LLMs, quantitatively and qualitatively demonstrating that our method significantly degrades their math problem-solving ability. We identify
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#26041;&#35328;&#20998;&#31867;&#22120;&#25552;&#21462;&#26041;&#35328;&#30340;&#35789;&#27719;&#29305;&#24449;&#65292;&#25104;&#21151;&#35782;&#21035;&#20102;&#26377;&#21161;&#20110;&#26041;&#35328;&#21464;&#21270;&#30340;&#20851;&#38190;&#35821;&#35328;&#29305;&#23450;&#35789;&#27719;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2402.17914</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#26041;&#35328;&#20998;&#31867;&#22120;&#25552;&#21462;&#26041;&#35328;&#30340;&#35789;&#27719;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Extracting Lexical Features from Dialects via Interpretable Dialect Classifiers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17914
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#26041;&#35328;&#20998;&#31867;&#22120;&#25552;&#21462;&#26041;&#35328;&#30340;&#35789;&#27719;&#29305;&#24449;&#65292;&#25104;&#21151;&#35782;&#21035;&#20102;&#26377;&#21161;&#20110;&#26041;&#35328;&#21464;&#21270;&#30340;&#20851;&#38190;&#35821;&#35328;&#29305;&#23450;&#35789;&#27719;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#19968;&#31181;&#35821;&#35328;&#30340;&#26041;&#35328;&#20043;&#38388;&#30340;&#35821;&#35328;&#24046;&#24322;&#36890;&#24120;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#21644;&#32454;&#33268;&#30340;&#20154;&#31867;&#20998;&#26512;&#12290;&#36825;&#20027;&#35201;&#26159;&#22240;&#20026;&#30740;&#31350;&#21508;&#31181;&#26041;&#35328;&#28041;&#21450;&#21040;&#22797;&#26434;&#24615;&#21644;&#24494;&#22937;&#20043;&#22788;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#26041;&#35328;&#20998;&#31867;&#22120;&#25552;&#21462;&#26041;&#35328;&#30340;&#21306;&#20998;&#24615;&#35789;&#27719;&#29305;&#24449;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#20154;&#31867;&#19987;&#23478;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#20107;&#21518;&#21644;&#20869;&#22312;&#30340;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#23545;&#26222;&#36890;&#35805;&#12289;&#24847;&#22823;&#21033;&#35821;&#21644;&#20302;&#22320;&#33832;&#20811;&#26862;&#35821;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#35782;&#21035;&#20102;&#26377;&#21161;&#20110;&#26041;&#35328;&#21464;&#21270;&#30340;&#20851;&#38190;&#35821;&#35328;&#29305;&#23450;&#35789;&#27719;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17914v1 Announce Type: cross  Abstract: Identifying linguistic differences between dialects of a language often requires expert knowledge and meticulous human analysis. This is largely due to the complexity and nuance involved in studying various dialects. We present a novel approach to extract distinguishing lexical features of dialects by utilizing interpretable dialect classifiers, even in the absence of human experts. We explore both post-hoc and intrinsic approaches to interpretability, conduct experiments on Mandarin, Italian, and Low Saxon, and experimentally demonstrate that our method successfully identifies key language-specific lexical features that contribute to dialectal variations.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#30340;&#26032;&#27010;&#24565;&#25554;&#20837;&#21040;&#26412;&#20307;&#20013;&#65292;&#22312;&#36793;&#25628;&#32034;&#12289;&#36793;&#24418;&#25104;&#21644;&#22686;&#24378;&#12289;&#36793;&#36873;&#25321;&#19977;&#20010;&#27493;&#39588;&#20013;&#20998;&#21035;&#21033;&#29992;&#31070;&#32463;&#26041;&#27861;&#65292;&#24182;&#22312; SNOMED CT &#26412;&#20307;&#21644; MedMentions &#23454;&#20307;&#38142;&#25509;&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;</title><link>https://arxiv.org/abs/2402.17897</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26412;&#20307;&#35770;&#20013;&#26032;&#27010;&#24565;&#25918;&#32622;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Language Model based Framework for New Concept Placement in Ontologies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17897
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#30340;&#26032;&#27010;&#24565;&#25554;&#20837;&#21040;&#26412;&#20307;&#20013;&#65292;&#22312;&#36793;&#25628;&#32034;&#12289;&#36793;&#24418;&#25104;&#21644;&#22686;&#24378;&#12289;&#36793;&#36873;&#25321;&#19977;&#20010;&#27493;&#39588;&#20013;&#20998;&#21035;&#21033;&#29992;&#31070;&#32463;&#26041;&#27861;&#65292;&#24182;&#22312; SNOMED CT &#26412;&#20307;&#21644; MedMentions &#23454;&#20307;&#38142;&#25509;&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#23558;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#30340;&#26032;&#27010;&#24565;&#25554;&#20837;&#26412;&#20307;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#20010;&#19977;&#27493;&#26041;&#27861;&#65306;&#36793;&#25628;&#32034;&#65292;&#21363;&#25214;&#21040;&#35201;&#25554;&#20837;&#30340;&#20505;&#36873;&#20301;&#32622;&#38598;&#65288;&#21363;&#27010;&#24565;&#20043;&#38388;&#30340;&#21253;&#21547;&#20851;&#31995;&#65289;&#65292;&#36793;&#24418;&#25104;&#21644;&#22686;&#24378;&#65292;&#21033;&#29992;&#26412;&#20307;&#32467;&#26500;&#29983;&#25104;&#21644;&#22686;&#24378;&#36793;&#20505;&#36873;&#65292;&#20197;&#21450;&#36793;&#36873;&#25321;&#65292;&#26368;&#32456;&#30830;&#23450;&#35201;&#25918;&#32622;&#30340;&#36793;&#12290;&#22312;&#25152;&#26377;&#27493;&#39588;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#31070;&#32463;&#26041;&#27861;&#65292;&#20854;&#20013;&#24212;&#29992;&#22522;&#20110;&#23884;&#20837;&#30340;&#26041;&#27861;&#21644;&#23545;&#27604;&#23398;&#20064;&#65292;&#22914;BERT&#29992;&#20110;&#36793;&#25628;&#32034;&#65292;&#37319;&#29992;&#22522;&#20110;BERT&#24494;&#35843;&#30340;&#22810;&#26631;&#31614;&#36793;&#20132;&#21449;&#32534;&#30721;&#22120;&#65292;&#20197;&#21450;GPT&#31995;&#21015;&#12289;FLAN-T5 &#21644; Llama 2 &#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29992;&#20110;&#36793;&#36873;&#25321;&#12290;&#25105;&#20204;&#22312;&#20351;&#29992; SNOMED CT &#26412;&#20307;&#21644; MedMentions &#23454;&#20307;&#38142;&#25509;&#22522;&#20934;&#21019;&#24314;&#30340;&#26368;&#26032;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#36825;&#20123;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17897v1 Announce Type: new  Abstract: We investigate the task of inserting new concepts extracted from texts into an ontology using language models. We explore an approach with three steps: edge search which is to find a set of candidate locations to insert (i.e., subsumptions between concepts), edge formation and enrichment which leverages the ontological structure to produce and enhance the edge candidates, and edge selection which eventually locates the edge to be placed into. In all steps, we propose to leverage neural methods, where we apply embedding-based methods and contrastive learning with Pre-trained Language Models (PLMs) such as BERT for edge search, and adapt a BERT fine-tuning-based multi-label Edge-Cross-encoder, and Large Language Models (LLMs) such as GPT series, FLAN-T5, and Llama 2, for edge selection. We evaluate the methods on recent datasets created using the SNOMED CT ontology and the MedMentions entity linking benchmark. The best settings in our fram
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#30740;&#31350;&#24615;&#38382;&#39064;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#38750;&#20107;&#23454;&#22411;&#12289;&#22810;&#36879;&#35270;&#30340;&#38382;&#39064;&#65292;&#33021;&#22815;&#25361;&#25112;&#30446;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.17896</link><description>&lt;p&gt;
&#30740;&#31350;&#24615;&#38382;&#39064;&#65306;LLM&#32593;&#32476;&#29305;&#24037;&#30340;&#22810;&#36879;&#35270;&#12289;&#20998;&#35299;&#38382;&#39064;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Researchy Questions: A Dataset of Multi-Perspective, Decompositional Questions for LLM Web Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17896
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#30740;&#31350;&#24615;&#38382;&#39064;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#38750;&#20107;&#23454;&#22411;&#12289;&#22810;&#36879;&#35270;&#30340;&#38382;&#39064;&#65292;&#33021;&#22815;&#25361;&#25112;&#30446;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#38382;&#31572;&#65288;QA&#65289;&#25968;&#25454;&#38598;&#23545;&#20110;&#22823;&#22810;&#25968;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#35828;&#19981;&#20877;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20256;&#32479;&#30340;QA&#22522;&#20934;&#22914;TriviaQA&#12289;NaturalQuestions&#12289;ELI5&#21644;HotpotQA&#20027;&#35201;&#30740;&#31350;&#26126;&#30830;&#25351;&#31034;&#20102;&#32570;&#23569;&#21738;&#20123;&#20449;&#24687;&#20197;&#21450;&#22914;&#20309;&#25214;&#21040;&#36825;&#20123;&#20449;&#24687;&#26469;&#22238;&#31572;&#38382;&#39064;&#30340;&#8220;&#24050;&#30693;&#26410;&#30693;s&#8221;&#12290;&#22240;&#27492;&#65292;&#23545;&#36825;&#20123;&#22522;&#20934;&#30340;&#20248;&#31168;&#34920;&#29616;&#25552;&#20379;&#20102;&#19968;&#31181;&#34394;&#20551;&#30340;&#23433;&#20840;&#24863;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31038;&#21306;&#23578;&#26410;&#28385;&#36275;&#30340;&#38656;&#27714;&#26159;&#19968;&#20010;&#38750;&#20107;&#23454;&#22411;&#12289;&#22810;&#36879;&#35270;&#38382;&#39064;&#30340;&#38134;&#34892;&#65292;&#28041;&#21450;&#22823;&#37327;&#19981;&#26126;&#30830;&#30340;&#20449;&#24687;&#38656;&#27714;&#65292;&#21363;&#8220;&#26410;&#30693;&#30340;&#26410;&#30693;s&#8221;&#12290;&#25105;&#20204;&#22768;&#31216;&#21487;&#20197;&#22312;&#25628;&#32034;&#24341;&#25806;&#26085;&#24535;&#20013;&#25214;&#21040;&#36825;&#26679;&#30340;&#38382;&#39064;&#65292;&#36825;&#20196;&#20154;&#24778;&#35766;&#65292;&#22240;&#20026;&#22823;&#22810;&#25968;&#38382;&#31572;&#24847;&#22270;&#26597;&#35810;&#23454;&#38469;&#19978;&#26159;&#20107;&#23454;&#22411;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Researchy Questions&#65292;&#19968;&#20010;&#32463;&#36807;&#32321;&#29712;&#36807;&#28388;&#20197;&#21464;&#20026;&#38750;&#20107;&#23454;&#22411;&#12289;&#8220;&#20998;&#35299;&#24335;&#8221;&#21644;&#22810;&#36879;&#35270;&#30340;&#25628;&#32034;&#24341;&#25806;&#26597;&#35810;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#29992;&#25143;&#22312;&#36825;&#20123;&#38382;&#39064;&#19978;&#25237;&#20837;&#20102;&#22823;&#37327;&#8220;&#21162;&#21147;&#8221;&#65292;&#36825;&#31181;&#21162;&#21147;&#34920;&#29616;&#20026;&#20449;&#21495;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17896v1 Announce Type: cross  Abstract: Existing question answering (QA) datasets are no longer challenging to most powerful Large Language Models (LLMs). Traditional QA benchmarks like TriviaQA, NaturalQuestions, ELI5 and HotpotQA mainly study ``known unknowns'' with clear indications of both what information is missing, and how to find it to answer the question. Hence, good performance on these benchmarks provides a false sense of security. A yet unmet need of the NLP community is a bank of non-factoid, multi-perspective questions involving a great deal of unclear information needs, i.e. ``unknown uknowns''. We claim we can find such questions in search engine logs, which is surprising because most question-intent queries are indeed factoid. We present Researchy Questions, a dataset of search engine queries tediously filtered to be non-factoid, ``decompositional'' and multi-perspective. We show that users spend a lot of ``effort'' on these questions in terms of signals lik
&lt;/p&gt;</description></item><item><title>JMLR&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#21307;&#23398;&#39046;&#22495;&#25552;&#39640;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#24615;&#33021;&#65292;&#38477;&#20302;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#65292;&#22686;&#24378;&#27169;&#22411;&#21033;&#29992;&#21307;&#30103;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#21644;&#22238;&#31572;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.17887</link><description>&lt;p&gt;
JMLR&#65306;&#32852;&#21512;&#21307;&#30103;LLM&#21644;&#26816;&#32034;&#35757;&#32451;&#20197;&#22686;&#24378;&#25512;&#29702;&#21644;&#19987;&#19994;&#38382;&#39064;&#22238;&#31572;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
JMLR: Joint Medical LLM and Retrieval Training for Enhancing Reasoning and Professional Question Answering Capability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17887
&lt;/p&gt;
&lt;p&gt;
JMLR&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#21307;&#23398;&#39046;&#22495;&#25552;&#39640;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#24615;&#33021;&#65292;&#38477;&#20302;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#65292;&#22686;&#24378;&#27169;&#22411;&#21033;&#29992;&#21307;&#30103;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#21644;&#22238;&#31572;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21307;&#30103;&#25968;&#25454;&#30340;&#29190;&#28856;&#24615;&#22686;&#38271;&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#31934;&#20934;&#21307;&#23398;&#24050;&#32463;&#25104;&#20026;&#22686;&#24378;&#21307;&#30103;&#26381;&#21153;&#36136;&#37327;&#21644;&#25928;&#29575;&#30340;&#20851;&#38190;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21307;&#30103;&#30693;&#35782;&#33719;&#21462;&#21644;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#20013;&#21457;&#25381;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#36825;&#20123;&#31995;&#32479;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#22312;&#24494;&#35843;&#38454;&#27573;&#21516;&#26102;&#35757;&#32451;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#31995;&#32479;&#21644;LLM&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#32852;&#21512;&#21307;&#30103;LLM&#21644;&#26816;&#32034;&#35757;&#32451;&#65288;JMLR&#65289;&#30340;&#26041;&#27861;&#26088;&#22312;&#20811;&#26381;&#20256;&#32479;&#27169;&#22411;&#22312;&#22788;&#29702;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#37319;&#29992;&#21516;&#27493;&#35757;&#32451;&#26426;&#21046;&#65292;JMLR&#20943;&#23569;&#20102;&#23545;&#35745;&#31639;&#36164;&#28304;&#30340;&#38656;&#27714;&#65292;&#24182;&#22686;&#24378;&#20102;&#27169;&#22411;&#21033;&#29992;&#21307;&#30103;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#21644;&#22238;&#31572;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17887v1 Announce Type: new  Abstract: With the explosive growth of medical data and the rapid development of artificial intelligence technology, precision medicine has emerged as a key to enhancing the quality and efficiency of healthcare services. In this context, Large Language Models (LLMs) play an increasingly vital role in medical knowledge acquisition and question-answering systems. To further improve the performance of these systems in the medical domain, we introduce an innovative method that jointly trains an Information Retrieval (IR) system and an LLM during the fine-tuning phase. This approach, which we call Joint Medical LLM and Retrieval Training (JMLR), is designed to overcome the challenges faced by traditional models in handling medical question-answering tasks. By employing a synchronized training mechanism, JMLR reduces the demand for computational resources and enhances the model's ability to leverage medical knowledge for reasoning and answering question
&lt;/p&gt;</description></item><item><title>BlendSQL&#26159;&#19968;&#20010;&#36229;&#38598;&#30340;SQLite&#65292;&#29992;&#20110;&#32479;&#19968;&#28151;&#21512;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#38750;&#32467;&#26500;&#21270;&#21644;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#21253;&#21547;&#22810;&#36339;&#25512;&#29702;&#30340;&#20219;&#21153;&#65292;&#24182;&#19988;&#22312;&#20351;&#29992;&#26356;&#23569;&#20196;&#29260;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.17882</link><description>&lt;p&gt;
BlendSQL&#65306;&#29992;&#20110;&#32479;&#19968;&#28151;&#21512;&#38382;&#39064;&#22238;&#31572;&#30340;&#21487;&#25193;&#23637;&#26041;&#35328;&#22312;&#20851;&#31995;&#20195;&#25968;&#20013;
&lt;/p&gt;
&lt;p&gt;
BlendSQL: A Scalable Dialect for Unifying Hybrid Question Answering in Relational Algebra
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17882
&lt;/p&gt;
&lt;p&gt;
BlendSQL&#26159;&#19968;&#20010;&#36229;&#38598;&#30340;SQLite&#65292;&#29992;&#20110;&#32479;&#19968;&#28151;&#21512;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#38750;&#32467;&#26500;&#21270;&#21644;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#21253;&#21547;&#22810;&#36339;&#25512;&#29702;&#30340;&#20219;&#21153;&#65292;&#24182;&#19988;&#22312;&#20351;&#29992;&#26356;&#23569;&#20196;&#29260;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#26377;&#31471;&#21040;&#31471;&#31995;&#32479;&#29992;&#20110;&#28151;&#21512;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#65292;&#24448;&#24448;&#21487;&#20197;&#24402;&#32467;&#20026;&#8220;&#25552;&#31034;-&#31048;&#31095;&#8221;&#33539;&#24335;&#65292;&#29992;&#25143;&#23545;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#30340;&#25511;&#21046;&#21644;&#27934;&#23519;&#21463;&#38480;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#35768;&#22810;&#22522;&#20110;Transformer&#30340;LLM&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#22823;&#23567;&#38480;&#21046;&#65292;&#24448;&#24448;&#19981;&#21512;&#29702;&#26399;&#26395;&#23436;&#25972;&#30340;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#19978;&#19979;&#25991;&#36866;&#21512;&#20110;&#32473;&#23450;&#25552;&#31034;&#22312;&#38646;&#27425;&#31034;&#33539;&#29615;&#22659;&#20013;&#65292;&#26356;&#19981;&#29992;&#35828;&#20960;&#27425;&#25552;&#31034;&#29615;&#22659;&#20013;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;BlendSQL&#65292;&#23427;&#26159;SQLite&#30340;&#19968;&#20010;&#36229;&#38598;&#65292;&#29992;&#20316;&#32479;&#19968;&#30340;&#26041;&#35328;&#65292;&#29992;&#20110;&#22312;&#38750;&#32467;&#26500;&#21270;&#21644;&#32467;&#26500;&#21270;&#25968;&#25454;&#20043;&#38388;&#32534;&#25490;&#25512;&#29702;&#12290;&#23545;&#20110;&#28041;&#21450;&#22810;&#36339;&#25512;&#29702;&#30340;&#28151;&#21512;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#65292;&#25105;&#20204;&#23558;&#23436;&#25972;&#30340;&#20998;&#35299;&#25512;&#29702;&#36335;&#32447;&#22270;&#32534;&#30721;&#20026;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;BlendSQL&#26597;&#35810;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;BlendSQL&#21487;&#20197;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#20351;&#29992;35%&#26356;&#23569;&#20196;&#29260;&#30340;&#24773;&#20917;&#19979;&#25913;&#21892;&#31471;&#21040;&#31471;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17882v1 Announce Type: new  Abstract: Many existing end-to-end systems for hybrid question answering tasks can often be boiled down to a "prompt-and-pray" paradigm, where the user has limited control and insight into the intermediate reasoning steps used to achieve the final result. Additionally, due to the context size limitation of many transformer-based LLMs, it is often not reasonable to expect that the full structured and unstructured context will fit into a given prompt in a zero-shot setting, let alone a few-shot setting. We introduce BlendSQL, a superset of SQLite to act as a unified dialect for orchestrating reasoning across both unstructured and structured data. For hybrid question answering tasks involving multi-hop reasoning, we encode the full decomposed reasoning roadmap into a single interpretable BlendSQL query. Notably, we show that BlendSQL can scale to massive datasets and improve the performance of end-to-end systems while using 35% fewer tokens. Our code
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#33258;&#21160;&#32479;&#35745;&#27169;&#22411;&#21457;&#29616;&#26041;&#27861;&#65292;&#19981;&#20877;&#38656;&#35201;&#23450;&#20041;&#29305;&#23450;&#39046;&#22495;&#27169;&#22411;&#35821;&#35328;&#25110;&#35774;&#35745;&#25163;&#24037;&#25628;&#32034;&#31243;&#24207;&#12290;</title><link>https://arxiv.org/abs/2402.17879</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#32479;&#35745;&#27169;&#22411;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Automated Statistical Model Discovery with Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17879
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#33258;&#21160;&#32479;&#35745;&#27169;&#22411;&#21457;&#29616;&#26041;&#27861;&#65292;&#19981;&#20877;&#38656;&#35201;&#23450;&#20041;&#29305;&#23450;&#39046;&#22495;&#27169;&#22411;&#35821;&#35328;&#25110;&#35774;&#35745;&#25163;&#24037;&#25628;&#32034;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#27169;&#22411;&#21457;&#29616;&#28041;&#21450;&#22312;&#21463;&#39046;&#22495;&#29305;&#23450;&#24314;&#27169;&#32422;&#26463;&#30340;&#24191;&#27867;&#27169;&#22411;&#31354;&#38388;&#19978;&#36827;&#34892;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25628;&#32034;&#12290;&#39640;&#25928;&#25628;&#32034;&#36825;&#19968;&#31354;&#38388;&#38656;&#35201;&#20855;&#26377;&#24314;&#27169;&#21644;&#38382;&#39064;&#22495;&#20154;&#31867;&#19987;&#38271;&#30340;&#19987;&#19994;&#30693;&#35782;&#12290;&#21463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#39046;&#22495;&#30693;&#35782;&#21644;&#32534;&#31243;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#33258;&#21160;&#32479;&#35745;&#27169;&#22411;&#21457;&#29616;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#33258;&#21160;&#21270;&#27969;&#31243;&#32622;&#20110;Box&#30340;&#24490;&#29615;&#26694;&#26550;&#20043;&#20869;&#65306;LM&#22312;&#25552;&#20986;&#34920;&#31034;&#20026;&#27010;&#29575;&#31243;&#24207;&#30340;&#32479;&#35745;&#27169;&#22411;&#65288;&#20805;&#24403;&#24314;&#27169;&#32773;&#65289;&#20043;&#38388;&#36845;&#20195;&#65292;&#24182;&#25209;&#21028;&#36825;&#20123;&#27169;&#22411;&#65288;&#20805;&#24403;&#39046;&#22495;&#19987;&#23478;&#65289;&#12290;&#36890;&#36807;&#21033;&#29992;LMs&#65292;&#25105;&#20204;&#19981;&#24517;&#23450;&#20041;&#19968;&#20010;&#39046;&#22495;&#29305;&#23450;&#30340;&#27169;&#22411;&#35821;&#35328;&#25110;&#35774;&#35745;&#25163;&#24037;&#25628;&#32034;&#31243;&#24207;&#65292;&#36825;&#26159;&#20808;&#21069;&#31995;&#32479;&#30340;&#37325;&#35201;&#38480;&#21046;&#12290;&#25105;&#20204;&#22312;&#27010;&#29575;&#24314;&#27169;&#30340;&#19977;&#31181;&#24120;&#35265;&#35774;&#32622;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65306;&#22312;&#21463;&#38480;&#27169;&#22411;&#31354;&#38388;&#20869;&#25628;&#32034;&#65292;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17879v1 Announce Type: cross  Abstract: Statistical model discovery involves a challenging search over a vast space of models subject to domain-specific modeling constraints. Efficiently searching over this space requires human expertise in modeling and the problem domain. Motivated by the domain knowledge and programming capabilities of large language models (LMs), we introduce a method for language model driven automated statistical model discovery. We cast our automated procedure within the framework of Box's Loop: the LM iterates between proposing statistical models represented as probabilistic programs, acting as a modeler, and critiquing those models, acting as a domain expert. By leveraging LMs, we do not have to define a domain-specific language of models or design a handcrafted search procedure, key restrictions of previous systems. We evaluate our method in three common settings in probabilistic modeling: searching within a restricted space of models, searching ove
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25581;&#31034;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#27844;&#38706;&#39118;&#38505;&#65292;&#25351;&#20986;&#23545;&#25163;&#21487;&#20197;&#21033;&#29992;LMs&#30340;&#25351;&#31034;&#36981;&#24490;&#33021;&#21147;&#36731;&#26494;&#22320;&#20174;&#25968;&#25454;&#23384;&#20648;&#20013;&#30452;&#25509;&#25552;&#21462;&#25991;&#26412;&#25968;&#25454;&#65292;&#24182;&#35774;&#35745;&#20102;&#25915;&#20987;&#23545;&#29983;&#20135;RAG&#27169;&#22411;GPTs&#36896;&#25104;&#25968;&#25454;&#23384;&#20648;&#27844;&#28431;&#12290;</title><link>https://arxiv.org/abs/2402.17840</link><description>&lt;p&gt;
&#36981;&#24490;&#25105;&#30340;&#25351;&#31034;&#24182;&#35828;&#20986;&#30495;&#30456;&#65306;&#26469;&#33258;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#30340;&#21487;&#25193;&#23637;&#25968;&#25454;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Follow My Instruction and Spill the Beans: Scalable Data Extraction from Retrieval-Augmented Generation Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17840
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25581;&#31034;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#27844;&#38706;&#39118;&#38505;&#65292;&#25351;&#20986;&#23545;&#25163;&#21487;&#20197;&#21033;&#29992;LMs&#30340;&#25351;&#31034;&#36981;&#24490;&#33021;&#21147;&#36731;&#26494;&#22320;&#20174;&#25968;&#25454;&#23384;&#20648;&#20013;&#30452;&#25509;&#25552;&#21462;&#25991;&#26412;&#25968;&#25454;&#65292;&#24182;&#35774;&#35745;&#20102;&#25915;&#20987;&#23545;&#29983;&#20135;RAG&#27169;&#22411;GPTs&#36896;&#25104;&#25968;&#25454;&#23384;&#20648;&#27844;&#28431;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#23558;&#22806;&#37096;&#30693;&#35782;&#32435;&#20837;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#23450;&#21046;&#36866;&#24212;&#65292;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;Retrieval-In-Context RAG&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#20013;&#30340;&#25968;&#25454;&#27844;&#38706;&#39118;&#38505;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#23545;&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#30340;LMs&#26500;&#24314;&#30340;RAG&#31995;&#32479;&#36827;&#34892;&#25552;&#31034;&#27880;&#20837;&#26102;&#65292;&#23545;&#25163;&#21487;&#20197;&#21033;&#29992;LMs&#30340;&#25351;&#31034;&#36981;&#24490;&#33021;&#21147;&#36731;&#26494;&#22320;&#20174;&#25968;&#25454;&#23384;&#20648;&#20013;&#30452;&#25509;&#25552;&#21462;&#25991;&#26412;&#25968;&#25454;&#12290;&#36825;&#31181;&#28431;&#27934;&#23384;&#22312;&#20110;&#35206;&#30422;Llama2&#12289;Mistral/Mixtral&#12289;Vicuna&#12289;SOLAR&#12289;WizardLM&#12289;Qwen1.5&#21644;Platypus2&#31561;&#22810;&#31181;&#29616;&#20195;LMs&#30340;&#24191;&#27867;&#33539;&#22260;&#20869;&#65292;&#24182;&#19988;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#30340;&#25193;&#22823;&#65292;&#21033;&#29992;&#33021;&#21147;&#21152;&#21095;&#12290;&#23558;&#30740;&#31350;&#25193;&#23637;&#21040;&#29983;&#20135;RAG&#27169;&#22411;GPTs&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#25915;&#20987;&#65292;&#21487;&#20197;&#22312;&#23545;25&#20010;&#38543;&#26426;&#36873;&#25321;&#30340;&#23450;&#21046;GPTs&#26045;&#21152;&#26368;&#22810;2&#20010;&#26597;&#35810;&#26102;&#20197;100%&#25104;&#21151;&#29575;&#23548;&#33268;&#25968;&#25454;&#23384;&#20648;&#27844;&#28431;&#65292;&#24182;&#19988;&#25105;&#20204;&#33021;&#22815;&#20197;77,000&#23383;&#30340;&#20070;&#31821;&#20013;&#30340;&#25991;&#26412;&#25968;&#25454;&#30340;&#25552;&#21462;&#29575;&#20026;41%&#65292;&#20197;&#21450;&#22312;&#21547;&#26377;1,569,00&#35789;&#30340;&#35821;&#26009;&#24211;&#20013;&#30340;&#25991;&#26412;&#25968;&#25454;&#30340;&#25552;&#21462;&#29575;&#20026;3%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17840v1 Announce Type: cross  Abstract: Retrieval-Augmented Generation (RAG) improves pre-trained models by incorporating external knowledge at test time to enable customized adaptation. We study the risk of datastore leakage in Retrieval-In-Context RAG Language Models (LMs). We show that an adversary can exploit LMs' instruction-following capabilities to easily extract text data verbatim from the datastore of RAG systems built with instruction-tuned LMs via prompt injection. The vulnerability exists for a wide range of modern LMs that span Llama2, Mistral/Mixtral, Vicuna, SOLAR, WizardLM, Qwen1.5, and Platypus2, and the exploitability exacerbates as the model size scales up. Extending our study to production RAG models GPTs, we design an attack that can cause datastore leakage with a 100% success rate on 25 randomly selected customized GPTs with at most 2 queries, and we extract text data verbatim at a rate of 41% from a book of 77,000 words and 3% from a corpus of 1,569,00
&lt;/p&gt;</description></item><item><title>&#31283;&#23450;LM 2 1.6B&#26159;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#20013;&#30340;&#26032;&#19968;&#20195;&#20135;&#21697;&#65292;&#22312;&#35813;&#25253;&#21578;&#20013;&#35814;&#32454;&#20171;&#32461;&#20102;&#20854;&#25968;&#25454;&#12289;&#35757;&#32451;&#36807;&#31243;&#21644;&#24615;&#33021;&#35780;&#20272;&#65292;&#35813;&#27169;&#22411;&#22312;&#21457;&#24067;&#26102;&#26159;2B&#21442;&#25968;&#33539;&#22260;&#20869;&#26368;&#20808;&#36827;&#30340;&#24320;&#28304;&#27169;&#22411;&#20043;&#19968;&#65292;&#24182;&#25552;&#20379;&#20102;&#19979;&#36733;&#38142;&#25509;&#21644;&#24615;&#33021;&#23545;&#27604;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.17834</link><description>&lt;p&gt;
&#31283;&#23450;LM 2 1.6B&#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Stable LM 2 1.6B Technical Report
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17834
&lt;/p&gt;
&lt;p&gt;
&#31283;&#23450;LM 2 1.6B&#26159;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#20013;&#30340;&#26032;&#19968;&#20195;&#20135;&#21697;&#65292;&#22312;&#35813;&#25253;&#21578;&#20013;&#35814;&#32454;&#20171;&#32461;&#20102;&#20854;&#25968;&#25454;&#12289;&#35757;&#32451;&#36807;&#31243;&#21644;&#24615;&#33021;&#35780;&#20272;&#65292;&#35813;&#27169;&#22411;&#22312;&#21457;&#24067;&#26102;&#26159;2B&#21442;&#25968;&#33539;&#22260;&#20869;&#26368;&#20808;&#36827;&#30340;&#24320;&#28304;&#27169;&#22411;&#20043;&#19968;&#65292;&#24182;&#25552;&#20379;&#20102;&#19979;&#36733;&#38142;&#25509;&#21644;&#24615;&#33021;&#23545;&#27604;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#31283;&#23450;LM 2 1.6B&#65292;&#36825;&#26159;&#25105;&#20204;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#30340;&#26032;&#19968;&#20195;&#20135;&#21697;&#12290;&#22312;&#36825;&#20221;&#25216;&#26415;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#23548;&#33268;StableLM 2 1.6B&#22522;&#30784;&#29256;&#26412;&#21644;&#25351;&#23548;&#35843;&#20248;&#29256;&#26412;&#30340;&#25968;&#25454;&#21644;&#35757;&#32451;&#36807;&#31243;&#12290;&#36825;&#20004;&#20010;&#27169;&#22411;&#30340;&#26435;&#37325;&#22343;&#21487;&#36890;&#36807;Hugging Face&#19979;&#36733;&#21644;&#20351;&#29992;&#12290;&#35813;&#25253;&#21578;&#21253;&#21547;&#20102;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#24443;&#24213;&#35780;&#20272;&#65292;&#21253;&#25324;&#38646; shot &#21644;&#23569; shot &#22522;&#20934;&#27979;&#35797;&#65292;&#22810;&#35821;&#35328;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#21450;&#37325;&#28857;&#25918;&#22312;&#22810;&#36718;&#23545;&#35805;&#30340; MT &#22522;&#20934;&#27979;&#35797;&#19978;&#12290;&#22312;&#21457;&#24067;&#26412;&#25253;&#21578;&#26102;&#65292;StableLM 2 1.6B&#26159;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#30340; 2B &#21442;&#25968;&#33539;&#22260;&#20869;&#26368;&#20808;&#36827;&#30340;&#24320;&#28304;&#27169;&#22411;&#12290;&#37492;&#20110;&#20854;&#21560;&#24341;&#20154;&#30340;&#23567;&#23610;&#23544;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#22312;&#22810;&#31181;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#21534;&#21520;&#37327;&#27979;&#35797;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#28304;&#20102;&#20960;&#20010;&#37327;&#21270;&#26816;&#26597;&#28857;&#65292;&#24182;&#25552;&#20379;&#20102;&#23427;&#20204;&#19982;&#21407;&#22987;&#27169;&#22411;&#30340;&#24615;&#33021;&#25351;&#26631;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17834v1 Announce Type: new  Abstract: We introduce StableLM 2 1.6B, the first in a new generation of our language model series. In this technical report, we present in detail the data and training procedure leading to the base and instruction-tuned versions of StableLM 2 1.6B. The weights for both models are available via Hugging Face for anyone to download and use. The report contains thorough evaluations of these models, including zero- and few-shot benchmarks, multilingual benchmarks, and the MT benchmark focusing on multi-turn dialogues. At the time of publishing this report, StableLM 2 1.6B was the state-of-the-art open model under 2B parameters by a significant margin. Given its appealing small size, we also provide throughput measurements on a number of edge devices. In addition, we open source several quantized checkpoints and provide their performance metrics compared to the original model.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#26694;&#26550;&#65292;&#21487;&#20197;&#34913;&#37327;&#20154;&#31867;&#19982;&#27169;&#22411;&#20559;&#22909;&#20043;&#38388;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#25490;&#21517;&#12290;</title><link>https://arxiv.org/abs/2402.17826</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
Prediction-Powered Ranking of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17826
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#26694;&#26550;&#65292;&#21487;&#20197;&#34913;&#37327;&#20154;&#31867;&#19982;&#27169;&#22411;&#20559;&#22909;&#20043;&#38388;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#26681;&#25454;&#20854;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#19968;&#33268;&#24615;&#27700;&#24179;&#36827;&#34892;&#25490;&#21517;--&#22914;&#26524;&#19968;&#20010;&#27169;&#22411;&#30340;&#36755;&#20986;&#26356;&#21463;&#20154;&#31867;&#20559;&#22909;&#65292;&#37027;&#20040;&#23427;&#23601;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#22909;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#26694;&#26550;&#26469;&#24357;&#21512;&#20154;&#31867;&#19982;&#27169;&#22411;&#20559;&#22909;&#20043;&#38388;&#21487;&#33021;&#24341;&#20837;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17826v1 Announce Type: cross  Abstract: Large language models are often ranked according to their level of alignment with human preferences -- a model is better than other models if its outputs are more frequently preferred by humans. One of the most popular ways to elicit human preferences utilizes pairwise comparisons between the outputs provided by different models to the same inputs. However, since gathering pairwise comparisons by humans is costly and time-consuming, it has become a very common practice to gather pairwise comparisons by a strong large language model -- a model strongly aligned with human preferences. Surprisingly, practitioners cannot currently measure the uncertainty that any mismatch between human and model preferences may introduce in the constructed rankings. In this work, we develop a statistical framework to bridge this gap. Given a small set of pairwise comparisons by humans and a large set of pairwise comparisons by a model, our framework provid
&lt;/p&gt;</description></item><item><title>DropBP&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#24335;&#26469;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#36890;&#36807;&#22312;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#38543;&#26426;&#20002;&#24323;&#23618;&#20197;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.17812</link><description>&lt;p&gt;
DropBP&#65306;&#36890;&#36807;&#20002;&#24323;&#21453;&#21521;&#20256;&#25773;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping Backward Propagation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17812
&lt;/p&gt;
&lt;p&gt;
DropBP&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#24335;&#26469;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#36890;&#36807;&#22312;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#38543;&#26426;&#20002;&#24323;&#23618;&#20197;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#28041;&#21450;&#27491;&#21521;&#21644;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#30340;&#22823;&#37327;&#35745;&#31639;&#25104;&#26412;&#12290;&#20256;&#32479;&#30340;&#23618;&#27425;&#20002;&#24323;&#25216;&#26415;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20002;&#24323;&#26576;&#20123;&#23618;&#20197;&#20943;&#23569;&#35745;&#31639;&#36127;&#25285;&#12290;&#28982;&#32780;&#65292;&#22312;&#27491;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#20002;&#24323;&#23618;&#20250;&#23545;&#35757;&#32451;&#36807;&#31243;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#65292;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DropBP&#65292;&#36825;&#26159;&#19968;&#31181;&#26088;&#22312;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;DropBP&#22312;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#38543;&#26426;&#20002;&#24323;&#23618;&#65292;&#19981;&#24433;&#21709;&#27491;&#21521;&#20256;&#25773;&#12290;&#27492;&#22806;&#65292;DropBP&#35745;&#31639;&#27599;&#20010;&#23618;&#30340;&#25935;&#24863;&#24615;&#20197;&#20998;&#37197;&#36866;&#24403;&#30340;&#20002;&#22833;&#29575;&#65292;&#20174;&#32780;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#12290;DropBP&#26088;&#22312;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#22686;&#24378;&#35757;&#32451;&#36807;&#31243;&#30340;&#25928;&#29575;&#65292;&#20174;&#32780;&#21152;&#36895;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#36827;&#34892;&#23436;&#20840;&#24494;&#35843;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17812v1 Announce Type: cross  Abstract: Training deep neural networks typically involves substantial computational costs during both forward and backward propagation. The conventional layer dropping techniques drop certain layers during training for reducing the computations burden. However, dropping layers during forward propagation adversely affects the training process by degrading accuracy. In this paper, we propose Dropping Backward Propagation (DropBP), a novel approach designed to reduce computational costs while maintaining accuracy. DropBP randomly drops layers during the backward propagation, which does not deviate forward propagation. Moreover, DropBP calculates the sensitivity of each layer to assign appropriate drop rate, thereby stabilizing the training process. DropBP is designed to enhance the efficiency of the training process with backpropagation, thereby enabling the acceleration of both full fine-tuning and parameter-efficient fine-tuning using backpropag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TruthX&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#30495;&#23454;&#31354;&#38388;&#20013;&#32534;&#36753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;TruthfulQA&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;TruthX&#24179;&#22343;&#25552;&#39640;&#20102;13&#31181;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.17811</link><description>&lt;p&gt;
TruthX: &#36890;&#36807;&#22312;&#30495;&#23454;&#31354;&#38388;&#20013;&#32534;&#36753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#20943;&#36731;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
TruthX: Alleviating Hallucinations by Editing Large Language Models in Truthful Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TruthX&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#30495;&#23454;&#31354;&#38388;&#20013;&#32534;&#36753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;TruthfulQA&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;TruthX&#24179;&#22343;&#25552;&#39640;&#20102;13&#31181;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26377;&#26102;&#20250;&#20135;&#29983;&#24187;&#35273;&#65292;&#29305;&#21035;&#26159;&#22312;&#23427;&#20204;&#21487;&#33021;&#29983;&#25104;&#19981;&#30495;&#23454;&#30340;&#22238;&#24212;&#65292;&#23613;&#31649;&#25317;&#26377;&#27491;&#30830;&#30340;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TruthX&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#30495;&#23454;&#31354;&#38388;&#20013;&#32534;&#36753;LLMs&#20869;&#37096;&#34920;&#31034;&#20197;&#33719;&#21462;&#20854;&#30495;&#23454;&#24615;&#30340;&#25512;&#26029;&#26102;&#38388;&#26041;&#27861;&#12290;TruthX&#21033;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#23558;LLM&#30340;&#34920;&#31034;&#20998;&#21035;&#26144;&#23556;&#21040;&#35821;&#20041;&#21644;&#30495;&#23454;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#24212;&#29992;&#23545;&#27604;&#23398;&#20064;&#22312;&#30495;&#23454;&#31354;&#38388;&#20013;&#35782;&#21035;&#30495;&#23454;&#30340;&#32534;&#36753;&#26041;&#21521;&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#22312;&#30495;&#23454;&#31354;&#38388;&#20013;&#32534;&#36753;LLM&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;TruthX&#26377;&#25928;&#22320;&#22686;&#24378;&#20102;LLMs&#30340;&#30495;&#23454;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;TruthX&#36890;&#36807;20%&#30340;&#24179;&#22343;&#20540;&#25552;&#39640;&#20102;13&#31181;&#20808;&#36827;LLMs&#22312;TruthfulQA&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#30495;&#23454;&#24615;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#30495;&#23454;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17811v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks. However, they sometimes suffer from producing hallucinations, particularly in cases where they may generate untruthful responses despite possessing the correct knowledge. In this paper, we propose TruthX, an inference-time method to elicit the truthfulness of LLMs by editing their internal representations in truthful space. TruthX employs an auto-encoder to map LLM's representations into semantic and truthful latent spaces respectively, and applies contrastive learning to identify a truthful editing direction within the truthful space. During inference, by editing LLM's internal representations in truthful space, TruthX effectively enhances the truthfulness of LLMs. Experiments show that TruthX effectively improves the truthfulness of 13 advanced LLMs by an average of 20% on TruthfulQA benchmark. Further analyses suggest that the truthful space
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#27169;LLMs&#22312;&#33258;&#28982;&#35821;&#35328;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;NLVR&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#38656;&#35201;&#32452;&#21512;&#21644;&#31354;&#38388;&#25512;&#29702;&#12289;&#23545;&#35821;&#20041;&#21644;&#31995;&#32479;&#24615;&#20559;&#35265;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;</title><link>https://arxiv.org/abs/2402.17793</link><description>&lt;p&gt;
&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#22833;&#36133;&#65311;&#22810;&#27169;LLMs&#21644;NLVR&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
A Surprising Failure? Multimodal LLMs and the NLVR Challenge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17793
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#27169;LLMs&#22312;&#33258;&#28982;&#35821;&#35328;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;NLVR&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#38656;&#35201;&#32452;&#21512;&#21644;&#31354;&#38388;&#25512;&#29702;&#12289;&#23545;&#35821;&#20041;&#21644;&#31995;&#32479;&#24615;&#20559;&#35265;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;MLLMs&#8212;&#8212;GPT-4V&#12289;Gemini Pro&#21644;&#24320;&#28304;&#27169;&#22411;IDEFICS&#8212;&#8212;&#23545;&#20110;&#32452;&#21512;&#33258;&#28982;&#35821;&#35328;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;NLVR&#30340;&#34920;&#29616;&#12290;NLVR&#35201;&#27714;&#27169;&#22411;&#26681;&#25454;&#19968;&#20010;&#20154;&#31867;&#20070;&#20889;&#30340;&#21477;&#23376;&#21644;&#19968;&#20010;&#21512;&#25104;&#22270;&#20687;&#26469;&#30830;&#23450;&#21477;&#23376;&#30456;&#23545;&#20110;&#22270;&#20687;&#30340;&#30495;&#20551;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#23427;&#20204;&#22312;NLVR&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#35813;&#20219;&#21153;&#26088;&#22312;&#38656;&#35201;&#32452;&#21512;&#21644;&#31354;&#38388;&#25512;&#29702;&#65292;&#24182;&#19988;&#23545;&#35821;&#20041;&#21644;&#31995;&#32479;&#24615;&#20559;&#35265;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17793v1 Announce Type: new  Abstract: This study evaluates three state-of-the-art MLLMs -- GPT-4V, Gemini Pro, and the open-source model IDEFICS -- on the compositional natural language vision reasoning task NLVR. Given a human-written sentence paired with a synthetic image, this task requires the model to determine the truth value of the sentence with respect to the image. Despite the strong performance demonstrated by these models, we observe they perform poorly on NLVR, which was constructed to require compositional and spatial reasoning, and to be robust for semantic and systematic biases.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SSC-CoT&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#20013;&#38388;&#27493;&#39588;&#30340;&#31574;&#30053;&#21644;&#26597;&#35810;&#30693;&#35782;&#22270;&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22797;&#26434;&#25968;&#23398;&#25512;&#29702;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2402.17786</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36880;&#27493;&#33258;&#27965;&#30340;&#25968;&#23398;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Stepwise Self-Consistent Mathematical Reasoning with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17786
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SSC-CoT&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#20013;&#38388;&#27493;&#39588;&#30340;&#31574;&#30053;&#21644;&#26597;&#35810;&#30693;&#35782;&#22270;&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22797;&#26434;&#25968;&#23398;&#25512;&#29702;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22797;&#26434;&#25968;&#23398;&#25512;&#29702;&#26159;&#22256;&#38590;&#30340;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#22810;&#27493;&#25512;&#29702;&#36807;&#31243;&#30340;&#22797;&#26434;&#24615;&#12290;&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#21517;&#20026;Stepwise Self-Consistent Chain-of-Thought&#65288;SSC-CoT&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;SSC-CoT&#21033;&#29992;&#36873;&#25321;&#22522;&#20110;&#19981;&#21516;&#25512;&#29702;&#38142;&#20132;&#38598;&#30340;&#20013;&#38388;&#27493;&#39588;&#30340;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#26597;&#35810;&#21253;&#21547;&#30456;&#20851;&#39046;&#22495;&#30693;&#35782;&#30340;&#30693;&#35782;&#22270;&#26469;&#21457;&#29616;&#20851;&#38190;&#30340;&#20013;&#38388;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17786v1 Announce Type: new  Abstract: Using Large Language Models for complex mathematical reasoning is difficult, primarily due to the complexity of multi-step reasoning. The main challenges of this process include (1) selecting critical intermediate results to advance the procedure, and (2) limited exploration of potential solutions. To address these issues, we introduce a novel algorithm, namely Stepwise Self-Consistent Chain-of-Thought (SSC-CoT). SSC-CoT employs a strategy of selecting intermediate steps based on the intersection of various reasoning chains. Additionally, SSC-CoT enables the model to discover critical intermediate steps by querying a knowledge graph comprising relevant domain knowledge. To validate SSC-CoT, we present a new dataset, TriMaster100, tailored for complex trigonometry problems. This dataset contains 100 questions, with each solution broken down into scored intermediate steps, facilitating a comprehensive evaluation of the mathematical reasoni
&lt;/p&gt;</description></item><item><title>OmniACT&#26159;&#19968;&#20010;&#38024;&#23545;&#20195;&#29702;&#29983;&#25104;&#21487;&#25191;&#34892;&#31243;&#24207;&#23436;&#25104;&#35745;&#31639;&#26426;&#20219;&#21153;&#33021;&#21147;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;Web&#33258;&#21160;&#21270;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#26700;&#38754;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.17553</link><description>&lt;p&gt;
OmniACT&#65306;&#29992;&#20110;&#21551;&#29992;&#26700;&#38754;&#21644;Web&#22810;&#27169;&#24335;&#36890;&#29992;&#20027;&#21160;&#26234;&#33021;&#20307;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17553
&lt;/p&gt;
&lt;p&gt;
OmniACT&#26159;&#19968;&#20010;&#38024;&#23545;&#20195;&#29702;&#29983;&#25104;&#21487;&#25191;&#34892;&#31243;&#24207;&#23436;&#25104;&#35745;&#31639;&#26426;&#20219;&#21153;&#33021;&#21147;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;Web&#33258;&#21160;&#21270;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#26700;&#38754;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#21313;&#24180;&#26469;&#65292;&#20154;&#26426;&#20132;&#20114;&#20174;&#26681;&#26412;&#19978;&#19968;&#30452;&#26159;&#25163;&#21160;&#30340;&#12290;&#21363;&#20351;&#22312;&#20170;&#22825;&#65292;&#20960;&#20046;&#25152;&#26377;&#22312;&#35745;&#31639;&#26426;&#19978;&#36827;&#34892;&#30340;&#39640;&#25928;&#24037;&#20316;&#37117;&#38656;&#35201;&#20154;&#31867;&#22312;&#27599;&#19968;&#27493;&#37117;&#25552;&#20379;&#36755;&#20837;&#12290;&#34394;&#25311;&#20027;&#21160;&#26234;&#33021;&#20195;&#34920;&#20102;&#33258;&#21160;&#21270;&#35768;&#22810;&#36825;&#20123;&#29712;&#30862;&#20219;&#21153;&#30340;&#19968;&#20010;&#28608;&#21160;&#20154;&#24515;&#30340;&#27493;&#39588;&#12290;&#34394;&#25311;&#20195;&#29702;&#23558;&#20351;&#25216;&#26415;&#33021;&#21147;&#26377;&#38480;&#30340;&#29992;&#25143;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#35745;&#31639;&#26426;&#31995;&#32479;&#30340;&#21508;&#31181;&#21487;&#33021;&#24615;&#12290;&#23427;&#20204;&#36824;&#21487;&#20197;&#23454;&#29616;&#39640;&#25928;&#22320;&#31616;&#21270;&#35768;&#22810;&#35745;&#31639;&#26426;&#20219;&#21153;&#65292;&#20174;&#26085;&#21382;&#31649;&#29702;&#21040;&#22797;&#26434;&#30340;&#26053;&#34892;&#39044;&#35746;&#65292;&#20943;&#23569;&#20154;&#31867;&#24178;&#39044;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; OmniACT&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#20195;&#29702;&#29983;&#25104;&#21487;&#25191;&#34892;&#31243;&#24207;&#26469;&#23436;&#25104;&#35745;&#31639;&#26426;&#20219;&#21153;&#33021;&#21147;&#30340;&#39318;&#20010;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#33539;&#22260;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;Web&#33258;&#21160;&#21270;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#26700;&#38754;&#24212;&#29992;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#35832;&#22914;"&#25773;&#25918;&#19979;&#19968;&#39318;&#27468;"&#20043;&#31867;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#20197;&#21450;&#26356;&#20026;&#38271;&#26399;&#30340;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17553v1 Announce Type: new  Abstract: For decades, human-computer interaction has fundamentally been manual. Even today, almost all productive work done on the computer necessitates human input at every step. Autonomous virtual agents represent an exciting step in automating many of these menial tasks. Virtual agents would empower users with limited technical proficiency to harness the full possibilities of computer systems. They could also enable the efficient streamlining of numerous computer tasks, ranging from calendar management to complex travel bookings, with minimal human intervention. In this paper, we introduce OmniACT, the first-of-a-kind dataset and benchmark for assessing an agent's capability to generate executable programs to accomplish computer tasks. Our scope extends beyond traditional web automation, covering a diverse range of desktop applications. The dataset consists of fundamental tasks such as "Play the next song", as well as longer horizon tasks such
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35780;&#20272;&#20020;&#24202;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26415;&#21518;&#39118;&#38505;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#19981;&#21516;&#35757;&#32451;&#31574;&#30053;&#30340;&#27169;&#22411;&#22312;&#22260;&#25163;&#26415;&#26399;&#25252;&#29702;&#20013;&#30340;&#28508;&#22312;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.17493</link><description>&lt;p&gt;
&#20026;&#22260;&#25163;&#26415;&#26399;&#25252;&#29702;&#24320;&#20855;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#27491;&#30830;&#21058;&#37327;&#26159;&#22810;&#23569;&#65311;
&lt;/p&gt;
&lt;p&gt;
Prescribing Large Language Models for Perioperative Care: What's The Right Dose for Pre-trained Models?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17493
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35780;&#20272;&#20020;&#24202;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26415;&#21518;&#39118;&#38505;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#19981;&#21516;&#35757;&#32451;&#31574;&#30053;&#30340;&#27169;&#22411;&#22312;&#22260;&#25163;&#26415;&#26399;&#25252;&#29702;&#20013;&#30340;&#28508;&#22312;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26415;&#21518;&#39118;&#38505;&#39044;&#27979;&#21487;&#20197;&#25351;&#23548;&#26377;&#25928;&#30340;&#22260;&#25163;&#26415;&#26399;&#25252;&#29702;&#31649;&#29702;&#21644;&#35268;&#21010;&#12290;&#25105;&#20204;&#26088;&#22312;&#35780;&#20272;&#20020;&#24202;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#21542;&#21487;&#20197;&#20351;&#29992;&#19981;&#21516;&#30340;&#35757;&#32451;&#31574;&#30053;&#39044;&#27979;&#26415;&#21518;&#39118;&#38505;&#12290;&#30740;&#31350;&#20027;&#35201;&#28041;&#21450;2018&#24180;&#33267;2021&#24180;&#38388;&#26469;&#33258;Barnes Jewish&#21307;&#38498;&#31995;&#32479;&#30340;84,875&#20221;&#35760;&#24405;&#12290;&#26041;&#27861;&#22312;Beth Israel Deaconess&#30340;MIMIC&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22797;&#21046;&#12290;&#20004;&#39033;&#30740;&#31350;&#30340;&#24179;&#22343;&#38543;&#35775;&#26102;&#38388;&#22522;&#20110;&#26415;&#21518;ICU&#20303;&#38498;&#26102;&#38388;&#23567;&#20110;7&#22825;&#12290;&#23545;&#20110;BJH&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#21253;&#25324;30&#22825;&#27515;&#20129;&#29575;&#12289;&#32954;&#26643;&#22622;&#65288;PE&#65289;&#21644;&#32954;&#28814;&#12290;&#23545;BioGPT&#12289;ClinicalBERT&#21644;BioClinicalBERT&#23454;&#26045;&#20102;&#19977;&#31181;&#22495;&#33258;&#36866;&#24212;&#21644;&#24494;&#35843;&#31574;&#30053;&#65306;&#33258;&#30417;&#30563;&#30446;&#26631;&#65307;&#32467;&#21512;&#21322;&#30417;&#30563;&#24494;&#35843;&#30340;&#26631;&#31614;&#65307;&#20197;&#21450;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#36827;&#34892;&#22522;&#30784;&#24314;&#27169;&#12290;&#27169;&#22411;&#24615;&#33021;&#20351;&#29992;&#25509;&#25910;&#22120;&#25805;&#20316;&#29305;&#24449;&#19979;&#30340;&#38754;&#31215;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17493v1 Announce Type: new  Abstract: Postoperative risk predictions can inform effective perioperative care management and planning. We aimed to assess whether clinical large language models (LLMs) can predict postoperative risks using clinical texts with various training strategies. The main cohort involved 84,875 records from Barnes Jewish Hospital (BJH) system between 2018 and 2021. Methods were replicated on Beth Israel Deaconess's MIMIC dataset. Both studies had mean duration of follow-up based on the length of postoperative ICU stay less than 7 days. For the BJH dataset, outcomes included 30-day mortality, pulmonary embolism (PE) and pneumonia. Three domain adaptation and finetuning strategies were implemented for BioGPT, ClinicalBERT and BioClinicalBERT: self-supervised objectives; incorporating labels with semi-supervised fine-tuning; and foundational modelling through multi-task learning. Model performance was compared using the area under the receiver operating ch
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Contrastive EEG-Text Masked Autoencoder&#65288;CET-MAE&#65289;&#21644;E2T-PTR&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#21644;&#26041;&#27861;&#26469;&#22686;&#24378;&#22522;&#20110;EEG&#30340;&#35821;&#35328;&#35299;&#30721;&#12290;</title><link>https://arxiv.org/abs/2402.17433</link><description>&lt;p&gt;
&#36890;&#36807;&#20174;&#39044;&#35757;&#32451;&#23545;&#27604;&#24615;EEG-&#25991;&#26412;&#33945;&#29256;&#33258;&#21160;&#32534;&#30721;&#22120;&#20013;&#36716;&#31227;&#30340;&#34920;&#31034;&#22686;&#24378;EEG&#21040;&#25991;&#26412;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Enhancing EEG-to-Text Decoding through Transferable Representations from Pre-trained Contrastive EEG-Text Masked Autoencoder
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17433
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Contrastive EEG-Text Masked Autoencoder&#65288;CET-MAE&#65289;&#21644;E2T-PTR&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#21644;&#26041;&#27861;&#26469;&#22686;&#24378;&#22522;&#20110;EEG&#30340;&#35821;&#35328;&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#26080;&#21019;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#37325;&#24314;&#33258;&#28982;&#35821;&#35328;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20316;&#20026;&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#30340;&#35821;&#35328;&#35299;&#30721;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;EEG&#30340;&#35821;&#35328;&#35299;&#30721;&#20173;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#65292;&#38754;&#20020;&#35832;&#22810;&#25216;&#26415;&#38382;&#39064;&#65292;&#22914;&#65306;1&#65289;&#32570;&#20047;&#19968;&#20010;&#33021;&#22815;&#26377;&#25928;&#25972;&#21512;&#36328;&#27169;&#24577;&#65288;EEG&#21644;&#25991;&#26412;&#20043;&#38388;&#65289;&#33258;&#23398;&#20064;&#19982;EEG&#29305;&#24449;&#25110;&#25991;&#26412;&#24207;&#21015;&#30340;&#27169;&#20869;&#33258;&#37325;&#26500;&#30340;&#28151;&#21512;&#31574;&#30053;&#65307;2&#65289;&#26410;&#20805;&#20998;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#22686;&#24378;&#22522;&#20110;EEG&#30340;&#35821;&#35328;&#35299;&#30721;&#12290;&#20026;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#24615;EEG-&#25991;&#26412;&#33945;&#29256;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;CET-MAE&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#19987;&#29992;&#30340;&#22810;&#27969;&#32534;&#30721;&#22120;&#22312;EEG&#21644;&#25991;&#26412;&#20043;&#38388;&#20197;&#21450;&#20869;&#37096;&#36827;&#34892;&#22797;&#21512;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#22411;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;E2T-PTR&#65288;&#20351;&#29992;&#39044;&#35757;&#32451;&#21487;&#36716;&#31227;&#34920;&#31034;&#36827;&#34892;EEG&#21040;&#25991;&#26412;&#35299;&#30721;&#65289;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#32452;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17433v1 Announce Type: new  Abstract: Reconstructing natural language from non-invasive electroencephalography (EEG) holds great promise as a language decoding technology for brain-computer interfaces (BCIs). However, EEG-based language decoding is still in its nascent stages, facing several technical issues such as: 1) Absence of a hybrid strategy that can effectively integrate cross-modality (between EEG and text) self-learning with intra-modality self-reconstruction of EEG features or textual sequences; 2) Under-utilization of large language models (LLMs) to enhance EEG-based language decoding. To address above issues, we propose the Contrastive EEG-Text Masked Autoencoder (CET-MAE), a novel model that orchestrates compound self-supervised learning across and within EEG and text through a dedicated multi-stream encoder. Furthermore, we develop a framework called E2T-PTR (EEG-to-Text decoding using Pretrained Transferable Representations), which leverages pre-trained modul
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#22312;&#29616;&#20195;&#36719;&#20214;&#31995;&#32479;&#20013;&#25198;&#28436;&#30528;&#19981;&#21487;&#25110;&#32570;&#30340;&#35282;&#33394;&#65292;&#30417;&#31649;&#29615;&#22659;&#30340;&#21464;&#21270;&#12289;&#36719;&#20214;&#20010;&#24615;&#21270;&#38656;&#27714;&#30340;&#22686;&#38271;&#20197;&#21450;&#23545;&#27835;&#29702;&#30340;&#24378;&#35843;&#25512;&#21160;&#30528;&#22823;&#22411;&#20225;&#19994;&#37319;&#29992;&#33258;&#21160;&#21270;&#25216;&#26415;&#65292;&#24182;&#22312;AI&#20026;&#20013;&#24515;&#30340;&#31995;&#32479;&#20013;&#19981;&#26029;&#24341;&#20837;&#26032;&#30340;&#25361;&#25112;&#21644;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2402.16977</link><description>&lt;p&gt;
&#22788;&#29702;&#25968;&#25454;&#20197;&#24212;&#23545;RE&#20013;&#30340;&#25361;&#25112;&#65306;&#21033;&#29992;NLP&#21644;&#29983;&#25104;AI&#32531;&#35299;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Dealing with Data for RE: Mitigating Challenges using NLP and Generative AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16977
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22312;&#29616;&#20195;&#36719;&#20214;&#31995;&#32479;&#20013;&#25198;&#28436;&#30528;&#19981;&#21487;&#25110;&#32570;&#30340;&#35282;&#33394;&#65292;&#30417;&#31649;&#29615;&#22659;&#30340;&#21464;&#21270;&#12289;&#36719;&#20214;&#20010;&#24615;&#21270;&#38656;&#27714;&#30340;&#22686;&#38271;&#20197;&#21450;&#23545;&#27835;&#29702;&#30340;&#24378;&#35843;&#25512;&#21160;&#30528;&#22823;&#22411;&#20225;&#19994;&#37319;&#29992;&#33258;&#21160;&#21270;&#25216;&#26415;&#65292;&#24182;&#22312;AI&#20026;&#20013;&#24515;&#30340;&#31995;&#32479;&#20013;&#19981;&#26029;&#24341;&#20837;&#26032;&#30340;&#25361;&#25112;&#21644;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#19981;&#26029;&#21464;&#21270;&#30340;&#21830;&#19994;&#29615;&#22659;&#20013;&#65292;&#20225;&#19994;&#38754;&#20020;&#30528;&#26085;&#30410;&#22686;&#22810;&#30340;&#25361;&#25112;&#12290;&#36825;&#20123;&#25361;&#25112;&#21253;&#25324;&#19981;&#26029;&#21464;&#21270;&#30340;&#30417;&#31649;&#29615;&#22659;&#12289;&#36719;&#20214;&#24212;&#29992;&#20013;&#20010;&#24615;&#21270;&#38656;&#27714;&#30340;&#22686;&#38271;&#20197;&#21450;&#23545;&#27835;&#29702;&#30340;&#24378;&#35843;&#12290;&#38024;&#23545;&#36825;&#20123;&#22810;&#26041;&#38754;&#30340;&#38656;&#27714;&#65292;&#22823;&#22411;&#20225;&#19994;&#19968;&#30452;&#22312;&#37319;&#29992;&#20174;&#20248;&#21270;&#26680;&#24515;&#19994;&#21153;&#27969;&#31243;&#21040;&#22686;&#24378;&#23458;&#25143;&#20307;&#39564;&#30340;&#33258;&#21160;&#21270;&#12290;&#23454;&#38469;&#19978;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24050;&#32463;&#25104;&#20026;&#29616;&#20195;&#36719;&#20214;&#31995;&#32479;&#30340;&#20851;&#38190;&#35201;&#32032;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#25968;&#25454;&#21457;&#25381;&#30528;&#19981;&#21487;&#25110;&#32570;&#30340;&#20316;&#29992;&#12290;&#22522;&#20110;&#30417;&#30563;&#23398;&#20064;&#19988;&#22312;&#24037;&#19994;&#35268;&#27169;&#19978;&#36816;&#34892;&#30340;&#20197;AI&#20026;&#20013;&#24515;&#30340;&#36719;&#20214;&#31995;&#32479;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#25165;&#33021;&#26377;&#25928;&#36816;&#34892;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#29983;&#25104;AI&#23548;&#33268;&#23545;&#36275;&#22815;&#35780;&#20272;&#22522;&#20934;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#38271;&#12290;&#25105;&#20204;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#32463;&#39564;&#26174;&#31034;&#20986;&#65292;&#28385;&#36275;&#36825;&#20123;&#35201;&#27714;&#23545;&#20110;&#23454;&#29616;RE&#26041;&#38754;&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16977v1 Announce Type: cross  Abstract: Across the dynamic business landscape today, enterprises face an ever-increasing range of challenges. These include the constantly evolving regulatory environment, the growing demand for personalization within software applications, and the heightened emphasis on governance. In response to these multifaceted demands, large enterprises have been adopting automation that spans from the optimization of core business processes to the enhancement of customer experiences. Indeed, Artificial Intelligence (AI) has emerged as a pivotal element of modern software systems. In this context, data plays an indispensable role. AI-centric software systems based on supervised learning and operating at an industrial scale require large volumes of training data to perform effectively. Moreover, the incorporation of generative AI has led to a growing demand for adequate evaluation benchmarks. Our experience in this field has revealed that the requirement 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102; GPT-4 &#21644; MTurk &#31649;&#36947;&#30340;&#25968;&#25454;&#26631;&#27880;&#20934;&#30830;&#24615;&#65292;&#21457;&#29616;&#23613;&#31649; MTurk &#37319;&#29992;&#20102;&#26368;&#20339;&#23454;&#36341;&#65292;&#20294; GPT-4 &#30340;&#20934;&#30830;&#29575;&#26356;&#39640;&#65292;&#24182;&#19988;&#32467;&#21512; GPT-4 &#21644;&#20247;&#21253;&#26631;&#31614;&#20351;&#29992;&#32858;&#21512;&#31639;&#27861;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.16795</link><description>&lt;p&gt;
&#22914;&#26524;&#22312;&#19968;&#20010;&#20247;&#21253;&#25968;&#25454;&#26631;&#27880;&#31649;&#36947;&#20013;&#65292;GPT-4
&lt;/p&gt;
&lt;p&gt;
If in a Crowdsourced Data Annotation Pipeline, a GPT-4
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102; GPT-4 &#21644; MTurk &#31649;&#36947;&#30340;&#25968;&#25454;&#26631;&#27880;&#20934;&#30830;&#24615;&#65292;&#21457;&#29616;&#23613;&#31649; MTurk &#37319;&#29992;&#20102;&#26368;&#20339;&#23454;&#36341;&#65292;&#20294; GPT-4 &#30340;&#20934;&#30830;&#29575;&#26356;&#39640;&#65292;&#24182;&#19988;&#32467;&#21512; GPT-4 &#21644;&#20247;&#21253;&#26631;&#31614;&#20351;&#29992;&#32858;&#21512;&#31639;&#27861;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;GPT-4&#22312;&#25968;&#25454;&#26631;&#27880;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#22312;&#32447;&#20247;&#21253;&#24037;&#20316;&#32773;&#65292;&#23588;&#20854;&#26159;&#26469;&#33258;&#20122;&#39532;&#36874;&#26426;&#26800;&#22303;&#32819;&#20854;&#65288;MTurk&#65289;&#30340;&#24037;&#20316;&#32773;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#22240;&#20559;&#31163;&#26631;&#20934;&#20247;&#21253;&#23454;&#36341;&#24182;&#24378;&#35843;&#20010;&#21035;&#24037;&#20316;&#32773;&#30340;&#34920;&#29616;&#32780;&#21463;&#21040;&#25209;&#35780;&#65292;&#32780;&#19981;&#26159;&#25972;&#20010;&#25968;&#25454;&#26631;&#27880;&#36807;&#31243;&#12290;&#26412;&#25991;&#27604;&#36739;&#20102;GPT-4&#21644;&#19968;&#20010;&#36947;&#24503;&#19988;&#25191;&#34892;&#33391;&#22909;&#30340;MTurk&#31649;&#36947;&#65292;&#20351;&#29992;415&#21517;&#24037;&#20316;&#32773;&#26631;&#27880;&#20102;&#26469;&#33258;200&#31687;&#23398;&#26415;&#25991;&#31456;&#30340;3,177&#20010;&#21477;&#27573;&#65292;&#20351;&#29992;&#20102;CODA-19&#26041;&#26696;&#12290;&#20004;&#20010;&#24037;&#20316;&#32773;&#30028;&#38754;&#20135;&#29983;&#20102;127,080&#20010;&#26631;&#31614;&#65292;&#28982;&#21518;&#36890;&#36807;&#20843;&#31181;&#26631;&#31614;&#32858;&#21512;&#31639;&#27861;&#25512;&#26029;&#20986;&#26368;&#32456;&#30340;&#26631;&#31614;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#23613;&#31649;&#37319;&#29992;&#20102;&#26368;&#20339;&#23454;&#36341;&#65292;MTurk&#31649;&#36947;&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#20026;81.5%&#65292;&#32780;GPT-4&#36798;&#21040;&#20102;83.6%&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#24403;&#23558;GPT-4&#30340;&#26631;&#31614;&#19982;&#36890;&#36807;&#20808;&#36827;&#24037;&#20316;&#32773;&#30028;&#38754;&#25910;&#38598;&#30340;&#20247;&#21253;&#26631;&#31614;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#32858;&#21512;&#26102;&#65292;8&#31181;&#31639;&#27861;&#20013;&#26377;2&#31181;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16795v1 Announce Type: cross  Abstract: Recent studies indicated GPT-4 outperforms online crowd workers in data labeling accuracy, notably workers from Amazon Mechanical Turk (MTurk). However, these studies were criticized for deviating from standard crowdsourcing practices and emphasizing individual workers' performances over the whole data-annotation process. This paper compared GPT-4 and an ethical and well-executed MTurk pipeline, with 415 workers labeling 3,177 sentence segments from 200 scholarly articles using the CODA-19 scheme. Two worker interfaces yielded 127,080 labels, which were then used to infer the final labels through eight label-aggregation algorithms. Our evaluation showed that despite best practices, MTurk pipeline's highest accuracy was 81.5%, whereas GPT-4 achieved 83.6%. Interestingly, when combining GPT-4's labels with crowd labels collected via an advanced worker interface for aggregation, 2 out of the 8 algorithms achieved an even higher accuracy (
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20915;&#31574;&#24863;&#30693;&#21644;&#21487;&#27867;&#21270;&#30340;&#24037;&#20855;&#20351;&#29992;&#26694;&#26550;&#65292;&#20197;&#24110;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25805;&#20316;&#24037;&#20855;&#26102;&#25552;&#39640;&#28789;&#27963;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;</title><link>https://arxiv.org/abs/2402.16696</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23457;&#24910;&#34892;&#20107;&#65306;&#36808;&#21521;&#20915;&#31574;&#24863;&#30693;&#21644;&#21487;&#27867;&#21270;&#30340;&#24037;&#20855;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
Look Before You Leap: Towards Decision-Aware and Generalizable Tool-Usage for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16696
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20915;&#31574;&#24863;&#30693;&#21644;&#21487;&#27867;&#21270;&#30340;&#24037;&#20855;&#20351;&#29992;&#26694;&#26550;&#65292;&#20197;&#24110;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25805;&#20316;&#24037;&#20855;&#26102;&#25552;&#39640;&#28789;&#27963;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#20855;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33719;&#21462;&#26368;&#26032;&#30693;&#35782;&#21644;&#32531;&#35299;&#20135;&#29983;&#24187;&#35273;&#38382;&#39064;&#26041;&#38754;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#24403;&#21069;&#65292;&#20808;&#36827;&#30340;&#38381;&#28304;LLM&#65288;&#22914;ChatGPT&#65289;&#36890;&#36807;&#25552;&#31034;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#25216;&#26415;&#23637;&#31034;&#20986;&#20196;&#20154;&#24778;&#35766;&#30340;&#24037;&#20855;&#20351;&#29992;&#33021;&#21147;&#12290;&#20026;&#20102;&#22686;&#24378;&#24320;&#28304;LLM&#65288;&#22914;LLaMA&#65289;&#22312;&#25805;&#20316;&#24037;&#20855;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24403;&#21069;&#30340;&#21162;&#21147;&#38598;&#20013;&#20110;&#22522;&#20110;&#27169;&#26495;&#39537;&#21160;&#25110;&#22522;&#20110;&#26631;&#35760;&#35302;&#21457;&#30340;&#24037;&#20855;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#21069;&#32773;&#30001;&#20110;&#21463;&#21040;&#38480;&#21046;&#30340;&#24037;&#20855;&#20132;&#20114;&#65292;&#38480;&#21046;&#20102;LLM&#28789;&#27963;&#22320;&#35299;&#20915;&#21508;&#31181;&#29992;&#25143;&#26597;&#35810;&#65292;&#32780;&#21518;&#32773;&#22312;&#20351;&#29992;&#26032;&#24037;&#20855;&#26102;&#38480;&#21046;&#20102;&#27867;&#21270;&#33021;&#21147;&#65292;&#22240;&#20026;&#24037;&#20855;&#20351;&#29992;&#23398;&#20064;&#22522;&#20110;&#20219;&#21153;&#21644;&#24037;&#20855;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20915;&#31574;&#24863;&#30693;&#21644;&#21487;&#27867;&#21270;&#30340;&#24037;&#20855;&#20351;&#29992;&#26694;&#26550;&#65288;DEER&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20855;&#26377;&#22810;&#20010;&#20915;&#31574;&#20998;&#25903;&#30340;&#24037;&#20855;&#20351;&#29992;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16696v1 Announce Type: new  Abstract: Tool-augmented large language models (LLMs) are attracting widespread attention when accessing up-to-date knowledge and alleviating hallucination issues. Nowadays, advanced closed-source LLMs (e.g., ChatGPT) have demonstrated surprising tool-usage capabilities through prompting and in-context learning techniques. To empower the capabilities of open-source LLMs (e.g., LLaMA) in manipulating tools, current efforts focus on either template-driven or token-triggered tool-usage. However, the former hampers LLMs' flexibility to address diverse user's queries due to constrained tool interactions, while the latter limits the generalizability when engaging with new tools, since tool-usage learning is based on task- and tool-specific datasets. To alleviate these concerns, in this paper, we propose a decision-aware and generalizable tool-usage framework (DEER). Specifically, we first construct the tool-usage samples with multiple decision branches 
&lt;/p&gt;</description></item><item><title>StructLM&#31995;&#21015;&#27169;&#22411;&#22522;&#20110;&#20840;&#38754;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#36229;&#36234;&#20102;&#22823;&#22810;&#25968;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#65292;&#22312;&#32467;&#26500;&#21270;&#30693;&#35782;&#36830;&#25509;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#25104;&#23601;&#12290;</title><link>https://arxiv.org/abs/2402.16671</link><description>&lt;p&gt;
StructLM: &#26397;&#21521;&#26500;&#24314;&#32467;&#26500;&#21270;&#30693;&#35782;&#36830;&#25509;&#30340;&#36890;&#29992;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
StructLM: Towards Building Generalist Models for Structured Knowledge Grounding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16671
&lt;/p&gt;
&lt;p&gt;
StructLM&#31995;&#21015;&#27169;&#22411;&#22522;&#20110;&#20840;&#38754;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#36229;&#36234;&#20102;&#22823;&#22810;&#25968;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#65292;&#22312;&#32467;&#26500;&#21270;&#30693;&#35782;&#36830;&#25509;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#25104;&#23601;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#25968;&#25454;&#28304;&#65292;&#22914;&#34920;&#26684;&#12289;&#22270;&#24418;&#21644;&#25968;&#25454;&#24211;&#65292;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#30693;&#35782;&#28304;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#32431;&#25991;&#26412;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#22312;&#35299;&#37322;&#21644;&#21033;&#29992;&#32467;&#26500;&#21270;&#25968;&#25454;&#26041;&#38754;&#30340;&#33021;&#21147;&#20173;&#28982;&#26377;&#38480;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;LLM&#22312;&#22788;&#29702;&#32467;&#26500;&#21270;&#25968;&#25454;&#26041;&#38754;&#30340;&#26174;&#30528;&#19981;&#36275;&#65292;&#20363;&#22914;&#65292;ChatGPT&#24179;&#22343;&#33853;&#21518;&#20110;&#26368;&#20808;&#36827;&#27169;&#22411;(SoTA)35%&#12290;&#20026;&#22686;&#24378;LLM&#20013;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#36830;&#25509;&#65288;SKG&#65289;&#33021;&#21147;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;110&#19975;&#20010;&#31034;&#20363;&#30340;&#20840;&#38754;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#12290;&#21033;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#31995;&#21015;&#22522;&#20110;Code-LLaMA&#26550;&#26500;&#30340;&#27169;&#22411;&#65292;&#31216;&#20026;StructLM&#65292;&#21442;&#25968;&#33539;&#22260;&#20174;7B&#21040;34B&#12290;&#25105;&#20204;&#30340;StructLM&#31995;&#21015;&#22312;18&#20010;&#35780;&#20272;&#25968;&#25454;&#38598;&#20013;&#26377;14&#20010;&#36229;&#36234;&#20102;&#29305;&#23450;&#20219;&#21153;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;7&#20010;SKG&#20219;&#21153;&#19978;&#30830;&#31435;&#20102;&#26032;&#30340;SoTA&#25104;&#23601;&#12290;&#27492;&#22806;&#65292;StructLM&#23637;&#29616;&#20102;&#21331;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16671v1 Announce Type: new  Abstract: Structured data sources, such as tables, graphs, and databases, are ubiquitous knowledge sources. Despite the demonstrated capabilities of large language models (LLMs) on plain text, their proficiency in interpreting and utilizing structured data remains limited. Our investigation reveals a notable deficiency in LLMs' ability to process structured data, e.g., ChatGPT lags behind state-of-the-art (SoTA) model by an average of 35%. To augment the Structured Knowledge Grounding (SKG) capabilities in LLMs, we have developed a comprehensive instruction tuning dataset comprising 1.1 million examples. Utilizing this dataset, we train a series of models, referred to as StructLM, based on the Code-LLaMA architecture, ranging from 7B to 34B parameters. Our StructLM series surpasses task-specific models on 14 out of 18 evaluated datasets and establishes new SoTA achievements on 7 SKG tasks. Furthermore, StructLM demonstrates exceptional generalizat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#21040;&#29305;&#23450;&#39046;&#22495;&#30340;&#22270;&#25968;&#25454;&#24211;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;ChatGPT&#29983;&#25104;NL-GQL&#25968;&#25454;&#23545;&#24182;&#24494;&#35843;LLMs&#65292;&#23454;&#29616;&#20102;&#20004;&#32773;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;</title><link>https://arxiv.org/abs/2402.16567</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#21040;&#29305;&#23450;&#39046;&#22495;&#30340;&#22270;&#25968;&#25454;&#24211;
&lt;/p&gt;
&lt;p&gt;
Aligning Large Language Models to a Domain-specific Graph Database
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16567
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#21040;&#29305;&#23450;&#39046;&#22495;&#30340;&#22270;&#25968;&#25454;&#24211;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;ChatGPT&#29983;&#25104;NL-GQL&#25968;&#25454;&#23545;&#24182;&#24494;&#35843;LLMs&#65292;&#23454;&#29616;&#20102;&#20004;&#32773;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#25968;&#25454;&#24211;&#65288;Graph DB&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#37329;&#34701;&#12289;&#31038;&#20132;&#32593;&#32476;&#21644;&#21307;&#33647;&#31561;&#21508;&#20010;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#23558;&#33258;&#28982;&#35821;&#35328;&#65288;NL&#65289;&#36716;&#25442;&#20026;&#22270;&#26597;&#35810;&#35821;&#35328;&#65288;GQL&#65289;&#65292;&#36890;&#24120;&#31216;&#20026;NL2GQL&#65292;&#30001;&#20110;&#20854;&#22266;&#26377;&#22797;&#26434;&#24615;&#21644;&#19987;&#19994;&#21270;&#29305;&#24615;&#32780;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#19968;&#20123;&#26041;&#27861;&#35797;&#22270;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#35299;&#20915;&#31867;&#20284;&#30340;&#20219;&#21153;&#65292;&#22914;&#25991;&#26412;&#36716;SQL&#12290;&#28982;&#32780;&#65292;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;NL2GQL&#20219;&#21153;&#20013;&#65292;&#32570;&#20047;&#29305;&#23450;&#39046;&#22495;&#30340;NL-GQL&#25968;&#25454;&#23545;&#20351;&#24471;&#38590;&#20197;&#24314;&#31435;LLMs&#21644;&#22270;&#25968;&#25454;&#24211;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26126;&#30830;&#23450;&#20041;&#30340;&#27969;&#27700;&#32447;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#21033;&#29992;ChatGPT&#22522;&#20110;&#32473;&#23450;&#30340;&#22270;&#25968;&#25454;&#24211;&#33258;&#25105;&#29983;&#25104;NL-GQL&#25968;&#25454;&#23545;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#21019;&#24314;&#30340;&#25968;&#25454;&#26469;&#23545;LLMs&#36827;&#34892;&#24494;&#35843;&#65292;&#20174;&#32780;&#23454;&#29616;LLMs&#19982;&#22270;&#25968;&#25454;&#24211;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16567v1 Announce Type: new  Abstract: Graph Databases (Graph DB) are widely applied in various fields, including finance, social networks, and medicine. However, translating Natural Language (NL) into the Graph Query Language (GQL), commonly known as NL2GQL, proves to be challenging due to its inherent complexity and specialized nature. Some approaches have sought to utilize Large Language Models (LLMs) to address analogous tasks like text2SQL. Nevertheless, when it comes to NL2GQL taskson a particular domain, the absence of domain-specific NL-GQL data pairs makes it difficult to establish alignment between LLMs and the graph DB. To address this challenge, we propose a well-defined pipeline. Specifically, we utilize ChatGPT to create NL-GQL data pairs based on the given graph DB with self-instruct. Then, we use the created data to fine-tune LLMs, thereby achieving alignment between LLMs and the graph DB. Additionally, during inference, we propose a method that extracts relev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Roofline&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#20998;&#26512;LLM&#25512;&#26029;&#25216;&#26415;&#65292;&#24110;&#21161;&#35782;&#21035;&#37096;&#32626;&#20013;&#30340;&#29942;&#39048;&#65292;&#24182;&#20026;&#26356;&#26377;&#25928;&#22320;&#37096;&#32626;LLM&#25552;&#20379;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.16363</link><description>&lt;p&gt;
LLM&#25512;&#26029;&#25581;&#31034;&#65306;&#35843;&#26597;&#19982;Roofline&#27169;&#22411;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
LLM Inference Unveiled: Survey and Roofline Model Insights
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Roofline&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#20998;&#26512;LLM&#25512;&#26029;&#25216;&#26415;&#65292;&#24110;&#21161;&#35782;&#21035;&#37096;&#32626;&#20013;&#30340;&#29942;&#39048;&#65292;&#24182;&#20026;&#26356;&#26377;&#25928;&#22320;&#37096;&#32626;LLM&#25552;&#20379;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25512;&#26029;&#39046;&#22495;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#65292;&#25552;&#20379;&#20102;&#26426;&#36935;&#21644;&#25361;&#25112;&#30340;&#29420;&#29305;&#32467;&#21512;&#12290;&#34429;&#28982;&#35813;&#39046;&#22495;&#24050;&#32463;&#25193;&#23637;&#24182;&#20805;&#28385;&#27963;&#21147;&#65292;&#20294;&#33267;&#20170;&#36824;&#27809;&#26377;&#19968;&#20010;&#31616;&#26126;&#30340;&#26694;&#26550;&#26469;&#20998;&#26512;LLM&#25512;&#26029;&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#20197;&#20415;&#28165;&#26224;&#22320;&#29702;&#35299;&#36825;&#19968;&#39046;&#22495;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#19981;&#20165;&#24635;&#32467;&#20102;&#24403;&#21069;&#30740;&#31350;&#29616;&#29366;&#65292;&#36824;&#22522;&#20110;Roofline&#27169;&#22411;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#20998;&#26512;LLM&#25512;&#26029;&#25216;&#26415;&#12290;&#36825;&#19968;&#26694;&#26550;&#33021;&#22815;&#24110;&#21161;&#35782;&#21035;LLM&#37096;&#32626;&#20013;&#30340;&#29942;&#39048;&#65292;&#24182;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#22312;&#23454;&#38469;&#35774;&#22791;&#19978;&#30340;&#23454;&#38469;&#26041;&#38754;&#65292;&#20174;&#32780;&#20026;&#37096;&#32626;LLM&#25552;&#20379;&#26356;&#26377;&#25928;&#30340;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#31995;&#32479;&#22320;&#27719;&#24635;&#20102;&#39640;&#25928;LLM&#25512;&#26029;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#28085;&#30422;&#20851;&#38190;&#39046;&#22495;&#65292;&#27604;&#22914;&#26435;&#37325;&#20248;&#21270;&#65288;&#22914;&#30693;&#35782;&#33976;&#39311;&#21644;&#37327;&#21270;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16363v1 Announce Type: cross  Abstract: The field of efficient Large Language Model (LLM) inference is rapidly evolving, presenting a unique blend of opportunities and challenges. Although the field has expanded and is vibrant, there hasn't been a concise framework that analyzes the various methods of LLM Inference to provide a clear understanding of this domain. Our survey stands out from traditional literature reviews by not only summarizing the current state of research but also by introducing a framework based on roofline model for systematic analysis of LLM inference techniques. This framework enables identifying the bottlenecks in LLM deployments and provides a deeper understanding of the practical aspects on real devices, thereby informing more effective strategies for deploying LLM. Furthermore, we systematically collate the latest advancements in efficient LLM inference, covering crucial areas such as weight optimization (e.g., Knowledge Distillation and Quantizatio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21305;&#37197;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#31181;&#26412;&#20307;&#23884;&#20837;&#27169;&#22411;&#25429;&#33719;&#20840;&#23616;&#21644;&#23616;&#37096;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#27010;&#24565;&#23376;&#31867;&#39044;&#27979;&#30340;&#31283;&#20581;&#24615;</title><link>https://arxiv.org/abs/2402.16278</link><description>&lt;p&gt;
&#19968;&#31181;&#20351;&#29992;&#27880;&#37322;&#23884;&#20837;&#27169;&#22411;&#30340;&#26412;&#20307;&#21253;&#21547;&#20851;&#31995;&#39044;&#27979;&#33258;&#21305;&#37197;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Self-matching Training Method with Annotation Embedding Models for Ontology Subsumption Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16278
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21305;&#37197;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#31181;&#26412;&#20307;&#23884;&#20837;&#27169;&#22411;&#25429;&#33719;&#20840;&#23616;&#21644;&#23616;&#37096;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#27010;&#24565;&#23376;&#31867;&#39044;&#27979;&#30340;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20302;&#32500;&#31354;&#38388;&#20013;&#34920;&#31034;&#23454;&#20307;&#30340;&#26412;&#20307;&#23884;&#20837;&#65292;&#29992;&#20110;&#26412;&#20307;&#23436;&#25104;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#27010;&#24565;&#23376;&#31867;&#39044;&#27979;&#30340;&#26412;&#20307;&#23884;&#20837;&#26410;&#35299;&#20915;&#31867;&#20284;&#21644;&#23396;&#31435;&#23454;&#20307;&#30340;&#22256;&#38590;&#65292;&#24182;&#19988;&#26410;&#25552;&#21462;&#26412;&#20307;&#20013;&#27880;&#37322;&#20844;&#29702;&#30340;&#20840;&#23616;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20004;&#31181;&#26412;&#20307;&#23884;&#20837;&#27169;&#22411;&#30340;&#33258;&#21305;&#37197;&#35757;&#32451;&#26041;&#27861;&#65306;Inverted-index Matrix Embedding (InME) &#21644; Co-occurrence Matrix Embedding (CoME)&#12290;&#36825;&#20004;&#31181;&#23884;&#20837;&#36890;&#36807;&#27599;&#20010;&#21333;&#35789;&#22312;&#19968;&#32452;&#20844;&#29702;&#20013;&#20986;&#29616;&#30340;&#20301;&#32622;&#20197;&#21450;&#27599;&#20010;&#20844;&#29702;&#20013;&#21333;&#35789;&#30340;&#20849;&#29616;&#26469;&#25429;&#33719;&#27880;&#37322;&#20844;&#29702;&#20013;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#20449;&#24687;&#12290;&#33258;&#21305;&#37197;&#35757;&#32451;&#26041;&#27861;&#25552;&#39640;&#20102;&#27010;&#24565;&#23376;&#31867;&#39044;&#27979;&#30340;&#31283;&#20581;&#24615;&#65292;&#24403;&#39044;&#27979;&#30340;&#36229;&#31867;&#19982;&#23376;&#31867;&#30456;&#20284;&#19988;&#23396;&#31435;&#20110;&#26412;&#20307;&#20013;&#30340;&#20854;&#20182;&#23454;&#20307;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16278v1 Announce Type: new  Abstract: Recently, ontology embeddings representing entities in a low-dimensional space have been proposed for ontology completion. However, the ontology embeddings for concept subsumption prediction do not address the difficulties of similar and isolated entities and fail to extract the global information of annotation axioms from an ontology. In this paper, we propose a self-matching training method for the two ontology embedding models: Inverted-index Matrix Embedding (InME) and Co-occurrence Matrix Embedding (CoME). The two embeddings capture the global and local information in annotation axioms by means of the occurring locations of each word in a set of axioms and the co-occurrences of words in each axiom. The self-matching training method increases the robustness of the concept subsumption prediction when predicted superclasses are similar to subclasses and are isolated to other entities in an ontology. Our evaluation experiments show that
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;UniRetriever&#26694;&#26550;&#65292;&#21033;&#29992;&#21452;&#32534;&#30721;&#22120;&#26550;&#26500;&#21644;&#20004;&#20010;&#25439;&#22833;&#32422;&#26463;&#23454;&#29616;&#20102;&#22810;&#20219;&#21153;&#20505;&#36873;&#32773;&#36873;&#25321;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#24773;&#22659;&#19979;&#30340;&#23545;&#35805;&#26816;&#32034;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.16261</link><description>&lt;p&gt;
UniRetriever&#65306;&#21508;&#31181;&#24773;&#22659;&#33258;&#36866;&#24212;&#23545;&#35805;&#26816;&#32034;&#30340;&#22810;&#20219;&#21153;&#20505;&#36873;&#32773;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
UniRetriever: Multi-task Candidates Selection for Various Context-Adaptive Conversational Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16261
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;UniRetriever&#26694;&#26550;&#65292;&#21033;&#29992;&#21452;&#32534;&#30721;&#22120;&#26550;&#26500;&#21644;&#20004;&#20010;&#25439;&#22833;&#32422;&#26463;&#23454;&#29616;&#20102;&#22810;&#20219;&#21153;&#20505;&#36873;&#32773;&#36873;&#25321;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#24773;&#22659;&#19979;&#30340;&#23545;&#35805;&#26816;&#32034;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#26816;&#32034;&#26159;&#25351;&#20197;&#36845;&#20195;&#21644;&#20132;&#20114;&#26041;&#24335;&#36816;&#34892;&#30340;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#65292;&#38656;&#35201;&#26816;&#32034;&#21508;&#31181;&#22806;&#37096;&#36164;&#28304;&#65288;&#22914;&#20154;&#35774;&#12289;&#30693;&#35782;&#29978;&#33267;&#22238;&#24212;&#65289;&#20197;&#26377;&#25928;&#19982;&#29992;&#25143;&#20132;&#20114;&#24182;&#25104;&#21151;&#23436;&#25104;&#23545;&#35805;&#12290;&#20026;&#20102;&#25552;&#39640;&#25928;&#29575;&#21644;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#26694;&#26550;&#65292;&#20316;&#20026;&#19977;&#20010;&#20027;&#35201;&#26816;&#32034;&#20219;&#21153;&#30340;&#36890;&#29992;&#26816;&#32034;&#22120;&#65306;&#20154;&#35774;&#36873;&#25321;&#12289;&#30693;&#35782;&#36873;&#25321;&#21644;&#22238;&#24212;&#36873;&#25321;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21452;&#32534;&#30721;&#22120;&#26550;&#26500;&#65292;&#21253;&#25324;&#19968;&#20010;&#24773;&#22659;&#33258;&#36866;&#24212;&#23545;&#35805;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#20505;&#36873;&#32773;&#32534;&#30721;&#22120;&#65292;&#26088;&#22312;&#36890;&#36807;&#31616;&#21333;&#30340;&#28857;&#31215;&#20851;&#27880;&#38271;&#23545;&#35805;&#20013;&#30340;&#30456;&#20851;&#19978;&#19979;&#25991;&#24182;&#26816;&#32034;&#21512;&#36866;&#30340;&#20505;&#36873;&#32773;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#25439;&#22833;&#32422;&#26463;&#20197;&#25429;&#25417;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16261v1 Announce Type: new  Abstract: Conversational retrieval refers to an information retrieval system that operates in an iterative and interactive manner, requiring the retrieval of various external resources, such as persona, knowledge, and even response, to effectively engage with the user and successfully complete the dialogue. However, most previous work trained independent retrievers for each specific resource, resulting in sub-optimal performance and low efficiency. Thus, we propose a multi-task framework function as a universal retriever for three dominant retrieval tasks during the conversation: persona selection, knowledge selection, and response selection. To this end, we design a dual-encoder architecture consisting of a context-adaptive dialogue encoder and a candidate encoder, aiming to attention to the relevant context from the long dialogue and retrieve suitable candidates by simply a dot product. Furthermore, we introduce two loss constraints to capture t
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;MATHWELL&#27169;&#22411;&#29983;&#25104;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#33521;&#25991;&#25968;&#23398;&#24212;&#29992;&#39064;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;20,490&#20010;&#38382;&#39064;&#65292;&#32463;&#39046;&#22495;&#19987;&#23478;&#35780;&#20998;&#32467;&#26524;&#26174;&#31034;&#65292;MATHWELL&#30340;&#38382;&#39064;&#20013;&#20855;&#26377;&#21487;&#25191;&#34892;&#35299;&#20915;&#26041;&#26696;&#24182;&#31526;&#21512;&#25152;&#26377;&#26631;&#20934;&#30340;&#20221;&#39069;&#27604;&#20854;&#20182;&#36873;&#25321;&#39640;&#20986;40%&#65292;&#20854;&#20013;74%&#30340;&#21487;&#35299;&#20915;&#38382;&#39064;&#21516;&#26102;&#20570;&#21040;&#20102;&#20934;&#30830;&#21644;&#36866;&#24403;&#12290;</title><link>https://arxiv.org/abs/2402.15861</link><description>&lt;p&gt;
MATHWELL: &#22312;&#35268;&#27169;&#19978;&#29983;&#25104;&#25945;&#32946;&#25968;&#23398;&#24212;&#29992;&#39064;
&lt;/p&gt;
&lt;p&gt;
MATHWELL: Generating Educational Math Word Problems at Scale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15861
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;MATHWELL&#27169;&#22411;&#29983;&#25104;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#33521;&#25991;&#25968;&#23398;&#24212;&#29992;&#39064;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;20,490&#20010;&#38382;&#39064;&#65292;&#32463;&#39046;&#22495;&#19987;&#23478;&#35780;&#20998;&#32467;&#26524;&#26174;&#31034;&#65292;MATHWELL&#30340;&#38382;&#39064;&#20013;&#20855;&#26377;&#21487;&#25191;&#34892;&#35299;&#20915;&#26041;&#26696;&#24182;&#31526;&#21512;&#25152;&#26377;&#26631;&#20934;&#30340;&#20221;&#39069;&#27604;&#20854;&#20182;&#36873;&#25321;&#39640;&#20986;40%&#65292;&#20854;&#20013;74%&#30340;&#21487;&#35299;&#20915;&#38382;&#39064;&#21516;&#26102;&#20570;&#21040;&#20102;&#20934;&#30830;&#21644;&#36866;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#24212;&#29992;&#39064;&#22312;K-8&#25945;&#32946;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#32534;&#20889;&#23427;&#20204;&#32791;&#26102;&#19988;&#38656;&#35201;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;&#25105;&#20204;&#35748;&#20026;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#35268;&#27169;&#21270;&#38382;&#39064;&#26469;&#25903;&#25345;K-8&#25968;&#23398;&#25945;&#32946;&#12290;&#20026;&#20102;&#25945;&#32946;&#24615;&#65292;&#29983;&#25104;&#30340;&#38382;&#39064;&#24517;&#39035;&#26159;1&#65289;&#21487;&#35299;&#20915;&#30340;&#65292;2&#65289;&#20934;&#30830;&#30340;&#65292;3&#65289;&#36866;&#24403;&#30340;&#12290;&#29616;&#26377;&#25968;&#25454;&#38598;&#26410;&#26631;&#35760;&#36825;&#20123;&#26631;&#20934;&#65292;&#22240;&#27492;&#19981;&#36866;&#21512;&#35757;&#32451;&#38382;&#39064;&#29983;&#25104;&#22120;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;MATHWELL&#65292;&#36825;&#26159;&#19968;&#20010;&#32463;&#36807;&#19987;&#23478;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#36845;&#20195;&#24494;&#35843;&#30340;70B Llama-2&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;K-8&#25968;&#23398;&#24212;&#29992;&#39064;&#12290;&#20511;&#21161;MATHWELL&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#33521;&#25991;&#24212;&#29992;&#39064;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;20,490&#20010;&#38382;&#39064;&#12290;&#32463;&#39046;&#22495;&#19987;&#23478;&#35780;&#20998;&#30340;3,484&#20010;&#38382;&#39064;&#21457;&#29616;&#65292;MATHWELL&#25317;&#26377;&#27604;&#20854;&#20182;&#36873;&#25321;&#26356;&#39640;&#30340;&#21487;&#25191;&#34892;&#35299;&#20915;&#26041;&#26696;&#21644;&#28385;&#36275;&#25152;&#26377;&#26631;&#20934;&#30340;&#38382;&#39064;&#20221;&#39069;&#39640;&#20986;40&#65285;&#65292;&#20854;&#20013;74&#65285;&#30340;&#38382;&#39064;&#20855;&#26377;&#21487;&#35299;&#30340;&#12289;&#20934;&#30830;&#30340;&#21644;&#36866;&#24403;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15861v1 Announce Type: new  Abstract: Math word problems are critical K-8 educational tools, but writing them is time-consuming and requires domain expertise. We suggest that language models can support K-8 math education by automatically generating problems at scale. To be educational, generated problems must be 1) solvable, 2) accurate, and 3) appropriate. Existing datasets are unlabeled for these criteria, making them ill-suited for training problem generators. We introduce MATHWELL, a Llama-2 (70B) model iteratively finetuned to generate K-8 math word problems using data from expert annotation. Using MATHWELL, we generate the largest English word problem dataset to date, containing 20,490 problems. 3,484 are scored by domain experts who find MATHWELL has a 40% higher share of problems that have executable solutions and meet all criteria than alternatives, with 74% of its problems with executable solutions being solvable, accurate, and appropriate.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#23558;&#22823;&#35268;&#27169;&#22810;&#27169;&#25968;&#25454;&#36716;&#21270;&#20026;&#36830;&#36143;&#27969;&#30021;&#25991;&#26412;&#65292;&#39318;&#27425;&#25512;&#20986;&#20102;&#29992;&#20110;&#20307;&#32946;&#21644;&#38899;&#20048;&#39046;&#22495;&#30340;AI&#35780;&#35770;&#31995;&#32479;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.15514</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25991;&#26412;&#22312;&#20307;&#32946;&#21644;&#38899;&#20048;&#39046;&#22495;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Large Scale Generative AI Text Applied to Sports and Music
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15514
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#23558;&#22823;&#35268;&#27169;&#22810;&#27169;&#25968;&#25454;&#36716;&#21270;&#20026;&#36830;&#36143;&#27969;&#30021;&#25991;&#26412;&#65292;&#39318;&#27425;&#25512;&#20986;&#20102;&#29992;&#20110;&#20307;&#32946;&#21644;&#38899;&#20048;&#39046;&#22495;&#30340;AI&#35780;&#35770;&#31995;&#32479;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#23558;&#23186;&#20307;&#20869;&#23481;&#65288;&#21253;&#25324;&#35780;&#35770;&#21644;&#20010;&#24615;&#21270;&#26032;&#38395;&#25253;&#36947;&#65289;&#25193;&#23637;&#21040;&#20840;&#29699;&#22823;&#22411;&#20307;&#32946;&#21644;&#38899;&#20048;&#27963;&#21160;&#30340;&#29983;&#20135;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#23558;&#22823;&#37327;&#22810;&#27169;&#25968;&#25454;&#65288;&#20363;&#22914;&#35270;&#39057;&#12289;&#25991;&#31456;&#12289;&#23454;&#26102;&#27604;&#20998;&#12289;&#32479;&#35745;&#25968;&#25454;&#21644;&#36164;&#26009;&#65289;&#36716;&#25442;&#20026;&#36830;&#36143;&#27969;&#30021;&#30340;&#25991;&#26412;&#12290;&#22522;&#20110;&#36825;&#19968;&#26041;&#27861;&#65292;&#25105;&#20204;&#39318;&#27425;&#25512;&#20986;&#20102;&#19968;&#27454;&#20154;&#24037;&#26234;&#33021;&#35780;&#35770;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#34987;&#37096;&#32626;&#29992;&#20110;&#20026;2023&#24180;&#32654;&#22269;&#20844;&#24320;&#36187;&#12289;&#28201;&#24067;&#23572;&#30331;&#20844;&#24320;&#36187;&#21644;&#22823;&#24072;&#36187;&#30340;&#31934;&#24425;&#29255;&#27573;&#21046;&#20316;&#33258;&#21160;&#21270;&#21465;&#36848;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#36824;&#34987;&#25193;&#23637;&#29992;&#20110;&#20026;ESPN&#26790;&#24187;&#27204;&#27012;&#29699;&#21644;&#26684;&#33713;&#32654;&#22870;&#38899;&#20048;&#33402;&#26415;&#23478;&#25925;&#20107;&#21019;&#36896;&#20010;&#24615;&#21270;&#20869;&#23481;&#12290;&#36825;&#20123;&#24212;&#29992;&#31243;&#24207;&#37319;&#29992;&#20102;&#30456;&#21516;&#30340;&#36719;&#20214;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;15&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#65292;&#24179;&#22343;Rouge-L&#20026;82.00&#65292;&#22256;&#24785;&#24230;&#20026;6.6&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15514v1 Announce Type: cross  Abstract: We address the problem of scaling up the production of media content, including commentary and personalized news stories, for large-scale sports and music events worldwide. Our approach relies on generative AI models to transform a large volume of multimodal data (e.g., videos, articles, real-time scoring feeds, statistics, and fact sheets) into coherent and fluent text. Based on this approach, we introduce, for the first time, an AI commentary system, which was deployed to produce automated narrations for highlight packages at the 2023 US Open, Wimbledon, and Masters tournaments. In the same vein, our solution was extended to create personalized content for ESPN Fantasy Football and stories about music artists for the Grammy awards. These applications were built using a common software architecture achieved a 15x speed improvement with an average Rouge-L of 82.00 and perplexity of 6.6. Our work was successfully deployed at the aforeme
&lt;/p&gt;</description></item><item><title>RED&#36890;&#36807;&#34920;&#31034;&#32534;&#36753;&#26174;&#33879;&#38477;&#20302;&#20102;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#65292;&#23454;&#29616;&#20102;&#19982;&#23436;&#20840;&#21442;&#25968;&#24494;&#35843;&#21644;&#20854;&#20182;PEFT&#26041;&#27861;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#32467;&#26524;</title><link>https://arxiv.org/abs/2402.15179</link><description>&lt;p&gt;
&#36890;&#36807;&#34920;&#31034;&#32534;&#36753;&#25512;&#36827;&#24494;&#35843;&#20013;&#30340;&#21442;&#25968;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Advancing Parameter Efficiency in Fine-tuning via Representation Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15179
&lt;/p&gt;
&lt;p&gt;
RED&#36890;&#36807;&#34920;&#31034;&#32534;&#36753;&#26174;&#33879;&#38477;&#20302;&#20102;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#65292;&#23454;&#29616;&#20102;&#19982;&#23436;&#20840;&#21442;&#25968;&#24494;&#35843;&#21644;&#20854;&#20182;PEFT&#26041;&#27861;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#26377;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#22240;&#20854;&#33021;&#22815;&#22312;&#20165;&#26356;&#26032;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#19968;&#20010;&#23567;&#23376;&#38598;&#26102;&#36798;&#21040;&#31454;&#20105;&#24615;&#32467;&#26524;&#32780;&#21463;&#21040;&#20102;&#37325;&#35270;&#12290;&#22312;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24494;&#35843;&#31070;&#32463;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#34920;&#31034;&#32534;&#36753;&#65288;RED&#65289;&#65292;&#20854;&#25193;&#25918;&#21644;&#20559;&#32622;&#27599;&#19968;&#23618;&#20135;&#29983;&#30340;&#34920;&#31034;&#12290;&#19982;&#23436;&#20840;&#21442;&#25968;&#24494;&#35843;&#30456;&#27604;&#65292;RED&#23558;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#38477;&#20302;&#20102;$25,700$&#20493;&#65292;&#24182;&#19982;LoRA&#30456;&#27604;&#38477;&#20302;&#20102;32&#20493;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;RED&#23454;&#29616;&#20102;&#19982;&#23436;&#20840;&#21442;&#25968;&#24494;&#35843;&#21644;&#20854;&#20182;PEFT&#26041;&#27861;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#23545;&#19981;&#21516;&#26550;&#26500;&#21644;&#35268;&#27169;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15179v1 Announce Type: cross  Abstract: Parameter Efficient Fine-Tuning (PEFT) has gained significant attention for its ability to achieve competitive results while updating only a small subset of trainable parameters. Despite the promising performance of current PEFT methods, they present challenges in hyperparameter selection, such as determining the rank of LoRA or Adapter, or specifying the length of soft prompts. In addressing these challenges, we propose a novel approach to fine-tuning neural models, termed Representation EDiting (RED), which scales and biases the representation produced at each layer. RED substantially reduces the number of trainable parameters by a factor of $25,700$ compared to full parameter fine-tuning, and by a factor of $32$ compared to LoRA. Remarkably, RED achieves comparable or superior results to full parameter fine-tuning and other PEFT methods. Extensive experiments were conducted across models of varying architectures and scales, includin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21518;&#38376;&#22686;&#24378;&#23545;&#40784;&#26041;&#27861;&#26377;&#25928;&#32531;&#35299;&#24494;&#35843;&#36234;&#29425;&#25915;&#20987;&#65292;&#36991;&#20813;&#38656;&#35201;&#22823;&#37327;&#23433;&#20840;&#31034;&#20363;&#30340;&#20302;&#25928;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.14968</link><description>&lt;p&gt;
&#20351;&#29992;&#21518;&#38376;&#22686;&#24378;&#23545;&#40784;&#26469;&#32531;&#35299;&#24494;&#35843;&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14968
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21518;&#38376;&#22686;&#24378;&#23545;&#40784;&#26041;&#27861;&#26377;&#25928;&#32531;&#35299;&#24494;&#35843;&#36234;&#29425;&#25915;&#20987;&#65292;&#36991;&#20813;&#38656;&#35201;&#22823;&#37327;&#23433;&#20840;&#31034;&#20363;&#30340;&#20302;&#25928;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-4&#21644;Llama-2&#20855;&#26377;&#19968;&#33324;&#33021;&#21147;&#65292;&#20294;&#22312;&#28385;&#36275;&#29305;&#23450;&#19994;&#21153;&#38656;&#27714;&#21644;&#23450;&#21046;&#29992;&#20363;&#30340;&#22797;&#26434;&#24615;&#26102;&#65292;&#20173;&#28982;&#38656;&#35201;&#23545;&#20854;&#36827;&#34892;&#24494;&#35843;&#25110;&#33258;&#36866;&#24212;&#20197;&#28385;&#36275;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36807;&#31243;&#19981;&#21487;&#36991;&#20813;&#22320;&#24341;&#20837;&#20102;&#26032;&#30340;&#23433;&#20840;&#23041;&#32961;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#22522;&#20110;&#24494;&#35843;&#30340;&#36234;&#29425;&#25915;&#20987;&#65288;FJAttack&#65289;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23558;&#20165;&#20960;&#20010;&#26377;&#23475;&#31034;&#20363;&#32435;&#20837;&#24494;&#35843;&#25968;&#25454;&#38598;&#23601;&#21487;&#33021;&#26174;&#30528;&#22320;&#25439;&#23475;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#28508;&#22312;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#20363;&#22914;&#23558;&#23433;&#20840;&#31034;&#20363;&#32435;&#20837;&#24494;&#35843;&#25968;&#25454;&#38598;&#20197;&#20943;&#23569;&#23433;&#20840;&#38382;&#39064;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#32435;&#20837;&#22823;&#37327;&#30340;&#23433;&#20840;&#31034;&#20363;&#65292;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#38024;&#23545;FJAttack&#36827;&#34892;&#38450;&#24481;&#24182;&#21482;&#20351;&#29992;&#26377;&#38480;&#30340;&#23433;&#20840;&#31034;&#20363;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#24863;&#26469;&#33258;&#21518;&#38376;&#25915;&#20987;&#27010;&#24565;&#30340;&#21518;&#38376;&#22686;&#24378;&#23433;&#20840;&#23545;&#40784;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14968v1 Announce Type: cross  Abstract: Despite the general capabilities of Large Language Models (LLMs) like GPT-4 and Llama-2, these models still request fine-tuning or adaptation with customized data when it comes to meeting the specific business demands and intricacies of tailored use cases. However, this process inevitably introduces new safety threats, particularly against the Fine-tuning based Jailbreak Attack (FJAttack), where incorporating just a few harmful examples into the fine-tuning dataset can significantly compromise the model safety. Though potential defenses have been proposed by incorporating safety examples into the fine-tuning dataset to reduce the safety issues, such approaches require incorporating a substantial amount of safety examples, making it inefficient. To effectively defend against the FJAttack with limited safety examples, we propose a Backdoor Enhanced Safety Alignment method inspired by an analogy with the concept of backdoor attacks. In pa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#33268;&#19988;&#22312;&#29702;&#35770;&#19978;&#26377;&#26681;&#25454;&#30340;&#26041;&#27861;&#26469;&#27880;&#37322;&#20998;&#35299;&#34164;&#28085;&#25968;&#25454;&#38598;&#65292;&#24418;&#25104;RDTE&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#22312;&#35299;&#20915;&#20309;&#20026;&#26377;&#25928;&#30340;&#32452;&#21512;&#34164;&#28085;&#30340;&#38382;&#39064;&#19978;&#26377;&#26174;&#33879;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2402.14798</link><description>&lt;p&gt;
&#21033;&#29992;&#38750;&#27491;&#24335;&#36923;&#36753;&#22686;&#24378;&#31995;&#32479;&#21270;&#20998;&#35299;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Enhancing Systematic Decompositional Natural Language Inference Using Informal Logic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#33268;&#19988;&#22312;&#29702;&#35770;&#19978;&#26377;&#26681;&#25454;&#30340;&#26041;&#27861;&#26469;&#27880;&#37322;&#20998;&#35299;&#34164;&#28085;&#25968;&#25454;&#38598;&#65292;&#24418;&#25104;RDTE&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#22312;&#35299;&#20915;&#20309;&#20026;&#26377;&#25928;&#30340;&#32452;&#21512;&#34164;&#28085;&#30340;&#38382;&#39064;&#19978;&#26377;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20195;&#35821;&#35328;&#27169;&#22411;&#20026;&#20351;&#29992;&#25991;&#26412;&#36827;&#34892;&#32467;&#26500;&#21270;&#25512;&#29702;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#65292;&#20363;&#22914;&#22312;&#19981;&#20381;&#36182;&#33030;&#24369;&#30340;&#24418;&#24335;&#36923;&#36753;&#30340;&#24773;&#20917;&#19979;&#26500;&#24314;&#21644;&#35780;&#20272;&#30452;&#35266;&#30340;&#12289;&#31867;&#20284;&#35777;&#26126;&#30340;&#25991;&#26412;&#34164;&#28085;&#26641;&#12290;&#28982;&#32780;&#65292;&#27839;&#30528;&#36825;&#20010;&#26041;&#21521;&#30340;&#36827;&#23637;&#21463;&#21040;&#19968;&#20010;&#38271;&#26399;&#20197;&#26469;&#32570;&#20047;&#26126;&#30830;&#30340;&#30830;&#23450;&#20309;&#20026;&#26377;&#25928;&#30340;&#32452;&#21512;&#34164;&#28085;&#30340;&#28165;&#26224;&#21327;&#35758;&#30340;&#38459;&#30861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19968;&#33268;&#19988;&#22312;&#29702;&#35770;&#19978;&#26377;&#26681;&#25454;&#30340;&#26041;&#27861;&#26469;&#27880;&#37322;&#20998;&#35299;&#34164;&#28085;&#25968;&#25454;&#38598;&#65292;&#24182;&#35780;&#20272;&#20854;&#23545;&#22522;&#20110;LLM&#30340;&#25991;&#26412;&#25512;&#29702;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#25968;&#25454;&#38598;RDTE (Recognizing Decompositional Textual Entailment) &#30340;&#20869;&#37096;&#19968;&#33268;&#24615;&#27604;&#20808;&#21069;&#30340;&#20998;&#35299;&#34164;&#28085;&#25968;&#25454;&#38598;&#39640;&#24471;&#22810;&#65288;+9%&#65289;&#65292;&#34920;&#26126;RDTE&#22312;&#38271;&#26399;&#23384;&#22312;&#30340;&#20851;&#20110;&#20309;&#20026;&#26377;&#25928;&#30340;&#32452;&#21512;&#34164;&#28085;&#30340;&#38382;&#39064;&#19978;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14798v1 Announce Type: cross  Abstract: Contemporary language models enable new opportunities for structured reasoning with text, such as the construction and evaluation of intuitive, proof-like textual entailment trees without relying on brittle formal logic. However, progress in this direction has been hampered by a long-standing lack of a clear protocol for determining what valid compositional entailment is. This absence causes noisy datasets and limited performance gains by modern neuro-symbolic engines. To address these problems, we formulate a consistent and theoretically grounded approach to annotating decompositional entailment datasets, and evaluate its impact on LLM-based textual inference. We find that our resulting dataset, RDTE (Recognizing Decompositional Textual Entailment), has a substantially higher internal consistency (+9%) than prior decompositional entailment datasets, suggesting that RDTE is a significant step forward in the long-standing problem of for
&lt;/p&gt;</description></item><item><title>OpenCodeInterpreter&#26159;&#19968;&#31181;&#24320;&#28304;&#20195;&#30721;&#31995;&#32479;&#65292;&#38598;&#25104;&#20102;&#25191;&#34892;&#12289;&#20154;&#31867;&#21453;&#39304;&#21644;&#21160;&#24577;&#20195;&#30721;&#32454;&#21270;&#30340;&#21151;&#33021;&#65292;&#24182;&#22312;&#20851;&#38190;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#29978;&#33267;&#19982;GPT-4&#30456;&#23218;&#32654;&#12290;</title><link>https://arxiv.org/abs/2402.14658</link><description>&lt;p&gt;
OpenCodeInterpreter&#65306;&#38598;&#25104;&#20195;&#30721;&#29983;&#25104;&#12289;&#25191;&#34892;&#21644;&#32454;&#21270;
&lt;/p&gt;
&lt;p&gt;
OpenCodeInterpreter: Integrating Code Generation with Execution and Refinement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14658
&lt;/p&gt;
&lt;p&gt;
OpenCodeInterpreter&#26159;&#19968;&#31181;&#24320;&#28304;&#20195;&#30721;&#31995;&#32479;&#65292;&#38598;&#25104;&#20102;&#25191;&#34892;&#12289;&#20154;&#31867;&#21453;&#39304;&#21644;&#21160;&#24577;&#20195;&#30721;&#32454;&#21270;&#30340;&#21151;&#33021;&#65292;&#24182;&#22312;&#20851;&#38190;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#29978;&#33267;&#19982;GPT-4&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24341;&#20837;&#26174;&#33879;&#25512;&#21160;&#20102;&#20195;&#30721;&#29983;&#25104;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#24320;&#28304;&#27169;&#22411;&#36890;&#24120;&#32570;&#20047;&#31867;&#20284;GPT-4 Code Interpreter&#36825;&#26679;&#30340;&#39640;&#32423;&#31995;&#32479;&#30340;&#25191;&#34892;&#33021;&#21147;&#21644;&#36845;&#20195;&#32454;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;OpenCodeInterpreter&#65292;&#36825;&#26159;&#19968;&#26063;&#26088;&#22312;&#29983;&#25104;&#12289;&#25191;&#34892;&#21644;&#36845;&#20195;&#32454;&#21270;&#20195;&#30721;&#30340;&#24320;&#28304;&#20195;&#30721;&#31995;&#32479;&#12290;&#36890;&#36807;Code-Feedback&#25903;&#25345;&#65292;&#35813;&#31995;&#32479;&#38598;&#25104;&#20102;&#25191;&#34892;&#21644;&#20154;&#31867;&#21453;&#39304;&#65292;&#29992;&#20110;&#21160;&#24577;&#20195;&#30721;&#32454;&#21270;&#12290;&#25105;&#20204;&#23545;OpenCodeInterpreter&#22312;&#35832;&#22914;HumanEval&#12289;MBPP&#20197;&#21450;&#23427;&#20204;&#26469;&#33258;EvalPlus&#30340;&#22686;&#24378;&#29256;&#26412;&#31561;&#20851;&#38190;&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#35777;&#23454;&#20102;&#20854;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;OpenCodeInterpreter-33B&#22312;HumanEval&#21644;MBPP&#30340;&#24179;&#22343;&#20540;&#65288;&#20197;&#21450;&#20854;&#22686;&#24378;&#29256;&#26412;&#65289;&#19978;&#21462;&#24471;&#20102;83.2&#65288;76.4&#65289;&#30340;&#20934;&#30830;&#29575;&#65292;&#19982;GPT-4&#30340;84.2&#65288;76.2&#65289;&#32039;&#23494;&#21305;&#25932;&#65292;&#24182;&#19988;&#36890;&#36807;&#21512;&#25104;hum
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14658v1 Announce Type: cross  Abstract: The introduction of large language models has significantly advanced code generation. However, open-source models often lack the execution capabilities and iterative refinement of advanced systems like the GPT-4 Code Interpreter. To address this, we introduce OpenCodeInterpreter, a family of open-source code systems designed for generating, executing, and iteratively refining code. Supported by Code-Feedback, a dataset featuring 68K multi-turn interactions, OpenCodeInterpreter integrates execution and human feedback for dynamic code refinement. Our comprehensive evaluation of OpenCodeInterpreter across key benchmarks such as HumanEval, MBPP, and their enhanced versions from EvalPlus reveals its exceptional performance. Notably, OpenCodeInterpreter-33B achieves an accuracy of 83.2 (76.4) on the average (and plus versions) of HumanEval and MBPP, closely rivaling GPT-4's 84.2 (76.2) and further elevates to 91.6 (84.6) with synthesized hum
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#33258;&#21160;&#21270;&#25968;&#25454;&#38598;&#26356;&#26032;&#31574;&#30053;&#65292;&#21033;&#29992;&#20004;&#31181;&#31995;&#32479;&#39564;&#35777;&#36807;&#30340;&#31574;&#30053;&#29983;&#25104;&#30475;&#19981;&#35265;&#21644;&#39640;&#36136;&#37327;&#30340;&#27979;&#35797;&#26679;&#26412;&#65292;&#20197;&#32531;&#35299;&#25968;&#25454;&#27844;&#28431;&#38382;&#39064;&#24182;&#23454;&#29616;&#35780;&#20272;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11894</link><description>&lt;p&gt;
&#20320;&#35265;&#36807;&#25105;&#21527;&#65311;&#33258;&#21160;&#21270;&#25968;&#25454;&#38598;&#26356;&#26032;&#20197;&#23454;&#29616;&#21487;&#38752;&#21450;&#21450;&#26102;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Have Seen Me Before? Automating Dataset Updates Towards Reliable and Timely Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11894
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#33258;&#21160;&#21270;&#25968;&#25454;&#38598;&#26356;&#26032;&#31574;&#30053;&#65292;&#21033;&#29992;&#20004;&#31181;&#31995;&#32479;&#39564;&#35777;&#36807;&#30340;&#31574;&#30053;&#29983;&#25104;&#30475;&#19981;&#35265;&#21644;&#39640;&#36136;&#37327;&#30340;&#27979;&#35797;&#26679;&#26412;&#65292;&#20197;&#32531;&#35299;&#25968;&#25454;&#27844;&#28431;&#38382;&#39064;&#24182;&#23454;&#29616;&#35780;&#20272;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#19981;&#26029;&#25193;&#22823;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#22686;&#21152;&#65292;LLMs&#38754;&#20020;&#30528;&#26085;&#30410;&#20005;&#37325;&#30340;&#35780;&#20272;&#25361;&#25112;&#12290;&#19968;&#26041;&#38754;&#65292;&#25968;&#25454;&#27844;&#28431;&#38382;&#39064;&#23548;&#33268;&#23545;&#29616;&#26377;&#22522;&#20934;&#30340;&#36807;&#24230;&#20272;&#35745;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23450;&#26399;&#25163;&#21160;&#25972;&#29702;&#25968;&#25454;&#38598;&#25104;&#26412;&#39640;&#26114;&#12290;&#26412;&#25991;&#25552;&#20986;&#33258;&#21160;&#21270;&#25968;&#25454;&#38598;&#26356;&#26032;&#20197;&#23454;&#29616;&#21487;&#38752;&#21450;&#21450;&#26102;&#30340;&#35780;&#20272;&#12290;&#22522;&#26412;&#24605;&#24819;&#26159;&#22522;&#20110;&#29616;&#26377;&#26679;&#26412;&#29983;&#25104;&#30475;&#19981;&#35265;&#30340;&#39640;&#36136;&#37327;&#27979;&#35797;&#26679;&#26412;&#20197;&#20943;&#36731;&#27844;&#28431;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#24182;&#36827;&#34892;&#20102;&#31995;&#32479;&#39564;&#35777;&#12290;&#31532;&#19968;&#31181;&#26159;&#27169;&#20223;&#31574;&#30053;&#65292;&#21033;&#29992;LLMs&#21019;&#24314;&#31867;&#20284;&#29616;&#26377;&#26679;&#26412;&#30340;&#26032;&#26679;&#26412;&#65292;&#26368;&#22823;&#31243;&#24230;&#22320;&#20445;&#30041;&#21407;&#22987;&#25968;&#25454;&#38598;&#30340;&#39118;&#26684;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#23427;&#22312;&#22810;&#27425;&#23454;&#20363;&#20013;&#30340;&#35780;&#20272;&#31283;&#23450;&#24615;&#20197;&#21450;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#22788;&#29702;&#25968;&#25454;&#27844;&#28431;&#38382;&#39064;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11894v1 Announce Type: new  Abstract: Due to the expanding capabilities and pre-training data, Large Language Models (LLMs) are facing increasingly serious evaluation challenges. On one hand, the data leakage issue cause over-estimation on existing benchmarks. On the other hand, periodically curating datasets manually is costly. In this paper, we propose to automate dataset updates for reliable and timely evaluation. The basic idea is to generate unseen and high-quality testing samples based on existing ones to mitigate leakage issues. In specific, we propose two strategies with systematically verification. First, the mimicking strategy employs LLMs to create new samples resembling existing ones, to the maximum extent preserving the stylistic of the original dataset. Our experiments demonstrate its evaluation stability across multiple instantiations and its effectiveness in dealing with data leakage issues in most cases. Second, for the cases that mimicking dataset works poo
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#23558;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#25351;&#26631;&#20174;&#22270;&#20687;&#29983;&#25104;&#36716;&#21270;&#20026;&#25991;&#26412;&#29983;&#25104;&#65292;&#32454;&#33268;&#35780;&#20272;&#20102;LLMs&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#65292;&#25581;&#31034;&#20102;&#24403;&#21069;LLMs&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#24615;&#33021;&#34920;&#29616;&#30340;&#37325;&#35201;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.10693</link><description>&lt;p&gt;
&#25506;&#32034;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#20197;&#35780;&#20272;LLMs&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring Precision and Recall to assess the quality and diversity of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10693
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#23558;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#25351;&#26631;&#20174;&#22270;&#20687;&#29983;&#25104;&#36716;&#21270;&#20026;&#25991;&#26412;&#29983;&#25104;&#65292;&#32454;&#33268;&#35780;&#20272;&#20102;LLMs&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#65292;&#25581;&#31034;&#20102;&#24403;&#21069;LLMs&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#24615;&#33021;&#34920;&#29616;&#30340;&#37325;&#35201;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;Llama-2&#21644;Mistral&#30340;&#26032;&#22411;&#35780;&#20272;&#26694;&#26550;&#65292;&#37325;&#28857;&#26159;&#23558;&#22270;&#20687;&#29983;&#25104;&#30340;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#25351;&#26631;&#36716;&#21270;&#20026;&#25991;&#26412;&#29983;&#25104;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#23545;&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#36827;&#34892;&#32454;&#33268;&#35780;&#20272;&#65292;&#32780;&#26080;&#38656;&#23545;&#40784;&#30340;&#35821;&#26009;&#24211;&#12290;&#36890;&#36807;&#23545;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#30740;&#31350;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#24320;&#25918;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#36825;&#26159;&#20256;&#32479;&#22522;&#20934;&#26080;&#27861;&#20805;&#20998;&#25429;&#25417;&#30340;&#12290;&#30740;&#31350;&#32467;&#26524;&#31361;&#20986;&#20102;&#22312;&#27169;&#22411;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#24494;&#35843;&#26102;&#65292;&#29983;&#25104;&#26679;&#26412;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#36825;&#39033;&#24037;&#20316;&#25193;&#23637;&#20102;&#22522;&#20110;&#20998;&#24067;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#35780;&#20272;&#24037;&#20855;&#21253;&#65292;&#20026;&#24403;&#21069;LLMs&#22312;&#29983;&#25104;&#22810;&#26679;&#24615;&#21644;&#39640;&#36136;&#37327;&#25991;&#26412;&#26041;&#38754;&#38754;&#20020;&#30340;&#23454;&#38469;&#33021;&#21147;&#21644;&#25361;&#25112;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10693v1 Announce Type: new  Abstract: This paper introduces a novel evaluation framework for Large Language Models (LLMs) such as Llama-2 and Mistral, focusing on the adaptation of Precision and Recall metrics from image generation to text generation. This approach allows for a nuanced assessment of the quality and diversity of generated text without the need for aligned corpora. By conducting a comprehensive evaluation of state-of-the-art language models, the study reveals significant insights into their performance on open-ended generation tasks, which are not adequately captured by traditional benchmarks. The findings highlight a trade-off between the quality and diversity of generated samples, particularly when models are fine-tuned with human feedback. This work extends the toolkit for distribution-based NLP evaluation, offering insights into the practical capabilities and challenges faced by current LLMs in generating diverse and high-quality text.
&lt;/p&gt;</description></item><item><title>BitDelta&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#20449;&#24687;&#20887;&#20313;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BitDelta&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#24494;&#35843;&#36807;&#31243;&#20013;&#28155;&#21152;&#30340;&#20449;&#24687;&#37327;&#21270;&#20026;&#19968;&#20010;&#27604;&#29305;&#65292;&#21516;&#26102;&#20445;&#25345;&#24615;&#33021;&#12290;&#36825;&#19968;&#21457;&#29616;&#23545;&#20110;&#22810;&#31199;&#25143;&#27169;&#22411;&#30340;&#26381;&#21153;&#21644;&#23384;&#20648;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#24182;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;GPU&#20869;&#23384;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2402.10193</link><description>&lt;p&gt;
BitDelta&#65306;&#20320;&#30340;&#24494;&#35843;&#21487;&#33021;&#21482;&#26377;&#19968;&#20010;&#27604;&#29305;&#30340;&#20215;&#20540;
&lt;/p&gt;
&lt;p&gt;
BitDelta: Your Fine-Tune May Only Be Worth One Bit
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10193
&lt;/p&gt;
&lt;p&gt;
BitDelta&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#20449;&#24687;&#20887;&#20313;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BitDelta&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#24494;&#35843;&#36807;&#31243;&#20013;&#28155;&#21152;&#30340;&#20449;&#24687;&#37327;&#21270;&#20026;&#19968;&#20010;&#27604;&#29305;&#65292;&#21516;&#26102;&#20445;&#25345;&#24615;&#33021;&#12290;&#36825;&#19968;&#21457;&#29616;&#23545;&#20110;&#22810;&#31199;&#25143;&#27169;&#22411;&#30340;&#26381;&#21153;&#21644;&#23384;&#20648;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#24182;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;GPU&#20869;&#23384;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#22312;&#20004;&#20010;&#38454;&#27573;&#36827;&#34892;&#35757;&#32451;&#65306;&#22312;&#22823;&#35268;&#27169;&#20114;&#32852;&#32593;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#30001;&#20110;&#39044;&#35757;&#32451;&#30340;&#39640;&#35745;&#31639;&#38656;&#27714;&#65292;&#30452;&#35273;&#19978;&#35748;&#20026;&#24494;&#35843;&#23545;&#27169;&#22411;&#30340;&#20449;&#24687;&#28155;&#21152;&#36739;&#23569;&#65292;&#22240;&#27492;&#26356;&#20855;&#26377;&#21487;&#21387;&#32553;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#24494;&#35843;&#27169;&#22411;&#30340;&#26435;&#37325;&#20998;&#35299;&#20026;&#39044;&#35757;&#32451;&#32452;&#20214;&#21644;&#39069;&#22806;&#30340;&#22686;&#37327;&#26469;&#25506;&#31350;&#36825;&#19968;&#20551;&#35774;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#8212;&#8212;BitDelta&#65292;&#25104;&#21151;&#22320;&#23558;&#36825;&#20010;&#22686;&#37327;&#37327;&#21270;&#20026;1&#27604;&#29305;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;&#12290;&#36825;&#19968;&#26377;&#36259;&#30340;&#21457;&#29616;&#19981;&#20165;&#31361;&#26174;&#20102;&#24494;&#35843;&#36807;&#31243;&#20013;&#28155;&#21152;&#30340;&#20449;&#24687;&#30340;&#28508;&#22312;&#20887;&#20313;&#24615;&#65292;&#32780;&#19988;&#23545;&#20110;&#22810;&#31199;&#25143;&#27169;&#22411;&#30340;&#26381;&#21153;&#21644;&#23384;&#20648;&#20063;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#39640;&#31934;&#24230;&#30340;&#22522;&#30784;&#27169;&#22411;&#20197;&#21450;&#22810;&#20010;1&#27604;&#29305;&#30340;&#22686;&#37327;&#65292;BitDelta&#22823;&#22823;&#38477;&#20302;&#20102;GPU&#20869;&#23384;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10193v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are typically trained in two phases: pre-training on large internet-scale datasets, and fine-tuning for downstream tasks. Given the higher computational demand of pre-training, it's intuitive to assume that fine-tuning adds less new information to the model, and is thus more compressible. We explore this assumption by decomposing the weights of fine-tuned models into their pre-trained components and an additional delta. We introduce a simple method, BitDelta, which successfully quantizes this delta down to 1 bit without compromising performance. This interesting finding not only highlights the potential redundancy of information added during fine-tuning, but also has significant implications for the multi-tenant serving and multi-tenant storage of fine-tuned models. By enabling the use of a single high-precision base model accompanied by multiple 1-bit deltas, BitDelta dramatically reduces GPU memory requir
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#21644;&#20998;&#26512;&#25581;&#31034;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#22810;&#20010;&#23616;&#38480;&#24615;&#65292;&#21253;&#25324;&#26080;&#27861;&#22686;&#24378;LLM&#30340;&#30693;&#35782;&#21644;&#25216;&#33021;&#12289;&#20174;&#20855;&#26377;&#30693;&#35782;&#26469;&#28304;&#30340;&#25968;&#25454;&#38598;&#22797;&#21046;&#22238;&#24212;&#27169;&#24335;&#23548;&#33268;&#36136;&#37327;&#19979;&#38477;&#12289;&#20840;&#21442;&#25968;&#24494;&#35843;&#22686;&#21152;&#20102;&#38169;&#35823;&#29983;&#25104;&#30340;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2402.05119</link><description>&lt;p&gt;
&#30740;&#31350;&#25351;&#20196;&#35843;&#25972;&#30340;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Closer Look at the Limitations of Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#21644;&#20998;&#26512;&#25581;&#31034;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#22810;&#20010;&#23616;&#38480;&#24615;&#65292;&#21253;&#25324;&#26080;&#27861;&#22686;&#24378;LLM&#30340;&#30693;&#35782;&#21644;&#25216;&#33021;&#12289;&#20174;&#20855;&#26377;&#30693;&#35782;&#26469;&#28304;&#30340;&#25968;&#25454;&#38598;&#22797;&#21046;&#22238;&#24212;&#27169;&#24335;&#23548;&#33268;&#36136;&#37327;&#19979;&#38477;&#12289;&#20840;&#21442;&#25968;&#24494;&#35843;&#22686;&#21152;&#20102;&#38169;&#35823;&#29983;&#25104;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#65288;IT&#65289;&#26159;&#20351;&#29992;&#25351;&#20196;-&#22238;&#24212;&#23545;&#26469;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36807;&#31243;&#65292;&#24050;&#25104;&#20026;&#23558;&#22522;&#30784;&#39044;&#35757;&#32451;LLM&#36716;&#21270;&#20026;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#20195;&#29702;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#34429;&#28982;IT&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#24182;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#20854;&#23616;&#38480;&#24615;&#21644;&#19981;&#36275;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#26412;&#25991;&#36890;&#36807;&#20005;&#26684;&#30340;&#23454;&#39564;&#21644;&#23545;LLM&#36890;&#36807;IT&#21457;&#29983;&#30340;&#21464;&#21270;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;IT&#30340;&#22810;&#31181;&#23616;&#38480;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#65306;&#65288;1&#65289;IT&#26080;&#27861;&#22686;&#24378;LLM&#30340;&#30693;&#35782;&#25110;&#25216;&#33021;&#12290;LoRA&#24494;&#35843;&#20165;&#38480;&#20110;&#23398;&#20064;&#22238;&#24212;&#30340;&#21551;&#21160;&#21644;&#26679;&#24335;&#20196;&#29260;&#65292;&#32780;&#20840;&#21442;&#25968;&#24494;&#35843;&#20250;&#23548;&#33268;&#30693;&#35782;&#36864;&#21270;&#12290;&#65288;2&#65289;&#20174;&#20855;&#26377;&#30693;&#35782;&#26469;&#28304;&#30340;IT&#25968;&#25454;&#38598;&#22797;&#21046;&#22238;&#24212;&#27169;&#24335;&#20250;&#23548;&#33268;&#22238;&#24212;&#36136;&#37327;&#19979;&#38477;&#12290;&#65288;3&#65289;&#20840;&#21442;&#25968;&#24494;&#35843;&#36890;&#36807;&#19981;&#20934;&#30830;&#22320;&#20174;IT&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#27010;&#24565;&#19978;&#30456;&#20284;&#23454;&#20363;&#30340;&#26631;&#35760;&#65292;&#22686;&#21152;&#20102;&#38169;&#35823;&#29983;&#25104;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction Tuning (IT), the process of training large language models (LLMs) using instruction-response pairs, has emerged as the predominant method for transforming base pre-trained LLMs into open-domain conversational agents. While IT has achieved notable success and widespread adoption, its limitations and shortcomings remain underexplored. In this paper, through rigorous experiments and an in-depth analysis of the changes LLMs undergo through IT, we reveal various limitations of IT. In particular, we show that (1) IT fails to enhance knowledge or skills in LLMs. LoRA fine-tuning is limited to learning response initiation and style tokens, and full-parameter fine-tuning leads to knowledge degradation. (2) Copying response patterns from IT datasets derived from knowledgeable sources leads to a decline in response quality. (3) Full-parameter fine-tuning increases hallucination by inaccurately borrowing tokens from conceptually similar instances in the IT dataset for generating respon
&lt;/p&gt;</description></item><item><title>OLMo&#26159;&#19968;&#31181;&#30495;&#27491;&#24320;&#25918;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#20379;&#32473;&#30740;&#31350;&#31038;&#21306;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#20197;&#20419;&#36827;&#23545;&#35821;&#35328;&#27169;&#22411;&#31185;&#23398;&#30340;&#30740;&#31350;&#21644;&#21019;&#26032;&#12290;</title><link>https://arxiv.org/abs/2402.00838</link><description>&lt;p&gt;
OLMo: &#21152;&#36895;&#35821;&#35328;&#27169;&#22411;&#31185;&#23398;
&lt;/p&gt;
&lt;p&gt;
OLMo: Accelerating the Science of Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00838
&lt;/p&gt;
&lt;p&gt;
OLMo&#26159;&#19968;&#31181;&#30495;&#27491;&#24320;&#25918;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#20379;&#32473;&#30740;&#31350;&#31038;&#21306;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#20197;&#20419;&#36827;&#23545;&#35821;&#35328;&#27169;&#22411;&#31185;&#23398;&#30340;&#30740;&#31350;&#21644;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#21644;&#21830;&#19994;&#20135;&#21697;&#12290;&#38543;&#30528;&#21830;&#19994;&#37325;&#35201;&#24615;&#30340;&#22686;&#21152;&#65292;&#26368;&#24378;&#22823;&#30340;&#27169;&#22411;&#24050;&#32463;&#23553;&#38381;&#36215;&#26469;&#65292;&#21482;&#33021;&#36890;&#36807;&#19987;&#26377;&#25509;&#21475;&#35775;&#38382;&#65292;&#20854;&#35757;&#32451;&#25968;&#25454;&#12289;&#26550;&#26500;&#21644;&#24320;&#21457;&#32454;&#33410;&#27809;&#26377;&#36879;&#38706;&#12290;&#32771;&#34385;&#21040;&#36825;&#20123;&#32454;&#33410;&#23545;&#20110;&#31185;&#23398;&#30740;&#31350;&#36825;&#20123;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#65292;&#21253;&#25324;&#20854;&#20559;&#35265;&#21644;&#28508;&#22312;&#39118;&#38505;&#65292;&#25105;&#20204;&#35748;&#20026;&#30740;&#31350;&#31038;&#21306;&#26377;&#26435;&#35775;&#38382;&#24378;&#22823;&#32780;&#30495;&#27491;&#24320;&#25918;&#30340;LM&#12290;&#20026;&#27492;&#65292;&#26412;&#25216;&#26415;&#25253;&#21578;&#35814;&#32454;&#20171;&#32461;&#20102;OLMo&#30340;&#39318;&#20010;&#29256;&#26412;&#65292;&#36825;&#26159;&#19968;&#31181;&#26368;&#20808;&#36827;&#12289;&#30495;&#27491;&#24320;&#25918;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#21450;&#26500;&#24314;&#21644;&#30740;&#31350;&#35821;&#35328;&#24314;&#27169;&#31185;&#23398;&#30340;&#26694;&#26550;&#12290;&#19982;&#20043;&#21069;&#21482;&#21457;&#24067;&#27169;&#22411;&#26435;&#37325;&#21644;&#25512;&#29702;&#20195;&#30721;&#30340;&#21162;&#21147;&#19981;&#21516;&#65292;&#25105;&#20204;&#21457;&#24067;OLMo&#21644;&#25972;&#20010;&#26694;&#26550;&#65292;&#21253;&#25324;&#35757;&#32451;&#25968;&#25454;&#12289;&#35757;&#32451;&#21644;&#35780;&#20272;&#20195;&#30721;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#20010;&#21457;&#24067;&#33021;&#22686;&#24378;&#24320;&#25918;&#30740;&#31350;&#31038;&#21306;&#30340;&#33021;&#21147;&#65292;&#24182;&#28608;&#21457;&#26356;&#22810;&#30340;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, this technical report details the first release of OLMo, a state-of-the-art, truly Open Language Model and its framework to build and study the science of language modeling. Unlike most prior efforts that have only released model weights and inference code, we release OLMo and the whole framework, including training data and training and evaluation code. We hope this release will empower and strengthen the open research community and inspi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20154;&#31867;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#38405;&#35835;&#29702;&#35299;&#38382;&#39064;&#26469;&#35780;&#20272;&#31616;&#21270;&#30340;&#25991;&#26412;&#26159;&#21542;&#20445;&#30041;&#20102;&#21547;&#20041;&#12290;</title><link>https://arxiv.org/abs/2312.10126</link><description>&lt;p&gt;
&#25991;&#26412;&#31616;&#21270;&#31995;&#32479;&#20445;&#30041;&#24847;&#20041;&#21527;&#65311;&#36890;&#36807;&#38405;&#35835;&#29702;&#35299;&#30340;&#20154;&#31867;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Do Text Simplification Systems Preserve Meaning? A Human Evaluation via Reading Comprehension
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10126
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20154;&#31867;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#38405;&#35835;&#29702;&#35299;&#38382;&#39064;&#26469;&#35780;&#20272;&#31616;&#21270;&#30340;&#25991;&#26412;&#26159;&#21542;&#20445;&#30041;&#20102;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#25991;&#26412;&#31616;&#21270;&#65288;TS&#65289;&#26088;&#22312;&#33258;&#21160;&#21270;&#37325;&#26032;&#32534;&#20889;&#25991;&#26412;&#30340;&#36807;&#31243;&#65292;&#20351;&#20854;&#26356;&#26131;&#20110;&#20154;&#20204;&#38405;&#35835;&#12290; TS&#33021;&#21542;&#34987;&#20351;&#29992;&#30340;&#21069;&#25552;&#26159;&#65292;&#23427;&#24212;&#20256;&#36798;&#19982;&#21407;&#22987;&#25991;&#26412;&#24847;&#20041;&#19968;&#33268;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;TS&#35780;&#20272;&#21327;&#35758;&#35780;&#20272;&#31995;&#32479;&#36755;&#20986;&#30340;&#31616;&#26131;&#24615;&#21644;&#24847;&#20041;&#20445;&#30041;&#32780;&#19981;&#32771;&#34385;&#36755;&#20986;&#21477;&#23376;&#20986;&#29616;&#22312;&#25991;&#26723;&#19978;&#19979;&#25991;&#20013;&#20197;&#21450;&#20154;&#20204;&#22914;&#20309;&#29702;&#35299;&#23427;&#20204;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20154;&#31867;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#38405;&#35835;&#29702;&#35299;&#38382;&#39064;&#26469;&#35780;&#20272;&#31616;&#21270;&#30340;&#25991;&#26412;&#26159;&#21542;&#20445;&#30041;&#20102;&#21547;&#20041;&#12290;&#20351;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#36890;&#36807;&#20154;&#31867;&#21644;&#20061;&#20010;&#33258;&#21160;&#31995;&#32479;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#20154;&#31867;&#35780;&#20272;&#12290;&#21033;&#29992;&#39044;&#35757;&#32451;&#30693;&#35782;&#30340;&#30417;&#30563;&#31995;&#32479;&#22312;&#33258;&#21160;&#21487;&#25511;TS&#31995;&#32479;&#20013;&#30340;&#38405;&#35835;&#29702;&#35299;&#65288;RC&#65289;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#39640;&#20998;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#34920;&#29616;&#26368;&#20339;&#30340;&#30417;&#30563;&#31995;&#32479;&#20063;&#22312;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#20013;&#36935;&#21040;&#20102;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.10126v2 Announce Type: replace  Abstract: Automatic text simplification (TS) aims to automate the process of rewriting text to make it easier for people to read. A pre-requisite for TS to be useful is that it should convey information that is consistent with the meaning of the original text. However, current TS evaluation protocols assess system outputs for simplicity and meaning preservation without regard for the document context in which output sentences occur and for how people understand them. In this work, we introduce a human evaluation framework to assess whether simplified texts preserve meaning using reading comprehension questions. With this framework, we conduct a thorough human evaluation of texts by humans and by nine automatic systems. Supervised systems that leverage pre-training knowledge achieve the highest scores on the reading comprehension (RC) tasks amongst the automatic controllable TS systems. However, even the best-performing supervised system strugg
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Adversarial In-Context Learning (adv-ICL)&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#22120;&#12289;&#37492;&#21035;&#22120;&#21644;&#25552;&#31034;&#20462;&#25913;&#22120;&#20043;&#38388;&#30340;&#23545;&#25239;&#23398;&#20064;&#20248;&#21270;&#25552;&#31034;&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#21462;&#24471;&#26174;&#30528;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2312.02614</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#24615;&#19978;&#19979;&#25991;&#23398;&#20064;&#20248;&#21270;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Prompt Optimization via Adversarial In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02614
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Adversarial In-Context Learning (adv-ICL)&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#22120;&#12289;&#37492;&#21035;&#22120;&#21644;&#25552;&#31034;&#20462;&#25913;&#22120;&#20043;&#38388;&#30340;&#23545;&#25239;&#23398;&#20064;&#20248;&#21270;&#25552;&#31034;&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#21462;&#24471;&#26174;&#30528;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;Adversarial In-Context Learning&#65288;adv-ICL&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#19968;&#20010;LLM&#20316;&#20026;&#29983;&#25104;&#22120;&#65292;&#21478;&#19968;&#20010;&#20316;&#20026;&#37492;&#21035;&#22120;&#65292;&#31532;&#19977;&#20010;&#20316;&#20026;&#25552;&#31034;&#20462;&#25913;&#22120;&#65292;&#26469;&#20248;&#21270;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#25552;&#31034;&#12290;&#31867;&#20284;&#20110;&#20256;&#32479;&#30340;&#23545;&#25239;&#24615;&#23398;&#20064;&#65292;adv-ICL&#34987;&#23454;&#29616;&#20026;&#29983;&#25104;&#22120;&#21644;&#37492;&#21035;&#22120;&#20043;&#38388;&#30340;&#21452;&#20154;&#21338;&#24328;&#65292;&#20854;&#20013;&#29983;&#25104;&#22120;&#35797;&#22270;&#29983;&#25104;&#36275;&#22815;&#36924;&#30495;&#30340;&#36755;&#20986;&#20197;&#27450;&#39575;&#37492;&#21035;&#22120;&#12290; &#22312;&#27599;&#19968;&#36718;&#20013;&#65292;&#32473;&#23450;&#30001;&#20219;&#21153;&#35828;&#26126;&#21069;&#32512;&#21644;&#20960;&#20010;&#31034;&#20363;&#32452;&#25104;&#30340;&#36755;&#20837;&#65292;&#29983;&#25104;&#22120;&#20135;&#29983;&#19968;&#20010;&#36755;&#20986;&#12290;&#28982;&#21518;&#65292;&#37492;&#21035;&#22120;&#36127;&#36131;&#23558;&#29983;&#25104;&#22120;&#30340;&#36755;&#20837;-&#36755;&#20986;&#23545;&#20998;&#31867;&#20026;&#27169;&#22411;&#29983;&#25104;&#30340;&#36824;&#26159;&#30495;&#23454;&#25968;&#25454;&#12290;&#26681;&#25454;&#37492;&#21035;&#22120;&#25439;&#22833;&#65292;&#25552;&#31034;&#20462;&#25913;&#22120;&#25552;&#20986;&#20102;&#21487;&#33021;&#23545;&#29983;&#25104;&#22120;&#21644;&#37492;&#21035;&#22120;&#25552;&#31034;&#36827;&#34892;&#30340;&#32534;&#36753;&#65292;&#24182;&#36873;&#25321;&#26368;&#22823;&#31243;&#24230;&#25913;&#21892;&#23545;&#25239;&#25439;&#22833;&#30340;&#32534;&#36753;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;adv-ICL&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#25552;&#31034;&#20248;&#21270;&#26377;&#26174;&#30528;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02614v2 Announce Type: replace-cross  Abstract: We propose a new method, Adversarial In-Context Learning (adv-ICL), to optimize prompt for in-context learning (ICL) by employing one LLM as a generator, another as a discriminator, and a third as a prompt modifier. As in traditional adversarial learning, adv-ICL is implemented as a two-player game between the generator and discriminator, where the generator tries to generate realistic enough output to fool the discriminator. In each round, given an input prefixed by task instructions and several exemplars, the generator produces an output. The discriminator is then tasked with classifying the generator input-output pair as model-generated or real data. Based on the discriminator loss, the prompt modifier proposes possible edits to the generator and discriminator prompts, and the edits that most improve the adversarial loss are selected. We show that adv-ICL results in significant improvements over state-of-the-art prompt optim
&lt;/p&gt;</description></item><item><title>ChatGPT&#22312;&#29983;&#25104;&#39044;&#22823;&#23398;&#25968;&#23398;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20026;&#22635;&#34917;&#29616;&#26377;&#25945;&#32946;&#38382;&#39064;&#29983;&#25104;&#27169;&#22411;&#30340;&#19981;&#36275;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;</title><link>https://arxiv.org/abs/2312.01661</link><description>&lt;p&gt;
ChatGPT&#20316;&#20026;&#25968;&#23398;&#25552;&#38382;&#32773;&#65311;&#35780;&#20272;ChatGPT&#22312;&#29983;&#25104;&#39044;&#22823;&#23398;&#25968;&#23398;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
ChatGPT as a Math Questioner? Evaluating ChatGPT on Generating Pre-university Math Questions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.01661
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#22312;&#29983;&#25104;&#39044;&#22823;&#23398;&#25968;&#23398;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20026;&#22635;&#34917;&#29616;&#26377;&#25945;&#32946;&#38382;&#39064;&#29983;&#25104;&#27169;&#22411;&#30340;&#19981;&#36275;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#25552;&#38382;&#23545;&#20110;&#35780;&#20272;&#23398;&#29983;&#30340;&#35299;&#20915;&#38382;&#39064;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#20110;&#25163;&#21160;&#21019;&#24314;&#36825;&#26679;&#30340;&#38382;&#39064;&#38656;&#35201;&#22823;&#37327;&#24037;&#20316;&#65292;&#22240;&#27492;&#20154;&#20204;&#24050;&#32463;&#25506;&#32034;&#20102;&#33258;&#21160;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#20381;&#36182;&#20110;&#24494;&#35843;&#31574;&#30053;&#65292;&#24182;&#19988;&#22312;&#29983;&#25104;&#28041;&#21450;&#22810;&#27493;&#36923;&#36753;&#21644;&#31639;&#26415;&#25512;&#29702;&#30340;&#38382;&#39064;&#26102;&#24456;&#38590;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#35832;&#22914;ChatGPT&#20043;&#31867;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35768;&#22810;&#28041;&#21450;&#36923;&#36753;&#21644;&#31639;&#26415;&#25512;&#29702;&#30340;NLP&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#29983;&#25104;&#25945;&#32946;&#38382;&#39064;&#26041;&#38754;&#30340;&#24212;&#29992;&#23578;&#26410;&#20805;&#20998;&#21033;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#23398;&#39046;&#22495;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#36808;&#20986;&#20102;&#31532;&#19968;&#27493;&#65292;&#23545;ChatGPT&#22312;&#29983;&#25104;&#39044;&#22823;&#23398;&#25968;&#23398;&#38382;&#39064;&#26041;&#38754;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#20998;&#20026;&#20004;&#20010;&#20027;&#35201;&#35774;&#32622;&#65306;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#21644;&#19981;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.01661v2 Announce Type: replace-cross  Abstract: Mathematical questioning is crucial for assessing students problem-solving skills. Since manually creating such questions requires substantial effort, automatic methods have been explored. Existing state-of-the-art models rely on fine-tuning strategies and struggle to generate questions that heavily involve multiple steps of logical and arithmetic reasoning. Meanwhile, large language models(LLMs) such as ChatGPT have excelled in many NLP tasks involving logical and arithmetic reasoning. Nonetheless, their applications in generating educational questions are underutilized, especially in the field of mathematics. To bridge this gap, we take the first step to conduct an in-depth analysis of ChatGPT in generating pre-university math questions. Our analysis is categorized into two main settings: context-aware and context-unaware. In the context-aware setting, we evaluate ChatGPT on existing math question-answering benchmarks coverin
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#22522;&#30784;&#27861;&#24459;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#20294;&#36890;&#36807;&#38024;&#23545;&#24615;&#24494;&#35843;&#65292;&#29978;&#33267;&#36739;&#23567;&#30340;&#27169;&#22411;&#20063;&#33021;&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#25552;&#21319;&#20102;&#30456;&#20851;&#27861;&#24459;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2311.09693</link><description>&lt;p&gt;
BLT: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22788;&#29702;&#22522;&#30784;&#27861;&#24459;&#25991;&#26412;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
BLT: Can Large Language Models Handle Basic Legal Text?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09693
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#22522;&#30784;&#27861;&#24459;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#20294;&#36890;&#36807;&#38024;&#23545;&#24615;&#24494;&#35843;&#65292;&#29978;&#33267;&#36739;&#23567;&#30340;&#27169;&#22411;&#20063;&#33021;&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#25552;&#21319;&#20102;&#30456;&#20851;&#27861;&#24459;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21457;&#29616;&#20687;GPT-4&#12289;Claude&#21644;{PaLM 2}&#36825;&#26679;&#30340;&#26368;&#22909;&#30340;&#20844;&#24320;&#21487;&#29992;&#30340;LLM&#22312;&#22788;&#29702;&#22522;&#30784;&#27861;&#24459;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#21547;&#24459;&#24072;&#21644;&#27861;&#24459;&#21161;&#29702;&#26399;&#26395;LLM&#38646;-shot&#22788;&#29702;&#30340;&#20219;&#21153;&#65292;&#27604;&#22914;&#26597;&#25214;&#35777;&#35789;&#25991;&#20214;&#30340;&#26576;&#19968;&#34892;&#25110;&#21512;&#21516;&#30340;&#26576;&#20010;&#23376;&#37096;&#20998;&#30340;&#25991;&#26412;&#12290;LLM&#22312;&#36825;&#20010;&#22522;&#20934;&#19978;&#30340;&#24046;&#21170;&#34920;&#29616;&#23545;&#23427;&#20204;&#22312;&#27861;&#24459;&#23454;&#36341;&#20013;&#30340;&#21487;&#38752;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;&#36825;&#20123;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#29978;&#33267;&#20351;&#19968;&#20010;&#36739;&#23567;&#30340;&#27169;&#22411;&#22312;&#25105;&#20204;&#30340;&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#25509;&#36817;&#23436;&#32654;&#65292;&#24182;&#19988;&#36824;&#25552;&#21319;&#20102;&#30456;&#20851;&#27861;&#24459;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#35768;&#22810;&#39046;&#22495;&#25152;&#38656;&#30340;&#31616;&#21333;&#34892;&#20026;&#22312;&#22522;&#30784;LLM&#20013;&#21487;&#33021;&#19981;&#23384;&#22312;&#65292;&#38500;&#38750;&#26377;&#39046;&#22495;&#19987;&#23478;&#30340;&#39069;&#22806;&#21442;&#19982;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09693v2 Announce Type: replace-cross  Abstract: We find that the best publicly available LLMs like GPT-4, Claude, and {PaLM 2} currently perform poorly at basic legal text handling. We introduce a benchmark consisting of tasks that lawyers and paralegals would expect LLMs to handle zero-shot, such as looking up the text at a line of a witness deposition or at a subsection of a contract. LLMs' poor performance on this benchmark casts into doubt their reliability as-is for legal practice. However, fine-tuning for these tasks brings even a smaller model to near-perfect performance on our test set and also raises performance on a related legal task. These results suggest that many simple behaviors needed for a domain may not be present in foundational LLMs, without additional engagement from subject matter experts.
&lt;/p&gt;</description></item><item><title>Clean-Eval&#25552;&#20986;&#20102;&#19968;&#31181;&#28165;&#27905;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;LLM&#23545;&#27745;&#26579;&#25968;&#25454;&#36827;&#34892;&#37322;&#20041;&#21644;&#21453;&#21521;&#32763;&#35793;&#65292;&#21033;&#29992;&#35821;&#20041;&#26816;&#27979;&#22120;&#36807;&#28388;&#20302;&#36136;&#37327;&#26679;&#26412;&#65292;&#26368;&#32456;&#36873;&#25321;&#26368;&#20339;&#20505;&#36873;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.09154</link><description>&lt;p&gt;
CLEAN-EVAL&#65306;&#28165;&#27905;&#35780;&#20272;&#27745;&#26579;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CLEAN-EVAL: Clean Evaluation on Contaminated Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09154
&lt;/p&gt;
&lt;p&gt;
Clean-Eval&#25552;&#20986;&#20102;&#19968;&#31181;&#28165;&#27905;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;LLM&#23545;&#27745;&#26579;&#25968;&#25454;&#36827;&#34892;&#37322;&#20041;&#21644;&#21453;&#21521;&#32763;&#35793;&#65292;&#21033;&#29992;&#35821;&#20041;&#26816;&#27979;&#22120;&#36807;&#28388;&#20302;&#36136;&#37327;&#26679;&#26412;&#65292;&#26368;&#32456;&#36873;&#25321;&#26368;&#20339;&#20505;&#36873;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30446;&#21069;&#27491;&#22788;&#20110;&#21508;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#28608;&#28872;&#31454;&#20105;&#30340;&#26102;&#20195;&#65292;&#19981;&#26029;&#25512;&#21160;&#22522;&#20934;&#24615;&#33021;&#30340;&#36793;&#30028;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28508;&#22312;&#30340;&#25968;&#25454;&#27745;&#26579;&#65292;&#30495;&#27491;&#35780;&#20272;&#36825;&#20123;LLM&#30340;&#33021;&#21147;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#20851;&#38190;&#24615;&#30340;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#21644;&#24037;&#31243;&#24072;&#38656;&#35201;&#33457;&#36153;&#22823;&#37327;&#26102;&#38388;&#21644;&#31934;&#21147;&#19979;&#36733;&#21644;&#23581;&#35797;&#36825;&#20123;&#21463;&#27745;&#26579;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#33410;&#30465;&#23453;&#36149;&#30340;&#26102;&#38388;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#26377;&#29992;&#30340;&#26041;&#27861;&#65292;Clean-Eval&#65292;&#23427;&#21487;&#20197;&#20943;&#36731;&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;&#65292;&#24182;&#20197;&#26356;&#25972;&#27905;&#30340;&#26041;&#24335;&#35780;&#20272;LLM&#12290;Clean-Eval&#21033;&#29992;LLM&#23545;&#21463;&#27745;&#26579;&#25968;&#25454;&#36827;&#34892;&#37322;&#20041;&#21644;&#21453;&#21521;&#32763;&#35793;&#65292;&#29983;&#25104;&#20855;&#26377;&#30456;&#21516;&#21547;&#20041;&#20294;&#19981;&#21516;&#34920;&#38754;&#24418;&#24335;&#30340;&#34920;&#36798;&#12290;&#28982;&#21518;&#20351;&#29992;&#35821;&#20041;&#26816;&#27979;&#22120;&#36807;&#28388;&#29983;&#25104;&#30340;&#20302;&#36136;&#37327;&#26679;&#26412;&#65292;&#32553;&#23567;&#20505;&#36873;&#38598;&#12290;&#26368;&#32456;&#20174;&#36825;&#20010;&#20505;&#36873;&#38598;&#20013;&#22522;&#20110;BLEURT&#24471;&#20998;&#36873;&#25321;&#26368;&#20339;&#20505;&#36873;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09154v2 Announce Type: replace  Abstract: We are currently in an era of fierce competition among various large language models (LLMs) continuously pushing the boundaries of benchmark performance. However, genuinely assessing the capabilities of these LLMs has become a challenging and critical issue due to potential data contamination, and it wastes dozens of time and effort for researchers and engineers to download and try those contaminated models. To save our precious time, we propose a novel and useful method, Clean-Eval, which mitigates the issue of data contamination and evaluates the LLMs in a cleaner manner. Clean-Eval employs an LLM to paraphrase and back-translate the contaminated data into a candidate set, generating expressions with the same meaning but in different surface forms. A semantic detector is then used to filter the generated low-quality samples to narrow down this candidate set. The best candidate is finally selected from this set based on the BLEURT s
&lt;/p&gt;</description></item><item><title>ChOiRe&#26159;&#19968;&#20010;&#36890;&#36807;&#35266;&#28857;&#38142;&#25512;&#29702;&#34920;&#24449;&#21644;&#39044;&#27979;&#20154;&#31867;&#35266;&#28857;&#30340;&#26694;&#26550;&#65292;&#32467;&#21512;&#29992;&#25143;&#26126;&#30830;&#21644;&#38544;&#24335;&#30340;&#20010;&#20154;&#35282;&#33394;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#23545;&#20154;&#31867;&#35266;&#28857;&#30340;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2311.08385</link><description>&lt;p&gt;
ChOiRe&#65306;&#36890;&#36807;&#35266;&#28857;&#38142;&#25512;&#29702;&#34920;&#24449;&#21644;&#39044;&#27979;&#20154;&#31867;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
ChOiRe: Characterizing and Predicting Human Opinions with Chain of Opinion Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08385
&lt;/p&gt;
&lt;p&gt;
ChOiRe&#26159;&#19968;&#20010;&#36890;&#36807;&#35266;&#28857;&#38142;&#25512;&#29702;&#34920;&#24449;&#21644;&#39044;&#27979;&#20154;&#31867;&#35266;&#28857;&#30340;&#26694;&#26550;&#65292;&#32467;&#21512;&#29992;&#25143;&#26126;&#30830;&#21644;&#38544;&#24335;&#30340;&#20010;&#20154;&#35282;&#33394;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#23545;&#20154;&#31867;&#35266;&#28857;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#35266;&#28857;&#23545;&#40784;&#23545;&#20110;&#22686;&#24378;&#23427;&#20204;&#25226;&#25569;&#20154;&#31867;&#20215;&#20540;&#35266;&#12289;&#21916;&#22909;&#21644;&#20449;&#20208;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ChOiRe&#65292;&#19968;&#20010;&#22235;&#27493;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;&#20154;&#31867;&#35266;&#28857;&#65292;&#35813;&#26694;&#26550;&#19981;&#21516;&#22320;&#23545;&#24453;&#29992;&#25143;&#26126;&#30830;&#22768;&#26126;&#30340;&#20010;&#20154;&#35282;&#33394;&#65288;&#21363;&#20154;&#21475;&#32479;&#35745;&#25110;&#24847;&#35782;&#24418;&#24577;&#23646;&#24615;&#65289;&#21644;&#20174;&#29992;&#25143;&#21382;&#21490;&#35266;&#28857;&#25512;&#26029;&#20986;&#30340;&#38544;&#24335;&#20010;&#20154;&#35282;&#33394;&#12290;ChOiRe&#21253;&#25324;&#65306;&#65288;i&#65289;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#20998;&#26512;&#29992;&#25143;&#26126;&#30830;&#30340;&#20010;&#20154;&#35282;&#33394;&#65292;&#20197;&#36807;&#28388;&#20986;&#19981;&#30456;&#20851;&#30340;&#23646;&#24615;&#65307;&#65288;ii&#65289;&#35821;&#35328;&#27169;&#22411;&#23558;&#38544;&#24335;&#20154;&#29289;&#35266;&#28857;&#25490;&#21517;&#25104;&#20248;&#20808;&#21015;&#34920;&#65307;&#65288;iii&#65289;&#35266;&#28857;&#38142;&#25512;&#29702;&#65288;CoO&#65289;&#65292;&#20854;&#20013;&#35821;&#35328;&#27169;&#22411;&#39034;&#24207;&#22320;&#20998;&#26512;&#26126;&#30830;&#30340;&#20010;&#20154;&#35282;&#33394;&#21644;&#26368;&#30456;&#20851;&#30340;&#38544;&#24335;&#20010;&#20154;&#35282;&#33394;&#20197;&#25191;&#34892;&#35266;&#28857;&#39044;&#27979;&#65307;&#65288;iv&#65289;&#20197;&#21450;ChOiRe&#25191;&#34892;&#31532;&#65288;iii&#65289;&#27493;CoO&#22810;&#27425;&#65292;&#38543;&#30528;&#38544;&#24335;&#20010;&#20154;&#35282;&#33394;&#21015;&#34920;&#19981;&#26029;&#22686;&#21152;&#26469;&#20811;&#26381;&#20010;&#20154;&#35282;&#33394;&#20449;&#24687;&#19981;&#36275;&#20197;&#25512;&#26029;&#26368;&#32456;&#32467;&#26524;&#12290;ChOiRe&#21462;&#24471;&#20102;&#26032;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08385v3 Announce Type: replace  Abstract: Aligning language models (LMs) with human opinion is challenging yet vital to enhance their grasp of human values, preferences, and beliefs. We present ChOiRe, a four-step framework to predict human opinion which differentially models the user explicit personae (i.e. demographic or ideological attributes) that are manually declared, and implicit personae inferred from user historical opinions. ChOiRe consists of (i) an LM analyzing the user explicit personae to filter out irrelevant attributes; (ii) the LM ranking the implicit persona opinions into a preferential list; (iii) Chain-of-Opinion (CoO) reasoning, where the LM sequentially analyzes the explicit personae and the most relevant implicit personae to perform opinion prediction; (iv) and where ChOiRe executes Step (iii) CoO multiple times with increasingly larger lists of implicit personae to overcome insufficient personae information to infer a final result. ChOiRe achieves new
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LINK&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#31995;&#32479;&#24615;&#22320;&#29983;&#25104;&#38271;&#23614;&#25512;&#29702;&#30693;&#35782;&#65292;&#20174;&#32780;&#26356;&#26377;&#25928;&#22320;&#35780;&#20272;LLMs&#22312;&#25512;&#29702;&#31354;&#38388;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2311.07237</link><description>&lt;p&gt;
&#22312;&#25628;&#32034;&#38271;&#23614;&#20013;&#65306;&#36890;&#36807;&#36923;&#36753;&#35268;&#21017;&#24341;&#23548;&#25628;&#32034;&#31995;&#32479;&#24615;&#29983;&#25104;&#38271;&#23614;&#25512;&#29702;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
In Search of the Long-Tail: Systematic Generation of Long-Tail Inferential Knowledge via Logical Rule Guided Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07237
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LINK&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#31995;&#32479;&#24615;&#22320;&#29983;&#25104;&#38271;&#23614;&#25512;&#29702;&#30693;&#35782;&#65292;&#20174;&#32780;&#26356;&#26377;&#25928;&#22320;&#35780;&#20272;LLMs&#22312;&#25512;&#29702;&#31354;&#38388;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;LLMs&#22312;&#35832;&#22914;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#31561;&#25512;&#29702;&#20219;&#21153;&#19978;&#32988;&#36807;&#20154;&#31867;&#12290;&#26368;&#36817;&#35780;&#20272;LLMs&#30340;&#30740;&#31350;&#25351;&#20986;&#65292;&#22312;&#26469;&#33258;&#20302;&#27010;&#29575;&#20998;&#24067;&#8212;&#8212;&#21363;&#38271;&#23614;&#30340;&#36755;&#20837;&#25968;&#25454;&#19978;&#34920;&#29616;&#22823;&#24133;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#31995;&#32479;&#29983;&#25104;&#28041;&#21450;&#38271;&#23614;&#25512;&#29702;&#30693;&#35782;&#30340;&#35821;&#21477;&#65292;&#20197;&#26356;&#26377;&#25928;&#22320;&#35780;&#20272;LLMs&#22312;&#25512;&#29702;&#31354;&#38388;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;Logic-Induced-Knowledge-Search&#65288;LINK&#65289;&#65292;&#35813;&#26694;&#26550;&#29983;&#25104;&#22522;&#20110;&#31526;&#21495;&#35268;&#21017;&#27169;&#26495;&#30340;&#20107;&#23454;&#27491;&#30830;&#19988;&#38271;&#23614;&#30693;&#35782;&#35821;&#21477;&#65307;LINK&#26377;&#25928;&#22320;&#29983;&#25104;&#38271;&#23614;&#20998;&#24067;&#25968;&#25454;&#65292;&#38646;-shot&#25552;&#31034;&#30340;LLMs&#26080;&#27861;&#21040;&#36798;&#65292;&#24182;&#19988;&#22312;&#20107;&#23454;&#27491;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#38646;-shot GPT4&#36798;&#21040;5%&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20351;&#29992;LINK&#29983;&#25104;&#30340;&#25968;&#25454;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;Logic-Induced-Long-Tail&#65288;LINT&#65289;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#38271;&#23614;&#20998;&#24067;&#19978;&#30340;&#19979;&#28216;&#27169;&#22411;&#65307;LINT&#21253;&#21547;108K&#20010;&#30693;&#35782;&#26465;&#30446;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07237v2 Announce Type: replace-cross  Abstract: State-of-the-art LLMs outperform humans on reasoning tasks such as Natural Language Inference. Recent works evaluating LLMs note a marked performance drop on input data from the low-probability distribution, i.e., the longtail. Therefore, we focus on systematically generating statements involving long-tail inferential knowledge for more effective evaluation of LLMs in the reasoning space. We first propose a novel framework Logic-Induced- Knowledge-Search (LINK) that generates factually correct and long-tail knowledge statements grounded on symbolic rule templates; LINK effectively generates data in the longtail distribution that zero-shot prompted LLMs are unable to reach, and outperforms zero-shot GPT4 on factual correctness by 5%. We further use the data generated by LINK to construct a dataset Logic-Induced-Long-Tail (LINT) that can be used to evaluate downstream models on the long-tail distribution; LINT contains 108K knowl
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#22810;&#35821;&#35328;&#36234;&#29425;&#25361;&#25112;&#65292;&#21253;&#25324;&#29992;&#25143;&#20351;&#29992;&#38750;&#33521;&#35821;&#25552;&#31034;&#32469;&#36807;&#23433;&#20840;&#26426;&#21046;&#30340;&#38750;&#25925;&#24847;&#22330;&#26223;&#21644;&#24694;&#24847;&#29992;&#25143;&#21033;&#29992;&#22810;&#35821;&#35328;&#25552;&#31034;&#24694;&#24847;&#25915;&#20987;LLMs&#30340;&#25925;&#24847;&#22330;&#26223;&#12290;</title><link>https://arxiv.org/abs/2310.06474</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22810;&#35821;&#35328;&#36234;&#29425;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Multilingual Jailbreak Challenges in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.06474
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#22810;&#35821;&#35328;&#36234;&#29425;&#25361;&#25112;&#65292;&#21253;&#25324;&#29992;&#25143;&#20351;&#29992;&#38750;&#33521;&#35821;&#25552;&#31034;&#32469;&#36807;&#23433;&#20840;&#26426;&#21046;&#30340;&#38750;&#25925;&#24847;&#22330;&#26223;&#21644;&#24694;&#24847;&#29992;&#25143;&#21033;&#29992;&#22810;&#35821;&#35328;&#25552;&#31034;&#24694;&#24847;&#25915;&#20987;LLMs&#30340;&#25925;&#24847;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#28508;&#22312;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#20363;&#22914;&#8220;&#36234;&#29425;&#8221;&#38382;&#39064;&#65292;&#21363;&#24694;&#24847;&#25351;&#20196;&#21487;&#33021;&#25805;&#32437;LLMs&#34920;&#29616;&#20986;&#19981;&#33391;&#34892;&#20026;&#12290;&#23613;&#31649;&#24050;&#32463;&#21046;&#23450;&#20102;&#20960;&#31181;&#39044;&#38450;&#25514;&#26045;&#26469;&#20943;&#36731;&#19982;LLMs&#30456;&#20851;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#20294;&#36825;&#20123;&#25514;&#26045;&#20027;&#35201;&#38598;&#20013;&#22312;&#33521;&#35821;&#19978;&#12290;&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;LLMs&#20013;&#23384;&#22312;&#30340;&#22810;&#35821;&#35328;&#36234;&#29425;&#25361;&#25112;&#65292;&#24182;&#32771;&#34385;&#20102;&#20004;&#31181;&#28508;&#22312;&#30340;&#39118;&#38505;&#22330;&#26223;&#65306;&#38750;&#25925;&#24847;&#21644;&#25925;&#24847;&#12290;&#38750;&#25925;&#24847;&#22330;&#26223;&#28041;&#21450;&#29992;&#25143;&#20351;&#29992;&#38750;&#33521;&#35821;&#25552;&#31034;&#26597;&#35810;LLMs&#24182;&#26080;&#24847;&#20013;&#32469;&#36807;&#23433;&#20840;&#26426;&#21046;&#65292;&#32780;&#25925;&#24847;&#22330;&#26223;&#28041;&#21450;&#24694;&#24847;&#29992;&#25143;&#23558;&#24694;&#24847;&#25351;&#20196;&#19982;&#22810;&#35821;&#35328;&#25552;&#31034;&#32467;&#21512;&#36215;&#26469;&#65292;&#25925;&#24847;&#25915;&#20987;LLMs&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#38750;&#25925;&#24847;&#22330;&#26223;&#20013;&#65292;&#19981;&#23433;&#20840;&#20869;&#23481;&#30340;&#27604;&#29575;&#22686;&#21152;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.06474v2 Announce Type: replace  Abstract: While large language models (LLMs) exhibit remarkable capabilities across a wide range of tasks, they pose potential safety concerns, such as the ``jailbreak'' problem, wherein malicious instructions can manipulate LLMs to exhibit undesirable behavior. Although several preventive measures have been developed to mitigate the potential risks associated with LLMs, they have primarily focused on English. In this study, we reveal the presence of multilingual jailbreak challenges within LLMs and consider two potential risky scenarios: unintentional and intentional. The unintentional scenario involves users querying LLMs using non-English prompts and inadvertently bypassing the safety mechanisms, while the intentional scenario concerns malicious users combining malicious instructions with multilingual prompts to deliberately attack LLMs. The experimental results reveal that in the unintentional scenario, the rate of unsafe content increases
&lt;/p&gt;</description></item><item><title>&#30446;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#21518;&#32493;&#38382;&#39064;&#26102;&#24120;&#24120;&#25671;&#25670;&#19981;&#23450;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#21518;&#32493;&#38382;&#39064;&#26426;&#21046;&#21644;&#20004;&#20010;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#24320;&#21457;&#20986;Unwavering-FQ&#26694;&#26550;&#26469;&#25945;&#23548;&#27169;&#22411;&#20445;&#25345;&#26368;&#21021;&#30340;&#27491;&#30830;&#21028;&#26029;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2310.02174</link><description>&lt;p&gt;
&#35753;&#24490;&#29615;&#30340;&#35810;&#38382;: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21028;&#26029;&#20013;&#30340;&#25671;&#25670;
&lt;/p&gt;
&lt;p&gt;
Ask Again, Then Fail: Large Language Models' Vacillations in Judgement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.02174
&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#21518;&#32493;&#38382;&#39064;&#26102;&#24120;&#24120;&#25671;&#25670;&#19981;&#23450;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#21518;&#32493;&#38382;&#39064;&#26426;&#21046;&#21644;&#20004;&#20010;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#24320;&#21457;&#20986;Unwavering-FQ&#26694;&#26550;&#26469;&#25945;&#23548;&#27169;&#22411;&#20445;&#25345;&#26368;&#21021;&#30340;&#27491;&#30830;&#21028;&#26029;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35266;&#23519;&#21040;&#30446;&#21069;&#30340;&#20250;&#35805;&#24335;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#21518;&#32493;&#38382;&#39064;&#26102;&#24448;&#24448;&#22312;&#20854;&#21028;&#26029;&#19978;&#25671;&#25670;&#19981;&#23450;&#65292;&#21363;&#20351;&#21407;&#22987;&#21028;&#26029;&#26159;&#27491;&#30830;&#30340;&#12290;&#36825;&#31181;&#25671;&#25670;&#23545;&#20110;&#29983;&#25104;&#21487;&#38752;&#22238;&#22797;&#21644;&#24314;&#31435;&#29992;&#25143;&#20449;&#20219;&#26500;&#25104;&#20102;&#37325;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#20840;&#38754;&#35780;&#20272;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21518;&#32493;&#38382;&#39064;&#26426;&#21046;&#20197;&#21450;&#20004;&#20010;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#30830;&#35748;&#20102;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#26222;&#36941;&#23384;&#22312;&#36825;&#31181;&#24773;&#20917;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#21508;&#31181;&#25552;&#31034;&#31574;&#30053;&#29992;&#20110;&#38381;&#28304;&#27169;&#22411;&#65307;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#35757;&#32451;&#30340;&#26694;&#26550;Unwavering-FQ&#65292;&#36890;&#36807;&#21512;&#25104;&#39640;&#36136;&#37327;&#30340;&#20559;&#22909;&#25968;&#25454;&#26469;&#25945;&#23548;&#35821;&#35328;&#27169;&#22411;&#20445;&#25345;&#20854;&#26368;&#21021;&#30340;&#27491;&#30830;&#21028;&#26029;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#20854;&#22686;&#24378;&#27169;&#22411;&#36890;&#29992;&#33021;&#21147;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.02174v2 Announce Type: replace-cross  Abstract: We observe that current conversational language models often waver in their judgements when faced with follow-up questions, even if the original judgement was correct. This wavering presents a significant challenge for generating reliable responses and building user trust. To comprehensively assess this issue, we introduce a Follow-up Questioning Mechanism along with two metrics to quantify this inconsistency, confirming its widespread presence in current language models. To mitigate this issue, we explore various prompting strategies for closed-source models; moreover, we develop a training-based framework Unwavering-FQ that teaches language models to maintain their originally correct judgements through synthesized high-quality preference data. Our experimental results confirm the effectiveness of our framework and its ability to enhance the general capabilities of models (https://github.com/NUSTM/LLMs-Waver-In-Judgements).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#21040;&#26356;&#23567;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21453;&#21521;KLD&#26367;&#25442;&#26631;&#20934;KD&#26041;&#27861;&#20013;&#30340;&#21069;&#21521;KLD&#30446;&#26631;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#23398;&#29983;&#27169;&#22411;&#39640;&#20272;&#25945;&#24072;&#20998;&#24067;&#30340;&#20302;&#27010;&#29575;&#21306;&#22495;&#12290;</title><link>https://arxiv.org/abs/2306.08543</link><description>&lt;p&gt;
MiniLLM&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
MiniLLM: Knowledge Distillation of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.08543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#21040;&#26356;&#23567;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21453;&#21521;KLD&#26367;&#25442;&#26631;&#20934;KD&#26041;&#27861;&#20013;&#30340;&#21069;&#21521;KLD&#30446;&#26631;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#23398;&#29983;&#27169;&#22411;&#39640;&#20272;&#25945;&#24072;&#20998;&#24067;&#30340;&#20302;&#27010;&#29575;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#26159;&#19968;&#31181;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39640;&#35745;&#31639;&#38656;&#27714;&#30340;&#26377;&#21069;&#36884;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;KD&#26041;&#27861;&#20027;&#35201;&#24212;&#29992;&#20110;&#30333;&#30418;&#20998;&#31867;&#27169;&#22411;&#25110;&#35757;&#32451;&#23567;&#27169;&#22411;&#26469;&#27169;&#20223;&#22914;ChatGPT&#20043;&#31867;&#30340;&#40657;&#30418;&#27169;&#22411;API&#12290;&#22914;&#20309;&#26377;&#25928;&#22320;&#23558;&#30333;&#30418;LLMs&#30340;&#30693;&#35782;&#33976;&#39311;&#21040;&#23567;&#27169;&#22411;&#20013;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#65292;&#38543;&#30528;&#24320;&#28304;LLMs&#30340;&#34028;&#21187;&#21457;&#23637;&#65292;&#36825;&#21464;&#24471;&#26356;&#20026;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;KD&#26041;&#27861;&#65292;&#23558;LLMs&#33976;&#39311;&#21040;&#26356;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.08543v2 Announce Type: replace-cross  Abstract: Knowledge Distillation (KD) is a promising technique for reducing the high computational demand of large language models (LLMs). However, previous KD methods are primarily applied to white-box classification models or training small models to imitate black-box model APIs like ChatGPT. How to effectively distill the knowledge of white-box LLMs into small models is still under-explored, which becomes more important with the prosperity of open-source LLMs. In this work, we propose a KD approach that distills LLMs into smaller language models. We first replace the forward Kullback-Leibler divergence (KLD) objective in the standard KD approaches with reverse KLD, which is more suitable for KD on generative language models, to prevent the student model from overestimating the low-probability regions of the teacher distribution. Then, we derive an effective optimization approach to learn this objective. The student models are named Mi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#25442;&#35821;&#35328;&#27169;&#22411;&#65288;RLM&#65289;&#65292;&#32467;&#21512;&#20102;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#21644;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#22312;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;&#20013;&#23454;&#29616;&#20102;&#26356;&#31934;&#30830;&#30340;&#29983;&#25104;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2211.07343</link><description>&lt;p&gt;
&#26367;&#25442;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Replacing Language Model for Style Transfer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.07343
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#25442;&#35821;&#35328;&#27169;&#22411;&#65288;RLM&#65289;&#65292;&#32467;&#21512;&#20102;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#21644;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#22312;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;&#20013;&#23454;&#29616;&#20102;&#26356;&#31934;&#30830;&#30340;&#29983;&#25104;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#26367;&#25442;&#35821;&#35328;&#27169;&#22411;&#65288;RLM&#65289;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#35821;&#35328;&#24314;&#27169;&#26694;&#26550;&#65292;&#29992;&#20110;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;&#65288;TST&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33258;&#22238;&#24402;&#22320;&#23558;&#28304;&#21477;&#23376;&#30340;&#27599;&#20010;&#26631;&#35760;&#26367;&#25442;&#20026;&#20855;&#26377;&#31867;&#20284;&#21547;&#20041;&#20294;&#20855;&#26377;&#30446;&#26631;&#39118;&#26684;&#30340;&#25991;&#26412;&#29255;&#27573;&#12290;&#26032;&#30340;&#29255;&#27573;&#26159;&#36890;&#36807;&#38750;&#33258;&#22238;&#24402;&#25513;&#34109;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#20445;&#30041;&#26367;&#25442;&#26631;&#35760;&#30340;&#23616;&#37096;&#19978;&#19979;&#25991;&#21547;&#20041;&#12290;&#36825;&#31181;RLM&#29983;&#25104;&#26041;&#26696;&#27719;&#38598;&#20102;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#21644;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#24357;&#21512;&#20102;&#21477;&#23376;&#32423;&#21644;&#35789;&#32423;&#39118;&#26684;&#36716;&#25442;&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#20026;&#20102;&#26356;&#31934;&#30830;&#22320;&#25511;&#21046;&#29983;&#25104;&#39118;&#26684;&#65292;&#25105;&#20204;&#22312;RLM&#30340;&#38544;&#34255;&#34920;&#31034;&#19978;&#36827;&#34892;&#20102;&#26631;&#35760;&#32423;&#39118;&#26684;&#20869;&#23481;&#35299;&#32544;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20854;&#20182;TST&#22522;&#32447;&#30456;&#27604;&#65292;RLM&#22312;&#30495;&#23454;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#20195;&#30721;&#22312;https://github.com/Linear95/RLM&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.07343v2 Announce Type: replace  Abstract: We introduce replacing language model (RLM), a sequence-to-sequence language modeling framework for text style transfer (TST). Our method autoregressively replaces each token of the source sentence with a text span that has a similar meaning but in the target style. The new span is generated via a non-autoregressive masked language model, which can better preserve the local-contextual meaning of the replaced token. This RLM generation scheme gathers the flexibility of autoregressive models and the accuracy of non-autoregressive models, which bridges the gap between sentence-level and word-level style transfer methods. To control the generation style more precisely, we conduct a token-level style-content disentanglement on the hidden representations of RLM. Empirical results on real-world text datasets demonstrate the effectiveness of RLM compared with other TST baselines. The code is at https://github.com/Linear95/RLM.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;REALTIME QA&#65292;&#19968;&#20010;&#21160;&#24577;&#38382;&#31572;&#24179;&#21488;&#65292;&#25361;&#25112;&#38745;&#24577;&#24320;&#25918;&#39046;&#22495;QA&#25968;&#25454;&#38598;&#20551;&#35774;&#65292;&#24378;&#35843;&#23545;&#23454;&#26102;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#22522;&#20110;GPT-3&#30340;&#23454;&#26102;&#35780;&#20272;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2207.13332</link><description>&lt;p&gt;
&#23454;&#26102;&#38382;&#31572;&#31995;&#32479;&#65306;&#29616;&#22312;&#30340;&#31572;&#26696;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
RealTime QA: What's the Answer Right Now?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2207.13332
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;REALTIME QA&#65292;&#19968;&#20010;&#21160;&#24577;&#38382;&#31572;&#24179;&#21488;&#65292;&#25361;&#25112;&#38745;&#24577;&#24320;&#25918;&#39046;&#22495;QA&#25968;&#25454;&#38598;&#20551;&#35774;&#65292;&#24378;&#35843;&#23545;&#23454;&#26102;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#22522;&#20110;GPT-3&#30340;&#23454;&#26102;&#35780;&#20272;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;REALTIME QA&#65292;&#36825;&#26159;&#19968;&#20010;&#21160;&#24577;&#38382;&#31572;&#24179;&#21488;&#65292;&#23450;&#26399;&#21457;&#24067;&#38382;&#39064;&#24182;&#35780;&#20272;&#31995;&#32479;&#65288;&#26412;&#29256;&#26412;&#27599;&#21608;&#19968;&#27425;&#65289;&#12290;REALTIME QA&#35810;&#38382;&#24403;&#21069;&#19990;&#30028;&#65292;QA&#31995;&#32479;&#38656;&#35201;&#22238;&#31572;&#20851;&#20110;&#26032;&#20107;&#20214;&#25110;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#23427;&#25361;&#25112;&#20102;&#24320;&#25918;&#39046;&#22495;QA&#25968;&#25454;&#38598;&#20013;&#30340;&#38745;&#24577;&#12289;&#20256;&#32479;&#20551;&#35774;&#65292;&#24182;&#36861;&#27714;&#21363;&#26102;&#24212;&#29992;&#12290;&#25105;&#20204;&#22522;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#21253;&#25324;GPT-3&#21644;T5&#65289;&#26500;&#24314;&#20102;&#24378;&#22823;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#24037;&#20316;&#26159;&#19968;&#20010;&#25345;&#32493;&#30340;&#21162;&#21147;&#65292;&#26412;&#25991;&#21576;&#29616;&#20102;&#36807;&#21435;&#19968;&#24180;&#30340;&#23454;&#26102;&#35780;&#20272;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-3&#36890;&#24120;&#21487;&#20197;&#26681;&#25454;&#26032;&#26816;&#32034;&#30340;&#25991;&#26723;&#27491;&#30830;&#22320;&#26356;&#26032;&#20854;&#29983;&#25104;&#32467;&#26524;&#65292;&#31361;&#26174;&#20102;&#26356;&#26032;&#33267;&#20851;&#37325;&#35201;&#30340;&#20449;&#24687;&#26816;&#32034;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#26816;&#32034;&#21040;&#30340;&#25991;&#26723;&#26410;&#25552;&#20379;&#36275;&#22815;&#20449;&#24687;&#20197;&#25214;&#21040;&#31572;&#26696;&#26102;&#65292;GPT-3&#24448;&#24448;&#20250;&#36820;&#22238;&#36807;&#26102;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2207.13332v2 Announce Type: replace  Abstract: We introduce REALTIME QA, a dynamic question answering (QA) platform that announces questions and evaluates systems on a regular basis (weekly in this version). REALTIME QA inquires about the current world, and QA systems need to answer questions about novel events or information. It therefore challenges static, conventional assumptions in open-domain QA datasets and pursues instantaneous applications. We build strong baseline models upon large pretrained language models, including GPT-3 and T5. Our benchmark is an ongoing effort, and this paper presents real-time evaluation results over the past year. Our experimental results show that GPT-3 can often properly update its generation results, based on newly-retrieved documents, highlighting the importance of up-to-date information retrieval. Nonetheless, we find that GPT-3 tends to return outdated answers when retrieved documents do not provide sufficient information to find an answer
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20462;&#25913;&#30340;&#32784;&#24515;&#22240;&#23376;&#26469;&#25913;&#21892;&#26463;&#25628;&#32034;&#35299;&#30721;&#31639;&#27861;&#65292;&#25552;&#39640;&#20102;&#24378;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#25991;&#26412;&#25688;&#35201;&#21644;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2204.05424</link><description>&lt;p&gt;
&#23545;&#26463;&#25628;&#32034;&#22312;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#24037;&#20316;&#21407;&#29702;&#21450;&#20572;&#27490;&#26102;&#26426;&#30340;&#26126;&#30830;&#35201;&#27714;
&lt;/p&gt;
&lt;p&gt;
A Call for Clarity in Beam Search: How It Works and When It Stops
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2204.05424
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20462;&#25913;&#30340;&#32784;&#24515;&#22240;&#23376;&#26469;&#25913;&#21892;&#26463;&#25628;&#32034;&#35299;&#30721;&#31639;&#27861;&#65292;&#25552;&#39640;&#20102;&#24378;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#25991;&#26412;&#25688;&#35201;&#21644;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26463;&#25628;&#32034;&#36827;&#34892;&#25991;&#26412;&#29983;&#25104;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#24050;&#34987;&#35777;&#26126;&#25104;&#21151;&#12290;&#25105;&#20204;&#25351;&#20986;&#65292;&#23613;&#31649;&#22312;&#25991;&#29486;&#20013;&#34987;&#22823;&#22810;&#25968;&#24573;&#35270;&#65292;&#20294;&#24120;&#29992;&#30340;&#26463;&#35299;&#30721;&#23454;&#29616;&#65288;&#20363;&#22914;Hugging Face Transformers&#21644;fairseq&#65289;&#20351;&#29992;&#20102;&#20808;&#21040;&#20808;&#26381;&#21153;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65306;&#23427;&#22312;&#26102;&#38388;&#27493;&#38271;&#19978;&#20445;&#30041;&#19968;&#32452;&#24050;&#23436;&#25104;&#30340;&#24207;&#21015;&#65292;&#24182;&#22312;&#27492;&#38598;&#21512;&#22823;&#23567;&#36798;&#21040;&#26463;&#22823;&#23567;&#26102;&#20572;&#27490;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#32784;&#24515;&#22240;&#23376;&#65292;&#23545;&#26463;&#35299;&#30721;&#23454;&#29616;&#36827;&#34892;&#20102;&#31616;&#21333;&#20462;&#25913;&#65292;&#20351;&#20572;&#27490;&#26465;&#20214;&#26356;&#19968;&#33324;&#21270;&#65292;&#24182;&#20026;&#25628;&#32034;&#28145;&#24230;&#25552;&#20379;&#20102;&#28789;&#27963;&#24615;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#35843;&#25972;&#36825;&#20010;&#32784;&#24515;&#22240;&#23376;&#25913;&#21892;&#20102;&#24378;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#26032;&#38395;&#25991;&#26412;&#25688;&#35201;&#21644;&#21508;&#31181;&#35821;&#35328;&#23545;&#30340;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#35299;&#30721;&#24615;&#33021;&#65292;&#32780;&#25512;&#29702;&#36895;&#24230;&#19979;&#38477;&#24494;&#19981;&#36275;&#36947;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#20462;&#25913;&#20102;&#19968;&#34892;&#20195;&#30721;&#65292;&#22240;&#27492;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#22312;&#20219;&#20309;i&#20013;&#34987;&#32435;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2204.05424v3 Announce Type: replace  Abstract: Text generation with beam search has proven successful in a wide range of applications. We point out that, though largely overlooked in the literature, the commonly-used implementation of beam decoding (e.g., Hugging Face Transformers and fairseq) uses a first come, first served heuristic: it keeps a set of already completed sequences over time steps and stops when the size of this set reaches the beam size. Based on this finding, we introduce a patience factor, a simple modification to this beam decoding implementation, that generalizes the stopping criterion and provides flexibility to the depth of search. Empirical results demonstrate that adjusting this patience factor improves decoding performance of strong pretrained models on news text summarization and machine translation over diverse language pairs, with a negligible inference slowdown. Our approach only modifies one line of code and can be thus readily incorporated in any i
&lt;/p&gt;</description></item><item><title>LegalDuet&#26159;&#19968;&#31181;&#36890;&#36807;&#21452;&#35270;&#35282;&#27861;&#24459;&#32447;&#32034;&#25512;&#29702;&#27169;&#22411;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#23450;&#21046;&#23884;&#20837;&#31354;&#38388;&#26469;&#36827;&#34892;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#27861;&#24459;&#26696;&#20363;&#25512;&#29702;&#21644;&#27861;&#24459;&#22522;&#30784;&#25512;&#29702;&#20004;&#20010;&#25512;&#29702;&#38142;&#36827;&#34892;&#21028;&#20915;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;LegalDuet&#22312;CAIL2018&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#36229;&#36807;&#20102;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.15371</link><description>&lt;p&gt;
LegalDuet: &#36890;&#36807;&#21452;&#35270;&#35282;&#27861;&#24459;&#32447;&#32034;&#25512;&#29702;&#23398;&#20064;&#26377;&#25928;&#30340;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
LegalDuet: Learning Effective Representations for Legal Judgment Prediction through a Dual-View Legal Clue Reasoning. (arXiv:2401.15371v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15371
&lt;/p&gt;
&lt;p&gt;
LegalDuet&#26159;&#19968;&#31181;&#36890;&#36807;&#21452;&#35270;&#35282;&#27861;&#24459;&#32447;&#32034;&#25512;&#29702;&#27169;&#22411;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#23450;&#21046;&#23884;&#20837;&#31354;&#38388;&#26469;&#36827;&#34892;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#27861;&#24459;&#26696;&#20363;&#25512;&#29702;&#21644;&#27861;&#24459;&#22522;&#30784;&#25512;&#29702;&#20004;&#20010;&#25512;&#29702;&#38142;&#36827;&#34892;&#21028;&#20915;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;LegalDuet&#22312;CAIL2018&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#36229;&#36807;&#20102;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#65288;LJP&#65289;&#27169;&#22411;&#20391;&#37325;&#20110;&#21457;&#29616;&#21009;&#20107;&#20107;&#23454;&#25551;&#36848;&#20013;&#30340;&#27861;&#24459;&#32447;&#32034;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#19987;&#19994;&#27861;&#23448;&#19981;&#20165;&#38656;&#35201;&#21560;&#25910;&#36807;&#21435;&#21028;&#20915;&#30340;&#27861;&#24459;&#26696;&#20363;&#32463;&#39564;&#65292;&#36824;&#20381;&#36182;&#20110;&#20174;&#19987;&#19994;&#27861;&#24459;&#30693;&#35782;&#20013;&#23398;&#21040;&#30340;&#19987;&#19994;&#27861;&#24459;&#22522;&#30784;&#25512;&#29702;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LegalDuet&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20197;&#23398;&#20064;&#29992;&#20110;&#36827;&#34892;&#27861;&#24459;&#21028;&#20915;&#30340;&#23450;&#21046;&#23884;&#20837;&#31354;&#38388;&#12290;&#23427;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#35270;&#35282;&#27861;&#24459;&#32447;&#32034;&#25512;&#29702;&#26426;&#21046;&#65292;&#30001;&#20004;&#20010;&#25512;&#29702;&#38142;&#32452;&#25104;&#65306;1&#65289;&#27861;&#24459;&#26696;&#20363;&#25512;&#29702;&#65292;&#26681;&#25454;&#20174;&#31867;&#27604;/&#28151;&#28102;&#30340;&#27861;&#24459;&#26696;&#20363;&#20013;&#23398;&#21040;&#30340;&#21028;&#20915;&#32463;&#39564;&#36827;&#34892;&#27861;&#24459;&#21028;&#20915;&#65307;2&#65289;&#27861;&#24459;&#22522;&#30784;&#25512;&#29702;&#65292;&#36890;&#36807;&#21305;&#37197;&#21009;&#20107;&#26696;&#20214;&#21644;&#27861;&#24459;&#20915;&#23450;&#20043;&#38388;&#30340;&#27861;&#24459;&#32447;&#32034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LegalDuet&#22312;CAIL2018&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#36229;&#36807;&#20102;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing Legal Judgment Prediction (LJP) models focus on discovering the legal triggers in the criminal fact description. However, in real-world scenarios, a professional judge not only needs to assimilate the law case experience that thrives on past sentenced legal judgments but also depends on the professional legal grounded reasoning that learned from professional legal knowledge. In this paper, we propose a LegalDuet model, which pretrains language models to learn a tailored embedding space for making legal judgments. It proposes a dual-view legal clue reasoning mechanism, which derives from two reasoning chains of judges: 1) Law Case Reasoning, which makes legal judgments according to the judgment experiences learned from analogy/confusing legal cases; 2) Legal Ground Reasoning, which lies in matching the legal clues between criminal cases and legal decisions. Our experiments show that LegalDuet achieves state-of-the-art performance on the CAIL2018 dataset and outperforms bas
&lt;/p&gt;</description></item><item><title>BIBench&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21830;&#19994;&#26234;&#33021;&#65288;BI&#65289;&#25968;&#25454;&#20998;&#26512;&#39046;&#22495;&#20013;&#33021;&#21147;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#36890;&#36807;&#27979;&#35797;&#27169;&#22411;&#22312;BI&#22522;&#30784;&#30693;&#35782;&#12289;&#24212;&#29992;&#30693;&#35782;&#21644;&#25216;&#26415;&#25216;&#33021;&#19977;&#20010;&#32500;&#24230;&#19978;&#30340;&#34920;&#29616;&#26469;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2401.02982</link><description>&lt;p&gt;
BIBench: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#20998;&#26512;&#30693;&#35782;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
BIBench: Benchmarking Data Analysis Knowledge of Large Language Models. (arXiv:2401.02982v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02982
&lt;/p&gt;
&lt;p&gt;
BIBench&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21830;&#19994;&#26234;&#33021;&#65288;BI&#65289;&#25968;&#25454;&#20998;&#26512;&#39046;&#22495;&#20013;&#33021;&#21147;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#36890;&#36807;&#27979;&#35797;&#27169;&#22411;&#22312;BI&#22522;&#30784;&#30693;&#35782;&#12289;&#24212;&#29992;&#30693;&#35782;&#21644;&#25216;&#26415;&#25216;&#33021;&#19977;&#20010;&#32500;&#24230;&#19978;&#30340;&#34920;&#29616;&#26469;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#25968;&#25454;&#20998;&#26512;&#30340;&#19987;&#19994;&#39046;&#22495;&#20013;&#30340;&#29087;&#32451;&#24230;&#21644;&#21487;&#38752;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#20197;&#25968;&#25454;&#39537;&#21160;&#24605;&#32500;&#20026;&#37325;&#28857;&#30340;&#39046;&#22495;&#20013;&#65292;&#20173;&#28982;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;BIBench&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#21830;&#19994;&#26234;&#33021;&#65288;BI&#65289;&#30340;&#32972;&#26223;&#19979;&#30340;&#25968;&#25454;&#20998;&#26512;&#33021;&#21147;&#12290;BIBench&#36890;&#36807;&#19977;&#20010;&#32500;&#24230;&#35780;&#20272;LLMs&#65306;1&#65289;BI&#22522;&#30784;&#30693;&#35782;&#65292;&#35780;&#20272;&#27169;&#22411;&#30340;&#25968;&#20540;&#25512;&#29702;&#33021;&#21147;&#21644;&#23545;&#37329;&#34701;&#27010;&#24565;&#30340;&#29087;&#24713;&#31243;&#24230;&#65307;2&#65289;BI&#30693;&#35782;&#24212;&#29992;&#65292;&#30830;&#23450;&#27169;&#22411;&#24555;&#36895;&#29702;&#35299;&#25991;&#26412;&#20449;&#24687;&#24182;&#20174;&#22810;&#20010;&#35270;&#35282;&#29983;&#25104;&#20998;&#26512;&#38382;&#39064;&#30340;&#33021;&#21147;&#65307;3&#65289;BI&#25216;&#26415;&#25216;&#33021;&#65292;&#26816;&#26597;&#27169;&#22411;&#20351;&#29992;&#25216;&#26415;&#30693;&#35782;&#35299;&#20915;&#29616;&#23454;&#25968;&#25454;&#20998;&#26512;&#25361;&#25112;&#30340;&#33021;&#21147;&#12290;BIBench&#21253;&#25324;11&#20010;&#23376;&#20219;&#21153;&#65292;&#28085;&#30422;&#20998;&#31867;&#12289;&#25552;&#21462;&#21644;&#29983;&#25104;&#19977;&#31181;&#20219;&#21153;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated impressive capabilities across a wide range of tasks. However, their proficiency and reliability in the specialized domain of Data Analysis, particularly with a focus on data-driven thinking, remain uncertain. To bridge this gap, we introduce BIBench, a comprehensive benchmark designed to evaluate the data analysis capabilities of LLMs within the context of Business Intelligence (BI). BIBench assesses LLMs across three dimensions: 1) BI foundational knowledge, evaluating the models' numerical reasoning and familiarity with financial concepts; 2) BI knowledge application, determining the models' ability to quickly comprehend textual information and generate analysis questions from multiple views; and 3) BI technical skills, examining the models' use of technical knowledge to address real-world data analysis challenges. BIBench comprises 11 sub-tasks, spanning three categories of task types: classification, extraction, and generation. Additi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MVRE&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20851;&#31995;&#35299;&#32806;&#20026;&#19981;&#21516;&#30340;&#35270;&#35282;&#65292;&#29983;&#25104;&#22810;&#35270;&#35282;&#20851;&#31995;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#33021;&#21147;&#26469;&#25552;&#39640;&#20302;&#36164;&#28304;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.17267</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#35270;&#35282;&#35299;&#32806;&#23398;&#20064;&#25913;&#36827;&#20302;&#36164;&#28304;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#20851;&#31995;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Improving Low-resource Prompt-based Relation Representation with Multi-view Decoupling Learning. (arXiv:2312.17267v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.17267
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MVRE&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20851;&#31995;&#35299;&#32806;&#20026;&#19981;&#21516;&#30340;&#35270;&#35282;&#65292;&#29983;&#25104;&#22810;&#35270;&#35282;&#20851;&#31995;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#33021;&#21147;&#26469;&#25552;&#39640;&#20302;&#36164;&#28304;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#36827;&#34892;&#25552;&#31034;&#35843;&#25972;&#24050;&#32463;&#23637;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#20851;&#31995;&#25277;&#21462;&#65288;RE&#65289;&#20219;&#21153;&#30340;&#22686;&#24378;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#20302;&#36164;&#28304;&#22330;&#26223;&#20013;&#65292;&#21363;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#23545;&#20851;&#31995;&#30340;&#34920;&#23618;&#29702;&#35299;&#65292;&#20808;&#21069;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#21487;&#33021;&#20173;&#28982;&#34920;&#29616;&#19981;&#20339;&#65292;&#29992;&#20110;&#34920;&#31034;&#23398;&#20064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24378;&#35843;&#22312;&#20302;&#36164;&#28304;&#22330;&#26223;&#20013;&#23398;&#20064;&#39640;&#36136;&#37327;&#20851;&#31995;&#34920;&#31034;&#23545;&#20110;RE&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#20851;&#31995;&#34920;&#31034;&#26041;&#27861;&#65292;&#21517;&#20026;MVRE&#65288;&#22810;&#35270;&#35282;&#20851;&#31995;&#25277;&#21462;&#65289;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;PLMs&#30340;&#33021;&#21147;&#26469;&#25913;&#21892;&#20302;&#36164;&#28304;&#25552;&#31034;&#35843;&#25972;&#33539;&#24335;&#19979;&#30340;RE&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MVRE&#23558;&#27599;&#20010;&#20851;&#31995;&#35299;&#32806;&#20026;&#19981;&#21516;&#30340;&#35270;&#35282;&#65292;&#20197;&#21253;&#21547;&#22810;&#35270;&#35282;&#30340;&#20851;&#31995;&#34920;&#31034;&#65292;&#20197;&#26368;&#22823;&#21270;&#20851;&#31995;&#25512;&#26029;&#36807;&#31243;&#20013;&#30340;&#20284;&#28982;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#20840;&#23616;&#24615;&#30340;&#20302;&#39046;&#22495;&#20219;&#21153;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#20851;&#31995;&#34920;&#31034;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, prompt-tuning with pre-trained language models (PLMs) has demonstrated the significantly enhancing ability of relation extraction (RE) tasks. However, in low-resource scenarios, where the available training data is scarce, previous prompt-based methods may still perform poorly for prompt-based representation learning due to a superficial understanding of the relation. To this end, we highlight the importance of learning high-quality relation representation in low-resource scenarios for RE, and propose a novel prompt-based relation representation method, named MVRE (\underline{M}ulti-\underline{V}iew \underline{R}elation \underline{E}xtraction), to better leverage the capacity of PLMs to improve the performance of RE within the low-resource prompt-tuning paradigm. Specifically, MVRE decouples each relation into different perspectives to encompass multi-view relation representations for maximizing the likelihood during relation inference. Furthermore, we also design a Global-Lo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#20195;&#30721;&#32534;&#36753;&#65292;&#24182;&#24341;&#20837;&#20102;InstructCoder&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#22810;&#26679;&#24615;&#30340;&#20195;&#30721;&#32534;&#36753;&#20219;&#21153;&#65292;&#20026;&#36890;&#29992;&#20195;&#30721;&#32534;&#36753;&#25552;&#20379;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2310.20329</link><description>&lt;p&gt;
InstructCoder: &#20026;&#20195;&#30721;&#32534;&#36753;&#36171;&#33021;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
InstructCoder: Empowering Language Models for Code Editing. (arXiv:2310.20329v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#20195;&#30721;&#32534;&#36753;&#65292;&#24182;&#24341;&#20837;&#20102;InstructCoder&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#22810;&#26679;&#24615;&#30340;&#20195;&#30721;&#32534;&#36753;&#20219;&#21153;&#65292;&#20026;&#36890;&#29992;&#20195;&#30721;&#32534;&#36753;&#25552;&#20379;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#32534;&#36753;&#28085;&#30422;&#20102;&#24320;&#21457;&#32773;&#26085;&#24120;&#22788;&#29702;&#30340;&#21508;&#31181;&#23454;&#29992;&#20219;&#21153;&#12290;&#23613;&#31649;&#20854;&#30456;&#20851;&#24615;&#21644;&#23454;&#29992;&#24615;&#65292;&#20294;&#33258;&#21160;&#20195;&#30721;&#32534;&#36753;&#20173;&#28982;&#26159;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#28436;&#21270;&#20013;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#30340;&#39046;&#22495;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#25968;&#25454;&#31232;&#32570;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26681;&#25454;&#29992;&#25143;&#25351;&#20196;&#32534;&#36753;&#20195;&#30721;&#30340;&#26041;&#27861;&#65292;&#28085;&#30422;&#20102;&#35832;&#22914;&#27880;&#37322;&#25554;&#20837;&#65292;&#20195;&#30721;&#20248;&#21270;&#21644;&#20195;&#30721;&#37325;&#26500;&#31561;&#19968;&#31995;&#21015;&#38544;&#21547;&#20219;&#21153;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;InstructCoder&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#20026;&#36890;&#29992;&#20195;&#30721;&#32534;&#36753;&#32780;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#39640;&#22810;&#26679;&#24615;&#30340;&#20195;&#30721;&#32534;&#36753;&#20219;&#21153;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#36229;&#36807;114,000&#20010;&#25351;&#20196;-&#36755;&#20837;-&#36755;&#20986;&#19977;&#20803;&#32452;&#65292;&#24182;&#28085;&#30422;&#20102;&#22810;&#20010;&#19981;&#21516;&#30340;&#20195;&#30721;&#32534;&#36753;&#22330;&#26223;&#12290;&#25968;&#25454;&#38598;&#36890;&#36807;&#19968;&#20010;&#36845;&#20195;&#36807;&#31243;&#36827;&#34892;&#31995;&#32479;&#25193;&#23637;&#65292;&#35813;&#36807;&#31243;&#20174;GitHub&#30340;&#25552;&#20132;&#20013;&#33719;&#21462;&#20195;&#30721;&#32534;&#36753;&#25968;&#25454;&#20316;&#20026;&#31181;&#23376;&#20219;&#21153;&#12290;&#31181;&#23376;&#20219;&#21153;&#21644;&#29983;&#25104;&#30340;&#20219;&#21153;&#38543;&#21518;&#29992;&#20110;&#25552;&#31034;ChatGPT&#33719;&#21462;&#26356;&#22810;&#20219;&#21153;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code editing encompasses a variety of pragmatic tasks that developers deal with daily. Despite its relevance and practical usefulness, automatic code editing remains an underexplored area in the evolution of deep learning models, partly due to data scarcity. In this work, we explore the use of large language models (LLMs) to edit code based on user instructions, covering a broad range of implicit tasks such as comment insertion, code optimization, and code refactoring. To facilitate this, we introduce InstructCoder, the first dataset designed to adapt LLMs for general-purpose code editing, containing highdiversity code-editing tasks. It consists of over 114,000 instruction-input-output triplets and covers multiple distinct code editing scenarios. The dataset is systematically expanded through an iterative process that commences with code editing data sourced from GitHub commits as seed tasks. Seed and generated tasks are used subsequently to prompt ChatGPT for more task data. Our exper
&lt;/p&gt;</description></item><item><title>MindShift&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#20102;&#22522;&#20110;&#24515;&#24577;&#30340;&#38382;&#39064;&#24615;&#26234;&#33021;&#25163;&#26426;&#20351;&#29992;&#24178;&#39044;&#65292;&#36890;&#36807;&#21160;&#24577;&#29983;&#25104;&#36866;&#24212;&#29992;&#25143;&#29615;&#22659;&#21644;&#24515;&#29702;&#29366;&#24577;&#30340;&#39640;&#36136;&#37327;&#35828;&#26381;&#20869;&#23481;&#26469;&#24110;&#21161;&#29992;&#25143;&#35299;&#20915;&#38382;&#39064;&#24615;&#26234;&#33021;&#25163;&#26426;&#20351;&#29992;&#30340;&#22256;&#25200;&#12290;</title><link>http://arxiv.org/abs/2309.16639</link><description>&lt;p&gt;
MindShift: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22522;&#20110;&#24515;&#24577;&#30340;&#38382;&#39064;&#24615;&#26234;&#33021;&#25163;&#26426;&#20351;&#29992;&#24178;&#39044;
&lt;/p&gt;
&lt;p&gt;
MindShift: Leveraging Large Language Models for Mental-States-Based Problematic Smartphone Use Intervention. (arXiv:2309.16639v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16639
&lt;/p&gt;
&lt;p&gt;
MindShift&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#20102;&#22522;&#20110;&#24515;&#24577;&#30340;&#38382;&#39064;&#24615;&#26234;&#33021;&#25163;&#26426;&#20351;&#29992;&#24178;&#39044;&#65292;&#36890;&#36807;&#21160;&#24577;&#29983;&#25104;&#36866;&#24212;&#29992;&#25143;&#29615;&#22659;&#21644;&#24515;&#29702;&#29366;&#24577;&#30340;&#39640;&#36136;&#37327;&#35828;&#26381;&#20869;&#23481;&#26469;&#24110;&#21161;&#29992;&#25143;&#35299;&#20915;&#38382;&#39064;&#24615;&#26234;&#33021;&#25163;&#26426;&#20351;&#29992;&#30340;&#22256;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#39064;&#24615;&#26234;&#33021;&#25163;&#26426;&#20351;&#29992;&#23545;&#36523;&#20307;&#21644;&#24515;&#29702;&#20581;&#24247;&#26377;&#36127;&#38754;&#24433;&#21709;&#12290;&#23613;&#31649;&#26377;&#22823;&#37327;&#30340;&#20808;&#21069;&#30740;&#31350;&#65292;&#29616;&#26377;&#30340;&#35828;&#26381;&#25216;&#24039;&#19981;&#36275;&#20197;&#26681;&#25454;&#29992;&#25143;&#30340;&#36523;&#20307;&#29615;&#22659;&#21644;&#24515;&#29702;&#29366;&#24577;&#25552;&#20379;&#21160;&#24577;&#35828;&#26381;&#20869;&#23481;&#12290;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#20102;&#19968;&#39033;&#20154;&#20026;&#25805;&#20316;&#30740;&#31350;&#65288;N = 12&#65289;&#21644;&#19968;&#39033;&#35775;&#35848;&#30740;&#31350;&#65288;N = 10&#65289;&#65292;&#24635;&#32467;&#20102;&#38382;&#39064;&#24615;&#26234;&#33021;&#25163;&#26426;&#20351;&#29992;&#32972;&#21518;&#30340;&#24515;&#24577;&#65306;&#26080;&#32842;&#12289;&#21387;&#21147;&#21644;&#24815;&#24615;&#12290;&#36825;&#20026;&#25105;&#20204;&#35774;&#35745;&#20102;&#22235;&#31181;&#35828;&#26381;&#31574;&#30053;&#65306;&#29702;&#35299;&#12289;&#23433;&#25242;&#12289;&#21796;&#36215;&#21644;&#25903;&#25345;&#20064;&#24815;&#12290;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23454;&#29616;&#20102;&#26377;&#25928;&#35828;&#26381;&#20869;&#23481;&#30340;&#33258;&#21160;&#21644;&#21160;&#24577;&#29983;&#25104;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;LLM&#25216;&#26415;&#39537;&#21160;&#30340;&#38382;&#39064;&#24615;&#26234;&#33021;&#25163;&#26426;&#20351;&#29992;&#24178;&#39044;&#25216;&#26415;MindShift&#12290;MindShift&#26681;&#25454;&#29992;&#25143;&#24403;&#19979;&#30340;&#36523;&#20307;&#29615;&#22659;&#12289;&#24515;&#24577;&#12289;&#24212;&#29992;&#20351;&#29992;&#34892;&#20026;&#12289;&#29992;&#25143;&#30340;&#30446;&#26631;&#19982;&#20064;&#24815;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#29983;&#25104;&#20855;&#26377;&#36866;&#24403;&#35828;&#26381;&#31574;&#30053;&#30340;&#39640;&#36136;&#37327;&#21644;&#28789;&#27963;&#30340;&#35828;&#26381;&#20869;&#23481;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;5-
&lt;/p&gt;
&lt;p&gt;
Problematic smartphone use negatively affects physical and mental health. Despite the wide range of prior research, existing persuasive techniques are not flexible enough to provide dynamic persuasion content based on users' physical contexts and mental states. We first conduct a Wizard-of-Oz study (N=12) and an interview study (N=10) to summarize the mental states behind problematic smartphone use: boredom, stress, and inertia. This informs our design of four persuasion strategies: understanding, comforting, evoking, and scaffolding habits. We leverage large language models (LLMs) to enable the automatic and dynamic generation of effective persuasion content. We develop MindShift, a novel LLM-powered problematic smartphone use intervention technique. MindShift takes users' in-the-moment physical contexts, mental states, app usage behaviors, users' goals &amp; habits as input, and generates high-quality and flexible persuasive content with appropriate persuasion strategies. We conduct a 5-
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#20849;&#32447;&#32422;&#26463;&#27880;&#24847;&#21147;&#65288;CoCA&#65289;&#32467;&#26500;&#65292;&#35299;&#20915;Transformer&#27169;&#22411;&#20013;&#30340;&#22836;&#30171;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20986;&#33394;&#30340;&#22806;&#25512;&#24615;&#33021;&#21644;&#25552;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.08646</link><description>&lt;p&gt;
&#36890;&#36807;&#20849;&#32447;&#32422;&#26463;&#27880;&#24847;&#21147;&#35299;&#20915;Transformer&#30340;&#22836;&#30171;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Cure the headache of Transformers via Collinear Constrained Attention. (arXiv:2309.08646v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08646
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#20849;&#32447;&#32422;&#26463;&#27880;&#24847;&#21147;&#65288;CoCA&#65289;&#32467;&#26500;&#65292;&#35299;&#20915;Transformer&#27169;&#22411;&#20013;&#30340;&#22836;&#30171;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20986;&#33394;&#30340;&#22806;&#25512;&#24615;&#33021;&#21644;&#25552;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#38469;&#24212;&#29992;&#30340;&#24555;&#36895;&#36827;&#23637;&#65292;&#25512;&#26029;&#24615;&#33021;&#30340;&#22806;&#25512;&#21464;&#24471;&#22312;&#30740;&#31350;&#39046;&#22495;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;Transformer&#27169;&#22411;&#20013;&#30340;&#19968;&#20010;&#34987;&#20043;&#21069;&#24573;&#35270;&#30340;&#24322;&#24120;&#34892;&#20026;&#65292;&#23548;&#33268;&#20102;&#26368;&#25509;&#36817;&#30340;&#26631;&#35760;&#20043;&#38388;&#30340;&#28151;&#20081;&#65292;&#36825;&#20123;&#26631;&#35760;&#25658;&#24102;&#20102;&#26368;&#37325;&#35201;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#23558;&#36825;&#19968;&#21457;&#29616;&#31216;&#20026;&#8220;Transformer&#30340;&#22836;&#30171;&#38382;&#39064;&#8221;&#12290;&#20026;&#20102;&#20174;&#26681;&#26412;&#19978;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#27880;&#24847;&#32467;&#26500;&#65292;&#21629;&#21517;&#20026;Collinear Constrained Attention&#65288;CoCA&#65289;&#12290;&#36825;&#20010;&#32467;&#26500;&#21487;&#20197;&#26080;&#32541;&#22320;&#19982;&#29616;&#26377;&#30340;&#25512;&#26029;&#12289;&#25554;&#20540;&#26041;&#27861;&#21644;&#20854;&#20182;&#38024;&#23545;&#20256;&#32479;Transformer&#27169;&#22411;&#35774;&#35745;&#30340;&#20248;&#21270;&#31574;&#30053;&#38598;&#25104;&#12290;&#25105;&#20204;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#23454;&#29616;&#20102;&#20248;&#31168;&#30340;&#22806;&#25512;&#24615;&#33021;&#65292;&#21363;&#20351;&#26159;16&#21040;24&#20493;&#30340;&#24207;&#21015;&#38271;&#24230;&#65292;&#32780;&#19988;&#27809;&#26377;&#23545;&#25105;&#20204;&#30340;&#27169;&#22411;&#36827;&#34892;&#20219;&#20309;&#24494;&#35843;&#12290;&#25105;&#20204;&#36824;&#22686;&#24378;&#20102;CoCA&#30340;&#35745;&#31639;&#21644;&#31354;&#38388;&#25928;&#29575;&#65292;&#20197;&#30830;&#20445;&#20854;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#35745;&#21010;...
&lt;/p&gt;
&lt;p&gt;
As the rapid progression of practical applications based on Large Language Models continues, the importance of extrapolating performance has grown exponentially in the research domain. In our study, we identified an anomalous behavior in Transformer models that had been previously overlooked, leading to a chaos around closest tokens which carried the most important information. We've coined this discovery the "headache of Transformers". To address this at its core, we introduced a novel self-attention structure named Collinear Constrained Attention (CoCA). This structure can be seamlessly integrated with existing extrapolation, interpolation methods, and other optimization strategies designed for traditional Transformer models. We have achieved excellent extrapolating performance even for 16 times to 24 times of sequence lengths during inference without any fine-tuning on our model. We have also enhanced CoCA's computational and spatial efficiency to ensure its practicality. We plan to
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#30740;&#31350;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#20013;&#30701;&#35821;&#23450;&#20301;&#21644;&#20219;&#21153;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#19988;&#36890;&#36807;&#39564;&#35777;&#23454;&#39564;&#21457;&#29616;&#20102;&#24403;&#20195;&#27169;&#22411;&#22312;&#30701;&#35821;&#23450;&#20301;&#21644;&#20219;&#21153;&#27714;&#35299;&#26041;&#38754;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.02691</link><description>&lt;p&gt;
&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#20013;&#30701;&#35821;&#23450;&#20301;&#21644;&#20219;&#21153;&#34920;&#29616;&#30340;&#32852;&#21512;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Joint Study of Phrase Grounding and Task Performance in Vision and Language Models. (arXiv:2309.02691v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02691
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#30740;&#31350;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#20013;&#30701;&#35821;&#23450;&#20301;&#21644;&#20219;&#21153;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#19988;&#36890;&#36807;&#39564;&#35777;&#23454;&#39564;&#21457;&#29616;&#20102;&#24403;&#20195;&#27169;&#22411;&#22312;&#30701;&#35821;&#23450;&#20301;&#21644;&#20219;&#21153;&#27714;&#35299;&#26041;&#38754;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38656;&#35201;&#23545;&#35270;&#35273;&#32972;&#26223;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#25512;&#29702;&#30340;&#20219;&#21153;&#20013;&#65292;&#20851;&#38190;&#26159;&#23558;&#21333;&#35789;&#21644;&#30701;&#35821;&#19982;&#22270;&#20687;&#21306;&#22495;&#32852;&#31995;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#36890;&#24120;&#39044;&#26399;&#20197;&#26377;&#21161;&#20110;&#27867;&#21270;&#30340;&#26041;&#24335;&#35299;&#20915;&#20219;&#21153;&#65292;&#35266;&#23519;&#21040;&#24403;&#20195;&#27169;&#22411;&#20013;&#30340;&#36825;&#31181;&#23450;&#20301;&#20063;&#26159;&#22797;&#26434;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#20849;&#21516;&#30740;&#31350;&#20219;&#21153;&#25191;&#34892;&#21644;&#30701;&#35821;&#23450;&#20301;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#20010;&#22522;&#20934;&#26469;&#30740;&#31350;&#20004;&#32773;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#20195;&#27169;&#22411;&#22312;&#23450;&#20301;&#30701;&#35821;&#21644;&#35299;&#20915;&#20219;&#21153;&#30340;&#33021;&#21147;&#20043;&#38388;&#23384;&#22312;&#19981;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#23545;&#23450;&#20301;&#26631;&#27880;&#36827;&#34892;&#24378;&#21046;&#24615;&#35757;&#32451;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#25152;&#21019;&#24314;&#30340;&#21160;&#24577;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/lil-lab/phrase_grounding&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Key to tasks that require reasoning about natural language in visual contexts is grounding words and phrases to image regions. However, observing this grounding in contemporary models is complex, even if it is generally expected to take place if the task is addressed in a way that is conductive to generalization. We propose a framework to jointly study task performance and phrase grounding, and propose three benchmarks to study the relation between the two. Our results show that contemporary models demonstrate inconsistency between their ability to ground phrases and solve tasks. We show how this can be addressed through brute-force training on ground phrasing annotations, and analyze the dynamics it creates. Code and at available at https://github.com/lil-lab/phrase_grounding.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20013;&#25991;&#25340;&#20889;&#32416;&#38169;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#20889;&#35821;&#35328;&#24314;&#27169;&#26469;&#37325;&#26032;&#34920;&#36798;&#25972;&#20010;&#21477;&#23376;&#65292;&#32780;&#19981;&#26159;&#20165;&#20165;&#20381;&#36182;&#38169;&#35823;&#27169;&#24335;&#36827;&#34892;&#23383;&#31526;&#32423;&#21035;&#26631;&#27880;&#65292;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.08796</link><description>&lt;p&gt;
&#12298;&#20013;&#25991;&#25340;&#20889;&#32416;&#38169;&#20316;&#20026;&#25913;&#20889;&#35821;&#35328;&#27169;&#22411;&#12299;
&lt;/p&gt;
&lt;p&gt;
Chinese Spelling Correction as Rephrasing Language Model. (arXiv:2308.08796v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20013;&#25991;&#25340;&#20889;&#32416;&#38169;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#20889;&#35821;&#35328;&#24314;&#27169;&#26469;&#37325;&#26032;&#34920;&#36798;&#25972;&#20010;&#21477;&#23376;&#65292;&#32780;&#19981;&#26159;&#20165;&#20165;&#20381;&#36182;&#38169;&#35823;&#27169;&#24335;&#36827;&#34892;&#23383;&#31526;&#32423;&#21035;&#26631;&#27880;&#65292;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20013;&#25991;&#25340;&#20889;&#32416;&#38169;&#65288;CSC&#65289;&#65292;&#26088;&#22312;&#26816;&#27979;&#21644;&#32416;&#27491;&#32473;&#23450;&#21477;&#23376;&#20013;&#30340;&#28508;&#22312;&#25340;&#20889;&#38169;&#35823;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#23558;CSC&#35270;&#20026;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#65292;&#24182;&#22312;&#21477;&#23376;&#23545;&#19978;&#24494;&#35843;&#22522;&#20110;BERT&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#22312;&#23558;&#19968;&#20010;&#23383;&#31526;&#26631;&#35760;&#20026;&#21478;&#19968;&#20010;&#23383;&#31526;&#30340;&#36807;&#31243;&#20013;&#23384;&#22312;&#19968;&#20010;&#20851;&#38190;&#32570;&#38519;&#65292;&#21363;&#32416;&#27491;&#36807;&#31243;&#36807;&#20110;&#20381;&#36182;&#38169;&#35823;&#12290;&#36825;&#19982;&#20154;&#31867;&#24605;&#32500;&#30456;&#21453;&#65292;&#20154;&#20204;&#26681;&#25454;&#21477;&#23376;&#30340;&#35821;&#20041;&#37325;&#26032;&#34920;&#36798;&#25972;&#20010;&#21477;&#23376;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#22522;&#20110;&#20043;&#21069;&#35760;&#24518;&#30340;&#38169;&#35823;&#27169;&#24335;&#12290;&#36825;&#31181;&#36829;&#21453;&#30452;&#35273;&#30340;&#23398;&#20064;&#36807;&#31243;&#23548;&#33268;&#26426;&#22120;&#25340;&#20889;&#32416;&#38169;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#21487;&#36801;&#31227;&#24615;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#25913;&#20889;&#35821;&#35328;&#24314;&#27169;&#8221;&#65288;ReLM&#65289;&#65292;&#20854;&#20013;&#27169;&#22411;&#36890;&#36807;&#22635;&#20805;&#39069;&#22806;&#30340;&#20301;&#32622;&#26469;&#37325;&#26032;&#34920;&#36798;&#25972;&#20010;&#21477;&#23376;&#65292;&#32780;&#19981;&#26159;&#36827;&#34892;&#23383;&#31526;&#32423;&#21035;&#30340;&#26631;&#27880;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#35757;&#32451;&#33539;&#24335;&#22312;&#24494;&#35843;&#21518;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies Chinese Spelling Correction (CSC), which aims to detect and correct potential spelling errors in a given sentence. Current state-of-the-art methods regard CSC as a sequence tagging task and fine-tune BERT-based models on sentence pairs. However, we note a critical flaw in the process of tagging one character to another, that the correction is excessively conditioned on the error. This is opposite from human mindset, where individuals rephrase the complete sentence based on its semantics, rather than solely on the error patterns memorized before. Such a counter-intuitive learning process results in the bottleneck of generalizability and transferability of machine spelling correction. To address this, we propose $Rephrasing Language Modeling$ (ReLM), where the model is trained to rephrase the entire sentence by infilling additional slots, instead of character-to-character tagging. This novel training paradigm achieves the new state-of-the-art results across fine-tuned 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#39044;&#20808;&#35757;&#32451;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#35777;&#26126;&#20854;&#33021;&#22815;&#36229;&#36234;&#34920;&#38754;&#24418;&#24335;&#29305;&#24449;&#65292;&#23398;&#20064;&#31934;&#30830;&#32780;&#24418;&#24335;&#21270;&#23450;&#20041;&#30340;&#20195;&#30721;&#30340;&#35745;&#31639;&#35821;&#20041;&#12290;</title><link>http://arxiv.org/abs/2306.11943</link><description>&lt;p&gt;
&#25506;&#31350;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#25152;&#23398;&#20064;&#30340;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding What Code Language Models Learned. (arXiv:2306.11943v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11943
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#39044;&#20808;&#35757;&#32451;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#35777;&#26126;&#20854;&#33021;&#22815;&#36229;&#36234;&#34920;&#38754;&#24418;&#24335;&#29305;&#24449;&#65292;&#23398;&#20064;&#31934;&#30830;&#32780;&#24418;&#24335;&#21270;&#23450;&#20041;&#30340;&#20195;&#30721;&#30340;&#35745;&#31639;&#35821;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#37117;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#26377;&#20154;&#35748;&#20026;&#23427;&#20204;&#30340;&#33021;&#21147;&#19981;&#36275;&#20197;&#23436;&#20840;&#23398;&#20064;&#35821;&#35328;&#30340;&#24847;&#20041;&#25110;&#29702;&#35299;&#35821;&#35328;&#12290;&#20026;&#20102;&#20102;&#35299;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#26576;&#31181;&#24418;&#24335;&#30340;&#24847;&#20041;&#30340;&#31243;&#24230;&#65292;&#25105;&#20204;&#30740;&#31350;&#23427;&#20204;&#25429;&#25417;&#20195;&#30721;&#35821;&#20041;&#30340;&#33021;&#21147;&#65292;&#36229;&#36234;&#34920;&#23618;&#39057;&#29575;&#21644;&#20849;&#29616;&#30340;&#38480;&#21046;&#12290;&#19982;&#20197;&#24448;&#30740;&#31350;&#27169;&#22411;&#35821;&#35328;&#29305;&#24449;&#30340;&#25506;&#31350;&#30456;&#27604;&#65292;&#25105;&#20204;&#22312;&#19968;&#31181;&#21487;&#20197;&#23458;&#35266;&#22320;&#12289;&#31616;&#21333;&#26126;&#20102;&#22320;&#35780;&#20272;&#27169;&#22411;&#23398;&#20064;&#35821;&#20041;&#33021;&#21147;&#30340;&#29615;&#22659;&#19979;&#30740;&#31350;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#26679;&#30340;&#27169;&#22411;&#26159;&#21542;&#33021;&#25429;&#25417;&#31934;&#30830;&#32780;&#24418;&#24335;&#21270;&#23450;&#20041;&#30340;&#20195;&#30721;&#30340;&#35821;&#20041;&#12290;&#36890;&#36807;&#23545;&#20195;&#30721;&#29255;&#27573;&#30340;&#25805;&#32437;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20195;&#30721;&#39044;&#20808;&#35757;&#32451;&#27169;&#22411;&#23398;&#20064;&#20102;&#20195;&#30721;&#30340;&#35745;&#31639;&#35821;&#20041;&#30340;&#24378;&#26377;&#21147;&#30340;&#34920;&#24449;&#65292;&#36229;&#36234;&#20102;&#20195;&#30721;&#34920;&#38754;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models are effective in a variety of natural language tasks, but it has been argued their capabilities fall short of fully learning meaning or understanding language. To understand the extent to which language models can learn some form of meaning, we investigate their ability to capture semantics of code beyond superficial frequency and co-occurrence. In contrast to previous research on probing models for linguistic features, we study pre-trained models in a setting that allows for objective and straightforward evaluation of a model's ability to learn semantics. In this paper, we examine whether such models capture the semantics of code, which is precisely and formally defined. Through experiments involving the manipulation of code fragments, we show that code pre-trained models of code learn a robust representation of the computational semantics of code that goes beyond superficial features of form alone
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#32654;&#22269;&#20154;&#21475;&#26222;&#26597;&#23616;&#24314;&#31435;&#30340;&#20840;&#32654;&#31038;&#21306;&#35843;&#26597;&#65288;ACS&#65289;&#35780;&#20272;&#20102;&#21313;&#20960;&#20010;&#19981;&#21516;&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21457;&#29616;&#23567;&#22411;&#27169;&#22411;&#20855;&#26377;&#26174;&#33879;&#30340;&#20301;&#32622;&#21644;&#26631;&#31614;&#20559;&#24046;&#65292;&#32780;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#33021;&#20943;&#36731;&#36825;&#31181;&#20559;&#24046;&#65292;&#20294;&#26080;&#27861;&#26681;&#25454;US&#32676;&#20307;&#25110;&#20219;&#20309;&#21487;&#35782;&#21035;&#30340;&#32676;&#20307;&#36235;&#21183;&#36827;&#34892;&#35843;&#25972;&#12290;</title><link>http://arxiv.org/abs/2306.07951</link><description>&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35843;&#26597;&#21709;&#24212;&#30340;&#36136;&#30097;
&lt;/p&gt;
&lt;p&gt;
Questioning the Survey Responses of Large Language Models. (arXiv:2306.07951v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#32654;&#22269;&#20154;&#21475;&#26222;&#26597;&#23616;&#24314;&#31435;&#30340;&#20840;&#32654;&#31038;&#21306;&#35843;&#26597;&#65288;ACS&#65289;&#35780;&#20272;&#20102;&#21313;&#20960;&#20010;&#19981;&#21516;&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21457;&#29616;&#23567;&#22411;&#27169;&#22411;&#20855;&#26377;&#26174;&#33879;&#30340;&#20301;&#32622;&#21644;&#26631;&#31614;&#20559;&#24046;&#65292;&#32780;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#33021;&#20943;&#36731;&#36825;&#31181;&#20559;&#24046;&#65292;&#20294;&#26080;&#27861;&#26681;&#25454;US&#32676;&#20307;&#25110;&#20219;&#20309;&#21487;&#35782;&#21035;&#30340;&#32676;&#20307;&#36235;&#21183;&#36827;&#34892;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#22686;&#24378;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#20197;&#21508;&#31181;&#31185;&#23398;&#21160;&#26426;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#35843;&#26597;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#32654;&#22269;&#20154;&#21475;&#26222;&#26597;&#23616;&#24050;&#32463;&#24314;&#31435;&#30340;&#20840;&#32654;&#31038;&#21306;&#35843;&#26597;&#65288;ACS&#65289;&#65292;&#23601;&#27169;&#22411;&#30340;&#35843;&#26597;&#21709;&#24212;&#32467;&#26524;&#25506;&#31350;&#25152;&#33021;&#20102;&#35299;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#23545;&#21313;&#20960;&#20010;&#19981;&#21516;&#22823;&#23567;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#21442;&#25968;&#33539;&#22260;&#20174;&#20960;&#20159;&#21040;&#19968;&#19975;&#20159;&#19981;&#31561;&#65292;&#20351;&#29992;ACS&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#25968;&#21313;&#19975;&#27425;&#30340;&#27979;&#35797;&#65292;&#31995;&#32479;&#22320;&#24471;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#27169;&#24335;&#12290;&#39318;&#20808;&#65292;&#23567;&#22411;&#27169;&#22411;&#23384;&#22312;&#26126;&#26174;&#30340;&#20301;&#32622;&#21644;&#26631;&#31614;&#20559;&#24046;&#65292;&#20363;&#22914;&#20559;&#21521;&#20110;&#37319;&#29992;&#26631;&#35760;&#20026;&#8220;A&#8221;&#30340;&#35843;&#26597;&#21709;&#24212;&#12290;&#38543;&#30528;&#27169;&#22411;&#23610;&#23544;&#30340;&#22686;&#21152;&#65292;A-&#20559;&#24046;&#34429;&#28982;&#26377;&#25152;&#20943;&#23569;&#65292;&#20294;&#20063;&#36827;&#23637;&#32531;&#24930;&#12290;&#20854;&#27425;&#65292;&#21363;&#20351;&#36890;&#36807;&#38543;&#26426;&#31572;&#26696;&#39034;&#24207;&#26469;&#35843;&#25972;&#36825;&#31181;&#26631;&#35760;&#20559;&#24046;&#65292;&#27169;&#22411;&#20173;&#28982;&#19981;&#20250;&#36235;&#21521;&#20110;&#32654;&#22269;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#25110;&#20219;&#20309;&#21487;&#35782;&#21035;&#30340;&#20154;&#21475;&#25490;&#24207;&#12290;&#30456;&#21453;&#65292;&#21508;&#31181;&#27169;&#22411;&#36235;&#21521;&#20110;&#22343;&#21248;&#38543;&#26426;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large language models increase in capability, researchers have started to conduct surveys of all kinds on these models with varying scientific motivations. In this work, we examine what we can learn from a model's survey responses on the basis of the well-established American Community Survey (ACS) by the U.S. Census Bureau. Evaluating more than a dozen different models, varying in size from a few hundred million to ten billion parameters, hundreds of thousands of times each on questions from the ACS, we systematically establish two dominant patterns. First, smaller models have a significant position and labeling bias, for example, towards survey responses labeled with the letter "A". This A-bias diminishes, albeit slowly, as model size increases. Second, when adjusting for this labeling bias through randomized answer ordering, models still do not trend toward US population statistics or those of any cognizable population. Rather, models across the board trend toward uniformly rando
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#29983;&#25104;&#21644;&#39044;&#27979;&#20449;&#24687;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#35821;&#38899;&#22686;&#24378;&#31995;&#32479;&#65292;&#20854;&#20013;&#20004;&#20010;&#35821;&#38899;&#22686;&#24378;&#27169;&#22359;&#22312;&#31532;&#19968;&#21644;&#26368;&#21518;&#19968;&#20010;&#25193;&#25955;&#27493;&#39588;&#20013;&#34987;&#34701;&#21512;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25193;&#25955;&#35780;&#20998;&#20272;&#35745;&#21487;&#20197;&#20174;&#39044;&#27979;&#20449;&#24687;&#20013;&#21463;&#30410;&#24182;&#21152;&#24555;&#35299;&#30721;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2305.10734</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#19982;&#32852;&#21512;&#29983;&#25104;&#39044;&#27979;&#35299;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Diffusion-Based Speech Enhancement with Joint Generative and Predictive Decoders. (arXiv:2305.10734v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10734
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#29983;&#25104;&#21644;&#39044;&#27979;&#20449;&#24687;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#35821;&#38899;&#22686;&#24378;&#31995;&#32479;&#65292;&#20854;&#20013;&#20004;&#20010;&#35821;&#38899;&#22686;&#24378;&#27169;&#22359;&#22312;&#31532;&#19968;&#21644;&#26368;&#21518;&#19968;&#20010;&#25193;&#25955;&#27493;&#39588;&#20013;&#34987;&#34701;&#21512;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25193;&#25955;&#35780;&#20998;&#20272;&#35745;&#21487;&#20197;&#20174;&#39044;&#27979;&#20449;&#24687;&#20013;&#21463;&#30410;&#24182;&#21152;&#24555;&#35299;&#30721;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30740;&#31350;&#20102;&#22522;&#20110;&#25193;&#25955;&#30340;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#65292;&#20294;&#20854;&#35299;&#30721;&#36807;&#31243;&#38750;&#24120;&#32791;&#26102;&#12290;&#35299;&#20915;&#26041;&#26696;&#20043;&#19968;&#26159;&#20351;&#29992;&#39044;&#27979;&#24615;&#35821;&#38899;&#22686;&#24378;&#31995;&#32479;&#20272;&#35745;&#22686;&#24378;&#29305;&#24449;&#65292;&#28982;&#21518;&#21021;&#22987;&#21270;&#35299;&#30721;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20004;&#38454;&#27573;&#26041;&#27861;&#24573;&#30053;&#20102;&#39044;&#27979;&#24615;&#19982;&#25193;&#25955;&#24615;&#35821;&#38899;&#22686;&#24378;&#20043;&#38388;&#30340;&#20114;&#34917;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#36825;&#20004;&#20010;&#35821;&#38899;&#22686;&#24378;&#27169;&#22359;&#30340;&#32479;&#19968;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#32534;&#30721;&#29983;&#25104;&#21644;&#39044;&#27979;&#20449;&#24687;&#65292;&#28982;&#21518;&#24212;&#29992;&#29983;&#25104;&#21644;&#39044;&#27979;&#35299;&#30721;&#22120;&#65292;&#23427;&#20204;&#30340;&#36755;&#20986;&#34987;&#34701;&#21512;&#12290;&#20855;&#20307;&#22320;&#65292;&#20004;&#20010;&#35821;&#38899;&#22686;&#24378;&#27169;&#22359;&#22312;&#31532;&#19968;&#21644;&#26368;&#21518;&#19968;&#20010;&#25193;&#25955;&#27493;&#39588;&#20013;&#34987;&#34701;&#21512;&#65306;&#31532;&#19968;&#20010;&#27493;&#39588;&#34701;&#21512;&#20351;&#29992;&#39044;&#27979;&#24615;&#35821;&#38899;&#22686;&#24378;&#26469;&#21021;&#22987;&#21270;&#25193;&#25955;&#36807;&#31243;&#65292;&#20197;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#65307;&#26368;&#21518;&#19968;&#20010;&#27493;&#39588;&#34701;&#21512;&#23558;&#20004;&#20010;&#20114;&#34917;&#30340;&#35821;&#38899;&#22686;&#24378;&#36755;&#20986;&#32452;&#21512;&#36215;&#26469;&#65292;&#20197;&#25552;&#39640;&#35821;&#38899;&#22686;&#24378;&#24615;&#33021;&#12290;&#22312;Voice-Bank&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25193;&#25955;&#35780;&#20998;&#20272;&#35745;&#21487;&#20197;&#20174;&#39044;&#27979;&#20449;&#24687;&#20013;&#33719;&#30410;&#24182;&#21152;&#24555;&#35299;&#30721;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based speech enhancement (SE) has been investigated recently, but its decoding is very time-consuming. One solution is to initialize the decoding process with the enhanced feature estimated by a predictive SE system. However, this two-stage method ignores the complementarity between predictive and diffusion SE. In this paper, we propose a unified system that integrates these two SE modules. The system encodes both generative and predictive information, and then applies both generative and predictive decoders, whose outputs are fused. Specifically, the two SE modules are fused in the first and final diffusion steps: the first step fusion initializes the diffusion process with the predictive SE for improving the convergence, and the final step fusion combines the two complementary SE outputs to improve the SE performance. Experiments on the Voice-Bank dataset show that the diffusion score estimation can benefit from the predictive information and speed up the decoding.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30701;&#35821;&#32423;&#22797;&#21046;&#26426;&#21046;-PROM&#65292;&#21487;&#20197;&#22686;&#24378;&#23545;n-gram&#30340;&#27880;&#24847;&#21147;&#65292;&#29992;&#20110;&#20855;&#26377;&#39044;&#35757;&#32451;&#30340;&#38646;&#26679;&#26412;&#25688;&#35201;&#29983;&#25104;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#25688;&#35201;&#30340;&#36136;&#37327;&#21644;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.06647</link><description>&lt;p&gt;
PROM&#65306;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;&#30701;&#35821;&#32423;&#22797;&#21046;&#26426;&#21046;&#65292;&#29992;&#20110;&#25277;&#35937;&#25688;&#35201;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
PROM: A Phrase-level Copying Mechanism with Pre-training for Abstractive Summarization. (arXiv:2305.06647v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06647
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30701;&#35821;&#32423;&#22797;&#21046;&#26426;&#21046;-PROM&#65292;&#21487;&#20197;&#22686;&#24378;&#23545;n-gram&#30340;&#27880;&#24847;&#21147;&#65292;&#29992;&#20110;&#20855;&#26377;&#39044;&#35757;&#32451;&#30340;&#38646;&#26679;&#26412;&#25688;&#35201;&#29983;&#25104;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#25688;&#35201;&#30340;&#36136;&#37327;&#21644;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#25277;&#35937;&#25688;&#35201;&#29983;&#25104;&#26041;&#38754;&#30340;&#26174;&#33879;&#25104;&#23601;&#65292;&#22797;&#21046;&#26426;&#21046;&#36890;&#36807;&#25552;&#39640;&#20107;&#23454;&#24615;&#12289;&#31283;&#23450;&#24615;&#21644;&#25972;&#20307;&#24615;&#33021;&#26041;&#38754;&#30340;&#25913;&#36827;&#35777;&#26126;&#20854;&#26377;&#21161;&#20110;&#36825;&#19968;&#36827;&#31243;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#30701;&#35821;&#32423;&#22797;&#21046;&#26426;&#21046;-PROM&#65292;&#23427;&#22686;&#24378;&#20102;&#23545;n-gram&#30340;&#27880;&#24847;&#21147;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20855;&#26377;&#39044;&#35757;&#32451;&#30340;&#38646;&#26679;&#26412;&#25688;&#35201;&#29983;&#25104;&#20013;&#12290;PROM&#28155;&#21152;&#20102;&#19968;&#20010;&#25351;&#31034;&#22120;&#23618;&#65292;&#20197;&#26126;&#30830;&#20174;&#28304;&#20013;&#21487;&#20197;&#22797;&#21046;&#30340;n-gram&#20013;&#30340;&#20196;&#29260;&#65292;&#24182;&#35745;&#31639;&#22797;&#21046;&#39044;&#27979;&#30340;&#36741;&#21161;&#25439;&#22833;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#22522;&#20934;&#24494;&#35843;&#20013;&#65292;PROM&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#20013;&#65292;PROM&#29992;&#20110;&#23545;&#21407;&#22987;&#35821;&#26009;&#24211;&#36827;&#34892;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#25688;&#35201;&#25968;&#25454;&#38598;&#19978;&#25552;&#20379;&#20102;&#26032;&#30340;&#36890;&#29992;&#22522;&#32447;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;PROM&#25191;&#34892;&#26356;&#21512;&#29702;&#30340;&#22797;&#21046;&#24182;&#26377;&#21161;&#20110;&#20445;&#25345;&#24544;&#23454;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Based on the remarkable achievements of pre-trained language models in abstractive summarization, the copying mechanism has proved helpful by improving the factuality, stability, and overall performance. This work proposes PROM, a new PhRase-level cOpying Mechanism that enhances attention on n-grams, which can be applied to zero-shot summarization with pre-training. PROM adds an indicator layer to explicitly pick up tokens in n-gram that can be copied from the source, and calculates an auxiliary loss for the copying prediction. Empirical studies show that PROM makes significant improvements in fine-tuning on benchmarks. In zero-shot setting, PROM is utilized in the self-supervised pre-training on raw corpora and provides new general baselines on a wide range of summarization datasets. Further analysis shows that PROM performs more reasonable copying and contributes to faithfulness.
&lt;/p&gt;</description></item></channel></rss>