<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#36890;&#36807;&#27604;&#36739;&#20154;&#31867;&#20247;&#21253;&#21465;&#20107;&#21644;AI&#21465;&#20107;&#65292;&#25506;&#31350;&#20102;&#25991;&#21270;&#20135;&#29289;&#21644;&#31038;&#20250;&#20559;&#35265;&#22312;&#25925;&#20107;&#20013;&#30340;&#34920;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-3.5&#21644;GPT-4&#29983;&#25104;&#30340;&#21465;&#20107;&#26356;&#20855;&#36827;&#23637;&#24615;&#65292;&#24182;&#19988;&#26222;&#32599;&#31859;&#20462;&#26031;&#31070;&#35805;&#22312;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24819;&#35937;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.12902</link><description>&lt;p&gt;
&#23454;&#39564;&#21465;&#20107;&#65306;&#20154;&#31867;&#20247;&#21253;&#21465;&#20107;&#21644;AI&#21465;&#20107;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Experimental Narratives: A Comparison of Human Crowdsourced Storytelling and AI Storytelling. (arXiv:2310.12902v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#36890;&#36807;&#27604;&#36739;&#20154;&#31867;&#20247;&#21253;&#21465;&#20107;&#21644;AI&#21465;&#20107;&#65292;&#25506;&#31350;&#20102;&#25991;&#21270;&#20135;&#29289;&#21644;&#31038;&#20250;&#20559;&#35265;&#22312;&#25925;&#20107;&#20013;&#30340;&#34920;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-3.5&#21644;GPT-4&#29983;&#25104;&#30340;&#21465;&#20107;&#26356;&#20855;&#36827;&#23637;&#24615;&#65292;&#24182;&#19988;&#26222;&#32599;&#31859;&#20462;&#26031;&#31070;&#35805;&#22312;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24819;&#35937;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#32467;&#21512;&#34892;&#20026;&#21644;&#35745;&#31639;&#23454;&#39564;&#65292;&#21033;&#29992;&#34394;&#26500;&#30340;&#25552;&#31034;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#24037;&#20855;&#65292;&#30740;&#31350;&#20154;&#31867;&#21644;&#29983;&#25104;&#24335;AI&#21465;&#20107;&#20013;&#30340;&#25991;&#21270;&#20135;&#29289;&#21644;&#31038;&#20250;&#20559;&#35265;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;2019&#24180;6&#26376;&#30001;&#20247;&#21253;&#24037;&#20316;&#32773;&#21019;&#20316;&#30340;250&#20010;&#25925;&#20107;&#21644;2023&#24180;3&#26376;&#30001;GPT-3.5&#21644;GPT-4&#29983;&#25104;&#30340;80&#20010;&#25925;&#20107;&#65292;&#23558;&#21465;&#20107;&#23398;&#21644;&#25512;&#29702;&#32479;&#35745;&#23398;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#20247;&#21253;&#24037;&#20316;&#32773;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#37117;&#22238;&#31572;&#20102;&#20851;&#20110;&#19982;&#20154;&#24037;&#26234;&#33021;&#20154;&#31867;&#30456;&#24651;&#30340;&#20027;&#39064;&#30340;&#30456;&#21516;&#25552;&#31034;&#12290;&#25552;&#20986;&#30340;&#23454;&#39564;&#33539;&#24335;&#20351;&#20154;&#31867;&#21644;LLM&#29983;&#25104;&#30340;&#21465;&#20107;&#21487;&#20197;&#30452;&#25509;&#36827;&#34892;&#27604;&#36739;&#12290;&#23545;&#20110;&#25552;&#21040;&#26222;&#32599;&#31859;&#20462;&#26031;&#20027;&#39064;&#30340;&#22238;&#24212;&#35777;&#23454;&#20102;&#26222;&#32599;&#31859;&#20462;&#26031;&#31070;&#35805;&#22312;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38598;&#20307;&#24819;&#35937;&#20013;&#30340;&#26222;&#36941;&#23384;&#22312;&#12290;&#25152;&#26377;&#25552;&#20379;&#30340;&#21465;&#20107;&#37117;&#34920;&#29616;&#20986;&#31185;&#23398;&#25110;&#25216;&#26415;&#30340;&#36861;&#27714;&#12290;&#20998;&#26512;&#34920;&#26126;&#65292;GPT-3.5&#21644;&#23588;&#20854;&#26159;GPT-4&#29983;&#25104;&#30340;&#21465;&#20107;&#26356;&#20855;&#36827;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper proposes a framework that combines behavioral and computational experiments employing fictional prompts as a novel tool for investigating cultural artifacts and social biases in storytelling both by humans and generative AI. The study analyzes 250 stories authored by crowdworkers in June 2019 and 80 stories generated by GPT-3.5 and GPT-4 in March 2023 by merging methods from narratology and inferential statistics. Both crowdworkers and large language models responded to identical prompts about creating and falling in love with an artificial human. The proposed experimental paradigm allows a direct comparison between human and LLM-generated storytelling. Responses to the Pygmalionesque prompts confirm the pervasive presence of the Pygmalion myth in the collective imaginary of both humans and large language models. All solicited narratives present a scientific or technological pursuit. The analysis reveals that narratives from GPT-3.5 and particularly GPT-4 are more more progre
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#31995;&#32479;&#24615;&#22320;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#24182;&#21457;&#29616;&#20102;&#36866;&#24212;&#24615;&#21644;&#20869;&#22312;&#20559;&#35265;&#23545;&#31995;&#32479;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.12892</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#20013;&#24615;&#33021;&#24046;&#24322;&#30340;&#31995;&#32479;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Systematic Study of Performance Disparities in Multilingual Task-Oriented Dialogue Systems. (arXiv:2310.12892v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12892
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#31995;&#32479;&#24615;&#22320;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#24182;&#21457;&#29616;&#20102;&#36866;&#24212;&#24615;&#21644;&#20869;&#22312;&#20559;&#35265;&#23545;&#31995;&#32479;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#33021;&#22815;&#22312;&#19990;&#30028;&#19978;&#22810;&#31181;&#35821;&#35328;&#20013;&#34920;&#29616;&#33391;&#22909;&#30340;&#24378;&#22823;&#35821;&#35328;&#25216;&#26415;&#26159;&#22810;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26680;&#24515;&#30446;&#26631;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#22810;&#35821;&#35328;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#65288;ToD&#65289;&#31995;&#32479;&#20043;&#38388;&#23384;&#22312;&#30340;&#20219;&#21153;&#24615;&#33021;&#24046;&#24322;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#24635;&#32467;&#21644;&#23454;&#35777;&#20998;&#26512;&#12290;&#25105;&#20204;&#39318;&#20808;&#23450;&#20041;&#20102;&#31995;&#32479;&#24615;&#33021;&#32477;&#23545;&#21644;&#30456;&#23545;&#31561;&#20215;&#30340;&#26032;&#30340;&#37327;&#21270;&#25351;&#26631;&#65292;&#25429;&#25417;&#20102;&#36328;&#35821;&#35328;&#21644;&#21333;&#20010;&#35821;&#35328;&#20869;&#30340;&#24046;&#24322;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#25511;&#21046;&#24615;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24615;&#33021;&#24046;&#24322;&#21462;&#20915;&#20110;&#22810;&#20010;&#22240;&#32032;&#65306;&#25163;&#22836;&#30340;ToD&#20219;&#21153;&#30340;&#24615;&#36136;&#65292;&#24213;&#23618;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#30446;&#26631;&#35821;&#35328;&#20197;&#21450;ToD&#27880;&#37322;&#25968;&#25454;&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#23454;&#35777;&#22320;&#35777;&#26126;&#20102;&#24403;&#21069;ToD&#31995;&#32479;&#20013;&#23384;&#22312;&#30340;&#36866;&#24212;&#24615;&#21644;&#20869;&#22312;&#20559;&#35265;&#65306;&#20363;&#22914;&#65292;&#20351;&#29992;&#19982;&#33521;&#35821;ToD&#25968;&#25454;&#23436;&#20840;&#24179;&#34892;&#30340;&#27880;&#37322;ToD&#25968;&#25454;&#35757;&#32451;&#30340;&#38463;&#25289;&#20271;&#35821;&#25110;&#22303;&#32819;&#20854;&#35821;ToD&#31995;&#32479;&#20173;&#28982;&#34920;&#29616;&#20986;&#38477;&#20302;&#30340;ToD&#20219;&#21153;&#24615;&#33021;&#12290;&#38500;&#20102;&#25552;&#20379;&#19968;&#31995;&#21015;&#23454;&#35777;&#32467;&#26524;&#22806;&#65292;
&lt;/p&gt;
&lt;p&gt;
Achieving robust language technologies that can perform well across the world's many languages is a central goal of multilingual NLP. In this work, we take stock of and empirically analyse task performance disparities that exist between multilingual task-oriented dialogue (ToD) systems. We first define new quantitative measures of absolute and relative equivalence in system performance, capturing disparities across languages and within individual languages. Through a series of controlled experiments, we demonstrate that performance disparities depend on a number of factors: the nature of the ToD task at hand, the underlying pretrained language model, the target language, and the amount of ToD annotated data. We empirically prove the existence of the adaptation and intrinsic biases in current ToD systems: e.g., ToD systems trained for Arabic or Turkish using annotated ToD data fully parallel to English ToD data still exhibit diminished ToD task performance. Beyond providing a series of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#26500;&#24314;&#25925;&#20107;&#32423;&#31867;&#27604;&#35821;&#26009;&#24211;StoryAnalogy&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35782;&#21035;&#21644;&#29983;&#25104;&#31867;&#27604;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#36825;&#20123;&#20219;&#21153;&#23545;&#20110;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#21644;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#37117;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#21457;&#29616;&#36890;&#36807;&#20351;&#29992;StoryAnalogy&#20013;&#30340;&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31867;&#27604;&#29983;&#25104;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.12874</link><description>&lt;p&gt;
StoryAnalogy: &#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#34893;&#29983;&#20986;&#25925;&#20107;&#32423;&#31867;&#27604;&#20197;&#35299;&#24320;&#31867;&#27604;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
StoryAnalogy: Deriving Story-level Analogies from Large Language Models to Unlock Analogical Understanding. (arXiv:2310.12874v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#26500;&#24314;&#25925;&#20107;&#32423;&#31867;&#27604;&#35821;&#26009;&#24211;StoryAnalogy&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35782;&#21035;&#21644;&#29983;&#25104;&#31867;&#27604;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#36825;&#20123;&#20219;&#21153;&#23545;&#20110;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#21644;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#37117;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#21457;&#29616;&#36890;&#36807;&#20351;&#29992;StoryAnalogy&#20013;&#30340;&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31867;&#27604;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25925;&#20107;&#20043;&#38388;&#30340;&#31867;&#27604;&#26159;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20013;&#26368;&#20851;&#38190;&#30340;&#33021;&#21147;&#20043;&#19968;&#12290;&#26412;&#25991;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#35268;&#27169;&#24040;&#22823;&#30340;&#25925;&#20107;&#32423;&#31867;&#27604;&#35821;&#26009;&#24211;StoryAnalogy&#26469;&#35780;&#20272;&#35782;&#21035;&#21644;&#29983;&#25104;&#31867;&#27604;&#30340;&#33021;&#21147;&#65292;&#35813;&#35821;&#26009;&#24211;&#21253;&#21547;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;24K&#20010;&#25925;&#20107;&#23545;&#65292;&#24182;&#23545;&#26469;&#33258;&#25193;&#23637;&#32467;&#26500;&#26144;&#23556;&#29702;&#35770;&#30340;&#20004;&#20010;&#30456;&#20284;&#24615;&#36827;&#34892;&#20154;&#24037;&#27880;&#37322;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31995;&#21015;&#22312;StoryAnalogy&#19978;&#30340;&#27979;&#35797;&#65292;&#39318;&#27425;&#35780;&#20272;&#20102;&#25925;&#20107;&#32423;&#31867;&#27604;&#30340;&#35782;&#21035;&#21644;&#29983;&#25104;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#31867;&#27604;&#35782;&#21035;&#20219;&#21153;&#23545;&#20110;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#20197;&#21450;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#21644;LLaMa&#65289;&#26469;&#35828;&#37117;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20854;&#20013;ChatGPT&#22312;&#22810;&#36873;&#39064;&#20013;&#21482;&#33021;&#36798;&#21040;&#32422;30%&#30340;&#20934;&#30830;&#29575;&#65288;&#23545;&#20110;&#20154;&#31867;&#32780;&#35328;&#65292;&#20934;&#30830;&#29575;&#36229;&#36807;85%&#65289;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;StoryAnalogy&#20013;&#30340;&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31867;&#27604;&#29983;&#25104;&#36136;&#37327;&#65292;&#20854;&#20013;&#32463;&#36807;&#24494;&#35843;&#30340;FlanT5-xxl&#27169;&#22411;&#33719;&#24471;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analogy-making between narratives is one of the most critical abilities in natural language understanding. In this paper, we evaluate the ability to identify and generate analogy by building a first-of-its-kind large-scale story-level analogy corpus, StoryAnalogy, which contains 24K story pairs from diverse domains with human annotations on two similarities from the extended Structure-Mapping Theory. We design a set of tests on StoryAnalogy, presenting the first evaluation of story-level analogy identification and generation. Interestingly, we find that the analogy identification tasks are extremely challenging not only for the sentence embedding models but also for the recent large language models (LLMs) such as ChatGPT and LLaMa, where ChatGPT only achieved around 30% accuracy in multiple-choice questions (&gt; 85% accuracy for humans). Finally, we find that data in StoryAnalogy can improve LLMs analogy generation quality, where a fine-tuned FlanT5-xxl model yields comparable performanc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#20301;&#32622;&#32534;&#30721;&#20013;&#30340;&#23616;&#37096;&#24615;&#21644;&#23545;&#31216;&#24615;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#21457;&#29616;&#20854;&#26680;&#24515;&#21151;&#33021;&#21644;&#19982;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#20043;&#38388;&#30340;&#23494;&#20999;&#20851;&#32852;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#24403;&#21069;&#20301;&#32622;&#32534;&#30721;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;</title><link>http://arxiv.org/abs/2310.12864</link><description>&lt;p&gt;
&#20301;&#32622;&#32534;&#30721;&#30340;&#23616;&#37096;&#24615;&#21644;&#23545;&#31216;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Locality and Symmetry of Positional Encodings. (arXiv:2310.12864v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#20301;&#32622;&#32534;&#30721;&#20013;&#30340;&#23616;&#37096;&#24615;&#21644;&#23545;&#31216;&#24615;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#21457;&#29616;&#20854;&#26680;&#24515;&#21151;&#33021;&#21644;&#19982;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#20043;&#38388;&#30340;&#23494;&#20999;&#20851;&#32852;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#24403;&#21069;&#20301;&#32622;&#32534;&#30721;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20301;&#32622;&#32534;&#30721;&#65288;PE&#65289;&#34987;&#29992;&#20110;&#22312;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#27880;&#20837;&#21333;&#35789;&#39034;&#24207;&#20449;&#24687;&#12290;&#34429;&#28982;&#23427;&#20204;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#21477;&#23376;&#34920;&#31034;&#30340;&#36136;&#37327;&#65292;&#20294;&#23427;&#20204;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20855;&#20307;&#36129;&#29486;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#65292;&#29305;&#21035;&#26159;&#32771;&#34385;&#21040;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#21508;&#31181;&#20301;&#32622;&#32534;&#30721;&#23545;&#21333;&#35789;&#39034;&#24207;&#19981;&#25935;&#24863;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#21452;&#21521;&#33945;&#29256;&#35821;&#35328;&#27169;&#22411;&#65288;BERT&#39118;&#26684;&#65289;&#20013;&#30340;&#20301;&#32622;&#32534;&#30721;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#34917;&#20805;&#20102;&#29616;&#26377;&#24037;&#20316;&#30340;&#19977;&#20010;&#26041;&#38754;&#65306;&#65288;1&#65289;&#36890;&#36807;&#30830;&#23450;&#23616;&#37096;&#24615;&#21644;&#23545;&#31216;&#24615;&#36825;&#20004;&#20010;&#24120;&#35265;&#23646;&#24615;&#65292;&#25581;&#31034;&#20986;PE&#30340;&#26680;&#24515;&#21151;&#33021;&#65307;&#65288;2&#65289;&#23637;&#31034;&#20102;&#36825;&#20004;&#20010;&#23646;&#24615;&#19982;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#23494;&#20999;&#30456;&#20851;&#65307;&#65288;3&#65289;&#36890;&#36807;&#24341;&#20837;&#20004;&#20010;&#26032;&#30340;&#25506;&#27979;&#20219;&#21153;&#65292;&#37327;&#21270;&#20102;&#24403;&#21069;PE&#30340;&#19981;&#36275;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#20123;&#32467;&#26524;&#26159;&#20026;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#26356;&#22909;&#30340;PE&#30340;&#22522;&#30784;&#12290;&#20195;&#30721;&#21487;&#22312; \faGITHUB &#19978;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Positional Encodings (PEs) are used to inject word-order information into transformer-based language models. While they can significantly enhance the quality of sentence representations, their specific contribution to language models is not fully understood, especially given recent findings that various positional encodings are insensitive to word order. In this work, we conduct a systematic study of positional encodings in \textbf{Bidirectional Masked Language Models} (BERT-style) , which complements existing work in three aspects: (1) We uncover the core function of PEs by identifying two common properties, Locality and Symmetry; (2) We show that the two properties are closely correlated with the performances of downstream tasks; (3) We quantify the weakness of current PEs by introducing two new probing tasks, on which current PEs perform poorly. We believe that these results are the basis for developing better PEs for transformer-based language models. The code is available at \faGi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26816;&#27979;&#20167;&#24680;&#35328;&#35770;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#22312;&#27169;&#22411;&#20013;&#21253;&#21547;&#30446;&#26631;&#20449;&#24687;&#21644;&#29702;&#30001;/&#35299;&#37322;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#27169;&#22411;&#20998;&#31867;&#21644;&#39044;&#27979;&#38169;&#35823;&#30340;&#38169;&#35823;&#26696;&#20363;&#30340;&#20998;&#31867;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.12860</link><description>&lt;p&gt;
&#25506;&#31350;&#29992;&#20110;&#26816;&#27979;&#20167;&#24680;&#35328;&#35770;&#30340;LLM&#65306;&#20248;&#21183;&#21644;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
Probing LLMs for hate speech detection: strengths and vulnerabilities. (arXiv:2310.12860v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26816;&#27979;&#20167;&#24680;&#35328;&#35770;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#22312;&#27169;&#22411;&#20013;&#21253;&#21547;&#30446;&#26631;&#20449;&#24687;&#21644;&#29702;&#30001;/&#35299;&#37322;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#27169;&#22411;&#20998;&#31867;&#21644;&#39044;&#27979;&#38169;&#35823;&#30340;&#38169;&#35823;&#26696;&#20363;&#30340;&#20998;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#21644;&#30740;&#31350;&#20154;&#21592;&#26368;&#36817;&#20570;&#20986;&#20102;&#21162;&#21147;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#26816;&#27979;&#20855;&#26377;&#20167;&#24680;&#25110;&#26377;&#27602;&#35821;&#35328;&#30340;&#23384;&#22312;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#24182;&#26410;&#22312;&#26816;&#27979;&#36807;&#31243;&#20013;&#21033;&#29992;&#35299;&#37322;&#12289;&#39069;&#22806;&#19978;&#19979;&#25991;&#21644;&#21463;&#23475;&#31038;&#21306;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#25552;&#31034;&#21464;&#21270;&#21644;&#36755;&#20837;&#20449;&#24687;&#65292;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#19981;&#28155;&#21152;&#20219;&#20309;&#19978;&#19979;&#25991;&#31034;&#20363;&#65289;&#12290;&#25105;&#20204;&#36873;&#25321;&#20102;&#19977;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;GPT-3.5&#12289;text-davinci&#21644;Flan-T5&#65289;&#21644;&#19977;&#20010;&#25968;&#25454;&#38598;HateXplain&#12289;implicit hate&#21644;ToxicSpans&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24179;&#22343;&#32780;&#35328;&#65292;&#22312;&#31649;&#36947;&#20013;&#21253;&#21547;&#30446;&#26631;&#20449;&#24687;&#22823;&#22823;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#65288;&#32422;20-30%&#65289;&#22312;&#21508;&#20010;&#25968;&#25454;&#38598;&#19978;&#30456;&#27604;&#22522;&#32447;&#12290;&#23558;&#29702;&#30001;/&#35299;&#37322;&#28155;&#21152;&#21040;&#31649;&#36947;&#20013;&#20063;&#26377;&#26174;&#33879;&#25928;&#26524;&#65288;&#32422;10-20%&#65289;&#22312;&#21508;&#20010;&#25968;&#25454;&#38598;&#19978;&#30456;&#27604;&#22522;&#32447;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#36825;&#20123;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#20998;&#31867;&#21644;&#39044;&#27979;&#38169;&#35823;&#30340;&#38169;&#35823;&#26696;&#20363;&#30340;&#20998;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently efforts have been made by social media platforms as well as researchers to detect hateful or toxic language using large language models. However, none of these works aim to use explanation, additional context and victim community information in the detection process. We utilise different prompt variation, input information and evaluate large language models in zero shot setting (without adding any in-context examples). We select three large language models (GPT-3.5, text-davinci and Flan-T5) and three datasets HateXplain, implicit hate and ToxicSpans. We find that on average including the target information in the pipeline improves the model performance substantially (~20-30%) over the baseline across the datasets. There is also a considerable effect of adding the rationales/explanations into the pipeline (~10-20%) over the baseline across the datasets. In addition, we further provide a typology of the error cases where these large language models fail to (i) classify and (i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;&#35828;&#35805;&#20154;&#26085;&#31243;&#21644;&#24773;&#24863;&#35782;&#21035;&#30456;&#32467;&#21512;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25552;&#39640;&#20102;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.12851</link><description>&lt;p&gt;
EmoDiarize: &#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20174;&#35821;&#38899;&#20449;&#21495;&#20013;&#36827;&#34892;&#35828;&#35805;&#20154;&#26085;&#31243;&#21644;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
EmoDiarize: Speaker Diarization and Emotion Identification from Speech Signals using Convolutional Neural Networks. (arXiv:2310.12851v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;&#35828;&#35805;&#20154;&#26085;&#31243;&#21644;&#24773;&#24863;&#35782;&#21035;&#30456;&#32467;&#21512;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25552;&#39640;&#20102;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#26426;&#20132;&#20114;&#26102;&#20195;&#65292;&#35782;&#21035;&#21475;&#22836;&#35821;&#35328;&#20013;&#30340;&#24773;&#24863;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#24212;&#23545;&#35828;&#35805;&#20154;&#26085;&#31243;&#21644;&#24773;&#24863;&#35782;&#21035;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#23427;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#29616;&#26377;&#30340;&#35828;&#35805;&#20154;&#26085;&#31243;&#27969;&#31243;&#21644;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476; (CNN) &#30340;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#31934;&#24230;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#26159;&#22312;&#20116;&#20010;&#35821;&#38899;&#24773;&#24863;&#25968;&#25454;&#38598; (RAVDESS&#65292;CREMA-D&#65292;SAVEE&#65292;TESS&#21644;&#30005;&#24433;&#29255;&#27573;) &#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#20854;&#20013;&#21518;&#32773;&#26159;&#19987;&#38376;&#20026;&#26412;&#30740;&#31350;&#21019;&#24314;&#30340;&#19968;&#20010;&#35821;&#38899;&#24773;&#24863;&#25968;&#25454;&#38598;&#12290;&#20174;&#27599;&#20010;&#26679;&#26412;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#21253;&#25324;Mel&#39057;&#29575;&#20498;&#35889;&#31995;&#25968; (MFCC)&#65292;&#36807;&#38646;&#29575; (ZCR)&#65292;&#22343;&#26041;&#26681; (RMS) &#21644;&#21508;&#31181;&#25968;&#25454;&#22686;&#24378;&#31639;&#27861;&#65292;&#22914;&#38899;&#39640;&#12289;&#22122;&#22768;&#12289;&#25289;&#20280;&#21644;&#31227;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of advanced artificial intelligence and human-computer interaction, identifying emotions in spoken language is paramount. This research explores the integration of deep learning techniques in speech emotion recognition, offering a comprehensive solution to the challenges associated with speaker diarization and emotion identification. It introduces a framework that combines a pre-existing speaker diarization pipeline and an emotion identification model built on a Convolutional Neural Network (CNN) to achieve higher precision. The proposed model was trained on data from five speech emotion datasets, namely, RAVDESS, CREMA-D, SAVEE, TESS, and Movie Clips, out of which the latter is a speech emotion dataset created specifically for this research. The features extracted from each sample include Mel Frequency Cepstral Coefficients (MFCC), Zero Crossing Rate (ZCR), Root Mean Square (RMS), and various data augmentation algorithms like pitch, noise, stretch, and shift. This feature e
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#39564;&#35777;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#20010;&#29420;&#31435;&#30340;&#39564;&#35777;&#22120;&#26469;&#26816;&#27979;&#27169;&#22411;&#36755;&#20986;&#21644;&#30693;&#35782;&#30340;&#38169;&#35823;&#65292;&#24182;&#36890;&#36807;&#26816;&#32034;&#26032;&#30693;&#35782;&#25110;&#20462;&#27491;&#25688;&#35201;&#26469;&#32416;&#27491;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2310.12836</link><description>&lt;p&gt;
&#30693;&#35782;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Knowledge-Augmented Language Model Verification. (arXiv:2310.12836v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12836
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#39564;&#35777;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#20010;&#29420;&#31435;&#30340;&#39564;&#35777;&#22120;&#26469;&#26816;&#27979;&#27169;&#22411;&#36755;&#20986;&#21644;&#30693;&#35782;&#30340;&#38169;&#35823;&#65292;&#24182;&#36890;&#36807;&#26816;&#32034;&#26032;&#30693;&#35782;&#25110;&#20462;&#27491;&#25688;&#35201;&#26469;&#32416;&#27491;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#25991;&#26412;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#33021;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#22791;&#20102;&#21442;&#25968;&#20013;&#20869;&#37096;&#21270;&#30340;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#30340;&#30693;&#35782;&#21487;&#33021;&#26159;&#19981;&#20934;&#30830;&#12289;&#19981;&#23436;&#25972;&#21644;&#36807;&#26102;&#30340;&#65292;&#22240;&#27492;&#35821;&#35328;&#27169;&#22411;&#24448;&#24448;&#20250;&#23545;&#32473;&#23450;&#26597;&#35810;&#29983;&#25104;&#20986;&#20107;&#23454;&#19978;&#19981;&#27491;&#30830;&#30340;&#22238;&#24212;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#21033;&#29992;&#20174;&#22806;&#37096;&#30693;&#35782;&#28304;&#26816;&#32034;&#21040;&#30340;&#30693;&#35782;&#26469;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#26041;&#27861;&#32463;&#24120;&#22240;&#20026;&#20004;&#20010;&#21407;&#22240;&#32780;&#26174;&#31034;&#20986;&#27425;&#20248;&#30340;&#25991;&#26412;&#29983;&#25104;&#24615;&#33021;&#65306;1&#65289;&#27169;&#22411;&#21487;&#33021;&#26080;&#27861;&#26816;&#32034;&#21040;&#19982;&#32473;&#23450;&#26597;&#35810;&#30456;&#20851;&#30340;&#30693;&#35782;&#65307;2&#65289;&#27169;&#22411;&#21487;&#33021;&#26080;&#27861;&#22312;&#29983;&#25104;&#25991;&#26412;&#20013;&#24544;&#23454;&#22320;&#21453;&#26144;&#26816;&#32034;&#21040;&#30340;&#30693;&#35782;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21033;&#29992;&#19968;&#20010;&#21333;&#29420;&#30340;&#39564;&#35777;&#22120;&#26469;&#39564;&#35777;&#30693;&#35782;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#21644;&#30693;&#35782;&#65292;&#36825;&#20010;&#39564;&#35777;&#22120;&#26159;&#19968;&#20010;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25351;&#23548;&#24494;&#35843;&#30340;&#26041;&#24335;&#35757;&#32451;&#26469;&#26816;&#27979;&#36825;&#20004;&#31181;&#31867;&#22411;&#30340;&#38169;&#35823;&#12290;&#28982;&#21518;&#65292;&#24403;&#39564;&#35777;&#22120;&#26816;&#27979;&#21040;&#38169;&#35823;&#26102;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#26816;&#32034;&#26032;&#30340;&#30693;&#35782;&#25110;&#33719;&#21462;&#26356;&#25913;&#25688;&#35201;&#26469;&#36827;&#34892;&#20462;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent Language Models (LMs) have shown impressive capabilities in generating texts with the knowledge internalized in parameters. Yet, LMs often generate the factually incorrect responses to the given queries, since their knowledge may be inaccurate, incomplete, and outdated. To address this problem, previous works propose to augment LMs with the knowledge retrieved from an external knowledge source. However, such approaches often show suboptimal text generation performance due to two reasons: 1) the model may fail to retrieve the knowledge relevant to the given query, or 2) the model may not faithfully reflect the retrieved knowledge in the generated text. To overcome these, we propose to verify the output and the knowledge of the knowledge-augmented LMs with a separate verifier, which is a small LM that is trained to detect those two types of errors through instruction-finetuning. Then, when the verifier recognizes an error, we can rectify it by either retrieving new knowledge or ge
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;AgentTuning&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#21487;&#25552;&#21319;LLMs&#30340;&#20195;&#29702;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#36890;&#29992;&#33021;&#21147;&#12290;&#36890;&#36807;&#26500;&#24314;AgentInstruct&#25968;&#25454;&#38598;&#65292;&#24182;&#37319;&#29992;&#19968;&#31181;&#28151;&#21512;&#35757;&#32451;&#26041;&#27861;&#65292;&#20316;&#32773;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#25552;&#39640;LLMs&#20195;&#29702;&#33021;&#21147;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2310.12823</link><description>&lt;p&gt;
AgentTuning: &#20026;LLMs&#23454;&#29616;&#36890;&#29992;&#20195;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
AgentTuning: Enabling Generalized Agent Abilities for LLMs. (arXiv:2310.12823v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;AgentTuning&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#21487;&#25552;&#21319;LLMs&#30340;&#20195;&#29702;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#36890;&#29992;&#33021;&#21147;&#12290;&#36890;&#36807;&#26500;&#24314;AgentInstruct&#25968;&#25454;&#38598;&#65292;&#24182;&#37319;&#29992;&#19968;&#31181;&#28151;&#21512;&#35757;&#32451;&#26041;&#27861;&#65292;&#20316;&#32773;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#25552;&#39640;LLMs&#20195;&#29702;&#33021;&#21147;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;LLMs&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#24403;&#23427;&#20204;&#20316;&#20026;&#20195;&#29702;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24212;&#23545;&#22797;&#26434;&#20219;&#21153;&#26102;&#65292;&#23427;&#20204;&#36828;&#19981;&#21450;ChatGPT&#21644;GPT-4&#31561;&#21830;&#19994;&#27169;&#22411;&#12290;&#36825;&#20123;&#20195;&#29702;&#20219;&#21153;&#23558;LLMs&#20316;&#20026;&#36127;&#36131;&#35268;&#21010;&#12289;&#35760;&#24518;&#21644;&#24037;&#20855;&#21033;&#29992;&#30340;&#20013;&#22830;&#25511;&#21046;&#22120;&#65292;&#38656;&#35201;&#32454;&#31890;&#24230;&#30340;&#25552;&#31034;&#26041;&#27861;&#21644;&#24378;&#22823;&#30340;LLMs&#25165;&#33021;&#36798;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#25552;&#31034;&#26041;&#27861;&#26469;&#23436;&#25104;&#29305;&#23450;&#30340;&#20195;&#29702;&#20219;&#21153;&#65292;&#20294;&#32570;&#20047;&#30740;&#31350;&#19987;&#27880;&#20110;&#25552;&#39640;LLMs&#33258;&#36523;&#30340;&#20195;&#29702;&#33021;&#21147;&#32780;&#19981;&#25439;&#23475;&#20854;&#36890;&#29992;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AgentTuning&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#21319;LLMs&#30340;&#20195;&#29702;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#36890;&#29992;&#30340;LLM&#33021;&#21147;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;AgentInstruct&#65292;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#39640;&#36136;&#37327;&#30340;&#20132;&#20114;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open large language models (LLMs) with great performance in various tasks have significantly advanced the development of LLMs. However, they are far inferior to commercial models such as ChatGPT and GPT-4 when acting as agents to tackle complex tasks in the real world. These agent tasks employ LLMs as the central controller responsible for planning, memorization, and tool utilization, necessitating both fine-grained prompting methods and robust LLMs to achieve satisfactory performance. Though many prompting methods have been proposed to complete particular agent tasks, there is lack of research focusing on improving the agent capabilities of LLMs themselves without compromising their general abilities. In this work, we present AgentTuning, a simple and general method to enhance the agent abilities of LLMs while maintaining their general LLM capabilities. We construct AgentInstruct, a lightweight instruction-tuning dataset containing high-quality interaction trajectories. We employ a hy
&lt;/p&gt;</description></item><item><title>GestureGPT&#26159;&#19968;&#20010;&#38646;&#26679;&#26412;&#20132;&#20114;&#25163;&#21183;&#29702;&#35299;&#21644;&#23545;&#25509;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#35299;&#35835;&#25163;&#21183;&#25551;&#36848;&#24182;&#26681;&#25454;&#20132;&#20114;&#29615;&#22659;&#25552;&#20379;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#33021;&#22815;&#23558;&#29992;&#25143;&#24847;&#22270;&#23545;&#25509;&#21040;&#20132;&#20114;&#21151;&#33021;&#19978;&#12290;</title><link>http://arxiv.org/abs/2310.12821</link><description>&lt;p&gt;
GestureGPT: &#38646;&#26679;&#26412;&#20132;&#20114;&#25163;&#21183;&#29702;&#35299;&#19982;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#30340;&#23545;&#25509;
&lt;/p&gt;
&lt;p&gt;
GestureGPT: Zero-shot Interactive Gesture Understanding and Grounding with Large Language Model Agents. (arXiv:2310.12821v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12821
&lt;/p&gt;
&lt;p&gt;
GestureGPT&#26159;&#19968;&#20010;&#38646;&#26679;&#26412;&#20132;&#20114;&#25163;&#21183;&#29702;&#35299;&#21644;&#23545;&#25509;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#35299;&#35835;&#25163;&#21183;&#25551;&#36848;&#24182;&#26681;&#25454;&#20132;&#20114;&#29615;&#22659;&#25552;&#20379;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#33021;&#22815;&#23558;&#29992;&#25143;&#24847;&#22270;&#23545;&#25509;&#21040;&#20132;&#20114;&#21151;&#33021;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#25163;&#21183;&#35782;&#21035;&#31995;&#32479;&#20027;&#35201;&#20851;&#27880;&#35782;&#21035;&#39044;&#23450;&#20041;&#38598;&#21512;&#20013;&#30340;&#25163;&#21183;&#65292;&#26410;&#33021;&#23558;&#36825;&#20123;&#25163;&#21183;&#19982;&#20132;&#20114;&#24335;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#20803;&#32032;&#25110;&#31995;&#32479;&#21151;&#33021;&#30456;&#36830;&#25509;&#65288;&#20363;&#22914;&#65292;&#23558;&#8220;&#31446;&#36215;&#22823;&#25287;&#25351;&#8221;&#25163;&#21183;&#19982;&#8220;&#21916;&#27426;&#8221;&#25353;&#38062;&#20851;&#32852;&#36215;&#26469;&#65289;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;GestureGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#38646;&#26679;&#26412;&#25163;&#21183;&#29702;&#35299;&#21644;&#23545;&#25509;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#25163;&#21183;&#25551;&#36848;&#26681;&#25454;&#25163;&#21183;&#35270;&#39057;&#20013;&#30340;&#25163;&#37096;&#20851;&#38190;&#28857;&#22352;&#26631;&#36827;&#34892;&#24418;&#24335;&#21270;&#65292;&#24182;&#36755;&#20837;&#21040;&#25105;&#20204;&#30340;&#21452;&#20195;&#29702;&#23545;&#35805;&#31995;&#32479;&#20013;&#12290;&#19968;&#20010;&#25163;&#21183;&#20195;&#29702;&#35299;&#35835;&#36825;&#20123;&#25551;&#36848;&#65292;&#24182;&#35810;&#38382;&#26377;&#20851;&#20132;&#20114;&#29615;&#22659;&#30340;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#30028;&#38754;&#12289;&#21382;&#21490;&#35760;&#24405;&#12289;&#20957;&#35270;&#25968;&#25454;&#65289;&#65292;&#19968;&#20010;&#19978;&#19979;&#25991;&#20195;&#29702;&#36127;&#36131;&#32452;&#32455;&#24182;&#25552;&#20379;&#36825;&#20123;&#20449;&#24687;&#12290;&#32463;&#36807;&#36845;&#20195;&#30340;&#20132;&#27969;&#65292;&#25163;&#21183;&#20195;&#29702;&#33021;&#22815;&#29702;&#35299;&#29992;&#25143;&#24847;&#22270;&#65292;&#24182;&#23558;&#20854;&#23545;&#25509;&#21040;&#19968;&#20010;&#20132;&#20114;&#21151;&#33021;&#19978;&#12290;&#25105;&#20204;&#20351;&#29992;&#20844;&#24320;&#30340;&#31532;&#19968;&#35270;&#35282;&#21644;&#31532;&#19977;&#35270;&#35282;&#25163;&#21183;&#25968;&#25454;&#38598;&#39564;&#35777;&#20102;&#25163;&#21183;&#25551;&#36848;&#27169;&#22359;&#65292;&#24182;&#22312;&#35270;&#39057;&#27969;&#21644;&#26234;&#33021;&#23478;&#23621;&#29289;&#32852;&#32593;&#25511;&#21046;&#30340;&#20004;&#20010;&#30495;&#23454;&#22330;&#26223;&#20013;&#27979;&#35797;&#20102;&#25972;&#20010;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current gesture recognition systems primarily focus on identifying gestures within a predefined set, leaving a gap in connecting these gestures to interactive GUI elements or system functions (e.g., linking a 'thumb-up' gesture to a 'like' button). We introduce GestureGPT, a novel zero-shot gesture understanding and grounding framework leveraging large language models (LLMs). Gesture descriptions are formulated based on hand landmark coordinates from gesture videos and fed into our dual-agent dialogue system. A gesture agent deciphers these descriptions and queries about the interaction context (e.g., interface, history, gaze data), which a context agent organizes and provides. Following iterative exchanges, the gesture agent discerns user intent, grounding it to an interactive function. We validated the gesture description module using public first-view and third-view gesture datasets and tested the whole system in two real-world settings: video streaming and smart home IoT control. T
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#21319;&#21442;&#25968;&#20849;&#20139;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#25928;&#29575;&#30340;&#31616;&#21333;&#25216;&#26415;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#26469;&#23454;&#29616;&#23436;&#20840;&#25110;&#37096;&#20998;&#20849;&#20139;&#30340;&#27169;&#22411;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#21508;&#31181;&#27169;&#22411;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#20026;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#21442;&#25968;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.12818</link><description>&lt;p&gt;
&#25552;&#21319;&#25512;&#29702;&#25928;&#29575;&#65306;&#37322;&#25918;&#21442;&#25968;&#20849;&#20139;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Boosting Inference Efficiency: Unleashing the Power of Parameter-Shared Pre-trained Language Models. (arXiv:2310.12818v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#21319;&#21442;&#25968;&#20849;&#20139;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#25928;&#29575;&#30340;&#31616;&#21333;&#25216;&#26415;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#26469;&#23454;&#29616;&#23436;&#20840;&#25110;&#37096;&#20998;&#20849;&#20139;&#30340;&#27169;&#22411;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#21508;&#31181;&#27169;&#22411;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#20026;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#21442;&#25968;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#20849;&#20139;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#24050;&#32463;&#25104;&#20026;&#22312;&#36164;&#28304;&#26377;&#38480;&#29615;&#22659;&#20013;&#30340;&#25104;&#21151;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#19981;&#26174;&#33879;&#38477;&#20302;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#27169;&#22411;&#23384;&#20648;&#21644;&#20869;&#23384;&#25104;&#26412;&#30340;&#22823;&#24133;&#38477;&#20302;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#27880;&#24847;&#30340;&#26159;&#65292;&#21442;&#25968;&#20849;&#20139;&#19981;&#33021;&#20943;&#36731;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#35745;&#31639;&#36127;&#25285;&#65292;&#36825;&#20351;&#24471;&#22312;&#20855;&#26377;&#20005;&#26684;&#26102;&#24310;&#35201;&#27714;&#25110;&#35745;&#31639;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#20854;&#23454;&#29992;&#24615;&#21463;&#21040;&#38480;&#21046;&#12290;&#22522;&#20110;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODEs&#65289;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25216;&#26415;&#26469;&#25552;&#39640;&#21442;&#25968;&#20849;&#20139;&#30340;PLMs&#30340;&#25512;&#29702;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#39044;&#35757;&#32451;&#25216;&#26415;&#65292;&#21487;&#20197;&#23454;&#29616;&#23436;&#20840;&#25110;&#37096;&#20998;&#20849;&#20139;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22823;&#30340;&#25512;&#29702;&#21152;&#36895;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#33258;&#22238;&#24402;&#21644;&#33258;&#32534;&#30721;PLMs&#37117;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#65292;&#20026;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#21442;&#25968;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter-shared pre-trained language models (PLMs) have emerged as a successful approach in resource-constrained environments, enabling substantial reductions in model storage and memory costs without significant performance compromise. However, it is important to note that parameter sharing does not alleviate computational burdens associated with inference, thus impeding its practicality in situations characterized by limited stringent latency requirements or computational resources. Building upon neural ordinary differential equations (ODEs), we introduce a straightforward technique to enhance the inference efficiency of parameter-shared PLMs. Additionally, we propose a simple pre-training technique that leads to fully or partially shared models capable of achieving even greater inference acceleration. The experimental results demonstrate the effectiveness of our methods on both autoregressive and autoencoding PLMs, providing novel insights into more efficient utilization of paramet
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#24418;&#24335;&#21270;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#65292;&#24182;&#31995;&#32479;&#21270;&#38450;&#24481;&#36825;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2310.12815</link><description>&lt;p&gt;
LLM-&#38598;&#25104;&#24212;&#29992;&#20013;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#21644;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Prompt Injection Attacks and Defenses in LLM-Integrated Applications. (arXiv:2310.12815v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#24418;&#24335;&#21270;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#65292;&#24182;&#31995;&#32479;&#21270;&#38450;&#24481;&#36825;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20316;&#21508;&#31181;&#31216;&#20026;LLM-&#38598;&#25104;&#24212;&#29992;&#30340;&#23454;&#38469;&#24212;&#29992;&#31243;&#24207;&#30340;&#21518;&#31471;&#12290;&#26368;&#36817;&#30340;&#22810;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;LLM-&#38598;&#25104;&#24212;&#29992;&#23481;&#26131;&#21463;&#21040;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#23041;&#32961;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#23558;&#24694;&#24847;&#25351;&#20196;/&#25968;&#25454;&#27880;&#20837;&#36825;&#20123;&#24212;&#29992;&#31243;&#24207;&#30340;&#36755;&#20837;&#20013;&#65292;&#20197;&#36798;&#21040;&#25915;&#20987;&#32773;&#30340;&#39044;&#26399;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20165;&#38480;&#20110;&#26696;&#20363;&#30740;&#31350;&#65292;&#32570;&#20047;&#23545;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#21450;&#20854;&#38450;&#24481;&#30340;&#31995;&#32479;&#29702;&#35299;&#12290;&#26412;&#35770;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#24418;&#24335;&#21270;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#65292;&#24182;&#23558;&#30740;&#31350;&#35770;&#25991;&#21644;&#21338;&#23458;&#25991;&#31456;&#20013;&#35752;&#35770;&#30340;&#29616;&#26377;&#25915;&#20987;&#35270;&#20026;&#25105;&#20204;&#26694;&#26550;&#30340;&#29305;&#20363;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#32452;&#21512;&#29616;&#26377;&#25915;&#20987;&#35774;&#35745;&#26032;&#30340;&#25915;&#20987;&#26041;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#21270;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#38450;&#24481;&#30340;&#26694;&#26550;&#12290;&#21033;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#21487;&#20197;&#39044;&#38450;&#21644;&#32531;&#35299;&#36825;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are increasingly deployed as the backend for a variety of real-world applications called LLM-Integrated Applications. Multiple recent works showed that LLM-Integrated Applications are vulnerable to prompt injection attacks, in which an attacker injects malicious instruction/data into the input of those applications such that they produce results as the attacker desires. However, existing works are limited to case studies. As a result, the literature lacks a systematic understanding of prompt injection attacks and their defenses. We aim to bridge the gap in this work. In particular, we propose a general framework to formalize prompt injection attacks. Existing attacks, which are discussed in research papers and blog posts, are special cases in our framework. Our framework enables us to design a new attack by combining existing attacks. Moreover, we also propose a framework to systematize defenses against prompt injection attacks. Using our frameworks, we con
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#26799;&#24230;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#21512;&#24182;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#33021;&#22815;&#20943;&#23569;&#26799;&#24230;&#19981;&#21305;&#37197;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#21512;&#24182;&#30340;&#24615;&#33021;&#24182;&#23545;&#36229;&#21442;&#25968;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12808</link><description>&lt;p&gt;
&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#26799;&#24230;&#21305;&#37197;&#30340;&#27169;&#22411;&#21512;&#24182;
&lt;/p&gt;
&lt;p&gt;
Model Merging by Uncertainty-Based Gradient Matching. (arXiv:2310.12808v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#26799;&#24230;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#21512;&#24182;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#33021;&#22815;&#20943;&#23569;&#26799;&#24230;&#19981;&#21305;&#37197;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#21512;&#24182;&#30340;&#24615;&#33021;&#24182;&#23545;&#36229;&#21442;&#25968;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#21442;&#25968;&#30340;&#21152;&#26435;&#24179;&#22343;&#26469;&#21512;&#24182;&#65292;&#20294;&#20026;&#20160;&#20040;&#20250;&#36215;&#20316;&#29992;&#65292;&#20160;&#20040;&#24773;&#20917;&#19979;&#20250;&#22833;&#36133;&#65311;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23558;&#21152;&#26435;&#24179;&#22343;&#30340;&#19981;&#20934;&#30830;&#24615;&#19982;&#26799;&#24230;&#19981;&#21305;&#37197;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#20943;&#23569;&#19981;&#21305;&#37197;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#36825;&#31181;&#32852;&#31995;&#36824;&#25581;&#31034;&#20102;&#20854;&#20182;&#26041;&#26696;&#65288;&#22914;&#24179;&#22343;&#20540;&#12289;&#20219;&#21153;&#31639;&#26415;&#21644;Fisher&#21152;&#26435;&#24179;&#22343;&#65289;&#20013;&#30340;&#38544;&#21547;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#26041;&#38754;&#37117;&#22312;&#24615;&#33021;&#21644;&#36229;&#21442;&#25968;&#40065;&#26834;&#24615;&#26041;&#38754;&#24471;&#21040;&#20102;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models trained on different datasets can be merged by a weighted-averaging of their parameters, but why does it work and when can it fail? Here, we connect the inaccuracy of weighted-averaging to mismatches in the gradients and propose a new uncertainty-based scheme to improve the performance by reducing the mismatch. The connection also reveals implicit assumptions in other schemes such as averaging, task arithmetic, and Fisher-weighted averaging. Our new method gives consistent improvements for large language models and vision transformers, both in terms of performance and robustness to hyperparameters.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#32467;&#26500;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#25991;&#26412;&#20998;&#31867;&#22120;&#22312;&#24212;&#29992;&#20013;&#30340;&#27867;&#21270;&#25928;&#26524;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#23384;&#22312;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#26631;&#31614;&#19982;&#23646;&#24615;&#39044;&#27979;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.12803</link><description>&lt;p&gt;
&#22522;&#20110;&#22240;&#26524;&#32467;&#26500;&#30340;&#25991;&#26412;&#31163;&#32676;&#20540;&#27867;&#21270;&#22686;&#24378;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Causal-structure Driven Augmentations for Text OOD Generalization. (arXiv:2310.12803v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#32467;&#26500;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#25991;&#26412;&#20998;&#31867;&#22120;&#22312;&#24212;&#29992;&#20013;&#30340;&#27867;&#21270;&#25928;&#26524;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#23384;&#22312;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#26631;&#31614;&#19982;&#23646;&#24615;&#39044;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#22120;&#23545;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#20381;&#36182;&#21487;&#33021;&#23548;&#33268;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#27867;&#21270;&#25928;&#26524;&#19981;&#20339;&#65292;&#36825;&#24341;&#21457;&#20102;&#23545;&#20854;&#22312;&#22914;&#21307;&#30103;&#39046;&#22495;&#31561;&#23433;&#20840;&#20851;&#38190;&#34892;&#19994;&#20013;&#20351;&#29992;&#30340;&#25285;&#24551;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22240;&#26524;&#32467;&#26500;&#30693;&#35782;&#25351;&#23548;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#27169;&#25311;&#23545;&#34394;&#20551;&#29305;&#24449;&#36827;&#34892;&#24178;&#39044;&#65292;&#20197;&#23398;&#20064;&#26356;&#21152;&#40065;&#26834;&#30340;&#25991;&#26412;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#26631;&#31614;&#19982;&#23646;&#24615;&#20043;&#38388;&#23384;&#22312;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#39044;&#27979;&#38382;&#39064;&#20013;&#65292;&#36825;&#31181;&#31574;&#30053;&#26159;&#21512;&#36866;&#30340;&#12290;&#22312;&#36825;&#31181;&#38382;&#39064;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#30456;&#23545;&#20110;&#37325;&#35201;&#24615;&#37325;&#21152;&#26435;&#30340;&#26377;&#21033;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;&#23454;&#38469;&#19978;&#65292;&#25105;&#20204;&#20351;&#29992;&#36741;&#21161;&#25968;&#25454;&#36890;&#36807;&#24046;&#20998;&#22312;&#24046;&#20998;&#30340;&#26041;&#27861;&#26469;&#21305;&#37197;&#26679;&#26412;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#34920;&#31034;&#25991;&#26412;&#30340;&#26465;&#20214;&#27010;&#29575;&#12290;&#36890;&#36807;&#23545;&#20174;&#21307;&#23398;&#21465;&#36848;&#20013;&#23398;&#20064;&#19982;&#30475;&#25252;&#32773;&#26080;&#20851;&#30340;&#20020;&#24202;&#35786;&#26029;&#39044;&#27979;&#22120;&#20197;&#21450;&#21322;&#21512;&#25104;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The reliance of text classifiers on spurious correlations can lead to poor generalization at deployment, raising concerns about their use in safety-critical domains such as healthcare. In this work, we propose to use counterfactual data augmentation, guided by knowledge of the causal structure of the data, to simulate interventions on spurious features and to learn more robust text classifiers. We show that this strategy is appropriate in prediction problems where the label is spuriously correlated with an attribute. Under the assumptions of such problems, we discuss the favorable sample complexity of counterfactual data augmentation, compared to importance re-weighting. Pragmatically, we match examples using auxiliary data, based on diff-in-diff methodology, and use a large language model (LLM) to represent a conditional probability of text. Through extensive experimentation on learning caregiver-invariant predictors of clinical diagnoses from medical narratives and on semi-synthetic 
&lt;/p&gt;</description></item><item><title>MolCA&#26159;&#19968;&#20010;&#21487;&#20197;&#36890;&#36807;&#36328;&#27169;&#24577;&#25237;&#24433;&#21644;&#21333;&#27169;&#24577;&#36866;&#37197;&#22120;&#23454;&#29616;&#20998;&#23376;&#22270;&#21644;&#35821;&#35328;&#30340;&#24314;&#27169;&#31995;&#32479;&#12290;&#23427;&#21487;&#20197;&#36890;&#36807;&#36830;&#25509;&#22270;&#32534;&#30721;&#22120;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;&#31354;&#38388;&#26469;&#29702;&#35299;&#25991;&#26412;&#21644;&#22270;&#24418;&#30340;&#20998;&#23376;&#20869;&#23481;&#65292;&#24182;&#36890;&#36807;&#21333;&#27169;&#24577;&#36866;&#37197;&#22120;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#39640;&#25928;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2310.12798</link><description>&lt;p&gt;
MolCA: &#36890;&#36807;&#36328;&#27169;&#24577;&#25237;&#24433;&#21644;&#21333;&#27169;&#24577;&#36866;&#37197;&#22120;&#30340;&#20998;&#23376;&#22270;-&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
MolCA: Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal Adapter. (arXiv:2310.12798v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12798
&lt;/p&gt;
&lt;p&gt;
MolCA&#26159;&#19968;&#20010;&#21487;&#20197;&#36890;&#36807;&#36328;&#27169;&#24577;&#25237;&#24433;&#21644;&#21333;&#27169;&#24577;&#36866;&#37197;&#22120;&#23454;&#29616;&#20998;&#23376;&#22270;&#21644;&#35821;&#35328;&#30340;&#24314;&#27169;&#31995;&#32479;&#12290;&#23427;&#21487;&#20197;&#36890;&#36807;&#36830;&#25509;&#22270;&#32534;&#30721;&#22120;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;&#31354;&#38388;&#26469;&#29702;&#35299;&#25991;&#26412;&#21644;&#22270;&#24418;&#30340;&#20998;&#23376;&#20869;&#23481;&#65292;&#24182;&#36890;&#36807;&#21333;&#27169;&#24577;&#36866;&#37197;&#22120;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#39640;&#25928;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#19982;&#25991;&#26412;&#30456;&#20851;&#30340;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#23545;&#20998;&#23376;&#30340;&#21331;&#36234;&#29702;&#35299;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26412;&#36136;&#19978;&#32570;&#20047;&#20154;&#31867;&#19987;&#19994;&#20154;&#21592;&#22312;&#29702;&#35299;&#20998;&#23376;&#25299;&#25169;&#32467;&#26500;&#20013;&#30340;&#20851;&#38190;&#33021;&#21147; - 2D&#22270;&#24418;&#24863;&#30693;&#33021;&#21147;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#20010;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MolCA: &#36890;&#36807;&#36328;&#27169;&#24577;&#25237;&#24433;&#21644;&#21333;&#27169;&#24577;&#36866;&#37197;&#22120;&#36827;&#34892;&#20998;&#23376;&#22270;-&#35821;&#35328;&#24314;&#27169;&#12290;MolCA&#36890;&#36807;&#36328;&#27169;&#24577;&#25237;&#24433;&#20351;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;Galactica&#65289;&#33021;&#22815;&#29702;&#35299;&#22522;&#20110;&#25991;&#26412;&#21644;&#22270;&#24418;&#30340;&#20998;&#23376;&#20869;&#23481;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36328;&#27169;&#24577;&#25237;&#24433;&#22120;&#34987;&#23454;&#29616;&#20026;&#19968;&#20010;Q-Former&#65292;&#36830;&#25509;&#19968;&#20010;&#22270;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#31354;&#38388;&#21644;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;MolCA&#20351;&#29992;&#21333;&#27169;&#24577;&#36866;&#37197;&#22120;&#65288;&#21363;LoRA&#65289;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#12290;&#19982;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#36328;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#24418;&#32534;&#30721;&#22120;&#32806;&#21512;&#19981;&#21516;&#65292;MolCA&#20445;&#30041;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#25918;&#24335;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#65292;&#24182;&#22686;&#21152;&#20102;2D&#22270;&#24418;&#20449;&#24687;&#12290;&#20026;&#20102;&#23637;&#31034;&#20854;&#26377;&#25928;&#24615;&#65292;
&lt;/p&gt;
&lt;p&gt;
Language Models (LMs) have demonstrated impressive molecule understanding ability on various 1D text-related tasks. However, they inherently lack 2D graph perception - a critical ability of human professionals in comprehending molecules' topological structures. To bridge this gap, we propose MolCA: Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal Adapter. MolCA enables an LM (e.g., Galactica) to understand both text- and graph-based molecular contents via the cross-modal projector. Specifically, the cross-modal projector is implemented as a Q-Former to connect a graph encoder's representation space and an LM's text space. Further, MolCA employs a uni-modal adapter (i.e., LoRA) for the LM's efficient adaptation to downstream tasks. Unlike previous studies that couple an LM with a graph encoder via cross-modal contrastive learning, MolCA retains the LM's ability of open-ended text generation and augments it with 2D graph information. To showcase its effectivenes
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#26126;&#30830;&#23545;&#40784;&#35821;&#35328;&#20043;&#38388;&#30340;&#27010;&#24565;&#23545;&#24212;&#20851;&#31995;&#30340;&#28508;&#21147;&#65292;&#20197;&#24378;&#21270;&#36328;&#35821;&#35328;&#27867;&#21270;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#26080;&#35770;&#26159;&#20165;&#26377;&#32534;&#30721;&#22120;&#36824;&#26159;&#20165;&#26377;&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;&#65292;&#21508;&#35821;&#35328;&#20869;&#30340;&#32467;&#26500;&#27010;&#24565;&#31354;&#38388;&#23545;&#40784;&#24230;&#39640;&#12290;&#36890;&#36807;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#23545;&#40784;&#19981;&#21516;&#35821;&#35328;&#30340;&#27010;&#24565;&#31354;&#38388;&#65292;&#23454;&#29616;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.12794</link><description>&lt;p&gt;
&#32467;&#26500;&#27010;&#24565;&#22312;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#26159;&#21542;&#20855;&#26377;&#26222;&#36866;&#24615;&#65311;&#36208;&#21521;&#21487;&#35299;&#37322;&#30340;&#36328;&#35821;&#35328;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Are Structural Concepts Universal in Transformer Language Models? Towards Interpretable Cross-Lingual Generalization. (arXiv:2310.12794v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#26126;&#30830;&#23545;&#40784;&#35821;&#35328;&#20043;&#38388;&#30340;&#27010;&#24565;&#23545;&#24212;&#20851;&#31995;&#30340;&#28508;&#21147;&#65292;&#20197;&#24378;&#21270;&#36328;&#35821;&#35328;&#27867;&#21270;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#26080;&#35770;&#26159;&#20165;&#26377;&#32534;&#30721;&#22120;&#36824;&#26159;&#20165;&#26377;&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;&#65292;&#21508;&#35821;&#35328;&#20869;&#30340;&#32467;&#26500;&#27010;&#24565;&#31354;&#38388;&#23545;&#40784;&#24230;&#39640;&#12290;&#36890;&#36807;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#23545;&#40784;&#19981;&#21516;&#35821;&#35328;&#30340;&#27010;&#24565;&#31354;&#38388;&#65292;&#23454;&#29616;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#36328;&#35821;&#35328;&#27867;&#21270;&#33021;&#21147;&#65292;&#21363;&#23427;&#20204;&#36890;&#36807;&#38544;&#24335;&#30693;&#35782;&#20256;&#36755;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#36827;&#34892;&#36716;&#31227;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#36716;&#31227;&#23545;&#20110;&#25152;&#26377;&#35821;&#35328;&#32780;&#35328;&#24182;&#19981;&#22343;&#34913;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#65292;&#36825;&#26159;&#19968;&#20010;&#25345;&#32493;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#25105;&#20204;&#26159;&#21542;&#24050;&#32463;&#36798;&#21040;&#20102;&#38544;&#24335;&#36328;&#35821;&#35328;&#27867;&#21270;&#30340;&#26497;&#38480;&#65292;&#24182;&#19988;&#26126;&#30830;&#30340;&#30693;&#35782;&#20256;&#36755;&#26159;&#21542;&#21487;&#34892;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#26126;&#30830;&#23545;&#40784;&#35821;&#35328;&#20043;&#38388;&#27010;&#24565;&#23545;&#24212;&#20851;&#31995;&#30340;&#28508;&#21147;&#65292;&#20197;&#22686;&#24378;&#36328;&#35821;&#35328;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;&#35821;&#27861;&#26041;&#38754;&#20316;&#20026;&#27979;&#35797;&#24179;&#21488;&#65292;&#25105;&#20204;&#23545;43&#31181;&#35821;&#35328;&#30340;&#20998;&#26512;&#26174;&#31034;&#65292;&#26080;&#35770;&#26159;&#20165;&#26377;&#32534;&#30721;&#22120;&#36824;&#26159;&#20165;&#26377;&#35299;&#30721;&#22120;&#30340;LLMs&#65292;&#21508;&#31181;&#35821;&#35328;&#20869;&#30340;&#32467;&#26500;&#27010;&#24565;&#31354;&#38388;&#20043;&#38388;&#23384;&#22312;&#39640;&#24230;&#30340;&#23545;&#20934;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23545;&#40784;&#19981;&#21516;&#35821;&#35328;&#30340;&#27010;&#24565;&#31354;&#38388;&#65292;&#20174;&#32780;&#20415;&#20110;&#22312;&#27010;&#24565;&#20998;&#31867;&#21644;&#23545;&#40784;&#19978;&#36827;&#34892;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have exhibited considerable cross-lingual generalization abilities, whereby they implicitly transfer knowledge across languages. However, the transfer is not equally successful for all languages, especially for low-resource ones, which poses an ongoing challenge. It is unclear whether we have reached the limits of implicit cross-lingual generalization and if explicit knowledge transfer is viable. In this paper, we investigate the potential for explicitly aligning conceptual correspondence between languages to enhance cross-lingual generalization. Using the syntactic aspect of language as a testbed, our analyses of 43 languages reveal a high degree of alignability among the spaces of structural concepts within each language for both encoder-only and decoder-only LLMs. We then propose a meta-learning-based method to learn to align conceptual spaces of different languages, which facilitates zero-shot and few-shot generalization in concept classification and al
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26631;&#31614;&#24863;&#30693;&#30340;&#33258;&#21160;&#35821;&#35328;&#34920;&#36798;&#26041;&#24335;&#65288;LAAV&#65289;&#65292;&#36890;&#36807;&#22686;&#21152;&#25163;&#21160;&#26631;&#31614;&#21644;&#36830;&#25509;&#35789;&#8220;&#21644;&#8221;&#26469;&#25552;&#21319;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#65292;&#20174;&#32780;&#22312;&#23569;&#26679;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.12778</link><description>&lt;p&gt;
&#20351;&#29992;&#26631;&#31614;&#24863;&#30693;&#30340;&#33258;&#21160;&#35821;&#35328;&#34920;&#36798;&#26041;&#24335;&#25552;&#20379;&#23569;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Label-Aware Automatic Verbalizer for Few-Shot Text Classification. (arXiv:2310.12778v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12778
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26631;&#31614;&#24863;&#30693;&#30340;&#33258;&#21160;&#35821;&#35328;&#34920;&#36798;&#26041;&#24335;&#65288;LAAV&#65289;&#65292;&#36890;&#36807;&#22686;&#21152;&#25163;&#21160;&#26631;&#31614;&#21644;&#36830;&#25509;&#35789;&#8220;&#21644;&#8221;&#26469;&#25552;&#21319;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#65292;&#20174;&#32780;&#22312;&#23569;&#26679;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#22312;&#23569;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#20013;&#26174;&#31034;&#20986;&#26377;&#25928;&#24615;&#12290;&#20854;&#25104;&#21151;&#30340;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#26159;&#19968;&#31181;&#35821;&#35328;&#34920;&#36798;&#26041;&#24335;&#65292;&#23558;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#36716;&#21270;&#20026;&#39044;&#27979;&#31867;&#21035;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#26368;&#31616;&#21333;&#21644;&#24191;&#27867;&#35748;&#21487;&#30340;&#35821;&#35328;&#34920;&#36798;&#26041;&#24335;&#20351;&#29992;&#25163;&#21160;&#26631;&#31614;&#26469;&#34920;&#31034;&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#36873;&#25321;&#24182;&#19981;&#33021;&#20445;&#35777;&#22312;&#36873;&#25321;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#26465;&#20214;&#19979;&#25152;&#36873;&#25321;&#30340;&#21333;&#35789;&#30340;&#26368;&#20248;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26631;&#31614;&#24863;&#30693;&#30340;&#33258;&#21160;&#35821;&#35328;&#34920;&#36798;&#26041;&#24335;&#65288;LAAV&#65289;&#65292;&#36890;&#36807;&#26377;&#25928;&#22686;&#21152;&#25163;&#21160;&#26631;&#31614;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#23569;&#26679;&#26412;&#20998;&#31867;&#32467;&#26524;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#25163;&#21160;&#26631;&#31614;&#20197;&#21450;&#36830;&#25509;&#35789;&#8220;&#21644;&#8221;&#26469;&#35825;&#23548;&#27169;&#22411;&#29983;&#25104;&#26356;&#26377;&#25928;&#30340;&#35821;&#35328;&#34920;&#36798;&#26041;&#24335;&#20013;&#30340;&#21333;&#35789;&#12290;&#22312;&#20116;&#20010;&#25968;&#25454;&#38598;&#36328;&#20116;&#31181;&#35821;&#35328;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LAAV&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#35821;&#35328;&#34920;&#36798;&#26041;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#36824;&#21457;&#29616;&#65292;&#19982;&#31867;&#20284;&#26041;&#27861;&#30456;&#27604;&#65292;LAAV&#25552;&#20379;&#26356;&#30456;&#20851;&#30340;&#21333;&#35789;&#65292;&#29305;&#21035;&#26159;&#22312;&#20013;&#21040;
&lt;/p&gt;
&lt;p&gt;
Prompt-based learning has shown its effectiveness in few-shot text classification. One important factor in its success is a verbalizer, which translates output from a language model into a predicted class. Notably, the simplest and widely acknowledged verbalizer employs manual labels to represent the classes. However, manual selection does not guarantee the optimality of the selected words when conditioned on the chosen language model. Therefore, we propose Label-Aware Automatic Verbalizer (LAAV), effectively augmenting the manual labels to achieve better few-shot classification results. Specifically, we use the manual labels along with the conjunction "and" to induce the model to generate more effective words for the verbalizer. The experimental results on five datasets across five languages demonstrate that LAAV significantly outperforms existing verbalizers. Furthermore, our analysis reveals that LAAV suggests more relevant words compared to similar approaches, especially in mid-to-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ClaPS&#30340;&#31616;&#21333;&#40657;&#30418;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#32858;&#31867;&#21644;&#20462;&#21098;&#25628;&#32034;&#31354;&#38388;&#20013;&#26368;&#26377;&#24433;&#21709;&#21147;&#30340;&#25552;&#31034;&#20196;&#29260;&#65292;&#35299;&#20915;&#20102;&#29616;&#20195;&#40657;&#30418;&#26041;&#27861;&#20013;&#30340;&#25928;&#29575;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.12774</link><description>&lt;p&gt;
&#23384;&#27963;&#26368;&#26377;&#24433;&#21709;&#21147;&#30340;&#25552;&#31034;&#65306;&#36890;&#36807;&#32858;&#31867;&#21644;&#20462;&#21098;&#23454;&#29616;&#39640;&#25928;&#30340;&#40657;&#30418;&#25552;&#31034;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Survival of the Most Influential Prompts: Efficient Black-Box Prompt Search via Clustering and Pruning. (arXiv:2310.12774v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ClaPS&#30340;&#31616;&#21333;&#40657;&#30418;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#32858;&#31867;&#21644;&#20462;&#21098;&#25628;&#32034;&#31354;&#38388;&#20013;&#26368;&#26377;&#24433;&#21709;&#21147;&#30340;&#25552;&#31034;&#20196;&#29260;&#65292;&#35299;&#20915;&#20102;&#29616;&#20195;&#40657;&#30418;&#26041;&#27861;&#20013;&#30340;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26377;&#25928;&#33539;&#20363;&#65292;&#20351;&#24471;&#23569;&#26679;&#26412;&#29978;&#33267;&#38646;&#26679;&#26412;&#23398;&#20064;&#25104;&#20026;&#21487;&#33021;&#12290;&#26368;&#36817;&#65292;&#40657;&#30418;&#25552;&#31034;&#25628;&#32034;&#22240;&#20854;&#26799;&#24230;-free&#20248;&#21270;&#30340;&#29420;&#29305;&#29305;&#24615;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#34987;&#35777;&#26126;&#22312;&#27169;&#22411;&#21363;&#26381;&#21153;&#30340;&#20351;&#29992;&#20013;&#29305;&#21035;&#26377;&#29992;&#21644;&#24378;&#22823;&#12290;&#28982;&#32780;&#65292;&#32452;&#21512;&#20248;&#21270;&#30340;&#31163;&#25955;&#26412;&#36136;&#21644;&#22797;&#26434;&#24615;&#38459;&#30861;&#20102;&#29616;&#20195;&#40657;&#30418;&#26041;&#27861;&#30340;&#25928;&#29575;&#12290;&#23613;&#31649;&#22312;&#25628;&#32034;&#31639;&#27861;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#25628;&#32034;&#31354;&#38388;&#35774;&#35745;&#21644;&#20248;&#21270;&#30340;&#20851;&#38190;&#26041;&#38754;&#21364;&#34987;&#22823;&#37096;&#20998;&#24573;&#35270;&#20102;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#25552;&#31034;LLM&#36827;&#34892;&#25935;&#24863;&#24615;&#20998;&#26512;&#65292;&#25581;&#31034;&#21482;&#26377;&#23569;&#37327;&#30340;&#20196;&#29260;&#23545;LLM&#39044;&#27979;&#20135;&#29983;&#20102;&#19981;&#25104;&#27604;&#20363;&#30340;&#24433;&#21709;&#12290;&#21033;&#29992;&#36825;&#19968;&#27934;&#35265;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Clustering and Pruning for Efficient Black-box Prompt Search&#65288;ClaPS&#65289;&#30340;&#31616;&#21333;&#40657;&#30418;&#25628;&#32034;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#39318;&#20808;&#23545;&#25628;&#32034;&#31354;&#38388;&#36827;&#34892;&#32858;&#31867;&#21644;&#20462;&#21098;&#65292;&#21482;&#20851;&#27880;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#25552;&#31034;&#20196;&#29260;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt-based learning has been an effective paradigm for large pretrained language models (LLM), enabling few-shot or even zero-shot learning. Black-box prompt search has received growing interest recently for its distinctive properties of gradient-free optimization, proven particularly useful and powerful for model-as-a-service usage. However, the discrete nature and the complexity of combinatorial optimization hinder the efficiency of modern black-box approaches. Despite extensive research on search algorithms, the crucial aspect of search space design and optimization has been largely overlooked. In this paper, we first conduct a sensitivity analysis by prompting LLM, revealing that only a small number of tokens exert a disproportionate amount of influence on LLM predictions. Leveraging this insight, we propose the Clustering and Pruning for Efficient Black-box Prompt Search (ClaPS), a simple black-box search method that first clusters and prunes the search space to focus exclusivel
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Transformer-based&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23454;&#20307;&#27861;&#24459;&#24418;&#24335;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#27604;&#36739;&#20013;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#24615;&#33021;&#24182;&#24471;&#21040;&#20102;&#31532;&#19977;&#26041;&#35780;&#23457;&#30340;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2310.12766</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#23454;&#20307;&#27861;&#24459;&#24418;&#24335;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Transformer-based Entity Legal Form Classification. (arXiv:2310.12766v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12766
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Transformer-based&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23454;&#20307;&#27861;&#24459;&#24418;&#24335;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#27604;&#36739;&#20013;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#24615;&#33021;&#24182;&#24471;&#21040;&#20102;&#31532;&#19977;&#26041;&#35780;&#23457;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#23545;&#21407;&#22987;&#27861;&#24459;&#23454;&#20307;&#21517;&#31216;&#36827;&#34892;&#23454;&#20307;&#27861;&#24459;&#24418;&#24335;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#21508;&#31181;BERT&#21464;&#31181;&#65292;&#24182;&#23558;&#23427;&#20204;&#30340;&#24615;&#33021;&#19982;&#22810;&#20010;&#20256;&#32479;&#22522;&#20934;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#28085;&#30422;&#20102;&#19968;&#20010;&#24222;&#22823;&#30340;&#33258;&#30001;&#21487;&#29992;&#30340;&#27861;&#24459;&#23454;&#20307;&#26631;&#35782;&#31526;&#65288;LEI&#65289;&#25968;&#25454;&#23376;&#38598;&#65292;&#21253;&#25324;&#26469;&#33258;30&#20010;&#19981;&#21516;&#27861;&#24459;&#21496;&#27861;&#36758;&#21306;&#30340;&#36229;&#36807;110&#19975;&#20010;&#27861;&#24459;&#23454;&#20307;&#12290;&#27599;&#20010;&#21496;&#27861;&#36758;&#21306;&#30340;&#20998;&#31867;&#30340;&#30495;&#23454;&#26631;&#31614;&#26469;&#33258;&#23454;&#20307;&#27861;&#24459;&#24418;&#24335;&#65288;ELF&#65289;&#20195;&#30721;&#26631;&#20934;&#65288;ISO 20275&#65289;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;BERT&#21464;&#31181;&#22312;F1&#20998;&#25968;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#65292;&#22312;Macro F1&#20998;&#25968;&#26041;&#38754;&#34920;&#29616;&#30456;&#24403;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#25552;&#26696;&#24471;&#21040;&#20102;&#22312;&#21313;&#20010;&#36873;&#25321;&#30340;&#21496;&#27861;&#36758;&#21306;&#36827;&#34892;&#30340;&#31532;&#19977;&#26041;&#19987;&#23478;&#35780;&#23457;&#30340;&#25903;&#25345;&#12290;&#26412;&#30740;&#31350;&#20984;&#26174;&#20102;&#22522;&#20110;Transformer&#27169;&#22411;&#22312;&#25512;&#36827;&#25968;&#25454;&#26631;&#20934;&#21270;&#26041;&#38754;&#30340;&#37325;&#35201;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the application of Transformer-based language models for classifying entity legal forms from raw legal entity names. Specifically, we employ various BERT variants and compare their performance against multiple traditional baselines. Our evaluation encompasses a substantial subset of freely available Legal Entity Identifier (LEI) data, comprising over 1.1 million legal entities from 30 different legal jurisdictions. The ground truth labels for classification per jurisdiction are taken from the Entity Legal Form (ELF) code standard (ISO 20275). Our findings demonstrate that pre-trained BERT variants outperform traditional text classification approaches in terms of F1 score, while also performing comparably well in the Macro F1 Score. Moreover, the validity of our proposal is supported by the outcome of third-party expert reviews conducted in ten selected jurisdictions. This study highlights the significant potential of Transformer-based models in advancing data standardization
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#20013;&#25991;&#36827;&#34892;&#23383;&#31526;&#26631;&#35760;&#21270;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#27604;Transformer&#26356;&#20855;&#35299;&#37322;&#24615;&#30340;&#20013;&#25991;&#32972;&#21253;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23398;&#20064;&#21040;&#20102;&#20016;&#23500;&#30340;&#23383;&#31526;&#32423;&#21035;&#21547;&#20041;&#65292;&#20854;&#32452;&#21512;&#24418;&#25104;&#35789;&#20041;&#65292;&#24182;&#19988;&#22312;&#35789;&#27719;&#35821;&#20041;&#35780;&#20272;&#20013;&#32988;&#36807;Transformer&#30340;&#36755;&#20837;&#23884;&#20837;&#12290;</title><link>http://arxiv.org/abs/2310.12751</link><description>&lt;p&gt;
&#23383;&#31526;&#32423;&#20013;&#25991;&#32972;&#21253;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Character-level Chinese Backpack Language Models. (arXiv:2310.12751v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12751
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#20013;&#25991;&#36827;&#34892;&#23383;&#31526;&#26631;&#35760;&#21270;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#27604;Transformer&#26356;&#20855;&#35299;&#37322;&#24615;&#30340;&#20013;&#25991;&#32972;&#21253;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23398;&#20064;&#21040;&#20102;&#20016;&#23500;&#30340;&#23383;&#31526;&#32423;&#21035;&#21547;&#20041;&#65292;&#20854;&#32452;&#21512;&#24418;&#25104;&#35789;&#20041;&#65292;&#24182;&#19988;&#22312;&#35789;&#27719;&#35821;&#20041;&#35780;&#20272;&#20013;&#32988;&#36807;Transformer&#30340;&#36755;&#20837;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#21253;&#26159;&#19968;&#31181;Transformer&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#39044;&#27979;&#20998;&#35299;&#20026;&#20196;&#29260;&#24847;&#20041;&#32452;&#20214;&#30340;&#21152;&#26435;&#21644;&#26469;&#25552;&#39640;&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#32972;&#21253;&#23545;&#20110;&#38500;&#33521;&#35821;&#20197;&#22806;&#30340;&#20854;&#20182;&#35821;&#35328;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#35789;&#39033;&#30340;&#23376;&#35789;&#26631;&#35760;&#21270;&#25552;&#20379;&#21512;&#29702;&#36817;&#20284;&#30340;&#35821;&#35328;&#32780;&#35328;&#65292;&#23427;&#20204;&#23545;&#20196;&#29260;&#23450;&#20041;&#30340;&#21547;&#20041;&#30340;&#20381;&#36182;&#24615;&#24341;&#21457;&#20102;&#19968;&#20123;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35757;&#32451;&#12289;&#35780;&#20272;&#12289;&#35299;&#37322;&#21644;&#25511;&#21046;&#23383;&#31526;&#26631;&#35760;&#21270;&#30340;&#20013;&#25991;&#32972;&#21253;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#20013;&#35789;&#36890;&#24120;&#30001;&#22810;&#20010;&#23383;&#31526;&#32452;&#25104;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25105;&#20204;&#30340;&#65288;134M&#21442;&#25968;&#65289;&#20013;&#25991;&#32972;&#21253;&#35821;&#35328;&#27169;&#22411;&#19982;&#65288;104M&#21442;&#25968;&#65289;Transformer&#30456;&#27604;&#34920;&#29616;&#30456;&#24403;&#65292;&#24182;&#23398;&#20064;&#21040;&#20102;&#20016;&#23500;&#30340;&#23383;&#31526;&#32423;&#21035;&#21547;&#20041;&#65292;&#36825;&#20123;&#21547;&#20041;&#20197;&#23545;&#25968;&#21152;&#27861;&#32452;&#21512;&#25104;&#35789;&#20041;&#12290;&#22312;&#31867;&#20284;SimLex&#30340;&#35789;&#27719;&#35821;&#20041;&#35780;&#20272;&#20013;&#65292;&#32972;&#21253;&#23383;&#31526;&#24847;&#20041;&#30340;&#31616;&#21333;&#24179;&#22343;&#20540;&#32988;&#36807;Transformer&#30340;&#36755;&#20837;&#23884;&#20837;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22797;&#26434;&#30340;&#22810;&#23383;&#31526;&#21547;&#20041;&#36890;&#24120;&#26159;&#36890;&#36807;&#20351;&#29992;...
&lt;/p&gt;
&lt;p&gt;
The Backpack is a Transformer alternative shown to improve interpretability in English language modeling by decomposing predictions into a weighted sum of token sense components. However, Backpacks' reliance on token-defined meaning raises questions as to their potential for languages other than English, a language for which subword tokenization provides a reasonable approximation for lexical items. In this work, we train, evaluate, interpret, and control Backpack language models in character-tokenized Chinese, in which words are often composed of many characters. We find that our (134M parameter) Chinese Backpack language model performs comparably to a (104M parameter) Transformer, and learns rich character-level meanings that log-additively compose to form word meanings. In SimLex-style lexical semantic evaluations, simple averages of Backpack character senses outperform input embeddings from a Transformer. We find that complex multi-character meanings are often formed by using the s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#35821;&#35328;&#37325;&#26500;&#20013;&#34920;&#31034;&#21644;&#35745;&#31639;&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#20511;&#37492;&#30417;&#30563;&#38899;&#38901;&#37325;&#26500;&#21644;&#33258;&#21160;&#39044;&#27979;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#27169;&#31946;&#37325;&#26500;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;</title><link>http://arxiv.org/abs/2310.12727</link><description>&lt;p&gt;
&#22312;&#38899;&#38901;&#37325;&#26500;&#20013;&#34920;&#31034;&#21644;&#35745;&#31639;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Representing and Computing Uncertainty in Phonological Reconstruction. (arXiv:2310.12727v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12727
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#35821;&#35328;&#37325;&#26500;&#20013;&#34920;&#31034;&#21644;&#35745;&#31639;&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#20511;&#37492;&#30417;&#30563;&#38899;&#38901;&#37325;&#26500;&#21644;&#33258;&#21160;&#39044;&#27979;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#27169;&#31946;&#37325;&#26500;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21382;&#21490;&#35821;&#35328;&#23398;&#20013;&#30340;&#37325;&#26500;&#26412;&#36136;&#19978;&#26159;&#27169;&#31946;&#30340;&#65292;&#22823;&#22810;&#25968;&#23398;&#32773;&#22312;&#25552;&#20986;&#21407;&#22411;&#24418;&#24335;&#26102;&#24182;&#19981;&#34920;&#31034;&#19981;&#30830;&#23450;&#24615;&#12290;&#38543;&#30528;&#33258;&#21160;&#21270;&#20256;&#32479;&#27604;&#36739;&#26041;&#27861;&#26576;&#20123;&#26041;&#38754;&#30340;&#26368;&#36817;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#25104;&#21151;&#65292;&#21407;&#22411;&#24418;&#24335;&#30340;&#24418;&#24335;&#21270;&#20063;&#24471;&#21040;&#20102;&#25913;&#21892;&#12290;&#36825;&#31181;&#24418;&#24335;&#21270;&#20351;&#24471;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#34920;&#31034;&#21644;&#35745;&#31639;&#19981;&#30830;&#23450;&#24615;&#12290;&#22522;&#20110;&#30417;&#30563;&#38899;&#38901;&#37325;&#26500;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20854;&#20013;&#31639;&#27861;&#36890;&#36807;&#20808;&#21069;&#27880;&#37322;&#30340;&#25968;&#25454;&#23398;&#20064;&#22914;&#20309;&#37325;&#26500;&#32473;&#23450;&#21407;&#22987;&#35821;&#35328;&#20013;&#30340;&#21333;&#35789;&#65292;&#24182;&#21463;&#21040;&#26469;&#33258;&#21516;&#28304;&#35789;&#38598;&#33258;&#21160;&#39044;&#27979;&#30340;&#25913;&#36827;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#35821;&#35328;&#37325;&#26500;&#20013;&#34920;&#31034;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#21253;&#25324;&#20174;&#35821;&#35328;&#25968;&#25454;&#35745;&#31639;&#27169;&#31946;&#37325;&#26500;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the inherently fuzzy nature of reconstructions in historical linguistics, most scholars do not represent their uncertainty when proposing proto-forms. With the increasing success of recently proposed approaches to automating certain aspects of the traditional comparative method, the formal representation of proto-forms has also improved. This formalization makes it possible to address both the representation and the computation of uncertainty. Building on recent advances in supervised phonological reconstruction, during which an algorithm learns how to reconstruct words in a given proto-language relying on previously annotated data, and inspired by improved methods for automated word prediction from cognate sets, we present a new framework that allows for the representation of uncertainty in linguistic reconstruction and also includes a workflow for the computation of fuzzy reconstructions from linguistic data.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#37329;&#34701;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;&#34429;&#28982;&#19968;&#20123;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#38646;&#26679;&#26412;&#25552;&#31034;&#22312;&#22823;&#22810;&#25968;&#37329;&#34701;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#30528;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#24635;&#20307;&#19978;&#33853;&#21518;&#20110;&#32463;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;&#19987;&#19994;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#19987;&#26377;&#25968;&#25454;&#38598;&#26102;&#12290;</title><link>http://arxiv.org/abs/2310.12664</link><description>&lt;p&gt;
ChatGPT&#26159;&#37329;&#34701;&#19987;&#23478;&#21527;&#65311;&#23545;&#37329;&#34701;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Is ChatGPT a Financial Expert? Evaluating Language Models on Financial Natural Language Processing. (arXiv:2310.12664v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#37329;&#34701;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;&#34429;&#28982;&#19968;&#20123;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#38646;&#26679;&#26412;&#25552;&#31034;&#22312;&#22823;&#22810;&#25968;&#37329;&#34701;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#30528;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#24635;&#20307;&#19978;&#33853;&#21518;&#20110;&#32463;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;&#19987;&#19994;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#19987;&#26377;&#25968;&#25454;&#38598;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#65292;&#22914;ChatGPT&#65292;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#19968;&#33324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#37329;&#34701;&#39046;&#22495;&#30340;&#19987;&#38271;&#32570;&#20047;&#20840;&#38754;&#35780;&#20272;&#12290;&#20026;&#20102;&#35780;&#20272;LLMs&#35299;&#20915;&#37329;&#34701;NLP&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FinLMEval&#65292;&#19968;&#20010;&#37329;&#34701;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#26694;&#26550;&#65292;&#21253;&#25324;&#20061;&#20010;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#25968;&#25454;&#38598;&#12290;&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20165;&#32534;&#30721;&#22120;&#21644;&#20165;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#19968;&#20123;&#20165;&#35299;&#30721;&#22120;&#30340;LLMs&#36890;&#36807;&#38646;&#26679;&#26412;&#25552;&#31034;&#22312;&#22823;&#22810;&#25968;&#37329;&#34701;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#30528;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#24635;&#20307;&#19978;&#33853;&#21518;&#20110;&#32463;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;&#19987;&#19994;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#19987;&#26377;&#25968;&#25454;&#38598;&#26102;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#39033;&#30740;&#31350;&#20026;&#32487;&#32493;&#21162;&#21147;&#26500;&#24314;&#26356;&#20808;&#36827;&#30340;&#37329;&#34701;&#39046;&#22495;LLMs&#25552;&#20379;&#22522;&#30784;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of Large Language Models (LLMs), such as ChatGPT, has revolutionized general natural language preprocessing (NLP) tasks. However, their expertise in the financial domain lacks a comprehensive evaluation. To assess the ability of LLMs to solve financial NLP tasks, we present FinLMEval, a framework for Financial Language Model Evaluation, comprising nine datasets designed to evaluate the performance of language models. This study compares the performance of encoder-only language models and the decoder-only language models. Our findings reveal that while some decoder-only LLMs demonstrate notable performance across most financial tasks via zero-shot prompting, they generally lag behind the fine-tuned expert models, especially when dealing with proprietary datasets. We hope this study provides foundation evaluations for continuing efforts to build more advanced LLMs in the financial domain.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#38754;&#21521;&#30495;&#23454;&#19990;&#30028;&#30340;&#28151;&#21512;&#35821;&#38899;&#27969;&#24335;&#32763;&#35793;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20027;&#35201;&#20851;&#27880;&#20102;&#27969;&#24335;&#35774;&#32622;&#21644;&#32763;&#35793;&#21040;&#31532;&#19977;&#31181;&#35821;&#35328;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#32447;&#27169;&#22411;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2310.12648</link><description>&lt;p&gt;
&#38754;&#21521;&#29616;&#23454;&#19990;&#30028;&#30340;&#28151;&#21512;&#35821;&#38899;&#27969;&#24335;&#32763;&#35793;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Real-World Streaming Speech Translation for Code-Switched Speech. (arXiv:2310.12648v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#38754;&#21521;&#30495;&#23454;&#19990;&#30028;&#30340;&#28151;&#21512;&#35821;&#38899;&#27969;&#24335;&#32763;&#35793;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20027;&#35201;&#20851;&#27880;&#20102;&#27969;&#24335;&#35774;&#32622;&#21644;&#32763;&#35793;&#21040;&#31532;&#19977;&#31181;&#35821;&#35328;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#32447;&#27169;&#22411;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#35821;&#35328;&#65288;CS&#65289;&#21363;&#22312;&#19968;&#21477;&#35805;&#20013;&#28151;&#21512;&#20351;&#29992;&#19981;&#21516;&#35821;&#35328;&#65292;&#26159;&#36890;&#20449;&#20013;&#24120;&#35265;&#30340;&#29616;&#35937;&#65292;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#29615;&#22659;&#19979;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20808;&#21069;&#20851;&#20110;CS&#35821;&#38899;&#30340;&#30740;&#31350;&#22312;&#31471;&#21040;&#31471;&#35821;&#38899;&#32763;&#35793;&#65288;ST&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#20165;&#38480;&#20110;&#31163;&#32447;&#22330;&#26223;&#65292;&#24182;&#19988;&#20165;&#33021;&#32763;&#35793;&#25104;&#28304;&#35821;&#20013;&#30340;&#19968;&#31181;&#35821;&#35328;&#65288;&#21333;&#35821;&#36716;&#24405;&#65289;&#12290;&#26412;&#25991;&#30528;&#37325;&#30740;&#31350;&#20102;&#20004;&#20010;&#23578;&#26410;&#25506;&#32034;&#30340;&#38754;&#21521;&#29616;&#23454;&#19990;&#30028;&#30340;CS&#35821;&#38899;&#32763;&#35793;&#39046;&#22495;&#65306;&#27969;&#24335;&#35774;&#32622;&#21644;&#32763;&#35793;&#21040;&#31532;&#19977;&#31181;&#35821;&#35328;&#65288;&#21363;&#28304;&#35821;&#20013;&#26410;&#21253;&#21547;&#30340;&#35821;&#35328;&#65289;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;Fisher&#21644;Miami&#27979;&#35797;&#21644;&#39564;&#35777;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;&#35199;&#29677;&#29273;&#35821;&#21644;&#24503;&#35821;&#20316;&#20026;&#26032;&#30340;&#30446;&#26631;&#35821;&#35328;&#12290;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#65292;&#25105;&#20204;&#23545;&#31163;&#32447;&#21644;&#27969;&#24335;ST&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#24314;&#31435;&#20102;&#20043;&#21069;&#25552;&#21040;&#30340;&#20004;&#20010;&#35774;&#32622;&#30340;&#22522;&#32447;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code-switching (CS), i.e. mixing different languages in a single sentence, is a common phenomenon in communication and can be challenging in many Natural Language Processing (NLP) settings. Previous studies on CS speech have shown promising results for end-to-end speech translation (ST), but have been limited to offline scenarios and to translation to one of the languages present in the source (\textit{monolingual transcription}).  In this paper, we focus on two essential yet unexplored areas for real-world CS speech translation: streaming settings, and translation to a third language (i.e., a language not included in the source). To this end, we extend the Fisher and Miami test and validation datasets to include new targets in Spanish and German. Using this data, we train a model for both offline and streaming ST and we establish baseline results for the two settings mentioned earlier.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#30340;&#21477;&#23376;&#25490;&#24207;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#21477;&#23376;&#20043;&#38388;&#30340;&#21452;&#21521;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#24182;&#34892;&#39044;&#27979;&#27599;&#20010;&#20301;&#32622;&#30340;&#21477;&#23376;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#22312;&#25490;&#24207;&#20219;&#21153;&#20013;&#21482;&#33021;&#21033;&#29992;&#21333;&#21521;&#20381;&#36182;&#20851;&#31995;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.12640</link><description>&lt;p&gt;
&#38750;&#33258;&#22238;&#24402;&#21477;&#23376;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Non-Autoregressive Sentence Ordering. (arXiv:2310.12640v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12640
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#30340;&#21477;&#23376;&#25490;&#24207;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#21477;&#23376;&#20043;&#38388;&#30340;&#21452;&#21521;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#24182;&#34892;&#39044;&#27979;&#27599;&#20010;&#20301;&#32622;&#30340;&#21477;&#23376;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#22312;&#25490;&#24207;&#20219;&#21153;&#20013;&#21482;&#33021;&#21033;&#29992;&#21333;&#21521;&#20381;&#36182;&#20851;&#31995;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#21477;&#23376;&#25490;&#24207;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#19982;&#25351;&#38024;&#32593;&#32476;&#65292;&#36890;&#36807;&#36880;&#27493;&#39044;&#27979;&#27599;&#20010;&#21477;&#23376;&#26469;&#24674;&#22797;&#36830;&#36143;&#24615;&#12290;&#36825;&#31181;&#33258;&#22238;&#24402;&#26041;&#24335;&#21482;&#33021;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#21033;&#29992;&#21333;&#21521;&#20381;&#36182;&#20851;&#31995;&#65292;&#26080;&#27861;&#20805;&#20998;&#25506;&#32034;&#21477;&#23376;&#20043;&#38388;&#30340;&#35821;&#20041;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#25490;&#24207;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#33258;&#22238;&#24402;&#25490;&#24207;&#32593;&#32476;&#65292;&#21517;&#20026;NAON&#65292;&#23427;&#25506;&#32034;&#20102;&#21477;&#23376;&#20043;&#38388;&#30340;&#21452;&#21521;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#24182;&#34892;&#39044;&#27979;&#27599;&#20010;&#20301;&#32622;&#30340;&#21477;&#23376;&#12290;&#25105;&#20204;&#35748;&#20026;&#38750;&#33258;&#22238;&#24402;&#26041;&#24335;&#19981;&#20165;&#36866;&#29992;&#20110;&#21477;&#23376;&#25490;&#24207;&#20219;&#21153;&#65292;&#32780;&#19988;&#29305;&#21035;&#36866;&#29992;&#65292;&#21407;&#22240;&#26377;&#20004;&#20010;&#65306;1&#65289;&#27599;&#20010;&#29983;&#25104;&#30446;&#26631;&#20855;&#26377;&#30830;&#23450;&#21270;&#30340;&#38271;&#24230;&#65292;2&#65289;&#21477;&#23376;&#21644;&#20301;&#32622;&#24212;&#35813;&#23436;&#20840;&#21305;&#37197;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35299;&#20915;&#26420;&#32032;&#30340;&#38750;&#33258;&#22238;&#24402;Transformer&#30340;&#37325;&#22797;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#25490;&#20182;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing sentence ordering approaches generally employ encoder-decoder frameworks with the pointer net to recover the coherence by recurrently predicting each sentence step-by-step. Such an autoregressive manner only leverages unilateral dependencies during decoding and cannot fully explore the semantic dependency between sentences for ordering. To overcome these limitations, in this paper, we propose a novel Non-Autoregressive Ordering Network, dubbed \textit{NAON}, which explores bilateral dependencies between sentences and predicts the sentence for each position in parallel. We claim that the non-autoregressive manner is not just applicable but also particularly suitable to the sentence ordering task because of two peculiar characteristics of the task: 1) each generation target is in deterministic length, and 2) the sentences and positions should match exclusively. Furthermore, to address the repetition issue of the naive non-autoregressive Transformer, we introduce an exclusive los
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#37329;&#34701;&#24773;&#32490;&#20998;&#31867;&#20013;&#23384;&#22312;&#30340;&#26102;&#38388;&#25968;&#25454;&#20998;&#24067;&#36716;&#31227;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#30495;&#23454;&#37329;&#34701;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#21457;&#29616;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#22312;&#26102;&#38388;&#20998;&#24067;&#36716;&#31227;&#30340;&#24773;&#20917;&#19979;&#24615;&#33021;&#19979;&#38477;&#12290;&#38024;&#23545;&#37329;&#34701;&#25991;&#26412;&#30340;&#29420;&#29305;&#26102;&#38388;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#31163;&#32676;&#28857;&#26816;&#27979;&#21644;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#22312;&#27874;&#21160;&#24615;&#37329;&#34701;&#24066;&#22330;&#20013;&#36866;&#24212;&#26102;&#38388;&#20998;&#24067;&#36716;&#31227;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.12620</link><description>&lt;p&gt;
&#39044;&#27979;&#26410;&#26469;&#26159;&#36807;&#21435;&#30340;&#24310;&#32493;&#65311;&#20851;&#20110;&#37329;&#34701;&#24773;&#32490;&#20998;&#31867;&#20013;&#30340;&#26102;&#38388;&#25968;&#25454;&#20998;&#24067;&#36716;&#31227;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Predict the Future from the Past? On the Temporal Data Distribution Shift in Financial Sentiment Classifications. (arXiv:2310.12620v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#37329;&#34701;&#24773;&#32490;&#20998;&#31867;&#20013;&#23384;&#22312;&#30340;&#26102;&#38388;&#25968;&#25454;&#20998;&#24067;&#36716;&#31227;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#30495;&#23454;&#37329;&#34701;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#21457;&#29616;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#22312;&#26102;&#38388;&#20998;&#24067;&#36716;&#31227;&#30340;&#24773;&#20917;&#19979;&#24615;&#33021;&#19979;&#38477;&#12290;&#38024;&#23545;&#37329;&#34701;&#25991;&#26412;&#30340;&#29420;&#29305;&#26102;&#38388;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#31163;&#32676;&#28857;&#26816;&#27979;&#21644;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#22312;&#27874;&#21160;&#24615;&#37329;&#34701;&#24066;&#22330;&#20013;&#36866;&#24212;&#26102;&#38388;&#20998;&#24067;&#36716;&#31227;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#25991;&#26412;&#20013;&#23384;&#22312;&#30528;&#26102;&#38388;&#25968;&#25454;&#20998;&#24067;&#36716;&#31227;&#30340;&#38382;&#39064;&#12290;&#22312;&#27874;&#21160;&#24615;&#24066;&#22330;&#29615;&#22659;&#19979;&#65292;&#22914;&#20309;&#35757;&#32451;&#19968;&#20010;&#33021;&#22815;&#20934;&#30830;&#25512;&#26029;&#24773;&#32490;&#24182;&#23545;&#26102;&#38388;&#25968;&#25454;&#20998;&#24067;&#36716;&#31227;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#37329;&#34701;&#24773;&#32490;&#20998;&#26512;&#31995;&#32479;&#65311;&#26412;&#25991;&#21033;&#29992;&#36328;&#36234;&#19977;&#24180;&#30340;&#30495;&#23454;&#37329;&#34701;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#38598;&#23545;&#37329;&#34701;&#24773;&#32490;&#20998;&#26512;&#31995;&#32479;&#22312;&#26102;&#38388;&#25968;&#25454;&#20998;&#24067;&#36716;&#31227;&#19979;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#22312;&#26102;&#38388;&#20998;&#24067;&#36716;&#31227;&#30340;&#24773;&#20917;&#19979;&#20250;&#20986;&#29616;&#24615;&#33021;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#21463;&#37329;&#34701;&#25991;&#26412;&#30340;&#29420;&#29305;&#26102;&#38388;&#29305;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#31163;&#32676;&#28857;&#26816;&#27979;&#21644;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#36827;&#34892;&#37329;&#34701;&#24773;&#32490;&#20998;&#26512;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;&#27169;&#22411;&#22312;&#27874;&#21160;&#24615;&#37329;&#34701;&#24066;&#22330;&#20013;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#26102;&#38388;&#20998;&#24067;&#36716;&#31227;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal data distribution shift is prevalent in the financial text. How can a financial sentiment analysis system be trained in a volatile market environment that can accurately infer sentiment and be robust to temporal data distribution shifts? In this paper, we conduct an empirical study on the financial sentiment analysis system under temporal data distribution shifts using a real-world financial social media dataset that spans three years. We find that the fine-tuned models suffer from general performance degradation in the presence of temporal distribution shifts. Furthermore, motivated by the unique temporal nature of the financial text, we propose a novel method that combines out-of-distribution detection with time series modeling for temporal financial sentiment analysis. Experimental results show that the proposed method enhances the model's capability to adapt to evolving temporal shifts in a volatile financial market.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#19977;&#31181;&#26041;&#27861;&#35782;&#21035;&#33521;&#25991;&#35821;&#35328;&#27169;&#22411;&#20013;&#36127;&#36131;&#24615;&#21035;&#20559;&#35265;&#30340;Transformer&#32452;&#20214;&#65292;&#28982;&#21518;&#20351;&#29992;&#36825;&#20123;&#32452;&#20214;&#36827;&#34892;&#21442;&#25968;&#39640;&#25928;&#30340;&#20559;&#35265;&#32531;&#35299;&#24494;&#35843;&#65292;&#21462;&#24471;&#20102;&#25104;&#21151;&#30340;&#24615;&#21035;&#20559;&#35265;&#32531;&#35299;&#25928;&#26524;&#24182;&#20943;&#23569;&#20102;&#23545;&#19968;&#33324;&#35821;&#35328;&#24314;&#27169;&#30340;&#25439;&#23475;&#12290;</title><link>http://arxiv.org/abs/2310.12611</link><description>&lt;p&gt;
&#35782;&#21035;&#21644;&#35843;&#25972;&#33521;&#25991;&#35821;&#35328;&#27169;&#22411;&#20013;&#36127;&#36131;&#24615;&#21035;&#20559;&#35265;&#30340;Transformer&#32452;&#20214;
&lt;/p&gt;
&lt;p&gt;
Identifying and Adapting Transformer-Components Responsible for Gender Bias in an English Language Model. (arXiv:2310.12611v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#19977;&#31181;&#26041;&#27861;&#35782;&#21035;&#33521;&#25991;&#35821;&#35328;&#27169;&#22411;&#20013;&#36127;&#36131;&#24615;&#21035;&#20559;&#35265;&#30340;Transformer&#32452;&#20214;&#65292;&#28982;&#21518;&#20351;&#29992;&#36825;&#20123;&#32452;&#20214;&#36827;&#34892;&#21442;&#25968;&#39640;&#25928;&#30340;&#20559;&#35265;&#32531;&#35299;&#24494;&#35843;&#65292;&#21462;&#24471;&#20102;&#25104;&#21151;&#30340;&#24615;&#21035;&#20559;&#35265;&#32531;&#35299;&#25928;&#26524;&#24182;&#20943;&#23569;&#20102;&#23545;&#19968;&#33324;&#35821;&#35328;&#24314;&#27169;&#30340;&#25439;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#23637;&#29616;&#21644;&#25918;&#22823;&#20102;&#35768;&#22810;&#31181;&#19981;&#24076;&#26395;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#21040;&#30340;&#20559;&#35265;&#65292;&#21253;&#25324;&#24615;&#21035;&#20559;&#35265;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#32570;&#20047;&#26377;&#25928;&#21644;&#39640;&#25928;&#22320;&#25913;&#21464;&#36825;&#31181;&#34892;&#20026;&#32780;&#19981;&#25439;&#23475;&#19968;&#33324;&#35821;&#35328;&#24314;&#27169;&#24615;&#33021;&#30340;&#24037;&#20855;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#31181;&#26041;&#27861;&#26469;&#35782;&#21035;LM&#32452;&#20214;&#19982;&#29305;&#23450;&#36755;&#20986;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65306;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#12289;&#33258;&#21160;&#30005;&#36335;&#21457;&#29616;&#21644;&#25105;&#20204;&#30340;&#26032;&#39062;&#39640;&#25928;&#30340;&#26041;&#27861;DiffMask+&#65292;&#22522;&#20110;&#24046;&#24322;&#25513;&#27169;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;GPT-2 small&#21644;&#24615;&#21035;&#20559;&#35265;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#21457;&#29616;&#30340;&#32452;&#20214;&#38598;&#36827;&#34892;&#21442;&#25968;&#39640;&#25928;&#30340;&#20559;&#35265;&#32531;&#35299;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#30340;&#35745;&#31639;&#35201;&#27714;&#23384;&#22312;&#24040;&#22823;&#24046;&#24322;&#65292;&#20294;&#35782;&#21035;&#20986;&#30340;&#32452;&#20214;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#37325;&#21472;&#65292;&#24182;&#19988;&#25104;&#21151;&#20943;&#36731;&#20102;&#24615;&#21035;&#20559;&#35265;&#65292;&#30456;&#27604;&#20110;&#23436;&#25972;&#27169;&#22411;&#24494;&#35843;&#23545;&#19968;&#33324;&#35821;&#35328;&#24314;&#27169;&#30340;&#25439;&#23475;&#36739;&#23567;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#20063;&#24378;&#35843;&#20102;&#19968;&#20123;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) exhibit and amplify many types of undesirable biases learned from the training data, including gender bias. However, we lack tools for effectively and efficiently changing this behavior without hurting general language modeling performance. In this paper, we study three methods for identifying causal relations between LM components and particular output: causal mediation analysis, automated circuit discovery and our novel, efficient method called DiffMask+ based on differential masking. We apply the methods to GPT-2 small and the problem of gender bias, and use the discovered sets of components to perform parameter-efficient fine-tuning for bias mitigation. Our results show significant overlap in the identified components (despite huge differences in the computational requirements of the methods) as well as success in mitigating gender bias, with less damage to general language modeling compared to full model fine-tuning. However, our work also underscores the dif
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#24577;&#24863;&#30693;&#30340;&#38382;&#39064;&#22238;&#31572;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26102;&#38388;&#19978;&#19979;&#25991;&#30340;&#21306;&#38388;&#25277;&#21462;&#20219;&#21153;&#21644;&#30456;&#24212;&#30340;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;QA&#27169;&#22411;&#30340;&#26102;&#38388;&#24863;&#30693;&#33021;&#21147;&#65292;&#22312;TimeQA&#25968;&#25454;&#38598;&#20013;&#30340;F1&#20998;&#25968;&#19978;&#36229;&#36807;&#20102;&#22522;&#20934;&#27169;&#22411;&#36798;8.5&#12290;</title><link>http://arxiv.org/abs/2310.12585</link><description>&lt;p&gt;
&#26102;&#24577;&#25935;&#24863;&#38382;&#39064;&#22238;&#31572;&#30340;&#26102;&#24577;&#24863;&#30693;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Time-Aware Representation Learning for Time-Sensitive Question Answering. (arXiv:2310.12585v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12585
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#24577;&#24863;&#30693;&#30340;&#38382;&#39064;&#22238;&#31572;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26102;&#38388;&#19978;&#19979;&#25991;&#30340;&#21306;&#38388;&#25277;&#21462;&#20219;&#21153;&#21644;&#30456;&#24212;&#30340;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;QA&#27169;&#22411;&#30340;&#26102;&#38388;&#24863;&#30693;&#33021;&#21147;&#65292;&#22312;TimeQA&#25968;&#25454;&#38598;&#20013;&#30340;F1&#20998;&#25968;&#19978;&#36229;&#36807;&#20102;&#22522;&#20934;&#27169;&#22411;&#36798;8.5&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#26159;&#29616;&#23454;&#19990;&#30028;&#20013;&#38382;&#39064;&#22238;&#31572;&#30340;&#20851;&#38190;&#22240;&#32032;&#20043;&#19968;&#65292;&#28982;&#32780;&#35821;&#35328;&#27169;&#22411;&#24456;&#38590;&#29702;&#35299;&#26102;&#38388;&#38480;&#23450;&#35789;&#22914;&#8220;&#20043;&#21518;&#8221;&#21644;&#8220;&#20043;&#21069;&#8221;&#19982;&#25968;&#23383;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#22240;&#20026;&#29616;&#26377;&#30340;&#38382;&#39064;&#22238;&#31572;&#25968;&#25454;&#38598;&#20013;&#27809;&#26377;&#36275;&#22815;&#30340;&#26102;&#38388;&#34920;&#36798;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#38382;&#39064;&#22238;&#31572;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#19978;&#19979;&#25991;&#30340;&#21306;&#38388;&#25277;&#21462;&#20219;&#21153;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#20381;&#36182;&#20110;&#26102;&#38388;&#19978;&#19979;&#25991;&#30340;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;QA&#27169;&#22411;&#30340;&#26102;&#38388;&#24863;&#30693;&#24230;&#30340;&#24230;&#37327;&#26041;&#27861;&#12290;&#21306;&#38388;&#25277;&#21462;&#20219;&#21153;&#21253;&#25324;&#19968;&#20010;&#38382;&#39064;&#21644;&#22235;&#20010;&#21477;&#23376;&#20505;&#36873;&#39033;&#65292;&#26681;&#25454;&#26102;&#38388;&#21644;&#19978;&#19979;&#25991;&#20998;&#31867;&#20026;&#27491;&#30830;&#25110;&#38169;&#35823;&#12290;&#27169;&#22411;&#34987;&#35757;&#32451;&#20026;&#20174;&#22312;&#26102;&#38388;&#21644;&#19978;&#19979;&#25991;&#19978;&#37117;&#27491;&#30830;&#30340;&#21477;&#23376;&#20013;&#25552;&#21462;&#31572;&#26696;&#21306;&#38388;&#12290;&#36890;&#36807;&#20351;&#29992;TCQA&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;TimeQA&#25968;&#25454;&#38598;&#19978;&#30340;F1&#20998;&#25968;&#19978;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#36798;8.5&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#21487;&#22312;&#20197;&#19979;&#38142;&#25509;&#20013;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time is one of the crucial factors in real-world question answering (QA) problems. However, language models have difficulty understanding the relationships between time specifiers, such as 'after' and 'before', and numbers, since existing QA datasets do not include sufficient time expressions. To address this issue, we propose a Time-Context aware Question Answering (TCQA) framework. We suggest a Time-Context dependent Span Extraction (TCSE) task, and build a time-context dependent data generation framework for model training. Moreover, we present a metric to evaluate the time awareness of the QA model using TCSE. The TCSE task consists of a question and four sentence candidates classified as correct or incorrect based on time and context. The model is trained to extract the answer span from the sentence that is both correct in time and context. The model trained with TCQA outperforms baseline models up to 8.5 of the F1-score in the TimeQA dataset. Our dataset and code are available at
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#33021;&#22815;&#26126;&#30830;&#32771;&#34385;&#21040;&#25991;&#26412;&#23646;&#24615;&#24322;&#26500;&#22270;&#20013;&#30340;&#25299;&#25169;&#21644;&#24322;&#26500;&#20449;&#24687;&#12290;&#36890;&#36807;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;&#21644;&#36741;&#21161;&#30340;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#39044;&#27979;&#20102;&#25991;&#26412;&#23646;&#24615;&#24322;&#26500;&#22270;&#20013;&#30340;&#33410;&#28857;&#12290;&#21516;&#26102;&#65292;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#25991;&#26412;&#20016;&#23500;&#24615;&#21152;&#26435;&#30340;&#33410;&#28857;&#25277;&#26679;&#31574;&#30053;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#25991;&#26412;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2310.12580</link><description>&lt;p&gt;
&#20351;&#29992;&#25991;&#26412;&#23646;&#24615;&#24322;&#26500;&#22270;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Pretraining Language Models with Text-Attributed Heterogeneous Graphs. (arXiv:2310.12580v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#33021;&#22815;&#26126;&#30830;&#32771;&#34385;&#21040;&#25991;&#26412;&#23646;&#24615;&#24322;&#26500;&#22270;&#20013;&#30340;&#25299;&#25169;&#21644;&#24322;&#26500;&#20449;&#24687;&#12290;&#36890;&#36807;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;&#21644;&#36741;&#21161;&#30340;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#39044;&#27979;&#20102;&#25991;&#26412;&#23646;&#24615;&#24322;&#26500;&#22270;&#20013;&#30340;&#33410;&#28857;&#12290;&#21516;&#26102;&#65292;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#25991;&#26412;&#20016;&#23500;&#24615;&#21152;&#26435;&#30340;&#33410;&#28857;&#25277;&#26679;&#31574;&#30053;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#25991;&#26412;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#65288;&#22914;&#23398;&#26415;&#32593;&#32476;&#12289;&#31038;&#20132;&#24179;&#21488;&#65289;&#65292;&#19981;&#21516;&#31867;&#22411;&#30340;&#23454;&#20307;&#19981;&#20165;&#19982;&#25991;&#26412;&#30456;&#20851;&#65292;&#36824;&#36890;&#36807;&#21508;&#31181;&#20851;&#31995;&#30456;&#36830;&#65292;&#36825;&#21487;&#20197;&#34987;&#25277;&#35937;&#20026;&#25991;&#26412;&#23646;&#24615;&#24322;&#26500;&#22270;&#65288;Text-Attributed Heterogeneous Graphs&#65292;TAHGs&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many real-world scenarios (e.g., academic networks, social platforms), different types of entities are not only associated with texts but also connected by various relationships, which can be abstracted as Text-Attributed Heterogeneous Graphs (TAHGs). Current pretraining tasks for Language Models (LMs) primarily focus on separately learning the textual information of each entity and overlook the crucial aspect of capturing topological connections among entities in TAHGs. In this paper, we present a new pretraining framework for LMs that explicitly considers the topological and heterogeneous information in TAHGs. Firstly, we define a context graph as neighborhoods of a target node within specific orders and propose a topology-aware pretraining task to predict nodes involved in the context graph by jointly optimizing an LM and an auxiliary heterogeneous graph neural network. Secondly, based on the observation that some nodes are text-rich while others have little text, we devise a tex
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#33258;&#21160;&#20998;&#26512;&#25919;&#20826;&#23459;&#35328;&#30340;&#32553;&#25918;&#20540;&#65306;&#26631;&#31614;&#32858;&#21512;&#21644;&#22522;&#20110;&#38271;&#25991;&#26412;&#36755;&#20837;Transformer&#30340;&#27169;&#22411;&#12290;&#20182;&#20204;&#22312;&#27604;&#36739;&#23459;&#35328;&#39033;&#30446;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#20204;&#26159;&#26377;&#25928;&#30340;&#24182;&#19988;&#21487;&#20197;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2310.12575</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#25919;&#20826;&#23450;&#20301;&#30340;&#20272;&#35745;&#65306;&#20174;&#26631;&#31614;&#32858;&#21512;&#21040;&#38271;&#25991;&#26412;&#36755;&#20837;Transformer
&lt;/p&gt;
&lt;p&gt;
Multilingual estimation of political-party positioning: From label aggregation to long-input Transformers. (arXiv:2310.12575v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12575
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#33258;&#21160;&#20998;&#26512;&#25919;&#20826;&#23459;&#35328;&#30340;&#32553;&#25918;&#20540;&#65306;&#26631;&#31614;&#32858;&#21512;&#21644;&#22522;&#20110;&#38271;&#25991;&#26412;&#36755;&#20837;Transformer&#30340;&#27169;&#22411;&#12290;&#20182;&#20204;&#22312;&#27604;&#36739;&#23459;&#35328;&#39033;&#30446;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#20204;&#26159;&#26377;&#25928;&#30340;&#24182;&#19988;&#21487;&#20197;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32553;&#25918;&#20998;&#26512;&#26159;&#35745;&#31639;&#26426;&#25919;&#27835;&#23398;&#20013;&#30340;&#19968;&#31181;&#25216;&#26415;&#65292;&#23427;&#26681;&#25454;&#39044;&#23450;&#20041;&#30340;&#35268;&#27169;&#20026;&#25919;&#27835;&#34892;&#20026;&#32773;&#65288;&#20363;&#22914;&#25919;&#27835;&#23478;&#25110;&#25919;&#20826;&#65289;&#20998;&#37197;&#19968;&#20010;&#20998;&#25968;&#65292;&#35813;&#20998;&#25968;&#22522;&#20110;&#65288;&#36890;&#24120;&#26159;&#38271;&#30340;&#65289;&#25991;&#26412;&#20307;&#65288;&#20363;&#22914;&#35758;&#20250;&#28436;&#35762;&#25110;&#36873;&#20030;&#23459;&#35328;&#65289;&#12290;&#20363;&#22914;&#65292;&#25919;&#27835;&#31185;&#23398;&#23478;&#32463;&#24120;&#20351;&#29992;&#24038;&#21491;&#21051;&#24230;&#26469;&#31995;&#32479;&#20998;&#26512;&#19981;&#21516;&#22269;&#23478;&#30340;&#25919;&#27835;&#26223;&#35266;&#12290;&#33258;&#21160;&#32553;&#25918;&#20998;&#26512;&#30340;NLP&#26041;&#27861;&#21487;&#20197;&#25214;&#21040;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#21069;&#25552;&#26159;&#23427;&#20204;&#33021;&#22815;&#22788;&#29702;&#38271;&#25991;&#26412;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#39046;&#22495;&#21644;&#35821;&#35328;&#20013;&#24037;&#20316;&#31283;&#20581;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23454;&#26045;&#24182;&#27604;&#36739;&#20102;&#20004;&#31181;&#33258;&#21160;&#32553;&#25918;&#20998;&#26512;&#25919;&#20826;&#23459;&#35328;&#30340;&#26041;&#27861;&#65306;&#26631;&#31614;&#32858;&#21512;&#65292;&#19968;&#31181;&#20381;&#36182;&#20110;&#23459;&#35328;&#20013;&#20010;&#21035;&#38472;&#36848;&#30340;&#27880;&#37322;&#30340;&#31649;&#36947;&#31574;&#30053;&#65292;&#20197;&#21450;&#22522;&#20110;&#38271;&#25991;&#26412;&#36755;&#20837;Transformer&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#30452;&#25509;&#20174;&#21407;&#22987;&#25991;&#26412;&#35745;&#31639;&#32553;&#25918;&#20540;&#12290;&#25105;&#20204;&#23545;41&#20010;&#22269;&#23478;&#21644;27&#31181;&#35821;&#35328;&#30340;&#27604;&#36739;&#23459;&#35328;&#39033;&#30446;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scaling analysis is a technique in computational political science that assigns a political actor (e.g. politician or party) a score on a predefined scale based on a (typically long) body of text (e.g. a parliamentary speech or an election manifesto). For example, political scientists have often used the left--right scale to systematically analyse political landscapes of different countries. NLP methods for automatic scaling analysis can find broad application provided they (i) are able to deal with long texts and (ii) work robustly across domains and languages. In this work, we implement and compare two approaches to automatic scaling analysis of political-party manifestos: label aggregation, a pipeline strategy relying on annotations of individual statements from the manifestos, and long-input-Transformer-based models, which compute scaling values directly from raw text. We carry out the analysis of the Comparative Manifestos Project dataset across 41 countries and 27 languages and f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#35821;&#35328;&#27169;&#22411;&#19982;&#25628;&#32034;&#24341;&#25806;&#22312;&#24110;&#21161;&#29992;&#25143;&#20107;&#23454;&#26680;&#26597;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#29992;&#25143;&#38405;&#35835;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#37322;&#27604;&#20351;&#29992;&#25628;&#32034;&#24341;&#25806;&#26356;&#39640;&#25928;&#65292;&#20294;&#24403;&#35299;&#37322;&#38169;&#35823;&#26102;&#65292;&#29992;&#25143;&#23481;&#26131;&#36807;&#24230;&#20381;&#36182;&#35821;&#35328;&#27169;&#22411;&#12290;&#20026;&#20102;&#20943;&#23569;&#36807;&#24230;&#20381;&#36182;&#65292;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#23545;&#27604;&#20449;&#24687;&#36827;&#34892;&#35299;&#37322;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.12558</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#24110;&#21161;&#20154;&#31867;&#39564;&#35777;&#30495;&#23454;&#24615;&#8212;&#8212;&#38500;&#38750;&#23427;&#20204;&#20196;&#20154;&#20449;&#26381;&#22320;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Help Humans Verify Truthfulness -- Except When They Are Convincingly Wrong. (arXiv:2310.12558v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#35821;&#35328;&#27169;&#22411;&#19982;&#25628;&#32034;&#24341;&#25806;&#22312;&#24110;&#21161;&#29992;&#25143;&#20107;&#23454;&#26680;&#26597;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#29992;&#25143;&#38405;&#35835;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#37322;&#27604;&#20351;&#29992;&#25628;&#32034;&#24341;&#25806;&#26356;&#39640;&#25928;&#65292;&#20294;&#24403;&#35299;&#37322;&#38169;&#35823;&#26102;&#65292;&#29992;&#25143;&#23481;&#26131;&#36807;&#24230;&#20381;&#36182;&#35821;&#35328;&#27169;&#22411;&#12290;&#20026;&#20102;&#20943;&#23569;&#36807;&#24230;&#20381;&#36182;&#65292;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#23545;&#27604;&#20449;&#24687;&#36827;&#34892;&#35299;&#37322;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;&#33719;&#21462;&#32593;&#32476;&#19978;&#30340;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#30340;&#30495;&#23454;&#24615;&#21644;&#20107;&#23454;&#24615;&#22791;&#21463;&#20851;&#27880;&#12290;&#20026;&#20102;&#24110;&#21161;&#29992;&#25143;&#20570;&#20986;&#27491;&#30830;&#30340;&#20449;&#24687;&#20915;&#31574;&#65292;LLMs&#19981;&#20165;&#24212;&#25552;&#20379;&#20449;&#24687;&#65292;&#36824;&#24212;&#24110;&#21161;&#29992;&#25143;&#20107;&#23454;&#26680;&#26597;&#12290;&#26412;&#25991;&#36890;&#36807;&#19982;80&#21517;&#20247;&#21253;&#24037;&#20316;&#32773;&#36827;&#34892;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#35821;&#35328;&#27169;&#22411;&#19982;&#25628;&#32034;&#24341;&#25806;&#65288;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#65289;&#22312;&#24110;&#21161;&#20154;&#31867;&#29992;&#25143;&#20107;&#23454;&#26680;&#26597;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#24341;&#23548;LLMs&#39564;&#35777;&#32473;&#23450;&#30340;&#22768;&#26126;&#24182;&#25552;&#20379;&#30456;&#24212;&#30340;&#35299;&#37322;&#12290;&#19982;&#20351;&#29992;&#20934;&#30830;&#29575;&#30456;&#20284;&#30340;&#25628;&#32034;&#24341;&#25806;&#30456;&#27604;&#65292;&#38405;&#35835;LLM&#30340;&#35299;&#37322;&#30340;&#29992;&#25143;&#25928;&#29575;&#26174;&#33879;&#25552;&#39640;&#12290;&#28982;&#32780;&#65292;&#24403;&#35299;&#37322;&#38169;&#35823;&#26102;&#65292;&#20182;&#20204;&#24448;&#24448;&#36807;&#24230;&#20381;&#36182;LLMs&#12290;&#20026;&#20102;&#20943;&#23569;&#23545;LLMs&#30340;&#36807;&#24230;&#20381;&#36182;&#65292;&#25105;&#20204;&#35201;&#27714;LLMs&#25552;&#20379;&#23545;&#27604;&#20449;&#24687;&#65292;&#35299;&#37322;&#20026;&#20160;&#20040;&#22768;&#26126;&#20026;&#30495;&#21644;&#20026;&#20551;&#65292;&#24182;&#23558;&#20004;&#26041;&#38754;&#30340;&#35299;&#37322;&#21576;&#29616;&#32473;&#29992;&#25143;&#12290;&#36825;&#31181;&#23545;&#27604;&#35299;&#37322;&#20943;&#36731;&#20102;&#29992;&#25143;&#30340;&#36807;&#24230;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are increasingly used for accessing information on the web. Their truthfulness and factuality are thus of great interest. To help users make the right decisions about the information they're getting, LLMs should not only provide but also help users fact-check information. In this paper, we conduct experiments with 80 crowdworkers in total to compare language models with search engines (information retrieval systems) at facilitating fact-checking by human users. We prompt LLMs to validate a given claim and provide corresponding explanations. Users reading LLM explanations are significantly more efficient than using search engines with similar accuracy. However, they tend to over-rely the LLMs when the explanation is wrong. To reduce over-reliance on LLMs, we ask LLMs to provide contrastive information explain both why the claim is true and false, and then we present both sides of the explanation to users. This contrastive explanation mitigates users' over-
&lt;/p&gt;</description></item><item><title>DepWiGNN&#26159;&#19968;&#31181;&#29992;&#20110;&#22810;&#36339;&#31354;&#38388;&#25512;&#29702;&#30340;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#23427;&#36890;&#36807;&#35774;&#35745;&#26032;&#39062;&#30340;&#33410;&#28857;&#35760;&#24518;&#26041;&#26696;&#65292;&#24182;&#22312;&#22270;&#30340;&#28145;&#24230;&#32500;&#24230;&#19978;&#32858;&#21512;&#20449;&#24687;&#65292;&#20174;&#32780;&#33021;&#22815;&#25910;&#38598;&#38271;&#26102;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#32780;&#26080;&#38656;&#22534;&#21472;&#22810;&#20010;&#23618;&#27425;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DepWiGNN&#22312;&#20004;&#20010;&#25361;&#25112;&#25968;&#25454;&#38598;&#19978;&#27604;&#20256;&#32479;GNN&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12557</link><description>&lt;p&gt;
DepWiGNN&#65306;&#19968;&#31181;&#29992;&#20110;&#22810;&#36339;&#31354;&#38388;&#25512;&#29702;&#30340;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
DepWiGNN: A Depth-wise Graph Neural Network for Multi-hop Spatial Reasoning in Text. (arXiv:2310.12557v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12557
&lt;/p&gt;
&lt;p&gt;
DepWiGNN&#26159;&#19968;&#31181;&#29992;&#20110;&#22810;&#36339;&#31354;&#38388;&#25512;&#29702;&#30340;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#23427;&#36890;&#36807;&#35774;&#35745;&#26032;&#39062;&#30340;&#33410;&#28857;&#35760;&#24518;&#26041;&#26696;&#65292;&#24182;&#22312;&#22270;&#30340;&#28145;&#24230;&#32500;&#24230;&#19978;&#32858;&#21512;&#20449;&#24687;&#65292;&#20174;&#32780;&#33021;&#22815;&#25910;&#38598;&#38271;&#26102;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#32780;&#26080;&#38656;&#22534;&#21472;&#22810;&#20010;&#23618;&#27425;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DepWiGNN&#22312;&#20004;&#20010;&#25361;&#25112;&#25968;&#25454;&#38598;&#19978;&#27604;&#20256;&#32479;GNN&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20013;&#30340;&#31354;&#38388;&#25512;&#29702;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#29616;&#26377;&#30340;&#31354;&#38388;&#25512;&#29702;&#26041;&#27861;&#36890;&#24120;&#20174;&#32431;&#25991;&#26412;&#20013;&#25512;&#26029;&#31354;&#38388;&#20851;&#31995;&#65292;&#24573;&#35270;&#20102;&#33258;&#28982;&#35821;&#35328;&#19982;&#31526;&#21495;&#32467;&#26500;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#24341;&#23548;&#21644;&#32858;&#21512;&#31526;&#21495;&#32467;&#26500;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;GNN&#22312;&#22788;&#29702;&#22810;&#36339;&#31354;&#38388;&#25512;&#29702;&#26102;&#38754;&#20020;&#30528;&#25361;&#25112;&#65292;&#30001;&#20110;&#36807;&#24230;&#24179;&#28369;&#30340;&#38382;&#39064;&#65292;&#21363;&#38543;&#30528;&#22270;&#23618;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Depth-Wise Graph Neural Network&#65288;DepWiGNN&#65289;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33410;&#28857;&#35760;&#24518;&#26041;&#26696;&#65292;&#24182;&#22312;&#22270;&#30340;&#28145;&#24230;&#32500;&#24230;&#19978;&#32858;&#21512;&#20449;&#24687;&#65292;&#32780;&#19981;&#26159;&#22312;&#24191;&#24230;&#32500;&#24230;&#19978;&#65292;&#36825;&#26679;&#21487;&#20197;&#25910;&#38598;&#38271;&#26102;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#32780;&#26080;&#38656;&#22534;&#21472;&#22810;&#20010;&#23618;&#27425;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20004;&#20010;&#25361;&#25112;&#25968;&#25454;&#38598;&#19978;&#65292;DepWiGNN&#21487;&#20197;&#20197;&#27604;&#20256;&#32479;GNN&#26041;&#27861;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#36827;&#34892;&#22810;&#36339;&#31354;&#38388;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatial reasoning in text plays a crucial role in various real-world applications. Existing approaches for spatial reasoning typically infer spatial relations from pure text, which overlook the gap between natural language and symbolic structures. Graph neural networks (GNNs) have showcased exceptional proficiency in inducing and aggregating symbolic structures. However, classical GNNs face challenges in handling multi-hop spatial reasoning due to the over-smoothing issue, \textit{i.e.}, the performance decreases substantially as the number of graph layers increases. To cope with these challenges, we propose a novel \textbf{Dep}th-\textbf{Wi}se \textbf{G}raph \textbf{N}eural \textbf{N}etwork (\textbf{DepWiGNN}). Specifically, we design a novel node memory scheme and aggregate the information over the depth dimension instead of the breadth dimension of the graph, which empowers the ability to collect long dependencies without stacking multiple layers. Experimental results on two challen
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35774;&#35745;MOEA&#25805;&#20316;&#31526;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#25104;&#21151;&#23558;&#36890;&#29992;&#30340;LLM&#20197;&#38646;-shot&#26041;&#24335;&#20316;&#20026;MOEA/D&#30340;&#40657;&#30418;&#25628;&#32034;&#25805;&#20316;&#31526;&#65292;&#24182;&#36890;&#36807;&#20174;LLM&#34892;&#20026;&#20013;&#23398;&#20064;&#35774;&#35745;&#20102;&#19968;&#20010;&#26174;&#24615;&#30340;&#30333;&#30418;&#25805;&#20316;&#31526;&#12290;</title><link>http://arxiv.org/abs/2310.12541</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#22810;&#30446;&#26631;&#36827;&#21270;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Large Language Model for Multi-objective Evolutionary Optimization. (arXiv:2310.12541v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35774;&#35745;MOEA&#25805;&#20316;&#31526;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#25104;&#21151;&#23558;&#36890;&#29992;&#30340;LLM&#20197;&#38646;-shot&#26041;&#24335;&#20316;&#20026;MOEA/D&#30340;&#40657;&#30418;&#25628;&#32034;&#25805;&#20316;&#31526;&#65292;&#24182;&#36890;&#36807;&#20174;LLM&#34892;&#20026;&#20013;&#23398;&#20064;&#35774;&#35745;&#20102;&#19968;&#20010;&#26174;&#24615;&#30340;&#30333;&#30418;&#25805;&#20316;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#65288;MOEAs&#65289;&#26159;&#35299;&#20915;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65288;MOPs&#65289;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#65292;&#25552;&#20986;&#20102;&#35768;&#22810;MOEAs&#65292;&#20854;&#25805;&#20316;&#31526;&#38656;&#35201;&#36890;&#36807;&#39046;&#22495;&#30693;&#35782;&#36827;&#34892;&#31934;&#24515;&#35774;&#35745;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#23581;&#35797;&#23558;MOEAs&#20013;&#25163;&#21160;&#35774;&#35745;&#30340;&#25805;&#20316;&#31526;&#26367;&#25442;&#20026;&#22522;&#20110;&#23398;&#20064;&#30340;&#25805;&#20316;&#31526;&#65288;&#22914;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65289;&#24050;&#32463;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#21644;&#35757;&#32451;&#36825;&#26679;&#30340;&#27169;&#22411;&#20173;&#28982;&#38656;&#35201;&#22823;&#37327;&#30340;&#24037;&#20316;&#65292;&#24182;&#19988;&#23398;&#20064;&#21040;&#30340;&#25805;&#20316;&#31526;&#21487;&#33021;&#19981;&#33021;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#35299;&#20915;&#26032;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#21033;&#29992;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#35774;&#35745;MOEA&#25805;&#20316;&#31526;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#36866;&#24403;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#35753;&#19968;&#20010;&#36890;&#29992;&#30340;LLM&#20197;&#38646;-shot&#30340;&#26041;&#24335;&#20316;&#20026;&#20998;&#35299;&#22411;MOEA&#65288;MOEA/D&#65289;&#30340;&#40657;&#30418;&#25628;&#32034;&#25805;&#20316;&#31526;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20174;LLM&#34892;&#20026;&#20013;&#23398;&#20064;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#19968;&#20010;&#26174;&#24615;&#30340;&#30333;&#30418;&#25805;&#20316;&#31526;&#65292;&#24182;&#25552;&#20986;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Multiobjective evolutionary algorithms (MOEAs) are major methods for solving multiobjective optimization problems (MOPs). Many MOEAs have been proposed in the past decades, of which the operators need carefully handcrafted design with domain knowledge. Recently, some attempts have been made to replace the manually designed operators in MOEAs with learning-based operators (e.g., neural network models). However, much effort is still required for designing and training such models, and the learned operators might not generalize well to solve new problems. To tackle the above challenges, this work investigates a novel approach that leverages the powerful large language model (LLM) to design MOEA operators. With proper prompt engineering, we successfully let a general LLM serve as a black-box search operator for decomposition-based MOEA (MOEA/D) in a zero-shot manner. In addition, by learning from the LLM behavior, we further design an explicit white-box operator with randomness and propose
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#39044;&#35757;&#32451;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#23646;&#24615;/&#20540;&#25552;&#21462;&#25216;&#26415;&#20013;&#38656;&#35201;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#21644;&#23545;&#26410;&#30693;&#23646;&#24615;&#20540;&#30340;&#25361;&#25112;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.12537</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20135;&#21697;&#23646;&#24615;&#20540;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Product Attribute Value Extraction using Large Language Models. (arXiv:2310.12537v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#39044;&#35757;&#32451;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#23646;&#24615;/&#20540;&#25552;&#21462;&#25216;&#26415;&#20013;&#38656;&#35201;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#21644;&#23545;&#26410;&#30693;&#23646;&#24615;&#20540;&#30340;&#25361;&#25112;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#21830;&#21153;&#24212;&#29992;&#65288;&#22914;&#38754;&#21521;&#23646;&#24615;&#30340;&#20135;&#21697;&#25628;&#32034;&#25110;&#20135;&#21697;&#27604;&#36739;&#65289;&#22522;&#20110;&#32467;&#26500;&#21270;&#30340;&#20135;&#21697;&#25551;&#36848;&#65292;&#22914;&#23646;&#24615;/&#20540;&#23545;&#12290;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#19978;&#30340;&#20379;&#24212;&#21830;&#19981;&#25552;&#20379;&#32467;&#26500;&#21270;&#30340;&#20135;&#21697;&#25551;&#36848;&#65292;&#32780;&#26159;&#20351;&#29992;&#26631;&#39064;&#25110;&#25551;&#36848;&#26469;&#25551;&#36848;&#20135;&#21697;&#12290;&#20026;&#20102;&#22788;&#29702;&#36825;&#26679;&#30340;&#20135;&#21697;&#65292;&#26377;&#24517;&#35201;&#20174;&#25991;&#26412;&#20135;&#21697;&#23646;&#24615;&#20013;&#25552;&#21462;&#23646;&#24615;/&#20540;&#23545;&#12290;&#29616;&#26377;&#25216;&#26415;&#20013;&#65292;&#23646;&#24615;/&#20540;&#25552;&#21462;&#26041;&#27861;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT&#65289;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#23646;&#24615;/&#20540;&#25552;&#21462;&#26041;&#38754;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#32570;&#28857;&#65306;&#65288;&#19968;&#65289;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#35757;&#32451;&#25968;&#25454;&#65307;&#65288;&#20108;&#65289;&#20248;&#21270;&#21518;&#30340;&#27169;&#22411;&#22312;&#25512;&#24191;&#21040;&#35757;&#32451;&#25968;&#25454;&#20013;&#26410;&#21253;&#21547;&#30340;&#23646;&#24615;&#20540;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#25928;&#29575;&#39640;&#19988;&#40065;&#26834;&#24615;&#24378;&#30340;&#26367;&#20195;&#26041;&#27861;&#22312;&#23646;&#24615;/&#20540;&#25552;&#21462;&#20013;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#25176;&#31649;&#30340;LLMs&#65292;&#22914;GPT-3.5&#21644;GPT-4&#12290;
&lt;/p&gt;
&lt;p&gt;
E-commerce applications such as faceted product search or product comparison are based on structured product descriptions like attribute/value pairs. The vendors on e-commerce platforms do not provide structured product descriptions but describe offers using titles or descriptions. To process such offers, it is necessary to extract attribute/value pairs from textual product attributes. State-of-the-art attribute/value extraction techniques rely on pre-trained language models (PLMs), such as BERT. Two major drawbacks of these models for attribute/value extraction are that (i) the models require significant amounts of task-specific training data and (ii) the fine-tuned models face challenges in generalizing to attribute values not included in the training data. This paper explores the potential of large language models (LLMs) as a training data-efficient and robust alternative to PLM-based attribute/value extraction methods. We consider hosted LLMs, such as GPT-3.5 and GPT-4, as well as 
&lt;/p&gt;</description></item><item><title>ICU&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#35270;&#35273;&#19982;&#35821;&#35328;&#24314;&#27169;&#20013;&#35821;&#35328;&#38556;&#30861;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#21010;&#20998;&#20026;&#22270;&#20687;&#23383;&#24149;&#21644;&#35821;&#35328;&#29702;&#35299;&#20004;&#20010;&#38454;&#27573;&#65292;&#23558;&#22810;&#35821;&#35328;&#22788;&#29702;&#36127;&#25285;&#36716;&#31227;&#21040;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#19978;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;ICU&#22312;&#22810;&#20010;&#35821;&#35328;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.12531</link><description>&lt;p&gt;
ICU&#65306;&#36890;&#36807;&#23558;&#20219;&#21153;&#21010;&#20998;&#20026;&#22270;&#20687;&#23383;&#24149;&#21644;&#35821;&#35328;&#29702;&#35299;&#26469;&#20811;&#26381;&#35270;&#35273;&#19982;&#35821;&#35328;&#24314;&#27169;&#20013;&#30340;&#35821;&#35328;&#38556;&#30861;
&lt;/p&gt;
&lt;p&gt;
ICU: Conquering Language Barriers in Vision-and-Language Modeling by Dividing the Tasks into Image Captioning and Language Understanding. (arXiv:2310.12531v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12531
&lt;/p&gt;
&lt;p&gt;
ICU&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#35270;&#35273;&#19982;&#35821;&#35328;&#24314;&#27169;&#20013;&#35821;&#35328;&#38556;&#30861;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#21010;&#20998;&#20026;&#22270;&#20687;&#23383;&#24149;&#21644;&#35821;&#35328;&#29702;&#35299;&#20004;&#20010;&#38454;&#27573;&#65292;&#23558;&#22810;&#35821;&#35328;&#22788;&#29702;&#36127;&#25285;&#36716;&#31227;&#21040;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#19978;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;ICU&#22312;&#22810;&#20010;&#35821;&#35328;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#35270;&#35273;&#19982;&#35821;&#35328;(V&amp;L)&#30740;&#31350;&#26088;&#22312;&#22312;&#19968;&#20010;&#27169;&#22411;&#20013;&#23454;&#29616;&#22810;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22270;&#20687;&#30340;&#22810;&#35821;&#35328;&#23383;&#24149;&#31232;&#32570;&#19968;&#30452;&#20197;&#26469;&#19968;&#30452;&#38459;&#30861;&#20102;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38556;&#30861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ICU&#65288;Image Caption Understanding&#65289;&#65292;&#23558;V&amp;L&#20219;&#21153;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;&#19968;&#20010;V&amp;L&#27169;&#22411;&#20197;&#33521;&#25991;&#36827;&#34892;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#65292;&#28982;&#21518;&#19968;&#20010;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#65288;mLM&#65289;&#20197;&#23383;&#24149;&#20316;&#20026;&#26367;&#20195;&#25991;&#26412;&#36827;&#34892;&#36328;&#35821;&#35328;&#35821;&#35328;&#29702;&#35299;&#12290;&#36825;&#31181;&#26041;&#24335;&#20943;&#36731;&#20102;V&amp;L&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#22788;&#29702;&#36127;&#25285;&#65292;&#23558;&#20854;&#36716;&#31227;&#21040;&#20102;mLM&#19978;&#12290;&#30001;&#20110;&#22810;&#35821;&#35328;&#25991;&#26412;&#25968;&#25454;&#30456;&#23545;&#20016;&#23500;&#21644;&#36136;&#37327;&#36739;&#39640;&#65292;ICU&#21487;&#20197;&#24110;&#21161;&#20811;&#26381;V&amp;L&#27169;&#22411;&#20013;&#30340;&#35821;&#35328;&#38556;&#30861;&#12290;&#22312;IGLUE&#22522;&#20934;&#27979;&#35797;&#30340;&#20004;&#20010;&#20219;&#21153;&#20013;&#65292;&#28041;&#21450;9&#31181;&#35821;&#35328;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ICU&#21487;&#20197;&#22312;&#20116;&#31181;&#35821;&#35328;&#19978;&#23454;&#29616;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#24182;&#22312;&#20854;&#20313;&#35821;&#35328;&#19978;&#21462;&#24471;&#20102;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most multilingual vision-and-language (V&amp;L) research aims to accomplish multilingual and multimodal capabilities within one model. However, the scarcity of multilingual captions for images has hindered the development. To overcome this obstacle, we propose ICU, Image Caption Understanding, which divides a V&amp;L task into two stages: a V&amp;L model performs image captioning in English, and a multilingual language model (mLM), in turn, takes the caption as the alt text and performs crosslingual language understanding. The burden of multilingual processing is lifted off V&amp;L model and placed on mLM. Since the multilingual text data is relatively of higher abundance and quality, ICU can facilitate the conquering of language barriers for V&amp;L models. In experiments on two tasks across 9 languages in the IGLUE benchmark, we show that ICU can achieve new state-of-the-art results for five languages, and comparable results for the rest.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;ChouBERT&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#30417;&#27979;&#26893;&#29289;&#20581;&#24247;&#23041;&#32961;&#12290;&#36890;&#36807;&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#25968;&#25454;&#65292;&#21487;&#20197;&#26816;&#27979;&#24182;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#65292;&#20174;&#32780;&#35299;&#20915;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#20013;&#32570;&#20047;&#26631;&#35760;&#25968;&#25454;&#21644;&#32454;&#31890;&#24230;&#35821;&#20041;&#36164;&#28304;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.12522</link><description>&lt;p&gt;
&#29992;&#20110;&#30417;&#27979;&#26893;&#29289;&#20581;&#24247;&#23041;&#32961;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65306;&#19968;&#31181;&#22522;&#20110;ChouBERT&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Named Entity Recognition for Monitoring Plant Health Threats in Tweets: a ChouBERT Approach. (arXiv:2310.12522v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;ChouBERT&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#30417;&#27979;&#26893;&#29289;&#20581;&#24247;&#23041;&#32961;&#12290;&#36890;&#36807;&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#25968;&#25454;&#65292;&#21487;&#20197;&#26816;&#27979;&#24182;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#65292;&#20174;&#32780;&#35299;&#20915;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#20013;&#32570;&#20047;&#26631;&#35760;&#25968;&#25454;&#21644;&#32454;&#31890;&#24230;&#35821;&#20041;&#36164;&#28304;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#23494;&#20892;&#19994;&#30340;&#19968;&#20010;&#37325;&#35201;&#24212;&#29992;&#22330;&#26223;&#26159;&#21033;&#29992;&#20256;&#24863;&#22120;&#21644;&#25968;&#25454;&#20998;&#26512;&#25216;&#26415;&#26816;&#27979;&#21644;&#27979;&#37327;&#20316;&#29289;&#20581;&#24247;&#23041;&#32961;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#26631;&#35760;&#25968;&#25454;&#21644;&#32454;&#31890;&#24230;&#35821;&#20041;&#36164;&#28304;&#65292;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#23545;&#25991;&#26412;&#25968;&#25454;&#20173;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20892;&#27665;&#20043;&#38388;&#26085;&#30410;&#22686;&#38271;&#30340;&#20114;&#32852;&#24615;&#21644;&#22312;&#32447;&#20892;&#19994;&#31038;&#21306;&#30340;&#20986;&#29616;&#20351;&#24471;Twitter&#31561;&#31038;&#20132;&#23186;&#20307;&#25104;&#20026;&#26816;&#27979;&#38476;&#29983;&#30340;&#26893;&#29289;&#20581;&#24247;&#20107;&#20214;&#30340;&#24191;&#27867;&#21442;&#19982;&#24179;&#21488;&#65292;&#21069;&#25552;&#26159;&#25105;&#20204;&#33021;&#22815;&#20174;&#38750;&#32467;&#26500;&#21270;&#30340;&#25991;&#26412;&#25968;&#25454;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#12290;ChouBERT&#26159;&#19968;&#20010;&#27861;&#35821;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#35782;&#21035;&#28041;&#21450;&#26893;&#29289;&#20581;&#24247;&#38382;&#39064;&#35266;&#23519;&#30340;&#25512;&#25991;&#65292;&#24182;&#23545;&#26410;&#35265;&#36807;&#30340;&#33258;&#28982;&#28798;&#23475;&#20855;&#26377;&#19968;&#33324;&#21270;&#33021;&#21147;&#12290;&#26412;&#25991;&#36890;&#36807;&#36827;&#19968;&#27493;&#30740;&#31350;ChouBERT&#22312;&#23567;&#22411;&#26631;&#35760;&#25968;&#25454;&#38598;&#19978;&#30340;&#20196;&#29260;&#32423;&#27880;&#37322;&#20219;&#21153;&#65292;&#35299;&#20915;&#20102;&#26631;&#35760;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
An important application scenario of precision agriculture is detecting and measuring crop health threats using sensors and data analysis techniques. However, the textual data are still under-explored among the existing solutions due to the lack of labelled data and fine-grained semantic resources. Recent research suggests that the increasing connectivity of farmers and the emergence of online farming communities make social media like Twitter a participatory platform for detecting unfamiliar plant health events if we can extract essential information from unstructured textual data. ChouBERT is a French pre-trained language model that can identify Tweets concerning observations of plant health issues with generalizability on unseen natural hazards. This paper tackles the lack of labelled data by further studying ChouBERT's know-how on token-level annotation tasks over small labeled sets.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#35270;&#35273;&#35821;&#35328;&#19968;&#33268;&#24615;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#35270;&#35273;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#36328;&#27169;&#24577;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2310.12520</link><description>&lt;p&gt;
&#36855;&#22833;&#22312;&#32763;&#35793;&#20013;&#65306;&#24403;GPT-4V(ision)&#26080;&#27861;&#19982;&#25991;&#26412;&#19968;&#33268;&#26102;&#12290;VLLM&#21644;&#26356;&#22810;&#30340;&#35270;&#35273;&#35821;&#35328;&#19968;&#33268;&#24615;&#20998;&#26512;&#12290; (arXiv:2310.12520v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
Lost in Translation: When GPT-4V(ision) Can't See Eye to Eye with Text. A Vision-Language-Consistency Analysis of VLLMs and Beyond. (arXiv:2310.12520v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#35270;&#35273;&#35821;&#35328;&#19968;&#33268;&#24615;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#35270;&#35273;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#36328;&#27169;&#24577;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22810;&#27169;&#24577;&#25216;&#26415;&#30340;&#36827;&#23637;&#20026;&#22312;&#28041;&#21450;&#25991;&#26412;&#12289;&#38899;&#39057;&#21644;&#22270;&#20687;&#22788;&#29702;&#30340;&#22810;&#26679;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#30340;&#27169;&#22411;&#24320;&#36767;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#21487;&#33021;&#24615;&#12290;&#20687;&#23558;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#35821;&#35328;&#24314;&#27169;&#32467;&#21512;&#22312;&#19968;&#36215;&#30340;GPT-4V&#36825;&#26679;&#30340;&#27169;&#22411;&#65292;&#22312;&#22797;&#26434;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#21162;&#21147;&#24050;&#32463;&#35748;&#30495;&#22320;&#32771;&#23519;&#20102;&#36825;&#20123;&#35270;&#35273;&#22823;&#35821;&#35328;&#27169;&#22411;(VLLMs)&#22312;&#23545;&#35937;&#26816;&#27979;&#12289;&#22270;&#20687;&#23383;&#24149;&#31561;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20998;&#26512;&#24448;&#24448;&#38598;&#20013;&#22312;&#35780;&#20272;&#27599;&#31181;&#27169;&#24577;&#22312;&#21333;&#29420;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#32570;&#20047;&#23545;&#23427;&#20204;&#36328;&#27169;&#24577;&#20132;&#20114;&#30340;&#27934;&#23519;&#12290;&#20855;&#20307;&#38382;&#39064;&#20851;&#20110;&#36825;&#20123;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#19968;&#33268;&#22320;&#25191;&#34892;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#65292;&#36824;&#26159;&#29420;&#31435;&#22320;&#25191;&#34892;&#65292;&#20173;&#28982;&#27809;&#26377;&#31572;&#26696;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#23545;&#22810;&#35821;&#35328;&#30340;&#26368;&#26032;&#30740;&#31350;&#65292;&#24182;&#23545;&#27169;&#22411;&#30340;&#36328;&#27169;&#24577;&#20132;&#20114;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31995;&#32479;&#26694;&#26550;&#65292;&#37327;&#21270;&#20102;&#33021;&#21147;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in multimodal techniques open exciting possibilities for models excelling in diverse tasks involving text, audio, and image processing. Models like GPT-4V, blending computer vision and language modeling, excel in complex text and image tasks. Numerous prior research endeavors have diligently examined the performance of these Vision Large Language Models (VLLMs) across tasks like object detection, image captioning and others. However, these analyses often focus on evaluating the performance of each modality in isolation, lacking insights into their cross-modal interactions. Specifically, questions concerning whether these vision-language models execute vision and language tasks consistently or independently have remained unanswered. In this study, we draw inspiration from recent investigations into multilingualism and conduct a comprehensive analysis of model's cross-modal interactions. We introduce a systematic framework that quantifies the capability disparities be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21487;&#36801;&#31227;&#30340;&#23545;&#25239;&#25915;&#20987;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#33258;&#21160;&#29983;&#25104;&#35780;&#20272;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;ChatGPT&#21644;Natural Questions&#65288;NQ&#65289;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.12516</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#36801;&#31227;&#30340;&#23545;&#25239;&#25915;&#20987;&#23454;&#29616;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#24187;&#35273;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Automatic Hallucination Assessment for Aligned Large Language Models via Transferable Adversarial Attacks. (arXiv:2310.12516v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21487;&#36801;&#31227;&#30340;&#23545;&#25239;&#25915;&#20987;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#33258;&#21160;&#29983;&#25104;&#35780;&#20272;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;ChatGPT&#21644;Natural Questions&#65288;NQ&#65289;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#21644;&#26816;&#32034;&#22686;&#24378;&#25216;&#26415;&#38450;&#27490;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24187;&#35273;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#34913;&#37327;LLM&#30340;&#21487;&#38752;&#24615;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#20154;&#24037;&#35780;&#20272;&#25968;&#25454;&#23545;&#20110;&#35768;&#22810;&#20219;&#21153;&#21644;&#39046;&#22495;&#26469;&#35828;&#24182;&#19981;&#21487;&#29992;&#19988;&#21487;&#33021;&#23384;&#22312;&#25968;&#25454;&#27844;&#28431;&#12290;&#21463;&#21040;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#36890;&#36807;&#36866;&#24403;&#20462;&#25913;LLM&#22312;&#20854;&#20013;&#34920;&#29616;&#24544;&#23454;&#30340;&#29616;&#26377;&#25968;&#25454;&#26469;&#33258;&#21160;&#29983;&#25104;&#35780;&#20272;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#26694;&#26550;AutoDebug&#65292;&#20351;&#29992;&#25552;&#31034;&#38142;&#25509;&#26469;&#29983;&#25104;&#20197;&#38382;&#31572;&#31034;&#20363;&#24418;&#24335;&#30340;&#21487;&#36801;&#31227;&#23545;&#25239;&#25915;&#20987;&#12290;&#25105;&#20204;&#24076;&#26395;&#20102;&#35299;&#36825;&#20123;&#31034;&#20363;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#35302;&#21457;&#20102;LLM&#30340;&#24187;&#35273;&#34892;&#20026;&#12290;&#25105;&#20204;&#20351;&#29992;ChatGPT&#23454;&#29616;&#20102;AutoDebug&#65292;&#24182;&#23545;&#19968;&#20010;&#28909;&#38376;&#30340;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#25968;&#25454;&#38598;Natural Questions&#65288;NQ&#65289;&#30340;&#20004;&#20010;&#21464;&#20307;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although remarkable progress has been achieved in preventing large language model (LLM) hallucinations using instruction tuning and retrieval augmentation, it remains challenging to measure the reliability of LLMs using human-crafted evaluation data which is not available for many tasks and domains and could suffer from data leakage. Inspired by adversarial machine learning, this paper aims to develop a method of automatically generating evaluation data by appropriately modifying existing data on which LLMs behave faithfully. Specifically, this paper presents AutoDebug, an LLM-based framework to use prompting chaining to generate transferable adversarial attacks in the form of question-answering examples. We seek to understand the extent to which these examples trigger the hallucination behaviors of LLMs.  We implement AutoDebug using ChatGPT and evaluate the resulting two variants of a popular open-domain question-answering dataset, Natural Questions (NQ), on a collection of open-sour
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;&#26469;&#32463;&#27982;&#22320;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25915;&#20987;&#25552;&#31034;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#27169;&#20223;&#20154;&#31867;&#29983;&#25104;&#30340;&#25552;&#31034;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#20132;&#20114;&#26469;&#21152;&#24378;&#21463;&#25915;&#20987;&#30340;LLMs&#30340;&#23433;&#20840;&#24615;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#19981;&#21516;LLMs&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12505</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32418;&#38431;&#25915;&#20987;&#25552;&#31034;&#29983;&#25104;&#21644;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Attack Prompt Generation for Red Teaming and Defending Large Language Models. (arXiv:2310.12505v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12505
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;&#26469;&#32463;&#27982;&#22320;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25915;&#20987;&#25552;&#31034;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#27169;&#20223;&#20154;&#31867;&#29983;&#25104;&#30340;&#25552;&#31034;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#20132;&#20114;&#26469;&#21152;&#24378;&#21463;&#25915;&#20987;&#30340;LLMs&#30340;&#23433;&#20840;&#24615;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#19981;&#21516;LLMs&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23481;&#26131;&#21463;&#21040;&#32418;&#38431;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;LLMs&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#25163;&#21160;&#25110;&#33258;&#21160;&#26041;&#27861;&#26500;&#24314;&#25915;&#20987;&#25552;&#31034;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#22312;&#26500;&#24314;&#25104;&#26412;&#21644;&#36136;&#37327;&#19978;&#37117;&#23384;&#22312;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#25163;&#21160;&#21644;&#33258;&#21160;&#26041;&#27861;&#65292;&#32463;&#27982;&#22320;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25915;&#20987;&#25552;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32771;&#34385;&#21040;&#26032;&#20852;LLMs&#30340;&#21331;&#36234;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25915;&#20987;&#26694;&#26550;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#25351;&#23548;LLMs&#27169;&#20223;&#20154;&#31867;&#29983;&#25104;&#30340;&#25552;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38450;&#24481;&#26694;&#26550;&#65292;&#36890;&#36807;&#19982;&#25915;&#20987;&#26694;&#26550;&#30340;&#36845;&#20195;&#20132;&#20114;&#26469;&#23545;&#21463;&#25915;&#20987;&#30340;LLMs&#36827;&#34892;&#24494;&#35843;&#65292;&#22686;&#24378;&#23427;&#20204;&#23545;&#32418;&#38431;&#25915;&#20987;&#30340;&#23433;&#20840;&#24615;&#12290;&#23545;&#19981;&#21516;LLMs&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#25915;&#20987;&#21644;&#38450;&#24481;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#31995;&#21015;&#25915;&#20987;&#25552;&#31034;&#25968;&#25454;&#38598;&#65292;&#21517;&#20026;SAP&#65292;&#22823;&#23567;&#19981;&#21516;&#65292;&#20415;&#20110;&#30740;&#31350;&#32773;&#36827;&#34892;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are susceptible to red teaming attacks, which can induce LLMs to generate harmful content. Previous research constructs attack prompts via manual or automatic methods, which have their own limitations on construction cost and quality. To address these issues, we propose an integrated approach that combines manual and automatic methods to economically generate high-quality attack prompts. Specifically, considering the impressive capabilities of newly emerged LLMs, we propose an attack framework to instruct LLMs to mimic human-generated prompts through in-context learning. Furthermore, we propose a defense framework that fine-tunes victim LLMs through iterative interactions with the attack framework to enhance their safety against red teaming attacks. Extensive experiments on different LLMs validate the effectiveness of our proposed attack and defense frameworks. Additionally, we release a series of attack prompts datasets named SAP with varying sizes, facili
&lt;/p&gt;</description></item><item><title>Co$^2$PT&#26159;&#19968;&#31181;&#36890;&#36807;&#21453;&#20107;&#23454;&#23545;&#27604;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#20943;&#36731;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#65292;&#24182;&#36866;&#24212;&#29616;&#26377;&#30340;&#21435;&#20559;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.12490</link><description>&lt;p&gt;
Co$^2$PT&#65306;&#36890;&#36807;&#21453;&#20107;&#23454;&#23545;&#27604;&#25552;&#31034;&#35843;&#25972;&#26469;&#32531;&#35299;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Co$^2$PT: Mitigating Bias in Pre-trained Language Models through Counterfactual Contrastive Prompt Tuning. (arXiv:2310.12490v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12490
&lt;/p&gt;
&lt;p&gt;
Co$^2$PT&#26159;&#19968;&#31181;&#36890;&#36807;&#21453;&#20107;&#23454;&#23545;&#27604;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#20943;&#36731;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#65292;&#24182;&#36866;&#24212;&#29616;&#26377;&#30340;&#21435;&#20559;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#35768;&#22810;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#20250;&#20174;&#22823;&#35268;&#27169;&#30340;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#32534;&#30721;&#31038;&#20250;&#20559;&#35265;&#65292;&#29978;&#33267;&#22312;&#21518;&#32493;&#24212;&#29992;&#20013;&#25918;&#22823;&#20559;&#35265;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Co$^2$PT&#65292;&#19968;&#31181;&#36890;&#36807;&#21453;&#20107;&#23454;&#23545;&#27604;&#25552;&#31034;&#35843;&#25972;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#20943;&#36731;&#20559;&#35265;&#30340;&#39640;&#25928;&#26377;&#25928;&#30340;&#21435;&#20559;&#35843;&#25972;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#22806;&#37096;&#20559;&#35265;&#22522;&#20934;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;Co$^2$PT&#22312;&#25552;&#31034;&#35843;&#25972;&#36807;&#31243;&#20013;&#20943;&#36731;&#20559;&#35265;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#23545;&#29616;&#26377;&#19978;&#28216;&#21435;&#20559;&#35821;&#35328;&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#20102;Co$^2$PT&#30340;&#20248;&#21183;&#65292;&#24182;&#20026;&#36827;&#19968;&#27493;&#25913;&#36827;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#20559;&#35265;&#20943;&#36731;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Language Models are widely used in many important real-world applications. However, recent studies show that these models can encode social biases from large pre-training corpora and even amplify biases in downstream applications. To address this challenge, we propose Co$^2$PT, an efficient and effective debias-while-prompt tuning method for mitigating biases via counterfactual contrastive prompt tuning on downstream tasks. Our experiments conducted on three extrinsic bias benchmarks demonstrate the effectiveness of Co$^2$PT on bias mitigation during the prompt tuning process and its adaptability to existing upstream debiased language models. These findings indicate the strength of Co$^2$PT and provide promising avenues for further enhancement in bias mitigation on downstream tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#38646;&#26679;&#26412;&#23398;&#20064;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#29983;&#21644;AI&#22312;&#20581;&#24247;&#21672;&#35810;&#20013;&#30340;&#22238;&#31572;&#30340;&#20934;&#30830;&#20998;&#31867;&#19978;&#30340;&#25928;&#26524;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#34429;&#28982;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#19968;&#33324;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24456;&#24378;&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#21307;&#30103;&#21672;&#35810;&#20013;&#65292;&#23427;&#20204;&#21487;&#33021;&#38656;&#35201;&#29305;&#23450;&#35821;&#26009;&#24211;&#35757;&#32451;&#25110;&#20854;&#20182;&#25216;&#26415;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#21307;&#29983;&#21644;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2310.12489</link><description>&lt;p&gt;
MedAI&#23545;&#35805;&#35821;&#26009;&#24211;&#65288;MEDIC&#65289;&#65306;&#38646;&#26679;&#26412;&#20998;&#31867;&#21307;&#29983;&#19982;AI&#22312;&#20581;&#24247;&#21672;&#35810;&#20013;&#30340;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;
MedAI Dialog Corpus (MEDIC): Zero-Shot Classification of Doctor and AI Responses in Health Consultations. (arXiv:2310.12489v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12489
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#38646;&#26679;&#26412;&#23398;&#20064;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#29983;&#21644;AI&#22312;&#20581;&#24247;&#21672;&#35810;&#20013;&#30340;&#22238;&#31572;&#30340;&#20934;&#30830;&#20998;&#31867;&#19978;&#30340;&#25928;&#26524;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#34429;&#28982;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#19968;&#33324;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24456;&#24378;&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#21307;&#30103;&#21672;&#35810;&#20013;&#65292;&#23427;&#20204;&#21487;&#33021;&#38656;&#35201;&#29305;&#23450;&#35821;&#26009;&#24211;&#35757;&#32451;&#25110;&#20854;&#20182;&#25216;&#26415;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#21307;&#29983;&#21644;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#20998;&#31867;&#20351;&#24471;&#21487;&#20197;&#23558;&#25991;&#26412;&#20998;&#31867;&#21040;&#22312;&#35757;&#32451;&#20013;&#27809;&#26377;&#35265;&#36807;&#30340;&#31867;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#38646;&#26679;&#26412;&#23398;&#20064;&#65292;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#20934;&#30830;&#20998;&#31867;&#26469;&#33258;&#21307;&#29983;&#21644;AI&#22312;&#20581;&#24247;&#21672;&#35810;&#20013;&#30340;&#22238;&#31572;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#30830;&#23450;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#22312;&#27809;&#26377;&#29305;&#23450;&#35821;&#26009;&#24211;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#26816;&#27979;&#25991;&#26412;&#26159;&#26469;&#33258;&#20154;&#31867;&#36824;&#26159;AI&#27169;&#22411;&#12290;&#23545;&#20110;&#25105;&#20204;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#26469;&#33258;&#21307;&#29983;&#23545;&#20110;&#24739;&#32773;&#20581;&#24247;&#21672;&#35810;&#30340;&#22238;&#31572;&#65292;&#24182;&#23545;&#21516;&#26679;&#30340;&#38382;&#39064;/&#22238;&#31572;&#25552;&#38382;&#20102;AI&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#34429;&#28982;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#19968;&#33324;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24456;&#24378;&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#21307;&#30103;&#21672;&#35810;&#20013;&#65292;&#23427;&#20204;&#21487;&#33021;&#38656;&#35201;&#29305;&#23450;&#35821;&#26009;&#24211;&#35757;&#32451;&#25110;&#20854;&#20182;&#25216;&#26415;&#20197;&#23454;&#29616;&#23545;&#21307;&#29983;&#21644;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#20934;&#30830;&#20998;&#31867;&#12290;&#20316;&#20026;&#22522;&#32447;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#20165;&#20381;&#38752;&#38646;&#26679;&#26412;&#20998;&#31867;&#22312;&#21307;&#30103;&#20998;&#31867;&#20013;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-shot classification has enabled the classification of text into classes that were not seen during training. In this paper, we investigate the effectiveness of pre-trained language models to accurately classify responses from Doctors and AI in health consultations through zero-shot learning. Our study aims to determine whether these models can effectively detect if a text originates from human or AI models without specific corpus training. For our experiments, we collected responses from doctors to patient inquiries about their health and posed the same question/response to AI models. Our findings revealed that while pre-trained language models demonstrate a strong understanding of language generally, they may require specific corpus training or other techniques to achieve accurate classification of doctor- and AI-generated text in healthcare consultations. As a baseline approach, this study shows the limitations of relying solely on zero-shot classification in medical classificati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#21270;&#20027;&#23548;&#38382;&#39064;&#65292;&#21457;&#29616;&#30001;&#20110;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#20027;&#35201;&#20351;&#29992;&#33521;&#35821;&#25968;&#25454;&#65292;&#24403;&#29992;&#25143;&#20351;&#29992;&#38750;&#33521;&#35821;&#35821;&#35328;&#25552;&#38382;&#26102;&#65292;&#27169;&#22411;&#24448;&#24448;&#25552;&#20379;&#19982;&#39044;&#26399;&#25991;&#21270;&#19981;&#30456;&#20851;&#30340;&#19981;&#24688;&#24403;&#31572;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#22810;&#26679;&#21270;&#25968;&#25454;&#39044;&#35757;&#32451;&#21644;&#25991;&#21270;&#24863;&#30693;&#25552;&#31034;&#20004;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.12481</link><description>&lt;p&gt;
&#24182;&#38750;&#25152;&#26377;&#22269;&#23478;&#37117;&#24198;&#31069;&#24863;&#24681;&#33410;&#65306;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#21270;&#20027;&#23548;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Not All Countries Celebrate Thanksgiving: On the Cultural Dominance in Large Language Models. (arXiv:2310.12481v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12481
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#21270;&#20027;&#23548;&#38382;&#39064;&#65292;&#21457;&#29616;&#30001;&#20110;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#20027;&#35201;&#20351;&#29992;&#33521;&#35821;&#25968;&#25454;&#65292;&#24403;&#29992;&#25143;&#20351;&#29992;&#38750;&#33521;&#35821;&#35821;&#35328;&#25552;&#38382;&#26102;&#65292;&#27169;&#22411;&#24448;&#24448;&#25552;&#20379;&#19982;&#39044;&#26399;&#25991;&#21270;&#19981;&#30456;&#20851;&#30340;&#19981;&#24688;&#24403;&#31572;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#22810;&#26679;&#21270;&#25968;&#25454;&#39044;&#35757;&#32451;&#21644;&#25991;&#21270;&#24863;&#30693;&#25552;&#31034;&#20004;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#23384;&#22312;&#30340;&#25991;&#21270;&#20027;&#23548;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#35813;&#38382;&#39064;&#28304;&#20110;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#20027;&#35201;&#20351;&#29992;&#33521;&#35821;&#25968;&#25454;&#65288;&#20363;&#22914;ChatGPT&#65289;&#12290;&#24403;&#29992;&#25143;&#20351;&#29992;&#38750;&#33521;&#35821;&#35821;&#35328;&#25552;&#38382;&#26102;&#65292;LLMs&#24448;&#24448;&#20250;&#25552;&#20379;&#19982;&#39044;&#26399;&#25991;&#21270;&#19981;&#30456;&#20851;&#30340;&#19981;&#24688;&#24403;&#30340;&#33521;&#35821;&#25991;&#21270;&#30456;&#20851;&#31572;&#26696;&#12290;&#20026;&#20102;&#31995;&#32479;&#35780;&#20272;&#25991;&#21270;&#20027;&#23548;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#20855;&#20307;&#25991;&#21270;&#23545;&#35937;&#65288;&#22914;&#20551;&#26085;&#21644;&#27468;&#26354;&#65289;&#21644;&#25277;&#35937;&#25991;&#21270;&#23545;&#35937;&#65288;&#22914;&#20215;&#20540;&#35266;&#21644;&#35266;&#28857;&#65289;&#30340;&#22522;&#20934;&#27979;&#35797;&#38598;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#20195;&#34920;&#24615;&#30340;GPT&#27169;&#22411;&#23384;&#22312;&#25991;&#21270;&#20027;&#23548;&#38382;&#39064;&#65292;&#20854;&#20013;GPT-4&#21463;&#21040;&#26368;&#20005;&#37325;&#24433;&#21709;&#65292;&#32780;text-davinci-003&#22312;&#36825;&#20010;&#38382;&#39064;&#19978;&#21463;&#24433;&#21709;&#26368;&#23567;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#22312;&#24320;&#21457;&#21644;&#37096;&#32626;&#36807;&#31243;&#20013;&#23545;&#25991;&#21270;&#20027;&#23548;&#38382;&#39064;&#36827;&#34892;&#25209;&#21028;&#24615;&#23457;&#35270;&#21644;&#20262;&#29702;&#32771;&#34385;&#30340;&#38656;&#35201;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20004;&#31181;&#30452;&#25509;&#30340;&#26041;&#27861;&#65306;&#27169;&#22411;&#24320;&#21457;&#20013;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#39044;&#35757;&#32451;&#21644;&#37096;&#32626;&#20013;&#30340;&#25991;&#21270;&#24863;&#30693;&#25552;&#31034;&#65292;&#21487;&#20197;&#26174;&#33879;&#32531;&#35299;&#25991;&#21270;&#20027;&#23548;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we identify a cultural dominance issue within large language models (LLMs) due to the predominant use of English data in model training (e.g. ChatGPT). LLMs often provide inappropriate English-culture-related answers that are not relevant to the expected culture when users ask in non-English languages. To systematically evaluate the cultural dominance issue, we build a benchmark that consists of both concrete (e.g. holidays and songs) and abstract (e.g. values and opinions) cultural objects. Empirical results show that the representative GPT models suffer from the culture dominance problem, where GPT-4 is the most affected while text-davinci-003 suffers the least from this problem. Our study emphasizes the need for critical examination of cultural dominance and ethical consideration in their development and deployment. We show two straightforward methods in model development (i.e. pretraining on more diverse data) and deployment (e.g. culture-aware prompting) can signifi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26159;&#39318;&#27425;&#25506;&#32034;&#20102;&#22312;&#35821;&#38899;&#22788;&#29702;&#20013;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#21487;&#33021;&#24615;&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#20013;&#21576;&#29616;LM&#35805;&#35821;-&#26631;&#31614;&#31034;&#33539;&#65292;&#35821;&#38899;LM&#21487;&#20197;&#22312;&#27809;&#26377;&#25991;&#26412;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#39564;&#35777;&#20102;&#22312;&#35821;&#38899;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;ICL&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12477</link><description>&lt;p&gt;
&#23545;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#36827;&#34892;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
An Exploration of In-Context Learning for Speech Language Model. (arXiv:2310.12477v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#39318;&#27425;&#25506;&#32034;&#20102;&#22312;&#35821;&#38899;&#22788;&#29702;&#20013;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#21487;&#33021;&#24615;&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#20013;&#21576;&#29616;LM&#35805;&#35821;-&#26631;&#31614;&#31034;&#33539;&#65292;&#35821;&#38899;LM&#21487;&#20197;&#22312;&#27809;&#26377;&#25991;&#26412;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#39564;&#35777;&#20102;&#22312;&#35821;&#38899;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;ICL&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;GPT-3&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#21457;&#23637;&#20197;&#26469;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26041;&#38754;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#36890;&#36807;&#22312;&#36755;&#20837;&#20013;&#21576;&#29616;LM&#35805;&#35821;-&#26631;&#31614;&#31034;&#33539;&#65292;LM&#21487;&#20197;&#22312;&#19981;&#20381;&#36182;&#26799;&#24230;&#19979;&#38477;&#25110;&#35201;&#27714;&#26174;&#24335;&#20462;&#25913;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#36825;&#20351;&#24471;LM&#33021;&#20197;&#40657;&#30418;&#30340;&#26041;&#24335;&#23398;&#20064;&#21644;&#35843;&#25972;&#12290;&#23613;&#31649;ICL&#22312;NLP&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22312;&#35821;&#38899;&#22788;&#29702;&#39046;&#22495;&#65292;&#24456;&#23569;&#26377;&#20154;&#30740;&#31350;ICL&#30340;&#21487;&#33021;&#24615;&#12290;&#26412;&#30740;&#31350;&#39318;&#27425;&#22312;&#27809;&#26377;&#25991;&#26412;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#25552;&#20986;&#20102;&#23545;&#35821;&#38899;LM&#30340;ICL&#30340;&#25506;&#32034;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#24403;&#21069;&#30340;&#35821;&#38899;LM&#27809;&#26377;ICL&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#28909;&#36523;&#35757;&#32451;&#65292;&#35821;&#38899;LM&#22240;&#27492;&#21487;&#20197;&#22312;&#26410;&#30693;&#20219;&#21153;&#19978;&#25191;&#34892;ICL&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#23545;&#35821;&#38899;LM&#22312;&#35821;&#38899;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;ICL&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ever since the development of GPT-3 in the natural language processing (NLP) field, in-context learning (ICL) has played an important role in utilizing large language models (LLMs). By presenting the LM utterance-label demonstrations at the input, the LM can accomplish few-shot learning without relying on gradient descent or requiring explicit modification of its parameters. This enables the LM to learn and adapt in a black-box manner. Despite the success of ICL in NLP, little work is exploring the possibility of ICL in speech processing. This study proposes the first exploration of ICL with a speech LM without text supervision. We first show that the current speech LM does not have the ICL capability. With the proposed warmup training, the speech LM can, therefore, perform ICL on unseen tasks. In this work, we verify the feasibility of ICL for speech LM on speech classification tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20998;&#26512;&#20102;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#20449;&#24687;&#24046;&#24322;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26469;&#32531;&#35299;&#36825;&#31181;&#20449;&#24687;&#24046;&#24322;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36127;&#26679;&#26412;&#26377;&#21161;&#20110;&#27169;&#22411;&#25913;&#36827;&#20854;&#25512;&#29702;&#29983;&#25104;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.12467</link><description>&lt;p&gt;
&#23545;&#35805;&#20013;&#30340;&#23545;&#27604;&#23398;&#20064;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Contrastive Learning for Inference in Dialogue. (arXiv:2310.12467v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20998;&#26512;&#20102;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#20449;&#24687;&#24046;&#24322;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26469;&#32531;&#35299;&#36825;&#31181;&#20449;&#24687;&#24046;&#24322;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36127;&#26679;&#26412;&#26377;&#21161;&#20110;&#27169;&#22411;&#25913;&#36827;&#20854;&#25512;&#29702;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;,&#23588;&#20854;&#26159;&#37027;&#20123;&#26469;&#33258;&#24402;&#32435;&#36807;&#31243;&#30340;&#25512;&#29702;,&#26159;&#25105;&#20204;&#23545;&#35805;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#29992;&#20110;&#34917;&#20805;&#30001;&#35762;&#35805;&#32773;&#38544;&#21547;&#25110;&#26126;&#30830;&#20256;&#36798;&#30340;&#20449;&#24687;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#22312;&#24402;&#32435;&#25512;&#29702;&#26041;&#38754;&#30340;&#34920;&#29616;&#36828;&#36828;&#33853;&#21518;&#20110;&#28436;&#32462;&#25512;&#29702;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26681;&#25454;&#35821;&#20041;&#20449;&#24687;&#24046;&#24322;&#26469;&#23450;&#20041;&#20219;&#21153;&#38590;&#24230;&#65292;&#20998;&#26512;&#20102;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#35813;&#24046;&#24322;&#21306;&#20998;&#20102;&#24402;&#32435;&#25512;&#29702;&#21644;&#28436;&#32462;&#25512;&#29702;&#65288;Johnson-Laird, 1988, 1993&#65289;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#23545;&#35805;&#19978;&#19979;&#25991;&#21644;&#25152;&#38656;&#25512;&#29702;&#20043;&#38388;&#20449;&#24687;&#24046;&#24322;&#30340;&#24046;&#36317;&#23545;&#24402;&#32435;&#25512;&#29702;&#36807;&#31243;&#26500;&#25104;&#20102;&#37325;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#20449;&#24687;&#24046;&#36317;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#36127;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36127;&#26679;&#26412;&#26377;&#21161;&#20110;&#27169;&#22411;&#29702;&#35299;&#38169;&#35823;&#24182;&#25913;&#36827;&#20854;&#25512;&#29702;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inference, especially those derived from inductive processes, is a crucial component in our conversation to complement the information implicitly or explicitly conveyed by a speaker. While recent large language models show remarkable advances in inference tasks, their performance in inductive reasoning, where not all information is present in the context, is far behind deductive reasoning. In this paper, we analyze the behavior of the models based on the task difficulty defined by the semantic information gap -- which distinguishes inductive and deductive reasoning (Johnson-Laird, 1988, 1993). Our analysis reveals that the disparity in information between dialogue contexts and desired inferences poses a significant challenge to the inductive inference process. To mitigate this information gap, we investigate a contrastive learning approach by feeding negative samples. Our experiments suggest negative samples help models understand what is wrong and improve their inference generations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26435;&#37325;&#21644;&#36755;&#20986;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#24674;&#22797;Transformer&#27169;&#22411;&#20013;&#30340;&#36755;&#20837;&#25968;&#25454;&#12290;&#30740;&#31350;&#32467;&#26524;&#26263;&#31034;&#27169;&#22411;&#35774;&#35745;&#23384;&#22312;&#28508;&#22312;&#30340;&#28431;&#27934;&#12290;</title><link>http://arxiv.org/abs/2310.12462</link><description>&lt;p&gt;
&#25581;&#31034;Transformer&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65306;&#22522;&#20110;&#27880;&#24847;&#21147;&#26435;&#37325;&#30340;&#25968;&#25454;&#24674;&#22797;&#30340;&#29702;&#35770;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unmasking Transformers: A Theoretical Approach to Data Recovery via Attention Weights. (arXiv:2310.12462v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26435;&#37325;&#21644;&#36755;&#20986;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#24674;&#22797;Transformer&#27169;&#22411;&#20013;&#30340;&#36755;&#20837;&#25968;&#25454;&#12290;&#30740;&#31350;&#32467;&#26524;&#26263;&#31034;&#27169;&#22411;&#35774;&#35745;&#23384;&#22312;&#28508;&#22312;&#30340;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;Transformer&#24050;&#32463;&#25104;&#20026;&#20102;&#19968;&#31181;&#20027;&#23548;&#30340;&#26550;&#26500;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#26377;&#20851;&#36825;&#20123;&#27169;&#22411;&#22788;&#29702;&#25968;&#25454;&#30340;&#23433;&#20840;&#24615;&#21644;&#38544;&#31169;&#24615;&#30340;&#38382;&#39064;&#24050;&#32463;&#24341;&#36215;&#20102;&#20851;&#27880;&#12290;&#26412;&#25991;&#38024;&#23545;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65306;&#26159;&#21542;&#21487;&#20197;&#20351;&#29992;Transformer&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;&#21644;&#36755;&#20986;&#26469;&#24674;&#22797;&#36755;&#20837;&#25968;&#25454;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#26368;&#23567;&#21270;&#25439;&#22833;&#20989;&#25968;$L(X)$&#20174;&#32473;&#23450;&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;$W = QK^\top$&#21644;&#36755;&#20986;$B$&#20013;&#24674;&#22797;&#36755;&#20837;&#25968;&#25454;$X$&#65292;&#20854;&#20013;$X \in \mathbb{R}^{d \times n}$&#65292;$W \in \mathbb{R}^{d \times d}$&#65292;$B \in \mathbb{R}^{n \times n}$&#12290;&#36825;&#20010;&#25439;&#22833;&#20989;&#25968;&#25429;&#25417;&#20102;&#39044;&#26399;&#36755;&#20986;&#19982;&#23454;&#38469;&#36755;&#20986;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#23545;&#20110;&#23616;&#37096;&#21270;&#20998;&#23618;&#26426;&#21046;&#65288;Localized Layer-wise Mechanism&#65292;LLM&#65289;&#20855;&#26377;&#37325;&#35201;&#30340;&#24433;&#21709;&#65292;&#34920;&#26126;&#27169;&#22411;&#35774;&#35745;&#23384;&#22312;&#28508;&#22312;&#30340;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of deep learning, transformers have emerged as a dominant architecture, particularly in natural language processing tasks. However, with their widespread adoption, concerns regarding the security and privacy of the data processed by these models have arisen. In this paper, we address a pivotal question: Can the data fed into transformers be recovered using their attention weights and outputs? We introduce a theoretical framework to tackle this problem. Specifically, we present an algorithm that aims to recover the input data $X \in \mathbb{R}^{d \times n}$ from given attention weights $W = QK^\top \in \mathbb{R}^{d \times d}$ and output $B \in \mathbb{R}^{n \times n}$ by minimizing the loss function $L(X)$. This loss function captures the discrepancy between the expected output and the actual output of the transformer. Our findings have significant implications for the Localized Layer-wise Mechanism (LLM), suggesting potential vulnerabilities in the model's design from a s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#24605;&#32771;&#20102;&#26500;&#24314;&#29992;&#20110;&#29702;&#35299;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26426;&#21046;&#30340;&#26377;&#25928;&#24230;&#37327;&#26041;&#27861;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#31995;&#21015;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#26641;&#25299;&#25169;&#25506;&#38024;&#27169;&#22411;&#23545;BERT-large&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2310.12454</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#26500;&#24314;&#29992;&#20110;&#29702;&#35299;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26426;&#21046;&#30340;&#26377;&#25928;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rethinking the Construction of Effective Metrics for Understanding the Mechanisms of Pretrained Language Models. (arXiv:2310.12454v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#24605;&#32771;&#20102;&#26500;&#24314;&#29992;&#20110;&#29702;&#35299;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26426;&#21046;&#30340;&#26377;&#25928;&#24230;&#37327;&#26041;&#27861;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#31995;&#21015;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#26641;&#25299;&#25169;&#25506;&#38024;&#27169;&#22411;&#23545;BERT-large&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#34987;&#26399;&#26395;&#33021;&#22815;&#26377;&#25928;&#22320;&#23558;&#36755;&#20837;&#25991;&#26412;&#26144;&#23556;&#21040;&#19968;&#32452;&#21521;&#37327;&#65292;&#21516;&#26102;&#20445;&#30041;&#25991;&#26412;&#20869;&#22312;&#30340;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#35774;&#35745;&#19968;&#20010;&#30333;&#30418;&#27169;&#22411;&#26469;&#35745;&#31639;&#21453;&#26144;&#36825;&#20123;&#21521;&#37327;&#20869;&#37096;&#20851;&#31995;&#23384;&#22312;&#30340;&#24230;&#37327;&#25104;&#20026;&#20998;&#26512;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21518;&#32493;&#21487;&#35299;&#37322;&#24615;&#30340;&#24120;&#35265;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#28304;&#27169;&#22411;&#32570;&#20047;&#20869;&#22312;&#21487;&#35299;&#37322;&#24615;&#26102;&#65292;&#22312;&#30333;&#30418;&#27169;&#22411;&#20013;&#23454;&#29616;&#21487;&#35299;&#37322;&#24615;&#24182;&#20445;&#35777;&#24230;&#37327;&#35745;&#31639;&#30340;&#20005;&#35880;&#24615;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;&#36825;&#31181;&#26435;&#34913;&#20013;&#23547;&#25214;&#24179;&#34913;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#26500;&#24314;&#29702;&#35299;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26426;&#21046;&#30340;&#24230;&#37327;&#26041;&#27861;&#12290;&#25105;&#20204;&#29305;&#21035;&#35774;&#35745;&#20102;&#19968;&#31995;&#21015;&#27839;&#36825;&#19968;&#30740;&#31350;&#26041;&#21521;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#20013;&#30340;&#26641;&#25299;&#25169;&#25506;&#38024;&#27169;&#22411;&#23545;BERT-large&#36827;&#34892;&#20102;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models are expected to effectively map input text to a set of vectors while preserving the inherent relationships within the text. Consequently, designing a white-box model to compute metrics that reflect the presence of specific internal relations in these vectors has become a common approach for post-hoc interpretability analysis of pretrained language models. However, achieving interpretability in white-box models and ensuring the rigor of metric computation becomes challenging when the source model lacks inherent interpretability. Therefore, in this paper, we discuss striking a balance in this trade-off and propose a novel line to constructing metrics for understanding the mechanisms of pretrained language models. We have specifically designed a family of metrics along this line of investigation, and the model used to compute these metrics is referred to as the tree topological probe. We conducted measurements on BERT-large by using these metrics. Based on the e
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#38646;&#26679;&#26412;&#23454;&#20307;&#38142;&#25509;&#30340;&#38405;&#35835;&#21644;&#36873;&#25321;&#26694;&#26550;&#65292;&#36890;&#36807;&#24314;&#27169;&#23454;&#20307;&#28040;&#27495;&#30340;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#21363;&#23454;&#20307;&#25552;&#21450;-&#23454;&#20307;&#21305;&#37197;&#21644;&#23454;&#20307;&#20043;&#38388;&#30340;&#27604;&#36739;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.12450</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#38646;&#26679;&#26412;&#23454;&#20307;&#38142;&#25509;&#30340;&#38405;&#35835;&#21644;&#36873;&#25321;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Read-and-Select Framework for Zero-shot Entity Linking. (arXiv:2310.12450v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12450
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#38646;&#26679;&#26412;&#23454;&#20307;&#38142;&#25509;&#30340;&#38405;&#35835;&#21644;&#36873;&#25321;&#26694;&#26550;&#65292;&#36890;&#36807;&#24314;&#27169;&#23454;&#20307;&#28040;&#27495;&#30340;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#21363;&#23454;&#20307;&#25552;&#21450;-&#23454;&#20307;&#21305;&#37197;&#21644;&#23454;&#20307;&#20043;&#38388;&#30340;&#27604;&#36739;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#23454;&#20307;&#38142;&#25509;&#26088;&#22312;&#23558;&#23454;&#20307;&#25552;&#21450;&#19982;&#26410;&#35265;&#23454;&#20307;&#36827;&#34892;&#23545;&#40784;&#65292;&#25361;&#25112;&#27867;&#21270;&#33021;&#21147;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#20505;&#36873;&#26816;&#32034;&#38454;&#27573;&#65292;&#24573;&#35270;&#20102;&#23454;&#36136;&#24615;&#30340;&#20505;&#36873;&#25490;&#24207;&#38454;&#27573;&#65292;&#36825;&#19968;&#38454;&#27573;&#23558;&#23454;&#20307;&#36827;&#34892;&#28040;&#27495;&#65292;&#20174;&#32780;&#20570;&#20986;&#26368;&#32456;&#30340;&#38142;&#25509;&#39044;&#27979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38405;&#35835;&#21644;&#36873;&#25321;&#65288;ReS&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#23454;&#20307;&#28040;&#27495;&#30340;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#36827;&#34892;&#24314;&#27169;&#65292;&#21363;&#23454;&#20307;&#25552;&#21450;-&#23454;&#20307;&#21305;&#37197;&#21644;&#23454;&#20307;&#20043;&#38388;&#30340;&#27604;&#36739;&#12290;&#39318;&#20808;&#65292;&#38405;&#35835;&#27169;&#22359;&#21033;&#29992;&#25552;&#21450;&#19978;&#19979;&#25991;&#29983;&#25104;&#25552;&#21450;&#24863;&#30693;&#30340;&#23454;&#20307;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#25552;&#21450;-&#23454;&#20307;&#21305;&#37197;&#12290;&#28982;&#21518;&#65292;&#22312;&#36873;&#25321;&#27169;&#22359;&#20013;&#65292;&#25105;&#20204;&#23558;&#20505;&#36873;&#30340;&#36873;&#25321;&#35270;&#20026;&#19968;&#20010;&#24207;&#21015;&#26631;&#35760;&#38382;&#39064;&#65292;&#24182;&#23558;&#25152;&#26377;&#20505;&#36873;&#34920;&#31034;&#36827;&#34892;&#34701;&#21512;&#65292;&#20197;&#23454;&#29616;&#23454;&#20307;&#20043;&#38388;&#30340;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24050;&#24314;&#31435;&#30340;&#38646;&#26679;&#26412;&#23454;&#20307;&#38142;&#25509;&#25968;&#25454;&#38598;ZESHEL&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;&#31934;&#24230;&#20026;2.55%&#65292;&#26080;&#38656;&#32791;&#26102;&#30340;&#20154;&#24037;&#21171;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-shot entity linking (EL) aims at aligning entity mentions to unseen entities to challenge the generalization ability. Previous methods largely focus on the candidate retrieval stage and ignore the essential candidate ranking stage, which disambiguates among entities and makes the final linking prediction. In this paper, we propose a read-and-select (ReS) framework by modeling the main components of entity disambiguation, i.e., mention-entity matching and cross-entity comparison. First, for each candidate, the reading module leverages mention context to output mention-aware entity representations, enabling mention-entity matching. Then, in the selecting module, we frame the choice of candidates as a sequence labeling problem, and all candidate representations are fused together to enable cross-entity comparison. Our method achieves the state-of-the-art performance on the established zero-shot EL dataset ZESHEL with a 2.55\% micro-average accuracy gain, with no need for laborious mu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#23569;&#26679;&#26412;&#23454;&#20307;&#38142;&#25509;&#20013;&#30340;&#31232;&#30095;&#26816;&#32034;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;ELECTRA&#27169;&#22411;&#25552;&#21462;&#20851;&#38190;&#35789;&#26469;&#25913;&#21892;&#26597;&#35810;&#34920;&#36798;&#24335;&#65292;&#22312;ZESHEL&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#25152;&#26377;&#27979;&#35797;&#39046;&#22495;&#20013;&#20248;&#20110;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.12444</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#31232;&#30095;&#26816;&#32034;&#22312;&#23569;&#26679;&#26412;&#23454;&#20307;&#38142;&#25509;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Revisiting Sparse Retrieval for Few-shot Entity Linking. (arXiv:2310.12444v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#23569;&#26679;&#26412;&#23454;&#20307;&#38142;&#25509;&#20013;&#30340;&#31232;&#30095;&#26816;&#32034;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;ELECTRA&#27169;&#22411;&#25552;&#21462;&#20851;&#38190;&#35789;&#26469;&#25913;&#21892;&#26597;&#35810;&#34920;&#36798;&#24335;&#65292;&#22312;ZESHEL&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#25152;&#26377;&#27979;&#35797;&#39046;&#22495;&#20013;&#20248;&#20110;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#38142;&#25509;&#26088;&#22312;&#23558;&#27169;&#31946;&#30340;&#25552;&#21450;&#38142;&#25509;&#21040;&#30693;&#35782;&#24211;&#20013;&#30340;&#30456;&#24212;&#23454;&#20307;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26469;&#33258;&#20110;&#29305;&#23450;&#39046;&#22495;&#26631;&#35760;&#25968;&#25454;&#19981;&#36275;&#12290;&#23613;&#31649;&#23494;&#38598;&#30340;&#26816;&#32034;&#26041;&#27861;&#22312;&#19968;&#20123;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#24403;&#21482;&#26377;&#26377;&#38480;&#25968;&#37327;&#30340;&#39046;&#22495;&#20869;&#26631;&#35760;&#25968;&#25454;&#26102;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#22312;&#36825;&#31181;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#31232;&#30095;&#26816;&#32034;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ELECTRA&#30340;&#20851;&#38190;&#35789;&#25552;&#21462;&#22120;&#26469;&#21435;&#38500;&#25552;&#21450;&#19978;&#19979;&#25991;&#30340;&#22122;&#22768;&#24182;&#26500;&#24314;&#26356;&#22909;&#30340;&#26597;&#35810;&#34920;&#36798;&#24335;&#12290;&#20026;&#20102;&#35757;&#32451;&#25552;&#21462;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36828;&#31243;&#30417;&#30563;&#26041;&#27861;&#65292;&#26681;&#25454;&#25552;&#21450;&#19978;&#19979;&#25991;&#21644;&#23454;&#20307;&#25551;&#36848;&#20043;&#38388;&#30340;&#37325;&#21472;&#26631;&#35760;&#33258;&#21160;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#12290;&#22312;ZESHEL&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#25152;&#26377;&#27979;&#35797;&#39046;&#22495;&#20013;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#24615;&#33021;&#26377;&#26174;&#33879;&#25552;&#39640;&#65292;&#23637;&#31034;&#20102;&#20851;&#38190;&#35789;&#22686;&#24378;&#30340;&#31232;&#30095;&#26816;&#32034;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity linking aims to link ambiguous mentions to their corresponding entities in a knowledge base. One of the key challenges comes from insufficient labeled data for specific domains. Although dense retrievers have achieved excellent performance on several benchmarks, their performance decreases significantly when only a limited amount of in-domain labeled data is available. In such few-shot setting, we revisit the sparse retrieval method, and propose an ELECTRA-based keyword extractor to denoise the mention context and construct a better query expression. For training the extractor, we propose a distant supervision method to automatically generate training data based on overlapping tokens between mention contexts and entity descriptions. Experimental results on the ZESHEL dataset demonstrate that the proposed method outperforms state-of-the-art models by a significant margin across all test domains, showing the effectiveness of keyword-enhanced sparse retrieval.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#26816;&#32034;&#26694;&#26550;&#65292;&#26088;&#22312;&#23558;LLM&#36716;&#21464;&#20026;&#19968;&#20010;&#30456;&#20851;&#12289;&#36127;&#36131;&#20219;&#19988;&#21487;&#20449;&#36182;&#30340;&#25628;&#32034;&#22120;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#29983;&#25104;&#22120;&#12289;&#39564;&#35777;&#22120;&#21644;&#20248;&#21270;&#22120;&#19977;&#20010;&#26680;&#24515;&#27169;&#22359;&#65292;&#20998;&#21035;&#29992;&#20110;&#29983;&#25104;&#21487;&#20449;&#36182;&#30340;&#22312;&#32447;&#26469;&#28304;&#12289;&#39564;&#35777;&#26469;&#28304;&#21487;&#38752;&#24615;&#21644;&#20248;&#21270;&#19981;&#21487;&#20449;&#36182;&#30340;&#26469;&#28304;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#22312;&#30456;&#20851;&#24615;&#12289;&#36127;&#36131;&#20219;&#24615;&#21644;&#21487;&#20449;&#24230;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.12443</link><description>&lt;p&gt;
&#20102;&#35299;&#20309;&#22788;&#21069;&#24448;&#65306;&#20351;LLM&#25104;&#20026;&#19968;&#20010;&#30456;&#20851;&#12289;&#36127;&#36131;&#20219;&#19988;&#21487;&#20449;&#36182;&#30340;&#25628;&#32034;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Know Where to Go: Make LLM a Relevant, Responsible, and Trustworthy Searcher. (arXiv:2310.12443v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12443
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#26816;&#32034;&#26694;&#26550;&#65292;&#26088;&#22312;&#23558;LLM&#36716;&#21464;&#20026;&#19968;&#20010;&#30456;&#20851;&#12289;&#36127;&#36131;&#20219;&#19988;&#21487;&#20449;&#36182;&#30340;&#25628;&#32034;&#22120;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#29983;&#25104;&#22120;&#12289;&#39564;&#35777;&#22120;&#21644;&#20248;&#21270;&#22120;&#19977;&#20010;&#26680;&#24515;&#27169;&#22359;&#65292;&#20998;&#21035;&#29992;&#20110;&#29983;&#25104;&#21487;&#20449;&#36182;&#30340;&#22312;&#32447;&#26469;&#28304;&#12289;&#39564;&#35777;&#26469;&#28304;&#21487;&#38752;&#24615;&#21644;&#20248;&#21270;&#19981;&#21487;&#20449;&#36182;&#30340;&#26469;&#28304;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#22312;&#30456;&#20851;&#24615;&#12289;&#36127;&#36131;&#20219;&#24615;&#21644;&#21487;&#20449;&#24230;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#24050;&#32463;&#26174;&#31034;&#20986;&#23427;&#22312;&#25552;&#39640;&#25628;&#32034;&#30456;&#20851;&#24615;&#21644;&#25552;&#20379;&#30452;&#25509;&#31572;&#26696;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20256;&#32479;&#20449;&#24687;&#26816;&#32034;&#31639;&#27861;&#30340;&#23616;&#38480;&#24615;&#21644;LLM&#30340;&#38169;&#35273;&#38382;&#39064;&#65292;&#39564;&#35777;&#29983;&#25104;&#32467;&#26524;&#30340;&#21487;&#38752;&#24615;&#21644;&#36129;&#29486;&#26469;&#28304;&#30340;&#21487;&#20449;&#24230;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20026;&#20102;&#21019;&#24314;LLM&#26102;&#20195;&#30340;&#8220;PageRank&#8221;&#65292;&#25105;&#20204;&#33268;&#21147;&#20110;&#23558;LLM&#36716;&#21464;&#20026;&#19968;&#20010;&#30456;&#20851;&#12289;&#36127;&#36131;&#20219;&#19988;&#21487;&#20449;&#36182;&#30340;&#25628;&#32034;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#29983;&#25104;&#26816;&#32034;&#26694;&#26550;&#65292;&#21033;&#29992;LLM&#30340;&#30693;&#35782;&#24314;&#31435;&#26597;&#35810;&#21644;&#22312;&#32447;&#26469;&#28304;&#20043;&#38388;&#30340;&#30452;&#25509;&#38142;&#25509;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#19977;&#20010;&#26680;&#24515;&#27169;&#22359;&#65306;&#29983;&#25104;&#22120;&#12289;&#39564;&#35777;&#22120;&#21644;&#20248;&#21270;&#22120;&#65292;&#20998;&#21035;&#19987;&#27880;&#20110;&#29983;&#25104;&#21487;&#20449;&#36182;&#30340;&#22312;&#32447;&#26469;&#28304;&#12289;&#39564;&#35777;&#26469;&#28304;&#30340;&#21487;&#38752;&#24615;&#21644;&#20248;&#21270;&#19981;&#21487;&#20449;&#36182;&#30340;&#26469;&#28304;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#30456;&#20851;&#24615;&#12289;&#36127;&#36131;&#20219;&#24615;&#21644;&#21487;&#20449;&#24230;&#26041;&#38754;&#30456;&#23545;&#20110;&#21508;&#31181;SOTA&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of Large Language Models (LLMs) has shown the potential to improve relevance and provide direct answers in web searches. However, challenges arise in validating the reliability of generated results and the credibility of contributing sources, due to the limitations of traditional information retrieval algorithms and the LLM hallucination problem. Aiming to create a "PageRank" for the LLM era, we strive to transform LLM into a relevant, responsible, and trustworthy searcher. We propose a novel generative retrieval framework leveraging the knowledge of LLMs to foster a direct link between queries and online sources. This framework consists of three core modules: Generator, Validator, and Optimizer, each focusing on generating trustworthy online sources, verifying source reliability, and refining unreliable sources, respectively. Extensive experiments and evaluations highlight our method's superior relevance, responsibility, and trustfulness against various SOTA methods.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#38271;&#31243;Transformer&#27169;&#22411;MASFormer&#65292;&#36890;&#36807;&#22312;&#23569;&#25968;&#23618;&#20351;&#29992;&#20840;&#23616;&#27880;&#24847;&#21147;&#21644;&#22312;&#20854;&#20182;&#23618;&#20351;&#29992;&#31232;&#30095;&#27880;&#24847;&#21147;&#65292;&#23454;&#29616;&#20102;&#22312;&#20855;&#26377;&#38271;&#24207;&#21015;&#30340;&#20219;&#21153;&#20013;&#39640;&#25928;&#30340;&#35745;&#31639;&#21644;&#24314;&#27169;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.12442</link><description>&lt;p&gt;
&#39640;&#25928;&#38271;&#31243;Transformer&#65306;&#38656;&#35201;&#26356;&#22810;&#20851;&#27880;&#65292;&#20294;&#19981;&#19968;&#23450;&#22312;&#27599;&#19968;&#23618;&#37117;&#38656;&#35201;
&lt;/p&gt;
&lt;p&gt;
Efficient Long-Range Transformers: You Need to Attend More, but Not Necessarily at Every Layer. (arXiv:2310.12442v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12442
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#38271;&#31243;Transformer&#27169;&#22411;MASFormer&#65292;&#36890;&#36807;&#22312;&#23569;&#25968;&#23618;&#20351;&#29992;&#20840;&#23616;&#27880;&#24847;&#21147;&#21644;&#22312;&#20854;&#20182;&#23618;&#20351;&#29992;&#31232;&#30095;&#27880;&#24847;&#21147;&#65292;&#23454;&#29616;&#20102;&#22312;&#20855;&#26377;&#38271;&#24207;&#21015;&#30340;&#20219;&#21153;&#20013;&#39640;&#25928;&#30340;&#35745;&#31639;&#21644;&#24314;&#27169;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#27169;&#22411;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#26469;&#25429;&#25417;&#24207;&#21015;&#20013;&#30340;&#38271;&#31243;&#21644;&#30701;&#31243;&#20381;&#36182;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#20840;&#23616;&#27880;&#24847;&#26426;&#21046;&#30340;&#35745;&#31639;&#25104;&#26412;&#19982;&#24207;&#21015;&#38271;&#24230;&#21576;&#20108;&#27425;&#20851;&#31995;&#65292;&#22312;&#20855;&#26377;&#38271;&#24207;&#21015;&#30340;&#20219;&#21153;&#20013;&#65288;&#20363;&#22914;8k&#20010;&#26631;&#35760;&#30340;&#36755;&#20837;&#65289;&#26159;&#19981;&#21487;&#25215;&#21463;&#30340;&#12290;&#23613;&#31649;&#29616;&#26377;&#24037;&#20316;&#20013;&#24314;&#35758;&#20351;&#29992;&#31232;&#30095;&#27880;&#24847;&#21147;&#26469;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#65292;&#20294;&#23427;&#30340;&#24314;&#27169;&#33021;&#21147;&#26377;&#38480;&#65292;&#24448;&#24448;&#26080;&#27861;&#25429;&#25417;&#38271;&#24207;&#21015;&#20013;&#30340;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MASFormer&#65292;&#19968;&#31181;&#26131;&#20110;&#23454;&#29616;&#30340;&#21464;&#31181;Transformer&#65292;&#20855;&#26377;&#28151;&#21512;&#27880;&#24847;&#33539;&#22260;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MASFormer&#37197;&#22791;&#20102;&#20840;&#23616;&#27880;&#24847;&#21147;&#26469;&#25429;&#25417;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#65292;&#20294;&#21482;&#22312;&#23569;&#25968;&#20960;&#23618;&#20351;&#29992;&#12290;&#23545;&#20110;&#21097;&#20313;&#23618;&#65292;MASFormer&#21482;&#37319;&#29992;&#31232;&#30095;&#27880;&#24847;&#21147;&#26469;&#25429;&#25417;&#30701;&#31243;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;n&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MASFormer&#22312;&#38271;&#24207;&#21015;&#20219;&#21153;&#19978;&#20855;&#26377;&#36739;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#24314;&#27169;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained transformer models have demonstrated remarkable performance across various natural language processing tasks. These models leverage the attention mechanism to capture long- and short-range dependencies in the sequence. However, the (full) attention mechanism incurs high computational cost quadratic in the sequence length, which is not affordable in tasks with long sequences, e.g., inputs with 8k tokens. Although sparse attention can be used to improve computational efficiency, as suggested in existing work, it has limited modeling capacity and often fails to capture complicated dependencies in long sequences. To tackle this challenge, we propose MASFormer, an easy-to-implement transformer variant with Mixed Attention Spans. Specifically, MASFormer is equipped with full attention to capture long-range dependencies, but only at a small number of layers. For the remaining layers, MASformer only employs sparse attention to capture short-range dependencies. Our experiments on n
&lt;/p&gt;</description></item><item><title>PoisonPrompt&#26159;&#19968;&#31181;&#26032;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#33021;&#22815;&#25104;&#21151;&#22320;&#30772;&#22351;&#22522;&#20110;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#25915;&#20987;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12289;&#20445;&#30495;&#24230;&#21644;&#40065;&#26834;&#24615;&#32463;&#36807;&#20102;&#24191;&#27867;&#23454;&#39564;&#39564;&#35777;&#65292;&#24378;&#35843;&#20102;&#22522;&#20110;&#25552;&#31034;&#30340;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30340;&#23433;&#20840;&#23041;&#32961;&#21644;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12439</link><description>&lt;p&gt;
PoisonPrompt: &#22522;&#20110;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
PoisonPrompt: Backdoor Attack on Prompt-based Large Language Models. (arXiv:2310.12439v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12439
&lt;/p&gt;
&lt;p&gt;
PoisonPrompt&#26159;&#19968;&#31181;&#26032;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#33021;&#22815;&#25104;&#21151;&#22320;&#30772;&#22351;&#22522;&#20110;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#25915;&#20987;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12289;&#20445;&#30495;&#24230;&#21644;&#40065;&#26834;&#24615;&#32463;&#36807;&#20102;&#24191;&#27867;&#23454;&#39564;&#39564;&#35777;&#65292;&#24378;&#35843;&#20102;&#22522;&#20110;&#25552;&#31034;&#30340;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30340;&#23433;&#20840;&#23041;&#32961;&#21644;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25552;&#31034;&#26174;&#33879;&#25913;&#21892;&#20102;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#20351;&#24471;&#23427;&#20204;&#22312;&#21508;&#31181;LLM&#24212;&#29992;&#22330;&#26223;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#19981;&#21487;&#25110;&#32570;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22522;&#20110;&#25552;&#31034;&#30340;LLM&#32780;&#35328;&#65292;&#21518;&#38376;&#28431;&#27934;&#8212;&#8212;&#19968;&#31181;&#21487;&#20197;&#24694;&#24847;&#26356;&#25913;&#21463;&#23475;&#27169;&#22411;&#27491;&#24120;&#39044;&#27979;&#30340;&#20005;&#37325;&#23433;&#20840;&#23041;&#32961;&#8212;&#8212;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30340;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21518;&#38376;&#25915;&#20987;POISONPROMPT&#65292;&#33021;&#22815;&#25104;&#21151;&#22320;&#30772;&#22351;&#30828;&#20214;&#21644;&#36719;&#20214;&#22522;&#20110;&#25552;&#31034;&#30340;LLM&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#19977;&#31181;&#27969;&#34892;&#30340;&#25552;&#31034;&#26041;&#27861;&#12289;&#20845;&#20010;&#25968;&#25454;&#38598;&#21644;&#19977;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;LLM&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#26469;&#35780;&#20272;POISONPROMPT&#30340;&#26377;&#25928;&#24615;&#12289;&#20445;&#30495;&#24230;&#21644;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#22522;&#20110;&#25552;&#31034;&#30340;LLM&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#28508;&#22312;&#23433;&#20840;&#23041;&#32961;&#65292;&#24182;&#24378;&#35843;&#20102;&#22312;&#36825;&#20010;&#39046;&#22495;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompts have significantly improved the performance of pretrained Large Language Models (LLMs) on various downstream tasks recently, making them increasingly indispensable for a diverse range of LLM application scenarios. However, the backdoor vulnerability, a serious security threat that can maliciously alter the victim model's normal predictions, has not been sufficiently explored for prompt-based LLMs. In this paper, we present POISONPROMPT, a novel backdoor attack capable of successfully compromising both hard and soft prompt-based LLMs. We evaluate the effectiveness, fidelity, and robustness of POISONPROMPT through extensive experiments on three popular prompt methods, using six datasets and three widely used LLMs. Our findings highlight the potential security threats posed by backdoor attacks on prompt-based LLMs and emphasize the need for further research in this area.
&lt;/p&gt;</description></item><item><title>DocXChain&#26159;&#19968;&#20010;&#24320;&#28304;&#24037;&#20855;&#38142;&#65292;&#36890;&#36807;&#25991;&#26723;&#35299;&#26512;&#23558;&#38750;&#32467;&#26500;&#21270;&#25991;&#26723;&#36716;&#25442;&#20026;&#21487;&#35835;&#21487;&#25805;&#20316;&#30340;&#32467;&#26500;&#21270;&#34920;&#31034;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#26412;&#33021;&#21147;&#21644;&#23436;&#20840;&#21151;&#33021;&#30340;&#25991;&#26723;&#35299;&#26512;&#27969;&#27700;&#32447;&#65292;&#21487;&#20197;&#19982;&#20854;&#20182;&#24037;&#20855;&#21644;&#27169;&#22411;&#38598;&#25104;&#65292;&#29992;&#20110;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.12430</link><description>&lt;p&gt;
DocXChain: &#19968;&#20010;&#24378;&#22823;&#30340;&#24320;&#28304;&#24037;&#20855;&#38142;&#29992;&#20110;&#25991;&#26723;&#35299;&#26512;&#21450;&#26356;&#22810;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
DocXChain: A Powerful Open-Source Toolchain for Document Parsing and Beyond. (arXiv:2310.12430v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12430
&lt;/p&gt;
&lt;p&gt;
DocXChain&#26159;&#19968;&#20010;&#24320;&#28304;&#24037;&#20855;&#38142;&#65292;&#36890;&#36807;&#25991;&#26723;&#35299;&#26512;&#23558;&#38750;&#32467;&#26500;&#21270;&#25991;&#26723;&#36716;&#25442;&#20026;&#21487;&#35835;&#21487;&#25805;&#20316;&#30340;&#32467;&#26500;&#21270;&#34920;&#31034;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#26412;&#33021;&#21147;&#21644;&#23436;&#20840;&#21151;&#33021;&#30340;&#25991;&#26723;&#35299;&#26512;&#27969;&#27700;&#32447;&#65292;&#21487;&#20197;&#19982;&#20854;&#20182;&#24037;&#20855;&#21644;&#27169;&#22411;&#38598;&#25104;&#65292;&#29992;&#20110;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DocXChain&#65292;&#19968;&#20010;&#24378;&#22823;&#30340;&#24320;&#28304;&#24037;&#20855;&#38142;&#65292;&#29992;&#20110;&#25991;&#26723;&#35299;&#26512;&#65292;&#26088;&#22312;&#33258;&#21160;&#23558;&#38750;&#32467;&#26500;&#21270;&#25991;&#26723;&#20013;&#21253;&#21547;&#30340;&#20016;&#23500;&#20449;&#24687;&#65288;&#22914;&#25991;&#26412;&#65292;&#34920;&#26684;&#21644;&#22270;&#34920;&#65289;&#36716;&#25442;&#20026;&#21487;&#35835;&#21462;&#21644;&#21487;&#25805;&#20316;&#30340;&#32467;&#26500;&#21270;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25552;&#20379;&#20102;&#22522;&#26412;&#33021;&#21147;&#65292;&#21253;&#25324;&#25991;&#26412;&#26816;&#27979;&#12289;&#25991;&#26412;&#35782;&#21035;&#12289;&#34920;&#26684;&#32467;&#26500;&#35782;&#21035;&#21644;&#24067;&#23616;&#20998;&#26512;&#12290;&#22312;&#36825;&#20123;&#22522;&#26412;&#33021;&#21147;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36824;&#26500;&#24314;&#20102;&#19968;&#32452;&#23436;&#20840;&#21151;&#33021;&#30340;&#25991;&#26723;&#35299;&#26512;&#27969;&#27700;&#32447;&#65292;&#21363;&#36890;&#29992;&#25991;&#26412;&#38405;&#35835;&#12289;&#34920;&#26684;&#35299;&#26512;&#21644;&#25991;&#26723;&#32467;&#26500;&#21270;&#65292;&#20197;&#25512;&#21160;&#19982;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#25991;&#26723;&#30456;&#20851;&#30340;&#21508;&#31181;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;DocXChain&#31616;&#27905;&#12289;&#27169;&#22359;&#21270;&#21644;&#28789;&#27963;&#65292;&#21487;&#20197;&#36731;&#26494;&#38598;&#25104;&#29616;&#26377;&#30340;&#24037;&#20855;&#12289;&#24211;&#25110;&#27169;&#22411;&#65288;&#22914;LangChain&#21644;ChatGPT&#65289;&#65292;&#26500;&#24314;&#26356;&#24378;&#22823;&#30340;&#31995;&#32479;&#65292;&#23436;&#25104;&#26356;&#22797;&#26434;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;DocXChain&#30340;&#20195;&#30721;&#21487;&#20197;&#22312;&#24320;&#28304;&#24179;&#21488;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this report, we introduce DocXChain, a powerful open-source toolchain for document parsing, which is designed and developed to automatically convert the rich information embodied in unstructured documents, such as text, tables and charts, into structured representations that are readable and manipulable by machines. Specifically, basic capabilities, including text detection, text recognition, table structure recognition and layout analysis, are provided. Upon these basic capabilities, we also build a set of fully functional pipelines for document parsing, i.e., general text reading, table parsing, and document structurization, to drive various applications related to documents in real-world scenarios. Moreover, DocXChain is concise, modularized and flexible, such that it can be readily integrated with existing tools, libraries or models (such as LangChain and ChatGPT), to construct more powerful systems that can accomplish more complicated and challenging tasks. The code of DocXChai
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26041;&#38754;&#21453;&#39304;&#30340;&#36845;&#20195;&#20248;&#21270;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#20923;&#32467;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#22806;&#37096;&#24037;&#20855;&#27169;&#22359;&#65292;&#27599;&#20010;&#27169;&#22359;&#37117;&#19987;&#27880;&#20110;&#29305;&#23450;&#30340;&#38169;&#35823;&#31867;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#25913;&#21892;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#30456;&#23545;&#25552;&#21319;&#20102;&#22810;&#36798;20%&#12290;</title><link>http://arxiv.org/abs/2310.12426</link><description>&lt;p&gt;
MAF: &#22810;&#26041;&#38754;&#21453;&#39304;&#20197;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
MAF: Multi-Aspect Feedback for Improving Reasoning in Large Language Models. (arXiv:2310.12426v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26041;&#38754;&#21453;&#39304;&#30340;&#36845;&#20195;&#20248;&#21270;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#20923;&#32467;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#22806;&#37096;&#24037;&#20855;&#27169;&#22359;&#65292;&#27599;&#20010;&#27169;&#22359;&#37117;&#19987;&#27880;&#20110;&#29305;&#23450;&#30340;&#38169;&#35823;&#31867;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#25913;&#21892;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#30456;&#23545;&#25552;&#21319;&#20102;&#22810;&#36798;20%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#28041;&#21450;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#24773;&#20917;&#19979;&#65292;&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#38754;&#20020;&#35832;&#22914;&#24187;&#35273;&#65292;&#29983;&#25104;&#38169;&#35823;&#30340;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#21644;&#25968;&#23398;&#38169;&#35823;&#31561;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#36890;&#36807;&#21453;&#39304;&#33258;&#25105;&#25913;&#36827;&#26469;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#20165;&#20381;&#36182;&#20110;&#21333;&#19968;&#30340;&#36890;&#29992;&#21453;&#39304;&#26469;&#28304;&#65292;&#26080;&#27861;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25512;&#29702;&#38142;&#20013;&#30340;&#22810;&#26679;&#38169;&#35823;&#31867;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#26041;&#38754;&#21453;&#39304;&#65292;&#36825;&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;&#22810;&#20010;&#21453;&#39304;&#27169;&#22359;&#30340;&#36845;&#20195;&#20248;&#21270;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#20923;&#32467;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#22806;&#37096;&#24037;&#20855;&#65292;&#27599;&#20010;&#27169;&#22359;&#37117;&#19987;&#27880;&#20110;&#29305;&#23450;&#30340;&#38169;&#35823;&#31867;&#21035;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25512;&#29702;&#38142;&#20013;&#30340;&#20960;&#20010;&#38169;&#35823;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#20960;&#20010;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#25105;&#20204;&#30475;&#21040;&#20102;&#22312;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#30456;&#23545;&#25552;&#21319;&#20102;&#22810;&#36798;20%&#12290;
&lt;/p&gt;
&lt;p&gt;
Language Models (LMs) have shown impressive performance in various natural language tasks. However, when it comes to natural language reasoning, LMs still face challenges such as hallucination, generating incorrect intermediate reasoning steps, and making mathematical errors. Recent research has focused on enhancing LMs through self-improvement using feedback. Nevertheless, existing approaches relying on a single generic feedback source fail to address the diverse error types found in LM-generated reasoning chains. In this work, we propose Multi-Aspect Feedback, an iterative refinement framework that integrates multiple feedback modules, including frozen LMs and external tools, each focusing on a specific error category. Our experimental results demonstrate the efficacy of our approach to addressing several errors in the LM-generated reasoning chain and thus improving the overall performance of an LM in several reasoning tasks. We see a relative improvement of up to 20% in Mathematical
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#29992;&#25143;-GPT&#23545;&#35805;&#65292;&#21457;&#29616;&#20102;&#24403;&#21069;NLP&#30740;&#31350;&#19982;&#23454;&#38469;&#24212;&#29992;&#38656;&#27714;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#29992;&#25143;&#32463;&#24120;&#35831;&#27714;&#30340;&#20219;&#21153;&#19982;&#23398;&#26415;&#30740;&#31350;&#20013;&#24120;&#30740;&#31350;&#30340;&#20219;&#21153;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#65292;&#22914;&#8220;&#35774;&#35745;&#8221;&#21644;&#8220;&#35268;&#21010;&#8221;&#31561;&#20219;&#21153;&#22312;&#23398;&#26415;&#30740;&#31350;&#20013;&#34987;&#24573;&#35270;&#12290;&#23545;&#36825;&#20123;&#34987;&#24573;&#30053;&#20219;&#21153;&#30340;&#30740;&#31350;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#28385;&#36275;&#23454;&#38469;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.12418</link><description>&lt;p&gt;
&#31227;&#20301;&#21644;&#24573;&#30053;&#65306;&#23545;&#29992;&#25143;-GPT&#20132;&#20114;&#30340;&#20219;&#21153;&#23548;&#21521;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
The Shifted and The Overlooked: A Task-oriented Investigation of User-GPT Interactions. (arXiv:2310.12418v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12418
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#29992;&#25143;-GPT&#23545;&#35805;&#65292;&#21457;&#29616;&#20102;&#24403;&#21069;NLP&#30740;&#31350;&#19982;&#23454;&#38469;&#24212;&#29992;&#38656;&#27714;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#29992;&#25143;&#32463;&#24120;&#35831;&#27714;&#30340;&#20219;&#21153;&#19982;&#23398;&#26415;&#30740;&#31350;&#20013;&#24120;&#30740;&#31350;&#30340;&#20219;&#21153;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#65292;&#22914;&#8220;&#35774;&#35745;&#8221;&#21644;&#8220;&#35268;&#21010;&#8221;&#31561;&#20219;&#21153;&#22312;&#23398;&#26415;&#30740;&#31350;&#20013;&#34987;&#24573;&#35270;&#12290;&#23545;&#36825;&#20123;&#34987;&#24573;&#30053;&#20219;&#21153;&#30340;&#30740;&#31350;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#28385;&#36275;&#23454;&#38469;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#24050;&#32463;&#20135;&#29983;&#20102;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;NLP&#30740;&#31350;&#30340;&#37325;&#28857;&#26159;&#21542;&#20934;&#30830;&#25429;&#25417;&#21040;&#20102;&#20154;&#31867;&#29992;&#25143;&#30340;&#30495;&#27491;&#38656;&#27714;&#12290;&#26412;&#25991;&#36890;&#36807;&#22823;&#35268;&#27169;&#25910;&#38598;&#29992;&#25143;-GPT&#23545;&#35805;&#30340;&#26041;&#24335;&#65292;&#23545;&#24403;&#21069;NLP&#30740;&#31350;&#19982;&#23454;&#38469;NLP&#24212;&#29992;&#38656;&#27714;&#20043;&#38388;&#30340;&#24046;&#24322;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#12290;&#25105;&#20204;&#23545;&#30495;&#23454;&#29992;&#25143;&#21521;GPT&#25552;&#20986;&#30340;&#22823;&#37327;&#26597;&#35810;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26597;&#35810;&#19982;&#29616;&#26377;NLP&#22522;&#20934;&#20219;&#21153;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#21457;&#29616;&#29992;&#25143;&#32463;&#24120;&#20174;LLMs&#35831;&#27714;&#30340;&#20219;&#21153;&#19982;&#23398;&#26415;&#30740;&#31350;&#20013;&#24120;&#24120;&#30740;&#31350;&#30340;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#29992;&#25143;&#20132;&#20114;&#20013;&#26222;&#36941;&#23384;&#22312;&#8220;&#35774;&#35745;&#8221;&#21644;&#8220;&#35268;&#21010;&#8221;&#31561;&#20219;&#21153;&#65292;&#20294;&#36825;&#20123;&#20219;&#21153;&#22312;&#20256;&#32479;&#30340;NLP&#22522;&#20934;&#20013;&#34987;&#24573;&#35270;&#25110;&#23384;&#22312;&#24046;&#24322;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#34987;&#24573;&#30053;&#30340;&#20219;&#21153;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#21078;&#26512;&#20102;&#23427;&#20204;&#25152;&#24102;&#26469;&#30340;&#23454;&#38469;&#25361;&#25112;&#21644;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress in Large Language Models (LLMs) has produced models that exhibit remarkable performance across a variety of NLP tasks. However, it remains unclear whether the existing focus of NLP research accurately captures the genuine requirements of human users. This paper provides a comprehensive analysis of the divergence between current NLP research and the needs of real-world NLP applications via a large-scale collection of user-GPT conversations. We analyze a large-scale collection of real user queries to GPT. We compare these queries against existing NLP benchmark tasks and identify a significant gap between the tasks that users frequently request from LLMs and the tasks that are commonly studied in academic research. For example, we find that tasks such as ``design'' and ``planning'' are prevalent in user interactions but are largely neglected or different from traditional NLP benchmarks. We investigate these overlooked tasks, dissect the practical challenges they pose, and 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;FinEntity&#30340;&#23454;&#20307;&#32423;&#24773;&#24863;&#20998;&#31867;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26631;&#27880;&#20102;&#37329;&#34701;&#26032;&#38395;&#20013;&#30340;&#37329;&#34701;&#23454;&#20307;&#33539;&#22260;&#21450;&#20854;&#24773;&#24863;&#65292;&#20026;&#37329;&#34701;&#39046;&#22495;&#30340;&#23454;&#20307;&#32423;&#24773;&#24863;&#20998;&#26512;&#25552;&#20379;&#20102;&#37325;&#35201;&#36164;&#28304;&#12290;&#36890;&#36807;&#22522;&#20934;&#27979;&#35797;&#65292;&#25351;&#20986;&#20102;&#20960;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#23454;&#20307;&#32423;&#24773;&#24863;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;FinEntity&#22312;&#30417;&#27979;&#21152;&#23494;&#36135;&#24065;&#24066;&#22330;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2310.12406</link><description>&lt;p&gt;
FinEntity: &#37329;&#34701;&#25991;&#26412;&#30340;&#23454;&#20307;&#32423;&#24773;&#24863;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
FinEntity: Entity-level Sentiment Classification for Financial Texts. (arXiv:2310.12406v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;FinEntity&#30340;&#23454;&#20307;&#32423;&#24773;&#24863;&#20998;&#31867;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26631;&#27880;&#20102;&#37329;&#34701;&#26032;&#38395;&#20013;&#30340;&#37329;&#34701;&#23454;&#20307;&#33539;&#22260;&#21450;&#20854;&#24773;&#24863;&#65292;&#20026;&#37329;&#34701;&#39046;&#22495;&#30340;&#23454;&#20307;&#32423;&#24773;&#24863;&#20998;&#26512;&#25552;&#20379;&#20102;&#37325;&#35201;&#36164;&#28304;&#12290;&#36890;&#36807;&#22522;&#20934;&#27979;&#35797;&#65292;&#25351;&#20986;&#20102;&#20960;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#23454;&#20307;&#32423;&#24773;&#24863;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;FinEntity&#22312;&#30417;&#27979;&#21152;&#23494;&#36135;&#24065;&#24066;&#22330;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37329;&#34701;&#39046;&#22495;&#65292;&#36827;&#34892;&#23454;&#20307;&#32423;&#24773;&#24863;&#20998;&#26512;&#23545;&#20934;&#30830;&#35780;&#20272;&#38024;&#23545;&#29305;&#23450;&#37329;&#34701;&#23454;&#20307;&#30340;&#24773;&#24863;&#33267;&#20851;&#37325;&#35201;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#30446;&#21069;&#23578;&#26080;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#29992;&#20110;&#27492;&#30446;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;FinEntity&#30340;&#23454;&#20307;&#32423;&#24773;&#24863;&#20998;&#31867;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26631;&#27880;&#20102;&#37329;&#34701;&#26032;&#38395;&#20013;&#30340;&#37329;&#34701;&#23454;&#20307;&#33539;&#22260;&#21450;&#20854;&#24773;&#24863;&#65288;&#31215;&#26497;&#12289;&#20013;&#24615;&#21644;&#28040;&#26497;&#65289;&#12290;&#25105;&#20204;&#22312;&#35770;&#25991;&#20013;&#35760;&#24405;&#20102;&#25968;&#25454;&#38598;&#26500;&#24314;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#23454;&#20307;&#32423;&#24773;&#24863;&#20998;&#31867;&#19978;&#23545;&#20960;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;BERT&#12289;FinBERT&#31561;&#65289;&#21644;ChatGPT&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#30417;&#27979;&#21152;&#23494;&#36135;&#24065;&#24066;&#22330;&#20013;&#20351;&#29992;FinEntity&#30340;&#23454;&#38469;&#25928;&#29992;&#12290;FinEntity&#30340;&#25968;&#25454;&#21644;&#20195;&#30721;&#21487;&#22312;\url{https://github.com/yixuantt/FinEntity}&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the financial domain, conducting entity-level sentiment analysis is crucial for accurately assessing the sentiment directed toward a specific financial entity. To our knowledge, no publicly available dataset currently exists for this purpose. In this work, we introduce an entity-level sentiment classification dataset, called \textbf{FinEntity}, that annotates financial entity spans and their sentiment (positive, neutral, and negative) in financial news. We document the dataset construction process in the paper. Additionally, we benchmark several pre-trained models (BERT, FinBERT, etc.) and ChatGPT on entity-level sentiment classification. In a case study, we demonstrate the practical utility of using FinEntity in monitoring cryptocurrency markets. The data and code of FinEntity is available at \url{https://github.com/yixuantt/FinEntity}
&lt;/p&gt;</description></item><item><title>Loop Copilot&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;AI&#38899;&#20048;&#21512;&#22863;&#31995;&#32479;&#65292;&#33021;&#22815;&#36890;&#36807;&#20132;&#20114;&#24335;&#22810;&#36718;&#23545;&#35805;&#30028;&#38754;&#29983;&#25104;&#21644;&#36845;&#20195;&#25913;&#36827;&#38899;&#20048;&#65292;&#36890;&#36807;&#36873;&#25321;&#36866;&#24403;&#30340;AI&#27169;&#22411;&#25191;&#34892;&#20219;&#21153;&#65292;&#24182;&#22312;&#19968;&#20010;&#38598;&#20013;&#30340;&#34920;&#20013;&#20445;&#25345;&#20851;&#38190;&#23646;&#24615;&#20197;&#30830;&#20445;&#38899;&#20048;&#30340;&#36830;&#36143;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12404</link><description>&lt;p&gt;
Loop Copilot: &#29992;&#20110;&#38899;&#20048;&#29983;&#25104;&#21644;&#36845;&#20195;&#32534;&#36753;&#30340;AI&#21512;&#22863;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Loop Copilot: Conducting AI Ensembles for Music Generation and Iterative Editing. (arXiv:2310.12404v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12404
&lt;/p&gt;
&lt;p&gt;
Loop Copilot&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;AI&#38899;&#20048;&#21512;&#22863;&#31995;&#32479;&#65292;&#33021;&#22815;&#36890;&#36807;&#20132;&#20114;&#24335;&#22810;&#36718;&#23545;&#35805;&#30028;&#38754;&#29983;&#25104;&#21644;&#36845;&#20195;&#25913;&#36827;&#38899;&#20048;&#65292;&#36890;&#36807;&#36873;&#25321;&#36866;&#24403;&#30340;AI&#27169;&#22411;&#25191;&#34892;&#20219;&#21153;&#65292;&#24182;&#22312;&#19968;&#20010;&#38598;&#20013;&#30340;&#34920;&#20013;&#20445;&#25345;&#20851;&#38190;&#23646;&#24615;&#20197;&#30830;&#20445;&#38899;&#20048;&#30340;&#36830;&#36143;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#24314;&#38899;&#20048;&#26159;&#19968;&#20010;&#36845;&#20195;&#36807;&#31243;&#65292;&#27599;&#20010;&#38454;&#27573;&#37117;&#38656;&#35201;&#19981;&#21516;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;AI&#38899;&#20048;&#31995;&#32479;&#22312;&#32452;&#32455;&#22810;&#20010;&#23376;&#31995;&#32479;&#20197;&#28385;&#36275;&#19981;&#21516;&#38656;&#27714;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Loop Copilot&#65292;&#36825;&#26159;&#19968;&#20010;&#33021;&#22815;&#36890;&#36807;&#20132;&#20114;&#24335;&#12289;&#22810;&#36718;&#23545;&#35805;&#30028;&#38754;&#29983;&#25104;&#21644;&#36845;&#20195;&#25913;&#36827;&#38899;&#20048;&#30340;&#26032;&#22411;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#20351;&#29992;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35299;&#37322;&#29992;&#25143;&#24847;&#22270;&#65292;&#24182;&#36873;&#25321;&#36866;&#24403;&#30340;AI&#27169;&#22411;&#36827;&#34892;&#20219;&#21153;&#25191;&#34892;&#12290;&#27599;&#20010;&#21518;&#31471;&#27169;&#22411;&#37117;&#19987;&#38376;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#65292;&#24182;&#23558;&#23427;&#20204;&#30340;&#36755;&#20986;&#32858;&#21512;&#36215;&#26469;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#35201;&#27714;&#12290;&#20026;&#20102;&#30830;&#20445;&#38899;&#20048;&#30340;&#36830;&#36143;&#24615;&#65292;&#20851;&#38190;&#23646;&#24615;&#34987;&#20445;&#30041;&#22312;&#19968;&#20010;&#38598;&#20013;&#30340;&#34920;&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#21322;&#32467;&#26500;&#21270;&#30340;&#35775;&#35848;&#21644;&#38382;&#21367;&#35843;&#26597;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#65292;&#31361;&#20986;&#20102;&#23427;&#22312;&#20419;&#36827;&#38899;&#20048;&#21019;&#20316;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#65292;&#20197;&#21450;&#23427;&#22312;&#26356;&#24191;&#27867;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Creating music is iterative, requiring varied methods at each stage. However, existing AI music systems fall short in orchestrating multiple subsystems for diverse needs. To address this gap, we introduce Loop Copilot, a novel system that enables users to generate and iteratively refine music through an interactive, multi-round dialogue interface. The system uses a large language model to interpret user intentions and select appropriate AI models for task execution. Each backend model is specialized for a specific task, and their outputs are aggregated to meet the user's requirements. To ensure musical coherence, essential attributes are maintained in a centralized table. We evaluate the effectiveness of the proposed system through semi-structured interviews and questionnaires, highlighting its utility not only in facilitating music creation but also its potential for broader applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22256;&#38590;&#30340;&#31867;&#27604;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20851;&#31995;&#24314;&#27169;&#20026;&#36335;&#24452;&#24182;&#20851;&#32852;&#20854;&#36793;&#32536;&#19982;&#20851;&#31995;&#23884;&#20837;&#65292;&#20197;&#33719;&#24471;&#21512;&#36866;&#30340;&#20013;&#38388;&#35789;&#21644;&#26377;&#20449;&#24687;&#37327;&#30340;&#20851;&#31995;&#23884;&#20837;&#65292;&#20174;&#32780;&#32467;&#21512;&#20102;&#30693;&#35782;&#22270;&#35889;&#21644;&#20851;&#31995;&#23884;&#20837;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.12379</link><description>&lt;p&gt;
&#29992;&#20851;&#31995;&#23884;&#20837;&#38142;&#35299;&#20915;&#22256;&#38590;&#30340;&#31867;&#27604;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving Hard Analogy Questions with Relation Embedding Chains. (arXiv:2310.12379v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22256;&#38590;&#30340;&#31867;&#27604;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20851;&#31995;&#24314;&#27169;&#20026;&#36335;&#24452;&#24182;&#20851;&#32852;&#20854;&#36793;&#32536;&#19982;&#20851;&#31995;&#23884;&#20837;&#65292;&#20197;&#33719;&#24471;&#21512;&#36866;&#30340;&#20013;&#38388;&#35789;&#21644;&#26377;&#20449;&#24687;&#37327;&#30340;&#20851;&#31995;&#23884;&#20837;&#65292;&#20174;&#32780;&#32467;&#21512;&#20102;&#30693;&#35782;&#22270;&#35889;&#21644;&#20851;&#31995;&#23884;&#20837;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35789;&#27719;&#35821;&#20041;&#23398;&#20013;&#65292;&#24314;&#27169;&#27010;&#24565;&#20043;&#38388;&#30340;&#20851;&#31995;&#26159;&#19968;&#20010;&#26680;&#24515;&#20027;&#39064;&#12290;&#19968;&#20010;&#24120;&#35265;&#31574;&#30053;&#26159;&#20381;&#36182;&#20110;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#22914;ConceptNet&#65292;&#24182;&#23558;&#20004;&#20010;&#27010;&#24565;&#20043;&#38388;&#30340;&#20851;&#31995;&#24314;&#27169;&#20026;&#19968;&#32452;&#36335;&#24452;&#12290;&#28982;&#32780;&#65292;KGs&#20165;&#38480;&#20110;&#22266;&#23450;&#30340;&#20851;&#31995;&#31867;&#22411;&#65292;&#19981;&#23436;&#25972;&#24182;&#19988;&#36890;&#24120;&#22024;&#26434;&#12290;&#21478;&#19968;&#20010;&#31574;&#30053;&#26159;&#20174;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#28860;&#20851;&#31995;&#23884;&#20837;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21482;&#38388;&#25509;&#30456;&#20851;&#30340;&#35789;&#26469;&#35828;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#22826;&#36866;&#29992;&#65292;&#24182;&#19988;&#19981;&#23481;&#26131;&#23558;&#32467;&#26500;&#21270;&#39046;&#22495;&#30693;&#35782;&#25972;&#21512;&#36827;&#26469;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#32467;&#21512;&#20004;&#32773;&#30340;&#20248;&#28857;&#12290;&#25105;&#20204;&#23558;&#20851;&#31995;&#24314;&#27169;&#20026;&#36335;&#24452;&#65292;&#20294;&#23558;&#20854;&#36793;&#32536;&#19982;&#20851;&#31995;&#23884;&#20837;&#30456;&#20851;&#32852;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#35782;&#21035;&#21512;&#36866;&#30340;&#20013;&#38388;&#35789;&#35821;&#26469;&#33719;&#21462;&#36335;&#24452;&#65292;&#28982;&#21518;&#36873;&#25321;&#37027;&#20123;&#21487;&#20197;&#33719;&#24471;&#26377;&#20449;&#24687;&#37327;&#30340;&#20851;&#31995;&#23884;&#20837;&#30340;&#35789;&#35821;&#12290;&#25105;&#20204;&#32463;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#34920;&#31034;&#26041;&#27861;&#23545;&#20110;&#35299;&#20915;&#22256;&#38590;&#30340;&#31867;&#27604;&#38382;&#39064;&#26159;&#26377;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modelling how concepts are related is a central topic in Lexical Semantics. A common strategy is to rely on knowledge graphs (KGs) such as ConceptNet, and to model the relation between two concepts as a set of paths. However, KGs are limited to a fixed set of relation types, and they are incomplete and often noisy. Another strategy is to distill relation embeddings from a fine-tuned language model. However, this is less suitable for words that are only indirectly related and it does not readily allow us to incorporate structured domain knowledge. In this paper, we aim to combine the best of both worlds. We model relations as paths but associate their edges with relation embeddings. The paths are obtained by first identifying suitable intermediate words and then selecting those words for which informative relation embeddings can be obtained. We empirically show that our proposed representations are useful for solving hard analogy questions.
&lt;/p&gt;</description></item><item><title>REMARK-LLM&#26159;&#19968;&#31181;&#38024;&#23545;&#29983;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#30340;&#40065;&#26834;&#39640;&#25928;&#30340;&#27700;&#21360;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;-based&#28040;&#24687;&#32534;&#30721;&#12289;&#37325;&#26032;&#21442;&#25968;&#21270;&#21644;&#35299;&#30721;&#27169;&#22359;&#20197;&#21450;&#20248;&#21270;&#30340;&#27874;&#26463;&#25628;&#32034;&#31639;&#27861;&#26469;&#20445;&#25252;&#29983;&#25104;&#20869;&#23481;&#30340;&#23436;&#25972;&#24615;&#21644;&#38450;&#27490;&#24694;&#24847;&#21033;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.12362</link><description>&lt;p&gt;
REMARK-LLM:&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#39640;&#25928;&#30340;&#27700;&#21360;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
REMARK-LLM: A Robust and Efficient Watermarking Framework for Generative Large Language Models. (arXiv:2310.12362v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12362
&lt;/p&gt;
&lt;p&gt;
REMARK-LLM&#26159;&#19968;&#31181;&#38024;&#23545;&#29983;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#30340;&#40065;&#26834;&#39640;&#25928;&#30340;&#27700;&#21360;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;-based&#28040;&#24687;&#32534;&#30721;&#12289;&#37325;&#26032;&#21442;&#25968;&#21270;&#21644;&#35299;&#30721;&#27169;&#22359;&#20197;&#21450;&#20248;&#21270;&#30340;&#27874;&#26463;&#25628;&#32034;&#31639;&#27861;&#26469;&#20445;&#25252;&#29983;&#25104;&#20869;&#23481;&#30340;&#23436;&#25972;&#24615;&#21644;&#38450;&#27490;&#24694;&#24847;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;REMARK-LLM&#30340;&#26032;&#22411;&#39640;&#25928;&#12289;&#24378;&#40065;&#26834;&#24615;&#30340;&#27700;&#21360;&#26694;&#26550;&#65292;&#19987;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#30340;&#25991;&#26412;&#35774;&#35745;&#12290;&#20351;&#29992;LLMs&#21512;&#25104;&#31867;&#20284;&#20154;&#31867;&#30340;&#20869;&#23481;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#24191;&#27867;&#30340;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#37325;&#35201;&#30340;&#30693;&#35782;&#20135;&#26435;&#65288;IP&#65289;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#30340;&#20869;&#23481;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#21033;&#29992;&#65292;&#21253;&#25324;&#22403;&#22334;&#37038;&#20214;&#21644;&#25220;&#34989;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;REMARK-LLM&#25552;&#20986;&#20102;&#19977;&#20010;&#26032;&#30340;&#32452;&#25104;&#37096;&#20998;&#65306;&#65288;i&#65289;&#22522;&#20110;&#23398;&#20064;&#30340;&#28040;&#24687;&#32534;&#30721;&#27169;&#22359;&#65292;&#23558;&#20108;&#36827;&#21046;&#31614;&#21517;&#27880;&#20837;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#65307;&#65288;ii&#65289;&#37325;&#26032;&#21442;&#25968;&#21270;&#27169;&#22359;&#65292;&#23558;&#28040;&#24687;&#32534;&#30721;&#30340;&#23494;&#38598;&#20998;&#24067;&#36716;&#25442;&#20026;&#27700;&#21360;&#25991;&#26412;&#26631;&#35760;&#30340;&#31232;&#30095;&#20998;&#24067;&#65307;&#65288;iii&#65289;&#19987;&#38376;&#29992;&#20110;&#31614;&#21517;&#25552;&#21462;&#30340;&#35299;&#30721;&#27169;&#22359;&#65307;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20248;&#21270;&#30340;&#27874;&#26463;&#25628;&#32034;&#31639;&#27861;&#65292;&#20197;&#20445;&#35777;&#29983;&#25104;&#20869;&#23481;&#30340;&#36830;&#36143;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;REMARK-LLM&#32463;&#36807;&#20005;&#26684;&#30340;&#35757;&#32451;&#65292;&#20197;&#40723;&#21169;&#35821;&#20041;&#23436;&#25972;&#24615;&#30340;&#20445;&#30041;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present REMARK-LLM, a novel efficient, and robust watermarking framework designed for texts generated by large language models (LLMs). Synthesizing human-like content using LLMs necessitates vast computational resources and extensive datasets, encapsulating critical intellectual property (IP). However, the generated content is prone to malicious exploitation, including spamming and plagiarism. To address the challenges, REMARK-LLM proposes three new components: (i) a learning-based message encoding module to infuse binary signatures into LLM-generated texts; (ii) a reparameterization module to transform the dense distributions from the message encoding to the sparse distribution of the watermarked textual tokens; (iii) a decoding module dedicated for signature extraction; Furthermore, we introduce an optimized beam search algorithm to guarantee the coherence and consistency of the generated content. REMARK-LLM is rigorously trained to encourage the preservation of semantic integrity
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GRI&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#20998;&#24067;&#35757;&#32451;&#30446;&#26631;&#21644;&#27880;&#24847;&#21147;&#22270;&#21367;&#31215;&#65292;&#19968;&#33268;&#22320;&#32771;&#34385;&#22810;&#20010;&#31354;&#38388;&#20043;&#38388;&#23450;&#20041;/&#35745;&#31639;&#30456;&#23545;&#21516;&#26500;&#24615;&#25152;&#38656;&#30340;&#35821;&#20041;&#30456;&#20284;&#35789;&#27719;&#30340;&#24433;&#21709;&#12290;&#22312;&#23454;&#39564;&#35780;&#20272;&#20013;&#65292;GRI&#22312;&#24179;&#22343;P@1&#19978;&#30456;&#23545;&#24471;&#20998;&#25552;&#21319;&#20102;&#39640;&#36798;63.6&#65285;&#12290;</title><link>http://arxiv.org/abs/2310.12360</link><description>&lt;p&gt;
GRI&#65306;&#22522;&#20110;&#22270;&#30340;&#35789;&#23884;&#20837;&#31354;&#38388;&#30340;&#30456;&#23545;&#21516;&#26500;
&lt;/p&gt;
&lt;p&gt;
GRI: Graph-based Relative Isomorphism of Word Embedding Spaces. (arXiv:2310.12360v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12360
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GRI&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#20998;&#24067;&#35757;&#32451;&#30446;&#26631;&#21644;&#27880;&#24847;&#21147;&#22270;&#21367;&#31215;&#65292;&#19968;&#33268;&#22320;&#32771;&#34385;&#22810;&#20010;&#31354;&#38388;&#20043;&#38388;&#23450;&#20041;/&#35745;&#31639;&#30456;&#23545;&#21516;&#26500;&#24615;&#25152;&#38656;&#30340;&#35821;&#20041;&#30456;&#20284;&#35789;&#27719;&#30340;&#24433;&#21709;&#12290;&#22312;&#23454;&#39564;&#35780;&#20272;&#20013;&#65292;GRI&#22312;&#24179;&#22343;P@1&#19978;&#30456;&#23545;&#24471;&#20998;&#25552;&#21319;&#20102;&#39640;&#36798;63.6&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#65292;&#20351;&#29992;&#21333;&#35821;&#35328;&#23884;&#20837;&#31354;&#38388;&#33258;&#21160;&#26500;&#24314;&#21452;&#35821;&#35789;&#20856;&#26159;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#12290;&#36825;&#20123;&#35789;&#20856;&#30340;&#26368;&#32456;&#24615;&#33021;&#21462;&#20915;&#20110;&#20010;&#21035;&#31354;&#38388;&#30340;&#20960;&#20309;&#30456;&#20284;&#24615;&#65292;&#21363;&#23427;&#20204;&#30340;&#21516;&#26500;&#31243;&#24230;&#12290;&#29616;&#26377;&#30340;&#23581;&#35797;&#25511;&#21046;&#19981;&#21516;&#31354;&#38388;&#30340;&#30456;&#23545;&#21516;&#26500;&#24615;&#26080;&#27861;&#23558;&#35821;&#20041;&#30456;&#20851;&#35789;&#27719;&#30340;&#24433;&#21709;&#32435;&#20837;&#21040;&#35757;&#32451;&#30446;&#26631;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GRI&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#20998;&#24067;&#24335;&#35757;&#32451;&#30446;&#26631;&#19982;&#27880;&#24847;&#21147;&#22270;&#21367;&#31215;&#30456;&#32467;&#21512;&#65292;&#20197;&#19968;&#33268;&#22320;&#32771;&#34385;&#23450;&#20041;/&#35745;&#31639;&#22810;&#20010;&#31354;&#38388;&#20043;&#38388;&#30340;&#30456;&#23545;&#21516;&#26500;&#24615;&#25152;&#38656;&#30340;&#35821;&#20041;&#30456;&#20284;&#35789;&#27719;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;GRI&#36890;&#36807;&#23558;&#24179;&#22343;P@1&#30340;&#30456;&#23545;&#24471;&#20998;&#25552;&#21319;&#20102;&#39640;&#36798;63.6&#65285;&#65292;&#20248;&#20110;&#29616;&#26377;&#30740;&#31350;&#12290;&#25105;&#20204;&#22312;https://github.com/asif6827/GRI&#19978;&#21457;&#24067;GRI&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated construction of bilingual dictionaries using monolingual embedding spaces is a core challenge in machine translation. The end performance of these dictionaries relies upon the geometric similarity of individual spaces, i.e., their degree of isomorphism. Existing attempts aimed at controlling the relative isomorphism of different spaces fail to incorporate the impact of semantically related words in the training objective. To address this, we propose GRI that combines the distributional training objectives with attentive graph convolutions to unanimously consider the impact of semantically similar words required to define/compute the relative isomorphism of multiple spaces. Experimental evaluation shows that GRI outperforms the existing research by improving the average P@1 by a relative score of up to 63.6%. We release the codes for GRI at https://github.com/asif6827/GRI.
&lt;/p&gt;</description></item><item><title>"knn-seq"&#26159;&#19968;&#20010;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#30340;kNN-MT&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#32763;&#35793;&#31034;&#20363;&#26469;&#25552;&#39640;&#39044;&#35757;&#32451;NMT&#27169;&#22411;&#30340;&#32763;&#35793;&#36136;&#37327;&#65292;&#32473;&#20986;&#20102;&#22312;&#21313;&#20159;&#32423;&#25968;&#25454;&#23384;&#20648;&#19979;&#20855;&#26377;&#21487;&#27604;&#36739;&#22686;&#30410;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#24182;&#22312;&#24503;&#33521;&#32763;&#35793;&#20219;&#21153;&#20013;&#20165;&#33457;&#36153;2.21&#23567;&#26102;&#26469;&#26500;&#24314;&#21313;&#20159;&#35268;&#27169;&#30340;&#25968;&#25454;&#23384;&#20648;&#12290;</title><link>http://arxiv.org/abs/2310.12352</link><description>&lt;p&gt;
knn-seq: &#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#30340;kNN-MT&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
knn-seq: Efficient, Extensible kNN-MT Framework. (arXiv:2310.12352v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12352
&lt;/p&gt;
&lt;p&gt;
"knn-seq"&#26159;&#19968;&#20010;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#30340;kNN-MT&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#32763;&#35793;&#31034;&#20363;&#26469;&#25552;&#39640;&#39044;&#35757;&#32451;NMT&#27169;&#22411;&#30340;&#32763;&#35793;&#36136;&#37327;&#65292;&#32473;&#20986;&#20102;&#22312;&#21313;&#20159;&#32423;&#25968;&#25454;&#23384;&#20648;&#19979;&#20855;&#26377;&#21487;&#27604;&#36739;&#22686;&#30410;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#24182;&#22312;&#24503;&#33521;&#32763;&#35793;&#20219;&#21153;&#20013;&#20165;&#33457;&#36153;2.21&#23567;&#26102;&#26469;&#26500;&#24314;&#21313;&#20159;&#35268;&#27169;&#30340;&#25968;&#25454;&#23384;&#20648;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
k-&#26368;&#36817;&#37051;&#26426;&#22120;&#32763;&#35793;&#65288;kNN-MT&#65289;&#36890;&#36807;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#21033;&#29992;&#32763;&#35793;&#31034;&#20363;&#26469;&#25552;&#39640;&#39044;&#35757;&#32451;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#27169;&#22411;&#30340;&#32763;&#35793;&#36136;&#37327;&#12290;&#32763;&#35793;&#31034;&#20363;&#34987;&#23384;&#20648;&#22312;&#19968;&#20010;&#21521;&#37327;&#25968;&#25454;&#24211;&#20013;&#65292;&#31216;&#20026;&#25968;&#25454;&#23384;&#20648;&#65292;&#23427;&#21253;&#21547;&#20102;&#26469;&#33258;&#24182;&#34892;&#25968;&#25454;&#30340;&#27599;&#20010;&#30446;&#26631;&#26631;&#35760;&#30340;&#19968;&#20010;&#26465;&#30446;&#12290;&#30001;&#20110;&#20854;&#35268;&#27169;&#36739;&#22823;&#65292;&#26500;&#24314;&#21644;&#26816;&#32034;&#25968;&#25454;&#23384;&#20648;&#30340;&#31034;&#20363;&#37117;&#20855;&#26377;&#35745;&#31639;&#19978;&#30340;&#26114;&#36149;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;kNN-MT&#26694;&#26550;knn-seq&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#26694;&#26550;&#65292;&#21363;&#20351;&#22312;&#25317;&#26377;&#21313;&#20159;&#32423;&#22823;&#22411;&#25968;&#25454;&#23384;&#20648;&#30340;&#24773;&#20917;&#19979;&#20063;&#21487;&#20197;&#39640;&#25928;&#22320;&#36816;&#34892;&#12290;knn-seq&#26159;&#20316;&#20026;fairseq&#30340;&#19968;&#20010;&#25554;&#20214;&#24320;&#21457;&#30340;&#65292;&#26131;&#20110;&#20999;&#25442;&#27169;&#22411;&#21644;kNN&#32034;&#24341;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#23454;&#29616;&#30340;kNN-MT&#19982;&#21407;&#22987;kNN-MT&#33719;&#24471;&#20102;&#21487;&#27604;&#36739;&#30340;&#22686;&#30410;&#65292;&#24182;&#19988;&#21313;&#20159;&#35268;&#27169;&#30340;&#25968;&#25454;&#23384;&#20648;&#26500;&#24314;&#22312;WMT'19&#24503;&#33521;&#32763;&#35793;&#20219;&#21153;&#20013;&#20165;&#33457;&#36153;&#20102;2.21&#23567;&#26102;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;knn-seq&#21457;&#24067;&#20026;MIT-li&#12290;
&lt;/p&gt;
&lt;p&gt;
k-nearest-neighbor machine translation (kNN-MT) boosts the translation quality of a pre-trained neural machine translation (NMT) model by utilizing translation examples during decoding. Translation examples are stored in a vector database, called a datastore, which contains one entry for each target token from the parallel data it is made from. Due to its size, it is computationally expensive both to construct and to retrieve examples from the datastore. In this paper, we present an efficient and extensible kNN-MT framework, knn-seq, for researchers and developers that is carefully designed to run efficiently, even with a billion-scale large datastore. knn-seq is developed as a plug-in on fairseq and easy to switch models and kNN indexes. Experimental results show that our implemented kNN-MT achieves a comparable gain to the original kNN-MT, and the billion-scale datastore construction took 2.21 hours in the WMT'19 German-to-English translation task. We publish our knn-seq as an MIT-li
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LACMA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#23454;&#29616;&#20102;&#26234;&#33021;&#20307;&#19982;&#25351;&#20196;&#30340;&#35821;&#35328;&#23545;&#40784;&#65292;&#36827;&#32780;&#36890;&#36807;&#24341;&#20837;&#20803;&#34892;&#21160;&#30340;&#27010;&#24565;&#26469;&#35299;&#20915;&#39640;&#32423;&#35821;&#35328;&#25351;&#20196;&#19982;&#20302;&#32423;&#34892;&#21160;&#31354;&#38388;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#36317;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26234;&#33021;&#20307;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.12344</link><description>&lt;p&gt;
LACMA: &#20351;&#29992;&#20803;&#34892;&#21160;&#30340;&#35821;&#35328;&#23545;&#40784;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#20855;&#35937;&#25351;&#20196;&#36319;&#38543;
&lt;/p&gt;
&lt;p&gt;
LACMA: Language-Aligning Contrastive Learning with Meta-Actions for Embodied Instruction Following. (arXiv:2310.12344v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12344
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LACMA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#23454;&#29616;&#20102;&#26234;&#33021;&#20307;&#19982;&#25351;&#20196;&#30340;&#35821;&#35328;&#23545;&#40784;&#65292;&#36827;&#32780;&#36890;&#36807;&#24341;&#20837;&#20803;&#34892;&#21160;&#30340;&#27010;&#24565;&#26469;&#35299;&#20915;&#39640;&#32423;&#35821;&#35328;&#25351;&#20196;&#19982;&#20302;&#32423;&#34892;&#21160;&#31354;&#38388;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#36317;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26234;&#33021;&#20307;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#29615;&#22659;&#22312;&#35757;&#32451;&#20013;&#34987;&#30475;&#21040;&#26102;&#65292;&#31471;&#21040;&#31471;&#30340;Transformer&#22312;&#20855;&#35937;&#25351;&#20196;&#36319;&#38543;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#21151;&#29575;&#12290;&#28982;&#32780;&#65292;&#24403;&#23427;&#20204;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#37096;&#32626;&#26102;&#24448;&#24448;&#36935;&#21040;&#22256;&#38590;&#12290;&#36825;&#31181;&#32570;&#20047;&#27867;&#21270;&#33021;&#21147;&#26159;&#30001;&#20110;&#26234;&#33021;&#20307;&#23545;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#20013;&#30340;&#32454;&#24494;&#21464;&#21270;&#19981;&#25935;&#24863;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#23558;&#26234;&#33021;&#20307;&#30340;&#38544;&#34255;&#29366;&#24577;&#19982;&#25351;&#20196;&#26126;&#30830;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#39640;&#32423;&#35821;&#35328;&#25351;&#20196;&#19982;&#26234;&#33021;&#20307;&#30340;&#20302;&#32423;&#34892;&#21160;&#31354;&#38388;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#36317;&#20173;&#28982;&#26159;&#19968;&#20010;&#38556;&#30861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#20803;&#34892;&#21160;&#30340;&#26032;&#27010;&#24565;&#26469;&#24357;&#21512;&#36825;&#20010;&#24046;&#36317;&#12290;&#20803;&#34892;&#21160;&#26159;&#20174;&#21407;&#22987;&#34892;&#21160;&#24207;&#21015;&#20013;&#35299;&#26512;&#20986;&#26469;&#30340;&#26222;&#36941;&#34892;&#21160;&#27169;&#24335;&#12290;&#36825;&#20123;&#27169;&#24335;&#20195;&#34920;&#20102;&#26356;&#39640;&#23618;&#27425;&#30340;&#35821;&#20041;&#65292;&#30452;&#35266;&#19978;&#26356;&#25509;&#36817;&#25351;&#20196;&#12290;&#24403;&#20803;&#34892;&#21160;&#34987;&#24212;&#29992;&#20026;&#39069;&#22806;&#30340;&#35757;&#32451;&#20449;&#21495;&#26102;&#65292;&#26234;&#33021;&#20307;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-end Transformers have demonstrated an impressive success rate for Embodied Instruction Following when the environment has been seen in training. However, they tend to struggle when deployed in an unseen environment. This lack of generalizability is due to the agent's insensitivity to subtle changes in natural language instructions. To mitigate this issue, we propose explicitly aligning the agent's hidden states with the instructions via contrastive learning. Nevertheless, the semantic gap between high-level language instructions and the agent's low-level action space remains an obstacle. Therefore, we further introduce a novel concept of meta-actions to bridge the gap. Meta-actions are ubiquitous action patterns that can be parsed from the original action sequence. These patterns represent higher-level semantics that are intuitively aligned closer to the instructions. When meta-actions are applied as additional training signals, the agent generalizes better to unseen environment
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25512;&#26029;&#24615;&#25490;&#38500;&#25552;&#31034;&#65288;IEP&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#25490;&#38500;&#21644;&#25512;&#29702;&#30340;&#21407;&#21017;&#65292;&#24341;&#23548;LLM&#36827;&#34892;&#38750;&#32447;&#24615;&#24605;&#32771;&#12290;IEP&#36890;&#36807;&#35268;&#21010;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65292;&#21487;&#20197;&#27169;&#25311;&#22797;&#26434;&#30340;&#20154;&#31867;&#24605;&#32500;&#36807;&#31243;&#65292;&#27604;&#20854;&#20182;&#26041;&#27861;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2310.12342</link><description>&lt;p&gt;
&#36890;&#36807;&#25512;&#29702;&#19982;&#35268;&#21010;&#28040;&#38500;&#25512;&#29702;&#65306;&#19968;&#31181;&#24341;&#23548;LLMs&#38750;&#32447;&#24615;&#24605;&#32500;&#30340;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Eliminating Reasoning via Inferring with Planning: A New Framework to Guide LLMs' Non-linear Thinking. (arXiv:2310.12342v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25512;&#26029;&#24615;&#25490;&#38500;&#25552;&#31034;&#65288;IEP&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#25490;&#38500;&#21644;&#25512;&#29702;&#30340;&#21407;&#21017;&#65292;&#24341;&#23548;LLM&#36827;&#34892;&#38750;&#32447;&#24615;&#24605;&#32771;&#12290;IEP&#36890;&#36807;&#35268;&#21010;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65292;&#21487;&#20197;&#27169;&#25311;&#22797;&#26434;&#30340;&#20154;&#31867;&#24605;&#32500;&#36807;&#31243;&#65292;&#27604;&#20854;&#20182;&#26041;&#27861;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Thought Chain&#65288;CoT&#65289;&#25552;&#31034;&#21450;&#20854;&#21464;&#20307;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#32447;&#24615;&#35748;&#30693;&#21644;&#36923;&#36753;&#65292;&#25506;&#32034;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35013;&#22791;&#39640;&#32423;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#24605;&#32500;&#22797;&#26434;&#19988;&#28151;&#21512;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#24605;&#32500;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#24335;&#65292;&#31216;&#20026;&#25512;&#26029;&#24615;&#25490;&#38500;&#25552;&#31034;&#65288;IEP&#65289;&#65292;&#23427;&#32467;&#21512;&#20102;&#25490;&#38500;&#21644;&#25512;&#29702;&#30340;&#21407;&#21017;&#65292;&#20197;&#24341;&#23548;LLM&#36827;&#34892;&#38750;&#32447;&#24615;&#24605;&#32771;&#12290;IEP&#25351;&#23548;LLM&#36827;&#34892;&#35268;&#21010;&#65292;&#24182;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#25512;&#26029;&#27599;&#20010;&#21487;&#33021;&#35299;&#19982;&#19978;&#19979;&#25991;&#12289;&#24120;&#35782;&#25110;&#20107;&#23454;&#30340;&#25512;&#29702;&#20851;&#31995;&#65292;&#20174;&#32780;&#36890;&#36807;&#22238;&#28335;&#25512;&#29702;&#33719;&#24471;&#26356;&#24191;&#27867;&#30340;&#35270;&#35282;&#12290;&#30456;&#27604;&#20854;&#20182;&#22522;&#20110;CoT&#30340;&#26041;&#27861;&#65292;IEP&#30340;&#21069;&#21521;&#35268;&#21010;&#21644;&#21518;&#21521;&#25490;&#38500;&#36807;&#31243;&#26356;&#22909;&#22320;&#27169;&#25311;&#20102;&#22797;&#26434;&#30340;&#20154;&#31867;&#24605;&#32500;&#36807;&#31243;&#65292;&#21518;&#32773;&#20165;&#21453;&#26144;&#32447;&#24615;&#35748;&#30693;&#36807;&#31243;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#39564;&#35777;&#20102;IEP&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought(CoT) prompting and its variants explore equipping large language models (LLMs) with high-level reasoning abilities by emulating human-like linear cognition and logic. However, the human mind is complicated and mixed with both linear and nonlinear thinking. In this work, we propose \textbf{I}nferential \textbf{E}xclusion \textbf{P}rompting (IEP), a novel prompting that combines the principles of elimination and inference in order to guide LLMs to think non-linearly. IEP guides LLMs to plan and then utilize Natural Language Inference (NLI) to deduce each possible solution's entailment relation with context, commonsense, or facts, therefore yielding a broader perspective by thinking back for inferring. This forward planning and backward eliminating process allows IEP to better simulate the complex human thinking processes compared to other CoT-based methods, which only reflect linear cognitive processes. We conducted a series of empirical studies and have corroborated tha
&lt;/p&gt;</description></item><item><title>GPT-3&#23478;&#26063;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#19968;&#31867;&#29305;&#27530;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25193;&#22823;&#27169;&#22411;&#35268;&#27169;&#12289;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#21644;&#35745;&#31639;&#65292;&#33021;&#22815;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#26080;&#38656;&#29305;&#23450;&#35757;&#32451;&#32780;&#21462;&#24471;&#26174;&#33879;&#24615;&#33021;&#12290;&#36825;&#31687;&#35770;&#25991;&#26159;&#23545;&#26368;&#36817;&#30740;&#31350;&#36827;&#23637;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#20026;&#30740;&#31350;&#31038;&#21306;&#25552;&#20379;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2310.12321</link><description>&lt;p&gt;
GPT-3&#23478;&#26063;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32508;&#36848;&#65292;&#21253;&#25324;ChatGPT&#21644;GPT-4
&lt;/p&gt;
&lt;p&gt;
A Survey of GPT-3 Family Large Language Models Including ChatGPT and GPT-4. (arXiv:2310.12321v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12321
&lt;/p&gt;
&lt;p&gt;
GPT-3&#23478;&#26063;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#19968;&#31867;&#29305;&#27530;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25193;&#22823;&#27169;&#22411;&#35268;&#27169;&#12289;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#21644;&#35745;&#31639;&#65292;&#33021;&#22815;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#26080;&#38656;&#29305;&#23450;&#35757;&#32451;&#32780;&#21462;&#24471;&#26174;&#33879;&#24615;&#33021;&#12290;&#36825;&#31687;&#35770;&#25991;&#26159;&#23545;&#26368;&#36817;&#30740;&#31350;&#36827;&#23637;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#20026;&#30740;&#31350;&#31038;&#21306;&#25552;&#20379;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#19968;&#31867;&#29305;&#27530;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25193;&#22823;&#27169;&#22411;&#35268;&#27169;&#12289;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#21644;&#35745;&#31639;&#24471;&#21040;&#12290;&#30001;&#20110;&#23427;&#20204;&#24040;&#22823;&#30340;&#35268;&#27169;&#21644;&#22312;&#22823;&#37327;&#25991;&#26412;&#25968;&#25454;&#19978;&#30340;&#39044;&#35757;&#32451;&#65292;LLMs&#34920;&#29616;&#20986;&#29305;&#27530;&#33021;&#21147;&#65292;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#33021;&#22815;&#36798;&#21040;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#29305;&#23450;&#20219;&#21153;&#30340;&#35757;&#32451;&#12290;LLMs&#30340;&#26102;&#20195;&#22987;&#20110;OpenAI&#30340;GPT-3&#27169;&#22411;&#65292;&#33258;&#20174;&#24341;&#20837;&#20102;ChatGPT&#21644;GPT4&#31561;&#27169;&#22411;&#21518;&#65292;LLMs&#30340;&#21463;&#27426;&#36814;&#31243;&#24230;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#25105;&#20204;&#23558;GPT-3&#21450;&#20854;&#32487;&#20219;&#32773;OpenAI&#27169;&#22411;&#65292;&#21253;&#25324;ChatGPT&#21644;GPT4&#65292;&#31216;&#20026;GPT-3&#23478;&#26063;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;GLLMs&#65289;&#12290;&#38543;&#30528;GLLMs&#65292;&#23588;&#20854;&#26159;&#22312;&#30740;&#31350;&#30028;&#30340;&#26222;&#21450;&#65292;&#23545;&#20110;&#32508;&#21512;&#27010;&#36848;&#36817;&#26399;&#22312;&#22810;&#20010;&#32500;&#24230;&#19978;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#24182;&#33021;&#20026;&#30740;&#31350;&#31038;&#21306;&#25552;&#20379;&#26377;&#35265;&#22320;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#25351;&#23548;&#65292;&#23384;&#22312;&#30528;&#24378;&#28872;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#22312;&#32508;&#36848;&#35770;&#25991;&#20013;&#20174;&#22522;&#30784;&#24320;&#22987;&#20171;&#32461;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are a special class of pretrained language models obtained by scaling model size, pretraining corpus and computation. LLMs, because of their large size and pretraining on large volumes of text data, exhibit special abilities which allow them to achieve remarkable performances without any task-specific training in many of the natural language processing tasks. The era of LLMs started with OpenAI GPT-3 model, and the popularity of LLMs is increasing exponentially after the introduction of models like ChatGPT and GPT4. We refer to GPT-3 and its successor OpenAI models, including ChatGPT and GPT4, as GPT-3 family large language models (GLLMs). With the ever-rising popularity of GLLMs, especially in the research community, there is a strong need for a comprehensive survey which summarizes the recent research progress in multiple dimensions and can guide the research community with insightful future research directions. We start the survey paper with foundation c
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;189&#31687;&#35770;&#25991;&#65292;&#25209;&#21028;&#24615;&#22320;&#30740;&#31350;&#20102;&#24773;&#24863;&#20998;&#26512;&#22312;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#25581;&#31034;&#20102;&#23545;&#24773;&#24863;&#30340;&#19981;&#21516;&#27010;&#24565;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20262;&#29702;&#34920;&#26684;&#26469;&#35299;&#20915;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#20844;&#24179;&#21033;&#29992;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.12318</link><description>&lt;p&gt;
The Sentiment Problem: A Critical Survey towards Deconstructing Sentiment Analysis.
&lt;/p&gt;
&lt;p&gt;
The Sentiment Problem: A Critical Survey towards Deconstructing Sentiment Analysis. (arXiv:2310.12318v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12318
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;189&#31687;&#35770;&#25991;&#65292;&#25209;&#21028;&#24615;&#22320;&#30740;&#31350;&#20102;&#24773;&#24863;&#20998;&#26512;&#22312;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#25581;&#31034;&#20102;&#23545;&#24773;&#24863;&#30340;&#19981;&#21516;&#27010;&#24565;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20262;&#29702;&#34920;&#26684;&#26469;&#35299;&#20915;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#20844;&#24179;&#21033;&#29992;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#25209;&#21028;&#24615;&#22320;&#30740;&#31350;189&#31687;&#21516;&#34892;&#35780;&#23457;&#30340;&#35770;&#25991;&#65292;&#23545;&#24773;&#24863;&#20998;&#26512;&#65288;SA&#65289;&#30340;&#24212;&#29992;&#12289;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#20197;&#25506;&#31350;&#24773;&#24863;&#20998;&#26512;&#30340;&#31038;&#20250;&#25216;&#26415;&#26041;&#38754;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26469;&#28304;&#20110;&#23545;&#24773;&#24863;&#20998;&#26512;&#22312;&#19981;&#21516;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#20013;&#25104;&#20026;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#30340;&#35748;&#35782;&#65292;&#24182;&#23545;&#31038;&#20250;&#23398;&#21644;&#25216;&#26415;&#25991;&#29486;&#20013;&#30340;&#24773;&#24863;&#27010;&#24565;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#37329;&#34701;&#12289;&#25919;&#24220;&#21644;&#21307;&#30103;&#31561;&#39046;&#22495;&#23545;&#24773;&#24863;&#30340;&#19981;&#21516;&#27010;&#24565;&#21270;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#21457;&#29616;&#22312;&#24773;&#24863;&#30340;&#23450;&#20041;&#21644;&#26694;&#26550;&#26041;&#38754;&#23384;&#22312;&#26126;&#30830;&#19981;&#36275;&#65292;&#21487;&#33021;&#23548;&#33268;&#25361;&#25112;&#21644;&#20559;&#35265;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28085;&#30422;&#20851;&#38190;&#38382;&#39064;&#30340;&#20262;&#29702;&#34920;&#26684;&#65292;&#20197;&#25351;&#23548;&#20174;&#19994;&#32773;&#30830;&#20445;&#24773;&#24863;&#20998;&#26512;&#30340;&#20844;&#24179;&#21033;&#29992;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#37319;&#29992;&#36328;&#23398;&#31185;&#26041;&#27861;&#26469;&#23450;&#20041;&#24773;&#24863;&#20998;&#26512;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#23454;&#26045;&#24773;&#24863;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We conduct an inquiry into the sociotechnical aspects of sentiment analysis (SA) by critically examining 189 peer-reviewed papers on their applications, models, and datasets. Our investigation stems from the recognition that SA has become an integral component of diverse sociotechnical systems, exerting influence on both social and technical users. By delving into sociological and technological literature on sentiment, we unveil distinct conceptualizations of this term in domains such as finance, government, and medicine. Our study exposes a lack of explicit definitions and frameworks for characterizing sentiment, resulting in potential challenges and biases. To tackle this issue, we propose an ethics sheet encompassing critical inquiries to guide practitioners in ensuring equitable utilization of SA. Our findings underscore the significance of adopting an interdisciplinary approach to defining sentiment in SA and offer a pragmatic solution for its implementation.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25991;&#26723;&#32423;&#21035;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#32763;&#35793;&#31995;&#32479;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#32467;&#21512;&#20219;&#20309;&#29616;&#26377;&#30340;&#21477;&#32423;&#21035;&#32763;&#35793;&#27169;&#22411;&#19982;&#25991;&#26723;&#32423;&#21035;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#20511;&#37492;&#27169;&#22411;&#32452;&#21512;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#23588;&#20854;&#26159;&#26435;&#37325;&#25216;&#26415;&#30340;&#25552;&#20986;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25991;&#26723;&#32423;&#21035;&#25351;&#26631;&#24182;&#38477;&#20302;&#35745;&#31639;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2310.12303</link><description>&lt;p&gt;
&#25991;&#26723;&#32423;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Document-Level Language Models for Machine Translation. (arXiv:2310.12303v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12303
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25991;&#26723;&#32423;&#21035;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#32763;&#35793;&#31995;&#32479;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#32467;&#21512;&#20219;&#20309;&#29616;&#26377;&#30340;&#21477;&#32423;&#21035;&#32763;&#35793;&#27169;&#22411;&#19982;&#25991;&#26723;&#32423;&#21035;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#20511;&#37492;&#27169;&#22411;&#32452;&#21512;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#23588;&#20854;&#26159;&#26435;&#37325;&#25216;&#26415;&#30340;&#25552;&#20986;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25991;&#26723;&#32423;&#21035;&#25351;&#26631;&#24182;&#38477;&#20302;&#35745;&#31639;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24050;&#30693;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#20294;&#22823;&#22810;&#25968;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#20173;&#28982;&#22312;&#21477;&#32423;&#21035;&#19978;&#36816;&#34892;&#12290;&#20854;&#20013;&#19968;&#20010;&#21407;&#22240;&#26159;&#65292;&#22823;&#22810;&#25968;&#24179;&#34892;&#35757;&#32451;&#25968;&#25454;&#21482;&#26377;&#21477;&#32423;&#21035;&#30340;&#23545;&#40784;&#65292;&#27809;&#26377;&#25991;&#26723;&#32423;&#21035;&#30340;&#20803;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#25991;&#26723;&#32423;&#21035;&#30340;&#21333;&#35821;&#25968;&#25454;&#26500;&#24314;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#32763;&#35793;&#31995;&#32479;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20219;&#20309;&#29616;&#26377;&#30340;&#21477;&#32423;&#21035;&#32763;&#35793;&#27169;&#22411;&#19982;&#25991;&#26723;&#32423;&#21035;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#27169;&#22411;&#32452;&#21512;&#30340;&#26368;&#26032;&#36827;&#23637;&#26469;&#25913;&#36827;&#29616;&#26377;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33021;&#22815;&#20351;&#31995;&#32479;&#32452;&#21512;&#26356;&#28789;&#27963;&#12289;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#24320;&#38144;&#30340;&#26435;&#37325;&#25216;&#26415;&#12290;&#36890;&#36807;&#23545;&#22235;&#20010;&#19981;&#21516;&#30340;&#32763;&#35793;&#20219;&#21153;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#25193;&#23637;&#26174;&#33879;&#25552;&#39640;&#20102;&#25991;&#26723;&#32423;&#21035;&#25351;&#26631;&#65292;&#24182;&#19988;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#20063;&#26356;&#20248;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#21453;&#21521;&#32763;&#35793;&#30340;&#32467;&#26524;&#26356;&#22909;&#65292;
&lt;/p&gt;
&lt;p&gt;
Despite the known limitations, most machine translation systems today still operate on the sentence-level. One reason for this is, that most parallel training data is only sentence-level aligned, without document-level meta information available. In this work, we set out to build context-aware translation systems utilizing document-level monolingual data instead. This can be achieved by combining any existing sentence-level translation model with a document-level language model. We improve existing approaches by leveraging recent advancements in model combination. Additionally, we propose novel weighting techniques that make the system combination more flexible and significantly reduce computational overhead. In a comprehensive evaluation on four diverse translation tasks, we show that our extensions improve document-targeted scores substantially and are also computationally more efficient. However, we also find that in most scenarios, back-translation gives even better results, at the
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36866;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#33539;&#24335;&#65292;&#36890;&#36807;&#35843;&#25972;&#36880;&#28857;&#21487;&#29992;&#20449;&#24687;&#24230;&#37327;&#25351;&#26631;&#20026;&#36866;&#29992;&#20110;&#19978;&#19979;&#25991;&#30340;&#29256;&#26412;&#65292;&#23558;&#20854;&#21629;&#21517;&#20026;&#19978;&#19979;&#25991;PVI&#65292;&#24182;&#35777;&#26126;&#20102;&#19978;&#19979;&#25991;PVI&#30340;&#21487;&#38752;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12300</link><description>&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#20013;&#27979;&#37327;&#36880;&#28857;&#21487;&#29992;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Measuring Pointwise $\mathcal{V}$-Usable Information In-Context-ly. (arXiv:2310.12300v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12300
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36866;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#33539;&#24335;&#65292;&#36890;&#36807;&#35843;&#25972;&#36880;&#28857;&#21487;&#29992;&#20449;&#24687;&#24230;&#37327;&#25351;&#26631;&#20026;&#36866;&#29992;&#20110;&#19978;&#19979;&#25991;&#30340;&#29256;&#26412;&#65292;&#23558;&#20854;&#21629;&#21517;&#20026;&#19978;&#19979;&#25991;PVI&#65292;&#24182;&#35777;&#26126;&#20102;&#19978;&#19979;&#25991;PVI&#30340;&#21487;&#38752;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#32780;&#21463;&#21040;&#38738;&#30544;&#12290;&#26412;&#25991;&#23558;&#26368;&#36817;&#25552;&#20986;&#30340;&#38590;&#24230;&#24230;&#37327;&#25351;&#26631;&#36880;&#28857;&#21487;&#29992;&#20449;&#24687;&#65288;PVI&#65289;&#35843;&#25972;&#20026;&#36866;&#29992;&#20110;&#19978;&#19979;&#25991;&#30340;&#29256;&#26412;&#65288;&#19978;&#19979;&#25991;PVI&#65289;&#12290;&#19982;&#21407;&#22987;PVI&#30456;&#27604;&#65292;&#19978;&#19979;&#25991;PVI&#26356;&#39640;&#25928;&#65292;&#22240;&#20026;&#23427;&#21482;&#38656;&#23569;&#37327;&#31034;&#20363;&#24182;&#19988;&#19981;&#38656;&#35201;&#24494;&#35843;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#35777;&#20998;&#26512;&#20197;&#35780;&#20272;&#19978;&#19979;&#25991;PVI&#30340;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19978;&#19979;&#25991;PVI&#30340;&#20272;&#35745;&#20540;&#34920;&#29616;&#20986;&#31867;&#20284;&#20110;&#21407;&#22987;PVI&#30340;&#29305;&#24449;&#12290;&#20855;&#20307;&#38024;&#23545;&#19978;&#19979;&#25991;&#29615;&#22659;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19978;&#19979;&#25991;PVI&#30340;&#20272;&#35745;&#20540;&#22312;&#19981;&#21516;&#31034;&#20363;&#36873;&#21462;&#21644;&#25293;&#25668;&#27425;&#25968;&#19979;&#20445;&#25345;&#19968;&#33268;&#12290;&#22312;&#19981;&#21516;&#30340;&#31034;&#20363;&#36873;&#21462;&#20013;&#65292;&#19978;&#19979;&#25991;PVI&#30340;&#20272;&#35745;&#20540;&#30340;&#26041;&#24046;&#26159;&#24494;&#19981;&#36275;&#36947;&#30340;&#65292;&#36825;&#34920;&#26126;&#19978;&#19979;&#25991;PVI&#26159;&#31283;&#23450;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#19978;&#19979;&#25991;PVI&#26469;&#35782;&#21035;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning (ICL) is a new learning paradigm that has gained popularity along with the development of large language models. In this work, we adapt a recently proposed hardness metric, pointwise $\mathcal{V}$-usable information (PVI), to an in-context version (in-context PVI). Compared to the original PVI, in-context PVI is more efficient in that it requires only a few exemplars and does not require fine-tuning. We conducted a comprehensive empirical analysis to evaluate the reliability of in-context PVI. Our findings indicate that in-context PVI estimates exhibit similar characteristics to the original PVI. Specific to the in-context setting, we show that in-context PVI estimates remain consistent across different exemplar selections and numbers of shots. The variance of in-context PVI estimates across different exemplar selections is insignificant, which suggests that in-context PVI are stable. Furthermore, we demonstrate how in-context PVI can be employed to identify challen
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27010;&#24565;&#25552;&#31034;&#23398;&#20064;&#65288;MCPL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#26102;&#23398;&#20064;&#22810;&#20010;&#26032;&#30340;&#8220;&#35789;&#8221;&#26469;&#35299;&#20915;&#22312;&#21333;&#20010;&#22330;&#26223;&#20013;&#35782;&#21035;&#21644;&#25972;&#21512;&#22810;&#20010;&#23545;&#35937;&#32423;&#27010;&#24565;&#30340;&#25361;&#25112;&#12290;&#38024;&#23545;&#35789;&#27010;&#24565;&#30456;&#20851;&#24615;&#20934;&#30830;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#27880;&#24847;&#21147;&#25513;&#30721;&#12289;&#25552;&#31034;&#23545;&#27604;&#25439;&#22833;&#21644;&#32465;&#23450;&#24418;&#23481;&#35789;&#31561;&#19977;&#31181;&#27491;&#21017;&#21270;&#25216;&#26415;&#12290;&#36890;&#36807;&#22270;&#20687;&#29983;&#25104;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26694;&#26550;&#33021;&#22815;&#29983;&#25104;&#26356;&#22810;&#26679;&#21270;&#21644;&#21512;&#25104;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2310.12274</link><description>&lt;p&gt;
&#19968;&#22270;&#25269;&#21315;&#35328;&#65306;&#20351;&#29992;&#22810;&#27010;&#24565;&#25552;&#31034;&#23398;&#20064;&#26469;&#23398;&#20064;&#23545;&#35937;&#32423;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
An Image is Worth Multiple Words: Learning Object Level Concepts using Multi-Concept Prompt Learning. (arXiv:2310.12274v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12274
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27010;&#24565;&#25552;&#31034;&#23398;&#20064;&#65288;MCPL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#26102;&#23398;&#20064;&#22810;&#20010;&#26032;&#30340;&#8220;&#35789;&#8221;&#26469;&#35299;&#20915;&#22312;&#21333;&#20010;&#22330;&#26223;&#20013;&#35782;&#21035;&#21644;&#25972;&#21512;&#22810;&#20010;&#23545;&#35937;&#32423;&#27010;&#24565;&#30340;&#25361;&#25112;&#12290;&#38024;&#23545;&#35789;&#27010;&#24565;&#30456;&#20851;&#24615;&#20934;&#30830;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#27880;&#24847;&#21147;&#25513;&#30721;&#12289;&#25552;&#31034;&#23545;&#27604;&#25439;&#22833;&#21644;&#32465;&#23450;&#24418;&#23481;&#35789;&#31561;&#19977;&#31181;&#27491;&#21017;&#21270;&#25216;&#26415;&#12290;&#36890;&#36807;&#22270;&#20687;&#29983;&#25104;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26694;&#26550;&#33021;&#22815;&#29983;&#25104;&#26356;&#22810;&#26679;&#21270;&#21644;&#21512;&#25104;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#23383;&#21453;&#36716;&#26159;&#19968;&#31181;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#23398;&#20064;&#19968;&#31181;&#26032;&#30340;&#8220;&#21333;&#35789;&#8221;&#30340;&#23884;&#20837;&#34920;&#31034;&#22270;&#20687;&#39118;&#26684;&#21644;&#22806;&#35266;&#65292;&#20351;&#20854;&#33021;&#22815;&#25972;&#21512;&#21040;&#33258;&#28982;&#35821;&#35328;&#21477;&#23376;&#20013;&#29983;&#25104;&#26032;&#30340;&#21512;&#25104;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#23545;&#20110;&#21487;&#33719;&#24471;&#20010;&#21035;&#27010;&#24565;&#30340;&#23884;&#20837;&#65292;&#35782;&#21035;&#21644;&#25972;&#21512;&#19968;&#20010;&#22330;&#26223;&#20013;&#30340;&#22810;&#20010;&#23545;&#35937;&#32423;&#27010;&#24565;&#20173;&#28982;&#38754;&#20020;&#30528;&#26174;&#33879;&#30340;&#25361;&#25112;&#65292;&#36825;&#20063;&#24471;&#21040;&#20102;&#25105;&#20204;&#30340;&#23454;&#35777;&#27979;&#35797;&#30340;&#36827;&#19968;&#27493;&#35777;&#23454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#27010;&#24565;&#25552;&#31034;&#23398;&#20064;&#65288;MCPL&#65289;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#19968;&#20010;&#21477;&#23376;-&#22270;&#20687;&#23545;&#20013;&#21516;&#26102;&#23398;&#20064;&#22810;&#20010;&#26032;&#30340;&#8220;&#35789;&#8221;&#12290;&#20026;&#20102;&#22686;&#24378;&#35789;&#27010;&#24565;&#30456;&#20851;&#24615;&#30340;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#27491;&#21017;&#21270;&#25216;&#26415;&#65306;&#27880;&#24847;&#21147;&#25513;&#30721;&#65288;AttnMask&#65289;&#23558;&#23398;&#20064;&#38598;&#20013;&#22312;&#30456;&#20851;&#21306;&#22495;&#65307;&#25552;&#31034;&#23545;&#27604;&#25439;&#22833;&#65288;PromptCL&#65289;&#23558;&#19981;&#21516;&#27010;&#24565;&#30340;&#23884;&#20837;&#20998;&#31163;&#24320;&#26469;&#65307;&#20197;&#21450;&#32465;&#23450;&#24418;&#23481;&#35789;&#65288;Bind adj.&#65289;&#23558;&#26032;&#30340;&#8220;&#35789;&#8221;&#19982;&#24050;&#30693;&#35789;&#30456;&#20851;&#32852;&#12290;&#25105;&#20204;&#36890;&#36807;&#22270;&#20687;&#29983;&#25104;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Textural Inversion, a prompt learning method, learns a singular embedding for a new "word" to represent image style and appearance, allowing it to be integrated into natural language sentences to generate novel synthesised images. However, identifying and integrating multiple object-level concepts within one scene poses significant challenges even when embeddings for individual concepts are attainable. This is further confirmed by our empirical tests. To address this challenge, we introduce a framework for Multi-Concept Prompt Learning (MCPL), where multiple new "words" are simultaneously learned from a single sentence-image pair. To enhance the accuracy of word-concept correlation, we propose three regularisation techniques: Attention Masking (AttnMask) to concentrate learning on relevant areas; Prompts Contrastive Loss (PromptCL) to separate the embeddings of different concepts; and Bind adjective (Bind adj.) to associate new "words" with known words. We evaluate via image generation
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20219;&#21153;&#32423;MoE&#22312;&#30452;&#25509;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#39640;&#24615;&#33021;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#37197;&#32622;&#65292;&#36890;&#36807;&#36825;&#20123;&#37197;&#32622;&#65292;&#20219;&#21153;&#32423;MoE&#30340;&#30452;&#25509;NMT&#31995;&#32479;&#22312;&#22823;&#37327;&#20302;&#36164;&#28304;&#35821;&#35328;&#23545;&#19978;&#20248;&#20110;&#21452;&#35821;&#21644;&#22522;&#20110;&#20013;&#38388;&#35821;&#35328;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.12236</link><description>&lt;p&gt;
&#20351;&#29992;&#20219;&#21153;&#32423;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#30340;&#30452;&#25509;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Direct Neural Machine Translation with Task-level Mixture of Experts models. (arXiv:2310.12236v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12236
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20219;&#21153;&#32423;MoE&#22312;&#30452;&#25509;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#39640;&#24615;&#33021;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#37197;&#32622;&#65292;&#36890;&#36807;&#36825;&#20123;&#37197;&#32622;&#65292;&#20219;&#21153;&#32423;MoE&#30340;&#30452;&#25509;NMT&#31995;&#32479;&#22312;&#22823;&#37327;&#20302;&#36164;&#28304;&#35821;&#35328;&#23545;&#19978;&#20248;&#20110;&#21452;&#35821;&#21644;&#22522;&#20110;&#20013;&#38388;&#35821;&#35328;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30452;&#25509;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;Direct NMT&#65289;&#26159;&#19968;&#31181;&#22312;&#20004;&#31181;&#38750;&#33521;&#35821;&#35821;&#35328;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#30340;NMT&#31995;&#32479;&#12290;&#30452;&#25509;NMT&#31995;&#32479;&#36890;&#24120;&#38754;&#20020;&#30001;&#20110;&#38750;&#33521;&#35821;&#35821;&#35328;&#23545;&#20043;&#38388;&#24179;&#34892;&#25968;&#25454;&#31232;&#32570;&#23548;&#33268;&#30340;&#38480;&#21046;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#20363;&#22914;&#22810;&#35821;NMT&#21644;&#22522;&#20110;&#20013;&#38388;&#35821;&#35328;&#65288;&#36890;&#36807;&#33521;&#35821;&#36827;&#34892;&#32763;&#35793;&#30340;NMT&#65289;&#30340;NMT&#12290;&#20219;&#21153;&#32423;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#65288;Task-level MoE&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#25512;&#29702;&#39640;&#25928;&#21464;&#20307;&#65292;&#23545;&#35768;&#22810;&#35821;&#35328;&#23545;&#23637;&#29616;&#20102;&#26377;&#21069;&#26223;&#30340;NMT&#24615;&#33021;&#12290;&#22312;&#20219;&#21153;&#32423;MoE&#20013;&#65292;&#19981;&#21516;&#30340;&#35821;&#35328;&#20998;&#32452;&#21487;&#20197;&#20351;&#29992;&#19981;&#21516;&#30340;&#36335;&#30001;&#31574;&#30053;&#26469;&#20248;&#21270;&#36328;&#35821;&#35328;&#23398;&#20064;&#21644;&#25512;&#29702;&#36895;&#24230;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20219;&#21153;&#32423;MoE&#22312;&#30452;&#25509;NMT&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#39640;&#24615;&#33021;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#37197;&#32622;&#65292;&#36890;&#36807;&#36825;&#20123;&#20219;&#21153;&#32423;MoE&#22522;&#30784;&#30340;&#30452;&#25509;NMT&#31995;&#32479;&#22312;&#22823;&#37327;&#20302;&#36164;&#28304;&#35821;&#35328;&#23545;&#19978;&#20248;&#20110;&#21452;&#35821;&#21644;&#22522;&#20110;&#20013;&#38388;&#35821;&#35328;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Direct neural machine translation (direct NMT) is a type of NMT system that translates text between two non-English languages. Direct NMT systems often face limitations due to the scarcity of parallel data between non-English language pairs. Several approaches have been proposed to address this limitation, such as multilingual NMT and pivot NMT (translation between two languages via English). Task-level Mixture of expert models (Task-level MoE), an inference-efficient variation of Transformer-based models, has shown promising NMT performance for a large number of language pairs. In Task-level MoE, different language groups can use different routing strategies to optimize cross-lingual learning and inference speed. In this work, we examine Task-level MoE's applicability in direct NMT and propose a series of high-performing training and evaluation configurations, through which Task-level MoE-based direct NMT systems outperform bilingual and pivot-based models for a large number of low an
&lt;/p&gt;</description></item><item><title>ImageArg-2023&#26159;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#35770;&#35777;&#25366;&#25496;&#30340;&#20849;&#20139;&#20219;&#21153;&#65292;&#28085;&#30422;&#20102;&#35770;&#35777;&#31435;&#22330;&#20998;&#31867;&#21644;&#22270;&#20687;&#35828;&#26381;&#21147;&#20998;&#31867;&#20004;&#20010;&#23376;&#20219;&#21153;&#12290;&#20849;&#25910;&#21040;&#20102;&#26469;&#33258;6&#20010;&#22269;&#23478;&#30340;9&#20010;&#22242;&#38431;&#25552;&#20132;&#30340;31&#20010;&#23376;&#20219;&#21153;A&#30340;&#25552;&#20132;&#21644;21&#20010;&#23376;&#20219;&#21153;B&#30340;&#25552;&#20132;&#65292;&#26368;&#22909;&#30340;&#25552;&#20132;&#22312;&#23376;&#20219;&#21153;A&#20013;&#36798;&#21040;&#20102;0.8647&#30340;F1&#24471;&#20998;&#65292;&#22312;&#23376;&#20219;&#21153;B&#20013;&#36798;&#21040;&#20102;0.5561&#30340;F1&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2310.12172</link><description>&lt;p&gt;
ImageArg-2023&#27010;&#36848;&#65306;&#22810;&#27169;&#24577;&#35770;&#35777;&#25366;&#25496;&#20013;&#30340;&#39318;&#20010;&#20849;&#20139;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Overview of ImageArg-2023: The First Shared Task in Multimodal Argument Mining. (arXiv:2310.12172v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12172
&lt;/p&gt;
&lt;p&gt;
ImageArg-2023&#26159;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#35770;&#35777;&#25366;&#25496;&#30340;&#20849;&#20139;&#20219;&#21153;&#65292;&#28085;&#30422;&#20102;&#35770;&#35777;&#31435;&#22330;&#20998;&#31867;&#21644;&#22270;&#20687;&#35828;&#26381;&#21147;&#20998;&#31867;&#20004;&#20010;&#23376;&#20219;&#21153;&#12290;&#20849;&#25910;&#21040;&#20102;&#26469;&#33258;6&#20010;&#22269;&#23478;&#30340;9&#20010;&#22242;&#38431;&#25552;&#20132;&#30340;31&#20010;&#23376;&#20219;&#21153;A&#30340;&#25552;&#20132;&#21644;21&#20010;&#23376;&#20219;&#21153;B&#30340;&#25552;&#20132;&#65292;&#26368;&#22909;&#30340;&#25552;&#20132;&#22312;&#23376;&#20219;&#21153;A&#20013;&#36798;&#21040;&#20102;0.8647&#30340;F1&#24471;&#20998;&#65292;&#22312;&#23376;&#20219;&#21153;B&#20013;&#36798;&#21040;&#20102;0.5561&#30340;F1&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ImageArg&#20849;&#20139;&#20219;&#21153;&#30340;&#27010;&#36848;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19982;EMNLP 2023 Argument Mining Workshop&#21516;&#26102;&#20030;&#21150;&#30340;&#22810;&#27169;&#24577;&#35770;&#35777;&#25366;&#25496;&#20849;&#20139;&#20219;&#21153;&#12290;&#35813;&#20849;&#20139;&#20219;&#21153;&#21253;&#25324;&#20004;&#20010;&#20998;&#31867;&#23376;&#20219;&#21153;&#65306;&#65288;1&#65289;&#23376;&#20219;&#21153;A&#65306;&#35770;&#35777;&#31435;&#22330;&#20998;&#31867;&#65307;&#65288;2&#65289;&#23376;&#20219;&#21153;B&#65306;&#22270;&#20687;&#35828;&#26381;&#21147;&#20998;&#31867;&#12290;&#21069;&#32773;&#30830;&#23450;&#20102;&#21253;&#21547;&#22270;&#20687;&#21644;&#19968;&#27573;&#25991;&#23383;&#30340;&#25512;&#25991;&#23545;&#20110;&#19968;&#20010;&#26377;&#20105;&#35758;&#30340;&#20027;&#39064;&#65288;&#22914;&#26538;&#25903;&#25511;&#21046;&#21644;&#22549;&#32974;&#65289;&#30340;&#31435;&#22330;&#12290;&#21518;&#32773;&#30830;&#23450;&#22270;&#20687;&#26159;&#21542;&#20351;&#25512;&#25991;&#30340;&#25991;&#23383;&#26356;&#20855;&#35828;&#26381;&#21147;&#12290;&#20849;&#20139;&#20219;&#21153;&#20849;&#25910;&#21040;&#26469;&#33258;6&#20010;&#22269;&#23478;&#30340;9&#20010;&#19981;&#21516;&#22242;&#38431;&#25552;&#20132;&#30340;31&#20010;&#23376;&#20219;&#21153;A&#30340;&#25552;&#20132;&#21644;21&#20010;&#23376;&#20219;&#21153;B&#30340;&#25552;&#20132;&#12290;&#23376;&#20219;&#21153;A&#20013;&#26368;&#22909;&#30340;&#25552;&#20132;&#30340;F1&#24471;&#20998;&#20026;0.8647&#65292;&#32780;&#23376;&#20219;&#21153;B&#20013;&#26368;&#22909;&#30340;&#25552;&#20132;&#30340;F1&#24471;&#20998;&#20026;0.5561&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an overview of the ImageArg shared task, the first multimodal Argument Mining shared task co-located with the 10th Workshop on Argument Mining at EMNLP 2023. The shared task comprises two classification subtasks - (1) Subtask-A: Argument Stance Classification; (2) Subtask-B: Image Persuasiveness Classification. The former determines the stance of a tweet containing an image and a piece of text toward a controversial topic (e.g., gun control and abortion). The latter determines whether the image makes the tweet text more persuasive. The shared task received 31 submissions for Subtask-A and 21 submissions for Subtask-B from 9 different teams across 6 countries. The top submission in Subtask-A achieved an F1-score of 0.8647 while the best submission in Subtask-B achieved an F1-score of 0.5561.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20851;&#27880;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20154;&#24037;&#26631;&#27880;&#30340;&#21464;&#24322;&#38382;&#39064;&#65292;&#36890;&#36807;&#25910;&#38598;&#19968;&#32452;&#24459;&#24072;&#23545;&#26696;&#20214;&#32467;&#26524;&#35780;&#20272;&#23384;&#22312;&#20998;&#27495;&#30340;&#25968;&#25454;&#38598;&#65292;&#23545;&#36825;&#20123;&#20998;&#27495;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#20004;&#32423;&#20998;&#31867;&#20307;&#31995;&#65292;&#24182;&#21457;&#29616;&#20998;&#27495;&#20027;&#35201;&#28304;&#20110;&#23545;&#27861;&#24459;&#32972;&#26223;&#30340;&#19981;&#26126;&#30830;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2310.11878</link><description>&lt;p&gt;
&#20174;&#19981;&#19968;&#33268;&#21040;&#27934;&#23519;&#65306;&#23545;&#26696;&#20363;&#32467;&#26524;&#20998;&#31867;&#30340;&#29702;&#30001;&#25968;&#25454;&#38598;&#26500;&#24314;&#36827;&#34892;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
From Dissonance to Insights: Dissecting Disagreements in Rationale Dataset Construction for Case Outcome Classification. (arXiv:2310.11878v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11878
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20154;&#24037;&#26631;&#27880;&#30340;&#21464;&#24322;&#38382;&#39064;&#65292;&#36890;&#36807;&#25910;&#38598;&#19968;&#32452;&#24459;&#24072;&#23545;&#26696;&#20214;&#32467;&#26524;&#35780;&#20272;&#23384;&#22312;&#20998;&#27495;&#30340;&#25968;&#25454;&#38598;&#65292;&#23545;&#36825;&#20123;&#20998;&#27495;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#20004;&#32423;&#20998;&#31867;&#20307;&#31995;&#65292;&#24182;&#21457;&#29616;&#20998;&#27495;&#20027;&#35201;&#28304;&#20110;&#23545;&#27861;&#24459;&#32972;&#26223;&#30340;&#19981;&#26126;&#30830;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#26696;&#20363;&#32467;&#26524;&#20998;&#31867;&#65288;COC&#65289;&#19981;&#20165;&#38656;&#35201;&#20934;&#30830;&#24615;&#65292;&#36824;&#38656;&#35201;&#21487;&#20449;&#36182;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#29616;&#26377;&#30340;&#21487;&#35299;&#37322;COC&#30740;&#31350;&#20165;&#38480;&#20110;&#30001;&#21333;&#20010;&#19987;&#23478;&#36827;&#34892;&#30340;&#27880;&#37322;&#12290;&#28982;&#32780;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;&#24459;&#24072;&#22312;&#23545;&#26696;&#20214;&#20107;&#23454;&#36827;&#34892;&#35780;&#20272;&#26102;&#21487;&#33021;&#23384;&#22312;&#20998;&#27495;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;RAVE&#65306;&#27431;&#27954;&#20154;&#26435;&#27861;&#39046;&#22495;&#30340;&#29702;&#30001;&#21464;&#24322;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#20174;&#22269;&#38469;&#20154;&#26435;&#27861;&#39046;&#22495;&#30340;&#20004;&#20301;&#19987;&#23478;&#37027;&#37324;&#33719;&#24471;&#30340;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20182;&#20204;&#20043;&#38388;&#23384;&#22312;&#24369;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20182;&#20204;&#30340;&#20998;&#27495;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#20004;&#32423;&#20219;&#21153;&#26080;&#20851;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#21516;&#26102;&#34917;&#20805;&#20102;COC&#29305;&#23450;&#30340;&#23376;&#31867;&#21035;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#39318;&#27425;&#20851;&#27880;&#20154;&#24037;&#26631;&#27880;&#30340;&#21464;&#24322;&#12290;&#25105;&#20204;&#23450;&#37327;&#35780;&#20272;&#20102;&#19981;&#21516;&#20998;&#31867;&#31867;&#21035;&#65292;&#24182;&#21457;&#29616;&#20998;&#27495;&#20027;&#35201;&#28304;&#20110;&#23545;&#27861;&#24459;&#32972;&#26223;&#30340;&#19981;&#26126;&#30830;&#25551;&#36848;&#65292;&#36825;&#22312;COC&#20803;&#25968;&#25454;&#36890;&#24120;&#20855;&#26377;&#26377;&#38480;&#32454;&#31890;&#24230;&#21644;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35780;&#20272;&#20102;SOTA COC&#27169;&#22411;&#22312;RAVE&#25968;&#25454;&#38598;&#19978;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#35266;&#23519;&#21040;...
&lt;/p&gt;
&lt;p&gt;
In legal NLP, Case Outcome Classification (COC) must not only be accurate but also trustworthy and explainable. Existing work in explainable COC has been limited to annotations by a single expert. However, it is well-known that lawyers may disagree in their assessment of case facts. We hence collect a novel dataset RAVE: Rationale Variation in ECHR1, which is obtained from two experts in the domain of international human rights law, for whom we observe weak agreement. We study their disagreements and build a two-level task-independent taxonomy, supplemented with COC-specific subcategories. To our knowledge, this is the first work in the legal NLP that focuses on human label variation. We quantitatively assess different taxonomy categories and find that disagreements mainly stem from underspecification of the legal context, which poses challenges given the typically limited granularity and noise in COC metadata. We further assess the explainablility of SOTA COC models on RAVE and observ
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#21407;&#22411;&#30340;&#36229;&#36866;&#37197;&#22120;&#65288;PHA&#65289;&#26694;&#26550;&#29992;&#20110;&#26679;&#26412;&#39640;&#25928;&#22810;&#20219;&#21153;&#35843;&#25972;&#65292;&#36890;&#36807;&#24341;&#20837;&#23454;&#20363;&#23494;&#38598;&#30340;&#26816;&#32034;&#22120;&#21644;&#26679;&#26412;&#39640;&#25928;&#30340;&#21407;&#22411;&#36229;&#32593;&#32476;&#29983;&#25104;&#26465;&#20214;&#27169;&#22359;&#65292;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#36801;&#31227;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#21487;&#27604;&#24615;&#33021;&#30340;&#25552;&#21319;&#65292;&#29978;&#33267;&#22312;&#25968;&#25454;&#37327;&#36739;&#23567;&#26102;&#20063;&#33021;&#36229;&#36807;&#20854;&#20182;&#24378;&#22522;&#32447;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.11670</link><description>&lt;p&gt;
&#22522;&#20110;&#21407;&#22411;&#30340;&#36229;&#36866;&#37197;&#22120;&#29992;&#20110;&#26679;&#26412;&#39640;&#25928;&#22810;&#20219;&#21153;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Prototype-based HyperAdapter for Sample-Efficient Multi-task Tuning. (arXiv:2310.11670v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11670
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21407;&#22411;&#30340;&#36229;&#36866;&#37197;&#22120;&#65288;PHA&#65289;&#26694;&#26550;&#29992;&#20110;&#26679;&#26412;&#39640;&#25928;&#22810;&#20219;&#21153;&#35843;&#25972;&#65292;&#36890;&#36807;&#24341;&#20837;&#23454;&#20363;&#23494;&#38598;&#30340;&#26816;&#32034;&#22120;&#21644;&#26679;&#26412;&#39640;&#25928;&#30340;&#21407;&#22411;&#36229;&#32593;&#32476;&#29983;&#25104;&#26465;&#20214;&#27169;&#22359;&#65292;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#36801;&#31227;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#21487;&#27604;&#24615;&#33021;&#30340;&#25552;&#21319;&#65292;&#29978;&#33267;&#22312;&#25968;&#25454;&#37327;&#36739;&#23567;&#26102;&#20063;&#33021;&#36229;&#36807;&#20854;&#20182;&#24378;&#22522;&#32447;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#24050;&#32463;&#35777;&#26126;&#22312;&#36866;&#24212;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21040;&#19979;&#28216;&#20219;&#21153;&#26102;&#26377;&#25928;&#65292;&#21516;&#26102;&#21482;&#26356;&#26032;&#20102;&#23569;&#37327;&#21442;&#25968;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#29420;&#31435;&#22320;&#36866;&#24212;&#27599;&#20010;&#20219;&#21153;&#65292;&#27809;&#26377;&#32771;&#34385;&#20219;&#21153;&#20043;&#38388;&#30340;&#30693;&#35782;&#20256;&#36755;&#65292;&#24182;&#19988;&#21463;&#38480;&#20110;&#20302;&#25968;&#25454;&#24773;&#26223;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#30340;&#36229;&#36866;&#37197;&#22120;&#65288;PHA&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#24314;&#31435;&#22312;&#36866;&#37197;&#22120;&#35843;&#25972;&#21644;&#36229;&#32593;&#32476;&#22522;&#30784;&#19978;&#12290;&#23427;&#24341;&#20837;&#20102;&#19968;&#20010;&#23454;&#20363;&#23494;&#38598;&#30340;&#26816;&#32034;&#22120;&#21644;&#19968;&#20010;&#26679;&#26412;&#39640;&#25928;&#30340;&#21407;&#22411;&#36229;&#32593;&#32476;&#26469;&#29983;&#25104;&#26465;&#20214;&#27169;&#22359;&#12290;&#36825;&#23548;&#33268;&#19982;&#29616;&#26377;PEFT&#26041;&#27861;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#36801;&#31227;&#23398;&#20064;&#19978;&#30456;&#24403;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#24403;&#21487;&#29992;&#25968;&#25454;&#37327;&#21464;&#23567;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#20854;&#20182;&#24378;&#22522;&#32447;&#26041;&#27861;&#26377;&#24456;&#22823;&#30340;&#20248;&#21183;&#12290;&#22522;&#20110;&#25105;&#20204;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#35777;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;PHA&#22312;&#26435;&#34913;&#26041;&#38754;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter-efficient fine-tuning (PEFT) has shown its effectiveness in adapting the pre-trained language models to downstream tasks while only updating a small number of parameters. Despite the success, most existing methods independently adapt to each task without considering knowledge transfer between tasks and are limited to low-data regimes. To overcome this issue, we propose Prototype-based HyperAdapter (PHA), a novel framework built on the adapter-tuning and hypernetwork. It introduces an instance-dense retriever and a prototypical hypernetwork to generate the conditional modules in a sample-efficient manner. This leads to comparable performance improvements against existing PEFT methods on multi-task learning and few-shot transfer learning. More importantly, when the available data size gets smaller, our method outperforms other strong baselines by a large margin. Based on our extensive empirical experiments across various datasets, we demonstrate that PHA strikes a better trade-
&lt;/p&gt;</description></item><item><title>VECHR&#26159;&#19968;&#20010;&#19987;&#23478;&#27880;&#37322;&#30340;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#27431;&#27954;&#20154;&#26435;&#27861;&#38498;&#28431;&#27934;&#31867;&#22411;&#30340;&#21487;&#35299;&#37322;&#21644;&#40065;&#26834;&#20998;&#31867;&#12290;&#35813;&#25968;&#25454;&#38598;&#24110;&#21161;&#35782;&#21035;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20379;&#35299;&#37322;&#29702;&#30001;&#12290;&#32467;&#26524;&#26174;&#31034;&#20102;&#35813;&#20219;&#21153;&#30340;&#25361;&#25112;&#24615;&#65292;&#27169;&#22411;&#19982;&#19987;&#23478;&#30340;&#19968;&#33268;&#24615;&#26377;&#38480;&#65292;&#27169;&#22411;&#22312;&#22788;&#29702;&#22495;&#22806;&#25968;&#25454;&#26102;&#40065;&#26834;&#24615;&#20063;&#36739;&#20302;&#12290;</title><link>http://arxiv.org/abs/2310.11368</link><description>&lt;p&gt;
VECHR&#65306;&#27431;&#27954;&#20154;&#26435;&#27861;&#38498;&#28431;&#27934;&#31867;&#22411;&#30340;&#21487;&#35299;&#37322;&#21644;&#40065;&#26834;&#20998;&#31867;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
VECHR: A Dataset for Explainable and Robust Classification of Vulnerability Type in the European Court of Human Rights. (arXiv:2310.11368v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11368
&lt;/p&gt;
&lt;p&gt;
VECHR&#26159;&#19968;&#20010;&#19987;&#23478;&#27880;&#37322;&#30340;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#27431;&#27954;&#20154;&#26435;&#27861;&#38498;&#28431;&#27934;&#31867;&#22411;&#30340;&#21487;&#35299;&#37322;&#21644;&#40065;&#26834;&#20998;&#31867;&#12290;&#35813;&#25968;&#25454;&#38598;&#24110;&#21161;&#35782;&#21035;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20379;&#35299;&#37322;&#29702;&#30001;&#12290;&#32467;&#26524;&#26174;&#31034;&#20102;&#35813;&#20219;&#21153;&#30340;&#25361;&#25112;&#24615;&#65292;&#27169;&#22411;&#19982;&#19987;&#23478;&#30340;&#19968;&#33268;&#24615;&#26377;&#38480;&#65292;&#27169;&#22411;&#22312;&#22788;&#29702;&#22495;&#22806;&#25968;&#25454;&#26102;&#40065;&#26834;&#24615;&#20063;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#33030;&#24369;&#24615;&#23545;&#20110;&#20102;&#35299;&#21644;&#23454;&#26045;&#26377;&#38024;&#23545;&#24615;&#30340;&#25903;&#25345;&#20197;&#22686;&#24378;&#26377;&#38656;&#35201;&#30340;&#20010;&#20154;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#19968;&#28857;&#22312;&#27431;&#27954;&#20154;&#26435;&#27861;&#38498;&#23588;&#20026;&#37325;&#35201;&#65292;&#27861;&#38498;&#23558;&#20844;&#32422;&#26631;&#20934;&#35843;&#25972;&#20026;&#28385;&#36275;&#23454;&#38469;&#20010;&#20307;&#38656;&#27714;&#65292;&#20174;&#32780;&#30830;&#20445;&#26377;&#25928;&#30340;&#20154;&#26435;&#20445;&#25252;&#12290;&#28982;&#32780;&#65292;&#33030;&#24369;&#24615;&#27010;&#24565;&#22312;&#27431;&#27954;&#20154;&#26435;&#27861;&#38498;&#20173;&#28982;&#27169;&#31946;&#19981;&#28165;&#65292;&#20043;&#21069;&#27809;&#26377;NLP&#30740;&#31350;&#28041;&#21450;&#21040;&#36825;&#20010;&#38382;&#39064;&#12290;&#20026;&#20102;&#20419;&#36827;&#26410;&#26469;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VECHR&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#19987;&#23478;&#27880;&#37322;&#30340;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#33030;&#24369;&#24615;&#31867;&#22411;&#20998;&#31867;&#21644;&#35299;&#37322;&#29702;&#30001;&#12290;&#25105;&#20204;&#20174;&#39044;&#27979;&#21644;&#35299;&#37322;&#24615;&#30340;&#35282;&#24230;&#23545;VECHR&#19978;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#20102;&#36825;&#19968;&#20219;&#21153;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29305;&#28857;&#65292;&#39044;&#27979;&#24615;&#33021;&#36739;&#20302;&#65292;&#24182;&#19988;&#27169;&#22411;&#21644;&#19987;&#23478;&#20043;&#38388;&#23384;&#22312;&#26377;&#38480;&#30340;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20123;&#27169;&#22411;&#22788;&#29702;&#22495;&#22806;&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#21457;&#29616;&#24635;&#20307;&#19978;&#21463;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recognizing vulnerability is crucial for understanding and implementing targeted support to empower individuals in need. This is especially important at the European Court of Human Rights (ECtHR), where the court adapts Convention standards to meet actual individual needs and thus ensures effective human rights protection. However, the concept of vulnerability remains elusive at the ECtHR and no prior NLP research has dealt with it. To enable future research in this area, we present VECHR, a novel expert-annotated multi-label dataset comprising of vulnerability type classification and explanation rationale. We benchmark the performance of state-of-the-art models on VECHR from both prediction and explainability perspectives. Our results demonstrate the challenging nature of the task with lower prediction performance and limited agreement between models and experts. Further, we analyze the robustness of these models in dealing with out-of-domain (OOD) data and observe overall limited per
&lt;/p&gt;</description></item><item><title>IDMO&#39033;&#30446;&#26088;&#22312;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#25171;&#20987;&#34394;&#20551;&#20449;&#24687;&#21644;&#20551;&#26032;&#38395;&#65292;&#20854;&#36129;&#29486;&#21253;&#25324;&#21019;&#24314;&#26032;&#22411;&#25968;&#25454;&#38598;&#12289;&#24320;&#21457;&#33258;&#21160;&#27169;&#22411;&#12289;&#35780;&#20272;GPT-4&#31561;&#12290;</title><link>http://arxiv.org/abs/2310.11097</link><description>&lt;p&gt;
&#29992;&#20110;&#25171;&#20987;&#34394;&#20551;&#20449;&#24687;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#23454;&#39564;&#65306;IDMO&#39033;&#30446;
&lt;/p&gt;
&lt;p&gt;
Experimenting AI Technologies for Disinformation Combat: the IDMO Project. (arXiv:2310.11097v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11097
&lt;/p&gt;
&lt;p&gt;
IDMO&#39033;&#30446;&#26088;&#22312;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#25171;&#20987;&#34394;&#20551;&#20449;&#24687;&#21644;&#20551;&#26032;&#38395;&#65292;&#20854;&#36129;&#29486;&#21253;&#25324;&#21019;&#24314;&#26032;&#22411;&#25968;&#25454;&#38598;&#12289;&#24320;&#21457;&#33258;&#21160;&#27169;&#22411;&#12289;&#35780;&#20272;GPT-4&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24847;&#22823;&#21033;&#25968;&#23383;&#23186;&#20307;&#35266;&#23519;&#39033;&#30446;&#65288;IDMO&#65289;&#26159;&#27431;&#27954;&#19968;&#39033;&#20513;&#35758;&#30340;&#19968;&#37096;&#20998;&#65292;&#19987;&#27880;&#20110;&#25171;&#20987;&#34394;&#20551;&#20449;&#24687;&#21644;&#20551;&#26032;&#38395;&#12290;&#26412;&#25253;&#21578;&#27010;&#36848;&#20102;Rai-CRITS&#22312;&#35813;&#39033;&#30446;&#20013;&#30340;&#36129;&#29486;&#65292;&#21253;&#25324;&#65306;&#65288;i&#65289;&#21019;&#24314;&#29992;&#20110;&#27979;&#35797;&#25216;&#26415;&#30340;&#26032;&#22411;&#25968;&#25454;&#38598;&#65292;&#65288;ii&#65289;&#24320;&#21457;&#33258;&#21160;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#31867;Pagella Politica&#30340;&#35009;&#20915;&#20197;&#20415;&#20110;&#26356;&#24191;&#27867;&#30340;&#20998;&#26512;&#65292;&#65288;iii&#65289;&#21019;&#24314;&#33258;&#21160;&#27169;&#22411;&#65292;&#23545;FEVER&#25968;&#25454;&#38598;&#19978;&#30340;&#25991;&#26412;&#34164;&#21547;&#20855;&#26377;&#24322;&#24120;&#31934;&#24230;&#30340;&#35782;&#21035;&#33021;&#21147;&#65292;&#65288;iv&#65289;&#20351;&#29992;GPT-4&#35780;&#20272;&#25991;&#26412;&#34164;&#21547;&#65292; &#65288;v&#65289;&#22312;&#22269;&#23478;&#27963;&#21160;&#20013;&#24320;&#23637;&#25552;&#39640;&#23545;&#20551;&#26032;&#38395;&#24847;&#35782;&#30340;&#28216;&#25103;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Italian Digital Media Observatory (IDMO) project, part of a European initiative, focuses on countering disinformation and fake news. This report outlines contributions from Rai-CRITS to the project, including: (i) the creation of novel datasets for testing technologies (ii) development of an automatic model for categorizing Pagella Politica verdicts to facilitate broader analysis (iii) creation of an automatic model for recognizing textual entailment with exceptional accuracy on the FEVER dataset (iv) assessment using GPT-4 to identify textual entailmen (v) a game to raise awareness about fake news at national events.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;BiomedJourney&#65292;&#36890;&#36807;&#25351;&#23548;&#23398;&#20064;&#22810;&#27169;&#24577;&#24739;&#32773;&#26053;&#31243;&#65292;&#36827;&#34892;&#21453;&#20107;&#23454;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#12290;&#20351;&#29992;GPT-4&#22788;&#29702;&#22270;&#20687;&#25253;&#21578;&#29983;&#25104;&#30142;&#30149;&#36827;&#23637;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#24182;&#35757;&#32451;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.10765</link><description>&lt;p&gt;
BiomedJourney: &#25351;&#23548;&#23398;&#20064;&#22810;&#27169;&#24577;&#24739;&#32773;&#26053;&#31243;&#20013;&#30340;&#21453;&#20107;&#23454;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
BiomedJourney: Counterfactual Biomedical Image Generation by Instruction-Learning from Multimodal Patient Journeys. (arXiv:2310.10765v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10765
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;BiomedJourney&#65292;&#36890;&#36807;&#25351;&#23548;&#23398;&#20064;&#22810;&#27169;&#24577;&#24739;&#32773;&#26053;&#31243;&#65292;&#36827;&#34892;&#21453;&#20107;&#23454;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#12290;&#20351;&#29992;GPT-4&#22788;&#29702;&#22270;&#20687;&#25253;&#21578;&#29983;&#25104;&#30142;&#30149;&#36827;&#23637;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#24182;&#35757;&#32451;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#22270;&#20687;&#32534;&#36753;&#30340;&#25351;&#23548;&#23398;&#20064;&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#65292;&#22914;InstructPix2Pix&#65292;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#21487;&#20197;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;&#21453;&#20107;&#23454;&#22270;&#20687;&#29983;&#25104;&#65292;&#20174;&#32780;&#24110;&#21161;&#21306;&#20998;&#22240;&#26524;&#32467;&#26500;&#21644;&#20266;&#30456;&#20851;&#65292;&#24182;&#20419;&#36827;&#30142;&#30149;&#36827;&#23637;&#24314;&#27169;&#30340;&#31283;&#20581;&#22270;&#20687;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#36890;&#29992;&#30340;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#24182;&#19981;&#36866;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#65292;&#21453;&#20107;&#23454;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#30340;&#30740;&#31350;&#36824;&#36828;&#26410;&#28145;&#20837;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;BiomedJourney&#65292;&#36890;&#36807;&#25351;&#23548;&#23398;&#20064;&#22810;&#27169;&#24577;&#24739;&#32773;&#26053;&#31243;&#65292;&#36827;&#34892;&#21453;&#20107;&#23454;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#12290;&#32473;&#23450;&#19968;&#20010;&#25293;&#25668;&#20110;&#19981;&#21516;&#26102;&#38388;&#28857;&#30340;&#20004;&#20010;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#30340;&#24739;&#32773;&#65292;&#25105;&#20204;&#20351;&#29992;GPT-4&#22788;&#29702;&#30456;&#24212;&#30340;&#22270;&#20687;&#25253;&#21578;&#65292;&#24182;&#29983;&#25104;&#30142;&#30149;&#36827;&#23637;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#29983;&#25104;&#30340;&#19977;&#20803;&#32452;&#65288;&#20808;&#21069;&#22270;&#20687;&#12289;&#36827;&#23637;&#25551;&#36848;&#12289;&#26032;&#22270;&#20687;&#65289;&#26469;&#35757;&#32451;&#19968;&#20010;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapid progress has been made in instruction-learning for image editing with natural-language instruction, as exemplified by InstructPix2Pix. In biomedicine, such methods can be applied to counterfactual image generation, which helps differentiate causal structure from spurious correlation and facilitate robust image interpretation for disease progression modeling. However, generic image-editing models are ill-suited for the biomedical domain, and counterfactual biomedical image generation is largely underexplored. In this paper, we present BiomedJourney, a novel method for counterfactual biomedical image generation by instruction-learning from multimodal patient journeys. Given a patient with two biomedical images taken at different time points, we use GPT-4 to process the corresponding imaging reports and generate a natural language description of disease progression. The resulting triples (prior image, progression description, new image) are then used to train a latent diffusion mode
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#25991;&#26723;&#36793;&#30028;&#30340;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#30456;&#20851;&#25991;&#26723;&#24207;&#21015;&#19978;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#40723;&#21169;&#27169;&#22411;&#36827;&#34892;&#36328;&#25991;&#26723;&#30340;&#38405;&#35835;&#21644;&#25512;&#29702;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25913;&#21464;&#25991;&#26723;&#39034;&#24207;&#24182;&#24212;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#31649;&#36947;&#26469;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.10638</link><description>&lt;p&gt;
&#36229;&#36234;&#25991;&#26723;&#36793;&#30028;&#30340;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#65306;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
In-Context Pretraining: Language Modeling Beyond Document Boundaries. (arXiv:2310.10638v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#25991;&#26723;&#36793;&#30028;&#30340;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#30456;&#20851;&#25991;&#26723;&#24207;&#21015;&#19978;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#40723;&#21169;&#27169;&#22411;&#36827;&#34892;&#36328;&#25991;&#26723;&#30340;&#38405;&#35835;&#21644;&#25512;&#29702;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25913;&#21464;&#25991;&#26723;&#39034;&#24207;&#24182;&#24212;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#31649;&#36947;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#36890;&#36807;&#39044;&#27979;&#32473;&#23450;&#25991;&#26723;&#21069;&#32512;&#30340;&#26631;&#35760;&#26469;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#33021;&#22815;&#30452;&#25509;&#36827;&#34892;&#38271;&#31687;&#29983;&#25104;&#21644;&#25552;&#31034;&#24335;&#20219;&#21153;&#65292;&#36825;&#21487;&#20197;&#31616;&#21270;&#20026;&#25991;&#26723;&#23436;&#25104;&#12290;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#31649;&#36947;&#36890;&#36807;&#36830;&#25509;&#38543;&#26426;&#32452;&#21512;&#30340;&#30701;&#25991;&#26723;&#26469;&#35757;&#32451;LMs&#65292;&#20197;&#21019;&#24314;&#36755;&#20837;&#19978;&#19979;&#25991;&#65292;&#20294;&#21069;&#19968;&#20010;&#25991;&#26723;&#23545;&#20110;&#39044;&#27979;&#19979;&#19968;&#20010;&#25991;&#26723;&#27809;&#26377;&#25552;&#20379;&#20219;&#20309;&#20449;&#21495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#65292;&#21363;&#22312;&#30456;&#20851;&#25991;&#26723;&#24207;&#21015;&#19978;&#39044;&#20808;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#26126;&#30830;&#40723;&#21169;&#23427;&#20204;&#36328;&#36234;&#25991;&#26723;&#36793;&#30028;&#36827;&#34892;&#38405;&#35835;&#21644;&#25512;&#29702;&#12290;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#25913;&#21464;&#25991;&#26723;&#39034;&#24207;&#65292;&#20351;&#27599;&#20010;&#19978;&#19979;&#25991;&#21253;&#21547;&#30456;&#20851;&#30340;&#25991;&#26723;&#65292;&#24182;&#30452;&#25509;&#24212;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#31649;&#36947;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#25991;&#26723;&#25490;&#24207;&#38382;&#39064;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26377;&#25968;&#21313;&#20159;&#20010;&#25991;&#26723;&#65292;&#25105;&#20204;&#24076;&#26395;&#22312;&#27599;&#20010;&#25991;&#26723;&#20013;&#26368;&#22823;&#21270;&#19978;&#19979;&#25991;&#30456;&#20284;&#24615;&#32780;&#19981;&#37325;&#22797;&#20219;&#20309;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LMs) are currently trained to predict tokens given document prefixes, enabling them to directly perform long-form generation and prompting-style tasks which can be reduced to document completion. Existing pretraining pipelines train LMs by concatenating random sets of short documents to create input contexts but the prior documents provide no signal for predicting the next document. We instead present In-Context Pretraining, a new approach where language models are pretrained on a sequence of related documents, thereby explicitly encouraging them to read and reason across document boundaries. We can do In-Context Pretraining by simply changing the document ordering so that each context contains related documents, and directly applying existing pretraining pipelines. However, this document sorting problem is challenging. There are billions of documents and we would like the sort to maximize contextual similarity for every document without repeating any data. To do
&lt;/p&gt;</description></item><item><title>VIBE&#26159;&#19968;&#31181;&#35299;&#20915;Twitter&#20998;&#31867;&#20013;&#35821;&#35328;&#29305;&#24449;&#28436;&#21464;&#38382;&#39064;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24314;&#27169;&#28508;&#22312;&#20027;&#39064;&#28436;&#21464;&#20197;&#36866;&#24212;&#21160;&#24577;&#29615;&#22659;&#65292;&#24182;&#19988;&#22312;&#22823;&#35268;&#27169;Twitter&#23454;&#39564;&#20013;&#23637;&#29616;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.10191</link><description>&lt;p&gt;
VIBE&#65306;Twitter&#20998;&#31867;&#30340;&#20027;&#39064;&#39537;&#21160;&#26102;&#38388;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
VIBE: Topic-Driven Temporal Adaptation for Twitter Classification. (arXiv:2310.10191v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10191
&lt;/p&gt;
&lt;p&gt;
VIBE&#26159;&#19968;&#31181;&#35299;&#20915;Twitter&#20998;&#31867;&#20013;&#35821;&#35328;&#29305;&#24449;&#28436;&#21464;&#38382;&#39064;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24314;&#27169;&#28508;&#22312;&#20027;&#39064;&#28436;&#21464;&#20197;&#36866;&#24212;&#21160;&#24577;&#29615;&#22659;&#65292;&#24182;&#19988;&#22312;&#22823;&#35268;&#27169;Twitter&#23454;&#39564;&#20013;&#23637;&#29616;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#29305;&#24449;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#31038;&#20132;&#23186;&#20307;&#20013;&#19981;&#26029;&#21464;&#21270;&#65292;&#23548;&#33268;&#25991;&#26412;&#20998;&#31867;&#22312;&#21160;&#24577;&#29615;&#22659;&#19979;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26102;&#38388;&#33258;&#36866;&#24212;&#65292;&#21363;&#22312;&#36807;&#21435;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#26410;&#26469;&#36827;&#34892;&#27979;&#35797;&#12290;&#20808;&#21069;&#30340;&#22823;&#37096;&#20998;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#32487;&#32493;&#39044;&#35757;&#32451;&#25110;&#30693;&#35782;&#26356;&#26032;&#19978;&#65292;&#36825;&#21487;&#33021;&#20250;&#24433;&#21709;&#23427;&#20204;&#22312;&#22122;&#22768;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#36807;&#24314;&#27169;&#28508;&#22312;&#20027;&#39064;&#28436;&#21464;&#26469;&#21453;&#26144;&#29305;&#24449;&#21464;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;VIBE&#65306;Evolutions&#30340;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#20004;&#20010;&#20449;&#24687;&#29942;&#39048;(Bottleneck)&#27491;&#21017;&#21270;&#22120;&#26469;&#21306;&#20998;&#36807;&#21435;&#21644;&#26410;&#26469;&#30340;&#20027;&#39064;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#21306;&#20998;&#30340;&#20027;&#39064;&#36890;&#36807;&#26102;&#38388;&#25139;&#21644;&#31867;&#21035;&#26631;&#31614;&#39044;&#27979;&#36827;&#34892;&#22810;&#20219;&#21153;&#35757;&#32451;&#65292;&#20316;&#20026;&#33258;&#36866;&#24212;&#29305;&#24449;&#12290;&#22312;&#33258;&#36866;&#24212;&#23398;&#20064;&#36807;&#31243;&#20013;&#65292;VIBE&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#26102;&#38388;&#20043;&#21518;&#21019;&#24314;&#30340;&#22312;&#32447;&#27969;&#31243;&#20013;&#26816;&#32034;&#21040;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#12290;&#22312;&#19977;&#20010;&#20998;&#31867;&#20219;&#21153;&#30340;&#22823;&#35268;&#27169;Twitter&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language features are evolving in real-world social media, resulting in the deteriorating performance of text classification in dynamics. To address this challenge, we study temporal adaptation, where models trained on past data are tested in the future. Most prior work focused on continued pretraining or knowledge updating, which may compromise their performance on noisy social media data. To tackle this issue, we reflect feature change via modeling latent topic evolution and propose a novel model, VIBE: Variational Information Bottleneck for Evolutions. Concretely, we first employ two Information Bottleneck (IB) regularizers to distinguish past and future topics. Then, the distinguished topics work as adaptive features via multi-task training with timestamp and class label prediction. In adaptive learning, VIBE utilizes retrieved unlabeled data from online streams created posterior to training data time. Substantial Twitter experiments on three classification tasks show that our mode
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#20998;&#24067;&#24335;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#22788;&#29702;&#25105;&#20204;&#26032;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#26102;&#37117;&#23384;&#22312;&#22256;&#38590;&#65292;&#23613;&#31649;&#23427;&#20204;&#22312;&#20854;&#20182;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#36825;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#30340;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#20173;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2310.09430</link><description>&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#20998;&#24067;&#24335;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#31995;&#32479;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Systematic Evaluation of Large Language Models on Out-of-Distribution Logical Reasoning Tasks. (arXiv:2310.09430v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09430
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#20998;&#24067;&#24335;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#22788;&#29702;&#25105;&#20204;&#26032;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#26102;&#37117;&#23384;&#22312;&#22256;&#38590;&#65292;&#23613;&#31649;&#23427;&#20204;&#22312;&#20854;&#20182;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#36825;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#30340;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#20173;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT-3.5&#21644;GPT-4&#65292;&#24050;&#32463;&#23558;&#20154;&#24037;&#31995;&#32479;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#21040;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#30340;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#35780;&#20272;&#12290;&#20026;&#20102;&#25506;&#32034;&#36825;&#31181;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#26032;&#30340;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#20998;&#21035;&#21517;&#20026;"ReClor-plus"&#12289;"LogiQA-plus"&#21644;"LogiQAv2-plus"&#65292;&#27599;&#20010;&#25968;&#25454;&#38598;&#37117;&#21253;&#21547;&#19977;&#20010;&#23376;&#38598;&#65306;&#31532;&#19968;&#20010;&#26159;&#36873;&#39033;&#38543;&#26426;&#25171;&#20081;&#65292;&#31532;&#20108;&#20010;&#26159;&#23558;&#27491;&#30830;&#36873;&#39033;&#26367;&#25442;&#20026;"&#27809;&#26377;&#20854;&#20182;&#36873;&#39033;&#26159;&#27491;&#30830;&#30340;"&#65292;&#31532;&#19977;&#20010;&#26159;&#21069;&#20004;&#20010;&#23376;&#38598;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20351;&#29992;&#20102;&#37492;&#21035;&#21644;&#29983;&#25104;&#22411;&#30340;LLMs&#65292;&#24182;&#34920;&#26126;&#36825;&#20123;&#31616;&#21333;&#30340;&#25216;&#24039;&#26497;&#22823;&#22320;&#38459;&#30861;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#22312;&#21407;&#22987;&#30340;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#25152;&#26377;&#27169;&#22411;&#37117;&#24456;&#38590;&#22238;&#31572;&#25105;&#20204;&#26032;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#25200;&#21160;&#24341;&#20837;&#20219;&#21153;&#21464;&#21270;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), such as GPT-3.5 and GPT-4, have greatly advanced the performance of artificial systems on various natural language processing tasks to human-like levels. However, their generalisation and robustness to perform logical reasoning remain under-evaluated. To probe this ability, we propose three new logical reasoning datasets named "ReClor-plus", "LogiQA-plus" and "LogiQAv2-plus", each featuring three subsets: the first with randomly shuffled options, the second with the correct choices replaced by "none of the other options are correct", and a combination of the previous two subsets. We carry out experiments on these datasets with both discriminative and generative LLMs and show that these simple tricks greatly hinder the performance of the language models. Despite their superior performance on the original publicly available datasets, we find that all models struggle to answer our newly constructed datasets. We show that introducing task variations by perturb
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;LLM&#29983;&#25104;&#32467;&#26524;&#36827;&#34892;&#37325;&#26032;&#25490;&#21517;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27491;&#30830;&#19981;&#21464;&#37327;&#30340;&#25490;&#21517;&#65292;&#20174;&#32780;&#20943;&#23569;&#31243;&#24207;&#39564;&#35777;&#30340;&#35843;&#29992;&#27425;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.09342</link><description>&lt;p&gt;
&#20026;&#31243;&#24207;&#39564;&#35777;&#23545;LLM&#29983;&#25104;&#30340;&#24490;&#29615;&#19981;&#21464;&#24335;&#36827;&#34892;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
Ranking LLM-Generated Loop Invariants for Program Verification. (arXiv:2310.09342v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;LLM&#29983;&#25104;&#32467;&#26524;&#36827;&#34892;&#37325;&#26032;&#25490;&#21517;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27491;&#30830;&#19981;&#21464;&#37327;&#30340;&#25490;&#21517;&#65292;&#20174;&#32780;&#20943;&#23569;&#31243;&#24207;&#39564;&#35777;&#30340;&#35843;&#29992;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#24402;&#32435;&#24490;&#29615;&#19981;&#21464;&#37327;&#26159;&#33258;&#21160;&#21270;&#31243;&#24207;&#39564;&#35777;&#30340;&#22522;&#30784;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;gpt-3.5&#25110;gpt-4&#65289;&#33021;&#22815;&#22312;0-shot&#29615;&#22659;&#19979;&#20026;&#19968;&#31867;&#31243;&#24207;&#21512;&#25104;&#24490;&#29615;&#19981;&#21464;&#37327;&#65292;&#20294;&#38656;&#35201;&#22810;&#20010;&#26679;&#26412;&#25165;&#33021;&#29983;&#25104;&#27491;&#30830;&#30340;&#19981;&#21464;&#37327;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#22823;&#37327;&#35843;&#29992;&#31243;&#24207;&#39564;&#35777;&#22120;&#26469;&#24314;&#31435;&#19981;&#21464;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;LLM&#29983;&#25104;&#32467;&#26524;&#36827;&#34892;&#37325;&#26032;&#25490;&#21517;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#25490;&#21517;&#22120;&#65292;&#21487;&#20197;&#26681;&#25454;&#38382;&#39064;&#23450;&#20041;&#21306;&#20998;&#27491;&#30830;&#30340;&#24402;&#32435;&#19981;&#21464;&#37327;&#21644;&#38169;&#35823;&#30340;&#23581;&#35797;&#12290;&#35813;&#25490;&#21517;&#22120;&#32463;&#36807;&#23545;&#27604;&#25490;&#21517;&#20248;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#37325;&#26032;&#25490;&#21517;&#26426;&#21046;&#26174;&#33879;&#25552;&#39640;&#20102;&#27491;&#30830;&#19981;&#21464;&#37327;&#22312;&#29983;&#25104;&#30340;&#20505;&#36873;&#39033;&#20013;&#30340;&#25490;&#21517;&#65292;&#20174;&#32780;&#22823;&#24133;&#20943;&#23569;&#20102;&#23545;&#39564;&#35777;&#22120;&#30340;&#35843;&#29992;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthesizing inductive loop invariants is fundamental to automating program verification. In this work, we observe that Large Language Models (such as gpt-3.5 or gpt-4) are capable of synthesizing loop invariants for a class of programs in a 0-shot setting, yet require several samples to generate the correct invariants. This can lead to a large number of calls to a program verifier to establish an invariant. To address this issue, we propose a {\it re-ranking} approach for the generated results of LLMs. We have designed a ranker that can distinguish between correct inductive invariants and incorrect attempts based on the problem definition. The ranker is optimized as a contrastive ranker. Experimental results demonstrate that this re-ranking mechanism significantly improves the ranking of correct invariants among the generated candidates, leading to a notable reduction in the number of calls to a verifier.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#37319;&#29992;&#25506;&#32034;&#25351;&#23548;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#36827;&#34892;&#20027;&#21160;&#25506;&#32034;&#65292;&#22686;&#24378;&#20102;&#39046;&#22495;&#29305;&#23450;&#25351;&#23548;&#35843;&#20248;&#30340;&#25968;&#25454;&#35206;&#30422;&#33539;&#22260;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.09168</link><description>&lt;p&gt;
&#25506;&#32034;&#25351;&#23548;&#65306;&#36890;&#36807;&#20027;&#21160;&#25506;&#32034;&#22686;&#24378;&#29305;&#23450;&#39046;&#22495;&#25351;&#23548;&#35206;&#30422;&#29575;
&lt;/p&gt;
&lt;p&gt;
Explore-Instruct: Enhancing Domain-Specific Instruction Coverage through Active Exploration. (arXiv:2310.09168v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09168
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37319;&#29992;&#25506;&#32034;&#25351;&#23548;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#36827;&#34892;&#20027;&#21160;&#25506;&#32034;&#65292;&#22686;&#24378;&#20102;&#39046;&#22495;&#29305;&#23450;&#25351;&#23548;&#35843;&#20248;&#30340;&#25968;&#25454;&#35206;&#30422;&#33539;&#22260;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22686;&#24378;&#22810;&#26679;&#24615;&#65292;&#21487;&#20197;&#22823;&#24133;&#20248;&#21270;&#25351;&#23548;&#35843;&#20248;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#26356;&#24191;&#27867;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#27492;&#31867;&#35843;&#20248;&#30340;&#29616;&#26377;&#25968;&#25454;&#24448;&#24448;&#23545;&#20010;&#21035;&#39046;&#22495;&#30340;&#35206;&#30422;&#19981;&#36275;&#65292;&#38480;&#21046;&#20102;&#23545;&#36825;&#20123;&#39046;&#22495;&#20869;&#32454;&#33268;&#29702;&#35299;&#21644;&#20132;&#20114;&#30340;&#33539;&#22260;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#25506;&#32034;&#25351;&#23548;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#30340;&#20027;&#21160;&#25506;&#32034;&#26469;&#22686;&#24378;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#25351;&#23548;&#35843;&#20248;&#30340;&#25968;&#25454;&#35206;&#30422;&#12290;&#25506;&#32034;&#25351;&#23548;&#22522;&#20110;&#20856;&#22411;&#30340;&#39046;&#22495;&#20351;&#29992;&#26696;&#20363;&#65292;&#36890;&#36807;&#23454;&#29616;&#25628;&#32034;&#31639;&#27861;&#26469;&#33719;&#21462;&#22810;&#26679;&#21270;&#21644;&#38754;&#21521;&#39046;&#22495;&#30340;&#25351;&#23548;&#35843;&#20248;&#25968;&#25454;&#30340;&#22810;&#31181;&#21464;&#20307;&#25110;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#20013;&#24515;&#20998;&#26512;&#39564;&#35777;&#20102;&#27492;&#26041;&#27861;&#22312;&#25913;&#36827;&#29305;&#23450;&#39046;&#22495;&#25351;&#23548;&#35206;&#30422;&#33539;&#22260;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#27169;&#22411;&#30340;&#24615;&#33021;&#26174;&#31034;&#20986;&#19982;&#22810;&#20010;&#22522;&#32447;&#27169;&#22411;&#30456;&#27604;&#30340;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction-tuning can be substantially optimized through enhanced diversity, resulting in models capable of handling a broader spectrum of tasks. However, existing data employed for such tuning often exhibit an inadequate coverage of individual domains, limiting the scope for nuanced comprehension and interactions within these areas. To address this deficiency, we propose Explore-Instruct, a novel approach to enhance the data coverage to be used in domain-specific instruction-tuning through active exploration via Large Language Models (LLMs). Built upon representative domain use cases, Explore-Instruct explores a multitude of variations or possibilities by implementing a search algorithm to obtain diversified and domain-focused instruction-tuning data. Our data-centric analysis validates the effectiveness of this proposed approach in improving domain-specific instruction coverage. Moreover, our model's performance demonstrates considerable advancements over multiple baselines, includi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24605;&#36335;&#38142;&#65288;CoT&#65289;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23569;&#26679;&#26412;&#30693;&#35782;&#24211;&#38382;&#39064;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#38382;&#39064;&#29983;&#25104;&#20219;&#21153;&#24418;&#24335;&#21270;&#20026;&#25512;&#29702;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26816;&#32034;&#25903;&#25345;&#24615;&#36923;&#36753;&#24418;&#24335;&#21644;&#32534;&#20889;&#25552;&#31034;&#26469;&#23454;&#29616;&#29983;&#25104;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2310.08395</link><description>&lt;p&gt;
&#20351;&#29992;&#24605;&#36335;&#38142;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#30693;&#35782;&#24211;&#38382;&#39064;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Prompting Large Language Models with Chain-of-Thought for Few-Shot Knowledge Base Question Generation. (arXiv:2310.08395v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24605;&#36335;&#38142;&#65288;CoT&#65289;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23569;&#26679;&#26412;&#30693;&#35782;&#24211;&#38382;&#39064;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#38382;&#39064;&#29983;&#25104;&#20219;&#21153;&#24418;&#24335;&#21270;&#20026;&#25512;&#29702;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26816;&#32034;&#25903;&#25345;&#24615;&#36923;&#36753;&#24418;&#24335;&#21644;&#32534;&#20889;&#25552;&#31034;&#26469;&#23454;&#29616;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#24211;&#38382;&#31572;&#29983;&#25104;&#65288;KBQG&#65289;&#30340;&#20219;&#21153;&#26159;&#23558;&#36923;&#36753;&#24418;&#24335;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#12290;&#30001;&#20110;&#22823;&#35268;&#27169;&#38382;&#39064;&#27880;&#37322;&#30340;&#26114;&#36149;&#25104;&#26412;&#65292;&#22312;&#20302;&#36164;&#28304;&#22330;&#26223;&#19979;&#24613;&#38656;&#24320;&#21457;KBQG&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#38382;&#39064;&#29983;&#25104;&#20013;&#36807;&#20110;&#20381;&#36182;&#27880;&#37322;&#25968;&#25454;&#30340;&#24494;&#35843;&#65292;&#36825;&#23545;&#20110;&#23569;&#26679;&#26412;&#38382;&#39064;&#29983;&#25104;&#24182;&#19981;&#21512;&#36866;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#23569;&#26679;&#26412;&#20219;&#21153;&#20013;&#30340;&#21360;&#35937;&#21147;&#27867;&#21270;&#33021;&#21147;&#12290;&#21463;&#21040;&#24605;&#36335;&#38142;&#65288;CoT&#65289;&#25552;&#31034;&#30340;&#21551;&#21457;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#25512;&#29702;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#31574;&#30053;&#65292;&#25105;&#20204;&#23558;KBQG&#20219;&#21153;&#24418;&#24335;&#21270;&#20026;&#25512;&#29702;&#38382;&#39064;&#65292;&#20854;&#20013;&#19968;&#20010;&#23436;&#25972;&#38382;&#39064;&#30340;&#29983;&#25104;&#34987;&#20998;&#20026;&#19968;&#31995;&#21015;&#30340;&#23376;&#38382;&#39064;&#29983;&#25104;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#25552;&#31034;&#26041;&#27861;KQG-CoT&#39318;&#20808;&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#27744;&#20013;&#26816;&#32034;&#25903;&#25345;&#24615;&#30340;&#36923;&#36753;&#24418;&#24335;&#65292;&#32771;&#34385;&#36923;&#36753;&#24418;&#24335;&#30340;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32534;&#20889;&#19968;&#20010;&#25552;&#31034;&#26469;&#26126;&#30830;&#25512;&#29702;&#38142;&#30340;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of Question Generation over Knowledge Bases (KBQG) aims to convert a logical form into a natural language question. For the sake of expensive cost of large-scale question annotation, the methods of KBQG under low-resource scenarios urgently need to be developed. However, current methods heavily rely on annotated data for fine-tuning, which is not well-suited for few-shot question generation. The emergence of Large Language Models (LLMs) has shown their impressive generalization ability in few-shot tasks. Inspired by Chain-of-Thought (CoT) prompting, which is an in-context learning strategy for reasoning, we formulate KBQG task as a reasoning problem, where the generation of a complete question is splitted into a series of sub-question generation. Our proposed prompting method KQG-CoT first retrieves supportive logical forms from the unlabeled data pool taking account of the characteristics of the logical form. Then, we write a prompt to explicit the reasoning chain of generati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#19978;&#20851;&#20110;&#27668;&#20505;&#21464;&#21270;&#30340;&#25512;&#25991;&#24773;&#24863;&#24577;&#24230;&#65292;&#36890;&#36807;&#20351;&#29992;ClimateBERT&#27169;&#22411;&#37327;&#21270;&#24773;&#24863;&#65292;&#20174;&#32780;&#33719;&#24471;&#26377;&#20851;&#20844;&#20247;&#23545;&#27668;&#20505;&#21464;&#21270;&#30340;&#35266;&#28857;&#21644;&#21453;&#39304;&#12290;</title><link>http://arxiv.org/abs/2310.08099</link><description>&lt;p&gt;
ClimateNLP: &#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20998;&#26512;&#20844;&#20247;&#23545;&#27668;&#20505;&#21464;&#21270;&#30340;&#24773;&#24863;&#24577;&#24230;
&lt;/p&gt;
&lt;p&gt;
ClimateNLP: Analyzing Public Sentiment Towards Climate Change Using Natural Language Processing. (arXiv:2310.08099v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#19978;&#20851;&#20110;&#27668;&#20505;&#21464;&#21270;&#30340;&#25512;&#25991;&#24773;&#24863;&#24577;&#24230;&#65292;&#36890;&#36807;&#20351;&#29992;ClimateBERT&#27169;&#22411;&#37327;&#21270;&#24773;&#24863;&#65292;&#20174;&#32780;&#33719;&#24471;&#26377;&#20851;&#20844;&#20247;&#23545;&#27668;&#20505;&#21464;&#21270;&#30340;&#35266;&#28857;&#21644;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27668;&#20505;&#21464;&#21270;&#23545;&#20154;&#31867;&#20581;&#24247;&#30340;&#24433;&#21709;&#24102;&#26469;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#25361;&#25112;&#12290;&#38500;&#38750;&#37319;&#21462;&#22522;&#20110;&#30830;&#20991;&#35777;&#25454;&#30340;&#31215;&#26497;&#25514;&#26045;&#65292;&#21542;&#21017;&#36825;&#20123;&#23041;&#32961;&#24456;&#21487;&#33021;&#20250;&#21319;&#32423;&#65292;&#24182;&#32487;&#32493;&#23041;&#32961;&#20154;&#31867;&#31119;&#31049;&#12290;&#20449;&#24687;&#21644;&#36890;&#20449;&#25216;&#26415;&#30340;&#19981;&#26029;&#21457;&#23637;&#24050;&#32463;&#20419;&#36827;&#20102;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#30340;&#24191;&#27867;&#21487;&#29992;&#24615;&#21644;&#21033;&#29992;&#29575;&#12290;&#20010;&#20154;&#21033;&#29992;Twitter&#21644;Facebook&#31561;&#24179;&#21488;&#34920;&#36798;&#33258;&#24049;&#23545;&#21508;&#31181;&#20027;&#39064;&#30340;&#24847;&#35265;&#12289;&#24819;&#27861;&#21644;&#35780;&#35770;&#65292;&#21253;&#25324;&#32039;&#36843;&#30340;&#27668;&#20505;&#21464;&#21270;&#38382;&#39064;&#12290;&#31038;&#20132;&#23186;&#20307;&#19978;&#19982;&#27668;&#20505;&#21464;&#21270;&#30456;&#20851;&#20869;&#23481;&#30340;&#28608;&#22686;&#38656;&#35201;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#20197;&#33719;&#24471;&#26377;&#24847;&#20041;&#30340;&#27934;&#23519;&#12290;&#26412;&#35770;&#25991;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#20998;&#26512;&#27668;&#20505;&#21464;&#21270;&#35805;&#35821;&#65292;&#24182;&#37327;&#21270;&#19982;&#27668;&#20505;&#21464;&#21270;&#30456;&#20851;&#30340;&#25512;&#25991;&#30340;&#24773;&#24863;&#12290;&#25105;&#20204;&#20351;&#29992;ClimateBERT&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#27668;&#20505;&#21464;&#21270;&#39046;&#22495;&#36827;&#34892;&#20102;&#24494;&#35843;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#30446;&#26631;&#26159;&#35782;&#21035;&#24773;&#24863;&#24577;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Climate change's impact on human health poses unprecedented and diverse challenges. Unless proactive measures based on solid evidence are implemented, these threats will likely escalate and continue to endanger human well-being. The escalating advancements in information and communication technologies have facilitated the widespread availability and utilization of social media platforms. Individuals utilize platforms such as Twitter and Facebook to express their opinions, thoughts, and critiques on diverse subjects, encompassing the pressing issue of climate change. The proliferation of climate change-related content on social media necessitates comprehensive analysis to glean meaningful insights. This paper employs natural language processing (NLP) techniques to analyze climate change discourse and quantify the sentiment of climate change-related tweets. We use ClimateBERT, a pretrained model fine-tuned specifically for the climate change domain. The objective is to discern the sentim
&lt;/p&gt;</description></item><item><title>KwaiYiiMath&#26159;&#19968;&#20010;&#29992;&#20110;&#22686;&#24378;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24212;&#29992;&#30417;&#30563;&#24494;&#35843;&#21644;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#33521;&#35821;&#21644;&#20013;&#25991;&#25968;&#23398;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#22815;&#27491;&#30830;&#35299;&#20915;&#29983;&#25104;&#30340;&#38382;&#39064;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2310.07488</link><description>&lt;p&gt;
KwaiYiiMath: &#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
KwaiYiiMath: Technical Report. (arXiv:2310.07488v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07488
&lt;/p&gt;
&lt;p&gt;
KwaiYiiMath&#26159;&#19968;&#20010;&#29992;&#20110;&#22686;&#24378;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24212;&#29992;&#30417;&#30563;&#24494;&#35843;&#21644;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#33521;&#35821;&#21644;&#20013;&#25991;&#25968;&#23398;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#22815;&#27491;&#30830;&#35299;&#20915;&#29983;&#25104;&#30340;&#38382;&#39064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22788;&#29702;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#19979;&#28216;&#20219;&#21153;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#65292;&#29978;&#33267;&#21487;&#20197;&#22788;&#29702;&#38656;&#35201;&#22810;&#27493;&#25512;&#29702;&#30340;&#25968;&#23398;&#20219;&#21153;&#12290;&#22312;&#26412;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;KwaiYiiMath&#65292;&#36890;&#36807;&#24212;&#29992;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#21644;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#22686;&#24378;&#20102;KwaiYiiBase1&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#21253;&#25324;&#33521;&#35821;&#21644;&#20013;&#25991;&#30340;&#25968;&#23398;&#20219;&#21153;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#26500;&#24314;&#20102;&#19968;&#20010;&#23567;&#35268;&#27169;&#30340;&#20013;&#23567;&#23398;&#25968;&#23398;&#27979;&#35797;&#38598;&#65288;&#21629;&#21517;&#20026;KMath&#65289;&#65292;&#21253;&#21547;188&#20010;&#20363;&#23376;&#65292;&#29992;&#26469;&#35780;&#20272;&#27169;&#22411;&#29983;&#25104;&#30340;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#30340;&#27491;&#30830;&#24615;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#19982;&#31867;&#20284;&#35268;&#27169;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;KwaiYiiMath&#22312;GSM8k&#12289;CMath&#21644;KMath&#19978;&#22343;&#33021;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65288;SOTA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large language models (LLMs) have demonstrated remarkable abilities in handling a variety of natural language processing (NLP) downstream tasks, even on mathematical tasks requiring multi-step reasoning. In this report, we introduce the KwaiYiiMath which enhances the mathematical reasoning abilities of KwaiYiiBase1, by applying Supervised Fine-Tuning (SFT) and Reinforced Learning from Human Feedback (RLHF), including on both English and Chinese mathematical tasks. Meanwhile, we also constructed a small-scale Chinese primary school mathematics test set (named KMath), consisting of 188 examples to evaluate the correctness of the problem-solving process generated by the models. Empirical studies demonstrate that KwaiYiiMath can achieve state-of-the-art (SOTA) performance on GSM8k, CMath, and KMath compared with the similar size models, respectively.
&lt;/p&gt;</description></item><item><title>Jaeger&#26159;&#19968;&#31181;&#22522;&#20110;&#36830;&#25509;&#30340;&#22810;&#21464;&#25442;&#22120;VQA&#27169;&#22411;&#65292;&#21033;&#29992;RoBERTa large&#21644;GPT2-xl&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#36890;&#36807;&#24182;&#34892;&#32771;&#34385;&#22810;&#28304;&#20449;&#24687;&#26469;&#22686;&#24378;&#27169;&#22411;&#34920;&#24449;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.07091</link><description>&lt;p&gt;
Jaeger:&#19968;&#31181;&#22522;&#20110;&#36830;&#25509;&#30340;&#22810;&#21464;&#25442;&#22120;VQA&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jaeger: A Concatenation-Based Multi-Transformer VQA Model. (arXiv:2310.07091v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07091
&lt;/p&gt;
&lt;p&gt;
Jaeger&#26159;&#19968;&#31181;&#22522;&#20110;&#36830;&#25509;&#30340;&#22810;&#21464;&#25442;&#22120;VQA&#27169;&#22411;&#65292;&#21033;&#29992;RoBERTa large&#21644;GPT2-xl&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#36890;&#36807;&#24182;&#34892;&#32771;&#34385;&#22810;&#28304;&#20449;&#24687;&#26469;&#22686;&#24378;&#27169;&#22411;&#34920;&#24449;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25991;&#26723;&#30340;&#35270;&#35273;&#38382;&#31572;&#22312;&#35821;&#35328;&#24847;&#20041;&#28040;&#27495;&#21644;&#32454;&#31890;&#24230;&#22810;&#27169;&#24577;&#26816;&#32034;&#20043;&#38388;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;&#30001;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#21644;&#24320;&#25918;&#19990;&#30028;&#20808;&#39564;&#27169;&#22411;&#30340;&#21033;&#29992;&#65292;&#25991;&#26723;&#38382;&#31572;&#21462;&#24471;&#20102;&#40723;&#33310;&#20154;&#24515;&#30340;&#36827;&#23637;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#65292;&#21253;&#25324;&#21709;&#24212;&#26102;&#38388;&#24310;&#38271;&#12289;&#25512;&#26029;&#25345;&#32493;&#26102;&#38388;&#24310;&#38271;&#21644;&#21305;&#37197;&#19981;&#20934;&#30830;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36830;&#25509;&#30340;&#22810;&#21464;&#25442;&#22120;VQA&#27169;&#22411;Jaegar&#12290;&#20026;&#20102;&#25552;&#21462;&#38382;&#39064;&#29305;&#24449;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;RoBERTa large&#21644;GPT2-xl&#31561;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24378;&#22823;&#33021;&#21147;&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23558;&#20004;&#31181;&#27169;&#22411;&#30340;&#36755;&#20986;&#36827;&#34892;&#36830;&#25509;&#25805;&#20316;&#12290;&#36825;&#20010;&#25805;&#20316;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#21516;&#26102;&#32771;&#34385;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#20449;&#24687;&#65292;&#22686;&#24378;&#20102;&#20854;&#34920;&#24449;&#33021;&#21147;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#21487;&#33021;&#22686;&#24378;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document-based Visual Question Answering poses a challenging task between linguistic sense disambiguation and fine-grained multimodal retrieval. Although there has been encouraging progress in document-based question answering due to the utilization of large language and open-world prior models\cite{1}, several challenges persist, including prolonged response times, extended inference durations, and imprecision in matching. In order to overcome these challenges, we propose Jaegar, a concatenation-based multi-transformer VQA model. To derive question features, we leverage the exceptional capabilities of RoBERTa large\cite{2} and GPT2-xl\cite{3} as feature extractors. Subsequently, we subject the outputs from both models to a concatenation process. This operation allows the model to consider information from diverse sources concurrently, strengthening its representational capability. By leveraging pre-trained models for feature extraction, our approach has the potential to amplify the pe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20851;&#32852;&#35789;&#24863;&#30693;&#30340;&#35789;&#32423;&#20849;&#25351;&#28040;&#35299;&#27169;&#22411;&#65288;CAW-coref&#65289;&#65292;&#22312;&#22788;&#29702;&#24182;&#21015;&#25552;&#21450;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#20102;&#36739;&#39640;&#30340;&#24615;&#33021;&#65292;&#26377;&#25928;&#22320;&#32553;&#23567;&#20102;&#19982;&#26114;&#36149;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2310.06165</link><description>&lt;p&gt;
CAW-coref: &#20851;&#32852;&#35789;&#24863;&#30693;&#30340;&#35789;&#32423;&#20849;&#25351;&#28040;&#35299;
&lt;/p&gt;
&lt;p&gt;
CAW-coref: Conjunction-Aware Word-level Coreference Resolution. (arXiv:2310.06165v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20851;&#32852;&#35789;&#24863;&#30693;&#30340;&#35789;&#32423;&#20849;&#25351;&#28040;&#35299;&#27169;&#22411;&#65288;CAW-coref&#65289;&#65292;&#22312;&#22788;&#29702;&#24182;&#21015;&#25552;&#21450;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#20102;&#36739;&#39640;&#30340;&#24615;&#33021;&#65292;&#26377;&#25928;&#22320;&#32553;&#23567;&#20102;&#19982;&#26114;&#36149;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#20849;&#25351;&#28040;&#35299;&#31995;&#32479;&#27599;&#31687;&#25991;&#31456;&#38656;&#35201;&#22810;&#27425;&#35843;&#29992;&#35821;&#35328;&#27169;&#22411;&#65292;&#22240;&#27492;&#23545;&#20110;&#35768;&#22810;&#24212;&#29992;&#22330;&#26223;&#26469;&#35828;&#65288;&#20363;&#22914;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#36827;&#34892;&#20449;&#24687;&#25552;&#21462;&#65289;&#65292;&#20195;&#20215;&#22826;&#39640;&#12290;&#32780;&#35789;&#32423;&#20849;&#25351;&#31995;&#32479; (WL-coref) &#22312;&#25928;&#29575;&#19978;&#26356;&#21152;&#39640;&#25928;&#65292;&#23454;&#29616;&#20102;&#36825;&#20123;&#26368;&#20808;&#36827;&#31995;&#32479; 96.6% &#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#21457;&#29616;&#20102; WL-coref &#30340;&#19968;&#20010;&#24120;&#35265;&#20294;&#37325;&#35201;&#30340;&#22833;&#36133;&#26696;&#20363;&#65306;&#22788;&#29702;&#8220;Tom &#21644; Mary&#8221;&#20043;&#31867;&#30340;&#24182;&#21015;&#25552;&#21450;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22312; OntoNotes &#27979;&#35797;&#38598;&#19978;&#23558;&#24615;&#33021;&#25552;&#39640;&#20102; 0.9% F1&#65292;&#23558;&#39640;&#25928;&#30340;&#35789;&#32423;&#20849;&#25351;&#28040;&#35299;&#19982;&#26114;&#36149;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#24046;&#36317;&#32553;&#23567;&#20102;34.6%&#12290;&#25105;&#20204;&#30340;&#20851;&#32852;&#35789;&#24863;&#30693;&#30340;&#35789;&#32423;&#20849;&#25351;&#27169;&#22411;&#65288;CAW-coref&#65289;&#21644;&#20195;&#30721;&#21487;&#22312; https://github.com/KarelDO/wl-coref &#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art coreference resolutions systems depend on multiple LLM calls per document and are thus prohibitively expensive for many use cases (e.g., information extraction with large corpora). The leading word-level coreference system (WL-coref) attains 96.6% of these SOTA systems' performance while being much more efficient. In this work, we identify a routine yet important failure case of WL-coref: dealing with conjoined mentions such as 'Tom and Mary'. We offer a simple yet effective solution that improves the performance on the OntoNotes test set by 0.9% F1, shrinking the gap between efficient word-level coreference resolution and expensive SOTA approaches by 34.6%. Our Conjunction-Aware Word-level coreference model (CAW-coref) and code is available at https://github.com/KarelDO/wl-coref.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;SCPRG&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;Span-Trigger-based Contextual Pooling(STCP)&#21644;Role-based Latent Information Guidance (RLIG)&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#20013;&#24573;&#30053;&#30340;&#38750;&#35770;&#35777;&#19978;&#19979;&#25991;&#32447;&#32034;&#20449;&#24687;&#20197;&#21450;&#35770;&#35777;&#35282;&#33394;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#12290;&#27169;&#22411;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#21644;&#27719;&#32858;&#19978;&#19979;&#25991;&#20013;&#30340;&#38750;&#35770;&#35777;&#32447;&#32034;&#35789;&#65292;&#20197;&#21450;&#26500;&#24314;&#28508;&#22312;&#30340;&#35282;&#33394;&#34920;&#31034;&#24182;&#25429;&#25417;&#35821;&#20041;&#30456;&#20851;&#24615;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.05991</link><description>&lt;p&gt;
&#20351;&#29992;&#19978;&#19979;&#25991;&#32447;&#32034;&#21644;&#35282;&#33394;&#30456;&#20851;&#24615;&#25552;&#21319;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Enhancing Document-level Event Argument Extraction with Contextual Clues and Role Relevance. (arXiv:2310.05991v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;SCPRG&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;Span-Trigger-based Contextual Pooling(STCP)&#21644;Role-based Latent Information Guidance (RLIG)&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#20013;&#24573;&#30053;&#30340;&#38750;&#35770;&#35777;&#19978;&#19979;&#25991;&#32447;&#32034;&#20449;&#24687;&#20197;&#21450;&#35770;&#35777;&#35282;&#33394;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#12290;&#27169;&#22411;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#21644;&#27719;&#32858;&#19978;&#19979;&#25991;&#20013;&#30340;&#38750;&#35770;&#35777;&#32447;&#32034;&#35789;&#65292;&#20197;&#21450;&#26500;&#24314;&#28508;&#22312;&#30340;&#35282;&#33394;&#34920;&#31034;&#24182;&#25429;&#25417;&#35821;&#20041;&#30456;&#20851;&#24615;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#21477;&#23376;&#32423;&#20107;&#20214;&#35770;&#35777;&#30456;&#27604;&#65292;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#38754;&#20020;&#30528;&#38271;&#36755;&#20837;&#21644;&#36328;&#21477;&#23376;&#25512;&#29702;&#30340;&#26032;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20043;&#21069;&#30340;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#25429;&#25417;&#27599;&#20010;&#20107;&#20214;&#20013;&#20505;&#36873;&#35770;&#35777;&#19982;&#20107;&#20214;&#35302;&#21457;&#22120;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24573;&#30053;&#20102;&#20004;&#20010;&#20851;&#38190;&#28857;&#65306;a&#65289;&#38750;&#35770;&#35777;&#30340;&#19978;&#19979;&#25991;&#32447;&#32034;&#20449;&#24687;&#65307;b&#65289;&#35770;&#35777;&#35282;&#33394;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;SCPRG&#65288;&#22522;&#20110;&#36328;&#24230;&#35302;&#21457;&#22120;&#30340;&#19978;&#19979;&#25991;&#27719;&#32858;&#21644;&#28508;&#22312;&#35282;&#33394;&#24341;&#23548;&#65289;&#27169;&#22411;&#65292;&#23427;&#21253;&#21547;&#20004;&#20010;&#26032;&#39062;&#32780;&#26377;&#25928;&#30340;&#27169;&#22359;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;&#22522;&#20110;&#36328;&#24230;&#35302;&#21457;&#22120;&#30340;&#19978;&#19979;&#25991;&#27719;&#32858;&#65288;STCP&#65289;&#26681;&#25454;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#27880;&#24847;&#21147;&#26435;&#37325;&#65292;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#21644;&#27719;&#32858;&#38750;&#35770;&#35777;&#32447;&#32034;&#35789;&#30340;&#20449;&#24687;&#12290;&#22522;&#20110;&#35282;&#33394;&#30340;&#28508;&#22312;&#20449;&#24687;&#24341;&#23548;&#65288;RLIG&#65289;&#27169;&#22359;&#26500;&#24314;&#28508;&#22312;&#30340;&#35282;&#33394;&#34920;&#31034;&#65292;&#36890;&#36807;&#35282;&#33394;&#20132;&#20114;&#32534;&#30721;&#20351;&#23427;&#20204;&#30456;&#20114;&#20316;&#29992;&#20197;&#25429;&#25417;&#35821;&#20041;&#30456;&#20851;&#24615;&#65292;&#24182;&#23558;&#23427;&#20204;&#21512;&#24182;&#21040;&#20505;&#36873;&#35770;&#35777;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document-level event argument extraction poses new challenges of long input and cross-sentence inference compared to its sentence-level counterpart. However, most prior works focus on capturing the relations between candidate arguments and the event trigger in each event, ignoring two crucial points: a) non-argument contextual clue information; b) the relevance among argument roles. In this paper, we propose a SCPRG (Span-trigger-based Contextual Pooling and latent Role Guidance) model, which contains two novel and effective modules for the above problem. The Span-Trigger-based Contextual Pooling(STCP) adaptively selects and aggregates the information of non-argument clue words based on the context attention weights of specific argument-trigger pairs from pre-trained model. The Role-based Latent Information Guidance (RLIG) module constructs latent role representations, makes them interact through role-interactive encoding to capture semantic relevance, and merges them into candidate ar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#24212;&#29992;&#8220;&#19987;&#23478;&#30340;&#20056;&#31215;&#8221;&#65288;PoE&#65289;&#25216;&#26415;&#26469;&#20943;&#36731;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38271;&#24230;&#20559;&#24046;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#20027;&#35201;&#30340;&#19987;&#23478;&#20851;&#27880;&#29702;&#35299;&#20154;&#31867;&#24847;&#22270;&#65292;&#32780;&#20559;&#35265;&#19987;&#23478;&#21017;&#33268;&#21147;&#20110;&#35782;&#21035;&#21644;&#25429;&#25417;&#38271;&#24230;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2310.05199</link><description>&lt;p&gt;
&#23485;&#26494;&#30340;&#22068;&#21767;&#20250;&#20351;&#33337;&#27785;&#27809;&#65306;&#20943;&#36731;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38271;&#24230;&#20559;&#24046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Loose lips sink ships: Mitigating Length Bias in Reinforcement Learning from Human Feedback. (arXiv:2310.05199v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05199
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#24212;&#29992;&#8220;&#19987;&#23478;&#30340;&#20056;&#31215;&#8221;&#65288;PoE&#65289;&#25216;&#26415;&#26469;&#20943;&#36731;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38271;&#24230;&#20559;&#24046;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#20027;&#35201;&#30340;&#19987;&#23478;&#20851;&#27880;&#29702;&#35299;&#20154;&#31867;&#24847;&#22270;&#65292;&#32780;&#20559;&#35265;&#19987;&#23478;&#21017;&#33268;&#21147;&#20110;&#35782;&#21035;&#21644;&#25429;&#25417;&#38271;&#24230;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#26159;&#37325;&#35201;&#30340;&#26725;&#26753;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#21644;&#31038;&#20250;&#20215;&#20540;&#35266;&#23545;&#40784;&#12290;&#36825;&#31181;&#23545;&#40784;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#31867;&#21453;&#39304;&#35821;&#26009;&#24211;&#26469;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#65292;&#28982;&#21518;&#29992;&#20110;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#22870;&#21169;&#27169;&#22411;&#24120;&#24120;&#20250;&#25214;&#21040;&#32469;&#36807;&#39044;&#26399;&#30446;&#26631;&#30340;&#25463;&#24452;&#65292;&#38169;&#35823;&#22320;&#20551;&#35774;&#20154;&#31867;&#26356;&#21916;&#27426;&#36739;&#38271;&#30340;&#22238;&#31572;&#12290;&#38271;&#24230;&#20559;&#24046;&#30340;&#20986;&#29616;&#24120;&#24120;&#20250;&#23548;&#33268;&#27169;&#22411;&#20542;&#21521;&#20110;&#36739;&#38271;&#30340;&#36755;&#20986;&#65292;&#20294;&#24182;&#19981;&#24847;&#21619;&#30528;&#36825;&#20123;&#36755;&#20986;&#20013;&#26377;&#26356;&#22810;&#26377;&#29992;&#30340;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24212;&#29992;&#20102;&#8220;&#19987;&#23478;&#30340;&#20056;&#31215;&#8221;&#65288;PoE&#65289;&#25216;&#26415;&#26469;&#23558;&#22870;&#21169;&#24314;&#27169;&#19982;&#24207;&#21015;&#38271;&#24230;&#30340;&#24433;&#21709;&#20998;&#31163;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#20027;&#35201;&#30340;&#19987;&#23478;&#20851;&#27880;&#29702;&#35299;&#20154;&#31867;&#24847;&#22270;&#65292;&#32780;&#20559;&#35265;&#19987;&#23478;&#21017;&#33268;&#21147;&#20110;&#35782;&#21035;&#21644;&#25429;&#25417;&#38271;&#24230;&#20559;&#24046;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;&#20559;&#35265;&#30340;&#23398;&#20064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25200;&#21160;&#36827;&#20837;&#20559;&#24046;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning from human feedback serves as a crucial bridge, aligning large language models with human and societal values. This alignment requires a vast corpus of human feedback to learn a reward model, which is subsequently used to finetune language models. However, we have identified that the reward model often finds shortcuts to bypass its intended objectives, misleadingly assuming that humans prefer longer responses. The emergence of length bias often induces the model to favor longer outputs, yet it doesn't equate to an increase in helpful information within these outputs. In this paper, we propose an innovative solution, applying the Product-of-Experts (PoE) technique to separate reward modeling from the influence of sequence length. In our framework, the main expert concentrates on understanding human intents, while the biased expert targets the identification and capture of length bias. To further enhance the learning of bias, we introduce perturbations into the bia
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#35821;&#35328;&#27169;&#22411;&#65288;RNN LMs&#65289;&#20316;&#20026;&#27010;&#29575;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#21482;&#33021;&#34920;&#31034;&#26377;&#38480;&#29366;&#24577;&#27169;&#22411;&#25152;&#33021;&#34920;&#36798;&#30340;&#27010;&#29575;&#20998;&#24067;&#30340;&#19968;&#20010;&#20005;&#26684;&#23376;&#38598;&#12290;</title><link>http://arxiv.org/abs/2310.05161</link><description>&lt;p&gt;
&#24490;&#29615;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#27010;&#29575;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;
&lt;/p&gt;
&lt;p&gt;
Recurrent Neural Language Models as Probabilistic Finite-state Automata. (arXiv:2310.05161v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05161
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#35821;&#35328;&#27169;&#22411;&#65288;RNN LMs&#65289;&#20316;&#20026;&#27010;&#29575;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#21482;&#33021;&#34920;&#31034;&#26377;&#38480;&#29366;&#24577;&#27169;&#22411;&#25152;&#33021;&#34920;&#36798;&#30340;&#27010;&#29575;&#20998;&#24067;&#30340;&#19968;&#20010;&#20005;&#26684;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20197;&#23481;&#26131;&#29702;&#35299;&#30340;&#24418;&#24335;&#26469;&#30740;&#31350;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#21487;&#20197;&#20351;&#25105;&#20204;&#31934;&#30830;&#22320;&#25551;&#36848;&#23427;&#20204;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#32771;&#23519;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#35821;&#35328;&#27169;&#22411;&#22312;&#35782;&#21035;&#26080;&#26435;&#37325;&#24418;&#24335;&#35821;&#35328;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LMs&#24182;&#19981;&#25551;&#36848;&#26080;&#26435;&#37325;&#24418;&#24335;&#35821;&#35328;&#65292;&#32780;&#26159;&#23450;&#20041;&#20102;&#23545;&#23383;&#31526;&#20018;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;RNN LMs&#21487;&#20197;&#34920;&#31034;&#21738;&#20123;&#31867;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#26356;&#30452;&#25509;&#22320;&#38472;&#36848;&#23427;&#20204;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#31616;&#21333;&#30340;RNN&#31561;&#20215;&#20110;&#27010;&#29575;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#30340;&#19968;&#20010;&#23376;&#31867;&#65292;&#22240;&#27492;&#21482;&#33021;&#27169;&#25311;&#26377;&#38480;&#29366;&#24577;&#27169;&#22411;&#25152;&#33021;&#34920;&#36798;&#30340;&#27010;&#29575;&#20998;&#24067;&#30340;&#19968;&#20010;&#20005;&#26684;&#23376;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29992;RNNs&#34920;&#31034;&#26377;&#38480;&#29366;&#24577;LMs&#30340;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#20026;&#20102;&#34920;&#31034;&#19968;&#20010;&#20219;&#24847;&#30830;&#23450;&#30340;&#26377;&#38480;&#29366;&#24577;LMs&#65292;&#20854;&#20013;&#26377;$N$&#20010;&#29366;&#24577;&#19988;&#23383;&#31526;&#38598;&#20026;$\Sigma$&#30340;RNN requir
&lt;/p&gt;
&lt;p&gt;
Studying language models (LMs) in terms of well-understood formalisms allows us to precisely characterize their abilities and limitations. Previous work has investigated the representational capacity of recurrent neural network (RNN) LMs in terms of their capacity to recognize unweighted formal languages. However, LMs do not describe unweighted formal languages -- rather, they define probability distributions over strings. In this work, we study what classes of such probability distributions RNN LMs can represent, which allows us to make more direct statements about their capabilities. We show that simple RNNs are equivalent to a subclass of probabilistic finite-state automata, and can thus model a strict subset of probability distributions expressible by finite-state models. Furthermore, we study the space complexity of representing finite-state LMs with RNNs. We show that, to represent an arbitrary deterministic finite-state LM with $N$ states over an alphabet $\Sigma$, an RNN requir
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27010;&#29575;&#26657;&#20934;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#32467;&#21512;&#26657;&#20934;&#25216;&#26415;&#21644;&#22810;&#35821;&#35328;&#32534;&#30721;&#22120;&#65292;&#35299;&#20915;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#23545;&#20110;&#39057;&#32321;&#20986;&#29616;&#30340;&#26631;&#31614;&#35789;&#39044;&#27979;&#20559;&#22909;&#30340;&#38382;&#39064;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#38646;-shot&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.05069</link><description>&lt;p&gt;
&#25581;&#31034;&#22810;&#35821;&#35328;&#32534;&#30721;&#22120;&#30340;&#28508;&#21147;&#65306;&#36890;&#36807;&#27010;&#29575;&#26657;&#20934;&#22686;&#24378;&#38646;-shot&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Multilingual Encoder Potential: Boosting Zero-Shot Performance via Probability Calibration. (arXiv:2310.05069v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05069
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27010;&#29575;&#26657;&#20934;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#32467;&#21512;&#26657;&#20934;&#25216;&#26415;&#21644;&#22810;&#35821;&#35328;&#32534;&#30721;&#22120;&#65292;&#35299;&#20915;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#23545;&#20110;&#39057;&#32321;&#20986;&#29616;&#30340;&#26631;&#31614;&#35789;&#39044;&#27979;&#20559;&#22909;&#30340;&#38382;&#39064;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#38646;-shot&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#32534;&#30721;&#22120;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#23558;&#36755;&#20837;&#31034;&#20363;&#37325;&#26032;&#36716;&#21270;&#20026;&#22635;&#31354;&#24335;&#25552;&#31034;&#65292;&#30452;&#25509;&#25191;&#34892;&#38646;-shot&#22810;&#35821;&#35328;&#20219;&#21153;&#25110;&#35821;&#35328;&#25506;&#27979;&#12290;&#36825;&#36890;&#36807;&#39044;&#27979;&#23631;&#34109;&#26631;&#35760;&#20301;&#32622;&#30340;&#26631;&#31614;&#35789;&#30340;&#27010;&#29575;&#26469;&#23436;&#25104;&#65292;&#26080;&#38656;&#23545;&#27169;&#22411;&#21442;&#25968;&#36827;&#34892;&#20219;&#20309;&#26356;&#26032;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#24615;&#33021;&#21463;&#21040;&#27169;&#22411;&#23545;&#20110;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#39057;&#32321;&#20986;&#29616;&#30340;&#26631;&#31614;&#35789;&#39044;&#27979;&#20559;&#22909;&#30340;&#38480;&#21046;&#12290;&#36825;&#20123;&#35789;&#36890;&#24120;&#33719;&#24471;&#36739;&#39640;&#30340;&#27010;&#29575;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#27169;&#22411;&#19982;&#26657;&#20934;&#25216;&#26415;&#32467;&#21512;&#65292;&#20462;&#25913;&#27169;&#22411;&#39044;&#27979;&#30340;&#26631;&#31614;&#35789;&#30340;&#27010;&#29575;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#19968;&#20010;&#25552;&#20986;&#30340;&#31616;&#21333;&#26657;&#20934;&#26041;&#27861;&#19982;&#20854;&#20182;&#24050;&#26377;&#25216;&#26415;&#22312;&#21333;&#35821;&#35328;&#32534;&#30721;&#22120;&#30340;&#38646;-shot&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#26657;&#20934;&#25216;&#26415;&#24212;&#29992;&#20110;&#22810;&#35821;&#35328;&#32534;&#30721;&#22120;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained multilingual encoder models can directly perform zero-shot multilingual tasks or linguistic probing by reformulating the input examples into cloze-style prompts. This is accomplished by predicting the probabilities of the label words at the masked token position, without requiring any updates to the model parameters. However, the performance of this method is limited by the model's bias toward predicting label words which frequently occurred during the pretraining. These words typically receive high probabilities. To address this issue, we combine the models with calibration techniques which modify the probabilities of label words predicted by the models. We first validate the effectiveness of a proposed simple calibration method together with other existing techniques on monolingual encoders in both zero- and few-shot scenarios. We subsequently employ these calibration techniques on multilingual encoders, resulting in substantial performance improvements across a wide range
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;DQ-LoRe&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#21452;&#37325;&#26597;&#35810;&#21644;&#20302;&#31209;&#36817;&#20284;&#37325;&#26032;&#25490;&#24207;&#33258;&#21160;&#36873;&#25321;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#31034;&#20363;&#65292;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#21644;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.02954</link><description>&lt;p&gt;
DQ-LoRe: &#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#20302;&#31209;&#36817;&#20284;&#21452;&#37325;&#26597;&#35810;&#37325;&#26032;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
DQ-LoRe: Dual Queries with Low Rank Approximation Re-ranking for In-Context Learning. (arXiv:2310.02954v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02954
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;DQ-LoRe&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#21452;&#37325;&#26597;&#35810;&#21644;&#20302;&#31209;&#36817;&#20284;&#37325;&#26032;&#25490;&#24207;&#33258;&#21160;&#36873;&#25321;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#31034;&#20363;&#65292;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#26032;&#36827;&#23637;&#65292;&#20027;&#35201;&#26159;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25512;&#21160;&#30340;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#24341;&#23548;LLMs&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#26159;&#21033;&#29992;&#38142;&#24335;&#24605;&#32500;&#65288;CoT&#65289;&#33539;&#24335;&#20013;&#30340;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#26368;&#20027;&#35201;&#30340;&#25361;&#25112;&#22312;&#20110;&#26377;&#25928;&#22320;&#36873;&#25321;&#31034;&#20363;&#26469;&#20419;&#36827;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#21452;&#37325;&#26597;&#35810;&#21644;&#20302;&#31209;&#36817;&#20284;&#37325;&#26032;&#25490;&#24207;&#65288;DQ-LoRe&#65289;&#26469;&#33258;&#21160;&#36873;&#25321;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#31034;&#20363;&#12290;&#21452;&#37325;&#26597;&#35810;&#39318;&#20808;&#26597;&#35810;LLM&#20197;&#33719;&#21462;LLM&#29983;&#25104;&#30340;&#30693;&#35782;&#65292;&#20363;&#22914;CoT&#65292;&#28982;&#21518;&#36890;&#36807;&#38382;&#39064;&#21644;&#30693;&#35782;&#26597;&#35810;&#26816;&#32034;&#22120;&#20197;&#33719;&#21462;&#26368;&#32456;&#30340;&#31034;&#20363;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#31532;&#20108;&#20010;&#26597;&#35810;&#65292;LoRe&#21033;&#29992;&#38477;&#32500;&#25216;&#26415;&#26469;&#25913;&#36827;&#31034;&#20363;&#36873;&#25321;&#65292;&#30830;&#20445;&#19982;&#36755;&#20837;&#38382;&#39064;&#30340;&#30693;&#35782;&#23494;&#20999;&#23545;&#40784;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;DQ-LoRe&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in natural language processing, primarily propelled by Large Language Models (LLMs), have showcased their remarkable capabilities grounded in in-context learning. A promising avenue for guiding LLMs in intricate reasoning tasks involves the utilization of intermediate reasoning steps within the Chain-of-Thought (CoT) paradigm. Nevertheless, the central challenge lies in the effective selection of exemplars for facilitating in-context learning. In this study, we introduce a framework that leverages Dual Queries and Low-rank approximation Re-ranking (DQ-LoRe) to automatically select exemplars for in-context learning. Dual Queries first query LLM to obtain LLM-generated knowledge such as CoT, then query the retriever to obtain the final exemplars via both question and the knowledge. Moreover, for the second query, LoRe employs dimensionality reduction techniques to refine exemplar selection, ensuring close alignment with the input question's knowledge. Through extensive ex
&lt;/p&gt;</description></item><item><title>OceanGPT&#26159;&#39318;&#20010;&#19987;&#20026;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#35774;&#35745;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;DoInstruct&#26694;&#26550;&#23454;&#29616;&#33258;&#21160;&#33719;&#21462;&#28023;&#27915;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#12290;&#36825;&#19968;&#27169;&#22411;&#30340;&#24341;&#20837;&#22635;&#34917;&#20102;&#28023;&#27915;&#31185;&#23398;&#39046;&#22495;&#20013;&#23545;LLM&#30340;&#38656;&#27714;&#32570;&#21475;&#65292;&#24182;&#20026;&#28023;&#27915;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#24037;&#20855;&#21644;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.02031</link><description>&lt;p&gt;
OceanGPT&#65306;&#29992;&#20110;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
OceanGPT: A Large Language Model for Ocean Science Tasks. (arXiv:2310.02031v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02031
&lt;/p&gt;
&lt;p&gt;
OceanGPT&#26159;&#39318;&#20010;&#19987;&#20026;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#35774;&#35745;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;DoInstruct&#26694;&#26550;&#23454;&#29616;&#33258;&#21160;&#33719;&#21462;&#28023;&#27915;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#12290;&#36825;&#19968;&#27169;&#22411;&#30340;&#24341;&#20837;&#22635;&#34917;&#20102;&#28023;&#27915;&#31185;&#23398;&#39046;&#22495;&#20013;&#23545;LLM&#30340;&#38656;&#27714;&#32570;&#21475;&#65292;&#24182;&#20026;&#28023;&#27915;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#24037;&#20855;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#27915;&#31185;&#23398;&#26159;&#25506;&#32034;&#20805;&#28385;&#29983;&#21629;&#21644;&#29983;&#29289;&#22810;&#26679;&#24615;&#30340;&#28023;&#27915;&#30340;&#31185;&#23398;&#65292;&#32771;&#34385;&#21040;&#28023;&#27915;&#35206;&#30422;&#20102;&#22320;&#29699;&#34920;&#38754;&#30340;70&#65285;&#20197;&#19978;&#65292;&#36825;&#19968;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#25913;&#21464;&#20102;&#31185;&#23398;&#30340;&#33539;&#24335;&#12290;&#23613;&#31649;&#22312;&#20854;&#20182;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#29616;&#26377;&#30340;LLM&#36890;&#24120;&#26080;&#27861;&#28385;&#36275;&#28023;&#27915;&#23398;&#23478;&#31561;&#39046;&#22495;&#19987;&#23478;&#30340;&#38656;&#27714;&#65292;&#21516;&#26102;&#23545;LLM&#22312;&#28023;&#27915;&#31185;&#23398;&#20013;&#30340;&#28508;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#36825;&#20854;&#20013;&#30340;&#26681;&#26412;&#21407;&#22240;&#21487;&#33021;&#26159;&#28023;&#27915;&#25968;&#25454;&#30340;&#24222;&#22823;&#32780;&#22797;&#26434;&#30340;&#24615;&#36136;&#65292;&#20197;&#21450;&#23545;&#26356;&#39640;&#30340;&#31890;&#24230;&#21644;&#20016;&#23500;&#30340;&#30693;&#35782;&#30340;&#38656;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;&#39318;&#20010;&#28023;&#27915;&#39046;&#22495;&#30340;LLM&#8212;&#8212;OceanGPT&#65292;&#35813;&#27169;&#22411;&#25797;&#38271;&#21508;&#31181;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;DoInstruct&#65292;&#29992;&#20110;&#33258;&#21160;&#33719;&#21462;&#22823;&#37327;&#30340;&#28023;&#27915;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#65292;&#23427;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#30340;&#21327;&#20316;&#29983;&#25104;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ocean science, which delves into the oceans that are reservoirs of life and biodiversity, is of great significance given that oceans cover over 70% of our planet's surface. Recently, advances in Large Language Models (LLMs) have transformed the paradigm in science. Despite the success in other domains, current LLMs often fall short in catering to the needs of domain experts like oceanographers, and the potential of LLMs for ocean science is under-explored. The intrinsic reason may be the immense and intricate nature of ocean data as well as the necessity for higher granularity and richness in knowledge. To alleviate these issues, we introduce OceanGPT, the first-ever LLM in the ocean domain, which is expert in various ocean science tasks. We propose DoInstruct, a novel framework to automatically obtain a large volume of ocean domain instruction data, which generates instructions based on multi-agent collaboration. Additionally, we construct the first oceanography benchmark, OceanBench,
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21019;&#24314;&#20803;&#35821;&#20041;&#27169;&#26495;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23545;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#38598;&#29983;&#25104;&#26032;&#30340;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#35780;&#20272;&#38598;&#12290;</title><link>http://arxiv.org/abs/2310.01448</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#20803;&#35821;&#20041;&#27169;&#26495;
&lt;/p&gt;
&lt;p&gt;
Meta Semantic Template for Evaluation of Large Language Models. (arXiv:2310.01448v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01448
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21019;&#24314;&#20803;&#35821;&#20041;&#27169;&#26495;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23545;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#38598;&#29983;&#25104;&#26032;&#30340;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#35780;&#20272;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26159;&#21542;&#30495;&#27491;&#29702;&#35299;&#35821;&#35328;&#30340;&#35821;&#20041;&#65292;&#36824;&#26159;&#20165;&#20165;&#35760;&#20303;&#35757;&#32451;&#25968;&#25454;&#65311;&#26368;&#36817;&#23545;LLM&#28508;&#22312;&#25968;&#25454;&#27745;&#26579;&#30340;&#25285;&#24551;&#24341;&#36215;&#20102;&#31038;&#21306;&#23545;LLM&#35780;&#20272;&#30740;&#31350;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MSTemp&#65292;&#19968;&#31181;&#36890;&#36807;&#21019;&#24314;&#20803;&#35821;&#20041;&#27169;&#26495;&#26469;&#35780;&#20272;LLM&#23545;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;MSTemp&#30340;&#26680;&#24515;&#19981;&#26159;&#30452;&#25509;&#22312;&#29616;&#26377;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#32780;&#26159;&#20351;&#29992;&#29616;&#26377;&#25968;&#25454;&#38598;&#20316;&#20026;&#31181;&#23376;&#29983;&#25104;&#26032;&#30340;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#35780;&#20272;&#38598;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#32473;&#23450;&#30340;&#21477;&#23376;&#65292;MSTemp&#21033;&#29992;&#21478;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26032;&#26679;&#26412;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#35821;&#20041;&#12290;&#36825;&#20123;&#26032;&#26679;&#26412;&#34987;&#31216;&#20026;&#21407;&#21477;&#23376;&#30340;&#35821;&#20041;&#27169;&#26495;&#12290;&#28982;&#21518;&#65292;MSTemp&#36890;&#36807;&#21477;&#23376;&#35299;&#26512;&#21644;&#38543;&#26426;&#26367;&#25442;&#35789;&#35821;&#26469;&#29983;&#25104;&#35780;&#20272;&#26679;&#26412;&#12290;MSTemp&#20855;&#26377;&#39640;&#24230;&#28789;&#27963;&#12289;&#21160;&#24577;&#21644;&#25104;&#26412;&#25928;&#30410;&#24615;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;MSTemp-
&lt;/p&gt;
&lt;p&gt;
Do large language models (LLMs) genuinely understand the semantics of the language, or just memorize the training data? The recent concern on potential data contamination of LLMs has raised awareness of the community to conduct research on LLMs evaluation. In this paper, we propose MSTemp, an approach that creates meta semantic templates to evaluate the semantic understanding ability of LLMs. The core of MSTemp is not to perform evaluation directly on existing benchmark datasets, but to generate new out-of-distribution (OOD) evaluation sets using existing datasets as seeds. Specifically, for a given sentence, MSTemp leverages another language model to generate new samples while preserving its semantics. The new samples are called semantic templates to the original sentence. Then, MSTemp generates evaluation samples via sentence parsing and random word replacement on the semantic templates. MSTemp is highly flexible, dynamic, and cost-effective. Our initial experiments show that MSTemp-
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#37492;&#21035;-&#25209;&#21028;&#24046;&#36317;&#27979;&#37327;LLMs&#23545;&#20154;&#31867;&#20215;&#20540;&#30340;&#29702;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20215;&#20540;&#29702;&#35299;&#27979;&#37327;&#65288;VUM&#65289;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;GPT-4&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;&#19968;&#21315;&#20010;&#23545;&#35805;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#23610;&#24230;&#23450;&#24459;&#23545;LLMs&#30340;&#8220;&#30693;&#36947;&#20160;&#20040;&#8221;&#26377;&#36739;&#22823;&#24433;&#21709;&#65292;&#32780;&#23545;&#8220;&#30693;&#36947;&#20026;&#20160;&#20040;&#8221;&#24433;&#21709;&#36739;&#23567;&#12290;</title><link>http://arxiv.org/abs/2310.00378</link><description>&lt;p&gt;
&#36890;&#36807;&#37492;&#21035;-&#25209;&#21028;&#24046;&#36317;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#23545;&#20215;&#20540;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Measuring Value Understanding in Language Models through Discriminator-Critique Gap. (arXiv:2310.00378v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00378
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37492;&#21035;-&#25209;&#21028;&#24046;&#36317;&#27979;&#37327;LLMs&#23545;&#20154;&#31867;&#20215;&#20540;&#30340;&#29702;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20215;&#20540;&#29702;&#35299;&#27979;&#37327;&#65288;VUM&#65289;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;GPT-4&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;&#19968;&#21315;&#20010;&#23545;&#35805;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#23610;&#24230;&#23450;&#24459;&#23545;LLMs&#30340;&#8220;&#30693;&#36947;&#20160;&#20040;&#8221;&#26377;&#36739;&#22823;&#24433;&#21709;&#65292;&#32780;&#23545;&#8220;&#30693;&#36947;&#20026;&#20160;&#20040;&#8221;&#24433;&#21709;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#24341;&#21457;&#20102;&#23545;&#23427;&#20204;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20043;&#38388;&#28508;&#22312;&#19981;&#19968;&#33268;&#24615;&#30340;&#25285;&#24551;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#30340;&#22797;&#26434;&#21644;&#36866;&#24212;&#24615;&#65292;&#35780;&#20272;&#23427;&#20204;&#23545;&#36825;&#20123;&#20215;&#20540;&#35266;&#30340;&#29702;&#35299;&#26159;&#22797;&#26434;&#30340;&#12290;&#25105;&#20204;&#35748;&#20026;&#30495;&#27491;&#29702;&#35299;LLMs&#20013;&#30340;&#20215;&#20540;&#35266;&#38656;&#35201;&#32771;&#34385;&#21040;&#8220;&#30693;&#36947;&#20160;&#20040;&#8221;&#21644;&#8220;&#30693;&#36947;&#20026;&#20160;&#20040;&#8221;&#20004;&#20010;&#26041;&#38754;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20215;&#20540;&#29702;&#35299;&#27979;&#37327;&#65288;VUM&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#37327;&#21270;&#37492;&#21035;-&#25209;&#21028;&#24046;&#36317;&#26469;&#23450;&#37327;&#35780;&#20272;&#8220;&#30693;&#36947;&#20160;&#20040;&#8221;&#21644;&#8220;&#30693;&#36947;&#20026;&#20160;&#20040;&#8221;&#12290;&#21033;&#29992;&#26045;&#29926;&#33576;&#20215;&#20540;&#35266;&#35843;&#26597;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#35780;&#20272;&#20215;&#20540;&#35266;&#30340;&#26631;&#20934;&#65292;&#24182;&#20351;&#29992;GPT-4&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;&#19968;&#21315;&#20010;&#23545;&#35805;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32771;&#23519;&#20102;LLMs&#30340;&#36755;&#20986;&#19982;&#22522;&#20934;&#31572;&#26696;&#20043;&#38388;&#30340;&#20215;&#20540;&#35266;&#19968;&#33268;&#24615;&#65292;&#20197;&#21450;LLMs&#30340;&#22238;&#31572;&#19982;GPT-4&#30340;&#27880;&#37322;&#22312;&#20215;&#20540;&#35748;&#30693;&#21407;&#22240;&#19978;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20116;&#20010;&#20195;&#34920;&#24615;LLMs&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#35777;&#25454;&#34920;&#26126;&#65292;&#23610;&#24230;&#23450;&#24459;&#23545;&#8220;&#30693;&#36947;&#20160;&#20040;&#8221;&#30340;&#24433;&#21709;&#36739;&#22823;&#65292;&#20294;&#23545;&#8220;&#30693;&#36947;&#20026;&#20160;&#20040;&#8221;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Large Language Models (LLMs) have heightened concerns about their potential misalignment with human values. However, evaluating their grasp of these values is complex due to their intricate and adaptable nature. We argue that truly understanding values in LLMs requires considering both "know what" and "know why". To this end, we present the Value Understanding Measurement (VUM) framework that quantitatively assess both "know what" and "know why" by measuring the discriminator-critique gap related to human values. Using the Schwartz Value Survey, we specify our evaluation values and develop a thousand-level dialogue dataset with GPT-4. Our assessment looks at both the value alignment of LLM's outputs compared to baseline answers and how LLM responses align with reasons for value recognition versus GPT-4's annotations. We evaluate five representative LLMs and provide strong evidence that the scaling law significantly impacts "know what" but not much on "know why", 
&lt;/p&gt;</description></item><item><title>NLPBench&#26159;&#19968;&#20010;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;NLP&#38382;&#39064;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20026;&#22635;&#34917;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#20316;&#32773;&#25910;&#38598;&#20102;&#26469;&#33258;&#32822;&#40065;&#22823;&#23398;&#26399;&#26411;&#32771;&#35797;&#30340;378&#20010;&#28085;&#30422;&#22810;&#20010;NLP&#20027;&#39064;&#30340;&#38382;&#39064;&#12290;&#35813;&#30740;&#31350;&#21457;&#29616;&#22312;&#20351;&#29992;&#39640;&#32423;&#25552;&#31034;&#31574;&#30053;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#21487;&#33021;&#19981;&#31283;&#23450;&#65292;&#24182;&#21487;&#33021;&#23545;&#36739;&#23567;&#30340;&#27169;&#22411;&#36896;&#25104;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.15630</link><description>&lt;p&gt;
NLPBench&#65306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;NLP&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
NLPBench: Evaluating Large Language Models on Solving NLP Problems. (arXiv:2309.15630v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15630
&lt;/p&gt;
&lt;p&gt;
NLPBench&#26159;&#19968;&#20010;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;NLP&#38382;&#39064;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20026;&#22635;&#34917;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#20316;&#32773;&#25910;&#38598;&#20102;&#26469;&#33258;&#32822;&#40065;&#22823;&#23398;&#26399;&#26411;&#32771;&#35797;&#30340;378&#20010;&#28085;&#30422;&#22810;&#20010;NLP&#20027;&#39064;&#30340;&#38382;&#39064;&#12290;&#35813;&#30740;&#31350;&#21457;&#29616;&#22312;&#20351;&#29992;&#39640;&#32423;&#25552;&#31034;&#31574;&#30053;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#21487;&#33021;&#19981;&#31283;&#23450;&#65292;&#24182;&#21487;&#33021;&#23545;&#36739;&#23567;&#30340;&#27169;&#22411;&#36896;&#25104;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#21457;&#23637;&#26174;&#31034;&#20986;&#22686;&#24378;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#19968;&#20123;&#25104;&#21151;&#65292;&#20294;&#22312;LLMs&#30340;NLP&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#26041;&#38754;&#20173;&#28982;&#32570;&#20047;&#19987;&#38376;&#30340;&#30740;&#31350;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#39046;&#22495;&#30340;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;NLPBench&#65292;&#21253;&#25324;378&#20010;&#28085;&#30422;&#21508;&#31181;NLP&#20027;&#39064;&#30340;&#22823;&#23398;&#27700;&#24179;NLP&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#28304;&#33258;&#32822;&#40065;&#22823;&#23398;&#20197;&#21069;&#30340;&#26399;&#26411;&#32771;&#35797;&#12290;NLPBench&#21253;&#25324;&#20855;&#26377;&#19978;&#19979;&#25991;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#22810;&#20010;&#23376;&#38382;&#39064;&#20998;&#20139;&#30456;&#21516;&#30340;&#20844;&#20849;&#20449;&#24687;&#65292;&#24182;&#19988;&#21253;&#25324;&#22810;&#36873;&#39064;&#12289;&#31616;&#31572;&#39064;&#21644;&#25968;&#23398;&#39064;&#31561;&#22810;&#31181;&#38382;&#39064;&#31867;&#22411;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#20197;GPT-3.5/4&#12289;PaLM-2&#21644;LLAMA-2&#31561;LLMs&#20026;&#20013;&#24515;&#65292;&#37319;&#29992;&#20102;&#35832;&#22914;&#38142;&#24335;&#24605;&#32500;&#65288;CoT&#65289;&#21644;&#24605;&#32500;&#26641;&#65288;ToT&#65289;&#31561;&#39640;&#32423;&#25552;&#31034;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#39640;&#32423;&#25552;&#31034;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#21487;&#33021;&#19981;&#19968;&#33268;&#65292;&#26377;&#26102;&#20250;&#25439;&#23475;LLMs&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#36739;&#23567;&#30340;&#27169;&#22411;&#65288;LLA&#65289;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent developments in large language models (LLMs) have shown promise in enhancing the capabilities of natural language processing (NLP). Despite these successes, there remains a dearth of research dedicated to the NLP problem-solving abilities of LLMs. To fill the gap in this area, we present a unique benchmarking dataset, NLPBench, comprising 378 college-level NLP questions spanning various NLP topics sourced from Yale University's prior final exams. NLPBench includes questions with context, in which multiple sub-questions share the same public information, and diverse question types, including multiple choice, short answer, and math. Our evaluation, centered on LLMs such as GPT-3.5/4, PaLM-2, and LLAMA-2, incorporates advanced prompting strategies like the chain-of-thought (CoT) and tree-of-thought (ToT). Our study reveals that the effectiveness of the advanced prompting strategies can be inconsistent, occasionally damaging LLM performance, especially in smaller models like the LLA
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AnglE&#30340;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#26469;&#32531;&#35299;&#25991;&#26412;&#23884;&#20837;&#20013;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#36896;&#25104;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;STS&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#65292;&#24182;&#22312;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#20013;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12871</link><description>&lt;p&gt;
&#35282;&#24230;&#20248;&#21270;&#30340;&#25991;&#26412;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
AnglE-Optimized Text Embeddings. (arXiv:2309.12871v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AnglE&#30340;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#26469;&#32531;&#35299;&#25991;&#26412;&#23884;&#20837;&#20013;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#36896;&#25104;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;STS&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#65292;&#24182;&#22312;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#20013;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#23545;&#20110;&#25552;&#21319;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#65288;STS&#65289;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#36825;&#20123;&#20219;&#21153;&#21448;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#38754;&#20020;&#30340;&#19968;&#20010;&#26222;&#36941;&#25361;&#25112;&#26159;&#28176;&#21464;&#28040;&#22833;&#38382;&#39064;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#23427;&#20204;&#22312;&#20248;&#21270;&#30446;&#26631;&#20013;&#20381;&#36182;&#20313;&#24358;&#20989;&#25968;&#65292;&#32780;&#20313;&#24358;&#20989;&#25968;&#20855;&#26377;&#39281;&#21644;&#21306;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;AnglE&#30340;&#26032;&#22411;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#12290;AnglE&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#22312;&#19968;&#20010;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#20135;&#29983;&#30340;&#19981;&#21033;&#24433;&#21709;&#65292;&#20174;&#32780;&#21487;&#20197;&#38459;&#30861;&#26799;&#24230;&#24182;&#38459;&#30861;&#20248;&#21270;&#36807;&#31243;&#12290;&#20026;&#20102;&#24314;&#31435;&#20840;&#38754;&#30340;STS&#35780;&#20272;&#65292;&#25105;&#20204;&#22312;&#29616;&#26377;&#30340;&#30701;&#25991;&#26412;STS&#25968;&#25454;&#38598;&#21644;&#20174;GitHub Issues&#20013;&#26032;&#25910;&#38598;&#30340;&#38271;&#25991;&#26412;STS&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20855;&#26377;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#65292;&#24182;&#25506;&#35752;&#20102;AnglE&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-quality text embedding is pivotal in improving semantic textual similarity (STS) tasks, which are crucial components in Large Language Model (LLM) applications. However, a common challenge existing text embedding models face is the problem of vanishing gradients, primarily due to their reliance on the cosine function in the optimization objective, which has saturation zones. To address this issue, this paper proposes a novel angle-optimized text embedding model called AnglE. The core idea of AnglE is to introduce angle optimization in a complex space. This novel approach effectively mitigates the adverse effects of the saturation zone in the cosine function, which can impede gradient and hinder optimization processes. To set up a comprehensive STS evaluation, we experimented on existing short-text STS datasets and a newly collected long-text STS dataset from GitHub Issues. Furthermore, we examine domain-specific STS scenarios with limited labeled data and explore how AnglE works w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#25105;&#24378;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#33258;&#21160;&#29983;&#25104;&#21644;&#35780;&#20272;&#23398;&#29983;&#29983;&#25104;&#30340;&#35299;&#37322;&#65292;&#29992;&#20110;&#25913;&#36827;&#23398;&#29983;&#36164;&#28304;&#20849;&#20139;&#20013;&#23398;&#29983;&#29983;&#25104;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#35299;&#37322;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.10444</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#32034;&#33258;&#25105;&#24378;&#21270;&#20197;&#25913;&#36827;&#23398;&#29983;&#29983;&#25104;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Exploring Self-Reinforcement for Improving Learnersourced Multiple-Choice Question Explanations with Large Language Models. (arXiv:2309.10444v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#25105;&#24378;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#33258;&#21160;&#29983;&#25104;&#21644;&#35780;&#20272;&#23398;&#29983;&#29983;&#25104;&#30340;&#35299;&#37322;&#65292;&#29992;&#20110;&#25913;&#36827;&#23398;&#29983;&#36164;&#28304;&#20849;&#20139;&#20013;&#23398;&#29983;&#29983;&#25104;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#35299;&#37322;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#29983;&#36164;&#28304;&#20849;&#20139;&#28041;&#21450;&#23398;&#29983;&#29983;&#25104;&#21644;&#20998;&#20139;&#23398;&#20064;&#36164;&#28304;&#12290;&#22312;&#23398;&#29983;&#29983;&#25104;&#22810;&#39033;&#36873;&#25321;&#39064;&#26102;&#65292;&#21019;&#24314;&#35299;&#37322;&#26159;&#19968;&#20010;&#20851;&#38190;&#27493;&#39588;&#65292;&#22240;&#20026;&#23427;&#26377;&#21161;&#20110;&#23545;&#30456;&#20851;&#27010;&#24565;&#30340;&#28145;&#20837;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#23398;&#29983;&#24448;&#24448;&#30001;&#20110;&#20027;&#39064;&#29702;&#35299;&#26377;&#38480;&#21644;&#20165;&#20165;&#37325;&#30003;&#38382;&#39064;&#12289;&#24178;&#25200;&#22240;&#32032;&#21644;&#27491;&#30830;&#31572;&#26696;&#30340;&#20542;&#21521;&#32780;&#38590;&#20197;&#32534;&#20889;&#26377;&#25928;&#30340;&#35299;&#37322;&#12290;&#20026;&#20102;&#24110;&#21161;&#25903;&#25745;&#36825;&#20010;&#20219;&#21153;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#25105;&#24378;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#26088;&#22312;&#33258;&#21160;&#29983;&#25104;&#21644;&#35780;&#20272;&#35299;&#37322;&#12290;&#35813;&#26694;&#26550;&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65292;&#29983;&#25104;&#19982;&#23398;&#29983;&#23545;&#40784;&#30340;&#35299;&#37322;&#65292;&#35780;&#20272;&#36825;&#20123;&#35299;&#37322;&#20197;&#30830;&#20445;&#20854;&#36136;&#37327;&#65292;&#24182;&#36845;&#20195;&#22686;&#24378;&#35299;&#37322;&#12290;&#22914;&#26524;&#19968;&#20010;&#35299;&#37322;&#30340;&#35780;&#20272;&#20998;&#25968;&#20302;&#20110;&#23450;&#20041;&#30340;&#38408;&#20540;&#65292;&#26694;&#26550;&#20250;&#36845;&#20195;&#22320;&#20248;&#21270;&#21644;&#37325;&#26032;&#35780;&#20272;&#35299;&#37322;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#27169;&#25311;&#20102;&#19968;&#20010;&#23398;&#29983;&#23398;&#20064;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learnersourcing involves students generating and sharing learning resources with their peers. When learnersourcing multiple-choice questions, creating explanations for the generated questions is a crucial step as it facilitates a deeper understanding of the related concepts. However, it is often difficult for students to craft effective explanations due to limited subject understanding and a tendency to merely restate the question stem, distractors, and correct answer. To help scaffold this task, in this work we propose a self-reinforcement large-language-model framework, with the goal of generating and evaluating explanations automatically. Comprising three modules, the framework generates student-aligned explanations, evaluates these explanations to ensure their quality and iteratively enhances the explanations. If an explanation's evaluation score falls below a defined threshold, the framework iteratively refines and reassesses the explanation. Importantly, our framework emulates th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;CONFLATOR&#65306;&#19968;&#31181;&#38024;&#23545;&#20195;&#30721;&#28151;&#21512;&#35821;&#35328;&#30340;&#31070;&#32463;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26059;&#36716;&#20301;&#32622;&#32534;&#30721;&#21644;&#20999;&#25442;&#28857;&#20449;&#24687;&#65292;&#22312;&#28151;&#21512;&#35821;&#35328;&#24314;&#27169;&#20013;&#21462;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.05270</link><description>&lt;p&gt;
CONFLATOR:&#23558;&#22522;&#20110;&#20999;&#25442;&#28857;&#30340;&#26059;&#36716;&#20301;&#32622;&#32534;&#30721;&#32435;&#20837;&#28151;&#21512;&#35821;&#35328;&#24314;&#27169;&#20013;
&lt;/p&gt;
&lt;p&gt;
CONFLATOR: Incorporating Switching Point based Rotatory Positional Encodings for Code-Mixed Language Modeling. (arXiv:2309.05270v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05270
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;CONFLATOR&#65306;&#19968;&#31181;&#38024;&#23545;&#20195;&#30721;&#28151;&#21512;&#35821;&#35328;&#30340;&#31070;&#32463;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26059;&#36716;&#20301;&#32622;&#32534;&#30721;&#21644;&#20999;&#25442;&#28857;&#20449;&#24687;&#65292;&#22312;&#28151;&#21512;&#35821;&#35328;&#24314;&#27169;&#20013;&#21462;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20004;&#31181;&#25110;&#22810;&#31181;&#35821;&#35328;&#30340;&#28151;&#21512;&#31216;&#20026;&#20195;&#30721;&#28151;&#21512;&#65288;CM&#65289;&#12290; CM&#26159;&#22810;&#35821;&#35328;&#31038;&#20250;&#30340;&#31038;&#20250;&#35268;&#33539;&#12290;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#65288;NLMs&#65289;&#65288;&#22914;&#21464;&#21387;&#22120;&#65289;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;CM&#30340;NLM&#26159;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#39046;&#22495;&#12290;&#23613;&#31649;&#21464;&#21387;&#22120;&#20855;&#26377;&#33021;&#21147;&#65292;&#20294;&#30001;&#20110;&#23427;&#20204;&#26159;&#38750;&#36882;&#24402;&#30340;&#65292;&#23427;&#20204;&#19981;&#33021;&#22987;&#32456;&#32534;&#30721;&#20301;&#32622;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#20016;&#23500;&#35789;&#30340;&#20449;&#24687;&#24182;&#32435;&#20837;&#20301;&#32622;&#20449;&#24687;&#65292;&#23450;&#20041;&#20102;&#20301;&#32622;&#32534;&#30721;&#12290;&#25105;&#20204;&#20551;&#35774;&#36716;&#25442;&#28857;&#65288;SPs&#65289;&#65292;&#21363;&#35821;&#35328;&#20999;&#25442;&#30340;&#25991;&#26412;&#20013;&#30340;&#20132;&#27719;&#28857;&#65288;L1-&gt; L2&#25110;L2-&gt; L1&#65289;&#65292;&#23545;CM&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#26500;&#25104;&#25361;&#25112;&#65292;&#24182;&#23545;&#24314;&#27169;&#36807;&#31243;&#20013;SPs&#32473;&#20104;&#29305;&#21035;&#37325;&#35270;&#12290;&#25105;&#20204;&#23581;&#35797;&#20102;&#20960;&#31181;&#20301;&#32622;&#32534;&#30721;&#26426;&#21046;&#65292;&#24182;&#34920;&#26126;&#26059;&#36716;&#20301;&#32622;&#32534;&#30721;&#20197;&#21450;&#20999;&#25442;&#28857;&#20449;&#24687;&#21487;&#20197;&#33719;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;&#25105;&#20204;&#24341;&#20837;CONFLATOR&#65306;&#19968;&#31181;&#38024;&#23545;&#28151;&#21512;&#35821;&#35328;&#30340;&#31070;&#32463;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The mixing of two or more languages is called Code-Mixing (CM). CM is a social norm in multilingual societies. Neural Language Models (NLMs) like transformers have been effective on many NLP tasks. However, NLM for CM is an under-explored area. Though transformers are capable and powerful, they cannot always encode positional information since they are non-recurrent. Therefore, to enrich word information and incorporate positional information, positional encoding is defined. We hypothesize that Switching Points (SPs), i.e., junctions in the text where the language switches (L1 -&gt; L2 or L2 -&gt; L1), pose a challenge for CM Language Models (LMs), and hence give special emphasis to SPs in the modeling process. We experiment with several positional encoding mechanisms and show that rotatory positional encodings along with switching point information yield the best results.  We introduce CONFLATOR: a neural language modeling approach for code-mixed languages. CONFLATOR tries to learn to empha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;Transformer&#23398;&#20064;&#36731;&#24230;&#19978;&#19979;&#25991;&#25935;&#24863;&#35821;&#35328;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#20854;&#22312;&#27867;&#21270;&#21040;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#22806;&#25512;&#21040;&#36739;&#38271;&#23383;&#31526;&#20018;&#19978;&#30340;&#33021;&#21147;&#19981;&#22914;LSTMs&#12290;&#20998;&#26512;&#32467;&#26524;&#26174;&#31034;&#65292;Transformer&#23398;&#20064;&#21040;&#30340;&#33258;&#27880;&#24847;&#21147;&#27169;&#24335;&#21644;&#34920;&#31034;&#33021;&#22815;&#25429;&#25417;&#20381;&#36182;&#20851;&#31995;&#24182;&#34920;&#29616;&#20986;&#35745;&#25968;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2309.00857</link><description>&lt;p&gt;
&#35780;&#20272;Transformer&#23398;&#20064;&#36731;&#24230;&#19978;&#19979;&#25991;&#25935;&#24863;&#35821;&#35328;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating Transformer's Ability to Learn Mildly Context-Sensitive Languages. (arXiv:2309.00857v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;Transformer&#23398;&#20064;&#36731;&#24230;&#19978;&#19979;&#25991;&#25935;&#24863;&#35821;&#35328;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#20854;&#22312;&#27867;&#21270;&#21040;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#22806;&#25512;&#21040;&#36739;&#38271;&#23383;&#31526;&#20018;&#19978;&#30340;&#33021;&#21147;&#19981;&#22914;LSTMs&#12290;&#20998;&#26512;&#32467;&#26524;&#26174;&#31034;&#65292;Transformer&#23398;&#20064;&#21040;&#30340;&#33258;&#27880;&#24847;&#21147;&#27169;&#24335;&#21644;&#34920;&#31034;&#33021;&#22815;&#25429;&#25417;&#20381;&#36182;&#20851;&#31995;&#24182;&#34920;&#29616;&#20986;&#35745;&#25968;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;Transformer&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#33258;&#27880;&#24847;&#21147;&#22312;&#23398;&#20064;&#19968;&#20123;&#35268;&#21017;&#21644;&#26080;&#19978;&#19979;&#25991;&#35821;&#35328;&#26041;&#38754;&#22312;&#29702;&#35770;&#19978;&#23384;&#22312;&#38480;&#21046;&#12290;&#36825;&#20123;&#21457;&#29616;&#28608;&#21457;&#25105;&#20204;&#24605;&#32771;&#23427;&#20204;&#22312;&#24314;&#27169;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#24433;&#21709;&#65292;&#33258;&#28982;&#35821;&#35328;&#34987;&#20551;&#35774;&#20026;&#36731;&#24230;&#19978;&#19979;&#25991;&#25935;&#24863;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;Transformer&#23398;&#20064;&#22810;&#31181;&#22797;&#26434;&#31243;&#24230;&#30340;&#36731;&#24230;&#19978;&#19979;&#25991;&#25935;&#24863;&#35821;&#35328;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#22312;&#26410;&#35265;&#36807;&#30340;&#20998;&#24067;&#25968;&#25454;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#20854;&#23545;&#20110;&#26356;&#38271;&#23383;&#31526;&#20018;&#30340;&#22806;&#25512;&#33021;&#21147;&#20302;&#20110;LSTMs&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#26174;&#31034;&#65292;&#23398;&#20064;&#21040;&#30340;&#33258;&#27880;&#24847;&#21147;&#27169;&#24335;&#21644;&#34920;&#31034;&#27169;&#22411;&#21270;&#20102;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#23637;&#31034;&#20102;&#35745;&#25968;&#34892;&#20026;&#65292;&#36825;&#21487;&#33021;&#26377;&#21161;&#20110;&#27169;&#22411;&#35299;&#20915;&#36825;&#20123;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the fact that Transformers perform well in NLP tasks, recent studies suggest that self-attention is theoretically limited in learning even some regular and context-free languages. These findings motivated us to think about their implications in modeling natural language, which is hypothesized to be mildly context-sensitive. We test the Transformer's ability to learn mildly context-sensitive languages of varying complexities, and find that they generalize well to unseen in-distribution data, but their ability to extrapolate to longer strings is worse than that of LSTMs. Our analyses show that the learned self-attention patterns and representations modeled dependency relations and demonstrated counting behavior, which may have helped the models solve the languages.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#20026;&#25918;&#23556;&#23398;&#26500;&#24314;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#21307;&#23398;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#21644;&#25903;&#25345;&#19981;&#21516;&#25918;&#23556;&#23398;&#20219;&#21153;&#30340;&#26550;&#26500;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2308.02463</link><description>&lt;p&gt;
&#20026;&#25918;&#23556;&#23398;&#26500;&#24314;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Towards Generalist Foundation Model for Radiology. (arXiv:2308.02463v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#20026;&#25918;&#23556;&#23398;&#26500;&#24314;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#21307;&#23398;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#21644;&#25903;&#25345;&#19981;&#21516;&#25918;&#23556;&#23398;&#20219;&#21153;&#30340;&#26550;&#26500;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#21551;&#21160;&#25918;&#23556;&#23398;&#22522;&#30784;&#27169;&#22411;&#30340;&#24320;&#21457;&#65292;&#31216;&#20026;RadFM&#12290;&#25105;&#20204;&#20174;&#25968;&#25454;&#12289;&#27169;&#22411;&#35774;&#35745;&#21644;&#35780;&#20272;&#30340;&#35282;&#24230;&#20840;&#38754;&#32771;&#34385;&#20102;&#22522;&#30784;&#27169;&#22411;&#30340;&#26500;&#24314;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#21487;&#24635;&#32467;&#22914;&#19979;&#65306;&#65288;i&#65289;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#21307;&#23398;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;MedMD&#65292;&#21253;&#25324;1600&#19975;&#20010;2D&#21644;3D&#21307;&#23398;&#25195;&#25551;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21253;&#21547;3D&#21307;&#23398;&#25195;&#25551;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#12290;&#65288;ii&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26550;&#26500;&#65292;&#20351;&#24471;&#21487;&#35270;&#26465;&#20214;&#29983;&#25104;&#39044;&#35757;&#32451;&#25104;&#20026;&#21487;&#33021;&#65292;&#21487;&#20197;&#23558;&#25991;&#26412;&#36755;&#20837;&#19982;2D&#25110;3D&#21307;&#23398;&#25195;&#25551;&#20132;&#38169;&#65292;&#29983;&#25104;&#19981;&#21516;&#25918;&#23556;&#23398;&#20219;&#21153;&#30340;&#21709;&#24212;&#12290;&#35813;&#27169;&#22411;&#39318;&#20808;&#22312;MedMD&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;RadMD&#19978;&#36827;&#34892;&#20102;&#29305;&#23450;&#39046;&#22495;&#30340;&#24494;&#35843;&#65292;RadMD&#26159;MedMD&#30340;&#25918;&#23556;&#23398;&#28165;&#29702;&#29256;&#26412;&#65292;&#21253;&#21547;300&#19975;&#20010;&#25918;&#23556;&#23398;&#30340;&#35270;&#35273;&#35821;&#35328;&#23545;&#12290;&#65288;iii&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#21253;&#25324;&#20116;&#20010;&#20219;&#21153;&#65292;&#26088;&#22312;&#20840;&#38754;&#35780;&#20272;&#35813;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we aim to initiate the development of Radiology Foundation Model, termed as RadFM.We consider the construction of foundational models from the perspectives of data, model design, and evaluation thoroughly. Our contribution can be concluded as follows: (i), we construct a large-scale Medical Multi-modal Dataset, MedMD, consisting of 16M 2D and 3D medical scans. To the best of our knowledge, this is the first multi-modal dataset containing 3D medical scans. (ii), We propose an architecture that enables visually conditioned generative pre-training, allowing for the integration of text input interleaved with 2D or 3D medical scans to generate response for diverse radiologic tasks. The model was initially pre-trained on MedMD and subsequently domain-specific fine-tuned on RadMD, a radiologic cleaned version of MedMD, containing 3M radiologic visual-language pairs. (iii), we propose a new evaluation benchmark that comprises five tasks, aiming to comprehensively assess the capa
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#26088;&#22312;&#25552;&#20379;&#23545;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#22312;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#21644;&#25991;&#26412;&#20013;&#36827;&#34892;&#32852;&#21512;&#23398;&#20064;&#30340;&#24403;&#21069;&#29366;&#20917;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#24182;&#25506;&#32034;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2307.07362</link><description>&lt;p&gt;
&#19968;&#31687;&#20851;&#20110;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#22312;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#21644;&#25991;&#26412;&#20013;&#30340;&#25195;&#25551;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A scoping review on multimodal deep learning in biomedical images and texts. (arXiv:2307.07362v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07362
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#26088;&#22312;&#25552;&#20379;&#23545;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#22312;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#21644;&#25991;&#26412;&#20013;&#36827;&#34892;&#32852;&#21512;&#23398;&#20064;&#30340;&#24403;&#21069;&#29366;&#20917;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#24182;&#25506;&#32034;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26410;&#26469;&#30340;&#35745;&#31639;&#36741;&#21161;&#35786;&#26029;&#21644;&#39044;&#21518;&#31995;&#32479;&#24212;&#35813;&#33021;&#22815;&#21516;&#26102;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#12290;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#65288;MDL&#65289;&#28041;&#21450;&#22810;&#31181;&#25968;&#25454;&#28304;&#65288;&#22914;&#22270;&#20687;&#21644;&#25991;&#26412;&#65289;&#30340;&#25972;&#21512;&#65292;&#26377;&#28508;&#21147;&#24443;&#24213;&#25913;&#21464;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#30340;&#20998;&#26512;&#21644;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#39046;&#22495;&#30452;&#21040;&#26368;&#36817;&#25165;&#24341;&#36215;&#30740;&#31350;&#20154;&#21592;&#30340;&#27880;&#24847;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#23545;&#36825;&#20010;&#20027;&#39064;&#36827;&#34892;&#31995;&#32479;&#32508;&#36848;&#65292;&#30830;&#23450;&#24403;&#21069;&#24037;&#20316;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25506;&#32034;&#26410;&#26469;&#30340;&#26041;&#21521;&#12290;&#22312;&#36825;&#31687;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#23545;&#35813;&#39046;&#22495; current state &#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#32852;&#21512;&#23398;&#20064;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#36825;&#20004;&#31181;&#25968;&#25454;&#31867;&#22411;&#22312; MDL &#30740;&#31350;&#20013;&#26368;&#24120;&#29992;&#12290;&#26412;&#30740;&#31350;&#22238;&#39038;&#20102;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#22312;&#20116;&#20010;&#20219;&#21153;&#20013;&#30340;&#24403;&#21069;&#24212;&#29992;&#65306;&#65288;1&#65289;&#25253;&#21578;&#29983;&#25104;&#65292;&#65288;2&#65289;&#35270;&#35273;&#38382;&#31572;&#65292;&#65288;3&#65289;&#20132;&#21449;...
&lt;/p&gt;
&lt;p&gt;
Computer-assisted diagnostic and prognostic systems of the future should be capable of simultaneously processing multimodal data. Multimodal deep learning (MDL), which involves the integration of multiple sources of data, such as images and text, has the potential to revolutionize the analysis and interpretation of biomedical data. However, it only caught researchers' attention recently. To this end, there is a critical need to conduct a systematic review on this topic, identify the limitations of current work, and explore future directions. In this scoping review, we aim to provide a comprehensive overview of the current state of the field and identify key concepts, types of studies, and research gaps with a focus on biomedical images and texts joint learning, mainly because these two were the most commonly available data types in MDL research. This study reviewed the current uses of multimodal deep learning on five tasks: (1) Report generation, (2) Visual question answering, (3) Cros
&lt;/p&gt;</description></item><item><title>Voicebox&#26159;&#19968;&#31181;&#22823;&#35268;&#27169;&#30340;&#22810;&#35821;&#35328;&#36890;&#29992;&#35821;&#38899;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#33258;&#22238;&#24402;&#30340;&#27969;&#21305;&#37197;&#27169;&#22411;&#65292;&#22312;&#25991;&#26412;&#21644;&#38899;&#39057;&#19978;&#19979;&#25991;&#26465;&#20214;&#19979;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#23454;&#29616;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#65292;&#22122;&#22768;&#21435;&#38500;&#65292;&#20869;&#23481;&#32534;&#36753;&#65292;&#39118;&#26684;&#36716;&#25442;&#21644;&#22810;&#26679;&#21270;&#30340;&#26679;&#26412;&#29983;&#25104;&#31561;&#22810;&#31181;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.15687</link><description>&lt;p&gt;
Voicebox&#65306;&#22823;&#35268;&#27169;&#30340;&#22810;&#35821;&#35328;&#36890;&#29992;&#35821;&#38899;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale. (arXiv:2306.15687v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15687
&lt;/p&gt;
&lt;p&gt;
Voicebox&#26159;&#19968;&#31181;&#22823;&#35268;&#27169;&#30340;&#22810;&#35821;&#35328;&#36890;&#29992;&#35821;&#38899;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#33258;&#22238;&#24402;&#30340;&#27969;&#21305;&#37197;&#27169;&#22411;&#65292;&#22312;&#25991;&#26412;&#21644;&#38899;&#39057;&#19978;&#19979;&#25991;&#26465;&#20214;&#19979;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#23454;&#29616;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#65292;&#22122;&#22768;&#21435;&#38500;&#65292;&#20869;&#23481;&#32534;&#36753;&#65292;&#39118;&#26684;&#36716;&#25442;&#21644;&#22810;&#26679;&#21270;&#30340;&#26679;&#26412;&#29983;&#25104;&#31561;&#22810;&#31181;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;GPT&#21644;DALL-E&#24050;&#32463;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#30740;&#31350;&#30340;&#26041;&#24335;&#12290;&#36825;&#20123;&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#25110;&#22270;&#20687;&#36755;&#20986;&#65292;&#32780;&#19988;&#36824;&#26159;&#36890;&#29992;&#30340;&#65292;&#21487;&#20197;&#35299;&#20915;&#26410;&#34987;&#26126;&#30830;&#25945;&#25480;&#30340;&#20219;&#21153;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#35821;&#38899;&#29983;&#25104;&#27169;&#22411;&#22312;&#35268;&#27169;&#21644;&#20219;&#21153;&#36890;&#29992;&#21270;&#26041;&#38754;&#20173;&#28982;&#27604;&#36739;&#21407;&#22987;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Voicebox&#65292;&#36825;&#26159;&#26368;&#22810;&#21151;&#33021;&#30340;&#38754;&#21521;&#35268;&#27169;&#30340;&#25991;&#26412;&#24341;&#23548;&#29983;&#25104;&#27169;&#22411;&#12290;Voicebox&#26159;&#19968;&#20010;&#38750;&#33258;&#22238;&#24402;&#30340;&#27969;&#21305;&#37197;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#38899;&#39057;&#19978;&#19979;&#25991;&#21644;&#25991;&#26412;&#26465;&#20214;&#19979;&#36827;&#34892;&#35757;&#32451;&#65292;&#29992;50,000&#23567;&#26102;&#30340;&#26410;&#32463;&#36807;&#28388;&#25110;&#22686;&#24378;&#30340;&#35821;&#38899;&#36827;&#34892;&#22635;&#20805;&#12290;&#19982;GPT&#31867;&#20284;&#65292;Voicebox&#21487;&#20197;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#25191;&#34892;&#22810;&#31181;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#20294;&#26356;&#21152;&#28789;&#27963;&#65292;&#22240;&#20026;&#23427;&#36824;&#21487;&#20197;&#23545;&#26410;&#26469;&#30340;&#19978;&#19979;&#25991;&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#12290;Voicebox&#21487;&#20197;&#29992;&#20110;&#21333;&#35821;&#25110;&#36328;&#35821;&#35328;&#38646;&#26679;&#26412;&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#65292;&#22122;&#22768;&#21435;&#38500;&#65292;&#20869;&#23481;&#32534;&#36753;&#65292;&#39118;&#26684;&#36716;&#25442;&#21644;&#22810;&#26679;&#21270;&#30340;&#26679;&#26412;&#29983;&#25104;&#12290;&#29305;&#21035;&#26159;&#65292;Voicebox
&lt;/p&gt;
&lt;p&gt;
Large-scale generative models such as GPT and DALL-E have revolutionized natural language processing and computer vision research. These models not only generate high fidelity text or image outputs, but are also generalists which can solve tasks not explicitly taught. In contrast, speech generative models are still primitive in terms of scale and task generalization. In this paper, we present Voicebox, the most versatile text-guided generative model for speech at scale. Voicebox is a non-autoregressive flow-matching model trained to infill speech, given audio context and text, trained on over 50K hours of speech that are neither filtered nor enhanced. Similar to GPT, Voicebox can perform many different tasks through in-context learning, but is more flexible as it can also condition on future context. Voicebox can be used for mono or cross-lingual zero-shot text-to-speech synthesis, noise removal, content editing, style conversion, and diverse sample generation. In particular, Voicebox 
&lt;/p&gt;</description></item><item><title>TransDis&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#35780;&#20998;&#31995;&#32479;&#65292;&#29992;&#20110;&#35780;&#20272;&#20013;&#25991;&#21457;&#25955;&#24615;&#24605;&#32500;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;TransDis&#30340;&#35780;&#20998;&#33021;&#22815;&#26377;&#25928;&#22320;&#39044;&#27979;&#20154;&#24037;&#35780;&#20998;&#65292;&#20855;&#26377;&#19982;&#20154;&#24037;&#35780;&#20998;&#30456;&#20284;&#30340;&#25928;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.14790</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#26041;&#27861;&#22312;&#20013;&#25991;&#21457;&#25955;&#24615;&#24605;&#32500;&#30340;&#33258;&#21160;&#35780;&#20272;&#20013;&#30340;&#24212;&#29992;&#8212;&#8212;TransDis
&lt;/p&gt;
&lt;p&gt;
Automatic Assessment of Divergent Thinking in Chinese Language with TransDis: A Transformer-Based Language Model Approach. (arXiv:2306.14790v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14790
&lt;/p&gt;
&lt;p&gt;
TransDis&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#35780;&#20998;&#31995;&#32479;&#65292;&#29992;&#20110;&#35780;&#20272;&#20013;&#25991;&#21457;&#25955;&#24615;&#24605;&#32500;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;TransDis&#30340;&#35780;&#20998;&#33021;&#22815;&#26377;&#25928;&#22320;&#39044;&#27979;&#20154;&#24037;&#35780;&#20998;&#65292;&#20855;&#26377;&#19982;&#20154;&#24037;&#35780;&#20998;&#30456;&#20284;&#30340;&#25928;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#21160;&#21019;&#36896;&#21147;&#35780;&#20272;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#36890;&#36807;&#29983;&#25104;&#35821;&#20041;&#36317;&#31163;&#26469;&#23458;&#35266;&#22320;&#34913;&#37327;&#21019;&#36896;&#24615;&#24605;&#32500;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#32570;&#20047;&#19968;&#20010;&#38024;&#23545;&#20013;&#25991;&#21019;&#36896;&#24615;&#24605;&#32500;&#35780;&#20272;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;TransDis&#65292;&#19968;&#20010;&#37319;&#29992;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20998;&#31995;&#32479;&#65292;&#33021;&#22815;&#20026;&#20013;&#25991;&#20013;&#30340;Alternate Uses Task&#65288;AUT&#65289;&#22238;&#31572;&#25552;&#20379;&#26377;&#25928;&#30340;&#29420;&#21019;&#24615;&#65288;&#36136;&#37327;&#65289;&#21644;&#28789;&#27963;&#24615;&#65288;&#22810;&#26679;&#24615;&#65289;&#35780;&#20998;&#12290;&#30740;&#31350;1&#34920;&#26126;&#65292;&#30001;&#19977;&#20010;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#32452;&#25104;&#30340;&#28508;&#22312;&#27169;&#22411;&#35780;&#20998;&#30340;&#21407;&#21019;&#24615;&#22240;&#32032;&#24378;&#28872;&#39044;&#27979;&#20154;&#24037;&#21407;&#21019;&#24615;&#35780;&#20998;&#65292;&#32780;&#27169;&#22411;&#35780;&#20998;&#30340;&#28789;&#27963;&#24615;&#19982;&#20154;&#24037;&#28789;&#27963;&#24615;&#35780;&#20998;&#20063;&#24378;&#30456;&#20851;&#12290;&#20934;&#21017;&#25928;&#24230;&#20998;&#26512;&#34920;&#26126;&#65292;&#27169;&#22411;&#35780;&#20998;&#30340;&#21407;&#21019;&#24615;&#21644;&#28789;&#27963;&#24615;&#19982;&#20854;&#20182;&#21019;&#36896;&#21147;&#27979;&#37327;&#27491;&#30456;&#20851;&#65292;&#35777;&#26126;&#19982;&#20154;&#24037;&#35780;&#20998;&#20855;&#26377;&#30456;&#20284;&#30340;&#25928;&#24230;&#12290;&#30740;&#31350;2&#21644;3&#34920;&#26126;&#65292;TransDis&#33021;&#22815;&#24555;&#36895;&#12289;&#20934;&#30830;&#22320;&#35780;&#20272;&#20013;&#25991;&#21457;&#25955;&#24615;&#24605;&#32500;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models have been increasingly popular for automatic creativity assessment, generating semantic distances to objectively measure the quality of creative ideas. However, there is currently a lack of an automatic assessment system for evaluating creative ideas in the Chinese language. To address this gap, we developed TransDis, a scoring system using transformer-based language models, capable of providing valid originality (quality) and flexibility (variety) scores for Alternative Uses Task (AUT) responses in Chinese. Study 1 demonstrated that the latent model-rated originality factor, comprised of three transformer-based models, strongly predicted human originality ratings, and the model-rated flexibility strongly correlated with human flexibility ratings as well. Criterion validity analyses indicated that model-rated originality and flexibility positively correlated to other creativity measures, demonstrating similar validity to human ratings. Study 2 &amp; 3 showed that TransDis e
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#29992;&#25143;&#24341;&#23548;&#21709;&#24212;&#20248;&#21270;&#65288;UGRO&#65289;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#26080;&#27880;&#37322;&#30340;&#29992;&#25143;&#27169;&#25311;&#22120;&#65292;&#20197;&#35780;&#20272;&#23545;&#35805;&#21709;&#24212;&#24182;&#20248;&#21270;&#30417;&#30563;&#24335;&#32463;&#36807;&#24494;&#35843;&#30340;&#31471;&#21040;&#31471;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#20379;&#28385;&#24847;&#24230;&#21453;&#39304;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20026;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#23545;&#35805;&#31995;&#32479;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2306.09821</link><description>&lt;p&gt;
&#24320;&#21457;&#29992;&#25143;&#21453;&#39304;&#30340;&#28508;&#21147;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#29992;&#25143;&#27169;&#25311;&#22120;&#20197;&#22686;&#24378;&#23545;&#35805;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Unlocking the Potential of User Feedback: Leveraging Large Language Model as User Simulator to Enhance Dialogue System. (arXiv:2306.09821v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09821
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#29992;&#25143;&#24341;&#23548;&#21709;&#24212;&#20248;&#21270;&#65288;UGRO&#65289;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#26080;&#27880;&#37322;&#30340;&#29992;&#25143;&#27169;&#25311;&#22120;&#65292;&#20197;&#35780;&#20272;&#23545;&#35805;&#21709;&#24212;&#24182;&#20248;&#21270;&#30417;&#30563;&#24335;&#32463;&#36807;&#24494;&#35843;&#30340;&#31471;&#21040;&#31471;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#20379;&#28385;&#24847;&#24230;&#21453;&#39304;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20026;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#23545;&#35805;&#31995;&#32479;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#31995;&#32479;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#26497;&#22823;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#19982;&#36739;&#23567;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#30456;&#27604;&#65292;&#30452;&#25509;&#21033;&#29992;LLMs&#20316;&#20026;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#65288;TOD&#65289;&#27169;&#22411;&#30340;&#24615;&#33021;&#36739;&#24046;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#25215;&#35748;LLMs&#30340;&#37325;&#22823;&#28508;&#21147;&#24182;&#25506;&#32034;&#21033;&#29992;&#23427;&#20204;&#30340;&#24778;&#20154;&#33021;&#21147;&#30340;&#25913;&#36827;&#26041;&#27861;&#38750;&#24120;&#37325;&#35201;&#12290;&#20026;&#20102;&#21033;&#29992;LLMs&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#29992;&#25143;&#24341;&#23548;&#21709;&#24212;&#20248;&#21270;&#65288;UGRO&#65289;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#23558;LLM&#19982;&#36739;&#23567;&#30340;TOD&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;LLM&#20316;&#20026;&#26080;&#27880;&#37322;&#30340;&#29992;&#25143;&#27169;&#25311;&#22120;&#26469;&#35780;&#20272;&#23545;&#35805;&#21709;&#24212;&#65292;&#23558;&#20854;&#19982;&#36739;&#23567;&#30340;&#32463;&#36807;&#24494;&#35843;&#30340;&#31471;&#21040;&#31471;TOD&#27169;&#22411;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#21033;&#29992;LLMs&#29983;&#25104;&#30340;&#28385;&#24847;&#24230;&#21453;&#39304;&#65292;UGRO&#36827;&#19968;&#27493;&#20248;&#21270;&#20102;&#30417;&#30563;&#24335;&#32463;&#36807;&#24494;&#35843;&#30340;TOD&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;TOD&#27169;&#22411;&#20197;&#23545;&#35805;&#21382;&#21490;&#35760;&#24405;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#22312;&#29992;&#25143;&#27169;&#25311;&#22120;&#21453;&#39304;&#30340;&#24110;&#21161;&#19979;&#29983;&#25104;&#31526;&#21512;&#29992;&#25143;&#38656;&#27714;&#30340;&#39640;&#28385;&#24847;&#24230;&#21709;&#24212;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;UGRO&#30340;&#26041;&#27861;&#30456;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20026;&#21033;&#29992;LLMs&#22686;&#24378;&#23545;&#35805;&#31995;&#32479;&#25552;&#20379;&#20102;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue systems and large language models (LLMs) have gained considerable attention. However, the direct utilization of LLMs as task-oriented dialogue (TOD) models has been found to underperform compared to smaller task-specific models. Nonetheless, it is crucial to acknowledge the significant potential of LLMs and explore improved approaches for leveraging their impressive abilities. Motivated by the goal of leveraging LLMs, we propose an alternative approach called User-Guided Response Optimization (UGRO) to combine it with a smaller TOD model. This approach uses LLM as annotation-free user simulator to assess dialogue responses, combining them with smaller fine-tuned end-to-end TOD models. By utilizing the satisfaction feedback generated by LLMs, UGRO further optimizes the supervised fine-tuned TOD model. Specifically, the TOD model takes the dialogue history as input and, with the assistance of the user simulator's feedback, generates high-satisfaction responses that meet the user
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WSPAlign&#30340;&#26080;&#38656;&#25163;&#21160;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;&#35789;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#29992;&#22823;&#35268;&#27169;&#24369;&#30417;&#30563;&#25968;&#25454;&#20013;&#30340;&#36328;&#24230;&#39044;&#27979;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#21462;&#24471;&#20102;&#27604;&#24403;&#21069;&#26368;&#20339;&#26041;&#27861;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.05644</link><description>&lt;p&gt;
WSPAlign: &#22823;&#35268;&#27169;&#24369;&#30417;&#30563;&#36328;&#24230;&#39044;&#27979;&#19979;&#30340;&#35789;&#23545;&#40784;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
WSPAlign: Word Alignment Pre-training via Large-Scale Weakly Supervised Span Prediction. (arXiv:2306.05644v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05644
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WSPAlign&#30340;&#26080;&#38656;&#25163;&#21160;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;&#35789;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#29992;&#22823;&#35268;&#27169;&#24369;&#30417;&#30563;&#25968;&#25454;&#20013;&#30340;&#36328;&#24230;&#39044;&#27979;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#21462;&#24471;&#20102;&#27604;&#24403;&#21069;&#26368;&#20339;&#26041;&#27861;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#35789;&#23545;&#40784;&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#21160;&#23545;&#40784;&#25968;&#25454;&#38598;&#25110;&#24179;&#34892;&#35821;&#26009;&#24211;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#29992;&#24615;&#12290;&#20026;&#20102;&#32531;&#35299;&#23545;&#25163;&#21160;&#25968;&#25454;&#30340;&#20381;&#36182;&#65292;&#25105;&#20204;&#36890;&#36807;&#25918;&#23485;&#23545;&#27491;&#30830;&#12289;&#23436;&#20840;&#23545;&#40784;&#21644;&#24179;&#34892;&#21477;&#23376;&#30340;&#35201;&#27714;&#65292;&#25193;&#22823;&#20102;&#30417;&#30563;&#25968;&#25454;&#30340;&#26469;&#28304;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#24102;&#26377;&#22122;&#22768;&#12289;&#37096;&#20998;&#23545;&#40784;&#21644;&#38750;&#24179;&#34892;&#27573;&#33853;&#20316;&#20026;&#22823;&#35268;&#27169;&#24369;&#30417;&#30563;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#36328;&#24230;&#39044;&#27979;&#23545;&#35789;&#23545;&#40784;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;WSPAlign&#65292;&#26159;&#19968;&#31181;&#26377;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#26080;&#38656;&#25163;&#21160;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;&#35789;&#23545;&#40784;&#26041;&#27861;&#12290;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;fine-tuning&#26102;&#65292;WSPAlign&#22312;F1&#21644;AER&#20004;&#20010;&#25351;&#26631;&#19978;&#30340;&#26368;&#20339;&#30417;&#30563;&#22522;&#32447;&#20998;&#21035;&#25552;&#39640;&#20102;3.3~6.1&#21644;1.5~6.1&#20010;&#28857;&#65292;&#25104;&#20026;&#20102;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;WSPAlign&#22312;&#23569;&#26679;&#26412;&#12289;&#38646;&#26679;&#26412;&#21644;&#36328;&#35821;&#35328;&#27979;&#35797;&#20013;&#20063;&#33719;&#24471;&#20102;&#19982;&#30456;&#24212;&#22522;&#32447;&#30456;&#21516;&#30340;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing word alignment methods rely on manual alignment datasets or parallel corpora, which limits their usefulness. Here, to mitigate the dependence on manual data, we broaden the source of supervision by relaxing the requirement for correct, fully-aligned, and parallel sentences. Specifically, we make noisy, partially aligned, and non-parallel paragraphs. We then use such a large-scale weakly-supervised dataset for word alignment pre-training via span prediction. Extensive experiments with various settings empirically demonstrate that our approach, which is named WSPAlign, is an effective and scalable way to pre-train word aligners without manual data. When fine-tuned on standard benchmarks, WSPAlign has set a new state-of-the-art by improving upon the best-supervised baseline by 3.3~6.1 points in F1 and 1.5~6.1 points in AER. Furthermore, WSPAlign also achieves competitive performance compared with the corresponding baselines in few-shot, zero-shot and cross-lingual tests, whi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23581;&#35797;&#23454;&#29616;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#36816;&#29992;&#21487;&#36870;&#27169;&#22411;&#23454;&#29616;&#39640;&#25928;&#30340;&#24494;&#35843;&#65292;&#24182;&#21457;&#29616;&#22312;&#21021;&#22987;&#21270;&#24494;&#35843;&#26102;&#20445;&#30041;PLM&#30340;&#36215;&#28857;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2306.00477</link><description>&lt;p&gt;
&#20351;&#39044;&#35757;&#32451;&#27169;&#22411;&#20855;&#26377;&#21487;&#36870;&#24615;&#65306;&#20174;&#21442;&#25968;&#21040;&#20869;&#23384;&#39640;&#25928;&#30340;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Make Your Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning. (arXiv:2306.00477v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23581;&#35797;&#23454;&#29616;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#36816;&#29992;&#21487;&#36870;&#27169;&#22411;&#23454;&#29616;&#39640;&#25928;&#30340;&#24494;&#35843;&#65292;&#24182;&#21457;&#29616;&#22312;&#21021;&#22987;&#21270;&#24494;&#35843;&#26102;&#20445;&#30041;PLM&#30340;&#36215;&#28857;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#38750;&#24120;&#25104;&#21151;&#30340;&#26041;&#27861;&#65292;&#21482;&#38656;&#35757;&#32451;&#23569;&#37327;&#21442;&#25968;&#32780;&#19981;&#20250;&#38477;&#20302;&#24615;&#33021;&#65292;&#24182;&#38543;&#30528;PLM&#36234;&#26469;&#36234;&#22823;&#32780;&#25104;&#20026;&#20107;&#23454;&#19978;&#30340;&#23398;&#20064;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;PEFT&#26041;&#27861;&#19981;&#20855;&#22791;&#20869;&#23384;&#25928;&#29575;&#65292;&#22240;&#20026;&#23427;&#20204;&#20173;&#38656;&#35201;&#23384;&#20648;&#22823;&#37096;&#20998;&#20013;&#38388;&#28608;&#27963;&#20540;&#20197;&#20415;&#35745;&#31639;&#26799;&#24230;&#65292;&#31867;&#20284;&#20110;&#24494;&#35843;&#12290;&#19968;&#20010;&#20943;&#23569;&#28608;&#27963;&#20869;&#23384;&#30340;&#26377;&#25928;&#26041;&#27861;&#26159;&#24212;&#29992;&#21487;&#36870;&#27169;&#22411;&#65292;&#36825;&#26679;&#20013;&#38388;&#28608;&#27963;&#20540;&#23601;&#26080;&#38656;&#32531;&#23384;&#65292;&#21487;&#20197;&#37325;&#26032;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#23558;PLM&#20462;&#25913;&#20026;&#23427;&#30340;&#21487;&#36870;&#21464;&#20307;&#24182;&#36827;&#34892;PEFT&#24182;&#19981;&#26159;&#19968;&#20214;&#23481;&#26131;&#30340;&#20107;&#65292;&#22240;&#20026;&#21487;&#36870;&#27169;&#22411;&#20855;&#26377;&#19982;&#24403;&#21069;&#21457;&#24067;&#30340;PLM&#19981;&#21516;&#30340;&#20307;&#31995;&#32467;&#26500;&#12290;&#26412;&#25991;&#39318;&#20808;&#35843;&#26597;&#29616;&#26377;PEFT&#26041;&#27861;&#25104;&#21151;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#35748;&#35782;&#21040;&#22312;&#21021;&#22987;&#21270;PEFT&#26102;&#20445;&#30041;PLM&#30340;&#36215;&#28857;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter-efficient fine-tuning (PEFT) of pre-trained language models (PLMs) has emerged as a highly successful approach, with training only a small number of parameters without sacrificing performance and becoming the de-facto learning paradigm with the increasing size of PLMs. However, existing PEFT methods are not memory-efficient, because they still require caching most of the intermediate activations for the gradient calculation, akin to fine-tuning. One effective way to reduce the activation memory is to apply a reversible model, so the intermediate activations are not necessary to be cached and can be recomputed. Nevertheless, modifying a PLM to its reversible variant with PEFT is not straightforward, since the reversible model has a distinct architecture from the currently released PLMs. In this paper, we first investigate what is a key factor for the success of existing PEFT methods, and realize that it's essential to preserve the PLM's starting point when initializing a PEFT 
&lt;/p&gt;</description></item><item><title>NavGPT&#26159;&#22522;&#20110;LLM&#30340;&#23548;&#33322;&#26234;&#33021;&#20307;&#65292;&#21487;&#20197;&#22312;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#65288;VLN&#65289;&#20013;&#65292;&#36890;&#36807;&#23545;&#25991;&#26412;&#25551;&#36848;&#36827;&#34892;&#25512;&#29702;&#65292;&#25191;&#34892;&#38646;-shot&#36830;&#32493;&#21160;&#20316;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#39640;&#32423;&#35268;&#21010;&#33021;&#21147;&#65292;&#21487;&#20197;&#23558;&#25351;&#20196;&#20998;&#35299;&#25104;&#23376;&#30446;&#26631;&#12289;&#25972;&#21512;&#24120;&#35782;&#30693;&#35782;&#20197;&#36827;&#34892;&#38556;&#30861;&#29289;&#36991;&#20813;&#65292;&#24182;&#21442;&#32771;&#20808;&#21069;&#30340;&#27493;&#39588;&#36827;&#34892;&#28548;&#28165;&#12290;NavGPT&#23637;&#31034;&#20102;&#36890;&#29992;&#20307;&#29616;&#26234;&#33021;&#20307;&#21457;&#23637;&#30340;&#32654;&#22909;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2305.16986</link><description>&lt;p&gt;
NavGPT: &#24102;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#20013;&#30340;&#26174;&#24335;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models. (arXiv:2305.16986v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16986
&lt;/p&gt;
&lt;p&gt;
NavGPT&#26159;&#22522;&#20110;LLM&#30340;&#23548;&#33322;&#26234;&#33021;&#20307;&#65292;&#21487;&#20197;&#22312;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#65288;VLN&#65289;&#20013;&#65292;&#36890;&#36807;&#23545;&#25991;&#26412;&#25551;&#36848;&#36827;&#34892;&#25512;&#29702;&#65292;&#25191;&#34892;&#38646;-shot&#36830;&#32493;&#21160;&#20316;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#39640;&#32423;&#35268;&#21010;&#33021;&#21147;&#65292;&#21487;&#20197;&#23558;&#25351;&#20196;&#20998;&#35299;&#25104;&#23376;&#30446;&#26631;&#12289;&#25972;&#21512;&#24120;&#35782;&#30693;&#35782;&#20197;&#36827;&#34892;&#38556;&#30861;&#29289;&#36991;&#20813;&#65292;&#24182;&#21442;&#32771;&#20808;&#21069;&#30340;&#27493;&#39588;&#36827;&#34892;&#28548;&#28165;&#12290;NavGPT&#23637;&#31034;&#20102;&#36890;&#29992;&#20307;&#29616;&#26234;&#33021;&#20307;&#21457;&#23637;&#30340;&#32654;&#22909;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20363;&#22914;ChatGPT&#21644;GPT-4&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#35268;&#27169;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#27169;&#22411;&#30340;&#25193;&#23637;&#20013;&#23637;&#29616;&#20986;&#26174;&#33879;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#31181;&#36235;&#21183;&#24378;&#35843;&#20102;&#20351;&#29992;&#26080;&#38480;&#35821;&#35328;&#25968;&#25454;&#35757;&#32451;LLM&#30340;&#28508;&#21147;&#65292;&#25512;&#21160;&#20102;&#36890;&#29992;&#20307;&#29616;&#26234;&#33021;&#20307;&#30340;&#21457;&#23637;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;NavGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#32431;&#31929;&#22522;&#20110;LLM&#30340;&#25351;&#20196;&#36319;&#38543;&#23548;&#33322;&#26234;&#33021;&#20307;&#65292;&#36890;&#36807;&#20026;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#65288;VLN&#65289;&#25191;&#34892;&#38646;-shot&#30340;&#36830;&#32493;&#21160;&#20316;&#39044;&#27979;&#65292;&#25581;&#31034;&#20102;&#23545;&#20110;&#22312;&#22797;&#26434;&#30340;&#29616;&#23454;&#22330;&#26223;&#19979;GPT&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#22312;&#27599;&#19968;&#27493;&#20013;&#65292;NavGPT&#23558;&#35270;&#35273;&#35266;&#23519;&#12289;&#23548;&#33322;&#21382;&#21490;&#21644;&#26410;&#26469;&#21487;&#25506;&#32034;&#26041;&#21521;&#30340;&#25991;&#26412;&#25551;&#36848;&#20316;&#20026;&#36755;&#20837;&#65292;&#25512;&#29702;&#20986;&#26234;&#33021;&#20307;&#30340;&#24403;&#21069;&#29366;&#24577;&#65292;&#24182;&#20915;&#23450;&#22914;&#20309;&#25509;&#36817;&#30446;&#26631;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;NavGPT&#21487;&#20197;&#26126;&#30830;&#22320;&#25191;&#34892;&#23548;&#33322;&#30340;&#39640;&#32423;&#35268;&#21010;&#65292;&#21253;&#25324;&#23558;&#25351;&#20196;&#20998;&#35299;&#25104;&#23376;&#30446;&#26631;&#12289;&#25972;&#21512;&#24120;&#35782;&#30693;&#35782;&#20197;&#36827;&#34892;&#38556;&#30861;&#29289;&#36991;&#20813;&#65292;&#24182;&#21442;&#32771;&#20808;&#21069;&#30340;&#27493;&#39588;&#36827;&#34892;&#28548;&#28165;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;LLM&#21487;&#33021;&#25104;&#20026;&#22797;&#26434;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#20256;&#32479;&#27969;&#31243;&#30340;&#24378;&#26377;&#21147;&#26367;&#20195;&#21697;&#65292;&#23637;&#31034;&#20102;&#36890;&#29992;&#20307;&#29616;&#26234;&#33021;&#20307;&#21457;&#23637;&#30340;&#32654;&#22909;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trained with an unprecedented scale of data, large language models (LLMs) like ChatGPT and GPT-4 exhibit the emergence of significant reasoning abilities from model scaling. Such a trend underscored the potential of training LLMs with unlimited language data, advancing the development of a universal embodied agent. In this work, we introduce the NavGPT, a purely LLM-based instruction-following navigation agent, to reveal the reasoning capability of GPT models in complex embodied scenes by performing zero-shot sequential action prediction for vision-and-language navigation (VLN). At each step, NavGPT takes the textual descriptions of visual observations, navigation history, and future explorable directions as inputs to reason the agent's current status, and makes the decision to approach the target. Through comprehensive experiments, we demonstrate NavGPT can explicitly perform high-level planning for navigation, including decomposing instruction into sub-goal, integrating commonsense k
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35789;&#27719;&#20462;&#21098;&#65288;VT&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21024;&#38500;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19981;&#30456;&#20851;&#26631;&#35760;&#65292;&#23558;&#20854;&#21387;&#32553;&#20026;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35789;&#27719;&#20462;&#21098;&#21487;&#20197;&#22312;&#20445;&#25345;&#22810;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#38477;&#20302;&#20102;&#27169;&#22411;&#30340;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2305.15020</link><description>&lt;p&gt;
&#36890;&#36807;&#35789;&#27719;&#20462;&#21098;&#23454;&#29616;&#39640;&#25928;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
An Efficient Multilingual Language Model Compression through Vocabulary Trimming. (arXiv:2305.15020v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15020
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35789;&#27719;&#20462;&#21098;&#65288;VT&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21024;&#38500;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19981;&#30456;&#20851;&#26631;&#35760;&#65292;&#23558;&#20854;&#21387;&#32553;&#20026;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35789;&#27719;&#20462;&#21098;&#21487;&#20197;&#22312;&#20445;&#25345;&#22810;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#38477;&#20302;&#20102;&#27169;&#22411;&#30340;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#24050;&#32463;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#38750;&#33521;&#35821;&#35821;&#35328;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28085;&#30422;&#19981;&#21516;&#35821;&#35328;&#26631;&#35760;&#30340;&#35789;&#27719;&#23884;&#20837;&#30697;&#38453;&#36739;&#22823;&#65292;&#22810;&#35821;&#35328;LM&#30340;&#27169;&#22411;&#21442;&#25968;&#20173;&#28982;&#24456;&#22823;&#12290;&#30456;&#21453;&#65292;&#21333;&#19968;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#29305;&#23450;&#20110;&#35821;&#35328;&#30340;&#35789;&#27719;&#22312;&#30446;&#26631;&#35821;&#35328;&#20013;&#35757;&#32451;&#65292;&#20294;&#36825;&#38656;&#35201;&#22823;&#37327;&#39044;&#31639;&#21644;&#21487;&#38752;&#35821;&#26009;&#24211;&#25165;&#33021;&#20174;&#22836;&#24320;&#22987;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35789;&#27719;&#20462;&#21098;&#65288;VT&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#35789;&#27719;&#20013;&#21024;&#38500;&#19981;&#30456;&#20851;&#30340;&#26631;&#35760;&#65292;&#23558;&#22810;&#35821;&#35328;LM&#30340;&#35789;&#27719;&#20943;&#23569;&#21040;&#30446;&#26631;&#35821;&#35328;&#12290;&#29702;&#35770;&#19978;&#65292;VT&#21487;&#20197;&#21387;&#32553;&#20219;&#20309;&#29616;&#26377;&#30340;&#22810;&#35821;&#35328;LM&#65292;&#20197;&#22312;&#22810;&#35821;&#35328;LM&#28085;&#30422;&#30340;&#20219;&#20309;&#35821;&#35328;&#20013;&#26500;&#24314;&#21333;&#19968;&#35821;&#35328;&#27169;&#22411;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;VT&#21487;&#20197;&#20445;&#30041;&#22810;&#35821;&#35328;LM&#30340;&#21407;&#22987;&#24615;&#33021;&#65292;&#21516;&#26102;&#23610;&#23544;&#26356;&#23567;&#65288;&#36890;&#24120;&#21482;&#38656;&#21407;&#22987;&#35789;&#27719;&#22823;&#23567;&#30340;&#32422;50&#65285;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual language model (LM) have become a powerful tool in NLP especially for non-English languages. Nevertheless, model parameters of multilingual LMs remain large due to the larger embedding matrix of the vocabulary covering tokens in different languages. On the contrary, monolingual LMs can be trained in a target language with the language-specific vocabulary only, but this requires a large budget and availability of reliable corpora to achieve a high-quality LM from scratch. In this paper, we propose vocabulary-trimming (VT), a method to reduce a multilingual LM vocabulary to a target language by deleting irrelevant tokens from its vocabulary. In theory, VT can compress any existing multilingual LM to build monolingual LMs in any language covered by the multilingual LM. In our experiments, we show that VT can retain the original performance of the multilingual LM, while being smaller in size (in general around 50% of the original vocabulary size is enough) than the original mu
&lt;/p&gt;</description></item><item><title>RefGPT&#26159;&#19968;&#31181;&#22522;&#20110;&#21442;&#32771;&#30340;&#23545;&#35805;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#22823;&#37327;&#30495;&#23454;&#19988;&#23450;&#21046;&#21270;&#30340;&#23545;&#35805;&#65292;&#24182;&#35299;&#20915;&#20102;&#23545;&#35805;&#29983;&#25104;&#20013;&#30340;&#27169;&#22411;&#24187;&#35273;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.14994</link><description>&lt;p&gt;
RefGPT: GPT&#27169;&#22411;&#20013;&#22522;&#20110;&#21442;&#32771;&#30340;&#30495;&#23454;&#19988;&#21487;&#23398;&#20064;&#21270;&#30340;&#23545;&#35805;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
RefGPT: Reference -&gt; Truthful &amp; Customized Dialogues Generation by GPTs and for GPTs. (arXiv:2305.14994v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14994
&lt;/p&gt;
&lt;p&gt;
RefGPT&#26159;&#19968;&#31181;&#22522;&#20110;&#21442;&#32771;&#30340;&#23545;&#35805;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#22823;&#37327;&#30495;&#23454;&#19988;&#23450;&#21046;&#21270;&#30340;&#23545;&#35805;&#65292;&#24182;&#35299;&#20915;&#20102;&#23545;&#35805;&#29983;&#25104;&#20013;&#30340;&#27169;&#22411;&#24187;&#35273;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#31561;&#36890;&#29992;&#30340;&#32842;&#22825;&#27169;&#22411;&#24050;&#32463;&#36890;&#36807;&#20351;&#29992;&#39640;&#36136;&#37327;&#25351;&#20196;&#25968;&#25454;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#35299;&#20915;&#21508;&#31181;NLP&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#20154;&#31867;&#32534;&#20889;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#65292;&#23588;&#20854;&#26159;&#22810;&#36718;&#23545;&#35805;&#65292;&#23545;&#22823;&#22810;&#25968;&#20154;&#26469;&#35828;&#26159;&#26114;&#36149;&#19988;&#38590;&#20197;&#23454;&#29616;&#30340;&#12290;&#23613;&#31649;&#20197;&#24448;&#30340;&#30740;&#31350;&#24050;&#32463;&#20351;&#29992;&#20102;&#24378;&#22823;&#30340;LLMs&#26469;&#33258;&#21160;&#29983;&#25104;&#23545;&#35805;&#65292;&#20294;&#30001;&#20110;LLMs&#23384;&#22312;&#24187;&#35273;&#65292;&#36825;&#20123;&#23545;&#35805;&#37117;&#26080;&#27861;&#23436;&#20840;&#30495;&#23454;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RefGPT&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#22823;&#37327;&#30495;&#23454;&#19988;&#23450;&#21046;&#21270;&#30340;&#23545;&#35805;&#65292;&#32780;&#26080;&#38656;&#25285;&#24515;&#27169;&#22411;&#24187;&#35273;&#36896;&#25104;&#30340;&#20107;&#23454;&#38169;&#35823;&#12290;RefGPT&#36890;&#36807;&#38480;&#21046;LLMs&#20351;&#29992;&#32473;&#23450;&#21442;&#32771;&#32780;&#19981;&#26159;&#22238;&#24518;&#33258;&#24049;&#30340;&#30693;&#35782;&#26469;&#29983;&#25104;&#23545;&#35805;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#23545;&#35805;&#29983;&#25104;&#20013;&#30340;&#27169;&#22411;&#24187;&#35273;&#12290;&#27492;&#22806;&#65292;RefGPT&#23545;&#27599;&#20010;&#35805;&#35821;&#37117;&#28155;&#21152;&#20102;&#35814;&#32454;&#30340;&#25511;&#21046;&#65292;&#20351;&#20854;&#20855;&#26377;&#39640;&#24230;&#23450;&#21046;&#21270;&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;&#20197;&#24448;&#30740;&#31350;&#25152;&#24573;&#30053;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
General chat models, like ChatGPT, have attained impressive capability to resolve a wide range of NLP tasks by tuning Large Language Models (LLMs) with high-quality instruction data. However, collecting human-written high-quality data, especially multi-turn dialogues, is expensive and unattainable for most people. Though previous studies have used powerful LLMs to generate the dialogues automatically, but they all suffer from generating untruthful dialogues because of the LLMs hallucination. Therefore, we propose a method called RefGPT to generate enormous truthful and customized dialogues without worrying about factual errors caused by the model hallucination. RefGPT solves the model hallucination in dialogue generation by restricting the LLMs to leverage the given reference instead of reciting their own knowledge to generate dialogues. Additionally, RefGPT adds detailed controls on every utterances to enable highly customization capability, which previous studies have ignored. On the
&lt;/p&gt;</description></item><item><title>BeamSearchQA&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36845;&#20195;&#24335;&#29983;&#25104;&#38382;&#39064;&#65292;&#20197;&#25429;&#25417;&#38544;&#21547;&#30693;&#35782;&#24182;&#20248;&#21270;&#38382;&#31572;&#36807;&#31243;&#65292;&#22312;NQ&#21644;WebQ&#27979;&#35797;&#38598;&#19978;&#20998;&#21035;&#36798;&#21040;&#20102;71.7&#65285;&#21644;46.7&#65285;&#30340;F1&#20998;&#25968;&#65292;&#26174;&#30528;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.14766</link><description>&lt;p&gt;
BeamSearchQA: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#24378;&#22823;&#30340;&#38646;-shot QA&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
BeamSearchQA: Large Language Models are Strong Zero-Shot QA Solver. (arXiv:2305.14766v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14766
&lt;/p&gt;
&lt;p&gt;
BeamSearchQA&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36845;&#20195;&#24335;&#29983;&#25104;&#38382;&#39064;&#65292;&#20197;&#25429;&#25417;&#38544;&#21547;&#30693;&#35782;&#24182;&#20248;&#21270;&#38382;&#31572;&#36807;&#31243;&#65292;&#22312;NQ&#21644;WebQ&#27979;&#35797;&#38598;&#19978;&#20998;&#21035;&#36798;&#21040;&#20102;71.7&#65285;&#21644;46.7&#65285;&#30340;F1&#20998;&#25968;&#65292;&#26174;&#30528;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#39046;&#22495;&#30340;&#38382;&#31572;&#26159;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#36890;&#24120;&#38656;&#35201;&#35775;&#38382;&#22806;&#37096;&#20449;&#24687;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#21333;&#36718;&#26816;&#32034;-&#38405;&#35835;&#26041;&#27861;&#65292;&#39318;&#20808;&#26816;&#32034;&#30456;&#20851;&#25991;&#26723;&#65292;&#28982;&#21518;&#22522;&#20110;&#26816;&#32034;&#30340;&#20449;&#24687;&#22238;&#31572;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#22238;&#31572;&#38382;&#39064;&#38656;&#35201;&#38544;&#21547;&#30340;&#30693;&#35782;&#65292;&#36825;&#20123;&#30693;&#35782;&#19981;&#30452;&#25509;&#20174;&#38382;&#39064;&#26412;&#36523;&#20013;&#33719;&#24471;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38382;&#31572;&#27969;&#31243;&#65292;&#31216;&#20026;BeamSearchQA&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36845;&#20195;&#29983;&#25104;&#20851;&#20110;&#21407;&#22987;&#38382;&#39064;&#30340;&#26032;&#38382;&#39064;&#65292;&#23454;&#29616;&#36845;&#20195;&#25512;&#29702;&#36807;&#31243;&#12290;&#36890;&#36807;&#36845;&#20195;&#32454;&#21270;&#21644;&#25193;&#23637;&#38382;&#39064;&#30340;&#33539;&#22260;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#25429;&#25417;&#24182;&#21033;&#29992;&#21487;&#33021;&#26080;&#27861;&#36890;&#36807;&#26816;&#32034;&#30452;&#25509;&#33719;&#21462;&#30340;&#38544;&#34255;&#30693;&#35782;&#12290;&#25105;&#20204;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;&#24320;&#25918;&#39046;&#22495;NQ&#21644;WebQ&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;BeamSearchQA&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#22312;NQ&#21644;WebQ&#27979;&#35797;&#38598;&#19978;&#20998;&#21035;&#36798;&#21040;&#20102;71.7&#65285;&#21644;46.7&#65285;&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open-domain question answering is a crucial task that often requires accessing external information. Existing methods typically adopt a single-turn retrieve-then-read approach, where relevant documents are first retrieved, and questions are then answered based on the retrieved information. However, there are cases where answering a question requires implicit knowledge that is not directly retrievable from the question itself. In this work, we propose a novel question-answering pipeline called eamSearchQA. Our approach leverages large language models(LLMs) to iteratively generate new questions about the original question, enabling an iterative reasoning process. By iteratively refining and expanding the scope of the question, our method aims to capture and utilize hidden knowledge that may not be directly obtainable through retrieval. We evaluate our approach on the widely-used open-domain NQ and WebQ datasets. The experimental results demonstrate that BeamSearchQA significantly outperf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22320;&#32536;&#25919;&#27835;&#20559;&#35265;&#30340;&#27010;&#24565;&#65292;&#24182;&#20197;&#39046;&#22303;&#20105;&#31471;&#20026;&#20363;&#65292;&#21033;&#29992;&#22810;&#35821;&#35328;&#12289;&#22810;&#36873;&#39064;&#30340;&#25968;&#25454;&#38598;BorderLines&#21644;&#20960;&#20010;&#23450;&#37327;&#25351;&#26631;&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;&#21709;&#24212;&#20013;&#30340;&#22320;&#32536;&#25919;&#27835;&#20559;&#35265;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2305.14610</link><description>&lt;p&gt;
&#36825;&#29255;&#22303;&#22320;&#26159;&#20320;&#25105;&#30340;&#22303;&#22320;&#65306;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22320;&#32536;&#25919;&#27835;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
This Land is {Your, My} Land: Evaluating Geopolitical Biases in Language Models. (arXiv:2305.14610v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22320;&#32536;&#25919;&#27835;&#20559;&#35265;&#30340;&#27010;&#24565;&#65292;&#24182;&#20197;&#39046;&#22303;&#20105;&#31471;&#20026;&#20363;&#65292;&#21033;&#29992;&#22810;&#35821;&#35328;&#12289;&#22810;&#36873;&#39064;&#30340;&#25968;&#25454;&#38598;BorderLines&#21644;&#20960;&#20010;&#23450;&#37327;&#25351;&#26631;&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;&#21709;&#24212;&#20013;&#30340;&#22320;&#32536;&#25919;&#27835;&#20559;&#35265;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#22320;&#32536;&#25919;&#27835;&#20559;&#35265;&#30340;&#27010;&#24565;&#8212;&#8212;&#21363;&#26681;&#25454;&#35821;&#35328;&#29615;&#22659;&#25253;&#36947;&#19981;&#21516;&#30340;&#22320;&#32536;&#25919;&#27835;&#30693;&#35782;&#30340;&#20542;&#21521;&#12290;&#25105;&#20204;&#20197;&#39046;&#22303;&#20105;&#31471;&#20026;&#26696;&#20363;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#20363;&#22914;&#65292;&#23545;&#20110;&#34987;&#24191;&#27867;&#20105;&#35758;&#30340;&#21335;&#27801;&#32676;&#23707;&#65292;&#22914;&#26524;&#29992;&#20013;&#25991;&#38382;&#65292;LM&#26159;&#21542;&#26356;&#26377;&#21487;&#33021;&#35828;&#23427;&#20204;&#23646;&#20110;&#20013;&#22269;&#65292;&#32780;&#22914;&#26524;&#29992;&#22612;&#21152;&#27931;&#35821;&#38382;&#65292;&#21017;&#26356;&#26377;&#21487;&#33021;&#35828;&#23427;&#20204;&#23646;&#20110;&#33778;&#24459;&#23486;&#65311;&#20026;&#20102;&#35780;&#20272;&#26159;&#21542;&#23384;&#22312;&#36825;&#31181;&#20559;&#35265;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#32500;&#22522;&#30334;&#31185;&#19978;&#25910;&#38598;&#20102;&#19968;&#32452;&#39046;&#22303;&#20105;&#31471;&#25968;&#25454;&#65292;&#28982;&#21518;&#23558;&#27599;&#20010;&#39046;&#22303;&#19982;&#19968;&#32452;&#22810;&#35821;&#35328;&#12289;&#22810;&#36873;&#39064;&#32852;&#31995;&#36215;&#26469;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#34987;&#31216;&#20026;BorderLines&#65292;&#23427;&#21253;&#25324;250&#20010;&#39046;&#22303;&#21644;45&#31181;&#35821;&#35328;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#38382;&#39064;&#38598;&#25552;&#20132;&#32473;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#20960;&#20010;&#25552;&#20986;&#30340;&#23450;&#37327;&#25351;&#26631;&#20998;&#26512;&#23427;&#20204;&#30340;&#21709;&#24212;&#20013;&#22320;&#32536;&#25919;&#27835;&#20559;&#35265;&#12290;&#36825;&#20123;&#25351;&#26631;&#27604;&#36739;&#19981;&#21516;&#35821;&#35328;&#30340;&#22238;&#31572;&#20197;&#21450;&#23454;&#38469;&#30340;&#22320;&#32536;&#25919;&#27835;&#24773;&#20917;&#12290;&#22320;&#32536;&#25919;&#27835;&#20559;&#35265;&#29616;&#35937;&#26159;&#19968;&#31181;&#29420;&#29305;&#30340;&#36328;&#35821;&#35328;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the notion of geopolitical bias -- a tendency to report different geopolitical knowledge depending on the linguistic context. As a case study, we consider territorial disputes between countries. For example, for the widely contested Spratly Islands, would an LM be more likely to say they belong to China if asked in Chinese, vs. to the Philippines if asked in Tagalog? To evaluate if such biases exist, we first collect a dataset of territorial disputes from Wikipedia, then associate each territory with a set of multilingual, multiple-choice questions. This dataset, termed BorderLines, consists of 250 territories with questions in 45 languages. We pose these question sets to language models, and analyze geopolitical bias in their responses through several proposed quantitative metrics. The metrics compare between responses in different question languages as well as to the actual geopolitical situation. The phenomenon of geopolitical bias is a uniquely cross-lingual evaluation
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#24369;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#20013;&#23398;&#20064;&#35270;&#35273;&#20851;&#31995;&#65292;&#36890;&#36807;&#36716;&#25442;&#35270;&#35273;&#20851;&#31995;&#25968;&#25454;&#20026;&#32467;&#26500;&#21270;&#26631;&#39064;&#21644;&#36974;&#32617;&#20851;&#31995;&#39044;&#27979;&#65292;&#23454;&#29616;&#20102;&#20174;&#24369;&#30417;&#30563;&#20851;&#31995;&#25968;&#25454;&#20013;&#23398;&#20064;&#22810;&#27169;&#24577;&#34920;&#31034;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14281</link><description>&lt;p&gt;
&#24369;&#30417;&#30563;&#23398;&#20064;&#22312;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#20013;&#30340;&#35270;&#35273;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Weakly-Supervised Learning of Visual Relations in Multimodal Pretraining. (arXiv:2305.14281v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14281
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#24369;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#20013;&#23398;&#20064;&#35270;&#35273;&#20851;&#31995;&#65292;&#36890;&#36807;&#36716;&#25442;&#35270;&#35273;&#20851;&#31995;&#25968;&#25454;&#20026;&#32467;&#26500;&#21270;&#26631;&#39064;&#21644;&#36974;&#32617;&#20851;&#31995;&#39044;&#27979;&#65292;&#23454;&#29616;&#20102;&#20174;&#24369;&#30417;&#30563;&#20851;&#31995;&#25968;&#25454;&#20013;&#23398;&#20064;&#22810;&#27169;&#24577;&#34920;&#31034;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#35270;&#35273;&#19982;&#35821;&#35328;&#39044;&#35757;&#32451;&#20013;&#30340;&#30740;&#31350;&#20013;&#65292;&#35843;&#26597;&#20102;&#20174;&#30446;&#26631;&#26816;&#27979;&#25968;&#25454;&#20013;&#30340;&#30417;&#30563;&#20449;&#21495;&#65292;&#20197;&#23398;&#20064;&#26356;&#22909;&#12289;&#32454;&#31890;&#24230;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#23567;&#35268;&#27169;&#30340;&#35270;&#35273;&#20851;&#31995;&#25968;&#25454;&#36827;&#34892;&#30417;&#30563;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#39044;&#35757;&#32451;&#26041;&#27861;&#26469;&#22312;&#22810;&#27169;&#24577;&#35774;&#32622;&#20013;&#23545;&#35270;&#35273;&#23454;&#20307;&#36827;&#34892;&#35821;&#22659;&#21270;&#12290;&#36890;&#36807;&#35328;&#35821;&#21270;&#22330;&#26223;&#22270;&#65292;&#25105;&#20204;&#23558;&#35270;&#35273;&#20851;&#31995;&#19977;&#20803;&#32452;&#36716;&#25442;&#20026;&#32467;&#26500;&#21270;&#26631;&#39064;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#39069;&#22806;&#30340;&#22270;&#20687;&#25551;&#36848;&#12290;&#36890;&#36807;&#36974;&#32617;&#20851;&#31995;&#39044;&#27979;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#40723;&#21169;&#23558;&#22270;&#20687;&#21306;&#22495;&#20013;&#30340;&#23454;&#20307;&#19982;&#35270;&#35273;&#19978;&#36974;&#32617;&#30340;&#19978;&#19979;&#25991;&#36827;&#34892;&#20851;&#32852;&#12290;&#24403;&#24212;&#29992;&#20110;&#22312;&#22823;&#37327;Web&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#30340;&#24378;&#22522;&#32447;&#27169;&#22411;&#26102;&#65292;&#23545;&#20110;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#35780;&#20272;&#26174;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20174;&#24369;&#30417;&#30563;&#20851;&#31995;&#25968;&#25454;&#20013;&#23398;&#20064;&#22810;&#27169;&#24577;&#34920;&#31034;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work in vision-and-language pretraining has investigated supervised signals from object detection data to learn better, fine-grained multimodal representations. In this work, we take a step further and explore how we can tap into supervision from small-scale visual relation data. In particular, we propose two pretraining approaches to contextualise visual entities in a multimodal setup. With verbalised scene graphs, we transform visual relation triplets into structured captions, and treat them as additional image descriptions. With masked relation prediction, we further encourage relating entities from image regions with visually masked contexts. When applied to strong baselines pretrained on large amounts of Web data, zero-shot evaluations on both coarse-grained and fine-grained tasks show the efficacy of our methods in learning multimodal representations from weakly-supervised relations data.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SciMult&#30340;&#22810;&#20219;&#21153;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#20849;&#20139;&#19981;&#21516;&#31185;&#23398;&#25991;&#29486;&#29702;&#35299;&#20219;&#21153;&#20043;&#38388;&#30340;&#36890;&#29992;&#30693;&#35782;&#65292;&#21516;&#26102;&#38450;&#27490;&#20219;&#21153;&#29305;&#23450;&#25216;&#33021;&#30456;&#20114;&#24178;&#25200;&#12290;</title><link>http://arxiv.org/abs/2305.14232</link><description>&lt;p&gt;
&#20026;&#31185;&#23398;&#25991;&#29486;&#29702;&#35299;&#39044;&#35757;&#32451;&#22810;&#20219;&#21153;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Pre-training Multi-task Contrastive Learning Models for Scientific Literature Understanding. (arXiv:2305.14232v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SciMult&#30340;&#22810;&#20219;&#21153;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#20849;&#20139;&#19981;&#21516;&#31185;&#23398;&#25991;&#29486;&#29702;&#35299;&#20219;&#21153;&#20043;&#38388;&#30340;&#36890;&#29992;&#30693;&#35782;&#65292;&#21516;&#26102;&#38450;&#27490;&#20219;&#21153;&#29305;&#23450;&#25216;&#33021;&#30456;&#20114;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#25991;&#29486;&#29702;&#35299;&#20219;&#21153;&#22240;&#20854;&#21152;&#36895;&#31185;&#23398;&#21457;&#29616;&#30340;&#28508;&#21147;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#36328;&#22810;&#20010;&#24322;&#26500;&#20219;&#21153;&#20849;&#21516;&#21033;&#29992;&#39044;&#35757;&#32451;&#25968;&#25454;&#65288;&#20363;&#22914;&#65292;&#26497;&#38480;&#22810;&#26631;&#31614;&#35770;&#25991;&#20998;&#31867;&#12289;&#24341;&#25991;&#39044;&#27979;&#21644;&#25991;&#29486;&#25628;&#32034;&#65289;&#20173;&#28982;&#22522;&#26412;&#19978;&#26410;&#34987;&#25506;&#32034;&#12290;&#20026;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;SciMult&#65292;&#37325;&#28857;&#26159;&#20419;&#36827;&#19981;&#21516;&#31185;&#23398;&#25991;&#29486;&#29702;&#35299;&#20219;&#21153;&#20043;&#38388;&#30340;&#20849;&#20139;&#36890;&#29992;&#30693;&#35782;&#65292;&#21516;&#26102;&#38450;&#27490;&#20219;&#21153;&#29305;&#23450;&#25216;&#33021;&#30456;&#20114;&#24178;&#25200;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20004;&#31181;&#25216;&#26415;-&#20219;&#21153;&#24863;&#30693;&#30340;&#29305;&#21270;&#21644;&#25351;&#20196;&#35843;&#25972;&#12290;&#21069;&#32773;&#37319;&#29992;&#20102;&#20855;&#26377;&#20219;&#21153;&#24863;&#30693;&#23376;&#23618;&#30340;&#22810;&#19987;&#23478;&#21464;&#21387;&#22120;&#26550;&#26500;&#65307;&#21518;&#32773;&#22312;&#36755;&#20837;&#25991;&#26412;&#20043;&#21069;&#28155;&#21152;&#20102;&#20219;&#21153;&#29305;&#23450;&#30340;&#25351;&#20196;&#20197;&#20135;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scientific literature understanding tasks have gained significant attention due to their potential to accelerate scientific discovery. Pre-trained language models (LMs) have shown effectiveness in these tasks, especially when tuned via contrastive learning. However, jointly utilizing pre-training data across multiple heterogeneous tasks (e.g., extreme multi-label paper classification, citation prediction, and literature search) remains largely unexplored. To bridge this gap, we propose a multi-task contrastive learning framework, SciMult, with a focus on facilitating common knowledge sharing across different scientific literature understanding tasks while preventing task-specific skills from interfering with each other. To be specific, we explore two techniques -task-aware specialization and instruction tuning. The former adopts a Mixture-of-Experts Transformer architecture with task-aware sub-layers; the latter prepends task-specific instructions to the input text so as to produce t
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#26080;&#27880;&#37322;&#24179;&#34892;&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#35789;&#20041;&#20849;&#23384;&#27169;&#24335;&#30340;&#22810;&#35821;&#35328;&#22270;&#26041;&#27861;&#65292;&#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#12290;&#36890;&#36807;&#24314;&#31435;&#39640;&#36136;&#37327;&#30340;&#22810;&#35821;&#35328;&#23884;&#20837;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#39640;&#21484;&#22238;&#29575;&#30340;&#35789;&#20041;&#20849;&#23384;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2305.12818</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#35821;&#35328;&#35789;&#20041;&#20851;&#31995;&#22270;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Crosslingual Transfer Learning for Low-Resource Languages Based on Multilingual Colexification Graphs. (arXiv:2305.12818v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12818
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26080;&#27880;&#37322;&#24179;&#34892;&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#35789;&#20041;&#20849;&#23384;&#27169;&#24335;&#30340;&#22810;&#35821;&#35328;&#22270;&#26041;&#27861;&#65292;&#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#12290;&#36890;&#36807;&#24314;&#31435;&#39640;&#36136;&#37327;&#30340;&#22810;&#35821;&#35328;&#23884;&#20837;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#39640;&#21484;&#22238;&#29575;&#30340;&#35789;&#20041;&#20849;&#23384;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27604;&#36739;&#35821;&#35328;&#23398;&#20013;&#65292;&#35789;&#20041;&#20849;&#23384;&#25351;&#30340;&#26159;&#19968;&#20010;&#35789;&#27719;&#24418;&#24335;&#20256;&#36798;&#20004;&#20010;&#25110;&#26356;&#22810;&#19981;&#21516;&#30340;&#24847;&#20041;&#30340;&#29616;&#35937;&#12290;&#29616;&#26377;&#30340;&#20851;&#20110;&#35789;&#20041;&#20849;&#23384;&#27169;&#24335;&#30340;&#24037;&#20316;&#22522;&#20110;&#27880;&#37322;&#30340;&#35789;&#27719;&#34920;&#65292;&#38480;&#21046;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#20174;&#26410;&#27880;&#37322;&#30340;&#24179;&#34892;&#35821;&#26009;&#24211;&#20013;&#30452;&#25509;&#35782;&#21035;&#20102;&#36229;&#36807;2,000&#20010;&#27010;&#24565;&#22312;1,335&#31181;&#35821;&#35328;&#20013;&#30340;&#35789;&#20041;&#20849;&#23384;&#27169;&#24335;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#24314;&#31435;&#22522;&#20110;&#35789;&#20041;&#20849;&#23384;&#27169;&#24335;&#30340;&#22810;&#35821;&#35328;&#22270;&#65306;ColexNet&#21644;ColexNet+&#12290;ColexNet&#30340;&#33410;&#28857;&#26159;&#27010;&#24565;&#65292;&#36793;&#26159;&#35789;&#20041;&#20849;&#23384;&#20851;&#31995;&#12290;&#22312;ColexNet+&#20013;&#65292;&#27010;&#24565;&#33410;&#28857;&#36890;&#36807;&#20013;&#38388;&#33410;&#28857;&#36827;&#34892;&#38468;&#21152;&#36830;&#25509;&#65292;&#27599;&#20010;&#20013;&#38388;&#33410;&#28857;&#20195;&#34920;1,334&#31181;&#35821;&#35328;&#20013;&#30340;&#19968;&#20010;ngram&#12290;&#25105;&#20204;&#20351;&#29992;ColexNet+&#35757;&#32451;&#39640;&#36136;&#37327;&#30340;&#22810;&#35821;&#35328;&#23884;&#20837;&#65292;&#31216;&#20043;&#20026;$\overrightarrow{\mbox{ColexNet+}}$&#65292;&#38750;&#24120;&#36866;&#21512;&#36801;&#31227;&#23398;&#20064;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;ColexNet&#22312;&#36328;&#35821;&#35328;&#35789;&#20041;&#20849;&#23384;&#25968;&#25454;&#38598;CLICS&#19978;&#30340;&#39640;&#21484;&#22238;&#29575;&#12290;&#28982;&#21518;&#25105;&#20204;&#23545;&#24314;&#35758;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;...
&lt;/p&gt;
&lt;p&gt;
In comparative linguistics, colexification refers to the phenomenon of a lexical form conveying two or more distinct meanings. Existing work on colexification patterns relies on annotated word lists, limiting scalability and usefulness in NLP. In contrast, we identify colexification patterns of more than 2,000 concepts across 1,335 languages directly from an unannotated parallel corpus. We then propose simple and effective methods to build multilingual graphs from the colexification patterns: ColexNet and ColexNet+. ColexNet's nodes are concepts and its edges are colexifications. In ColexNet+, concept nodes are additionally linked through intermediate nodes, each representing an ngram in one of 1,334 languages. We use ColexNet+ to train $\overrightarrow{\mbox{ColexNet+}}$, high-quality multilingual embeddings that are well-suited for transfer learning. In our experiments, we first show that ColexNet achieves high recall on CLICS, a dataset of crosslingual colexifications. We then evalu
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;&#30340;&#32467;&#26500;&#21270;&#39044;&#27979;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#37096;&#20998;&#27880;&#37322;&#21644;&#33258;&#35757;&#32451;&#26469;&#20943;&#23569;&#27880;&#37322;&#25104;&#26412;&#12290;&#25105;&#20204;&#21033;&#29992;&#37096;&#20998;&#27880;&#37322;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#23376;&#32467;&#26500;&#36827;&#34892;&#26631;&#27880;&#65292;&#24182;&#23558;&#33258;&#21160;&#39044;&#27979;&#32467;&#26524;&#20316;&#20026;&#26410;&#27880;&#37322;&#23376;&#32467;&#26500;&#30340;&#20266;&#26631;&#31614;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#38169;&#35823;&#20272;&#35745;&#22120;&#33258;&#36866;&#24212;&#22320;&#20915;&#23450;&#37096;&#20998;&#36873;&#25321;&#27604;&#20363;&#65292;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#12290;&#22312;&#22235;&#20010;&#32467;&#26500;&#21270;&#39044;&#27979;&#20219;&#21153;&#30340;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#33258;&#36866;&#24212;&#36873;&#25321;&#27604;&#20363;&#30340;&#37096;&#20998;&#27880;&#37322;&#21644;&#33258;&#35757;&#32451;&#32452;&#21512;&#21487;&#20197;&#38477;&#20302;&#27880;&#37322;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2305.12634</link><description>&lt;p&gt;
&#25968;&#25454;&#39640;&#25928;&#30340;&#32467;&#26500;&#21270;&#39044;&#27979;&#20027;&#21160;&#23398;&#20064;&#65292;&#38024;&#23545;&#37096;&#20998;&#27880;&#37322;&#21644;&#33258;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Data-efficient Active Learning for Structured Prediction with Partial Annotation and Self-Training. (arXiv:2305.12634v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12634
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;&#30340;&#32467;&#26500;&#21270;&#39044;&#27979;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#37096;&#20998;&#27880;&#37322;&#21644;&#33258;&#35757;&#32451;&#26469;&#20943;&#23569;&#27880;&#37322;&#25104;&#26412;&#12290;&#25105;&#20204;&#21033;&#29992;&#37096;&#20998;&#27880;&#37322;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#23376;&#32467;&#26500;&#36827;&#34892;&#26631;&#27880;&#65292;&#24182;&#23558;&#33258;&#21160;&#39044;&#27979;&#32467;&#26524;&#20316;&#20026;&#26410;&#27880;&#37322;&#23376;&#32467;&#26500;&#30340;&#20266;&#26631;&#31614;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#38169;&#35823;&#20272;&#35745;&#22120;&#33258;&#36866;&#24212;&#22320;&#20915;&#23450;&#37096;&#20998;&#36873;&#25321;&#27604;&#20363;&#65292;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#12290;&#22312;&#22235;&#20010;&#32467;&#26500;&#21270;&#39044;&#27979;&#20219;&#21153;&#30340;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#33258;&#36866;&#24212;&#36873;&#25321;&#27604;&#20363;&#30340;&#37096;&#20998;&#27880;&#37322;&#21644;&#33258;&#35757;&#32451;&#32452;&#21512;&#21487;&#20197;&#38477;&#20302;&#27880;&#37322;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20027;&#21160;&#23398;&#20064;&#26469;&#20943;&#23569;&#32467;&#26500;&#21270;&#26631;&#31614;&#31354;&#38388;&#30340;&#27880;&#37322;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#37096;&#20998;&#27880;&#37322;&#65292;&#36890;&#36807;&#21482;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#23376;&#32467;&#26500;&#36827;&#34892;&#27880;&#37322;&#65292;&#20174;&#32780;&#20943;&#23569;&#32467;&#26500;&#21270;&#36755;&#20986;&#30340;&#26631;&#27880;&#25104;&#26412;&#12290;&#25105;&#20204;&#36824;&#21033;&#29992;&#33258;&#35757;&#32451;&#65292;&#23558;&#24403;&#21069;&#27169;&#22411;&#30340;&#33258;&#21160;&#39044;&#27979;&#20316;&#20026;&#26410;&#27880;&#37322;&#23376;&#32467;&#26500;&#30340;&#20266;&#26631;&#31614;&#12290;&#26377;&#25928;&#22320;&#32467;&#21512;&#37096;&#20998;&#27880;&#37322;&#21644;&#33258;&#35757;&#32451;&#20197;&#20943;&#23569;&#27880;&#37322;&#25104;&#26412;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#30830;&#23450;&#36873;&#25321;&#21738;&#20123;&#23376;&#32467;&#26500;&#36827;&#34892;&#26631;&#27880;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#38169;&#35823;&#20272;&#35745;&#22120;&#65292;&#26681;&#25454;&#24403;&#21069;&#27169;&#22411;&#30340;&#33021;&#21147;&#33258;&#36866;&#24212;&#22320;&#20915;&#23450;&#37096;&#20998;&#36873;&#25321;&#27604;&#20363;&#12290;&#36890;&#36807;&#23545;&#22235;&#20010;&#32467;&#26500;&#21270;&#39044;&#27979;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#37096;&#20998;&#27880;&#37322;&#21644;&#33258;&#35757;&#32451;&#32452;&#21512;&#22312;&#19968;&#20010;&#20844;&#24179;&#27604;&#36739;&#26041;&#26696;&#19979;&#65292;&#20351;&#29992;&#33258;&#36866;&#24212;&#36873;&#25321;&#27604;&#20363;&#65292;&#21487;&#20197;&#38477;&#20302;&#27880;&#37322;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we propose a pragmatic method that reduces the annotation cost for structured label spaces using active learning. Our approach leverages partial annotation, which reduces labeling costs for structured outputs by selecting only the most informative sub-structures for annotation. We also utilize self-training to incorporate the current model's automatic predictions as pseudo-labels for un-annotated sub-structures. A key challenge in effectively combining partial annotation with self-training to reduce annotation cost is determining which sub-structures to select to label. To address this challenge, we adopt an error estimator to adaptively decide the partial selection ratio according to the current model's capability. In evaluations spanning four structured prediction tasks, we show that our combination of partial annotation and self-training using an adaptive selection ratio reduces annotation cost over strong full annotation baselines under a fair comparison scheme that ta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Logic-LM&#30340;&#26032;&#26694;&#26550;&#65292;&#23427;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#31526;&#21495;&#27714;&#35299;&#22120;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#25552;&#21319;&#22312;&#22797;&#26434;&#36923;&#36753;&#38382;&#39064;&#19978;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#36716;&#21270;&#20026;&#31526;&#21495;&#34920;&#36798;&#65292;&#24182;&#21033;&#29992;&#30830;&#23450;&#24615;&#31526;&#21495;&#27714;&#35299;&#22120;&#36827;&#34892;&#25512;&#29702;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#25913;&#36827;&#20934;&#30830;&#36923;&#36753;&#25512;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;Logic-LM&#22312;&#22810;&#20010;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#19988;&#30456;&#27604;&#20351;&#29992;&#26631;&#20934;&#25552;&#31034;&#25110;&#24605;&#32500;&#38142;&#25552;&#31034;&#65292;&#25928;&#26524;&#20998;&#21035;&#25552;&#39640;&#20102;39.2%&#21644;18.4%&#12290;&#36825;&#34920;&#26126;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#31526;&#21495;&#36923;&#36753;&#30456;&#32467;&#21512;&#26159;&#23454;&#29616;&#20934;&#30830;&#36923;&#36753;&#25512;&#29702;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.12295</link><description>&lt;p&gt;
Logic-LM&#65306;&#36890;&#36807;&#31526;&#21495;&#27714;&#35299;&#22120;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20934;&#30830;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning. (arXiv:2305.12295v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Logic-LM&#30340;&#26032;&#26694;&#26550;&#65292;&#23427;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#31526;&#21495;&#27714;&#35299;&#22120;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#25552;&#21319;&#22312;&#22797;&#26434;&#36923;&#36753;&#38382;&#39064;&#19978;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#36716;&#21270;&#20026;&#31526;&#21495;&#34920;&#36798;&#65292;&#24182;&#21033;&#29992;&#30830;&#23450;&#24615;&#31526;&#21495;&#27714;&#35299;&#22120;&#36827;&#34892;&#25512;&#29702;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#25913;&#36827;&#20934;&#30830;&#36923;&#36753;&#25512;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;Logic-LM&#22312;&#22810;&#20010;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#19988;&#30456;&#27604;&#20351;&#29992;&#26631;&#20934;&#25552;&#31034;&#25110;&#24605;&#32500;&#38142;&#25552;&#31034;&#65292;&#25928;&#26524;&#20998;&#21035;&#25552;&#39640;&#20102;39.2%&#21644;18.4%&#12290;&#36825;&#34920;&#26126;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#31526;&#21495;&#36923;&#36753;&#30456;&#32467;&#21512;&#26159;&#23454;&#29616;&#20934;&#30830;&#36923;&#36753;&#25512;&#29702;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#23637;&#31034;&#20102;&#20154;&#31867;&#19968;&#26679;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#20173;&#28982;&#22312;&#22797;&#26434;&#30340;&#36923;&#36753;&#38382;&#39064;&#19978;&#36935;&#21040;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;Logic-LM&#65292;&#23558;LLMs&#19982;&#31526;&#21495;&#27714;&#35299;&#22120;&#38598;&#25104;&#65292;&#20197;&#25913;&#36827;&#36923;&#36753;&#38382;&#39064;&#30340;&#35299;&#20915;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#21033;&#29992;LLMs&#23558;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#36716;&#21270;&#20026;&#31526;&#21495;&#21270;&#34920;&#36848;&#12290;&#28982;&#21518;&#65292;&#30830;&#23450;&#24615;&#31526;&#21495;&#27714;&#35299;&#22120;&#23545;&#38382;&#39064;&#36827;&#34892;&#25512;&#29702;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#33258;&#25105;&#23436;&#21892;&#27169;&#22359;&#65292;&#21033;&#29992;&#31526;&#21495;&#27714;&#35299;&#22120;&#30340;&#38169;&#35823;&#20449;&#24687;&#20462;&#27491;&#31526;&#21495;&#21270;&#34920;&#31034;&#12290;&#36890;&#36807;&#22312;&#20116;&#20010;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;ProofWriter&#12289;PrOntoQA&#12289;FOLIO&#12289;LogicalDeduction&#21644;AR-LSAT&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;Logic-LM&#30340;&#26377;&#25928;&#24615;&#12290;&#24179;&#22343;&#32780;&#35328;&#65292;Logic-LM&#30456;&#27604;&#20165;&#20351;&#29992;LLMs&#30340;&#26631;&#20934;&#25552;&#31034;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;39.2%&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#20351;&#29992;LLMs&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;&#21487;&#20197;&#25552;&#39640;18.4%&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#23558;LLMs&#19982;&#31526;&#21495;&#36923;&#36753;&#30456;&#32467;&#21512;&#65292;Logic-LM&#20026;&#20934;&#30830;&#30340;&#36923;&#36753;&#25512;&#29702;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown human-like reasoning abilities but still struggle with complex logical problems. This paper introduces a novel framework, Logic-LM, which integrates LLMs with symbolic solvers to improve logical problem-solving. Our method first utilizes LLMs to translate a natural language problem into a symbolic formulation. Afterward, a deterministic symbolic solver performs inference on the formulated problem. We also introduce a self-refinement module, which utilizes the symbolic solver's error messages to revise symbolic formalizations. We demonstrate Logic-LM's effectiveness on five logical reasoning datasets: ProofWriter, PrOntoQA, FOLIO, LogicalDeduction, and AR-LSAT. On average, Logic-LM achieves a significant performance boost of 39.2% over using LLM alone with standard prompting and 18.4% over LLM with chain-of-thought prompting. Our findings suggest that Logic-LM, by combining LLMs with symbolic logic, offers a promising avenue for faithful logical r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20266;&#20195;&#30721;&#25351;&#20196;&#25552;&#31034;&#33021;&#21542;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23454;&#39564;&#35777;&#26126;&#20351;&#29992;&#20266;&#20195;&#30721;&#25552;&#31034;&#21487;&#20197;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#25552;&#39640;7-16&#20998;&#65292;&#24182;&#30456;&#23545;&#25913;&#21892;12-38%&#12290;</title><link>http://arxiv.org/abs/2305.11790</link><description>&lt;p&gt;
&#20266;&#20195;&#30721;&#25351;&#20196;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Prompting with Pseudo-Code Instructions. (arXiv:2305.11790v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11790
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20266;&#20195;&#30721;&#25351;&#20196;&#25552;&#31034;&#33021;&#21542;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23454;&#39564;&#35777;&#26126;&#20351;&#29992;&#20266;&#20195;&#30721;&#25552;&#31034;&#21487;&#20197;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#25552;&#39640;7-16&#20998;&#65292;&#24182;&#30456;&#23545;&#25913;&#21892;12-38%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#25552;&#31034;&#24050;&#25104;&#20026;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#12290;&#37492;&#20110;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#22266;&#26377;&#27495;&#20041;&#65292;&#22240;&#27492;&#32771;&#34385;&#20351;&#29992;&#26356;&#23569;&#27495;&#20041;&#30340;&#25552;&#31034;&#26679;&#24335;&#65292;&#22914;&#20266;&#20195;&#30721;&#25552;&#31034;&#65292;&#21487;&#33021;&#20855;&#26377;&#20248;&#21183;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#36890;&#36807;&#20266;&#20195;&#30721;&#25351;&#20196;&#25552;&#31034;&#26159;&#21542;&#26377;&#21161;&#20110;&#25913;&#21892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25163;&#21160;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;Super-NaturalInstructions&#25968;&#25454;&#38598;&#30340;132&#20010;&#19981;&#21516;&#20219;&#21153;&#30340;&#20266;&#20195;&#30721;&#25552;&#31034;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20998;&#31867;&#12289;QA&#21644;&#29983;&#25104;&#35821;&#35328;&#20219;&#21153;&#12290;&#20351;&#29992;&#36825;&#20123;&#20266;&#20195;&#30721;&#25552;&#31034;&#20197;&#21450;&#23427;&#20204;&#30340;&#33258;&#28982;&#35821;&#35328;&#23545;&#24212;&#29289;&#65292;&#22312;&#20004;&#20010;LLM&#23478;&#26063;-BLOOM&#21644;CodeGen&#19978;&#30740;&#31350;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#20266;&#20195;&#30721;&#25351;&#20196;&#25552;&#31034;&#20250;&#24102;&#26469;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#23545;&#20110;&#20998;&#31867;&#20219;&#21153;&#65292;F1&#20998;&#25968;&#24179;&#22343;&#22686;&#21152;&#65288;&#32477;&#23545;&#20540;&#65289;7-16&#20998;&#65292;&#30456;&#23545;&#25913;&#21892;12-38%&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompting with natural language instructions has recently emerged as a popular method of harnessing the capabilities of large language models. Given the inherent ambiguity present in natural language, it is intuitive to consider the possible advantages of prompting with less ambiguous prompt styles, such as the use of pseudo-code.  In this paper we explore if prompting via pseudo-code instructions helps improve the performance of pre-trained language models. We manually create a dataset of pseudo-code prompts for 132 different tasks spanning classification, QA and generative language tasks, sourced from the Super-NaturalInstructions dataset. Using these prompts along with their counterparts in natural language, we study their performance on two LLM families - BLOOM and CodeGen. Our experiments show that using pseudo-code instructions leads to better results, with an average increase (absolute) of 7-16 points in F1 scores for classification tasks and an improvement (relative) of 12-38% 
&lt;/p&gt;</description></item><item><title>TrueTeacher&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26469;&#36827;&#34892;&#20107;&#23454;&#19968;&#33268;&#24615;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;TrueTeacher&#19981;&#20381;&#36182;&#20110;&#20154;&#24037;&#32534;&#20889;&#30340;&#25688;&#35201;&#65292;&#22810;&#35821;&#35328;&#29305;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11171</link><description>&lt;p&gt;
TrueTeacher: &#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#20107;&#23454;&#19968;&#33268;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
TrueTeacher: Learning Factual Consistency Evaluation with Large Language Models. (arXiv:2305.11171v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11171
&lt;/p&gt;
&lt;p&gt;
TrueTeacher&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26469;&#36827;&#34892;&#20107;&#23454;&#19968;&#33268;&#24615;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;TrueTeacher&#19981;&#20381;&#36182;&#20110;&#20154;&#24037;&#32534;&#20889;&#30340;&#25688;&#35201;&#65292;&#22810;&#35821;&#35328;&#29305;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#23454;&#19968;&#33268;&#24615;&#35780;&#20272;&#36890;&#24120;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#27169;&#22411;&#36827;&#34892;&#65292;&#28982;&#32780;&#36825;&#20123;&#27169;&#22411;&#22312;&#35780;&#20272;&#25688;&#35201;&#26102;&#21462;&#24471;&#30340;&#25104;&#21151;&#26377;&#38480;&#12290;&#20197;&#24448;&#30340;&#24037;&#20316;&#36890;&#36807;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#25913;&#36827;&#20102;&#27492;&#31867;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#36890;&#24120;&#22522;&#20110;&#25200;&#21160;&#30340;&#20154;&#24037;&#32534;&#20889;&#25688;&#35201;&#65292;&#19982;&#30495;&#23454;&#30340;&#27169;&#22411;&#29983;&#25104;&#25688;&#35201;&#22312;&#29305;&#24615;&#19978;&#24120;&#24120;&#23384;&#22312;&#24046;&#24322;&#65292;&#24182;&#19988;&#23545;&#21487;&#33021;&#23384;&#22312;&#30340;&#20107;&#23454;&#38169;&#35823;&#30340;&#35206;&#30422;&#33539;&#22260;&#26377;&#38480;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26368;&#36817;&#22312;&#30452;&#25509;&#35780;&#20272;&#29983;&#25104;&#20219;&#21153;&#26041;&#38754;&#26174;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#35745;&#31639;&#24320;&#38144;&#36807;&#22823;&#65292;&#26080;&#27861;&#23454;&#38469;&#24212;&#29992;&#12290;&#20986;&#20110;&#23545;&#36825;&#20123;&#38480;&#21046;&#30340;&#21160;&#26426;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;TrueTeacher&#65292;&#19968;&#31181;&#20351;&#29992;LLM&#27880;&#37322;&#22810;&#26679;&#30340;&#27169;&#22411;&#29983;&#25104;&#25688;&#35201;&#26469;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;TrueTeacher&#19981;&#20381;&#36182;&#20110;&#20154;&#24037;&#32534;&#20889;&#30340;&#25688;&#35201;&#65292;&#24182;&#19988;&#20855;&#26377;&#22810;&#35821;&#35328;&#29305;&#24615;&#12290;&#22312;TRUE&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#35757;&#32451;&#30340;&#23398;&#29983;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#36828;&#36828;&#36229;&#36807;&#20102;NLI&#27169;&#22411;&#21644;&#20043;&#21069;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Factual consistency evaluation is often conducted using Natural Language Inference (NLI) models, yet these models exhibit limited success in evaluating summaries. Previous work improved such models with synthetic training data. However, the data is typically based on perturbed human-written summaries, which often differ in their characteristics from real model-generated summaries and have limited coverage of possible factual errors. Alternatively, large language models (LLMs) have recently shown promising results in directly evaluating generative tasks, but are too computationally expensive for practical use. Motivated by these limitations, we introduce TrueTeacher, a method for generating synthetic data by annotating diverse model-generated summaries using a LLM. Unlike prior work, TrueTeacher does not rely on human-written summaries, and is multilingual by nature. Experiments on the TRUE benchmark show that a student model trained using our data, substantially outperforms both the st
&lt;/p&gt;</description></item><item><title>CooK&#26159;&#19968;&#31181;&#29992;&#20110;&#36171;&#33021;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#36890;&#36807;&#19987;&#38376;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#21327;&#20316;&#30340;&#30693;&#35782;&#36129;&#29486;&#32773;&#65292;&#25552;&#20379;&#27169;&#22359;&#21270;&#12289;&#19981;&#26029;&#22686;&#38271;&#21644;&#22810;&#28304;&#30340;&#30693;&#35782;&#12290;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#65292;CooK&#23637;&#29616;&#20986;&#20102;&#26126;&#26174;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.09955</link><description>&lt;p&gt;
CooK: &#29992;&#27169;&#22359;&#21270;&#21644;&#21327;&#20316;&#30693;&#35782;&#36171;&#33021;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CooK: Empowering General-Purpose Language Models with Modular and Collaborative Knowledge. (arXiv:2305.09955v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09955
&lt;/p&gt;
&lt;p&gt;
CooK&#26159;&#19968;&#31181;&#29992;&#20110;&#36171;&#33021;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#36890;&#36807;&#19987;&#38376;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#21327;&#20316;&#30340;&#30693;&#35782;&#36129;&#29486;&#32773;&#65292;&#25552;&#20379;&#27169;&#22359;&#21270;&#12289;&#19981;&#26029;&#22686;&#38271;&#21644;&#22810;&#28304;&#30340;&#30693;&#35782;&#12290;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#65292;CooK&#23637;&#29616;&#20986;&#20102;&#26126;&#26174;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#21644;&#35821;&#22659;&#20013;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#26816;&#32034;&#25110;&#29983;&#25104;&#30693;&#35782;&#25552;&#31034;&#26469;&#25913;&#21892;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#26410;&#33021;&#21453;&#26144;&#30693;&#35782;&#20016;&#23500;&#27169;&#22411;&#30340;&#20004;&#20010;&#20851;&#38190;&#23646;&#24615;&#65306;&#30693;&#35782;&#24212;&#35813;&#26159;&#27169;&#22359;&#21270;&#65292;&#19981;&#26029;&#22686;&#38271;&#65292;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#65307;&#30693;&#35782;&#33719;&#21462;&#21644;&#29983;&#25104;&#24212;&#35813;&#26159;&#21327;&#20316;&#30340;&#36807;&#31243;&#65292;&#20854;&#20013;&#21508;&#31181;&#21033;&#30410;&#30456;&#20851;&#32773; contribue &#26032;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; CooK&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21487;&#20026;&#36890;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#27169;&#22359;&#21270;&#21644;&#21327;&#20316;&#26469;&#28304;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#19987;&#38376;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21363;&#22312;&#24191;&#27867;&#39046;&#22495;&#21644;&#26469;&#28304;&#19978;&#35757;&#32451;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#12290;&#36825;&#20123;&#19987;&#38376;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#21442;&#25968;&#21270;&#30340;&#30693;&#35782;&#24211;&#65292;&#21518;&#26469;&#34987;&#25552;&#31034;&#29983;&#25104;&#36890;&#29992;&#30340; LLM &#30340;&#32972;&#26223;&#30693;&#35782;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#30693;&#35782;&#36807;&#28388;&#22120;&#65292;&#20197;&#21160;&#24577;&#36873;&#25321;&#36866;&#21512;&#32473;&#23450;&#19978;&#19979;&#25991;&#30340;&#30693;&#35782;&#28304;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21576;&#29616;&#20102;&#19968;&#20010;&#30693;&#35782;&#36129;&#29486;&#32773;&#32452;&#20214;&#65292;&#20351;&#21033;&#30410;&#30456;&#20851;&#32773;&#33021;&#22815;&#36731;&#26494;&#22320;&#20026;&#31995;&#32479;&#36129;&#29486;&#29305;&#23450;&#20110;&#22495;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; CooK &#22312;&#19968;&#32452;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#26174;&#31034;&#20986;&#26126;&#26174;&#30340;&#36229;&#36234;&#29616;&#26377;&#25216;&#26415;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are increasingly adopted for knowledge-intensive tasks and contexts. Existing approaches improve the knowledge capabilities of general-purpose LLMs through retrieval or generated knowledge prompting, but they fall short of reflecting two key properties of knowledge-rich models: knowledge should be modular, ever-growing, sourced from diverse domains; knowledge acquisition and production should be a collaborative process, where diverse stakeholders contribute new information. To this end, we propose CooK, a novel framework to empower general-purpose large language models with modular and collaboratively sourced knowledge. We first introduce specialized language models, autoregressive models trained on corpora from a wide range of domains and sources. These specialized LMs serve as parametric knowledge repositories that are later prompted to generate background knowledge for general-purpose LLMs. We then propose three knowledge filters to dynamically select an
&lt;/p&gt;</description></item><item><title>FactKB&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#20107;&#23454;&#24615;&#35780;&#20272;&#26041;&#27861;&#65292;&#20351;&#29992;&#22686;&#24378;&#20102;&#20107;&#23454;&#30693;&#35782;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#25688;&#35201;&#29983;&#25104;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.08281</link><description>&lt;p&gt;
FactKB: &#20351;&#29992;&#22686;&#24378;&#20102;&#20107;&#23454;&#30693;&#35782;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21487;&#25512;&#24191;&#20107;&#23454;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
FactKB: Generalizable Factuality Evaluation using Language Models Enhanced with Factual Knowledge. (arXiv:2305.08281v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08281
&lt;/p&gt;
&lt;p&gt;
FactKB&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#20107;&#23454;&#24615;&#35780;&#20272;&#26041;&#27861;&#65292;&#20351;&#29992;&#22686;&#24378;&#20102;&#20107;&#23454;&#30693;&#35782;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#25688;&#35201;&#29983;&#25104;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#33258;&#21160;&#29983;&#25104;&#30340;&#25688;&#35201;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#23545;&#20110;&#21487;&#38752;&#30340;&#25688;&#35201;&#31995;&#32479;&#30340;&#36827;&#27493;&#21644;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#30340;&#20107;&#23454;&#24615;&#35780;&#20272;&#27169;&#22411;&#20173;&#28982;&#19981;&#22815;&#31283;&#20581;&#65292;&#22312;&#26032;&#30340;&#39046;&#22495;&#20013;&#29305;&#21035;&#23481;&#26131;&#20986;&#29616;&#23454;&#20307;&#21644;&#20851;&#31995;&#38169;&#35823;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26032;&#26041;&#27861;FactKB&#65292;&#29992;&#20110;&#20107;&#23454;&#24615;&#35780;&#20272;&#65292;&#23427;&#21487;&#20197;&#22312;&#19981;&#21516;&#39046;&#22495;&#36890;&#29992;&#65292;&#23588;&#20854;&#26159;&#22312;&#23454;&#20307;&#21644;&#20851;&#31995;&#26041;&#38754;&#12290;FactKB&#22522;&#20110;&#20351;&#29992;&#20174;&#22806;&#37096;&#30693;&#35782;&#24211;&#20013;&#25552;&#21462;&#30340;&#20107;&#23454;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#30452;&#25509;&#23454;&#20307;&#20107;&#23454;&#12289;&#22522;&#20110;&#23454;&#20307;&#36741;&#21161;&#30693;&#35782;&#30340;&#20107;&#23454;&#20197;&#21450;&#36890;&#36807;&#30693;&#35782;&#24211;&#36335;&#24452;&#32452;&#21512;&#26500;&#24314;&#30340;&#20107;&#23454;&#19977;&#31181;&#20114;&#34917;&#30340;&#20107;&#23454;&#39044;&#35757;&#32451;&#30446;&#26631;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#24471;&#21040;&#30340;&#20107;&#23454;&#24615;&#35780;&#20272;&#27169;&#22411;&#22312;&#20004;&#20010;&#39046;&#22495;&#20869;&#30340;&#26032;&#38395;&#25688;&#35201;&#22522;&#20934;&#20197;&#21450;&#19977;&#20010;&#39046;&#22495;&#22806;&#30340;&#31185;&#23398;&#25991;&#29486;&#25968;&#25454;&#38598;&#19978;&#37117;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating the factual consistency of automatically generated summaries is essential for the progress and adoption of reliable summarization systems. Despite recent advances, existing factuality evaluation models are not robust, being especially prone to entity and relation errors in new domains. We propose FactKB, a simple new approach to factuality evaluation that is generalizable across domains, in particular with respect to entities and relations. FactKB is based on language models pretrained using facts extracted from external knowledge bases. We introduce three types of complementary factuality pretraining objectives based on direct entity facts, facts grounded in auxiliary knowledge about entities, and facts constructed compositionally through knowledge base walks. The resulting factuality evaluation model achieves state-of-the-art performance on two in-domain news summarization benchmarks as well as on three out-of-domain scientific literature datasets. Further analysis of Fact
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#31574;&#30053;&#65292;&#22312;&#34507;&#30333;&#36136;&#24314;&#27169;&#20013;&#24212;&#29992;transformer&#21644;&#22270;&#21367;&#31215;&#30340;&#32467;&#26500;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#36827;&#19968;&#27493;&#35757;&#32451;&#21518;&#33021;&#22815;&#35774;&#35745;&#20855;&#26377;&#29305;&#23450;&#24615;&#36136;&#30340;&#34507;&#30333;&#36136;&#65292;&#26696;&#20363;&#39564;&#35777;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#29983;&#25104;&#29702;&#24819;&#30446;&#26631;&#24615;&#36136;&#30340;&#34507;&#30333;&#36136;&#12290;</title><link>http://arxiv.org/abs/2305.04934</link><description>&lt;p&gt;
&#24212;&#29992;&#22522;&#20110;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#33258;&#22238;&#24402;Transformer&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#20998;&#26512;&#21644;&#21457;&#29616;&#26032;&#22411;&#34507;&#30333;&#36136;
&lt;/p&gt;
&lt;p&gt;
Generative Pretrained Autoregressive Transformer Graph Neural Network applied to the Analysis and Discovery of Novel Proteins. (arXiv:2305.04934v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#31574;&#30053;&#65292;&#22312;&#34507;&#30333;&#36136;&#24314;&#27169;&#20013;&#24212;&#29992;transformer&#21644;&#22270;&#21367;&#31215;&#30340;&#32467;&#26500;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#36827;&#19968;&#27493;&#35757;&#32451;&#21518;&#33021;&#22815;&#35774;&#35745;&#20855;&#26377;&#29305;&#23450;&#24615;&#36136;&#30340;&#34507;&#30333;&#36136;&#65292;&#26696;&#20363;&#39564;&#35777;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#29983;&#25104;&#29702;&#24819;&#30446;&#26631;&#24615;&#36136;&#30340;&#34507;&#30333;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25253;&#36947;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#31574;&#30053;&#65292;&#24212;&#29992;&#20110;&#35299;&#20915;&#34507;&#30333;&#36136;&#24314;&#27169;&#20013;&#30340;&#27491;&#21521;&#21644;&#21453;&#21521;&#38382;&#39064;&#65292;&#20351;&#29992;&#19968;&#20010;&#25972;&#21512;&#20102;transformer&#21644;&#22270;&#21367;&#31215;&#30340;&#27880;&#24847;&#21147;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#22312;&#22240;&#26524;&#22810;&#22836;&#22270;&#26426;&#21046;&#20013;&#23454;&#29616;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#34987;&#29992;&#20110;&#39044;&#27979;&#20108;&#32423;&#32467;&#26500;&#20869;&#23481;&#65288;&#27599;&#20010;&#27531;&#22522;&#30340;&#27700;&#24179;&#21644;&#24635;&#20307;&#20869;&#23481;&#65289;&#12289;&#34507;&#30333;&#36136;&#21487;&#28342;&#24615;&#21644;&#27979;&#24207;&#20219;&#21153;&#12290;&#36827;&#19968;&#27493;&#22312;&#21453;&#21521;&#20219;&#21153;&#19978;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#35774;&#35745;&#20855;&#26377;&#36825;&#20123;&#24615;&#36136;&#20316;&#20026;&#30446;&#26631;&#29305;&#24449;&#30340;&#34507;&#30333;&#36136;&#12290;&#35813;&#27169;&#22411;&#34987;&#21046;&#23450;&#20026;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#23436;&#20840;&#22522;&#20110;&#25552;&#31034;&#65292;&#21487;&#20197;&#20026;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#36866;&#24212;&#12290;&#25105;&#20204;&#21457;&#29616;&#28155;&#21152;&#39069;&#22806;&#20219;&#21153;&#20250;&#20135;&#29983;&#30456;&#20114;&#21327;&#21516;&#20316;&#29992;&#65292;&#20351;&#27169;&#22411;&#22312;&#25972;&#20307;&#24615;&#33021;&#19978;&#24471;&#21040;&#25552;&#39640;&#65292;&#36229;&#36807;&#20165;&#22312;&#27599;&#20010;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#27169;&#22411;&#30340;&#21487;&#33021;&#24615;&#12290;&#26696;&#20363;&#30740;&#31350;&#29992;&#20110;&#39564;&#35777;&#35813;&#26041;&#27861;&#65292;&#29983;&#25104;&#20855;&#26377;&#29702;&#24819;&#30446;&#26631;&#24615;&#36136;&#65292;&#21253;&#25324;&#31283;&#23450;&#24615;&#21644;&#21487;&#28342;&#24615;&#30340;&#34507;&#30333;&#36136;&#65292;&#24182;&#36827;&#34892;&#23454;&#39564;&#24615;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We report a flexible language-model based deep learning strategy, applied here to solve complex forward and inverse problems in protein modeling, based on an attention neural network that integrates transformer and graph convolutional architectures in a causal multi-headed graph mechanism, to realize a generative pretrained model. The model is applied to predict secondary structure content (per-residue level and overall content), protein solubility, and sequencing tasks. Further trained on inverse tasks, the model is rendered capable of designing proteins with these properties as target features. The model is formulated as a general framework, completely prompt-based, and can be adapted for a variety of downstream tasks. We find that adding additional tasks yields emergent synergies that the model exploits in improving overall performance, beyond what would be possible by training a model on each dataset alone. Case studies are presented to validate the method, yielding protein designs
&lt;/p&gt;</description></item><item><title>&#22312;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#21644; beam search &#30340;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#25913;&#36827;&#25552;&#31034;&#65292;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.03495</link><description>&lt;p&gt;
&#22522;&#20110;&#8220;&#26799;&#24230;&#19979;&#38477;&#8221;&#19982; beam search &#30340;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Automatic Prompt Optimization with "Gradient Descent" and Beam Search. (arXiv:2305.03495v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03495
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#21644; beam search &#30340;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#25913;&#36827;&#25552;&#31034;&#65292;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#36890;&#29992;&#26234;&#33021;&#26041;&#38754;&#23637;&#29616;&#20102;&#20986;&#33394;&#24615;&#33021;&#65292;&#20294;&#20854;&#33021;&#21147;&#20173;&#39640;&#24230;&#20381;&#36182;&#20110;&#25163;&#20889;&#30340;&#25552;&#31034;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35797;&#38169;&#23581;&#35797;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#38750;&#21442;&#25968;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#8212;&#8212;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#65288;APO&#65289;&#65292;&#20854;&#28789;&#24863;&#26469;&#33258;&#20110;&#20351;&#29992;&#25968;&#20540;&#26799;&#24230;&#19979;&#38477;&#33258;&#21160;&#25913;&#36827;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown impressive performance as general purpose agents, but their abilities remain highly dependent on prompts which are hand written with onerous trial-and-error effort. We propose a simple and nonparametric solution to this problem, Automatic Prompt Optimization (APO), which is inspired by numerical gradient descent to automatically improve prompts, assuming access to training data and an LLM API. The algorithm uses minibatches of data to form natural language ``gradients'' that criticize the current prompt. The gradients are then ``propagated'' into the prompt by editing the prompt in the opposite semantic direction of the gradient. These gradient descent steps are guided by a beam search and bandit selection procedure which significantly improves algorithmic efficiency. Preliminary results across three benchmark NLP tasks and the novel problem of LLM jailbreak detection suggest that Automatic Prompt Optimization can outperform prior prompt editing 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;PEFT-Ref&#21442;&#32771;&#26550;&#26500;&#65292;&#26631;&#20934;&#21270;&#20102;&#19981;&#21516;PEFT&#25216;&#26415;&#20849;&#20139;&#30340;&#26041;&#38754;&#65292;&#38548;&#31163;&#20102;&#24046;&#24322;&#21040;&#29305;&#23450;&#20301;&#32622;&#21644;&#20132;&#20114;&#20013;&#65292;&#27169;&#22359;&#21270;&#30340;&#35270;&#22270;&#26377;&#21161;&#20110;&#27604;&#36739;&#19981;&#21516;&#25216;&#26415;&#21450;&#20854;&#25928;&#29575;&#21644;&#20219;&#21153;&#24615;&#33021;&#65292;&#24182;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;PEFT&#30340;&#22522;&#26412;&#21407;&#29702;&#12290;</title><link>http://arxiv.org/abs/2304.12410</link><description>&lt;p&gt;
PEFT-Ref: &#19968;&#31181;&#27169;&#22359;&#21270;&#30340;&#21442;&#32771;&#26550;&#26500;&#21644;&#31867;&#22411;&#65292;&#29992;&#20110;&#21442;&#25968;&#25928;&#29575;&#24494;&#35843;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
PEFT-Ref: A Modular Reference Architecture and Typology for Parameter-Efficient Finetuning Techniques. (arXiv:2304.12410v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;PEFT-Ref&#21442;&#32771;&#26550;&#26500;&#65292;&#26631;&#20934;&#21270;&#20102;&#19981;&#21516;PEFT&#25216;&#26415;&#20849;&#20139;&#30340;&#26041;&#38754;&#65292;&#38548;&#31163;&#20102;&#24046;&#24322;&#21040;&#29305;&#23450;&#20301;&#32622;&#21644;&#20132;&#20114;&#20013;&#65292;&#27169;&#22359;&#21270;&#30340;&#35270;&#22270;&#26377;&#21161;&#20110;&#27604;&#36739;&#19981;&#21516;&#25216;&#26415;&#21450;&#20854;&#25928;&#29575;&#21644;&#20219;&#21153;&#24615;&#33021;&#65292;&#24182;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;PEFT&#30340;&#22522;&#26412;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#21442;&#25968;&#25928;&#29575;&#24494;&#35843;(PEFT)&#25216;&#26415;&#26088;&#22312;&#25913;&#21892;&#23436;&#20840;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLM)&#30340;&#39640;&#26114;&#25104;&#26412;&#12290;&#38543;&#30528;&#19981;&#21516;&#30340;PEFT&#25216;&#26415;&#19981;&#26029;&#20986;&#29616;&#65292;&#23545;&#23427;&#20204;&#36827;&#34892;&#27604;&#36739;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#22312;&#20197;&#19979;&#26041;&#38754;&#65306;(i)&#23427;&#20204;&#28155;&#21152;&#21040;PLM&#30340;&#32467;&#26500;&#21644;&#21151;&#33021;&#65292;(ii)&#19981;&#21516;&#31867;&#22411;&#21644;&#31243;&#24230;&#30340;&#25928;&#29575;&#25913;&#36827;&#65292;(iii)&#22312;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#20197;&#21450;(iv)&#32467;&#26500;&#21644;&#21151;&#33021;&#24046;&#24322;&#22914;&#20309;&#19982;&#25928;&#29575;&#21644;&#20219;&#21153;&#24615;&#33021;&#30456;&#20851;&#32852;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#26679;&#30340;&#27604;&#36739;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21442;&#32771;&#26694;&#26550;&#65292;&#26631;&#20934;&#21270;&#20102;&#19981;&#21516;PEFT&#25216;&#26415;&#20849;&#20139;&#30340;&#26041;&#38754;&#65292;&#21516;&#26102;&#23558;&#24046;&#24322;&#38548;&#31163;&#21040;&#19982;&#26631;&#20934;&#32452;&#20214;&#30340;&#29305;&#23450;&#20301;&#32622;&#21644;&#20132;&#20114;&#20013;&#12290;&#36890;&#36807;&#36825;&#20010;&#26631;&#20934;&#21270;&#21644;&#38548;&#31163;&#24046;&#24322;&#30340;&#36807;&#31243;&#65292;&#20986;&#29616;&#20102;PEFT&#25216;&#26415;&#30340;&#27169;&#22359;&#21270;&#35270;&#22270;&#65292;&#19981;&#20165;&#25903;&#25345;&#30452;&#25509;&#27604;&#36739;&#19981;&#21516;&#25216;&#26415;&#21450;&#20854;&#25928;&#29575;&#21644;&#20219;&#21153;&#24615;&#33021;&#65292;&#32780;&#19988;&#36824;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;PEFT&#30340;&#22522;&#26412;&#21407;&#29702;&#12290;&#25152;&#25552;&#20986;&#30340;&#21442;&#32771;&#26550;&#26500;&#31216;&#20026;PEFT-Ref&#65292;&#21253;&#25324;&#19971;&#20010;&#26680;&#24515;&#27169;&#22359;&#65292;&#27599;&#20010;&#27169;&#22359;&#37117;&#22788;&#29702;PEFT&#30340;&#29305;&#23450;&#26041;&#38754;&#65292;&#24182;&#21487;&#29992;&#20316;&#24320;&#21457;&#26032;PEFT&#25216;&#26415;&#21644;&#27604;&#36739;&#29616;&#26377;&#25216;&#26415;&#30340;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent parameter-efficient finetuning (PEFT) techniques aim to improve over the considerable cost of fully finetuning large pretrained language models (PLM). As different PEFT techniques proliferate, it is becoming difficult to compare them, in particular in terms of (i) the structure and functionality they add to the PLM, (ii) the different types and degrees of efficiency improvements achieved, (iii) performance at different downstream tasks, and (iv) how differences in structure and functionality relate to efficiency and task performance. To facilitate such comparisons, this paper presents a reference framework which standardises aspects shared by different PEFT techniques, while isolating differences to specific locations and interactions with the standard components. Through this process of standardising and isolating differences, a modular view of PEFT techniques emerges, supporting not only direct comparison of different techniques and their efficiency and task performance, but a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35789;&#20041;&#28040;&#27495;&#26469;&#35299;&#20915;&#22810;&#35789;&#34920;&#36798;&#24335;&#35782;&#21035;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#20351;&#29992;&#35789;&#20041;&#30340;&#25551;&#36848;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#36807;&#28388;&#20505;&#36873;&#39033;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#31934;&#30830;&#24230;&#12290;&#30740;&#31350;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#21452;&#32534;&#30721;&#22120;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;MWE&#35782;&#21035;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.06623</link><description>&lt;p&gt;
MWE&#20316;&#20026;WSD&#65306;&#29992;&#35789;&#20041;&#28040;&#27495;&#35299;&#20915;&#22810;&#35789;&#34920;&#36798;&#24335;&#35782;&#21035;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
MWE as WSD: Solving Multiword Expression Identification with Word Sense Disambiguation. (arXiv:2303.06623v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06623
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35789;&#20041;&#28040;&#27495;&#26469;&#35299;&#20915;&#22810;&#35789;&#34920;&#36798;&#24335;&#35782;&#21035;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#20351;&#29992;&#35789;&#20041;&#30340;&#25551;&#36848;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#36807;&#28388;&#20505;&#36873;&#39033;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#31934;&#30830;&#24230;&#12290;&#30740;&#31350;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#21452;&#32534;&#30721;&#22120;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;MWE&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#35789;&#20041;&#28040;&#27495;&#65288;WSD&#65289;&#26041;&#27861;&#21033;&#29992;&#35789;&#20041;&#30340;&#25551;&#36848;&#65288;&#23450;&#20041;&#65289;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#20351;&#29992;&#36755;&#20837;&#19978;&#19979;&#25991;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#22810;&#35789;&#34920;&#36798;&#24335;&#65288;MWE&#65289;&#30340;&#35782;&#21035;&#65292;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#20351;&#29992;&#25551;&#36848;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#36807;&#28388;&#22522;&#20110;&#35268;&#21017;&#30340;&#25552;&#21462;&#27969;&#31243;&#20135;&#29983;&#30340;MWE&#20505;&#36873;&#39033;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#31934;&#30830;&#24230;&#65292;&#22312;DiMSUM&#25968;&#25454;&#38598;&#19978;&#30340;MWE&#35782;&#21035;&#20013;&#36229;&#36807;&#20102;&#29616;&#26377;&#25216;&#26415;1.9&#20010;F1&#20998;&#25968;&#65292;&#24182;&#22312;PARSEME 1.1&#33521;&#35821;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#31454;&#20105;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#20445;&#30041;&#20102;&#22823;&#37096;&#20998;WSD&#24615;&#33021;&#65292;&#34920;&#26126;&#19968;&#20010;&#27169;&#22411;&#21487;&#20197;&#21516;&#26102;&#29992;&#20110;&#20004;&#20010;&#20219;&#21153;&#12290;&#26368;&#21518;&#65292;&#22522;&#20110;&#31867;&#20284;&#20110;&#29992;&#20110;WSD&#30340;&#21452;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Poly-encoder&#26550;&#26500;&#65292;&#25552;&#39640;&#20102;MWE&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent approaches to word sense disambiguation (WSD) utilize encodings of the sense gloss (definition), in addition to the input context, to improve performance. In this work we demonstrate that this approach can be adapted for use in multiword expression (MWE) identification by training models which use gloss and context information to filter MWE candidates produced by a rule-based extraction pipeline. Our approach substantially improves precision, outperforming the state-of-the-art in MWE identification on the DiMSUM dataset by up to 1.9 F1 points and achieving competitive results on the PARSEME 1.1 English dataset. Our models also retain most of their WSD performance, showing that a single model can be used for both tasks. Finally, building on similar approaches using Bi-encoders for WSD, we introduce a novel Poly-encoder architecture which improves MWE identification performance.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#38024;&#23545;&#23545;&#27604;&#23398;&#20064;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#27979;&#35797;&#26102;&#20998;&#24067;&#24402;&#19968;&#21270;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#24120;&#35265;&#30340;&#28857;&#20056;&#25805;&#20316;&#23548;&#33268;&#27979;&#35797;&#26102;&#20449;&#24687;&#20002;&#22833;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#27979;&#35797;&#38454;&#27573;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2302.11084</link><description>&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#27979;&#35797;&#26102;&#20998;&#24067;&#24402;&#19968;&#21270;
&lt;/p&gt;
&lt;p&gt;
Test-Time Distribution Normalization for Contrastively Learned Vision-language Models. (arXiv:2302.11084v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11084
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#38024;&#23545;&#23545;&#27604;&#23398;&#20064;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#27979;&#35797;&#26102;&#20998;&#24067;&#24402;&#19968;&#21270;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#24120;&#35265;&#30340;&#28857;&#20056;&#25805;&#20316;&#23548;&#33268;&#27979;&#35797;&#26102;&#20449;&#24687;&#20002;&#22833;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#27979;&#35797;&#38454;&#27573;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#23545;&#27604;&#23398;&#20064;&#30340;&#36827;&#23637;&#20351;&#24471;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#22320;&#23545;&#22270;&#20687;&#21644;&#25991;&#26412;&#34920;&#31034;&#36827;&#34892;&#28857;&#20056;&#26469;&#39640;&#25928;&#20934;&#30830;&#22320;&#36827;&#34892;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#20195;&#34920;&#24615;&#26041;&#27861;&#20043;&#19968;&#26159;CLIP&#65292;&#30001;&#20110;&#20854;&#26377;&#25928;&#24615;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#37319;&#29992;&#12290;CLIP&#20351;&#29992;InfoNCE&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#25439;&#22833;&#21516;&#26102;&#32771;&#34385;&#20102;&#27491;&#26679;&#26412;&#21644;&#36127;&#26679;&#26412;&#65292;&#20197;&#24110;&#21161;&#23398;&#20064;&#26356;&#21152;&#31283;&#20581;&#30340;&#34920;&#31034;&#31354;&#38388;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#24120;&#35265;&#30340;&#19979;&#28216;&#23454;&#36341;&#8212;&#8212;&#36827;&#34892;&#28857;&#20056;&#20165;&#20165;&#26159;&#23545;&#20248;&#21270;&#30446;&#26631;&#30340;&#38646;&#38454;&#36817;&#20284;&#65292;&#23548;&#33268;&#20102;&#27979;&#35797;&#26102;&#20449;&#24687;&#30340;&#20002;&#22833;&#12290;&#30452;&#35266;&#19978;&#65292;&#30001;&#20110;&#27169;&#22411;&#26159;&#22522;&#20110;InfoNCE&#25439;&#22833;&#36827;&#34892;&#20248;&#21270;&#30340;&#65292;&#27979;&#35797;&#26102;&#30340;&#36807;&#31243;&#20063;&#24212;&#35813;&#20445;&#25345;&#19968;&#33268;&#12290;&#38382;&#39064;&#22312;&#20110;&#22914;&#20309;&#20197;&#19968;&#31181;&#35745;&#31639;&#39640;&#25928;&#30340;&#26041;&#24335;&#26816;&#32034;&#21040;&#20219;&#20309;&#36127;&#26679;&#26412;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#35797;&#26102;&#20998;&#24067;&#24402;&#19968;&#21270;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Advances in the field of vision-language contrastive learning have made it possible for many downstream applications to be carried out efficiently and accurately by simply taking the dot product between image and text representations. One of the most representative approaches proposed recently known as CLIP has garnered widespread adoption due to its effectiveness. CLIP is trained with an InfoNCE loss that takes into account both positive and negative samples to help learn a much more robust representation space. This paper reveals that the common downstream practice of taking a dot product is only a zeroth-order approximation of the optimization goal, resulting in a loss of information during test-time. Intuitively, since the model has been optimized based on the InfoNCE loss, test-time procedures should also be in alignment. The question lies in how one can retrieve any semblance of negative samples information during inference in a computationally efficient way. To this end, we prop
&lt;/p&gt;</description></item><item><title>PK-ICR&#26159;&#19968;&#31181;&#22522;&#20110;&#35282;&#33394;&#21644;&#30693;&#35782;&#30340;&#20114;&#21160;&#19978;&#19979;&#25991;&#26816;&#32034;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22797;&#26434;&#30340;&#22810;&#22330;&#26223;&#23545;&#35805;&#20013;&#21516;&#26102;&#35782;&#21035;&#35282;&#33394;&#21644;&#30693;&#35782;&#12290;&#36890;&#36807;&#21033;&#29992;&#31070;&#32463;&#38382;&#31572;&#26816;&#32034;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#36739;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#19979;&#23454;&#29616;&#26816;&#32034;&#65292;&#24182;&#19988;&#36890;&#36807;&#24341;&#20837;&#31354;-&#27491;&#21521;&#25490;&#21517;&#27979;&#35797;&#26041;&#27861;&#26469;&#25552;&#39640;&#25490;&#21517;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.06674</link><description>&lt;p&gt;
PK-ICR: &#22522;&#20110;&#35282;&#33394;&#21644;&#30693;&#35782;&#30340;&#20114;&#21160;&#19978;&#19979;&#25991;&#26816;&#32034;&#36827;&#34892;&#22522;&#20110;&#22330;&#26223;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
PK-ICR: Persona-Knowledge Interactive Context Retrieval for Grounded Dialogue. (arXiv:2302.06674v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06674
&lt;/p&gt;
&lt;p&gt;
PK-ICR&#26159;&#19968;&#31181;&#22522;&#20110;&#35282;&#33394;&#21644;&#30693;&#35782;&#30340;&#20114;&#21160;&#19978;&#19979;&#25991;&#26816;&#32034;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22797;&#26434;&#30340;&#22810;&#22330;&#26223;&#23545;&#35805;&#20013;&#21516;&#26102;&#35782;&#21035;&#35282;&#33394;&#21644;&#30693;&#35782;&#12290;&#36890;&#36807;&#21033;&#29992;&#31070;&#32463;&#38382;&#31572;&#26816;&#32034;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#36739;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#19979;&#23454;&#29616;&#26816;&#32034;&#65292;&#24182;&#19988;&#36890;&#36807;&#24341;&#20837;&#31354;-&#27491;&#21521;&#25490;&#21517;&#27979;&#35797;&#26041;&#27861;&#26469;&#25552;&#39640;&#25490;&#21517;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#21035;&#19982;&#23545;&#35805;&#31995;&#32479;&#30456;&#20851;&#30340;&#35282;&#33394;&#21644;&#30693;&#35782;&#23545;&#20110;&#22522;&#20110;&#22330;&#26223;&#30340;&#23545;&#35805;&#24212;&#31572;&#29983;&#25104;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27599;&#20010;&#23545;&#35805;&#22522;&#26412;&#19978;&#37117;&#26159;&#23396;&#31435;&#30740;&#31350;&#30340;&#65292;&#32780;&#26368;&#36817;&#30340;&#24037;&#20316;&#20013;&#24341;&#20837;&#20102;&#26356;&#23454;&#38469;&#30340;&#22810;&#22330;&#26223;&#23545;&#35805;&#20219;&#21153;&#12290;&#25105;&#20204;&#23558;&#35282;&#33394;&#21644;&#30693;&#35782;&#21452;&#19978;&#19979;&#25991;&#35782;&#21035;&#23450;&#20041;&#20026;&#20026;&#32473;&#23450;&#30340;&#23545;&#35805;&#21516;&#26102;&#35782;&#21035;&#35282;&#33394;&#21644;&#30693;&#35782;&#30340;&#20219;&#21153;&#65292;&#22312;&#22797;&#26434;&#30340;&#22810;&#22330;&#26223;&#23545;&#35805;&#35774;&#32622;&#20013;&#21487;&#33021;&#20855;&#26377;&#25552;&#21319;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26816;&#32034;&#30340;&#26816;&#32034;&#26041;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#21033;&#29992;&#23545;&#35805;&#30340;&#25152;&#26377;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#38382;&#31572;&#26816;&#32034;&#27169;&#22411;&#65292;&#38656;&#35201;&#36739;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31354;-&#27491;&#21521;&#25490;&#21517;&#27979;&#35797;&#26041;&#27861;&#65292;&#29992;&#20110;&#34913;&#37327;&#19982;&#25968;&#25454;&#22686;&#24378;&#30456;&#20851;&#30340;&#35821;&#20041;&#24046;&#24322;&#26679;&#26412;&#65288;&#21363;&#22256;&#38590;&#36127;&#26679;&#26412;&#65289;&#30340;&#25490;&#21517;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying relevant persona or knowledge for conversational systems is critical to grounded dialogue response generation. However, each grounding has been mostly researched in isolation with more practical multi-context dialogue tasks introduced in recent works. We define Persona and Knowledge Dual Context Identification as the task to identify persona and knowledge jointly for a given dialogue, which could be of elevated importance in complex multi-context dialogue settings. We develop a novel grounding retrieval method that utilizes all contexts of dialogue simultaneously. Our method requires less computational power via utilizing neural QA retrieval models. We further introduce our novel null-positive rank test which measures ranking performance on semantically dissimilar samples (i.e. hard negatives) in relation to data augmentation.
&lt;/p&gt;</description></item><item><title>NNKGC&#26159;&#19968;&#31181;&#36890;&#36807;&#33410;&#28857;&#37051;&#23621;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#24182;&#24341;&#20837;&#36793;&#36830;&#25509;&#39044;&#27979;&#20219;&#21153;&#30340;&#26694;&#26550;&#65292;&#31616;&#21333;&#32780;&#26377;&#25928;&#65292;&#21487;&#20197;&#39044;&#27979;&#20986;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.06132</link><description>&lt;p&gt;
NNKGC: &#29992;&#33410;&#28857;&#37051;&#23621;&#25913;&#36827;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
NNKGC: Improving Knowledge Graph Completion with Node Neighborhoods. (arXiv:2302.06132v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06132
&lt;/p&gt;
&lt;p&gt;
NNKGC&#26159;&#19968;&#31181;&#36890;&#36807;&#33410;&#28857;&#37051;&#23621;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#24182;&#24341;&#20837;&#36793;&#36830;&#25509;&#39044;&#27979;&#20219;&#21153;&#30340;&#26694;&#26550;&#65292;&#31616;&#21333;&#32780;&#26377;&#25928;&#65292;&#21487;&#20197;&#39044;&#27979;&#20986;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26088;&#22312;&#21457;&#29616;&#26597;&#35810;&#23454;&#20307;&#30340;&#32570;&#22833;&#20851;&#31995;&#12290;&#30446;&#21069;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#27169;&#22411;&#21033;&#29992;&#23454;&#20307;&#21517;&#31216;&#21644;&#25551;&#36848;&#25512;&#26029;&#22836;&#23454;&#20307;&#21644;&#29305;&#23450;&#20851;&#31995;&#32473;&#23450;&#30340;&#23614;&#23454;&#20307;&#12290;&#29616;&#26377;&#26041;&#27861;&#36824;&#32771;&#34385;&#20102;&#22836;&#23454;&#20307;&#30340;&#37051;&#23621;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#20351;&#29992;&#25153;&#24179;&#32467;&#26500;&#27169;&#25311;&#37051;&#23621;&#65292;&#19988;&#20165;&#38480;&#20110;1&#36339;&#37051;&#23621;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#33410;&#28857;&#37051;&#23621;&#26694;&#26550;&#12290;&#23427;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23545;&#22836;&#23454;&#20307;&#37051;&#23621;&#36827;&#34892;&#22810;&#36339;&#24314;&#27169;&#65292;&#20197;&#20016;&#23500;&#22836;&#33410;&#28857;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#36793;&#36830;&#25509;&#39044;&#27979;&#20219;&#21153;&#26469;&#25913;&#36827;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#12290;&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#31616;&#21333;&#32780;&#26377;&#25928;&#12290;&#26696;&#20363;&#30740;&#31350;&#36824;&#34920;&#26126;&#65292;&#27169;&#22411;&#33021;&#22815;&#39044;&#27979;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph completion (KGC) aims to discover missing relations of query entities. Current text-based models utilize the entity name and description to infer the tail entity given the head entity and a certain relation. Existing approaches also consider the neighborhood of the head entity. However, these methods tend to model the neighborhood using a flat structure and are only restricted to 1-hop neighbors. In this work, we propose a node neighborhood-enhanced framework for knowledge graph completion. It models the head entity neighborhood from multiple hops using graph neural networks to enrich the head node information. Moreover, we introduce an additional edge link prediction task to improve KGC. Evaluation on two public datasets shows that this framework is simple yet effective. The case study also shows that the model is able to predict explainable predictions.
&lt;/p&gt;</description></item><item><title>"IC3: Image Captioning by Committee Consensus"&#24341;&#20837;&#20102;&#19968;&#31181;&#36890;&#36807;&#22996;&#21592;&#20250;&#20849;&#35782;&#29983;&#25104;&#22270;&#20687;&#23383;&#24149;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#22810;&#20010;&#27880;&#37322;&#32773;&#30340;&#35270;&#35282;&#25429;&#25417;&#39640;&#23618;&#32454;&#33410;&#65292;&#20248;&#20110;&#21333;&#20010;&#20154;&#29983;&#25104;&#30340;&#21442;&#32771;&#23383;&#24149;&#65292;&#24182;&#22312;&#35270;&#35273;&#25551;&#36848;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2302.01328</link><description>&lt;p&gt;
IC3&#65306;&#36890;&#36807;&#22996;&#21592;&#20250;&#20849;&#35782;&#36827;&#34892;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
IC3: Image Captioning by Committee Consensus. (arXiv:2302.01328v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01328
&lt;/p&gt;
&lt;p&gt;
"IC3: Image Captioning by Committee Consensus"&#24341;&#20837;&#20102;&#19968;&#31181;&#36890;&#36807;&#22996;&#21592;&#20250;&#20849;&#35782;&#29983;&#25104;&#22270;&#20687;&#23383;&#24149;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#22810;&#20010;&#27880;&#37322;&#32773;&#30340;&#35270;&#35282;&#25429;&#25417;&#39640;&#23618;&#32454;&#33410;&#65292;&#20248;&#20110;&#21333;&#20010;&#20154;&#29983;&#25104;&#30340;&#21442;&#32771;&#23383;&#24149;&#65292;&#24182;&#22312;&#35270;&#35273;&#25551;&#36848;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#20320;&#35831;&#19968;&#20010;&#20154;&#25551;&#36848;&#19968;&#24133;&#22270;&#20687;&#65292;&#20182;&#20204;&#21487;&#33021;&#20250;&#29992;&#19968;&#21315;&#31181;&#19981;&#21516;&#30340;&#26041;&#24335;&#26469;&#25551;&#36848;&#12290;&#20256;&#32479;&#19978;&#65292;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#27169;&#22411;&#34987;&#35757;&#32451;&#25104;&#29983;&#25104;&#19968;&#20010;&#8220;&#26368;&#20339;&#8221;&#65288;&#19982;&#21442;&#32771;&#26368;&#30456;&#20284;&#65289;&#30340;&#22270;&#20687;&#23383;&#24149;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#20570;&#20250;&#40723;&#21169;&#29983;&#25104;&#8220;&#20449;&#24687;&#36139;&#20047;&#8221;&#30340;&#23383;&#24149;&#65292;&#24182;&#19988;&#21482;&#20851;&#27880;&#21487;&#33021;&#32454;&#33410;&#30340;&#19968;&#20010;&#23376;&#38598;&#65292;&#32780;&#24573;&#30053;&#20102;&#22330;&#26223;&#20013;&#20854;&#20182;&#21487;&#33021;&#26377;&#29992;&#30340;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26032;&#39062;&#30340;&#26041;&#27861;&#65306;"&#36890;&#36807;&#22996;&#21592;&#20250;&#20849;&#35782;&#36827;&#34892;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;"&#65288;IC3&#65289;&#65292;&#26088;&#22312;&#29983;&#25104;&#19968;&#20010;&#33021;&#22815;&#20174;&#22810;&#20010;&#27880;&#37322;&#32773;&#30340;&#35270;&#35282;&#25429;&#25417;&#21040;&#39640;&#23618;&#32454;&#33410;&#30340;&#21333;&#20010;&#23383;&#24149;&#12290;&#20154;&#31867;&#35780;&#20215;IC3&#29983;&#25104;&#30340;&#23383;&#24149;&#33267;&#23569;&#19982;&#22522;&#20934;SOTA&#27169;&#22411;&#19968;&#26679;&#26377;&#24110;&#21161;&#30340;&#24773;&#20917;&#21344;&#20102;&#19977;&#20998;&#20043;&#20108;&#20197;&#19978;&#65292;&#24182;&#19988;IC3&#21487;&#20197;&#23558;SOTA&#33258;&#21160;&#21484;&#22238;&#31995;&#32479;&#30340;&#24615;&#33021;&#25552;&#21319;&#39640;&#36798;84%&#65292;&#32988;&#36807;&#21333;&#20010;&#20154;&#29983;&#25104;&#30340;&#21442;&#32771;&#23383;&#24149;&#65292;&#24182;&#26174;&#31034;&#20986;&#22312;&#35270;&#35273;&#25551;&#36848;&#26041;&#38754;&#30456;&#27604;&#20110;SOTA&#26041;&#27861;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;&#20195;&#30721;&#21487;&#36890;&#36807;https://davidmchan&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
If you ask a human to describe an image, they might do so in a thousand different ways. Traditionally, image captioning models are trained to generate a single "best" (most like a reference) image caption. Unfortunately, doing so encourages captions that are "informationally impoverished," and focus on only a subset of the possible details, while ignoring other potentially useful information in the scene. In this work, we introduce a simple, yet novel, method: "Image Captioning by Committee Consensus" (IC3), designed to generate a single caption that captures high-level details from several annotator viewpoints. Humans rate captions produced by IC3 at least as helpful as baseline SOTA models more than two thirds of the time, and IC3 can improve the performance of SOTA automated recall systems by up to 84%, outperforming single human-generated reference captions, and indicating significant improvements over SOTA approaches for visual description. Code is available at https://davidmchan.
&lt;/p&gt;</description></item><item><title>DISSC&#26159;&#19968;&#31181;&#26032;&#39062;&#12289;&#36731;&#37327;&#32423;&#30340;&#35821;&#38899;&#39118;&#26684;&#36716;&#25442;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#20197;&#26080;&#38656;&#25991;&#26412;&#30340;&#26041;&#24335;&#23558;&#24405;&#38899;&#30340;&#33410;&#22863;&#12289;&#38899;&#39640;&#36718;&#24275;&#21644;&#38899;&#33394;&#36716;&#25442;&#20026;&#30446;&#26631;&#35828;&#35805;&#32773;&#30340;&#39118;&#26684;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#33258;&#30417;&#30563;&#27169;&#22411;&#32534;&#30721;&#35821;&#38899;&#20026;&#31163;&#25955;&#21333;&#20803;&#65292;&#20855;&#26377;&#31616;&#21333;&#12289;&#26377;&#25928;&#19988;&#24555;&#36895;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#36866;&#29992;&#20110;&#26080;&#37197;&#23545;&#25968;&#25454;&#30340;&#22810;&#23545;&#22810;&#35821;&#38899;&#36716;&#25442;&#12290;</title><link>http://arxiv.org/abs/2212.09730</link><description>&lt;p&gt;
&#22312;&#27874;&#24418;&#22495;&#20013;&#20351;&#29992;&#31163;&#25955;&#33258;&#30417;&#30563;&#21333;&#20803;&#36827;&#34892;&#35821;&#38899;&#39118;&#26684;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Speaking Style Conversion in the Waveform Domain Using Discrete Self-Supervised Units. (arXiv:2212.09730v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09730
&lt;/p&gt;
&lt;p&gt;
DISSC&#26159;&#19968;&#31181;&#26032;&#39062;&#12289;&#36731;&#37327;&#32423;&#30340;&#35821;&#38899;&#39118;&#26684;&#36716;&#25442;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#20197;&#26080;&#38656;&#25991;&#26412;&#30340;&#26041;&#24335;&#23558;&#24405;&#38899;&#30340;&#33410;&#22863;&#12289;&#38899;&#39640;&#36718;&#24275;&#21644;&#38899;&#33394;&#36716;&#25442;&#20026;&#30446;&#26631;&#35828;&#35805;&#32773;&#30340;&#39118;&#26684;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#33258;&#30417;&#30563;&#27169;&#22411;&#32534;&#30721;&#35821;&#38899;&#20026;&#31163;&#25955;&#21333;&#20803;&#65292;&#20855;&#26377;&#31616;&#21333;&#12289;&#26377;&#25928;&#19988;&#24555;&#36895;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#36866;&#29992;&#20110;&#26080;&#37197;&#23545;&#25968;&#25454;&#30340;&#22810;&#23545;&#22810;&#35821;&#38899;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DISSC&#30340;&#26032;&#39062;&#12289;&#36731;&#37327;&#32423;&#30340;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#20197;&#26080;&#38656;&#25991;&#26412;&#30340;&#26041;&#24335;&#23558;&#24405;&#38899;&#30340;&#33410;&#22863;&#12289;&#38899;&#39640;&#36718;&#24275;&#21644;&#38899;&#33394;&#36716;&#25442;&#20026;&#30446;&#26631;&#35828;&#35805;&#32773;&#30340;&#39118;&#26684;&#12290;&#19982;DISSC&#19981;&#21516;&#65292;&#22823;&#22810;&#25968;&#35821;&#38899;&#36716;&#25442;&#65288;VC&#65289;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#38899;&#33394;&#65292;&#24182;&#24573;&#30053;&#20154;&#20204;&#29420;&#29305;&#30340;&#35828;&#35805;&#39118;&#26684;&#65288;&#38901;&#24459;&#65289;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#27169;&#22411;&#23558;&#35821;&#38899;&#32534;&#30721;&#20026;&#31163;&#25955;&#21333;&#20803;&#65292;&#20351;&#24471;&#35757;&#32451;&#31616;&#21333;&#12289;&#26377;&#25928;&#19988;&#24555;&#36895;&#12290;&#25152;&#26377;&#30340;&#36716;&#25442;&#27169;&#22359;&#20165;&#22312;&#37325;&#24314;&#20219;&#21153;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#22240;&#27492;&#36866;&#29992;&#20110;&#26080;&#37197;&#23545;&#25968;&#25454;&#30340;&#22810;&#23545;&#22810;&#35821;&#38899;&#36716;&#25442;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#22871;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;DISSC&#22312;&#36825;&#20010;&#35774;&#32622;&#19979;&#26126;&#26174;&#20248;&#20110;&#35780;&#20272;&#22522;&#32447;&#12290;&#20195;&#30721;&#21644;&#26679;&#20363;&#21487;&#22312;https://pages.cs.huji.ac.il/adiyoss-lab/dissc/ &#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce DISSC, a novel, lightweight method that converts the rhythm, pitch contour and timbre of a recording to a target speaker in a textless manner. Unlike DISSC, most voice conversion (VC) methods focus primarily on timbre, and ignore people's unique speaking style (prosody). The proposed approach uses a pretrained, self-supervised model for encoding speech to discrete units, which makes it simple, effective, and fast to train. All conversion modules are only trained on reconstruction like tasks, thus suitable for any-to-many VC with no paired data. We introduce a suite of quantitative and qualitative evaluation metrics for this setup, and empirically demonstrate that DISSC significantly outperforms the evaluated baselines. Code and samples are available at https://pages.cs.huji.ac.il/adiyoss-lab/dissc/.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#21644;&#38405;&#35835;&#26694;&#26550;&#26469;&#35299;&#20915;&#29616;&#26377;&#30693;&#35782;&#22270;&#35889;&#38142;&#25509;&#39044;&#27979;&#31995;&#32479;&#30340;&#23616;&#38480;&#24615;&#12290;&#36890;&#36807;&#39318;&#20808;&#26816;&#32034;&#30456;&#20851;&#23376;&#22270;&#19978;&#19979;&#25991;&#65292;&#28982;&#21518;&#20351;&#29992;&#39640;&#23481;&#37327;&#38405;&#35835;&#22120;&#32852;&#21512;&#25512;&#29702;&#19978;&#19979;&#25991;&#21644;&#26597;&#35810;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#25552;&#20379;&#26356;&#26377;&#29992;&#30340;&#20449;&#24687;&#21644;&#26356;&#24378;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.09724</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#38142;&#25509;&#39044;&#27979;&#30340;&#26816;&#32034;&#21644;&#38405;&#35835;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Retrieve-and-Read Framework for Knowledge Graph Link Prediction. (arXiv:2212.09724v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09724
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#21644;&#38405;&#35835;&#26694;&#26550;&#26469;&#35299;&#20915;&#29616;&#26377;&#30693;&#35782;&#22270;&#35889;&#38142;&#25509;&#39044;&#27979;&#31995;&#32479;&#30340;&#23616;&#38480;&#24615;&#12290;&#36890;&#36807;&#39318;&#20808;&#26816;&#32034;&#30456;&#20851;&#23376;&#22270;&#19978;&#19979;&#25991;&#65292;&#28982;&#21518;&#20351;&#29992;&#39640;&#23481;&#37327;&#38405;&#35835;&#22120;&#32852;&#21512;&#25512;&#29702;&#19978;&#19979;&#25991;&#21644;&#26597;&#35810;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#25552;&#20379;&#26356;&#26377;&#29992;&#30340;&#20449;&#24687;&#21644;&#26356;&#24378;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#38142;&#25509;&#39044;&#27979;&#26088;&#22312;&#26681;&#25454;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#29616;&#26377;&#20107;&#23454;&#25512;&#26029;&#20986;&#26032;&#30340;&#20107;&#23454;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#20351;&#29992;&#33410;&#28857;&#30340;&#22270;&#37051;&#22495;&#25552;&#20379;&#20102;&#27604;&#20165;&#20351;&#29992;&#26597;&#35810;&#20449;&#24687;&#26356;&#26377;&#29992;&#30340;&#20449;&#24687;&#12290;&#20256;&#32479;&#30340;KG&#38142;&#25509;&#39044;&#27979;&#30340;GNNs&#36981;&#24490;&#25972;&#20010;KG&#19978;&#30340;&#26631;&#20934;&#28040;&#24687;&#20256;&#36882;&#33539;&#24335;&#65292;&#36825;&#23548;&#33268;&#20102;&#20887;&#20313;&#35745;&#31639;&#12289;&#33410;&#28857;&#34920;&#31034;&#30340;&#36807;&#24230;&#24179;&#28369;&#20197;&#21450;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#22312;&#22823;&#35268;&#27169;&#19978;&#65292;&#20174;&#25972;&#20010;KG&#20013;&#32858;&#21512;&#26377;&#29992;&#30340;&#20449;&#24687;&#36827;&#34892;&#25512;&#29702;&#21464;&#24471;&#35745;&#31639;&#19978;&#26114;&#36149;&#12290;&#20026;&#20102;&#35299;&#20915;&#29616;&#26377;KG&#38142;&#25509;&#39044;&#27979;&#26694;&#26550;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#32034;&#21644;&#38405;&#35835;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#39318;&#20808;&#26816;&#32034;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#23376;&#22270;&#19978;&#19979;&#25991;&#65292;&#28982;&#21518;&#36890;&#36807;&#39640;&#23481;&#37327;&#38405;&#35835;&#22120;&#32852;&#21512;&#25512;&#29702;&#19978;&#19979;&#25991;&#21644;&#26597;&#35810;&#12290;&#20316;&#20026;&#25105;&#20204;&#26032;&#26694;&#26550;&#30340;&#23454;&#20363;&#21270;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;GNN&#20316;&#20026;r&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph (KG) link prediction aims to infer new facts based on existing facts in the KG. Recent studies have shown that using the graph neighborhood of a node via graph neural networks (GNNs) provides more useful information compared to just using the query information. Conventional GNNs for KG link prediction follow the standard message-passing paradigm on the entire KG, which leads to superfluous computation, over-smoothing of node representations, and also limits their expressive power. On a large scale, it becomes computationally expensive to aggregate useful information from the entire KG for inference. To address the limitations of existing KG link prediction frameworks, we propose a novel retrieve-and-read framework, which first retrieves a relevant subgraph context for the query and then jointly reasons over the context and the query with a high-capacity reader. As part of our exemplar instantiation for the new framework, we propose a novel Transformer-based GNN as the r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#25105;&#39564;&#35777;&#26041;&#27861;&#65292;&#20351;&#29992;CoT&#30340;&#32467;&#35770;&#26469;&#26500;&#24314;&#26032;&#26679;&#26412;&#24182;&#35201;&#27714;LLM&#37325;&#26032;&#39044;&#27979;&#21407;&#22987;&#26465;&#20214;&#65292;&#20197;&#25552;&#39640;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LLMs&#21487;&#20197;&#23545;&#20854;&#33258;&#24049;&#30340;&#32467;&#35770;&#36827;&#34892;&#33258;&#25105;&#39564;&#35777;&#24182;&#23454;&#29616;&#31454;&#20105;&#24615;&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.09561</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#24102;&#26377;&#33258;&#25105;&#39564;&#35777;&#30340;&#25512;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are reasoners with Self-Verification. (arXiv:2212.09561v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#25105;&#39564;&#35777;&#26041;&#27861;&#65292;&#20351;&#29992;CoT&#30340;&#32467;&#35770;&#26469;&#26500;&#24314;&#26032;&#26679;&#26412;&#24182;&#35201;&#27714;LLM&#37325;&#26032;&#39044;&#27979;&#21407;&#22987;&#26465;&#20214;&#65292;&#20197;&#25552;&#39640;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LLMs&#21487;&#20197;&#23545;&#20854;&#33258;&#24049;&#30340;&#32467;&#35770;&#36827;&#34892;&#33258;&#25105;&#39564;&#35777;&#24182;&#23454;&#29616;&#31454;&#20105;&#24615;&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#36827;&#34892;&#22797;&#26434;&#25512;&#29702;&#26102;&#65292;&#23427;&#38750;&#24120;&#25935;&#24863;&#20110;&#20010;&#21035;&#38169;&#35823;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24517;&#39035;&#35757;&#32451;&#39564;&#35777;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#31216;&#20026;&#33258;&#25105;&#39564;&#35777;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;CoT&#30340;&#32467;&#35770;&#20316;&#20026;&#26465;&#20214;&#26469;&#26500;&#24314;&#19968;&#20010;&#26032;&#26679;&#26412;&#65292;&#24182;&#35201;&#27714;LLM&#37325;&#26032;&#39044;&#27979;&#34987;&#25513;&#30422;&#30340;&#21407;&#22987;&#26465;&#20214;&#12290;&#25105;&#20204;&#22522;&#20110;&#20934;&#30830;&#24615;&#35745;&#31639;&#21487;&#35299;&#37322;&#30340;&#39564;&#35777;&#20998;&#25968;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20351;&#29992;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#26102;&#25552;&#39640;&#22810;&#20010;&#31639;&#26415;&#21644;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#24050;&#32463;&#35777;&#26126;LLM&#21487;&#20197;&#23545;&#20854;&#33258;&#24049;&#30340;&#32467;&#35770;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#33258;&#25105;&#39564;&#35777;&#24182;&#23454;&#29616;&#31454;&#20105;&#24615;&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#22810;&#31181;&#24102;&#26377;&#33258;&#25105;&#39564;&#35777;&#21151;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36991;&#20813;&#28151;&#28102;&#12290;
&lt;/p&gt;
&lt;p&gt;
When a large language model (LLM) performs complex reasoning by chain of thought (CoT), it can be highly sensitive to individual mistakes. We have had to train verifiers to address this issue. As we all know, after human inferring a conclusion, they often check it by re-verifying it, which can avoid some mistakes. We propose a new method called self-verification that uses the conclusion of the CoT as a condition to build a new sample and asks the LLM to re-predict the original conditions which be masked. We calculate an explainable verification score based on the accuracy. This method can improve the accuracy of multiple arithmetics and logical reasoning datasets when using few-shot learning. we have demonstrated that LLMs can conduct explainable self-verification of their own conclusions and achieve competitive reasoning performance. Extensive experimentals have demonstrated that our method can help multiple large language models with self-verification can avoid interference from inco
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#25968;&#23383;&#35777;&#25454;&#30340;&#26041;&#27861;&#25913;&#36827;&#28151;&#28102;&#30340;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#21644;&#21033;&#29992;&#25968;&#23383;&#35777;&#25454;&#39044;&#27979;&#22788;&#32602;&#26399;&#38480;&#65292;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#21306;&#20998;&#20998;&#31867;&#38169;&#35823;&#21644;&#21033;&#29992;&#25968;&#23383;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.08238</link><description>&lt;p&gt;
&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#25968;&#23383;&#35777;&#25454;&#25913;&#36827;&#28151;&#28102;&#30340;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Exploiting Contrastive Learning and Numerical Evidence for Improving Confusing Legal Judgment Prediction. (arXiv:2211.08238v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#25968;&#23383;&#35777;&#25454;&#30340;&#26041;&#27861;&#25913;&#36827;&#28151;&#28102;&#30340;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#21644;&#21033;&#29992;&#25968;&#23383;&#35777;&#25454;&#39044;&#27979;&#22788;&#32602;&#26399;&#38480;&#65292;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#21306;&#20998;&#20998;&#31867;&#38169;&#35823;&#21644;&#21033;&#29992;&#25968;&#23383;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#19968;&#20010;&#27861;&#24459;&#26696;&#20363;&#30340;&#20107;&#23454;&#25551;&#36848;&#25991;&#26412;&#65292;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#65288;LJP&#65289;&#26088;&#22312;&#39044;&#27979;&#26696;&#20363;&#30340;&#32618;&#21517;&#12289;&#27861;&#24459;&#26465;&#27454;&#21644;&#22788;&#32602;&#26399;&#38480;&#12290;LJP&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#22914;&#20309;&#21306;&#20998;&#28151;&#28102;&#30340;&#27861;&#24459;&#26696;&#20363;&#65292;&#20854;&#20013;&#21482;&#23384;&#22312;&#24494;&#22937;&#30340;&#25991;&#26412;&#24046;&#24322;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#22312;&#20351;&#29992;&#26631;&#20934;&#30340;&#20132;&#21449;&#29109;&#20998;&#31867;&#25439;&#22833;&#26080;&#27861;&#21306;&#20998;&#19981;&#21516;&#30340;&#20998;&#31867;&#38169;&#35823;&#65292;&#24182;&#24573;&#30053;&#20102;&#20107;&#23454;&#25551;&#36848;&#20013;&#30340;&#25968;&#23383;&#65292;&#29992;&#20110;&#39044;&#27979;&#22788;&#32602;&#26399;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;moco&#30340;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65292;&#20197;&#23398;&#20064;&#21487;&#21306;&#20998;&#30340;&#34920;&#31034;&#65292;&#24182;&#25506;&#32034;&#26500;&#24314;&#27491;&#20363;&#23545;&#30340;&#26368;&#20339;&#31574;&#30053;&#65292;&#20174;&#32780;&#21516;&#26102;&#26377;&#21033;&#20110;LJP&#30340;&#19977;&#20010;&#23376;&#20219;&#21153;&#12290;&#20854;&#27425;&#65292;&#20026;&#20102;&#21033;&#29992;&#27861;&#24459;&#26696;&#20363;&#20013;&#30340;&#25968;&#23383;&#26469;&#39044;&#27979;&#26576;&#20123;&#26696;&#20363;&#30340;&#22788;&#32602;&#26399;&#38480;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#30001;&#39044;&#35757;&#32451;&#25968;&#20540;&#27169;&#22411;&#32534;&#30721;&#30340;&#25552;&#21462;&#30340;&#29359;&#32618;&#37329;&#39069;&#23545;&#20107;&#23454;&#25551;&#36848;&#30340;&#34920;&#31034;&#12290;&#23545;&#20844;&#24320;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the fact description text of a legal case, legal judgment prediction (LJP) aims to predict the case's charge, law article and penalty term. A core problem of LJP is how to distinguish confusing legal cases, where only subtle text differences exist. Previous studies fail to distinguish different classification errors with a standard cross-entropy classification loss, and ignore the numbers in the fact description for predicting the term of penalty. To tackle these issues, in this work, first, we propose a moco-based supervised contrastive learning to learn distinguishable representations, and explore the best strategy to construct positive example pairs to benefit all three subtasks of LJP simultaneously. Second, in order to exploit the numbers in legal cases for predicting the penalty terms of certain cases, we further enhance the representation of the fact description with extracted crime amounts which are encoded by a pre-trained numeracy model. Extensive experiments on public 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35843;&#26597;&#20102;&#24378;&#21270;&#23398;&#20064;&#21644;&#36172;&#21338;&#26426;&#22312;&#35821;&#38899;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#35752;&#35770;&#20102;&#22914;&#20309;&#26377;&#25928;&#21033;&#29992;&#23427;&#20204;&#35299;&#20915;&#30456;&#20851;&#38382;&#39064;&#65292;&#26500;&#24314;&#36866;&#24212;&#24615;&#12289;&#20132;&#20114;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2210.13623</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#21644;&#36172;&#21338;&#26426;&#22312;&#35821;&#38899;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;: &#25945;&#31243;&#65292;&#22238;&#39038;&#21644;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook. (arXiv:2210.13623v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13623
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35843;&#26597;&#20102;&#24378;&#21270;&#23398;&#20064;&#21644;&#36172;&#21338;&#26426;&#22312;&#35821;&#38899;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#35752;&#35770;&#20102;&#22914;&#20309;&#26377;&#25928;&#21033;&#29992;&#23427;&#20204;&#35299;&#20915;&#30456;&#20851;&#38382;&#39064;&#65292;&#26500;&#24314;&#36866;&#24212;&#24615;&#12289;&#20132;&#20114;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#24378;&#21270;&#23398;&#20064;&#21644;&#36172;&#21338;&#26426;&#24050;&#32463;&#22312;&#21253;&#25324;&#21307;&#30103;&#20445;&#20581;&#65292;&#37329;&#34701;&#65292;&#25512;&#33616;&#31995;&#32479;&#65292;&#26426;&#22120;&#20154;&#20197;&#21450;&#35821;&#38899;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#21457;&#29983;&#20102;&#36716;&#21464;&#12290;&#34429;&#28982;&#22823;&#37096;&#20998;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#35821;&#38899;&#21644;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#24212;&#29992;&#20027;&#35201;&#38598;&#20013;&#20110;&#21033;&#29992;&#20854;&#28789;&#27963;&#30340;&#20248;&#21270;&#23646;&#24615;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#65292;&#20294;&#20173;&#26377;&#35768;&#22810;&#30740;&#31350;&#31354;&#38388;&#21487;&#20197;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#22909;&#22788;&#65292;&#27604;&#22914;&#22522;&#20110;&#22870;&#21169;&#39537;&#21160;&#30340;&#36866;&#24212;&#24615;&#12289;&#29366;&#24577;&#34920;&#31034;&#12289;&#26102;&#38388;&#32467;&#26500;&#21644;&#36890;&#29992;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24378;&#21270;&#23398;&#20064;&#21644;&#36172;&#21338;&#26426;&#30340;&#26368;&#26032;&#36827;&#23637;&#32508;&#36848;&#65292;&#24182;&#35752;&#35770;&#20102;&#22914;&#20309;&#26377;&#25928;&#22320;&#21033;&#29992;&#23427;&#20204;&#26469;&#35299;&#20915;&#35821;&#38899;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38382;&#39064;&#65292;&#26500;&#24314;&#36866;&#24212;&#24615;&#12289;&#20132;&#20114;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, reinforcement learning and bandits have transformed a wide range of real-world applications including healthcare, finance, recommendation systems, robotics, and last but not least, the speech and natural language processing. While most speech and language applications of reinforcement learning algorithms are centered around improving the training of deep neural networks with its flexible optimization properties, there are still many grounds to explore to utilize the benefits of reinforcement learning, such as its reward-driven adaptability, state representations, temporal structures and generalizability. In this survey, we present an overview of recent advancements of reinforcement learning and bandits, and discuss how they can be effectively employed to solve speech and natural language processing problems with models that are adaptive, interactive and scalable.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#33041;&#20449;&#21495;&#21644;&#20154;&#31867;&#35821;&#35328;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MTAM&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#24773;&#24863;&#20998;&#26512;&#21644;&#20851;&#31995;&#26816;&#27979;&#31561;&#19979;&#28216;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2208.06348</link><description>&lt;p&gt;
&#33021;&#21542;&#36890;&#36807;&#33041;&#20449;&#21495;&#25581;&#31034;&#20154;&#31867;&#35821;&#35328;&#30340;&#20869;&#37096;&#19968;&#33268;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Brain Signals Reveal Inner Alignment with Human Languages?. (arXiv:2208.06348v4 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.06348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#33041;&#20449;&#21495;&#21644;&#20154;&#31867;&#35821;&#35328;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MTAM&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#24773;&#24863;&#20998;&#26512;&#21644;&#20851;&#31995;&#26816;&#27979;&#31561;&#19979;&#28216;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#20449;&#21495;&#65288;&#22914;&#33041;&#30005;&#22270;&#65289;&#21644;&#20154;&#31867;&#35821;&#35328;&#22312;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#20013;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#20108;&#32773;&#20043;&#38388;&#30340;&#32852;&#31995;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#33041;&#30005;&#22270;&#21644;&#35821;&#35328;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#20381;&#36182;&#24615;&#12290;&#22312;&#34920;&#31034;&#23618;&#38754;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;MTAM&#65288;Multimodal Transformer Alignment Model&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35266;&#23519;&#36825;&#20004;&#31181;&#27169;&#24577;&#20043;&#38388;&#30340;&#21327;&#35843;&#34920;&#31034;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#22810;&#31181;&#20851;&#31995;&#23545;&#40784;&#25216;&#26415;&#65292;&#22914;&#20856;&#22411;&#30456;&#20851;&#20998;&#26512;&#21644;Wasserstein&#36317;&#31163;&#65292;&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#26469;&#36716;&#25442;&#29305;&#24449;&#12290;&#22312;&#24773;&#24863;&#20998;&#26512;&#21644;&#20851;&#31995;&#26816;&#27979;&#31561;&#19979;&#28216;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#22312;ZuCo&#21644;K-EmoCon&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24773;&#24863;&#20998;&#26512;&#26041;&#38754;&#20351;K-EmoCon&#25968;&#25454;&#38598;&#30340;F1&#20998;&#25968;&#25552;&#39640;&#20102;1.7&#65285;&#65292;ZuCo&#25968;&#25454;&#38598;&#25552;&#39640;&#20102;9.3&#65285;&#65292;&#22312;&#20851;&#31995;&#26816;&#27979;&#26041;&#38754;ZuCo&#25968;&#25454;&#38598;&#25552;&#39640;&#20102;7.4&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22269;&#38469;&#19978;&#26368;&#22823;&#30340;&#20154;&#31867;&#31867;&#27604;&#25512;&#29702;&#25968;&#25454;&#38598;&#30340;&#32534;&#30721;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Brain Signals, such as Electroencephalography (EEG), and human languages have been widely explored independently for many downstream tasks, however, the connection between them has not been well explored. In this study, we explore the relationship and dependency between EEG and language. To study at the representation level, we introduced \textbf{MTAM}, a \textbf{M}ultimodal \textbf{T}ransformer \textbf{A}lignment \textbf{M}odel, to observe coordinated representations between the two modalities. We used various relationship alignment-seeking techniques, such as Canonical Correlation Analysis and Wasserstein Distance, as loss functions to transfigure features. On downstream applications, sentiment analysis and relation detection, we achieved new state-of-the-art results on two datasets, ZuCo and K-EmoCon. Our method achieved an F1-score improvement of 1.7% on K-EmoCon and 9.3% on Zuco datasets for sentiment analysis, and 7.4% on ZuCo for relation detection. In addition, we provide inter
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#32039;&#24613;&#36890;&#20449;&#65288;EC&#65289;&#21644;&#39044;&#20808;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#22522;&#20110;&#35270;&#35273;&#20219;&#21153;&#28608;&#21169;&#27169;&#22411;&#26469;&#25913;&#36827;&#36164;&#28304;&#21294;&#20047;&#35821;&#35328;&#30340;&#38750;&#30417;&#30563;NMT&#31995;&#32479;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22235;&#31181;&#35821;&#35328;&#20013;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#36164;&#28304;&#21294;&#20047;&#30340;&#23612;&#27850;&#23572;&#35821;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20165;&#20351;&#29992;&#22238;&#35793;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2207.07025</link><description>&lt;p&gt;
&#23398;&#20064;&#36890;&#36807;&#23398;&#20064;&#20132;&#27969;&#26469;&#36827;&#34892;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Learning to translate by learning to communicate. (arXiv:2207.07025v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.07025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#32039;&#24613;&#36890;&#20449;&#65288;EC&#65289;&#21644;&#39044;&#20808;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#22522;&#20110;&#35270;&#35273;&#20219;&#21153;&#28608;&#21169;&#27169;&#22411;&#26469;&#25913;&#36827;&#36164;&#28304;&#21294;&#20047;&#35821;&#35328;&#30340;&#38750;&#30417;&#30563;NMT&#31995;&#32479;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22235;&#31181;&#35821;&#35328;&#20013;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#36164;&#28304;&#21294;&#20047;&#30340;&#23612;&#27850;&#23572;&#35821;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20165;&#20351;&#29992;&#22238;&#35793;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#24182;&#27979;&#35797;&#20102;&#19968;&#31181;&#25216;&#26415;&#65292;&#21033;&#29992;&#32039;&#24613;&#36890;&#20449;&#65288;EC&#65289;&#21644;&#39044;&#20808;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#25913;&#36827;&#20102;&#29616;&#20195;&#38750;&#30417;&#30563;NMT&#31995;&#32479;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#12290;&#24050;&#26377;&#35266;&#28857;&#35748;&#20026;&#65292;&#24403;&#21069;&#22312;NLP&#39046;&#22495;&#20027;&#23548;&#22320;&#20301;&#30340;&#25991;&#26412;&#39044;&#35757;&#32451;&#27169;&#22411;&#26080;&#27861;&#20135;&#29983;&#31283;&#20581;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#31995;&#32479;&#65292;&#24182;&#31361;&#20986;&#20102;&#23545;&#22522;&#20110;&#30446;&#26631;&#12289;&#30446;&#26631;&#23548;&#21521;&#21644;&#20132;&#20114;&#24335;&#35821;&#35328;&#23398;&#20064;&#30340;&#38656;&#27714;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#23558;&#22810;&#35821;&#35328;&#27169;&#22411;&#65288;mBART&#65292;Liu&#31561;&#65292;2020&#65289;&#23884;&#20837;&#21040;&#19968;&#20010;EC&#22270;&#20687;&#21442;&#32771;&#28216;&#25103;&#20013;&#65292;&#27169;&#22411;&#34987;&#28608;&#21169;&#20351;&#29992;&#22810;&#35821;&#35328;&#29983;&#25104;&#26469;&#23436;&#25104;&#19968;&#20010;&#22522;&#20110;&#35270;&#35273;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#20551;&#35774;&#26159;&#65292;&#36825;&#23558;&#20351;&#22810;&#31181;&#35821;&#35328;&#23545;&#40784;&#21040;&#19968;&#20010;&#20849;&#20139;&#30340;&#20219;&#21153;&#31354;&#38388;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;EC&#24494;&#35843;&#30340;&#21464;&#20307;&#65288;Steinert-Threlkeld&#31561;&#20154;&#65292;2022&#65289;&#65292;&#20854;&#20013;&#19968;&#31181;&#22312;&#21253;&#25324;&#36164;&#28304;&#21294;&#20047;&#30340;&#23612;&#27850;&#23572;&#35821;&#22312;&#20869;&#30340;&#22235;&#31181;&#35821;&#35328;&#20013;&#37117;&#20248;&#20110;&#20165;&#20351;&#29992;&#22238;&#35793;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We formulate and test a technique to use Emergent Communication (EC) with a pre-trained multilingual model to improve on modern Unsupervised NMT systems, especially for low-resource languages. It has been argued that the current dominant paradigm in NLP of pre-training on text-only corpora will not yield robust natural language understanding systems, and the need for grounded, goal-oriented, and interactive language learning has been high lighted. In our approach, we embed a multilingual model (mBART, Liu et al., 2020) into an EC image-reference game, in which the model is incentivized to use multilingual generations to accomplish a vision-grounded task. The hypothesis is that this will align multiple languages to a shared task space. We present two variants of EC Fine-Tuning (Steinert-Threlkeld et al., 2022), one of which outperforms a backtranslation-only baseline in all four languages investigated, including the low-resource language Nepali.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Dict-TTS&#27169;&#22411;&#65292;&#21033;&#29992;&#22312;&#32447;&#23383;&#20856;&#20316;&#20026;&#20808;&#21069;&#20449;&#24687;&#26469;&#35299;&#20915;&#22810;&#38899;&#23383;&#28040;&#27495;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#35821;&#20041;&#21040;&#21457;&#38899;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#27809;&#26377;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#33258;&#21160;&#21305;&#37197;&#25991;&#26412;&#35821;&#20041;&#21644;&#23383;&#20856;&#20013;&#30340;&#35821;&#20041;&#27169;&#24335;&#65292;&#24182;&#29983;&#25104;&#30456;&#24212;&#30340;&#21457;&#38899;&#65292;&#20351;&#24471;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#31995;&#32479;&#33021;&#22815;&#26356;&#23481;&#26131;&#22320;&#25193;&#23637;&#21040;&#19981;&#21516;&#39046;&#22495;&#21644;&#35821;&#35328;&#12290;</title><link>http://arxiv.org/abs/2206.02147</link><description>&lt;p&gt;
Dict-TTS: &#21033;&#29992;&#20808;&#21069;&#30340;&#23383;&#20856;&#30693;&#35782;&#23398;&#20064;&#21457;&#38899;&#20197;&#29992;&#20110;&#25991;&#26412;&#21040;&#35821;&#38899;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Dict-TTS: Learning to Pronounce with Prior Dictionary Knowledge for Text-to-Speech. (arXiv:2206.02147v3 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Dict-TTS&#27169;&#22411;&#65292;&#21033;&#29992;&#22312;&#32447;&#23383;&#20856;&#20316;&#20026;&#20808;&#21069;&#20449;&#24687;&#26469;&#35299;&#20915;&#22810;&#38899;&#23383;&#28040;&#27495;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#35821;&#20041;&#21040;&#21457;&#38899;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#27809;&#26377;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#33258;&#21160;&#21305;&#37197;&#25991;&#26412;&#35821;&#20041;&#21644;&#23383;&#20856;&#20013;&#30340;&#35821;&#20041;&#27169;&#24335;&#65292;&#24182;&#29983;&#25104;&#30456;&#24212;&#30340;&#21457;&#38899;&#65292;&#20351;&#24471;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#31995;&#32479;&#33021;&#22815;&#26356;&#23481;&#26131;&#22320;&#25193;&#23637;&#21040;&#19981;&#21516;&#39046;&#22495;&#21644;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#38899;&#23383;&#28040;&#27495;&#26088;&#22312;&#20174;&#33258;&#28982;&#25991;&#26412;&#24207;&#21015;&#20013;&#33719;&#21462;&#20934;&#30830;&#30340;&#21457;&#38899;&#30693;&#35782;&#65292;&#20197;&#26500;&#24314;&#21487;&#38752;&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#27880;&#37322;&#35757;&#32451;&#25968;&#25454;&#21644;&#35821;&#35328;&#19987;&#23478;&#30340;&#39069;&#22806;&#21162;&#21147;&#65292;&#20351;&#24471;&#38590;&#20197;&#23558;&#39640;&#36136;&#37327;&#30340;&#31070;&#32463;&#32593;&#32476;&#35821;&#38899;&#21512;&#25104;&#31995;&#32479;&#25193;&#23637;&#21040;&#26085;&#24120;&#23545;&#35805;&#21644;&#20840;&#29699;&#21508;&#31181;&#35821;&#35328;&#20013;&#12290;&#26412;&#25991;&#20174;&#31616;&#27905;&#32780;&#26032;&#39062;&#30340;&#35282;&#24230;&#35299;&#20915;&#20102;&#22810;&#38899;&#23383;&#28040;&#27495;&#38382;&#39064;&#65306;&#25105;&#20204;&#25552;&#20986;&#20102;Dict-TTS&#65292;&#36825;&#26159;&#19968;&#20010;&#35821;&#20041;&#24863;&#30693;&#30340;&#29983;&#25104;&#25991;&#26412;&#21040;&#35821;&#38899;&#27169;&#22411;&#65292;&#21033;&#29992;&#22312;&#32447;&#32593;&#31449;&#23383;&#20856;&#65288;&#21363;&#33258;&#28982;&#35821;&#35328;&#20013;&#24050;&#23384;&#22312;&#30340;&#20808;&#21069;&#20449;&#24687;&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#35821;&#20041;&#21040;&#21457;&#38899;&#27880;&#24847;&#21147;&#65288;S2PA&#65289;&#27169;&#22359;&#65292;&#29992;&#20110;&#21305;&#37197;&#36755;&#20837;&#25991;&#26412;&#24207;&#21015;&#19982;&#23383;&#20856;&#20013;&#20808;&#21069;&#35821;&#20041;&#20043;&#38388;&#30340;&#35821;&#20041;&#27169;&#24335;&#65292;&#24182;&#33719;&#21462;&#30456;&#24212;&#30340;&#21457;&#38899;&#65307;S2PA&#27169;&#22359;&#21487;&#20197;&#36731;&#26494;&#22320;&#19982;&#31471;&#21040;&#31471;&#30340;&#35821;&#38899;&#21512;&#25104;&#27169;&#22411;&#19968;&#36215;&#35757;&#32451;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#27880;&#37322;&#30340;&#38899;&#32032;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Polyphone disambiguation aims to capture accurate pronunciation knowledge from natural text sequences for reliable Text-to-speech (TTS) systems. However, previous approaches require substantial annotated training data and additional efforts from language experts, making it difficult to extend high-quality neural TTS systems to out-of-domain daily conversations and countless languages worldwide. This paper tackles the polyphone disambiguation problem from a concise and novel perspective: we propose Dict-TTS, a semantic-aware generative text-to-speech model with an online website dictionary (the existing prior information in the natural language). Specifically, we design a semantics-to-pronunciation attention (S2PA) module to match the semantic patterns between the input text sequence and the prior semantics in the dictionary and obtain the corresponding pronunciations; The S2PA module can be easily trained with the end-to-end TTS model without any annotated phoneme labels. Experimental 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26080;&#26631;&#31614;&#35821;&#26009;&#24211;&#21644;&#22522;&#20110;&#21477;&#23376;&#23884;&#20837;&#30340;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#26500;&#24314;WordNet&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#26032;&#30340;WordNet&#65288;FilWordNet&#65289;&#65292;&#20197;&#26367;&#20195;&#24182;&#25913;&#36827;&#33778;&#24459;&#23486;&#35821;&#20013;&#36807;&#26102;&#30340;WordNet&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20154;&#24037;&#30417;&#30563;&#12290;</title><link>http://arxiv.org/abs/2204.03251</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#21477;&#23376;&#23884;&#20837;&#30340;&#35789;&#20041;&#24402;&#32435;&#33258;&#21160;&#26500;&#24314;WordNet
&lt;/p&gt;
&lt;p&gt;
Automatic WordNet Construction using Word Sense Induction through Sentence Embeddings. (arXiv:2204.03251v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.03251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26080;&#26631;&#31614;&#35821;&#26009;&#24211;&#21644;&#22522;&#20110;&#21477;&#23376;&#23884;&#20837;&#30340;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#26500;&#24314;WordNet&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#26032;&#30340;WordNet&#65288;FilWordNet&#65289;&#65292;&#20197;&#26367;&#20195;&#24182;&#25913;&#36827;&#33778;&#24459;&#23486;&#35821;&#20013;&#36807;&#26102;&#30340;WordNet&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20154;&#24037;&#30417;&#30563;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#36164;&#28304;&#22914;WordNet&#23545;&#20110;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#21644;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#65288;&#22914;&#33778;&#24459;&#23486;&#35821;&#65289;&#65292;&#29616;&#26377;&#30340;WordNet&#36807;&#26102;&#19988;&#19981;&#23436;&#25972;&#65292;&#24182;&#19988;&#29983;&#25104;&#26032;&#30340;WordNet&#21487;&#33021;&#38656;&#35201;&#22823;&#37327;&#30340;&#26102;&#38388;&#21644;&#36164;&#28304;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26080;&#26631;&#31614;&#35821;&#26009;&#24211;&#21644;&#22522;&#20110;&#21477;&#23376;&#23884;&#20837;&#30340;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#26500;&#24314;WordNet&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#29983;&#25104;&#20102;&#19968;&#20010;&#26032;&#30340;WordNet&#65288;FilWordNet&#65289;&#65292;&#20197;&#26367;&#20195;&#24182;&#25913;&#36827;&#33778;&#24459;&#23486;&#35821;&#20013;&#36807;&#26102;&#30340;WordNet&#12290;&#36890;&#36807;&#23558;&#25105;&#20204;&#33258;&#21160;&#35825;&#23548;&#20986;&#30340;&#35789;&#20041;&#21644;&#35789;&#27719;&#38598;&#19982;Princeton WordNet&#20013;&#30340;&#35789;&#20041;&#36827;&#34892;&#21305;&#37197;&#65292;&#20197;&#21450;&#23558;&#35789;&#27719;&#38598;&#19982;&#26087;&#30340;&#33778;&#24459;&#23486;&#35821;WordNet&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#32463;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#35825;&#23548;&#29616;&#26377;&#30340;&#35789;&#20041;&#21644;&#35789;&#27719;&#38598;&#65292;&#20063;&#21487;&#20197;&#28508;&#22312;&#22320;&#33258;&#21160;&#35825;&#23548;&#26032;&#30340;&#35789;&#20041;&#21644;&#35789;&#27719;&#38598;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20154;&#24037;&#30417;&#30563;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language resources such as wordnets remain indispensable tools for different natural language tasks and applications. However, for low-resource languages such as Filipino, existing wordnets are old and outdated, and producing new ones may be slow and costly in terms of time and resources. In this paper, we propose an automatic method for constructing a wordnet from scratch using only an unlabeled corpus and a sentence embeddings-based language model. Using this, we produce FilWordNet, a new wordnet that supplants and improves the outdated Filipino WordNet. We evaluate our automatically-induced senses and synsets by matching them with senses from the Princeton WordNet, as well as comparing the synsets to the old Filipino WordNet. We empirically show that our method can induce existing, as well as potentially new, senses and synsets automatically without the need for human supervision.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31034;&#20363;&#30340;&#36229;&#32593;&#32476;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#20010;&#28304;&#39046;&#22495;&#30340;&#26631;&#35760;&#25968;&#25454;&#26469;&#36827;&#34892;&#39046;&#22495;&#22806;&#27867;&#21270;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#29983;&#25104;&#36755;&#20837;&#31034;&#20363;&#30340;&#21807;&#19968;&#31614;&#21517;&#65292;&#24182;&#23558;&#20854;&#23884;&#20837;&#28304;&#39046;&#22495;&#30340;&#35821;&#20041;&#31354;&#38388;&#20013;&#65292;&#24182;&#21033;&#29992;&#36229;&#32593;&#32476;&#29983;&#25104;&#20219;&#21153;&#20998;&#31867;&#22120;&#30340;&#26435;&#37325;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;29&#20010;&#36866;&#24212;&#22330;&#26223;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#65292;&#19988;&#22312;&#36755;&#20837;&#31034;&#20363;&#30340;&#34920;&#31034;&#19978;&#20855;&#26377;&#20016;&#23500;&#24615;&#12290;&#21516;&#26102;&#65292;&#19982;&#23569;&#26679;&#26412;GPT-3&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2203.14276</link><description>&lt;p&gt;
&#22522;&#20110;&#31034;&#20363;&#30340;&#36229;&#32593;&#32476;&#29992;&#20110;&#39046;&#22495;&#22806;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Example-based Hypernetworks for Out-of-Distribution Generalization. (arXiv:2203.14276v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.14276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31034;&#20363;&#30340;&#36229;&#32593;&#32476;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#20010;&#28304;&#39046;&#22495;&#30340;&#26631;&#35760;&#25968;&#25454;&#26469;&#36827;&#34892;&#39046;&#22495;&#22806;&#27867;&#21270;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#29983;&#25104;&#36755;&#20837;&#31034;&#20363;&#30340;&#21807;&#19968;&#31614;&#21517;&#65292;&#24182;&#23558;&#20854;&#23884;&#20837;&#28304;&#39046;&#22495;&#30340;&#35821;&#20041;&#31354;&#38388;&#20013;&#65292;&#24182;&#21033;&#29992;&#36229;&#32593;&#32476;&#29983;&#25104;&#20219;&#21153;&#20998;&#31867;&#22120;&#30340;&#26435;&#37325;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;29&#20010;&#36866;&#24212;&#22330;&#26223;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#65292;&#19988;&#22312;&#36755;&#20837;&#31034;&#20363;&#30340;&#34920;&#31034;&#19978;&#20855;&#26377;&#20016;&#23500;&#24615;&#12290;&#21516;&#26102;&#65292;&#19982;&#23569;&#26679;&#26412;GPT-3&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#31639;&#27861;&#19981;&#26029;&#31361;&#30772;&#26032;&#30340;&#37324;&#31243;&#30865;&#65292;&#39046;&#22495;&#22806;&#27867;&#21270;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#23545;&#20110;&#38476;&#29983;&#39046;&#22495;&#30340;&#22810;&#28304;&#36866;&#24212;&#38382;&#39064;&#65306;&#25105;&#20204;&#21033;&#29992;&#26469;&#33258;&#22810;&#20010;&#28304;&#39046;&#22495;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#22312;&#35757;&#32451;&#20013;&#27867;&#21270;&#21040;&#26410;&#30693;&#30446;&#26631;&#39046;&#22495;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#24615;&#26694;&#26550;&#37319;&#29992;&#22522;&#20110;&#31034;&#20363;&#30340;&#36229;&#32593;&#32476;&#36866;&#24212;&#65306;&#19968;&#20010;T5&#32534;&#30721;-&#35299;&#30721;&#22120;&#39318;&#20808;&#20174;&#36755;&#20837;&#31034;&#20363;&#20013;&#29983;&#25104;&#19968;&#20010;&#21807;&#19968;&#30340;&#31614;&#21517;&#65292;&#24182;&#23558;&#20854;&#23884;&#20837;&#21040;&#28304;&#39046;&#22495;&#30340;&#35821;&#20041;&#31354;&#38388;&#20013;&#12290;&#28982;&#21518;&#65292;&#36825;&#20010;&#31614;&#21517;&#34987;&#19968;&#20010;&#36229;&#32593;&#32476;&#21033;&#29992;&#26469;&#29983;&#25104;&#20219;&#21153;&#20998;&#31867;&#22120;&#30340;&#26435;&#37325;&#12290;&#25105;&#20204;&#22312;29&#31181;&#36866;&#24212;&#22330;&#26223;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#28041;&#21450;&#24773;&#24863;&#20998;&#31867;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20004;&#20010;&#20219;&#21153;&#65292;&#22312;&#36825;&#20123;&#22330;&#26223;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36229;&#36807;&#20102;&#24050;&#26377;&#31639;&#27861;&#12290;&#22312;&#39640;&#32423;&#29256;&#26412;&#20013;&#65292;&#31614;&#21517;&#36824;&#20016;&#23500;&#20102;&#36755;&#20837;&#31034;&#20363;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#24494;&#35843;&#26550;&#26500;&#19982;&#23569;&#26679;&#26412;GPT-3&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Natural Language Processing (NLP) algorithms continually achieve new milestones, out-of-distribution generalization remains a significant challenge. This paper addresses the issue of multi-source adaptation for unfamiliar domains: We leverage labeled data from multiple source domains to generalize to unknown target domains at training. Our innovative framework employs example-based Hypernetwork adaptation: a T5 encoder-decoder initially generates a unique signature from an input example, embedding it within the source domains' semantic space. This signature is subsequently utilized by a Hypernetwork to generate the task classifier's weights. We evaluated our method across two tasks - sentiment classification and natural language inference - in 29 adaptation scenarios, where it outpaced established algorithms. In an advanced version, the signature also enriches the input example's representation. We also compare our finetuned architecture to few-shot GPT-3, demonstrating its effectiv
&lt;/p&gt;</description></item><item><title>Bhasacitra&#26159;&#19968;&#20010;&#29992;&#20110;&#21335;&#20122;&#30340;&#26041;&#35328;&#22320;&#29702;&#21487;&#35270;&#21270;&#31995;&#32479;&#65292;&#38500;&#20102;&#33021;&#22815;&#36827;&#34892;&#29305;&#24449;&#26144;&#23556;&#22806;&#65292;&#36824;&#21487;&#20197;&#20316;&#20026;&#21335;&#20122;&#35821;&#35328;&#23398;&#23478;&#30340;&#20114;&#21160;&#21442;&#32771;&#20070;&#12290;</title><link>http://arxiv.org/abs/2105.14082</link><description>&lt;p&gt;
Bhasacitra: &#21335;&#20122;&#26041;&#35328;&#22320;&#29702;&#21487;&#35270;&#21270;
&lt;/p&gt;
&lt;p&gt;
Bhasacitra: Visualising the dialect geography of South Asia. (arXiv:2105.14082v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.14082
&lt;/p&gt;
&lt;p&gt;
Bhasacitra&#26159;&#19968;&#20010;&#29992;&#20110;&#21335;&#20122;&#30340;&#26041;&#35328;&#22320;&#29702;&#21487;&#35270;&#21270;&#31995;&#32479;&#65292;&#38500;&#20102;&#33021;&#22815;&#36827;&#34892;&#29305;&#24449;&#26144;&#23556;&#22806;&#65292;&#36824;&#21487;&#20197;&#20316;&#20026;&#21335;&#20122;&#35821;&#35328;&#23398;&#23478;&#30340;&#20114;&#21160;&#21442;&#32771;&#20070;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Bhasacitra&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#21335;&#20122;&#35821;&#35328;&#23398;&#30740;&#31350;&#25968;&#25454;&#24211;&#30340;&#26041;&#35328;&#26144;&#23556;&#31995;&#32479;&#65292;&#35813;&#25968;&#25454;&#24211;&#24102;&#26377;&#20027;&#39064;&#21644;&#20301;&#32622;&#25968;&#25454;&#30340;&#27880;&#37322;&#12290;&#25105;&#20204;&#36890;&#36807;&#21487;&#35270;&#21270;&#31034;&#20363;&#25968;&#25454;&#38598;&#65292;&#20998;&#26512;&#35821;&#35328;&#35206;&#30422;&#24182;&#23637;&#26395;&#20854;&#22312;&#31867;&#22411;&#23398;&#20013;&#30340;&#24212;&#29992;&#12290;&#36825;&#20010;&#24212;&#29992;&#19981;&#20165;&#23545;&#21151;&#33021;&#26144;&#23556;&#26377;&#29992;&#65292;&#36824;&#20316;&#20026;&#19968;&#31181;&#26032;&#22411;&#30340;&#20114;&#21160;&#21442;&#32771;&#20070;&#65292;&#20026;&#21335;&#20122;&#35821;&#35328;&#23398;&#23478;&#25552;&#20379;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Bhasacitra, a dialect mapping system for South Asia built on a database of linguistic studies of languages of the region annotated for topic and location data. We analyse language coverage and look towards applications to typology by visualising example datasets. The application is not only meant to be useful for feature mapping, but also serves as a new kind of interactive bibliography for linguists of South Asian languages.
&lt;/p&gt;</description></item></channel></rss>