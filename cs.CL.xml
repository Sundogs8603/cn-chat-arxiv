<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;CaKE-LM&#65292;&#23427;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22240;&#26524;&#24120;&#35782;&#30693;&#35782;&#26469;&#22788;&#29702;&#22240;&#26524;&#35270;&#39057;&#38382;&#31572;&#65288;CVidQA&#65289;&#20219;&#21153;&#65292;&#33021;&#22815;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#22240;&#26524;&#30693;&#35782;&#24110;&#21161;&#29983;&#25104;&#38382;&#39064;-&#31572;&#26696;&#23545;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#26041;&#27861;&#22312;CVidQA&#20219;&#21153;&#19978;&#26377;&#30528;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2304.03754</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#38646;&#26679;&#26412;&#35270;&#39057;&#38382;&#31572;&#30340;&#22240;&#26524;&#30693;&#35782;&#25552;&#21462;&#22120;
&lt;/p&gt;
&lt;p&gt;
Language Models are Causal Knowledge Extractors for Zero-shot Video Question Answering. (arXiv:2304.03754v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;CaKE-LM&#65292;&#23427;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22240;&#26524;&#24120;&#35782;&#30693;&#35782;&#26469;&#22788;&#29702;&#22240;&#26524;&#35270;&#39057;&#38382;&#31572;&#65288;CVidQA&#65289;&#20219;&#21153;&#65292;&#33021;&#22815;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#22240;&#26524;&#30693;&#35782;&#24110;&#21161;&#29983;&#25104;&#38382;&#39064;-&#31572;&#26696;&#23545;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#26041;&#27861;&#22312;CVidQA&#20219;&#21153;&#19978;&#26377;&#30528;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#35270;&#39057;&#38382;&#31572;&#65288;CVidQA&#65289;&#19981;&#20165;&#26597;&#35810;&#30456;&#20851;&#25110;&#26102;&#38388;&#20851;&#31995;&#65292;&#36824;&#26597;&#35810;&#35270;&#39057;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#29616;&#26377;&#30340;&#38382;&#39064;&#29983;&#25104;&#26041;&#27861;&#22312;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;&#19978;&#39044;&#20808;&#35757;&#32451;&#38382;&#39064;&#29983;&#25104;&#65288;QG&#65289;&#31995;&#32479;&#65292;&#36755;&#20837;&#20026;&#25991;&#26412;&#25551;&#36848;&#12290;&#20294;&#26159;&#65292;QG&#27169;&#22411;&#21482;&#23398;&#20064;&#25552;&#20986;&#30456;&#20851;&#38382;&#39064;&#65288;&#20363;&#22914;&#65292;&#8220;&#26576;&#20154;&#22312;&#20570;&#20160;&#20040;...&#8221;&#65289;&#65292;&#23548;&#33268;&#36716;&#25442;&#21040;CVidQA&#30340;&#20851;&#32852;&#30693;&#35782;&#24046;&#65292;CVidQA&#37325;&#28857;&#20851;&#27880;&#8220;&#26576;&#20154;&#20026;&#20160;&#20040;&#36825;&#26679;&#20570;...&#8221;&#36825;&#26679;&#30340;&#22240;&#26524;&#38382;&#39064;&#12290;&#22312;&#35266;&#23519;&#21040;&#36825;&#19968;&#28857;&#21518;&#65292;&#25105;&#20204;&#25552;&#35758;&#21033;&#29992;&#22240;&#26524;&#30693;&#35782;&#29983;&#25104;&#38382;&#39064;-&#31572;&#26696;&#23545;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21363;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#22240;&#26524;&#30693;&#35782;&#65288;CaKE-LM&#65289;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22240;&#26524;&#24120;&#35782;&#30693;&#35782;&#26469;&#22788;&#29702;CVidQA&#12290;&#20026;&#20102;&#20174;LM&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;CaKE-LM&#29983;&#25104;&#21253;&#21547;&#20004;&#20010;&#20107;&#20214;&#30340;&#22240;&#26524;&#38382;&#39064;&#65288;&#20363;&#22914;&#65292;&#8220;&#24471;&#20998;&#8221;&#35302;&#21457;&#8220;&#36275;&#29699;&#36816;&#21160;&#21592;&#36386;&#29699;&#8221;&#65289;&#65292;&#36890;&#36807;&#25552;&#31034;LM&#26469;&#25552;&#21462;&#22240;&#26524;&#24120;&#35782;&#30693;&#35782;&#20316;&#20026;&#36825;&#20004;&#20010;&#20107;&#20214;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24418;&#25104;&#38382;&#39064;-&#31572;&#26696;&#23545;&#12290;&#22312;TVQA +&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;CVidQA&#20219;&#21153;&#19978;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal Video Question Answering (CVidQA) queries not only association or temporal relations but also causal relations in a video. Existing question synthesis methods pre-trained question generation (QG) systems on reading comprehension datasets with text descriptions as inputs. However, QG models only learn to ask association questions (e.g., ``what is someone doing...'') and result in inferior performance due to the poor transfer of association knowledge to CVidQA, which focuses on causal questions like ``why is someone doing ...''. Observing this, we proposed to exploit causal knowledge to generate question-answer pairs, and proposed a novel framework, Causal Knowledge Extraction from Language Models (CaKE-LM), leveraging causal commonsense knowledge from language models to tackle CVidQA. To extract knowledge from LMs, CaKE-LM generates causal questions containing two events with one triggering another (e.g., ``score a goal'' triggers ``soccer player kicking ball'') by prompting LM w
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#37325;&#35201;&#38382;&#39064;&#65306;&#22914;&#20309;&#21033;&#29992;&#38376;&#25511;&#26426;&#21046;&#22686;&#24378;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#20197;&#22312;&#21508;&#31181;&#23545;&#35805;&#25968;&#25454;&#20043;&#38388;&#21457;&#29616;&#25968;&#25454;-&#20219;&#21153;&#21644;&#20219;&#21153;-&#20219;&#21153;&#20851;&#31995;&#65292;&#25552;&#39640;&#23545;&#35805;&#36335;&#30001;&#25928;&#29575;&#65292;&#38477;&#20302;&#20154;&#21147;&#25104;&#26412;&#65292;&#22686;&#24378;&#29992;&#25143;&#20307;&#39564;&#12290;&#25552;&#20986;&#30340;&#38376;&#25511;&#26426;&#21046;&#22686;&#24378;&#30340;&#22810;&#20219;&#21153;&#27169;&#22411;&#20855;&#26377;&#23618;&#27425;&#21270;&#20449;&#24687;&#36807;&#28388;&#30340;&#20316;&#29992;&#65292;&#20854;&#23545;&#29616;&#26377;&#31995;&#32479;&#19981;&#20250;&#20135;&#29983;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.03730</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#38376;&#25511;&#26426;&#21046;&#22686;&#24378;&#23545;&#35805;&#36335;&#30001;
&lt;/p&gt;
&lt;p&gt;
Gated Mechanism Enhanced Multi-Task Learning for Dialog Routing. (arXiv:2304.03730v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03730
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#37325;&#35201;&#38382;&#39064;&#65306;&#22914;&#20309;&#21033;&#29992;&#38376;&#25511;&#26426;&#21046;&#22686;&#24378;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#20197;&#22312;&#21508;&#31181;&#23545;&#35805;&#25968;&#25454;&#20043;&#38388;&#21457;&#29616;&#25968;&#25454;-&#20219;&#21153;&#21644;&#20219;&#21153;-&#20219;&#21153;&#20851;&#31995;&#65292;&#25552;&#39640;&#23545;&#35805;&#36335;&#30001;&#25928;&#29575;&#65292;&#38477;&#20302;&#20154;&#21147;&#25104;&#26412;&#65292;&#22686;&#24378;&#29992;&#25143;&#20307;&#39564;&#12290;&#25552;&#20986;&#30340;&#38376;&#25511;&#26426;&#21046;&#22686;&#24378;&#30340;&#22810;&#20219;&#21153;&#27169;&#22411;&#20855;&#26377;&#23618;&#27425;&#21270;&#20449;&#24687;&#36807;&#28388;&#30340;&#20316;&#29992;&#65292;&#20854;&#23545;&#29616;&#26377;&#31995;&#32479;&#19981;&#20250;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#20154;-&#26426;&#22120;&#20154;&#21327;&#20316;&#23545;&#35805;&#31995;&#32479;&#65288;&#20363;&#22914;&#65292;&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#30340;&#21806;&#21069;&#21644;&#21806;&#21518;&#65289;&#26159;&#26080;&#22788;&#19981;&#22312;&#30340;&#65292;&#32780;&#23545;&#35805;&#36335;&#30001;&#32452;&#20214;&#23545;&#20110;&#25552;&#39640;&#25972;&#20307;&#25928;&#29575;&#65292;&#38477;&#20302;&#20154;&#21147;&#25104;&#26412;&#21644;&#22686;&#24378;&#29992;&#25143;&#20307;&#39564;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#21487;&#20197;&#28385;&#36275;&#36825;&#19968;&#35201;&#27714;&#65292;&#20294;&#23427;&#20204;&#21482;&#33021;&#23545;&#21333;&#19968;&#26469;&#28304;&#30340;&#23545;&#35805;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#19988;&#19981;&#33021;&#26377;&#25928;&#22320;&#25429;&#33719;&#25968;&#25454;&#21644;&#23376;&#20219;&#21153;&#20043;&#38388;&#30340;&#20851;&#31995;&#30340;&#22522;&#30784;&#30693;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24443;&#24213;&#25366;&#25496;&#21508;&#31181;&#31867;&#22411;&#30340;&#23545;&#35805;&#25968;&#25454;&#20043;&#38388;&#30340;&#25968;&#25454;-&#20219;&#21153;&#21644;&#20219;&#21153;-&#20219;&#21153;&#30693;&#35782;&#65292;&#26469;&#30740;&#31350;&#36825;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#20026;&#20102;&#23454;&#29616;&#19978;&#36848;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38376;&#25511;&#26426;&#21046;&#22686;&#24378;&#30340;&#22810;&#20219;&#21153;&#27169;&#22411;&#65288;G3M&#65289;&#65292;&#20855;&#20307;&#21253;&#25324;&#19968;&#31181;&#26032;&#22411;&#23545;&#35805;&#32534;&#30721;&#22120;&#21644;&#20004;&#20010;&#37327;&#36523;&#23450;&#21046;&#30340;&#38376;&#25511;&#26426;&#21046;&#27169;&#22359;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#21457;&#25381;&#23618;&#27425;&#21270;&#20449;&#24687;&#36807;&#28388;&#30340;&#20316;&#29992;&#65292;&#24182;&#19988;&#19981;&#20250;&#23545;&#29616;&#26377;&#30340;&#23545;&#35805;&#31995;&#32479;&#20135;&#29983;&#24433;&#21709;&#12290;&#22522;&#20110;&#20174;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#31243;&#24207;&#25910;&#38598;&#30340;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Currently, human-bot symbiosis dialog systems, e.g., pre- and after-sales in E-commerce, are ubiquitous, and the dialog routing component is essential to improve the overall efficiency, reduce human resource cost, and enhance user experience. Although most existing methods can fulfil this requirement, they can only model single-source dialog data and cannot effectively capture the underlying knowledge of relations among data and subtasks. In this paper, we investigate this important problem by thoroughly mining both the data-to-task and task-to-task knowledge among various kinds of dialog data. To achieve the above targets, we propose a Gated Mechanism enhanced Multi-task Model (G3M), specifically including a novel dialog encoder and two tailored gated mechanism modules. The proposed method can play the role of hierarchical information filtering and is non-invasive to existing dialog systems. Based on two datasets collected from real world applications, extensive experimental results d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#32479;&#19968;&#35821;&#35328;&#26816;&#26597;&#65288;UniLC&#65289;&#26041;&#27861;&#65292;&#26088;&#22312;&#26816;&#26597;&#35821;&#35328;&#36755;&#20837;&#30340;&#20107;&#23454;&#21644;&#20844;&#27491;&#24615;&#12290;LLM&#21487;&#20197;&#22312;&#19968;&#20010;&#31616;&#21333;&#12289;&#23569;&#37327;&#26679;&#26412;&#30340;&#25552;&#31034;&#38598;&#19978;&#23454;&#29616;&#20107;&#23454;&#26816;&#26597;&#12289;&#38472;&#35268;&#38475;&#20064;&#26816;&#27979;&#21644;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20219;&#21153;&#30340;&#39640;&#24615;&#33021;&#65292;&#36825;&#20026;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#32479;&#19968;&#35821;&#35328;&#26816;&#26597;&#25552;&#20379;&#20102;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.03728</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#32479;&#19968;&#35821;&#35328;&#26816;&#26597;
&lt;/p&gt;
&lt;p&gt;
Interpretable Unified Language Checking. (arXiv:2304.03728v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#32479;&#19968;&#35821;&#35328;&#26816;&#26597;&#65288;UniLC&#65289;&#26041;&#27861;&#65292;&#26088;&#22312;&#26816;&#26597;&#35821;&#35328;&#36755;&#20837;&#30340;&#20107;&#23454;&#21644;&#20844;&#27491;&#24615;&#12290;LLM&#21487;&#20197;&#22312;&#19968;&#20010;&#31616;&#21333;&#12289;&#23569;&#37327;&#26679;&#26412;&#30340;&#25552;&#31034;&#38598;&#19978;&#23454;&#29616;&#20107;&#23454;&#26816;&#26597;&#12289;&#38472;&#35268;&#38475;&#20064;&#26816;&#27979;&#21644;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20219;&#21153;&#30340;&#39640;&#24615;&#33021;&#65292;&#36825;&#20026;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#32479;&#19968;&#35821;&#35328;&#26816;&#26597;&#25552;&#20379;&#20102;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#19981;&#33391;&#34892;&#20026;&#25552;&#20986;&#20102;&#20851;&#27880;&#65292;&#21253;&#25324;&#38750;&#20107;&#23454;&#24615;&#12289;&#26377;&#20559;&#35265;&#21644;&#20805;&#28385;&#20167;&#24680;&#30340;&#35821;&#35328;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#65292;&#22522;&#20110;&#33258;&#28982;&#21644;&#31038;&#20250;&#30693;&#35782;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;LLM&#26412;&#36136;&#19978;&#26159;&#22810;&#20219;&#21153;&#35821;&#35328;&#26816;&#26597;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#32479;&#19968;&#35821;&#35328;&#26816;&#26597;&#65288;UniLC&#65289;&#26041;&#27861;&#65292;&#26088;&#22312;&#26816;&#26597;&#35821;&#35328;&#36755;&#20837;&#30340;&#20107;&#23454;&#21644;&#20844;&#27491;&#24615;&#12290;&#34429;&#28982;&#20844;&#24179;&#24615;&#21644;&#20107;&#23454;&#26816;&#26597;&#20219;&#21153;&#20197;&#21069;&#26159;&#30001;&#19981;&#21516;&#30340;&#27169;&#22411;&#22788;&#29702;&#30340;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;LLM&#21487;&#20197;&#22312;&#19968;&#20010;&#31616;&#21333;&#12289;&#23569;&#37327;&#26679;&#26412;&#30340;&#25552;&#31034;&#38598;&#19978;&#23454;&#29616;&#20107;&#23454;&#26816;&#26597;&#12289;&#38472;&#35268;&#38475;&#20064;&#26816;&#27979;&#21644;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20219;&#21153;&#30340;&#39640;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#30340;&#8220;&#19968;&#21322;&#26679;&#26412;&#8221;&#22810;&#20219;&#21153;&#35821;&#35328;&#26816;&#26597;&#26041;&#27861;&#65292;&#20351;&#24471;GPT3.5-turbo&#27169;&#22411;&#22312;&#22810;&#39033;&#35821;&#35328;&#20219;&#21153;&#19978;&#20248;&#20110;&#23436;&#20840;&#30417;&#30563;&#30340;&#22522;&#20934;&#32447;&#12290;&#36825;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#21644;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#24378;&#22823;&#30340;&#28508;&#22312;&#30693;&#35782;&#34920;&#31034;&#65292;LLM&#21487;&#33021;&#26159;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#32479;&#19968;&#35821;&#35328;&#26816;&#26597;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent concerns about undesirable behaviors generated by large language models (LLMs), including non-factual, biased, and hateful language, we find LLMs are inherent multi-task language checkers based on their latent representations of natural and social knowledge. We present an interpretable, unified, language checking (UniLC) method for both human and machine-generated language that aims to check if language input is factual and fair. While fairness and fact-checking tasks have been handled separately with dedicated models, we find that LLMs can achieve high performance on a combination of fact-checking, stereotype detection, and hate speech detection tasks with a simple, few-shot, unified set of prompts. With the ``1/2-shot'' multi-task language checking method proposed in this work, the GPT3.5-turbo model outperforms fully supervised baselines on several language tasks. The simple approach and results suggest that based on strong latent knowledge representations, an LLM can
&lt;/p&gt;</description></item><item><title>&#23545;&#27604;&#25439;&#22833;&#22312;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#24179;&#34913;&#25152;&#23398;&#34920;&#31034;&#65292;&#27491;&#23545;&#25512;&#21160;&#27169;&#22411;&#23545;&#40784;&#34920;&#31034;&#65292;&#32780;&#36127;&#23545;&#21017;&#20445;&#25345;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2304.03717</link><description>&lt;p&gt;
&#20851;&#20110;&#23545;&#27604;&#25439;&#22833;&#22312;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Importance of Contrastive Loss in Multimodal Learning. (arXiv:2304.03717v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03717
&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#25439;&#22833;&#22312;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#24179;&#34913;&#25152;&#23398;&#34920;&#31034;&#65292;&#27491;&#23545;&#25512;&#21160;&#27169;&#22411;&#23545;&#40784;&#34920;&#31034;&#65292;&#32780;&#36127;&#23545;&#21017;&#20445;&#25345;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65288;&#20363;&#22914; CLIP&#65288;Radford &#31561;&#20154;&#65292;2021&#65289;&#65289;&#22312;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20854;&#20013;&#27169;&#22411;&#23581;&#35797;&#26368;&#23567;&#21270;&#21516;&#19968;&#25968;&#25454;&#28857;&#30340;&#19981;&#21516;&#35270;&#22270;&#65288;&#20363;&#22914;&#22270;&#20687;&#21644;&#20854;&#26631;&#39064;&#65289;&#30340;&#34920;&#31034;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#21516;&#26102;&#20351;&#19981;&#21516;&#25968;&#25454;&#28857;&#30340;&#34920;&#31034;&#24444;&#27492;&#20998;&#31163;&#12290;&#28982;&#32780;&#65292;&#20174;&#29702;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#24403;&#25968;&#25454;&#19981;&#26159;&#21508;&#21521;&#21516;&#24615;&#26102;&#65292;&#23545;&#27604;&#23398;&#20064;&#22914;&#20309;&#26377;&#25928;&#22320;&#23398;&#20064;&#26469;&#33258;&#19981;&#21516;&#35270;&#22270;&#30340;&#34920;&#31034;&#20173;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#24182;&#34920;&#26126;&#23545;&#27604;&#23545;&#26159;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#24179;&#34913;&#25152;&#23398;&#34920;&#31034;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;&#23588;&#20854;&#26159;&#65292;&#25105;&#20204;&#34920;&#26126;&#27491;&#23545;&#26159;&#33021;&#22815;&#25512;&#21160;&#27169;&#22411;&#22312;&#22686;&#21152;&#26465;&#20214;&#25968;&#30340;&#20195;&#20215;&#19979;&#23545;&#40784;&#34920;&#31034;&#65292;&#32780;&#36127;&#23545;&#21017;&#38477;&#20302;&#26465;&#20214;&#25968;&#65292;&#20445;&#25345;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, contrastive learning approaches (e.g., CLIP (Radford et al., 2021)) have received huge success in multimodal learning, where the model tries to minimize the distance between the representations of different views (e.g., image and its caption) of the same data point while keeping the representations of different data points away from each other. However, from a theoretical perspective, it is unclear how contrastive learning can learn the representations from different views efficiently, especially when the data is not isotropic. In this work, we analyze the training dynamics of a simple multimodal contrastive learning model and show that contrastive pairs are important for the model to efficiently balance the learned representations. In particular, we show that the positive pairs will drive the model to align the representations at the cost of increasing the condition number, while the negative pairs will reduce the condition number, keeping the learned representations balance
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#25324;&#22235;&#20010;&#19981;&#21516;&#39046;&#22495;Bengali&#25991;&#26412;&#30340;&#26032;&#25968;&#25454;&#38598;- BenCoref&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#24110;&#21161;&#29702;&#35299;Bengali&#22810;&#20010;&#39046;&#22495;&#20013;&#20849;&#25351;&#28040;&#35299;&#29616;&#35937;&#30340;&#24046;&#24322;&#65292;&#24182;&#20419;&#36827;Bengali&#30340;&#36164;&#28304;&#24320;&#21457;&#12290;&#22810;&#20010;&#27169;&#22411;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#24615;&#33021;&#20063;&#24471;&#21040;&#20102;&#25253;&#21578;&#12290;&#22312;&#36328;&#35821;&#35328;&#27979;&#35797;&#20013;&#65292;&#20174;&#33521;&#35821;&#21040;Bengali&#30340;&#20132;&#21449;&#35821;&#35328;&#24615;&#33021;&#36739;&#24046;&#65292;&#26174;&#31034;&#20986;&#38656;&#35201;&#35821;&#35328;&#29305;&#23450;&#30340;&#20849;&#25351;&#28040;&#35299;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2304.03682</link><description>&lt;p&gt;
BenCoref:&#19968;&#31181;&#21517;&#35789;&#30701;&#35821;&#21644;&#20195;&#35789;&#25351;&#20195;&#27880;&#37322;&#30340;&#22810;&#39046;&#22495;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
BenCoref: A Multi-Domain Dataset of Nominal Phrases and Pronominal Reference Annotations. (arXiv:2304.03682v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03682
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#25324;&#22235;&#20010;&#19981;&#21516;&#39046;&#22495;Bengali&#25991;&#26412;&#30340;&#26032;&#25968;&#25454;&#38598;- BenCoref&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#24110;&#21161;&#29702;&#35299;Bengali&#22810;&#20010;&#39046;&#22495;&#20013;&#20849;&#25351;&#28040;&#35299;&#29616;&#35937;&#30340;&#24046;&#24322;&#65292;&#24182;&#20419;&#36827;Bengali&#30340;&#36164;&#28304;&#24320;&#21457;&#12290;&#22810;&#20010;&#27169;&#22411;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#24615;&#33021;&#20063;&#24471;&#21040;&#20102;&#25253;&#21578;&#12290;&#22312;&#36328;&#35821;&#35328;&#27979;&#35797;&#20013;&#65292;&#20174;&#33521;&#35821;&#21040;Bengali&#30340;&#20132;&#21449;&#35821;&#35328;&#24615;&#33021;&#36739;&#24046;&#65292;&#26174;&#31034;&#20986;&#38656;&#35201;&#35821;&#35328;&#29305;&#23450;&#30340;&#20849;&#25351;&#28040;&#35299;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20849;&#25351;&#28040;&#35299;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#19968;&#20010;&#34987;&#24191;&#27867;&#30740;&#31350;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#30456;&#20851;&#25968;&#25454;&#38598;&#65292;Bengali &#30340;&#20849;&#25351;&#28040;&#35299;&#30740;&#31350;&#20027;&#35201;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;BenCoref&#65292;&#21253;&#25324;&#26469;&#33258;&#22235;&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;Bengali&#25991;&#26412;&#30340;&#20849;&#25351;&#27880;&#37322;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;5200&#20010;&#25552;&#21450;&#27880;&#37322;&#65292;&#24418;&#25104;48,569&#20010;&#26631;&#35760;&#20013;&#30340;502&#20010;&#25552;&#21450;&#31751;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#21019;&#24314;&#27492;&#25968;&#25454;&#38598;&#30340;&#36807;&#31243;&#65292;&#24182;&#25253;&#21578;&#20102;&#20351;&#29992;BenCoref&#35757;&#32451;&#30340;&#22810;&#20010;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#39044;&#35745;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;&#25581;&#31034;Bengali&#22810;&#20010;&#39046;&#22495;&#20013;&#20849;&#25351;&#29616;&#35937;&#30340;&#24046;&#24322;&#65292;&#24182;&#40723;&#21169;&#24320;&#21457;&#20854;&#20182;Bengali&#36164;&#28304;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#20174;&#33521;&#35821;&#21040;Bengali&#30340;&#20132;&#21449;&#35821;&#35328;&#24615;&#33021;&#24456;&#24046;&#65292;&#36825;&#31361;&#26174;&#20986;&#38656;&#35201;&#35821;&#35328;&#29305;&#23450;&#30340;&#20849;&#25351;&#28040;&#35299;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Coreference Resolution is a well studied problem in NLP. While widely studied for English and other resource-rich languages, research on coreference resolution in Bengali largely remains unexplored due to the absence of relevant datasets. Bengali, being a low-resource language, exhibits greater morphological richness compared to English. In this article, we introduce a new dataset, BenCoref, comprising coreference annotations for Bengali texts gathered from four distinct domains. This relatively small dataset contains 5200 mention annotations forming 502 mention clusters within 48,569 tokens. We describe the process of creating this dataset and report performance of multiple models trained using BenCoref. We anticipate that our work sheds some light on the variations in coreference phenomena across multiple domains in Bengali and encourages the development of additional resources for Bengali. Furthermore, we found poor crosslingual performance at zero-shot setting from English, highlig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#31616;&#21333;&#30340;&#32447;&#24615;&#21333;&#20803;RNN&#22312;&#38271;&#24207;&#21015;&#35745;&#25968;&#38382;&#39064;&#19978;&#30340;&#26497;&#38480;&#65292;&#29702;&#35770;&#19978;&#20351;&#29992;&#30340;&#26465;&#20214;&#20855;&#26377;&#20805;&#20998;&#24517;&#35201;&#24615;&#65307;&#23454;&#39564;&#25968;&#25454;&#34920;&#26126;&#65292;&#36890;&#36807;&#26631;&#20934;&#30340;&#26041;&#27861;&#65292;&#35813;&#32593;&#32476;&#36890;&#24120;&#26080;&#27861;&#23454;&#29616;&#20934;&#30830;&#30340;&#35745;&#25968;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2304.03639</link><description>&lt;p&gt;
&#32447;&#24615;&#36882;&#24402;&#32593;&#32476;&#22312;&#38271;&#24207;&#21015;&#35745;&#25968;&#38382;&#39064;&#19978;&#30340;&#29702;&#35770;&#19982;&#23454;&#39564;&#22256;&#22659;
&lt;/p&gt;
&lt;p&gt;
Theoretical Conditions and Empirical Failure of Bracket Counting on Long Sequences with Linear Recurrent Networks. (arXiv:2304.03639v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#31616;&#21333;&#30340;&#32447;&#24615;&#21333;&#20803;RNN&#22312;&#38271;&#24207;&#21015;&#35745;&#25968;&#38382;&#39064;&#19978;&#30340;&#26497;&#38480;&#65292;&#29702;&#35770;&#19978;&#20351;&#29992;&#30340;&#26465;&#20214;&#20855;&#26377;&#20805;&#20998;&#24517;&#35201;&#24615;&#65307;&#23454;&#39564;&#25968;&#25454;&#34920;&#26126;&#65292;&#36890;&#36807;&#26631;&#20934;&#30340;&#26041;&#27861;&#65292;&#35813;&#32593;&#32476;&#36890;&#24120;&#26080;&#27861;&#23454;&#29616;&#20934;&#30830;&#30340;&#35745;&#25968;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20855;&#26377;&#26080;&#38480;&#28608;&#27963;&#20989;&#25968;&#30340;RNN&#26377;&#35745;&#25968;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;RNN&#30340;&#26377;&#25928;&#35757;&#32451;&#24448;&#24448;&#22256;&#38590;&#65292;&#36890;&#24120;&#26080;&#27861;&#23398;&#20064;&#20934;&#30830;&#30340;&#35745;&#25968;&#34892;&#20026;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#26368;&#31616;&#21333;&#30340;&#32447;&#24615;&#21333;&#20803;RNN&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;&#32447;&#24615;RNN&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#30830;&#23450;&#20102;&#27169;&#22411;&#23637;&#29616;&#31934;&#30830;&#35745;&#25968;&#34892;&#20026;&#30340;&#26465;&#20214;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#20123;&#26465;&#20214;&#26159;&#24517;&#35201;&#19988;&#20805;&#20998;&#30340;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#28041;&#21450;&#31867;&#20284;Dyck-1&#24179;&#34913;&#31526;&#21495;&#30340;&#20219;&#21153;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#35774;&#32622;&#19979;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#12290;&#25105;&#20204;&#21457;&#29616;&#32447;&#24615;RNN&#36890;&#24120;&#26080;&#27861;&#22312;&#26631;&#20934;&#26041;&#27861;&#35757;&#32451;&#19979;&#28385;&#36275;&#35745;&#25968;&#34892;&#20026;&#30340;&#24517;&#35201;&#19988;&#20805;&#20998;&#26465;&#20214;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#35757;&#32451;&#24207;&#21015;&#38271;&#24230;&#21644;&#21033;&#29992;&#19981;&#21516;&#30340;&#30446;&#26631;&#31867;&#21035;&#23545;&#27169;&#22411;&#34892;&#20026;&#22312;&#35757;&#32451;&#26399;&#38388;&#21644;&#35745;&#25968;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous work has established that RNNs with an unbounded activation function have the capacity to count exactly. However, it has also been shown that RNNs are challenging to train effectively and generally do not learn exact counting behaviour. In this paper, we focus on this problem by studying the simplest possible RNN, a linear single-cell network. We conduct a theoretical analysis of linear RNNs and identify conditions for the models to exhibit exact counting behaviour. We provide a formal proof that these conditions are necessary and sufficient. We also conduct an empirical analysis using tasks involving a Dyck-1-like Balanced Bracket language under two different settings. We observe that linear RNNs generally do not meet the necessary and sufficient conditions for counting behaviour when trained with the standard approach. We investigate how varying the length of training sequences and utilising different target classes impacts model behaviour during training and the ability of 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#24515;&#29702;&#23398;&#30340;&#20215;&#20540;&#29702;&#35770;&#27979;&#35797;&#20102;ChatGPT&#20013;&#30340;&#20215;&#20540;&#20559;&#24046;&#65292;&#24182;&#27809;&#26377;&#21457;&#29616;&#26126;&#26174;&#30340;&#20215;&#20540;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2304.03612</link><description>&lt;p&gt;
ChatGPT&#23545;&#20110;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#21453;&#26144;&#26159;&#20160;&#20040;&#65311;&#20351;&#29992;&#25551;&#36848;&#24615;&#20215;&#20540;&#29702;&#35770;&#25506;&#32034;ChatGPT&#20013;&#30340;&#20215;&#20540;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
What does ChatGPT return about human values? Exploring value bias in ChatGPT using a descriptive value theory. (arXiv:2304.03612v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#24515;&#29702;&#23398;&#30340;&#20215;&#20540;&#29702;&#35770;&#27979;&#35797;&#20102;ChatGPT&#20013;&#30340;&#20215;&#20540;&#20559;&#24046;&#65292;&#24182;&#27809;&#26377;&#21457;&#29616;&#26126;&#26174;&#30340;&#20215;&#20540;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25152;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#23384;&#22312;&#30340;&#24847;&#35782;&#24418;&#24577;&#22522;&#30784;&#21644;&#28508;&#22312;&#27495;&#35270;&#38382;&#39064;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#24515;&#29702;&#23398;&#30340;&#20215;&#20540;&#29702;&#35770;&#26469;&#27979;&#35797;ChatGPT&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#20215;&#20540;&#20559;&#24046;&#12290;&#36890;&#36807;OpenAI API&#21453;&#22797;&#25552;&#31034;ChatGPT&#29983;&#25104;&#25991;&#26412;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#29702;&#35770;&#39537;&#21160;&#30340;&#20215;&#20540;&#35789;&#20856;&#21644;&#35789;&#34955;&#27861;&#23545;&#29983;&#25104;&#30340;&#35821;&#26009;&#24211;&#30340;&#20215;&#20540;&#20869;&#23481;&#36827;&#34892;&#20998;&#26512;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#27809;&#26377;&#21457;&#29616;&#26126;&#26174;&#30340;&#20215;&#20540;&#20559;&#24046;&#12290;&#32467;&#26524;&#31526;&#21512;&#24515;&#29702;&#27169;&#22411;&#30340;&#29702;&#35770;&#39044;&#27979;&#65292;&#29983;&#25104;&#30340;&#25991;&#26412;&#34920;&#29616;&#20986;&#36275;&#22815;&#30340;&#26500;&#25928;&#25928;&#24230;&#65292;&#36825;&#34920;&#26126;&#20215;&#20540;&#20869;&#23481;&#34987;&#39640;&#24230;&#20445;&#30041;&#21040;&#20102;&#29983;&#25104;&#30340;&#36755;&#20986;&#20013;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#26576;&#20123;&#31038;&#20250;&#21462;&#21521;&#20215;&#20540;&#30340;&#34701;&#21512;&#65292;&#36825;&#21487;&#33021;&#34920;&#26126;ChatGPT&#36890;&#36807;&#36755;&#20986;&#21512;&#29702;&#22320;&#21453;&#26144;&#20102;&#20154;&#31867;&#30340;&#20215;&#20540;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been concern about ideological basis and possible discrimination in text generated by Large Language Models (LLMs). We test possible value biases in ChatGPT using a psychological value theory. We designed a simple experiment in which we used a number of different probes derived from the Schwartz basic value theory (items from the revised Portrait Value Questionnaire, the value type definitions, value names). We prompted ChatGPT via the OpenAI API repeatedly to generate text and then analyzed the generated corpus for value content with a theory-driven value dictionary using a bag of words approach. Overall, we found little evidence of explicit value bias. The results showed sufficient construct and discriminant validity for the generated text in line with the theoretical predictions of the psychological model, which suggests that the value content was carried through into the outputs with high fidelity. We saw some merging of socially oriented values, which may suggest that th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#23457;&#33258;&#21160;&#25552;&#31034;&#25216;&#26415;&#22312;&#20845;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;&#26356;&#24191;&#27867;&#33539;&#22260;&#30340;K-shot&#23398;&#20064;&#35774;&#32622;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#33258;&#21160;&#25552;&#31034;&#24182;&#19981;&#33021;&#22987;&#32456;&#20248;&#20110;&#25163;&#21160;&#25552;&#31034;&#65292;&#22240;&#27492;&#25163;&#21160;&#25552;&#31034;&#24212;&#35813;&#20316;&#20026;&#33258;&#21160;&#25552;&#31034;&#30340;&#19968;&#20010;&#22522;&#20934;&#32447;&#12290;</title><link>http://arxiv.org/abs/2304.03609</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#33258;&#21160;&#25552;&#31034;&#65306;&#25105;&#20204;&#30495;&#30340;&#20570;&#24471;&#26356;&#22909;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Revisiting Automated Prompting: Are We Actually Doing Better?. (arXiv:2304.03609v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#23457;&#33258;&#21160;&#25552;&#31034;&#25216;&#26415;&#22312;&#20845;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;&#26356;&#24191;&#27867;&#33539;&#22260;&#30340;K-shot&#23398;&#20064;&#35774;&#32622;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#33258;&#21160;&#25552;&#31034;&#24182;&#19981;&#33021;&#22987;&#32456;&#20248;&#20110;&#25163;&#21160;&#25552;&#31034;&#65292;&#22240;&#27492;&#25163;&#21160;&#25552;&#31034;&#24212;&#35813;&#20316;&#20026;&#33258;&#21160;&#25552;&#31034;&#30340;&#19968;&#20010;&#22522;&#20934;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#25991;&#29486;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#26159;&#20986;&#33394;&#30340;&#20960;&#20046;&#19981;&#29992;&#23398;&#20064;&#30340;&#23398;&#20064;&#32773;&#65292;&#22312;&#20960;&#20046;&#19981;&#29992;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#31034;&#26174;&#30528;&#25552;&#39640;&#20102;&#23427;&#20204;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#38543;&#21518;&#36827;&#34892;&#20102;&#35797;&#22270;&#33258;&#21160;&#21270;&#20154;&#31867;&#25552;&#31034;&#30340;&#23581;&#35797;&#65292;&#24182;&#21462;&#24471;&#20102;&#19968;&#23450;&#36827;&#23637;&#12290;&#29305;&#21035;&#26159;&#65292;&#38543;&#21518;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;K-shot&#23398;&#20064;&#22330;&#26223;&#20013;&#65292;&#33258;&#21160;&#21270;&#21487;&#20197;&#20248;&#20110;&#24494;&#35843;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#33258;&#21160;&#25552;&#31034;&#22312;&#20845;&#20010;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#21644;&#26356;&#22823;&#33539;&#22260;&#30340;K-shot&#23398;&#20064;&#35774;&#32622;&#19978;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#33258;&#21160;&#25552;&#31034;&#19981;&#33021;&#22987;&#32456;&#20248;&#20110;&#31616;&#21333;&#30340;&#25163;&#21160;&#25552;&#31034;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#22312;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#20013;&#65292;&#38500;&#20102;&#24494;&#35843;&#20043;&#22806;&#65292;&#25163;&#21160;&#25552;&#31034;&#24212;&#20316;&#20026;&#22522;&#32447;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current literature demonstrates that Large Language Models (LLMs) are great few-shot learners, and prompting significantly increases their performance on a range of downstream tasks in a few-shot learning setting. An attempt to automate human-led prompting followed, with some progress achieved. In particular, subsequent work demonstrates automation can outperform fine-tuning in certain K-shot learning scenarios.  In this paper, we revisit techniques for automated prompting on six different downstream tasks and a larger range of K-shot learning settings. We find that automated prompting does not consistently outperform simple manual prompts. Our work suggests that, in addition to fine-tuning, manual prompts should be used as a baseline in this line of research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21333;&#21457;&#35328;&#20154;&#27874;&#26031;&#35821;&#25968;&#25454;&#38598;ArmanTTS&#65292;&#24182;&#20351;&#29992;Tacotron2&#21644;HiFi GAN&#27169;&#22411;&#35774;&#35745;&#20102;&#19968;&#31181;&#23558;&#38899;&#32032;&#20316;&#20026;&#36755;&#20837;&#24182;&#29983;&#25104;&#30456;&#24212;&#35821;&#38899;&#36755;&#20986;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.03585</link><description>&lt;p&gt;
ArmanTTS&#21333;&#21457;&#35328;&#20154;&#27874;&#26031;&#35821;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ArmanTTS single-speaker Persian dataset. (arXiv:2304.03585v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21333;&#21457;&#35328;&#20154;&#27874;&#26031;&#35821;&#25968;&#25454;&#38598;ArmanTTS&#65292;&#24182;&#20351;&#29992;Tacotron2&#21644;HiFi GAN&#27169;&#22411;&#35774;&#35745;&#20102;&#19968;&#31181;&#23558;&#38899;&#32032;&#20316;&#20026;&#36755;&#20837;&#24182;&#29983;&#25104;&#30456;&#24212;&#35821;&#38899;&#36755;&#20986;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
TTS&#65292;&#21363;&#25991;&#26412;&#36716;&#35821;&#38899;&#65292;&#26159;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#36866;&#24403;&#24314;&#27169;&#21487;&#20197;&#23436;&#25104;&#30340;&#19968;&#31181;&#22797;&#26434;&#36807;&#31243;&#12290;&#20026;&#20102;&#23454;&#29616;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#38656;&#35201;&#19968;&#20010;&#21512;&#36866;&#30340;&#25968;&#25454;&#38598;&#12290;&#30001;&#20110;&#22312;&#27874;&#26031;&#35821;&#39046;&#22495;&#20013;&#20570;&#20102;&#36739;&#23569;&#30340;&#24037;&#20316;&#65292;&#22240;&#27492;&#26412;&#25991;&#23558;&#20171;&#32461;&#21333;&#19968;&#35828;&#35805;&#20154;&#25968;&#25454;&#38598;&#65306;ArmanTTS&#12290;&#25105;&#20204;&#23558;&#27492;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#19982;&#21508;&#31181;&#26222;&#36941;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#20197;&#35777;&#26126;ArmanTTS&#31526;&#21512;&#25945;&#25480;&#27874;&#26031;&#25991;&#26412;&#21040;&#35821;&#38899;&#36716;&#25442;&#27169;&#22411;&#25152;&#38656;&#30340;&#26631;&#20934;&#12290;&#25105;&#20204;&#36824;&#32467;&#21512;&#20102;Tacotron 2&#21644;HiFi GAN&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#21487;&#20197;&#23558;&#38899;&#32032;&#20316;&#20026;&#36755;&#20837;&#65292;&#36755;&#20986;&#23545;&#24212;&#30340;&#35821;&#38899;&#12290;&#20174;&#30495;&#23454;&#35821;&#38899;&#20013;&#33719;&#24471;&#20102;4.0&#30340;MOS&#20540;&#65292;&#29992;&#22768;&#30721;&#22120;&#39044;&#27979;&#33719;&#24471;&#20102;3.87&#30340;&#20540;&#65292;&#24182;&#29992;TTS&#27169;&#22411;&#29983;&#25104;&#30340;&#21512;&#25104;&#35821;&#38899;&#33719;&#24471;&#20102;2.98&#30340;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
TTS, or text-to-speech, is a complicated process that can be accomplished through appropriate modeling using deep learning methods. In order to implement deep learning models, a suitable dataset is required. Since there is a scarce amount of work done in this field for the Persian language, this paper will introduce the single speaker dataset: ArmanTTS. We compared the characteristics of this dataset with those of various prevalent datasets to prove that ArmanTTS meets the necessary standards for teaching a Persian text-to-speech conversion model. We also combined the Tacotron 2 and HiFi GAN to design a model that can receive phonemes as input, with the output being the corresponding speech. 4.0 value of MOS was obtained from real speech, 3.87 value was obtained by the vocoder prediction and 2.98 value was reached with the synthetic speech generated by the TTS model.
&lt;/p&gt;</description></item><item><title>GEMINI&#27169;&#22411;&#23558;&#21477;&#23376;&#37325;&#20889;&#21644;&#34701;&#21512;&#25216;&#26415;&#38598;&#25104;&#65292;&#23454;&#29616;&#20102;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#20013;&#30340;&#21477;&#23376;&#32423;&#20889;&#20316;&#39118;&#26684;&#25511;&#21046;&#65292;&#35813;&#33258;&#36866;&#24212;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#25454;&#38598;&#20855;&#26377;&#24179;&#34913;&#30340;&#39118;&#26684;&#20998;&#24067;&#26102;&#12290;</title><link>http://arxiv.org/abs/2304.03548</link><description>&lt;p&gt;
GEMINI&#65306;&#38024;&#23545;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#25511;&#21046;&#21477;&#23376;&#32423;&#20889;&#20316;&#39118;&#26684;
&lt;/p&gt;
&lt;p&gt;
GEMINI: Controlling the Sentence-level Writing Style for Abstractive Text Summarization. (arXiv:2304.03548v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03548
&lt;/p&gt;
&lt;p&gt;
GEMINI&#27169;&#22411;&#23558;&#21477;&#23376;&#37325;&#20889;&#21644;&#34701;&#21512;&#25216;&#26415;&#38598;&#25104;&#65292;&#23454;&#29616;&#20102;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#20013;&#30340;&#21477;&#23376;&#32423;&#20889;&#20316;&#39118;&#26684;&#25511;&#21046;&#65292;&#35813;&#33258;&#36866;&#24212;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#25454;&#38598;&#20855;&#26377;&#24179;&#34913;&#30340;&#39118;&#26684;&#20998;&#24067;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#19987;&#23478;&#20351;&#29992;&#19981;&#21516;&#25216;&#26415;&#32534;&#20889;&#25688;&#35201;&#65292;&#21253;&#25324;&#37325;&#20889;&#25991;&#26723;&#20013;&#30340;&#21477;&#23376;&#25110;&#21512;&#24182;&#22810;&#20010;&#21477;&#23376;&#29983;&#25104;&#25688;&#35201;&#21477;&#12290;&#36825;&#20123;&#25216;&#26415;&#26159;&#28789;&#27963;&#30340;&#65292;&#22240;&#27492;&#24456;&#38590;&#36890;&#36807;&#20219;&#20309;&#21333;&#19968;&#26041;&#27861;&#27169;&#25311;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#27169;&#22411;GEMINI&#65292;&#23558;&#37325;&#20889;&#22120;&#21644;&#34701;&#21512;&#22120;&#38598;&#25104;&#36215;&#26469;&#65292;&#20197;&#27169;&#25311;&#21477;&#23376;&#37325;&#20889;&#21644;&#34701;&#21512;&#25216;&#26415;&#12290;GEMINI&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#37325;&#20889;&#29305;&#23450;&#30340;&#25991;&#26723;&#21477;&#23376;&#25110;&#20174;&#22836;&#29983;&#25104;&#25688;&#35201;&#21477;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#33258;&#36866;&#24212;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#32431;&#25277;&#35937;&#21644;&#37325;&#20889;&#22522;&#32447;&#65292;&#29305;&#21035;&#26159;&#24403;&#25968;&#25454;&#38598;&#20855;&#26377;&#24179;&#34913;&#30340;&#39118;&#26684;&#20998;&#24067;&#26102;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27599;&#20010;&#25688;&#35201;&#21477;&#30340;&#20154;&#31867;&#20889;&#20316;&#39118;&#26684;&#22312;&#20854;&#19978;&#19979;&#25991;&#20013;&#26159;&#21487;&#20197;&#39044;&#27979;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human experts write summaries using different techniques, including rewriting a sentence in the document or fusing multiple sentences to generate a summary sentence. These techniques are flexible and thus difficult to be imitated by any single method. To address this issue, we propose an adaptive model, GEMINI, that integrates a rewriter and a fuser to mimic the sentence rewriting and fusion techniques, respectively. GEMINI adaptively chooses to rewrite a specific document sentence or generate a summary sentence from scratch. Experiments demonstrate that our adaptive approach outperforms the pure abstractive and rewriting baselines on various benchmark datasets, especially when the dataset has a balanced distribution of styles. Interestingly, empirical results show that the human writing style of each summary sentence is consistently predictable given its context.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#30340;&#36328;&#35821;&#35328;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;InfoCTM&#65292;&#36890;&#36807;&#20027;&#39064;&#23545;&#40784;&#26041;&#27861;&#35268;&#33539;&#21270;&#20027;&#39064;&#29983;&#25104;&#24182;&#23547;&#25214;&#26356;&#22810;&#38142;&#25509;&#30340;&#36328;&#35821;&#35328;&#35789;&#27719;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#37325;&#22797;&#20027;&#39064;&#21644;&#20302;&#35206;&#30422;&#23383;&#20856;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.03544</link><description>&lt;p&gt;
InfoCTM: &#19968;&#31181;&#22522;&#20110;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#30340;&#36328;&#35821;&#35328;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
InfoCTM: A Mutual Information Maximization Perspective of Cross-Lingual Topic Modeling. (arXiv:2304.03544v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#30340;&#36328;&#35821;&#35328;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;InfoCTM&#65292;&#36890;&#36807;&#20027;&#39064;&#23545;&#40784;&#26041;&#27861;&#35268;&#33539;&#21270;&#20027;&#39064;&#29983;&#25104;&#24182;&#23547;&#25214;&#26356;&#22810;&#38142;&#25509;&#30340;&#36328;&#35821;&#35328;&#35789;&#27719;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#37325;&#22797;&#20027;&#39064;&#21644;&#20302;&#35206;&#30422;&#23383;&#20856;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#35821;&#35328;&#20027;&#39064;&#27169;&#22411;&#22312;&#36328;&#35821;&#35328;&#25991;&#26412;&#20998;&#26512;&#20013;&#20351;&#29992;&#24191;&#27867;&#65292;&#21487;&#20197;&#25581;&#31034;&#23545;&#40784;&#30340;&#28508;&#22312;&#20027;&#39064;&#12290;&#20294;&#26159;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#20135;&#29983;&#37325;&#22797;&#20027;&#39064;&#30340;&#38382;&#39064;&#20197;&#21450;&#30001;&#20110;&#20302;&#35206;&#30422;&#23383;&#20856;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20114;&#20449;&#24687;&#30340;&#36328;&#35821;&#35328;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;InfoCTM&#12290;&#19982;&#20808;&#21069;&#24037;&#20316;&#20013;&#30340;&#30452;&#25509;&#23545;&#40784;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20114;&#20449;&#24687;&#30340;&#20027;&#39064;&#23545;&#40784;&#26041;&#27861;&#12290;&#36825;&#21487;&#20316;&#20026;&#35268;&#33539;&#21270;&#65292;&#20197;&#27491;&#30830;&#23545;&#40784;&#20027;&#39064;&#24182;&#38450;&#27490;&#35789;&#30340;&#36864;&#21270;&#20027;&#39064;&#34920;&#31034;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#37325;&#22797;&#20027;&#39064;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#20302;&#35206;&#30422;&#23383;&#20856;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#35821;&#35328;&#35789;&#27719;&#38142;&#25509;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#32473;&#23450;&#23383;&#20856;&#30340;&#32763;&#35793;&#20197;&#22806;&#65292;&#23547;&#25214;&#26356;&#22810;&#38142;&#25509;&#30340;&#36328;&#35821;&#35328;&#35789;&#27719;&#36827;&#34892;&#20027;&#39064;&#23545;&#40784;&#12290;&#22312;&#33521;&#35821;&#65292;&#20013;&#25991;&#21644;&#26085;&#35821;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#32447;&#65292;&#20135;&#29983;&#26356;&#22810;&#22810;&#26679;&#21270;&#19988;&#20449;&#24687;&#20016;&#23500;&#30340;&#20027;&#39064;&#65292;&#21516;&#26102;&#19981;&#25439;&#23475;&#36328;&#35821;&#35328;&#23545;&#40784;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-lingual topic models have been prevalent for cross-lingual text analysis by revealing aligned latent topics. However, most existing methods suffer from producing repetitive topics that hinder further analysis and performance decline caused by low-coverage dictionaries. In this paper, we propose the Cross-lingual Topic Modeling with Mutual Information (InfoCTM). Instead of the direct alignment in previous work, we propose a topic alignment with mutual information method. This works as a regularization to properly align topics and prevent degenerate topic representations of words, which mitigates the repetitive topic issue. To address the low-coverage dictionary issue, we further propose a cross-lingual vocabulary linking method that finds more linked cross-lingual words for topic alignment beyond the translations of a given dictionary. Extensive experiments on English, Chinese, and Japanese datasets demonstrate that our method outperforms state-of-the-art baselines, producing more
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;GenExpan&#65292;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#20307;&#38598;&#25193;&#23637;&#26694;&#26550;&#65292;&#21033;&#29992;&#21069;&#32512;&#26641;&#20445;&#35777;&#23454;&#20307;&#29983;&#25104;&#30340;&#26377;&#25928;&#24615;&#65292;&#37319;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#31867;&#21517;&#26469;&#24341;&#23548;&#27169;&#22411;&#29983;&#25104;&#21516;&#19968;&#31867;&#23454;&#20307;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.03531</link><description>&lt;p&gt;
&#20174;&#26816;&#32034;&#21040;&#29983;&#25104;&#65306;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#23454;&#20307;&#38598;&#25193;&#23637;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
From Retrieval to Generation: Efficient and Effective Entity Set Expansion. (arXiv:2304.03531v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;GenExpan&#65292;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#20307;&#38598;&#25193;&#23637;&#26694;&#26550;&#65292;&#21033;&#29992;&#21069;&#32512;&#26641;&#20445;&#35777;&#23454;&#20307;&#29983;&#25104;&#30340;&#26377;&#25928;&#24615;&#65292;&#37319;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#31867;&#21517;&#26469;&#24341;&#23548;&#27169;&#22411;&#29983;&#25104;&#21516;&#19968;&#31867;&#23454;&#20307;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#38598;&#25193;&#23637;&#65288;ESE&#65289;&#26159;&#19968;&#39033;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#25193;&#23637;&#30001;&#23567;&#30340;&#31181;&#23376;&#23454;&#20307;&#38598;&#25551;&#36848;&#30340;&#30446;&#26631;&#35821;&#20041;&#31867;&#30340;&#23454;&#20307;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;ESE&#26041;&#27861;&#26159;&#22522;&#20110;&#26816;&#32034;&#30340;&#26694;&#26550;&#65292;&#38656;&#35201;&#25552;&#21462;&#23454;&#20307;&#30340;&#19978;&#19979;&#25991;&#29305;&#24449;&#65292;&#24182;&#35745;&#31639;&#31181;&#23376;&#23454;&#20307;&#21644;&#20505;&#36873;&#23454;&#20307;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20004;&#20010;&#30446;&#30340;&#65292;&#23427;&#20204;&#24517;&#39035;&#36845;&#20195;&#22320;&#36941;&#21382;&#35821;&#26009;&#24211;&#21644;&#25968;&#25454;&#38598;&#20013;&#25552;&#20379;&#30340;&#23454;&#20307;&#35789;&#27719;&#65292;&#23548;&#33268;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#36739;&#24046;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#26816;&#32034;&#30340;ESE&#26041;&#27861;&#28040;&#32791;&#30340;&#26102;&#38388;&#19982;&#23454;&#20307;&#35789;&#27719;&#21644;&#35821;&#26009;&#24211;&#30340;&#22823;&#23567;&#25104;&#32447;&#24615;&#22686;&#38271;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;ESE&#26694;&#26550;&#65292;Generative Entity Set Expansion (GenExpan)&#65292;&#23427;&#21033;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#23436;&#25104;ESE&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#37319;&#29992;&#21069;&#32512;&#26641;&#26469;&#20445;&#35777;&#23454;&#20307;&#29983;&#25104;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#37319;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#31867;&#21517;&#26469;&#24341;&#23548;&#27169;&#22411;&#29983;&#25104;&#21516;&#19968;&#31867;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity Set Expansion (ESE) is a critical task aiming to expand entities of the target semantic class described by a small seed entity set. Most existing ESE methods are retrieval-based frameworks that need to extract the contextual features of entities and calculate the similarity between seed entities and candidate entities. To achieve the two purposes, they should iteratively traverse the corpus and the entity vocabulary provided in the datasets, resulting in poor efficiency and scalability. The experimental results indicate that the time consumed by the retrieval-based ESE methods increases linearly with entity vocabulary and corpus size. In this paper, we firstly propose a generative ESE framework, Generative Entity Set Expansion (GenExpan), which utilizes a generative pre-trained language model to accomplish ESE task. Specifically, a prefix tree is employed to guarantee the validity of entity generation, and automatically generated class names are adopted to guide the model to gen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#20351;&#29992;&#32454;&#35843;BERT&#27169;&#22411;&#21644;&#22810;&#25968;&#25237;&#31080;&#38598;&#25104;&#27169;&#22411;&#26469;&#26816;&#27979;&#21644;&#35299;&#37322;&#22312;&#32447;&#24615;&#21035;&#27495;&#35270;&#30340;&#26041;&#27861;&#12290;&#32763;&#36716;&#26174;&#30528;&#38477;&#20302;&#20102;&#22899;&#24615;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#32463;&#21382;&#19981;&#25104;&#27604;&#20363;&#30340;&#24615;&#21035;&#27495;&#35270;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2304.03518</link><description>&lt;p&gt;
SSS&#22312;SemEval-2023&#20219;&#21153;10&#20013;&#30340;&#35770;&#25991;&#65306;&#20351;&#29992;&#25237;&#31080;&#32454;&#35843;&#21464;&#21387;&#22120;&#21487;&#35299;&#37322;&#30340;&#26816;&#27979;&#22312;&#32447;&#24615;&#21035;&#27495;&#35270;&#12290; (arXiv&#65306;2304.03518v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
SSS at SemEval-2023 Task 10: Explainable Detection of Online Sexism using Majority Voted Fine-Tuned Transformers. (arXiv:2304.03518v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#20351;&#29992;&#32454;&#35843;BERT&#27169;&#22411;&#21644;&#22810;&#25968;&#25237;&#31080;&#38598;&#25104;&#27169;&#22411;&#26469;&#26816;&#27979;&#21644;&#35299;&#37322;&#22312;&#32447;&#24615;&#21035;&#27495;&#35270;&#30340;&#26041;&#27861;&#12290;&#32763;&#36716;&#26174;&#30528;&#38477;&#20302;&#20102;&#22899;&#24615;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#32463;&#21382;&#19981;&#25104;&#27604;&#20363;&#30340;&#24615;&#21035;&#27495;&#35270;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#22312;SemEval 2023&#20219;&#21153;10&#20013;&#25552;&#20132;&#30340;&#20316;&#21697;-&#21487;&#35299;&#37322;&#30340;&#22312;&#32447;&#24615;&#21035;&#27495;&#35270;&#26816;&#27979;&#65288;EDOS&#65289;&#65292;&#20998;&#20026;&#19977;&#20010;&#23376;&#20219;&#21153;&#12290;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#30340;&#19981;&#26029;&#22686;&#38271;&#23548;&#33268;&#22899;&#24615;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#38754;&#20020;&#19981;&#25104;&#27604;&#20363;&#30340;&#24615;&#21035;&#27495;&#35270;&#12290;&#36825;&#20351;&#24471;&#26816;&#27979;&#21644;&#35299;&#37322;&#22312;&#32447;&#24615;&#21035;&#27495;&#35270;&#20869;&#23481;&#21464;&#24471;&#27604;&#20197;&#24448;&#26356;&#21152;&#37325;&#35201;&#65292;&#20197;&#20351;&#31038;&#20132;&#23186;&#20307;&#23545;&#22899;&#24615;&#26356;&#21152;&#23433;&#20840;&#21644;&#21487;&#35775;&#38382;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#23454;&#39564;&#21644;&#24494;&#35843;&#22522;&#20110;BERT&#30340;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#22810;&#25968;&#25237;&#31080;&#38598;&#21512;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20248;&#20110;&#21333;&#20010;&#22522;&#32447;&#27169;&#22411;&#24471;&#20998;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#20219;&#21153;A&#20013;&#23454;&#29616;&#20102;&#23439;F1&#20998;&#25968;0.8392&#65292;&#22312;&#20219;&#21153;B&#20013;&#20026;0.6092&#65292;&#22312;&#20219;&#21153;C&#20013;&#20026;0.4319&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes our submission to Task 10 at SemEval 2023-Explainable Detection of Online Sexism (EDOS), divided into three subtasks. The recent rise in social media platforms has seen an increase in disproportionate levels of sexism experienced by women on social media platforms. This has made detecting and explaining online sexist content more important than ever to make social media safer and more accessible for women. Our approach consists of experimenting and finetuning BERT-based models and using a Majority Voting ensemble model that outperforms individual baseline model scores. Our system achieves a macro F1 score of 0.8392 for Task A, 0.6092 for Task B, and 0.4319 for Task C.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HiCatGLR&#20219;&#21153;&#65292;&#33268;&#21147;&#20110;&#20026;&#25991;&#29486;&#32508;&#36848;&#29983;&#25104;&#20998;&#23618;&#30446;&#24405;&#65292;&#23427;&#21487;&#20197;&#20174;&#22810;&#31687;&#35770;&#25991;&#20013;&#25552;&#21462;&#21644;&#32452;&#32455;&#37325;&#35201;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#29616;&#26377;&#30740;&#31350;&#20013;&#32570;&#23569;&#28165;&#26224;&#36923;&#36753;&#23618;&#27425;&#32467;&#26500;&#27010;&#36848;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#24182;&#21019;&#24314;&#20102;&#26032;&#30340;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#27492;&#39033;&#30740;&#31350;&#65292;&#21487;&#20197;&#26356;&#21152;&#20934;&#30830;&#22320;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#24182;&#39564;&#35777;&#25968;&#25454;&#38598;&#30340;&#39640;&#36136;&#37327;&#21644;&#35780;&#20272;&#25351;&#26631;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.03512</link><description>&lt;p&gt;
&#25991;&#29486;&#32508;&#36848;&#30340;&#20998;&#23618;&#30446;&#24405;&#29983;&#25104;&#65306;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Catalogue Generation for Literature Review: A Benchmark. (arXiv:2304.03512v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03512
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HiCatGLR&#20219;&#21153;&#65292;&#33268;&#21147;&#20110;&#20026;&#25991;&#29486;&#32508;&#36848;&#29983;&#25104;&#20998;&#23618;&#30446;&#24405;&#65292;&#23427;&#21487;&#20197;&#20174;&#22810;&#31687;&#35770;&#25991;&#20013;&#25552;&#21462;&#21644;&#32452;&#32455;&#37325;&#35201;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#29616;&#26377;&#30740;&#31350;&#20013;&#32570;&#23569;&#28165;&#26224;&#36923;&#36753;&#23618;&#27425;&#32467;&#26500;&#27010;&#36848;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#24182;&#21019;&#24314;&#20102;&#26032;&#30340;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#27492;&#39033;&#30740;&#31350;&#65292;&#21487;&#20197;&#26356;&#21152;&#20934;&#30830;&#22320;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#24182;&#39564;&#35777;&#25968;&#25454;&#38598;&#30340;&#39640;&#36136;&#37327;&#21644;&#35780;&#20272;&#25351;&#26631;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#25991;&#26723;&#31185;&#23398;&#25688;&#35201;&#21487;&#20197;&#20174;&#22823;&#37327;&#30340;&#35770;&#25991;&#20013;&#25552;&#21462;&#21644;&#32452;&#32455;&#37325;&#35201;&#20449;&#24687;&#65292;&#26368;&#36817;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20135;&#29983;&#32570;&#20047;&#28165;&#26224;&#21644;&#36923;&#36753;&#23618;&#27425;&#32467;&#26500;&#30340;&#20887;&#38271;&#27010;&#36848;&#19978;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;Hierarchical Catalogue Generation for Literature Review (HiCatGLR)&#8221;&#30340;&#21407;&#23376;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20854;&#30446;&#26631;&#26159;&#26681;&#25454;&#21508;&#31181;&#21442;&#32771;&#25991;&#29486;&#20026;&#32508;&#36848;&#35770;&#25991;&#29983;&#25104;&#20998;&#23618;&#30446;&#24405;&#12290;&#25105;&#20204;&#31934;&#24515;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#33521;&#25991;&#25991;&#29486;&#32508;&#36848;&#20998;&#23618;&#30446;&#24405;&#25968;&#25454;&#38598;(HiCaD)&#65292;&#20854;&#20013;&#21253;&#21547;13.8k&#31687;&#25991;&#29486;&#32508;&#36848;&#30446;&#24405;&#21644;120k&#31687;&#21442;&#32771;&#35770;&#25991;&#65292;&#24182;&#36890;&#36807;&#31471;&#21040;&#31471;&#21644;&#27969;&#27700;&#32447;&#26041;&#27861;&#36827;&#34892;&#20102;&#21508;&#31181;&#23454;&#39564;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#20026;&#20102;&#20934;&#30830;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20174;&#35821;&#20041;&#21644;&#32467;&#26500;&#19978;&#19982;&#21442;&#32771;&#26631;&#20934;&#30456;&#20284;&#24230;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#24191;&#27867;&#20998;&#26512;&#39564;&#35777;&#20102;&#25105;&#20204;&#25968;&#25454;&#38598;&#30340;&#39640;&#36136;&#37327;&#21644;&#25105;&#20204;&#35780;&#20272;&#25351;&#26631;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-document scientific summarization can extract and organize important information from an abundant collection of papers, arousing widespread attention recently. However, existing efforts focus on producing lengthy overviews lacking a clear and logical hierarchy. To alleviate this problem, we present an atomic and challenging task named Hierarchical Catalogue Generation for Literature Review (HiCatGLR), which aims to generate a hierarchical catalogue for a review paper given various references. We carefully construct a novel English Hierarchical Catalogues of Literature Reviews Dataset (HiCaD) with 13.8k literature review catalogues and 120k reference papers, where we benchmark diverse experiments via the end-to-end and pipeline methods. To accurately assess the model performance, we design evaluation metrics for similarity to ground truth from semantics and structure. Besides, our extensive analyses verify the high quality of our dataset and the effectiveness of our evaluation met
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CLIPPINGS&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;&#65292;&#29992;&#20110;&#35760;&#24405;&#38142;&#25509;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#35757;&#32451;&#23545;&#31216;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#32534;&#30721;&#22120;&#65292;&#22312;&#24230;&#37327;&#31354;&#38388;&#20013;&#23398;&#20064;&#30456;&#36817;&#25110;&#19981;&#21516;&#31867;&#21035;&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#20010;&#24212;&#29992;&#22330;&#26223;&#65292;&#22914;&#26500;&#24314;&#20840;&#38754;&#30340;&#34917;&#20805;&#19987;&#21033;&#27880;&#20876;&#34920;&#21644;&#35782;&#21035;&#19981;&#21516;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#30340;&#20010;&#20154;&#12290;</title><link>http://arxiv.org/abs/2304.03464</link><description>&lt;p&gt;
&#29992;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#36830;&#25509;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Linking Representations with Multimodal Contrastive Learning. (arXiv:2304.03464v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03464
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CLIPPINGS&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;&#65292;&#29992;&#20110;&#35760;&#24405;&#38142;&#25509;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#35757;&#32451;&#23545;&#31216;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#32534;&#30721;&#22120;&#65292;&#22312;&#24230;&#37327;&#31354;&#38388;&#20013;&#23398;&#20064;&#30456;&#36817;&#25110;&#19981;&#21516;&#31867;&#21035;&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#20010;&#24212;&#29992;&#22330;&#26223;&#65292;&#22914;&#26500;&#24314;&#20840;&#38754;&#30340;&#34917;&#20805;&#19987;&#21033;&#27880;&#20876;&#34920;&#21644;&#35782;&#21035;&#19981;&#21516;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#30340;&#20010;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#24212;&#29992;&#38656;&#35201;&#23558;&#21253;&#21547;&#22312;&#21508;&#31181;&#25991;&#26723;&#25968;&#25454;&#38598;&#20013;&#30340;&#23454;&#20363;&#20998;&#32452;&#25104;&#31867;&#12290;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#26041;&#27861;&#19981;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#65292;&#20063;&#19981;&#21033;&#29992;&#25991;&#26723;&#22266;&#26377;&#30340;&#22810;&#27169;&#24577;&#24615;&#36136;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#35760;&#24405;&#38142;&#25509;&#36890;&#24120;&#34987;&#27010;&#24565;&#21270;&#20026;&#23383;&#31526;&#20018;&#21305;&#37197;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102; CLIPPINGS&#65292;&#19968;&#31181;&#29992;&#20110;&#35760;&#24405;&#38142;&#25509;&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;&#12290;CLIPPINGS &#37319;&#29992;&#31471;&#21040;&#31471;&#35757;&#32451;&#23545;&#31216;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#21452;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#36827;&#34892;&#23545;&#40784;&#65292;&#23398;&#20064;&#19968;&#20010;&#24230;&#37327;&#31354;&#38388;&#65292;&#20854;&#20013;&#32473;&#23450;&#23454;&#20363;&#30340;&#27719;&#24635;&#22270;&#20687;-&#25991;&#26412;&#34920;&#31034;&#38752;&#36817;&#21516;&#19968;&#31867;&#20013;&#30340;&#34920;&#31034;&#65292;&#24182;&#36828;&#31163;&#19981;&#21516;&#31867;&#20013;&#30340;&#34920;&#31034;&#12290;&#22312;&#25512;&#29702;&#26102;&#65292;&#21487;&#20197;&#36890;&#36807;&#20174;&#31163;&#32447;&#31034;&#20363;&#23884;&#20837;&#32034;&#24341;&#20013;&#26816;&#32034;&#23427;&#20204;&#26368;&#36817;&#30340;&#37051;&#23621;&#25110;&#32858;&#31867;&#23427;&#20204;&#30340;&#34920;&#31034;&#26469;&#38142;&#25509;&#23454;&#20363;&#12290;&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#20004;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24212;&#29992;&#65306;&#36890;&#36807;&#23558;&#19987;&#21033;&#19982;&#20854;&#23545;&#24212;&#30340;&#30417;&#31649;&#25991;&#20214;&#38142;&#25509;&#26469;&#26500;&#24314;&#20840;&#38754;&#30340;&#34917;&#20805;&#19987;&#21033;&#27880;&#20876;&#34920;&#65292;&#20197;&#21450;&#22312;&#19981;&#21516;&#30340;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#35782;&#21035;&#20010;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many applications require grouping instances contained in diverse document datasets into classes. Most widely used methods do not employ deep learning and do not exploit the inherently multimodal nature of documents. Notably, record linkage is typically conceptualized as a string-matching problem. This study develops CLIPPINGS, (Contrastively Linking Pooled Pre-trained Embeddings), a multimodal framework for record linkage. CLIPPINGS employs end-to-end training of symmetric vision and language bi-encoders, aligned through contrastive language-image pre-training, to learn a metric space where the pooled image-text representation for a given instance is close to representations in the same class and distant from representations in different classes. At inference time, instances can be linked by retrieving their nearest neighbor from an offline exemplar embedding index or by clustering their representations. The study examines two challenging applications: constructing comprehensive suppl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#22810;&#20010;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#20102;ChatGPT&#21644;GPT-4&#22312;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#26500;&#36896;&#20102;&#19968;&#20010;&#36923;&#36753;&#25512;&#29702;&#30340;&#20998;&#24067;&#20043;&#22806;&#30340;&#25968;&#25454;&#38598;&#26469;&#30740;&#31350;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#22312;&#22823;&#22810;&#25968;&#36923;&#36753;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#36828;&#20248;&#20110;RoBERTa&#24494;&#35843;&#26041;&#27861;&#65292;&#32780;GPT-4&#30340;&#34920;&#29616;&#21017;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2304.03439</link><description>&lt;p&gt;
&#35780;&#20272;ChatGPT&#21644;GPT-4&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4. (arXiv:2304.03439v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#22810;&#20010;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#20102;ChatGPT&#21644;GPT-4&#22312;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#26500;&#36896;&#20102;&#19968;&#20010;&#36923;&#36753;&#25512;&#29702;&#30340;&#20998;&#24067;&#20043;&#22806;&#30340;&#25968;&#25454;&#38598;&#26469;&#30740;&#31350;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#22312;&#22823;&#22810;&#25968;&#36923;&#36753;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#36828;&#20248;&#20110;RoBERTa&#24494;&#35843;&#26041;&#27861;&#65292;&#32780;GPT-4&#30340;&#34920;&#29616;&#21017;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#12290;&#38543;&#30528;&#20808;&#36827;&#30340;&#29983;&#25104;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;4&#65288;GPT-4&#65289;&#30340;&#21457;&#24067;&#65292;&#25105;&#20204;&#28212;&#26395;&#20102;&#35299;GPT-4&#22312;&#21508;&#31181;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#22810;&#20010;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;LogiQA&#21644;ReClor&#31561;&#24120;&#29992;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#21450;&#20687;AR-LSAT&#36825;&#26679;&#30340;&#26032;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23545;&#38656;&#35201;&#36923;&#36753;&#25512;&#29702;&#30340;&#22522;&#20934;&#27979;&#35797;&#36827;&#34892;&#20102;&#22810;&#39033;&#36873;&#25321;&#38405;&#35835;&#29702;&#35299;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#27979;&#35797;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#26500;&#36896;&#20102;&#19968;&#20010;&#36923;&#36753;&#25512;&#29702;&#30340;&#20998;&#24067;&#20043;&#22806;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#30740;&#31350;ChatGPT&#21644;GPT-4&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;ChatGPT&#21644;GPT-4&#20043;&#38388;&#30340;&#24615;&#33021;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#36923;&#36753;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;ChatGPT&#30340;&#34920;&#29616;&#36828;&#36828;&#20248;&#20110;RoBERTa&#24494;&#35843;&#26041;&#27861;&#12290;GPT-4&#22312;&#25105;&#20204;&#30340;&#25163;&#21160;&#27979;&#35797;&#20013;&#34920;&#29616;&#26356;&#39640;&#12290;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;ChatGPT&#21644;GPT-4&#30340;&#34920;&#29616;&#30456;&#23545;&#36739;&#20026;&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Harnessing logical reasoning ability is a comprehensive natural language understanding endeavor. With the release of Generative Pretrained Transformer 4 (GPT-4), highlighted as "advanced" at reasoning tasks, we are eager to learn the GPT-4 performance on various logical reasoning tasks. This report analyses multiple logical reasoning datasets, with popular benchmarks like LogiQA and ReClor, and newly-released datasets like AR-LSAT. We test the multi-choice reading comprehension and natural language inference tasks with benchmarks requiring logical reasoning. We further construct a logical reasoning out-of-distribution dataset to investigate the robustness of ChatGPT and GPT-4. We also make a performance comparison between ChatGPT and GPT-4. Experiment results show that ChatGPT performs significantly better than the RoBERTa fine-tuning method on most logical reasoning benchmarks. GPT-4 shows even higher performance on our manual tests. Among benchmarks, ChatGPT and GPT-4 do relatively w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35895;&#27468;OCR&#25195;&#25551;&#30340;&#34255;&#25991;&#25163;&#31295;&#30340;&#31070;&#32463;&#25340;&#20889;&#32416;&#38169;&#27169;&#22411;&#65292;&#21487;&#20197;&#33258;&#21160;&#32416;&#27491;OCR&#36755;&#20986;&#20013;&#30340;&#22122;&#22768;&#12290;</title><link>http://arxiv.org/abs/2304.03427</link><description>&lt;p&gt;
&#22522;&#20110;&#35895;&#27468;OCR&#25195;&#25551;&#30340;&#34255;&#25991;&#25163;&#31295;&#30340;&#31070;&#32463;&#25340;&#20889;&#32416;&#38169;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Cleansing Jewel: A Neural Spelling Correction Model Built On Google OCR-ed Tibetan Manuscripts. (arXiv:2304.03427v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35895;&#27468;OCR&#25195;&#25551;&#30340;&#34255;&#25991;&#25163;&#31295;&#30340;&#31070;&#32463;&#25340;&#20889;&#32416;&#38169;&#27169;&#22411;&#65292;&#21487;&#20197;&#33258;&#21160;&#32416;&#27491;OCR&#36755;&#20986;&#20013;&#30340;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#25991;&#23398;&#32773;&#22312;&#30740;&#31350;&#21382;&#21490;&#12289;&#23447;&#25945;&#21644;&#31038;&#20250;&#25919;&#27835;&#32467;&#26500;&#31561;&#26041;&#38754;&#32463;&#24120;&#20381;&#36182;&#20110;&#21476;&#20195;&#25163;&#31295;&#12290;&#34429;&#28982;OCR&#25216;&#26415;&#21487;&#20197;&#23558;&#36825;&#20123;&#23453;&#36149;&#25163;&#31295;&#25968;&#23383;&#21270;&#65292;&#20294;&#22810;&#25968;&#25163;&#31295;&#22240;&#30952;&#25439;&#32780;&#36807;&#26102;&#65292;OCR&#31243;&#24207;&#27809;&#21150;&#27861;&#35782;&#21035;&#32763;&#39029;&#30340;&#34394;&#28129;&#25110;&#27745;&#28173;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35895;&#27468;OCR&#25195;&#25551;&#30340;&#34255;&#25991;&#25163;&#31295;&#30340;&#31070;&#32463;&#25340;&#20889;&#32416;&#38169;&#27169;&#22411;&#65292;&#21487;&#20197;&#33258;&#21160;&#32416;&#27491;OCR&#36755;&#20986;&#20013;&#30340;&#22122;&#22768;&#12290;&#26412;&#25991;&#20998;&#20026;&#22235;&#20010;&#37096;&#20998;&#65306;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#26550;&#26500;&#12289;&#35757;&#32451;&#21644;&#20998;&#26512;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23545;&#21407;&#22987;&#34255;&#25991;&#30005;&#23376;&#25991;&#26412;&#35821;&#26009;&#24211;&#36827;&#34892;&#20102;&#29305;&#24449;&#24037;&#31243;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#20004;&#32452;&#32467;&#26500;&#21270;&#25968;&#25454;&#26694;&#8212;&#8212;&#19968;&#32452;&#21305;&#37197;&#30340;&#29609;&#20855;&#25968;&#25454;&#21644;&#19968;&#32452;&#21305;&#37197;&#30340;&#30495;&#23454;&#25968;&#25454;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;Transformer&#26550;&#26500;&#20013;&#23454;&#29616;&#20102;&#32622;&#20449;&#24230;&#24471;&#20998;&#26426;&#21046;&#26469;&#25191;&#34892;&#25340;&#20889;&#26657;&#27491;&#20219;&#21153;&#12290;&#26681;&#25454;&#25439;&#22833;&#21644;&#23383;&#31526;&#38169;&#35823;&#29575;&#65292;&#25105;&#20204;&#30340;Transformer + &#32622;&#20449;&#24230;&#24471;&#20998;&#26426;&#21046;&#27604;&#20854;&#20182;&#24120;&#29992;&#30340;&#25340;&#20889;&#26657;&#27491;&#31639;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scholars in the humanities rely heavily on ancient manuscripts to study history, religion, and socio-political structures in the past. Many efforts have been devoted to digitizing these precious manuscripts using OCR technology, but most manuscripts were blemished over the centuries so that an Optical Character Recognition (OCR) program cannot be expected to capture faded graphs and stains on pages. This work presents a neural spelling correction model built on Google OCR-ed Tibetan Manuscripts to auto-correct OCR-ed noisy output. This paper is divided into four sections: dataset, model architecture, training and analysis. First, we feature-engineered our raw Tibetan etext corpus into two sets of structured data frames -- a set of paired toy data and a set of paired real data. Then, we implemented a Confidence Score mechanism into the Transformer architecture to perform spelling correction tasks. According to the Loss and Character Error Rate, our Transformer + Confidence score mechani
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#22914;&#20309;&#20174;&#22823;&#35268;&#27169;&#26032;&#38395;&#35821;&#26009;&#24211;&#20013;&#30452;&#25509;&#21457;&#29616;&#23186;&#20307;&#36873;&#25321;&#24615;&#20559;&#35265;&#27169;&#24335;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#20197;&#19981;&#21516;&#26469;&#28304;&#25253;&#36947;&#30340;&#23454;&#20307;&#20026;&#36215;&#28857;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#35745;&#31639;&#36873;&#25321;&#24615;&#20559;&#35265;&#12290;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#21457;&#29616;&#23481;&#26131;&#21463;&#21040;&#20559;&#35265;&#25253;&#36947;&#30340;&#35805;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.03414</link><description>&lt;p&gt;
&#22312;&#26032;&#38395;&#25253;&#36947;&#20013;&#25506;&#31350;&#36873;&#25321;&#24615;&#20559;&#35265;&#30340;&#35821;&#26009;&#24211;&#35268;&#27169;&#21457;&#29616;&#65306;&#20197;&#27604;&#36739;&#19981;&#21516;&#26469;&#28304;&#23545;&#23454;&#20307;&#30340;&#25253;&#36947;&#20026;&#36215;&#28857;
&lt;/p&gt;
&lt;p&gt;
Towards Corpus-Scale Discovery of Selection Biases in News Coverage: Comparing What Sources Say About Entities as a Start. (arXiv:2304.03414v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03414
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#22914;&#20309;&#20174;&#22823;&#35268;&#27169;&#26032;&#38395;&#35821;&#26009;&#24211;&#20013;&#30452;&#25509;&#21457;&#29616;&#23186;&#20307;&#36873;&#25321;&#24615;&#20559;&#35265;&#27169;&#24335;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#20197;&#19981;&#21516;&#26469;&#28304;&#25253;&#36947;&#30340;&#23454;&#20307;&#20026;&#36215;&#28857;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#35745;&#31639;&#36873;&#25321;&#24615;&#20559;&#35265;&#12290;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#21457;&#29616;&#23481;&#26131;&#21463;&#21040;&#20559;&#35265;&#25253;&#36947;&#30340;&#35805;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#38395;&#25253;&#36947;&#28041;&#21450;&#36873;&#25321;&#24615;&#36807;&#31243;&#65292;&#28085;&#30422;&#26576;&#19968;&#20027;&#39064;&#26102;&#19981;&#21487;&#36991;&#20813;&#22320;&#20986;&#29616;&#36873;&#25321;&#24615;&#20559;&#35265;&#65292;&#21363;&#30001;&#20110;&#19981;&#21516;&#35758;&#39064;&#20135;&#29983;&#30340;&#25253;&#36947;&#36873;&#25321;&#19981;&#21516;&#25152;&#36896;&#25104;&#30340;&#35759;&#24687;&#20559;&#24046;&#12290;&#20026;&#20102;&#20102;&#35299;&#20559;&#35265;&#30340;&#31243;&#24230;&#21644;&#24433;&#21709;&#65292;&#24517;&#39035;&#20808;&#21457;&#29616;&#20197;&#19979;&#20004;&#26041;&#38754;&#30340;&#20869;&#23481;&#65306;&#65288;1&#65289;&#26032;&#38395;&#26469;&#28304;&#36890;&#24120;&#22312;&#21738;&#20123;&#35758;&#39064;&#19978;&#23545;&#8220;&#20540;&#24471;&#25253;&#36947;&#8221;&#30340;&#20449;&#24687;&#23450;&#20041;&#23384;&#22312;&#20998;&#27495;&#65307;&#65288;2&#65289;&#20869;&#23481;&#36873;&#25321;&#27169;&#24335;&#26159;&#21542;&#19982;&#26032;&#38395;&#26469;&#28304;&#30340;&#26576;&#20123;&#23646;&#24615;&#65288;&#22914;&#24847;&#35782;&#24418;&#24577;&#20542;&#21521;&#31561;&#65289;&#30456;&#20851;&#32852;&#12290;&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#20174;&#28023;&#37327;&#26032;&#38395;&#35821;&#26009;&#24211;&#30452;&#25509;&#21457;&#29616;&#23186;&#20307;&#36873;&#25321;&#24615;&#20559;&#35265;&#27169;&#24335;&#30340;&#21487;&#25193;&#23637;NLP&#31995;&#32479;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#21644;&#30740;&#31350;&#19968;&#20010;&#27010;&#24565;&#26694;&#26550;&#65292;&#27604;&#36739;&#19981;&#21516;&#26469;&#28304;&#22914;&#20309;&#25253;&#36947;&#26576;&#20123;&#26377;&#20105;&#35758;&#30340;&#23454;&#20307;&#65292;&#24182;&#21033;&#29992;&#24046;&#24322;&#24230;&#37327;&#36873;&#25321;&#24615;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#36873;&#25321;&#24615;&#20559;&#35265;&#65292;&#24182;&#29992;&#20110;&#21457;&#29616;&#23481;&#26131;&#21463;&#21040;&#20559;&#35265;&#25253;&#36947;&#30340;&#35805;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
News sources undergo the process of selecting newsworthy information when covering a certain topic. The process inevitably exhibits selection biases, i.e. news sources' typical patterns of choosing what information to include in news coverage, due to their agenda differences. To understand the magnitude and implications of selection biases, one must first discover (1) on what topics do sources typically have diverging definitions of "newsworthy" information, and (2) do the content selection patterns correlate with certain attributes of the news sources, e.g. ideological leaning, etc.  The goal of the paper is to investigate and discuss the challenges of building scalable NLP systems for discovering patterns of media selection biases directly from news content in massive-scale news corpora, without relying on labeled data. To facilitate research in this domain, we propose and study a conceptual framework, where we compare how sources typically mention certain controversial entities, and
&lt;/p&gt;</description></item><item><title>CAPOT&#20351;&#29992;&#21518;&#35757;&#32451;&#23545;&#27604;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#27169;&#22411;&#23545;&#20110;&#22122;&#22768;&#26597;&#35810;&#30340;&#20581;&#22766;&#24615;&#65292;&#34920;&#29616;&#31867;&#20284;&#20110;&#25968;&#25454;&#22686;&#24378;&#20294;&#27809;&#26377;&#20854;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2304.03401</link><description>&lt;p&gt;
CAPOT: &#20351;&#29992;&#21518;&#35757;&#32451;&#23545;&#27604;&#23545;&#40784;&#21019;&#24314;&#24378;&#20581;&#30340;&#23494;&#38598;&#26597;&#35810;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
CAPOT: Creating Robust Dense Query Encoders using Post Training Contrastive Alignment. (arXiv:2304.03401v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03401
&lt;/p&gt;
&lt;p&gt;
CAPOT&#20351;&#29992;&#21518;&#35757;&#32451;&#23545;&#27604;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#27169;&#22411;&#23545;&#20110;&#22122;&#22768;&#26597;&#35810;&#30340;&#20581;&#22766;&#24615;&#65292;&#34920;&#29616;&#31867;&#20284;&#20110;&#25968;&#25454;&#22686;&#24378;&#20294;&#27809;&#26377;&#20854;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#35789;&#34920;&#31034;&#30340;&#25104;&#21151;&#21644;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#30340;&#36827;&#27493;&#20351;&#24471;&#22522;&#20110;&#23494;&#38598;&#21521;&#37327;&#30340;&#26816;&#32034;&#25104;&#20026;&#27573;&#33853;&#21644;&#25991;&#26723;&#25490;&#21517;&#30340;&#26631;&#20934;&#26041;&#27861;&#12290;&#21452;&#32534;&#30721;&#22120;&#34429;&#28982;&#26377;&#25928;&#21644;&#39640;&#25928;&#65292;&#20294;&#23545;&#26597;&#35810;&#20998;&#24067;&#21644;&#22024;&#26434;&#26597;&#35810;&#21464;&#21270;&#24456;&#33030;&#24369;&#12290;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;&#20351;&#27169;&#22411;&#26356;&#21152;&#20581;&#22766;&#65292;&#20294;&#20250;&#24341;&#20837;&#35757;&#32451;&#38598;&#29983;&#25104;&#30340;&#24320;&#38144;&#65292;&#24182;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#21644;&#32034;&#24341;&#37325;&#24314;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; Contrastive Alignment POst Training (CAPOT)&#65292;&#19968;&#31181;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#20923;&#32467;&#25991;&#26723;&#32534;&#30721;&#22120;&#65292;&#35753;&#26597;&#35810;&#32534;&#30721;&#22120;&#23398;&#20064;&#23558;&#22024;&#26434;&#26597;&#35810;&#19982;&#20854;&#26410;&#26356;&#25913;&#30340;&#26681;&#23545;&#40784;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#20581;&#22766;&#24615;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102; CAPOT &#22312; MSMARCO&#12289;&#33258;&#28982;&#38382;&#39064;&#21644; Trivia QA &#27573;&#33853;&#26816;&#32034;&#30340;&#22024;&#26434;&#21464;&#20307;&#19978;&#65292;&#21457;&#29616; CAPOT &#20855;&#26377;&#19982;&#25968;&#25454;&#22686;&#24378;&#31867;&#20284;&#30340;&#24433;&#21709;&#65292;&#20294;&#27809;&#26377;&#23427;&#30340;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of contextual word representations and advances in neural information retrieval have made dense vector-based retrieval a standard approach for passage and document ranking. While effective and efficient, dual-encoders are brittle to variations in query distributions and noisy queries. Data augmentation can make models more robust but introduces overhead to training set generation and requires retraining and index regeneration. We present Contrastive Alignment POst Training (CAPOT), a highly efficient finetuning method that improves model robustness without requiring index regeneration, the training set optimization, or alteration. CAPOT enables robust retrieval by freezing the document encoder while the query encoder learns to align noisy queries with their unaltered root. We evaluate CAPOT noisy variants of MSMARCO, Natural Questions, and Trivia QA passage retrieval, finding CAPOT has a similar impact as data augmentation with none of its overhead.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;LSTM&#12289;GRU&#27169;&#22411;&#36827;&#34892;&#38463;&#25289;&#20271;&#35821;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#33391;&#22909;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;80%&#12290;</title><link>http://arxiv.org/abs/2304.03399</link><description>&lt;p&gt;
&#20351;&#29992;&#26032;&#25968;&#25454;&#38598;&#30340;LSTM&#21644;GRU&#36827;&#34892;&#38463;&#25289;&#20271;&#35821;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Using LSTM and GRU With a New Dataset for Named Entity Recognition in the Arabic Language. (arXiv:2304.03399v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;LSTM&#12289;GRU&#27169;&#22411;&#36827;&#34892;&#38463;&#25289;&#20271;&#35821;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#33391;&#22909;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;80%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;(NER)&#26159;&#19968;&#39033;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;(NLP)&#65292;&#26088;&#22312;&#35782;&#21035;&#21629;&#21517;&#23454;&#20307;&#24182;&#23545;&#20854;&#36827;&#34892;&#20998;&#31867;&#65292;&#22914;&#20154;&#29289;&#12289;&#22320;&#28857;&#12289;&#32452;&#32455;&#31561;&#12290;&#22312;&#38463;&#25289;&#20271;&#35821;&#20013;&#65292;&#21487;&#20197;&#25214;&#21040;&#30456;&#24403;&#25968;&#37327;&#30340;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#38656;&#35201;&#20351;&#29992;&#19981;&#21516;&#30340;&#39044;&#22788;&#29702;&#24037;&#20855;&#22788;&#29702;&#65292;&#32780;&#19981;&#21516;&#20110;&#33521;&#35821;&#12289;&#20420;&#35821;&#12289;&#24503;&#35821;&#31561;&#30340;&#35821;&#35328;&#12290;&#22240;&#27492;&#65292;&#22312;&#35299;&#20915;&#32467;&#26500;&#21270;&#25968;&#25454;&#32570;&#20047;&#30340;&#38382;&#39064;&#26102;&#65292;&#26500;&#24314;&#19968;&#20010;&#26032;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#38598;&#30340;&#37325;&#35201;&#24615;&#24341;&#20154;&#27880;&#30446;&#12290;&#26412;&#25991;&#20351;&#29992;BIOES&#26684;&#24335;&#26631;&#35760;&#21333;&#35789;&#65292;&#36825;&#20351;&#25105;&#20204;&#21487;&#20197;&#22788;&#29702;&#30001;&#22810;&#20010;&#21477;&#23376;&#32452;&#25104;&#30340;&#23884;&#22871;&#21629;&#21517;&#23454;&#20307;&#65292;&#24182;&#23450;&#20041;&#21517;&#31216;&#30340;&#24320;&#22987;&#21644;&#32467;&#26463;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#19977;&#19975;&#20845;&#21315;&#22810;&#26465;&#35760;&#24405;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#38271;&#30701;&#26102;&#35760;&#24518;(LSTM)&#21333;&#20803;&#21644;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;(GRU)&#26500;&#24314;&#38463;&#25289;&#20271;&#35821;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#32473;&#20986;&#20102;&#36817;80%&#30340;&#33391;&#22909;&#32467;&#26524;&#65292;&#22240;&#20026;LSTM&#21644;GRU&#27169;&#22411;&#21487;&#20197;&#25214;&#21040;&#24403;&#21069;&#35789;&#21644;&#21069;&#38754;&#35789;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#36825;&#23545;&#20110;&#38463;&#25289;&#20271;&#35821;&#30340;&#22797;&#26434;&#35821;&#27861;&#32467;&#26500;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Named entity recognition (NER) is a natural language processing task (NLP), which aims to identify named entities and classify them like person, location, organization, etc. In the Arabic language, we can find a considerable size of unstructured data, and it needs to different preprocessing tool than languages like (English, Russian, German...). From this point, we can note the importance of building a new structured dataset to solve the lack of structured data. In this work, we use the BIOES format to tag the word, which allows us to handle the nested name entity that consists of more than one sentence and define the start and the end of the name. The dataset consists of more than thirty-six thousand records. In addition, this work proposes long short term memory (LSTM) units and Gated Recurrent Units (GRU) for building the named entity recognition model in the Arabic language. The models give an approximately good result (80%) because LSTM and GRU models can find the relationships be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#27604;&#36739;&#20256;&#32479;&#26041;&#27861;&#21644;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#22788;&#29702;&#22823;&#37327;&#35838;&#31243;&#35780;&#35770;&#65292;&#36827;&#34892;&#24773;&#24863;&#26497;&#24615;&#20998;&#26512;&#21644;&#20027;&#39064;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2304.03394</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20110;&#35838;&#31243;&#35780;&#35770;&#30340;&#35266;&#28857;&#25366;&#25496;&#21644;&#20027;&#39064;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for Opinion Mining and Topic Classification of Course Reviews. (arXiv:2304.03394v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03394
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#27604;&#36739;&#20256;&#32479;&#26041;&#27861;&#21644;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#22788;&#29702;&#22823;&#37327;&#35838;&#31243;&#35780;&#35770;&#65292;&#36827;&#34892;&#24773;&#24863;&#26497;&#24615;&#20998;&#26512;&#21644;&#20027;&#39064;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#25945;&#32946;&#24037;&#20316;&#32773;&#21644;&#31649;&#29702;&#32773;&#26469;&#35828;&#65292;&#23398;&#29983;&#23545;&#35838;&#31243;&#30340;&#21453;&#39304;&#24847;&#35265;&#38750;&#24120;&#37325;&#35201;&#65292;&#26080;&#35770;&#35838;&#31243;&#30340;&#31867;&#22411;&#25110;&#26426;&#26500;&#22914;&#20309;&#12290;&#22312;&#26426;&#26500;&#32423;&#21035;&#25110;&#22312;&#32447;&#35770;&#22363;&#19978;&#22788;&#29702;&#22823;&#37327;&#30340;&#24320;&#25918;&#21453;&#39304;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25910;&#38598;&#21644;&#39044;&#22788;&#29702;&#20102;&#22823;&#37327;&#20844;&#24320;&#21487;&#29992;&#30340;&#35838;&#31243;&#35780;&#35770;&#12290;&#25105;&#20204;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#30446;&#30340;&#26159;&#20102;&#35299;&#23398;&#29983;&#30340;&#24773;&#24863;&#21644;&#20027;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#24403;&#21069;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#22914;&#35789;&#23884;&#20837;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#37319;&#29992;&#26368;&#20808;&#36827;&#30340;BERT&#65288;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#26469;&#33258;&#21464;&#21387;&#22120;&#65289;&#12289;RoBERTa&#65288;&#32463;&#36807;&#20248;&#21270;&#30340;BERT&#26041;&#27861;&#65289;&#21644;XLNet&#65288;&#24191;&#20041;&#33258;&#22238;&#24402;&#39044;&#35757;&#32451;&#65289;&#25216;&#26415;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#36825;&#20123;&#25216;&#26415;&#19982;&#20256;&#32479;&#26041;&#27861;&#30340;&#24046;&#24322;&#12290;&#36825;&#39033;&#27604;&#36739;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#24212;&#29992;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#24773;&#24863;&#26497;&#24615;&#20998;&#26512;&#21644;&#20027;&#39064;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Student opinions for a course are important to educators and administrators, regardless of the type of the course or the institution. Reading and manually analyzing open-ended feedback becomes infeasible for massive volumes of comments at institution level or online forums. In this paper, we collected and pre-processed a large number of course reviews publicly available online. We applied machine learning techniques with the goal to gain insight into student sentiments and topics. Specifically, we utilized current Natural Language Processing (NLP) techniques, such as word embeddings and deep neural networks, and state-of-the-art BERT (Bidirectional Encoder Representations from Transformers), RoBERTa (Robustly optimized BERT approach) and XLNet (Generalized Auto-regression Pre-training). We performed extensive experimentation to compare these techniques versus traditional approaches. This comparative study demonstrates how to apply modern machine learning approaches for sentiment polari
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#35780;&#20272;&#20102;ChatGPT&#22312;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#21644;&#24773;&#24863;&#25512;&#29702;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#20197;&#21450;&#19981;&#21516;&#25552;&#31034;&#31574;&#30053;&#21644;&#24773;&#24863;&#20449;&#24687;&#23545;&#20854;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#22312;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#21152;&#20837;&#24773;&#24863;&#22686;&#24378;&#25552;&#31034;&#23545;&#26576;&#20123;&#20219;&#21153;&#25928;&#26524;&#26174;&#33879;&#12290;</title><link>http://arxiv.org/abs/2304.03347</link><description>&lt;p&gt;
&#35770;ChatGPT&#21644;&#24773;&#24863;&#22686;&#24378;&#25552;&#31034;&#22312;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#20013;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
On the Evaluations of ChatGPT and Emotion-enhanced Prompting for Mental Health Analysis. (arXiv:2304.03347v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#35780;&#20272;&#20102;ChatGPT&#22312;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#21644;&#24773;&#24863;&#25512;&#29702;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#20197;&#21450;&#19981;&#21516;&#25552;&#31034;&#31574;&#30053;&#21644;&#24773;&#24863;&#20449;&#24687;&#23545;&#20854;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#22312;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#21152;&#20837;&#24773;&#24863;&#22686;&#24378;&#25552;&#31034;&#23545;&#26576;&#20123;&#20219;&#21153;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#26174;&#31034;&#20986;&#25552;&#39640;&#24515;&#29702;&#20581;&#24247;&#25252;&#29702;&#25928;&#29575;&#21644;&#21487;&#35775;&#38382;&#24615;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#32780;&#26368;&#36817;&#30340;&#20027;&#27969;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;(PLMs)&#20316;&#20026;&#39592;&#24178;&#65292;&#24182;&#34701;&#20837;&#24773;&#24863;&#20449;&#24687;&#12290;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#65292;&#22914;ChatGPT&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;ChatGPT&#38646;-shot&#24615;&#33021;&#30740;&#31350;&#22312;&#19981;&#20805;&#20998;&#30340;&#35780;&#20272;&#12289;&#24773;&#24863;&#20449;&#24687;&#21033;&#29992;&#21644;&#26041;&#27861;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#20840;&#38754;&#35780;&#20272;&#20102;ChatGPT&#22312;11&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#21644;&#24773;&#24863;&#25512;&#29702;&#33021;&#21147;&#65292;&#28085;&#30422;&#20102;5&#20010;&#20219;&#21153;&#65292;&#21253;&#25324;&#20108;&#36827;&#21046;&#21644;&#22810;&#31867;&#24515;&#29702;&#20581;&#24247;&#29366;&#20917;&#26816;&#27979;&#12289;&#24515;&#29702;&#20581;&#24247;&#29366;&#20917;&#30340;&#21407;&#22240;/&#22240;&#32032;&#26816;&#27979;&#12289;&#23545;&#35805;&#20013;&#30340;&#24773;&#32490;&#35782;&#21035;&#21644;&#22240;&#26524;&#24773;&#24863;&#34164;&#21547;&#12290;&#25105;&#20204;&#22312;&#23454;&#35777;&#20998;&#26512;&#20013;&#25506;&#31350;&#20102;&#19981;&#21516;&#25552;&#31034;&#31574;&#30053;&#20197;&#21450;&#24773;&#24863;&#22686;&#24378;&#25552;&#31034;&#23545;ChatGPT&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#22312;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#20013;&#26377;&#30528;&#33391;&#22909;&#30340;&#34920;&#29616;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#20219;&#21153;&#20013;&#21152;&#20837;&#24773;&#24863;&#22686;&#24378;&#25552;&#31034;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated mental health analysis shows great potential for enhancing the efficiency and accessibility of mental health care, whereas the recent dominant methods utilized pre-trained language models (PLMs) as the backbone and incorporated emotional information. The latest large language models (LLMs), such as ChatGPT, exhibit dramatic capabilities on diverse natural language processing tasks. However, existing studies on ChatGPT's zero-shot performance for mental health analysis have limitations in inadequate evaluation, utilization of emotional information, and explainability of methods. In this work, we comprehensively evaluate the mental health analysis and emotional reasoning ability of ChatGPT on 11 datasets across 5 tasks, including binary and multi-class mental health condition detection, cause/factor detection of mental health conditions, emotion recognition in conversations, and causal emotion entailment. We empirically analyze the impact of different prompting strategies with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#20174;&#19981;&#21516;&#23545;&#35805;QA&#35821;&#26009;&#24211;&#20013;&#29983;&#25104;&#30340;ChatGPT&#30340;&#21709;&#24212;&#65292;&#24182;&#27604;&#36739;&#20102;&#20854;&#19982;&#27491;&#30830;&#31572;&#26696;&#30340;&#30456;&#20284;&#24230;&#12290;&#30740;&#31350;&#21457;&#29616;ChatGPT&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;&#38169;&#35823;&#30340;&#31572;&#26696;&#65292;&#25552;&#20379;&#20102;&#28508;&#22312;&#29992;&#25143;&#21644;&#24320;&#21457;&#32773;&#30340;&#23453;&#36149;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2304.03325</link><description>&lt;p&gt;
ChatGPT-Crawler&#65306;&#21457;&#29616;ChatGPT&#26159;&#21542;&#30495;&#30340;&#30693;&#36947;&#33258;&#24049;&#22312;&#35828;&#20160;&#20040;&#12290;&#65288;arXiv:2304.03325v1 [cs.CL]&#65289;
&lt;/p&gt;
&lt;p&gt;
ChatGPT-Crawler: Find out if ChatGPT really knows what it's talking about. (arXiv:2304.03325v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#20174;&#19981;&#21516;&#23545;&#35805;QA&#35821;&#26009;&#24211;&#20013;&#29983;&#25104;&#30340;ChatGPT&#30340;&#21709;&#24212;&#65292;&#24182;&#27604;&#36739;&#20102;&#20854;&#19982;&#27491;&#30830;&#31572;&#26696;&#30340;&#30456;&#20284;&#24230;&#12290;&#30740;&#31350;&#21457;&#29616;ChatGPT&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;&#38169;&#35823;&#30340;&#31572;&#26696;&#65292;&#25552;&#20379;&#20102;&#28508;&#22312;&#29992;&#25143;&#21644;&#24320;&#21457;&#32773;&#30340;&#23453;&#36149;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22240;&#20854;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#20986;&#33394;&#34920;&#29616;&#32780;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#26497;&#22823;&#20852;&#36259;&#12290;&#20854;&#20013;&#65292;OpenAI&#24320;&#21457;&#30340;ChatGPT&#24050;&#32463;&#25104;&#20026;&#26089;&#26399;&#37319;&#29992;&#32773;&#20013;&#38750;&#24120;&#27969;&#34892;&#30340;&#27169;&#22411;&#65292;&#20182;&#20204;&#29978;&#33267;&#23558;&#20854;&#35270;&#20026;&#23458;&#25143;&#26381;&#21153;&#12289;&#25945;&#32946;&#12289;&#21307;&#30103;&#21644;&#37329;&#34701;&#31561;&#35768;&#22810;&#39046;&#22495;&#30340;&#30772;&#22351;&#24615;&#25216;&#26415;&#12290;&#29702;&#35299;&#36825;&#20123;&#21021;&#26399;&#29992;&#25143;&#30340;&#35266;&#28857;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#20026;&#19981;&#21516;&#39046;&#22495;&#25216;&#26415;&#30340;&#28508;&#22312;&#20248;&#21183;&#12289;&#21155;&#21183;&#12289;&#25104;&#21151;&#25110;&#22833;&#36133;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#27934;&#35265;&#12290;&#26412;&#30740;&#31350;&#32771;&#23519;&#20102;ChatGPT&#20174;&#19981;&#21516;&#23545;&#35805;QA&#35821;&#26009;&#24211;&#20013;&#29983;&#25104;&#30340;&#21709;&#24212;&#12290;&#30740;&#31350;&#20351;&#29992;BERT&#30456;&#20284;&#24230;&#20998;&#25968;&#23558;&#36825;&#20123;&#21709;&#24212;&#19982;&#27491;&#30830;&#31572;&#26696;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#33719;&#24471;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#26631;&#31614;&#12290;&#36824;&#35745;&#31639;&#24182;&#27604;&#36739;&#20102;&#35780;&#20272;&#20998;&#25968;&#65292;&#20197;&#30830;&#23450;GPT-3&#65286;GPT-4&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#30830;&#23450;&#20102;ChatGPT&#25552;&#20379;&#38169;&#35823;&#31572;&#26696;&#30340;&#24773;&#20917;&#65292;&#20026;&#30456;&#20851;&#39046;&#22495;&#25552;&#20379;&#20102;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have gained considerable interest for their impressive performance on various tasks. Among these models, ChatGPT developed by OpenAI has become extremely popular among early adopters who even regard it as a disruptive technology in many fields like customer service, education, healthcare, and finance. It is essential to comprehend the opinions of these initial users as it can provide valuable insights into the potential strengths, weaknesses, and success or failure of the technology in different areas. This research examines the responses generated by ChatGPT from different Conversational QA corpora. The study employed BERT similarity scores to compare these responses with correct answers and obtain Natural Language Inference(NLI) labels. Evaluation scores were also computed and compared to determine the overall performance of GPT-3 \&amp; GPT-4. Additionally, the study identified instances where ChatGPT provided incorrect answers to questions, providing insights into
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20851;&#27880;&#20110;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#35268;&#33539;&#20013;&#30340;&#30446;&#26631;&#21644;&#32422;&#26463;&#21512;&#25104;&#25968;&#23398;&#31243;&#24207;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;CodeT5&#21644;&#20351;&#29992;GPT-3&#26469;&#29983;&#25104;&#38656;&#35201;&#30340;&#31034;&#20363;&#36827;&#34892;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2304.03287</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#35268;&#33539;&#20013;&#30340;&#25968;&#23398;&#31243;&#24207;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Synthesis of Mathematical programs from Natural Language Specifications. (arXiv:2304.03287v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03287
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20851;&#27880;&#20110;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#35268;&#33539;&#20013;&#30340;&#30446;&#26631;&#21644;&#32422;&#26463;&#21512;&#25104;&#25968;&#23398;&#31243;&#24207;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;CodeT5&#21644;&#20351;&#29992;GPT-3&#26469;&#29983;&#25104;&#38656;&#35201;&#30340;&#31034;&#20363;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#20010;&#21830;&#19994;&#39046;&#22495;&#20013;&#36935;&#21040;&#30340;&#20960;&#20010;&#20915;&#31574;&#38382;&#39064;&#21487;&#20197;&#34987;&#24314;&#27169;&#20026;&#25968;&#23398;&#31243;&#24207;&#65292;&#21363;&#20248;&#21270;&#38382;&#39064;&#12290;&#36827;&#34892;&#36825;&#31181;&#24314;&#27169;&#30340;&#36807;&#31243;&#36890;&#24120;&#38656;&#35201;&#28041;&#21450;&#21040;&#21463;&#36807;&#36816;&#31609;&#23398;&#21644;&#39640;&#32423;&#31639;&#27861;&#22521;&#35757;&#30340;&#19987;&#23478;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23613;&#31649;&#31243;&#24207;&#21644;&#20195;&#30721;&#21512;&#25104;&#65292;AutoML&#65292;&#23398;&#20064;&#20248;&#21270;&#31561;&#26041;&#38754;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#20960;&#20046;&#27809;&#26377;&#20154;&#20851;&#27880;&#33258;&#21160;&#21270;&#21512;&#25104;&#25968;&#23398;&#31243;&#24207;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#24819;&#35937;&#19968;&#31181;&#24773;&#26223;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#24314;&#27169;&#30340;&#35268;&#33539;&#65292;&#21363;&#30446;&#26631;&#21644;&#32422;&#26463;&#20197;&#33258;&#28982;&#35821;&#35328;&#30340;&#24418;&#24335;&#34920;&#36798;&#65292;&#24182;&#19988;&#24517;&#39035;&#20174;&#36825;&#26679;&#30340;&#33258;&#28982;&#35821;&#35328;&#35268;&#33539;&#20013;&#21512;&#25104;&#25968;&#23398;&#31243;&#24207;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20351;&#29992;&#24102;&#26377;&#25968;&#25454;&#22686;&#24378;&#21644;&#26463;&#21518;&#22788;&#29702;&#30340;CodeT5&#30340;&#21151;&#25928;&#12290;&#25105;&#20204;&#21033;&#29992;GPT-3&#36827;&#34892;&#32972;&#32763;&#35793;&#20197;&#29983;&#25104;&#21512;&#25104;&#31034;&#20363;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24212;&#29992;&#32447;&#24615;&#35268;&#21010;&#35268;&#21017;&#26469;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several decision problems that are encountered in various business domains can be modeled as mathematical programs, i.e. optimization problems. The process of conducting such modeling often requires the involvement of experts trained in operations research and advanced algorithms. Surprisingly, despite the significant advances in the methods for program and code synthesis, AutoML, learning to optimize etc., there has been little or no attention paid to automating the task of synthesizing mathematical programs. We imagine a scenario where the specifications for modeling, i.e. the objective and constraints are expressed in an unstructured form in natural language (NL) and the mathematical program has to be synthesized from such an NL specification. In this work we evaluate the efficacy of employing CodeT5 with data augmentation and post-processing of beams. We utilize GPT-3 with back translation for generation of synthetic examples. Further we apply rules of linear programming to score b
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36827;&#34892;&#25991;&#23398;&#27573;&#33853;&#32763;&#35793;&#26102;&#20250;&#21033;&#29992;&#26356;&#22810;&#30340;&#25991;&#26723;&#32423;&#19978;&#19979;&#25991;&#65292;&#20174;&#32780;&#20943;&#23569;&#20851;&#38190;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#19982;&#19978;&#19979;&#25991;&#21644;&#24847;&#20041;&#30456;&#20851;&#30340;&#38169;&#35823;&#20173;&#28982;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2304.03245</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#23398;&#32763;&#35793;&#20013;&#39640;&#25928;&#21033;&#29992;&#25991;&#26723;&#32423;&#19978;&#19979;&#25991;&#65292;&#20294;&#20851;&#38190;&#38169;&#35823;&#20173;&#28982;&#23384;&#22312;
&lt;/p&gt;
&lt;p&gt;
Large language models effectively leverage document-level context for literary translation, but critical errors persist. (arXiv:2304.03245v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03245
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36827;&#34892;&#25991;&#23398;&#27573;&#33853;&#32763;&#35793;&#26102;&#20250;&#21033;&#29992;&#26356;&#22810;&#30340;&#25991;&#26723;&#32423;&#19978;&#19979;&#25991;&#65292;&#20174;&#32780;&#20943;&#23569;&#20851;&#38190;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#19982;&#19978;&#19979;&#25991;&#21644;&#24847;&#20041;&#30456;&#20851;&#30340;&#38169;&#35823;&#20173;&#28982;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#21477;&#23376;&#32423;&#21035;&#30340;&#32763;&#35793;&#25968;&#25454;&#38598;&#19978;&#19982;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#30456;&#24403;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#27573;&#33853;&#21644;&#25991;&#26723;&#32763;&#35793;&#26041;&#38754;&#30340;&#33021;&#21147;&#23578;&#26410;&#24471;&#21040;&#25506;&#31350;&#65292;&#22240;&#20026;&#36825;&#20123;&#29615;&#22659;&#19979;&#30340;&#35780;&#20272;&#20195;&#20215;&#39640;&#19988;&#22256;&#38590;&#12290;&#36890;&#36807;&#19968;&#39033;&#20005;&#35880;&#30340;&#20154;&#24037;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35201;&#27714;Gpt-3.5&#65288;text-davinci-003&#65289;LLM&#23558;&#25972;&#20010;&#25991;&#23398;&#27573;&#33853;&#65288;&#20363;&#22914;&#65292;&#20174;&#23567;&#35828;&#20013;&#65289;&#36827;&#34892;&#32763;&#35793;&#30340;&#32467;&#26524;&#27604;&#26631;&#20934;&#30340;&#36880;&#21477;&#32763;&#35793;&#22312;18&#20010;&#35821;&#35328;&#23545;&#65288;&#20363;&#22914;&#65292;&#26085;&#35821;&#12289;&#27874;&#20848;&#35821;&#21644;&#33521;&#35821;&#30340;&#32763;&#35793;&#65289;&#19978;&#20135;&#29983;&#26356;&#39640;&#36136;&#37327;&#30340;&#32763;&#35793;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#38656;&#35201;&#32422;350&#20010;&#23567;&#26102;&#30340;&#27880;&#37322;&#21644;&#20998;&#26512;&#24037;&#20316;&#65292;&#36890;&#36807;&#32856;&#35831;&#29087;&#32451;&#25484;&#25569;&#28304;&#35821;&#35328;&#21644;&#30446;&#26631;&#35821;&#35328;&#30340;&#35793;&#32773;&#65292;&#24182;&#35201;&#27714;&#20182;&#20204;&#25552;&#20379;&#36328;&#24230;&#32423;&#21035;&#30340;&#38169;&#35823;&#27880;&#37322;&#20197;&#21450;&#21738;&#31181;&#31995;&#32479;&#30340;&#32763;&#35793;&#26356;&#22909;&#30340;&#20559;&#22909;&#21028;&#26029;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#31687;&#31456;&#32423;&#21035;&#30340;LLM&#32763;&#35793;&#22312;&#25991;&#23398;&#27573;&#33853;&#30340;&#32763;&#35793;&#20013;&#20986;&#29616;&#30340;&#20851;&#38190;&#38169;&#35823;&#26356;&#23569;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#19982;&#19978;&#19979;&#25991;&#21644;&#24847;&#20041;&#30456;&#20851;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are competitive with the state of the art on a wide range of sentence-level translation datasets. However, their ability to translate paragraphs and documents remains unexplored because evaluation in these settings is costly and difficult. We show through a rigorous human evaluation that asking the Gpt-3.5 (text-davinci-003) LLM to translate an entire literary paragraph (e.g., from a novel) at once results in higher-quality translations than standard sentence-by-sentence translation across 18 linguistically-diverse language pairs (e.g., translating into and out of Japanese, Polish, and English). Our evaluation, which took approximately 350 hours of effort for annotation and analysis, is conducted by hiring translators fluent in both the source and target language and asking them to provide both span-level error annotations as well as preference judgments of which system's translations are better. We observe that discourse-level LLM translators commit fewer 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25552;&#20986;&#21452;&#37325;&#24130;&#24459;&#26041;&#27861;&#29992;&#20110;&#39044;&#27979;&#29420;&#29305;&#30340;&#24615;&#33021;&#26435;&#34913;&#21069;&#27839;&#65292;&#24182;&#24314;&#31435;&#22522;&#20110;&#35813;&#26041;&#27861;&#30340;&#26679;&#26412;&#27604;&#20363;&#36873;&#25321;&#20248;&#21270;&#38382;&#39064;&#65292;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.03216</link><description>&lt;p&gt;
&#20851;&#20110;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;Pareto&#21069;&#27839;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Pareto Front of Multilingual Neural Machine Translation. (arXiv:2304.03216v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25552;&#20986;&#21452;&#37325;&#24130;&#24459;&#26041;&#27861;&#29992;&#20110;&#39044;&#27979;&#29420;&#29305;&#30340;&#24615;&#33021;&#26435;&#34913;&#21069;&#27839;&#65292;&#24182;&#24314;&#31435;&#22522;&#20110;&#35813;&#26041;&#27861;&#30340;&#26679;&#26412;&#27604;&#20363;&#36873;&#25321;&#20248;&#21270;&#38382;&#39064;&#65292;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#65292;&#32473;&#23450;&#26041;&#21521;&#30340;&#27867;&#21270;&#24615;&#33021;&#22914;&#20309;&#38543;&#20854;&#37319;&#26679;&#27604;&#20363;&#30340;&#21464;&#21270;&#32780;&#21464;&#21270;&#12290;&#36890;&#36807;&#35757;&#32451;200&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#27169;&#22411;&#22823;&#23567;&#12289;&#26041;&#21521;&#21644;&#24635;&#20219;&#21153;&#25968;&#37327;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#35757;&#32451;&#35821;&#26009;&#24211;&#23384;&#22312;&#25968;&#25454;&#19981;&#24179;&#34913;&#26102;&#65292;&#26631;&#37327;&#21270;&#23548;&#33268;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#26435;&#34913;&#21069;&#27839;&#65292;&#35813;&#21069;&#27839;&#20559;&#31163;&#20102;&#20256;&#32479;&#30340;Pareto&#21069;&#27839;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21452;&#37325;&#24130;&#24459;&#26469;&#39044;&#27979;MNMT&#20013;&#29420;&#29305;&#30340;&#24615;&#33021;&#26435;&#34913;&#21069;&#27839;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#35821;&#35328;&#12289;&#25968;&#25454;&#20805;&#36275;&#24615;&#21644;&#20219;&#21153;&#25968;&#37327;&#26041;&#38754;&#37117;&#24456;&#40065;&#26834;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;MNMT&#20013;&#30340;&#26679;&#26412;&#27604;&#20363;&#36873;&#25321;&#38382;&#39064;&#24314;&#27169;&#20026;&#22522;&#20110;&#21452;&#37325;&#24130;&#24459;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we study how the generalization performance of a given direction changes with its sampling ratio in Multilingual Neural Machine Translation (MNMT). By training over 200 multilingual models with various model sizes, directions, and total numbers of tasks, we find that scalarization leads to a multitask trade-off front that deviates from the traditional Pareto front when there exists data imbalance in the training corpus. That is, the performance of certain translation directions does not improve with the increase of its weight in the multi-task optimization objective, which poses greater challenge to improve the overall performance of all directions. Based on our observations, we propose the Double Power Law to predict the unique performance trade-off front in MNMT, which is robust across various languages, data adequacy and number of tasks. Finally, we formulate sample ratio selection in MNMT as an optimization problem based on the Double Power Law, which achieves better 
&lt;/p&gt;</description></item><item><title>ETPNav&#26159;&#19968;&#20010;&#33021;&#22815;&#22312;&#36830;&#32493;&#29615;&#22659;&#20013;&#36827;&#34892;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#30340;&#26032;&#23548;&#33322;&#26694;&#26550;&#65292;&#23427;&#20855;&#26377;&#20004;&#20010;&#20851;&#38190;&#25216;&#33021;&#65306;&#33021;&#22815;&#25277;&#35937;&#29615;&#22659;&#19982;&#29983;&#25104;&#38271;&#31243;&#23548;&#33322;&#35745;&#21010;&#20197;&#21450;&#22312;&#36830;&#32493;&#29615;&#22659;&#20013;&#36991;&#38556;&#25511;&#21046;&#30340;&#33021;&#21147;&#12290;ETPNav&#20351;&#29992;&#28436;&#21270;&#31639;&#27861;&#20248;&#21270;&#25299;&#25169;&#35268;&#21010;&#27169;&#22359;&#24182;&#22312;Matterport3D&#27169;&#25311;&#22120;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102;&#20154;&#31867;&#27700;&#24179;&#30340;VLN-CE&#20219;&#21153;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.03047</link><description>&lt;p&gt;
ETPNav: &#22312;&#36830;&#32493;&#29615;&#22659;&#20013;&#28436;&#21270;&#25299;&#25169;&#35268;&#21010;&#30340;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
ETPNav: Evolving Topological Planning for Vision-Language Navigation in Continuous Environments. (arXiv:2304.03047v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03047
&lt;/p&gt;
&lt;p&gt;
ETPNav&#26159;&#19968;&#20010;&#33021;&#22815;&#22312;&#36830;&#32493;&#29615;&#22659;&#20013;&#36827;&#34892;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#30340;&#26032;&#23548;&#33322;&#26694;&#26550;&#65292;&#23427;&#20855;&#26377;&#20004;&#20010;&#20851;&#38190;&#25216;&#33021;&#65306;&#33021;&#22815;&#25277;&#35937;&#29615;&#22659;&#19982;&#29983;&#25104;&#38271;&#31243;&#23548;&#33322;&#35745;&#21010;&#20197;&#21450;&#22312;&#36830;&#32493;&#29615;&#22659;&#20013;&#36991;&#38556;&#25511;&#21046;&#30340;&#33021;&#21147;&#12290;ETPNav&#20351;&#29992;&#28436;&#21270;&#31639;&#27861;&#20248;&#21270;&#25299;&#25169;&#35268;&#21010;&#27169;&#22359;&#24182;&#22312;Matterport3D&#27169;&#25311;&#22120;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102;&#20154;&#31867;&#27700;&#24179;&#30340;VLN-CE&#20219;&#21153;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#38656;&#35201;&#26234;&#33021;&#20307;&#36981;&#24490;&#25351;&#31034;&#22312;&#29615;&#22659;&#20013;&#23548;&#33322;&#65292;&#35813;&#20219;&#21153;&#22312;&#20307;&#39564;&#24335;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#65292;&#22914;&#33258;&#27835;&#23548;&#33322;&#12289;&#25628;&#32034;&#19982;&#25937;&#25588;&#21644;&#20154;&#26426;&#20132;&#20114;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#20026;&#23454;&#29992;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#26223; - &#22312;&#36830;&#32493;&#29615;&#22659;&#20013;&#36827;&#34892;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#65288;VLN-CE&#65289;&#12290;&#20026;&#20102;&#24320;&#21457;&#19968;&#20010;&#24378;&#22823;&#30340;VLN-CE&#20195;&#29702;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23548;&#33322;&#26694;&#26550;ETPNav&#65292;&#23427;&#19987;&#27880;&#20110;&#20004;&#20010;&#20851;&#38190;&#25216;&#33021;&#65306;1&#65289;&#25277;&#35937;&#29615;&#22659;&#21644;&#29983;&#25104;&#38271;&#31243;&#23548;&#33322;&#35745;&#21010;&#30340;&#33021;&#21147;&#65307;&#21644;2&#65289;&#22312;&#36830;&#32493;&#29615;&#22659;&#20013;&#36991;&#38556;&#25511;&#21046;&#30340;&#33021;&#21147;&#12290;ETPNav&#36890;&#36807;&#33258;&#32452;&#32455;&#27839;&#30528;&#32463;&#36807;&#30340;&#36335;&#24452;&#39044;&#27979;&#30340;&#36335;&#26631;&#36827;&#34892;&#22312;&#32447;&#29615;&#22659;&#25299;&#25169;&#26144;&#23556;&#65292;&#32780;&#19981;&#38656;&#35201;&#20808;&#21069;&#30340;&#29615;&#22659;&#32463;&#39564;&#12290;&#23427;&#23558;&#23548;&#33322;&#36807;&#31243;&#20998;&#35299;&#20026;&#39640;&#23618;&#35268;&#21010;&#21644;&#20302;&#23618;&#25511;&#21046;&#12290;&#21516;&#26102;&#65292;ETPNav&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#28436;&#21270;&#31639;&#27861;&#26469;&#20248;&#21270;&#25299;&#25169;&#35268;&#21010;&#27169;&#22359;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#38271;&#26399;&#23548;&#33322;&#35745;&#21010;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;Matterport3D&#27169;&#25311;&#22120;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#20219;&#24847;&#36215;&#28857;&#21644;&#32456;&#28857;&#30340;VLN-CE&#20219;&#21153;&#20013;&#36798;&#21040;&#20102;&#20154;&#31867;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-language navigation is a task that requires an agent to follow instructions to navigate in environments. It becomes increasingly crucial in the field of embodied AI, with potential applications in autonomous navigation, search and rescue, and human-robot interaction. In this paper, we propose to address a more practical yet challenging counterpart setting - vision-language navigation in continuous environments (VLN-CE). To develop a robust VLN-CE agent, we propose a new navigation framework, ETPNav, which focuses on two critical skills: 1) the capability to abstract environments and generate long-range navigation plans, and 2) the ability of obstacle-avoiding control in continuous environments. ETPNav performs online topological mapping of environments by self-organizing predicted waypoints along a traversed path, without prior environmental experience. It privileges the agent to break down the navigation procedure into high-level planning and low-level control. Concurrently, ET
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#25506;&#35752;&#20102;ChatGPT&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#12289;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#65292;&#24378;&#35843;&#20102;&#20351;&#29992;&#36825;&#20010;&#24378;&#22823;&#24037;&#20855;&#26102;&#30340;&#36947;&#24503;&#32771;&#34385;&#65292;&#20026;&#20154;&#24037;&#26234;&#33021;&#21644;NLP&#39046;&#22495;&#30340;&#35752;&#35770;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2304.02017</link><description>&lt;p&gt;
&#35299;&#38145;ChatGPT&#30340;&#28508;&#21147;&#65306;&#23545;&#20854;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#24212;&#29992;&#12289;&#20248;&#28857;&#12289;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#26041;&#21521;&#30340;&#20840;&#38754;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
Unlocking the Potential of ChatGPT: A Comprehensive Exploration of its Applications, Advantages, Limitations, and Future Directions in Natural Language Processing. (arXiv:2304.02017v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#25506;&#35752;&#20102;ChatGPT&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#12289;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#65292;&#24378;&#35843;&#20102;&#20351;&#29992;&#36825;&#20010;&#24378;&#22823;&#24037;&#20855;&#26102;&#30340;&#36947;&#24503;&#32771;&#34385;&#65292;&#20026;&#20154;&#24037;&#26234;&#33021;&#21644;NLP&#39046;&#22495;&#30340;&#35752;&#35770;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#24191;&#27867;&#24212;&#29992;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#32842;&#22825;&#26426;&#22120;&#20154;&#12289;&#20869;&#23481;&#29983;&#25104;&#12289;&#35821;&#35328;&#32763;&#35793;&#12289;&#20010;&#24615;&#21270;&#25512;&#33616;&#21644;&#21307;&#30103;&#35786;&#26029;&#27835;&#30103;&#12290;&#23427;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#20934;&#30830;&#24615;&#20351;&#20854;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#20294;&#26159;&#65292;ChatGPT&#20063;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#20363;&#22914;&#20854;&#20542;&#21521;&#20110;&#20135;&#29983;&#26377;&#20559;&#35265;&#30340;&#21709;&#24212;&#20197;&#21450;&#23384;&#22312;&#28508;&#22312;&#30340;&#26377;&#23475;&#35821;&#35328;&#27169;&#24335;&#12290;&#26412;&#25991;&#20840;&#38754;&#27010;&#36848;&#20102;ChatGPT&#21450;&#20854;&#24212;&#29992;&#12289;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#20351;&#29992;&#36825;&#20010;&#24378;&#22823;&#24037;&#20855;&#26102;&#36947;&#24503;&#32771;&#34385;&#30340;&#37325;&#35201;&#24615;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#36890;&#36807;&#25552;&#20379;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#30340;&#35265;&#35299;&#65292;&#20026;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#21450;&#20854;&#23545;&#35270;&#35273;&#21644;NLP&#39046;&#22495;&#30340;&#24433;&#21709;&#30340;&#25345;&#32493;&#35752;&#35770;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is a powerful tool in the field of artificial intelligence that has been widely used in various applications. ChatGPT has been applied successfully in chatbots, content generation, language translation, personalized recommendations, and medical diagnosis and treatment. Its versatility and accuracy make it a powerful tool for natural language processing (NLP). However, there are also limitations to ChatGPT, such as its tendency to produce biased responses and its potential to perpetuate harmful language patterns. This article provides a comprehensive overview of ChatGPT, its applications, advantages, and limitations. Additionally, the paper emphasizes the importance of ethical considerations when using this robust tool in real-world scenarios. Finally, This paper contributes to ongoing discussions surrounding artificial intelligence and its impact on vision and NLP domains by providing insights into prompt engineering techniques.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;GPTEval&#65292;&#19968;&#20010;&#21033;&#29992;&#38142;&#24335;&#24605;&#32771;&#21644;&#24418;&#24335;&#22635;&#20805;&#35780;&#20215;NLG&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#25991;&#26412;&#25688;&#35201;&#20219;&#21153;&#20013;&#65292;GPTEval&#32467;&#21512;GPT-4&#21462;&#24471;&#20102;0.514&#30340;&#26031;&#30382;&#23572;&#26364;&#30456;&#20851;&#31995;&#25968;&#65292;&#32988;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.16634</link><description>&lt;p&gt;
GPTEval&#65306;&#20351;&#29992;GPT-4&#21644;&#26356;&#22909;&#30340;&#20154;&#31867;&#23545;&#40784;&#26469;&#35780;&#20272;NLG
&lt;/p&gt;
&lt;p&gt;
GPTEval: NLG Evaluation using GPT-4 with Better Human Alignment. (arXiv:2303.16634v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;GPTEval&#65292;&#19968;&#20010;&#21033;&#29992;&#38142;&#24335;&#24605;&#32771;&#21644;&#24418;&#24335;&#22635;&#20805;&#35780;&#20215;NLG&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#25991;&#26412;&#25688;&#35201;&#20219;&#21153;&#20013;&#65292;GPTEval&#32467;&#21512;GPT-4&#21462;&#24471;&#20102;0.514&#30340;&#26031;&#30382;&#23572;&#26364;&#30456;&#20851;&#31995;&#25968;&#65292;&#32988;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#31995;&#32479;&#29983;&#25104;&#30340;&#25991;&#26412;&#36136;&#37327;&#24456;&#38590;&#36827;&#34892;&#33258;&#21160;&#27979;&#37327;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#21442;&#32771;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#22914;BLEU&#21644;ROUGE&#24050;&#34987;&#35777;&#26126;&#22312;&#38656;&#35201;&#21019;&#36896;&#24615;&#21644;&#22810;&#26679;&#24615;&#30340;&#20219;&#21153;&#20013;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#30456;&#23545;&#36739;&#20302;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24314;&#35758;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#26080;&#21442;&#32771;&#30340;NLG&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#65292;&#36825;&#20123;&#27169;&#22411;&#36866;&#29992;&#20110;&#32570;&#20047;&#20154;&#31867;&#21442;&#32771;&#30340;&#26032;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20110;LLM&#30340;&#35780;&#20272;&#22120;&#20173;&#28982;&#27604;&#20013;&#31561;&#35268;&#27169;&#30340;&#31070;&#32463;&#35780;&#20272;&#22120;&#30340;&#20154;&#31867;&#23545;&#24212;&#24230;&#20302;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GPTEval&#65292;&#19968;&#20010;&#20351;&#29992;&#38142;&#24335;&#24605;&#32771;&#65288;CoT&#65289;&#21644;&#24418;&#24335;&#22635;&#20805;&#33539;&#24335;&#26469;&#35780;&#20272;NLG&#36755;&#20986;&#36136;&#37327;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#38024;&#23545;&#20004;&#20010;&#29983;&#25104;&#20219;&#21153;&#65292;&#25991;&#26412;&#25688;&#35201;&#21644;&#23545;&#35805;&#29983;&#25104;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#25105;&#20204;&#23637;&#31034;&#20986;&#65292;GPTEval&#32467;&#21512;GPT-4&#20316;&#20026;&#39592;&#24178;&#27169;&#22411;&#65292;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;0.514&#30340;&#26031;&#30382;&#23572;&#26364;&#30456;&#20851;&#31995;&#25968;&#65292;&#32988;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators. In this work, we present GPTEval, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs. We experiment with two generation tasks, text summarization and dialogue generation. We show that GPTEval with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperform
&lt;/p&gt;</description></item><item><title>VideoXum&#26159;&#19968;&#20010;&#26032;&#30340;&#32852;&#21512;&#35270;&#39057;&#21644;&#25991;&#26412;&#25688;&#35201;&#20219;&#21153;&#65292;&#23427;&#30340;&#30446;&#26631;&#26159;&#20174;&#38271;&#35270;&#39057;&#20013;&#29983;&#25104;&#23545;&#24212;&#30340;&#31616;&#21270;&#35270;&#39057;&#21098;&#36753;&#21644;&#25991;&#26412;&#25688;&#35201;&#65292;&#21033;&#29992;&#20102;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#20851;&#32852;&#21644;&#21452;&#37325;&#27880;&#24847;&#26426;&#21046;&#12290;&#35813;&#27169;&#22411;&#27604;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#22312;&#35270;&#39057;&#21644;&#25991;&#26412;&#25688;&#35201;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2303.12060</link><description>&lt;p&gt;
VideoXum: &#35270;&#39057;&#30340;&#36328;&#27169;&#24577;&#35270;&#35273;&#21644;&#25991;&#26412;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
VideoXum: Cross-modal Visual and Textural Summarization of Videos. (arXiv:2303.12060v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12060
&lt;/p&gt;
&lt;p&gt;
VideoXum&#26159;&#19968;&#20010;&#26032;&#30340;&#32852;&#21512;&#35270;&#39057;&#21644;&#25991;&#26412;&#25688;&#35201;&#20219;&#21153;&#65292;&#23427;&#30340;&#30446;&#26631;&#26159;&#20174;&#38271;&#35270;&#39057;&#20013;&#29983;&#25104;&#23545;&#24212;&#30340;&#31616;&#21270;&#35270;&#39057;&#21098;&#36753;&#21644;&#25991;&#26412;&#25688;&#35201;&#65292;&#21033;&#29992;&#20102;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#20851;&#32852;&#21644;&#21452;&#37325;&#27880;&#24847;&#26426;&#21046;&#12290;&#35813;&#27169;&#22411;&#27604;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#22312;&#35270;&#39057;&#21644;&#25991;&#26412;&#25688;&#35201;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#25688;&#35201;&#26088;&#22312;&#20174;&#28304;&#35270;&#39057;&#20013;&#25552;&#28860;&#20986;&#26368;&#37325;&#35201;&#30340;&#20449;&#24687;&#65292;&#20197;&#29983;&#25104;&#31616;&#30701;&#30340;&#35270;&#39057;&#21098;&#36753;&#25110;&#25991;&#26412;&#21465;&#36848;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#21512;&#35270;&#39057;&#21644;&#25991;&#26412;&#25688;&#35201;&#20219;&#21153;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#20154;&#24037;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598; -- VideoXum&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21033;&#29992;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#21033;&#29992;&#21452;&#37325;&#27880;&#24847;&#26426;&#21046;&#26469;&#23545;&#40784;&#35270;&#35273;&#21644;&#25991;&#26412;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35270;&#39057;&#21644;&#25991;&#26412;&#25688;&#35201;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video summarization aims to distill the most important information from a source video to produce either an abridged clip or a textual narrative. Traditionally, different methods have been proposed depending on whether the output is a video or text, thus ignoring the correlation between the two semantically related tasks of visual summarization and textual summarization. We propose a new joint video and text summarization task. The goal is to generate both a shortened video clip along with the corresponding textual summary from a long video, collectively referred to as a cross-modal summary. The generated shortened video clip and text narratives should be semantically well aligned. To this end, we first build a large-scale human-annotated dataset -- VideoXum (X refers to different modalities). The dataset is reannotated based on ActivityNet. After we filter out the videos that do not meet the length requirements, 14,001 long videos remain in our new dataset. Each video in our reannotat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20020;&#24202;BERTScore&#65288;CBERTScore&#65289;&#24230;&#37327;&#65292;&#23427;&#27604;&#20854;&#20182;&#24230;&#37327;&#26356;&#20005;&#21385;&#22320;&#24809;&#32602;&#20020;&#24202;&#30456;&#20851;&#30340;&#38169;&#35823;&#65292;&#26356;&#25509;&#36817;&#20110;&#20020;&#24202;&#21307;&#29983;&#23545;&#21307;&#23398;&#21477;&#23376;&#30340;&#20559;&#22909;&#12290;&#20316;&#32773;&#36824;&#25910;&#38598;&#20102;13&#20010;&#20020;&#24202;&#21307;&#29983;&#23545;149&#20010;&#29616;&#23454;&#21307;&#23398;&#21477;&#23376;&#30340;&#20559;&#22909;&#22522;&#20934;&#65292;&#31216;&#20026;&#20020;&#24202;&#36716;&#24405;&#20559;&#22909;&#22522;&#20934;&#65288;CTP&#65289;&#65292;&#35777;&#26126;CBERTScore&#26356;&#25509;&#36817;&#20110;&#20020;&#24202;&#21307;&#29983;&#30340;&#20559;&#22909;&#65292;&#24182;&#23558;&#22522;&#20934;&#21457;&#24067;&#32473;&#31038;&#21306;&#20197;&#36827;&#19968;&#27493;&#24320;&#21457;&#20855;&#26377;&#20020;&#24202;&#24847;&#35782;&#30340;ASR&#24230;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.05737</link><description>&lt;p&gt;
&#20020;&#24202;BERTScore&#65306;&#20020;&#24202;&#29615;&#22659;&#19979;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#30340;&#25913;&#36827;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Clinical BERTScore: An Improved Measure of Automatic Speech Recognition Performance in Clinical Settings. (arXiv:2303.05737v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20020;&#24202;BERTScore&#65288;CBERTScore&#65289;&#24230;&#37327;&#65292;&#23427;&#27604;&#20854;&#20182;&#24230;&#37327;&#26356;&#20005;&#21385;&#22320;&#24809;&#32602;&#20020;&#24202;&#30456;&#20851;&#30340;&#38169;&#35823;&#65292;&#26356;&#25509;&#36817;&#20110;&#20020;&#24202;&#21307;&#29983;&#23545;&#21307;&#23398;&#21477;&#23376;&#30340;&#20559;&#22909;&#12290;&#20316;&#32773;&#36824;&#25910;&#38598;&#20102;13&#20010;&#20020;&#24202;&#21307;&#29983;&#23545;149&#20010;&#29616;&#23454;&#21307;&#23398;&#21477;&#23376;&#30340;&#20559;&#22909;&#22522;&#20934;&#65292;&#31216;&#20026;&#20020;&#24202;&#36716;&#24405;&#20559;&#22909;&#22522;&#20934;&#65288;CTP&#65289;&#65292;&#35777;&#26126;CBERTScore&#26356;&#25509;&#36817;&#20110;&#20020;&#24202;&#21307;&#29983;&#30340;&#20559;&#22909;&#65292;&#24182;&#23558;&#22522;&#20934;&#21457;&#24067;&#32473;&#31038;&#21306;&#20197;&#36827;&#19968;&#27493;&#24320;&#21457;&#20855;&#26377;&#20020;&#24202;&#24847;&#35782;&#30340;ASR&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper proposes a Clinical BERTScore (CBERTScore) metric for ASR in medical contexts, which penalizes clinically-relevant mistakes more than other metrics and aligns more closely with clinician preferences. The authors also collect a benchmark of clinician preferences on medical sentences and release it for the community to further develop clinically-aware ASR metrics.
&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#29615;&#22659;&#20013;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#26377;&#28508;&#21147;&#33410;&#30465;&#26102;&#38388;&#65292;&#38477;&#20302;&#25104;&#26412;&#65292;&#25552;&#39640;&#25253;&#21578;&#20934;&#30830;&#24615;&#24182;&#20943;&#23569;&#21307;&#29983;&#30340;&#30130;&#21171;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36991;&#20813;&#21307;&#23398;&#30456;&#20851;&#30340;&#36716;&#24405;&#38169;&#35823;&#30340;&#37325;&#35201;&#24615;&#65292;&#21307;&#30103;&#34892;&#19994;&#37319;&#29992;&#36825;&#31181;&#25216;&#26415;&#30340;&#36895;&#24230;&#36739;&#24930;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20020;&#24202;BERTScore&#65288;CBERTScore&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;ASR&#24230;&#37327;&#65292;&#23427;&#27604;&#20854;&#20182;&#24230;&#37327;&#65288;WER&#12289;BLUE&#12289;METEOR&#31561;&#65289;&#26356;&#20005;&#21385;&#22320;&#24809;&#32602;&#20020;&#24202;&#30456;&#20851;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#24230;&#37327;&#26356;&#25509;&#36817;&#20110;&#20020;&#24202;&#21307;&#29983;&#23545;&#21307;&#23398;&#21477;&#23376;&#30340;&#20559;&#22909;&#65292;&#26377;&#26102;&#24046;&#36317;&#24456;&#22823;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;13&#20010;&#20020;&#24202;&#21307;&#29983;&#23545;149&#20010;&#29616;&#23454;&#21307;&#23398;&#21477;&#23376;&#30340;&#20559;&#22909;&#22522;&#20934;&#65292;&#31216;&#20026;&#20020;&#24202;&#36716;&#24405;&#20559;&#22909;&#22522;&#20934;&#65288;CTP&#65289;&#65292;&#35777;&#26126;CBERTScore&#26356;&#25509;&#36817;&#20110;&#20020;&#24202;&#21307;&#29983;&#30340;&#20559;&#22909;&#65292;&#24182;&#23558;&#22522;&#20934;&#21457;&#24067;&#32473;&#31038;&#21306;&#20197;&#36827;&#19968;&#27493;&#24320;&#21457;&#20855;&#26377;&#20020;&#24202;&#24847;&#35782;&#30340;ASR&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic Speech Recognition (ASR) in medical contexts has the potential to save time, cut costs, increase report accuracy, and reduce physician burnout. However, the healthcare industry has been slower to adopt this technology, in part due to the importance of avoiding medically-relevant transcription mistakes. In this work, we present the Clinical BERTScore (CBERTScore), an ASR metric that penalizes clinically-relevant mistakes more than others. We demonstrate that this metric more closely aligns with clinician preferences on medical sentences as compared to other metrics (WER, BLUE, METEOR, etc), sometimes by wide margins. We collect a benchmark of 13 clinician preferences on 149 realistic medical sentences called the Clinician Transcript Preference benchmark (CTP), demonstrate that CBERTScore more closely matches what clinicians prefer, and release the benchmark for the community to further develop clinically-aware ASR metrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#36719;&#30828;&#26631;&#31614;&#39044;&#27979;&#20013;&#65292;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#23545;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36755;&#20986;&#30340;&#24433;&#21709;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#24358;&#28608;&#27963;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.02468</link><description>&lt;p&gt;
SemEval-2023&#20219;&#21153;11&#30340;Lon-ea&#65306;&#36719;&#30828;&#26631;&#31614;&#39044;&#27979;&#20013;&#28608;&#27963;&#20989;&#25968;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lon-ea at SemEval-2023 Task 11: A Comparison of Activation Functions for Soft and Hard Label Prediction. (arXiv:2303.02468v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#36719;&#30828;&#26631;&#31614;&#39044;&#27979;&#20013;&#65292;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#23545;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36755;&#20986;&#30340;&#24433;&#21709;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#24358;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#22312;&#23398;&#20064;&#19981;&#21516;&#24847;&#20219;&#21153;&#30340;&#36719;&#30828;&#26631;&#31614;&#39044;&#27979;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36755;&#20986;&#23618;&#20013;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#30340;&#24433;&#21709;&#12290;&#22312;&#35813;&#20219;&#21153;&#20013;&#65292;&#30446;&#26631;&#26159;&#36890;&#36807;&#39044;&#27979;&#36719;&#26631;&#31614;&#26469;&#37327;&#21270;&#19981;&#21516;&#24847;&#37327;&#12290;&#20026;&#20102;&#39044;&#27979;&#36719;&#26631;&#31614;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;BERT&#30340;&#39044;&#22788;&#29702;&#22120;&#21644;&#32534;&#30721;&#22120;&#65292;&#24182;&#25913;&#21464;&#36755;&#20986;&#23618;&#20013;&#20351;&#29992;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#20182;&#21442;&#25968;&#19981;&#21464;&#12290;&#28982;&#21518;&#23558;&#36719;&#26631;&#31614;&#29992;&#20110;&#30828;&#26631;&#31614;&#39044;&#27979;&#12290;&#32771;&#34385;&#30340;&#28608;&#27963;&#20989;&#25968;&#21253;&#25324;sigmoid&#20989;&#25968;&#20197;&#21450;&#28155;&#21152;&#21040;&#27169;&#22411;&#20013;&#30340;&#38454;&#36291;&#20989;&#25968;&#21644;&#26412;&#25991;&#20013;&#39318;&#27425;&#20171;&#32461;&#30340;&#27491;&#24358;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the influence of different activation functions in the output layer of deep neural network models for soft and hard label prediction in the learning with disagreement task. In this task, the goal is to quantify the amount of disagreement via predicting soft labels. To predict the soft labels, we use BERT-based preprocessors and encoders and vary the activation function used in the output layer, while keeping other parameters constant. The soft labels are then used for the hard label prediction. The activation functions considered are sigmoid as well as a step-function that is added to the model post-training and a sinusoidal activation function, which is introduced for the first time in this paper.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#21644;&#31574;&#30053;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#28151;&#21512;&#25216;&#26415;&#22312;&#22797;&#26434;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#24212;&#29992;&#65292;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2302.09051</link><description>&lt;p&gt;
&#22797;&#26434;&#38382;&#31572;&#21644;&#35821;&#35328;&#27169;&#22411;&#28151;&#21512;&#26550;&#26500;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Complex QA and language models hybrid architectures, Survey. (arXiv:2302.09051v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#21644;&#31574;&#30053;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#28151;&#21512;&#25216;&#26415;&#22312;&#22797;&#26434;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#24212;&#29992;&#65292;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#21644;&#31574;&#30053;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;&#28151;&#21512;&#25216;&#26415;&#22312;&#22797;&#26434;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#24212;&#29992;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#26631;&#20934;&#38382;&#39064;&#19978;&#21033;&#29992;&#20844;&#20849;&#25968;&#25454;&#65292;&#20294;&#22312;&#35299;&#20915;&#26356;&#20855;&#20307;&#30340;&#22797;&#26434;&#38382;&#39064;&#26102;&#65288;&#22914;&#22312;&#19981;&#21516;&#25991;&#21270;&#20013;&#20010;&#20154;&#33258;&#30001;&#27010;&#24565;&#30340;&#21464;&#21270;&#22914;&#20309;&#65311;&#20160;&#20040;&#26159;&#20026;&#20943;&#23569;&#27668;&#20505;&#21464;&#21270;&#32780;&#23454;&#29616;&#30340;&#26368;&#20339;&#21457;&#30005;&#26041;&#27861;&#32452;&#21512;&#65311;&#65289;&#65292;&#38656;&#35201;&#29305;&#23450;&#30340;&#26550;&#26500;&#12289;&#30693;&#35782;&#12289;&#25216;&#33021;&#12289;&#26041;&#27861;&#12289;&#25935;&#24863;&#25968;&#25454;&#20445;&#25252;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#20154;&#31867;&#23457;&#25209;&#21644;&#22810;&#21151;&#33021;&#21453;&#39304;&#12290;&#26368;&#36817;&#30340;&#39033;&#30446;&#22914;ChatGPT&#21644;GALACTICA&#20801;&#35768;&#38750;&#19987;&#19994;&#20154;&#21592;&#20102;&#35299;LLM&#22312;&#22797;&#26434;QA&#20013;&#30340;&#24040;&#22823;&#28508;&#21147;&#20197;&#21450;&#21516;&#31561;&#24378;&#22823;&#30340;&#23616;&#38480;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23457;&#26597;&#25152;&#38656;&#30340;&#25216;&#33021;&#21644;&#35780;&#20272;&#25216;&#26415;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32508;&#36848;&#20102;&#29616;&#26377;&#30340;&#28151;&#21512;&#26550;&#26500;&#65292;&#23558;LLM&#19982;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#12289;&#20449;&#24687;&#26816;&#32034;&#12289;&#30693;&#35782;&#22270;&#35889;&#21644;&#20854;&#20182;AI/ML&#25216;&#26415;&#30456;&#32467;&#21512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25351;&#20986;&#36825;&#20123;CQA&#31995;&#32479;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#26410;&#26469;&#30740;&#31350;&#30340;&#21487;&#33021;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper reviews the state-of-the-art of language models architectures and strategies for "complex" question-answering (QA, CQA, CPS) with a focus on hybridization. Large Language Models (LLM) are good at leveraging public data on standard problems but once you want to tackle more specific complex questions or problems (e.g. How does the concept of personal freedom vary between different cultures ? What is the best mix of power generation methods to reduce climate change ?) you may need specific architecture, knowledge, skills, methods, sensitive data protection, explainability, human approval and versatile feedback... Recent projects like ChatGPT and GALACTICA have allowed non-specialists to grasp the great potential as well as the equally strong limitations of LLM in complex QA. In this paper, we start by reviewing required skills and evaluation techniques. We integrate findings from the robust community edited research papers BIG, BLOOM and HELM which open source, benchmark and an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21453;&#20363;&#27979;&#35797;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#20869;&#37096;&#21051;&#26495;&#21360;&#35937;&#30340;&#26041;&#27861;&#65292;&#37325;&#28857;&#26159;&#24615;&#21035;&#20559;&#35265;&#65292;&#32467;&#26524;&#34920;&#26126;&#27169;&#22411;&#22312;&#20351;&#29992;&#19981;&#30456;&#20851;&#30340;&#30693;&#35782;&#26102;&#34920;&#29616;&#20986;&#19968;&#23450;&#30340;&#40065;&#26834;&#24615;&#65292;&#26356;&#20542;&#21521;&#20110;&#20351;&#29992;&#27973;&#23618;&#30340;&#35821;&#35328;&#32447;&#32034;&#26469;&#25913;&#21464;&#20869;&#37096;&#21051;&#26495;&#21360;&#35937;&#12290;</title><link>http://arxiv.org/abs/2301.04347</link><description>&lt;p&gt;
Counteracts&#65306;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#27979;&#35797;&#21051;&#26495;&#21360;&#35937;&#30340;&#23545;&#25239;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counteracts: Testing Stereotypical Representation in Pre-trained Language Models. (arXiv:2301.04347v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21453;&#20363;&#27979;&#35797;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#20869;&#37096;&#21051;&#26495;&#21360;&#35937;&#30340;&#26041;&#27861;&#65292;&#37325;&#28857;&#26159;&#24615;&#21035;&#20559;&#35265;&#65292;&#32467;&#26524;&#34920;&#26126;&#27169;&#22411;&#22312;&#20351;&#29992;&#19981;&#30456;&#20851;&#30340;&#30693;&#35782;&#26102;&#34920;&#29616;&#20986;&#19968;&#23450;&#30340;&#40065;&#26834;&#24615;&#65292;&#26356;&#20542;&#21521;&#20110;&#20351;&#29992;&#27973;&#23618;&#30340;&#35821;&#35328;&#32447;&#32034;&#26469;&#25913;&#21464;&#20869;&#37096;&#21051;&#26495;&#21360;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#19982;&#20154;&#31867;&#19968;&#26679;&#65292;&#35821;&#35328;&#27169;&#22411;&#20063;&#21487;&#33021;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#33258;&#24049;&#30340;&#20559;&#35265;&#12290;&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#19979;&#28216;&#20219;&#21153;&#23558;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#31649;&#36947;&#30340;&#19968;&#37096;&#20998;&#38598;&#25104;&#65292;&#26377;&#24517;&#35201;&#20102;&#35299;&#20869;&#37096;&#21051;&#26495;&#21360;&#35937;&#21644;&#20943;&#36731;&#36127;&#38754;&#24433;&#21709;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21453;&#20363;&#26469;&#27979;&#35797;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20869;&#37096;&#21051;&#26495;&#21360;&#35937;&#12290;&#25105;&#20204;&#20027;&#35201;&#20851;&#27880;&#24615;&#21035;&#20559;&#35265;&#65292;&#20294;&#35813;&#26041;&#27861;&#21487;&#20197;&#25193;&#23637;&#21040;&#20854;&#20182;&#31867;&#22411;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;9&#20010;&#19981;&#21516;&#30340;&#22635;&#31354;&#24335;&#25552;&#31034;&#30340;&#27169;&#22411;&#65292;&#21253;&#25324;&#30693;&#35782;&#21644;&#22522;&#30784;&#25552;&#31034;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20351;&#29992;&#19981;&#30456;&#20851;&#30340;&#30693;&#35782;&#26102;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#19968;&#23450;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#26356;&#20542;&#21521;&#20110;&#20351;&#29992;&#27973;&#23618;&#30340;&#35821;&#35328;&#32447;&#32034;&#65292;&#22914;&#21333;&#35789;&#20301;&#32622;&#21644;&#21477;&#27861;&#32467;&#26500;&#26469;&#25913;&#21464;&#20869;&#37096;&#21051;&#26495;&#21360;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models have demonstrated strong performance on various natural language understanding tasks. Similar to humans, language models could also have their own bias that is learned from the training data. As more and more downstream tasks integrate language models as part of the pipeline, it is necessary to understand the internal stereotypical representation and the methods to mitigate the negative effects. In this paper, we proposed a simple method to test the internal stereotypical representation in pre-trained language models using counterexamples. We mainly focused on gender bias, but the method can be extended to other types of bias. We evaluated models on 9 different cloze-style prompts consisting of knowledge and base prompts. Our results indicate that pre-trained language models show a certain amount of robustness when using unrelated knowledge, and prefer shallow linguistic cues, such as word position and syntactic structure, to alter the internal stereotypical representat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21644;&#21487;&#35299;&#37322;&#30340;&#20114;&#32852;&#32593;&#36855;&#22240;&#20998;&#31867;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20013;&#24573;&#30053;&#36855;&#22240;&#35821;&#20041;&#21644;&#21019;&#24314;&#19978;&#19979;&#25991;&#23548;&#33268;&#20844;&#27491;&#20869;&#23481;&#31649;&#29702;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#20316;&#32773;&#37319;&#29992;&#31034;&#20363;&#21644;&#22522;&#20110;&#21407;&#22411;&#30340;&#25512;&#29702;&#24182;&#32467;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;SOTA&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#25104;&#21151;&#22312;&#20004;&#20010;&#20219;&#21153;&#20013;&#26816;&#27979;&#20102;&#26377;&#23475;&#30340;&#36855;&#22240;&#12290;</title><link>http://arxiv.org/abs/2212.05612</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#21644;&#21487;&#35299;&#37322;&#30340;&#20114;&#32852;&#32593;&#36855;&#22240;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Multimodal and Explainable Internet Meme Classification. (arXiv:2212.05612v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21644;&#21487;&#35299;&#37322;&#30340;&#20114;&#32852;&#32593;&#36855;&#22240;&#20998;&#31867;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20013;&#24573;&#30053;&#36855;&#22240;&#35821;&#20041;&#21644;&#21019;&#24314;&#19978;&#19979;&#25991;&#23548;&#33268;&#20844;&#27491;&#20869;&#23481;&#31649;&#29702;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#20316;&#32773;&#37319;&#29992;&#31034;&#20363;&#21644;&#22522;&#20110;&#21407;&#22411;&#30340;&#25512;&#29702;&#24182;&#32467;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;SOTA&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#25104;&#21151;&#22312;&#20004;&#20010;&#20219;&#21153;&#20013;&#26816;&#27979;&#20102;&#26377;&#23475;&#30340;&#36855;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#21069;&#30340;&#29615;&#22659;&#20013;&#65292;&#32593;&#32476;&#24179;&#21488;&#24050;&#32463;&#34987;&#26377;&#25928;&#22320;&#27494;&#22120;&#21270;&#65292;&#34987;&#29992;&#20110;&#21508;&#31181;&#22320;&#32536;&#25919;&#27835;&#20107;&#20214;&#21644;&#31038;&#20250;&#38382;&#39064;&#20013;&#65292;&#20114;&#32852;&#32593;&#36855;&#22240;&#20351;&#24471;&#22823;&#35268;&#27169;&#30340;&#20844;&#27491;&#20869;&#23481;&#31649;&#29702;&#26356;&#21152;&#22256;&#38590;&#12290;&#29616;&#26377;&#30340;&#36855;&#22240;&#20998;&#31867;&#21644;&#36319;&#36394;&#24037;&#20316;&#20027;&#35201;&#37319;&#29992;&#40657;&#30418;&#26041;&#27861;&#65292;&#27809;&#26377;&#26126;&#30830;&#32771;&#34385;&#36855;&#22240;&#30340;&#35821;&#20041;&#25110;&#20854;&#21019;&#24314;&#30340;&#19978;&#19979;&#25991;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36861;&#27714;&#19968;&#31181;&#27169;&#22359;&#21270;&#21644;&#21487;&#35299;&#37322;&#30340;&#20114;&#32852;&#32593;&#36855;&#22240;&#29702;&#35299;&#26550;&#26500;&#12290;&#25105;&#20204;&#35774;&#35745;&#24182;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#20998;&#31867;&#26041;&#27861;&#65292;&#23545;&#35757;&#32451;&#26696;&#20363;&#36827;&#34892;&#31034;&#20363;&#21644;&#22522;&#20110;&#21407;&#22411;&#30340;&#25512;&#29702;&#65292;&#24182;&#21033;&#29992;&#25991;&#26412;&#21644;&#35270;&#35273;SOTA&#27169;&#22411;&#26469;&#34920;&#31034;&#21508;&#20010;&#26696;&#20363;&#12290; &#25105;&#20204;&#30740;&#31350;&#20102;&#25105;&#20204;&#30340;&#27169;&#22359;&#21270;&#21644;&#21487;&#35299;&#37322;&#27169;&#22411;&#22312;&#26816;&#27979;&#20004;&#20010;&#29616;&#26377;&#20219;&#21153;&#20013;&#26377;&#23475;&#36855;&#22240;&#30340;&#30456;&#20851;&#24615;&#65306;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#21644;&#21388;&#22899;&#30151;&#20998;&#31867;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#22522;&#20110;&#31034;&#20363;&#21644;&#22522;&#20110;&#21407;&#22411;&#30340;&#26041;&#27861;&#20197;&#21450;&#25991;&#26412;&#65292;&#35270;&#35273;&#21644;&#22810;&#27169;&#24577;&#27169;&#22411;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#22312;&#19981;&#21516;&#30340;&#20219;&#21153;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the current context where online platforms have been effectively weaponized in a variety of geo-political events and social issues, Internet memes make fair content moderation at scale even more difficult. Existing work on meme classification and tracking has focused on black-box methods that do not explicitly consider the semantics of the memes or the context of their creation. In this paper, we pursue a modular and explainable architecture for Internet meme understanding. We design and implement multimodal classification methods that perform example- and prototype-based reasoning over training cases, while leveraging both textual and visual SOTA models to represent the individual cases. We study the relevance of our modular and explainable models in detecting harmful memes on two existing tasks: Hate Speech Detection and Misogyny Classification. We compare the performance between example- and prototype-based methods, and between text, vision, and multimodal models, across differen
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#65292;&#21487;&#20197;&#33258;&#21160;&#20174;&#30005;&#23376;&#20581;&#24247;&#26723;&#26696;&#65288;EHR&#65289;&#35760;&#24405;&#20013;&#26816;&#27979;&#21040;&#36880;&#20986;&#29366;&#24577;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27169;&#22411;KIRESH&#65292;&#24050;&#32463;&#26174;&#31034;&#20986;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2212.02762</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#20174;&#30005;&#23376;&#20581;&#24247;&#26723;&#26696;&#35760;&#24405;&#20013;&#35782;&#21035;&#36880;&#20986;&#29366;&#24577;
&lt;/p&gt;
&lt;p&gt;
Automated Identification of Eviction Status from Electronic Health Record Notes. (arXiv:2212.02762v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#65292;&#21487;&#20197;&#33258;&#21160;&#20174;&#30005;&#23376;&#20581;&#24247;&#26723;&#26696;&#65288;EHR&#65289;&#35760;&#24405;&#20013;&#26816;&#27979;&#21040;&#36880;&#20986;&#29366;&#24577;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27169;&#22411;KIRESH&#65292;&#24050;&#32463;&#26174;&#31034;&#20986;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#36880;&#20986;&#26159;&#20581;&#24247;&#30340;&#37325;&#35201;&#31038;&#20250;&#21644;&#34892;&#20026;&#20915;&#23450;&#22240;&#32032;&#12290;&#36880;&#20986;&#19982;&#19968;&#31995;&#21015;&#36127;&#38754;&#20107;&#20214;&#30456;&#20851;&#65292;&#21487;&#33021;&#23548;&#33268;&#22833;&#19994;&#12289;&#20303;&#25151;&#19981;&#23433;&#20840;/&#26080;&#23478;&#21487;&#24402;&#12289;&#38271;&#26399;&#36139;&#22256;&#21644;&#31934;&#31070;&#20581;&#24247;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#65292;&#21487;&#20197;&#33258;&#21160;&#20174;&#30005;&#23376;&#20581;&#24247;&#26723;&#26696;&#65288;EHR&#65289;&#35760;&#24405;&#20013;&#26816;&#27979;&#21040;&#36880;&#20986;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: Evictions are important social and behavioral determinants of health. Evictions are associated with a cascade of negative events that can lead to unemployment, housing insecurity/homelessness, long-term poverty, and mental health problems. In this study, we developed a natural language processing system to automatically detect eviction status from electronic health record (EHR) notes.  Materials and Methods: We first defined eviction status (eviction presence and eviction period) and then annotated eviction status in 5000 EHR notes from the Veterans Health Administration (VHA). We developed a novel model, KIRESH, that has shown to substantially outperform other state-of-the-art models such as fine-tuning pre-trained language models like BioBERT and BioClinicalBERT. Moreover, we designed a novel prompt to further improve the model performance by using the intrinsic connection between the two sub-tasks of eviction presence and period prediction. Finally, we used the Temperatur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;PINTO&#65292;&#22522;&#20110;&#25552;&#31034;&#29983;&#25104;&#30340;&#26041;&#24335;&#23454;&#29616;LM&#30340;&#24544;&#23454;&#25512;&#29702;&#65292;&#24182;&#36890;&#36807;&#21453;&#20107;&#23454;&#27491;&#21017;&#21270;&#26469;&#23398;&#20064;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;PINTO&#26174;&#33879;&#20248;&#20110;&#20351;&#29992;&#22806;&#37096;&#21407;&#29702;&#30340;&#22522;&#32447;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#21487;&#29702;&#35299;&#30340;&#12289;&#24544;&#23454;&#21453;&#26144;LM&#20915;&#31574;&#36807;&#31243;&#30340;&#29702;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.01562</link><description>&lt;p&gt;
&#20351;&#29992;&#25552;&#31034;&#29983;&#25104;&#30340;&#21407;&#29702;&#23454;&#29616;&#24544;&#23454;&#30340;&#35821;&#35328;&#25512;&#29702;&#8212;&#8212;PINTO
&lt;/p&gt;
&lt;p&gt;
PINTO: Faithful Language Reasoning Using Prompt-Generated Rationales. (arXiv:2211.01562v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01562
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;PINTO&#65292;&#22522;&#20110;&#25552;&#31034;&#29983;&#25104;&#30340;&#26041;&#24335;&#23454;&#29616;LM&#30340;&#24544;&#23454;&#25512;&#29702;&#65292;&#24182;&#36890;&#36807;&#21453;&#20107;&#23454;&#27491;&#21017;&#21270;&#26469;&#23398;&#20064;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;PINTO&#26174;&#33879;&#20248;&#20110;&#20351;&#29992;&#22806;&#37096;&#21407;&#29702;&#30340;&#22522;&#32447;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#21487;&#29702;&#35299;&#30340;&#12289;&#24544;&#23454;&#21453;&#26144;LM&#20915;&#31574;&#36807;&#31243;&#30340;&#29702;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#21033;&#29992;&#33258;&#24049;&#39044;&#35757;&#32451;&#21442;&#25968;&#20013;&#30340;&#28508;&#22312;&#30693;&#35782;&#22312;&#21508;&#31181;&#22522;&#20110;&#35821;&#35328;&#30340;&#25512;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#26524;&#12290;&#20026;&#20102;&#20351;&#36825;&#20010;&#25512;&#29702;&#36807;&#31243;&#26356;&#21152;&#26126;&#30830;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21033;&#29992;&#35757;&#32451;&#25110;&#25552;&#31034;&#30340;&#26041;&#24335;&#26816;&#32034;LM&#30340;&#20869;&#37096;&#30693;&#35782;&#29983;&#25104;&#33258;&#30001;&#25991;&#26412;&#21407;&#29702;&#65292;&#21487;&#20197;&#29992;&#20110;&#24341;&#23548;&#30456;&#21516;LM&#25110;&#21333;&#29420;&#30340;&#25512;&#29702;LM&#36827;&#34892;&#20219;&#21153;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#29702;&#24615;&#21270;&#30340;LM&#38656;&#35201;&#26114;&#36149;&#30340;&#21407;&#29702;&#27880;&#37322;&#21644;&#65288;&#25110;&#65289;&#35745;&#31639;&#65292;&#24182;&#19981;&#33021;&#20445;&#35777;&#23427;&#20204;&#29983;&#25104;&#30340;&#21407;&#29702;&#20250;&#25913;&#21892;LM&#30340;&#20219;&#21153;&#34920;&#29616;&#25110;&#24544;&#23454;&#22320;&#21453;&#26144;LM&#30340;&#20915;&#31574;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;PINTO&#65292;&#19968;&#31181;&#36890;&#36807;&#25552;&#31034;&#29983;&#25104;&#30340;&#23398;&#20064;&#26469;&#29702;&#24615;&#21270;&#30340;LM&#31649;&#36947;&#65292;&#24182;&#36890;&#36807;&#21453;&#20107;&#23454;&#27491;&#21017;&#21270;&#23398;&#20064;&#24544;&#23454;&#22320;&#25512;&#29702;&#21407;&#29702;&#12290;&#39318;&#20808;&#65292;PINTO&#36890;&#36807;&#25552;&#31034;&#20923;&#32467;&#30340;&#29702;&#24615;&#21270;LM&#29983;&#25104;&#33258;&#30001;&#25991;&#26412;&#30340;&#21407;&#29702;&#65292;&#20026;&#20219;&#21153;&#36755;&#20837;&#21046;&#23450;&#20102;&#21512;&#36866;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#20854;&#27425;&#65292;PINTO&#30340;&#25512;&#29702;LM&#20351;&#29992;&#29983;&#25104;&#30340;&#21407;&#29702;&#36827;&#34892;&#39044;&#27979;&#65292;&#21516;&#26102;&#34987;&#21453;&#20107;&#23454;&#27491;&#21017;&#21270;&#39033;&#25351;&#23548;&#65292;&#35813;&#39033;&#40723;&#21169;&#21363;&#20351;&#23545;&#21407;&#29702;&#36827;&#34892;&#24494;&#23567;&#30340;&#26356;&#25913;&#65292;&#39044;&#27979;&#20063;&#30456;&#21516;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#35821;&#35328;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;PINTO&#26174;&#33879;&#20248;&#20110;&#20351;&#29992;&#22806;&#37096;&#21407;&#29702;&#30340;&#22522;&#32447;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#21487;&#29702;&#35299;&#30340;&#12289;&#24544;&#23454;&#21453;&#26144;LM&#20915;&#31574;&#36807;&#31243;&#30340;&#29702;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural language models (LMs) have achieved impressive results on various language-based reasoning tasks by utilizing latent knowledge encoded in their own pretrained parameters. To make this reasoning process more explicit, recent works retrieve a rationalizing LM's internal knowledge by training or prompting it to generate free-text rationales, which can be used to guide task predictions made by either the same LM or a separate reasoning LM. However, rationalizing LMs require expensive rationale annotation and/or computation, without any assurance that their generated rationales improve LM task performance or faithfully reflect LM decision-making. In this paper, we propose PINTO, an LM pipeline that rationalizes via prompt-based learning, and learns to faithfully reason over rationales via counterfactual regularization. First, PINTO maps out a suitable reasoning process for the task input by prompting a frozen rationalizing LM to generate a free-text rationale. Second, PINTO's reasoni
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#38024;&#23545;&#33521;&#35821;&#23398;&#20064;&#32773;&#35774;&#35745;&#30340;&#22823;&#35268;&#27169;&#21477;&#23376;&#22635;&#31354;&#38382;&#39064;&#25968;&#25454;&#38598;\textsc{SC-Ques}&#65292;&#24182;&#19988;&#24050;&#32463;&#26500;&#24314;&#20102;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#26469;&#33258;&#21160;&#35299;&#20915; SC &#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2206.12036</link><description>&lt;p&gt;
SC-Ques&#65306;&#20026;&#33521;&#35821;&#23398;&#20064;&#32773;&#35774;&#35745;&#30340;&#21477;&#23376;&#22635;&#31354;&#38382;&#39064;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SC-Ques: A Sentence Completion Question Dataset for English as a Second Language Learners. (arXiv:2206.12036v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.12036
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#38024;&#23545;&#33521;&#35821;&#23398;&#20064;&#32773;&#35774;&#35745;&#30340;&#22823;&#35268;&#27169;&#21477;&#23376;&#22635;&#31354;&#38382;&#39064;&#25968;&#25454;&#38598;\textsc{SC-Ques}&#65292;&#24182;&#19988;&#24050;&#32463;&#26500;&#24314;&#20102;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#26469;&#33258;&#21160;&#35299;&#20915; SC &#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21477;&#23376;&#22635;&#31354;&#65288;SC&#65289;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#24102;&#26377;&#19968;&#20010;&#25110;&#22810;&#20010;&#31354;&#30333;&#38656;&#35201;&#22635;&#20889;&#30340;&#21477;&#23376;&#65292;&#24182;&#25552;&#20379;&#19977;&#33267;&#20116;&#20010;&#21487;&#33021;&#30340;&#21333;&#35789;&#25110;&#30701;&#35821;&#20316;&#20026;&#36873;&#39033;&#12290; SC&#38382;&#39064;&#24191;&#27867;&#29992;&#20110;&#23398;&#20064;&#33521;&#35821;&#20316;&#20026;&#31532;&#20108;&#35821;&#35328;&#65288;ESL&#65289;&#30340;&#23398;&#29983;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;SC&#25968;&#25454;&#38598;\textsc{SC-Ques}&#65292;&#30001;&#26469;&#33258;&#30495;&#23454;&#26631;&#20934;&#33521;&#35821;&#32771;&#35797;&#30340;289,148&#20010;ESL SC&#38382;&#39064;&#32452;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#25552;&#20986;&#30340;\textsc{SC-Ques}&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#33258;&#21160;&#35299;&#20915;SC&#38382;&#39064;&#30340;&#32508;&#21512;&#22522;&#20934;&#12290;&#25105;&#20204;&#23545;&#22522;&#32447;&#27169;&#22411;&#30340;&#24615;&#33021;&#12289;&#38480;&#21046;&#21644;&#26435;&#34913;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#20998;&#26512;&#12290;&#25968;&#25454;&#21644;&#20195;&#30721;&#21487;&#29992;&#20110;&#30740;&#31350;&#30446;&#30340;&#65306;\url{https://github.com/ai4ed/SC-Ques}&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentence completion (SC) questions present a sentence with one or more blanks that need to be filled in, three to five possible words or phrases as options. SC questions are widely used for students learning English as a Second Language (ESL). In this paper, we present a large-scale SC dataset, \textsc{SC-Ques}, which is made up of 289,148 ESL SC questions from real-world standardized English examinations. Furthermore, we build a comprehensive benchmark of automatically solving the SC questions by training the large-scale pre-trained language models on the proposed \textsc{SC-Ques} dataset. We conduct detailed analysis of the baseline models performance, limitations and trade-offs. The data and our code are available for research purposes from: \url{https://github.com/ai4ed/SC-Ques}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#38454;&#32447;&#24615;&#36923;&#36753;&#19982;&#25193;&#23637;&#24352;&#37327;&#31867;&#22411;&#28436;&#31639;&#30340;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22266;&#26377;&#30340;&#28436;&#32462;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2206.08955</link><description>&lt;p&gt;
&#35753;&#19968;&#38454;&#32447;&#24615;&#36923;&#36753;&#25104;&#20026;&#29983;&#25104;&#35821;&#27861;
&lt;/p&gt;
&lt;p&gt;
Making first order linear logic a generating grammar. (arXiv:2206.08955v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.08955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#38454;&#32447;&#24615;&#36923;&#36753;&#19982;&#25193;&#23637;&#24352;&#37327;&#31867;&#22411;&#28436;&#31639;&#30340;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22266;&#26377;&#30340;&#28436;&#32462;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#19981;&#21516;&#30340;&#33539;&#30068;&#35821;&#27861;&#22312;&#19968;&#38454;&#20056;&#27861;&#32447;&#24615;&#36923;&#36753;&#30340;&#19968;&#20010;&#29255;&#27573;&#20013;&#20855;&#26377;&#34920;&#38754;&#34920;&#31034;&#12290; &#25105;&#20204;&#34920;&#26126;&#65292;&#35813;&#29255;&#27573;&#31561;&#20215;&#20110;&#26368;&#36817;&#24341;&#20837;&#30340;&#25193;&#23637;&#24352;&#37327;&#31867;&#22411;&#28436;&#31639;&#12290; &#36825;&#19981;&#20165;&#20026;&#21069;&#32773;&#25552;&#20379;&#20102;&#19968;&#20123;&#26367;&#20195;&#30340;&#35821;&#27861;&#21644;&#30452;&#35266;&#30340;&#20960;&#20309;&#34920;&#31034;&#65292;&#32780;&#19988;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#22266;&#26377;&#30340;&#28436;&#32462;&#31995;&#32479;&#65292;&#36825;&#26159;&#20197;&#21069;&#32570;&#23569;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is known that different categorial grammars have surface representation in a fragment of first order multiplicative linear logic. We show that the fragment of interest is equivalent to the recently introduced {\it extended tensor type calculus}. This provides the former not only with some alternative syntax and intuitive geometric representation, but also with an intrinsic deductive system, which has been absent.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#36328;&#25991;&#21270;&#30340;&#20215;&#20540;&#35266;&#24046;&#24322;&#65292;&#21457;&#29616;PTLMs&#25429;&#25417;&#21040;&#20102;&#25991;&#21270;&#20043;&#38388;&#30340;&#20215;&#20540;&#24046;&#24322;&#65292;&#20294;&#26159;&#36825;&#20123;&#20215;&#20540;&#24046;&#24322;&#21482;&#26377;&#24494;&#24369;&#30340;&#30456;&#20851;&#24615;&#12290;&#36328;&#25991;&#21270;&#24212;&#29992;&#20013;&#65292;&#23558;PTLMs&#19982;&#20215;&#20540;&#35266;&#23545;&#40784;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2203.13722</link><description>&lt;p&gt;
&#8220;&#25506;&#31350;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36328;&#25991;&#21270;&#20215;&#20540;&#24046;&#24322;&#8221;
&lt;/p&gt;
&lt;p&gt;
Probing Pre-Trained Language Models for Cross-Cultural Differences in Values. (arXiv:2203.13722v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.13722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#36328;&#25991;&#21270;&#30340;&#20215;&#20540;&#35266;&#24046;&#24322;&#65292;&#21457;&#29616;PTLMs&#25429;&#25417;&#21040;&#20102;&#25991;&#21270;&#20043;&#38388;&#30340;&#20215;&#20540;&#24046;&#24322;&#65292;&#20294;&#26159;&#36825;&#20123;&#20215;&#20540;&#24046;&#24322;&#21482;&#26377;&#24494;&#24369;&#30340;&#30456;&#20851;&#24615;&#12290;&#36328;&#25991;&#21270;&#24212;&#29992;&#20013;&#65292;&#23558;PTLMs&#19982;&#20215;&#20540;&#35266;&#23545;&#40784;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#25215;&#36733;&#20102;&#20154;&#20204;&#25152;&#25345;&#26377;&#30340;&#31038;&#20250;&#12289;&#25991;&#21270;&#12289;&#25919;&#27835;&#20215;&#20540;&#35266;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PTLMs&#65289;&#20013;&#32534;&#30721;&#30340;&#31038;&#20250;&#21644;&#28508;&#22312;&#26377;&#23475;&#30340;&#20559;&#35265;&#12290;&#28982;&#32780;&#65292;&#22312;&#36879;&#24443;&#30740;&#31350;&#36825;&#20123;&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#19981;&#21516;&#25991;&#21270;&#20215;&#20540;&#35266;&#26041;&#38754;&#65292;&#36824;&#27809;&#26377;&#31995;&#32479;&#30340;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25506;&#38024;&#26469;&#30740;&#31350;&#19981;&#21516;&#25991;&#21270;&#20013;&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#20215;&#20540;&#35266;&#65292;&#20197;&#21450;&#23427;&#20204;&#26159;&#21542;&#19982;&#29616;&#26377;&#30340;&#29702;&#35770;&#21644;&#36328;&#25991;&#21270;&#20215;&#20540;&#35843;&#26597;&#30456;&#31526;&#12290;&#25105;&#20204;&#21457;&#29616;PTLMs&#25429;&#25417;&#21040;&#20102;&#25991;&#21270;&#20043;&#38388;&#30340;&#20215;&#20540;&#24046;&#24322;&#65292;&#20294;&#26159;&#36825;&#20123;&#20215;&#20540;&#19982;&#24050;&#26377;&#30340;&#20215;&#20540;&#35843;&#26597;&#21482;&#26377;&#24494;&#24369;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;&#36328;&#25991;&#21270;&#29615;&#22659;&#19979;&#20351;&#29992;&#19981;&#31526;&#21512;&#20215;&#20540;&#35266;&#30340;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#23558;PTLMs&#19982;&#20215;&#20540;&#35266;&#23545;&#40784;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language embeds information about social, cultural, and political values people hold. Prior work has explored social and potentially harmful biases encoded in Pre-Trained Language models (PTLMs). However, there has been no systematic study investigating how values embedded in these models vary across cultures. In this paper, we introduce probes to study which values across cultures are embedded in these models, and whether they align with existing theories and cross-cultural value surveys. We find that PTLMs capture differences in values across cultures, but those only weakly align with established value surveys. We discuss implications of using mis-aligned models in cross-cultural settings, as well as ways of aligning PTLMs with value surveys.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#25104;&#21151;&#39044;&#27979;&#20102;&#30002;&#22411;&#27969;&#24863;&#30149;&#27602;&#30340;&#21407;&#22987;&#23487;&#20027;&#65292;&#20026;&#26089;&#26399;&#21644;&#24555;&#36895;&#25511;&#21046;&#30149;&#27602;&#20256;&#25773;&#25552;&#20379;&#20102;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2201.01140</link><description>&lt;p&gt;
&#21033;&#29992;PSSM&#21644;&#35789;&#23884;&#20837;&#39044;&#27979;&#30002;&#22411;&#27969;&#24863;&#30149;&#27602;&#23487;&#20027;
&lt;/p&gt;
&lt;p&gt;
Predicting Influenza A Viral Host Using PSSM and Word Embeddings. (arXiv:2201.01140v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.01140
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#25104;&#21151;&#39044;&#27979;&#20102;&#30002;&#22411;&#27969;&#24863;&#30149;&#27602;&#30340;&#21407;&#22987;&#23487;&#20027;&#65292;&#20026;&#26089;&#26399;&#21644;&#24555;&#36895;&#25511;&#21046;&#30149;&#27602;&#20256;&#25773;&#25552;&#20379;&#20102;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#24863;&#30149;&#27602;&#30340;&#24555;&#36895;&#31361;&#21464;&#23041;&#32961;&#20844;&#20849;&#20581;&#24247;&#65292;&#21487;&#33021;&#24341;&#21457;&#33268;&#21629;&#30340;&#22823;&#27969;&#34892;&#30149;&#12290;&#28982;&#32780;&#65292;&#26816;&#27979;&#30149;&#27602;&#30340;&#21407;&#22987;&#23487;&#20027;&#22312;&#29190;&#21457;&#26399;&#38388;&#25110;&#29190;&#21457;&#21518;&#26159;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#27969;&#24863;&#30149;&#27602;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#29289;&#31181;&#20043;&#38388;&#24490;&#29615;&#20256;&#25773;&#12290;&#22240;&#27492;&#65292;&#26089;&#26399;&#21644;&#24555;&#36895;&#26816;&#27979;&#30149;&#27602;&#23487;&#20027;&#23558;&#26377;&#21161;&#20110;&#20943;&#23569;&#30149;&#27602;&#30340;&#36827;&#19968;&#27493;&#20256;&#25773;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#20301;&#32622;&#29305;&#24322;&#24615;&#24471;&#20998;&#30697;&#38453;&#65288;PSSM&#65289;&#21644;&#23398;&#20064;&#33258;&#35789;&#23884;&#20837;&#21644;&#35789;&#32534;&#30721;&#30340;&#29305;&#24449;&#30340;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#25512;&#26029;&#30149;&#27602;&#30340;&#21407;&#22987;&#23487;&#20027;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;PSSM&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#36798;&#21040;&#20102;95%&#24038;&#21491;&#30340;MCC&#21644;96%&#24038;&#21491;&#30340;F1&#12290;&#20351;&#29992;&#35789;&#23884;&#20837;&#30340;&#27169;&#22411;&#24471;&#21040;&#30340;MCC&#32422;&#20026;96&#65285;&#65292;F1&#32422;&#20026;97&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid mutation of the influenza virus threatens public health. Reassortment among viruses with different hosts can lead to a fatal pandemic. However, it is difficult to detect the original host of the virus during or after an outbreak as influenza viruses can circulate between different species. Therefore, early and rapid detection of the viral host would help reduce the further spread of the virus. We use various machine learning models with features derived from the position-specific scoring matrix (PSSM) and features learned from word embedding and word encoding to infer the origin host of viruses. The results show that the performance of the PSSM-based model reaches the MCC around 95%, and the F1 around 96%. The MCC obtained using the model with word embedding is around 96%, and the F1 is around 97%.
&lt;/p&gt;</description></item></channel></rss>