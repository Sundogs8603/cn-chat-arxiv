<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;&#35828;&#35805;&#20154;&#26085;&#31243;&#21644;&#24773;&#24863;&#35782;&#21035;&#30456;&#32467;&#21512;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25552;&#39640;&#20102;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.12851</link><description>&lt;p&gt;
EmoDiarize: &#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20174;&#35821;&#38899;&#20449;&#21495;&#20013;&#36827;&#34892;&#35828;&#35805;&#20154;&#26085;&#31243;&#21644;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
EmoDiarize: Speaker Diarization and Emotion Identification from Speech Signals using Convolutional Neural Networks. (arXiv:2310.12851v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;&#35828;&#35805;&#20154;&#26085;&#31243;&#21644;&#24773;&#24863;&#35782;&#21035;&#30456;&#32467;&#21512;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25552;&#39640;&#20102;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#26426;&#20132;&#20114;&#26102;&#20195;&#65292;&#35782;&#21035;&#21475;&#22836;&#35821;&#35328;&#20013;&#30340;&#24773;&#24863;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#24212;&#23545;&#35828;&#35805;&#20154;&#26085;&#31243;&#21644;&#24773;&#24863;&#35782;&#21035;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#23427;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#29616;&#26377;&#30340;&#35828;&#35805;&#20154;&#26085;&#31243;&#27969;&#31243;&#21644;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476; (CNN) &#30340;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#31934;&#24230;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#26159;&#22312;&#20116;&#20010;&#35821;&#38899;&#24773;&#24863;&#25968;&#25454;&#38598; (RAVDESS&#65292;CREMA-D&#65292;SAVEE&#65292;TESS&#21644;&#30005;&#24433;&#29255;&#27573;) &#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#20854;&#20013;&#21518;&#32773;&#26159;&#19987;&#38376;&#20026;&#26412;&#30740;&#31350;&#21019;&#24314;&#30340;&#19968;&#20010;&#35821;&#38899;&#24773;&#24863;&#25968;&#25454;&#38598;&#12290;&#20174;&#27599;&#20010;&#26679;&#26412;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#21253;&#25324;Mel&#39057;&#29575;&#20498;&#35889;&#31995;&#25968; (MFCC)&#65292;&#36807;&#38646;&#29575; (ZCR)&#65292;&#22343;&#26041;&#26681; (RMS) &#21644;&#21508;&#31181;&#25968;&#25454;&#22686;&#24378;&#31639;&#27861;&#65292;&#22914;&#38899;&#39640;&#12289;&#22122;&#22768;&#12289;&#25289;&#20280;&#21644;&#31227;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of advanced artificial intelligence and human-computer interaction, identifying emotions in spoken language is paramount. This research explores the integration of deep learning techniques in speech emotion recognition, offering a comprehensive solution to the challenges associated with speaker diarization and emotion identification. It introduces a framework that combines a pre-existing speaker diarization pipeline and an emotion identification model built on a Convolutional Neural Network (CNN) to achieve higher precision. The proposed model was trained on data from five speech emotion datasets, namely, RAVDESS, CREMA-D, SAVEE, TESS, and Movie Clips, out of which the latter is a speech emotion dataset created specifically for this research. The features extracted from each sample include Mel Frequency Cepstral Coefficients (MFCC), Zero Crossing Rate (ZCR), Root Mean Square (RMS), and various data augmentation algorithms like pitch, noise, stretch, and shift. This feature e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20851;&#27880;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20154;&#24037;&#26631;&#27880;&#30340;&#21464;&#24322;&#38382;&#39064;&#65292;&#36890;&#36807;&#25910;&#38598;&#19968;&#32452;&#24459;&#24072;&#23545;&#26696;&#20214;&#32467;&#26524;&#35780;&#20272;&#23384;&#22312;&#20998;&#27495;&#30340;&#25968;&#25454;&#38598;&#65292;&#23545;&#36825;&#20123;&#20998;&#27495;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#20004;&#32423;&#20998;&#31867;&#20307;&#31995;&#65292;&#24182;&#21457;&#29616;&#20998;&#27495;&#20027;&#35201;&#28304;&#20110;&#23545;&#27861;&#24459;&#32972;&#26223;&#30340;&#19981;&#26126;&#30830;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2310.11878</link><description>&lt;p&gt;
&#20174;&#19981;&#19968;&#33268;&#21040;&#27934;&#23519;&#65306;&#23545;&#26696;&#20363;&#32467;&#26524;&#20998;&#31867;&#30340;&#29702;&#30001;&#25968;&#25454;&#38598;&#26500;&#24314;&#36827;&#34892;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
From Dissonance to Insights: Dissecting Disagreements in Rationale Dataset Construction for Case Outcome Classification. (arXiv:2310.11878v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11878
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20154;&#24037;&#26631;&#27880;&#30340;&#21464;&#24322;&#38382;&#39064;&#65292;&#36890;&#36807;&#25910;&#38598;&#19968;&#32452;&#24459;&#24072;&#23545;&#26696;&#20214;&#32467;&#26524;&#35780;&#20272;&#23384;&#22312;&#20998;&#27495;&#30340;&#25968;&#25454;&#38598;&#65292;&#23545;&#36825;&#20123;&#20998;&#27495;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#20004;&#32423;&#20998;&#31867;&#20307;&#31995;&#65292;&#24182;&#21457;&#29616;&#20998;&#27495;&#20027;&#35201;&#28304;&#20110;&#23545;&#27861;&#24459;&#32972;&#26223;&#30340;&#19981;&#26126;&#30830;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#26696;&#20363;&#32467;&#26524;&#20998;&#31867;&#65288;COC&#65289;&#19981;&#20165;&#38656;&#35201;&#20934;&#30830;&#24615;&#65292;&#36824;&#38656;&#35201;&#21487;&#20449;&#36182;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#29616;&#26377;&#30340;&#21487;&#35299;&#37322;COC&#30740;&#31350;&#20165;&#38480;&#20110;&#30001;&#21333;&#20010;&#19987;&#23478;&#36827;&#34892;&#30340;&#27880;&#37322;&#12290;&#28982;&#32780;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;&#24459;&#24072;&#22312;&#23545;&#26696;&#20214;&#20107;&#23454;&#36827;&#34892;&#35780;&#20272;&#26102;&#21487;&#33021;&#23384;&#22312;&#20998;&#27495;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;RAVE&#65306;&#27431;&#27954;&#20154;&#26435;&#27861;&#39046;&#22495;&#30340;&#29702;&#30001;&#21464;&#24322;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#20174;&#22269;&#38469;&#20154;&#26435;&#27861;&#39046;&#22495;&#30340;&#20004;&#20301;&#19987;&#23478;&#37027;&#37324;&#33719;&#24471;&#30340;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20182;&#20204;&#20043;&#38388;&#23384;&#22312;&#24369;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20182;&#20204;&#30340;&#20998;&#27495;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#20004;&#32423;&#20219;&#21153;&#26080;&#20851;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#21516;&#26102;&#34917;&#20805;&#20102;COC&#29305;&#23450;&#30340;&#23376;&#31867;&#21035;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#39318;&#27425;&#20851;&#27880;&#20154;&#24037;&#26631;&#27880;&#30340;&#21464;&#24322;&#12290;&#25105;&#20204;&#23450;&#37327;&#35780;&#20272;&#20102;&#19981;&#21516;&#20998;&#31867;&#31867;&#21035;&#65292;&#24182;&#21457;&#29616;&#20998;&#27495;&#20027;&#35201;&#28304;&#20110;&#23545;&#27861;&#24459;&#32972;&#26223;&#30340;&#19981;&#26126;&#30830;&#25551;&#36848;&#65292;&#36825;&#22312;COC&#20803;&#25968;&#25454;&#36890;&#24120;&#20855;&#26377;&#26377;&#38480;&#32454;&#31890;&#24230;&#21644;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35780;&#20272;&#20102;SOTA COC&#27169;&#22411;&#22312;RAVE&#25968;&#25454;&#38598;&#19978;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#35266;&#23519;&#21040;...
&lt;/p&gt;
&lt;p&gt;
In legal NLP, Case Outcome Classification (COC) must not only be accurate but also trustworthy and explainable. Existing work in explainable COC has been limited to annotations by a single expert. However, it is well-known that lawyers may disagree in their assessment of case facts. We hence collect a novel dataset RAVE: Rationale Variation in ECHR1, which is obtained from two experts in the domain of international human rights law, for whom we observe weak agreement. We study their disagreements and build a two-level task-independent taxonomy, supplemented with COC-specific subcategories. To our knowledge, this is the first work in the legal NLP that focuses on human label variation. We quantitatively assess different taxonomy categories and find that disagreements mainly stem from underspecification of the legal context, which poses challenges given the typically limited granularity and noise in COC metadata. We further assess the explainablility of SOTA COC models on RAVE and observ
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#21407;&#22411;&#30340;&#36229;&#36866;&#37197;&#22120;&#65288;PHA&#65289;&#26694;&#26550;&#29992;&#20110;&#26679;&#26412;&#39640;&#25928;&#22810;&#20219;&#21153;&#35843;&#25972;&#65292;&#36890;&#36807;&#24341;&#20837;&#23454;&#20363;&#23494;&#38598;&#30340;&#26816;&#32034;&#22120;&#21644;&#26679;&#26412;&#39640;&#25928;&#30340;&#21407;&#22411;&#36229;&#32593;&#32476;&#29983;&#25104;&#26465;&#20214;&#27169;&#22359;&#65292;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#36801;&#31227;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#21487;&#27604;&#24615;&#33021;&#30340;&#25552;&#21319;&#65292;&#29978;&#33267;&#22312;&#25968;&#25454;&#37327;&#36739;&#23567;&#26102;&#20063;&#33021;&#36229;&#36807;&#20854;&#20182;&#24378;&#22522;&#32447;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.11670</link><description>&lt;p&gt;
&#22522;&#20110;&#21407;&#22411;&#30340;&#36229;&#36866;&#37197;&#22120;&#29992;&#20110;&#26679;&#26412;&#39640;&#25928;&#22810;&#20219;&#21153;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Prototype-based HyperAdapter for Sample-Efficient Multi-task Tuning. (arXiv:2310.11670v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11670
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21407;&#22411;&#30340;&#36229;&#36866;&#37197;&#22120;&#65288;PHA&#65289;&#26694;&#26550;&#29992;&#20110;&#26679;&#26412;&#39640;&#25928;&#22810;&#20219;&#21153;&#35843;&#25972;&#65292;&#36890;&#36807;&#24341;&#20837;&#23454;&#20363;&#23494;&#38598;&#30340;&#26816;&#32034;&#22120;&#21644;&#26679;&#26412;&#39640;&#25928;&#30340;&#21407;&#22411;&#36229;&#32593;&#32476;&#29983;&#25104;&#26465;&#20214;&#27169;&#22359;&#65292;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#36801;&#31227;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#21487;&#27604;&#24615;&#33021;&#30340;&#25552;&#21319;&#65292;&#29978;&#33267;&#22312;&#25968;&#25454;&#37327;&#36739;&#23567;&#26102;&#20063;&#33021;&#36229;&#36807;&#20854;&#20182;&#24378;&#22522;&#32447;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#24050;&#32463;&#35777;&#26126;&#22312;&#36866;&#24212;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21040;&#19979;&#28216;&#20219;&#21153;&#26102;&#26377;&#25928;&#65292;&#21516;&#26102;&#21482;&#26356;&#26032;&#20102;&#23569;&#37327;&#21442;&#25968;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#29420;&#31435;&#22320;&#36866;&#24212;&#27599;&#20010;&#20219;&#21153;&#65292;&#27809;&#26377;&#32771;&#34385;&#20219;&#21153;&#20043;&#38388;&#30340;&#30693;&#35782;&#20256;&#36755;&#65292;&#24182;&#19988;&#21463;&#38480;&#20110;&#20302;&#25968;&#25454;&#24773;&#26223;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#30340;&#36229;&#36866;&#37197;&#22120;&#65288;PHA&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#24314;&#31435;&#22312;&#36866;&#37197;&#22120;&#35843;&#25972;&#21644;&#36229;&#32593;&#32476;&#22522;&#30784;&#19978;&#12290;&#23427;&#24341;&#20837;&#20102;&#19968;&#20010;&#23454;&#20363;&#23494;&#38598;&#30340;&#26816;&#32034;&#22120;&#21644;&#19968;&#20010;&#26679;&#26412;&#39640;&#25928;&#30340;&#21407;&#22411;&#36229;&#32593;&#32476;&#26469;&#29983;&#25104;&#26465;&#20214;&#27169;&#22359;&#12290;&#36825;&#23548;&#33268;&#19982;&#29616;&#26377;PEFT&#26041;&#27861;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#36801;&#31227;&#23398;&#20064;&#19978;&#30456;&#24403;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#24403;&#21487;&#29992;&#25968;&#25454;&#37327;&#21464;&#23567;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#20854;&#20182;&#24378;&#22522;&#32447;&#26041;&#27861;&#26377;&#24456;&#22823;&#30340;&#20248;&#21183;&#12290;&#22522;&#20110;&#25105;&#20204;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#35777;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;PHA&#22312;&#26435;&#34913;&#26041;&#38754;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter-efficient fine-tuning (PEFT) has shown its effectiveness in adapting the pre-trained language models to downstream tasks while only updating a small number of parameters. Despite the success, most existing methods independently adapt to each task without considering knowledge transfer between tasks and are limited to low-data regimes. To overcome this issue, we propose Prototype-based HyperAdapter (PHA), a novel framework built on the adapter-tuning and hypernetwork. It introduces an instance-dense retriever and a prototypical hypernetwork to generate the conditional modules in a sample-efficient manner. This leads to comparable performance improvements against existing PEFT methods on multi-task learning and few-shot transfer learning. More importantly, when the available data size gets smaller, our method outperforms other strong baselines by a large margin. Based on our extensive empirical experiments across various datasets, we demonstrate that PHA strikes a better trade-
&lt;/p&gt;</description></item><item><title>IDMO&#39033;&#30446;&#26088;&#22312;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#25171;&#20987;&#34394;&#20551;&#20449;&#24687;&#21644;&#20551;&#26032;&#38395;&#65292;&#20854;&#36129;&#29486;&#21253;&#25324;&#21019;&#24314;&#26032;&#22411;&#25968;&#25454;&#38598;&#12289;&#24320;&#21457;&#33258;&#21160;&#27169;&#22411;&#12289;&#35780;&#20272;GPT-4&#31561;&#12290;</title><link>http://arxiv.org/abs/2310.11097</link><description>&lt;p&gt;
&#29992;&#20110;&#25171;&#20987;&#34394;&#20551;&#20449;&#24687;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#23454;&#39564;&#65306;IDMO&#39033;&#30446;
&lt;/p&gt;
&lt;p&gt;
Experimenting AI Technologies for Disinformation Combat: the IDMO Project. (arXiv:2310.11097v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11097
&lt;/p&gt;
&lt;p&gt;
IDMO&#39033;&#30446;&#26088;&#22312;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#25171;&#20987;&#34394;&#20551;&#20449;&#24687;&#21644;&#20551;&#26032;&#38395;&#65292;&#20854;&#36129;&#29486;&#21253;&#25324;&#21019;&#24314;&#26032;&#22411;&#25968;&#25454;&#38598;&#12289;&#24320;&#21457;&#33258;&#21160;&#27169;&#22411;&#12289;&#35780;&#20272;GPT-4&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24847;&#22823;&#21033;&#25968;&#23383;&#23186;&#20307;&#35266;&#23519;&#39033;&#30446;&#65288;IDMO&#65289;&#26159;&#27431;&#27954;&#19968;&#39033;&#20513;&#35758;&#30340;&#19968;&#37096;&#20998;&#65292;&#19987;&#27880;&#20110;&#25171;&#20987;&#34394;&#20551;&#20449;&#24687;&#21644;&#20551;&#26032;&#38395;&#12290;&#26412;&#25253;&#21578;&#27010;&#36848;&#20102;Rai-CRITS&#22312;&#35813;&#39033;&#30446;&#20013;&#30340;&#36129;&#29486;&#65292;&#21253;&#25324;&#65306;&#65288;i&#65289;&#21019;&#24314;&#29992;&#20110;&#27979;&#35797;&#25216;&#26415;&#30340;&#26032;&#22411;&#25968;&#25454;&#38598;&#65292;&#65288;ii&#65289;&#24320;&#21457;&#33258;&#21160;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#31867;Pagella Politica&#30340;&#35009;&#20915;&#20197;&#20415;&#20110;&#26356;&#24191;&#27867;&#30340;&#20998;&#26512;&#65292;&#65288;iii&#65289;&#21019;&#24314;&#33258;&#21160;&#27169;&#22411;&#65292;&#23545;FEVER&#25968;&#25454;&#38598;&#19978;&#30340;&#25991;&#26412;&#34164;&#21547;&#20855;&#26377;&#24322;&#24120;&#31934;&#24230;&#30340;&#35782;&#21035;&#33021;&#21147;&#65292;&#65288;iv&#65289;&#20351;&#29992;GPT-4&#35780;&#20272;&#25991;&#26412;&#34164;&#21547;&#65292; &#65288;v&#65289;&#22312;&#22269;&#23478;&#27963;&#21160;&#20013;&#24320;&#23637;&#25552;&#39640;&#23545;&#20551;&#26032;&#38395;&#24847;&#35782;&#30340;&#28216;&#25103;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Italian Digital Media Observatory (IDMO) project, part of a European initiative, focuses on countering disinformation and fake news. This report outlines contributions from Rai-CRITS to the project, including: (i) the creation of novel datasets for testing technologies (ii) development of an automatic model for categorizing Pagella Politica verdicts to facilitate broader analysis (iii) creation of an automatic model for recognizing textual entailment with exceptional accuracy on the FEVER dataset (iv) assessment using GPT-4 to identify textual entailmen (v) a game to raise awareness about fake news at national events.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;BiomedJourney&#65292;&#36890;&#36807;&#25351;&#23548;&#23398;&#20064;&#22810;&#27169;&#24577;&#24739;&#32773;&#26053;&#31243;&#65292;&#36827;&#34892;&#21453;&#20107;&#23454;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#12290;&#20351;&#29992;GPT-4&#22788;&#29702;&#22270;&#20687;&#25253;&#21578;&#29983;&#25104;&#30142;&#30149;&#36827;&#23637;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#24182;&#35757;&#32451;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.10765</link><description>&lt;p&gt;
BiomedJourney: &#25351;&#23548;&#23398;&#20064;&#22810;&#27169;&#24577;&#24739;&#32773;&#26053;&#31243;&#20013;&#30340;&#21453;&#20107;&#23454;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
BiomedJourney: Counterfactual Biomedical Image Generation by Instruction-Learning from Multimodal Patient Journeys. (arXiv:2310.10765v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10765
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;BiomedJourney&#65292;&#36890;&#36807;&#25351;&#23548;&#23398;&#20064;&#22810;&#27169;&#24577;&#24739;&#32773;&#26053;&#31243;&#65292;&#36827;&#34892;&#21453;&#20107;&#23454;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#12290;&#20351;&#29992;GPT-4&#22788;&#29702;&#22270;&#20687;&#25253;&#21578;&#29983;&#25104;&#30142;&#30149;&#36827;&#23637;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#24182;&#35757;&#32451;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#22270;&#20687;&#32534;&#36753;&#30340;&#25351;&#23548;&#23398;&#20064;&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#65292;&#22914;InstructPix2Pix&#65292;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#21487;&#20197;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;&#21453;&#20107;&#23454;&#22270;&#20687;&#29983;&#25104;&#65292;&#20174;&#32780;&#24110;&#21161;&#21306;&#20998;&#22240;&#26524;&#32467;&#26500;&#21644;&#20266;&#30456;&#20851;&#65292;&#24182;&#20419;&#36827;&#30142;&#30149;&#36827;&#23637;&#24314;&#27169;&#30340;&#31283;&#20581;&#22270;&#20687;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#36890;&#29992;&#30340;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#24182;&#19981;&#36866;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#65292;&#21453;&#20107;&#23454;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#30340;&#30740;&#31350;&#36824;&#36828;&#26410;&#28145;&#20837;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;BiomedJourney&#65292;&#36890;&#36807;&#25351;&#23548;&#23398;&#20064;&#22810;&#27169;&#24577;&#24739;&#32773;&#26053;&#31243;&#65292;&#36827;&#34892;&#21453;&#20107;&#23454;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#12290;&#32473;&#23450;&#19968;&#20010;&#25293;&#25668;&#20110;&#19981;&#21516;&#26102;&#38388;&#28857;&#30340;&#20004;&#20010;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#30340;&#24739;&#32773;&#65292;&#25105;&#20204;&#20351;&#29992;GPT-4&#22788;&#29702;&#30456;&#24212;&#30340;&#22270;&#20687;&#25253;&#21578;&#65292;&#24182;&#29983;&#25104;&#30142;&#30149;&#36827;&#23637;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#29983;&#25104;&#30340;&#19977;&#20803;&#32452;&#65288;&#20808;&#21069;&#22270;&#20687;&#12289;&#36827;&#23637;&#25551;&#36848;&#12289;&#26032;&#22270;&#20687;&#65289;&#26469;&#35757;&#32451;&#19968;&#20010;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapid progress has been made in instruction-learning for image editing with natural-language instruction, as exemplified by InstructPix2Pix. In biomedicine, such methods can be applied to counterfactual image generation, which helps differentiate causal structure from spurious correlation and facilitate robust image interpretation for disease progression modeling. However, generic image-editing models are ill-suited for the biomedical domain, and counterfactual biomedical image generation is largely underexplored. In this paper, we present BiomedJourney, a novel method for counterfactual biomedical image generation by instruction-learning from multimodal patient journeys. Given a patient with two biomedical images taken at different time points, we use GPT-4 to process the corresponding imaging reports and generate a natural language description of disease progression. The resulting triples (prior image, progression description, new image) are then used to train a latent diffusion mode
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#20998;&#24067;&#24335;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#22788;&#29702;&#25105;&#20204;&#26032;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#26102;&#37117;&#23384;&#22312;&#22256;&#38590;&#65292;&#23613;&#31649;&#23427;&#20204;&#22312;&#20854;&#20182;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#36825;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#30340;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#20173;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2310.09430</link><description>&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#20998;&#24067;&#24335;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#31995;&#32479;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Systematic Evaluation of Large Language Models on Out-of-Distribution Logical Reasoning Tasks. (arXiv:2310.09430v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09430
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#20998;&#24067;&#24335;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#22788;&#29702;&#25105;&#20204;&#26032;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#26102;&#37117;&#23384;&#22312;&#22256;&#38590;&#65292;&#23613;&#31649;&#23427;&#20204;&#22312;&#20854;&#20182;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#36825;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#30340;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#20173;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT-3.5&#21644;GPT-4&#65292;&#24050;&#32463;&#23558;&#20154;&#24037;&#31995;&#32479;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#21040;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#30340;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#35780;&#20272;&#12290;&#20026;&#20102;&#25506;&#32034;&#36825;&#31181;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#26032;&#30340;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#20998;&#21035;&#21517;&#20026;"ReClor-plus"&#12289;"LogiQA-plus"&#21644;"LogiQAv2-plus"&#65292;&#27599;&#20010;&#25968;&#25454;&#38598;&#37117;&#21253;&#21547;&#19977;&#20010;&#23376;&#38598;&#65306;&#31532;&#19968;&#20010;&#26159;&#36873;&#39033;&#38543;&#26426;&#25171;&#20081;&#65292;&#31532;&#20108;&#20010;&#26159;&#23558;&#27491;&#30830;&#36873;&#39033;&#26367;&#25442;&#20026;"&#27809;&#26377;&#20854;&#20182;&#36873;&#39033;&#26159;&#27491;&#30830;&#30340;"&#65292;&#31532;&#19977;&#20010;&#26159;&#21069;&#20004;&#20010;&#23376;&#38598;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20351;&#29992;&#20102;&#37492;&#21035;&#21644;&#29983;&#25104;&#22411;&#30340;LLMs&#65292;&#24182;&#34920;&#26126;&#36825;&#20123;&#31616;&#21333;&#30340;&#25216;&#24039;&#26497;&#22823;&#22320;&#38459;&#30861;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#22312;&#21407;&#22987;&#30340;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#25152;&#26377;&#27169;&#22411;&#37117;&#24456;&#38590;&#22238;&#31572;&#25105;&#20204;&#26032;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#25200;&#21160;&#24341;&#20837;&#20219;&#21153;&#21464;&#21270;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), such as GPT-3.5 and GPT-4, have greatly advanced the performance of artificial systems on various natural language processing tasks to human-like levels. However, their generalisation and robustness to perform logical reasoning remain under-evaluated. To probe this ability, we propose three new logical reasoning datasets named "ReClor-plus", "LogiQA-plus" and "LogiQAv2-plus", each featuring three subsets: the first with randomly shuffled options, the second with the correct choices replaced by "none of the other options are correct", and a combination of the previous two subsets. We carry out experiments on these datasets with both discriminative and generative LLMs and show that these simple tricks greatly hinder the performance of the language models. Despite their superior performance on the original publicly available datasets, we find that all models struggle to answer our newly constructed datasets. We show that introducing task variations by perturb
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;LLM&#29983;&#25104;&#32467;&#26524;&#36827;&#34892;&#37325;&#26032;&#25490;&#21517;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27491;&#30830;&#19981;&#21464;&#37327;&#30340;&#25490;&#21517;&#65292;&#20174;&#32780;&#20943;&#23569;&#31243;&#24207;&#39564;&#35777;&#30340;&#35843;&#29992;&#27425;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.09342</link><description>&lt;p&gt;
&#20026;&#31243;&#24207;&#39564;&#35777;&#23545;LLM&#29983;&#25104;&#30340;&#24490;&#29615;&#19981;&#21464;&#24335;&#36827;&#34892;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
Ranking LLM-Generated Loop Invariants for Program Verification. (arXiv:2310.09342v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;LLM&#29983;&#25104;&#32467;&#26524;&#36827;&#34892;&#37325;&#26032;&#25490;&#21517;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27491;&#30830;&#19981;&#21464;&#37327;&#30340;&#25490;&#21517;&#65292;&#20174;&#32780;&#20943;&#23569;&#31243;&#24207;&#39564;&#35777;&#30340;&#35843;&#29992;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#24402;&#32435;&#24490;&#29615;&#19981;&#21464;&#37327;&#26159;&#33258;&#21160;&#21270;&#31243;&#24207;&#39564;&#35777;&#30340;&#22522;&#30784;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;gpt-3.5&#25110;gpt-4&#65289;&#33021;&#22815;&#22312;0-shot&#29615;&#22659;&#19979;&#20026;&#19968;&#31867;&#31243;&#24207;&#21512;&#25104;&#24490;&#29615;&#19981;&#21464;&#37327;&#65292;&#20294;&#38656;&#35201;&#22810;&#20010;&#26679;&#26412;&#25165;&#33021;&#29983;&#25104;&#27491;&#30830;&#30340;&#19981;&#21464;&#37327;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#22823;&#37327;&#35843;&#29992;&#31243;&#24207;&#39564;&#35777;&#22120;&#26469;&#24314;&#31435;&#19981;&#21464;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;LLM&#29983;&#25104;&#32467;&#26524;&#36827;&#34892;&#37325;&#26032;&#25490;&#21517;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#25490;&#21517;&#22120;&#65292;&#21487;&#20197;&#26681;&#25454;&#38382;&#39064;&#23450;&#20041;&#21306;&#20998;&#27491;&#30830;&#30340;&#24402;&#32435;&#19981;&#21464;&#37327;&#21644;&#38169;&#35823;&#30340;&#23581;&#35797;&#12290;&#35813;&#25490;&#21517;&#22120;&#32463;&#36807;&#23545;&#27604;&#25490;&#21517;&#20248;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#37325;&#26032;&#25490;&#21517;&#26426;&#21046;&#26174;&#33879;&#25552;&#39640;&#20102;&#27491;&#30830;&#19981;&#21464;&#37327;&#22312;&#29983;&#25104;&#30340;&#20505;&#36873;&#39033;&#20013;&#30340;&#25490;&#21517;&#65292;&#20174;&#32780;&#22823;&#24133;&#20943;&#23569;&#20102;&#23545;&#39564;&#35777;&#22120;&#30340;&#35843;&#29992;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthesizing inductive loop invariants is fundamental to automating program verification. In this work, we observe that Large Language Models (such as gpt-3.5 or gpt-4) are capable of synthesizing loop invariants for a class of programs in a 0-shot setting, yet require several samples to generate the correct invariants. This can lead to a large number of calls to a program verifier to establish an invariant. To address this issue, we propose a {\it re-ranking} approach for the generated results of LLMs. We have designed a ranker that can distinguish between correct inductive invariants and incorrect attempts based on the problem definition. The ranker is optimized as a contrastive ranker. Experimental results demonstrate that this re-ranking mechanism significantly improves the ranking of correct invariants among the generated candidates, leading to a notable reduction in the number of calls to a verifier.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#37319;&#29992;&#25506;&#32034;&#25351;&#23548;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#36827;&#34892;&#20027;&#21160;&#25506;&#32034;&#65292;&#22686;&#24378;&#20102;&#39046;&#22495;&#29305;&#23450;&#25351;&#23548;&#35843;&#20248;&#30340;&#25968;&#25454;&#35206;&#30422;&#33539;&#22260;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.09168</link><description>&lt;p&gt;
&#25506;&#32034;&#25351;&#23548;&#65306;&#36890;&#36807;&#20027;&#21160;&#25506;&#32034;&#22686;&#24378;&#29305;&#23450;&#39046;&#22495;&#25351;&#23548;&#35206;&#30422;&#29575;
&lt;/p&gt;
&lt;p&gt;
Explore-Instruct: Enhancing Domain-Specific Instruction Coverage through Active Exploration. (arXiv:2310.09168v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09168
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37319;&#29992;&#25506;&#32034;&#25351;&#23548;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#36827;&#34892;&#20027;&#21160;&#25506;&#32034;&#65292;&#22686;&#24378;&#20102;&#39046;&#22495;&#29305;&#23450;&#25351;&#23548;&#35843;&#20248;&#30340;&#25968;&#25454;&#35206;&#30422;&#33539;&#22260;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22686;&#24378;&#22810;&#26679;&#24615;&#65292;&#21487;&#20197;&#22823;&#24133;&#20248;&#21270;&#25351;&#23548;&#35843;&#20248;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#26356;&#24191;&#27867;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#27492;&#31867;&#35843;&#20248;&#30340;&#29616;&#26377;&#25968;&#25454;&#24448;&#24448;&#23545;&#20010;&#21035;&#39046;&#22495;&#30340;&#35206;&#30422;&#19981;&#36275;&#65292;&#38480;&#21046;&#20102;&#23545;&#36825;&#20123;&#39046;&#22495;&#20869;&#32454;&#33268;&#29702;&#35299;&#21644;&#20132;&#20114;&#30340;&#33539;&#22260;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#25506;&#32034;&#25351;&#23548;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#30340;&#20027;&#21160;&#25506;&#32034;&#26469;&#22686;&#24378;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#25351;&#23548;&#35843;&#20248;&#30340;&#25968;&#25454;&#35206;&#30422;&#12290;&#25506;&#32034;&#25351;&#23548;&#22522;&#20110;&#20856;&#22411;&#30340;&#39046;&#22495;&#20351;&#29992;&#26696;&#20363;&#65292;&#36890;&#36807;&#23454;&#29616;&#25628;&#32034;&#31639;&#27861;&#26469;&#33719;&#21462;&#22810;&#26679;&#21270;&#21644;&#38754;&#21521;&#39046;&#22495;&#30340;&#25351;&#23548;&#35843;&#20248;&#25968;&#25454;&#30340;&#22810;&#31181;&#21464;&#20307;&#25110;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#20013;&#24515;&#20998;&#26512;&#39564;&#35777;&#20102;&#27492;&#26041;&#27861;&#22312;&#25913;&#36827;&#29305;&#23450;&#39046;&#22495;&#25351;&#23548;&#35206;&#30422;&#33539;&#22260;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#27169;&#22411;&#30340;&#24615;&#33021;&#26174;&#31034;&#20986;&#19982;&#22810;&#20010;&#22522;&#32447;&#27169;&#22411;&#30456;&#27604;&#30340;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction-tuning can be substantially optimized through enhanced diversity, resulting in models capable of handling a broader spectrum of tasks. However, existing data employed for such tuning often exhibit an inadequate coverage of individual domains, limiting the scope for nuanced comprehension and interactions within these areas. To address this deficiency, we propose Explore-Instruct, a novel approach to enhance the data coverage to be used in domain-specific instruction-tuning through active exploration via Large Language Models (LLMs). Built upon representative domain use cases, Explore-Instruct explores a multitude of variations or possibilities by implementing a search algorithm to obtain diversified and domain-focused instruction-tuning data. Our data-centric analysis validates the effectiveness of this proposed approach in improving domain-specific instruction coverage. Moreover, our model's performance demonstrates considerable advancements over multiple baselines, includi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#19978;&#20851;&#20110;&#27668;&#20505;&#21464;&#21270;&#30340;&#25512;&#25991;&#24773;&#24863;&#24577;&#24230;&#65292;&#36890;&#36807;&#20351;&#29992;ClimateBERT&#27169;&#22411;&#37327;&#21270;&#24773;&#24863;&#65292;&#20174;&#32780;&#33719;&#24471;&#26377;&#20851;&#20844;&#20247;&#23545;&#27668;&#20505;&#21464;&#21270;&#30340;&#35266;&#28857;&#21644;&#21453;&#39304;&#12290;</title><link>http://arxiv.org/abs/2310.08099</link><description>&lt;p&gt;
ClimateNLP: &#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20998;&#26512;&#20844;&#20247;&#23545;&#27668;&#20505;&#21464;&#21270;&#30340;&#24773;&#24863;&#24577;&#24230;
&lt;/p&gt;
&lt;p&gt;
ClimateNLP: Analyzing Public Sentiment Towards Climate Change Using Natural Language Processing. (arXiv:2310.08099v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#19978;&#20851;&#20110;&#27668;&#20505;&#21464;&#21270;&#30340;&#25512;&#25991;&#24773;&#24863;&#24577;&#24230;&#65292;&#36890;&#36807;&#20351;&#29992;ClimateBERT&#27169;&#22411;&#37327;&#21270;&#24773;&#24863;&#65292;&#20174;&#32780;&#33719;&#24471;&#26377;&#20851;&#20844;&#20247;&#23545;&#27668;&#20505;&#21464;&#21270;&#30340;&#35266;&#28857;&#21644;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27668;&#20505;&#21464;&#21270;&#23545;&#20154;&#31867;&#20581;&#24247;&#30340;&#24433;&#21709;&#24102;&#26469;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#25361;&#25112;&#12290;&#38500;&#38750;&#37319;&#21462;&#22522;&#20110;&#30830;&#20991;&#35777;&#25454;&#30340;&#31215;&#26497;&#25514;&#26045;&#65292;&#21542;&#21017;&#36825;&#20123;&#23041;&#32961;&#24456;&#21487;&#33021;&#20250;&#21319;&#32423;&#65292;&#24182;&#32487;&#32493;&#23041;&#32961;&#20154;&#31867;&#31119;&#31049;&#12290;&#20449;&#24687;&#21644;&#36890;&#20449;&#25216;&#26415;&#30340;&#19981;&#26029;&#21457;&#23637;&#24050;&#32463;&#20419;&#36827;&#20102;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#30340;&#24191;&#27867;&#21487;&#29992;&#24615;&#21644;&#21033;&#29992;&#29575;&#12290;&#20010;&#20154;&#21033;&#29992;Twitter&#21644;Facebook&#31561;&#24179;&#21488;&#34920;&#36798;&#33258;&#24049;&#23545;&#21508;&#31181;&#20027;&#39064;&#30340;&#24847;&#35265;&#12289;&#24819;&#27861;&#21644;&#35780;&#35770;&#65292;&#21253;&#25324;&#32039;&#36843;&#30340;&#27668;&#20505;&#21464;&#21270;&#38382;&#39064;&#12290;&#31038;&#20132;&#23186;&#20307;&#19978;&#19982;&#27668;&#20505;&#21464;&#21270;&#30456;&#20851;&#20869;&#23481;&#30340;&#28608;&#22686;&#38656;&#35201;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#20197;&#33719;&#24471;&#26377;&#24847;&#20041;&#30340;&#27934;&#23519;&#12290;&#26412;&#35770;&#25991;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#20998;&#26512;&#27668;&#20505;&#21464;&#21270;&#35805;&#35821;&#65292;&#24182;&#37327;&#21270;&#19982;&#27668;&#20505;&#21464;&#21270;&#30456;&#20851;&#30340;&#25512;&#25991;&#30340;&#24773;&#24863;&#12290;&#25105;&#20204;&#20351;&#29992;ClimateBERT&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#27668;&#20505;&#21464;&#21270;&#39046;&#22495;&#36827;&#34892;&#20102;&#24494;&#35843;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#30446;&#26631;&#26159;&#35782;&#21035;&#24773;&#24863;&#24577;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Climate change's impact on human health poses unprecedented and diverse challenges. Unless proactive measures based on solid evidence are implemented, these threats will likely escalate and continue to endanger human well-being. The escalating advancements in information and communication technologies have facilitated the widespread availability and utilization of social media platforms. Individuals utilize platforms such as Twitter and Facebook to express their opinions, thoughts, and critiques on diverse subjects, encompassing the pressing issue of climate change. The proliferation of climate change-related content on social media necessitates comprehensive analysis to glean meaningful insights. This paper employs natural language processing (NLP) techniques to analyze climate change discourse and quantify the sentiment of climate change-related tweets. We use ClimateBERT, a pretrained model fine-tuned specifically for the climate change domain. The objective is to discern the sentim
&lt;/p&gt;</description></item><item><title>KwaiYiiMath&#26159;&#19968;&#20010;&#29992;&#20110;&#22686;&#24378;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24212;&#29992;&#30417;&#30563;&#24494;&#35843;&#21644;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#33521;&#35821;&#21644;&#20013;&#25991;&#25968;&#23398;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#22815;&#27491;&#30830;&#35299;&#20915;&#29983;&#25104;&#30340;&#38382;&#39064;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2310.07488</link><description>&lt;p&gt;
KwaiYiiMath: &#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
KwaiYiiMath: Technical Report. (arXiv:2310.07488v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07488
&lt;/p&gt;
&lt;p&gt;
KwaiYiiMath&#26159;&#19968;&#20010;&#29992;&#20110;&#22686;&#24378;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24212;&#29992;&#30417;&#30563;&#24494;&#35843;&#21644;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#33521;&#35821;&#21644;&#20013;&#25991;&#25968;&#23398;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#22815;&#27491;&#30830;&#35299;&#20915;&#29983;&#25104;&#30340;&#38382;&#39064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22788;&#29702;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#19979;&#28216;&#20219;&#21153;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#65292;&#29978;&#33267;&#21487;&#20197;&#22788;&#29702;&#38656;&#35201;&#22810;&#27493;&#25512;&#29702;&#30340;&#25968;&#23398;&#20219;&#21153;&#12290;&#22312;&#26412;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;KwaiYiiMath&#65292;&#36890;&#36807;&#24212;&#29992;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#21644;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#22686;&#24378;&#20102;KwaiYiiBase1&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#21253;&#25324;&#33521;&#35821;&#21644;&#20013;&#25991;&#30340;&#25968;&#23398;&#20219;&#21153;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#26500;&#24314;&#20102;&#19968;&#20010;&#23567;&#35268;&#27169;&#30340;&#20013;&#23567;&#23398;&#25968;&#23398;&#27979;&#35797;&#38598;&#65288;&#21629;&#21517;&#20026;KMath&#65289;&#65292;&#21253;&#21547;188&#20010;&#20363;&#23376;&#65292;&#29992;&#26469;&#35780;&#20272;&#27169;&#22411;&#29983;&#25104;&#30340;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#30340;&#27491;&#30830;&#24615;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#19982;&#31867;&#20284;&#35268;&#27169;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;KwaiYiiMath&#22312;GSM8k&#12289;CMath&#21644;KMath&#19978;&#22343;&#33021;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65288;SOTA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large language models (LLMs) have demonstrated remarkable abilities in handling a variety of natural language processing (NLP) downstream tasks, even on mathematical tasks requiring multi-step reasoning. In this report, we introduce the KwaiYiiMath which enhances the mathematical reasoning abilities of KwaiYiiBase1, by applying Supervised Fine-Tuning (SFT) and Reinforced Learning from Human Feedback (RLHF), including on both English and Chinese mathematical tasks. Meanwhile, we also constructed a small-scale Chinese primary school mathematics test set (named KMath), consisting of 188 examples to evaluate the correctness of the problem-solving process generated by the models. Empirical studies demonstrate that KwaiYiiMath can achieve state-of-the-art (SOTA) performance on GSM8k, CMath, and KMath compared with the similar size models, respectively.
&lt;/p&gt;</description></item><item><title>Jaeger&#26159;&#19968;&#31181;&#22522;&#20110;&#36830;&#25509;&#30340;&#22810;&#21464;&#25442;&#22120;VQA&#27169;&#22411;&#65292;&#21033;&#29992;RoBERTa large&#21644;GPT2-xl&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#36890;&#36807;&#24182;&#34892;&#32771;&#34385;&#22810;&#28304;&#20449;&#24687;&#26469;&#22686;&#24378;&#27169;&#22411;&#34920;&#24449;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.07091</link><description>&lt;p&gt;
Jaeger:&#19968;&#31181;&#22522;&#20110;&#36830;&#25509;&#30340;&#22810;&#21464;&#25442;&#22120;VQA&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jaeger: A Concatenation-Based Multi-Transformer VQA Model. (arXiv:2310.07091v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07091
&lt;/p&gt;
&lt;p&gt;
Jaeger&#26159;&#19968;&#31181;&#22522;&#20110;&#36830;&#25509;&#30340;&#22810;&#21464;&#25442;&#22120;VQA&#27169;&#22411;&#65292;&#21033;&#29992;RoBERTa large&#21644;GPT2-xl&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#36890;&#36807;&#24182;&#34892;&#32771;&#34385;&#22810;&#28304;&#20449;&#24687;&#26469;&#22686;&#24378;&#27169;&#22411;&#34920;&#24449;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25991;&#26723;&#30340;&#35270;&#35273;&#38382;&#31572;&#22312;&#35821;&#35328;&#24847;&#20041;&#28040;&#27495;&#21644;&#32454;&#31890;&#24230;&#22810;&#27169;&#24577;&#26816;&#32034;&#20043;&#38388;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;&#30001;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#21644;&#24320;&#25918;&#19990;&#30028;&#20808;&#39564;&#27169;&#22411;&#30340;&#21033;&#29992;&#65292;&#25991;&#26723;&#38382;&#31572;&#21462;&#24471;&#20102;&#40723;&#33310;&#20154;&#24515;&#30340;&#36827;&#23637;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#65292;&#21253;&#25324;&#21709;&#24212;&#26102;&#38388;&#24310;&#38271;&#12289;&#25512;&#26029;&#25345;&#32493;&#26102;&#38388;&#24310;&#38271;&#21644;&#21305;&#37197;&#19981;&#20934;&#30830;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36830;&#25509;&#30340;&#22810;&#21464;&#25442;&#22120;VQA&#27169;&#22411;Jaegar&#12290;&#20026;&#20102;&#25552;&#21462;&#38382;&#39064;&#29305;&#24449;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;RoBERTa large&#21644;GPT2-xl&#31561;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24378;&#22823;&#33021;&#21147;&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23558;&#20004;&#31181;&#27169;&#22411;&#30340;&#36755;&#20986;&#36827;&#34892;&#36830;&#25509;&#25805;&#20316;&#12290;&#36825;&#20010;&#25805;&#20316;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#21516;&#26102;&#32771;&#34385;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#20449;&#24687;&#65292;&#22686;&#24378;&#20102;&#20854;&#34920;&#24449;&#33021;&#21147;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#21487;&#33021;&#22686;&#24378;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document-based Visual Question Answering poses a challenging task between linguistic sense disambiguation and fine-grained multimodal retrieval. Although there has been encouraging progress in document-based question answering due to the utilization of large language and open-world prior models\cite{1}, several challenges persist, including prolonged response times, extended inference durations, and imprecision in matching. In order to overcome these challenges, we propose Jaegar, a concatenation-based multi-transformer VQA model. To derive question features, we leverage the exceptional capabilities of RoBERTa large\cite{2} and GPT2-xl\cite{3} as feature extractors. Subsequently, we subject the outputs from both models to a concatenation process. This operation allows the model to consider information from diverse sources concurrently, strengthening its representational capability. By leveraging pre-trained models for feature extraction, our approach has the potential to amplify the pe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20851;&#32852;&#35789;&#24863;&#30693;&#30340;&#35789;&#32423;&#20849;&#25351;&#28040;&#35299;&#27169;&#22411;&#65288;CAW-coref&#65289;&#65292;&#22312;&#22788;&#29702;&#24182;&#21015;&#25552;&#21450;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#20102;&#36739;&#39640;&#30340;&#24615;&#33021;&#65292;&#26377;&#25928;&#22320;&#32553;&#23567;&#20102;&#19982;&#26114;&#36149;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2310.06165</link><description>&lt;p&gt;
CAW-coref: &#20851;&#32852;&#35789;&#24863;&#30693;&#30340;&#35789;&#32423;&#20849;&#25351;&#28040;&#35299;
&lt;/p&gt;
&lt;p&gt;
CAW-coref: Conjunction-Aware Word-level Coreference Resolution. (arXiv:2310.06165v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20851;&#32852;&#35789;&#24863;&#30693;&#30340;&#35789;&#32423;&#20849;&#25351;&#28040;&#35299;&#27169;&#22411;&#65288;CAW-coref&#65289;&#65292;&#22312;&#22788;&#29702;&#24182;&#21015;&#25552;&#21450;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#20102;&#36739;&#39640;&#30340;&#24615;&#33021;&#65292;&#26377;&#25928;&#22320;&#32553;&#23567;&#20102;&#19982;&#26114;&#36149;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#20849;&#25351;&#28040;&#35299;&#31995;&#32479;&#27599;&#31687;&#25991;&#31456;&#38656;&#35201;&#22810;&#27425;&#35843;&#29992;&#35821;&#35328;&#27169;&#22411;&#65292;&#22240;&#27492;&#23545;&#20110;&#35768;&#22810;&#24212;&#29992;&#22330;&#26223;&#26469;&#35828;&#65288;&#20363;&#22914;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#36827;&#34892;&#20449;&#24687;&#25552;&#21462;&#65289;&#65292;&#20195;&#20215;&#22826;&#39640;&#12290;&#32780;&#35789;&#32423;&#20849;&#25351;&#31995;&#32479; (WL-coref) &#22312;&#25928;&#29575;&#19978;&#26356;&#21152;&#39640;&#25928;&#65292;&#23454;&#29616;&#20102;&#36825;&#20123;&#26368;&#20808;&#36827;&#31995;&#32479; 96.6% &#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#21457;&#29616;&#20102; WL-coref &#30340;&#19968;&#20010;&#24120;&#35265;&#20294;&#37325;&#35201;&#30340;&#22833;&#36133;&#26696;&#20363;&#65306;&#22788;&#29702;&#8220;Tom &#21644; Mary&#8221;&#20043;&#31867;&#30340;&#24182;&#21015;&#25552;&#21450;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22312; OntoNotes &#27979;&#35797;&#38598;&#19978;&#23558;&#24615;&#33021;&#25552;&#39640;&#20102; 0.9% F1&#65292;&#23558;&#39640;&#25928;&#30340;&#35789;&#32423;&#20849;&#25351;&#28040;&#35299;&#19982;&#26114;&#36149;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#24046;&#36317;&#32553;&#23567;&#20102;34.6%&#12290;&#25105;&#20204;&#30340;&#20851;&#32852;&#35789;&#24863;&#30693;&#30340;&#35789;&#32423;&#20849;&#25351;&#27169;&#22411;&#65288;CAW-coref&#65289;&#21644;&#20195;&#30721;&#21487;&#22312; https://github.com/KarelDO/wl-coref &#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art coreference resolutions systems depend on multiple LLM calls per document and are thus prohibitively expensive for many use cases (e.g., information extraction with large corpora). The leading word-level coreference system (WL-coref) attains 96.6% of these SOTA systems' performance while being much more efficient. In this work, we identify a routine yet important failure case of WL-coref: dealing with conjoined mentions such as 'Tom and Mary'. We offer a simple yet effective solution that improves the performance on the OntoNotes test set by 0.9% F1, shrinking the gap between efficient word-level coreference resolution and expensive SOTA approaches by 34.6%. Our Conjunction-Aware Word-level coreference model (CAW-coref) and code is available at https://github.com/KarelDO/wl-coref.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;SCPRG&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;Span-Trigger-based Contextual Pooling(STCP)&#21644;Role-based Latent Information Guidance (RLIG)&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#20013;&#24573;&#30053;&#30340;&#38750;&#35770;&#35777;&#19978;&#19979;&#25991;&#32447;&#32034;&#20449;&#24687;&#20197;&#21450;&#35770;&#35777;&#35282;&#33394;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#12290;&#27169;&#22411;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#21644;&#27719;&#32858;&#19978;&#19979;&#25991;&#20013;&#30340;&#38750;&#35770;&#35777;&#32447;&#32034;&#35789;&#65292;&#20197;&#21450;&#26500;&#24314;&#28508;&#22312;&#30340;&#35282;&#33394;&#34920;&#31034;&#24182;&#25429;&#25417;&#35821;&#20041;&#30456;&#20851;&#24615;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.05991</link><description>&lt;p&gt;
&#20351;&#29992;&#19978;&#19979;&#25991;&#32447;&#32034;&#21644;&#35282;&#33394;&#30456;&#20851;&#24615;&#25552;&#21319;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Enhancing Document-level Event Argument Extraction with Contextual Clues and Role Relevance. (arXiv:2310.05991v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;SCPRG&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;Span-Trigger-based Contextual Pooling(STCP)&#21644;Role-based Latent Information Guidance (RLIG)&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#20013;&#24573;&#30053;&#30340;&#38750;&#35770;&#35777;&#19978;&#19979;&#25991;&#32447;&#32034;&#20449;&#24687;&#20197;&#21450;&#35770;&#35777;&#35282;&#33394;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#12290;&#27169;&#22411;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#21644;&#27719;&#32858;&#19978;&#19979;&#25991;&#20013;&#30340;&#38750;&#35770;&#35777;&#32447;&#32034;&#35789;&#65292;&#20197;&#21450;&#26500;&#24314;&#28508;&#22312;&#30340;&#35282;&#33394;&#34920;&#31034;&#24182;&#25429;&#25417;&#35821;&#20041;&#30456;&#20851;&#24615;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#21477;&#23376;&#32423;&#20107;&#20214;&#35770;&#35777;&#30456;&#27604;&#65292;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#38754;&#20020;&#30528;&#38271;&#36755;&#20837;&#21644;&#36328;&#21477;&#23376;&#25512;&#29702;&#30340;&#26032;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20043;&#21069;&#30340;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#25429;&#25417;&#27599;&#20010;&#20107;&#20214;&#20013;&#20505;&#36873;&#35770;&#35777;&#19982;&#20107;&#20214;&#35302;&#21457;&#22120;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24573;&#30053;&#20102;&#20004;&#20010;&#20851;&#38190;&#28857;&#65306;a&#65289;&#38750;&#35770;&#35777;&#30340;&#19978;&#19979;&#25991;&#32447;&#32034;&#20449;&#24687;&#65307;b&#65289;&#35770;&#35777;&#35282;&#33394;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;SCPRG&#65288;&#22522;&#20110;&#36328;&#24230;&#35302;&#21457;&#22120;&#30340;&#19978;&#19979;&#25991;&#27719;&#32858;&#21644;&#28508;&#22312;&#35282;&#33394;&#24341;&#23548;&#65289;&#27169;&#22411;&#65292;&#23427;&#21253;&#21547;&#20004;&#20010;&#26032;&#39062;&#32780;&#26377;&#25928;&#30340;&#27169;&#22359;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;&#22522;&#20110;&#36328;&#24230;&#35302;&#21457;&#22120;&#30340;&#19978;&#19979;&#25991;&#27719;&#32858;&#65288;STCP&#65289;&#26681;&#25454;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#27880;&#24847;&#21147;&#26435;&#37325;&#65292;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#21644;&#27719;&#32858;&#38750;&#35770;&#35777;&#32447;&#32034;&#35789;&#30340;&#20449;&#24687;&#12290;&#22522;&#20110;&#35282;&#33394;&#30340;&#28508;&#22312;&#20449;&#24687;&#24341;&#23548;&#65288;RLIG&#65289;&#27169;&#22359;&#26500;&#24314;&#28508;&#22312;&#30340;&#35282;&#33394;&#34920;&#31034;&#65292;&#36890;&#36807;&#35282;&#33394;&#20132;&#20114;&#32534;&#30721;&#20351;&#23427;&#20204;&#30456;&#20114;&#20316;&#29992;&#20197;&#25429;&#25417;&#35821;&#20041;&#30456;&#20851;&#24615;&#65292;&#24182;&#23558;&#23427;&#20204;&#21512;&#24182;&#21040;&#20505;&#36873;&#35770;&#35777;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document-level event argument extraction poses new challenges of long input and cross-sentence inference compared to its sentence-level counterpart. However, most prior works focus on capturing the relations between candidate arguments and the event trigger in each event, ignoring two crucial points: a) non-argument contextual clue information; b) the relevance among argument roles. In this paper, we propose a SCPRG (Span-trigger-based Contextual Pooling and latent Role Guidance) model, which contains two novel and effective modules for the above problem. The Span-Trigger-based Contextual Pooling(STCP) adaptively selects and aggregates the information of non-argument clue words based on the context attention weights of specific argument-trigger pairs from pre-trained model. The Role-based Latent Information Guidance (RLIG) module constructs latent role representations, makes them interact through role-interactive encoding to capture semantic relevance, and merges them into candidate ar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#24212;&#29992;&#8220;&#19987;&#23478;&#30340;&#20056;&#31215;&#8221;&#65288;PoE&#65289;&#25216;&#26415;&#26469;&#20943;&#36731;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38271;&#24230;&#20559;&#24046;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#20027;&#35201;&#30340;&#19987;&#23478;&#20851;&#27880;&#29702;&#35299;&#20154;&#31867;&#24847;&#22270;&#65292;&#32780;&#20559;&#35265;&#19987;&#23478;&#21017;&#33268;&#21147;&#20110;&#35782;&#21035;&#21644;&#25429;&#25417;&#38271;&#24230;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2310.05199</link><description>&lt;p&gt;
&#23485;&#26494;&#30340;&#22068;&#21767;&#20250;&#20351;&#33337;&#27785;&#27809;&#65306;&#20943;&#36731;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38271;&#24230;&#20559;&#24046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Loose lips sink ships: Mitigating Length Bias in Reinforcement Learning from Human Feedback. (arXiv:2310.05199v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05199
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#24212;&#29992;&#8220;&#19987;&#23478;&#30340;&#20056;&#31215;&#8221;&#65288;PoE&#65289;&#25216;&#26415;&#26469;&#20943;&#36731;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38271;&#24230;&#20559;&#24046;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#20027;&#35201;&#30340;&#19987;&#23478;&#20851;&#27880;&#29702;&#35299;&#20154;&#31867;&#24847;&#22270;&#65292;&#32780;&#20559;&#35265;&#19987;&#23478;&#21017;&#33268;&#21147;&#20110;&#35782;&#21035;&#21644;&#25429;&#25417;&#38271;&#24230;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#26159;&#37325;&#35201;&#30340;&#26725;&#26753;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#21644;&#31038;&#20250;&#20215;&#20540;&#35266;&#23545;&#40784;&#12290;&#36825;&#31181;&#23545;&#40784;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#31867;&#21453;&#39304;&#35821;&#26009;&#24211;&#26469;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#65292;&#28982;&#21518;&#29992;&#20110;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#22870;&#21169;&#27169;&#22411;&#24120;&#24120;&#20250;&#25214;&#21040;&#32469;&#36807;&#39044;&#26399;&#30446;&#26631;&#30340;&#25463;&#24452;&#65292;&#38169;&#35823;&#22320;&#20551;&#35774;&#20154;&#31867;&#26356;&#21916;&#27426;&#36739;&#38271;&#30340;&#22238;&#31572;&#12290;&#38271;&#24230;&#20559;&#24046;&#30340;&#20986;&#29616;&#24120;&#24120;&#20250;&#23548;&#33268;&#27169;&#22411;&#20542;&#21521;&#20110;&#36739;&#38271;&#30340;&#36755;&#20986;&#65292;&#20294;&#24182;&#19981;&#24847;&#21619;&#30528;&#36825;&#20123;&#36755;&#20986;&#20013;&#26377;&#26356;&#22810;&#26377;&#29992;&#30340;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24212;&#29992;&#20102;&#8220;&#19987;&#23478;&#30340;&#20056;&#31215;&#8221;&#65288;PoE&#65289;&#25216;&#26415;&#26469;&#23558;&#22870;&#21169;&#24314;&#27169;&#19982;&#24207;&#21015;&#38271;&#24230;&#30340;&#24433;&#21709;&#20998;&#31163;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#20027;&#35201;&#30340;&#19987;&#23478;&#20851;&#27880;&#29702;&#35299;&#20154;&#31867;&#24847;&#22270;&#65292;&#32780;&#20559;&#35265;&#19987;&#23478;&#21017;&#33268;&#21147;&#20110;&#35782;&#21035;&#21644;&#25429;&#25417;&#38271;&#24230;&#20559;&#24046;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;&#20559;&#35265;&#30340;&#23398;&#20064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25200;&#21160;&#36827;&#20837;&#20559;&#24046;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning from human feedback serves as a crucial bridge, aligning large language models with human and societal values. This alignment requires a vast corpus of human feedback to learn a reward model, which is subsequently used to finetune language models. However, we have identified that the reward model often finds shortcuts to bypass its intended objectives, misleadingly assuming that humans prefer longer responses. The emergence of length bias often induces the model to favor longer outputs, yet it doesn't equate to an increase in helpful information within these outputs. In this paper, we propose an innovative solution, applying the Product-of-Experts (PoE) technique to separate reward modeling from the influence of sequence length. In our framework, the main expert concentrates on understanding human intents, while the biased expert targets the identification and capture of length bias. To further enhance the learning of bias, we introduce perturbations into the bia
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;DQ-LoRe&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#21452;&#37325;&#26597;&#35810;&#21644;&#20302;&#31209;&#36817;&#20284;&#37325;&#26032;&#25490;&#24207;&#33258;&#21160;&#36873;&#25321;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#31034;&#20363;&#65292;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#21644;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.02954</link><description>&lt;p&gt;
DQ-LoRe: &#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#20302;&#31209;&#36817;&#20284;&#21452;&#37325;&#26597;&#35810;&#37325;&#26032;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
DQ-LoRe: Dual Queries with Low Rank Approximation Re-ranking for In-Context Learning. (arXiv:2310.02954v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02954
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;DQ-LoRe&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#21452;&#37325;&#26597;&#35810;&#21644;&#20302;&#31209;&#36817;&#20284;&#37325;&#26032;&#25490;&#24207;&#33258;&#21160;&#36873;&#25321;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#31034;&#20363;&#65292;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#26032;&#36827;&#23637;&#65292;&#20027;&#35201;&#26159;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25512;&#21160;&#30340;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#24341;&#23548;LLMs&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#26159;&#21033;&#29992;&#38142;&#24335;&#24605;&#32500;&#65288;CoT&#65289;&#33539;&#24335;&#20013;&#30340;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#26368;&#20027;&#35201;&#30340;&#25361;&#25112;&#22312;&#20110;&#26377;&#25928;&#22320;&#36873;&#25321;&#31034;&#20363;&#26469;&#20419;&#36827;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#21452;&#37325;&#26597;&#35810;&#21644;&#20302;&#31209;&#36817;&#20284;&#37325;&#26032;&#25490;&#24207;&#65288;DQ-LoRe&#65289;&#26469;&#33258;&#21160;&#36873;&#25321;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#31034;&#20363;&#12290;&#21452;&#37325;&#26597;&#35810;&#39318;&#20808;&#26597;&#35810;LLM&#20197;&#33719;&#21462;LLM&#29983;&#25104;&#30340;&#30693;&#35782;&#65292;&#20363;&#22914;CoT&#65292;&#28982;&#21518;&#36890;&#36807;&#38382;&#39064;&#21644;&#30693;&#35782;&#26597;&#35810;&#26816;&#32034;&#22120;&#20197;&#33719;&#21462;&#26368;&#32456;&#30340;&#31034;&#20363;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#31532;&#20108;&#20010;&#26597;&#35810;&#65292;LoRe&#21033;&#29992;&#38477;&#32500;&#25216;&#26415;&#26469;&#25913;&#36827;&#31034;&#20363;&#36873;&#25321;&#65292;&#30830;&#20445;&#19982;&#36755;&#20837;&#38382;&#39064;&#30340;&#30693;&#35782;&#23494;&#20999;&#23545;&#40784;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;DQ-LoRe&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in natural language processing, primarily propelled by Large Language Models (LLMs), have showcased their remarkable capabilities grounded in in-context learning. A promising avenue for guiding LLMs in intricate reasoning tasks involves the utilization of intermediate reasoning steps within the Chain-of-Thought (CoT) paradigm. Nevertheless, the central challenge lies in the effective selection of exemplars for facilitating in-context learning. In this study, we introduce a framework that leverages Dual Queries and Low-rank approximation Re-ranking (DQ-LoRe) to automatically select exemplars for in-context learning. Dual Queries first query LLM to obtain LLM-generated knowledge such as CoT, then query the retriever to obtain the final exemplars via both question and the knowledge. Moreover, for the second query, LoRe employs dimensionality reduction techniques to refine exemplar selection, ensuring close alignment with the input question's knowledge. Through extensive ex
&lt;/p&gt;</description></item><item><title>OceanGPT&#26159;&#39318;&#20010;&#19987;&#20026;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#35774;&#35745;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;DoInstruct&#26694;&#26550;&#23454;&#29616;&#33258;&#21160;&#33719;&#21462;&#28023;&#27915;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#12290;&#36825;&#19968;&#27169;&#22411;&#30340;&#24341;&#20837;&#22635;&#34917;&#20102;&#28023;&#27915;&#31185;&#23398;&#39046;&#22495;&#20013;&#23545;LLM&#30340;&#38656;&#27714;&#32570;&#21475;&#65292;&#24182;&#20026;&#28023;&#27915;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#24037;&#20855;&#21644;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.02031</link><description>&lt;p&gt;
OceanGPT&#65306;&#29992;&#20110;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
OceanGPT: A Large Language Model for Ocean Science Tasks. (arXiv:2310.02031v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02031
&lt;/p&gt;
&lt;p&gt;
OceanGPT&#26159;&#39318;&#20010;&#19987;&#20026;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#35774;&#35745;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;DoInstruct&#26694;&#26550;&#23454;&#29616;&#33258;&#21160;&#33719;&#21462;&#28023;&#27915;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#12290;&#36825;&#19968;&#27169;&#22411;&#30340;&#24341;&#20837;&#22635;&#34917;&#20102;&#28023;&#27915;&#31185;&#23398;&#39046;&#22495;&#20013;&#23545;LLM&#30340;&#38656;&#27714;&#32570;&#21475;&#65292;&#24182;&#20026;&#28023;&#27915;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#24037;&#20855;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#27915;&#31185;&#23398;&#26159;&#25506;&#32034;&#20805;&#28385;&#29983;&#21629;&#21644;&#29983;&#29289;&#22810;&#26679;&#24615;&#30340;&#28023;&#27915;&#30340;&#31185;&#23398;&#65292;&#32771;&#34385;&#21040;&#28023;&#27915;&#35206;&#30422;&#20102;&#22320;&#29699;&#34920;&#38754;&#30340;70&#65285;&#20197;&#19978;&#65292;&#36825;&#19968;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#25913;&#21464;&#20102;&#31185;&#23398;&#30340;&#33539;&#24335;&#12290;&#23613;&#31649;&#22312;&#20854;&#20182;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#29616;&#26377;&#30340;LLM&#36890;&#24120;&#26080;&#27861;&#28385;&#36275;&#28023;&#27915;&#23398;&#23478;&#31561;&#39046;&#22495;&#19987;&#23478;&#30340;&#38656;&#27714;&#65292;&#21516;&#26102;&#23545;LLM&#22312;&#28023;&#27915;&#31185;&#23398;&#20013;&#30340;&#28508;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#36825;&#20854;&#20013;&#30340;&#26681;&#26412;&#21407;&#22240;&#21487;&#33021;&#26159;&#28023;&#27915;&#25968;&#25454;&#30340;&#24222;&#22823;&#32780;&#22797;&#26434;&#30340;&#24615;&#36136;&#65292;&#20197;&#21450;&#23545;&#26356;&#39640;&#30340;&#31890;&#24230;&#21644;&#20016;&#23500;&#30340;&#30693;&#35782;&#30340;&#38656;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;&#39318;&#20010;&#28023;&#27915;&#39046;&#22495;&#30340;LLM&#8212;&#8212;OceanGPT&#65292;&#35813;&#27169;&#22411;&#25797;&#38271;&#21508;&#31181;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;DoInstruct&#65292;&#29992;&#20110;&#33258;&#21160;&#33719;&#21462;&#22823;&#37327;&#30340;&#28023;&#27915;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#65292;&#23427;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#30340;&#21327;&#20316;&#29983;&#25104;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ocean science, which delves into the oceans that are reservoirs of life and biodiversity, is of great significance given that oceans cover over 70% of our planet's surface. Recently, advances in Large Language Models (LLMs) have transformed the paradigm in science. Despite the success in other domains, current LLMs often fall short in catering to the needs of domain experts like oceanographers, and the potential of LLMs for ocean science is under-explored. The intrinsic reason may be the immense and intricate nature of ocean data as well as the necessity for higher granularity and richness in knowledge. To alleviate these issues, we introduce OceanGPT, the first-ever LLM in the ocean domain, which is expert in various ocean science tasks. We propose DoInstruct, a novel framework to automatically obtain a large volume of ocean domain instruction data, which generates instructions based on multi-agent collaboration. Additionally, we construct the first oceanography benchmark, OceanBench,
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21019;&#24314;&#20803;&#35821;&#20041;&#27169;&#26495;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23545;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#38598;&#29983;&#25104;&#26032;&#30340;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#35780;&#20272;&#38598;&#12290;</title><link>http://arxiv.org/abs/2310.01448</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#20803;&#35821;&#20041;&#27169;&#26495;
&lt;/p&gt;
&lt;p&gt;
Meta Semantic Template for Evaluation of Large Language Models. (arXiv:2310.01448v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01448
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21019;&#24314;&#20803;&#35821;&#20041;&#27169;&#26495;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23545;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#38598;&#29983;&#25104;&#26032;&#30340;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#35780;&#20272;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26159;&#21542;&#30495;&#27491;&#29702;&#35299;&#35821;&#35328;&#30340;&#35821;&#20041;&#65292;&#36824;&#26159;&#20165;&#20165;&#35760;&#20303;&#35757;&#32451;&#25968;&#25454;&#65311;&#26368;&#36817;&#23545;LLM&#28508;&#22312;&#25968;&#25454;&#27745;&#26579;&#30340;&#25285;&#24551;&#24341;&#36215;&#20102;&#31038;&#21306;&#23545;LLM&#35780;&#20272;&#30740;&#31350;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MSTemp&#65292;&#19968;&#31181;&#36890;&#36807;&#21019;&#24314;&#20803;&#35821;&#20041;&#27169;&#26495;&#26469;&#35780;&#20272;LLM&#23545;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;MSTemp&#30340;&#26680;&#24515;&#19981;&#26159;&#30452;&#25509;&#22312;&#29616;&#26377;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#32780;&#26159;&#20351;&#29992;&#29616;&#26377;&#25968;&#25454;&#38598;&#20316;&#20026;&#31181;&#23376;&#29983;&#25104;&#26032;&#30340;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#35780;&#20272;&#38598;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#32473;&#23450;&#30340;&#21477;&#23376;&#65292;MSTemp&#21033;&#29992;&#21478;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26032;&#26679;&#26412;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#35821;&#20041;&#12290;&#36825;&#20123;&#26032;&#26679;&#26412;&#34987;&#31216;&#20026;&#21407;&#21477;&#23376;&#30340;&#35821;&#20041;&#27169;&#26495;&#12290;&#28982;&#21518;&#65292;MSTemp&#36890;&#36807;&#21477;&#23376;&#35299;&#26512;&#21644;&#38543;&#26426;&#26367;&#25442;&#35789;&#35821;&#26469;&#29983;&#25104;&#35780;&#20272;&#26679;&#26412;&#12290;MSTemp&#20855;&#26377;&#39640;&#24230;&#28789;&#27963;&#12289;&#21160;&#24577;&#21644;&#25104;&#26412;&#25928;&#30410;&#24615;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;MSTemp-
&lt;/p&gt;
&lt;p&gt;
Do large language models (LLMs) genuinely understand the semantics of the language, or just memorize the training data? The recent concern on potential data contamination of LLMs has raised awareness of the community to conduct research on LLMs evaluation. In this paper, we propose MSTemp, an approach that creates meta semantic templates to evaluate the semantic understanding ability of LLMs. The core of MSTemp is not to perform evaluation directly on existing benchmark datasets, but to generate new out-of-distribution (OOD) evaluation sets using existing datasets as seeds. Specifically, for a given sentence, MSTemp leverages another language model to generate new samples while preserving its semantics. The new samples are called semantic templates to the original sentence. Then, MSTemp generates evaluation samples via sentence parsing and random word replacement on the semantic templates. MSTemp is highly flexible, dynamic, and cost-effective. Our initial experiments show that MSTemp-
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#37492;&#21035;-&#25209;&#21028;&#24046;&#36317;&#27979;&#37327;LLMs&#23545;&#20154;&#31867;&#20215;&#20540;&#30340;&#29702;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20215;&#20540;&#29702;&#35299;&#27979;&#37327;&#65288;VUM&#65289;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;GPT-4&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;&#19968;&#21315;&#20010;&#23545;&#35805;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#23610;&#24230;&#23450;&#24459;&#23545;LLMs&#30340;&#8220;&#30693;&#36947;&#20160;&#20040;&#8221;&#26377;&#36739;&#22823;&#24433;&#21709;&#65292;&#32780;&#23545;&#8220;&#30693;&#36947;&#20026;&#20160;&#20040;&#8221;&#24433;&#21709;&#36739;&#23567;&#12290;</title><link>http://arxiv.org/abs/2310.00378</link><description>&lt;p&gt;
&#36890;&#36807;&#37492;&#21035;-&#25209;&#21028;&#24046;&#36317;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#23545;&#20215;&#20540;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Measuring Value Understanding in Language Models through Discriminator-Critique Gap. (arXiv:2310.00378v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00378
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37492;&#21035;-&#25209;&#21028;&#24046;&#36317;&#27979;&#37327;LLMs&#23545;&#20154;&#31867;&#20215;&#20540;&#30340;&#29702;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20215;&#20540;&#29702;&#35299;&#27979;&#37327;&#65288;VUM&#65289;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;GPT-4&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;&#19968;&#21315;&#20010;&#23545;&#35805;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#23610;&#24230;&#23450;&#24459;&#23545;LLMs&#30340;&#8220;&#30693;&#36947;&#20160;&#20040;&#8221;&#26377;&#36739;&#22823;&#24433;&#21709;&#65292;&#32780;&#23545;&#8220;&#30693;&#36947;&#20026;&#20160;&#20040;&#8221;&#24433;&#21709;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#24341;&#21457;&#20102;&#23545;&#23427;&#20204;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20043;&#38388;&#28508;&#22312;&#19981;&#19968;&#33268;&#24615;&#30340;&#25285;&#24551;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#30340;&#22797;&#26434;&#21644;&#36866;&#24212;&#24615;&#65292;&#35780;&#20272;&#23427;&#20204;&#23545;&#36825;&#20123;&#20215;&#20540;&#35266;&#30340;&#29702;&#35299;&#26159;&#22797;&#26434;&#30340;&#12290;&#25105;&#20204;&#35748;&#20026;&#30495;&#27491;&#29702;&#35299;LLMs&#20013;&#30340;&#20215;&#20540;&#35266;&#38656;&#35201;&#32771;&#34385;&#21040;&#8220;&#30693;&#36947;&#20160;&#20040;&#8221;&#21644;&#8220;&#30693;&#36947;&#20026;&#20160;&#20040;&#8221;&#20004;&#20010;&#26041;&#38754;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20215;&#20540;&#29702;&#35299;&#27979;&#37327;&#65288;VUM&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#37327;&#21270;&#37492;&#21035;-&#25209;&#21028;&#24046;&#36317;&#26469;&#23450;&#37327;&#35780;&#20272;&#8220;&#30693;&#36947;&#20160;&#20040;&#8221;&#21644;&#8220;&#30693;&#36947;&#20026;&#20160;&#20040;&#8221;&#12290;&#21033;&#29992;&#26045;&#29926;&#33576;&#20215;&#20540;&#35266;&#35843;&#26597;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#35780;&#20272;&#20215;&#20540;&#35266;&#30340;&#26631;&#20934;&#65292;&#24182;&#20351;&#29992;GPT-4&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;&#19968;&#21315;&#20010;&#23545;&#35805;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32771;&#23519;&#20102;LLMs&#30340;&#36755;&#20986;&#19982;&#22522;&#20934;&#31572;&#26696;&#20043;&#38388;&#30340;&#20215;&#20540;&#35266;&#19968;&#33268;&#24615;&#65292;&#20197;&#21450;LLMs&#30340;&#22238;&#31572;&#19982;GPT-4&#30340;&#27880;&#37322;&#22312;&#20215;&#20540;&#35748;&#30693;&#21407;&#22240;&#19978;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20116;&#20010;&#20195;&#34920;&#24615;LLMs&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#35777;&#25454;&#34920;&#26126;&#65292;&#23610;&#24230;&#23450;&#24459;&#23545;&#8220;&#30693;&#36947;&#20160;&#20040;&#8221;&#30340;&#24433;&#21709;&#36739;&#22823;&#65292;&#20294;&#23545;&#8220;&#30693;&#36947;&#20026;&#20160;&#20040;&#8221;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Large Language Models (LLMs) have heightened concerns about their potential misalignment with human values. However, evaluating their grasp of these values is complex due to their intricate and adaptable nature. We argue that truly understanding values in LLMs requires considering both "know what" and "know why". To this end, we present the Value Understanding Measurement (VUM) framework that quantitatively assess both "know what" and "know why" by measuring the discriminator-critique gap related to human values. Using the Schwartz Value Survey, we specify our evaluation values and develop a thousand-level dialogue dataset with GPT-4. Our assessment looks at both the value alignment of LLM's outputs compared to baseline answers and how LLM responses align with reasons for value recognition versus GPT-4's annotations. We evaluate five representative LLMs and provide strong evidence that the scaling law significantly impacts "know what" but not much on "know why", 
&lt;/p&gt;</description></item><item><title>NLPBench&#26159;&#19968;&#20010;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;NLP&#38382;&#39064;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20026;&#22635;&#34917;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#20316;&#32773;&#25910;&#38598;&#20102;&#26469;&#33258;&#32822;&#40065;&#22823;&#23398;&#26399;&#26411;&#32771;&#35797;&#30340;378&#20010;&#28085;&#30422;&#22810;&#20010;NLP&#20027;&#39064;&#30340;&#38382;&#39064;&#12290;&#35813;&#30740;&#31350;&#21457;&#29616;&#22312;&#20351;&#29992;&#39640;&#32423;&#25552;&#31034;&#31574;&#30053;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#21487;&#33021;&#19981;&#31283;&#23450;&#65292;&#24182;&#21487;&#33021;&#23545;&#36739;&#23567;&#30340;&#27169;&#22411;&#36896;&#25104;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.15630</link><description>&lt;p&gt;
NLPBench&#65306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;NLP&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
NLPBench: Evaluating Large Language Models on Solving NLP Problems. (arXiv:2309.15630v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15630
&lt;/p&gt;
&lt;p&gt;
NLPBench&#26159;&#19968;&#20010;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;NLP&#38382;&#39064;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20026;&#22635;&#34917;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#20316;&#32773;&#25910;&#38598;&#20102;&#26469;&#33258;&#32822;&#40065;&#22823;&#23398;&#26399;&#26411;&#32771;&#35797;&#30340;378&#20010;&#28085;&#30422;&#22810;&#20010;NLP&#20027;&#39064;&#30340;&#38382;&#39064;&#12290;&#35813;&#30740;&#31350;&#21457;&#29616;&#22312;&#20351;&#29992;&#39640;&#32423;&#25552;&#31034;&#31574;&#30053;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#21487;&#33021;&#19981;&#31283;&#23450;&#65292;&#24182;&#21487;&#33021;&#23545;&#36739;&#23567;&#30340;&#27169;&#22411;&#36896;&#25104;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#21457;&#23637;&#26174;&#31034;&#20986;&#22686;&#24378;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#19968;&#20123;&#25104;&#21151;&#65292;&#20294;&#22312;LLMs&#30340;NLP&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#26041;&#38754;&#20173;&#28982;&#32570;&#20047;&#19987;&#38376;&#30340;&#30740;&#31350;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#39046;&#22495;&#30340;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;NLPBench&#65292;&#21253;&#25324;378&#20010;&#28085;&#30422;&#21508;&#31181;NLP&#20027;&#39064;&#30340;&#22823;&#23398;&#27700;&#24179;NLP&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#28304;&#33258;&#32822;&#40065;&#22823;&#23398;&#20197;&#21069;&#30340;&#26399;&#26411;&#32771;&#35797;&#12290;NLPBench&#21253;&#25324;&#20855;&#26377;&#19978;&#19979;&#25991;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#22810;&#20010;&#23376;&#38382;&#39064;&#20998;&#20139;&#30456;&#21516;&#30340;&#20844;&#20849;&#20449;&#24687;&#65292;&#24182;&#19988;&#21253;&#25324;&#22810;&#36873;&#39064;&#12289;&#31616;&#31572;&#39064;&#21644;&#25968;&#23398;&#39064;&#31561;&#22810;&#31181;&#38382;&#39064;&#31867;&#22411;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#20197;GPT-3.5/4&#12289;PaLM-2&#21644;LLAMA-2&#31561;LLMs&#20026;&#20013;&#24515;&#65292;&#37319;&#29992;&#20102;&#35832;&#22914;&#38142;&#24335;&#24605;&#32500;&#65288;CoT&#65289;&#21644;&#24605;&#32500;&#26641;&#65288;ToT&#65289;&#31561;&#39640;&#32423;&#25552;&#31034;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#39640;&#32423;&#25552;&#31034;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#21487;&#33021;&#19981;&#19968;&#33268;&#65292;&#26377;&#26102;&#20250;&#25439;&#23475;LLMs&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#36739;&#23567;&#30340;&#27169;&#22411;&#65288;LLA&#65289;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent developments in large language models (LLMs) have shown promise in enhancing the capabilities of natural language processing (NLP). Despite these successes, there remains a dearth of research dedicated to the NLP problem-solving abilities of LLMs. To fill the gap in this area, we present a unique benchmarking dataset, NLPBench, comprising 378 college-level NLP questions spanning various NLP topics sourced from Yale University's prior final exams. NLPBench includes questions with context, in which multiple sub-questions share the same public information, and diverse question types, including multiple choice, short answer, and math. Our evaluation, centered on LLMs such as GPT-3.5/4, PaLM-2, and LLAMA-2, incorporates advanced prompting strategies like the chain-of-thought (CoT) and tree-of-thought (ToT). Our study reveals that the effectiveness of the advanced prompting strategies can be inconsistent, occasionally damaging LLM performance, especially in smaller models like the LLA
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AnglE&#30340;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#26469;&#32531;&#35299;&#25991;&#26412;&#23884;&#20837;&#20013;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#36896;&#25104;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;STS&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#65292;&#24182;&#22312;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#20013;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12871</link><description>&lt;p&gt;
&#35282;&#24230;&#20248;&#21270;&#30340;&#25991;&#26412;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
AnglE-Optimized Text Embeddings. (arXiv:2309.12871v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AnglE&#30340;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#26469;&#32531;&#35299;&#25991;&#26412;&#23884;&#20837;&#20013;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#36896;&#25104;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;STS&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#65292;&#24182;&#22312;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#20013;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#23545;&#20110;&#25552;&#21319;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#65288;STS&#65289;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#36825;&#20123;&#20219;&#21153;&#21448;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#38754;&#20020;&#30340;&#19968;&#20010;&#26222;&#36941;&#25361;&#25112;&#26159;&#28176;&#21464;&#28040;&#22833;&#38382;&#39064;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#23427;&#20204;&#22312;&#20248;&#21270;&#30446;&#26631;&#20013;&#20381;&#36182;&#20313;&#24358;&#20989;&#25968;&#65292;&#32780;&#20313;&#24358;&#20989;&#25968;&#20855;&#26377;&#39281;&#21644;&#21306;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;AnglE&#30340;&#26032;&#22411;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#12290;AnglE&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#22312;&#19968;&#20010;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#20135;&#29983;&#30340;&#19981;&#21033;&#24433;&#21709;&#65292;&#20174;&#32780;&#21487;&#20197;&#38459;&#30861;&#26799;&#24230;&#24182;&#38459;&#30861;&#20248;&#21270;&#36807;&#31243;&#12290;&#20026;&#20102;&#24314;&#31435;&#20840;&#38754;&#30340;STS&#35780;&#20272;&#65292;&#25105;&#20204;&#22312;&#29616;&#26377;&#30340;&#30701;&#25991;&#26412;STS&#25968;&#25454;&#38598;&#21644;&#20174;GitHub Issues&#20013;&#26032;&#25910;&#38598;&#30340;&#38271;&#25991;&#26412;STS&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20855;&#26377;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#65292;&#24182;&#25506;&#35752;&#20102;AnglE&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-quality text embedding is pivotal in improving semantic textual similarity (STS) tasks, which are crucial components in Large Language Model (LLM) applications. However, a common challenge existing text embedding models face is the problem of vanishing gradients, primarily due to their reliance on the cosine function in the optimization objective, which has saturation zones. To address this issue, this paper proposes a novel angle-optimized text embedding model called AnglE. The core idea of AnglE is to introduce angle optimization in a complex space. This novel approach effectively mitigates the adverse effects of the saturation zone in the cosine function, which can impede gradient and hinder optimization processes. To set up a comprehensive STS evaluation, we experimented on existing short-text STS datasets and a newly collected long-text STS dataset from GitHub Issues. Furthermore, we examine domain-specific STS scenarios with limited labeled data and explore how AnglE works w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#25105;&#24378;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#33258;&#21160;&#29983;&#25104;&#21644;&#35780;&#20272;&#23398;&#29983;&#29983;&#25104;&#30340;&#35299;&#37322;&#65292;&#29992;&#20110;&#25913;&#36827;&#23398;&#29983;&#36164;&#28304;&#20849;&#20139;&#20013;&#23398;&#29983;&#29983;&#25104;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#35299;&#37322;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.10444</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#32034;&#33258;&#25105;&#24378;&#21270;&#20197;&#25913;&#36827;&#23398;&#29983;&#29983;&#25104;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Exploring Self-Reinforcement for Improving Learnersourced Multiple-Choice Question Explanations with Large Language Models. (arXiv:2309.10444v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#25105;&#24378;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#33258;&#21160;&#29983;&#25104;&#21644;&#35780;&#20272;&#23398;&#29983;&#29983;&#25104;&#30340;&#35299;&#37322;&#65292;&#29992;&#20110;&#25913;&#36827;&#23398;&#29983;&#36164;&#28304;&#20849;&#20139;&#20013;&#23398;&#29983;&#29983;&#25104;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#35299;&#37322;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#29983;&#36164;&#28304;&#20849;&#20139;&#28041;&#21450;&#23398;&#29983;&#29983;&#25104;&#21644;&#20998;&#20139;&#23398;&#20064;&#36164;&#28304;&#12290;&#22312;&#23398;&#29983;&#29983;&#25104;&#22810;&#39033;&#36873;&#25321;&#39064;&#26102;&#65292;&#21019;&#24314;&#35299;&#37322;&#26159;&#19968;&#20010;&#20851;&#38190;&#27493;&#39588;&#65292;&#22240;&#20026;&#23427;&#26377;&#21161;&#20110;&#23545;&#30456;&#20851;&#27010;&#24565;&#30340;&#28145;&#20837;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#23398;&#29983;&#24448;&#24448;&#30001;&#20110;&#20027;&#39064;&#29702;&#35299;&#26377;&#38480;&#21644;&#20165;&#20165;&#37325;&#30003;&#38382;&#39064;&#12289;&#24178;&#25200;&#22240;&#32032;&#21644;&#27491;&#30830;&#31572;&#26696;&#30340;&#20542;&#21521;&#32780;&#38590;&#20197;&#32534;&#20889;&#26377;&#25928;&#30340;&#35299;&#37322;&#12290;&#20026;&#20102;&#24110;&#21161;&#25903;&#25745;&#36825;&#20010;&#20219;&#21153;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#25105;&#24378;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#26088;&#22312;&#33258;&#21160;&#29983;&#25104;&#21644;&#35780;&#20272;&#35299;&#37322;&#12290;&#35813;&#26694;&#26550;&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65292;&#29983;&#25104;&#19982;&#23398;&#29983;&#23545;&#40784;&#30340;&#35299;&#37322;&#65292;&#35780;&#20272;&#36825;&#20123;&#35299;&#37322;&#20197;&#30830;&#20445;&#20854;&#36136;&#37327;&#65292;&#24182;&#36845;&#20195;&#22686;&#24378;&#35299;&#37322;&#12290;&#22914;&#26524;&#19968;&#20010;&#35299;&#37322;&#30340;&#35780;&#20272;&#20998;&#25968;&#20302;&#20110;&#23450;&#20041;&#30340;&#38408;&#20540;&#65292;&#26694;&#26550;&#20250;&#36845;&#20195;&#22320;&#20248;&#21270;&#21644;&#37325;&#26032;&#35780;&#20272;&#35299;&#37322;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#27169;&#25311;&#20102;&#19968;&#20010;&#23398;&#29983;&#23398;&#20064;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learnersourcing involves students generating and sharing learning resources with their peers. When learnersourcing multiple-choice questions, creating explanations for the generated questions is a crucial step as it facilitates a deeper understanding of the related concepts. However, it is often difficult for students to craft effective explanations due to limited subject understanding and a tendency to merely restate the question stem, distractors, and correct answer. To help scaffold this task, in this work we propose a self-reinforcement large-language-model framework, with the goal of generating and evaluating explanations automatically. Comprising three modules, the framework generates student-aligned explanations, evaluates these explanations to ensure their quality and iteratively enhances the explanations. If an explanation's evaluation score falls below a defined threshold, the framework iteratively refines and reassesses the explanation. Importantly, our framework emulates th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#20026;&#25918;&#23556;&#23398;&#26500;&#24314;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#21307;&#23398;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#21644;&#25903;&#25345;&#19981;&#21516;&#25918;&#23556;&#23398;&#20219;&#21153;&#30340;&#26550;&#26500;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2308.02463</link><description>&lt;p&gt;
&#20026;&#25918;&#23556;&#23398;&#26500;&#24314;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Towards Generalist Foundation Model for Radiology. (arXiv:2308.02463v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#20026;&#25918;&#23556;&#23398;&#26500;&#24314;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#21307;&#23398;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#21644;&#25903;&#25345;&#19981;&#21516;&#25918;&#23556;&#23398;&#20219;&#21153;&#30340;&#26550;&#26500;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#21551;&#21160;&#25918;&#23556;&#23398;&#22522;&#30784;&#27169;&#22411;&#30340;&#24320;&#21457;&#65292;&#31216;&#20026;RadFM&#12290;&#25105;&#20204;&#20174;&#25968;&#25454;&#12289;&#27169;&#22411;&#35774;&#35745;&#21644;&#35780;&#20272;&#30340;&#35282;&#24230;&#20840;&#38754;&#32771;&#34385;&#20102;&#22522;&#30784;&#27169;&#22411;&#30340;&#26500;&#24314;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#21487;&#24635;&#32467;&#22914;&#19979;&#65306;&#65288;i&#65289;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#21307;&#23398;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;MedMD&#65292;&#21253;&#25324;1600&#19975;&#20010;2D&#21644;3D&#21307;&#23398;&#25195;&#25551;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21253;&#21547;3D&#21307;&#23398;&#25195;&#25551;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#12290;&#65288;ii&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26550;&#26500;&#65292;&#20351;&#24471;&#21487;&#35270;&#26465;&#20214;&#29983;&#25104;&#39044;&#35757;&#32451;&#25104;&#20026;&#21487;&#33021;&#65292;&#21487;&#20197;&#23558;&#25991;&#26412;&#36755;&#20837;&#19982;2D&#25110;3D&#21307;&#23398;&#25195;&#25551;&#20132;&#38169;&#65292;&#29983;&#25104;&#19981;&#21516;&#25918;&#23556;&#23398;&#20219;&#21153;&#30340;&#21709;&#24212;&#12290;&#35813;&#27169;&#22411;&#39318;&#20808;&#22312;MedMD&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;RadMD&#19978;&#36827;&#34892;&#20102;&#29305;&#23450;&#39046;&#22495;&#30340;&#24494;&#35843;&#65292;RadMD&#26159;MedMD&#30340;&#25918;&#23556;&#23398;&#28165;&#29702;&#29256;&#26412;&#65292;&#21253;&#21547;300&#19975;&#20010;&#25918;&#23556;&#23398;&#30340;&#35270;&#35273;&#35821;&#35328;&#23545;&#12290;&#65288;iii&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#21253;&#25324;&#20116;&#20010;&#20219;&#21153;&#65292;&#26088;&#22312;&#20840;&#38754;&#35780;&#20272;&#35813;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we aim to initiate the development of Radiology Foundation Model, termed as RadFM.We consider the construction of foundational models from the perspectives of data, model design, and evaluation thoroughly. Our contribution can be concluded as follows: (i), we construct a large-scale Medical Multi-modal Dataset, MedMD, consisting of 16M 2D and 3D medical scans. To the best of our knowledge, this is the first multi-modal dataset containing 3D medical scans. (ii), We propose an architecture that enables visually conditioned generative pre-training, allowing for the integration of text input interleaved with 2D or 3D medical scans to generate response for diverse radiologic tasks. The model was initially pre-trained on MedMD and subsequently domain-specific fine-tuned on RadMD, a radiologic cleaned version of MedMD, containing 3M radiologic visual-language pairs. (iii), we propose a new evaluation benchmark that comprises five tasks, aiming to comprehensively assess the capa
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#26088;&#22312;&#25552;&#20379;&#23545;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#22312;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#21644;&#25991;&#26412;&#20013;&#36827;&#34892;&#32852;&#21512;&#23398;&#20064;&#30340;&#24403;&#21069;&#29366;&#20917;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#24182;&#25506;&#32034;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2307.07362</link><description>&lt;p&gt;
&#19968;&#31687;&#20851;&#20110;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#22312;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#21644;&#25991;&#26412;&#20013;&#30340;&#25195;&#25551;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A scoping review on multimodal deep learning in biomedical images and texts. (arXiv:2307.07362v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07362
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#26088;&#22312;&#25552;&#20379;&#23545;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#22312;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#21644;&#25991;&#26412;&#20013;&#36827;&#34892;&#32852;&#21512;&#23398;&#20064;&#30340;&#24403;&#21069;&#29366;&#20917;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#24182;&#25506;&#32034;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26410;&#26469;&#30340;&#35745;&#31639;&#36741;&#21161;&#35786;&#26029;&#21644;&#39044;&#21518;&#31995;&#32479;&#24212;&#35813;&#33021;&#22815;&#21516;&#26102;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#12290;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#65288;MDL&#65289;&#28041;&#21450;&#22810;&#31181;&#25968;&#25454;&#28304;&#65288;&#22914;&#22270;&#20687;&#21644;&#25991;&#26412;&#65289;&#30340;&#25972;&#21512;&#65292;&#26377;&#28508;&#21147;&#24443;&#24213;&#25913;&#21464;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#30340;&#20998;&#26512;&#21644;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#39046;&#22495;&#30452;&#21040;&#26368;&#36817;&#25165;&#24341;&#36215;&#30740;&#31350;&#20154;&#21592;&#30340;&#27880;&#24847;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#23545;&#36825;&#20010;&#20027;&#39064;&#36827;&#34892;&#31995;&#32479;&#32508;&#36848;&#65292;&#30830;&#23450;&#24403;&#21069;&#24037;&#20316;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25506;&#32034;&#26410;&#26469;&#30340;&#26041;&#21521;&#12290;&#22312;&#36825;&#31687;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#23545;&#35813;&#39046;&#22495; current state &#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#32852;&#21512;&#23398;&#20064;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#36825;&#20004;&#31181;&#25968;&#25454;&#31867;&#22411;&#22312; MDL &#30740;&#31350;&#20013;&#26368;&#24120;&#29992;&#12290;&#26412;&#30740;&#31350;&#22238;&#39038;&#20102;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#22312;&#20116;&#20010;&#20219;&#21153;&#20013;&#30340;&#24403;&#21069;&#24212;&#29992;&#65306;&#65288;1&#65289;&#25253;&#21578;&#29983;&#25104;&#65292;&#65288;2&#65289;&#35270;&#35273;&#38382;&#31572;&#65292;&#65288;3&#65289;&#20132;&#21449;...
&lt;/p&gt;
&lt;p&gt;
Computer-assisted diagnostic and prognostic systems of the future should be capable of simultaneously processing multimodal data. Multimodal deep learning (MDL), which involves the integration of multiple sources of data, such as images and text, has the potential to revolutionize the analysis and interpretation of biomedical data. However, it only caught researchers' attention recently. To this end, there is a critical need to conduct a systematic review on this topic, identify the limitations of current work, and explore future directions. In this scoping review, we aim to provide a comprehensive overview of the current state of the field and identify key concepts, types of studies, and research gaps with a focus on biomedical images and texts joint learning, mainly because these two were the most commonly available data types in MDL research. This study reviewed the current uses of multimodal deep learning on five tasks: (1) Report generation, (2) Visual question answering, (3) Cros
&lt;/p&gt;</description></item><item><title>Voicebox&#26159;&#19968;&#31181;&#22823;&#35268;&#27169;&#30340;&#22810;&#35821;&#35328;&#36890;&#29992;&#35821;&#38899;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#33258;&#22238;&#24402;&#30340;&#27969;&#21305;&#37197;&#27169;&#22411;&#65292;&#22312;&#25991;&#26412;&#21644;&#38899;&#39057;&#19978;&#19979;&#25991;&#26465;&#20214;&#19979;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#23454;&#29616;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#65292;&#22122;&#22768;&#21435;&#38500;&#65292;&#20869;&#23481;&#32534;&#36753;&#65292;&#39118;&#26684;&#36716;&#25442;&#21644;&#22810;&#26679;&#21270;&#30340;&#26679;&#26412;&#29983;&#25104;&#31561;&#22810;&#31181;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.15687</link><description>&lt;p&gt;
Voicebox&#65306;&#22823;&#35268;&#27169;&#30340;&#22810;&#35821;&#35328;&#36890;&#29992;&#35821;&#38899;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale. (arXiv:2306.15687v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15687
&lt;/p&gt;
&lt;p&gt;
Voicebox&#26159;&#19968;&#31181;&#22823;&#35268;&#27169;&#30340;&#22810;&#35821;&#35328;&#36890;&#29992;&#35821;&#38899;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#33258;&#22238;&#24402;&#30340;&#27969;&#21305;&#37197;&#27169;&#22411;&#65292;&#22312;&#25991;&#26412;&#21644;&#38899;&#39057;&#19978;&#19979;&#25991;&#26465;&#20214;&#19979;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#23454;&#29616;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#65292;&#22122;&#22768;&#21435;&#38500;&#65292;&#20869;&#23481;&#32534;&#36753;&#65292;&#39118;&#26684;&#36716;&#25442;&#21644;&#22810;&#26679;&#21270;&#30340;&#26679;&#26412;&#29983;&#25104;&#31561;&#22810;&#31181;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;GPT&#21644;DALL-E&#24050;&#32463;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#30740;&#31350;&#30340;&#26041;&#24335;&#12290;&#36825;&#20123;&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#25110;&#22270;&#20687;&#36755;&#20986;&#65292;&#32780;&#19988;&#36824;&#26159;&#36890;&#29992;&#30340;&#65292;&#21487;&#20197;&#35299;&#20915;&#26410;&#34987;&#26126;&#30830;&#25945;&#25480;&#30340;&#20219;&#21153;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#35821;&#38899;&#29983;&#25104;&#27169;&#22411;&#22312;&#35268;&#27169;&#21644;&#20219;&#21153;&#36890;&#29992;&#21270;&#26041;&#38754;&#20173;&#28982;&#27604;&#36739;&#21407;&#22987;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Voicebox&#65292;&#36825;&#26159;&#26368;&#22810;&#21151;&#33021;&#30340;&#38754;&#21521;&#35268;&#27169;&#30340;&#25991;&#26412;&#24341;&#23548;&#29983;&#25104;&#27169;&#22411;&#12290;Voicebox&#26159;&#19968;&#20010;&#38750;&#33258;&#22238;&#24402;&#30340;&#27969;&#21305;&#37197;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#38899;&#39057;&#19978;&#19979;&#25991;&#21644;&#25991;&#26412;&#26465;&#20214;&#19979;&#36827;&#34892;&#35757;&#32451;&#65292;&#29992;50,000&#23567;&#26102;&#30340;&#26410;&#32463;&#36807;&#28388;&#25110;&#22686;&#24378;&#30340;&#35821;&#38899;&#36827;&#34892;&#22635;&#20805;&#12290;&#19982;GPT&#31867;&#20284;&#65292;Voicebox&#21487;&#20197;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#25191;&#34892;&#22810;&#31181;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#20294;&#26356;&#21152;&#28789;&#27963;&#65292;&#22240;&#20026;&#23427;&#36824;&#21487;&#20197;&#23545;&#26410;&#26469;&#30340;&#19978;&#19979;&#25991;&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#12290;Voicebox&#21487;&#20197;&#29992;&#20110;&#21333;&#35821;&#25110;&#36328;&#35821;&#35328;&#38646;&#26679;&#26412;&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#65292;&#22122;&#22768;&#21435;&#38500;&#65292;&#20869;&#23481;&#32534;&#36753;&#65292;&#39118;&#26684;&#36716;&#25442;&#21644;&#22810;&#26679;&#21270;&#30340;&#26679;&#26412;&#29983;&#25104;&#12290;&#29305;&#21035;&#26159;&#65292;Voicebox
&lt;/p&gt;
&lt;p&gt;
Large-scale generative models such as GPT and DALL-E have revolutionized natural language processing and computer vision research. These models not only generate high fidelity text or image outputs, but are also generalists which can solve tasks not explicitly taught. In contrast, speech generative models are still primitive in terms of scale and task generalization. In this paper, we present Voicebox, the most versatile text-guided generative model for speech at scale. Voicebox is a non-autoregressive flow-matching model trained to infill speech, given audio context and text, trained on over 50K hours of speech that are neither filtered nor enhanced. Similar to GPT, Voicebox can perform many different tasks through in-context learning, but is more flexible as it can also condition on future context. Voicebox can be used for mono or cross-lingual zero-shot text-to-speech synthesis, noise removal, content editing, style conversion, and diverse sample generation. In particular, Voicebox 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#29992;&#25143;&#24341;&#23548;&#21709;&#24212;&#20248;&#21270;&#65288;UGRO&#65289;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#26080;&#27880;&#37322;&#30340;&#29992;&#25143;&#27169;&#25311;&#22120;&#65292;&#20197;&#35780;&#20272;&#23545;&#35805;&#21709;&#24212;&#24182;&#20248;&#21270;&#30417;&#30563;&#24335;&#32463;&#36807;&#24494;&#35843;&#30340;&#31471;&#21040;&#31471;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#20379;&#28385;&#24847;&#24230;&#21453;&#39304;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20026;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#23545;&#35805;&#31995;&#32479;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2306.09821</link><description>&lt;p&gt;
&#24320;&#21457;&#29992;&#25143;&#21453;&#39304;&#30340;&#28508;&#21147;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#29992;&#25143;&#27169;&#25311;&#22120;&#20197;&#22686;&#24378;&#23545;&#35805;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Unlocking the Potential of User Feedback: Leveraging Large Language Model as User Simulator to Enhance Dialogue System. (arXiv:2306.09821v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09821
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#29992;&#25143;&#24341;&#23548;&#21709;&#24212;&#20248;&#21270;&#65288;UGRO&#65289;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#26080;&#27880;&#37322;&#30340;&#29992;&#25143;&#27169;&#25311;&#22120;&#65292;&#20197;&#35780;&#20272;&#23545;&#35805;&#21709;&#24212;&#24182;&#20248;&#21270;&#30417;&#30563;&#24335;&#32463;&#36807;&#24494;&#35843;&#30340;&#31471;&#21040;&#31471;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#20379;&#28385;&#24847;&#24230;&#21453;&#39304;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20026;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#23545;&#35805;&#31995;&#32479;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#31995;&#32479;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#26497;&#22823;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#19982;&#36739;&#23567;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#30456;&#27604;&#65292;&#30452;&#25509;&#21033;&#29992;LLMs&#20316;&#20026;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#65288;TOD&#65289;&#27169;&#22411;&#30340;&#24615;&#33021;&#36739;&#24046;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#25215;&#35748;LLMs&#30340;&#37325;&#22823;&#28508;&#21147;&#24182;&#25506;&#32034;&#21033;&#29992;&#23427;&#20204;&#30340;&#24778;&#20154;&#33021;&#21147;&#30340;&#25913;&#36827;&#26041;&#27861;&#38750;&#24120;&#37325;&#35201;&#12290;&#20026;&#20102;&#21033;&#29992;LLMs&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#29992;&#25143;&#24341;&#23548;&#21709;&#24212;&#20248;&#21270;&#65288;UGRO&#65289;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#23558;LLM&#19982;&#36739;&#23567;&#30340;TOD&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;LLM&#20316;&#20026;&#26080;&#27880;&#37322;&#30340;&#29992;&#25143;&#27169;&#25311;&#22120;&#26469;&#35780;&#20272;&#23545;&#35805;&#21709;&#24212;&#65292;&#23558;&#20854;&#19982;&#36739;&#23567;&#30340;&#32463;&#36807;&#24494;&#35843;&#30340;&#31471;&#21040;&#31471;TOD&#27169;&#22411;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#21033;&#29992;LLMs&#29983;&#25104;&#30340;&#28385;&#24847;&#24230;&#21453;&#39304;&#65292;UGRO&#36827;&#19968;&#27493;&#20248;&#21270;&#20102;&#30417;&#30563;&#24335;&#32463;&#36807;&#24494;&#35843;&#30340;TOD&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;TOD&#27169;&#22411;&#20197;&#23545;&#35805;&#21382;&#21490;&#35760;&#24405;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#22312;&#29992;&#25143;&#27169;&#25311;&#22120;&#21453;&#39304;&#30340;&#24110;&#21161;&#19979;&#29983;&#25104;&#31526;&#21512;&#29992;&#25143;&#38656;&#27714;&#30340;&#39640;&#28385;&#24847;&#24230;&#21709;&#24212;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;UGRO&#30340;&#26041;&#27861;&#30456;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20026;&#21033;&#29992;LLMs&#22686;&#24378;&#23545;&#35805;&#31995;&#32479;&#25552;&#20379;&#20102;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue systems and large language models (LLMs) have gained considerable attention. However, the direct utilization of LLMs as task-oriented dialogue (TOD) models has been found to underperform compared to smaller task-specific models. Nonetheless, it is crucial to acknowledge the significant potential of LLMs and explore improved approaches for leveraging their impressive abilities. Motivated by the goal of leveraging LLMs, we propose an alternative approach called User-Guided Response Optimization (UGRO) to combine it with a smaller TOD model. This approach uses LLM as annotation-free user simulator to assess dialogue responses, combining them with smaller fine-tuned end-to-end TOD models. By utilizing the satisfaction feedback generated by LLMs, UGRO further optimizes the supervised fine-tuned TOD model. Specifically, the TOD model takes the dialogue history as input and, with the assistance of the user simulator's feedback, generates high-satisfaction responses that meet the user
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WSPAlign&#30340;&#26080;&#38656;&#25163;&#21160;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;&#35789;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#29992;&#22823;&#35268;&#27169;&#24369;&#30417;&#30563;&#25968;&#25454;&#20013;&#30340;&#36328;&#24230;&#39044;&#27979;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#21462;&#24471;&#20102;&#27604;&#24403;&#21069;&#26368;&#20339;&#26041;&#27861;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.05644</link><description>&lt;p&gt;
WSPAlign: &#22823;&#35268;&#27169;&#24369;&#30417;&#30563;&#36328;&#24230;&#39044;&#27979;&#19979;&#30340;&#35789;&#23545;&#40784;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
WSPAlign: Word Alignment Pre-training via Large-Scale Weakly Supervised Span Prediction. (arXiv:2306.05644v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05644
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WSPAlign&#30340;&#26080;&#38656;&#25163;&#21160;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;&#35789;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#29992;&#22823;&#35268;&#27169;&#24369;&#30417;&#30563;&#25968;&#25454;&#20013;&#30340;&#36328;&#24230;&#39044;&#27979;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#21462;&#24471;&#20102;&#27604;&#24403;&#21069;&#26368;&#20339;&#26041;&#27861;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#35789;&#23545;&#40784;&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#21160;&#23545;&#40784;&#25968;&#25454;&#38598;&#25110;&#24179;&#34892;&#35821;&#26009;&#24211;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#29992;&#24615;&#12290;&#20026;&#20102;&#32531;&#35299;&#23545;&#25163;&#21160;&#25968;&#25454;&#30340;&#20381;&#36182;&#65292;&#25105;&#20204;&#36890;&#36807;&#25918;&#23485;&#23545;&#27491;&#30830;&#12289;&#23436;&#20840;&#23545;&#40784;&#21644;&#24179;&#34892;&#21477;&#23376;&#30340;&#35201;&#27714;&#65292;&#25193;&#22823;&#20102;&#30417;&#30563;&#25968;&#25454;&#30340;&#26469;&#28304;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#24102;&#26377;&#22122;&#22768;&#12289;&#37096;&#20998;&#23545;&#40784;&#21644;&#38750;&#24179;&#34892;&#27573;&#33853;&#20316;&#20026;&#22823;&#35268;&#27169;&#24369;&#30417;&#30563;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#36328;&#24230;&#39044;&#27979;&#23545;&#35789;&#23545;&#40784;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;WSPAlign&#65292;&#26159;&#19968;&#31181;&#26377;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#26080;&#38656;&#25163;&#21160;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;&#35789;&#23545;&#40784;&#26041;&#27861;&#12290;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;fine-tuning&#26102;&#65292;WSPAlign&#22312;F1&#21644;AER&#20004;&#20010;&#25351;&#26631;&#19978;&#30340;&#26368;&#20339;&#30417;&#30563;&#22522;&#32447;&#20998;&#21035;&#25552;&#39640;&#20102;3.3~6.1&#21644;1.5~6.1&#20010;&#28857;&#65292;&#25104;&#20026;&#20102;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;WSPAlign&#22312;&#23569;&#26679;&#26412;&#12289;&#38646;&#26679;&#26412;&#21644;&#36328;&#35821;&#35328;&#27979;&#35797;&#20013;&#20063;&#33719;&#24471;&#20102;&#19982;&#30456;&#24212;&#22522;&#32447;&#30456;&#21516;&#30340;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing word alignment methods rely on manual alignment datasets or parallel corpora, which limits their usefulness. Here, to mitigate the dependence on manual data, we broaden the source of supervision by relaxing the requirement for correct, fully-aligned, and parallel sentences. Specifically, we make noisy, partially aligned, and non-parallel paragraphs. We then use such a large-scale weakly-supervised dataset for word alignment pre-training via span prediction. Extensive experiments with various settings empirically demonstrate that our approach, which is named WSPAlign, is an effective and scalable way to pre-train word aligners without manual data. When fine-tuned on standard benchmarks, WSPAlign has set a new state-of-the-art by improving upon the best-supervised baseline by 3.3~6.1 points in F1 and 1.5~6.1 points in AER. Furthermore, WSPAlign also achieves competitive performance compared with the corresponding baselines in few-shot, zero-shot and cross-lingual tests, whi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23581;&#35797;&#23454;&#29616;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#36816;&#29992;&#21487;&#36870;&#27169;&#22411;&#23454;&#29616;&#39640;&#25928;&#30340;&#24494;&#35843;&#65292;&#24182;&#21457;&#29616;&#22312;&#21021;&#22987;&#21270;&#24494;&#35843;&#26102;&#20445;&#30041;PLM&#30340;&#36215;&#28857;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2306.00477</link><description>&lt;p&gt;
&#20351;&#39044;&#35757;&#32451;&#27169;&#22411;&#20855;&#26377;&#21487;&#36870;&#24615;&#65306;&#20174;&#21442;&#25968;&#21040;&#20869;&#23384;&#39640;&#25928;&#30340;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Make Your Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning. (arXiv:2306.00477v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23581;&#35797;&#23454;&#29616;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#36816;&#29992;&#21487;&#36870;&#27169;&#22411;&#23454;&#29616;&#39640;&#25928;&#30340;&#24494;&#35843;&#65292;&#24182;&#21457;&#29616;&#22312;&#21021;&#22987;&#21270;&#24494;&#35843;&#26102;&#20445;&#30041;PLM&#30340;&#36215;&#28857;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#38750;&#24120;&#25104;&#21151;&#30340;&#26041;&#27861;&#65292;&#21482;&#38656;&#35757;&#32451;&#23569;&#37327;&#21442;&#25968;&#32780;&#19981;&#20250;&#38477;&#20302;&#24615;&#33021;&#65292;&#24182;&#38543;&#30528;PLM&#36234;&#26469;&#36234;&#22823;&#32780;&#25104;&#20026;&#20107;&#23454;&#19978;&#30340;&#23398;&#20064;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;PEFT&#26041;&#27861;&#19981;&#20855;&#22791;&#20869;&#23384;&#25928;&#29575;&#65292;&#22240;&#20026;&#23427;&#20204;&#20173;&#38656;&#35201;&#23384;&#20648;&#22823;&#37096;&#20998;&#20013;&#38388;&#28608;&#27963;&#20540;&#20197;&#20415;&#35745;&#31639;&#26799;&#24230;&#65292;&#31867;&#20284;&#20110;&#24494;&#35843;&#12290;&#19968;&#20010;&#20943;&#23569;&#28608;&#27963;&#20869;&#23384;&#30340;&#26377;&#25928;&#26041;&#27861;&#26159;&#24212;&#29992;&#21487;&#36870;&#27169;&#22411;&#65292;&#36825;&#26679;&#20013;&#38388;&#28608;&#27963;&#20540;&#23601;&#26080;&#38656;&#32531;&#23384;&#65292;&#21487;&#20197;&#37325;&#26032;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#23558;PLM&#20462;&#25913;&#20026;&#23427;&#30340;&#21487;&#36870;&#21464;&#20307;&#24182;&#36827;&#34892;PEFT&#24182;&#19981;&#26159;&#19968;&#20214;&#23481;&#26131;&#30340;&#20107;&#65292;&#22240;&#20026;&#21487;&#36870;&#27169;&#22411;&#20855;&#26377;&#19982;&#24403;&#21069;&#21457;&#24067;&#30340;PLM&#19981;&#21516;&#30340;&#20307;&#31995;&#32467;&#26500;&#12290;&#26412;&#25991;&#39318;&#20808;&#35843;&#26597;&#29616;&#26377;PEFT&#26041;&#27861;&#25104;&#21151;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#35748;&#35782;&#21040;&#22312;&#21021;&#22987;&#21270;PEFT&#26102;&#20445;&#30041;PLM&#30340;&#36215;&#28857;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter-efficient fine-tuning (PEFT) of pre-trained language models (PLMs) has emerged as a highly successful approach, with training only a small number of parameters without sacrificing performance and becoming the de-facto learning paradigm with the increasing size of PLMs. However, existing PEFT methods are not memory-efficient, because they still require caching most of the intermediate activations for the gradient calculation, akin to fine-tuning. One effective way to reduce the activation memory is to apply a reversible model, so the intermediate activations are not necessary to be cached and can be recomputed. Nevertheless, modifying a PLM to its reversible variant with PEFT is not straightforward, since the reversible model has a distinct architecture from the currently released PLMs. In this paper, we first investigate what is a key factor for the success of existing PEFT methods, and realize that it's essential to preserve the PLM's starting point when initializing a PEFT 
&lt;/p&gt;</description></item><item><title>NavGPT&#26159;&#22522;&#20110;LLM&#30340;&#23548;&#33322;&#26234;&#33021;&#20307;&#65292;&#21487;&#20197;&#22312;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#65288;VLN&#65289;&#20013;&#65292;&#36890;&#36807;&#23545;&#25991;&#26412;&#25551;&#36848;&#36827;&#34892;&#25512;&#29702;&#65292;&#25191;&#34892;&#38646;-shot&#36830;&#32493;&#21160;&#20316;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#39640;&#32423;&#35268;&#21010;&#33021;&#21147;&#65292;&#21487;&#20197;&#23558;&#25351;&#20196;&#20998;&#35299;&#25104;&#23376;&#30446;&#26631;&#12289;&#25972;&#21512;&#24120;&#35782;&#30693;&#35782;&#20197;&#36827;&#34892;&#38556;&#30861;&#29289;&#36991;&#20813;&#65292;&#24182;&#21442;&#32771;&#20808;&#21069;&#30340;&#27493;&#39588;&#36827;&#34892;&#28548;&#28165;&#12290;NavGPT&#23637;&#31034;&#20102;&#36890;&#29992;&#20307;&#29616;&#26234;&#33021;&#20307;&#21457;&#23637;&#30340;&#32654;&#22909;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2305.16986</link><description>&lt;p&gt;
NavGPT: &#24102;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#20013;&#30340;&#26174;&#24335;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models. (arXiv:2305.16986v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16986
&lt;/p&gt;
&lt;p&gt;
NavGPT&#26159;&#22522;&#20110;LLM&#30340;&#23548;&#33322;&#26234;&#33021;&#20307;&#65292;&#21487;&#20197;&#22312;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#65288;VLN&#65289;&#20013;&#65292;&#36890;&#36807;&#23545;&#25991;&#26412;&#25551;&#36848;&#36827;&#34892;&#25512;&#29702;&#65292;&#25191;&#34892;&#38646;-shot&#36830;&#32493;&#21160;&#20316;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#39640;&#32423;&#35268;&#21010;&#33021;&#21147;&#65292;&#21487;&#20197;&#23558;&#25351;&#20196;&#20998;&#35299;&#25104;&#23376;&#30446;&#26631;&#12289;&#25972;&#21512;&#24120;&#35782;&#30693;&#35782;&#20197;&#36827;&#34892;&#38556;&#30861;&#29289;&#36991;&#20813;&#65292;&#24182;&#21442;&#32771;&#20808;&#21069;&#30340;&#27493;&#39588;&#36827;&#34892;&#28548;&#28165;&#12290;NavGPT&#23637;&#31034;&#20102;&#36890;&#29992;&#20307;&#29616;&#26234;&#33021;&#20307;&#21457;&#23637;&#30340;&#32654;&#22909;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20363;&#22914;ChatGPT&#21644;GPT-4&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#35268;&#27169;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#27169;&#22411;&#30340;&#25193;&#23637;&#20013;&#23637;&#29616;&#20986;&#26174;&#33879;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#31181;&#36235;&#21183;&#24378;&#35843;&#20102;&#20351;&#29992;&#26080;&#38480;&#35821;&#35328;&#25968;&#25454;&#35757;&#32451;LLM&#30340;&#28508;&#21147;&#65292;&#25512;&#21160;&#20102;&#36890;&#29992;&#20307;&#29616;&#26234;&#33021;&#20307;&#30340;&#21457;&#23637;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;NavGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#32431;&#31929;&#22522;&#20110;LLM&#30340;&#25351;&#20196;&#36319;&#38543;&#23548;&#33322;&#26234;&#33021;&#20307;&#65292;&#36890;&#36807;&#20026;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#65288;VLN&#65289;&#25191;&#34892;&#38646;-shot&#30340;&#36830;&#32493;&#21160;&#20316;&#39044;&#27979;&#65292;&#25581;&#31034;&#20102;&#23545;&#20110;&#22312;&#22797;&#26434;&#30340;&#29616;&#23454;&#22330;&#26223;&#19979;GPT&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#22312;&#27599;&#19968;&#27493;&#20013;&#65292;NavGPT&#23558;&#35270;&#35273;&#35266;&#23519;&#12289;&#23548;&#33322;&#21382;&#21490;&#21644;&#26410;&#26469;&#21487;&#25506;&#32034;&#26041;&#21521;&#30340;&#25991;&#26412;&#25551;&#36848;&#20316;&#20026;&#36755;&#20837;&#65292;&#25512;&#29702;&#20986;&#26234;&#33021;&#20307;&#30340;&#24403;&#21069;&#29366;&#24577;&#65292;&#24182;&#20915;&#23450;&#22914;&#20309;&#25509;&#36817;&#30446;&#26631;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;NavGPT&#21487;&#20197;&#26126;&#30830;&#22320;&#25191;&#34892;&#23548;&#33322;&#30340;&#39640;&#32423;&#35268;&#21010;&#65292;&#21253;&#25324;&#23558;&#25351;&#20196;&#20998;&#35299;&#25104;&#23376;&#30446;&#26631;&#12289;&#25972;&#21512;&#24120;&#35782;&#30693;&#35782;&#20197;&#36827;&#34892;&#38556;&#30861;&#29289;&#36991;&#20813;&#65292;&#24182;&#21442;&#32771;&#20808;&#21069;&#30340;&#27493;&#39588;&#36827;&#34892;&#28548;&#28165;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;LLM&#21487;&#33021;&#25104;&#20026;&#22797;&#26434;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#20256;&#32479;&#27969;&#31243;&#30340;&#24378;&#26377;&#21147;&#26367;&#20195;&#21697;&#65292;&#23637;&#31034;&#20102;&#36890;&#29992;&#20307;&#29616;&#26234;&#33021;&#20307;&#21457;&#23637;&#30340;&#32654;&#22909;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trained with an unprecedented scale of data, large language models (LLMs) like ChatGPT and GPT-4 exhibit the emergence of significant reasoning abilities from model scaling. Such a trend underscored the potential of training LLMs with unlimited language data, advancing the development of a universal embodied agent. In this work, we introduce the NavGPT, a purely LLM-based instruction-following navigation agent, to reveal the reasoning capability of GPT models in complex embodied scenes by performing zero-shot sequential action prediction for vision-and-language navigation (VLN). At each step, NavGPT takes the textual descriptions of visual observations, navigation history, and future explorable directions as inputs to reason the agent's current status, and makes the decision to approach the target. Through comprehensive experiments, we demonstrate NavGPT can explicitly perform high-level planning for navigation, including decomposing instruction into sub-goal, integrating commonsense k
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35789;&#27719;&#20462;&#21098;&#65288;VT&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21024;&#38500;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19981;&#30456;&#20851;&#26631;&#35760;&#65292;&#23558;&#20854;&#21387;&#32553;&#20026;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35789;&#27719;&#20462;&#21098;&#21487;&#20197;&#22312;&#20445;&#25345;&#22810;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#38477;&#20302;&#20102;&#27169;&#22411;&#30340;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2305.15020</link><description>&lt;p&gt;
&#36890;&#36807;&#35789;&#27719;&#20462;&#21098;&#23454;&#29616;&#39640;&#25928;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
An Efficient Multilingual Language Model Compression through Vocabulary Trimming. (arXiv:2305.15020v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15020
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35789;&#27719;&#20462;&#21098;&#65288;VT&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21024;&#38500;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19981;&#30456;&#20851;&#26631;&#35760;&#65292;&#23558;&#20854;&#21387;&#32553;&#20026;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35789;&#27719;&#20462;&#21098;&#21487;&#20197;&#22312;&#20445;&#25345;&#22810;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#38477;&#20302;&#20102;&#27169;&#22411;&#30340;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#24050;&#32463;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#38750;&#33521;&#35821;&#35821;&#35328;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28085;&#30422;&#19981;&#21516;&#35821;&#35328;&#26631;&#35760;&#30340;&#35789;&#27719;&#23884;&#20837;&#30697;&#38453;&#36739;&#22823;&#65292;&#22810;&#35821;&#35328;LM&#30340;&#27169;&#22411;&#21442;&#25968;&#20173;&#28982;&#24456;&#22823;&#12290;&#30456;&#21453;&#65292;&#21333;&#19968;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#29305;&#23450;&#20110;&#35821;&#35328;&#30340;&#35789;&#27719;&#22312;&#30446;&#26631;&#35821;&#35328;&#20013;&#35757;&#32451;&#65292;&#20294;&#36825;&#38656;&#35201;&#22823;&#37327;&#39044;&#31639;&#21644;&#21487;&#38752;&#35821;&#26009;&#24211;&#25165;&#33021;&#20174;&#22836;&#24320;&#22987;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35789;&#27719;&#20462;&#21098;&#65288;VT&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#35789;&#27719;&#20013;&#21024;&#38500;&#19981;&#30456;&#20851;&#30340;&#26631;&#35760;&#65292;&#23558;&#22810;&#35821;&#35328;LM&#30340;&#35789;&#27719;&#20943;&#23569;&#21040;&#30446;&#26631;&#35821;&#35328;&#12290;&#29702;&#35770;&#19978;&#65292;VT&#21487;&#20197;&#21387;&#32553;&#20219;&#20309;&#29616;&#26377;&#30340;&#22810;&#35821;&#35328;LM&#65292;&#20197;&#22312;&#22810;&#35821;&#35328;LM&#28085;&#30422;&#30340;&#20219;&#20309;&#35821;&#35328;&#20013;&#26500;&#24314;&#21333;&#19968;&#35821;&#35328;&#27169;&#22411;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;VT&#21487;&#20197;&#20445;&#30041;&#22810;&#35821;&#35328;LM&#30340;&#21407;&#22987;&#24615;&#33021;&#65292;&#21516;&#26102;&#23610;&#23544;&#26356;&#23567;&#65288;&#36890;&#24120;&#21482;&#38656;&#21407;&#22987;&#35789;&#27719;&#22823;&#23567;&#30340;&#32422;50&#65285;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual language model (LM) have become a powerful tool in NLP especially for non-English languages. Nevertheless, model parameters of multilingual LMs remain large due to the larger embedding matrix of the vocabulary covering tokens in different languages. On the contrary, monolingual LMs can be trained in a target language with the language-specific vocabulary only, but this requires a large budget and availability of reliable corpora to achieve a high-quality LM from scratch. In this paper, we propose vocabulary-trimming (VT), a method to reduce a multilingual LM vocabulary to a target language by deleting irrelevant tokens from its vocabulary. In theory, VT can compress any existing multilingual LM to build monolingual LMs in any language covered by the multilingual LM. In our experiments, we show that VT can retain the original performance of the multilingual LM, while being smaller in size (in general around 50% of the original vocabulary size is enough) than the original mu
&lt;/p&gt;</description></item><item><title>RefGPT&#26159;&#19968;&#31181;&#22522;&#20110;&#21442;&#32771;&#30340;&#23545;&#35805;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#22823;&#37327;&#30495;&#23454;&#19988;&#23450;&#21046;&#21270;&#30340;&#23545;&#35805;&#65292;&#24182;&#35299;&#20915;&#20102;&#23545;&#35805;&#29983;&#25104;&#20013;&#30340;&#27169;&#22411;&#24187;&#35273;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.14994</link><description>&lt;p&gt;
RefGPT: GPT&#27169;&#22411;&#20013;&#22522;&#20110;&#21442;&#32771;&#30340;&#30495;&#23454;&#19988;&#21487;&#23398;&#20064;&#21270;&#30340;&#23545;&#35805;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
RefGPT: Reference -&gt; Truthful &amp; Customized Dialogues Generation by GPTs and for GPTs. (arXiv:2305.14994v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14994
&lt;/p&gt;
&lt;p&gt;
RefGPT&#26159;&#19968;&#31181;&#22522;&#20110;&#21442;&#32771;&#30340;&#23545;&#35805;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#22823;&#37327;&#30495;&#23454;&#19988;&#23450;&#21046;&#21270;&#30340;&#23545;&#35805;&#65292;&#24182;&#35299;&#20915;&#20102;&#23545;&#35805;&#29983;&#25104;&#20013;&#30340;&#27169;&#22411;&#24187;&#35273;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#31561;&#36890;&#29992;&#30340;&#32842;&#22825;&#27169;&#22411;&#24050;&#32463;&#36890;&#36807;&#20351;&#29992;&#39640;&#36136;&#37327;&#25351;&#20196;&#25968;&#25454;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#35299;&#20915;&#21508;&#31181;NLP&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#20154;&#31867;&#32534;&#20889;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#65292;&#23588;&#20854;&#26159;&#22810;&#36718;&#23545;&#35805;&#65292;&#23545;&#22823;&#22810;&#25968;&#20154;&#26469;&#35828;&#26159;&#26114;&#36149;&#19988;&#38590;&#20197;&#23454;&#29616;&#30340;&#12290;&#23613;&#31649;&#20197;&#24448;&#30340;&#30740;&#31350;&#24050;&#32463;&#20351;&#29992;&#20102;&#24378;&#22823;&#30340;LLMs&#26469;&#33258;&#21160;&#29983;&#25104;&#23545;&#35805;&#65292;&#20294;&#30001;&#20110;LLMs&#23384;&#22312;&#24187;&#35273;&#65292;&#36825;&#20123;&#23545;&#35805;&#37117;&#26080;&#27861;&#23436;&#20840;&#30495;&#23454;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RefGPT&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#22823;&#37327;&#30495;&#23454;&#19988;&#23450;&#21046;&#21270;&#30340;&#23545;&#35805;&#65292;&#32780;&#26080;&#38656;&#25285;&#24515;&#27169;&#22411;&#24187;&#35273;&#36896;&#25104;&#30340;&#20107;&#23454;&#38169;&#35823;&#12290;RefGPT&#36890;&#36807;&#38480;&#21046;LLMs&#20351;&#29992;&#32473;&#23450;&#21442;&#32771;&#32780;&#19981;&#26159;&#22238;&#24518;&#33258;&#24049;&#30340;&#30693;&#35782;&#26469;&#29983;&#25104;&#23545;&#35805;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#23545;&#35805;&#29983;&#25104;&#20013;&#30340;&#27169;&#22411;&#24187;&#35273;&#12290;&#27492;&#22806;&#65292;RefGPT&#23545;&#27599;&#20010;&#35805;&#35821;&#37117;&#28155;&#21152;&#20102;&#35814;&#32454;&#30340;&#25511;&#21046;&#65292;&#20351;&#20854;&#20855;&#26377;&#39640;&#24230;&#23450;&#21046;&#21270;&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;&#20197;&#24448;&#30740;&#31350;&#25152;&#24573;&#30053;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
General chat models, like ChatGPT, have attained impressive capability to resolve a wide range of NLP tasks by tuning Large Language Models (LLMs) with high-quality instruction data. However, collecting human-written high-quality data, especially multi-turn dialogues, is expensive and unattainable for most people. Though previous studies have used powerful LLMs to generate the dialogues automatically, but they all suffer from generating untruthful dialogues because of the LLMs hallucination. Therefore, we propose a method called RefGPT to generate enormous truthful and customized dialogues without worrying about factual errors caused by the model hallucination. RefGPT solves the model hallucination in dialogue generation by restricting the LLMs to leverage the given reference instead of reciting their own knowledge to generate dialogues. Additionally, RefGPT adds detailed controls on every utterances to enable highly customization capability, which previous studies have ignored. On the
&lt;/p&gt;</description></item><item><title>BeamSearchQA&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36845;&#20195;&#24335;&#29983;&#25104;&#38382;&#39064;&#65292;&#20197;&#25429;&#25417;&#38544;&#21547;&#30693;&#35782;&#24182;&#20248;&#21270;&#38382;&#31572;&#36807;&#31243;&#65292;&#22312;NQ&#21644;WebQ&#27979;&#35797;&#38598;&#19978;&#20998;&#21035;&#36798;&#21040;&#20102;71.7&#65285;&#21644;46.7&#65285;&#30340;F1&#20998;&#25968;&#65292;&#26174;&#30528;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.14766</link><description>&lt;p&gt;
BeamSearchQA: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#24378;&#22823;&#30340;&#38646;-shot QA&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
BeamSearchQA: Large Language Models are Strong Zero-Shot QA Solver. (arXiv:2305.14766v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14766
&lt;/p&gt;
&lt;p&gt;
BeamSearchQA&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36845;&#20195;&#24335;&#29983;&#25104;&#38382;&#39064;&#65292;&#20197;&#25429;&#25417;&#38544;&#21547;&#30693;&#35782;&#24182;&#20248;&#21270;&#38382;&#31572;&#36807;&#31243;&#65292;&#22312;NQ&#21644;WebQ&#27979;&#35797;&#38598;&#19978;&#20998;&#21035;&#36798;&#21040;&#20102;71.7&#65285;&#21644;46.7&#65285;&#30340;F1&#20998;&#25968;&#65292;&#26174;&#30528;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#39046;&#22495;&#30340;&#38382;&#31572;&#26159;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#36890;&#24120;&#38656;&#35201;&#35775;&#38382;&#22806;&#37096;&#20449;&#24687;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#21333;&#36718;&#26816;&#32034;-&#38405;&#35835;&#26041;&#27861;&#65292;&#39318;&#20808;&#26816;&#32034;&#30456;&#20851;&#25991;&#26723;&#65292;&#28982;&#21518;&#22522;&#20110;&#26816;&#32034;&#30340;&#20449;&#24687;&#22238;&#31572;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#22238;&#31572;&#38382;&#39064;&#38656;&#35201;&#38544;&#21547;&#30340;&#30693;&#35782;&#65292;&#36825;&#20123;&#30693;&#35782;&#19981;&#30452;&#25509;&#20174;&#38382;&#39064;&#26412;&#36523;&#20013;&#33719;&#24471;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38382;&#31572;&#27969;&#31243;&#65292;&#31216;&#20026;BeamSearchQA&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36845;&#20195;&#29983;&#25104;&#20851;&#20110;&#21407;&#22987;&#38382;&#39064;&#30340;&#26032;&#38382;&#39064;&#65292;&#23454;&#29616;&#36845;&#20195;&#25512;&#29702;&#36807;&#31243;&#12290;&#36890;&#36807;&#36845;&#20195;&#32454;&#21270;&#21644;&#25193;&#23637;&#38382;&#39064;&#30340;&#33539;&#22260;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#25429;&#25417;&#24182;&#21033;&#29992;&#21487;&#33021;&#26080;&#27861;&#36890;&#36807;&#26816;&#32034;&#30452;&#25509;&#33719;&#21462;&#30340;&#38544;&#34255;&#30693;&#35782;&#12290;&#25105;&#20204;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;&#24320;&#25918;&#39046;&#22495;NQ&#21644;WebQ&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;BeamSearchQA&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#22312;NQ&#21644;WebQ&#27979;&#35797;&#38598;&#19978;&#20998;&#21035;&#36798;&#21040;&#20102;71.7&#65285;&#21644;46.7&#65285;&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open-domain question answering is a crucial task that often requires accessing external information. Existing methods typically adopt a single-turn retrieve-then-read approach, where relevant documents are first retrieved, and questions are then answered based on the retrieved information. However, there are cases where answering a question requires implicit knowledge that is not directly retrievable from the question itself. In this work, we propose a novel question-answering pipeline called eamSearchQA. Our approach leverages large language models(LLMs) to iteratively generate new questions about the original question, enabling an iterative reasoning process. By iteratively refining and expanding the scope of the question, our method aims to capture and utilize hidden knowledge that may not be directly obtainable through retrieval. We evaluate our approach on the widely-used open-domain NQ and WebQ datasets. The experimental results demonstrate that BeamSearchQA significantly outperf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22320;&#32536;&#25919;&#27835;&#20559;&#35265;&#30340;&#27010;&#24565;&#65292;&#24182;&#20197;&#39046;&#22303;&#20105;&#31471;&#20026;&#20363;&#65292;&#21033;&#29992;&#22810;&#35821;&#35328;&#12289;&#22810;&#36873;&#39064;&#30340;&#25968;&#25454;&#38598;BorderLines&#21644;&#20960;&#20010;&#23450;&#37327;&#25351;&#26631;&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;&#21709;&#24212;&#20013;&#30340;&#22320;&#32536;&#25919;&#27835;&#20559;&#35265;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2305.14610</link><description>&lt;p&gt;
&#36825;&#29255;&#22303;&#22320;&#26159;&#20320;&#25105;&#30340;&#22303;&#22320;&#65306;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22320;&#32536;&#25919;&#27835;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
This Land is {Your, My} Land: Evaluating Geopolitical Biases in Language Models. (arXiv:2305.14610v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22320;&#32536;&#25919;&#27835;&#20559;&#35265;&#30340;&#27010;&#24565;&#65292;&#24182;&#20197;&#39046;&#22303;&#20105;&#31471;&#20026;&#20363;&#65292;&#21033;&#29992;&#22810;&#35821;&#35328;&#12289;&#22810;&#36873;&#39064;&#30340;&#25968;&#25454;&#38598;BorderLines&#21644;&#20960;&#20010;&#23450;&#37327;&#25351;&#26631;&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;&#21709;&#24212;&#20013;&#30340;&#22320;&#32536;&#25919;&#27835;&#20559;&#35265;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#22320;&#32536;&#25919;&#27835;&#20559;&#35265;&#30340;&#27010;&#24565;&#8212;&#8212;&#21363;&#26681;&#25454;&#35821;&#35328;&#29615;&#22659;&#25253;&#36947;&#19981;&#21516;&#30340;&#22320;&#32536;&#25919;&#27835;&#30693;&#35782;&#30340;&#20542;&#21521;&#12290;&#25105;&#20204;&#20197;&#39046;&#22303;&#20105;&#31471;&#20026;&#26696;&#20363;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#20363;&#22914;&#65292;&#23545;&#20110;&#34987;&#24191;&#27867;&#20105;&#35758;&#30340;&#21335;&#27801;&#32676;&#23707;&#65292;&#22914;&#26524;&#29992;&#20013;&#25991;&#38382;&#65292;LM&#26159;&#21542;&#26356;&#26377;&#21487;&#33021;&#35828;&#23427;&#20204;&#23646;&#20110;&#20013;&#22269;&#65292;&#32780;&#22914;&#26524;&#29992;&#22612;&#21152;&#27931;&#35821;&#38382;&#65292;&#21017;&#26356;&#26377;&#21487;&#33021;&#35828;&#23427;&#20204;&#23646;&#20110;&#33778;&#24459;&#23486;&#65311;&#20026;&#20102;&#35780;&#20272;&#26159;&#21542;&#23384;&#22312;&#36825;&#31181;&#20559;&#35265;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#32500;&#22522;&#30334;&#31185;&#19978;&#25910;&#38598;&#20102;&#19968;&#32452;&#39046;&#22303;&#20105;&#31471;&#25968;&#25454;&#65292;&#28982;&#21518;&#23558;&#27599;&#20010;&#39046;&#22303;&#19982;&#19968;&#32452;&#22810;&#35821;&#35328;&#12289;&#22810;&#36873;&#39064;&#32852;&#31995;&#36215;&#26469;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#34987;&#31216;&#20026;BorderLines&#65292;&#23427;&#21253;&#25324;250&#20010;&#39046;&#22303;&#21644;45&#31181;&#35821;&#35328;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#38382;&#39064;&#38598;&#25552;&#20132;&#32473;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#20960;&#20010;&#25552;&#20986;&#30340;&#23450;&#37327;&#25351;&#26631;&#20998;&#26512;&#23427;&#20204;&#30340;&#21709;&#24212;&#20013;&#22320;&#32536;&#25919;&#27835;&#20559;&#35265;&#12290;&#36825;&#20123;&#25351;&#26631;&#27604;&#36739;&#19981;&#21516;&#35821;&#35328;&#30340;&#22238;&#31572;&#20197;&#21450;&#23454;&#38469;&#30340;&#22320;&#32536;&#25919;&#27835;&#24773;&#20917;&#12290;&#22320;&#32536;&#25919;&#27835;&#20559;&#35265;&#29616;&#35937;&#26159;&#19968;&#31181;&#29420;&#29305;&#30340;&#36328;&#35821;&#35328;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the notion of geopolitical bias -- a tendency to report different geopolitical knowledge depending on the linguistic context. As a case study, we consider territorial disputes between countries. For example, for the widely contested Spratly Islands, would an LM be more likely to say they belong to China if asked in Chinese, vs. to the Philippines if asked in Tagalog? To evaluate if such biases exist, we first collect a dataset of territorial disputes from Wikipedia, then associate each territory with a set of multilingual, multiple-choice questions. This dataset, termed BorderLines, consists of 250 territories with questions in 45 languages. We pose these question sets to language models, and analyze geopolitical bias in their responses through several proposed quantitative metrics. The metrics compare between responses in different question languages as well as to the actual geopolitical situation. The phenomenon of geopolitical bias is a uniquely cross-lingual evaluation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20266;&#20195;&#30721;&#25351;&#20196;&#25552;&#31034;&#33021;&#21542;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23454;&#39564;&#35777;&#26126;&#20351;&#29992;&#20266;&#20195;&#30721;&#25552;&#31034;&#21487;&#20197;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#25552;&#39640;7-16&#20998;&#65292;&#24182;&#30456;&#23545;&#25913;&#21892;12-38%&#12290;</title><link>http://arxiv.org/abs/2305.11790</link><description>&lt;p&gt;
&#20266;&#20195;&#30721;&#25351;&#20196;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Prompting with Pseudo-Code Instructions. (arXiv:2305.11790v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11790
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20266;&#20195;&#30721;&#25351;&#20196;&#25552;&#31034;&#33021;&#21542;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23454;&#39564;&#35777;&#26126;&#20351;&#29992;&#20266;&#20195;&#30721;&#25552;&#31034;&#21487;&#20197;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#25552;&#39640;7-16&#20998;&#65292;&#24182;&#30456;&#23545;&#25913;&#21892;12-38%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#25552;&#31034;&#24050;&#25104;&#20026;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#12290;&#37492;&#20110;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#22266;&#26377;&#27495;&#20041;&#65292;&#22240;&#27492;&#32771;&#34385;&#20351;&#29992;&#26356;&#23569;&#27495;&#20041;&#30340;&#25552;&#31034;&#26679;&#24335;&#65292;&#22914;&#20266;&#20195;&#30721;&#25552;&#31034;&#65292;&#21487;&#33021;&#20855;&#26377;&#20248;&#21183;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#36890;&#36807;&#20266;&#20195;&#30721;&#25351;&#20196;&#25552;&#31034;&#26159;&#21542;&#26377;&#21161;&#20110;&#25913;&#21892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25163;&#21160;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;Super-NaturalInstructions&#25968;&#25454;&#38598;&#30340;132&#20010;&#19981;&#21516;&#20219;&#21153;&#30340;&#20266;&#20195;&#30721;&#25552;&#31034;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20998;&#31867;&#12289;QA&#21644;&#29983;&#25104;&#35821;&#35328;&#20219;&#21153;&#12290;&#20351;&#29992;&#36825;&#20123;&#20266;&#20195;&#30721;&#25552;&#31034;&#20197;&#21450;&#23427;&#20204;&#30340;&#33258;&#28982;&#35821;&#35328;&#23545;&#24212;&#29289;&#65292;&#22312;&#20004;&#20010;LLM&#23478;&#26063;-BLOOM&#21644;CodeGen&#19978;&#30740;&#31350;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#20266;&#20195;&#30721;&#25351;&#20196;&#25552;&#31034;&#20250;&#24102;&#26469;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#23545;&#20110;&#20998;&#31867;&#20219;&#21153;&#65292;F1&#20998;&#25968;&#24179;&#22343;&#22686;&#21152;&#65288;&#32477;&#23545;&#20540;&#65289;7-16&#20998;&#65292;&#30456;&#23545;&#25913;&#21892;12-38%&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompting with natural language instructions has recently emerged as a popular method of harnessing the capabilities of large language models. Given the inherent ambiguity present in natural language, it is intuitive to consider the possible advantages of prompting with less ambiguous prompt styles, such as the use of pseudo-code.  In this paper we explore if prompting via pseudo-code instructions helps improve the performance of pre-trained language models. We manually create a dataset of pseudo-code prompts for 132 different tasks spanning classification, QA and generative language tasks, sourced from the Super-NaturalInstructions dataset. Using these prompts along with their counterparts in natural language, we study their performance on two LLM families - BLOOM and CodeGen. Our experiments show that using pseudo-code instructions leads to better results, with an average increase (absolute) of 7-16 points in F1 scores for classification tasks and an improvement (relative) of 12-38% 
&lt;/p&gt;</description></item><item><title>TrueTeacher&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26469;&#36827;&#34892;&#20107;&#23454;&#19968;&#33268;&#24615;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;TrueTeacher&#19981;&#20381;&#36182;&#20110;&#20154;&#24037;&#32534;&#20889;&#30340;&#25688;&#35201;&#65292;&#22810;&#35821;&#35328;&#29305;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11171</link><description>&lt;p&gt;
TrueTeacher: &#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#20107;&#23454;&#19968;&#33268;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
TrueTeacher: Learning Factual Consistency Evaluation with Large Language Models. (arXiv:2305.11171v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11171
&lt;/p&gt;
&lt;p&gt;
TrueTeacher&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26469;&#36827;&#34892;&#20107;&#23454;&#19968;&#33268;&#24615;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;TrueTeacher&#19981;&#20381;&#36182;&#20110;&#20154;&#24037;&#32534;&#20889;&#30340;&#25688;&#35201;&#65292;&#22810;&#35821;&#35328;&#29305;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#23454;&#19968;&#33268;&#24615;&#35780;&#20272;&#36890;&#24120;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#27169;&#22411;&#36827;&#34892;&#65292;&#28982;&#32780;&#36825;&#20123;&#27169;&#22411;&#22312;&#35780;&#20272;&#25688;&#35201;&#26102;&#21462;&#24471;&#30340;&#25104;&#21151;&#26377;&#38480;&#12290;&#20197;&#24448;&#30340;&#24037;&#20316;&#36890;&#36807;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#25913;&#36827;&#20102;&#27492;&#31867;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#36890;&#24120;&#22522;&#20110;&#25200;&#21160;&#30340;&#20154;&#24037;&#32534;&#20889;&#25688;&#35201;&#65292;&#19982;&#30495;&#23454;&#30340;&#27169;&#22411;&#29983;&#25104;&#25688;&#35201;&#22312;&#29305;&#24615;&#19978;&#24120;&#24120;&#23384;&#22312;&#24046;&#24322;&#65292;&#24182;&#19988;&#23545;&#21487;&#33021;&#23384;&#22312;&#30340;&#20107;&#23454;&#38169;&#35823;&#30340;&#35206;&#30422;&#33539;&#22260;&#26377;&#38480;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26368;&#36817;&#22312;&#30452;&#25509;&#35780;&#20272;&#29983;&#25104;&#20219;&#21153;&#26041;&#38754;&#26174;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#35745;&#31639;&#24320;&#38144;&#36807;&#22823;&#65292;&#26080;&#27861;&#23454;&#38469;&#24212;&#29992;&#12290;&#20986;&#20110;&#23545;&#36825;&#20123;&#38480;&#21046;&#30340;&#21160;&#26426;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;TrueTeacher&#65292;&#19968;&#31181;&#20351;&#29992;LLM&#27880;&#37322;&#22810;&#26679;&#30340;&#27169;&#22411;&#29983;&#25104;&#25688;&#35201;&#26469;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;TrueTeacher&#19981;&#20381;&#36182;&#20110;&#20154;&#24037;&#32534;&#20889;&#30340;&#25688;&#35201;&#65292;&#24182;&#19988;&#20855;&#26377;&#22810;&#35821;&#35328;&#29305;&#24615;&#12290;&#22312;TRUE&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#35757;&#32451;&#30340;&#23398;&#29983;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#36828;&#36828;&#36229;&#36807;&#20102;NLI&#27169;&#22411;&#21644;&#20043;&#21069;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Factual consistency evaluation is often conducted using Natural Language Inference (NLI) models, yet these models exhibit limited success in evaluating summaries. Previous work improved such models with synthetic training data. However, the data is typically based on perturbed human-written summaries, which often differ in their characteristics from real model-generated summaries and have limited coverage of possible factual errors. Alternatively, large language models (LLMs) have recently shown promising results in directly evaluating generative tasks, but are too computationally expensive for practical use. Motivated by these limitations, we introduce TrueTeacher, a method for generating synthetic data by annotating diverse model-generated summaries using a LLM. Unlike prior work, TrueTeacher does not rely on human-written summaries, and is multilingual by nature. Experiments on the TRUE benchmark show that a student model trained using our data, substantially outperforms both the st
&lt;/p&gt;</description></item><item><title>CooK&#26159;&#19968;&#31181;&#29992;&#20110;&#36171;&#33021;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#36890;&#36807;&#19987;&#38376;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#21327;&#20316;&#30340;&#30693;&#35782;&#36129;&#29486;&#32773;&#65292;&#25552;&#20379;&#27169;&#22359;&#21270;&#12289;&#19981;&#26029;&#22686;&#38271;&#21644;&#22810;&#28304;&#30340;&#30693;&#35782;&#12290;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#65292;CooK&#23637;&#29616;&#20986;&#20102;&#26126;&#26174;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.09955</link><description>&lt;p&gt;
CooK: &#29992;&#27169;&#22359;&#21270;&#21644;&#21327;&#20316;&#30693;&#35782;&#36171;&#33021;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CooK: Empowering General-Purpose Language Models with Modular and Collaborative Knowledge. (arXiv:2305.09955v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09955
&lt;/p&gt;
&lt;p&gt;
CooK&#26159;&#19968;&#31181;&#29992;&#20110;&#36171;&#33021;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#36890;&#36807;&#19987;&#38376;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#21327;&#20316;&#30340;&#30693;&#35782;&#36129;&#29486;&#32773;&#65292;&#25552;&#20379;&#27169;&#22359;&#21270;&#12289;&#19981;&#26029;&#22686;&#38271;&#21644;&#22810;&#28304;&#30340;&#30693;&#35782;&#12290;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#65292;CooK&#23637;&#29616;&#20986;&#20102;&#26126;&#26174;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#21644;&#35821;&#22659;&#20013;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#26816;&#32034;&#25110;&#29983;&#25104;&#30693;&#35782;&#25552;&#31034;&#26469;&#25913;&#21892;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#26410;&#33021;&#21453;&#26144;&#30693;&#35782;&#20016;&#23500;&#27169;&#22411;&#30340;&#20004;&#20010;&#20851;&#38190;&#23646;&#24615;&#65306;&#30693;&#35782;&#24212;&#35813;&#26159;&#27169;&#22359;&#21270;&#65292;&#19981;&#26029;&#22686;&#38271;&#65292;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#65307;&#30693;&#35782;&#33719;&#21462;&#21644;&#29983;&#25104;&#24212;&#35813;&#26159;&#21327;&#20316;&#30340;&#36807;&#31243;&#65292;&#20854;&#20013;&#21508;&#31181;&#21033;&#30410;&#30456;&#20851;&#32773; contribue &#26032;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; CooK&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21487;&#20026;&#36890;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#27169;&#22359;&#21270;&#21644;&#21327;&#20316;&#26469;&#28304;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#19987;&#38376;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21363;&#22312;&#24191;&#27867;&#39046;&#22495;&#21644;&#26469;&#28304;&#19978;&#35757;&#32451;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#12290;&#36825;&#20123;&#19987;&#38376;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#21442;&#25968;&#21270;&#30340;&#30693;&#35782;&#24211;&#65292;&#21518;&#26469;&#34987;&#25552;&#31034;&#29983;&#25104;&#36890;&#29992;&#30340; LLM &#30340;&#32972;&#26223;&#30693;&#35782;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#30693;&#35782;&#36807;&#28388;&#22120;&#65292;&#20197;&#21160;&#24577;&#36873;&#25321;&#36866;&#21512;&#32473;&#23450;&#19978;&#19979;&#25991;&#30340;&#30693;&#35782;&#28304;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21576;&#29616;&#20102;&#19968;&#20010;&#30693;&#35782;&#36129;&#29486;&#32773;&#32452;&#20214;&#65292;&#20351;&#21033;&#30410;&#30456;&#20851;&#32773;&#33021;&#22815;&#36731;&#26494;&#22320;&#20026;&#31995;&#32479;&#36129;&#29486;&#29305;&#23450;&#20110;&#22495;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; CooK &#22312;&#19968;&#32452;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#26174;&#31034;&#20986;&#26126;&#26174;&#30340;&#36229;&#36234;&#29616;&#26377;&#25216;&#26415;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are increasingly adopted for knowledge-intensive tasks and contexts. Existing approaches improve the knowledge capabilities of general-purpose LLMs through retrieval or generated knowledge prompting, but they fall short of reflecting two key properties of knowledge-rich models: knowledge should be modular, ever-growing, sourced from diverse domains; knowledge acquisition and production should be a collaborative process, where diverse stakeholders contribute new information. To this end, we propose CooK, a novel framework to empower general-purpose large language models with modular and collaboratively sourced knowledge. We first introduce specialized language models, autoregressive models trained on corpora from a wide range of domains and sources. These specialized LMs serve as parametric knowledge repositories that are later prompted to generate background knowledge for general-purpose LLMs. We then propose three knowledge filters to dynamically select an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#31574;&#30053;&#65292;&#22312;&#34507;&#30333;&#36136;&#24314;&#27169;&#20013;&#24212;&#29992;transformer&#21644;&#22270;&#21367;&#31215;&#30340;&#32467;&#26500;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#36827;&#19968;&#27493;&#35757;&#32451;&#21518;&#33021;&#22815;&#35774;&#35745;&#20855;&#26377;&#29305;&#23450;&#24615;&#36136;&#30340;&#34507;&#30333;&#36136;&#65292;&#26696;&#20363;&#39564;&#35777;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#29983;&#25104;&#29702;&#24819;&#30446;&#26631;&#24615;&#36136;&#30340;&#34507;&#30333;&#36136;&#12290;</title><link>http://arxiv.org/abs/2305.04934</link><description>&lt;p&gt;
&#24212;&#29992;&#22522;&#20110;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#33258;&#22238;&#24402;Transformer&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#20998;&#26512;&#21644;&#21457;&#29616;&#26032;&#22411;&#34507;&#30333;&#36136;
&lt;/p&gt;
&lt;p&gt;
Generative Pretrained Autoregressive Transformer Graph Neural Network applied to the Analysis and Discovery of Novel Proteins. (arXiv:2305.04934v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#31574;&#30053;&#65292;&#22312;&#34507;&#30333;&#36136;&#24314;&#27169;&#20013;&#24212;&#29992;transformer&#21644;&#22270;&#21367;&#31215;&#30340;&#32467;&#26500;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#36827;&#19968;&#27493;&#35757;&#32451;&#21518;&#33021;&#22815;&#35774;&#35745;&#20855;&#26377;&#29305;&#23450;&#24615;&#36136;&#30340;&#34507;&#30333;&#36136;&#65292;&#26696;&#20363;&#39564;&#35777;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#29983;&#25104;&#29702;&#24819;&#30446;&#26631;&#24615;&#36136;&#30340;&#34507;&#30333;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25253;&#36947;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#31574;&#30053;&#65292;&#24212;&#29992;&#20110;&#35299;&#20915;&#34507;&#30333;&#36136;&#24314;&#27169;&#20013;&#30340;&#27491;&#21521;&#21644;&#21453;&#21521;&#38382;&#39064;&#65292;&#20351;&#29992;&#19968;&#20010;&#25972;&#21512;&#20102;transformer&#21644;&#22270;&#21367;&#31215;&#30340;&#27880;&#24847;&#21147;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#22312;&#22240;&#26524;&#22810;&#22836;&#22270;&#26426;&#21046;&#20013;&#23454;&#29616;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#34987;&#29992;&#20110;&#39044;&#27979;&#20108;&#32423;&#32467;&#26500;&#20869;&#23481;&#65288;&#27599;&#20010;&#27531;&#22522;&#30340;&#27700;&#24179;&#21644;&#24635;&#20307;&#20869;&#23481;&#65289;&#12289;&#34507;&#30333;&#36136;&#21487;&#28342;&#24615;&#21644;&#27979;&#24207;&#20219;&#21153;&#12290;&#36827;&#19968;&#27493;&#22312;&#21453;&#21521;&#20219;&#21153;&#19978;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#35774;&#35745;&#20855;&#26377;&#36825;&#20123;&#24615;&#36136;&#20316;&#20026;&#30446;&#26631;&#29305;&#24449;&#30340;&#34507;&#30333;&#36136;&#12290;&#35813;&#27169;&#22411;&#34987;&#21046;&#23450;&#20026;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#23436;&#20840;&#22522;&#20110;&#25552;&#31034;&#65292;&#21487;&#20197;&#20026;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#36866;&#24212;&#12290;&#25105;&#20204;&#21457;&#29616;&#28155;&#21152;&#39069;&#22806;&#20219;&#21153;&#20250;&#20135;&#29983;&#30456;&#20114;&#21327;&#21516;&#20316;&#29992;&#65292;&#20351;&#27169;&#22411;&#22312;&#25972;&#20307;&#24615;&#33021;&#19978;&#24471;&#21040;&#25552;&#39640;&#65292;&#36229;&#36807;&#20165;&#22312;&#27599;&#20010;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#27169;&#22411;&#30340;&#21487;&#33021;&#24615;&#12290;&#26696;&#20363;&#30740;&#31350;&#29992;&#20110;&#39564;&#35777;&#35813;&#26041;&#27861;&#65292;&#29983;&#25104;&#20855;&#26377;&#29702;&#24819;&#30446;&#26631;&#24615;&#36136;&#65292;&#21253;&#25324;&#31283;&#23450;&#24615;&#21644;&#21487;&#28342;&#24615;&#30340;&#34507;&#30333;&#36136;&#65292;&#24182;&#36827;&#34892;&#23454;&#39564;&#24615;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We report a flexible language-model based deep learning strategy, applied here to solve complex forward and inverse problems in protein modeling, based on an attention neural network that integrates transformer and graph convolutional architectures in a causal multi-headed graph mechanism, to realize a generative pretrained model. The model is applied to predict secondary structure content (per-residue level and overall content), protein solubility, and sequencing tasks. Further trained on inverse tasks, the model is rendered capable of designing proteins with these properties as target features. The model is formulated as a general framework, completely prompt-based, and can be adapted for a variety of downstream tasks. We find that adding additional tasks yields emergent synergies that the model exploits in improving overall performance, beyond what would be possible by training a model on each dataset alone. Case studies are presented to validate the method, yielding protein designs
&lt;/p&gt;</description></item><item><title>&#22312;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#21644; beam search &#30340;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#25913;&#36827;&#25552;&#31034;&#65292;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.03495</link><description>&lt;p&gt;
&#22522;&#20110;&#8220;&#26799;&#24230;&#19979;&#38477;&#8221;&#19982; beam search &#30340;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Automatic Prompt Optimization with "Gradient Descent" and Beam Search. (arXiv:2305.03495v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03495
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#21644; beam search &#30340;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#25913;&#36827;&#25552;&#31034;&#65292;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#36890;&#29992;&#26234;&#33021;&#26041;&#38754;&#23637;&#29616;&#20102;&#20986;&#33394;&#24615;&#33021;&#65292;&#20294;&#20854;&#33021;&#21147;&#20173;&#39640;&#24230;&#20381;&#36182;&#20110;&#25163;&#20889;&#30340;&#25552;&#31034;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35797;&#38169;&#23581;&#35797;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#38750;&#21442;&#25968;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#8212;&#8212;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#65288;APO&#65289;&#65292;&#20854;&#28789;&#24863;&#26469;&#33258;&#20110;&#20351;&#29992;&#25968;&#20540;&#26799;&#24230;&#19979;&#38477;&#33258;&#21160;&#25913;&#36827;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown impressive performance as general purpose agents, but their abilities remain highly dependent on prompts which are hand written with onerous trial-and-error effort. We propose a simple and nonparametric solution to this problem, Automatic Prompt Optimization (APO), which is inspired by numerical gradient descent to automatically improve prompts, assuming access to training data and an LLM API. The algorithm uses minibatches of data to form natural language ``gradients'' that criticize the current prompt. The gradients are then ``propagated'' into the prompt by editing the prompt in the opposite semantic direction of the gradient. These gradient descent steps are guided by a beam search and bandit selection procedure which significantly improves algorithmic efficiency. Preliminary results across three benchmark NLP tasks and the novel problem of LLM jailbreak detection suggest that Automatic Prompt Optimization can outperform prior prompt editing 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;PEFT-Ref&#21442;&#32771;&#26550;&#26500;&#65292;&#26631;&#20934;&#21270;&#20102;&#19981;&#21516;PEFT&#25216;&#26415;&#20849;&#20139;&#30340;&#26041;&#38754;&#65292;&#38548;&#31163;&#20102;&#24046;&#24322;&#21040;&#29305;&#23450;&#20301;&#32622;&#21644;&#20132;&#20114;&#20013;&#65292;&#27169;&#22359;&#21270;&#30340;&#35270;&#22270;&#26377;&#21161;&#20110;&#27604;&#36739;&#19981;&#21516;&#25216;&#26415;&#21450;&#20854;&#25928;&#29575;&#21644;&#20219;&#21153;&#24615;&#33021;&#65292;&#24182;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;PEFT&#30340;&#22522;&#26412;&#21407;&#29702;&#12290;</title><link>http://arxiv.org/abs/2304.12410</link><description>&lt;p&gt;
PEFT-Ref: &#19968;&#31181;&#27169;&#22359;&#21270;&#30340;&#21442;&#32771;&#26550;&#26500;&#21644;&#31867;&#22411;&#65292;&#29992;&#20110;&#21442;&#25968;&#25928;&#29575;&#24494;&#35843;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
PEFT-Ref: A Modular Reference Architecture and Typology for Parameter-Efficient Finetuning Techniques. (arXiv:2304.12410v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;PEFT-Ref&#21442;&#32771;&#26550;&#26500;&#65292;&#26631;&#20934;&#21270;&#20102;&#19981;&#21516;PEFT&#25216;&#26415;&#20849;&#20139;&#30340;&#26041;&#38754;&#65292;&#38548;&#31163;&#20102;&#24046;&#24322;&#21040;&#29305;&#23450;&#20301;&#32622;&#21644;&#20132;&#20114;&#20013;&#65292;&#27169;&#22359;&#21270;&#30340;&#35270;&#22270;&#26377;&#21161;&#20110;&#27604;&#36739;&#19981;&#21516;&#25216;&#26415;&#21450;&#20854;&#25928;&#29575;&#21644;&#20219;&#21153;&#24615;&#33021;&#65292;&#24182;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;PEFT&#30340;&#22522;&#26412;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#21442;&#25968;&#25928;&#29575;&#24494;&#35843;(PEFT)&#25216;&#26415;&#26088;&#22312;&#25913;&#21892;&#23436;&#20840;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLM)&#30340;&#39640;&#26114;&#25104;&#26412;&#12290;&#38543;&#30528;&#19981;&#21516;&#30340;PEFT&#25216;&#26415;&#19981;&#26029;&#20986;&#29616;&#65292;&#23545;&#23427;&#20204;&#36827;&#34892;&#27604;&#36739;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#22312;&#20197;&#19979;&#26041;&#38754;&#65306;(i)&#23427;&#20204;&#28155;&#21152;&#21040;PLM&#30340;&#32467;&#26500;&#21644;&#21151;&#33021;&#65292;(ii)&#19981;&#21516;&#31867;&#22411;&#21644;&#31243;&#24230;&#30340;&#25928;&#29575;&#25913;&#36827;&#65292;(iii)&#22312;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#20197;&#21450;(iv)&#32467;&#26500;&#21644;&#21151;&#33021;&#24046;&#24322;&#22914;&#20309;&#19982;&#25928;&#29575;&#21644;&#20219;&#21153;&#24615;&#33021;&#30456;&#20851;&#32852;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#26679;&#30340;&#27604;&#36739;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21442;&#32771;&#26694;&#26550;&#65292;&#26631;&#20934;&#21270;&#20102;&#19981;&#21516;PEFT&#25216;&#26415;&#20849;&#20139;&#30340;&#26041;&#38754;&#65292;&#21516;&#26102;&#23558;&#24046;&#24322;&#38548;&#31163;&#21040;&#19982;&#26631;&#20934;&#32452;&#20214;&#30340;&#29305;&#23450;&#20301;&#32622;&#21644;&#20132;&#20114;&#20013;&#12290;&#36890;&#36807;&#36825;&#20010;&#26631;&#20934;&#21270;&#21644;&#38548;&#31163;&#24046;&#24322;&#30340;&#36807;&#31243;&#65292;&#20986;&#29616;&#20102;PEFT&#25216;&#26415;&#30340;&#27169;&#22359;&#21270;&#35270;&#22270;&#65292;&#19981;&#20165;&#25903;&#25345;&#30452;&#25509;&#27604;&#36739;&#19981;&#21516;&#25216;&#26415;&#21450;&#20854;&#25928;&#29575;&#21644;&#20219;&#21153;&#24615;&#33021;&#65292;&#32780;&#19988;&#36824;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;PEFT&#30340;&#22522;&#26412;&#21407;&#29702;&#12290;&#25152;&#25552;&#20986;&#30340;&#21442;&#32771;&#26550;&#26500;&#31216;&#20026;PEFT-Ref&#65292;&#21253;&#25324;&#19971;&#20010;&#26680;&#24515;&#27169;&#22359;&#65292;&#27599;&#20010;&#27169;&#22359;&#37117;&#22788;&#29702;PEFT&#30340;&#29305;&#23450;&#26041;&#38754;&#65292;&#24182;&#21487;&#29992;&#20316;&#24320;&#21457;&#26032;PEFT&#25216;&#26415;&#21644;&#27604;&#36739;&#29616;&#26377;&#25216;&#26415;&#30340;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent parameter-efficient finetuning (PEFT) techniques aim to improve over the considerable cost of fully finetuning large pretrained language models (PLM). As different PEFT techniques proliferate, it is becoming difficult to compare them, in particular in terms of (i) the structure and functionality they add to the PLM, (ii) the different types and degrees of efficiency improvements achieved, (iii) performance at different downstream tasks, and (iv) how differences in structure and functionality relate to efficiency and task performance. To facilitate such comparisons, this paper presents a reference framework which standardises aspects shared by different PEFT techniques, while isolating differences to specific locations and interactions with the standard components. Through this process of standardising and isolating differences, a modular view of PEFT techniques emerges, supporting not only direct comparison of different techniques and their efficiency and task performance, but a
&lt;/p&gt;</description></item><item><title>PK-ICR&#26159;&#19968;&#31181;&#22522;&#20110;&#35282;&#33394;&#21644;&#30693;&#35782;&#30340;&#20114;&#21160;&#19978;&#19979;&#25991;&#26816;&#32034;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22797;&#26434;&#30340;&#22810;&#22330;&#26223;&#23545;&#35805;&#20013;&#21516;&#26102;&#35782;&#21035;&#35282;&#33394;&#21644;&#30693;&#35782;&#12290;&#36890;&#36807;&#21033;&#29992;&#31070;&#32463;&#38382;&#31572;&#26816;&#32034;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#36739;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#19979;&#23454;&#29616;&#26816;&#32034;&#65292;&#24182;&#19988;&#36890;&#36807;&#24341;&#20837;&#31354;-&#27491;&#21521;&#25490;&#21517;&#27979;&#35797;&#26041;&#27861;&#26469;&#25552;&#39640;&#25490;&#21517;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.06674</link><description>&lt;p&gt;
PK-ICR: &#22522;&#20110;&#35282;&#33394;&#21644;&#30693;&#35782;&#30340;&#20114;&#21160;&#19978;&#19979;&#25991;&#26816;&#32034;&#36827;&#34892;&#22522;&#20110;&#22330;&#26223;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
PK-ICR: Persona-Knowledge Interactive Context Retrieval for Grounded Dialogue. (arXiv:2302.06674v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06674
&lt;/p&gt;
&lt;p&gt;
PK-ICR&#26159;&#19968;&#31181;&#22522;&#20110;&#35282;&#33394;&#21644;&#30693;&#35782;&#30340;&#20114;&#21160;&#19978;&#19979;&#25991;&#26816;&#32034;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22797;&#26434;&#30340;&#22810;&#22330;&#26223;&#23545;&#35805;&#20013;&#21516;&#26102;&#35782;&#21035;&#35282;&#33394;&#21644;&#30693;&#35782;&#12290;&#36890;&#36807;&#21033;&#29992;&#31070;&#32463;&#38382;&#31572;&#26816;&#32034;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#36739;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#19979;&#23454;&#29616;&#26816;&#32034;&#65292;&#24182;&#19988;&#36890;&#36807;&#24341;&#20837;&#31354;-&#27491;&#21521;&#25490;&#21517;&#27979;&#35797;&#26041;&#27861;&#26469;&#25552;&#39640;&#25490;&#21517;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#21035;&#19982;&#23545;&#35805;&#31995;&#32479;&#30456;&#20851;&#30340;&#35282;&#33394;&#21644;&#30693;&#35782;&#23545;&#20110;&#22522;&#20110;&#22330;&#26223;&#30340;&#23545;&#35805;&#24212;&#31572;&#29983;&#25104;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27599;&#20010;&#23545;&#35805;&#22522;&#26412;&#19978;&#37117;&#26159;&#23396;&#31435;&#30740;&#31350;&#30340;&#65292;&#32780;&#26368;&#36817;&#30340;&#24037;&#20316;&#20013;&#24341;&#20837;&#20102;&#26356;&#23454;&#38469;&#30340;&#22810;&#22330;&#26223;&#23545;&#35805;&#20219;&#21153;&#12290;&#25105;&#20204;&#23558;&#35282;&#33394;&#21644;&#30693;&#35782;&#21452;&#19978;&#19979;&#25991;&#35782;&#21035;&#23450;&#20041;&#20026;&#20026;&#32473;&#23450;&#30340;&#23545;&#35805;&#21516;&#26102;&#35782;&#21035;&#35282;&#33394;&#21644;&#30693;&#35782;&#30340;&#20219;&#21153;&#65292;&#22312;&#22797;&#26434;&#30340;&#22810;&#22330;&#26223;&#23545;&#35805;&#35774;&#32622;&#20013;&#21487;&#33021;&#20855;&#26377;&#25552;&#21319;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26816;&#32034;&#30340;&#26816;&#32034;&#26041;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#21033;&#29992;&#23545;&#35805;&#30340;&#25152;&#26377;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#38382;&#31572;&#26816;&#32034;&#27169;&#22411;&#65292;&#38656;&#35201;&#36739;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31354;-&#27491;&#21521;&#25490;&#21517;&#27979;&#35797;&#26041;&#27861;&#65292;&#29992;&#20110;&#34913;&#37327;&#19982;&#25968;&#25454;&#22686;&#24378;&#30456;&#20851;&#30340;&#35821;&#20041;&#24046;&#24322;&#26679;&#26412;&#65288;&#21363;&#22256;&#38590;&#36127;&#26679;&#26412;&#65289;&#30340;&#25490;&#21517;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying relevant persona or knowledge for conversational systems is critical to grounded dialogue response generation. However, each grounding has been mostly researched in isolation with more practical multi-context dialogue tasks introduced in recent works. We define Persona and Knowledge Dual Context Identification as the task to identify persona and knowledge jointly for a given dialogue, which could be of elevated importance in complex multi-context dialogue settings. We develop a novel grounding retrieval method that utilizes all contexts of dialogue simultaneously. Our method requires less computational power via utilizing neural QA retrieval models. We further introduce our novel null-positive rank test which measures ranking performance on semantically dissimilar samples (i.e. hard negatives) in relation to data augmentation.
&lt;/p&gt;</description></item><item><title>NNKGC&#26159;&#19968;&#31181;&#36890;&#36807;&#33410;&#28857;&#37051;&#23621;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#24182;&#24341;&#20837;&#36793;&#36830;&#25509;&#39044;&#27979;&#20219;&#21153;&#30340;&#26694;&#26550;&#65292;&#31616;&#21333;&#32780;&#26377;&#25928;&#65292;&#21487;&#20197;&#39044;&#27979;&#20986;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.06132</link><description>&lt;p&gt;
NNKGC: &#29992;&#33410;&#28857;&#37051;&#23621;&#25913;&#36827;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
NNKGC: Improving Knowledge Graph Completion with Node Neighborhoods. (arXiv:2302.06132v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06132
&lt;/p&gt;
&lt;p&gt;
NNKGC&#26159;&#19968;&#31181;&#36890;&#36807;&#33410;&#28857;&#37051;&#23621;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#24182;&#24341;&#20837;&#36793;&#36830;&#25509;&#39044;&#27979;&#20219;&#21153;&#30340;&#26694;&#26550;&#65292;&#31616;&#21333;&#32780;&#26377;&#25928;&#65292;&#21487;&#20197;&#39044;&#27979;&#20986;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26088;&#22312;&#21457;&#29616;&#26597;&#35810;&#23454;&#20307;&#30340;&#32570;&#22833;&#20851;&#31995;&#12290;&#30446;&#21069;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#27169;&#22411;&#21033;&#29992;&#23454;&#20307;&#21517;&#31216;&#21644;&#25551;&#36848;&#25512;&#26029;&#22836;&#23454;&#20307;&#21644;&#29305;&#23450;&#20851;&#31995;&#32473;&#23450;&#30340;&#23614;&#23454;&#20307;&#12290;&#29616;&#26377;&#26041;&#27861;&#36824;&#32771;&#34385;&#20102;&#22836;&#23454;&#20307;&#30340;&#37051;&#23621;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#20351;&#29992;&#25153;&#24179;&#32467;&#26500;&#27169;&#25311;&#37051;&#23621;&#65292;&#19988;&#20165;&#38480;&#20110;1&#36339;&#37051;&#23621;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#33410;&#28857;&#37051;&#23621;&#26694;&#26550;&#12290;&#23427;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23545;&#22836;&#23454;&#20307;&#37051;&#23621;&#36827;&#34892;&#22810;&#36339;&#24314;&#27169;&#65292;&#20197;&#20016;&#23500;&#22836;&#33410;&#28857;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#36793;&#36830;&#25509;&#39044;&#27979;&#20219;&#21153;&#26469;&#25913;&#36827;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#12290;&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#31616;&#21333;&#32780;&#26377;&#25928;&#12290;&#26696;&#20363;&#30740;&#31350;&#36824;&#34920;&#26126;&#65292;&#27169;&#22411;&#33021;&#22815;&#39044;&#27979;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph completion (KGC) aims to discover missing relations of query entities. Current text-based models utilize the entity name and description to infer the tail entity given the head entity and a certain relation. Existing approaches also consider the neighborhood of the head entity. However, these methods tend to model the neighborhood using a flat structure and are only restricted to 1-hop neighbors. In this work, we propose a node neighborhood-enhanced framework for knowledge graph completion. It models the head entity neighborhood from multiple hops using graph neural networks to enrich the head node information. Moreover, we introduce an additional edge link prediction task to improve KGC. Evaluation on two public datasets shows that this framework is simple yet effective. The case study also shows that the model is able to predict explainable predictions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#25105;&#39564;&#35777;&#26041;&#27861;&#65292;&#20351;&#29992;CoT&#30340;&#32467;&#35770;&#26469;&#26500;&#24314;&#26032;&#26679;&#26412;&#24182;&#35201;&#27714;LLM&#37325;&#26032;&#39044;&#27979;&#21407;&#22987;&#26465;&#20214;&#65292;&#20197;&#25552;&#39640;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LLMs&#21487;&#20197;&#23545;&#20854;&#33258;&#24049;&#30340;&#32467;&#35770;&#36827;&#34892;&#33258;&#25105;&#39564;&#35777;&#24182;&#23454;&#29616;&#31454;&#20105;&#24615;&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.09561</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#24102;&#26377;&#33258;&#25105;&#39564;&#35777;&#30340;&#25512;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are reasoners with Self-Verification. (arXiv:2212.09561v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#25105;&#39564;&#35777;&#26041;&#27861;&#65292;&#20351;&#29992;CoT&#30340;&#32467;&#35770;&#26469;&#26500;&#24314;&#26032;&#26679;&#26412;&#24182;&#35201;&#27714;LLM&#37325;&#26032;&#39044;&#27979;&#21407;&#22987;&#26465;&#20214;&#65292;&#20197;&#25552;&#39640;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LLMs&#21487;&#20197;&#23545;&#20854;&#33258;&#24049;&#30340;&#32467;&#35770;&#36827;&#34892;&#33258;&#25105;&#39564;&#35777;&#24182;&#23454;&#29616;&#31454;&#20105;&#24615;&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#36827;&#34892;&#22797;&#26434;&#25512;&#29702;&#26102;&#65292;&#23427;&#38750;&#24120;&#25935;&#24863;&#20110;&#20010;&#21035;&#38169;&#35823;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24517;&#39035;&#35757;&#32451;&#39564;&#35777;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#31216;&#20026;&#33258;&#25105;&#39564;&#35777;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;CoT&#30340;&#32467;&#35770;&#20316;&#20026;&#26465;&#20214;&#26469;&#26500;&#24314;&#19968;&#20010;&#26032;&#26679;&#26412;&#65292;&#24182;&#35201;&#27714;LLM&#37325;&#26032;&#39044;&#27979;&#34987;&#25513;&#30422;&#30340;&#21407;&#22987;&#26465;&#20214;&#12290;&#25105;&#20204;&#22522;&#20110;&#20934;&#30830;&#24615;&#35745;&#31639;&#21487;&#35299;&#37322;&#30340;&#39564;&#35777;&#20998;&#25968;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20351;&#29992;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#26102;&#25552;&#39640;&#22810;&#20010;&#31639;&#26415;&#21644;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#24050;&#32463;&#35777;&#26126;LLM&#21487;&#20197;&#23545;&#20854;&#33258;&#24049;&#30340;&#32467;&#35770;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#33258;&#25105;&#39564;&#35777;&#24182;&#23454;&#29616;&#31454;&#20105;&#24615;&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#22810;&#31181;&#24102;&#26377;&#33258;&#25105;&#39564;&#35777;&#21151;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36991;&#20813;&#28151;&#28102;&#12290;
&lt;/p&gt;
&lt;p&gt;
When a large language model (LLM) performs complex reasoning by chain of thought (CoT), it can be highly sensitive to individual mistakes. We have had to train verifiers to address this issue. As we all know, after human inferring a conclusion, they often check it by re-verifying it, which can avoid some mistakes. We propose a new method called self-verification that uses the conclusion of the CoT as a condition to build a new sample and asks the LLM to re-predict the original conditions which be masked. We calculate an explainable verification score based on the accuracy. This method can improve the accuracy of multiple arithmetics and logical reasoning datasets when using few-shot learning. we have demonstrated that LLMs can conduct explainable self-verification of their own conclusions and achieve competitive reasoning performance. Extensive experimentals have demonstrated that our method can help multiple large language models with self-verification can avoid interference from inco
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#32039;&#24613;&#36890;&#20449;&#65288;EC&#65289;&#21644;&#39044;&#20808;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#22522;&#20110;&#35270;&#35273;&#20219;&#21153;&#28608;&#21169;&#27169;&#22411;&#26469;&#25913;&#36827;&#36164;&#28304;&#21294;&#20047;&#35821;&#35328;&#30340;&#38750;&#30417;&#30563;NMT&#31995;&#32479;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22235;&#31181;&#35821;&#35328;&#20013;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#36164;&#28304;&#21294;&#20047;&#30340;&#23612;&#27850;&#23572;&#35821;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20165;&#20351;&#29992;&#22238;&#35793;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2207.07025</link><description>&lt;p&gt;
&#23398;&#20064;&#36890;&#36807;&#23398;&#20064;&#20132;&#27969;&#26469;&#36827;&#34892;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Learning to translate by learning to communicate. (arXiv:2207.07025v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.07025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#32039;&#24613;&#36890;&#20449;&#65288;EC&#65289;&#21644;&#39044;&#20808;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#22522;&#20110;&#35270;&#35273;&#20219;&#21153;&#28608;&#21169;&#27169;&#22411;&#26469;&#25913;&#36827;&#36164;&#28304;&#21294;&#20047;&#35821;&#35328;&#30340;&#38750;&#30417;&#30563;NMT&#31995;&#32479;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22235;&#31181;&#35821;&#35328;&#20013;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#36164;&#28304;&#21294;&#20047;&#30340;&#23612;&#27850;&#23572;&#35821;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20165;&#20351;&#29992;&#22238;&#35793;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#24182;&#27979;&#35797;&#20102;&#19968;&#31181;&#25216;&#26415;&#65292;&#21033;&#29992;&#32039;&#24613;&#36890;&#20449;&#65288;EC&#65289;&#21644;&#39044;&#20808;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#25913;&#36827;&#20102;&#29616;&#20195;&#38750;&#30417;&#30563;NMT&#31995;&#32479;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#12290;&#24050;&#26377;&#35266;&#28857;&#35748;&#20026;&#65292;&#24403;&#21069;&#22312;NLP&#39046;&#22495;&#20027;&#23548;&#22320;&#20301;&#30340;&#25991;&#26412;&#39044;&#35757;&#32451;&#27169;&#22411;&#26080;&#27861;&#20135;&#29983;&#31283;&#20581;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#31995;&#32479;&#65292;&#24182;&#31361;&#20986;&#20102;&#23545;&#22522;&#20110;&#30446;&#26631;&#12289;&#30446;&#26631;&#23548;&#21521;&#21644;&#20132;&#20114;&#24335;&#35821;&#35328;&#23398;&#20064;&#30340;&#38656;&#27714;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#23558;&#22810;&#35821;&#35328;&#27169;&#22411;&#65288;mBART&#65292;Liu&#31561;&#65292;2020&#65289;&#23884;&#20837;&#21040;&#19968;&#20010;EC&#22270;&#20687;&#21442;&#32771;&#28216;&#25103;&#20013;&#65292;&#27169;&#22411;&#34987;&#28608;&#21169;&#20351;&#29992;&#22810;&#35821;&#35328;&#29983;&#25104;&#26469;&#23436;&#25104;&#19968;&#20010;&#22522;&#20110;&#35270;&#35273;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#20551;&#35774;&#26159;&#65292;&#36825;&#23558;&#20351;&#22810;&#31181;&#35821;&#35328;&#23545;&#40784;&#21040;&#19968;&#20010;&#20849;&#20139;&#30340;&#20219;&#21153;&#31354;&#38388;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;EC&#24494;&#35843;&#30340;&#21464;&#20307;&#65288;Steinert-Threlkeld&#31561;&#20154;&#65292;2022&#65289;&#65292;&#20854;&#20013;&#19968;&#31181;&#22312;&#21253;&#25324;&#36164;&#28304;&#21294;&#20047;&#30340;&#23612;&#27850;&#23572;&#35821;&#22312;&#20869;&#30340;&#22235;&#31181;&#35821;&#35328;&#20013;&#37117;&#20248;&#20110;&#20165;&#20351;&#29992;&#22238;&#35793;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We formulate and test a technique to use Emergent Communication (EC) with a pre-trained multilingual model to improve on modern Unsupervised NMT systems, especially for low-resource languages. It has been argued that the current dominant paradigm in NLP of pre-training on text-only corpora will not yield robust natural language understanding systems, and the need for grounded, goal-oriented, and interactive language learning has been high lighted. In our approach, we embed a multilingual model (mBART, Liu et al., 2020) into an EC image-reference game, in which the model is incentivized to use multilingual generations to accomplish a vision-grounded task. The hypothesis is that this will align multiple languages to a shared task space. We present two variants of EC Fine-Tuning (Steinert-Threlkeld et al., 2022), one of which outperforms a backtranslation-only baseline in all four languages investigated, including the low-resource language Nepali.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26080;&#26631;&#31614;&#35821;&#26009;&#24211;&#21644;&#22522;&#20110;&#21477;&#23376;&#23884;&#20837;&#30340;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#26500;&#24314;WordNet&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#26032;&#30340;WordNet&#65288;FilWordNet&#65289;&#65292;&#20197;&#26367;&#20195;&#24182;&#25913;&#36827;&#33778;&#24459;&#23486;&#35821;&#20013;&#36807;&#26102;&#30340;WordNet&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20154;&#24037;&#30417;&#30563;&#12290;</title><link>http://arxiv.org/abs/2204.03251</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#21477;&#23376;&#23884;&#20837;&#30340;&#35789;&#20041;&#24402;&#32435;&#33258;&#21160;&#26500;&#24314;WordNet
&lt;/p&gt;
&lt;p&gt;
Automatic WordNet Construction using Word Sense Induction through Sentence Embeddings. (arXiv:2204.03251v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.03251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26080;&#26631;&#31614;&#35821;&#26009;&#24211;&#21644;&#22522;&#20110;&#21477;&#23376;&#23884;&#20837;&#30340;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#26500;&#24314;WordNet&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#26032;&#30340;WordNet&#65288;FilWordNet&#65289;&#65292;&#20197;&#26367;&#20195;&#24182;&#25913;&#36827;&#33778;&#24459;&#23486;&#35821;&#20013;&#36807;&#26102;&#30340;WordNet&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20154;&#24037;&#30417;&#30563;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#36164;&#28304;&#22914;WordNet&#23545;&#20110;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#21644;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#65288;&#22914;&#33778;&#24459;&#23486;&#35821;&#65289;&#65292;&#29616;&#26377;&#30340;WordNet&#36807;&#26102;&#19988;&#19981;&#23436;&#25972;&#65292;&#24182;&#19988;&#29983;&#25104;&#26032;&#30340;WordNet&#21487;&#33021;&#38656;&#35201;&#22823;&#37327;&#30340;&#26102;&#38388;&#21644;&#36164;&#28304;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26080;&#26631;&#31614;&#35821;&#26009;&#24211;&#21644;&#22522;&#20110;&#21477;&#23376;&#23884;&#20837;&#30340;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#26500;&#24314;WordNet&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#29983;&#25104;&#20102;&#19968;&#20010;&#26032;&#30340;WordNet&#65288;FilWordNet&#65289;&#65292;&#20197;&#26367;&#20195;&#24182;&#25913;&#36827;&#33778;&#24459;&#23486;&#35821;&#20013;&#36807;&#26102;&#30340;WordNet&#12290;&#36890;&#36807;&#23558;&#25105;&#20204;&#33258;&#21160;&#35825;&#23548;&#20986;&#30340;&#35789;&#20041;&#21644;&#35789;&#27719;&#38598;&#19982;Princeton WordNet&#20013;&#30340;&#35789;&#20041;&#36827;&#34892;&#21305;&#37197;&#65292;&#20197;&#21450;&#23558;&#35789;&#27719;&#38598;&#19982;&#26087;&#30340;&#33778;&#24459;&#23486;&#35821;WordNet&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#32463;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#35825;&#23548;&#29616;&#26377;&#30340;&#35789;&#20041;&#21644;&#35789;&#27719;&#38598;&#65292;&#20063;&#21487;&#20197;&#28508;&#22312;&#22320;&#33258;&#21160;&#35825;&#23548;&#26032;&#30340;&#35789;&#20041;&#21644;&#35789;&#27719;&#38598;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20154;&#24037;&#30417;&#30563;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language resources such as wordnets remain indispensable tools for different natural language tasks and applications. However, for low-resource languages such as Filipino, existing wordnets are old and outdated, and producing new ones may be slow and costly in terms of time and resources. In this paper, we propose an automatic method for constructing a wordnet from scratch using only an unlabeled corpus and a sentence embeddings-based language model. Using this, we produce FilWordNet, a new wordnet that supplants and improves the outdated Filipino WordNet. We evaluate our automatically-induced senses and synsets by matching them with senses from the Princeton WordNet, as well as comparing the synsets to the old Filipino WordNet. We empirically show that our method can induce existing, as well as potentially new, senses and synsets automatically without the need for human supervision.
&lt;/p&gt;</description></item></channel></rss>