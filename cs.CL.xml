<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20197;&#32842;&#22825;&#20026;&#26680;&#24515;&#30340;&#35270;&#39057;&#29702;&#35299;&#31995;&#32479;VideoChat&#65292;&#23427;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#31070;&#32463;&#25509;&#21475;&#23558;&#35270;&#39057;&#22522;&#30784;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#25797;&#38271;&#20110;&#26102;&#31354;&#25512;&#29702;&#12289;&#20107;&#20214;&#23450;&#20301;&#21644;&#22240;&#26524;&#20851;&#31995;&#25512;&#26029;&#12290;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#35270;&#39057;&#20026;&#20013;&#24515;&#30340;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#35813;&#31995;&#32479;&#22312;&#24191;&#27867;&#30340;&#35270;&#39057;&#24212;&#29992;&#20013;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.06355</link><description>&lt;p&gt;
&#35270;&#39057;&#32842;&#22825;&#65306;&#20197;&#32842;&#22825;&#20026;&#26680;&#24515;&#30340;&#35270;&#39057;&#29702;&#35299;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
VideoChat: Chat-Centric Video Understanding. (arXiv:2305.06355v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20197;&#32842;&#22825;&#20026;&#26680;&#24515;&#30340;&#35270;&#39057;&#29702;&#35299;&#31995;&#32479;VideoChat&#65292;&#23427;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#31070;&#32463;&#25509;&#21475;&#23558;&#35270;&#39057;&#22522;&#30784;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#25797;&#38271;&#20110;&#26102;&#31354;&#25512;&#29702;&#12289;&#20107;&#20214;&#23450;&#20301;&#21644;&#22240;&#26524;&#20851;&#31995;&#25512;&#26029;&#12290;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#35270;&#39057;&#20026;&#20013;&#24515;&#30340;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#35813;&#31995;&#32479;&#22312;&#24191;&#27867;&#30340;&#35270;&#39057;&#24212;&#29992;&#20013;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#35270;&#39057;&#32842;&#22825;&#65288;VideoChat&#65289;&#8212;&#8212;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#20197;&#32842;&#22825;&#20026;&#26680;&#24515;&#30340;&#35270;&#39057;&#29702;&#35299;&#31995;&#32479;&#65292;&#23427;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#31070;&#32463;&#25509;&#21475;&#23558;&#35270;&#39057;&#22522;&#30784;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#25797;&#38271;&#20110;&#26102;&#31354;&#25512;&#29702;&#12289;&#20107;&#20214;&#23450;&#20301;&#21644;&#22240;&#26524;&#20851;&#31995;&#25512;&#26029;&#12290;&#20026;&#20102;&#25945;&#25480;&#35813;&#31995;&#32479;&#30340;&#20351;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35270;&#39057;&#20026;&#20013;&#24515;&#30340;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#25104;&#21315;&#19978;&#19975;&#20010;&#35270;&#39057;&#21644;&#35814;&#32454;&#30340;&#25551;&#36848;&#21644;&#23545;&#35805;&#65292;&#36825;&#20010;&#25968;&#25454;&#38598;&#24378;&#35843;&#26102;&#31354;&#25512;&#29702;&#21644;&#22240;&#26524;&#20851;&#31995;&#65292;&#20026;&#22521;&#35757;&#20197;&#32842;&#22825;&#20026;&#26680;&#24515;&#30340;&#35270;&#39057;&#29702;&#35299;&#31995;&#32479;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#36164;&#20135;&#12290;&#21021;&#27493;&#30340;&#23450;&#24615;&#23454;&#39564;&#25581;&#31034;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#24191;&#27867;&#30340;&#35270;&#39057;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#35774;&#23450;&#20102;&#26631;&#20934;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#20197;&#22312; https://github.com/OpenGVLab/Ask-Anything &#19978;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we initiate an exploration into video understanding by introducing VideoChat, an end-to-end chat-centric video understanding system. It integrates video foundation models and large language models via a learnable neural interface, excelling in spatiotemporal reasoning, event localization, and causal relationship inference. To instructively tune this system, we propose a video-centric instruction dataset, composed of thousands of videos matched with detailed descriptions and conversations. This dataset emphasizes spatiotemporal reasoning and causal relationships, providing a valuable asset for training chat-centric video understanding systems. Preliminary qualitative experiments reveal our system's potential across a broad spectrum of video applications and set the standard for future research. Access our code and data at https://github.com/OpenGVLab/Ask-Anything
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38889;&#35821;&#36890;&#29992;&#35789;&#24418;&#23398;&#25968;&#25454;&#38598;&#65292;&#20445;&#30041;&#38889;&#35821;&#29305;&#33394;&#24182;&#37319;&#29992;Sylak-Glassman&#31561;&#20154;&#30340;&#35789;&#24418;&#29305;&#24449;&#27169;&#24335;&#65292;&#20026;&#38889;&#35821;&#24418;&#24577;&#23398;&#33539;&#24335;&#39046;&#22495;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2305.06335</link><description>&lt;p&gt;
K-UniMorph&#65306;&#38889;&#35821;&#36890;&#29992;&#35789;&#24418;&#23398;&#21450;&#20854;&#29305;&#24449;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
K-UniMorph: Korean Universal Morphology and its Feature Schema. (arXiv:2305.06335v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38889;&#35821;&#36890;&#29992;&#35789;&#24418;&#23398;&#25968;&#25454;&#38598;&#65292;&#20445;&#30041;&#38889;&#35821;&#29305;&#33394;&#24182;&#37319;&#29992;Sylak-Glassman&#31561;&#20154;&#30340;&#35789;&#24418;&#29305;&#24449;&#27169;&#24335;&#65292;&#20026;&#38889;&#35821;&#24418;&#24577;&#23398;&#33539;&#24335;&#39046;&#22495;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38889;&#35821;&#36890;&#29992;&#35789;&#24418;&#23398;&#25968;&#25454;&#38598;&#65292;&#20043;&#21069;&#65292;&#38889;&#35821;&#22312;&#25968;&#30334;&#31181;&#22810;&#26679;&#30340;&#19990;&#30028;&#35821;&#35328;&#20013;&#30340;&#24418;&#24577;&#23398;&#33539;&#24335;&#39046;&#22495;&#20013;&#19968;&#30452;&#22788;&#20110;&#23569;&#25968;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36825;&#31181;&#20445;&#30041;&#38889;&#35821;&#29305;&#33394;&#30340;&#36890;&#29992;&#35789;&#24418;&#23398;&#33539;&#24335;&#12290;&#25105;&#20204;&#30340;K-UniMorph&#25968;&#25454;&#38598;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#27010;&#36848;&#20102;&#27599;&#20010;&#35821;&#27861;&#26631;&#20934;&#30340;&#21160;&#35789;&#32467;&#23614;&#65292;&#24182;&#38416;&#26126;&#22914;&#20309;&#25552;&#21462;&#21464;&#24418;&#24418;&#24335;&#20197;&#21450;&#22914;&#20309;&#29983;&#25104;&#35789;&#24418;&#27169;&#24335;&#12290;&#27492;&#25968;&#25454;&#38598;&#37319;&#29992;Sylak-Glassman&#31561;&#20154;&#65288;2015&#65289;&#21644;Sylak-Glassman&#65288;2016&#65289;&#30340;&#35789;&#24418;&#29305;&#24449;&#27169;&#24335;&#65292;&#32780;&#25105;&#20204;&#20174;Sejong&#24418;&#24577;&#20998;&#26512;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#21464;&#24418;&#24418;&#24335;&#65292;&#36825;&#26159;&#38889;&#35821;&#26368;&#22823;&#30340;&#27880;&#37322;&#35821;&#26009;&#24211;&#20043;&#19968;&#12290;&#22312;&#25968;&#25454;&#21019;&#24314;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#21253;&#25324;&#35843;&#26597;&#20174;Sejong&#35821;&#26009;&#24211;&#20013;&#30340;&#36716;&#25442;&#30340;&#27491;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#19977;&#31181;&#19981;&#21516;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#21464;&#24418;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present in this work a new Universal Morphology dataset for Korean. Previously, the Korean language has been underrepresented in the field of morphological paradigms amongst hundreds of diverse world languages. Hence, we propose this Universal Morphological paradigms for the Korean language that preserve its distinct characteristics. For our K-UniMorph dataset, we outline each grammatical criterion in detail for the verbal endings, clarify how to extract inflected forms, and demonstrate how we generate the morphological schemata. This dataset adopts morphological feature schema from Sylak-Glassman et al. (2015) and Sylak-Glassman (2016) for the Korean language as we extract inflected verb forms from the Sejong morphologically analyzed corpus that is one of the largest annotated corpora for Korean. During the data creation, our methodology also includes investigating the correctness of the conversion from the Sejong corpus. Furthermore, we carry out the inflection task using three di
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#24402;&#23646;&#39564;&#35777;&#30340;&#33258;&#21160;&#35780;&#20272;&#12290;&#30740;&#31350;&#21457;&#29616;&#36890;&#36807;&#25552;&#31034;LLMs&#21644;&#24494;&#35843;&#36739;&#23567;&#30340;LLMs&#20004;&#31181;&#26041;&#27861;&#37117;&#26377;&#25928;&#22320;&#26816;&#27979;&#21040;&#20102;&#38169;&#35823;&#30340;&#24402;&#23646;&#38472;&#36848;&#12290;</title><link>http://arxiv.org/abs/2305.06311</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#24402;&#23646;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Automatic Evaluation of Attribution by Large Language Models. (arXiv:2305.06311v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#24402;&#23646;&#39564;&#35777;&#30340;&#33258;&#21160;&#35780;&#20272;&#12290;&#30740;&#31350;&#21457;&#29616;&#36890;&#36807;&#25552;&#31034;LLMs&#21644;&#24494;&#35843;&#36739;&#23567;&#30340;LLMs&#20004;&#31181;&#26041;&#27861;&#37117;&#26377;&#25928;&#22320;&#26816;&#27979;&#21040;&#20102;&#38169;&#35823;&#30340;&#24402;&#23646;&#38472;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#21457;&#23637;&#26041;&#21521;&#26159;&#36890;&#36807;&#24341;&#29992;&#22806;&#37096;&#21442;&#32771;&#26469;&#29983;&#25104;&#21644;&#25903;&#25345;&#23427;&#20204;&#30340;&#20027;&#24352;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#24402;&#23646;&#38382;&#39064;&#65292;&#21363;&#39564;&#35777;&#29983;&#25104;&#30340;&#38472;&#36848;&#26159;&#21542;&#30830;&#23454;&#34987;&#24341;&#29992;&#21442;&#32771;&#20840;&#38754;&#25903;&#25345;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#24402;&#23646;&#39564;&#35777;&#30340;&#33258;&#21160;&#35780;&#20272;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;&#24402;&#23646;&#30340;&#23450;&#20041;&#65292;&#28982;&#21518;&#25506;&#35752;&#20102;&#20004;&#31181;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#65306;&#25552;&#31034;LLMs&#21644;&#24494;&#35843;&#36739;&#23567;&#30340;LLMs&#12290;&#24494;&#35843;&#25968;&#25454;&#20174;&#30456;&#20851;&#20219;&#21153;&#65288;&#20363;&#22914;&#65292;&#38382;&#31572;&#12289;&#20107;&#23454;&#26816;&#26597;&#12289;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#21644;&#25688;&#35201;&#65289;&#20013;&#37325;&#26032;&#21033;&#29992;&#12290;&#20026;&#20102;&#20415;&#20110;&#35780;&#20272;&#65292;&#25105;&#20204;&#25163;&#21160;&#31574;&#21010;&#20102;&#19968;&#32452;&#27979;&#35797;&#20363;&#23376;&#65292;&#20854;&#20013;&#21253;&#25324;12&#20010;&#39046;&#22495;&#30340;&#26469;&#33258;&#26032;&#24517;&#24212;&#21457;&#29983;&#22120;&#30340;&#27979;&#35797;&#20363;&#23376;&#12290;&#25105;&#20204;&#22312;&#32463;&#36807;&#31574;&#21010;&#30340;&#27979;&#35797;&#38598;&#21644;&#26469;&#33258;&#22806;&#37096;&#35821;&#26009;&#24211;&#30340;&#27169;&#25311;&#27979;&#35797;&#20363;&#23376;&#19978;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#26816;&#27979;&#21040;&#38169;&#35823;&#30340;&#24402;&#23646;&#38472;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
A recent focus of large language model (LLM) development, as exemplified by generative search engines, is to incorporate external references to generate and support their claims. However, evaluating the attribution, i.e., verifying whether the generated statement is indeed fully supported by the cited reference, remains an open problem. Although human evaluation is common practice, it is costly and time-consuming. In this paper, we investigate the automatic evaluation of attribution by LLMs. We begin by providing a definition of attribution and then explore two approaches for automatic evaluation: prompting LLMs and fine-tuning smaller LMs. The fine-tuning data is repurposed from related tasks, such as question answering, fact-checking, natural language inference, and summarization. To facilitate the evaluation, we manually curate a set of test examples covering 12 domains from a generative search engine, New Bing. Our results on the curated test set and simulated test examples from ex
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#23545;&#35821;&#20041;&#23884;&#20837;API&#22312;&#23454;&#38469;&#26816;&#32034;&#22330;&#26223;&#20013;&#30340;&#20998;&#26512;,&#20026;&#20174;&#19994;&#32773;&#21644;&#30740;&#31350;&#20154;&#21592;&#25214;&#21040;&#36866;&#24403;&#30340;&#26381;&#21153;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#33521;&#35821;&#19978;&#20351;&#29992;API&#37325;&#26032;&#25490;&#21517;BM25&#30340;&#32467;&#26524;&#26159;&#19968;&#31181;&#39044;&#31639;&#21451;&#22909;&#30340;&#26368;&#20248;&#20570;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.06300</link><description>&lt;p&gt;
&#35780;&#20272;&#20449;&#24687;&#26816;&#32034;&#30340;&#23884;&#20837;&#24335;API
&lt;/p&gt;
&lt;p&gt;
Evaluating Embedding APIs for Information Retrieval. (arXiv:2305.06300v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#23545;&#35821;&#20041;&#23884;&#20837;API&#22312;&#23454;&#38469;&#26816;&#32034;&#22330;&#26223;&#20013;&#30340;&#20998;&#26512;,&#20026;&#20174;&#19994;&#32773;&#21644;&#30740;&#31350;&#20154;&#21592;&#25214;&#21040;&#36866;&#24403;&#30340;&#26381;&#21153;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#33521;&#35821;&#19978;&#20351;&#29992;API&#37325;&#26032;&#25490;&#21517;BM25&#30340;&#32467;&#26524;&#26159;&#19968;&#31181;&#39044;&#31639;&#21451;&#22909;&#30340;&#26368;&#20248;&#20570;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#19981;&#26029;&#22686;&#22823;&#20351;&#24471;&#20854;&#26222;&#21450;&#21270;&#25104;&#20026;&#20102;&#19968;&#39033;&#25361;&#25112;&#65292;&#22240;&#27492;&#35768;&#22810;&#20844;&#21496;&#21644;&#21021;&#21019;&#20225;&#19994;&#36890;&#36807;API&#21521;&#31038;&#21306;&#25552;&#20379;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35775;&#38382;&#26435;&#38480;&#12290;&#20854;&#20013;&#19968;&#20010;&#36866;&#29992;&#20110;&#23494;&#38598;&#26816;&#32034;&#30340;&#29305;&#23450;API&#26159;&#35821;&#20041;&#23884;&#20837;&#24335;API&#65292;&#20854;&#21487;&#26500;&#24314;&#32473;&#23450;&#25991;&#26412;&#30340;&#21521;&#37327;&#34920;&#31034;&#12290;&#22312;&#25317;&#26377;&#36234;&#26469;&#36234;&#22810;API&#30340;&#24773;&#20917;&#19979;&#65292;&#26412;&#25991;&#26088;&#22312;&#20998;&#26512;&#22312;&#23454;&#38469;&#26816;&#32034;&#22330;&#26223;&#20013;&#35821;&#20041;&#23884;&#20837;&#24335;API&#20197;&#24110;&#21161;&#20174;&#19994;&#32773;&#21644;&#30740;&#31350;&#20154;&#21592;&#26681;&#25454;&#20182;&#20204;&#30340;&#38656;&#27714;&#25214;&#21040;&#36866;&#24403;&#30340;&#26381;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24076;&#26395;&#35843;&#26597;&#29616;&#26377;API&#22312;&#39046;&#22495;&#27867;&#21270;&#21644;&#22810;&#35821;&#35328;&#26816;&#32034;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#26631;&#20934;&#22522;&#20934;BEIR&#21644;MIRACL&#19978;&#35780;&#20272;&#20102;&#23884;&#20837;&#24335;API&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20351;&#29992;API&#37325;&#26032;&#25490;&#21517;BM25&#32467;&#26524;&#26159;&#19968;&#31181;&#39044;&#31639;&#21451;&#22909;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#33521;&#35821;&#19978;&#26368;&#26377;&#25928;&#65292;&#19982;&#26631;&#20934;&#20570;&#27861;&#21363;&#20316;&#20026;&#31532;&#19968;&#38454;&#27573;&#26816;&#32034;&#22120;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ever-increasing size of language models curtails their widespread access to the community, thereby galvanizing many companies and startups into offering access to large language models through APIs. One particular API, suitable for dense retrieval, is the semantic embedding API that builds vector representations of a given text. With a growing number of APIs at our disposal, in this paper, our goal is to analyze semantic embedding APIs in realistic retrieval scenarios in order to assist practitioners and researchers in finding suitable services according to their needs. Specifically, we wish to investigate the capabilities of existing APIs on domain generalization and multilingual retrieval. For this purpose, we evaluate the embedding APIs on two standard benchmarks, BEIR, and MIRACL. We find that re-ranking BM25 results using the APIs is a budget-friendly approach and is most effective on English, in contrast to the standard practice, i.e., employing them as first-stage retrievers
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;GPT-3&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#29983;&#25104;&#25991;&#31456;&#25688;&#35201;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#23427;&#23545;&#21333;&#20010;&#25991;&#31456;&#30340;&#24635;&#32467;&#21644;&#31616;&#21270;&#25928;&#26524;&#36739;&#22909;&#65292;&#20294;&#22312;&#32508;&#21512;&#22810;&#31687;&#25991;&#31456;&#20013;&#25152;&#25253;&#21578;&#30340;&#35777;&#25454;&#26041;&#38754;&#34920;&#29616;&#27424;&#20339;&#12290;</title><link>http://arxiv.org/abs/2305.06299</link><description>&lt;p&gt;
&#20351;&#29992;GPT-3&#23545;&#21307;&#23398;&#35777;&#25454;&#36827;&#34892;&#24635;&#32467;&#12289;&#31616;&#21270;&#21644;&#32508;&#21512;&#65288;&#25104;&#26524;&#21442;&#24046;&#19981;&#40784;&#65289;
&lt;/p&gt;
&lt;p&gt;
Summarizing, Simplifying, and Synthesizing Medical Evidence Using GPT-3 (with Varying Success). (arXiv:2305.06299v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;GPT-3&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#29983;&#25104;&#25991;&#31456;&#25688;&#35201;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#23427;&#23545;&#21333;&#20010;&#25991;&#31456;&#30340;&#24635;&#32467;&#21644;&#31616;&#21270;&#25928;&#26524;&#36739;&#22909;&#65292;&#20294;&#22312;&#32508;&#21512;&#22810;&#31687;&#25991;&#31456;&#20013;&#25152;&#25253;&#21578;&#30340;&#35777;&#25454;&#26041;&#38754;&#34920;&#29616;&#27424;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;GPT-3&#65292;&#33021;&#22815;&#22312;&#20960;&#20046;&#27809;&#26377;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#19968;&#27969;&#30340;&#26222;&#36890;&#39046;&#22495;&#26032;&#38395;&#25991;&#31456;&#25688;&#35201;&#12290;&#20294;&#26159;&#65292;&#23578;&#19981;&#28165;&#26970;&#36825;&#26679;&#30340;&#27169;&#22411;&#26159;&#21542;&#22312;&#26356;&#19987;&#19994;&#21644;&#39640;&#39118;&#38505;&#30340;&#39046;&#22495;&#65292;&#22914;&#29983;&#29289;&#21307;&#23398;&#20013;&#21516;&#26679;&#20855;&#22791;&#36825;&#26679;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35831;&#39046;&#22495;&#19987;&#23478;&#65288;&#20855;&#22791;&#21307;&#23398;&#22521;&#35757;&#30340;&#20154;&#65289;&#35780;&#20272;&#30001;GPT-3&#29983;&#25104;&#30340;&#29983;&#29289;&#21307;&#23398;&#25991;&#31456;&#25688;&#35201;&#65292;&#24182;&#32771;&#34385;&#21333;&#19968;&#21644;&#22810;&#25991;&#26723;&#25688;&#35201;&#24773;&#20917;&#12290;&#21069;&#32773;&#20013;&#65292;GPT-3&#30340;&#20219;&#21153;&#26159;&#29983;&#25104;&#25551;&#36848;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#30340;&#25991;&#31456;&#30340;&#24120;&#35268;&#21644;&#31616;&#26126;&#35821;&#35328;&#25688;&#35201;&#65307;&#21518;&#32773;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;GPT-3&#22312;&#25972;&#20010;&#25991;&#31456;&#38598;&#20013;&#32508;&#21512;&#25253;&#21578;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#27880;&#37322;&#26041;&#26696;&#26469;&#35780;&#20272;&#27169;&#22411;&#36755;&#20986;&#65292;&#24182;&#37325;&#28857;&#35780;&#20272;&#29983;&#25104;&#25688;&#35201;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;GPT-3&#33021;&#22815;&#24544;&#23454;&#22320;&#24635;&#32467;&#21644;&#31616;&#21270;&#21333;&#20010;&#29983;&#29289;&#21307;&#23398;&#25991;&#31456;&#65292;&#20294;&#23427;&#22312;&#32508;&#21512;&#22810;&#20010;&#25991;&#31456;&#25152;&#25552;&#20379;&#30340;&#35777;&#25454;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models, particularly GPT-3, are able to produce high quality summaries of general domain news articles in few- and zero-shot settings. However, it is unclear if such models are similarly capable in more specialized, high-stakes domains such as biomedicine. In this paper, we enlist domain experts (individuals with medical training) to evaluate summaries of biomedical articles generated by GPT-3, given zero supervision. We consider both single- and multi-document settings. In the former, GPT-3 is tasked with generating regular and plain-language summaries of articles describing randomized controlled trials; in the latter, we assess the degree to which GPT-3 is able to \emph{synthesize} evidence reported across a collection of articles. We design an annotation scheme for evaluating model outputs, with an emphasis on assessing the factual accuracy of generated summaries. We find that while GPT-3 is able to summarize and simplify single biomedical articles faithfully, it stru
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#22270;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#21487;&#20197;&#23558;&#19978;&#19979;&#25991;&#22686;&#24378;&#30340;&#30693;&#35782;&#32858;&#21512;&#36807;&#31243;&#19982;&#30456;&#20851;&#30693;&#35782;&#22270;&#30340;&#20840;&#23616;&#29305;&#24449;&#26377;&#25928;&#34701;&#21512;&#65292;&#23558;&#22686;&#24378;&#30340;&#22270;&#32467;&#26500;&#30693;&#35782;&#38598;&#25104;&#21040;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#23545;&#35805;&#29983;&#25104;&#27169;&#22411;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#33258;&#21160;&#24230;&#37327;&#21644;&#20154;&#31867;&#35780;&#20272;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.06294</link><description>&lt;p&gt;
CADGE&#65306;&#22522;&#20110;&#22270;&#32467;&#26500;&#30693;&#35782;&#32858;&#21512;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#23545;&#35805;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
CADGE: Context-Aware Dialogue Generation Enhanced with Graph-Structured Knowledge Aggregation. (arXiv:2305.06294v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#22270;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#21487;&#20197;&#23558;&#19978;&#19979;&#25991;&#22686;&#24378;&#30340;&#30693;&#35782;&#32858;&#21512;&#36807;&#31243;&#19982;&#30456;&#20851;&#30693;&#35782;&#22270;&#30340;&#20840;&#23616;&#29305;&#24449;&#26377;&#25928;&#34701;&#21512;&#65292;&#23558;&#22686;&#24378;&#30340;&#22270;&#32467;&#26500;&#30693;&#35782;&#38598;&#25104;&#21040;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#23545;&#35805;&#29983;&#25104;&#27169;&#22411;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#33258;&#21160;&#24230;&#37327;&#21644;&#20154;&#31867;&#35780;&#20272;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24120;&#35782;&#30693;&#35782;&#65288;commonsense knowledge&#65289;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#23558;&#22270;&#30693;&#35782;&#19982;&#20256;&#32479;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30456;&#32467;&#21512;&#65292;&#23548;&#33268;&#25991;&#26412;&#21644;&#22270;&#30693;&#35782;&#32534;&#30721;&#36807;&#31243;&#22312;&#20018;&#34892;&#27969;&#27700;&#32447;&#20013;&#34987;&#20998;&#31163;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#20123;&#20998;&#31163;&#30340;&#34920;&#31034;&#23398;&#20064;&#38454;&#27573;&#21487;&#33021;&#23545;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21253;&#21547;&#22312;&#20004;&#31181;&#36755;&#20837;&#30693;&#35782;&#31867;&#22411;&#20013;&#30340;&#25972;&#20307;&#19978;&#19979;&#25991;&#26159;&#27425;&#20248;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#22270;&#27880;&#24847;&#21147;&#27169;&#22411;&#65288;Context-aware GAT&#65289;&#65292;&#23427;&#21487;&#20197;&#22522;&#20110;&#19978;&#19979;&#25991;&#22686;&#24378;&#30340;&#30693;&#35782;&#32858;&#21512;&#36807;&#31243;&#26377;&#25928;&#22320;&#34701;&#21512;&#30456;&#20851;&#30693;&#35782;&#22270;&#30340;&#20840;&#23616;&#29305;&#24449;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21033;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#26469;&#22788;&#29702;&#24322;&#26500;&#29305;&#24449;&#8212;&#8212;&#23558;&#22270;&#30693;&#35782;&#19982;&#25991;&#26412;&#30456;&#32467;&#21512;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#27425;&#23581;&#35797;&#22312;&#36830;&#25509;&#23376;&#22270;&#19978;&#20998;&#23618;&#24212;&#29992;&#22270;&#30693;&#35782;&#32858;&#21512;&#20197;&#21450;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#23558;&#22686;&#24378;&#30340;&#22270;&#32467;&#26500;&#30693;&#35782;&#38598;&#25104;&#21040;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#23545;&#35805;&#29983;&#25104;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#33258;&#21160;&#24230;&#37327;&#21644;&#20154;&#31867;&#35780;&#20272;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Commonsense knowledge is crucial to many natural language processing tasks. Existing works usually incorporate graph knowledge with conventional graph neural networks (GNNs), leading to the text and graph knowledge encoding processes being separated in a serial pipeline. We argue that these separate representation learning stages may be suboptimal for neural networks to learn the overall context contained in both types of input knowledge. In this paper, we propose a novel context-aware graph-attention model (Context-aware GAT), which can effectively incorporate global features of relevant knowledge graphs based on a context-enhanced knowledge aggregation process. Specifically, our framework leverages a novel representation learning approach to process heterogeneous features - combining flattened graph knowledge with text. To the best of our knowledge, this is the first attempt at hierarchically applying graph knowledge aggregation on a connected subgraph in addition to contextual infor
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#22312;&#25991;&#20214;&#31616;&#21270;&#36807;&#31243;&#20013;&#20351;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20197;&#25552;&#39640;&#36755;&#20986;&#36136;&#37327;&#12290;&#36890;&#36807;&#36845;&#20195;&#26356;&#22823;&#30340;&#25991;&#26412;&#21333;&#20803;&#25110;&#25193;&#23637;&#31995;&#32479;&#26550;&#26500;&#26469;&#20851;&#27880;&#25991;&#26723;&#30340;&#39640;&#32423;&#35805;&#35821;&#34920;&#31034;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#20351;&#31616;&#21270;&#27169;&#22411;&#30452;&#25509;&#35775;&#38382;&#23616;&#37096;&#30340;&#36328;&#21477;&#23376;&#25991;&#26723;&#19978;&#19979;&#25991;&#65292;&#20174;&#32780;&#23454;&#29616;&#26368;&#26032;&#30340;&#25991;&#26723;&#31616;&#21270;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.06274</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#25991;&#20214;&#31616;&#21270;
&lt;/p&gt;
&lt;p&gt;
Context-Aware Document Simplification. (arXiv:2305.06274v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06274
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#22312;&#25991;&#20214;&#31616;&#21270;&#36807;&#31243;&#20013;&#20351;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20197;&#25552;&#39640;&#36755;&#20986;&#36136;&#37327;&#12290;&#36890;&#36807;&#36845;&#20195;&#26356;&#22823;&#30340;&#25991;&#26412;&#21333;&#20803;&#25110;&#25193;&#23637;&#31995;&#32479;&#26550;&#26500;&#26469;&#20851;&#27880;&#25991;&#26723;&#30340;&#39640;&#32423;&#35805;&#35821;&#34920;&#31034;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#20351;&#31616;&#21270;&#27169;&#22411;&#30452;&#25509;&#35775;&#38382;&#23616;&#37096;&#30340;&#36328;&#21477;&#23376;&#25991;&#26723;&#19978;&#19979;&#25991;&#65292;&#20174;&#32780;&#23454;&#29616;&#26368;&#26032;&#30340;&#25991;&#26723;&#31616;&#21270;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#22823;&#22810;&#25968;&#25991;&#26412;&#31616;&#21270;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#21477;&#23376;&#32423;&#36755;&#20837;&#19978;&#12290;&#26089;&#26399;&#30340;&#25991;&#20214;&#31616;&#21270;&#23581;&#35797;&#20165;&#36890;&#36807;&#36845;&#20195;&#25991;&#26723;&#20013;&#30340;&#21477;&#23376;&#26469;&#24212;&#29992;&#36825;&#20123;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#26080;&#27861;&#36830;&#36143;&#22320;&#20445;&#25345;&#35805;&#35821;&#32467;&#26500;&#65292;&#23548;&#33268;&#36755;&#20986;&#36136;&#37327;&#19981;&#20339;&#12290;&#26368;&#36817;&#65292;&#25511;&#21046;&#31616;&#21270;&#30340;&#31574;&#30053;&#24050;&#34987;&#21033;&#29992;&#20026;&#20102;&#22312;&#29983;&#25104;&#25991;&#26723;&#32423;&#35745;&#21010;&#65288;&#21477;&#23376;&#32423;&#31616;&#21270;&#25805;&#20316;&#24207;&#21015;&#65289;&#20043;&#21518;&#65292;&#22312;&#19979;&#28216;&#20351;&#29992;&#35813;&#35745;&#21010;&#26469;&#24341;&#23548;&#21477;&#23376;&#32423;&#31616;&#21270;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25991;&#26723;&#31616;&#21270;&#30340;&#26368;&#26032;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20173;&#28982;&#20855;&#26377;&#23616;&#38480;&#24615;&#65292;&#22240;&#20026;&#31616;&#21270;&#27169;&#22411;&#27809;&#26377;&#30452;&#25509;&#35775;&#38382;&#23616;&#37096;&#30340;&#36328;&#21477;&#23376;&#25991;&#26723;&#19978;&#19979;&#25991;&#65292;&#24456;&#21487;&#33021;&#20250;&#23545;&#34920;&#38754;&#23454;&#29616;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#22810;&#31181;&#20351;&#29992;&#25991;&#20214;&#19978;&#19979;&#25991;&#22312;&#31616;&#21270;&#36807;&#31243;&#20013;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#36845;&#20195;&#26356;&#22823;&#30340;&#25991;&#26412;&#21333;&#20803;&#25110;&#36890;&#36807;&#25193;&#23637;&#31995;&#32479;&#26550;&#26500;&#26469;&#20851;&#27880;&#25991;&#26723;&#30340;&#39640;&#32423;&#35805;&#35821;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
To date, most work on text simplification has focused on sentence-level inputs. Early attempts at document simplification merely applied these approaches iteratively over the sentences of a document. However, this fails to coherently preserve the discourse structure, leading to suboptimal output quality. Recently, strategies from controllable simplification have been leveraged to achieve state-of-the-art results on document simplification by first generating a document-level plan (a sequence of sentence-level simplification operations) and using this plan to guide sentence-level simplification downstream. However, this is still limited in that the simplification model has no direct access to the local inter-sentence document context, likely having a negative impact on surface realisation. We explore various systems that use document context within the simplification process itself, either by iterating over larger text units or by extending the system architecture to attend over a high-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#21644;&#26631;&#31614;&#33258;&#36866;&#24212;Mixup&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#22312;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#20013;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#24212;&#23545;&#20154;&#31867;&#24773;&#24863;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.06273</link><description>&lt;p&gt;
&#23398;&#20064;&#40065;&#26834;&#24615;&#30340;&#33258;&#27880;&#24847;&#21147;&#29305;&#24449;&#21644;&#26631;&#31614;&#33258;&#36866;&#24212;Mixup&#22312;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Learning Robust Self-attention Features for Speech Emotion Recognition with Label-adaptive Mixup. (arXiv:2305.06273v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#21644;&#26631;&#31614;&#33258;&#36866;&#24212;Mixup&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#22312;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#20013;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#24212;&#23545;&#20154;&#31867;&#24773;&#24863;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;(SER)&#22312;&#19982;&#26426;&#22120;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#30340;&#24773;&#22659;&#19979;&#65292;&#26088;&#22312;&#35782;&#21035;&#20154;&#31867;&#30340;&#24773;&#24863;&#65292;&#30001;&#20110;&#20154;&#31867;&#24773;&#24863;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36825;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#22312;SER&#30340;&#30740;&#31350;&#20013;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20173;&#28982;&#38590;&#20197;&#36798;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#26631;&#31614;&#33258;&#36866;&#24212;Mixup&#21644;&#20013;&#24515;&#25439;&#22833;&#12290;&#36890;&#36807;&#23558;&#28151;&#21512;&#25968;&#25454;&#20013;&#30340;&#26631;&#31614;&#27010;&#29575;&#36827;&#34892;&#36866;&#24212;&#24615;&#35843;&#25972;&#24182;&#23558;&#20013;&#24515;&#25439;&#22833;&#36866;&#24212;&#20110;&#35813;&#28151;&#21512;&#35757;&#32451;&#31574;&#30053;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech Emotion Recognition (SER) is to recognize human emotions in a natural verbal interaction scenario with machines, which is considered as a challenging problem due to the ambiguous human emotions. Despite the recent progress in SER, state-of-the-art models struggle to achieve a satisfactory performance. We propose a self-attention based method with combined use of label-adaptive mixup and center loss. By adapting label probabilities in mixup and fitting center loss to the mixup training scheme, our proposed method achieves a superior performance to the state-of-the-art methods.
&lt;/p&gt;</description></item><item><title>ComputeGPT&#26159;&#19968;&#31181;&#35745;&#31639;&#22411;&#32842;&#22825;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#36816;&#34892;&#20195;&#30721;&#35299;&#20915;&#25968;&#20540;&#38382;&#39064;&#65292;&#32467;&#21512;&#26412;&#22320;&#27983;&#35272;&#22120;&#30340;Python&#35299;&#37322;&#22120;&#21644;&#20248;&#21270;&#30340;&#25552;&#31034;&#65292;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#25968;&#20540;&#38382;&#39064;&#25928;&#29575;&#24182;&#20026;&#20195;&#30721;&#25552;&#20379;&#21512;&#36866;&#30340;&#21069;&#31471;&#21644;&#23433;&#20840;&#29615;&#22659;&#12290;</title><link>http://arxiv.org/abs/2305.06223</link><description>&lt;p&gt;
ComputeGPT&#65306;&#36866;&#29992;&#20110;&#25968;&#20540;&#38382;&#39064;&#30340;&#35745;&#31639;&#22411;&#32842;&#22825;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ComputeGPT: A computational chat model for numerical problems. (arXiv:2305.06223v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06223
&lt;/p&gt;
&lt;p&gt;
ComputeGPT&#26159;&#19968;&#31181;&#35745;&#31639;&#22411;&#32842;&#22825;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#36816;&#34892;&#20195;&#30721;&#35299;&#20915;&#25968;&#20540;&#38382;&#39064;&#65292;&#32467;&#21512;&#26412;&#22320;&#27983;&#35272;&#22120;&#30340;Python&#35299;&#37322;&#22120;&#21644;&#20248;&#21270;&#30340;&#25552;&#31034;&#65292;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#25968;&#20540;&#38382;&#39064;&#25928;&#29575;&#24182;&#20026;&#20195;&#30721;&#25552;&#20379;&#21512;&#36866;&#30340;&#21069;&#31471;&#21644;&#23433;&#20840;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#25968;&#20540;&#38382;&#39064;&#19978;&#19981;&#22815;&#31934;&#30830;&#65292;&#20854;&#32467;&#26500;&#35201;&#27714;&#30340;&#26159;&#27010;&#29575;&#24615;&#30340;&#19979;&#19968;&#20010;&#21333;&#35789;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ComputeGPT&#65306;&#19968;&#31181;&#36890;&#36807;&#25353;&#38656;&#36816;&#34892;&#20195;&#30721;&#26469;&#35299;&#20915;&#35745;&#31639;&#38382;&#39064;&#30340;&#32842;&#22825;&#27169;&#22411;&#12290;ComputeGPT&#23558;&#27599;&#20010;&#38382;&#39064;&#36716;&#25442;&#20026;&#30456;&#20851;&#30340;&#20195;&#30721;&#65292;&#36816;&#34892;&#20195;&#30721;&#24182;&#23558;&#35745;&#31639;&#32467;&#26524;&#20316;&#20026;&#32842;&#22825;&#30340;&#19968;&#37096;&#20998;&#36820;&#22238;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#19982;&#22522;&#20110;&#26412;&#22320;&#27983;&#35272;&#22120;&#30340;Python&#35299;&#37322;&#22120;&#21644;&#20248;&#21270;&#30340;&#25552;&#31034;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#25968;&#20540;&#38382;&#39064;&#25928;&#29575;&#65292;&#24182;&#20026;&#20195;&#30721;&#25552;&#20379;&#21512;&#36866;&#30340;&#21069;&#31471;&#21644;&#23433;&#20840;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models are not accurate in numerical problems. Their architecture does not allow for anything less than a probabilistic next word. This paper introduces ComputeGPT: an approach of creating a chat model able to answer computational problems through running on-demand code. ComputeGPT converts each question to relevant code, runs the code, and returns the computed answer as part of the chat. We combine this approach with a local browser-based Python interpretation and fine-tuned prompts in order to achieve state-of-the-art efficiency on numerical problems and provide a suitable front-end and safe environment for the code to be executed in.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#34920;&#26126;&#65292;&#37319;&#29992;&#22810;&#20219;&#21153;&#31471;&#21040;&#31471;Transformer&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#23545;&#35805;&#24335;&#25512;&#33616;&#30340;&#24615;&#33021;&#65292;&#21487;&#31454;&#20105;&#20110;&#20808;&#21069;&#37319;&#29992;&#22797;&#26434;&#22810;&#32452;&#20214;&#26041;&#27861;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24494;&#35843;&#25105;&#20204;&#30340;&#27169;&#22411;&#21644;&#39069;&#22806;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#35774;&#32622;&#65292;&#23454;&#29616;&#20102;&#30693;&#35782;&#22312;&#39046;&#22495;&#38388;&#30340;&#36716;&#31227;&#12290;</title><link>http://arxiv.org/abs/2305.06218</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#31471;&#21040;&#31471;&#35757;&#32451;&#25913;&#36827;&#20102;&#23545;&#35805;&#24335;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Multi-Task End-to-End Training Improves Conversational Recommendation. (arXiv:2305.06218v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#34920;&#26126;&#65292;&#37319;&#29992;&#22810;&#20219;&#21153;&#31471;&#21040;&#31471;Transformer&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#23545;&#35805;&#24335;&#25512;&#33616;&#30340;&#24615;&#33021;&#65292;&#21487;&#31454;&#20105;&#20110;&#20808;&#21069;&#37319;&#29992;&#22797;&#26434;&#22810;&#32452;&#20214;&#26041;&#27861;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24494;&#35843;&#25105;&#20204;&#30340;&#27169;&#22411;&#21644;&#39069;&#22806;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#35774;&#32622;&#65292;&#23454;&#29616;&#20102;&#30693;&#35782;&#22312;&#39046;&#22495;&#38388;&#30340;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#31471;&#21040;&#31471;Transformer&#27169;&#22411;&#22312;&#23545;&#35805;&#24335;&#25512;&#33616;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#35813;&#20219;&#21153;&#26088;&#22312;&#22522;&#20110;&#29992;&#25143;&#22312;&#23545;&#35805;&#20013;&#26126;&#30830;&#34920;&#31034;&#30340;&#20559;&#22909;&#25552;&#20379;&#25512;&#33616;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#30740;&#31350;&#22312;&#27492;&#39046;&#22495;&#37319;&#29992;&#20102;&#22797;&#26434;&#30340;&#22810;&#32452;&#20214;&#26041;&#27861;&#65292;&#20854;&#20013;&#23545;&#35805;&#31649;&#29702;&#21644;&#23454;&#20307;&#25512;&#33616;&#20219;&#21153;&#30001;&#21333;&#29420;&#30340;&#32452;&#20214;&#22788;&#29702;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#65292;&#22522;&#20110;T5&#25991;&#26412;-&#25991;&#26412;Transformer&#27169;&#22411;&#30340;&#32479;&#19968;Transformer&#27169;&#22411;&#22312;&#25512;&#33616;&#30456;&#20851;&#39033;&#30446;&#21644;&#29983;&#25104;&#23545;&#35805;&#26041;&#38754;&#37117;&#21487;&#20197;&#31454;&#20105;&#12290;&#25105;&#20204;&#22312;ReDIAL&#23545;&#35805;&#24335;&#30005;&#24433;&#25512;&#33616;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#35774;&#32622;&#20013;&#21019;&#24314;&#20102;&#34893;&#29983;&#33258;MovieLens&#30340;&#39069;&#22806;&#35757;&#32451;&#20219;&#21153;&#65288;&#20363;&#22914;&#22522;&#20110;&#36755;&#20837;&#30005;&#24433;&#39044;&#27979;&#30005;&#24433;&#23646;&#24615;&#21644;&#30456;&#20851;&#30005;&#24433;&#65289;&#12290;&#20351;&#29992;&#19968;&#31995;&#21015;&#25506;&#38024;&#30740;&#31350;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#39069;&#22806;&#20219;&#21153;&#20013;&#23398;&#20064;&#21040;&#30340;&#30693;&#35782;&#34987;&#36716;&#31227;&#21040;&#20102;&#23545;&#35805;&#24335;&#25512;&#33616;&#39046;&#22495;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we analyze the performance of a multitask end-to-end transformer model on the task of conversational recommendations, which aim to provide recommendations based on a user's explicit preferences expressed in dialogue. While previous works in this area adopt complex multi-component approaches where the dialogue management and entity recommendation tasks are handled by separate components, we show that a unified transformer model, based on the T5 text-to-text transformer model, can perform competitively in both recommending relevant items and generating conversation dialogue. We fine-tune our model on the ReDIAL conversational movie recommendation dataset, and create additional training tasks derived from MovieLens (such as the prediction of movie attributes and related movies based on an input movie), in a multitask learning setting. Using a series of probe studies, we demonstrate that the learned knowledge in the additional tasks is transferred to the conversational setti
&lt;/p&gt;</description></item><item><title>RAPT&#26159;&#19968;&#20010;&#25552;&#20379;&#38544;&#31169;&#20445;&#35777;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;&#30340;&#25552;&#31034;&#35843;&#25972;&#26694;&#26550;&#65292;&#37319;&#29992;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#35774;&#32622;&#21644;&#26032;&#39062;&#30340;&#38544;&#31169;&#21270;&#26631;&#35760;&#37325;&#24314;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#21644;&#33391;&#22909;&#30340;&#38544;&#31169;&#20445;&#25252;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.06212</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;&#30340;&#38544;&#31169;&#20445;&#25252;&#25552;&#31034;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving Prompt Tuning for Large Language Model Services. (arXiv:2305.06212v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06212
&lt;/p&gt;
&lt;p&gt;
RAPT&#26159;&#19968;&#20010;&#25552;&#20379;&#38544;&#31169;&#20445;&#35777;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;&#30340;&#25552;&#31034;&#35843;&#25972;&#26694;&#26550;&#65292;&#37319;&#29992;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#35774;&#32622;&#21644;&#26032;&#39062;&#30340;&#38544;&#31169;&#21270;&#26631;&#35760;&#37325;&#24314;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#21644;&#33391;&#22909;&#30340;&#38544;&#31169;&#20445;&#25252;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#35843;&#25972;&#20026;&#29992;&#25143;&#22312;&#26032;&#20852;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;&#22330;&#26223;&#19979;&#20351;&#29992;&#20854;&#31169;&#26377;&#25968;&#25454;&#33258;&#23450;&#20041;&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#26377;&#25928;&#26041;&#24335;&#12290;&#20294;&#26159;&#65292;&#31169;&#26377;&#25968;&#25454;&#30340;&#25935;&#24863;&#24615;&#38656;&#35201;&#22312;LLM&#26381;&#21153;&#23450;&#21046;&#20013;&#20445;&#25252;&#38544;&#31169;&#12290;&#22522;&#20110;&#25552;&#31034;&#35843;&#25972;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#38544;&#31169;&#20445;&#25252;&#25552;&#31034;&#35843;&#25972;(RAPT)&#30340;&#26694;&#26550;&#65292;&#20026;LLM&#26381;&#21153;&#25552;&#20379;&#38544;&#31169;&#20445;&#35777;&#12290;RAPT&#37319;&#29992;&#26412;&#22320;&#38544;&#31169;&#35774;&#32622;&#65292;&#20801;&#35768;&#29992;&#25143;&#20351;&#29992;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#23545;&#20854;&#25968;&#25454;&#36827;&#34892;&#26412;&#22320;&#21270;&#38544;&#31169;&#22788;&#29702;&#12290;&#30001;&#20110;&#22312;&#30452;&#25509;&#35757;&#32451;&#38544;&#31169;&#21270;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#31034;&#35843;&#25972;&#34920;&#29616;&#19981;&#20339;&#65292;&#22240;&#27492;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38544;&#31169;&#21270;&#26631;&#35760;&#37325;&#24314;&#20219;&#21153;&#65292;&#19982;&#19979;&#28216;&#20219;&#21153;&#19968;&#36215;&#36827;&#34892;&#22521;&#35757;&#65292;&#20351;LLM&#23398;&#20064;&#26356;&#22909;&#30340;&#20219;&#21153;&#30456;&#20851;&#34920;&#31034;&#12290;&#23613;&#31649;&#25105;&#20204;&#30340;&#26694;&#26550;&#31616;&#21333;&#65292;&#20294;&#23454;&#39564;&#34920;&#26126;&#65292;RAPT&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#22343;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#25269;&#24481;&#23545;&#25163;&#30340;&#38544;&#31169;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt tuning provides an efficient way for users to customize Large Language Models (LLMs) with their private data in the emerging LLM service scenario. However, the sensitive nature of private data brings the need for privacy preservation in LLM service customization. Based on prompt tuning, we propose Privacy-Preserving Prompt Tuning (RAPT), a framework that provides privacy guarantees for LLM services. \textsc{rapt} adopts a local privacy setting, allowing users to privatize their data locally with local differential privacy. As prompt tuning performs poorly when directly trained on privatized data, we introduce a novel privatized token reconstruction task that is trained jointly with the downstream task, allowing LLMs to learn better task-dependent representations. Despite the simplicity of our framework, experiments show that RAPT achieves competitive performance across tasks while providing privacy guarantees against adversaries.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;(RLGAF)&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#21462;&#20195;&#20165;&#21463;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;(RLHF)&#65292;&#20174;&#32780;&#28040;&#38500;&#35780;&#20272;&#32773;&#30340;&#19987;&#19994;&#38480;&#21046;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06176</link><description>&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#21453;&#39304;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning Language Models with Generative Adversarial Feedback. (arXiv:2305.06176v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;(RLGAF)&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#21462;&#20195;&#20165;&#21463;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;(RLHF)&#65292;&#20174;&#32780;&#28040;&#38500;&#35780;&#20272;&#32773;&#30340;&#19987;&#19994;&#38480;&#21046;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#26174;&#33879;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#24615;&#33021;&#65292;&#20351;&#20854;&#36755;&#20986;&#19982;&#20154;&#31867;&#26399;&#26395;&#30340;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;RLHF&#21463;&#21040;&#20154;&#31867;&#35780;&#20272;&#32773;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#29983;&#20135;&#21147;&#38480;&#21046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;: &#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;(RLGAF)&#20195;&#26367;RLHF&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#21457;&#29616;&#34920;&#26126;&#65292;RLGAF&#21487;&#20197;&#24110;&#21161;&#23545;&#40784;LLM&#30340;&#36755;&#20986;&#65292;&#21516;&#26102;&#19981;&#20250;&#21463;&#21040;RLHF&#22266;&#26377;&#30340;&#38480;&#21046;&#65292;&#20026;&#36827;&#19968;&#27493;&#33258;&#21160;&#21270;AI&#23545;&#40784;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Human Feedback (RLHF) has been demonstrated to significantly enhance the performance of large language models (LLMs) by aligning their outputs with desired human values. However, RLHF is constrained by the expertise and productivity limitations of human evaluators. In this study, we investigate an alternative approach: Reinforcement Learning with Generative Adversarial Feedback (RLGAF) to RLHF. Our preliminary findings indicate that RLGAF can help align LLMs outputs while not suffering from the inherent restrictions of RLHF, suggesting promising avenues for further research on automating AI alignment.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#24037;&#19994;&#12289;&#20513;&#23548;&#32452;&#32455;&#21644;&#27668;&#20505;&#20513;&#23548;&#32452;&#32455;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#22914;&#20309;&#24433;&#21709;&#27668;&#20505;&#21464;&#21270;&#30340;&#21465;&#20107;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26368;&#23567;&#21270;&#30417;&#30563;&#27169;&#22411;&#32452;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;Facebook&#19978;&#27668;&#20505;&#24191;&#21578;&#30340;&#31435;&#22330;&#12290;</title><link>http://arxiv.org/abs/2305.06174</link><description>&lt;p&gt;
&#20351;&#29992;&#36125;&#21494;&#26031;&#27169;&#22411;&#24179;&#22343;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#27668;&#20505;&#23459;&#20256;&#27963;&#21160;
&lt;/p&gt;
&lt;p&gt;
Analysis of Climate Campaigns on Social Media using Bayesian Model Averaging. (arXiv:2305.06174v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06174
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#24037;&#19994;&#12289;&#20513;&#23548;&#32452;&#32455;&#21644;&#27668;&#20505;&#20513;&#23548;&#32452;&#32455;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#22914;&#20309;&#24433;&#21709;&#27668;&#20505;&#21464;&#21270;&#30340;&#21465;&#20107;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26368;&#23567;&#21270;&#30417;&#30563;&#27169;&#22411;&#32452;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;Facebook&#19978;&#27668;&#20505;&#24191;&#21578;&#30340;&#31435;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27668;&#20505;&#21464;&#21270;&#26159;&#25105;&#20204;&#26102;&#20195;&#30340;&#26680;&#24515;&#38382;&#39064;&#65292;&#25105;&#20204;&#27491;&#22788;&#20110;&#19968;&#20010;&#20851;&#38190;&#26102;&#21051;&#12290;&#21508;&#31181;&#21033;&#30410;&#38598;&#22242;&#12289;&#31038;&#20250;&#36816;&#21160;&#32452;&#32455;&#21644;&#20010;&#20154;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#24320;&#23637;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#30340;&#38598;&#20307;&#34892;&#21160;&#12290;&#27492;&#22806;&#65292;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#38382;&#39064;&#20513;&#23548;&#27963;&#21160;&#24448;&#24448;&#26159;&#38024;&#23545;&#24403;&#21069;&#31038;&#20250;&#20851;&#27880;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#33021;&#28304;&#34892;&#19994;&#38754;&#20020;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#20998;&#26512;&#24037;&#19994;&#12289;&#20513;&#23548;&#32452;&#32455;&#21644;&#27668;&#20505;&#20513;&#23548;&#32452;&#32455;&#22914;&#20309;&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#24433;&#21709;&#27668;&#20505;&#21464;&#21270;&#30340;&#21465;&#20107;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26368;&#23567;&#21270;&#30417;&#30563;&#27169;&#22411;&#32452;&#21512;&#26041;&#27861;&#65292;&#24182;&#32467;&#21512;&#28040;&#24687;&#20027;&#39064;&#26469;&#35782;&#21035;Facebook&#19978;&#27668;&#20505;&#24191;&#21578;&#30340;&#31435;&#22330;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#31435;&#22330;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#19982;&#27668;&#20505;&#23459;&#20256;&#27963;&#21160;&#30456;&#20851;&#30340;&#20027;&#39064;&#65292;&#20379;&#26410;&#26469;&#30340;&#33286;&#24773;&#25366;&#25496;&#21644;&#33258;&#21160;&#26816;&#27979;&#27668;&#20505;&#21464;&#21270;&#31435;&#22330;&#30340;&#30740;&#31350;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Climate change is the defining issue of our time, and we are at a defining moment. Various interest groups, social movement organizations, and individuals engage in collective action on this issue on social media. In addition, issue advocacy campaigns on social media often arise in response to ongoing societal concerns, especially those faced by energy industries. Our goal in this paper is to analyze how those industries, their advocacy group, and climate advocacy group use social media to influence the narrative on climate change. In this work, we propose a minimally supervised model soup [56] approach combined with messaging themes to identify the stances of climate ads on Facebook. Finally, we release our stance dataset, model, and set of themes related to climate campaigns for future work on opinion mining and the automatic detection of climate change stances.
&lt;/p&gt;</description></item><item><title>QICHWABASE&#20026;&#23569;&#25968;&#27665;&#26063;&#35821;&#35328;&#21644;&#30693;&#35782;&#26500;&#24314;Wikibase&#23454;&#20363;&#65292;&#25903;&#25345;&#20975;&#26970;&#20122;&#31038;&#21306;&#21644;&#35856;&#36827;&#31243;&#65292;&#33021;&#22686;&#24378;&#23569;&#25968;&#27665;&#26063;&#22312;&#32593;&#32476;&#19978;&#30340;&#23384;&#22312;&#24863;&#12290;</title><link>http://arxiv.org/abs/2305.06173</link><description>&lt;p&gt;
QICHWABASE: &#19968;&#20010;&#38754;&#21521;&#20975;&#26970;&#20122;&#31038;&#21306;&#30340;&#20975;&#26970;&#20122;&#35821;&#35328;&#21644;&#30693;&#35782;&#24211;
&lt;/p&gt;
&lt;p&gt;
QICHWABASE: A Quechua Language and Knowledge Base for Quechua Communities. (arXiv:2305.06173v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06173
&lt;/p&gt;
&lt;p&gt;
QICHWABASE&#20026;&#23569;&#25968;&#27665;&#26063;&#35821;&#35328;&#21644;&#30693;&#35782;&#26500;&#24314;Wikibase&#23454;&#20363;&#65292;&#25903;&#25345;&#20975;&#26970;&#20122;&#31038;&#21306;&#21644;&#35856;&#36827;&#31243;&#65292;&#33021;&#22686;&#24378;&#23569;&#25968;&#27665;&#26063;&#22312;&#32593;&#32476;&#19978;&#30340;&#23384;&#22312;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#21313;&#24180;&#26469;&#65292;&#32593;&#32476;&#36234;&#26469;&#36234;&#25104;&#20026;&#35821;&#35328;&#21644;&#30693;&#35782;&#34920;&#31034;&#30340;&#31354;&#38388;&#12290;&#28982;&#32780;&#65292;&#36825;&#21482;&#38024;&#23545;&#24191;&#27867;&#27969;&#34892;&#30340;&#35821;&#35328;&#21644;&#31038;&#21306;&#65292;&#22312;&#23569;&#25968;&#27665;&#26063;&#31038;&#21306;&#21644;&#20854;&#36164;&#28304;&#26041;&#38754;&#21364;&#21463;&#21040;&#36739;&#23569;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;QICHWABASE&#20197;&#25903;&#25345;&#20975;&#26970;&#20122;&#35821;&#35328;&#21644;&#30693;&#35782;&#21450;&#20854;&#31038;&#21306;&#30340;&#21644;&#35856;&#36827;&#31243;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20123;&#26041;&#27861;&#21644;&#24037;&#20855;&#65292;&#33021;&#22815;&#25104;&#20026;&#20840;&#29699;&#20975;&#26970;&#20122;&#31038;&#21306;&#30340;&#21161;&#25512;&#22120;&#12290;&#25105;&#20204;&#24471;&#20986;&#30340;&#32467;&#35770;&#26159;&#65292;&#22312;&#26500;&#24314;QICHWABASE&#65292;&#21363;&#19968;&#20010;Wikibase&#23454;&#20363;&#26102;&#37319;&#29992;&#30340;&#26041;&#27861;&#21644;&#24037;&#20855;&#33021;&#22815;&#22686;&#24378;&#23569;&#25968;&#27665;&#26063;&#22312;&#32593;&#32476;&#19978;&#30340;&#23384;&#22312;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the last decade, the Web has increasingly become a space of language and knowledge representation. However, it is only true for well-spread languages and well-established communities, while minority communities and their resources received less attention. In this paper, we propose QICHWABASE to support the harmonization process of the Quechua language and knowledge, and its community. For doing it, we adopt methods and tools that could become a game changer in favour of Quechua communities around the world. We conclude that the methodology and tools adopted on building QICHWABASE, which is a Wikibase instance, could enhance the presence of minorities on the Web.
&lt;/p&gt;</description></item><item><title>ChatGPT&#20316;&#20026;&#25991;&#26412;&#31616;&#21270;&#24037;&#20855;&#21487;&#20197;&#21435;&#38500;&#35821;&#35328;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#26576;&#20123;&#29305;&#23450;&#32676;&#20307;&#30340;&#20559;&#35265;&#65292;&#20943;&#23569;&#27169;&#22411;&#30340;&#27495;&#35270;&#24615;&#12290;&#65288;&#27880;&#65306;ChatGPT&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#65289;</title><link>http://arxiv.org/abs/2305.06166</link><description>&lt;p&gt;
ChatGPT&#20316;&#20026;&#21435;&#38500;&#20559;&#35265;&#30340;&#25991;&#26412;&#31616;&#21270;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
ChatGPT as a Text Simplification Tool to Remove Bias. (arXiv:2305.06166v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06166
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#20316;&#20026;&#25991;&#26412;&#31616;&#21270;&#24037;&#20855;&#21487;&#20197;&#21435;&#38500;&#35821;&#35328;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#26576;&#20123;&#29305;&#23450;&#32676;&#20307;&#30340;&#20559;&#35265;&#65292;&#20943;&#23569;&#27169;&#22411;&#30340;&#27495;&#35270;&#24615;&#12290;&#65288;&#27880;&#65306;ChatGPT&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#65289;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#26399;&#38388;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#25429;&#25417;&#21040;&#29305;&#23450;&#23376;&#32676;&#20307;&#30340;&#29305;&#23450;&#35821;&#35328;&#20449;&#21495;&#65292;&#22914;&#26524;&#27169;&#22411;&#23398;&#20064;&#20102;&#25429;&#25417;&#26576;&#20010;&#32676;&#20307;&#30340;&#35821;&#35328;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#27495;&#35270;&#12290;&#22914;&#26524;&#27169;&#22411;&#24320;&#22987;&#23558;&#29305;&#23450;&#35821;&#35328;&#19982;&#26576;&#20010;&#29305;&#23450;&#32676;&#20307;&#32852;&#31995;&#36215;&#26469;&#65292;&#22522;&#20110;&#27492;&#35821;&#35328;&#20570;&#20986;&#30340;&#20219;&#20309;&#20915;&#31574;&#37117;&#23558;&#19982;&#20854;&#21463;&#20445;&#25252;&#29305;&#24449;&#26377;&#30528;&#24378;&#28872;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#21487;&#33021;&#30340;&#20559;&#35265;&#32531;&#35299;&#25216;&#26415;&#65292;&#21363;&#25991;&#26412;&#31616;&#21270;&#12290;&#36825;&#20010;&#24819;&#27861;&#30340;&#39537;&#21160;&#21147;&#26159;&#31616;&#21270;&#25991;&#26412;&#24212;&#35813;&#26631;&#20934;&#21270;&#35821;&#35328;&#65292;&#20351;&#20854;&#20197;&#19968;&#31181;&#26041;&#24335;&#35828;&#35805;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#21516;&#30340;&#21547;&#20041;&#12290;&#23454;&#39564;&#26174;&#31034;&#65292;&#38024;&#23545;&#25935;&#24863;&#23646;&#24615;&#39044;&#27979;&#30340;&#20998;&#31867;&#22120;&#31934;&#24230;&#20250;&#22240;&#20351;&#29992;&#31616;&#21270;&#25968;&#25454;&#32780;&#19979;&#38477;&#39640;&#36798;17%&#12290;
&lt;/p&gt;
&lt;p&gt;
The presence of specific linguistic signals particular to a certain sub-group of people can be picked up by language models during training. This may lead to discrimination if the model has learnt to pick up on a certain group's language. If the model begins to associate specific language with a distinct group, any decisions made based upon this language would hold a strong correlation to a decision based on their protected characteristic. We explore a possible technique for bias mitigation in the form of simplification of text. The driving force of this idea is that simplifying text should standardise language to one way of speaking while keeping the same meaning. The experiment shows promising results as the classifier accuracy for predicting the sensitive attribute drops by up to 17% for the simplified data.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21160;&#24577;&#21019;&#24314;&#30340;&#23376;&#22270;&#34920;&#31034;&#35805;&#35821;&#21450;&#19978;&#19979;&#25991;&#30340;&#20449;&#24687;&#26469;&#36827;&#34892;&#20250;&#35805;&#35821;&#20041;&#35299;&#26512;&#65292;&#24182;&#21033;&#29992;&#22270;&#24418;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#65292;&#21487;&#34920;&#31034;&#22823;&#37327;&#30475;&#19981;&#35265;&#30340;&#33410;&#28857;&#65292;&#27604;&#38745;&#24577;&#26041;&#27861;&#26356;&#20026;&#20248;&#36234;&#12290;</title><link>http://arxiv.org/abs/2305.06164</link><description>&lt;p&gt;
&#21160;&#24577;&#19978;&#19979;&#25991;&#22270;&#24418;&#23454;&#29616;&#23545;&#19975;&#29289;&#30693;&#35782;&#22270;&#35889;&#30340;&#20250;&#35805;&#35821;&#20041;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
Conversational Semantic Parsing using Dynamic Context Graphs. (arXiv:2305.06164v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21160;&#24577;&#21019;&#24314;&#30340;&#23376;&#22270;&#34920;&#31034;&#35805;&#35821;&#21450;&#19978;&#19979;&#25991;&#30340;&#20449;&#24687;&#26469;&#36827;&#34892;&#20250;&#35805;&#35821;&#20041;&#35299;&#26512;&#65292;&#24182;&#21033;&#29992;&#22270;&#24418;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#65292;&#21487;&#34920;&#31034;&#22823;&#37327;&#30475;&#19981;&#35265;&#30340;&#33410;&#28857;&#65292;&#27604;&#38745;&#24577;&#26041;&#27861;&#26356;&#20026;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#22312;&#25317;&#26377;&#25968;&#30334;&#19975;&#20010;&#23454;&#20307;&#21644;&#25968;&#21315;&#31181;&#20851;&#31995;&#31867;&#22411;&#30340;&#36890;&#29992;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#20250;&#35805;&#35821;&#20041;&#35299;&#26512;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#33268;&#21147;&#20110;&#24320;&#21457;&#33021;&#22815;&#20132;&#20114;&#22320;&#23558;&#29992;&#25143;&#35821;&#35328;&#26144;&#23556;&#20026;&#21487;&#25191;&#34892;&#36923;&#36753;&#24418;&#24335;&#65288;&#20363;&#22914;SPARQL&#65289;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#23545;&#35805;&#21382;&#21490;&#30340;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24819;&#27861;&#26159;&#36890;&#36807;&#19968;&#20010;&#21160;&#24577;&#21019;&#24314;&#30340;&#23376;&#22270;&#26469;&#34920;&#31034;&#26377;&#20851;&#35805;&#35821;&#21450;&#20854;&#19978;&#19979;&#25991;&#30340;&#20449;&#24687;&#65292;&#21363;&#27599;&#20010;&#35805;&#35821;&#30340;&#33410;&#28857;&#25968;&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;&#32780;&#19988;&#65292;&#25105;&#20204;&#21033;&#29992;&#23376;&#22270;&#30340;&#22522;&#26412;&#32467;&#26500;&#65292;&#32780;&#19981;&#26159;&#23558;&#20854;&#35270;&#20026;&#24207;&#21015;&#65292;&#20351;&#29992;&#22270;&#24418;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#32534;&#30721;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#20801;&#35768;&#25105;&#20204;&#34920;&#31034;&#22823;&#37327;&#65288;&#30475;&#19981;&#35265;&#30340;&#65289;&#33410;&#28857;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21160;&#24577;&#24314;&#27169;&#19978;&#19979;&#25991;&#20248;&#20110;&#38745;&#24577;&#26041;&#27861;&#65292;&#21487;&#22312;&#21508;&#20010;&#26041;&#38754;&#65288;&#21363;&#31616;&#21333;&#21644;&#22797;&#26434;&#38382;&#39064;&#65289;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36827;&#19968;&#27493;&#35777;&#23454;&#65292;&#27169;&#22411;&#21270;&#19978;&#19979;&#25991;&#32467;&#26500;&#27604;&#20165;&#32771;&#34385;&#21333;&#20010;&#35805;&#35821;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we consider the task of conversational semantic parsing over general purpose knowledge graphs (KGs) with millions of entities, and thousands of relation-types. We are interested in developing models capable of interactively mapping user utterances into executable logical forms (e.g., SPARQL) in the context of the conversational history. Our key idea is to represent information about an utterance and its context via a subgraph which is created dynamically, i.e., the number of nodes varies per utterance. Moreover, rather than treating the subgraph as a sequence we exploit its underlying structure, and thus encode it using a graph neural network which further allows us to represent a large number of (unseen) nodes. Experimental results show that modeling context dynamically is superior to static approaches, delivering performance improvements across the board (i.e., for simple and complex questions). Our results further confirm that modeling the structure of context is bette
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#25968;&#38169;&#35823;&#20998;&#31867;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#22788;&#29702;&#23398;&#29983;&#22238;&#31572;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.06163</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#25968;&#38169;&#35823;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Algebra Error Classification with Large Language Models. (arXiv:2305.06163v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#25968;&#38169;&#35823;&#20998;&#31867;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#22788;&#29702;&#23398;&#29983;&#22238;&#31572;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22238;&#31572;&#24320;&#25918;&#24335;&#25968;&#23398;&#38382;&#39064;&#26102;&#33258;&#21160;&#21453;&#39304;&#20855;&#26377;&#26174;&#33879;&#30340;&#28508;&#21147;&#26469;&#25552;&#39640;&#22823;&#35268;&#27169;&#23398;&#20064;&#25104;&#26524;&#12290;&#33258;&#21160;&#21453;&#39304;&#31995;&#32479;&#30340;&#20851;&#38190;&#37096;&#20998;&#26159;&#38169;&#35823;&#20998;&#31867;&#32452;&#20214;&#65292;&#35813;&#32452;&#20214;&#35782;&#21035;&#23398;&#29983;&#30340;&#38169;&#35823;&#65292;&#24182;&#21551;&#29992;&#36866;&#24403;&#30340;&#39044;&#23450;&#20041;&#21453;&#39304;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#38169;&#35823;&#20998;&#31867;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#65292;&#20854;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#12290;&#29616;&#26377;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#36991;&#20813;&#20102;&#36825;&#20123;&#38480;&#21046;&#65292;&#20294;&#20855;&#20307;&#35201;&#27714;&#23558;&#23398;&#29983;&#31572;&#22797;&#20013;&#30340;&#25968;&#23398;&#34920;&#36798;&#24335;&#35299;&#26512;&#20026;&#35821;&#27861;&#26641;&#12290;&#36825;&#19968;&#35201;&#27714;&#26412;&#36523;&#23601;&#26159;&#19968;&#20010;&#38480;&#21046;&#65292;&#22240;&#20026;&#23398;&#29983;&#30340;&#31572;&#22797;&#24182;&#19981;&#24635;&#26159;&#31526;&#21512;&#35821;&#27861;&#30340;&#65292;&#26080;&#27861;&#36716;&#25442;&#20026;&#26641;&#24418;&#32467;&#26500;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38169;&#35823;&#20998;&#31867;&#30340;&#28789;&#27963;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#20195;&#25968;&#38169;&#35823;&#20998;&#31867;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#33021;&#22815;&#23545;&#26356;&#22823;&#30340;&#23398;&#29983;&#22238;&#31572;&#36827;&#34892;&#20998;&#31867;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25512;&#24191;&#21040;&#20854;&#20182;&#39046;&#22495;&#21644;&#35821;&#35328;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated feedback as students answer open-ended math questions has significant potential in improving learning outcomes at large scale. A key part of automated feedback systems is an error classification component, which identifies student errors and enables appropriate, predefined feedback to be deployed. Most existing approaches to error classification use a rule-based method, which has limited capacity to generalize. Existing data-driven methods avoid these limitations but specifically require mathematical expressions in student responses to be parsed into syntax trees. This requirement is itself a limitation, since student responses are not always syntactically valid and cannot be converted into trees. In this work, we introduce a flexible method for error classification using pre-trained large language models. We demonstrate that our method can outperform existing methods in algebra error classification, and is able to classify a larger set of student responses. Additionally, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#20855;&#26377;15.5B&#21442;&#25968;&#21644;8K&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#8212;&#8212;StarCoder&#65292;&#20854;&#21487;&#20197;&#36827;&#34892;&#24555;&#36895;&#22823;&#25209;&#37327;&#25512;&#29702;&#12290;&#32463;&#35780;&#20272;&#35777;&#26126;&#65292;&#22312;Python&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#33021;&#22815;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#33719;&#24471;40\%&#30340;pass@1&#30340;&#24471;&#20998;&#65292;&#19988;&#22312;&#20854;&#20182;&#31243;&#24207;&#20013;&#20063;&#34920;&#29616;&#20986;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06161</link><description>&lt;p&gt;
StarCoder: &#28304;&#20195;&#30721;&#19982;&#20320;&#21516;&#22312;&#65281;
&lt;/p&gt;
&lt;p&gt;
StarCoder: may the source be with you!. (arXiv:2305.06161v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06161
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#20855;&#26377;15.5B&#21442;&#25968;&#21644;8K&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#8212;&#8212;StarCoder&#65292;&#20854;&#21487;&#20197;&#36827;&#34892;&#24555;&#36895;&#22823;&#25209;&#37327;&#25512;&#29702;&#12290;&#32463;&#35780;&#20272;&#35777;&#26126;&#65292;&#22312;Python&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#33021;&#22815;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#33719;&#24471;40\%&#30340;pass@1&#30340;&#24471;&#20998;&#65292;&#19988;&#22312;&#20854;&#20182;&#31243;&#24207;&#20013;&#20063;&#34920;&#29616;&#20986;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
BigCode&#31038;&#21306;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#31185;&#23398;&#21512;&#20316;&#32452;&#32455;&#65292;&#33268;&#21147;&#20110;&#24320;&#21457;&#20195;&#34920;&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Code LLMs&#65289;&#30340;&#36127;&#36131;&#20219;&#21457;&#23637;&#12290;&#35813;&#25991;&#20171;&#32461;&#20102;StarCoder&#21644;StarCoderBase&#65292;&#36825;&#26159;&#20855;&#26377;15.5B&#21442;&#25968;&#27169;&#22411;&#21644;8K&#19978;&#19979;&#25991;&#38271;&#24230;&#12289;&#22635;&#20805;&#33021;&#21147;&#20197;&#21450;&#22810;&#31181;&#26597;&#35810;&#27880;&#24847;&#21147;&#23454;&#29616;&#30340;&#24555;&#36895;&#22823;&#25209;&#37327;&#25512;&#29702;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#23545;StarCoderBase&#30340;1&#19975;&#20159;&#20010;&#26631;&#35760;&#36827;&#34892; fine-tuning&#65292;&#21019;&#24314;&#20102;StarCoder&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#20840;&#38754;&#30340;Code LLMs&#35780;&#20272;&#65292;&#24182;&#34920;&#26126;StarCoderBase&#20248;&#20110;&#25903;&#25345;&#22810;&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;&#27599;&#20010;&#24320;&#25918;Code LLM&#65292;&#24182;&#19982;OpenAI code-cushman-001&#27169;&#22411;&#30456;&#21305;&#37197;&#25110;&#20248;&#20110;&#35813;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;StarCoder&#22312;Python&#19978;&#20063;&#34920;&#29616;&#20986;&#20248;&#24322;&#24615;&#33021;&#65292;&#33021;&#22815;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#33719;&#24471;40\%&#30340;pass@1&#30340;&#24471;&#20998;&#65292;&#24182;&#20173;&#28982;&#20445;&#25345;&#20854;&#22312;&#20854;&#20182;&#31243;&#24207;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process. We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder. We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms every model that is fine-tuned on Python, can be prompted to achieve 40\% pass@1 on HumanEval, and still retains its performance on other program
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#21450;&#20854;&#22312;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#23457;&#26680;&#19978;&#30340;&#24212;&#29992;&#65292;&#30740;&#31350;&#21457;&#29616;&#26089;&#26399;&#34701;&#21512;&#27169;&#22411;&#27604;&#26202;&#26399;&#34701;&#21512;&#27169;&#22411;&#26356;&#26377;&#25928;&#65292;&#20854;&#20013;&#34920;&#29616;&#26368;&#20339;&#30340;&#26089;&#26399;&#34701;&#21512;&#27169;&#22411;&#26159;ClipBERT&#12290;</title><link>http://arxiv.org/abs/2305.06159</link><description>&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#32508;&#36848;&#21450;&#22312;&#8220;&#24694;&#24847;&#34920;&#24773;&#8221;&#25361;&#25112;&#20013;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
A Review of Vision-Language Models and their Performance on the Hateful Memes Challenge. (arXiv:2305.06159v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06159
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#21450;&#20854;&#22312;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#23457;&#26680;&#19978;&#30340;&#24212;&#29992;&#65292;&#30740;&#31350;&#21457;&#29616;&#26089;&#26399;&#34701;&#21512;&#27169;&#22411;&#27604;&#26202;&#26399;&#34701;&#21512;&#27169;&#22411;&#26356;&#26377;&#25928;&#65292;&#20854;&#20013;&#34920;&#29616;&#26368;&#20339;&#30340;&#26089;&#26399;&#34701;&#21512;&#27169;&#22411;&#26159;ClipBERT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#30340;&#23457;&#26680;&#30446;&#21069;&#20173;&#28982;&#26159;&#19968;&#39033;&#39640;&#24230;&#25163;&#21160;&#30340;&#20219;&#21153;&#65292;&#28982;&#32780;&#27599;&#22825;&#21457;&#24067;&#30340;&#20869;&#23481;&#37327;&#22826;&#22810;&#65292;&#38590;&#20197;&#26377;&#25928;&#25191;&#34892;&#12290;&#38543;&#30528;&#35768;&#22810;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#26377;&#28508;&#21147;&#38477;&#20302;&#35813;&#20219;&#21153;&#30340;&#25163;&#21160;&#21171;&#21160;&#37327;&#12290;&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#19981;&#21516;&#30340;&#27169;&#22411;&#24182;&#30830;&#23450;&#22312;&#8220;&#24694;&#24847;&#34920;&#24773;&#8221;&#25361;&#25112;&#20013;&#26368;&#26377;&#25928;&#30340;&#27169;&#22411;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26089;&#26399;&#34701;&#21512;&#21644;&#26202;&#26399;&#34701;&#21512;&#27169;&#22411;&#22312;&#20998;&#31867;&#21253;&#21547;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#22810;&#27169;&#24577;&#34920;&#24773;&#20013;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;BERT&#21644;ResNet-152&#20998;&#21035;&#23454;&#29616;&#20102;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#21333;&#27169;&#24577;&#22522;&#32447;&#27169;&#22411;&#12290;&#28982;&#21518;&#23558;&#36825;&#20123;&#21333;&#27169;&#24577;&#27169;&#22411;&#30340;&#36755;&#20986;&#36830;&#25509;&#22312;&#19968;&#36215;&#21019;&#24314;&#20102;&#19968;&#20010;&#26202;&#26399;&#34701;&#21512;&#27169;&#22411;&#12290;&#22312;&#26089;&#26399;&#34701;&#21512;&#27169;&#22411;&#26041;&#38754;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;ConcatBERT&#12289;VisualBERT&#12289;ViLT&#12289;CLIP&#21644;BridgeTower&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;&#26202;&#26399;&#34701;&#21512;&#27169;&#22411;&#30340;&#34920;&#29616;&#26126;&#26174;&#19981;&#22914;&#26089;&#26399;&#34701;&#21512;&#27169;&#22411;&#65292;&#32780;&#34920;&#29616;&#26368;&#20339;&#30340;&#26089;&#26399;&#34701;&#21512;&#27169;&#22411;&#26159;ClipBERT&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#26377;&#28508;&#21147;&#25913;&#21892;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#24694;&#24847;&#20869;&#23481;&#23457;&#26680;&#65292;&#20294;&#36824;&#38656;&#35201;&#26356;&#22810;&#30340;&#30740;&#31350;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Moderation of social media content is currently a highly manual task, yet there is too much content posted daily to do so effectively. With the advent of a number of multimodal models, there is the potential to reduce the amount of manual labor for this task. In this work, we aim to explore different models and determine what is most effective for the Hateful Memes Challenge, a challenge by Meta designed to further machine learning research in content moderation. Specifically, we explore the differences between early fusion and late fusion models in classifying multimodal memes containing text and images. We first implement a baseline using unimodal models for text and images separately using BERT and ResNet-152, respectively. The outputs from these unimodal models were then concatenated together to create a late fusion model. In terms of early fusion models, we implement ConcatBERT, VisualBERT, ViLT, CLIP, and BridgeTower. It was found that late fusion performed significantly worse th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#32473;&#22522;&#32447;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#28155;&#21152;&#35821;&#35328;&#30693;&#35782;&#21450;&#22810;&#35789;&#35821;&#32763;&#35793;&#23376;&#27169;&#22359;&#65292;&#22312;&#33521;&#35821;&#21040;&#24052;&#25552;&#30450;&#25991;&#26426;&#22120;&#32763;&#35793;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.06157</link><description>&lt;p&gt;
&#22810;&#35789;&#35821;&#23545;&#20110;&#33521;&#35821;&#21040;&#24052;&#25552;&#30450;&#25991;&#26426;&#22120;&#32763;&#35793;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Implications of Multi-Word Expressions on English to Bharti Braille Machine Translation. (arXiv:2305.06157v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#32473;&#22522;&#32447;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#28155;&#21152;&#35821;&#35328;&#30693;&#35782;&#21450;&#22810;&#35789;&#35821;&#32763;&#35793;&#23376;&#27169;&#22359;&#65292;&#22312;&#33521;&#35821;&#21040;&#24052;&#25552;&#30450;&#25991;&#26426;&#22120;&#32763;&#35793;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#25552;&#39640;&#33521;&#35821;&#21040;&#24052;&#25552;&#30450;&#25991;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#65292;&#36890;&#36807;&#22312;&#22522;&#32447;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#20013;&#21152;&#20837;&#35821;&#35328;&#30693;&#35782;&#65292;&#23545;&#20116;&#31181;&#35821;&#35328;&#23545;&#36827;&#34892;&#23454;&#39564;&#65292;&#21363;&#23558;&#33521;&#35821;&#21477;&#23376;&#32763;&#35793;&#25104;&#20116;&#31181;&#21360;&#24230;&#35821;&#35328;&#65292;&#24182;&#38543;&#21518;&#32763;&#35793;&#25104;&#30456;&#24212;&#30340;&#24052;&#25552;&#30450;&#25991;&#12290;&#36890;&#36807;&#28155;&#21152;&#23376;&#27169;&#22359;&#32763;&#35793;&#22810;&#35789;&#35821;&#65292;&#26412;&#30740;&#31350;&#26174;&#31034;&#20102;&#21487;&#34892;&#30340;&#26041;&#27861;&#65292;&#36328;&#35821;&#35328;&#23545; NMT &#36755;&#20986;&#36136;&#37327;&#26377;&#25152;&#25552;&#39640;&#12290;&#26368;&#23567;&#30340;&#25913;&#36827;&#20986;&#29616;&#22312;&#33521;&#35821;-&#23612;&#27850;&#23572;&#35821;&#23545;&#20013;&#65292;&#20026; 22.08%&#65292;&#26368;&#22823;&#30340;&#25913;&#36827;&#20986;&#29616;&#22312;&#33521;&#35821;-&#21360;&#22320;&#35821;&#23545;&#20013;&#65292;&#20026; 23.30%&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we have shown the improvement of English to Bharti Braille machine translation system. We have shown how we can improve a baseline NMT model by adding some linguistic knowledge to it. This was done for five language pairs where English sentences were translated into five Indian languages and then subsequently to corresponding Bharti Braille. This has been demonstrated by adding a sub-module for translating multi-word expressions. The approach shows promising results as across language pairs, we could see improvement in the quality of NMT outputs. The least improvement was observed in English-Nepali language pair with 22.08% and the most improvement was observed in the English-Hindi language pair with 23.30%.
&lt;/p&gt;</description></item><item><title>The Vault&#26159;&#19968;&#20010;&#25552;&#20379;&#20102;10&#31181;&#27969;&#34892;&#32534;&#31243;&#35821;&#35328;&#30340;40&#30334;&#19975;&#34892;&#20195;&#30721;-&#25991;&#26412;&#23545;&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#22686;&#24378;&#38754;&#21521;&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35757;&#32451;&#65292;&#26377;&#26395;&#22312;&#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#19978;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2305.06156</link><description>&lt;p&gt;
The Vault&#65306;&#19968;&#20010;&#20840;&#38754;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#20026;&#20419;&#36827;&#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#32780;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
The Vault: A Comprehensive Multilingual Dataset for Advancing Code Understanding and Generation. (arXiv:2305.06156v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06156
&lt;/p&gt;
&lt;p&gt;
The Vault&#26159;&#19968;&#20010;&#25552;&#20379;&#20102;10&#31181;&#27969;&#34892;&#32534;&#31243;&#35821;&#35328;&#30340;40&#30334;&#19975;&#34892;&#20195;&#30721;-&#25991;&#26412;&#23545;&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#22686;&#24378;&#38754;&#21521;&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35757;&#32451;&#65292;&#26377;&#26395;&#22312;&#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#19978;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102; The Vault&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#22823;&#35268;&#27169;&#20195;&#30721;&#25991;&#26412;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#22686;&#24378;&#38754;&#21521;&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35757;&#32451;&#12290;&#29616;&#26377;&#30340;&#29992;&#20110;&#35757;&#32451;&#22522;&#20110;&#20195;&#30721;&#30340;LLM&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#22312;&#22823;&#23567;&#12289;&#36136;&#37327;(&#30001;&#20110;&#22122;&#22768;&#20449;&#21495;)&#21644;&#26684;&#24335;&#65288;&#20165;&#21253;&#21547;&#20195;&#30721;&#20989;&#25968;&#21644;&#25991;&#26412;&#35828;&#26126;&#37197;&#23545;&#65289;&#26041;&#38754;&#32463;&#24120;&#38754;&#20020;&#25361;&#25112;&#12290;The Vault&#36890;&#36807;&#25552;&#20379;10&#31181;&#27969;&#34892;&#32534;&#31243;&#35821;&#35328;&#30340;40&#30334;&#19975;&#34892;&#20195;&#30721;-&#25991;&#26412;&#23545;&#65292;&#24443;&#24213;&#28165;&#38500;10&#31181;&#22810;&#26679;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#21508;&#31181;&#32423;&#21035;&#30340;&#20195;&#30721;-&#25991;&#26412;&#23545;&#65292;&#21253;&#25324;&#31867;&#12289;&#20989;&#25968;&#21644;&#20195;&#30721;&#34892;&#31561;&#32423;&#21035;&#65292;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#20154;&#21592;&#21487;&#20197;&#21033;&#29992;The Vault&#26469;&#35757;&#32451;&#19981;&#21516;&#30340;&#38754;&#21521;&#20195;&#30721;&#30340;LLM&#65292;&#25110;&#32773;&#23558;&#25552;&#20379;&#30340;&#25968;&#25454;&#28165;&#27927;&#26041;&#27861;&#21644;&#33050;&#26412;&#21512;&#24182;&#21040;&#33258;&#24049;&#30340;&#25968;&#25454;&#38598;&#20013;&#26469;&#25913;&#36827;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#23558;The Vault&#20316;&#20026;&#38754;&#21521;&#20195;&#30721;&#30340;LLMs&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#39044;&#35745;&#22312;&#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#19978;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#65292;&#20419;&#36827;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#21644;&#23454;&#36341;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present The Vault, an open-source, large-scale code-text dataset designed to enhance the training of code-focused large language models (LLMs). Existing open-source datasets for training code-based LLMs often face challenges in terms of size, quality (due to noisy signals), and format (only containing code function and text explanation pairings). The Vault overcomes these limitations by providing 40 million code-text pairs across 10 popular programming languages, thorough cleaning for 10+ prevalent issues, and various levels of code-text pairings, including class, function, and line levels. Researchers and practitioners can utilize The Vault for training diverse code-focused LLMs or incorporate the provided data cleaning methods and scripts to improve their datasets. By employing The Vault as the training dataset for code-centric LLMs, we anticipate significant advancements in code understanding and generation tasks, fostering progress in both artificial intelligence research and so
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#30446;&#26631;&#20174;&#32780;&#25552;&#39640;&#26426;&#22120;&#32763;&#35793;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20854;&#22312;&#19981;&#21516;&#27979;&#35797;&#22522;&#20934;&#19979;&#30340;&#34920;&#29616;&#20248;&#20110;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#35757;&#32451;&#65292;&#36825;&#19968;&#26041;&#27861;&#22312;&#26377;&#38480;&#36164;&#28304;&#30340;&#24773;&#20917;&#19979;&#23588;&#20854;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.06155</link><description>&lt;p&gt;
&#21033;&#29992;&#21512;&#25104;&#30446;&#26631;&#36827;&#34892;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Leveraging Synthetic Targets for Machine Translation. (arXiv:2305.06155v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#30446;&#26631;&#20174;&#32780;&#25552;&#39640;&#26426;&#22120;&#32763;&#35793;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20854;&#22312;&#19981;&#21516;&#27979;&#35797;&#22522;&#20934;&#19979;&#30340;&#34920;&#29616;&#20248;&#20110;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#35757;&#32451;&#65292;&#36825;&#19968;&#26041;&#27861;&#22312;&#26377;&#38480;&#36164;&#28304;&#30340;&#24773;&#20917;&#19979;&#23588;&#20854;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#26377;&#38480;&#36164;&#28304;&#24773;&#20917;&#19979;&#35757;&#32451;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#36890;&#36807;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#29983;&#25104;&#30340;&#21512;&#25104;&#30446;&#26631;&#25968;&#25454;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#21452;&#35821;&#12289;&#22810;&#35821;&#35328;&#21644;&#35821;&#38899;&#32763;&#35793;&#35774;&#32622;&#30340;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#20351;&#29992;&#21512;&#25104;&#30446;&#26631;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#33021;&#20248;&#20110;&#20351;&#29992;&#23454;&#38469;&#30340;&#27491;&#30830;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#12290;&#36825;&#31181;&#24615;&#33021;&#24046;&#36317;&#38543;&#30528;&#21487;&#29992;&#36164;&#28304;&#30340;&#38480;&#21046;&#65288;&#25968;&#25454;&#38598;&#22823;&#23567;&#21644;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#65289;&#30340;&#22686;&#21152;&#32780;&#21464;&#24471;&#26356;&#22823;&#12290;&#25105;&#20204;&#36824;&#23545;&#24615;&#33021;&#25552;&#21319;&#26159;&#21542;&#19982;&#20248;&#21270;&#30340;&#20415;&#21033;&#24615;&#25110;&#39044;&#27979;&#30340;&#26356;&#30830;&#23450;&#24615;&#30456;&#20851;&#36827;&#34892;&#20102;&#21021;&#27493;&#20998;&#26512;&#65292;&#20197;&#21450;&#36825;&#31181;&#33539;&#20363;&#26159;&#21542;&#33021;&#22815;&#25552;&#39640;&#22312;&#19981;&#21516;&#27979;&#35797;&#39046;&#22495;&#30340;&#36229;&#20986;&#20998;&#24067;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we provide a recipe for training machine translation models in a limited resource setting by leveraging synthetic target data generated using a large pre-trained model. We show that consistently across different benchmarks in bilingual, multilingual, and speech translation setups, training models on synthetic targets outperforms training on the actual ground-truth data. This performance gap grows bigger with increasing limits on the amount of available resources in the form of the size of the dataset and the number of parameters in the model. We also provide preliminary analysis into whether this boost in performance is linked to ease of optimization or more deterministic nature of the predictions, and whether this paradigm leads to better out-of-distribution performance across different testing domains.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#23545;&#25239;&#23398;&#20064;&#65288;SSCL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;PLMs&#36739;&#20302;&#23618;&#20013;&#37319;&#26679;&#36127;&#26679;&#26412;&#65292;&#32531;&#35299;&#20102;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#24179;&#28369;&#38382;&#39064;&#65292;&#25552;&#21319;&#20102;&#21477;&#23376;&#34920;&#31034;&#30340;&#36136;&#37327;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06154</link><description>&lt;p&gt;
&#32531;&#35299;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#20013;&#30340;&#24179;&#28369;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Alleviating Over-smoothing for Unsupervised Sentence Representation. (arXiv:2305.06154v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#23545;&#25239;&#23398;&#20064;&#65288;SSCL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;PLMs&#36739;&#20302;&#23618;&#20013;&#37319;&#26679;&#36127;&#26679;&#26412;&#65292;&#32531;&#35299;&#20102;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#24179;&#28369;&#38382;&#39064;&#65292;&#25552;&#21319;&#20102;&#21477;&#23376;&#34920;&#31034;&#30340;&#36136;&#37327;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#65292;&#23398;&#20064;&#26356;&#22909;&#30340;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#26159;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31038;&#21306;&#30340;&#36861;&#27714;&#12290;&#35768;&#22810;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21644;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;&#27492;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#36807;&#24230;&#24179;&#28369;&#30340;&#38382;&#39064;&#38477;&#20302;&#20102;&#36825;&#20123;&#24378;&#22823;PLMs&#30340;&#33021;&#21147;&#65292;&#23548;&#33268;&#23376;&#20248;&#21477;&#23376;&#34920;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#33258;&#23545;&#25239;&#23398;&#20064;&#65288;SSCL&#65289;&#65292;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#20174;PLMs&#20013;&#38388;&#23618;&#20013;&#37319;&#26679;&#36127;&#38754;&#26679;&#26412;&#65292;&#25552;&#39640;&#21477;&#23376;&#34920;&#31034;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#38750;&#24120;&#31616;&#21333;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#25193;&#23637;&#21040;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#24615;&#33021;&#25552;&#21319;&#65292;&#21487;&#20197;&#30475;&#20316;&#26159;&#19968;&#31181;&#25554;&#20214;&#24335;&#30340;&#23545;&#27604;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#12290;&#24191;&#27867;&#30340;&#32467;&#26524;&#35777;&#26126;&#65292;SSCL&#24102;&#26469;&#20102;&#19981;&#21516;&#24378;&#22522;&#32447;&#30340;&#21331;&#36234;&#24615;&#33021;&#25913;&#36827;&#65288;&#20363;&#22914;BERT&#21644;SimCSE&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Currently, learning better unsupervised sentence representations is the pursuit of many natural language processing communities. Lots of approaches based on pre-trained language models (PLMs) and contrastive learning have achieved promising results on this task. Experimentally, we observe that the over-smoothing problem reduces the capacity of these powerful PLMs, leading to sub-optimal sentence representations. In this paper, we present a Simple method named Self-Contrastive Learning (SSCL) to alleviate this issue, which samples negatives from PLMs intermediate layers, improving the quality of the sentence representation. Our proposed method is quite simple and can be easily extended to various state-of-the-art models for performance boosting, which can be seen as a plug-and-play contrastive framework for learning unsupervised sentence representation. Extensive results prove that SSCL brings the superior performance improvements of different strong baselines (e.g., BERT and SimCSE) on
&lt;/p&gt;</description></item><item><title>Structure-CLIP&#20351;&#29992;&#25991;&#26412;&#20013;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#20351;&#29992;&#22330;&#26223;&#22270;&#24378;&#21270;&#22810;&#27169;&#24577;&#35821;&#35328;&#34920;&#31034;&#65292;&#20174;&#32780;&#22312;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06152</link><description>&lt;p&gt;
Structure-CLIP: &#32467;&#21512;&#32467;&#26500;&#30693;&#35782;&#20248;&#21270;&#22810;&#27169;&#24577;&#35821;&#35328;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Structure-CLIP: Enhance Multi-modal Language Representations with Structure Knowledge. (arXiv:2305.06152v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06152
&lt;/p&gt;
&lt;p&gt;
Structure-CLIP&#20351;&#29992;&#25991;&#26412;&#20013;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#20351;&#29992;&#22330;&#26223;&#22270;&#24378;&#21270;&#22810;&#27169;&#24577;&#35821;&#35328;&#34920;&#31034;&#65292;&#20174;&#32780;&#22312;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#27169;&#24577;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#38656;&#35201;&#23545;&#25991;&#26412;&#36827;&#34892;&#35814;&#32454;&#35821;&#20041;&#29702;&#35299;&#30340;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#20219;&#21153;&#19978;&#36890;&#24120;&#34920;&#29616;&#36739;&#24046;&#12290;&#23613;&#31649;&#24050;&#32463;&#26377;&#19968;&#20123;&#30740;&#31350;&#22312;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#21477;&#23376;&#20013;&#23384;&#22312;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#26469;&#22686;&#24378;&#22810;&#27169;&#24577;&#35821;&#35328;&#34920;&#31034;&#65292;&#23548;&#33268;&#24615;&#33021;&#36739;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#26694;&#26550;Structure-CLIP&#65292;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#30340;&#38544;&#24335;&#35814;&#32454;&#35821;&#20041;&#65292;&#20197;&#22686;&#24378;&#31934;&#32454;&#30340;&#35821;&#20041;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;(1)&#25105;&#20204;&#20351;&#29992;&#22330;&#26223;&#22270;&#26469;&#26356;&#21152;&#20851;&#27880;&#25991;&#26412;&#20013;&#30340;&#35814;&#32454;&#35821;&#20041;&#23398;&#20064;&#65292;&#24182;&#20805;&#20998;&#25506;&#32034;&#32454;&#31890;&#24230;&#35821;&#20041;&#20043;&#38388;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;(2)&#25105;&#20204;&#32467;&#21512;&#22330;&#26223;&#22270;&#30340;&#30693;&#35782;&#24378;&#21270;&#26694;&#26550;&#26469;&#20805;&#20998;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale vision-language pre-training has shown promising advances on various downstream tasks and achieved significant performance in multi-modal understanding and generation tasks. However, existing methods often perform poorly on image-text matching tasks that require a detailed semantics understanding of the text. Although there have been some works on this problem, they do not sufficiently exploit the structural knowledge present in sentences to enhance multi-modal language representations, which leads to poor performance. In this paper, we present an end-to-end framework Structure-CLIP, which integrates latent detailed semantics from the text to enhance fine-grained semantic representations. Specifically, (1) we use scene graphs in order to pay more attention to the detailed semantic learning in the text and fully explore structured knowledge between fine-grained semantics, and (2) we utilize the knowledge-enhanced framework with the help of the scene graph to make full use of
&lt;/p&gt;</description></item><item><title>&#36816;&#36755;&#34892;&#19994;&#20351;&#29992;OCR&#25216;&#26415;&#23545;&#25991;&#20214;&#36827;&#34892;&#20998;&#31867;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#35813;&#30740;&#31350;&#25552;&#20986;&#21322;&#33258;&#21160;&#21270;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#20851;&#38190;&#35789;&#39057;&#29575;&#26500;&#24314;&#19968;&#20010;&#31283;&#20581;&#30340;&#25991;&#26723;&#20998;&#31867;&#31995;&#32479;&#65292;&#20854;&#22312;&#25910;&#38598;&#30340;85&#20010;&#36829;&#32422;&#26696;&#20363;&#21644;555&#20010;&#38750;&#36829;&#32422;&#26696;&#20363;&#19978;&#23454;&#29616;&#20102;93.31%&#30340;&#39640;&#20998;&#31867;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.06148</link><description>&lt;p&gt;
&#19968;&#31181;&#36816;&#36755;&#34892;&#19994;&#25991;&#26723;&#20998;&#31867;&#30340;&#21322;&#33258;&#21160;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A semi-automatic method for document classification in the shipping industry. (arXiv:2305.06148v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06148
&lt;/p&gt;
&lt;p&gt;
&#36816;&#36755;&#34892;&#19994;&#20351;&#29992;OCR&#25216;&#26415;&#23545;&#25991;&#20214;&#36827;&#34892;&#20998;&#31867;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#35813;&#30740;&#31350;&#25552;&#20986;&#21322;&#33258;&#21160;&#21270;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#20851;&#38190;&#35789;&#39057;&#29575;&#26500;&#24314;&#19968;&#20010;&#31283;&#20581;&#30340;&#25991;&#26723;&#20998;&#31867;&#31995;&#32479;&#65292;&#20854;&#22312;&#25910;&#38598;&#30340;85&#20010;&#36829;&#32422;&#26696;&#20363;&#21644;555&#20010;&#38750;&#36829;&#32422;&#26696;&#20363;&#19978;&#23454;&#29616;&#20102;93.31%&#30340;&#39640;&#20998;&#31867;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36816;&#36755;&#34892;&#19994;&#20013;&#65292;&#25991;&#26723;&#20998;&#31867;&#23545;&#20110;&#30830;&#20445;&#24517;&#35201;&#30340;&#25991;&#20214;&#34987;&#27491;&#30830;&#35782;&#21035;&#24182;&#22788;&#29702;&#20197;&#36890;&#36807;&#28023;&#20851;&#28165;&#20851;&#27969;&#31243;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;OCR&#25216;&#26415;&#34987;&#29992;&#20110;&#33258;&#21160;&#21270;&#25991;&#26723;&#20998;&#31867;&#36807;&#31243;&#65292;&#20854;&#20013;&#21253;&#25324;&#35782;&#21035;&#21830;&#19994;&#21457;&#31080;&#12289;&#35013;&#31665;&#28165;&#21333;&#12289;&#36827;&#20986;&#21475;&#25253;&#20851;&#21333;&#12289;&#25552;&#21333;&#12289;&#28023;&#36816;&#25552;&#21333;&#12289;&#35777;&#20070;&#12289;&#33322;&#31354;&#25110;&#38081;&#36335;&#36135;&#36816;&#25552;&#21333;&#12289;&#21040;&#36798;&#36890;&#30693;&#20070;&#12289;&#21407;&#20135;&#22320;&#35777;&#26126;&#12289;&#36827;&#21475;&#21830;&#23433;&#20840;&#30003;&#25253;&#21644;&#20449;&#29992;&#35777;&#31561;&#37325;&#35201;&#25991;&#20214;&#12290;&#36890;&#36807;&#20351;&#29992;OCR&#25216;&#26415;&#65292;&#36816;&#36755;&#34892;&#19994;&#21487;&#20197;&#25552;&#39640;&#25991;&#26723;&#20998;&#31867;&#21644;&#28023;&#20851;&#28165;&#20851;&#27969;&#31243;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#22522;&#20110;&#20851;&#38190;&#35789;&#39057;&#29575;&#26500;&#24314;&#19968;&#20010;&#31283;&#20581;&#30340;&#25991;&#26723;&#20998;&#31867;&#31995;&#32479;&#12290;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;IN-D&#20844;&#21496;&#25552;&#20379;&#30340;&#36829;&#32422;&#35785;&#35772;&#25991;&#26723;&#24471;&#20986;&#65292;&#35813;&#25991;&#26723;&#25910;&#38598;&#33258;&#26032;&#21152;&#22369;&#25919;&#24220;&#21496;&#27861;&#32593;&#31449;&#12290;&#25968;&#25454;&#24211;&#20013;&#21253;&#21547;85&#20010;&#36829;&#32422;&#26696;&#20363;&#21644;555&#20010;&#38750;&#36829;&#32422;&#26696;&#20363;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;93.31%&#30340;&#39640;&#20998;&#31867;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the shipping industry, document classification plays a crucial role in ensuring that the necessary documents are properly identified and processed for customs clearance. OCR technology is being used to automate the process of document classification, which involves identifying important documents such as Commercial Invoices, Packing Lists, Export/Import Customs Declarations, Bills of Lading, Sea Waybills, Certificates, Air or Rail Waybills, Arrival Notices, Certificate of Origin, Importer Security Filings, and Letters of Credit. By using OCR technology, the shipping industry can improve accuracy and efficiency in document classification and streamline the customs clearance process. The aim of this study is to build a robust document classification system based on keyword frequencies. The research is carried out by analyzing Contract-Breach law documents available with IN-D. The documents were collected by scraping the Singapore Government Judiciary website. The database developed ha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;ChatGPT&#37325;&#26032;&#29983;&#25104;&#20854;&#26597;&#35810;&#20197;&#28165;&#29702;Debatepedia&#25968;&#25454;&#38598;&#22122;&#22768;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#26597;&#35810;&#30456;&#20851;&#24615;&#21644;&#25688;&#35201;&#29983;&#25104;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.06147</link><description>&lt;p&gt;
&#22522;&#20110;ChatGPT&#21644;Debatepedia&#30340;&#26597;&#35810;&#23548;&#21521;&#25688;&#35201;&#36164;&#28304;CQSumDP
&lt;/p&gt;
&lt;p&gt;
CQSumDP: A ChatGPT-Annotated Resource for Query-Focused Abstractive Summarization Based on Debatepedia. (arXiv:2305.06147v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;ChatGPT&#37325;&#26032;&#29983;&#25104;&#20854;&#26597;&#35810;&#20197;&#28165;&#29702;Debatepedia&#25968;&#25454;&#38598;&#22122;&#22768;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#26597;&#35810;&#30456;&#20851;&#24615;&#21644;&#25688;&#35201;&#29983;&#25104;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Debatepedia&#26159;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#26377;&#20851;&#26377;&#20105;&#35758;&#30340;&#35805;&#39064;&#30340;&#35770;&#28857;&#21644;&#21453;&#39539;&#35770;&#28857;&#65292;&#36817;&#24180;&#26469;&#24050;&#24191;&#27867;&#29992;&#20110;&#21333;&#25991;&#26723;&#26597;&#35810;&#23548;&#21521;&#30340;&#27987;&#32553;&#25688;&#35201;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#21457;&#29616;&#35813;&#25968;&#25454;&#38598;&#23384;&#22312;&#22122;&#22768;&#65292;&#21363;&#20351;&#22312;&#35813;&#25968;&#25454;&#38598;&#20013;&#30340;&#22823;&#22810;&#25968;&#26597;&#35810;&#37117;&#19982;&#30456;&#24212;&#30340;&#25991;&#26723;&#26080;&#20851;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#28165;&#29702;Debatepedia&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#20351;&#20854;&#36866;&#29992;&#20110;&#26597;&#35810;&#23548;&#21521;&#30340;&#27987;&#32553;&#25688;&#35201;&#65292;&#20855;&#20307;&#32780;&#35328;&#65292;&#21033;&#29992;ChatGPT&#30340;&#35821;&#35328;&#29983;&#25104;&#33021;&#21147;&#37325;&#26032;&#29983;&#25104;&#20854;&#26597;&#35810;&#12290;&#25105;&#20204;&#20351;&#29992;&#20960;&#20010;&#22522;&#20934;&#25688;&#35201;&#27169;&#22411;&#35780;&#20272;&#20102;ChatGPT&#27880;&#37322;&#29256;&#26412;&#30340;Debatepedia&#25968;&#25454;&#38598;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#26032;&#27880;&#37322;&#30340;Debatepedia&#22312;&#26597;&#35810;&#30456;&#20851;&#24615;&#21644;&#25688;&#35201;&#29983;&#25104;&#26041;&#38754;&#20248;&#20110;&#21407;&#22987;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Debatepedia is a publicly available dataset consisting of arguments and counter-arguments on controversial topics that has been widely used for the single-document query-focused abstractive summarization task in recent years. However, it has been recently found that this dataset is limited by noise and even most queries in this dataset do not have any relevance to the respective document. In this paper, we present a methodology for cleaning the Debatepedia dataset by leveraging the generative power of large language models to make it suitable for query-focused abstractive summarization. More specifically, we harness the language generation capabilities of ChatGPT to regenerate its queries. We evaluate the effectiveness of the proposed ChatGPT annotated version of the Debatepedia dataset using several benchmark summarization models and demonstrate that the newly annotated version of Debatepedia outperforms the original dataset in terms of both query relevance as well as summary generati
&lt;/p&gt;</description></item><item><title>PAI&#22242;&#38431;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#32500;&#22522;&#30334;&#31185;&#31561;&#22806;&#37096;&#23454;&#20307;&#20449;&#24687;&#30340;&#36890;&#29992;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31995;&#32479;&#65292;&#22312;SemEval-2023&#20219;&#21153;2&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20849;&#36194;&#24471;&#20102;7&#20010;&#22870;&#39033;&#12290;</title><link>http://arxiv.org/abs/2305.06099</link><description>&lt;p&gt;
PAI&#22312;SemEval-2023&#20219;&#21153;2&#20013;&#65306;&#21033;&#29992;&#22806;&#37096;&#23454;&#20307;&#20449;&#24687;&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#36890;&#29992;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
PAI at SemEval-2023 Task 2: A Universal System for Named Entity Recognition with External Entity Information. (arXiv:2305.06099v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06099
&lt;/p&gt;
&lt;p&gt;
PAI&#22242;&#38431;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#32500;&#22522;&#30334;&#31185;&#31561;&#22806;&#37096;&#23454;&#20307;&#20449;&#24687;&#30340;&#36890;&#29992;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31995;&#32479;&#65292;&#22312;SemEval-2023&#20219;&#21153;2&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20849;&#36194;&#24471;&#20102;7&#20010;&#22870;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
MultiCoNER II&#20219;&#21153;&#26088;&#22312;&#22312;&#20302;&#25991;&#26412;&#24773;&#22659;&#21644;&#23384;&#22312;&#25340;&#20889;&#38169;&#35823;&#21644;&#38169;&#21035;&#23383;&#31561;&#22122;&#38899;&#22330;&#26223;&#19979;&#65292;&#26816;&#27979;&#22810;&#31181;&#35821;&#35328;&#30340;&#22797;&#26434;&#12289;&#27169;&#31946;&#21644;&#32454;&#31890;&#24230;&#21629;&#21517;&#23454;&#20307;&#12290;&#35813;&#20219;&#21153;&#30001;&#20110;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#32570;&#20047;&#12289;&#23454;&#20307;&#30340;&#39640;&#31890;&#24230;&#65288;&#39640;&#36798;33&#31181;&#31867;&#65289;&#20197;&#21450;&#22122;&#38899;&#25968;&#25454;&#30340;&#24178;&#25200;&#32780;&#20855;&#26377;&#37325;&#35201;&#30340;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#22242;&#38431;PAI&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#38598;&#25104;&#20102;&#22806;&#37096;&#23454;&#20307;&#20449;&#24687;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#20174;&#30693;&#35782;&#24211;&#65288;&#21363;&#32500;&#22522;&#30334;&#31185;&#65289;&#20013;&#26816;&#32034;&#32473;&#23450;&#25991;&#26412;&#30340;&#23454;&#20307;&#23646;&#24615;&#65292;&#28982;&#21518;&#23558;&#23454;&#20307;&#20449;&#24687;&#19982;&#36755;&#20837;&#21477;&#23376;&#36830;&#25509;&#36215;&#26469;&#65292;&#23558;&#20854;&#39304;&#20837;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;13&#20010;&#36712;&#36947;&#20013;&#36194;&#24471;&#20102;2&#20010;&#19968;&#31561;&#22870;&#65292;4&#20010;&#20108;&#31561;&#22870;&#21644;1&#20010;&#19977;&#31561;&#22870;&#12290;&#35813;&#31995;&#32479;&#30340;&#20195;&#30721;&#20844;&#24320;&#21487;&#20379;&#20351;&#29992;&#65292;&#32593;&#22336;&#20026;\url{https://github.com/diqiuzhuanzhuan/semeval-2023}&#12290;
&lt;/p&gt;
&lt;p&gt;
The MultiCoNER II task aims to detect complex, ambiguous, and fine-grained named entities in low-context situations and noisy scenarios like the presence of spelling mistakes and typos for multiple languages. The task poses significant challenges due to the scarcity of contextual information, the high granularity of the entities(up to 33 classes), and the interference of noisy data. To address these issues, our team {\bf PAI} proposes a universal Named Entity Recognition (NER) system that integrates external entity information to improve performance. Specifically, our system retrieves entities with properties from the knowledge base (i.e. Wikipedia) for a given text, then concatenates entity information with the input sentence and feeds it into Transformer-based models. Finally, our system wins 2 first places, 4 second places, and 1 third place out of 13 tracks. The code is publicly available at \url{https://github.com/diqiuzhuanzhuan/semeval-2023}.
&lt;/p&gt;</description></item><item><title>ChatGPT&#23637;&#31034;&#20102;LLMs&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#36816;&#34892;&#38656;&#35201;&#24040;&#22823;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#39640;&#26114;&#30340;&#25104;&#26412;&#65292;&#39044;&#35745;&#20250;&#32473;AI&#30740;&#31350;&#24102;&#26469;&#37325;&#22823;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.06087</link><description>&lt;p&gt;
ChatGPT&#33021;&#21147;&#23637;&#31034;&#21450;&#20854;&#23545;AI&#30740;&#31350;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
A Glimpse in ChatGPT Capabilities and its impact for AI research. (arXiv:2305.06087v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06087
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#23637;&#31034;&#20102;LLMs&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#36816;&#34892;&#38656;&#35201;&#24040;&#22823;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#39640;&#26114;&#30340;&#25104;&#26412;&#65292;&#39044;&#35745;&#20250;&#32473;AI&#30740;&#31350;&#24102;&#26469;&#37325;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26368;&#36817;&#22312;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30740;&#31350;&#39046;&#22495;&#25104;&#20026;&#28909;&#38376;&#35805;&#39064;&#12290;&#20687;Google&#12289;&#20122;&#39532;&#36874;&#12289;Facebook&#12289;&#29305;&#26031;&#25289;&#21644;&#33529;&#26524;&#65288;GAFA&#65289;&#36825;&#26679;&#30340;&#20844;&#21496;&#27491;&#22312;&#22823;&#21147;&#21457;&#23637;&#36825;&#20123;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#20250;&#20351;&#29992;&#28023;&#37327;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#35821;&#35328;&#32763;&#35793;&#12289;&#25991;&#31456;&#29983;&#25104;&#21644;&#38382;&#31572;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#21644;&#36816;&#34892;&#36825;&#20123;&#27169;&#22411;&#25152;&#38656;&#30340;&#35745;&#31639;&#36164;&#28304;&#38750;&#24120;&#24040;&#22823;&#65292;&#32780;&#30828;&#20214;&#21644;&#30005;&#21147;&#30340;&#25104;&#26412;&#21487;&#33021;&#38480;&#21046;&#20102;&#37027;&#20123;&#27809;&#26377;GAFA&#36164;&#37329;&#21644;&#36164;&#28304;&#30340;&#30740;&#31350;&#23454;&#39564;&#23460;&#12290;&#26412;&#25991;&#23558;&#30740;&#31350;LLMs&#23545;AI&#30740;&#31350;&#30340;&#24433;&#21709;&#65292;&#37325;&#28857;&#20851;&#27880;GPT3.5/ChatGPT3.4&#65292;&#24182;&#32473;&#20986;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#30740;&#31350;&#30340;&#19968;&#20123;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently become a popular topic in the field of Artificial Intelligence (AI) research, with companies such as Google, Amazon, Facebook, Amazon, Tesla, and Apple (GAFA) investing heavily in their development. These models are trained on massive amounts of data and can be used for a wide range of tasks, including language translation, text generation, and question answering. However, the computational resources required to train and run these models are substantial, and the cost of hardware and electricity can be prohibitive for research labs that do not have the funding and resources of the GAFA. In this paper, we will examine the impact of LLMs on AI research. The pace at which such models are generated as well as the range of domains covered is an indication of the trend which not only the public but also the scientific community is currently experiencing. We give some examples on how to use such models in research by focusing on GPT3.5/ChatGPT3.4 and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#24314;&#27169;&#27880;&#37322;&#32773;&#19981;&#19968;&#33268;&#24615;&#30340;&#20004;&#31181;&#26041;&#27861;&#65306;&#20998;&#24067;&#24335;&#36719;&#26631;&#35760;&#26041;&#27861;&#21644;&#24314;&#27169;&#20010;&#20307;&#27880;&#37322;&#32773;&#25110;&#20854;&#32452;&#30340;&#35266;&#28857;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#20004;&#31181;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#30340;&#26041;&#24335;&#26469;&#27169;&#25311;&#27880;&#37322;&#32773;&#19981;&#19968;&#33268;&#24615;&#12290;&#23613;&#31649;&#22810;&#20219;&#21153;&#26041;&#27861;&#20043;&#21069;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#20854;&#22312;&#21253;&#21547;&#19981;&#21516;&#27880;&#37322;&#32773;&#35266;&#28857;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26356;&#32454;&#33268;&#22320;&#29702;&#35299;&#27880;&#37322;&#32773;&#19981;&#19968;&#33268;&#24615;&#65292;&#20026;&#26356;&#20934;&#30830;&#30340;&#35266;&#28857;&#24314;&#27169;&#25552;&#20379;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06074</link><description>&lt;p&gt;
SemEval-2023&#20219;&#21153;11&#20013;&#30340;iLab Le-Wi-Di&#65306;&#27169;&#25311;&#19981;&#19968;&#33268;&#24615;&#36824;&#26159;&#27169;&#25311;&#35266;&#28857;&#65311;
&lt;/p&gt;
&lt;p&gt;
iLab at SemEval-2023 Task 11 Le-Wi-Di: Modelling Disagreement or Modelling Perspectives?. (arXiv:2305.06074v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#24314;&#27169;&#27880;&#37322;&#32773;&#19981;&#19968;&#33268;&#24615;&#30340;&#20004;&#31181;&#26041;&#27861;&#65306;&#20998;&#24067;&#24335;&#36719;&#26631;&#35760;&#26041;&#27861;&#21644;&#24314;&#27169;&#20010;&#20307;&#27880;&#37322;&#32773;&#25110;&#20854;&#32452;&#30340;&#35266;&#28857;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#20004;&#31181;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#30340;&#26041;&#24335;&#26469;&#27169;&#25311;&#27880;&#37322;&#32773;&#19981;&#19968;&#33268;&#24615;&#12290;&#23613;&#31649;&#22810;&#20219;&#21153;&#26041;&#27861;&#20043;&#21069;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#20854;&#22312;&#21253;&#21547;&#19981;&#21516;&#27880;&#37322;&#32773;&#35266;&#28857;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26356;&#32454;&#33268;&#22320;&#29702;&#35299;&#27880;&#37322;&#32773;&#19981;&#19968;&#33268;&#24615;&#65292;&#20026;&#26356;&#20934;&#30830;&#30340;&#35266;&#28857;&#24314;&#27169;&#25552;&#20379;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#24314;&#27169;&#27880;&#37322;&#32773;&#19981;&#19968;&#33268;&#24615;&#65292;&#26377;&#20004;&#31181;&#31454;&#20105;&#30340;&#26041;&#27861;&#65306;&#20998;&#24067;&#24335;&#36719;&#26631;&#35760;&#26041;&#27861;&#65288;&#26088;&#22312;&#25429;&#25417;&#19981;&#19968;&#33268;&#31243;&#24230;&#65289;&#25110;&#24314;&#27169;&#20010;&#20307;&#27880;&#37322;&#32773;&#25110;&#20854;&#32452;&#30340;&#35266;&#28857;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#20808;&#21069;&#22312;&#24314;&#27169;&#35266;&#28857;&#26041;&#38754;&#34920;&#29616;&#25104;&#21151;&#30340;&#22810;&#20219;&#21153;&#26550;&#26500;&#65292;&#23545;SEMEVAL&#20219;&#21153;11&#30340;&#24615;&#33021;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#23558;&#20004;&#31181;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#21363;&#39044;&#27979;&#20010;&#21035;&#27880;&#37322;&#32773;&#30340;&#35266;&#28857;&#20316;&#20026;&#39044;&#27979;&#27880;&#37322;&#32773;&#19981;&#19968;&#33268;&#24615;&#30340;&#36807;&#28193;&#27493;&#39588;&#12290;&#23613;&#31649;&#27492;&#26041;&#27861;&#20043;&#21069;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#22312;&#21253;&#21547;&#19981;&#21516;&#27880;&#37322;&#32773;&#35266;&#28857;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#22810;&#20219;&#21153;&#26041;&#27861;&#30340;&#34920;&#29616;&#19981;&#20339;&#65292;&#36825;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#24314;&#27169;&#35266;&#28857;&#26102;&#21487;&#33021;&#24182;&#19981;&#24635;&#26159;&#36866;&#29992;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#24378;&#28872;&#30340;&#35266;&#28857;&#20027;&#20041;&#26041;&#27861;&#21487;&#33021;&#19981;&#20250;&#26681;&#25454;&#20998;&#24067;&#24335;&#26041;&#27861;&#20351;&#29992;&#30340;&#35780;&#20272;&#25351;&#26631;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#26356;&#32454;&#33268;&#30340;&#29702;&#35299;&#27880;&#37322;&#32773;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#21487;&#20197;&#23548;&#33268;&#26356;&#20934;&#30830;&#30340;&#35266;&#28857;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
There are two competing approaches for modelling annotator disagreement: distributional soft-labelling approaches (which aim to capture the level of disagreement) or modelling perspectives of individual annotators or groups thereof. We adapt a multi-task architecture -- which has previously shown success in modelling perspectives -- to evaluate its performance on the SEMEVAL Task 11. We do so by combining both approaches, i.e. predicting individual annotator perspectives as an interim step towards predicting annotator disagreement. Despite its previous success, we found that a multi-task approach performed poorly on datasets which contained distinct annotator opinions, suggesting that this approach may not always be suitable when modelling perspectives. Furthermore, our results explain that while strongly perspectivist approaches might not achieve state-of-the-art performance according to evaluation metrics used by distributional approaches, our approach allows for a more nuanced under
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ANALOGYKB&#65292;&#19968;&#31181;&#20351;&#29992;&#30334;&#19975;&#35268;&#27169;&#30693;&#35782;&#24211;&#30340;&#31867;&#27604;&#25512;&#29702;&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#35821;&#35328;&#27169;&#22411;&#22312;&#31867;&#27604;&#25512;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#27604;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.05994</link><description>&lt;p&gt;
ANALOGYKB&#65306;&#20351;&#29992;&#30334;&#19975;&#35268;&#27169;&#30693;&#35782;&#24211;&#24320;&#21551;&#35821;&#35328;&#27169;&#22411;&#30340;&#31867;&#27604;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
ANALOGYKB: Unlocking Analogical Reasoning of Language Models with A Million-scale Knowledge Base. (arXiv:2305.05994v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ANALOGYKB&#65292;&#19968;&#31181;&#20351;&#29992;&#30334;&#19975;&#35268;&#27169;&#30693;&#35782;&#24211;&#30340;&#31867;&#27604;&#25512;&#29702;&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#35821;&#35328;&#27169;&#22411;&#22312;&#31867;&#27604;&#25512;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#27604;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#27604;&#25512;&#29702;&#26159;&#20154;&#31867;&#30340;&#19968;&#39033;&#22522;&#26412;&#35748;&#30693;&#33021;&#21147;&#65292;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#27169;&#22411;&#35757;&#32451;&#36164;&#28304;&#65292;&#30446;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#38590;&#20197;&#22312;&#31867;&#27604;&#25512;&#29702;&#20219;&#21153;&#20013;&#36798;&#21040;&#20154;&#31867;&#30340;&#34920;&#29616;&#27700;&#24179;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ANALOGYKB&#65292;&#36825;&#26159;&#19968;&#20010;&#30334;&#19975;&#35268;&#27169;&#30340;&#31867;&#27604;&#30693;&#35782;&#24211;&#65292;&#23427;&#30001;&#29616;&#26377;&#30340;&#30693;&#35782;&#22270;&#35889;&#23548;&#20986;&#12290;ANALOGYKB&#20174;&#30693;&#35782;&#22270;&#35889;&#20013;&#35782;&#21035;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#31867;&#27604;&#65306;1&#65289;&#30456;&#21516;&#20851;&#31995;&#30340;&#31867;&#27604;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#30693;&#35782;&#22270;&#35889;&#20013;&#25552;&#21462;&#65307;2&#65289;&#31867;&#20284;&#20851;&#31995;&#30340;&#31867;&#27604;&#65292;&#21017;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;InstructGPT&#65289;&#21551;&#29992;&#30340;&#36873;&#25321;&#21644;&#36807;&#28388;&#31649;&#36947;&#36827;&#34892;&#35782;&#21035;&#65292;&#20877;&#32463;&#36807;&#23569;&#37327;&#20154;&#24037;&#36136;&#37327;&#25511;&#21046;&#12290;&#22312;&#20004;&#20010;&#31867;&#27604;&#25512;&#29702;&#20219;&#21153;&#65288;&#31867;&#27604;&#35782;&#21035;&#21644;&#29983;&#25104;&#65289;&#30340;&#19968;&#31995;&#21015;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;ANALOGYKB&#25104;&#21151;&#22320;&#20351;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#27604;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analogical reasoning is a fundamental cognitive ability of humans. However, current language models (LMs) still struggle to achieve human-like performance in analogical reasoning tasks due to a lack of resources for model training. In this work, we address this gap by proposing ANALOGYKB, a million-scale analogy knowledge base (KB) derived from existing knowledge graphs (KGs). ANALOGYKB identifies two types of analogies from the KGs: 1) analogies of the same relations, which can be directly extracted from the KGs, and 2) analogies of analogous relations, which are identified with a selection and filtering pipeline enabled by large LMs (InstructGPT), followed by minor human efforts for data quality control. Evaluations on a series of datasets of two analogical reasoning tasks (analogy recognition and generation) demonstrate that ANALOGYKB successfully enables LMs to achieve much better results than previous state-of-the-art methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38656;&#35201;&#35299;&#20915;&#21307;&#30103;&#23545;&#35805;&#25688;&#35201;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#21010;&#20998;&#20026;&#20960;&#20010;&#23567;&#30340;&#23545;&#35805;&#20219;&#21153;&#20381;&#27425;&#26500;&#24314;&#65292;&#20351;&#29992;GPT-3&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21160;&#24577;&#26500;&#24314;&#25552;&#31034;&#20219;&#21153;&#20197;&#21450;&#30830;&#23450;&#21307;&#23398;&#23454;&#20307;&#21450;&#20854;&#30830;&#35748;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2305.05982</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#38454;&#27573;&#26041;&#27861;&#29983;&#25104;&#21307;&#23398;&#31934;&#30830;&#30340;&#24739;&#32773;-&#21307;&#29983;&#23545;&#35805;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Generating medically-accurate summaries of patient-provider dialogue: A multi-stage approach using large language models. (arXiv:2305.05982v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05982
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38656;&#35201;&#35299;&#20915;&#21307;&#30103;&#23545;&#35805;&#25688;&#35201;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#21010;&#20998;&#20026;&#20960;&#20010;&#23567;&#30340;&#23545;&#35805;&#20219;&#21153;&#20381;&#27425;&#26500;&#24314;&#65292;&#20351;&#29992;GPT-3&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21160;&#24577;&#26500;&#24314;&#25552;&#31034;&#20219;&#21153;&#20197;&#21450;&#30830;&#23450;&#21307;&#23398;&#23454;&#20307;&#21450;&#20854;&#30830;&#35748;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#25552;&#20379;&#32773;&#23545;&#24739;&#32773;&#35775;&#38382;&#30340;&#25688;&#35201;&#20855;&#26377;&#20851;&#38190;&#20316;&#29992;&#65292;&#21253;&#25324;&#20020;&#24202;&#20915;&#31574;&#12289;&#21327;&#35843;&#21307;&#30103;&#22242;&#38431;&#21644;&#24739;&#32773;&#30340;&#21442;&#32771;&#36164;&#26009;&#12290;&#26377;&#25928;&#30340;&#25688;&#35201;&#38656;&#35201;&#27969;&#30021;&#65292;&#24182;&#20934;&#30830;&#25429;&#25417;&#23545;&#35805;&#20013;&#30340;&#25152;&#26377;&#21307;&#23398;&#30456;&#20851;&#20449;&#24687;&#65292;&#23613;&#31649;&#24739;&#32773;&#35821;&#35328;&#30340;&#22797;&#26434;&#24615;&#12290;&#21363;&#20351;&#22312;&#35775;&#38382;&#25688;&#35201;&#20013;&#20986;&#29616;&#36731;&#24494;&#30340;&#19981;&#20934;&#30830; (&#20363;&#22914;&#65292;&#22312;&#21457;&#28909;&#26102;&#24635;&#32467;&#20026;&#8220;&#24739;&#32773;&#27809;&#26377;&#21457;&#28903;&#8221;) &#20063;&#20250;&#23545;&#24739;&#32773;&#30340;&#25252;&#29702;&#32467;&#26524;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#26412;&#25991;&#23558;&#21307;&#23398;&#20250;&#35805;&#25688;&#35201;&#38382;&#39064;&#21010;&#20998;&#20026;&#25968;&#20010;&#23567;&#22411;&#23545;&#35805;&#29702;&#35299;&#20219;&#21153;&#65292;&#20854;&#20250;&#20381;&#27425;&#26500;&#24314;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30830;&#23450;&#23545;&#35805;&#20013;&#30340;&#21307;&#23398;&#23454;&#20307;&#21450;&#20854;&#30830;&#35748;&#29366;&#24577;&#65292;&#20316;&#20026;&#26500;&#24314;&#27169;&#22359;&#65292;&#36890;&#36807;&#22312;&#20851;&#32852;&#24739;&#32773;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#21160;&#24577;&#26500;&#24314;&#23569;&#37327;&#25552;&#31034;&#20219;&#21153;&#12290;&#20351;&#29992;GPT-3&#20316;&#20026;&#25105;&#20204;&#30340;&#25903;&#25745;&#12290;
&lt;/p&gt;
&lt;p&gt;
A medical provider's summary of a patient visit serves several critical purposes, including clinical decision-making, facilitating hand-offs between providers, and as a reference for the patient. An effective summary is required to be coherent and accurately capture all the medically relevant information in the dialogue, despite the complexity of patient-generated language. Even minor inaccuracies in visit summaries (for example, summarizing "patient does not have a fever" when a fever is present) can be detrimental to the outcome of care for the patient.  This paper tackles the problem of medical conversation summarization by discretizing the task into several smaller dialogue-understanding tasks that are sequentially built upon. First, we identify medical entities and their affirmations within the conversation to serve as building blocks. We study dynamically constructing few-shot prompts for tasks by conditioning on relevant patient information and use GPT-3 as the backbone for our 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23545;&#36127;&#38754;&#24120;&#35782;&#30693;&#35782;&#30340;&#20102;&#35299;&#31243;&#24230;&#65292;&#21457;&#29616;LLMs&#22312;&#29983;&#25104;&#22522;&#20110;&#36127;&#38754;&#30693;&#35782;&#30340;&#26377;&#25928;&#21477;&#23376;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#20294;&#22312;&#22238;&#31572;&#26497;&#24615;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#36825;&#31181;&#20449;&#24565;&#20914;&#31361;&#20027;&#35201;&#28304;&#20110;&#35821;&#35328;&#39044;&#35757;&#32451;&#26102;&#30340;&#32479;&#35745;&#24555;&#25463;&#26041;&#24335;&#21644;&#21542;&#23450;&#25253;&#21578;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2305.05976</link><description>&lt;p&gt;
&#35828;&#21040;&#20570;&#21040;! &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36127;&#38754;&#24120;&#35782;&#30693;&#35782;&#26041;&#38754;&#23384;&#22312;&#22826;&#36807;&#20048;&#35266;&#30340;&#34920;&#36848;
&lt;/p&gt;
&lt;p&gt;
Say What You Mean! Large Language Models Speak Too Positively about Negative Commonsense Knowledge. (arXiv:2305.05976v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23545;&#36127;&#38754;&#24120;&#35782;&#30693;&#35782;&#30340;&#20102;&#35299;&#31243;&#24230;&#65292;&#21457;&#29616;LLMs&#22312;&#29983;&#25104;&#22522;&#20110;&#36127;&#38754;&#30693;&#35782;&#30340;&#26377;&#25928;&#21477;&#23376;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#20294;&#22312;&#22238;&#31572;&#26497;&#24615;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#36825;&#31181;&#20449;&#24565;&#20914;&#31361;&#20027;&#35201;&#28304;&#20110;&#35821;&#35328;&#39044;&#35757;&#32451;&#26102;&#30340;&#32479;&#35745;&#24555;&#25463;&#26041;&#24335;&#21644;&#21542;&#23450;&#25253;&#21578;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22240;&#33021;&#22815;&#23384;&#20648;&#21644;&#21033;&#29992;&#27491;&#38754;&#30693;&#35782;&#32780;&#34987;&#24191;&#27867;&#30740;&#31350;&#12290;&#20294;&#26159;&#65292;&#36127;&#38754;&#30693;&#35782;&#65292;&#22914;&#8220;&#29422;&#23376;&#19981;&#29983;&#27963;&#22312;&#28023;&#27915;&#20013;&#8221;&#65292;&#20063;&#26159;&#19990;&#30028;&#19978;&#26080;&#22788;&#19981;&#22312;&#30340;&#65292;&#20294;&#24456;&#23569;&#22312;&#25991;&#26412;&#20013;&#26126;&#30830;&#25552;&#21040;&#12290;LLMs&#23545;&#36127;&#38754;&#24120;&#35782;&#30693;&#35782;&#20102;&#35299;&#22810;&#23569;&#65311;&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#23545;&#36127;&#38754;&#24120;&#35782;&#30693;&#35782;&#30340;&#20102;&#35299;&#33021;&#21147;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26377;&#38480;&#21046;&#30340;&#20851;&#38190;&#35789;&#21040;&#21477;&#23376;&#29983;&#25104;&#20219;&#21153;(CG)&#21644;&#19968;&#20010;&#24067;&#23572;&#22411;&#38382;&#31572;&#20219;&#21153;(QA)&#26469;&#25506;&#27979;LLMs&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#65292;LLMs&#32463;&#24120;&#26080;&#27861;&#29983;&#25104;&#22522;&#20110;&#36127;&#38754;&#24120;&#35782;&#30693;&#35782;&#30340;&#26377;&#25928;&#21477;&#23376;&#65292;&#20294;&#23427;&#20204;&#21487;&#20197;&#27491;&#30830;&#22320;&#22238;&#31572;&#26497;&#24615;&#30340;&#26159;&#25110;&#21542;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#36825;&#19968;&#29616;&#35937;&#31216;&#20026;LLMs&#30340;&#20449;&#24565;&#20914;&#31361;&#12290;&#25105;&#20204;&#30340;&#36827;&#19968;&#27493;&#20998;&#26512;&#34920;&#26126;&#65292;&#35821;&#35328;&#24314;&#27169;&#39044;&#35757;&#32451;&#38454;&#27573;&#30340;&#32479;&#35745;&#24555;&#25463;&#26041;&#24335;&#21644;&#21542;&#23450;&#25253;&#21578;&#20559;&#35265;&#24341;&#36215;&#20102;&#36825;&#31181;&#20914;&#31361;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have been widely studied for their ability to store and utilize positive knowledge. However, negative knowledge, such as "lions don't live in the ocean", is also ubiquitous in the world but rarely mentioned explicitly in the text. What do LLMs know about negative knowledge? This work examines the ability of LLMs to negative commonsense knowledge. We design a constrained keywords-to-sentence generation task (CG) and a Boolean question-answering task (QA) to probe LLMs. Our experiments reveal that LLMs frequently fail to generate valid sentences grounded in negative commonsense knowledge, yet they can correctly answer polar yes-or-no questions. We term this phenomenon the belief conflict of LLMs. Our further analysis shows that statistical shortcuts and negation reporting bias from language modeling pre-training cause this conflict.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#22823;&#35821;&#35328;&#27169;&#22411;&#21512;&#25104;&#26597;&#35810;&#30340;&#38544;&#31169;&#20445;&#25252;&#25512;&#33616;&#31995;&#32479;&#65292;&#21487;&#20197;&#23433;&#20840;&#26377;&#25928;&#22320;&#35757;&#32451;&#28145;&#24230;&#26816;&#32034;&#27169;&#22411;&#24182;&#25552;&#39640;&#26816;&#32034;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.05973</link><description>&lt;p&gt;
&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#22823;&#35821;&#35328;&#27169;&#22411;&#21512;&#25104;&#26597;&#35810;&#30340;&#38544;&#31169;&#20445;&#25252;&#25512;&#33616;&#31995;&#32479;.
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving Recommender Systems with Synthetic Query Generation using Differentially Private Large Language Models. (arXiv:2305.05973v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05973
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#22823;&#35821;&#35328;&#27169;&#22411;&#21512;&#25104;&#26597;&#35810;&#30340;&#38544;&#31169;&#20445;&#25252;&#25512;&#33616;&#31995;&#32479;&#65292;&#21487;&#20197;&#23433;&#20840;&#26377;&#25928;&#22320;&#35757;&#32451;&#28145;&#24230;&#26816;&#32034;&#27169;&#22411;&#24182;&#25552;&#39640;&#26816;&#32034;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24320;&#21457;&#38544;&#31169;&#20445;&#25252;&#30340;&#22823;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;&#65292;&#20811;&#26381;&#20102;&#22312;&#35757;&#32451;&#36825;&#20123;&#22797;&#26434;&#31995;&#32479;&#26102;&#30340;&#26576;&#20123;&#25361;&#25112;&#21644;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29305;&#21035;&#36866;&#29992;&#20110;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#26032;&#20852;&#39046;&#22495;&#65292;&#20294;&#20063;&#21487;&#20197;&#36731;&#26494;&#22320;&#29992;&#20110;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#34920;&#31034;&#30340;&#20219;&#20309;&#25512;&#33616;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;DP&#35757;&#32451;&#26041;&#27861;&#65292;&#23545;&#20844;&#24320;&#39044;&#35757;&#32451;&#30340;LLM&#22312;&#26597;&#35810;&#29983;&#25104;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#29983;&#25104;&#30340;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#31169;&#26377;&#21512;&#25104;&#26597;&#35810;&#65292;&#20195;&#34920;&#21407;&#22987;&#26597;&#35810;&#65292;&#21487;&#20197;&#22312;&#20219;&#20309;&#19979;&#28216;&#38750;&#31169;&#26377;&#25512;&#33616;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#30001;&#20849;&#20139;&#65292;&#32780;&#19981;&#20250;&#20135;&#29983;&#20219;&#20309;&#39069;&#22806;&#30340;&#38544;&#31169;&#25104;&#26412;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#23433;&#20840;&#35757;&#32451;&#26377;&#25928;&#30340;&#28145;&#24230;&#26816;&#32034;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#23427;&#20204;&#30340;&#26816;&#32034;&#36136;&#37327;&#26377;&#26174;&#30528;&#30340;&#25552;&#39640;&#65292;&#32780;&#19981;&#20250;&#25439;&#23475;&#26597;&#35810;&#32423;&#21035;&#30340;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel approach for developing privacy-preserving large-scale recommender systems using differentially private (DP) large language models (LLMs) which overcomes certain challenges and limitations in DP training these complex systems. Our method is particularly well suited for the emerging area of LLM-based recommender systems, but can be readily employed for any recommender systems that process representations of natural language inputs. Our approach involves using DP training methods to fine-tune a publicly pre-trained LLM on a query generation task. The resulting model can generate private synthetic queries representative of the original queries which can be freely shared for any downstream non-private recommendation training procedures without incurring any additional privacy cost. We evaluate our method on its ability to securely train effective deep retrieval models, and we observe significant improvements in their retrieval quality without compromising query-level pri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#34920;&#31034;&#36951;&#24536;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36890;&#29992;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#20943;&#36731;&#35813;&#36951;&#24536;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.05968</link><description>&lt;p&gt;
&#36890;&#36807;&#25345;&#32493;&#23398;&#20064;&#30740;&#31350;&#39044;&#35757;&#32451;&#34920;&#31034;&#20013;&#30340;&#36951;&#24536;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Investigating Forgetting in Pre-Trained Representations Through Continual Learning. (arXiv:2305.05968v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05968
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#34920;&#31034;&#36951;&#24536;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36890;&#29992;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#20943;&#36731;&#35813;&#36951;&#24536;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#31034;&#36951;&#24536;&#26159;&#25351;&#22312;&#19981;&#26029;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#19978;&#19979;&#25991;&#34920;&#31034;&#30340;&#28418;&#31227;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#34920;&#31034;&#36951;&#24536;&#20250;&#24433;&#21709;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#20648;&#23384;&#30340;&#36890;&#29992;&#30693;&#35782;&#65292;&#20294;&#20855;&#20307;&#24433;&#21709;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#30740;&#31350;&#34920;&#31034;&#36951;&#24536;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#24615;&#30340;&#24433;&#21709;&#65292;&#21363;&#22788;&#29702;&#26410;&#26469;&#19979;&#28216;&#20219;&#21153;&#30340;&#28508;&#22312;&#33021;&#21147;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#20010;&#24230;&#37327;&#26631;&#20934;&#65292;&#21253;&#25324;&#24635;&#20307;&#36890;&#29992;&#24615;&#30772;&#22351;&#65288;GD&#65289;&#12289;&#21477;&#27861;&#30693;&#35782;&#36951;&#24536;&#65288;SynF&#65289;&#21644;&#35821;&#20041;&#30693;&#35782;&#36951;&#24536;&#65288;SemF&#65289;&#65292;&#20197;&#34913;&#37327;&#36890;&#29992;&#30693;&#35782;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#28436;&#21464;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#22810;&#31181;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#24615;&#36973;&#21040;&#30772;&#22351;&#65292;&#32780;&#19988;&#35821;&#27861;&#21644;&#35821;&#20041;&#30693;&#35782;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#20063;&#20250;&#36951;&#24536;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24471;&#20986;&#20004;&#20010;&#20943;&#36731;&#36890;&#29992;&#30693;&#35782;&#36951;&#24536;&#30340;&#35265;&#35299;&#65306;1&#65289;&#22312;&#36890;&#29992;&#21644;&#29305;&#23450;&#20219;&#21153;&#20043;&#38388;&#24179;&#34913;&#65292; 2&#65289;&#22312;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#20013;&#21152;&#20837;&#36741;&#21161;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representation forgetting refers to the drift of contextualized representations during continual training. Intuitively, the representation forgetting can influence the general knowledge stored in pre-trained language models (LMs), but the concrete effect is still unclear. In this paper, we study the effect of representation forgetting on the generality of pre-trained language models, i.e. the potential capability for tackling future downstream tasks. Specifically, we design three metrics, including overall generality destruction (GD), syntactic knowledge forgetting (SynF), and semantic knowledge forgetting (SemF), to measure the evolution of general knowledge in continual learning. With extensive experiments, we find that the generality is destructed in various pre-trained LMs, and syntactic and semantic knowledge is forgotten through continual learning. Based on our experiments and analysis, we further get two insights into alleviating general knowledge forgetting: 1) training on gene
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36923;&#36753;&#30340;&#22810;&#27169;&#24577;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#31070;&#32463;&#27169;&#22411;&#65292;&#36890;&#36807;&#38598;&#25104;&#21487;&#35299;&#37322;&#24615;&#36923;&#36753;&#23376;&#21477;&#34920;&#36798;&#30446;&#26631;&#20219;&#21153;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#24182;&#20351;&#29992;&#31070;&#32463;&#34920;&#24449;&#21442;&#25968;&#21270;&#31526;&#21495;&#36923;&#36753;&#20803;&#32032;&#65292;&#20174;&#32780;&#20415;&#20110;&#33258;&#21160;&#29983;&#25104;&#21644;&#35780;&#20272;&#26377;&#24847;&#20041;&#30340;&#36923;&#36753;&#23376;&#21477;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#20116;&#20010;&#20803;&#39044;&#27979;&#22120;&#26469;&#25429;&#33719;&#34394;&#20551;&#20449;&#24687;&#30340;&#22522;&#26412;&#27169;&#24335;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#19981;&#20165;&#24615;&#33021;&#26174;&#33879;&#20248;&#20110;&#24403;&#21069;&#26041;&#27861;&#65292;&#32780;&#19988;&#25552;&#20379;&#20102;&#36879;&#26126;&#19988;&#21487;&#35299;&#37322;&#30340;&#36923;&#36753;&#25512;&#29702;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2305.05964</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#34394;&#20551;&#20449;&#24687;&#35299;&#37322;&#24615;&#26816;&#27979;&#19982;&#36923;&#36753;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Interpretable Multimodal Misinformation Detection with Logic Reasoning. (arXiv:2305.05964v1 [cs.MM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36923;&#36753;&#30340;&#22810;&#27169;&#24577;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#31070;&#32463;&#27169;&#22411;&#65292;&#36890;&#36807;&#38598;&#25104;&#21487;&#35299;&#37322;&#24615;&#36923;&#36753;&#23376;&#21477;&#34920;&#36798;&#30446;&#26631;&#20219;&#21153;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#24182;&#20351;&#29992;&#31070;&#32463;&#34920;&#24449;&#21442;&#25968;&#21270;&#31526;&#21495;&#36923;&#36753;&#20803;&#32032;&#65292;&#20174;&#32780;&#20415;&#20110;&#33258;&#21160;&#29983;&#25104;&#21644;&#35780;&#20272;&#26377;&#24847;&#20041;&#30340;&#36923;&#36753;&#23376;&#21477;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#20116;&#20010;&#20803;&#39044;&#27979;&#22120;&#26469;&#25429;&#33719;&#34394;&#20551;&#20449;&#24687;&#30340;&#22522;&#26412;&#27169;&#24335;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#19981;&#20165;&#24615;&#33021;&#26174;&#33879;&#20248;&#20110;&#24403;&#21069;&#26041;&#27861;&#65292;&#32780;&#19988;&#25552;&#20379;&#20102;&#36879;&#26126;&#19988;&#21487;&#35299;&#37322;&#30340;&#36923;&#36753;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#31038;&#20132;&#24179;&#21488;&#19978;&#30340;&#22810;&#27169;&#24577;&#34394;&#20551;&#20449;&#24687;&#30001;&#20110;&#22810;&#23186;&#20307;&#20869;&#23481;&#30340;&#21487;&#20449;&#24230;&#21644;&#20256;&#25773;&#26356;&#23481;&#26131;&#32780;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#26816;&#27979;&#26041;&#27861;&#24050;&#32463;&#36798;&#21040;&#20102;&#36739;&#39640;&#30340;&#24615;&#33021;&#65292;&#20294;&#32570;&#20047;&#35299;&#37322;&#24615;&#38459;&#30861;&#20102;&#36825;&#20123;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#21644;&#23454;&#38469;&#37096;&#32626;&#12290;&#21463;&#21040; NeuralSymbolic AI &#30340;&#21551;&#21457;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#33021;&#21147;&#21644;&#31526;&#21495;&#23398;&#20064;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36923;&#36753;&#30340;&#22810;&#27169;&#24577;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#31070;&#32463;&#27169;&#22411;&#65292;&#23427;&#38598;&#25104;&#20102;&#21487;&#35299;&#37322;&#24615;&#36923;&#36753;&#23376;&#21477;&#20197;&#34920;&#36798;&#30446;&#26631;&#20219;&#21153;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#20026;&#20102;&#20351;&#23398;&#20064;&#26377;&#25928;&#65292;&#25105;&#20204;&#20351;&#29992;&#31070;&#32463;&#34920;&#24449;&#26469;&#21442;&#25968;&#21270;&#31526;&#21495;&#36923;&#36753;&#20803;&#32032;&#65292;&#20174;&#32780;&#20415;&#20110;&#33258;&#21160;&#29983;&#25104;&#21644;&#35780;&#20272;&#26377;&#24847;&#20041;&#30340;&#36923;&#36753;&#23376;&#21477;&#12290;&#21478;&#22806;&#65292;&#20026;&#20102;&#20351;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#36866;&#29992;&#20110;&#21508;&#31181;&#34394;&#20551;&#20449;&#24687;&#26469;&#28304;&#65292;&#25105;&#20204;&#22312;&#22810;&#27169;&#24577;&#34701;&#21512;&#32593;&#32476;&#20013;&#24341;&#20837;&#20102;&#20116;&#20010;&#20803;&#39044;&#27979;&#22120;&#26469;&#25429;&#33719;&#34394;&#20551;&#20449;&#24687;&#30340;&#22522;&#26412;&#27169;&#24335;&#12290;&#25105;&#20204;&#22312;&#23454;&#38469;&#30340;&#22810;&#27169;&#24577;&#34394;&#20551;&#20449;&#24687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#19981;&#20165;&#26174;&#30528;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#36824;&#20026;&#27599;&#20010;&#39044;&#27979;&#25552;&#20379;&#20102;&#36879;&#26126;&#19988;&#21487;&#35299;&#37322;&#30340;&#36923;&#36753;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal misinformation on online social platforms is becoming a critical concern due to increasing credibility and easier dissemination brought by multimedia content, compared to traditional text-only information. While existing multimodal detection approaches have achieved high performance, the lack of interpretability hinders these systems' reliability and practical deployment. Inspired by NeuralSymbolic AI which combines the learning ability of neural networks with the explainability of symbolic learning, we propose a novel logic-based neural model for multimodal misinformation detection which integrates interpretable logic clauses to express the reasoning process of the target task. To make learning effective, we parameterize symbolic logical elements using neural representations, which facilitate the automatic generation and evaluation of meaningful logic clauses. Additionally, to make our framework generalizable across diverse misinformation sources, we introduce five meta-pre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#36335;&#24452;&#32467;&#26500;&#23545;Transformer&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#23376;&#23618;&#20013;&#28155;&#21152;&#24402;&#19968;&#21270;&#12289;&#20135;&#29983;&#26356;&#22810;&#29305;&#24449;&#30340;&#24265;&#20215;&#25805;&#20316;&#21644;&#21487;&#23398;&#20064;&#30340;&#21152;&#26435;&#26426;&#21046;&#26469;&#34701;&#21512;&#20174;&#19981;&#21516;&#36335;&#24452;&#25552;&#21462;&#30340;&#29305;&#24449;&#65292;&#23454;&#39564;&#21457;&#29616;&#30456;&#21516;&#21442;&#25968;&#19979;&#27973;&#23618;&#22810;&#36335;&#24452;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#19982;&#28145;&#23618;&#27169;&#22411;&#30456;&#20284;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.05948</link><description>&lt;p&gt;
&#22810;&#36335;&#24452;Transformer&#26356;&#22909;&#65306;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Multi-Path Transformer is Better: A Case Study on Neural Machine Translation. (arXiv:2305.05948v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#36335;&#24452;&#32467;&#26500;&#23545;Transformer&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#23376;&#23618;&#20013;&#28155;&#21152;&#24402;&#19968;&#21270;&#12289;&#20135;&#29983;&#26356;&#22810;&#29305;&#24449;&#30340;&#24265;&#20215;&#25805;&#20316;&#21644;&#21487;&#23398;&#20064;&#30340;&#21152;&#26435;&#26426;&#21046;&#26469;&#34701;&#21512;&#20174;&#19981;&#21516;&#36335;&#24452;&#25552;&#21462;&#30340;&#29305;&#24449;&#65292;&#23454;&#39564;&#21457;&#29616;&#30456;&#21516;&#21442;&#25968;&#19979;&#27973;&#23618;&#22810;&#36335;&#24452;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#19982;&#28145;&#23618;&#27169;&#22411;&#30456;&#20284;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#36981;&#24490;&#21442;&#25968;&#23610;&#23544;&#20026;&#24130;&#24459;&#20998;&#24067;&#30340;&#35268;&#24459;&#12290;&#20026;&#20102;&#32771;&#34385;&#21442;&#25968;&#25928;&#29575;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#22686;&#21152;&#27169;&#22411;&#28145;&#24230;&#32780;&#38750;&#23485;&#24230;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#36890;&#36807;&#19968;&#20010;&#21442;&#25968;&#39640;&#25928;&#30340;&#22810;&#36335;&#24452;&#32467;&#26500;&#26469;&#30740;&#31350;&#27169;&#22411;&#23485;&#24230;&#22914;&#20309;&#24433;&#21709;Transformer&#27169;&#22411;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#34701;&#21512;&#20174;&#19981;&#21516;&#36335;&#24452;&#25552;&#21462;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#22312;&#27599;&#20010;&#23376;&#23618;&#20013;&#28155;&#21152;&#20102;&#19977;&#20010;&#38468;&#21152;&#25805;&#20316;&#65306;&#27599;&#20010;&#36335;&#24452;&#26411;&#23614;&#30340;&#24402;&#19968;&#21270;&#12289;&#20135;&#29983;&#26356;&#22810;&#29305;&#24449;&#30340;&#24265;&#20215;&#25805;&#20316;&#20197;&#21450;&#21487;&#23398;&#20064;&#30340;&#21152;&#26435;&#26426;&#21046;&#65292;&#20197;&#28789;&#27963;&#22320;&#34701;&#21512;&#25152;&#26377;&#29305;&#24449;&#12290;&#22312;12&#20010;WMT&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25317;&#26377;&#30456;&#21516;&#25968;&#37327;&#21442;&#25968;&#30340;&#27973;&#23618;&#22810;&#36335;&#24452;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#19982;&#28145;&#23618;&#27169;&#22411;&#30456;&#20284;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#25581;&#31034;&#20102;&#24212;&#26356;&#21152;&#20851;&#27880;&#22810;&#36335;&#24452;&#32467;&#26500;&#65292;&#24182;&#24212;&#22312;&#27169;&#22411;&#28145;&#24230;&#21644;&#23485;&#24230;&#20043;&#38388;&#36798;&#25104;&#24179;&#34913;&#65292;&#20197;&#35757;&#32451;&#26356;&#22909;&#30340;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
For years the model performance in machine learning obeyed a power-law relationship with the model size. For the consideration of parameter efficiency, recent studies focus on increasing model depth rather than width to achieve better performance. In this paper, we study how model width affects the Transformer model through a parameter-efficient multi-path structure. To better fuse features extracted from different paths, we add three additional operations to each sublayer: a normalization at the end of each path, a cheap operation to produce more features, and a learnable weighted mechanism to fuse all features flexibly. Extensive experiments on 12 WMT machine translation tasks show that, with the same number of parameters, the shallower multi-path model can achieve similar or even better performance than the deeper model. It reveals that we should pay more attention to the multi-path structure, and there should be a balance between the model depth and width to train a better large-sc
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#36328;&#35821;&#35328;ICL&#20013;&#26080;&#27861;&#23545;&#20934;&#36755;&#20837;&#36755;&#20986;&#31354;&#38388;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26500;&#24314;&#31574;&#30053;X-InSTA&#65292;&#21487;&#20197;&#21516;&#26102;&#23545;&#28304;&#35821;&#35328;&#21644;&#30446;&#26631;&#35821;&#35328;&#30340;&#35821;&#22659;&#36827;&#34892;&#32534;&#30721;&#21644;&#23545;&#40784;&#65292;&#20174;&#32780;&#25552;&#39640;&#36328;&#35821;&#35328;ICL&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.05940</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;LLMs&#26159;&#26356;&#22909;&#30340;&#36328;&#35821;&#35328;&#19978;&#19979;&#25991;&#23398;&#20064;&#32773;&#19982;&#23545;&#40784;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual LLMs are Better Cross-lingual In-context Learners with Alignment. (arXiv:2305.05940v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05940
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#36328;&#35821;&#35328;ICL&#20013;&#26080;&#27861;&#23545;&#20934;&#36755;&#20837;&#36755;&#20986;&#31354;&#38388;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26500;&#24314;&#31574;&#30053;X-InSTA&#65292;&#21487;&#20197;&#21516;&#26102;&#23545;&#28304;&#35821;&#35328;&#21644;&#30446;&#26631;&#35821;&#35328;&#30340;&#35821;&#22659;&#36827;&#34892;&#32534;&#30721;&#21644;&#23545;&#40784;&#65292;&#20174;&#32780;&#25552;&#39640;&#36328;&#35821;&#35328;ICL&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#27809;&#26377;&#20219;&#20309;&#26799;&#24230;&#26356;&#26032;&#30340;&#24773;&#20917;&#19979;&#25512;&#26029;&#20986;&#20197;&#23569;&#25968;&#26631;&#35760;&#26679;&#26412;&#20026;&#26465;&#20214;&#30340;&#27979;&#35797;&#26631;&#31614;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#25104;&#20026;&#21487;&#33021;&#12290;&#21551;&#29992;ICL&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#20026;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#35268;&#36991;&#22797;&#21457;&#24615;&#27880;&#37322;&#25104;&#26412;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#21069;&#36827;&#27493;&#20240;&#12290;&#28982;&#32780;&#65292;&#36807;&#21435;&#21482;&#26377;&#23569;&#25968;&#20960;&#39033;&#30740;&#31350;&#25506;&#31350;&#20102;&#36328;&#35821;&#35328;&#35774;&#32622;&#19979;&#30340;ICL&#65292;&#36825;&#22312;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#21040;&#20302;&#36164;&#28304;&#35821;&#35328;&#36716;&#31227;&#26631;&#31614;&#30693;&#35782;&#30340;&#38656;&#35201;&#19979;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#40511;&#27807;&#65292;&#25105;&#20204;&#39318;&#27425;&#23545;&#36328;&#35821;&#35328;&#25991;&#26412;&#20998;&#31867;&#30340;ICL&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#36328;&#35821;&#35328;ICL&#30340;&#24773;&#20917;&#19979;&#65292;&#26222;&#36941;&#36873;&#25321;&#38543;&#26426;&#30340;&#36755;&#20837;-&#26631;&#31614;&#23545;&#26469;&#26500;&#24314;&#25552;&#31034;&#19978;&#19979;&#25991;&#30340;&#27169;&#24335;&#20005;&#37325;&#21463;&#38480;&#20110;&#36755;&#20837;&#21644;&#36755;&#20986;&#31354;&#38388;&#30340;&#32570;&#20047;&#23545;&#20934;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26500;&#24314;&#31574;&#30053;&#8212;&#8212;&#36328;&#35821;&#35328;&#19978;&#19979;&#25991;&#28304;-&#30446;&#26631;&#23545;&#40784;&#65288;X-InSTA&#65289;&#12290;&#36890;&#36807;&#27880;&#20837;&#20849;&#21516;&#35757;&#32451;&#30340;&#38408;&#20540;&#20803;&#32032;&#65292;X-InSTA&#21487;&#20197;&#21516;&#26102;&#23545;&#28304;&#35821;&#35328;&#21644;&#30446;&#26631;&#35821;&#35328;&#30340;&#35821;&#22659;&#36827;&#34892;&#32534;&#30721;&#21644;&#23545;&#40784;&#65292;&#20174;&#32780;&#25552;&#39640;&#36328;&#35821;&#35328;ICL&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning (ICL) unfolds as large language models become capable of inferring test labels conditioned on a few labeled samples without any gradient update. ICL-enabled large language models provide a promising step forward toward bypassing recurrent annotation costs in a low-resource setting. Yet, only a handful of past studies have explored ICL in a cross-lingual setting, in which the need for transferring label-knowledge from a high-resource language to a low-resource one is immensely crucial. To bridge the gap, we provide the first in-depth analysis of ICL for cross-lingual text classification. We find that the prevalent mode of selecting random input-label pairs to construct the prompt-context is severely limited in the case of cross-lingual ICL, primarily due to the lack of alignment in the input as well as the output spaces. To mitigate this, we propose a novel prompt construction strategy -- Cross-lingual In-context Source-Target Alignment (X-InSTA). With an injected co
&lt;/p&gt;</description></item><item><title>WikiSQE&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#32500;&#22522;&#30334;&#31185;&#20013;&#21477;&#23376;&#36136;&#37327;&#20272;&#35745;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#32422;3.4M&#20010;&#21477;&#23376;&#21644;153&#20010;&#36136;&#37327;&#26631;&#31614;&#12290;&#22312;&#36825;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20855;&#26377;&#24341;&#25991;&#12289;&#35821;&#27861;/&#35821;&#20041;&#25110;&#21629;&#39064;&#38382;&#39064;&#30340;&#21477;&#23376;&#26356;&#38590;&#20197;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.05928</link><description>&lt;p&gt;
WikiSQE&#65306;&#32500;&#22522;&#30334;&#31185;&#20013;&#21477;&#23376;&#36136;&#37327;&#20272;&#35745;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
WikiSQE: A Large-Scale Dataset for Sentence Quality Estimation in Wikipedia. (arXiv:2305.05928v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05928
&lt;/p&gt;
&lt;p&gt;
WikiSQE&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#32500;&#22522;&#30334;&#31185;&#20013;&#21477;&#23376;&#36136;&#37327;&#20272;&#35745;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#32422;3.4M&#20010;&#21477;&#23376;&#21644;153&#20010;&#36136;&#37327;&#26631;&#31614;&#12290;&#22312;&#36825;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20855;&#26377;&#24341;&#25991;&#12289;&#35821;&#27861;/&#35821;&#20041;&#25110;&#21629;&#39064;&#38382;&#39064;&#30340;&#21477;&#23376;&#26356;&#38590;&#20197;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32500;&#22522;&#30334;&#31185;&#21487;&#20197;&#34987;&#20219;&#20309;&#20154;&#32534;&#36753;&#65292;&#22240;&#27492;&#21253;&#21547;&#21508;&#31181;&#36136;&#37327;&#30340;&#21477;&#23376;&#12290;&#22240;&#27492;&#65292;&#32500;&#22522;&#30334;&#31185;&#21253;&#21547;&#19968;&#20123;&#36136;&#37327;&#36739;&#24046;&#30340;&#32534;&#36753;&#65292;&#36825;&#20123;&#32534;&#36753;&#36890;&#24120;&#20250;&#34987;&#20854;&#20182;&#32534;&#36753;&#26631;&#35760;&#12290;&#34429;&#28982;&#32534;&#36753;&#30340;&#35780;&#35770;&#22686;&#24378;&#20102;&#32500;&#22522;&#30334;&#31185;&#30340;&#21487;&#20449;&#24230;&#65292;&#20294;&#24456;&#38590;&#26816;&#26597;&#25152;&#26377;&#32534;&#36753;&#30340;&#25991;&#26412;&#12290;&#21327;&#21161;&#36825;&#20010;&#36807;&#31243;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#30446;&#21069;&#36824;&#27809;&#26377;&#19968;&#20010;&#22823;&#32780;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#26469;&#30740;&#31350;&#23427;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; WikiSQE&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#32500;&#22522;&#30334;&#31185;&#20013;&#21477;&#23376;&#36136;&#37327;&#20272;&#35745;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#27599;&#20010;&#21477;&#23376;&#37117;&#26159;&#20174;&#32500;&#22522;&#30334;&#31185;&#30340;&#25972;&#20010;&#20462;&#35746;&#21382;&#21490;&#20013;&#25552;&#21462;&#30340;&#65292;&#24182;&#19988;&#30446;&#26631;&#36136;&#37327;&#26631;&#31614;&#32463;&#36807;&#20102;&#20180;&#32454;&#30340;&#35843;&#26597;&#21644;&#36873;&#25321;&#12290;WikiSQE&#20855;&#26377;&#32422;3.4 million&#20010;&#21477;&#23376;&#21644;153&#20010;&#36136;&#37327;&#26631;&#31614;&#12290;&#22312;&#20351;&#29992;&#31454;&#20105;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#20998;&#31867;&#30340;&#23454;&#39564;&#20013;&#65292;&#21457;&#29616;&#20855;&#26377;&#24341;&#25991;&#65292;&#35821;&#27861;/&#35821;&#20041;&#25110;&#21629;&#39064;&#38382;&#39064;&#30340;&#21477;&#23376;&#26356;&#38590;&#20197;&#26816;&#27979;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#33258;&#21160;&#20316;&#25991;&#35780;&#20998;&#23454;&#39564;&#65292;&#20197;&#35780;&#20272;&#29983;&#25104;&#25688;&#35201;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wikipedia can be edited by anyone and thus contains various quality sentences. Therefore, Wikipedia includes some poor-quality edits, which are often marked up by other editors. While editors' reviews enhance the credibility of Wikipedia, it is hard to check all edited text. Assisting in this process is very important, but a large and comprehensive dataset for studying it does not currently exist. Here, we propose WikiSQE, the first large-scale dataset for sentence quality estimation in Wikipedia. Each sentence is extracted from the entire revision history of Wikipedia, and the target quality labels were carefully investigated and selected. WikiSQE has about 3.4 M sentences with 153 quality labels. In the experiment with automatic classification using competitive machine learning models, sentences that had problems with citation, syntax/semantics, or propositions were found to be more difficult to detect. In addition, we conducted automated essay scoring experiments to evaluate the gen
&lt;/p&gt;</description></item><item><title>Decker&#26159;&#19968;&#31181;&#33021;&#22815;&#21033;&#29992;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#20043;&#38388;&#30340;&#28508;&#22312;&#20851;&#31995;&#26469;&#26725;&#25509;&#24322;&#26500;&#30693;&#35782;&#30340;&#24120;&#35782;&#20107;&#23454;&#39564;&#35777;&#27169;&#22411;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#39564;&#35777;&#25928;&#26524;&#21644;&#33719;&#21462;&#29645;&#36149;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.05921</link><description>&lt;p&gt;
Decker: &#21452;&#37325;&#26816;&#26597;&#19982;&#24322;&#26500;&#30693;&#35782;&#29992;&#20110;&#24120;&#35782;&#20107;&#23454;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Decker: Double Check with Heterogeneous Knowledge for Commonsense Fact Verification. (arXiv:2305.05921v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05921
&lt;/p&gt;
&lt;p&gt;
Decker&#26159;&#19968;&#31181;&#33021;&#22815;&#21033;&#29992;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#20043;&#38388;&#30340;&#28508;&#22312;&#20851;&#31995;&#26469;&#26725;&#25509;&#24322;&#26500;&#30693;&#35782;&#30340;&#24120;&#35782;&#20107;&#23454;&#39564;&#35777;&#27169;&#22411;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#39564;&#35777;&#25928;&#26524;&#21644;&#33719;&#21462;&#29645;&#36149;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24120;&#35782;&#20107;&#23454;&#39564;&#35777;&#20316;&#20026;&#24120;&#35782;&#38382;&#31572;&#30340;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20998;&#25903;&#65292;&#26088;&#22312;&#36890;&#36807;&#20107;&#23454;&#26469;&#39564;&#35777;&#19968;&#20010;&#32473;&#23450;&#30340;&#24120;&#35782;&#35770;&#26029;&#26159;&#21542;&#27491;&#30830;&#12290;&#22238;&#31572;&#24120;&#35782;&#38382;&#39064;&#38656;&#35201;&#20174;&#19981;&#21516;&#23618;&#27425;&#30340;&#30693;&#35782;&#20013;&#36827;&#34892;&#32452;&#21512;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#20381;&#38752;&#25226;&#25569;&#38750;&#32467;&#26500;&#21270;&#35777;&#25454;&#25110;&#20174;&#32467;&#26500;&#21270;&#30693;&#35782;&#24211;&#20013;&#25214;&#21040;&#28508;&#22312;&#30340;&#25512;&#29702;&#36335;&#24452;&#65292;&#20294;&#27809;&#26377;&#21516;&#26102;&#21033;&#29992;&#24322;&#26500;&#30693;&#35782;&#30340;&#22909;&#22788;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Decker&#65292;&#19968;&#31181;&#24120;&#35782;&#20107;&#23454;&#39564;&#35777;&#27169;&#22411;&#65292;&#33021;&#22815;&#36890;&#36807;&#21457;&#29616;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#20043;&#38388;&#30340;&#28508;&#22312;&#20851;&#31995;&#26469;&#26725;&#25509;&#24322;&#26500;&#30693;&#35782;&#12290;&#22312;&#20004;&#20010;&#24120;&#35782;&#20107;&#23454;&#39564;&#35777;&#22522;&#20934;&#25968;&#25454;&#38598;CSQA2.0&#21644;CREAK&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;Decker&#30340;&#26377;&#25928;&#24615;&#65292;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#39564;&#35777;&#20102;&#23427;&#22312;&#25512;&#29702;&#20013;&#33719;&#21462;&#26356;&#22810;&#29645;&#36149;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Commonsense fact verification, as a challenging branch of commonsense question-answering (QA), aims to verify through facts whether a given commonsense claim is correct or not. Answering commonsense questions necessitates a combination of knowledge from various levels. However, existing studies primarily rest on grasping either unstructured evidence or potential reasoning paths from structured knowledge bases, yet failing to exploit the benefits of heterogeneous knowledge simultaneously. In light of this, we propose Decker, a commonsense fact verification model that is capable of bridging heterogeneous knowledge by uncovering latent relationships between structured and unstructured knowledge. Experimental results on two commonsense fact verification benchmark datasets, CSQA2.0 and CREAK demonstrate the effectiveness of our Decker and further analysis verifies its capability to seize more precious information through reasoning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#32423;&#20449;&#24687;&#30340;&#22320;&#22336;&#21305;&#37197;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20013;&#30340;&#23618;&#32423;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#29616;&#26377;&#26041;&#27861;&#22788;&#29702;&#19981;&#35268;&#21017;&#22320;&#22336;&#30340;&#33021;&#21147;&#65292;&#24182;&#21487;&#20197;&#26356;&#21152;&#20851;&#27880;&#22320;&#22336;&#30340;&#29305;&#27530;&#37096;&#20998;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#24615;&#33021;&#25552;&#39640;&#20102;3.2&#20010;&#30334;&#20998;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.05874</link><description>&lt;p&gt;
&#22522;&#20110;&#23618;&#32423;&#20449;&#24687;&#30340;&#22320;&#22336;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Address Matching Based On Hierarchical Information. (arXiv:2305.05874v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#32423;&#20449;&#24687;&#30340;&#22320;&#22336;&#21305;&#37197;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20013;&#30340;&#23618;&#32423;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#29616;&#26377;&#26041;&#27861;&#22788;&#29702;&#19981;&#35268;&#21017;&#22320;&#22336;&#30340;&#33021;&#21147;&#65292;&#24182;&#21487;&#20197;&#26356;&#21152;&#20851;&#27880;&#22320;&#22336;&#30340;&#29305;&#27530;&#37096;&#20998;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#24615;&#33021;&#25552;&#39640;&#20102;3.2&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#35777;&#25454;&#26174;&#31034;&#65292;&#22320;&#22336;&#21305;&#37197;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#27604;&#22914;&#24555;&#36882;&#12289;&#22312;&#32447;&#36141;&#29289;&#31561;&#12290;&#22320;&#22336;&#20855;&#26377;&#20998;&#23618;&#32467;&#26500;&#65292;&#19982;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#24418;&#25104;&#23545;&#27604;&#65292;&#36825;&#21487;&#20197;&#20026;&#22320;&#22336;&#21305;&#37197;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;&#22522;&#20110;&#36825;&#31181;&#24819;&#27861;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20013;&#30340;&#23618;&#32423;&#20449;&#24687;&#65292;&#19981;&#20165;&#25552;&#39640;&#20102;&#29616;&#26377;&#26041;&#27861;&#22788;&#29702;&#19981;&#35268;&#21017;&#22320;&#22336;&#30340;&#33021;&#21147;&#65292;&#36824;&#21487;&#20197;&#26356;&#23494;&#20999;&#22320;&#20851;&#27880;&#22320;&#22336;&#30340;&#29305;&#27530;&#37096;&#20998;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23558;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;3.2&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is evidence that address matching plays a crucial role in many areas such as express delivery, online shopping and so on. Address has a hierarchical structure, in contrast to unstructured texts, which can contribute valuable information for address matching. Based on this idea, this paper proposes a novel method to leverage the hierarchical information in deep learning method that not only improves the ability of existing methods to handle irregular address, but also can pay closer attention to the special part of address. Experimental findings demonstrate that the proposed method improves the current approach by 3.2% points.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;ChatGPT&#21644;GPT-4&#22312;&#37329;&#34701;&#25991;&#26412;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#23427;&#20204;&#22312;&#25968;&#20540;&#25512;&#29702;&#19978;&#34920;&#29616;&#20986;&#33394;&#20294;&#22312;&#38656;&#35201;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;</title><link>http://arxiv.org/abs/2305.05862</link><description>&lt;p&gt;
ChatGPT&#21644;GPT-4&#26159;&#21542;&#26159;&#37329;&#34701;&#25991;&#26412;&#20998;&#26512;&#30340;&#36890;&#29992;&#27714;&#35299;&#22120;&#65311;&#23545;&#20960;&#31181;&#20856;&#22411;&#20219;&#21153;&#36827;&#34892;&#26816;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are ChatGPT and GPT-4 General-Purpose Solvers for Financial Text Analytics? An Examination on Several Typical Tasks. (arXiv:2305.05862v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;ChatGPT&#21644;GPT-4&#22312;&#37329;&#34701;&#25991;&#26412;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#23427;&#20204;&#22312;&#25968;&#20540;&#25512;&#29702;&#19978;&#34920;&#29616;&#20986;&#33394;&#20294;&#22312;&#38656;&#35201;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;ChatGPT&#21644;GPT-4&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#23545;&#35805;&#22238;&#24212;&#12290;&#23613;&#31649;ChatGPT&#21644;GPT-4&#22312;&#36890;&#29992;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#32463;&#36807;&#20102;&#24191;&#27867;&#30340;&#27979;&#35797;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#36824;&#27809;&#26377;&#23545;&#37329;&#34701;&#35821;&#26009;&#24211;&#36827;&#34892;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#22312;&#38646;&#26679;&#26412;&#25110;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#32771;&#23519;ChatGPT&#21644;GPT-4&#20316;&#20026;&#20856;&#22411;&#37329;&#34701;&#25991;&#26412;&#20998;&#26512;&#38382;&#39064;&#27714;&#35299;&#22120;&#30340;&#28508;&#21147;&#26469;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#20116;&#20010;&#19981;&#21516;&#30340;&#37329;&#34701;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22235;&#39033;&#20195;&#34920;&#24615;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#21021;&#27493;&#30740;&#31350;&#34920;&#26126;&#65292;ChatGPT&#21644;GPT-4&#22312;&#38656;&#35201;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#37329;&#34701;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#21644;&#24773;&#24863;&#20998;&#26512;&#31561;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;&#22312;&#25968;&#20540;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#24403;&#21069;&#29256;&#26412;ChatGPT&#21644;GPT-4&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#29616;&#26377;&#25216;&#26415;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
The most recent large language models such as ChatGPT and GPT-4 have garnered significant attention, as they are capable of generating high-quality responses to human input. Despite the extensive testing of ChatGPT and GPT-4 on generic text corpora, showcasing their impressive capabilities, a study focusing on financial corpora has not been conducted. In this study, we aim to bridge this gap by examining the potential of ChatGPT and GPT-4 as a solver for typical financial text analytic problems in the zero-shot or few-shot setting. Specifically, we assess their capabilities on four representative tasks over five distinct financial textual datasets. The preliminary study shows that ChatGPT and GPT-4 struggle on tasks such as financial named entity recognition (NER) and sentiment analysis, where domain-specific knowledge is required, while they excel in numerical reasoning tasks. We report both the strengths and limitations of the current versions of ChatGPT and GPT-4, comparing them to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#21360;&#24230;&#35821;&#35328;&#22836;&#26465;&#29983;&#25104;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598; V\=arta&#65292;&#21253;&#21547;&#26469;&#33258;14&#31181;&#19981;&#21516;&#21360;&#24230;&#35821;&#35328;&#65288;&#21644;&#33521;&#35821;&#65289;&#30340;4180&#19975;&#26465;&#26032;&#38395;&#25991;&#31456;&#65292;&#21487;&#29992;&#20110;&#39044;&#35757;&#32451;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#21487;&#29992;&#20110;&#22238;&#31572;&#19982;&#21360;&#24230;&#35821;&#35328;&#22788;&#29702;&#21644;&#22810;&#35821;&#35328;&#30740;&#31350;&#30456;&#20851;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.05858</link><description>&lt;p&gt;
V\=arta&#65306;&#19968;&#20010;&#29992;&#20110;&#21360;&#24230;&#35821;&#35328;&#22836;&#26465;&#29983;&#25104;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
V\=arta: A Large-Scale Headline-Generation Dataset for Indic Languages. (arXiv:2305.05858v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#21360;&#24230;&#35821;&#35328;&#22836;&#26465;&#29983;&#25104;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598; V\=arta&#65292;&#21253;&#21547;&#26469;&#33258;14&#31181;&#19981;&#21516;&#21360;&#24230;&#35821;&#35328;&#65288;&#21644;&#33521;&#35821;&#65289;&#30340;4180&#19975;&#26465;&#26032;&#38395;&#25991;&#31456;&#65292;&#21487;&#29992;&#20110;&#39044;&#35757;&#32451;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#21487;&#29992;&#20110;&#22238;&#31572;&#19982;&#21360;&#24230;&#35821;&#35328;&#22788;&#29702;&#21644;&#22810;&#35821;&#35328;&#30740;&#31350;&#30456;&#20851;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; V\=arta&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#21360;&#24230;&#35821;&#35328;&#22836;&#26465;&#29983;&#25104;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#26469;&#33258;14&#31181;&#19981;&#21516;&#21360;&#24230;&#35821;&#35328;&#65288;&#21644;&#33521;&#35821;&#65289;&#30340;4180&#19975;&#26465;&#26032;&#38395;&#25991;&#31456;&#65292;&#36825;&#20123;&#25991;&#31456;&#26469;&#33258;&#21508;&#31181;&#39640;&#36136;&#37327;&#26469;&#28304;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#30446;&#21069;&#21487;&#29992;&#30340;&#21360;&#24230;&#35821;&#35328;&#31934;&#36873;&#25991;&#31456;&#26368;&#22823;&#30340;&#38598;&#21512;&#12290;&#25105;&#20204;&#20351;&#29992;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#36827;&#34892;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#20197;&#22238;&#31572;&#19982;&#21360;&#24230;&#35821;&#35328;&#22788;&#29702;&#21644;&#22810;&#35821;&#35328;&#30740;&#31350;&#30456;&#20851;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#25105;&#20204;&#34920;&#26126;&#21363;&#20351;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#25277;&#35937;&#27169;&#22411;&#65292;&#35813;&#25968;&#25454;&#38598;&#20063;&#26159;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#23427;&#20204;&#30340;&#34920;&#29616;&#20063;&#20165;&#27604;&#25277;&#21462;&#22522;&#32447;&#30053;&#20248;&#12290;&#30001;&#20110;&#20854;&#35268;&#27169;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#35813;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#39044;&#35757;&#32451;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#22343;&#20248;&#20110;&#31454;&#20105;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present V\=arta, a large-scale multilingual dataset for headline generation in Indic languages. This dataset includes 41.8 million news articles in 14 different Indic languages (and English), which come from a variety of high-quality sources. To the best of our knowledge, this is the largest collection of curated articles for Indic languages currently available. We use the data collected in a series of experiments to answer important questions related to Indic NLP and multilinguality research in general. We show that the dataset is challenging even for state-of-the-art abstractive models and that they perform only slightly better than extractive baselines. Owing to its size, we also show that the dataset can be used to pretrain strong language models that outperform competitive baselines in both NLU and NLG benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21387;&#32553;&#35789;&#27719;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#21033;&#29992;&#29615;&#22659;&#21387;&#21147;&#20419;&#36827;&#24773;&#22659;&#20381;&#36182;&#24615;&#27807;&#36890;&#30340;&#20986;&#29616;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#25509;&#25910;&#32773;&#26080;&#27861;&#22788;&#29702;&#27495;&#20041;&#30340;&#24773;&#20917;&#19979;&#65292;&#21457;&#36865;&#32773;&#22914;&#20309;&#21033;&#29992;&#29615;&#22659;&#30340;&#21046;&#32422;&#22240;&#32032;&#23454;&#29616;&#27807;&#36890;&#12290;</title><link>http://arxiv.org/abs/2305.05821</link><description>&lt;p&gt;
&#29615;&#22659;&#32422;&#26463;&#19979;&#30340;&#24773;&#22659;&#20381;&#36182;&#24615;&#27807;&#36890;
&lt;/p&gt;
&lt;p&gt;
Context-dependent communication under environmental constraints. (arXiv:2305.05821v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21387;&#32553;&#35789;&#27719;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#21033;&#29992;&#29615;&#22659;&#21387;&#21147;&#20419;&#36827;&#24773;&#22659;&#20381;&#36182;&#24615;&#27807;&#36890;&#30340;&#20986;&#29616;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#25509;&#25910;&#32773;&#26080;&#27861;&#22788;&#29702;&#27495;&#20041;&#30340;&#24773;&#20917;&#19979;&#65292;&#21457;&#36865;&#32773;&#22914;&#20309;&#21033;&#29992;&#29615;&#22659;&#30340;&#21046;&#32422;&#22240;&#32032;&#23454;&#29616;&#27807;&#36890;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23384;&#22312;&#22823;&#37327;&#30340;&#35777;&#25454;&#34920;&#26126;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#27807;&#36890;&#19981;&#33021;&#31616;&#21333;&#22320;&#36890;&#36807;&#21457;&#36865;&#20855;&#26377;&#29420;&#31435;&#20110;&#24773;&#22659;&#24847;&#20041;&#30340;&#20449;&#21495;&#26469;&#23454;&#29616;&#12290;&#26412;&#25991;&#20197;&#32463;&#20856;&#30340;Lewis(1969)&#20449;&#21495;&#27169;&#22411;&#30340;&#21464;&#20307;&#20026;&#22522;&#30784;&#65292;&#25506;&#35752;&#22312;&#24773;&#22659;&#21270;&#22330;&#26223;&#19979;&#20135;&#29983;&#24773;&#22659;&#20381;&#36182;&#24615;&#27807;&#36890;&#30340;&#26465;&#20214;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#26368;&#23567;&#21270;&#35789;&#27719;&#37327;&#30340;&#21387;&#21147;&#19979;&#65292;&#36825;&#31181;&#27807;&#36890;&#30340;&#20986;&#29616;&#26159;&#36275;&#22815;&#30340;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21487;&#33021;&#20351;&#31526;&#21495;&#21547;&#20041;&#24471;&#21040;&#24773;&#22659;&#21306;&#20998;&#30340;&#29615;&#22659;&#26465;&#20214;&#21644;&#35748;&#30693;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#25509;&#21463;&#32773;&#30340;&#25351;&#20195;&#36873;&#25321;&#21463;&#21040;&#29615;&#22659;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#21457;&#36865;&#32773;&#21487;&#20197;&#21333;&#26041;&#38754;&#22320;&#21033;&#29992;&#36825;&#20123;&#38480;&#21046;&#65292;&#32780;&#26080;&#38656;&#25509;&#25910;&#32773;&#20855;&#26377;&#28548;&#28165;&#27495;&#20041;&#30340;&#33021;&#21147;&#12290;&#19982;&#24120;&#35265;&#30340;&#20551;&#35774;&#19968;&#33268;&#65292;&#21457;&#36865;&#32773;&#23545;&#24773;&#22659;&#30340;&#24847;&#35782;&#20284;&#20046;&#26159;&#38656;&#35201;&#30340;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#24773;&#22659;&#20381;&#36182;&#24615;&#27807;&#36890;&#26159;&#19968;&#31181;&#22810;&#23618;&#27425;&#30340;&#24773;&#22659;&#21270;&#29616;&#35937;&#65292;&#20854;&#21463;&#29615;&#22659;&#29305;&#24615;&#30340;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is significant evidence that real-world communication cannot be reduced to sending signals with context-independent meaning. In this work, based on a variant of the classical Lewis (1969) signaling model, we explore the conditions for the emergence of context-dependent communication in a situated scenario. In particular, we demonstrate that pressure to minimise the vocabulary size is sufficient for such emergence. At the same time, we study the environmental conditions and cognitive capabilities that enable contextual disambiguation of symbol meanings. We show that environmental constraints on the receiver's referent choice can be unilaterally exploited by the sender, without disambiguation capabilities on the receiver's end. Consistent with common assumptions, the sender's awareness of the context appears to be required for contextual communication. We suggest that context-dependent communication is a situated multilayered phenomenon, crucially influenced by environment properti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25240;&#25187;&#32047;&#31215;&#22686;&#30410;&#65288;DCG&#65289;&#25490;&#24207;&#24182;&#21152;&#26435;&#22788;&#29702;&#35757;&#32451;&#25968;&#25454;&#20197;&#25552;&#39640;&#27169;&#22411;&#23545;&#20302;&#20195;&#34920;&#24615;&#32452;&#30340;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#20808;&#21069;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.05759</link><description>&lt;p&gt;
&#25490;&#21517;&#21644;&#37325;&#26032;&#21152;&#26435;&#25552;&#39640;&#20102;&#32452;&#30340;&#20998;&#24067;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Ranking &amp; Reweighting Improves Group Distributional Robustness. (arXiv:2305.05759v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25240;&#25187;&#32047;&#31215;&#22686;&#30410;&#65288;DCG&#65289;&#25490;&#24207;&#24182;&#21152;&#26435;&#22788;&#29702;&#35757;&#32451;&#25968;&#25454;&#20197;&#25552;&#39640;&#27169;&#22411;&#23545;&#20302;&#20195;&#34920;&#24615;&#32452;&#30340;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#20808;&#21069;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#36827;&#34892;&#26631;&#20934;&#35757;&#32451;&#21487;&#33021;&#20250;&#20135;&#29983;&#22312;&#24179;&#22343;&#31934;&#24230;&#19978;&#34920;&#29616;&#20986;&#33394;&#20294;&#22312;&#20302;&#20195;&#34920;&#24615;&#32452;&#19978;&#20934;&#30830;&#24615;&#36739;&#20302;&#30340;&#27169;&#22411;&#65292;&#36825;&#26159;&#30001;&#20110;&#34920;&#24449;&#20013;&#34394;&#20551;&#29305;&#24449;&#30340;&#26222;&#36941;&#23384;&#22312;&#25152;&#33268;&#12290;&#35299;&#20915;&#36825;&#20010;&#32452;&#40065;&#26834;&#24615;&#38382;&#39064;&#30340;&#20027;&#35201;&#26041;&#27861;&#26159;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#26368;&#23567;&#21270;&#26368;&#22351;&#30340;&#32452;&#35823;&#24046;&#65288;&#31867;&#20284;&#20110;&#26497;&#23567;&#20540;&#31574;&#30053;&#65289;&#65292;&#24076;&#26395;&#23427;&#20250;&#22312;&#27979;&#35797;&#25968;&#25454;&#19978;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24448;&#24448;&#26159;&#27425;&#20248;&#30340;&#65292;&#23588;&#20854;&#26159;&#24403;&#27979;&#35797;&#25968;&#25454;&#38598;&#20013;&#21253;&#21547;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#32452;&#26102;&#12290;&#26412;&#25991;&#21463;&#20449;&#24687;&#26816;&#32034;&#21644;Learning-to-Rank&#25991;&#29486;&#30340;&#21551;&#21457;&#65292;&#39318;&#20808;&#25552;&#20986;&#20351;&#29992;&#25240;&#25187;&#32047;&#31215;&#22686;&#30410;&#65288;DCG&#65289;&#20316;&#20026;&#27169;&#22411;&#36136;&#37327;&#24230;&#37327;&#26631;&#20934;&#65292;&#20197;&#20419;&#36827;&#26356;&#22909;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#27169;&#22411;&#36873;&#25321;&#12290;&#20316;&#20026;&#19968;&#31181;&#22522;&#20110;&#25490;&#24207;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;DCG&#21152;&#26435;&#22810;&#20010;&#24615;&#33021;&#36739;&#24046;&#30340;&#32452;&#65288;&#32780;&#19981;&#20165;&#20165;&#26159;&#32771;&#34385;&#24615;&#33021;&#26368;&#24046;&#30340;&#32452;&#65289;&#12290;&#20316;&#20026;&#33258;&#28982;&#30340;&#19979;&#19968;&#27493;&#65292;&#25105;&#20204;&#22522;&#20110;&#36825;&#20123;&#32467;&#26524;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32452;&#37325;&#26032;&#21152;&#26435;&#26041;&#27861;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#40723;&#21169;&#27169;&#22411;&#38598;&#20013;&#20110;&#20302;&#20195;&#34920;&#24615;&#30340;&#32452;&#12290;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#23545;&#32452;&#19981;&#24179;&#34913;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has shown that standard training via empirical risk minimization (ERM) can produce models that achieve high accuracy on average but low accuracy on underrepresented groups due to the prevalence of spurious features. A predominant approach to tackle this group robustness problem minimizes the worst group error (akin to a minimax strategy) on the training data, hoping it will generalize well on the testing data. However, this is often suboptimal, especially when the out-of-distribution (OOD) test data contains previously unseen groups. Inspired by ideas from the information retrieval and learning-to-rank literature, this paper first proposes to use Discounted Cumulative Gain (DCG) as a metric of model quality for facilitating better hyperparameter tuning and model selection. Being a ranking-based metric, DCG weights multiple poorly-performing groups (instead of considering just the group with the worst performance). As a natural next step, we build on our results to propose a
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#35299;&#20915;&#20102;&#22312;&#21327;&#20316;&#24314;&#31569;&#20219;&#21153;&#20013;&#22914;&#20309;&#35299;&#20915;&#27169;&#26865;&#20004;&#21487;&#30340;&#24773;&#20917;&#65292;&#20174;&#32780;&#25552;&#20986;&#20102;&#20309;&#26102;&#23547;&#27714;&#28548;&#28165;&#20197;&#21450;&#24212;&#35813;&#35810;&#38382;&#20160;&#20040;&#28548;&#28165;&#38382;&#39064;&#36825;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#30340;&#35299;&#31572;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.05754</link><description>&lt;p&gt;
&#36890;&#36807;&#19990;&#30028;&#29366;&#24577;&#21644;&#25991;&#26412;&#25351;&#20196;&#25552;&#38382;&#30340;&#26102;&#38388;&#21644;&#38382;&#39064;&#65306;IGLU NLP&#25361;&#25112;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
When and What to Ask Through World States and Text Instructions: IGLU NLP Challenge Solution. (arXiv:2305.05754v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05754
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#35299;&#20915;&#20102;&#22312;&#21327;&#20316;&#24314;&#31569;&#20219;&#21153;&#20013;&#22914;&#20309;&#35299;&#20915;&#27169;&#26865;&#20004;&#21487;&#30340;&#24773;&#20917;&#65292;&#20174;&#32780;&#25552;&#20986;&#20102;&#20309;&#26102;&#23547;&#27714;&#28548;&#28165;&#20197;&#21450;&#24212;&#35813;&#35810;&#38382;&#20160;&#20040;&#28548;&#28165;&#38382;&#39064;&#36825;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#30340;&#35299;&#31572;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21327;&#20316;&#20219;&#21153;&#20013;&#65292;&#26377;&#25928;&#30340;&#20132;&#27969;&#23545;&#20110;&#23454;&#29616;&#20849;&#21516;&#30446;&#26631;&#33267;&#20851;&#37325;&#35201;&#12290;&#20854;&#20013;&#19968;&#20010;&#20219;&#21153;&#26159;&#21327;&#20316;&#24314;&#31569;&#65292;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#65292;&#24314;&#31569;&#32773;&#24517;&#39035;&#30456;&#20114;&#36890;&#20449;&#65292;&#22312;&#35832;&#22914;Minecraft&#20043;&#31867;&#30340;&#27169;&#25311;&#29615;&#22659;&#20013;&#26500;&#24314;&#25152;&#38656;&#30340;&#32467;&#26500;&#12290;&#25105;&#20204;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#26234;&#33021;&#24314;&#31569;&#20195;&#29702;&#65292;&#26681;&#25454;&#29992;&#25143;&#23545;&#35805;&#24314;&#36896;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#22312;&#21327;&#20316;&#24314;&#31569;&#20013;&#65292;&#24314;&#31569;&#32773;&#21487;&#33021;&#20250;&#36935;&#21040;&#38590;&#20197;&#35299;&#35835;&#30340;&#24773;&#20917;&#65292;&#22240;&#20026;&#20449;&#24687;&#21644;&#25351;&#20196;&#26377;&#38480;&#65292;&#23548;&#33268;&#27169;&#26865;&#20004;&#21487;&#12290;&#22312;NeurIPS 2022&#31454;&#36187;&#30340;NLP&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#22238;&#31572;&#20102;&#20004;&#20010;&#20851;&#38190;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65306;&#20195;&#29702;&#20309;&#26102;&#24212;&#35813;&#23547;&#27714;&#28548;&#28165;&#65292;&#24212;&#35813;&#35810;&#38382;&#20160;&#20040;&#28548;&#28165;&#38382;&#39064;&#65311;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#23376;&#20219;&#21153;&#26397;&#30528;&#36825;&#20010;&#30446;&#26631;&#36808;&#36827;&#65292;&#19968;&#20010;&#26159;&#20998;&#31867;&#20219;&#21153;&#65292;&#19968;&#20010;&#26159;&#25490;&#24207;&#20219;&#21153;&#12290;&#23545;&#20110;&#20998;&#31867;&#20219;&#21153;&#65292;&#30446;&#26631;&#26159;&#26681;&#25454;&#24403;&#21069;&#30340;&#19990;&#30028;&#29366;&#24577;&#21644;&#23545;&#35805;&#21382;&#21490;&#30830;&#23450;&#20195;&#29702;&#26159;&#21542;&#24212;&#35813;&#23547;&#27714;&#28548;&#28165;&#12290;&#23545;&#20110;&#25490;&#24207;&#20219;&#21153;&#65292;&#30446;&#26631;&#26159;&#25552;&#20379;&#19968;&#20221;&#21487;&#33021;&#30340;&#28548;&#28165;&#38382;&#39064;&#30340;&#25490;&#21517;&#21015;&#34920;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#22522;&#20110;&#35268;&#21017;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#22312;IGLU NLP&#25361;&#25112;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In collaborative tasks, effective communication is crucial for achieving joint goals. One such task is collaborative building where builders must communicate with each other to construct desired structures in a simulated environment such as Minecraft. We aim to develop an intelligent builder agent to build structures based on user input through dialogue. However, in collaborative building, builders may encounter situations that are difficult to interpret based on the available information and instructions, leading to ambiguity. In the NeurIPS 2022 Competition NLP Task, we address two key research questions, with the goal of filling this gap: when should the agent ask for clarification, and what clarification questions should it ask? We move towards this target with two sub-tasks, a classification task and a ranking task. For the classification task, the goal is to determine whether the agent should ask for clarification based on the current world state and dialogue history. For the ran
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#23558;&#25991;&#26412;&#34920;&#31034;&#20026;&#22810;&#32500;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#21487;&#26144;&#23556;&#20855;&#26377;&#22797;&#26434;&#30340;&#22810;&#23618;&#32467;&#26500;&#30340;&#21477;&#23376;&#65292;&#24182;&#24212;&#29992;&#20110;&#20154;&#26684;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.05748</link><description>&lt;p&gt;
&#22810;&#23618;&#21477;&#23376;&#23884;&#20837;&#29992;&#20110;&#20154;&#26684;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multilevel Sentence Embeddings for Personality Prediction. (arXiv:2305.05748v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#23558;&#25991;&#26412;&#34920;&#31034;&#20026;&#22810;&#32500;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#21487;&#26144;&#23556;&#20855;&#26377;&#22797;&#26434;&#30340;&#22810;&#23618;&#32467;&#26500;&#30340;&#21477;&#23376;&#65292;&#24182;&#24212;&#29992;&#20110;&#20154;&#26684;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#65292;&#22914;Sentence-BERT&#65288;SBERT&#65289;&#65292;&#21487;&#23558;&#25991;&#26412;&#34920;&#31034;&#20026;&#22810;&#32500;&#31354;&#38388;&#12290;&#28982;&#32780;&#65292;&#24403;&#25968;&#25454;&#20855;&#26377;&#22797;&#26434;&#30340;&#22810;&#23618;&#32467;&#26500;&#26102;&#65292;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#21333;&#29420;&#35757;&#32451;&#31867;&#21035;&#29305;&#23450;&#30340;&#27169;&#22411;&#65292;&#22686;&#21152;&#20102;&#26102;&#38388;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#27493;&#27861;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#26681;&#25454;&#21477;&#23376;&#30340;&#23618;&#27425;&#25104;&#21592;&#36164;&#26684;&#21644;&#26497;&#24615;&#26469;&#26144;&#23556;&#21477;&#23376;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;AdaCos loss&#20989;&#25968;&#25945;&#25480;&#19978;&#23618;&#21477;&#23376;&#31354;&#38388;&#65292;&#28982;&#21518;&#36890;&#36807;&#19968;&#31181;&#20027;&#35201;&#22522;&#20110;&#23618;&#20869;&#25104;&#23545;&#20313;&#24358;&#30456;&#20284;&#24230;&#30340;&#26032;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#23558;&#27492;&#26041;&#27861;&#24212;&#29992;&#20110;&#19977;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65306;&#20174;&#33521;&#35821;&#21644;&#26085;&#35821;Twitter&#25968;&#25454;&#20013;&#33719;&#21462;&#30340;&#20004;&#20010;&#24369;&#30417;&#30563;&#30340;Big Five&#20154;&#26684;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;MNLI&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#21333;&#19968;&#27169;&#22411;&#26041;&#27861;&#27604;&#22810;&#20010;&#31867;&#21035;&#29305;&#23450;&#20998;&#31867;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representing text into a multidimensional space can be done with sentence embedding models such as Sentence-BERT (SBERT). However, training these models when the data has a complex multilevel structure requires individually trained class-specific models, which increases time and computing costs. We propose a two step approach which enables us to map sentences according to their hierarchical memberships and polarity. At first we teach the upper level sentence space through an AdaCos loss function and then finetune with a novel loss function mainly based on the cosine similarity of intra-level pairs. We apply this method to three different datasets: two weakly supervised Big Five personality dataset obtained from English and Japanese Twitter data and the benchmark MNLI dataset. We show that our single model approach performs better than multiple class-specific classification models.
&lt;/p&gt;</description></item><item><title>CodeIE&#25552;&#20986;&#20102;&#20351;&#29992;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#65288;Code-LLMs&#65289;&#20195;&#26367;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#65288;NL-LLMs&#65289;&#23545;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25277;&#21462;&#36825;&#31867;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#36827;&#34892;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#21462;&#24471;&#20248;&#20110;&#20960;&#20010;&#24378;&#22522;&#20934;&#39640;&#36798;4.5%&#30340;&#32477;&#23545;&#31934;&#24230;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.05711</link><description>&lt;p&gt;
CodeIE: &#22823;&#22411;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#20248;&#20110;&#23569;&#26679;&#26412;&#20449;&#24687;&#25552;&#21462;&#22120;
&lt;/p&gt;
&lt;p&gt;
CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors. (arXiv:2305.05711v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05711
&lt;/p&gt;
&lt;p&gt;
CodeIE&#25552;&#20986;&#20102;&#20351;&#29992;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#65288;Code-LLMs&#65289;&#20195;&#26367;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#65288;NL-LLMs&#65289;&#23545;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25277;&#21462;&#36825;&#31867;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#36827;&#34892;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#21462;&#24471;&#20248;&#20110;&#20960;&#20010;&#24378;&#22522;&#20934;&#39640;&#36798;4.5%&#30340;&#32477;&#23545;&#31934;&#24230;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#39044;&#35757;&#32451;&#26041;&#38754;&#65292;&#24050;&#32463;&#34920;&#29616;&#20986;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#20855;&#26377;&#24778;&#20154;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#12290;&#36890;&#24120;&#30340;&#20570;&#27861;&#26159;&#23558;&#20219;&#21153;&#37325;&#26500;&#20026;&#25991;&#26412;&#21040;&#25991;&#26412;&#30340;&#26684;&#24335;&#65292;&#20197;&#20415;&#33258;&#28982;&#35821;&#35328;&#30340;&#29983;&#25104;&#24335;LLMs&#65288;&#22914;GPT-3&#65289;&#21487;&#20197;&#34987;&#25552;&#31034;&#35299;&#20915;&#23427;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;NL-LLMs&#36827;&#34892;&#20449;&#24687;&#25552;&#21462;&#65288;IE&#65289;&#20219;&#21153;&#26159;&#19981;&#26131;&#30340;&#65292;&#22240;&#20026;IE&#20219;&#21153;&#30340;&#36755;&#20986;&#36890;&#24120;&#26159;&#32467;&#26500;&#21270;&#30340;&#65292;&#22240;&#27492;&#24456;&#38590;&#36716;&#25442;&#25104;&#32431;&#25991;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#20195;&#30721;&#24418;&#24335;&#32780;&#38750;&#33258;&#28982;&#35821;&#35328;&#26469;&#34920;&#36798;&#32467;&#26500;&#21270;&#30340;&#36755;&#20986;&#65292;&#24182;&#21033;&#29992;&#20195;&#30721;&#29983;&#25104;LLMs&#65288;&#22914;Codex&#65289;&#26469;&#25191;&#34892;IE&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25277;&#21462;&#12290;&#19982;NL-LLMs&#30456;&#27604;&#65292;&#25105;&#20204;&#34920;&#26126;&#36890;&#36807;&#35774;&#35745;&#20195;&#30721;&#39118;&#26684;&#30340;&#25552;&#31034;&#21644;&#23558;&#36825;&#20123;IE&#20219;&#21153;&#26356;&#25913;&#20026;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#65292;Code-LLMs&#21487;&#20197;&#19982;&#36825;&#20123;IE&#20219;&#21153;&#24456;&#22909;&#22320;&#23545;&#40784;&#12290;&#22312;&#19971;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#29615;&#22659;&#19979;&#19968;&#30452;&#20248;&#20110;&#20960;&#20010;&#24378;&#22522;&#20934;&#65292;&#24182;&#21462;&#24471;&#20102;&#39640;&#36798;4.5%&#30340;&#32477;&#23545;&#31934;&#24230;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) pre-trained on massive corpora have demonstrated impressive few-shot learning ability on many NLP tasks. A common practice is to recast the task into a text-to-text format such that generative LLMs of natural language (NL-LLMs) like GPT-3 can be prompted to solve it. However, it is nontrivial to perform information extraction (IE) tasks with NL-LLMs since the output of the IE task is usually structured and therefore is hard to be converted into plain text. In this paper, we propose to recast the structured output in the form of code instead of natural language and utilize generative LLMs of code (Code-LLMs) such as Codex to perform IE tasks, in particular, named entity recognition and relation extraction. In contrast to NL-LLMs, we show that Code-LLMs can be well-aligned with these IE tasks by designing code-style prompts and formulating these IE tasks as code generation tasks. Experiment results on seven benchmarks show that our method consistently outperf
&lt;/p&gt;</description></item><item><title>&#20026;&#20102;&#35299;&#20915;&#20107;&#20214;&#25351;&#31216;&#20849;&#25351;&#28040;&#35299;&#20013;&#30340;&#35745;&#31639;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#20854;&#20998;&#25104;&#20004;&#20010;&#37096;&#20998;&#65306;&#19968;&#31181;&#21551;&#21457;&#24335;&#36807;&#28388;&#26041;&#27861;&#21644;&#22522;&#20110;&#22343;&#34913;&#25968;&#25454;&#38598;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.05672</link><description>&lt;p&gt;
$2n$&#27604;$n^2$&#26356;&#22909;&#65306;&#23558;&#20107;&#20214;&#25351;&#31216;&#20849;&#25351;&#28040;&#35299;&#25286;&#20998;&#25104;&#20004;&#20010;&#26131;&#20110;&#22788;&#29702;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
$2 * n$ is better than $n^2$: Decomposing Event Coreference Resolution into Two Tractable Problems. (arXiv:2305.05672v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05672
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#20107;&#20214;&#25351;&#31216;&#20849;&#25351;&#28040;&#35299;&#20013;&#30340;&#35745;&#31639;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#20854;&#20998;&#25104;&#20004;&#20010;&#37096;&#20998;&#65306;&#19968;&#31181;&#21551;&#21457;&#24335;&#36807;&#28388;&#26041;&#27861;&#21644;&#22522;&#20110;&#22343;&#34913;&#25968;&#25454;&#38598;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#25351;&#31216;&#20849;&#25351;&#28040;&#35299;(ECR)&#26159;&#23558;&#21516;&#19968;&#20107;&#20214;&#22312;&#25991;&#26723;&#20013;&#25110;&#36328;&#25991;&#26723;&#20013;&#30340;&#25351;&#31216;&#36827;&#34892;&#38142;&#25509;&#30340;&#20219;&#21153;&#12290;&#22823;&#22810;&#25968;&#25351;&#31216;&#23545;&#19981;&#20849;&#25351;&#65292;&#20294;&#35768;&#22810;&#20849;&#25351;&#25351;&#31216;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#25216;&#26415;&#36827;&#34892;&#35782;&#21035;&#65292;&#20363;&#22914;&#20107;&#20214;&#35302;&#21457;&#22120;&#25110;&#23427;&#20204;&#20986;&#29616;&#30340;&#21477;&#23376;&#30340;&#35789;&#24418;&#21305;&#37197;&#12290;&#29616;&#26377;&#30340;&#20849;&#25351;&#28040;&#35299;&#31995;&#32479;&#35757;&#32451;&#26041;&#27861;&#20174;&#19968;&#20010;&#39640;&#24230;&#20542;&#26012;&#30340;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#65292;&#36825;&#20351;&#24471;&#31639;&#27861;&#38590;&#20197;&#23398;&#20064;&#36229;&#20986;&#34920;&#38754;&#21305;&#37197;&#30340;&#20849;&#25351;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#38656;&#35201;&#24179;&#26041;&#25805;&#20316;&#65292;&#36825;&#20123;&#26041;&#27861;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#23558;ECR&#38382;&#39064;&#20998;&#25104;&#20004;&#20010;&#37096;&#20998;&#65306;a) &#19968;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#36807;&#28388;&#20986;&#22823;&#37327;&#19981;&#20849;&#25351;&#30340;&#25351;&#31216;&#23545;&#65292;b) &#22312;&#19968;&#32452;&#22343;&#34913;&#30340;&#20849;&#25351;&#21644;&#38750;&#20849;&#25351;&#25351;&#31216;&#23545;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#36890;&#36807;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;ECR&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#19982;&#26368;&#20808;&#36827;&#25216;&#26415;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#26174;&#30528;&#20943;&#23569;&#20102;&#35745;&#31639;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event Coreference Resolution (ECR) is the task of linking mentions of the same event either within or across documents. Most mention pairs are not coreferent, yet many that are coreferent can be identified through simple techniques such as lemma matching of the event triggers or the sentences in which they appear. Existing methods for training coreference systems sample from a largely skewed distribution, making it difficult for the algorithm to learn coreference beyond surface matching. Additionally, these methods are intractable because of the quadratic operations needed. To address these challenges, we break the problem of ECR into two parts: a) a heuristic to efficiently filter out a large number of non-coreferent pairs, and b) a training approach on a balanced set of coreferent and non-coreferent mention pairs. By following this approach, we show that we get comparable results to the state of the art on two popular ECR datasets while significantly reducing compute requirements. We
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#26500;&#24314;&#38754;&#21521;&#23454;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#30340;&#31471;&#21040;&#31471;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861; HEER&#65292;&#36890;&#36807;&#23558;&#39046;&#22495;&#29305;&#23450;&#30340;&#32422;&#26463;&#21644;&#29305;&#24449;&#32435;&#20837;&#21040;&#22270;&#23884;&#20837;&#31639;&#27861;&#20013;&#65292;&#26377;&#25928;&#22320;&#25913;&#21892;&#20102;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.05640</link><description>&lt;p&gt;
&#38754;&#21521;&#20010;&#20154;&#25110;&#23454;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#65306;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#24212;&#29992;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Representation Learning for Person or Entity-centric Knowledge Graphs: an application in Healthcare. (arXiv:2305.05640v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#26500;&#24314;&#38754;&#21521;&#23454;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#30340;&#31471;&#21040;&#31471;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861; HEER&#65292;&#36890;&#36807;&#23558;&#39046;&#22495;&#29305;&#23450;&#30340;&#32422;&#26463;&#21644;&#29305;&#24449;&#32435;&#20837;&#21040;&#22270;&#23884;&#20837;&#31639;&#27861;&#20013;&#65292;&#26377;&#25928;&#22320;&#25913;&#21892;&#20102;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#26159;&#19968;&#31181;&#25353;&#26412;&#20307;&#25110;&#27169;&#24335;&#32452;&#32455;&#20449;&#24687;&#30340;&#27969;&#34892;&#26041;&#24335;&#65292;&#24050;&#32463;&#22312;&#20174;&#25628;&#32034;&#21040;&#25512;&#33616;&#30340;&#21508;&#31181;&#22330;&#26223;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;&#23613;&#31649;&#22312;&#30693;&#35782;&#22270;&#35889;&#26041;&#38754;&#26377;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#30693;&#35782;&#34920;&#31034;&#20173;&#28982;&#26159;&#36328;&#34892;&#19994;&#30340;&#19968;&#20010;&#38750;&#24120;&#26840;&#25163;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#22312;&#29983;&#29289;&#21307;&#23398;&#21644;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#65292;&#30001;&#20110;&#23454;&#20307;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20851;&#31995;&#12289;&#24322;&#36136;&#24615;&#12289;&#32570;&#20047;&#26631;&#20934;&#21270;&#21644;&#25968;&#25454;&#31232;&#30095;&#24615;&#31561;&#22240;&#32032;&#65292;&#36825;&#19968;&#20219;&#21153;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#26500;&#24314;&#38754;&#21521;&#23454;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#30340;&#31471;&#21040;&#31471;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#37325;&#28857;&#26159;&#25429;&#25417;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21517;&#20026;HEER&#65288;Healthcare Entity-Entity Representation learning&#65289;&#65292;&#23558;&#39046;&#22495;&#29305;&#23450;&#30340;&#32422;&#26463;&#21644;&#29305;&#24449;&#32435;&#20837;&#21040;&#22270;&#23884;&#20837;&#31639;&#27861;&#20013;&#12290;&#23545;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;HEER&#22312;&#25913;&#21892;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs (KGs) are a popular way to organise information based on ontologies or schemas and have been used across a variety of scenarios from search to recommendation. Despite advances in KGs, representing knowledge remains a non-trivial task across industries and it is especially challenging in the biomedical and healthcare domains due to complex interdependent relations between entities, heterogeneity, lack of standardization, and sparseness of data. KGs are used to discover diagnoses or prioritize genes relevant to disease, but they often rely on schemas that are not centred around a node or entity of interest, such as a person. Entity-centric KGs are relatively unexplored but hold promise in representing important facets connected to a central node and unlocking downstream tasks beyond graph traversal and reasoning, such as generating graph embeddings and training graph neural networks for a wide range of predictive tasks. This paper presents an end-to-end representation le
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;Holistically Thought&#65288;HoT&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32508;&#21512;&#24615;&#24605;&#32771;&#65292;&#20197;&#22312;&#21307;&#30103;&#23545;&#35805;&#38382;&#31572;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21307;&#23398;&#21709;&#24212;&#12290;</title><link>http://arxiv.org/abs/2305.05410</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#30103;&#23545;&#35805;&#38382;&#31572;&#20013;&#38656;&#35201;&#36827;&#34892;&#25972;&#20307;&#24615;&#24605;&#32771;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Need Holistically Thought in Medical Conversational QA. (arXiv:2305.05410v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;Holistically Thought&#65288;HoT&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32508;&#21512;&#24615;&#24605;&#32771;&#65292;&#20197;&#22312;&#21307;&#30103;&#23545;&#35805;&#38382;&#31572;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21307;&#23398;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#23545;&#35805;&#38382;&#31572;&#31995;&#32479;&#26088;&#22312;&#25552;&#20379;&#19968;&#31995;&#21015;&#19987;&#19994;&#30340;&#21307;&#30103;&#26381;&#21153;&#65292;&#20197;&#25552;&#39640;&#21307;&#30103;&#25252;&#29702;&#25928;&#29575;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#12289;&#36923;&#36753;&#21644;&#24120;&#35782;&#38382;&#31572;&#31561;&#21508;&#20010;&#39046;&#22495;&#30340;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#38543;&#30528;&#21307;&#23398;&#39046;&#22495;&#30340;&#26085;&#30410;&#22797;&#26434;&#21644;&#19987;&#19994;&#21270;&#65292;&#23427;&#20204;&#20173;&#38656;&#35201;&#25552;&#39640;&#12290;&#36825;&#26159;&#22240;&#20026;&#21307;&#30103;&#23545;&#35805;&#38382;&#31572;&#20219;&#21153;&#19981;&#20165;&#38656;&#35201;&#24378;&#22823;&#30340;&#21307;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#36824;&#38656;&#35201;&#24191;&#27867;&#28145;&#20837;&#30340;&#24605;&#32500;&#33021;&#21147;&#12290;&#26412;&#25991;&#38024;&#23545;&#36825;&#20123;&#38656;&#35201;&#20174;&#35768;&#22810;&#26041;&#38754;&#32771;&#34385;&#21644;&#29702;&#35299;&#30340;&#21307;&#30103;&#23545;&#35805;&#38382;&#31572;&#20219;&#21153;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#20840;&#38754;&#24605;&#32771;&#65288;HoT&#65289;&#26041;&#27861;&#65292;&#26088;&#22312;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25193;&#25955;&#21644;&#32858;&#28966;&#24605;&#32771;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21307;&#23398;&#21709;&#24212;&#12290;&#25152;&#25552;&#20986;&#30340;HoT&#26041;&#27861;&#24050;&#36890;&#36807;&#33258;&#21160;&#21270;&#21644;&#25163;&#21160;&#35780;&#20272;&#65292;&#22312;&#21253;&#21547;&#33521;&#25991;&#21644;&#20013;&#25991;&#30340;&#19977;&#20010;&#19981;&#21516;&#30340;&#21307;&#30103;&#23545;&#35805;&#38382;&#31572;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The medical conversational question answering (CQA) system aims at providing a series of professional medical services to improve the efficiency of medical care. Despite the success of large language models (LLMs) in complex reasoning tasks in various fields, such as mathematics, logic, and commonsense QA, they still need to improve with the increased complexity and specialization of the medical field. This is because medical CQA tasks require not only strong medical reasoning, but also the ability to think broadly and deeply. In this paper, to address these challenges in medical CQA tasks that need to be considered and understood in many aspects, we propose the Holistically Thought (HoT) method, which is designed to guide the LLMs to perform the diffused and focused thinking for generating high-quality medical responses. The proposed HoT method has been evaluated through automated and manual assessments in three different medical CQA datasets containing the English and Chinese languag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#23450;&#20041;&#20102;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#21462;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#20854;&#22312;&#32422;&#26463;&#24544;&#23454;&#24230;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#23545;&#36171;&#20104;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#33021;&#21147;&#38750;&#24120;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2305.05252</link><description>&lt;p&gt;
&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#33050;&#26412;&#30693;&#35782;&#20197;&#36827;&#34892;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Distilling Script Knowledge from Large Language Models for Constrained Language Planning. (arXiv:2305.05252v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05252
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#23450;&#20041;&#20102;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#21462;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#20854;&#22312;&#32422;&#26463;&#24544;&#23454;&#24230;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#23545;&#36171;&#20104;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#33021;&#21147;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#65292;&#20154;&#20204;&#32463;&#24120;&#36890;&#36807;&#36981;&#24490;&#30446;&#26631;&#23548;&#21521;&#30340;&#33050;&#26412;&#24418;&#24335;&#30340;&#36880;&#27493;&#35828;&#26126;&#26469;&#35268;&#21010;&#33258;&#24049;&#30340;&#34892;&#21160;&#12290;&#20197;&#24448;&#30340;&#24037;&#20316;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#26469;&#20026;&#31435;&#20307;&#27963;&#21160;&#30340;&#25277;&#35937;&#30446;&#26631;&#65288;&#20363;&#22914;&#65292;&#8220;&#21046;&#20316;&#34507;&#31957;&#8221;&#65289;&#36827;&#34892;&#35268;&#21010;&#65292;&#20294;&#23545;&#20110;&#20855;&#26377;&#22810;&#26041;&#38754;&#32422;&#26463;&#30340;&#26356;&#20855;&#20307;&#30446;&#26631;&#65288;&#20363;&#22914;&#65292;&#8220;&#20026;&#31958;&#23615;&#30149;&#24739;&#32773;&#21046;&#20316;&#34507;&#31957;&#8221;&#65289;&#40092;&#26377;&#30740;&#31350;&#12290;&#26412;&#25991;&#39318;&#27425;&#23450;&#20041;&#20102;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36807;&#24230;&#29983;&#25104;&#24182;&#36807;&#28388;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#21033;&#29992;&#23427;&#26469;&#25552;&#21462;&#19968;&#31181;&#26032;&#39062;&#30340;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#25968;&#25454;&#38598;CoScript&#65292;&#20854;&#20013;&#21253;&#25324;55,000&#20010;&#33050;&#26412;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;LLM&#22312;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#32422;&#26463;&#24544;&#23454;&#24230;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;CoScript&#34987;&#35777;&#26126;&#23545;&#36171;&#20104;&#36739;&#23567;&#30340;LM&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#33021;&#21147;&#26159;&#38750;&#24120;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In everyday life, humans often plan their actions by following step-by-step instructions in the form of goal-oriented scripts. Previous work has exploited language models (LMs) to plan for abstract goals of stereotypical activities (e.g., "make a cake"), but leaves more specific goals with multi-facet constraints understudied (e.g., "make a cake for diabetics"). In this paper, we define the task of constrained language planning for the first time. We propose an overgenerate-then-filter approach to improve large language models (LLMs) on this task, and use it to distill a novel constrained language planning dataset, CoScript, which consists of 55,000 scripts. Empirical results demonstrate that our method significantly improves the constrained language planning ability of LLMs, especially on constraint faithfulness. Furthermore, CoScript is demonstrated to be quite effective in endowing smaller LMs with constrained language planning ability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#25945;&#24072;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#30693;&#35782;&#26377;&#25928;&#22320;&#33976;&#39311;&#21040;&#31649;&#36947;&#27169;&#22411;&#20013;&#24182;&#20256;&#36882;&#32473;&#31471;&#21040;&#31471;TIMT&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.05226</link><description>&lt;p&gt;
&#25991;&#26412;&#22270;&#20687;&#26426;&#22120;&#32763;&#35793;&#22810;&#25945;&#24072;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Multi-Teacher Knowledge Distillation For Text Image Machine Translation. (arXiv:2305.05226v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05226
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#25945;&#24072;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#30693;&#35782;&#26377;&#25928;&#22320;&#33976;&#39311;&#21040;&#31649;&#36947;&#27169;&#22411;&#20013;&#24182;&#20256;&#36882;&#32473;&#31471;&#21040;&#31471;TIMT&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#22270;&#20687;&#26426;&#22120;&#32763;&#35793;&#65288;TIMT&#65289;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#23427;&#23558;&#22270;&#20687;&#20013;&#30340;&#28304;&#35821;&#35328;&#25991;&#26412;&#32763;&#35793;&#25104;&#21478;&#19968;&#31181;&#30446;&#26631;&#35821;&#35328;&#21477;&#23376;&#12290;TIMT&#30340;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20998;&#20026;&#20004;&#31181;&#31867;&#21035;&#65306;&#35782;&#21035;-&#28982;&#21518;-&#32763;&#35793;&#27969;&#31243;&#27169;&#22411;&#21644;&#31471;&#21040;&#31471;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#20174;&#31649;&#36947;&#27169;&#22411;&#21521;&#31471;&#21040;&#31471;&#27169;&#22411;&#20256;&#36882;&#30693;&#35782;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#25945;&#24072;&#30693;&#35782;&#33976;&#39311;&#65288;MTKD&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23558;&#30693;&#35782;&#33976;&#39311;&#21040;&#31649;&#36947;&#27169;&#22411;&#20013;&#24182;&#20256;&#36882;&#32473;&#31471;&#21040;&#31471;TIMT&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#21033;&#29992;&#19977;&#20010;&#25945;&#24072;&#26469;&#25552;&#39640;&#31471;&#21040;&#31471;TIMT&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#31471;&#21040;&#31471;TIMT&#27169;&#22411;&#20013;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#20351;&#29992;&#35782;&#21035;&#25945;&#24072;&#32534;&#30721;&#22120;&#30340;&#30693;&#35782;&#33976;&#39311;&#25351;&#23548;&#36827;&#34892;&#20248;&#21270;&#65292;&#32780;&#39034;&#24207;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#21017;&#36890;&#36807;&#20174;&#32763;&#35793;&#39034;&#24207;&#21644;&#35299;&#30721;&#22120;&#25945;&#24072;&#27169;&#22411;&#20256;&#36882;&#30693;&#35782;&#26469;&#25913;&#21892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text image machine translation (TIMT) has been widely used in various real-world applications, which translates source language texts in images into another target language sentence. Existing methods on TIMT are mainly divided into two categories: the recognition-then-translation pipeline model and the end-to-end model. However, how to transfer knowledge from the pipeline model into the end-to-end model remains an unsolved problem. In this paper, we propose a novel Multi-Teacher Knowledge Distillation (MTKD) method to effectively distillate knowledge into the end-to-end TIMT model from the pipeline model. Specifically, three teachers are utilized to improve the performance of the end-to-end TIMT model. The image encoder in the end-to-end TIMT model is optimized with the knowledge distillation guidance from the recognition teacher encoder, while the sequential encoder and decoder are improved by transferring knowledge from the translation sequential and decoder teacher models. Furthermo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#26377;&#25928;&#30340;&#25991;&#22270;&#26426;&#22120;&#32763;&#35793;&#27169;&#24577;&#36866;&#37197;&#22120;&#65292;&#21033;&#29992;&#29616;&#26377;OCR&#21644;MT&#25968;&#25454;&#24211;&#21644;&#26032;&#22411;&#27169;&#24577;&#36866;&#37197;&#22120;&#23558;OCR&#32534;&#30721;&#22120;&#21644;MT&#35299;&#30721;&#22120;&#36830;&#25509;&#65292;&#20351;&#24471;&#31471;&#21040;&#31471;TIMT&#27169;&#22411;&#22312;&#32763;&#35793;&#36136;&#37327;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#20004;&#38454;&#27573;&#32423;&#32852;&#27169;&#22411;&#21644;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#19968;&#32423;&#31471;&#21040;&#31471;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.05166</link><description>&lt;p&gt;
E2TIMT: &#39640;&#25928;&#26377;&#25928;&#30340;&#25991;&#22270;&#26426;&#22120;&#32763;&#35793;&#27169;&#24577;&#36866;&#37197;&#22120;
&lt;/p&gt;
&lt;p&gt;
E2TIMT: Efficient and Effective Modal Adapter for Text Image Machine Translation. (arXiv:2305.05166v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#26377;&#25928;&#30340;&#25991;&#22270;&#26426;&#22120;&#32763;&#35793;&#27169;&#24577;&#36866;&#37197;&#22120;&#65292;&#21033;&#29992;&#29616;&#26377;OCR&#21644;MT&#25968;&#25454;&#24211;&#21644;&#26032;&#22411;&#27169;&#24577;&#36866;&#37197;&#22120;&#23558;OCR&#32534;&#30721;&#22120;&#21644;MT&#35299;&#30721;&#22120;&#36830;&#25509;&#65292;&#20351;&#24471;&#31471;&#21040;&#31471;TIMT&#27169;&#22411;&#22312;&#32763;&#35793;&#36136;&#37327;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#20004;&#38454;&#27573;&#32423;&#32852;&#27169;&#22411;&#21644;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#19968;&#32423;&#31471;&#21040;&#31471;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#22270;&#20687;&#26426;&#22120;&#32763;&#35793;&#26088;&#22312;&#23558;&#23884;&#20837;&#22270;&#20687;&#20013;&#30340;&#25991;&#26412;&#20174;&#19968;&#31181;&#28304;&#35821;&#35328;&#32763;&#35793;&#25104;&#21478;&#19968;&#31181;&#30446;&#26631;&#35821;&#35328;&#12290;&#29616;&#26377;&#26041;&#27861;&#65292;&#26080;&#35770;&#26159;&#20004;&#38454;&#27573;&#32423;&#32852;&#36824;&#26159;&#19968;&#32423;&#31471;&#21040;&#31471;&#26550;&#26500;&#65292;&#37117;&#23384;&#22312;&#19981;&#21516;&#30340;&#38382;&#39064;&#12290;&#32423;&#32852;&#27169;&#22411;&#21487;&#20197;&#21463;&#30410;&#20110;&#22823;&#35268;&#27169;&#30340;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#65288;OCR&#65289;&#21644; MT &#25968;&#25454;&#38598;&#65292;&#20294;&#20004;&#38454;&#27573;&#26550;&#26500;&#21017;&#26159;&#20887;&#20313;&#30340;&#12290;&#31471;&#21040;&#31471;&#27169;&#22411;&#26159;&#39640;&#25928;&#30340;&#65292;&#20294;&#36973;&#21463;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#30340;&#22256;&#25200;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;TIMT&#27169;&#22411;&#65292;&#20805;&#20998;&#21033;&#29992;&#29616;&#26377;OCR&#21644;MT&#25968;&#25454;&#38598;&#20013;&#30340;&#30693;&#35782;&#65292;&#36861;&#27714;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#26694;&#26550;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#22411;&#27169;&#24577;&#36866;&#37197;&#22120;&#65292;&#26377;&#25928;&#22320;&#36830;&#25509;OCR&#32534;&#30721;&#22120;&#21644;MT&#35299;&#30721;&#22120;&#12290;&#21516;&#26102;&#20351;&#29992;&#31471;&#21040;&#31471;TIMT&#25439;&#22833;&#21644;&#36328;&#27169;&#24577;&#23545;&#27604;&#25439;&#22833;&#26469;&#23545;&#40784;OCR&#21644;MT&#20219;&#21153;&#30340;&#29305;&#24449;&#20998;&#24067;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#32763;&#35793;&#36136;&#37327;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#20004;&#38454;&#27573;&#32423;&#32852;&#27169;&#22411;&#21644;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#19968;&#32423;&#31471;&#21040;&#31471;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text image machine translation (TIMT) aims to translate texts embedded in images from one source language to another target language. Existing methods, both two-stage cascade and one-stage end-to-end architectures, suffer from different issues. The cascade models can benefit from the large-scale optical character recognition (OCR) and MT datasets but the two-stage architecture is redundant. The end-to-end models are efficient but suffer from training data deficiency. To this end, in our paper, we propose an end-to-end TIMT model fully making use of the knowledge from existing OCR and MT datasets to pursue both an effective and efficient framework. More specifically, we build a novel modal adapter effectively bridging the OCR encoder and MT decoder. End-to-end TIMT loss and cross-modal contrastive loss are utilized jointly to align the feature distribution of the OCR and MT tasks. Extensive experiments show that the proposed method outperforms the existing two-stage cascade models and o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#33976;&#39311;&#30340; URL &#20998;&#31867;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#32593;&#32476;&#20869;&#23481;&#36807;&#28388;&#65292;&#20854;&#23398;&#29983;&#27169;&#22411;&#22312;&#21442;&#25968;&#25968;&#37327;&#20943;&#23569; 175 &#20493;&#30340;&#24773;&#20917;&#19979;&#65292;&#31934;&#24230;&#25552;&#21319;&#20102; 9%&#65292;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.05027</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#33976;&#39311;&#30340;&#32593;&#32476;&#20869;&#23481;&#36807;&#28388;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Web Content Filtering through knowledge distillation of Large Language Models. (arXiv:2305.05027v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#33976;&#39311;&#30340; URL &#20998;&#31867;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#32593;&#32476;&#20869;&#23481;&#36807;&#28388;&#65292;&#20854;&#23398;&#29983;&#27169;&#22411;&#22312;&#21442;&#25968;&#25968;&#37327;&#20943;&#23569; 175 &#20493;&#30340;&#24773;&#20917;&#19979;&#65292;&#31934;&#24230;&#25552;&#21319;&#20102; 9%&#65292;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340; URL &#20998;&#31867;&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;&#32593;&#32476;&#20869;&#23481;&#36807;&#28388;&#30340;&#20027;&#35201;&#30446;&#26631;&#65306;&#20445;&#38556;&#32452;&#32455;&#20813;&#21463;&#27861;&#24459;&#21644;&#20262;&#29702;&#39118;&#38505;&#65292;&#38480;&#21046;&#35775;&#38382;&#39640;&#39118;&#38505;&#25110;&#21487;&#30097;&#32593;&#31449;&#65292;&#20197;&#21450;&#20419;&#36827;&#23433;&#20840;&#30340;&#19987;&#19994;&#24037;&#20316;&#29615;&#22659;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20934;&#30830;&#30340;&#20998;&#31867;&#65292;&#24182;&#21033;&#29992;&#24050;&#26377;&#30340;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#21019;&#24314;&#26356;&#23567;&#12289;&#26356;&#19987;&#19994;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#20197;&#29992;&#20110;&#32593;&#32476;&#20869;&#23481;&#36807;&#28388;&#12290;&#22312;&#23558;&#36890;&#36807;&#22823;&#22411;&#23433;&#20840;&#20379;&#24212;&#21830;&#25910;&#38598;&#30340;&#23458;&#25143;&#36965;&#27979;&#25968;&#25454;&#30340; 30 &#20010;&#19981;&#21516;&#20869;&#23481;&#31867;&#21035;&#30340;&#32593;&#31449;&#36827;&#34892;&#20998;&#31867;&#30340;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#30340;&#23398;&#29983;&#27169;&#22411;&#36890;&#36807;&#33976;&#39311;&#32467;&#26524;&#23454;&#29616;&#20102; 9% &#30340;&#20998;&#31867;&#31934;&#24230;&#25552;&#21319;&#65292;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23398;&#29983;&#27169;&#22411;&#22312;&#21442;&#25968;&#25968;&#37327;&#19978;&#19982;&#21407;&#22987;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#30456;&#27604;&#20943;&#23569;&#20102; 175 &#20493;&#65292;&#20174;&#32780;&#36798;&#21040;&#20102;&#19982;&#32769;&#24072;&#27169;&#22411;&#30456;&#21305;&#37197;&#30340;&#24615;&#33021;&#65292;&#21487;&#20197;&#29992;&#20110;&#22823;&#35268;&#27169;&#30340;&#22312;&#32447;&#25195;&#25551;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a state-of-the-art approach for URL categorization that leverages the power of Large Language Models (LLMs) to address the primary objectives of web content filtering: safeguarding organizations from legal and ethical risks, limiting access to high-risk or suspicious websites, and fostering a secure and professional work environment. Our method utilizes LLMs to generate accurate classifications and then employs established knowledge distillation techniques to create smaller, more specialized student models tailored for web content filtering. Distillation results in a student model with a 9\% accuracy rate improvement in classifying websites, sourced from customer telemetry data collected by a large security vendor, into 30 distinct content categories based on their URLs, surpassing the current state-of-the-art approach. Our student model matches the performance of the teacher LLM with 175 times less parameters, allowing the model to be used for in-line scanning of large vo
&lt;/p&gt;</description></item><item><title>CAT&#25552;&#20986;&#20102;&#19968;&#31181;&#24773;&#26223;&#21270;&#24120;&#35782;&#25512;&#29702;&#30340;&#27010;&#24565;&#21270;&#21644;&#23454;&#20363;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#35813;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#22823;&#35268;&#27169;&#22330;&#26223;&#19979;&#27010;&#24565;&#21270;&#24120;&#35782;&#30693;&#35782;&#24211;&#65292;&#24182;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.04808</link><description>&lt;p&gt;
CAT: &#19968;&#31181;&#24773;&#26223;&#21270;&#24120;&#35782;&#25512;&#29702;&#30340;&#27010;&#24565;&#21270;&#21644;&#23454;&#20363;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CAT: A Contextualized Conceptualization and Instantiation Framework for Commonsense Reasoning. (arXiv:2305.04808v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04808
&lt;/p&gt;
&lt;p&gt;
CAT&#25552;&#20986;&#20102;&#19968;&#31181;&#24773;&#26223;&#21270;&#24120;&#35782;&#25512;&#29702;&#30340;&#27010;&#24565;&#21270;&#21644;&#23454;&#20363;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#35813;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#22823;&#35268;&#27169;&#22330;&#26223;&#19979;&#27010;&#24565;&#21270;&#24120;&#35782;&#30693;&#35782;&#24211;&#65292;&#24182;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24120;&#35782;&#25512;&#29702;&#26088;&#22312;&#20026;&#26426;&#22120;&#36171;&#20104;&#31867;&#20284;&#20110;&#20154;&#30340;&#24773;&#26223;&#25512;&#26029;&#33021;&#21147;&#65292;&#36825;&#26497;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290; &#23545;&#20110;&#19968;&#20010;&#20960;&#20046;&#19981;&#20102;&#35299;&#8220;&#20901;&#24819;&#8221;&#20294;&#29087;&#24713;&#8220;&#21809;&#27468;&#8221;&#30340;&#20154;&#65292;&#20182;&#20173;&#28982;&#21487;&#20197;&#20174;&#29616;&#26377;&#30340;&#30693;&#35782;&#20013;&#36890;&#36807;&#23558;&#8220;&#21809;&#27468;&#8221;&#27010;&#24565;&#21270;&#20026;&#8220;&#19968;&#31181;&#25918;&#26494;&#30340;&#20107;&#20214;&#8221;&#65292;&#28982;&#21518;&#23558;&#35813;&#20107;&#20214;&#23454;&#20363;&#21270;&#20026;&#8220;&#20901;&#24819;&#8221;&#65292;&#25512;&#26029;&#20986;&#8220;&#20901;&#24819;&#20351;&#20154;&#25918;&#26494;&#8221;&#12290; &#36825;&#20010;&#36807;&#31243;&#31216;&#20026;&#27010;&#24565;&#24402;&#32435;&#21644;&#28436;&#32462;&#65292;&#22312;&#32570;&#20047;&#26631;&#35760;&#25968;&#25454;&#21644;&#25552;&#39640;&#24120;&#35782;&#24314;&#27169;&#26041;&#27861;&#30340;&#24773;&#20917;&#19979;&#21464;&#24471;&#26356;&#21152;&#26681;&#26412;&#12290;&#20026;&#22635;&#34917;&#36825;&#26679;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CAT&#65288;&#24773;&#26223;&#21270;&#27010;&#24565;&#21270;&#21644;&#23454;&#20363;&#21270;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;&#20107;&#20214;&#27010;&#24565;&#21270;&#21644;&#23454;&#20363;&#21270;&#34701;&#21512;&#36215;&#26469;&#65292;&#20197;&#22823;&#35268;&#27169;&#27010;&#24565;&#21270;&#24120;&#35782;&#30693;&#35782;&#24211;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#20004;&#20010;&#27010;&#24565;&#25512;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Commonsense reasoning, aiming at endowing machines with a human-like ability to make situational presumptions, is extremely challenging to generalize. For someone who barely knows about "meditation," while is knowledgeable about "singing," he can still infer that "meditation makes people relaxed" from the existing knowledge that "singing makes people relaxed" by first conceptualizing "singing" as a "relaxing event" and then instantiating that event to "meditation." This process, known as conceptual induction and deduction, is fundamental to commonsense reasoning while lacking both labeled data and methodologies to enhance commonsense modeling. To fill such a research gap, we propose CAT (Contextualized ConceptuAlization and InsTantiation), a semi-supervised learning framework that integrates event conceptualization and instantiation to conceptualize commonsense knowledge bases at scale. Extensive experiments show that our framework achieves state-of-the-art performances on two conceptu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;X-LLM&#30340;&#26041;&#27861;&#65292;&#23558;&#22810;&#27169;&#24577;&#20449;&#24687;&#36716;&#25442;&#20026;&#22806;&#35821;&#24182;&#36755;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#36171;&#20104;LLM&#22810;&#27169;&#24577;&#33021;&#21147;&#65292;&#23545;&#20110;LLM&#21152;&#20837;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#25506;&#31350;&#21644;&#25299;&#23637;&#12290;</title><link>http://arxiv.org/abs/2305.04160</link><description>&lt;p&gt;
X-LLM: &#36890;&#36807;&#23558;&#22810;&#27169;&#24577;&#35270;&#20026;&#22806;&#35821;&#24341;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#21551;&#21160;&#39640;&#32423;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages. (arXiv:2305.04160v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;X-LLM&#30340;&#26041;&#27861;&#65292;&#23558;&#22810;&#27169;&#24577;&#20449;&#24687;&#36716;&#25442;&#20026;&#22806;&#35821;&#24182;&#36755;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#36171;&#20104;LLM&#22810;&#27169;&#24577;&#33021;&#21147;&#65292;&#23545;&#20110;LLM&#21152;&#20837;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#25506;&#31350;&#21644;&#25299;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#35821;&#35328;&#33021;&#21147;&#12290;&#22522;&#20110;&#39640;&#32423;LLM&#30340;GPT-4&#34920;&#29616;&#20986;&#36229;&#24120;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#65292;&#36229;&#36234;&#20102;&#20197;&#24448;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#36825;&#24402;&#21151;&#20110;&#19982;&#20197;&#21069;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#30456;&#27604;&#20351;&#29992;&#20102;&#26356;&#20808;&#36827;&#30340;LLM&#12290;&#20294;&#19981;&#24184;&#30340;&#26159;&#65292;GPT-4&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#35757;&#32451;&#31574;&#30053;&#26159;&#26410;&#30693;&#30340;&#12290;&#20026;&#20102;&#36171;&#20104;LLM&#22810;&#27169;&#24577;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;X-LLM&#65292;&#36890;&#36807;&#20351;&#29992;X2L&#25509;&#21475;&#23558;&#22810;&#27169;&#24577;&#65288;&#22270;&#20687;&#12289;&#35821;&#38899;&#12289;&#35270;&#39057;&#65289;&#36716;&#25442;&#20026;&#22806;&#35821;&#24182;&#23558;&#20854;&#36755;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;ChatGLM&#65289;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;X-LLM&#20351;&#29992;X2L&#25509;&#21475;&#23558;&#22810;&#20010;&#20923;&#32467;&#30340;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#21644;&#20923;&#32467;&#30340;LLM&#23545;&#40784;&#65292;&#20854;&#20013;&#8220;X&#8221;&#34920;&#31034;&#22810;&#27169;&#24577;&#65292;&#20363;&#22914;&#22270;&#20687;&#12289;&#35821;&#38899;&#21644;&#35270;&#39057;&#65292;&#8220;L&#8221;&#34920;&#31034;&#35821;&#35328;&#12290;X-LLM&#30340;&#35757;&#32451;&#30001;&#19977;&#20010;&#38454;&#27573;&#32452;&#25104;&#65306;&#65288;1&#65289;&#36716;&#25442;&#22810;&#27169;&#24577;&#20449;&#24687;&#65306;&#31532;&#19968;&#38454;&#27573;&#20998;&#21035;&#35757;&#32451;&#27599;&#20010;X2L&#25509;&#21475;&#19982;&#20854;&#21508;&#33258;&#30340;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#23545;&#40784;&#65292;&#23558;&#22810;&#27169;&#24577;&#20449;&#24687;&#36716;&#25442;&#20026;&#22806;&#35821;&#36755;&#20837;&#21040;ChatGLM&#20013;&#12290;...
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable language abilities. GPT-4, based on advanced LLMs, exhibits extraordinary multimodal capabilities beyond previous visual language models. We attribute this to the use of more advanced LLMs compared with previous multimodal models. Unfortunately, the model architecture and training strategies of GPT-4 are unknown. To endow LLMs with multimodal capabilities, we propose X-LLM, which converts Multi-modalities (images, speech, videos) into foreign languages using X2L interfaces and inputs them into a large Language model (ChatGLM). Specifically, X-LLM aligns multiple frozen single-modal encoders and a frozen LLM using X2L interfaces, where ``X'' denotes multi-modalities such as image, speech, and videos, and ``L'' denotes languages. X-LLM's training consists of three stages: (1) Converting Multimodal Information: The first stage trains each X2L interface to align with its respective single-modal encoder separately to convert multimod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#20998;&#31867;&#22120;&#26469;&#24555;&#36895;&#36873;&#25321;&#26368;&#20339;&#30340;&#30456;&#20851;&#25991;&#26723;&#35821;&#26009;&#24211;&#36827;&#34892;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#36807;&#28388;&#25481;&#19981;&#30456;&#20851;&#30340;&#25512;&#25991;&#30340;&#26041;&#27861;&#65292;&#20197;&#36827;&#34892;&#22312;&#32447;&#28165;&#27905;&#33021;&#28304;&#24773;&#24863;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2305.03092</link><description>&lt;p&gt;
&#21033;&#29992;&#20998;&#31867;&#22120;&#26469;&#31579;&#36873;&#35821;&#26009;&#24211;&#65306;&#20197;&#22312;&#32447;&#28165;&#27905;&#33021;&#28304;&#24773;&#24863;&#20998;&#26512;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Curating corpora with classifiers: A case study of clean energy sentiment online. (arXiv:2305.03092v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#20998;&#31867;&#22120;&#26469;&#24555;&#36895;&#36873;&#25321;&#26368;&#20339;&#30340;&#30456;&#20851;&#25991;&#26723;&#35821;&#26009;&#24211;&#36827;&#34892;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#36807;&#28388;&#25481;&#19981;&#30456;&#20851;&#30340;&#25512;&#25991;&#30340;&#26041;&#27861;&#65292;&#20197;&#36827;&#34892;&#22312;&#32447;&#28165;&#27905;&#33021;&#28304;&#24773;&#24863;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#24515;&#31574;&#21010;&#30340;&#12289;&#22823;&#35268;&#27169;&#30340;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#35821;&#26009;&#24211;&#26159;&#34917;&#20805;&#20256;&#32479;&#35843;&#26597;&#30340;&#26367;&#20195;&#25968;&#25454;&#26469;&#28304;&#65292;&#21487;&#20197;&#25552;&#20379;&#24191;&#27867;&#30340;&#20844;&#20247;&#24847;&#35265;&#12290;&#34429;&#28982;&#35843;&#26597;&#22312;&#25910;&#38598;&#20195;&#34920;&#24615;&#26679;&#26412;&#21644;&#23454;&#29616;&#39640;&#20934;&#30830;&#29575;&#26041;&#38754;&#24456;&#26377;&#25928;&#65292;&#20294;&#36816;&#34892;&#25104;&#26412;&#24456;&#39640;&#65292;&#32780;&#19988;&#20250;&#28382;&#21518;&#20110;&#20844;&#20247;&#24847;&#35265;&#25968;&#22825;&#25110;&#25968;&#21608;&#12290;&#36825;&#20004;&#20010;&#32570;&#28857;&#21487;&#20197;&#36890;&#36807;&#23454;&#26102;&#12289;&#39640;&#23481;&#37327;&#30340;&#25968;&#25454;&#27969;&#21644;&#24555;&#36895;&#30340;&#20998;&#26512;&#31649;&#36947;&#20811;&#26381;&#12290;&#22312;&#32452;&#32455;&#36825;&#26679;&#30340;&#25968;&#25454;&#31649;&#36947;&#26041;&#38754;&#30340;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#26159;&#35774;&#35745;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#24555;&#36895;&#36873;&#25321;&#26368;&#20339;&#30340;&#30456;&#20851;&#25991;&#26723;&#35821;&#26009;&#24211;&#36827;&#34892;&#20998;&#26512;&#12290;&#20165;&#20165;&#36890;&#36807;&#20851;&#38190;&#35789;&#26597;&#35810;&#24448;&#24448;&#20250;&#21253;&#25324;&#19981;&#30456;&#20851;&#30340;&#25991;&#26723;&#65292;&#32780;&#36825;&#20123;&#25991;&#26723;&#24456;&#38590;&#29992;&#35789;&#34955;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#28040;&#27495;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#25163;&#21160;&#26631;&#27880;&#30340;&#25512;&#25991;&#19978;&#23545;&#20854;&#36827;&#34892;&#24494;&#35843;&#65292;&#25506;&#32034;&#20102;&#35821;&#26009;&#24211;&#31574;&#21010;&#30340;&#26041;&#27861;&#65292;&#20197;&#36807;&#28388;&#25481;&#19981;&#30456;&#20851;&#30340;&#25512;&#25991;&#12290;&#25105;&#20204;&#33021;&#22815;&#23454;&#29616;&#39640;&#36798;0.8&#20197;&#19978;&#30340;F1&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Well curated, large-scale corpora of social media posts containing broad public opinion offer an alternative data source to complement traditional surveys. While surveys are effective at collecting representative samples and are capable of achieving high accuracy, they can be both expensive to run and lag public opinion by days or weeks. Both of these drawbacks could be overcome with a real-time, high volume data stream and fast analysis pipeline. A central challenge in orchestrating such a data pipeline is devising an effective method for rapidly selecting the best corpus of relevant documents for analysis. Querying with keywords alone often includes irrelevant documents that are not easily disambiguated with bag-of-words natural language processing methods. Here, we explore methods of corpus curation to filter irrelevant tweets using pre-trained transformer-based models, fine-tuned for our binary classification task on hand-labeled tweets. We are able to achieve F1 scores of up to 0.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#24863;&#30693;&#30340;&#30693;&#35782;&#24341;&#23548;&#25552;&#31034;&#26041;&#27861;&#65292;&#23558;&#20854;&#20316;&#20026;&#24178;&#39044;&#22120;&#35013;&#22791;&#21040;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21477;&#23376;&#25552;&#21462;&#22120;&#20013;&#65292;&#20197;&#32531;&#35299;&#27010;&#24565;&#20559;&#24046;&#12290;&#22312;&#20195;&#34920;&#24615;&#30340;&#22810;&#35821;&#35328;KG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.01876</link><description>&lt;p&gt;
&#22522;&#20110;&#22240;&#26524;&#24863;&#30693;&#30340;&#30693;&#35782;&#24341;&#23548;&#21477;&#23376;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Causality-aware Concept Extraction based on Knowledge-guided Prompting. (arXiv:2305.01876v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01876
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#24863;&#30693;&#30340;&#30693;&#35782;&#24341;&#23548;&#25552;&#31034;&#26041;&#27861;&#65292;&#23558;&#20854;&#20316;&#20026;&#24178;&#39044;&#22120;&#35013;&#22791;&#21040;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21477;&#23376;&#25552;&#21462;&#22120;&#20013;&#65292;&#20197;&#32531;&#35299;&#27010;&#24565;&#20559;&#24046;&#12290;&#22312;&#20195;&#34920;&#24615;&#30340;&#22810;&#35821;&#35328;KG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#26377;&#21161;&#20110;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65292;&#20294;&#29616;&#26377;&#30340;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#20013;&#36828;&#26410;&#23436;&#21892;&#12290;&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#22522;&#20110;&#25991;&#26412;&#30340;&#27010;&#24565;&#25552;&#21462;&#65288;CE&#65289;&#12290;&#28982;&#32780;&#65292;PLM&#24448;&#24448;&#20174;&#22823;&#37327;&#35821;&#26009;&#24211;&#30340;&#20849;&#29616;&#20851;&#32852;&#20013;&#36827;&#34892;&#39044;&#35757;&#32451;&#30693;&#35782;&#25366;&#25496;&#65292;&#32780;&#38750;Token&#20043;&#38388;&#30340;&#30495;&#23454;&#22240;&#26524;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#39044;&#35757;&#32451;&#30693;&#35782;&#28151;&#28102;&#20102;PLM&#65292;&#23548;&#33268;&#25552;&#21462;&#22522;&#20110;&#34394;&#20551;&#20849;&#29616;&#30456;&#20851;&#24615;&#30340;&#26377;&#20559;&#27010;&#24565;&#65292;&#19981;&#21487;&#36991;&#20813;&#22320;&#23548;&#33268;&#20302;&#31934;&#24230;&#12290;&#26412;&#25991;&#36890;&#36807;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24341;&#23548;&#25552;&#31034;&#26041;&#27861;&#65292;&#23558;&#20854;&#20316;&#20026;&#24178;&#39044;&#22120;&#35013;&#22791;&#21040;&#22522;&#20110;PLM&#30340;&#25552;&#21462;&#22120;&#20013;&#65292;&#20197;&#20943;&#36731;&#27010;&#24565;&#20559;&#24046;&#12290;&#25552;&#31034;&#37319;&#29992;&#29616;&#26377;KG&#20013;&#30340;&#32473;&#23450;&#23454;&#20307;&#20027;&#39064;&#26469;&#32531;&#35299;&#23454;&#20307;&#21644;&#26377;&#20559;&#27010;&#24565;&#20043;&#38388;&#30340;&#34394;&#20551;&#20849;&#29616;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#22312;&#20195;&#34920;&#24615;&#30340;&#22810;&#35821;&#35328;KG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#25552;&#31034;&#26174;&#33879;&#25913;&#36827;&#20102;&#25552;&#21462;&#24615;&#33021;&#65292;&#24182;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concepts benefit natural language understanding but are far from complete in existing knowledge graphs (KGs). Recently, pre-trained language models (PLMs) have been widely used in text-based concept extraction (CE). However, PLMs tend to mine the co-occurrence associations from massive corpus as pre-trained knowledge rather than the real causal effect between tokens.As a result, the pre-trained knowledge confounds PLMs to extract biased concepts based on spurious co-occurrence correlations, inevitably resulting in low precision. In this paper, through the lens of a Structural Causal Model (SCM), we propose equipping the PLM-based extractor with a knowledge-guided prompt as an intervention to alleviate concept bias. The prompt adopts the topic of the given entity from the existing knowledge in KGs to mitigate the spurious co-occurrence correlations between entities and biased concepts. Our extensive experiments on representative multilingual KG datasets justify that our proposed prompt 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35745;&#21010;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#33021;&#22815;&#23558;&#31616;&#27905;&#30340;&#27969;&#31243;&#22270;&#36716;&#21270;&#25104;&#23545;&#35805;&#65292;&#20197;&#29983;&#25104;&#36275;&#22815;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#20197;&#27969;&#31243;&#22270;&#20026;&#22522;&#30784;&#30340;&#25925;&#38556;&#25490;&#38500;&#23545;&#35805;&#31995;&#32479;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#31995;&#32479;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.01323</link><description>&lt;p&gt;
&#23558;&#27969;&#31243;&#22270;&#36716;&#21270;&#20026;&#23545;&#35805;&#65306;&#22522;&#20110;&#35745;&#21010;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#29992;&#20110;&#20302;&#36164;&#28304;&#27969;&#31243;&#22270;&#30456;&#20851;&#25925;&#38556;&#25490;&#38500;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
Turning Flowchart into Dialog: Plan-based Data Augmentation for Low-Resource Flowchart-grounded Troubleshooting Dialogs. (arXiv:2305.01323v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01323
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35745;&#21010;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#33021;&#22815;&#23558;&#31616;&#27905;&#30340;&#27969;&#31243;&#22270;&#36716;&#21270;&#25104;&#23545;&#35805;&#65292;&#20197;&#29983;&#25104;&#36275;&#22815;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#20197;&#27969;&#31243;&#22270;&#20026;&#22522;&#30784;&#30340;&#25925;&#38556;&#25490;&#38500;&#23545;&#35805;&#31995;&#32479;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#31995;&#32479;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20197;&#27969;&#31243;&#22270;&#20026;&#22522;&#30784;&#30340;&#25925;&#38556;&#25490;&#38500;&#23545;&#35805;&#31995;&#32479;&#65288;FTD&#31995;&#32479;&#65289;&#19968;&#30452;&#22791;&#21463;&#30740;&#31350;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#20805;&#20998;&#30340;&#33258;&#28982;&#22522;&#20110;&#27969;&#31243;&#22270;&#30340;&#23545;&#35805;&#25968;&#25454;&#25104;&#26412;&#36739;&#39640;&#65292;&#22240;&#27492;FTD&#31995;&#32479;&#21463;&#21040;&#25968;&#25454;&#31232;&#32570;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#32531;&#35299;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#35745;&#21010;&#30340;&#25968;&#25454;&#22686;&#24378;&#65288;PlanDA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31616;&#27905;&#30340;&#27969;&#31243;&#22270;&#36716;&#21270;&#20026;&#23545;&#35805;&#65292;&#29983;&#25104;&#22823;&#37327;&#22810;&#26679;&#30340;&#21512;&#25104;&#23545;&#35805;&#25968;&#25454;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23427;&#30340;&#29983;&#25104;&#27169;&#22411;&#37319;&#29992;&#20855;&#26377;&#20840;&#23616;&#21644;&#23616;&#37096;&#28508;&#22312;&#35268;&#21010;&#21464;&#37327;&#30340;&#20998;&#23618;&#35268;&#21010;&#31574;&#30053;&#30340;&#21464;&#20998;&#22522;&#26694;&#26550;&#12290;&#22312;FloDial&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;PlanDA&#29983;&#25104;&#30340;&#21512;&#25104;&#23545;&#35805;&#25913;&#21892;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#27969;&#31243;&#22270;&#36335;&#24452;&#26816;&#32034;&#21644;&#21709;&#24212;&#29983;&#25104;&#65292;&#29305;&#21035;&#26159;&#22312;&#27969;&#31243;&#22270;&#20197;&#22806;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Flowchart-grounded troubleshooting dialogue (FTD) systems, which follow the instructions of a flowchart to diagnose users' problems in specific domains (eg., vehicle, laptop), have been gaining research interest in recent years. However, collecting sufficient dialogues that are naturally grounded on flowcharts is costly, thus FTD systems are impeded by scarce training data. To mitigate the data sparsity issue, we propose a plan-based data augmentation (PlanDA) approach that generates diverse synthetic dialog data at scale by transforming concise flowchart into dialogues. Specifically, its generative model employs a variational-base framework with a hierarchical planning strategy that includes global and local latent planning variables. Experiments on the FloDial dataset show that synthetic dialogue produced by PlanDA improves the performance of downstream tasks, including flowchart path retrieval and response generation, in particular on the Out-of-Flowchart settings. In addition, furt
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25299;&#25169;&#35270;&#35282;&#65292;&#29992;&#20110;&#25551;&#36848;&#21487;&#20197;&#34987;&#36712;&#36947;&#26377;&#38480;&#30340;&#21517;&#20041;&#21333;&#23376;&#32676;&#35782;&#21035;&#30340;&#25968;&#25454;&#35821;&#35328;&#65292;&#24182;&#25506;&#35752;&#20102; pro-&#36712;&#36947;&#26377;&#38480;&#26041;&#31243;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.13337</link><description>&lt;p&gt;
&#25968;&#25454;&#35821;&#35328;&#30340;&#21517;&#20041;&#25299;&#25169;
&lt;/p&gt;
&lt;p&gt;
Nominal Topology for Data Languages. (arXiv:2304.13337v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13337
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25299;&#25169;&#35270;&#35282;&#65292;&#29992;&#20110;&#25551;&#36848;&#21487;&#20197;&#34987;&#36712;&#36947;&#26377;&#38480;&#30340;&#21517;&#20041;&#21333;&#23376;&#32676;&#35782;&#21035;&#30340;&#25968;&#25454;&#35821;&#35328;&#65292;&#24182;&#25506;&#35752;&#20102; pro-&#36712;&#36947;&#26377;&#38480;&#26041;&#31243;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25299;&#25169;&#35270;&#35282;&#65292;&#29992;&#20110;&#25551;&#36848;&#21487;&#20197;&#34987;&#36712;&#36947;&#26377;&#38480;&#30340;&#21517;&#20041;&#21333;&#23376;&#32676;&#35782;&#21035;&#30340;&#25968;&#25454;&#35821;&#35328;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; pro- &#36712;&#36947;&#26377;&#38480;&#30340;&#21517;&#20041;&#25299;&#25169;&#31354;&#38388;&#12290;&#22312;&#20840;&#23616;&#26377;&#30028;&#25903;&#25345;&#22823;&#23567;&#30340;&#21069;&#25552;&#19979;&#65292;&#23427;&#20204;&#19982;&#21517;&#20041; Stone &#31354;&#38388;&#37325;&#21512;&#65292;&#24182;&#19988;&#34987;&#35777;&#26126;&#19982;&#21517;&#20041;&#24067;&#23572;&#20195;&#25968;&#30340;&#19968;&#20010;&#23376;&#33539;&#30068;&#21452;&#37325;&#31561;&#20215;&#12290;&#21487;&#35782;&#21035;&#30340;&#25968;&#25454;&#35821;&#35328;&#34987;&#34920;&#24449;&#20026; pro-&#36712;&#36947;&#26377;&#38480;&#21333;&#35789;&#30340;&#25299;&#25169;&#38381;&#24320;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#24314;&#31435; Reiterman &#30340;&#20266;&#21464;&#31181;&#23450;&#29702;&#30340;&#21517;&#20041;&#29256;&#26412;&#65292;&#25506;&#35752;&#20102; pro-&#36712;&#36947;&#26377;&#38480;&#26041;&#31243;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel topological perspective on data languages recognizable by orbit-finite nominal monoids. For this purpose, we introduce pro-orbit-finite nominal topological spaces. Assuming globally bounded support sizes, they coincide with nominal Stone spaces and are shown to be dually equivalent to a subcategory of nominal boolean algebras. Recognizable data languages are characterized as topologically clopen sets of pro-orbit-finite words. In addition, we explore the expressive power of pro-orbit-finite equations by establishing a nominal version of Reiterman's pseudovariety theorem.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#26469;&#25913;&#21892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20026;&#20195;&#30721;&#29983;&#25104;&#21644;&#20195;&#30721;&#25688;&#35201;&#31561;&#20219;&#21153;&#24494;&#35843;&#30340;&#29942;&#39048;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.01228</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#25105;&#25913;&#36827;&#30340;&#26041;&#24335;&#23454;&#29616;&#26356;&#22909;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Better Language Models of Code through Self-Improvement. (arXiv:2304.01228v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#26469;&#25913;&#21892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20026;&#20195;&#30721;&#29983;&#25104;&#21644;&#20195;&#30721;&#25688;&#35201;&#31561;&#20219;&#21153;&#24494;&#35843;&#30340;&#29942;&#39048;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#21508;&#31181;&#39044;&#35757;&#32451;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#22810;&#27169;&#24335;&#30446;&#26631;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#20294;&#26159;&#65292;&#23545;&#20854;&#36827;&#34892;&#24494;&#35843;&#38656;&#35201;&#22823;&#37327;&#30417;&#30563;&#65292;&#24182;&#19988;&#21463;&#21040;&#25552;&#20379;&#30340;&#25968;&#25454;&#38598;&#35268;&#27169;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#20197;&#25913;&#21892;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#20010;&#26694;&#26550;&#21033;&#29992;&#20102;&#22312;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#38454;&#27573;&#33719;&#24471;&#30340;&#30693;&#35782;&#26469;&#29983;&#25104;&#20266;&#25968;&#25454;&#65292;&#24182;&#23558;&#20854;&#29992;&#20316;&#19979;&#19968;&#27493;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#26694;&#26550;&#24212;&#29992;&#21040;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#22914;CodeT5&#12289;CodeBERT&#21644;UnixCoder&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#26174;&#33879;&#25552;&#39640;&#20102;PLMC&#22312;&#19982;&#20195;&#30721;&#30456;&#20851;&#30340;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#22914;CodeXGLUE&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#20195;&#30721;&#25688;&#35201;&#21644;&#20195;&#30721;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models for code (PLMCs) have gained attention in recent research. These models are pre-trained on large-scale datasets using multi-modal objectives. However, fine-tuning them requires extensive supervision and is limited by the size of the dataset provided. We aim to improve this issue by proposing a simple data augmentation framework. Our framework utilizes knowledge gained during the pre-training and fine-tuning stage to generate pseudo data, which is then used as training data for the next step. We incorporate this framework into the state-of-the-art language models, such as CodeT5, CodeBERT, and UnixCoder. The results show that our framework significantly improves PLMCs' performance in code-related sequence generation tasks, such as code summarization and code generation in the CodeXGLUE benchmark.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#65292;CNN&#22312;&#20581;&#24247;&#35760;&#24405;&#25991;&#26412;&#32534;&#30721;&#26041;&#38754;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#38544;&#21547;&#23618;&#27425;&#32467;&#26500;&#21487;&#20197;&#25552;&#39640;&#20854;&#24615;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN&#30340;&#32534;&#30721;&#22120;&#26469;&#22788;&#29702;&#19981;&#21516;&#31867;&#22411;&#30340;EHR&#29305;&#24449;&#65292;&#24182;&#22312;&#20020;&#24202;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.08290</link><description>&lt;p&gt;
&#37325;&#26032;&#21457;&#29616;CNN&#22312;&#21407;&#22987;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25991;&#26412;&#32534;&#30721;&#20013;&#30340;&#22810;&#21151;&#33021;&#24615;
&lt;/p&gt;
&lt;p&gt;
Rediscovery of CNN's Versatility for Text-based Encoding of Raw Electronic Health Records. (arXiv:2303.08290v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#65292;CNN&#22312;&#20581;&#24247;&#35760;&#24405;&#25991;&#26412;&#32534;&#30721;&#26041;&#38754;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#38544;&#21547;&#23618;&#27425;&#32467;&#26500;&#21487;&#20197;&#25552;&#39640;&#20854;&#24615;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN&#30340;&#32534;&#30721;&#22120;&#26469;&#22788;&#29702;&#19981;&#21516;&#31867;&#22411;&#30340;EHR&#29305;&#24449;&#65292;&#24182;&#22312;&#20020;&#24202;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20805;&#20998;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20013;&#20016;&#23500;&#30340;&#20449;&#24687;&#27491;&#36880;&#28176;&#25104;&#20026;&#21307;&#23398;&#39046;&#22495;&#30340;&#37325;&#35201;&#35805;&#39064;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#22312;&#19981;&#32771;&#34385;&#20854;&#26684;&#24335;&#21644;&#21307;&#23398;&#32534;&#30721;&#26631;&#20934;&#30340;&#24773;&#20917;&#19979;&#23884;&#20837;&#21407;&#22987;EHR&#25968;&#25454;&#30340;&#25972;&#20010;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#35813;&#26694;&#26550;&#20165;&#20391;&#37325;&#20110;&#23545;EHR&#36827;&#34892;&#26368;&#23567;&#30340;&#39044;&#22788;&#29702;&#65292;&#26410;&#32771;&#34385;&#22914;&#20309;&#23398;&#20064;&#39640;&#25928;&#30340;EHR&#34920;&#31034;&#65292;&#21253;&#25324;&#35745;&#31639;&#21644;&#20869;&#23384;&#20351;&#29992;&#31561;&#26041;&#38754;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23547;&#25214;&#19968;&#31181;&#22810;&#21151;&#33021;&#30340;&#32534;&#30721;&#22120;&#65292;&#19981;&#20165;&#23558;&#22823;&#37327;&#25968;&#25454;&#32553;&#23567;&#21040;&#21487;&#31649;&#29702;&#30340;&#22823;&#23567;&#65292;&#36824;&#33021;&#24456;&#22909;&#22320;&#20445;&#30041;&#24739;&#32773;&#30340;&#26680;&#24515;&#20449;&#24687;&#65292;&#20197;&#25191;&#34892;&#21508;&#31181;&#20020;&#24202;&#20219;&#21153;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20855;&#26377;&#20998;&#23618;&#32467;&#26500;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#65288;&#22914;&#37325;&#24314;&#65292;&#39044;&#27979;&#21644;&#29983;&#25104;&#65289;&#20013;&#32463;&#24120;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#21363;&#20351;&#21442;&#25968;&#36739;&#23569;&#19988;&#35757;&#32451;&#26102;&#38388;&#36739;&#30701;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;EHR&#25968;&#25454;&#30340;&#22266;&#26377;&#23618;&#27425;&#32467;&#26500;&#21487;&#20197;&#25552;&#39640;CNN&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#20123;&#21457;&#29616;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN&#30340;&#32534;&#30721;&#22120;&#65292;&#21487;&#20197;&#22788;&#29702;&#19981;&#21516;&#31867;&#22411;&#30340;EHR&#29305;&#24449;&#65292;&#24182;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#20960;&#31181;&#20020;&#24202;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Making the most use of abundant information in electronic health records (EHR) is rapidly becoming an important topic in the medical domain. Recent work presented a promising framework that embeds entire features in raw EHR data regardless of its form and medical code standards. The framework, however, only focuses on encoding EHR with minimal preprocessing and fails to consider how to learn efficient EHR representation in terms of computation and memory usage. In this paper, we search for a versatile encoder not only reducing the large data into a manageable size but also well preserving the core information of patients to perform diverse clinical tasks. We found that hierarchically structured Convolutional Neural Network (CNN) often outperforms the state-of-the-art model on diverse tasks such as reconstruction, prediction, and generation, even with fewer parameters and less training time. Moreover, it turns out that making use of the inherent hierarchy of EHR data can boost the perfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26059;&#36716;&#21644;&#32553;&#25918;&#30340;&#29305;&#24449;&#21464;&#25442;&#26657;&#20934;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22312;&#36716;&#25442;&#22120;&#20013;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#26102;&#36935;&#21040;&#30340;&#20449;&#24687;&#25193;&#25955;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.06198</link><description>&lt;p&gt;
&#21306;&#20998;&#24230;&#26657;&#20934;&#21040;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Distinguishability Calibration to In-Context Learning. (arXiv:2302.06198v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26059;&#36716;&#21644;&#32553;&#25918;&#30340;&#29305;&#24449;&#21464;&#25442;&#26657;&#20934;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22312;&#36716;&#25442;&#22120;&#20013;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#26102;&#36935;&#21040;&#30340;&#20449;&#24687;&#25193;&#25955;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#23545;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#26041;&#27861;&#30340;&#20852;&#36259;&#22686;&#21152;&#65292;&#27169;&#22411;&#33021;&#22815;&#22312;&#23569;&#37327;&#26631;&#27880;&#23454;&#20363;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#23427;&#20204;&#36866;&#29992;&#20110;&#20302;&#36164;&#28304;&#29615;&#22659;&#12290;&#20351;&#29992;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#26102;&#65292;&#30446;&#26631;&#26159;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411; (PLM) &#26469;&#39044;&#27979;&#39044;&#23450;&#20041;&#27169;&#26495;&#20013;&#30340;&#32570;&#22833;&#26631;&#35760;&#65292;&#24182;&#23558;&#20854;&#26144;&#23556;&#21040;&#31867;&#21035;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#36716;&#25442;&#22120;&#26550;&#26500;&#26500;&#24314;&#30340; PLM &#20542;&#21521;&#20110;&#29983;&#25104;&#30456;&#20284;&#30340;&#36755;&#20986;&#23884;&#20837;&#65292;&#24456;&#38590;&#21306;&#20998;&#19981;&#21516;&#30340;&#31867;&#21035;&#26631;&#31614;&#12290;&#24403;&#22788;&#29702;&#28041;&#21450;&#35768;&#22810;&#32454;&#31890;&#24230;&#31867;&#21035;&#26631;&#31614;&#30340;&#20998;&#31867;&#20219;&#21153;&#26102;&#65292;&#36825;&#20010;&#38382;&#39064;&#20250;&#36827;&#19968;&#27493;&#21152;&#21095;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#22522;&#20110;&#29305;&#24449;&#26059;&#36716;&#21644;&#32553;&#25918;&#30340;&#26657;&#20934;&#26041;&#27861;&#26469;&#32531;&#35299;&#36825;&#20010;&#20449;&#24687;&#25193;&#25955;&#38382;&#39064;&#65292;&#21363;&#24403;&#19981;&#21516;&#30340;&#20196;&#29260;&#32463;&#36807;&#36716;&#25442;&#22120;&#20013;&#22534;&#21472;&#30340;&#22810;&#20010;&#33258;&#27880;&#24847;&#23618;&#26102;&#65292;&#23427;&#20204;&#20849;&#20139;&#22823;&#37327;&#30456;&#20284;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed increasing interests in prompt-based learning in which models can be trained on only a few annotated instances, making them suitable in low-resource settings. When using prompt-based learning for text classification, the goal is to use a pre-trained language model (PLM) to predict a missing token in a pre-defined template given an input text, which can be mapped to a class label. However, PLMs built on the transformer architecture tend to generate similar output embeddings, making it difficult to discriminate between different class labels. The problem is further exacerbated when dealing with classification tasks involving many fine-grained class labels. In this work, we alleviate this information diffusion issue, i.e., different tokens share a large proportion of similar information after going through stacked multiple self-attention layers in a transformer, by proposing a calibration method built on feature transformations through rotation and scaling to m
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#26412;&#22320;&#26032;&#38395;&#26816;&#27979;&#21644;&#22522;&#20110;&#20869;&#23481;&#30340;&#26412;&#22320;&#26032;&#38395;&#25512;&#33616;&#26041;&#27861;&#65292;&#36890;&#36807;&#24369;&#30417;&#30563;&#26694;&#26550;&#21644;&#33258;&#21160;&#21270;&#25968;&#25454;&#22788;&#29702;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#35206;&#30422;&#29575;&#12290;</title><link>http://arxiv.org/abs/2301.08146</link><description>&lt;p&gt;
&#20320;&#25152;&#22312;&#31038;&#21306;&#21457;&#29983;&#20102;&#20160;&#20040;&#65311;&#19968;&#31181;&#24369;&#30417;&#30563;&#26041;&#27861;&#29992;&#20110;&#21457;&#29616;&#26412;&#22320;&#26032;&#38395;&#12290;
&lt;/p&gt;
&lt;p&gt;
What's happening in your neighborhood? A Weakly Supervised Approach to Detect Local News. (arXiv:2301.08146v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08146
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#26412;&#22320;&#26032;&#38395;&#26816;&#27979;&#21644;&#22522;&#20110;&#20869;&#23481;&#30340;&#26412;&#22320;&#26032;&#38395;&#25512;&#33616;&#26041;&#27861;&#65292;&#36890;&#36807;&#24369;&#30417;&#30563;&#26694;&#26550;&#21644;&#33258;&#21160;&#21270;&#25968;&#25454;&#22788;&#29702;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#35206;&#30422;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#22320;&#26032;&#38395;&#26159;&#24433;&#21709;&#29305;&#23450;&#22320;&#29702;&#21306;&#22495;&#65288;&#22914;&#22478;&#24066;&#12289;&#21439;&#21644;&#24030;&#65289;&#29992;&#25143;&#30340;&#26032;&#38395;&#23376;&#38598;&#12290;&#26816;&#27979;&#26412;&#22320;&#26032;&#38395;&#26159;&#20934;&#30830;&#22320;&#25512;&#33616;&#26412;&#22320;&#26032;&#38395;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#22522;&#20110;&#26368;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#38598;&#25104;&#21270;&#30340;&#27969;&#31243;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#21270;&#26412;&#22320;&#26032;&#38395;&#26816;&#27979;&#21644;&#22522;&#20110;&#20869;&#23481;&#30340;&#26412;&#22320;&#26032;&#38395;&#25512;&#33616;&#12290;&#26412;&#25991;&#30528;&#37325;&#20171;&#32461;&#20102;&#31649;&#36947;&#30340;&#31532;&#19968;&#27493;&#39588;&#65306;&#65288;1&#65289;&#32467;&#21512;&#39046;&#22495;&#30693;&#35782;&#21644;&#33258;&#21160;&#25968;&#25454;&#22788;&#29702;&#30340;&#24369;&#30417;&#30563;&#26694;&#26550;&#65292;&#65288;2&#65289;&#21487;&#25193;&#23637;&#21040;&#22810;&#35821;&#35328;&#35774;&#32622;&#12290;&#19982;&#26031;&#22374;&#31119;CoreNLP NER&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27969;&#31243;&#22312;&#32463;&#36807;&#30495;&#23454;&#19990;&#30028;&#21644;&#20154;&#24037;&#26631;&#35760;&#25968;&#25454;&#30340;&#35780;&#20272;&#26102;&#20855;&#26377;&#26356;&#39640;&#30340;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Local news articles are a subset of news that impact users in a geographical area, such as a city, county, or state. Detecting local news (Step 1) and subsequently deciding its geographical location as well as radius of impact (Step 2) are two important steps towards accurate local news recommendation. Naive rule-based methods, such as detecting city names from the news title, tend to give erroneous results due to lack of understanding of the news content. Empowered by the latest development in natural language processing, we develop an integrated pipeline that enables automatic local news detection and content-based local news recommendations. In this paper, we focus on Step 1 of the pipeline, which highlights: (1) a weakly supervised framework incorporated with domain knowledge and auto data processing, and (2) scalability to multi-lingual settings. Compared with Stanford CoreNLP NER model, our pipeline has higher precision and recall evaluated on a real-world and human-labeled datas
&lt;/p&gt;</description></item><item><title>QuaLA-MiniLM &#26159;&#19968;&#31181;&#37327;&#21270;&#38271;&#24230;&#33258;&#36866;&#24212;&#30340; MiniLM &#27169;&#22411;&#65292;&#36890;&#36807;&#24212;&#29992;&#20302;&#27604;&#29305;&#21270;&#25216;&#26415;&#21644; LAT &#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181; NLP &#20219;&#21153;&#19978;&#30340;&#26368;&#26032;&#25104;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#36739;&#23567;&#30340;&#27169;&#22411;&#23610;&#23544;&#21644;&#39640;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2210.17114</link><description>&lt;p&gt;
QuaLA-MiniLM: &#19968;&#31181;&#37327;&#21270;&#38271;&#24230;&#33258;&#36866;&#24212;&#30340; MiniLM
&lt;/p&gt;
&lt;p&gt;
QuaLA-MiniLM: a Quantized Length Adaptive MiniLM. (arXiv:2210.17114v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17114
&lt;/p&gt;
&lt;p&gt;
QuaLA-MiniLM &#26159;&#19968;&#31181;&#37327;&#21270;&#38271;&#24230;&#33258;&#36866;&#24212;&#30340; MiniLM &#27169;&#22411;&#65292;&#36890;&#36807;&#24212;&#29992;&#20302;&#27604;&#29305;&#21270;&#25216;&#26415;&#21644; LAT &#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181; NLP &#20219;&#21153;&#19978;&#30340;&#26368;&#26032;&#25104;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#36739;&#23567;&#30340;&#27169;&#22411;&#23610;&#23544;&#21644;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#32463;&#24120;&#38459;&#27490; Transformer &#27169;&#22411;&#34987;&#24212;&#29992;&#20110;&#29983;&#20135;&#29615;&#22659;&#65292;&#24182;&#21457;&#25381;&#20854;&#39640;&#31934;&#24230;&#20248;&#21183;&#12290;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#36890;&#36807;&#23558; BERT &#33258;&#25105;&#33976;&#39311;&#20026;&#36739;&#23567;&#30340; Transformer &#34920;&#31034;&#26469;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#65292;&#20854;&#23618;&#25968;&#26356;&#23569;&#65292;&#20869;&#37096;&#23884;&#20837;&#26356;&#23567;&#12290;&#28982;&#32780;&#65292;&#24403;&#25105;&#20204;&#20943;&#23569;&#23618;&#25968;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#23588;&#20854;&#26159;&#22312;&#19968;&#20123;&#20808;&#36827;&#30340; NLP &#20219;&#21153;&#22914;&#36328;&#24230;&#38382;&#31572;&#20013;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#27599;&#20010;&#19981;&#21516;&#30340;&#25512;&#29702;&#22330;&#26223;&#65292;&#37117;&#24517;&#39035;&#35757;&#32451;&#19968;&#20010;&#21333;&#29420;&#30340;&#27169;&#22411;&#20197;&#28385;&#36275;&#20854;&#19981;&#21516;&#30340;&#35745;&#31639;&#39044;&#31639;&#12290;Dynamic-TinyBERT &#36890;&#36807;&#37096;&#20998;&#23454;&#29616; Length Adaptive Transformer&#65288;LAT&#65289;&#25216;&#26415;&#21040; TinyBERT &#19978;&#35299;&#20915;&#20102;&#36825;&#20004;&#20010;&#38480;&#21046;&#65292;&#23454;&#29616;&#20102;&#19982; BERT-base &#30456;&#27604; x3 &#30340;&#21152;&#36895;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20102;&#31934;&#24230;&#25439;&#22833;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25193;&#23637; Dynamic-TinyBERT &#26041;&#27861;&#65292;&#29983;&#25104;&#20102;&#19968;&#20010;&#26356;&#21152;&#39640;&#25928;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558; MiniLM &#33976;&#39311;&#21644; LAT &#26041;&#27861;&#32852;&#21512;&#20351;&#29992;&#65292;&#24182;&#36890;&#36807;&#24212;&#29992;&#20302;&#27604;&#29305;&#21270;&#25216;&#26415;&#36827;&#19968;&#27493;&#25552;&#39640;&#25928;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411; QuaLA-MiniLM &#22312;&#21508;&#31181; NLP &#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#23567;&#27169;&#22411;&#23610;&#23544;&#21644;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Limited computational budgets often prevent transformers from being used in production and from having their high accuracy utilized. A knowledge distillation approach addresses the computational efficiency by self-distilling BERT into a smaller transformer representation having fewer layers and smaller internal embedding. However, the performance of these models drops as we reduce the number of layers, notably in advanced NLP tasks such as span question answering. In addition, a separate model must be trained for each inference scenario with its distinct computational budget. Dynamic-TinyBERT tackles both limitations by partially implementing the Length Adaptive Transformer (LAT) technique onto TinyBERT, achieving x3 speedup over BERT-base with minimal accuracy loss. In this work, we expand the Dynamic-TinyBERT approach to generate a much more highly efficient model. We use MiniLM distillation jointly with the LAT method, and we further enhance the efficiency by applying low-bit quanti
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CANDLE&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#32593;&#32476;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#25991;&#21270;&#24120;&#35782;&#30693;&#35782;&#65292;&#20854;&#20248;&#20110;&#20043;&#21069;&#30340;&#24037;&#20316;&#12290;&#36825;&#20123;&#30693;&#35782;&#23545;&#20110;&#24773;&#22659;&#21270;&#20154;&#24037;&#26234;&#33021;&#21644;GPT-3&#35821;&#35328;&#27169;&#22411;&#37117;&#26377;&#22909;&#22788;&#12290;</title><link>http://arxiv.org/abs/2210.07763</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#25552;&#21462;&#25991;&#21270;&#24120;&#35782;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Extracting Cultural Commonsense Knowledge at Scale. (arXiv:2210.07763v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CANDLE&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#32593;&#32476;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#25991;&#21270;&#24120;&#35782;&#30693;&#35782;&#65292;&#20854;&#20248;&#20110;&#20043;&#21069;&#30340;&#24037;&#20316;&#12290;&#36825;&#20123;&#30693;&#35782;&#23545;&#20110;&#24773;&#22659;&#21270;&#20154;&#24037;&#26234;&#33021;&#21644;GPT-3&#35821;&#35328;&#27169;&#22411;&#37117;&#26377;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#30693;&#35782;&#23545;&#20110;&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#12290;&#24120;&#35782;&#30693;&#35782;&#23545;&#20110;&#24378;&#22823;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20154;&#24037;&#26234;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#26159;&#24403;&#21069;&#21015;&#20986;&#30340;&#23569;&#25968;&#32467;&#26500;&#21270;&#24120;&#35782;&#39033;&#30446;&#32570;&#20047;&#26377;&#20851;&#31038;&#20250;&#25991;&#21270;&#32972;&#26223;&#19979;&#20154;&#31867;&#29305;&#24449;&#21644;&#34892;&#20026;&#30340;&#30693;&#35782;&#65292;&#36825;&#23545;&#20110;&#24773;&#22659;&#21270;&#20154;&#24037;&#26234;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;CANDLE&#65292;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#25552;&#21462;&#39640;&#36136;&#37327;&#25991;&#21270;&#24120;&#35782;&#30693;&#35782;&#65288;CCSK&#65289;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#12290;CANDLE&#20174;&#24222;&#22823;&#30340;&#32593;&#32476;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;CCSK&#26029;&#35328;&#65292;&#24182;&#23558;&#20854;&#32452;&#32455;&#25104;&#19968;&#33268;&#30340;&#32858;&#31867;&#65292;&#38024;&#23545;&#19977;&#20010;&#20027;&#39064;&#39046;&#22495;&#65288;&#22320;&#29702;&#65292;&#23447;&#25945;&#65292;&#32844;&#19994;&#65289;&#21644;&#20960;&#20010;&#25991;&#21270;&#26041;&#38754;&#36827;&#34892;&#20998;&#31867;&#65288;&#39135;&#29289;&#65292;&#39278;&#26009;&#65292;&#26381;&#35013;&#65292;&#20256;&#32479;&#65292;&#20202;&#24335;&#65292;&#34892;&#20026;&#65289;&#12290;CANDLE&#21253;&#25324;&#20998;&#31867;&#36807;&#28388;&#21644;&#36259;&#21619;&#24615;&#35780;&#20998;&#30340;&#23457;&#24910;&#25216;&#26415;&#12290;&#23454;&#39564;&#35780;&#20272;&#26174;&#31034;&#65292;CANDLE CCSK&#38598;&#21512;&#20248;&#20110;&#20043;&#21069;&#30340;&#24037;&#20316;&#65292;&#24182;&#19988;&#22806;&#37096;&#29992;&#20363;&#23637;&#31034;&#20102;CCSK&#23545;&#20110;GPT-3&#35821;&#35328;&#27169;&#22411;&#30340;&#22909;&#22788;&#12290;CANDLE&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#26159;&#20844;&#24320;&#21487;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structured knowledge is important for many AI applications. Commonsense knowledge, which is crucial for robust human-centric AI, is covered by a small number of structured knowledge projects. However, they lack knowledge about human traits and behaviors conditioned on socio-cultural contexts, which is crucial for situative AI. This paper presents CANDLE, an end-to-end methodology for extracting high-quality cultural commonsense knowledge (CCSK) at scale. CANDLE extracts CCSK assertions from a huge web corpus and organizes them into coherent clusters, for 3 domains of subjects (geography, religion, occupation) and several cultural facets (food, drinks, clothing, traditions, rituals, behaviors). CANDLE includes judicious techniques for classification-based filtering and scoring of interestingness. Experimental evaluations show the superiority of the CANDLE CCSK collection over prior works, and an extrinsic use case demonstrates the benefits of CCSK for the GPT-3 language model. Code and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#35821;&#35328;&#36328;&#27169;&#24577;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#20351;&#29992;&#19981;&#21516;&#35821;&#35328;&#30340;&#36755;&#20837;&#25991;&#26412;&#35757;&#32451;&#19968;&#20010;&#23398;&#29983;&#27169;&#22411;&#65292;&#19982;&#20351;&#29992;&#33521;&#35821;&#36755;&#20837;&#25991;&#26412;&#30340;&#25945;&#24072;&#27169;&#22411;&#30340;&#36328;&#27169;&#24577;&#39044;&#27979;&#30456;&#21305;&#37197;&#65292;&#20197;&#25552;&#21319;&#22810;&#35821;&#35328;&#25991;&#26412;-&#35270;&#39057;&#26816;&#32034;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#35821;&#35328;&#35270;&#39057;&#25968;&#25454;&#38598;&#65292;Multi-YouCook2&#65292;&#20197;&#21450;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2210.03625</link><description>&lt;p&gt;
C2KD: &#36328;&#35821;&#35328;&#36328;&#27169;&#24577;&#30693;&#35782;&#33976;&#39311;&#20197;&#25552;&#21319;&#22810;&#35821;&#35328;&#25991;&#26412;-&#35270;&#39057;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
C2KD: Cross-Lingual Cross-Modal Knowledge Distillation for Multilingual Text-Video Retrieval. (arXiv:2210.03625v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#35821;&#35328;&#36328;&#27169;&#24577;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#20351;&#29992;&#19981;&#21516;&#35821;&#35328;&#30340;&#36755;&#20837;&#25991;&#26412;&#35757;&#32451;&#19968;&#20010;&#23398;&#29983;&#27169;&#22411;&#65292;&#19982;&#20351;&#29992;&#33521;&#35821;&#36755;&#20837;&#25991;&#26412;&#30340;&#25945;&#24072;&#27169;&#22411;&#30340;&#36328;&#27169;&#24577;&#39044;&#27979;&#30456;&#21305;&#37197;&#65292;&#20197;&#25552;&#21319;&#22810;&#35821;&#35328;&#25991;&#26412;-&#35270;&#39057;&#26816;&#32034;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#35821;&#35328;&#35270;&#39057;&#25968;&#25454;&#38598;&#65292;Multi-YouCook2&#65292;&#20197;&#21450;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22810;&#35821;&#35328;&#25991;&#26412;-&#35270;&#39057;&#26816;&#32034;&#26041;&#27861;&#26377;&#20102;&#26174;&#33879;&#25552;&#21319;&#65292;&#20294;&#20854;&#23427;&#35821;&#35328;&#30340;&#34920;&#29616;&#20173;&#28982;&#33853;&#21518;&#20110;&#33521;&#35821;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#35821;&#35328;&#36328;&#27169;&#24577;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#20197;&#25552;&#21319;&#22810;&#35821;&#35328;&#25991;&#26412;-&#35270;&#39057;&#26816;&#32034;&#12290;&#21463;&#21040;&#33521;&#35821;&#25991;&#26412;-&#35270;&#39057;&#26816;&#32034;&#32988;&#36807;&#20854;&#23427;&#35821;&#35328;&#30340;&#20107;&#23454;&#21551;&#21457;&#65292;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#35821;&#35328;&#30340;&#36755;&#20837;&#25991;&#26412;&#35757;&#32451;&#19968;&#20010;&#23398;&#29983;&#27169;&#22411;&#65292;&#20351;&#20854;&#19982;&#20351;&#29992;&#33521;&#35821;&#36755;&#20837;&#25991;&#26412;&#30340;&#25945;&#24072;&#27169;&#22411;&#30340;&#36328;&#27169;&#24577;&#39044;&#27979;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20132;&#21449;&#29109;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#24378;&#21046;&#23398;&#29983;&#30340;&#25991;&#26412;-&#35270;&#39057;&#30456;&#20284;&#24230;&#20998;&#25968;&#20998;&#24067;&#19982;&#25945;&#24072;&#27169;&#22411;&#30340;&#30456;&#20284;&#24230;&#20998;&#25968;&#20998;&#24067;&#30456;&#20284;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#35821;&#35328;&#35270;&#39057;&#25968;&#25454;&#38598;Multi-YouCook2&#65292;&#36890;&#36807;&#23558;YouCook2&#35270;&#39057;&#25968;&#25454;&#38598;&#20013;&#30340;&#33521;&#35821;&#23383;&#24149;&#32763;&#35793;&#20026;8&#31181;&#20854;&#20182;&#35821;&#35328;&#26469;&#21019;&#24314;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;Multi-YouCook2&#20197;&#21450;&#22810;&#20010;&#20854;&#20182;&#25968;&#25454;&#38598;&#65292;&#27604;&#22914;Multi-MSRVTT&#21644;VATEX&#30340;&#22810;&#35821;&#35328;&#25991;&#26412;-&#35270;&#39057;&#26816;&#32034;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#23545;&#26041;&#27861;&#25928;&#26524;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual text-video retrieval methods have improved significantly in recent years, but the performance for other languages lags behind English. We propose a Cross-Lingual Cross-Modal Knowledge Distillation method to improve multilingual text-video retrieval. Inspired by the fact that English text-video retrieval outperforms other languages, we train a student model using input text in different languages to match the cross-modal predictions from teacher models using input text in English. We propose a cross entropy based objective which forces the distribution over the student's text-video similarity scores to be similar to those of the teacher models. We introduce a new multilingual video dataset, Multi-YouCook2, by translating the English captions in the YouCook2 video dataset to 8 other languages. Our method improves multilingual text-video retrieval performance on Multi-YouCook2 and several other datasets such as Multi-MSRVTT and VATEX. We also conducted an analysis on the effe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ViL-Sum&#65292;&#29992;&#20110;&#24314;&#27169;&#27573;&#33853;&#32423;&#21035;&#30340;&#35270;&#35273;-&#35821;&#35328;&#35821;&#20041;&#23545;&#40784;&#21644;&#22810;&#27169;&#24577;&#25688;&#35201;&#29983;&#25104;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ViL-Sum&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#34920;&#26126;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.11303</link><description>&lt;p&gt;
&#24314;&#27169;&#27573;&#33853;&#32423;&#21035;&#30340;&#35270;&#35273;-&#35821;&#35328;&#35821;&#20041;&#23545;&#40784;&#29992;&#20110;&#22810;&#27169;&#24577;&#25688;&#35201;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Modeling Paragraph-Level Vision-Language Semantic Alignment for Multi-Modal Summarization. (arXiv:2208.11303v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.11303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ViL-Sum&#65292;&#29992;&#20110;&#24314;&#27169;&#27573;&#33853;&#32423;&#21035;&#30340;&#35270;&#35273;-&#35821;&#35328;&#35821;&#20041;&#23545;&#40784;&#21644;&#22810;&#27169;&#24577;&#25688;&#35201;&#29983;&#25104;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ViL-Sum&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#34920;&#26126;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#22823;&#22810;&#25968;&#22810;&#27169;&#24577;&#25688;&#35201;&#26041;&#27861;&#37319;&#29992;&#32423;&#32852;&#26041;&#24335;&#65306;&#39318;&#20808;&#20351;&#29992;&#19968;&#20010;&#29616;&#25104;&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#25552;&#21462;&#35270;&#35273;&#29305;&#24449;, &#28982;&#21518;&#23558;&#36825;&#20123;&#29305;&#24449;&#19982;&#35821;&#35328;&#34920;&#31034;&#30456;&#34701;&#21512;&#65292;&#20351;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#29983;&#25104;&#25688;&#35201;&#12290;&#32423;&#32852;&#30340;&#26041;&#24335;&#26080;&#27861;&#25429;&#25417;&#22270;&#20687;&#21644;&#27573;&#33853;&#20043;&#38388;&#30340;&#35821;&#20041;&#23545;&#40784;&#65292;&#36825;&#23545;&#20110;&#20934;&#30830;&#30340;&#25688;&#35201;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ViL-Sum&#65292;&#29992;&#20110;&#32852;&#21512;&#24314;&#27169;&#27573;&#33853;&#32423;&#21035;&#30340;&#35270;&#35273;-&#35821;&#35328;&#35821;&#20041;&#23545;&#40784;&#21644;&#22810;&#27169;&#24577;&#25688;&#35201;&#29983;&#25104;&#12290;ViL-Sum&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#32852;&#21512;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#65292;&#20855;&#26377;&#20004;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#20219;&#21153;&#65306;&#22270;&#20687;&#37325;&#25490;&#24207;&#21644;&#22270;&#20687;&#36873;&#25321;&#12290;&#32852;&#21512;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#25429;&#25417;&#20102;&#27169;&#24577;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#65292;&#20854;&#20013;&#37325;&#25490;&#24207;&#20219;&#21153;&#24341;&#23548;&#27169;&#22411;&#23398;&#20064;&#27573;&#33853;&#32423;&#21035;&#30340;&#35821;&#20041;&#23545;&#40784;&#65292;&#32780;&#36873;&#25321;&#20219;&#21153;&#24341;&#23548;&#27169;&#22411;&#22312;&#26368;&#32456;&#29983;&#25104;&#30340;&#25688;&#35201;&#20013;&#36873;&#25321;&#19982;&#25688;&#35201;&#30456;&#20851;&#30340;&#22270;&#20687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;ViL-Sum&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#35828;&#26126;&#20102;&#25105;&#20204;&#30340;&#22810;&#27169;&#24577;&#25688;&#35201;&#29983;&#25104;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most current multi-modal summarization methods follow a cascaded manner, where an off-the-shelf object detector is first used to extract visual features, then these features are fused with language representations to generate the summary with an encoder-decoder model. The cascaded way cannot capture the semantic alignments between images and paragraphs, which are crucial to a precise summary. In this paper, we propose ViL-Sum to jointly model paragraph-level \textbf{Vi}sion-\textbf{L}anguage Semantic Alignment and Multi-Modal \textbf{Sum}marization. The core of ViL-Sum is a joint multi-modal encoder with two well-designed tasks, image reordering and image selection. The joint multi-modal encoder captures the interactions between modalities, where the reordering task guides the model to learn paragraph-level semantic alignment and the selection task guides the model to selected summary-related images in the final summary. Experimental results show that our proposed ViL-Sum significantly
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#22810;&#27573;&#33853;&#22810;&#31572;&#26696;&#38382;&#39064;&#30340;&#24320;&#25918;&#22495;&#38382;&#31572;&#22522;&#20934;&#27979;&#35797;QAMPARI&#65292;&#24182;&#35757;&#32451;&#20102;ODQA&#27169;&#22411;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;QAMPARI&#22312;&#27573;&#33853;&#26816;&#32034;&#21644;&#31572;&#26696;&#29983;&#25104;&#26041;&#38754;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#24378;&#35843;&#20102;&#38656;&#35201;&#21457;&#23637;&#33021;&#22815;&#22788;&#29702;&#27492;&#31867;&#38382;&#39064;&#30340;ODQA&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2205.12665</link><description>&lt;p&gt;
QAMPARI: &#19968;&#20010;&#22810;&#27573;&#33853;&#22810;&#31572;&#26696;&#30340;&#24320;&#25918;&#22495;&#38382;&#31572;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
QAMPARI: An Open-domain Question Answering Benchmark for Questions with Many Answers from Multiple Paragraphs. (arXiv:2205.12665v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12665
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#22810;&#27573;&#33853;&#22810;&#31572;&#26696;&#38382;&#39064;&#30340;&#24320;&#25918;&#22495;&#38382;&#31572;&#22522;&#20934;&#27979;&#35797;QAMPARI&#65292;&#24182;&#35757;&#32451;&#20102;ODQA&#27169;&#22411;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;QAMPARI&#22312;&#27573;&#33853;&#26816;&#32034;&#21644;&#31572;&#26696;&#29983;&#25104;&#26041;&#38754;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#24378;&#35843;&#20102;&#38656;&#35201;&#21457;&#23637;&#33021;&#22815;&#22788;&#29702;&#27492;&#31867;&#38382;&#39064;&#30340;ODQA&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#24320;&#25918;&#22495;&#38382;&#31572;&#65288;ODQA&#65289;&#22522;&#20934;&#27979;&#35797;&#36890;&#24120;&#19987;&#27880;&#20110;&#21487;&#20197;&#20174;&#21333;&#20010;&#27573;&#33853;&#20013;&#25552;&#21462;&#31572;&#26696;&#30340;&#38382;&#39064;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#35768;&#22810;&#33258;&#28982;&#38382;&#39064;&#65292;&#20363;&#22914;&#8220;&#24067;&#40065;&#20811;&#26519;&#31726;&#32593;&#38431;&#36873;&#20102;&#21738;&#20123;&#29699;&#21592;&#65311;&#8221;&#65292;&#37117;&#26377;&#19968;&#31995;&#21015;&#31572;&#26696;&#12290;&#22238;&#31572;&#27492;&#31867;&#38382;&#39064;&#38656;&#35201;&#22312;&#22823;&#22411;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#21644;&#38405;&#35835;&#26469;&#33258;&#35768;&#22810;&#27573;&#33853;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;QAMPARI&#65292;&#19968;&#31181;ODQA&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#38382;&#39064;&#31572;&#26696;&#26159;&#20998;&#24067;&#22312;&#35768;&#22810;&#27573;&#33853;&#20013;&#30340;&#23454;&#20307;&#21015;&#34920;&#12290;&#25105;&#20204;&#36890;&#36807;&#65288;a&#65289;&#20174;&#32500;&#22522;&#30334;&#31185;&#30340;&#30693;&#35782;&#22270;&#35889;&#21644;&#34920;&#20013;&#29983;&#25104;&#20855;&#26377;&#22810;&#20010;&#31572;&#26696;&#30340;&#38382;&#39064;&#65292;&#65288;b&#65289;&#33258;&#21160;&#23558;&#31572;&#26696;&#19982;&#32500;&#22522;&#30334;&#31185;&#27573;&#33853;&#20013;&#30340;&#25903;&#25345;&#35777;&#25454;&#37197;&#23545;&#65292;&#20197;&#21450;&#65288;c&#65289;&#25163;&#21160;&#25913;&#20889;&#38382;&#39064;&#24182;&#39564;&#35777;&#27599;&#20010;&#31572;&#26696;&#26469;&#21019;&#24314;QAMPARI&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#26469;&#33258;&#26816;&#32034;&#21644;&#38405;&#35835;&#26063;&#30340;ODQA&#27169;&#22411;&#65292;&#21457;&#29616;QAMPARI&#22312;&#27573;&#33853;&#26816;&#32034;&#21644;&#31572;&#26696;&#29983;&#25104;&#26041;&#38754;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#26368;&#39640;&#36798;&#21040;32.8&#30340;F1&#20998;&#25968;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#38656;&#35201;&#24320;&#21457;&#33021;&#22815;&#22788;&#29702;&#22810;&#27573;&#33853;&#22810;&#31572;&#26696;&#38382;&#39064;&#30340;ODQA&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing benchmarks for open-domain question answering (ODQA) typically focus on questions whose answers can be extracted from a single paragraph. By contrast, many natural questions, such as "What players were drafted by the Brooklyn Nets?" have a list of answers. Answering such questions requires retrieving and reading from many passages, in a large corpus. We introduce QAMPARI, an ODQA benchmark, where question answers are lists of entities, spread across many paragraphs. We created QAMPARI by (a) generating questions with multiple answers from Wikipedia's knowledge graph and tables, (b) automatically pairing answers with supporting evidence in Wikipedia paragraphs, and (c) manually paraphrasing questions and validating each answer. We train ODQA models from the retrieve-and-read family and find that QAMPARI is challenging in terms of both passage retrieval and answer generation, reaching an F1 score of 32.8 at best. Our results highlight the need for developing ODQA models that han
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861; REPINA&#65292;&#26088;&#22312;&#20943;&#23569;&#34920;&#31034;&#23849;&#28291;&#38382;&#39064;&#65292;&#32467;&#26524;&#22312; 13 &#20010;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2205.11603</link><description>&lt;p&gt;
&#34920;&#31034;&#25237;&#24433;&#19981;&#21464;&#24615;&#32531;&#35299;&#34920;&#31034;&#23849;&#28291;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Representation Projection Invariance Mitigates Representation Collapse. (arXiv:2205.11603v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.11603
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861; REPINA&#65292;&#26088;&#22312;&#20943;&#23569;&#34920;&#31034;&#23849;&#28291;&#38382;&#39064;&#65292;&#32467;&#26524;&#22312; 13 &#20010;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#30340;&#19978;&#19979;&#25991;&#21270;&#34920;&#31034;&#36827;&#34892;&#24494;&#35843;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#20173;&#28982;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#20570;&#27861;&#12290;&#28982;&#32780;&#65292;&#24494;&#35843;&#21487;&#33021;&#20250;&#23548;&#33268;&#34920;&#31034;&#38477;&#32423;&#65288;&#20063;&#34987;&#31216;&#20026;&#34920;&#31034;&#23849;&#28291;&#65289;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#31283;&#23450;&#24615;&#12289;&#27425;&#20248;&#24615;&#33021;&#21644;&#24369;&#27867;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#34920;&#31034;&#25237;&#24433;&#19981;&#21464;&#24615;&#8221;&#65288;REPINA&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#25233;&#21046;&#34920;&#31034;&#20013;&#30340;&#19981;&#33391;&#21464;&#21270;&#26469;&#32500;&#25252;&#34920;&#31034;&#30340;&#20449;&#24687;&#20869;&#23481;&#24182;&#20943;&#23569;&#34920;&#31034;&#23849;&#28291;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25152;&#25552;&#20986;&#30340;&#27491;&#21017;&#21270;&#19982;5&#20010;&#21487;&#27604;&#36739;&#22522;&#32447;&#22312;13&#20010;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#65288;GLUE&#22522;&#20934;&#27979;&#35797;&#21644;&#20854;&#20182;&#20845;&#20010;&#25968;&#25454;&#38598;&#65289;&#20013;&#30340;&#23454;&#35777;&#34892;&#20026;&#12290;&#22312;&#35780;&#20272;&#20869;&#22495;&#24615;&#33021;&#26102;&#65292;REPINA &#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#65288;13&#39033;&#20013;&#30340;10&#39033;&#65289;&#19978;&#22987;&#32456;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#23427;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#23545;&#26631;&#31614;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#12290;&#20316;&#20026;&#21103;&#20135;&#21697;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#24050;&#26377;&#30340;&#20808;&#21069;&#24037;&#20316;&#30340;&#33539;&#22260;&#65292;&#36825;&#20123;&#24037;&#20316;&#36890;&#36807;&#21253;&#25324;&#39044;&#27979;&#20219;&#21153;&#22312;&#20869;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26469;&#38477;&#20302;&#34920;&#31034;&#23849;&#28291;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning contextualized representations learned by pre-trained language models remains a prevalent practice in NLP. However, fine-tuning can lead to representation degradation (also known as representation collapse), which may result in instability, sub-optimal performance, and weak generalization.  In this paper, we propose Representation Projection Invariance (REPINA), a novel regularization method to maintain the information content of representation and reduce representation collapse during fine-tuning by discouraging undesirable changes in the representations. We study the empirical behavior of the proposed regularization in comparison to 5 comparable baselines across 13 language understanding tasks (GLUE benchmark and six additional datasets). When evaluating in-domain performance, REPINA consistently outperforms other baselines on most tasks (10 out of 13). We also demonstrate its effectiveness in few-shot settings and robustness to label perturbation. As a by-product, we ext
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21387;&#32553;&#23376;&#23618;&#30340;&#39640;&#25928;Transformer&#35299;&#30721;&#22120;&#65292;&#36890;&#36807;&#20943;&#23569;&#23376;&#23618;&#24182;&#25552;&#39640;&#24182;&#34892;&#24615;&#33021;&#22815;&#36798;&#21040;1.42&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#65292;&#21516;&#26102;&#30830;&#20445;&#24615;&#33021;&#19982;&#22522;&#32447;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2101.00542</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#21387;&#32553;&#23376;&#23618;&#30340;&#39640;&#25928;Transformer&#35299;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
An Efficient Transformer Decoder with Compressed Sub-layers. (arXiv:2101.00542v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2101.00542
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21387;&#32553;&#23376;&#23618;&#30340;&#39640;&#25928;Transformer&#35299;&#30721;&#22120;&#65292;&#36890;&#36807;&#20943;&#23569;&#23376;&#23618;&#24182;&#25552;&#39640;&#24182;&#34892;&#24615;&#33021;&#22815;&#36798;&#21040;1.42&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#65292;&#21516;&#26102;&#30830;&#20445;&#24615;&#33021;&#19982;&#22522;&#32447;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30001;&#20110;&#20854;&#39640;&#25928;&#32780;&#27969;&#34892;&#30340;&#22823;&#22411;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32593;&#32476;&#65288;Transformer&#65289;&#30340;&#35299;&#30721;&#22120;&#35745;&#31639;&#22797;&#26434;&#24230;&#39640;&#65292;&#23548;&#33268;&#25928;&#29575;&#38382;&#39064;&#12290;&#36890;&#36807;&#26597;&#30475;&#35299;&#30721;&#22120;&#30340;&#25968;&#23398;&#20844;&#24335;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#65292;&#21487;&#20197;&#36890;&#36807;&#21387;&#32553;&#20854;&#23376;&#23618;&#65288;Transformer&#30340;&#22522;&#26412;&#26500;&#24314;&#22359;&#65289;&#31616;&#21270;&#26550;&#26500;&#24182;&#23454;&#29616;&#26356;&#39640;&#30340;&#24182;&#34892;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21387;&#32553;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#20854;&#35299;&#30721;&#22120;&#23618;&#20165;&#21253;&#21547;&#19968;&#20010;&#23376;&#23618;&#32780;&#19981;&#26159;&#19977;&#20010;&#23376;&#23618;&#12290;&#22312;14&#20010;WMT&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#30340;&#24191;&#27867;&#23454;&#39564;&#20013;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#27604;&#24378;&#22522;&#32447;&#24555;1.42&#20493;&#65292;&#24182;&#19988;&#24615;&#33021;&#30456;&#24403;&#12290;&#32780;&#36825;&#20010;&#24378;&#22522;&#32447;&#24050;&#32463;&#27604;&#24191;&#27867;&#20351;&#29992;&#30340;&#26631;&#20934;&#22522;&#32447;&#24555;2&#20493;&#32780;&#19988;&#24615;&#33021;&#19981;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
The large attention-based encoder-decoder network (Transformer) has become prevailing recently due to its effectiveness. But the high computation complexity of its decoder raises the inefficiency issue. By examining the mathematic formulation of the decoder, we show that under some mild conditions, the architecture could be simplified by compressing its sub-layers, the basic building block of Transformer, and achieves a higher parallelism. We thereby propose Compressed Attention Network, whose decoder layer consists of only one sub-layer instead of three. Extensive experiments on 14 WMT machine translation tasks show that our model is 1.42x faster with performance on par with a strong baseline. This strong baseline is already 2x faster than the widely used standard baseline without loss in performance.
&lt;/p&gt;</description></item></channel></rss>