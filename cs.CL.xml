<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;SPAE&#65292;&#20351;&#29992;&#35821;&#20041;&#37329;&#23383;&#22612;&#33258;&#32534;&#30721;&#22120;&#23454;&#29616;&#20102;&#20923;&#32467;LLM&#25191;&#34892;&#28041;&#21450;&#38750;&#35821;&#35328;&#27169;&#24577;&#30340;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#12290;&#36890;&#36807;&#23558;&#22270;&#20687;&#36716;&#21270;&#20026;LLM&#21487;&#29702;&#35299;&#30340;&#35789;&#27719;&#26631;&#35760;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#25552;&#21319;&#20102;&#20923;&#32467;LLM&#22312;&#22270;&#20687;&#29702;&#35299;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#25216;&#26415;25%&#20197;&#19978;&#12290;</title><link>http://arxiv.org/abs/2306.17842</link><description>&lt;p&gt;
SPAE: &#22522;&#20110;&#35821;&#20041;&#37329;&#23383;&#22612;&#33258;&#32534;&#30721;&#22120;&#30340;&#20923;&#32467;LLM&#30340;&#22810;&#27169;&#24577;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs. (arXiv:2306.17842v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;SPAE&#65292;&#20351;&#29992;&#35821;&#20041;&#37329;&#23383;&#22612;&#33258;&#32534;&#30721;&#22120;&#23454;&#29616;&#20102;&#20923;&#32467;LLM&#25191;&#34892;&#28041;&#21450;&#38750;&#35821;&#35328;&#27169;&#24577;&#30340;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#12290;&#36890;&#36807;&#23558;&#22270;&#20687;&#36716;&#21270;&#20026;LLM&#21487;&#29702;&#35299;&#30340;&#35789;&#27719;&#26631;&#35760;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#25552;&#21319;&#20102;&#20923;&#32467;LLM&#22312;&#22270;&#20687;&#29702;&#35299;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#25216;&#26415;25%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;Semantic Pyramid AutoEncoder (SPAE)&#65292;&#20351;&#20923;&#32467;&#30340;LLM&#33021;&#22815;&#25191;&#34892;&#28041;&#21450;&#38750;&#35821;&#35328;&#27169;&#24577;&#65288;&#22914;&#22270;&#20687;&#25110;&#35270;&#39057;&#65289;&#30340;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#12290;SPAE&#22312;&#21407;&#22987;&#20687;&#32032;&#21644;&#20174;LLM&#35789;&#27719;&#34920;&#20013;&#25552;&#21462;&#30340;&#21487;&#35299;&#37322;&#30340;&#35789;&#27719;&#26631;&#35760;&#65288;&#25110;&#21333;&#35789;&#65289;&#20043;&#38388;&#36827;&#34892;&#36716;&#25442;&#12290;&#29983;&#25104;&#30340;&#26631;&#35760;&#25429;&#25417;&#20102;&#35270;&#35273;&#37325;&#24314;&#25152;&#38656;&#30340;&#35821;&#20041;&#21547;&#20041;&#21644;&#32454;&#31890;&#24230;&#32454;&#33410;&#65292;&#23558;&#35270;&#35273;&#20869;&#23481;&#36716;&#21270;&#20026;LLM&#33021;&#29702;&#35299;&#30340;&#35821;&#35328;&#65292;&#24182;&#20351;&#20854;&#33021;&#22815;&#25191;&#34892;&#21508;&#31181;&#22810;&#27169;&#24577;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;&#22810;&#26679;&#21270;&#30340;&#22270;&#20687;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#19978;&#65292;&#19982;&#20923;&#32467;&#30340;PaLM 2&#21644;GPT 3.5&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#23454;&#39564;&#35777;&#23454;&#12290;&#22312;&#30456;&#21516;&#30340;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#31532;&#19968;&#20010;&#25104;&#21151;&#20351;&#20923;&#32467;LLM&#29983;&#25104;&#22270;&#20687;&#20869;&#23481;&#65292;&#24182;&#22312;&#22270;&#20687;&#29702;&#35299;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#36229;&#36807;&#29616;&#26377;&#25216;&#26415;25%&#20197;&#19978;&#30340;&#23581;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos. SPAE converts between raw pixels and interpretable lexical tokens (or words) extracted from the LLM's vocabulary. The resulting tokens capture both the semantic meaning and the fine-grained details needed for visual reconstruction, effectively translating the visual content into a language comprehensible to the LLM, and empowering it to perform a wide array of multimodal tasks. Our approach is validated through in-context learning experiments with frozen PaLM 2 and GPT 3.5 on a diverse set of image understanding and generation tasks. Our method marks the first successful attempt to enable a frozen LLM to generate image content while surpassing state-of-the-art performance in image understanding tasks, under the same setting, by over 25%.
&lt;/p&gt;</description></item><item><title>Statler&#26159;&#19968;&#20010;&#20026;LLMs&#36171;&#20104;&#20102;&#26126;&#30830;&#30340;&#12289;&#32500;&#25345;&#29366;&#24577;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#35299;&#20915;&#24403;&#20195;LLMs&#22312;&#38271;&#26102;&#38388;&#33539;&#22260;&#20869;&#25512;&#29702;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2306.17840</link><description>&lt;p&gt;
Statler&#65306;&#29992;&#20110;&#20855;&#36523;&#25512;&#29702;&#30340;&#20445;&#25345;&#29366;&#24577;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Statler: State-Maintaining Language Models for Embodied Reasoning. (arXiv:2306.17840v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17840
&lt;/p&gt;
&lt;p&gt;
Statler&#26159;&#19968;&#20010;&#20026;LLMs&#36171;&#20104;&#20102;&#26126;&#30830;&#30340;&#12289;&#32500;&#25345;&#29366;&#24577;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#35299;&#20915;&#24403;&#20195;LLMs&#22312;&#38271;&#26102;&#38388;&#33539;&#22260;&#20869;&#25512;&#29702;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#26426;&#22120;&#20154;&#25191;&#34892;&#22797;&#26434;&#30340;&#26426;&#22120;&#20154;&#25512;&#29702;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#24403;&#20195;LLMs&#30340;&#26377;&#38480;&#19978;&#19979;&#25991;&#31383;&#21475;&#20351;&#24471;&#22312;&#38271;&#26102;&#38388;&#33539;&#22260;&#20869;&#36827;&#34892;&#25512;&#29702;&#21464;&#24471;&#22256;&#38590;&#12290;&#20855;&#36523;&#20219;&#21153;&#65288;&#20363;&#22914;&#25105;&#20204;&#26399;&#26395;&#19968;&#20010;&#23478;&#24237;&#26426;&#22120;&#20154;&#25191;&#34892;&#30340;&#20219;&#21153;&#65289;&#36890;&#24120;&#38656;&#35201;&#35268;&#21010;&#32773;&#32771;&#34385;&#24456;&#20037;&#20043;&#21069;&#33719;&#24471;&#30340;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#26426;&#22120;&#20154;&#22312;&#29615;&#22659;&#20013;&#36935;&#21040;&#30340;&#35768;&#22810;&#23545;&#35937;&#30340;&#23646;&#24615;&#65289;&#12290;&#36890;&#36807;LLM&#30340;&#38544;&#21547;&#20869;&#37096;&#34920;&#31034;&#26469;&#25429;&#33719;&#19990;&#30028;&#29366;&#24577;&#30340;&#23581;&#35797;&#20250;&#22240;&#20026;&#26426;&#22120;&#20154;&#25805;&#20316;&#21382;&#21490;&#20013;&#21487;&#29992;&#30340;&#19982;&#20219;&#21153;&#21644;&#29615;&#22659;&#30456;&#20851;&#30340;&#20449;&#24687;&#26377;&#38480;&#32780;&#21464;&#24471;&#22797;&#26434;&#65292;&#32780;&#20381;&#36182;&#36890;&#36807;&#25552;&#31034;&#21521;LLM&#20256;&#36882;&#20449;&#24687;&#30340;&#26041;&#27861;&#21017;&#21463;&#20854;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Statler&#65292;&#19968;&#20010;&#20026;LLMs&#36171;&#20104;&#20102;&#26126;&#30830;&#30340;&#12289;&#20316;&#20026;&#8220;&#35760;&#24518;&#8221;&#30340;&#19990;&#30028;&#29366;&#24577;&#34920;&#31034;&#30340;&#26694;&#26550;&#65292;&#36825;&#31181;&#35760;&#24518;&#38543;&#26102;&#38388;&#20445;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) provide a promising tool that enable robots to perform complex robot reasoning tasks. However, the limited context window of contemporary LLMs makes reasoning over long time horizons difficult. Embodied tasks such as those that one might expect a household robot to perform typically require that the planner consider information acquired a long time ago (e.g., properties of the many objects that the robot previously encountered in the environment). Attempts to capture the world state using an LLM's implicit internal representation is complicated by the paucity of task- and environment-relevant information available in a robot's action history, while methods that rely on the ability to convey information via the prompt to the LLM are subject to its limited context window. In this paper, we propose Statler, a framework that endows LLMs with an explicit representation of the world state as a form of ``memory'' that is maintained over time. Integral to Statler i
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#20803;&#25512;&#29702;&#8221;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#35821;&#20041;&#31526;&#21495;&#35299;&#26500;&#30340;&#26041;&#24335;&#65292;&#23558;&#19981;&#21516;&#25512;&#29702;&#38382;&#39064;&#36716;&#21270;&#20026;&#31867;&#20284;&#30340;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.17820</link><description>&lt;p&gt;
&#20803;&#25512;&#29702;&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#31526;&#21495;&#35299;&#26500;
&lt;/p&gt;
&lt;p&gt;
Meta-Reasoning: Semantics-Symbol Deconstruction For Large Language Models. (arXiv:2306.17820v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17820
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#20803;&#25512;&#29702;&#8221;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#35821;&#20041;&#31526;&#21495;&#35299;&#26500;&#30340;&#26041;&#24335;&#65292;&#23558;&#19981;&#21516;&#25512;&#29702;&#38382;&#39064;&#36716;&#21270;&#20026;&#31867;&#20284;&#30340;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31526;&#21495;&#21270;&#26041;&#27861;&#24050;&#32463;&#34987;&#35777;&#26126;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#23558;&#33258;&#28982;&#35821;&#35328;&#26144;&#23556;&#21040;&#26356;&#21152;&#35821;&#27861;&#23436;&#22791;&#19988;&#27809;&#26377;&#27495;&#20041;&#30340;&#24418;&#24335;&#35821;&#35328;&#65288;&#20363;&#22914;Python&#12289;SQL&#65289;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#31163;&#24320;&#20102;&#33258;&#28982;&#35821;&#35328;&#26412;&#36523;&#65292;&#20559;&#31163;&#20102;&#20154;&#31867;&#24605;&#32500;&#30340;&#20064;&#24815;&#65292;&#32780;&#26356;&#22810;&#22320;&#36814;&#21512;&#20102;&#35745;&#31639;&#26426;&#30340;&#25191;&#34892;&#24605;&#32500;&#26041;&#24335;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#24076;&#26395;&#20174;&#35821;&#35328;&#23398;&#20013;&#31526;&#21495;&#30340;&#27010;&#24565;&#20986;&#21457;&#26469;&#31616;&#21270;&#33258;&#28982;&#35821;&#35328;&#65292;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#19981;&#21516;&#33258;&#28982;&#35821;&#20041;&#20013;&#21253;&#21547;&#30340;&#25512;&#29702;&#38382;&#39064;&#30340;&#24120;&#35265;&#34920;&#36798;&#26041;&#24335;&#21644;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#12290;&#22522;&#20110;&#36825;&#31181;&#32771;&#34385;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#20803;&#25512;&#29702;&#8221;&#65292;&#23427;&#20801;&#35768;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#23436;&#25104;&#35821;&#20041;&#31526;&#21495;&#30340;&#35299;&#26500;&#65292;&#21363;&#35821;&#20041;&#35299;&#26512;&#65292;&#20174;&#32780;&#26368;&#22823;&#31243;&#24230;&#22320;&#23558;&#26576;&#20123;&#25512;&#29702;&#20219;&#21153;&#30340;&#19981;&#21516;&#38382;&#39064;&#20943;&#23569;&#21040;&#31867;&#20284;&#30340;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#65292;&#20174;&#32780;&#33719;&#24471;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symbolization methods in large language models (LLMs) have been shown effective to improve LLMs' reasoning ability. However, most of these approaches hinge on mapping natural languages to formal languages (e.g., Python, SQL) that are more syntactically complete and free of ambiguity. Although effective, they depart from the natural language itself and deviate from the habits of human thinking, and instead cater more to the execution mindset of computers. In contrast, we hope to simplify natural language by starting from the concept of symbols in linguistics itself, so that LLMs can learn the common formulation and general solution of reasoning problems wrapped in different natural semantics. From this consideration, we propose \textbf{Meta-Reasoning}, which allows LLMs to automatically accomplish semantic-symbol deconstruction, i.e., semantic resolution, to maximally reduce different questions of certain reasoning tasks to similar natural language representation, thus gaining the abili
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#37325;&#26032;&#25968;&#23383;&#21270;&#30340;&#26080;&#29256;&#26435;&#32654;&#22269;&#26412;&#22320;&#25253;&#32440;&#25991;&#31456;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#36328;&#36234;&#20102;70&#24180;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#25968;&#25454;&#38598;&#65292;&#24182;&#21253;&#21547;&#36817;4&#20159;&#20010;&#27491;&#21521;&#35821;&#20041;&#30456;&#20284;&#24615;&#23545;&#12290;</title><link>http://arxiv.org/abs/2306.17810</link><description>&lt;p&gt;
&#19968;&#20010;&#21382;&#21490;&#33521;&#35821;&#30340;&#22823;&#35268;&#27169;&#35821;&#20041;&#30456;&#20284;&#24615;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A Massive Scale Semantic Similarity Dataset of Historical English. (arXiv:2306.17810v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#37325;&#26032;&#25968;&#23383;&#21270;&#30340;&#26080;&#29256;&#26435;&#32654;&#22269;&#26412;&#22320;&#25253;&#32440;&#25991;&#31456;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#36328;&#36234;&#20102;70&#24180;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#25968;&#25454;&#38598;&#65292;&#24182;&#21253;&#21547;&#36817;4&#20159;&#20010;&#27491;&#21521;&#35821;&#20041;&#30456;&#20284;&#24615;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21508;&#31181;&#20219;&#21153;&#20351;&#29992;&#22312;&#35821;&#20041;&#30456;&#20284;&#24615;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#34429;&#28982;&#26377;&#22810;&#31181;&#25968;&#25454;&#38598;&#21487;&#25429;&#25417;&#35821;&#20041;&#30456;&#20284;&#24615;&#65292;&#20294;&#23427;&#20204;&#35201;&#20040;&#26159;&#20174;&#29616;&#20195;&#32593;&#32476;&#25968;&#25454;&#26500;&#24314;&#30340;&#65292;&#35201;&#20040;&#26159;&#30001;&#20154;&#24037;&#26631;&#27880;&#21592;&#22312;&#36807;&#21435;&#21313;&#24180;&#20013;&#21019;&#24314;&#30340;&#30456;&#23545;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#26469;&#28304;&#65292;&#21363;&#37325;&#26032;&#25968;&#23383;&#21270;&#30340;&#26080;&#29256;&#26435;&#32654;&#22269;&#26412;&#22320;&#25253;&#32440;&#25991;&#31456;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#25968;&#25454;&#38598;&#65292;&#36328;&#36234;&#20102;1920&#24180;&#21040;1989&#24180;&#30340;70&#24180;&#65292;&#24182;&#21253;&#21547;&#36817;4&#20159;&#20010;&#27491;&#21521;&#35821;&#20041;&#30456;&#20284;&#24615;&#23545;&#12290;&#22312;&#32654;&#22269;&#26412;&#22320;&#25253;&#32440;&#20013;&#65292;&#22823;&#32422;&#19968;&#21322;&#30340;&#25991;&#31456;&#26469;&#33258;&#26032;&#38395;&#26426;&#26500;&#30340;&#26032;&#38395;&#31295;&#65292;&#32780;&#26412;&#22320;&#25253;&#32440;&#22797;&#21046;&#20102;&#26032;&#38395;&#31295;&#30340;&#25991;&#31456;&#65292;&#24182;&#25776;&#20889;&#20102;&#33258;&#24049;&#30340;&#26631;&#39064;&#65292;&#36825;&#20123;&#26631;&#39064;&#24418;&#25104;&#20102;&#19982;&#25991;&#31456;&#30456;&#20851;&#30340;&#25552;&#21462;&#24615;&#25688;&#35201;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#25991;&#26723;&#24067;&#23616;&#21644;&#35821;&#35328;&#29702;&#35299;&#23558;&#25991;&#31456;&#21644;&#26631;&#39064;&#20851;&#32852;&#36215;&#26469;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#26041;&#27861;&#26469;&#26816;&#27979;&#21738;&#20123;&#25991;&#31456;&#26469;&#33258;&#30456;&#21516;&#30340;&#22522;&#30784;&#26469;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
A diversity of tasks use language models trained on semantic similarity data. While there are a variety of datasets that capture semantic similarity, they are either constructed from modern web data or are relatively small datasets created in the past decade by human annotators. This study utilizes a novel source, newly digitized articles from off-copyright, local U.S. newspapers, to assemble a massive-scale semantic similarity dataset spanning 70 years from 1920 to 1989 and containing nearly 400M positive semantic similarity pairs. Historically, around half of articles in U.S. local newspapers came from newswires like the Associated Press. While local papers reproduced articles from the newswire, they wrote their own headlines, which form abstractive summaries of the associated articles. We associate articles and their headlines by exploiting document layouts and language understanding. We then use deep neural methods to detect which articles are from the same underlying source, in th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#20998;&#31867;&#22120;&#26080;&#20851;&#30340;&#25351;&#23548;&#65288;CFG&#65289;&#21487;&#20197;&#20316;&#20026;&#19968;&#31181;&#25512;&#26029;&#26102;&#38388;&#25216;&#26415;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#32431;&#35821;&#35328;&#24314;&#27169;&#20013;&#21508;&#31181;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#33021;&#22815;&#22686;&#24378;&#21161;&#25163;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25552;&#31034;&#20013;&#30340;&#20934;&#30830;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.17806</link><description>&lt;p&gt;
&#19981;&#20351;&#29992;&#20998;&#31867;&#22120;&#30340;&#25351;&#23548;&#19979;&#20445;&#25345;&#35805;&#39064;&#30340;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Stay on topic with Classifier-Free Guidance. (arXiv:2306.17806v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#20998;&#31867;&#22120;&#26080;&#20851;&#30340;&#25351;&#23548;&#65288;CFG&#65289;&#21487;&#20197;&#20316;&#20026;&#19968;&#31181;&#25512;&#26029;&#26102;&#38388;&#25216;&#26415;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#32431;&#35821;&#35328;&#24314;&#27169;&#20013;&#21508;&#31181;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#33021;&#22815;&#22686;&#24378;&#21161;&#25163;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25552;&#31034;&#20013;&#30340;&#20934;&#30830;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#22120;&#26080;&#20851;&#30340;&#25351;&#23548;&#65288;CFG&#65289;&#26368;&#36817;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#20986;&#29616;&#65292;&#20316;&#20026;&#19968;&#31181;&#36731;&#37327;&#32423;&#25216;&#26415;&#20419;&#36827;&#29983;&#25104;&#30340;&#31435;&#21363;&#36981;&#24490;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;CFG&#21487;&#20197;&#24191;&#27867;&#29992;&#20316;&#32431;&#35821;&#35328;&#24314;&#27169;&#30340;&#25512;&#26029;&#26102;&#38388;&#25216;&#26415;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;CFG&#22312;&#19968;&#31995;&#21015;&#20219;&#21153;&#19978;&#25552;&#39640;&#20102;Pythia&#12289;GPT-2&#21644;LLaMA-family&#27169;&#22411;&#30340;&#24615;&#33021;&#65306;&#38382;&#31572;&#65292;&#25512;&#29702;&#65292;&#20195;&#30721;&#29983;&#25104;&#21644;&#26426;&#22120;&#32763;&#35793;&#65292;&#22312;LAMBADA&#19978;&#20351;&#29992;LLaMA-7B&#36229;&#36807;PaLM-540B&#30340;SOTA&#65307;&#65288;2&#65289;&#24102;&#26469;&#20102;&#30456;&#24403;&#20110;&#21452;&#20493;&#21442;&#25968;&#25968;&#30340;&#27169;&#22411;&#30340;&#25913;&#36827;&#65307;&#65288;3&#65289;&#21487;&#20197;&#19982;&#20854;&#20182;&#25512;&#26029;&#26102;&#38388;&#26041;&#27861;&#22914;Chain-of-Thought&#21644;Self-Consistency&#19968;&#36215;&#20351;&#29992;&#65292;&#22312;&#22256;&#38590;&#20219;&#21153;&#20013;&#21462;&#24471;&#36827;&#19968;&#27493;&#25913;&#36827;&#65307;&#65288;4&#65289;&#21487;&#20197;&#29992;&#20110;&#22686;&#21152;&#21161;&#25163;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24418;&#24335;&#39537;&#21160;&#21644;&#20869;&#23481;&#39537;&#21160;&#25552;&#31034;&#20013;&#30340;&#24544;&#23454;&#24230;&#21644;&#36830;&#36143;&#24615;&#65306;&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;75&#65285;&#30340;&#29992;&#25143;&#26356;&#21916;&#27426;&#20351;&#29992;CFG&#30340;GPT4All&#32780;&#19981;&#26159;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classifier-Free Guidance (CFG) has recently emerged in text-to-image generation as a lightweight technique to encourage prompt-adherence in generations. In this work, we demonstrate that CFG can be used broadly as an inference-time technique in pure language modeling. We show that CFG (1) improves the performance of Pythia, GPT-2 and LLaMA-family models across an array of tasks: Q\&amp;A, reasoning, code generation, and machine translation, achieving SOTA on LAMBADA with LLaMA-7B over PaLM-540B; (2) brings improvements equivalent to a model with twice the parameter-count; (3) can stack alongside other inference-time methods like Chain-of-Thought and Self-Consistency, yielding further improvements in difficult tasks; (4) can be used to increase the faithfulness and coherence of assistants in challenging form-driven and content-driven prompts: in a human evaluation we show a 75\% preference for GPT4All using CFG over baseline.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#24494;&#35843;&#23494;&#38598;&#23618;&#26367;&#25442;&#20026;&#20391;&#25233;&#21046;&#23618;&#65292;&#25552;&#39640;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#39044;&#35757;&#32451;&#35821;&#38899;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#22312;&#32599;&#39532;&#23612;&#20122;&#35821;&#35821;&#26009;&#24211;&#21644;Robin&#25216;&#26415;&#37319;&#38598;&#35821;&#26009;&#24211;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.17792</link><description>&lt;p&gt;
&#36890;&#36807;&#20391;&#25233;&#21046;&#26041;&#27861;&#25552;&#39640;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#39044;&#35757;&#32451;&#35821;&#38899;&#27169;&#22411;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Towards Improving the Performance of Pre-Trained Speech Models for Low-Resource Languages Through Lateral Inhibition. (arXiv:2306.17792v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#24494;&#35843;&#23494;&#38598;&#23618;&#26367;&#25442;&#20026;&#20391;&#25233;&#21046;&#23618;&#65292;&#25552;&#39640;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#39044;&#35757;&#32451;&#35821;&#38899;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#22312;&#32599;&#39532;&#23612;&#20122;&#35821;&#35821;&#26009;&#24211;&#21644;Robin&#25216;&#26415;&#37319;&#38598;&#35821;&#26009;&#24211;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21452;&#21521;&#32534;&#30721;&#22120;Transformer&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#20852;&#36215;&#65292;&#35821;&#38899;&#39046;&#22495;&#37319;&#29992;&#20102;&#20854;&#20013;&#19968;&#20123;&#24320;&#21457;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;Wav2Vec&#27169;&#22411;&#34987;&#24341;&#20837;&#20197;&#20943;&#23569;&#33719;&#21462;&#26368;&#20808;&#36827;&#32467;&#26524;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#36825;&#20123;&#30693;&#35782;&#65292;&#36890;&#36807;&#23558;&#24494;&#35843;&#23494;&#38598;&#23618;&#26367;&#25442;&#20026;&#21463;&#29983;&#29289;&#36807;&#31243;&#21551;&#21457;&#30340;&#20391;&#25233;&#21046;&#23618;&#26469;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#38899;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#32599;&#39532;&#23612;&#20122;&#35821;&#36825;&#31181;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#20391;&#25233;&#21046;&#23618;&#30340;&#24179;&#22343;&#23383;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#25552;&#39640;&#20102;12.5%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#32599;&#39532;&#23612;&#20122;&#35821;&#35821;&#38899;&#35821;&#26009;&#24211;&#21644;Robin&#25216;&#26415;&#37319;&#38598;&#35821;&#26009;&#24211;&#19978;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20998;&#21035;&#20026;1.78% WER&#21644;29.64% WER&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rise of bidirectional encoder representations from Transformer models in natural language processing, the speech community has adopted some of their development methodologies. Therefore, the Wav2Vec models were introduced to reduce the data required to obtain state-of-the-art results. This work leverages this knowledge and improves the performance of the pre-trained speech models by simply replacing the fine-tuning dense layer with a lateral inhibition layer inspired by the biological process. Our experiments on Romanian, a low-resource language, show an average improvement of 12.5% word error rate (WER) using the lateral inhibition layer. In addition, we obtain state-of-the-art results on both the Romanian Speech Corpus and the Robin Technical Acquisition Corpus with 1.78% WER and 29.64% WER, respectively.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#23383;&#31526;&#20018;&#27010;&#29575;&#26102;&#26159;&#21542;&#24212;&#35813;&#36793;&#32536;&#21270;&#25152;&#26377;&#21487;&#33021;&#30340;&#26631;&#35760;&#21270;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#24573;&#30053;&#36793;&#32536;&#21270;&#35745;&#31639;&#30340;&#24046;&#36317;&#19981;&#36229;&#36807;0.5%&#65292;&#20294;&#23545;&#20110;&#21547;&#26377;&#38271;&#22797;&#26434;&#21333;&#35789;&#30340;&#25968;&#25454;&#26469;&#35828;&#65292;&#36825;&#31181;&#24046;&#36317;&#26356;&#21152;&#26126;&#26174;&#12290;</title><link>http://arxiv.org/abs/2306.17757</link><description>&lt;p&gt;
&#26159;&#21542;&#24212;&#35813;&#23545;&#21487;&#33021;&#30340;&#26631;&#35760;&#21270;&#36827;&#34892;&#36793;&#32536;&#21270;&#35745;&#31639;&#65311;
&lt;/p&gt;
&lt;p&gt;
Should you marginalize over possible tokenizations?. (arXiv:2306.17757v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17757
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#23383;&#31526;&#20018;&#27010;&#29575;&#26102;&#26159;&#21542;&#24212;&#35813;&#36793;&#32536;&#21270;&#25152;&#26377;&#21487;&#33021;&#30340;&#26631;&#35760;&#21270;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#24573;&#30053;&#36793;&#32536;&#21270;&#35745;&#31639;&#30340;&#24046;&#36317;&#19981;&#36229;&#36807;0.5%&#65292;&#20294;&#23545;&#20110;&#21547;&#26377;&#38271;&#22797;&#26434;&#21333;&#35789;&#30340;&#25968;&#25454;&#26469;&#35828;&#65292;&#36825;&#31181;&#24046;&#36317;&#26356;&#21152;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;(LMs)&#23558;&#20196;&#29260;&#24207;&#21015;&#26144;&#23556;&#21040;&#27010;&#29575;&#12290;&#35745;&#31639;&#20219;&#20309;&#23383;&#31526;&#20018;(&#20363;&#22914;&#33521;&#25991;&#21477;&#23376;)&#30340;&#27010;&#29575;&#30340;&#24120;&#35265;&#20570;&#27861;&#26159;&#20808;&#23558;&#20854;&#36716;&#25442;&#20026;&#30001;&#27169;&#22411;&#35780;&#20998;&#30340;&#20196;&#29260;&#24207;&#21015;&#12290;&#28982;&#32780;&#65292;&#26377;&#25351;&#25968;&#32423;&#30340;&#20196;&#29260;&#24207;&#21015;&#21487;&#20197;&#34920;&#31034;&#20219;&#20309;&#32473;&#23450;&#30340;&#23383;&#31526;&#20018;&#12290;&#20026;&#20102;&#30495;&#27491;&#35745;&#31639;&#23383;&#31526;&#20018;&#30340;&#27010;&#29575;&#65292;&#24212;&#35813;&#23545;&#25152;&#26377;&#26631;&#35760;&#21270;&#36827;&#34892;&#36793;&#32536;&#21270;&#35745;&#31639;&#65292;&#20294;&#36825;&#36890;&#24120;&#26159;&#38590;&#20197;&#22788;&#29702;&#30340;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20998;&#26512;&#24573;&#30053;&#36793;&#32536;&#21270;&#35745;&#31639;&#30340;&#20570;&#27861;&#26159;&#21542;&#21512;&#29702;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#37325;&#35201;&#24615;&#37319;&#26679;&#30340;&#31639;&#27861;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#35745;&#31639;&#36793;&#32536;&#27010;&#29575;&#30340;&#20272;&#35745;&#65292;&#24182;&#23558;&#20854;&#19982;&#19968;&#31995;&#21015;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#20013;&#30340;&#40664;&#35748;&#36807;&#31243;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#23545;&#25968;&#20284;&#28982;&#24046;&#36317;&#19981;&#36229;&#36807;0.5&#65285;&#65292;&#20294;&#23545;&#20110;&#21253;&#21547;&#38271;&#22797;&#26434;&#21333;&#35789;&#30340;&#25968;&#25454;&#65292;&#36825;&#31181;&#24046;&#36317;&#21464;&#24471;&#26356;&#21152;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autoregressive language models (LMs) map token sequences to probabilities. The usual practice for computing the probability of any character string (e.g. English sentences) is to first transform it into a sequence of tokens that is scored by the model. However, there are exponentially many token sequences that represent any given string. To truly compute the probability of a string one should marginalize over all tokenizations, which is typically intractable. Here, we analyze whether the practice of ignoring the marginalization is justified. To this end, we devise an importance-sampling-based algorithm that allows us to compute estimates of the marginal probabilities and compare them to the default procedure in a range of state-of-the-art models and datasets. Our results show that the gap in log-likelihood is no larger than 0.5% in most cases, but that it becomes more pronounced for data with long complex words.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Token-Event-Role&#32467;&#26500;&#30340;&#22810;&#36890;&#36947;&#25991;&#26723;&#32423;&#20107;&#20214;&#25277;&#21462;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#25968;&#25454;&#32467;&#26500;&#21644;&#39044;&#27979;&#27169;&#22359;&#65292;&#33021;&#22815;&#26356;&#20840;&#38754;&#22320;&#29702;&#35299;&#20107;&#20214;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#39044;&#27979;token-event&#23545;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#23454;&#20307;&#21644;&#22810;&#20107;&#20214;&#25277;&#21462;&#30340;&#38598;&#25104;&#65292;&#20943;&#23569;&#20102;&#27169;&#22411;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.17733</link><description>&lt;p&gt;
&#22522;&#20110;Token-Event-Role&#32467;&#26500;&#30340;&#22810;&#36890;&#36947;&#25991;&#26723;&#32423;&#20107;&#20214;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Token-Event-Role Structure-based Multi-Channel Document-Level Event Extraction. (arXiv:2306.17733v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17733
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Token-Event-Role&#32467;&#26500;&#30340;&#22810;&#36890;&#36947;&#25991;&#26723;&#32423;&#20107;&#20214;&#25277;&#21462;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#25968;&#25454;&#32467;&#26500;&#21644;&#39044;&#27979;&#27169;&#22359;&#65292;&#33021;&#22815;&#26356;&#20840;&#38754;&#22320;&#29702;&#35299;&#20107;&#20214;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#39044;&#27979;token-event&#23545;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#23454;&#20307;&#21644;&#22810;&#20107;&#20214;&#25277;&#21462;&#30340;&#38598;&#25104;&#65292;&#20943;&#23569;&#20102;&#27169;&#22411;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#32423;&#20107;&#20214;&#25277;&#21462;&#26159;&#19968;&#20010;&#21382;&#21490;&#24736;&#20037;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20449;&#24687;&#26816;&#32034;&#38382;&#39064;&#65292;&#28041;&#21450;&#19968;&#31995;&#21015;&#23376;&#20219;&#21153;&#65306;&#23454;&#20307;&#25277;&#21462;&#12289;&#20107;&#20214;&#31867;&#22411;&#21028;&#26029;&#21644;&#29305;&#23450;&#20107;&#20214;&#31867;&#22411;&#30340;&#22810;&#20107;&#20214;&#25277;&#21462;&#12290;&#28982;&#32780;&#65292;&#23558;&#38382;&#39064;&#35270;&#20026;&#22810;&#20010;&#23398;&#20064;&#20219;&#21153;&#20250;&#22686;&#21152;&#27169;&#22411;&#22797;&#26434;&#24615;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#26041;&#27861;&#26410;&#20805;&#20998;&#21033;&#29992;&#36328;&#36234;&#19981;&#21516;&#20107;&#20214;&#30340;&#23454;&#20307;&#30340;&#30456;&#20851;&#24615;&#65292;&#23548;&#33268;&#20107;&#20214;&#25277;&#21462;&#24615;&#33021;&#26377;&#38480;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25991;&#26723;&#32423;&#20107;&#20214;&#25277;&#21462;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#31216;&#20026;token-event-role&#30340;&#26032;&#25968;&#25454;&#32467;&#26500;&#21644;&#19968;&#20010;&#22810;&#36890;&#36947;&#21442;&#25968;&#35282;&#33394;&#39044;&#27979;&#27169;&#22359;&#12290;&#25152;&#25552;&#20986;&#30340;&#25968;&#25454;&#32467;&#26500;&#20351;&#24471;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#25581;&#31034;&#22810;&#20010;&#20107;&#20214;&#20013;token&#30340;&#20027;&#35201;&#20316;&#29992;&#65292;&#20174;&#32780;&#26356;&#20840;&#38754;&#22320;&#29702;&#35299;&#20107;&#20214;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36890;&#36807;&#21033;&#29992;&#22810;&#36890;&#36947;&#39044;&#27979;&#27169;&#22359;&#65292;&#25105;&#20204;&#23558;&#23454;&#20307;&#21644;&#22810;&#20107;&#20214;&#25277;&#21462;&#36716;&#21270;&#20026;&#39044;&#27979;token-event&#23545;&#30340;&#21333;&#19968;&#20219;&#21153;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#27169;&#22411;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document-level event extraction is a long-standing challenging information retrieval problem involving a sequence of sub-tasks: entity extraction, event type judgment, and event type-specific multi-event extraction. However, addressing the problem as multiple learning tasks leads to increased model complexity. Also, existing methods insufficiently utilize the correlation of entities crossing different events, resulting in limited event extraction performance. This paper introduces a novel framework for document-level event extraction, incorporating a new data structure called token-event-role and a multi-channel argument role prediction module. The proposed data structure enables our model to uncover the primary role of tokens in multiple events, facilitating a more comprehensive understanding of event relationships. By leveraging the multi-channel prediction module, we transform entity and multi-event extraction into a single task of predicting token-event pairs, thereby reducing the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#23618;&#19987;&#23478;&#29983;&#25104;SQL&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#19987;&#29992;&#30340;&#22810;&#20219;&#21153;&#20998;&#23618;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#30001;&#20110;&#19981;&#21516;&#20998;&#31867;&#20219;&#21153;&#30340;&#36127;&#36801;&#31227;&#38382;&#39064;&#23548;&#33268;&#29983;&#25104;&#19981;&#20934;&#30830;SQL&#35821;&#21477;&#30340;&#38480;&#21046;&#12290;&#35813;&#26041;&#27861;&#22312;WiKSQL&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.17727</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#23618;&#19987;&#23478;&#32593;&#32476;&#30340;&#25913;&#36827;NL2SQL&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Improved NL2SQL based on Multi-layer Expert Network. (arXiv:2306.17727v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#23618;&#19987;&#23478;&#29983;&#25104;SQL&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#19987;&#29992;&#30340;&#22810;&#20219;&#21153;&#20998;&#23618;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#30001;&#20110;&#19981;&#21516;&#20998;&#31867;&#20219;&#21153;&#30340;&#36127;&#36801;&#31227;&#38382;&#39064;&#23548;&#33268;&#29983;&#25104;&#19981;&#20934;&#30830;SQL&#35821;&#21477;&#30340;&#38480;&#21046;&#12290;&#35813;&#26041;&#27861;&#22312;WiKSQL&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#21040;SQL&#65288;NL2SQL&#65289;&#25216;&#26415;&#29992;&#20110;&#23558;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#36716;&#25442;&#20026;&#21487;&#25191;&#34892;&#30340;SQL&#35821;&#21477;&#12290;&#36890;&#24120;&#65292;&#36890;&#36807;&#25554;&#27133;&#22635;&#20805;&#20316;&#20026;&#22810;&#20219;&#21153;&#20998;&#31867;&#26041;&#27861;&#26469;&#23454;&#29616;&#27492;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19981;&#21516;&#20998;&#31867;&#20219;&#21153;&#30340;&#36127;&#36801;&#31227;&#38382;&#39064;&#65292;&#25554;&#27133;&#22635;&#20805;&#21487;&#33021;&#23548;&#33268;&#29983;&#25104;&#19981;&#20934;&#30830;&#30340;SQL&#35821;&#21477;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#23618;&#19987;&#23478;&#29983;&#25104;SQL&#65288;MLEG-SQL&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#19987;&#29992;&#30340;&#22810;&#20219;&#21153;&#20998;&#23618;&#32593;&#32476;&#12290;&#32593;&#32476;&#30340;&#19979;&#23618;&#25552;&#21462;&#33258;&#28982;&#35821;&#35328;&#35821;&#21477;&#30340;&#35821;&#20041;&#29305;&#24449;&#65292;&#32780;&#19978;&#23618;&#26500;&#24314;&#19968;&#20010;&#19987;&#38376;&#30340;&#19987;&#23478;&#31995;&#32479;&#26469;&#22788;&#29702;&#29305;&#23450;&#30340;&#20998;&#31867;&#20219;&#21153;&#12290;&#36825;&#31181;&#20998;&#23618;&#26041;&#27861;&#20943;&#36731;&#20102;&#19981;&#21516;&#20219;&#21153;&#20914;&#31361;&#24102;&#26469;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#35813;&#26041;&#27861;&#22312;WiKSQL&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#35777;&#26126;&#22312;&#29983;&#25104;&#20934;&#30830;&#30340;SQL&#35821;&#21477;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Natural Language to SQL (NL2SQL) technique is used to convert natural language queries into executable SQL statements. Typically, slot-filling is employed as a classification method for multi-task cases to achieve this goal. However, slot-filling can result in inaccurate SQL statement generation due to negative migration issues arising from different classification tasks. To overcome this limitation, this study introduces a new approach called Multi-Layer Expert Generate SQL (MLEG-SQL), which utilizes a dedicated multi-task hierarchical network. The lower layer of the network extracts semantic features of natural language statements, while the upper layer builds a specialized expert system for handling specific classification tasks. This hierarchical approach mitigates performance degradation resulting from different task conflicts. The proposed method was evaluated on the WiKSQL dataset and was found to be effective in generating accurate SQL statements.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36229;&#36234;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#24615;&#21035;&#20445;&#25252;&#30340;&#30740;&#31350;&#26041;&#27861;&#65292;&#24182;&#24378;&#35843;&#20102;&#27979;&#35797;&#22522;&#20110;&#35821;&#38899;&#29305;&#24449;&#30340;&#24615;&#21035;&#25512;&#27979;&#25915;&#20987;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#21450;&#19982;&#20154;&#31867;&#25191;&#34892;&#30340;&#22768;&#38899;&#36866;&#24212;&#36827;&#34892;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2306.17700</link><description>&lt;p&gt;
&#36229;&#36234;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#20445;&#25252;&#28436;&#35762;&#32773;&#24615;&#21035;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Beyond Neural-on-Neural Approaches to Speaker Gender Protection. (arXiv:2306.17700v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17700
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36229;&#36234;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#24615;&#21035;&#20445;&#25252;&#30340;&#30740;&#31350;&#26041;&#27861;&#65292;&#24182;&#24378;&#35843;&#20102;&#27979;&#35797;&#22522;&#20110;&#35821;&#38899;&#29305;&#24449;&#30340;&#24615;&#21035;&#25512;&#27979;&#25915;&#20987;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#21450;&#19982;&#20154;&#31867;&#25191;&#34892;&#30340;&#22768;&#38899;&#36866;&#24212;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20123;&#20462;&#25913;&#35821;&#38899;&#20197;&#38450;&#27490;&#24615;&#21035;&#25512;&#27979;&#25915;&#20987;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#20445;&#25252;&#31639;&#27861;&#30340;&#30446;&#26631;&#26159;&#25511;&#21046;&#20851;&#20110;&#28436;&#35762;&#32773;&#24615;&#21035;&#36825;&#20010;&#38544;&#31169;&#25935;&#24863;&#23646;&#24615;&#30340;&#20449;&#24687;&#30340;&#21487;&#29992;&#24615;&#12290;&#30446;&#21069;&#65292;&#24320;&#21457;&#21644;&#27979;&#35797;&#24615;&#21035;&#20445;&#25252;&#31639;&#27861;&#30340;&#24120;&#35265;&#20570;&#27861;&#26159; "&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;"&#65292;&#21363;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#21644;&#27979;&#35797;&#25200;&#21160;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36229;&#36234;&#36825;&#31181;&#20570;&#27861;&#20197;&#21152;&#24378;&#23545;&#24615;&#21035;&#20445;&#25252;&#30340;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#27979;&#35797;&#22522;&#20110;&#35821;&#38899;&#31185;&#23398;&#23478;&#21382;&#21490;&#19978;&#24320;&#21457;&#30340;&#35821;&#38899;&#29305;&#24449;&#30340;&#24615;&#21035;&#25512;&#27979;&#25915;&#20987;&#30340;&#37325;&#35201;&#24615;&#65292;&#21516;&#26102;&#36824;&#19982;&#20256;&#32479;&#30340;&#31070;&#32463;&#20998;&#31867;&#22120;&#36827;&#34892;&#27604;&#36739;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35748;&#20026;&#30740;&#31350;&#20154;&#21592;&#24212;&#35813;&#20351;&#29992;&#35821;&#38899;&#29305;&#24449;&#26469;&#27934;&#23519;&#20445;&#25252;&#24615;&#20462;&#25913;&#22914;&#20309;&#25913;&#21464;&#35821;&#38899;&#20449;&#21495;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25351;&#20986;&#24615;&#21035;&#20445;&#25252;&#31639;&#27861;&#24212;&#35813;&#19982;&#26032;&#22411;&#30340; "&#35821;&#38899;&#23545;&#25163;"&#65292;&#21363;&#20154;&#31867;&#25191;&#34892;&#30340;&#22768;&#38899;&#36866;&#24212;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research has proposed approaches that modify speech to defend against gender inference attacks. The goal of these protection algorithms is to control the availability of information about a speaker's gender, a privacy-sensitive attribute. Currently, the common practice for developing and testing gender protection algorithms is "neural-on-neural", i.e., perturbations are generated and tested with a neural network. In this paper, we propose to go beyond this practice to strengthen the study of gender protection. First, we demonstrate the importance of testing gender inference attacks that are based on speech features historically developed by speech scientists, alongside the conventionally used neural classifiers. Next, we argue that researchers should use speech features to gain insight into how protective modifications change the speech signal. Finally, we point out that gender-protection algorithms should be compared with novel "vocal adversaries", human-executed voice adaptati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#26816;&#27979;&#23545;&#20154;&#26435;&#25421;&#21355;&#32773;&#30340;&#25915;&#20987;&#12290;&#21033;&#29992;NLP&#26469;&#22788;&#29702;&#22823;&#37327;&#26032;&#38395;&#25991;&#31456;&#65292;&#20197;&#26816;&#27979;&#21644;&#24635;&#32467;&#25915;&#20987;&#30340;&#29305;&#24449;&#12290;&#36890;&#36807;&#25552;&#20379;&#20855;&#26377;&#31934;&#32454;&#20449;&#24687;&#30340;&#20247;&#21253;&#27880;&#37322;&#65292;&#23637;&#31034;&#20102;&#25968;&#25454;&#38598;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.17695</link><description>&lt;p&gt;
&#22312;&#26816;&#27979;&#23545;&#20154;&#26435;&#25421;&#21355;&#32773;&#30340;&#25915;&#20987;&#26041;&#38754;&#30340;&#26032;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A New Task and Dataset on Detecting Attacks on Human Rights Defenders. (arXiv:2306.17695v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#26816;&#27979;&#23545;&#20154;&#26435;&#25421;&#21355;&#32773;&#30340;&#25915;&#20987;&#12290;&#21033;&#29992;NLP&#26469;&#22788;&#29702;&#22823;&#37327;&#26032;&#38395;&#25991;&#31456;&#65292;&#20197;&#26816;&#27979;&#21644;&#24635;&#32467;&#25915;&#20987;&#30340;&#29305;&#24449;&#12290;&#36890;&#36807;&#25552;&#20379;&#20855;&#26377;&#31934;&#32454;&#20449;&#24687;&#30340;&#20247;&#21253;&#27880;&#37322;&#65292;&#23637;&#31034;&#20102;&#25968;&#25454;&#38598;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#23545;&#20154;&#26435;&#25421;&#21355;&#32773;&#30340;&#25915;&#20987;&#36827;&#34892;&#21382;&#21490;&#24615;&#21644;&#22320;&#22495;&#24615;&#30340;&#22238;&#39038;&#24615;&#20998;&#26512;&#23545;&#20110;&#20154;&#36947;&#20027;&#20041;&#32452;&#32455;&#26356;&#22909;&#22320;&#20102;&#35299;&#21382;&#21490;&#25110;&#27491;&#22312;&#36827;&#34892;&#30340;&#20154;&#26435;&#20405;&#29359;&#65292;&#24182;&#22240;&#27492;&#26356;&#22909;&#22320;&#31649;&#29702;&#27492;&#31867;&#20107;&#20214;&#30340;&#20840;&#29699;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#20551;&#35774;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21487;&#20197;&#36890;&#36807;&#24555;&#36895;&#22788;&#29702;&#22823;&#37327;&#26032;&#38395;&#25991;&#31456;&#26469;&#26816;&#27979;&#21644;&#24635;&#32467;&#23545;&#20154;&#26435;&#25421;&#21355;&#32773;&#30340;&#25915;&#20987;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#25903;&#25345;&#36825;&#20123;&#21162;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#29992;&#20110;&#26816;&#27979;&#23545;&#20154;&#26435;&#25421;&#21355;&#32773;&#30340;&#25915;&#20987;&#30340;&#25968;&#25454;&#38598;&#65288;HRDsAttack&#65289;&#65292;&#21253;&#25324;&#23545;500&#31687;&#22312;&#32447;&#26032;&#38395;&#25991;&#31456;&#30340;&#20247;&#21253;&#27880;&#37322;&#12290;&#36825;&#20123;&#27880;&#37322;&#21253;&#25324;&#23545;&#25915;&#20987;&#31867;&#22411;&#21644;&#22320;&#28857;&#20197;&#21450;&#21463;&#23475;&#32773;&#20449;&#24687;&#30340;&#32454;&#31890;&#24230;&#20449;&#24687;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#35757;&#32451;&#21644;&#35780;&#20272;&#22522;&#20934;&#27169;&#22411;&#22312;&#20960;&#20010;&#23376;&#20219;&#21153;&#19978;&#39044;&#27979;&#27880;&#37322;&#29305;&#24449;&#30340;&#26041;&#27861;&#26469;&#23637;&#31034;&#25968;&#25454;&#38598;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to conduct retrospective analyses of attacks on human rights defenders over time and by location is important for humanitarian organizations to better understand historical or ongoing human rights violations and thus better manage the global impact of such events. We hypothesize that NLP can support such efforts by quickly processing large collections of news articles to detect and summarize the characteristics of attacks on human rights defenders. To that end, we propose a new dataset for detecting Attacks on Human Rights Defenders (HRDsAttack) consisting of crowdsourced annotations on 500 online news articles. The annotations include fine-grained information about the type and location of the attacks, as well as information about the victim(s). We demonstrate the usefulness of the dataset by using it to train and evaluate baseline models on several sub-tasks to predict the annotated characteristics.
&lt;/p&gt;</description></item><item><title>X-RiSAWOZ&#26159;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#22810;&#35821;&#35328;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102; &#32473;&#26500;&#24314;&#23436;&#20840;&#21151;&#33021;&#20195;&#29702;&#20154;&#30340;&#31471;&#21040;&#31471;&#25968;&#25454;&#38598;&#12290;&#24320;&#21457;&#20102;&#19968;&#22871;&#24037;&#20855;&#26469;&#21152;&#24555;&#32763;&#35793;&#21518;&#30340;&#26032;&#35821;&#35328;&#25968;&#25454;&#38598;&#30340;&#21518;&#26399;&#32534;&#36753;&#65292;&#25552;&#39640;&#20102;&#26426;&#22120;&#32763;&#35793;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.17674</link><description>&lt;p&gt;
X-RiSAWOZ: &#39640;&#36136;&#37327;&#30340;&#22810;&#35821;&#35328;&#23545;&#35805;&#25968;&#25454;&#38598;&#21644;&#23569;&#26679;&#26412;&#20195;&#29702;&#20154;
&lt;/p&gt;
&lt;p&gt;
X-RiSAWOZ: High-Quality End-to-End Multilingual Dialogue Datasets and Few-shot Agents. (arXiv:2306.17674v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17674
&lt;/p&gt;
&lt;p&gt;
X-RiSAWOZ&#26159;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#22810;&#35821;&#35328;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102; &#32473;&#26500;&#24314;&#23436;&#20840;&#21151;&#33021;&#20195;&#29702;&#20154;&#30340;&#31471;&#21040;&#31471;&#25968;&#25454;&#38598;&#12290;&#24320;&#21457;&#20102;&#19968;&#22871;&#24037;&#20855;&#26469;&#21152;&#24555;&#32763;&#35793;&#21518;&#30340;&#26032;&#35821;&#35328;&#25968;&#25454;&#38598;&#30340;&#21518;&#26399;&#32534;&#36753;&#65292;&#25552;&#39640;&#20102;&#26426;&#22120;&#32763;&#35793;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#33521;&#35821;&#21644;&#20013;&#25991;&#31561;&#20960;&#31181;&#27969;&#34892;&#35821;&#35328;&#19978;&#65292;&#36825;&#26159;&#22240;&#20026;&#20026;&#26032;&#35821;&#35328;&#21019;&#24314;&#25968;&#25454;&#38598;&#30340;&#25104;&#26412;&#36739;&#39640;&#12290;&#20026;&#20102;&#38477;&#20302;&#25104;&#26412;&#65292;&#25105;&#20204;&#23545;&#33258;&#21160;&#32763;&#35793;&#30340;&#25968;&#25454;&#36827;&#34892;&#25163;&#21160;&#32534;&#36753;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20013;&#25991;RiSAWOZ&#32763;&#35793;&#20026;&#33521;&#35821;&#12289;&#27861;&#35821;&#12289;&#21360;&#22320;&#35821;&#12289;&#38889;&#35821;&#20197;&#21450;&#28151;&#21512;&#33521;&#21360;&#22320;&#35821;&#30340;&#35821;&#35328;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#35821;&#35328;&#22522;&#20934;&#25968;&#25454;&#38598;X-RiSAWOZ&#12290;X-RiSAWOZ&#27599;&#31181;&#35821;&#35328;&#37117;&#26377;&#36229;&#36807;18,000&#20010;&#32463;&#20154;&#24037;&#39564;&#35777;&#30340;&#23545;&#35805;&#35821;&#21477;&#65292;&#24182;&#19988;&#19982;&#22823;&#22810;&#25968;&#22810;&#35821;&#35328;&#20043;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#23427;&#26159;&#19968;&#20010;&#29992;&#20110;&#26500;&#24314;&#23436;&#20840;&#21151;&#33021;&#20195;&#29702;&#20154;&#30340;&#31471;&#21040;&#31471;&#25968;&#25454;&#38598;&#12290;&#22312;&#21019;&#24314;X-RiSAWOZ&#26102;&#65292;&#25105;&#20204;&#36935;&#21040;&#20102;&#35768;&#22810;&#22256;&#38590;&#65292;&#22240;&#27492;&#24320;&#21457;&#20102;&#19968;&#22871;&#24037;&#20855;&#26469;&#21152;&#24555;&#32763;&#35793;&#21518;&#30340;&#26032;&#35821;&#35328;&#25968;&#25454;&#38598;&#30340;&#21518;&#26399;&#32534;&#36753;&#12290;&#36825;&#22871;&#24037;&#20855;&#20351;&#29992;&#28151;&#21512;&#23454;&#20307;&#23545;&#40784;&#25216;&#26415;&#65292;&#32467;&#21512;&#20102;&#31070;&#32463;&#32593;&#32476;&#21644;&#22522;&#20110;&#23383;&#20856;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#35768;&#22810;&#33258;&#21160;&#21270;&#21644;&#21322;&#33258;&#21160;&#21270;&#30340;&#39564;&#35777;&#26816;&#26597;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#26426;&#22120;&#32763;&#35793;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Task-oriented dialogue research has mainly focused on a few popular languages like English and Chinese, due to the high dataset creation cost for a new language. To reduce the cost, we apply manual editing to automatically translated data. We create a new multilingual benchmark, X-RiSAWOZ, by translating the Chinese RiSAWOZ to 4 languages: English, French, Hindi, Korean; and a code-mixed English-Hindi language. X-RiSAWOZ has more than 18,000 human-verified dialogue utterances for each language, and unlike most multilingual prior work, is an end-to-end dataset for building fully-functioning agents.  The many difficulties we encountered in creating X-RiSAWOZ led us to develop a toolset to accelerate the post-editing of a new language dataset after translation. This toolset improves machine translation with a hybrid entity alignment technique that combines neural with dictionary-based methods, along with many automated and semi-automated validation checks.  We establish strong baselines f
&lt;/p&gt;</description></item><item><title>&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#23545;&#29983;&#29289;&#21307;&#23398;&#26415;&#35821;&#30340;&#26631;&#35760;&#20998;&#21106;&#26041;&#24335;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#36825;&#23545;&#20110;&#25913;&#36827;&#19979;&#28216;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2306.17649</link><description>&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#23545;&#19981;&#29702;&#24819;&#30340;&#26631;&#35760;&#20998;&#21106;&#26041;&#24335;&#20855;&#26377;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Biomedical Language Models are Robust to Sub-optimal Tokenization. (arXiv:2306.17649v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17649
&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#23545;&#29983;&#29289;&#21307;&#23398;&#26415;&#35821;&#30340;&#26631;&#35760;&#20998;&#21106;&#26041;&#24335;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#36825;&#23545;&#20110;&#25913;&#36827;&#19979;&#28216;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#19968;&#33324;&#30340;&#33521;&#35821;&#30456;&#21453;&#65292;&#29983;&#29289;&#21307;&#23398;&#26415;&#35821;&#20013;&#30340;&#35768;&#22810;&#27010;&#24565;&#26159;&#30001;&#29983;&#29289;&#21307;&#23398;&#19987;&#19994;&#20154;&#21592;&#35774;&#35745;&#30340;&#65292;&#30446;&#30340;&#26159;&#35201;&#31934;&#30830;&#19988;&#31616;&#26126;&#12290;&#36890;&#24120;&#36890;&#36807;&#23558;&#26377;&#24847;&#20041;&#30340;&#29983;&#29289;&#21307;&#23398;&#35789;&#32032;&#36830;&#25509;&#36215;&#26469;&#21019;&#24314;&#26032;&#30340;&#35821;&#20041;&#21333;&#20301;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#20195;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411; (LMs) &#26159;&#20351;&#29992;&#20174;&#22823;&#35268;&#27169;&#29983;&#29289;&#21307;&#23398;&#35821;&#26009;&#24211;&#32479;&#35745;&#20013;&#23548;&#20986;&#30340;&#26631;&#20934;&#39046;&#22495;&#29305;&#23450;&#26631;&#35760;&#22120;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#65292;&#32780;&#27809;&#26377;&#26126;&#30830;&#21033;&#29992;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#30340;&#31896;&#38468;&#24615;&#29305;&#28857;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#21457;&#29616;&#26631;&#20934;&#36890;&#29992;&#39046;&#22495;&#21644;&#29983;&#29289;&#21307;&#23398;&#26631;&#35760;&#22120;&#22312;&#23558;&#29983;&#29289;&#21307;&#23398;&#26415;&#35821;&#20998;&#21106;&#25104;&#26377;&#24847;&#20041;&#30340;&#32452;&#25104;&#37096;&#20998;&#26041;&#38754;&#33021;&#21147;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20551;&#35774;&#20351;&#29992;&#19968;&#31181;&#26356;&#20934;&#30830;&#20998;&#21106;&#29983;&#29289;&#21307;&#23398;&#26415;&#35821;&#30340;&#26631;&#35760;&#22120;&#23558;&#20351;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#22312;&#19979;&#28216;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#25552;&#39640;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#28041;&#21450;&#29983;&#29289;&#21307;&#23398;&#26415;&#35821;&#30340;&#20219;&#21153;&#65292;&#22914;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035; (NER) &#21644;&#23454;&#20307;&#38142;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
As opposed to general English, many concepts in biomedical terminology have been designed in recent history by biomedical professionals with the goal of being precise and concise. This is often achieved by concatenating meaningful biomedical morphemes to create new semantic units. Nevertheless, most modern biomedical language models (LMs) are pre-trained using standard domain-specific tokenizers derived from large scale biomedical corpus statistics without explicitly leveraging the agglutinating nature of biomedical language. In this work, we first find that standard open-domain and biomedical tokenizers are largely unable to segment biomedical terms into meaningful components. Therefore, we hypothesize that using a tokenizer which segments biomedical terminology more accurately would enable biomedical LMs to improve their performance on downstream biomedical NLP tasks, especially ones which involve biomedical terms directly such as named entity recognition (NER) and entity linking. Su
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32806;&#21512;&#21644;&#35299;&#32806;&#30340;NL2SQL&#29983;&#25104;&#20219;&#21153;&#30340;&#29305;&#24449;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26174;&#24335;&#21644;&#38544;&#24335;&#30456;&#20851;&#29305;&#24449;&#34920;&#31034;&#30340;&#35299;&#32806;&#21644;&#32806;&#21512;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.17646</link><description>&lt;p&gt;
&#22522;&#20110;&#32806;&#21512;&#21644;&#35299;&#32806;&#30340;NL2SQL&#29983;&#25104;&#30340;&#29305;&#24449;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Feature Representation Learning for NL2SQL Generation Based on Coupling and Decoupling. (arXiv:2306.17646v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17646
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32806;&#21512;&#21644;&#35299;&#32806;&#30340;NL2SQL&#29983;&#25104;&#20219;&#21153;&#30340;&#29305;&#24449;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26174;&#24335;&#21644;&#38544;&#24335;&#30456;&#20851;&#29305;&#24449;&#34920;&#31034;&#30340;&#35299;&#32806;&#21644;&#32806;&#21512;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
NL2SQL&#20219;&#21153;&#28041;&#21450;&#23558;&#33258;&#28982;&#35821;&#35328;&#35821;&#21477;&#35299;&#26512;&#20026;SQL&#26597;&#35810;&#12290;&#30446;&#21069;&#22823;&#22810;&#25968;&#20808;&#36827;&#30340;&#26041;&#27861;&#23558;NL2SQL&#35270;&#20026;&#22635;&#27133;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#29305;&#24449;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#65292;&#20294;&#24573;&#35270;&#20102;SELECT&#21644;WHERE&#23376;&#21477;&#20043;&#38388;&#30340;&#26174;&#24335;&#30456;&#20851;&#29305;&#24449;&#20197;&#21450;&#21333;&#20010;&#23376;&#21477;&#20869;&#37096;&#30340;&#38544;&#24335;&#30456;&#20851;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Clause Feature Correlation Decoupling and Coupling (CFCDC)&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#29305;&#24449;&#34920;&#31034;&#35299;&#32806;&#26041;&#27861;&#22312;&#21442;&#25968;&#32423;&#21035;&#19978;&#23558;SELECT&#21644;WHERE&#23376;&#21477;&#20998;&#31163;&#24320;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#23398;&#20064;&#26550;&#26500;&#65292;&#20197;&#35299;&#32806;&#29305;&#23450;&#23376;&#21477;&#20013;&#19981;&#21516;SQL&#20219;&#21153;&#20043;&#38388;&#30340;&#38544;&#24335;&#30456;&#20851;&#29305;&#24449;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#29305;&#24449;&#34920;&#31034;&#32806;&#21512;&#27169;&#22359;&#65292;&#23558;&#35299;&#32806;&#30340;&#20219;&#21153;&#38598;&#25104;&#21040;SELECT&#21644;WHERE&#23376;&#21477;&#20013;&#65292;&#24182;&#39044;&#27979;&#26368;&#32456;&#30340;SQL&#26597;&#35810;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;CFCDC&#27169;&#22411;&#22312;WikiSQL&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The NL2SQL task involves parsing natural language statements into SQL queries. While most state-of-the-art methods treat NL2SQL as a slot-filling task and use feature representation learning techniques, they overlook explicit correlation features between the SELECT and WHERE clauses and implicit correlation features between sub-tasks within a single clause. To address this issue, we propose the Clause Feature Correlation Decoupling and Coupling (CFCDC) model, which uses a feature representation decoupling method to separate the SELECT and WHERE clauses at the parameter level. Next, we introduce a multi-task learning architecture to decouple implicit correlation feature representation between different SQL tasks in a specific clause. Moreover, we present an improved feature representation coupling module to integrate the decoupled tasks in the SELECT and WHERE clauses and predict the final SQL query. Our proposed CFCDC model demonstrates excellent performance on the WikiSQL dataset, wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;ChatGPT&#36827;&#34892;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#23454;&#39564;&#30740;&#31350;&#65292;&#36890;&#36807;&#35774;&#35745;&#21407;&#21017;&#21644;&#20989;&#25968;&#24211;&#30340;&#32467;&#21512;&#65292;ChatGPT&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.17582</link><description>&lt;p&gt;
ChatGPT&#29992;&#20110;&#26426;&#22120;&#20154;&#25216;&#26415;&#65306;&#35774;&#35745;&#21407;&#21017;&#21644;&#27169;&#22411;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
ChatGPT for Robotics: Design Principles and Model Abilities. (arXiv:2306.17582v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;ChatGPT&#36827;&#34892;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#23454;&#39564;&#30740;&#31350;&#65292;&#36890;&#36807;&#35774;&#35745;&#21407;&#21017;&#21644;&#20989;&#25968;&#24211;&#30340;&#32467;&#21512;&#65292;ChatGPT&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;OpenAI&#30340;ChatGPT&#36827;&#34892;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#23454;&#39564;&#30740;&#31350;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#19968;&#31181;&#31574;&#30053;&#65292;&#23558;&#25552;&#31034;&#24037;&#31243;&#30340;&#35774;&#35745;&#21407;&#21017;&#19982;&#39640;&#32423;&#20989;&#25968;&#24211;&#30340;&#21019;&#24314;&#30456;&#32467;&#21512;&#65292;&#20351;ChatGPT&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#12289;&#27169;&#25311;&#22120;&#21644;&#24418;&#24577;&#12290;&#25105;&#20204;&#37325;&#28857;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#21644;&#23545;&#35805;&#31574;&#30053;&#23545;&#25191;&#34892;&#21508;&#31181;&#31867;&#22411;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;ChatGPT&#20351;&#29992;&#33258;&#30001;&#24418;&#24335;&#23545;&#35805;&#12289;&#35299;&#26512;XML&#26631;&#35760;&#21644;&#21512;&#25104;&#20195;&#30721;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#20351;&#29992;&#20219;&#21153;&#29305;&#23450;&#25552;&#31034;&#20989;&#25968;&#21644;&#36890;&#36807;&#23545;&#35805;&#36827;&#34892;&#38381;&#29615;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#28085;&#30422;&#20102;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#19968;&#31995;&#21015;&#20219;&#21153;&#65292;&#20174;&#22522;&#26412;&#30340;&#36923;&#36753;&#12289;&#20960;&#20309;&#21644;&#25968;&#23398;&#25512;&#29702;&#21040;&#22797;&#26434;&#30340;&#39046;&#22495;&#65292;&#22914;&#31354;&#20013;&#23548;&#33322;&#12289;&#25805;&#32437;&#21644;&#20855;&#36523;&#20195;&#29702;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;ChatGPT&#22312;&#35299;&#20915;&#36825;&#20123;&#20219;&#21153;&#26041;&#38754;&#21487;&#20197;&#21462;&#24471;&#26377;&#25928;&#32467;&#26524;&#65292;&#21516;&#26102;&#20351;&#25105;&#20204;&#33021;&#22815;&#36827;&#34892;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an experimental study regarding the use of OpenAI's ChatGPT for robotics applications. We outline a strategy that combines design principles for prompt engineering and the creation of a high-level function library which allows ChatGPT to adapt to different robotics tasks, simulators, and form factors. We focus our evaluations on the effectiveness of different prompt engineering techniques and dialog strategies towards the execution of various types of robotics tasks. We explore ChatGPT's ability to use free-form dialog, parse XML tags, and to synthesize code, in addition to the use of task-specific prompting functions and closed-loop reasoning through dialogues. Our study encompasses a range of tasks within the robotics domain, from basic logical, geometrical, and mathematical reasoning all the way to complex domains such as aerial navigation, manipulation, and embodied agents. We show that ChatGPT can be effective at solving several of such tasks, while allowing us
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23454;&#35777;&#35780;&#20272;&#21457;&#29616;&#65292;&#22312;&#22823;&#23398;&#24405;&#21462;&#36807;&#31243;&#20013;&#25490;&#38500;&#21463;&#20445;&#25252;&#23646;&#24615;&#20250;&#23548;&#33268;&#39044;&#27979;&#34920;&#29616;&#19979;&#38477;&#65292;&#32780;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#20449;&#24687;&#21487;&#20197;&#37096;&#20998;&#24674;&#22797;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.17575</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22686;&#24378;&#22823;&#23398;&#24405;&#21462;&#20013;&#30340;&#25972;&#20307;&#35780;&#20272;&#65292;&#20197;&#20998;&#26512;&#35770;&#25991;&#21644;&#25512;&#33616;&#20449;
&lt;/p&gt;
&lt;p&gt;
Augmenting Holistic Review in University Admission using Natural Language Processing for Essays and Recommendation Letters. (arXiv:2306.17575v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17575
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23454;&#35777;&#35780;&#20272;&#21457;&#29616;&#65292;&#22312;&#22823;&#23398;&#24405;&#21462;&#36807;&#31243;&#20013;&#25490;&#38500;&#21463;&#20445;&#25252;&#23646;&#24615;&#20250;&#23548;&#33268;&#39044;&#27979;&#34920;&#29616;&#19979;&#38477;&#65292;&#32780;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#20449;&#24687;&#21487;&#20197;&#37096;&#20998;&#24674;&#22797;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#39640;&#24230;&#36873;&#25321;&#24615;&#30340;&#26426;&#26500;&#20013;&#65292;&#22823;&#23398;&#24405;&#21462;&#37319;&#29992;&#20840;&#38754;&#35780;&#20272;&#36807;&#31243;&#65292;&#32771;&#34385;&#30003;&#35831;&#30340;&#25152;&#26377;&#26041;&#38754;&#65292;&#21253;&#25324;&#38544;&#31169;&#23646;&#24615;&#65288;&#22914;&#31181;&#26063;&#12289;&#24615;&#21035;&#65289;&#12289;&#25104;&#32489;&#12289;&#35770;&#25991;&#21644;&#25512;&#33616;&#20449;&#65292;&#20197;&#32452;&#25104;&#19968;&#25903;&#20248;&#31168;&#21644;&#22810;&#26679;&#21270;&#30340;&#29677;&#32423;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#23454;&#35777;&#35780;&#20272;&#21463;&#20445;&#25252;&#23646;&#24615;&#23545;&#39044;&#27979;&#24405;&#21462;&#20915;&#31574;&#30340;&#24433;&#21709;&#65292;&#24182;&#25506;&#35752;&#25991;&#26412;&#20449;&#24687;&#65288;&#22914;&#20010;&#20154;&#35770;&#25991;&#12289;&#25945;&#24072;&#25512;&#33616;&#20449;&#65289;&#22312;&#27169;&#22411;&#20013;&#20195;&#26367;&#21463;&#20445;&#25252;&#23646;&#24615;&#30340;&#31243;&#24230;&#12290;&#36890;&#36807;&#20351;&#29992;2022-2023&#23398;&#24180;&#22312;&#19968;&#25152;&#20855;&#26377;&#36873;&#25321;&#24615;&#30340;&#32654;&#22269;&#26412;&#31185;&#20837;&#23398;&#21150;&#20844;&#23460;&#30340;14,915&#21517;&#30003;&#35831;&#20154;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#21457;&#29616;&#20174;ML&#27169;&#22411;&#20013;&#25490;&#38500;&#21463;&#20445;&#25252;&#23646;&#24615;&#20250;&#26174;&#33879;&#38477;&#20302;&#39044;&#27979;&#24405;&#21462;&#34920;&#29616;&#12290;&#36890;&#36807;TF-IDF&#34920;&#31034;&#21644;&#38544;&#29380;&#21033;&#20811;&#38647;&#20998;&#37197;&#65288;LDA&#65289;&#27169;&#22411;&#65292;&#25991;&#26412;&#20449;&#24687;&#30340;&#21253;&#21547;&#37096;&#20998;&#24674;&#22797;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
University admission at many highly selective institutions uses a holistic review process, where all aspects of the application, including protected attributes (e.g., race, gender), grades, essays, and recommendation letters are considered, to compose an excellent and diverse class. In this study, we empirically evaluate how influential protected attributes are for predicting admission decisions using a machine learning (ML) model, and in how far textual information (e.g., personal essay, teacher recommendation) may substitute for the loss of protected attributes in the model. Using data from 14,915 applicants to an undergraduate admission office at a selective U.S. institution in the 2022-2023 cycle, we find that the exclusion of protected attributes from the ML model leads to substantially reduced admission-prediction performance. The inclusion of textual information via both a TF-IDF representation and a Latent Dirichlet allocation (LDA) model partially restores model performance, b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#22810;&#31181;&#25991;&#26412;&#34920;&#31034;&#26041;&#27861;&#21644;&#20027;&#39064;&#12289;&#24773;&#24863;&#20449;&#24687;&#30340;&#24773;&#26223;&#21270;&#27169;&#24335;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#25233;&#37057;&#12290;&#36890;&#36807;&#35780;&#20272;&#23454;&#39564;&#21457;&#29616;&#65292;&#35813;&#26041;&#27861;&#22312;&#25913;&#21892;&#20998;&#31867;&#25928;&#26524;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2306.17564</link><description>&lt;p&gt;
&#20351;&#29992;&#20027;&#39064;&#21644;&#24773;&#24863;&#19978;&#19979;&#25991;&#21270;&#23545;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#25233;&#37057;&#35821;&#35328;&#36827;&#34892;&#25104;&#26412;&#24863;&#30693;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Cost-aware Study of Depression Language on Social Media using Topic and Affect Contextualization. (arXiv:2306.17564v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#22810;&#31181;&#25991;&#26412;&#34920;&#31034;&#26041;&#27861;&#21644;&#20027;&#39064;&#12289;&#24773;&#24863;&#20449;&#24687;&#30340;&#24773;&#26223;&#21270;&#27169;&#24335;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#25233;&#37057;&#12290;&#36890;&#36807;&#35780;&#20272;&#23454;&#39564;&#21457;&#29616;&#65292;&#35813;&#26041;&#27861;&#22312;&#25913;&#21892;&#20998;&#31867;&#25928;&#26524;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25233;&#37057;&#26159;&#31038;&#20250;&#24515;&#29702;&#20581;&#24247;&#20013;&#26085;&#30410;&#20005;&#37325;&#30340;&#38382;&#39064;&#65292;&#24433;&#21709;&#29983;&#27963;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#29978;&#33267;&#21487;&#33021;&#23548;&#33268;&#33258;&#26432;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#39044;&#38450;&#35745;&#21010;&#21487;&#20197;&#22312;&#25233;&#37057;&#30340;&#27835;&#30103;&#20013;&#36215;&#21040;&#20316;&#29992;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#30340;&#31038;&#20132;&#23186;&#20307;&#25233;&#37057;&#26816;&#27979;&#33258;&#21160;&#31995;&#32479;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20197;&#19979;&#36129;&#29486;&#65306;&#65288;i&#65289;&#19968;&#20010;&#23558;&#22810;&#31181;&#25991;&#26412;&#34920;&#31034;&#26041;&#24335;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#25233;&#37057;&#26816;&#27979;&#30340;&#38598;&#25104;&#23398;&#20064;&#31995;&#32479;&#65292;&#21253;&#25324;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65307;&#65288;ii&#65289;&#36890;&#36807;&#20027;&#39064;&#21644;&#24773;&#24863;&#20449;&#24687;&#30340;&#24773;&#26223;&#21270;&#27169;&#24335;&#65307;&#65288;iii&#65289;&#20998;&#26512;&#27169;&#22411;&#30340;&#33021;&#32791;&#65292;&#24314;&#31435;&#20998;&#31867;&#24615;&#33021;&#21644;&#24635;&#20307;&#35745;&#31639;&#25104;&#26412;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#20026;&#20102;&#35780;&#20272;&#25152;&#25552;&#20986;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#23545;&#20004;&#20010;&#27169;&#25311;&#25233;&#37057;&#25991;&#26412;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#24773;&#26223;&#21270;&#31574;&#30053;&#21487;&#20197;&#25913;&#21892;&#20998;&#31867;&#25928;&#26524;&#65292;&#24182;&#19988;&#24212;&#29992;&#20110;&#33258;&#24049;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#26816;&#27979;&#25233;&#37057;&#39118;&#38505;&#26041;&#38754;&#20248;&#20110;&#30446;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Depression is a growing issue in society's mental health that affects all areas of life and can even lead to suicide. Fortunately, prevention programs can be effective in its treatment. In this context, this work proposes an automatic system for detecting depression on social media based on machine learning and natural language processing methods. This paper presents the following contributions: (i) an ensemble learning system that combines several types of text representations for depression detection, including recent advances in the field; (ii) a contextualization schema through topic and affective information; (iii) an analysis of models' energy consumption, establishing a trade-off between classification performance and overall computational costs. To assess the proposed models' effectiveness, a thorough evaluation is performed in two datasets that model depressive text. Experiments indicate that the proposed contextualization strategies can improve the classification and that app
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PRP&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#29992;&#20004;&#20004;&#25490;&#21517;&#25552;&#31034;&#26469;&#26174;&#33879;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36127;&#25285;&#65292;&#24182;&#39318;&#27425;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25490;&#21517;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.17563</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#26377;&#25928;&#30340;&#25991;&#26412;&#25490;&#24207;&#22120;&#65292;&#20855;&#26377;&#20004;&#20004;&#25490;&#21517;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting. (arXiv:2306.17563v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PRP&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#29992;&#20004;&#20004;&#25490;&#21517;&#25552;&#31034;&#26469;&#26174;&#33879;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36127;&#25285;&#65292;&#24182;&#39318;&#27425;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25490;&#21517;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#30452;&#25509;&#23558;&#26597;&#35810;&#21644;&#20505;&#36873;&#25991;&#26723;&#36755;&#20837;&#25552;&#31034;&#36827;&#34892;&#25991;&#26723;&#25490;&#24207;&#26159;&#19968;&#20010;&#26377;&#36259;&#19988;&#23454;&#29992;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#21462;&#24471;&#20102;&#26377;&#38480;&#30340;&#25104;&#21151;&#65292;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#24456;&#38590;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36229;&#36234;&#31934;&#35843;&#22522;&#20934;&#25490;&#24207;&#22120;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#30340;&#28857;&#23545;&#28857;&#21644;&#21015;&#34920;&#25490;&#24207;&#25552;&#31034;&#65292;&#24182;&#35748;&#20026;&#29616;&#25104;&#30340;LLM&#27809;&#26377;&#23436;&#20840;&#29702;&#35299;&#36825;&#20123;&#25490;&#24207;&#20844;&#24335;&#65292;&#21487;&#33021;&#26159;&#30001;&#20110;LLM&#30340;&#35757;&#32451;&#26041;&#24335;&#30340;&#29305;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20004;&#20004;&#25490;&#21517;&#25552;&#31034;&#65288;PRP&#65289;&#30340;&#26032;&#25216;&#26415;&#65292;&#22823;&#22823;&#20943;&#36731;&#20102;LLM&#30340;&#36127;&#25285;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26159;&#25991;&#29486;&#20013;&#39318;&#27425;&#20351;&#29992;&#20013;&#31561;&#35268;&#27169;&#30340;&#24320;&#28304;LLM&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25490;&#21517;&#24615;&#33021;&#12290;&#22312;TREC-DL2020&#19978;&#65292;&#22522;&#20110;20B&#21442;&#25968;&#30340;Flan-UL2&#27169;&#22411;&#30340;PRP&#36229;&#36807;&#20102;&#25991;&#29486;&#20013;&#22522;&#20110;&#21830;&#19994;&#40657;&#30418;GPT-4&#30340;&#26368;&#20339;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ranking documents using Large Language Models (LLMs) by directly feeding the query and candidate documents into the prompt is an interesting and practical problem. However, there has been limited success so far, as researchers have found it difficult to outperform fine-tuned baseline rankers on benchmark datasets. We analyze pointwise and listwise ranking prompts used by existing methods and argue that off-the-shelf LLMs do not fully understand these ranking formulations, possibly due to the nature of how LLMs are trained. In this paper, we propose to significantly reduce the burden on LLMs by using a new technique called Pairwise Ranking Prompting (PRP). Our results are the first in the literature to achieve state-of-the-art ranking performance on standard benchmarks using moderate-sized open-sourced LLMs. On TREC-DL2020, PRP based on the Flan-UL2 model with 20B parameters outperforms the previous best approach in the literature, which is based on the blackbox commercial GPT-4 that ha
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#23454;&#29616;&#23545;&#20302;&#36164;&#28304;&#25163;&#21183;&#35821;&#35328;&#35782;&#21035;&#30340;&#31283;&#20581;&#25163;&#21183;&#23884;&#20837;&#25552;&#21462;&#12290;&#38024;&#23545;&#24403;&#21069;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#22120;&#34429;&#28982;&#26159;&#29702;&#24819;&#30340;&#36873;&#25321;&#65292;&#20294;&#30001;&#20110;&#22495;&#19981;&#21305;&#37197;&#21644;&#25163;&#21183;&#35821;&#35328;&#20013;&#30340;&#25361;&#25112;&#24615;&#23039;&#21183;&#65292;&#20854;&#22312;&#25163;&#21183;&#35821;&#35328;&#25968;&#25454;&#19978;&#30340;&#31283;&#20581;&#24615;&#26377;&#25152;&#27424;&#32570;&#12290;&#20851;&#38190;&#28857;&#22522;&#20110;&#30340;&#27169;&#22411;&#20173;&#28982;&#20248;&#20110;&#22270;&#20687;&#22522;&#20110;&#30340;&#27169;&#22411;&#65292;&#20294;&#20854;&#35757;&#32451;&#26041;&#24335;&#38480;&#21046;&#20102;&#20854;&#22312;&#25163;&#21183;&#35821;&#35328;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.17558</link><description>&lt;p&gt;
&#23454;&#29616;&#23545;&#20302;&#36164;&#28304;&#25163;&#21183;&#35821;&#35328;&#35782;&#21035;&#30340;&#31283;&#20581;&#25163;&#21183;&#23884;&#20837;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Towards the extraction of robust sign embeddings for low resource sign language recognition. (arXiv:2306.17558v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#23454;&#29616;&#23545;&#20302;&#36164;&#28304;&#25163;&#21183;&#35821;&#35328;&#35782;&#21035;&#30340;&#31283;&#20581;&#25163;&#21183;&#23884;&#20837;&#25552;&#21462;&#12290;&#38024;&#23545;&#24403;&#21069;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#22120;&#34429;&#28982;&#26159;&#29702;&#24819;&#30340;&#36873;&#25321;&#65292;&#20294;&#30001;&#20110;&#22495;&#19981;&#21305;&#37197;&#21644;&#25163;&#21183;&#35821;&#35328;&#20013;&#30340;&#25361;&#25112;&#24615;&#23039;&#21183;&#65292;&#20854;&#22312;&#25163;&#21183;&#35821;&#35328;&#25968;&#25454;&#19978;&#30340;&#31283;&#20581;&#24615;&#26377;&#25152;&#27424;&#32570;&#12290;&#20851;&#38190;&#28857;&#22522;&#20110;&#30340;&#27169;&#22411;&#20173;&#28982;&#20248;&#20110;&#22270;&#20687;&#22522;&#20110;&#30340;&#27169;&#22411;&#65292;&#20294;&#20854;&#35757;&#32451;&#26041;&#24335;&#38480;&#21046;&#20102;&#20854;&#22312;&#25163;&#21183;&#35821;&#35328;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23396;&#31435;&#30340;&#25163;&#21183;&#35821;&#35328;&#35782;&#21035;&#36890;&#24120;&#24212;&#29992;&#20110;&#21253;&#21547;&#30001;&#19968;&#32452;&#26377;&#38480;&#25163;&#21183;&#25191;&#34892;&#32773;&#32531;&#24930;&#32780;&#28165;&#26224;&#25191;&#34892;&#30340;&#30456;&#23545;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#20013;&#65292;&#25105;&#20204;&#38754;&#20020;&#30528;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35270;&#35273;&#26465;&#20214;&#12289;&#20849;&#21516;&#21457;&#38899;&#30340;&#25163;&#21183;&#12289;&#23567;&#25968;&#25454;&#38598;&#20197;&#21450;&#23545;&#29420;&#31435;&#28436;&#35762;&#32773;&#27169;&#22411;&#30340;&#38656;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#38656;&#35201;&#19968;&#20010;&#31283;&#20581;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#26469;&#22788;&#29702;&#25163;&#21183;&#35821;&#35328;&#35270;&#39057;&#12290;&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#22120;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#29702;&#24819;&#30340;&#20505;&#36873;&#32773;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#35757;&#32451;&#38598;&#19982;&#25163;&#21183;&#35821;&#35328;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23039;&#21183;&#20043;&#38388;&#23384;&#22312;&#39046;&#22495;&#19981;&#21305;&#37197;&#65292;&#23427;&#20204;&#22312;&#25163;&#21183;&#35821;&#35328;&#25968;&#25454;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#27169;&#22411;&#19978;&#20173;&#28982;&#32570;&#20047;&#31283;&#20581;&#24615;&#65292;&#20851;&#38190;&#28857;&#22522;&#20110;&#30340;&#27169;&#22411;&#36890;&#24120;&#20173;&#28982;&#20248;&#20110;&#22522;&#20110;&#22270;&#20687;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#19982;&#22522;&#20110;&#22270;&#20687;&#30340;&#27169;&#22411;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#30340;&#24120;&#35265;&#23454;&#36341;&#21487;&#20197;&#33719;&#24471;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#20851;&#38190;&#28857;&#22522;&#20110;&#30340;&#27169;&#22411;&#36890;&#24120;&#22312;&#27599;&#20010;&#25163;&#21183;&#35821;&#35328;&#35782;&#21035;&#25968;&#25454;&#38598;&#19978;&#37117;&#26159;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30340;&#12290;&#36825;&#20123;&#22240;&#32032;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#25163;&#21183;&#35821;&#35328;&#35782;&#21035;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Isolated Sign Language Recognition (SLR) has mostly been applied on relatively large datasets containing signs executed slowly and clearly by a limited group of signers. In real-world scenarios, however, we are met with challenging visual conditions, coarticulated signing, small datasets, and the need for signer independent models. To tackle this difficult problem, we require a robust feature extractor to process the sign language videos. One could expect human pose estimators to be ideal candidates. However, due to a domain mismatch with their training sets and challenging poses in sign language, they lack robustness on sign language data and image based models often still outperform keypoint based models. Furthermore, whereas the common practice of transfer learning with image based models yields even higher accuracy, keypoint based models are typically trained from scratch on every SLR dataset. These factors limit their usefulness for SLR. From the existing literature, it is also no
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#26694;&#26550;&#36827;&#34892;&#37329;&#34701;&#20851;&#31995;&#25552;&#21462;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#20004;&#31181;&#26816;&#32034;&#31574;&#30053;&#65292;&#26080;&#38656;&#23398;&#20064;&#30340;&#23494;&#38598;&#26816;&#32034;&#22120;&#21644;&#22522;&#20110;&#23398;&#20064;&#30340;&#26816;&#32034;&#22120;&#65292;&#25105;&#20204;&#33021;&#22815;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#25214;&#21040;&#19982;&#32473;&#23450;&#27979;&#35797;&#31034;&#20363;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#31034;&#33539;&#12290;</title><link>http://arxiv.org/abs/2306.17519</link><description>&lt;p&gt;
GPT-FinRE: &#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#37329;&#34701;&#20851;&#31995;&#25552;&#21462;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
GPT-FinRE: In-context Learning for Financial Relation Extraction using Large Language Models. (arXiv:2306.17519v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#26694;&#26550;&#36827;&#34892;&#37329;&#34701;&#20851;&#31995;&#25552;&#21462;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#20004;&#31181;&#26816;&#32034;&#31574;&#30053;&#65292;&#26080;&#38656;&#23398;&#20064;&#30340;&#23494;&#38598;&#26816;&#32034;&#22120;&#21644;&#22522;&#20110;&#23398;&#20064;&#30340;&#26816;&#32034;&#22120;&#65292;&#25105;&#20204;&#33021;&#22815;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#25214;&#21040;&#19982;&#32473;&#23450;&#27979;&#35797;&#31034;&#20363;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#31034;&#33539;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25552;&#21462;&#65288;RE&#65289;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#26088;&#22312;&#35782;&#21035;&#21644;&#20998;&#31867;&#25991;&#26412;&#20013;&#25552;&#21450;&#30340;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22312;&#37329;&#34701;&#39046;&#22495;&#65292;&#20851;&#31995;&#25552;&#21462;&#22312;&#20174;&#36130;&#32463;&#25991;&#20214;&#65288;&#22914;&#26032;&#38395;&#25991;&#31456;&#12289;&#30408;&#21033;&#25253;&#21578;&#21644;&#20844;&#21496;&#30003;&#25253;&#65289;&#20013;&#25552;&#21462;&#26377;&#20215;&#20540;&#20449;&#24687;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#22312;&#19968;&#20010;&#21517;&#20026;REFinD&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20851;&#31995;&#25552;&#21462;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#20316;&#20026;SIGIR 2023&#20030;&#21150;&#30340;&#31532;&#22235;&#23626;&#20174;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#21457;&#29616;&#30693;&#35782;&#30340;&#30740;&#35752;&#20250;&#30340;&#20849;&#20139;&#20219;&#21153;&#30340;&#19968;&#37096;&#20998;&#21457;&#24067;&#30340;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#37319;&#29992;&#20102;OpenAI&#27169;&#22411;&#65292;&#24182;&#24212;&#29992;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#26694;&#26550;&#12290;&#25105;&#20204;&#21033;&#29992;&#20004;&#31181;&#26816;&#32034;&#31574;&#30053;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#25214;&#20986;&#19982;&#32473;&#23450;&#27979;&#35797;&#31034;&#20363;&#30456;&#20851;&#30340;&#21069;K&#20010;&#19978;&#19979;&#25991;&#23398;&#20064;&#31034;&#33539;/&#31034;&#20363;&#12290;&#31532;&#19968;&#20010;&#26816;&#32034;&#26426;&#21046;&#26159;&#26080;&#38656;&#23398;&#20064;&#30340;&#23494;&#38598;&#26816;&#32034;&#22120;&#65292;&#32780;&#21478;&#19968;&#20010;&#31995;&#32479;&#26159;&#22522;&#20110;&#23398;&#20064;&#30340;&#26816;&#32034;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation extraction (RE) is a crucial task in natural language processing (NLP) that aims to identify and classify relationships between entities mentioned in text. In the financial domain, relation extraction plays a vital role in extracting valuable information from financial documents, such as news articles, earnings reports, and company filings. This paper describes our solution to relation extraction on one such dataset REFinD. The dataset was released along with shared task as a part of the Fourth Workshop on Knowledge Discovery from Unstructured Data in Financial Services, co-located with SIGIR 2023. In this paper, we employed OpenAI models under the framework of in-context learning (ICL). We utilized two retrieval strategies to find top K relevant in-context learning demonstrations / examples from training data for a given test example. The first retrieval mechanism, we employed, is a learning-free dense retriever and the other system is a learning-based retriever. We were able
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Preference Ranking Optimization (PRO)&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#27604;&#36739;&#65292;&#37319;&#29992;&#20559;&#22909;&#25490;&#24207;&#30340;&#26041;&#24335;&#26469;&#30452;&#25509;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#35299;&#20915;&#20102;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#22797;&#26434;&#24615;&#12289;&#19981;&#31283;&#23450;&#24615;&#21644;&#23545;&#36229;&#21442;&#25968;&#30340;&#25935;&#24863;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.17492</link><description>&lt;p&gt;
&#20154;&#31867;&#23545;&#40784;&#30340;&#20559;&#22909;&#25490;&#24207;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Preference Ranking Optimization for Human Alignment. (arXiv:2306.17492v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Preference Ranking Optimization (PRO)&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#27604;&#36739;&#65292;&#37319;&#29992;&#20559;&#22909;&#25490;&#24207;&#30340;&#26041;&#24335;&#26469;&#30452;&#25509;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#35299;&#20915;&#20102;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#22797;&#26434;&#24615;&#12289;&#19981;&#31283;&#23450;&#24615;&#21644;&#23545;&#36229;&#21442;&#25968;&#30340;&#25935;&#24863;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#32463;&#24120;&#21253;&#21547;&#35823;&#23548;&#24615;&#20869;&#23481;&#65292;&#24378;&#35843;&#20102;&#23558;&#20854;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#20197;&#30830;&#20445;&#23433;&#20840;&#30340;AI&#31995;&#32479;&#30340;&#24517;&#35201;&#24615;&#12290;&#37319;&#29992;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26469;&#23454;&#29616;&#36825;&#31181;&#23545;&#40784;&#65292;&#36890;&#36807;&#23558;&#22522;&#20110;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#37197;&#23545;&#27604;&#36739;&#30340;&#22870;&#21169;&#27169;&#22411;&#19982;Proximal Policy Optimization&#65288;PPO&#65289;&#31561;RL&#31639;&#27861;&#32467;&#21512;&#36215;&#26469;&#26469;&#20248;&#21270;LLM&#30340;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;RLHF&#34920;&#29616;&#20986;&#22797;&#26434;&#24615;&#12289;&#19981;&#31283;&#23450;&#24615;&#21644;&#23545;&#36229;&#21442;&#25968;&#30340;&#25935;&#24863;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Preference Ranking Optimization&#65288;PRO&#65289;&#20316;&#20026;PPO&#30340;&#21478;&#19968;&#31181;&#30452;&#25509;&#23558;LLM&#19982;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#27604;&#36739;&#23545;&#40784;&#30340;&#26041;&#27861;&#12290;PRO&#23558;&#37197;&#23545;&#30340;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#27604;&#36739;&#25193;&#23637;&#21040;&#36866;&#24212;&#20219;&#24847;&#38271;&#24230;&#30340;&#20559;&#22909;&#25490;&#24207;&#12290;&#36890;&#36807;&#21453;&#22797;&#23545;&#27604;&#29983;&#25104;&#21709;&#24212;&#30340;&#21487;&#33021;&#24615;&#65292;PRO&#25351;&#23548;LLM&#20248;&#20808;&#32771;&#34385;&#26368;&#20339;&#21709;&#24212;&#65292;&#24182;&#36880;&#28176;&#23545;&#21097;&#20313;&#30340;&#21709;&#24212;&#36827;&#34892;&#25490;&#24207;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;PRO&#23558;&#20154;&#31867;&#23545;&#40784;&#26377;&#25928;&#22320;&#36716;&#21270;&#20026;&#27010;&#29575;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) often contain misleading content, emphasizing the need to align them with human values to ensure secur AI systems. Reinforcement learning from human feedback (RLHF) has been employed to achieve this alignment by combining a reward model, typically based on Bradley-Terry paired comparison, with an RL algorithm such as Proximal Policy Optimization (PPO) to optimize LLM responses. However, RLHF exhibits complexity, instability, and sensitivity to hyperparameters. In this paper, we propose Preference Ranking Optimization (PRO) as an alternative to PPO for directly aligning LLMs with the Bradley-Terry comparison. PRO extends the pairwise Bradley-Terry comparison to accommodate preference rankings of any length. By iteratively contrasting the likelihood of generating responses, PRO instructs the LLM to prioritize the best response while progressively ranking the remaining responses. In this manner, PRO effectively transforms human alignment into aligning the prob
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38271;&#23614;&#23454;&#20307;&#20107;&#23454;&#30340;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#24211;&#23436;&#21892;&#26041;&#27861;&#65292;&#22312;F1&#24471;&#20998;&#19978;&#36229;&#36807;&#20102;&#25152;&#26377;&#30340;&#22522;&#20934;&#32447;&#65292;&#23588;&#20854;&#22312;&#21484;&#22238;&#29575;&#19978;&#21462;&#24471;&#20102;&#37325;&#22823;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.17472</link><description>&lt;p&gt;
&#38754;&#21521;&#38271;&#23614;&#23454;&#20307;&#30340;&#30693;&#35782;&#24211;&#23436;&#21892;
&lt;/p&gt;
&lt;p&gt;
Knowledge Base Completion for Long-Tail Entities. (arXiv:2306.17472v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38271;&#23614;&#23454;&#20307;&#20107;&#23454;&#30340;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#24211;&#23436;&#21892;&#26041;&#27861;&#65292;&#22312;F1&#24471;&#20998;&#19978;&#36229;&#36807;&#20102;&#25152;&#26377;&#30340;&#22522;&#20934;&#32447;&#65292;&#23588;&#20854;&#22312;&#21484;&#22238;&#29575;&#19978;&#21462;&#24471;&#20102;&#37325;&#22823;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#30693;&#35782;&#24211;&#65288;KB&#65289;&#22914;Wikidata&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#35268;&#27169;&#65292;&#20294;&#20173;&#23384;&#22312;&#37325;&#22823;&#31354;&#32570;&#12290;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#34987;&#25552;&#20986;&#20316;&#20026;&#22635;&#34917;&#36825;&#20123;&#31354;&#32570;&#30340;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20855;&#26377;&#20016;&#23500;LM&#35206;&#30422;&#30340;&#31361;&#20986;&#23454;&#20307;&#65292;&#24573;&#35270;&#20102;&#20851;&#38190;&#30340;&#38271;&#23614;&#23454;&#20307;&#26696;&#20363;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38271;&#23614;&#23454;&#20307;&#20107;&#23454;&#30340;&#22522;&#20110;LM&#30340;KB&#23436;&#21892;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#38454;&#27573;&#21033;&#29992;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;LM&#65306;&#29992;&#20110;&#20505;&#36873;&#26816;&#32034;&#21644;&#29992;&#20110;&#20505;&#36873;&#39564;&#35777;&#21644;&#28040;&#27495;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#21644;&#19981;&#21516;&#30340;&#22522;&#32447;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;MALT&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#20854;&#26681;&#28304;&#20110;Wikidata&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;F1&#24471;&#20998;&#19978;&#36229;&#36807;&#20102;&#25152;&#26377;&#30340;&#22522;&#20934;&#32447;&#65292;&#23588;&#20854;&#22312;&#21484;&#22238;&#29575;&#19978;&#21462;&#24471;&#20102;&#37325;&#22823;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their impressive scale, knowledge bases (KBs), such as Wikidata, still contain significant gaps. Language models (LMs) have been proposed as a source for filling these gaps. However, prior works have focused on prominent entities with rich coverage by LMs, neglecting the crucial case of long-tail entities. In this paper, we present a novel method for LM-based-KB completion that is specifically geared for facts about long-tail entities. The method leverages two different LMs in two stages: for candidate retrieval and for candidate verification and disambiguation. To evaluate our method and various baselines, we introduce a novel dataset, called MALT, rooted in Wikidata. Our method outperforms all baselines in F1, with major gains especially in recall.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#20351;&#29992;GPT-4&#22312;&#20154;&#24037;&#26234;&#33021;&#35838;&#31243;&#20013;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#23398;&#20064;&#30446;&#26631;&#30340;&#33021;&#21147;&#65292;&#24378;&#35843;&#20102;&#23398;&#20064;&#30446;&#26631;&#30340;&#37325;&#35201;&#24615;&#21644;&#25776;&#20889;&#39640;&#36136;&#37327;&#30446;&#26631;&#30340;&#25361;&#25112;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.17459</link><description>&lt;p&gt;
&#22312;&#35838;&#31243;&#35774;&#35745;&#20013;&#21033;&#29992;LLMs: &#20351;&#29992;GPT-4&#25903;&#25345;&#23398;&#20064;&#30446;&#26631;&#30340;&#21019;&#20316;
&lt;/p&gt;
&lt;p&gt;
Harnessing LLMs in Curricular Design: Using GPT-4 to Support Authoring of Learning Objectives. (arXiv:2306.17459v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#20351;&#29992;GPT-4&#22312;&#20154;&#24037;&#26234;&#33021;&#35838;&#31243;&#20013;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#23398;&#20064;&#30446;&#26631;&#30340;&#33021;&#21147;&#65292;&#24378;&#35843;&#20102;&#23398;&#20064;&#30446;&#26631;&#30340;&#37325;&#35201;&#24615;&#21644;&#25776;&#20889;&#39640;&#36136;&#37327;&#30446;&#26631;&#30340;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35780;&#20272;&#20102;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#65288;GPT-4&#65289;&#22312;&#23454;&#36341;&#23548;&#21521;&#30340;&#20154;&#24037;&#26234;&#33021;&#22823;&#23398;&#35838;&#31243;&#20013;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#23398;&#20064;&#30446;&#26631;&#65288;LOs&#65289;&#30340;&#33021;&#21147;&#12290;&#23545;&#20110;&#25945;&#32946;&#20013;&#36825;&#19968;&#26032;&#20852;&#25216;&#26415;&#30340;&#26426;&#20250;&#65288;&#20363;&#22914;&#20869;&#23481;&#29983;&#25104;&#65292;&#35299;&#37322;&#65289;&#21644;&#39118;&#38505;&#65288;&#20363;&#22914;&#20316;&#24330;&#65289;&#30340;&#35752;&#35770;&#26085;&#30410;&#21152;&#24378;&#65292;&#20294;&#21040;&#30446;&#21069;&#20026;&#27490;&#36824;&#27809;&#26377;&#19968;&#39033;&#30740;&#31350;&#35780;&#20272;&#27169;&#22411;&#22312;&#35838;&#31243;&#35774;&#35745;&#21644;LOs&#25776;&#20889;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;LOs&#28165;&#26224;&#34920;&#36798;&#20102;&#23398;&#20064;&#32773;&#36890;&#36807;&#21442;&#19982;&#35838;&#31243;&#25152;&#26399;&#26395;&#33719;&#24471;&#30340;&#30693;&#35782;&#21644;&#25216;&#33021;&#12290;&#20026;&#20102;&#26377;&#25928;&#65292;LOs&#24517;&#39035;&#19987;&#27880;&#20110;&#23398;&#29983;&#30340;&#39044;&#26399;&#25104;&#23601;&#65292;&#19987;&#27880;&#20110;&#29305;&#23450;&#30340;&#35748;&#30693;&#36807;&#31243;&#65292;&#24182;&#19988;&#21487;&#20197;&#34913;&#37327;&#12290;&#22240;&#27492;&#65292;&#25776;&#20889;&#39640;&#36136;&#37327;LOs&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#32791;&#26102;&#65288;&#21363;&#26114;&#36149;&#65289;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#23545;127&#20010;LOs&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#36825;&#20123;LOs&#26159;&#22522;&#20110;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#65288;&#20851;&#20110;&#39640;&#36136;&#37327;LOs&#25776;&#20889;&#30340;&#35814;&#32454;&#25351;&#21335;&#65289;&#33258;&#21160;&#29983;&#25104;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We evaluated the capability of a generative pre-trained transformer (GPT-4) to automatically generate high-quality learning objectives (LOs) in the context of a practically oriented university course on Artificial Intelligence. Discussions of opportunities (e.g., content generation, explanation) and risks (e.g., cheating) of this emerging technology in education have intensified, but to date there has not been a study of the models' capabilities in supporting the course design and authoring of LOs. LOs articulate the knowledge and skills learners are intended to acquire by engaging with a course. To be effective, LOs must focus on what students are intended to achieve, focus on specific cognitive processes, and be measurable. Thus, authoring high-quality LOs is a challenging and time consuming (i.e., expensive) effort. We evaluated 127 LOs that were automatically generated based on a carefully crafted prompt (detailed guidelines on high-quality LOs authoring) submitted to GPT-4 for con
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20013;&#25991;&#25991;&#26412;&#38169;&#35823;&#26657;&#27491;&#30340;&#28176;&#36827;&#24335;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;ProTEC&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#24341;&#23548;&#27169;&#22411;&#20174;&#26131;&#21040;&#38590;&#22320;&#23398;&#20064;&#38169;&#35823;&#26816;&#27979;&#12289;&#38169;&#35823;&#31867;&#22411;&#35782;&#21035;&#21644;&#26657;&#27491;&#32467;&#26524;&#29983;&#25104;&#65292;&#20197;&#35299;&#20915;&#36807;&#32416;&#27491;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.17447</link><description>&lt;p&gt;
&#38754;&#21521;&#20013;&#25991;&#25991;&#26412;&#38169;&#35823;&#26657;&#27491;&#30340;&#28176;&#36827;&#24335;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Progressive Multi-task Learning Framework for Chinese Text Error Correction. (arXiv:2306.17447v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17447
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20013;&#25991;&#25991;&#26412;&#38169;&#35823;&#26657;&#27491;&#30340;&#28176;&#36827;&#24335;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;ProTEC&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#24341;&#23548;&#27169;&#22411;&#20174;&#26131;&#21040;&#38590;&#22320;&#23398;&#20064;&#38169;&#35823;&#26816;&#27979;&#12289;&#38169;&#35823;&#31867;&#22411;&#35782;&#21035;&#21644;&#26657;&#27491;&#32467;&#26524;&#29983;&#25104;&#65292;&#20197;&#35299;&#20915;&#36807;&#32416;&#27491;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20013;&#25991;&#25991;&#26412;&#38169;&#35823;&#26657;&#27491;&#26088;&#22312;&#26816;&#27979;&#21644;&#32416;&#27491;&#36755;&#20837;&#25991;&#26412;&#20013;&#30340;&#38169;&#35823;&#65292;&#36825;&#26377;&#30410;&#20110;&#20154;&#31867;&#26085;&#24120;&#29983;&#27963;&#21644;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#36817;&#26399;&#30340;&#26041;&#27861;&#20027;&#35201;&#37319;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLM)&#26469;&#35299;&#20915;&#20013;&#25991;&#25991;&#26412;&#38169;&#35823;&#26657;&#27491;&#20219;&#21153;&#65292;&#24182;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#26041;&#27861;&#23384;&#22312;&#36807;&#32416;&#27491;&#21644;&#27424;&#32416;&#27491;&#30340;&#38382;&#39064;&#65292;&#21069;&#32773;&#22312;&#23545;&#31934;&#30830;&#24615;&#35201;&#27714;&#36739;&#39640;&#30340;&#20013;&#25991;&#25991;&#26412;&#38169;&#35823;&#26657;&#27491;&#20219;&#21153;&#20013;&#23588;&#20026;&#26126;&#26174;&#12290;&#20026;&#20102;&#32531;&#35299;&#36807;&#32416;&#27491;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#26080;&#20851;&#30340;&#28176;&#36827;&#24335;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;ProTEC&#65292;&#23427;&#24341;&#23548;&#19968;&#20010;CTEC&#27169;&#22411;&#20174;&#31616;&#21333;&#21040;&#22256;&#38590;&#22320;&#23398;&#20064;&#20219;&#21153;&#12290;&#25105;&#20204;&#23558;CTEC&#20219;&#21153;&#20998;&#20026;&#19977;&#20010;&#23376;&#20219;&#21153;&#65292;&#20174;&#26131;&#21040;&#38590;&#20998;&#21035;&#20026;&#38169;&#35823;&#26816;&#27979;&#12289;&#38169;&#35823;&#31867;&#22411;&#35782;&#21035;&#21644;&#26657;&#27491;&#32467;&#26524;&#29983;&#25104;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;ProTEC&#23558;&#36825;&#20123;&#23376;&#20219;&#21153;&#32435;&#20837;&#22810;&#20219;&#21153;&#35757;&#32451;&#30446;&#26631;&#65292;&#24341;&#23548;&#27169;&#22411;&#36880;&#28176;&#23398;&#20064;&#25991;&#26412;&#38169;&#35823;&#26657;&#27491;&#12290;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#27169;&#22411;&#21017;...
&lt;/p&gt;
&lt;p&gt;
Chinese Text Error Correction (CTEC) aims to detect and correct errors in the input text, which benefits human's daily life and various downstream tasks. Recent approaches mainly employ Pre-trained Language Models (PLMs) to resolve CTEC task and achieve tremendous success. However, previous approaches suffer from issues of over-correction and under-correction, and the former is especially conspicuous in the precision-critical CTEC task. To mitigate the issue of overcorrection, we propose a novel model-agnostic progressive multitask learning framework for CTEC, named ProTEC, which guides a CTEC model to learn the task from easy to difficult. We divide CTEC task into three sub-tasks from easy to difficult: Error Detection, Error Type Identification, and Correction Result Generation. During the training process, ProTEC guides the model to learn text error correction progressively by incorporating these sub-tasks into a multi-task training objective. During the inference process, the model
&lt;/p&gt;</description></item><item><title>GPTWatermark&#26159;&#19968;&#31181;&#38024;&#23545;&#24615;&#27169;&#22411;&#27700;&#21360;&#25216;&#26415;&#65292;&#36890;&#36807;&#22266;&#23450;&#20998;&#32452;&#35774;&#35745;&#21644;&#24378;&#22823;&#30340;&#21487;&#35777;&#26126;&#20445;&#35777;&#65292;&#25552;&#20379;&#20102;&#23545;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#40065;&#26834;&#24615;&#26816;&#27979;&#21644;&#23433;&#20840;&#24615;&#38450;&#24481;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#26816;&#27979;&#20934;&#30830;&#24615;&#21644;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#65292;&#25512;&#21160;&#20102;LLMs&#36127;&#36131;&#20219;&#20351;&#29992;&#30340;&#36827;&#27493;&#12290;</title><link>http://arxiv.org/abs/2306.17439</link><description>&lt;p&gt;
&#21487;&#35777;&#26126;&#30340;&#38024;&#23545;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#40065;&#26834;&#27700;&#21360;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Provable Robust Watermarking for AI-Generated Text. (arXiv:2306.17439v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17439
&lt;/p&gt;
&lt;p&gt;
GPTWatermark&#26159;&#19968;&#31181;&#38024;&#23545;&#24615;&#27169;&#22411;&#27700;&#21360;&#25216;&#26415;&#65292;&#36890;&#36807;&#22266;&#23450;&#20998;&#32452;&#35774;&#35745;&#21644;&#24378;&#22823;&#30340;&#21487;&#35777;&#26126;&#20445;&#35777;&#65292;&#25552;&#20379;&#20102;&#23545;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#40065;&#26834;&#24615;&#26816;&#27979;&#21644;&#23433;&#20840;&#24615;&#38450;&#24481;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#26816;&#27979;&#20934;&#30830;&#24615;&#21644;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#65292;&#25512;&#21160;&#20102;LLMs&#36127;&#36131;&#20219;&#20351;&#29992;&#30340;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#36234;&#26469;&#36234;&#25509;&#36817;&#20154;&#31867;&#25776;&#20889;&#30340;&#20869;&#23481;&#65292;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#33021;&#21147;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GPTWatermark&#65292;&#19968;&#31181;&#24378;&#22823;&#19988;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#30830;&#23450;&#19968;&#27573;&#25991;&#26412;&#26159;&#21542;&#26469;&#33258;&#29305;&#23450;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#27700;&#21360;&#31574;&#30053;&#65292;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#22266;&#23450;&#30340;&#20998;&#32452;&#35774;&#35745;&#65292;&#20197;&#22686;&#24378;&#23545;&#32534;&#36753;&#21644;&#25913;&#20889;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#24102;&#27700;&#21360;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#36136;&#37327;&#12289;&#26816;&#27979;&#27491;&#30830;&#24615;&#21644;&#23545;&#25239;&#35268;&#36991;&#25915;&#20987;&#30340;&#23433;&#20840;&#24615;&#26041;&#38754;&#20855;&#26377;&#24378;&#22823;&#30340;&#21487;&#35777;&#26126;&#20445;&#35777;&#12290;&#22312;&#21508;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26816;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#36798;&#21040;&#20102;&#20248;&#36234;&#30340;&#34920;&#29616;&#65292;&#24182;&#19988;&#19982;&#29983;&#25104;&#36136;&#37327;&#22312;&#22256;&#24785;&#24230;&#26041;&#38754;&#30456;&#24403;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;LLMs&#30340;&#36127;&#36131;&#20219;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
As AI-generated text increasingly resembles human-written content, the ability to detect machine-generated text becomes crucial. To address this challenge, we present GPTWatermark, a robust and high-quality solution designed to ascertain whether a piece of text originates from a specific model. Our approach extends existing watermarking strategies and employs a fixed group design to enhance robustness against editing and paraphrasing attacks. We show that our watermarked language model enjoys strong provable guarantees on generation quality, correctness in detection, and security against evasion attacks. Experimental results on various large language models (LLMs) and diverse datasets demonstrate that our method achieves superior detection accuracy and comparable generation quality in perplexity, thus promoting the responsible use of LLMs.
&lt;/p&gt;</description></item><item><title>LMBot&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#25512;&#29305;&#26426;&#22120;&#20154;&#26816;&#27979;&#26694;&#26550;&#65292;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#30693;&#35782;&#34701;&#20837;&#21040;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#23454;&#29616;&#20102;&#26080;&#22270;&#24418;&#37096;&#32626;&#65292;&#20197;&#35299;&#20915;&#25968;&#25454;&#20381;&#36182;&#24615;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.17408</link><description>&lt;p&gt;
LMBot: &#23558;&#22270;&#24418;&#30693;&#35782;&#34701;&#20837;&#35821;&#35328;&#27169;&#22411;&#20197;&#36827;&#34892;&#26080;&#22270;&#24418;&#37096;&#32626;&#30340;&#25512;&#29305;&#26426;&#22120;&#20154;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
LMBot: Distilling Graph Knowledge into Language Model for Graph-less Deployment in Twitter Bot Detection. (arXiv:2306.17408v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17408
&lt;/p&gt;
&lt;p&gt;
LMBot&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#25512;&#29305;&#26426;&#22120;&#20154;&#26816;&#27979;&#26694;&#26550;&#65292;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#30693;&#35782;&#34701;&#20837;&#21040;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#23454;&#29616;&#20102;&#26080;&#22270;&#24418;&#37096;&#32626;&#65292;&#20197;&#35299;&#20915;&#25968;&#25454;&#20381;&#36182;&#24615;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24694;&#24847;&#34892;&#20026;&#32773;&#20351;&#29992;&#36234;&#26469;&#36234;&#20808;&#36827;&#21644;&#24191;&#27867;&#30340;&#26426;&#22120;&#20154;&#26469;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#21644;&#25805;&#32437;&#33286;&#35770;&#65292;&#25512;&#29305;&#26426;&#22120;&#20154;&#30340;&#26816;&#27979;&#24050;&#25104;&#20026;&#19968;&#39033;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#23613;&#31649;&#22522;&#20110;&#22270;&#24418;&#30340;&#25512;&#29305;&#26426;&#22120;&#20154;&#26816;&#27979;&#26041;&#27861;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#30340;&#25512;&#29702;&#20381;&#36182;&#20110;&#36317;&#31163;&#30446;&#26631;&#29992;&#25143;&#22810;&#36339;&#30340;&#37051;&#23621;&#29992;&#25143;&#65292;&#24182;&#19988;&#33719;&#21462;&#37051;&#23621;&#29992;&#25143;&#26159;&#32791;&#26102;&#30340;&#65292;&#24182;&#21487;&#33021;&#24341;&#20837;&#20559;&#24046;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#25512;&#29305;&#26426;&#22120;&#20154;&#26816;&#27979;&#19978;&#24494;&#35843;&#21518;&#65292;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#31454;&#20105;&#24615;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#34920;&#29616;&#65292;&#24182;&#19988;&#22312;&#37096;&#32626;&#36807;&#31243;&#20013;&#19981;&#38656;&#35201;&#22270;&#24418;&#32467;&#26500;&#12290;&#21463;&#21040;&#36825;&#19968;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#20154;&#26816;&#27979;&#26694;&#26550;LMBot&#65292;&#23427;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#30340;&#30693;&#35782;&#34701;&#20837;&#35821;&#35328;&#27169;&#22411;(LMs)&#65292;&#20197;&#22312;&#25512;&#29305;&#26426;&#22120;&#20154;&#26816;&#27979;&#20013;&#36827;&#34892;&#26080;&#22270;&#24418;&#37096;&#32626;&#65292;&#20197;&#24212;&#23545;&#25968;&#25454;&#20381;&#36182;&#24615;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;LMBot&#23545;&#22522;&#20110;&#22270;&#24418;&#21644;&#19981;&#20351;&#29992;&#22270;&#24418;&#30340;&#25968;&#25454;&#38598;&#20860;&#23481;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#27599;&#20010;&#29992;&#25143;&#34920;&#31034;&#20026;&#19968;&#27573;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
As malicious actors employ increasingly advanced and widespread bots to disseminate misinformation and manipulate public opinion, the detection of Twitter bots has become a crucial task. Though graph-based Twitter bot detection methods achieve state-of-the-art performance, we find that their inference depends on the neighbor users multi-hop away from the targets, and fetching neighbors is time-consuming and may introduce bias. At the same time, we find that after finetuning on Twitter bot detection, pretrained language models achieve competitive performance and do not require a graph structure during deployment. Inspired by this finding, we propose a novel bot detection framework LMBot that distills the knowledge of graph neural networks (GNNs) into language models (LMs) for graph-less deployment in Twitter bot detection to combat the challenge of data dependency. Moreover, LMBot is compatible with graph-based and graph-less datasets. Specifically, we first represent each user as a tex
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#26500;&#24314;&#20102;&#31532;&#19968;&#20010;&#26085;&#35821;&#35789;&#27719;&#22797;&#26434;&#24615;&#39044;&#27979;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#25552;&#20379;&#19981;&#21516;&#30340;&#22797;&#26434;&#24230;&#35780;&#20998;&#28385;&#36275;&#20102;&#38750;&#27597;&#35821;&#35835;&#32773;&#30340;&#38656;&#35201;&#65292;&#24182;&#23637;&#31034;&#20102;&#22522;&#20110;BERT&#30340;&#31995;&#32479;&#22312;&#26085;&#35821;LCP&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.17399</link><description>&lt;p&gt;
&#38750;&#27597;&#35821;&#35835;&#32773;&#30340;&#26085;&#35821;&#35789;&#27719;&#22797;&#26434;&#24615;&#65306;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Japanese Lexical Complexity for Non-Native Readers: A New Dataset. (arXiv:2306.17399v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17399
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26500;&#24314;&#20102;&#31532;&#19968;&#20010;&#26085;&#35821;&#35789;&#27719;&#22797;&#26434;&#24615;&#39044;&#27979;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#25552;&#20379;&#19981;&#21516;&#30340;&#22797;&#26434;&#24230;&#35780;&#20998;&#28385;&#36275;&#20102;&#38750;&#27597;&#35821;&#35835;&#32773;&#30340;&#38656;&#35201;&#65292;&#24182;&#23637;&#31034;&#20102;&#22522;&#20110;BERT&#30340;&#31995;&#32479;&#22312;&#26085;&#35821;LCP&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#27719;&#22797;&#26434;&#24230;&#39044;&#27979;&#65288;LCP&#65289;&#26159;&#22312;&#36830;&#32493;&#30340;&#23610;&#24230;&#19978;&#39044;&#27979;&#25991;&#26412;&#20013;&#35789;&#27719;&#22797;&#26434;&#24230;&#30340;&#20219;&#21153;&#12290;&#23427;&#22312;&#31616;&#21270;&#25110;&#26631;&#27880;&#22797;&#26434;&#21333;&#35789;&#26469;&#36741;&#21161;&#35835;&#32773;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20026;&#20102;&#30740;&#31350;&#26085;&#35821;&#20013;&#30340;&#35789;&#27719;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#31532;&#19968;&#20010;&#26085;&#35821;LCP&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#38024;&#23545;&#20013;&#25991;/&#38889;&#25991;&#26631;&#27880;&#32773;&#21644;&#20854;&#20182;&#20154;&#25552;&#20379;&#20102;&#19981;&#21516;&#30340;&#22797;&#26434;&#24230;&#35780;&#20998;&#65292;&#20197;&#28385;&#36275;&#35835;&#32773;&#30340;&#27597;&#35821;&#29305;&#23450;&#38656;&#27714;&#12290;&#22312;&#22522;&#32447;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;BERT&#30340;&#26085;&#35821;LCP&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lexical complexity prediction (LCP) is the task of predicting the complexity of words in a text on a continuous scale. It plays a vital role in simplifying or annotating complex words to assist readers. To study lexical complexity in Japanese, we construct the first Japanese LCP dataset. Our dataset provides separate complexity scores for Chinese/Korean annotators and others to address the readers' L1-specific needs. In the baseline experiment, we demonstrate the effectiveness of a BERT-based system for Japanese LCP.
&lt;/p&gt;</description></item><item><title>SummQA&#22312;MEDIQA-Chat 2023&#19978;&#36890;&#36807;&#20351;&#29992;GPT-4&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#38024;&#23545;&#21307;&#23398;&#25688;&#35201;&#20219;&#21153;&#21462;&#24471;&#20102;&#26174;&#33879;&#25928;&#26524;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;few-shot prompting&#30340;&#26377;&#25928;&#24212;&#29992;&#65292;&#34429;&#28982;&#20063;&#21457;&#29616;&#20102;&#22522;&#20110;prompting&#26041;&#27861;&#30340;&#20960;&#20010;&#19981;&#36275;&#20043;&#22788;&#12290;</title><link>http://arxiv.org/abs/2306.17384</link><description>&lt;p&gt;
MEDIQA-Chat 2023&#19978;&#30340;SummQA: &#20351;&#29992;GPT-4&#36827;&#34892;&#21307;&#23398;&#25688;&#35201;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SummQA at MEDIQA-Chat 2023:In-Context Learning with GPT-4 for Medical Summarization. (arXiv:2306.17384v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17384
&lt;/p&gt;
&lt;p&gt;
SummQA&#22312;MEDIQA-Chat 2023&#19978;&#36890;&#36807;&#20351;&#29992;GPT-4&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#38024;&#23545;&#21307;&#23398;&#25688;&#35201;&#20219;&#21153;&#21462;&#24471;&#20102;&#26174;&#33879;&#25928;&#26524;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;few-shot prompting&#30340;&#26377;&#25928;&#24212;&#29992;&#65292;&#34429;&#28982;&#20063;&#21457;&#29616;&#20102;&#22522;&#20110;prompting&#26041;&#27861;&#30340;&#20960;&#20010;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21307;&#23398;&#23545;&#35805;&#30340;&#38750;&#32467;&#26500;&#21270;&#24615;&#36136;&#12289;&#40644;&#37329;&#25688;&#35201;&#20013;&#30340;&#21307;&#23398;&#26415;&#35821;&#20351;&#29992;&#20197;&#21450;&#22312;&#22810;&#20010;&#30151;&#29366;&#38598;&#21512;&#20013;&#35782;&#21035;&#20851;&#38190;&#20449;&#24687;&#30340;&#38656;&#35201;&#65292;&#21307;&#23398;&#23545;&#35805;&#25688;&#35201;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#22312;MEDIQA 2023&#20849;&#20139;&#20219;&#21153;&#30340;Dialogue2Note&#21307;&#23398;&#25688;&#35201;&#20219;&#21153;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#38024;&#23545;&#25353;&#37096;&#20998;&#25688;&#35201;&#65288;&#20219;&#21153;A&#65289;&#30340;&#26041;&#27861;&#26159;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#36807;&#31243;&#65292;&#39318;&#20808;&#36873;&#25321;&#35821;&#20041;&#30456;&#20284;&#30340;&#23545;&#35805;&#65292;&#28982;&#21518;&#20351;&#29992;&#21069;k&#20010;&#30456;&#20284;&#30340;&#23545;&#35805;&#20316;&#20026;GPT-4&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#12290;&#23545;&#20110;&#23436;&#25972;&#25688;&#35201;&#65288;&#20219;&#21153;B&#65289;&#65292;&#25105;&#20204;&#20351;&#29992;&#31867;&#20284;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;k=1&#12290;&#22312;&#20219;&#21153;A&#20013;&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;&#31532;&#19977;&#21517;&#65288;&#22312;&#25152;&#26377;&#22242;&#38431;&#20013;&#25490;&#21517;&#31532;&#20108;&#65289;&#65292;&#22312;&#20219;&#21153;B&#30340;&#20998;&#31867;&#25688;&#35201;&#20013;&#21462;&#24471;&#20102;&#31532;&#22235;&#21517;&#65288;&#22312;&#25152;&#26377;&#22242;&#38431;&#20013;&#25490;&#21517;&#31532;&#20108;&#65289;&#65292;&#22312;&#20219;&#21153;A&#30340;&#37096;&#20998;&#26631;&#39064;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#31532;15&#21517;&#65288;&#22312;&#25152;&#26377;&#22242;&#38431;&#20013;&#25490;&#21517;&#31532;&#20061;&#65289;&#65292;&#20197;&#21450;&#22312;&#20219;&#21153;B&#20013;&#22312;&#25152;&#26377;&#22242;&#38431;&#20013;&#25490;&#21517;&#31532;&#20843;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#20986;&#20102;&#23545;&#20110;&#36825;&#20010;&#20219;&#21153;&#26469;&#35828;few-shot prompting&#30340;&#26377;&#25928;&#24615;&#65292;&#21516;&#26102;&#25105;&#20204;&#20063;&#21457;&#29616;&#20102;&#20960;&#20010;&#22522;&#20110;prompting&#30340;&#26041;&#27861;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical dialogue summarization is challenging due to the unstructured nature of medical conversations, the use of medical terminology in gold summaries, and the need to identify key information across multiple symptom sets. We present a novel system for the Dialogue2Note Medical Summarization tasks in the MEDIQA 2023 Shared Task. Our approach for section-wise summarization (Task A) is a two-stage process of selecting semantically similar dialogues and using the top-k similar dialogues as in-context examples for GPT-4. For full-note summarization (Task B), we use a similar solution with k=1. We achieved 3rd place in Task A (2nd among all teams), 4th place in Task B Division Wise Summarization (2nd among all teams), 15th place in Task A Section Header Classification (9th among all teams), and 8th place among all teams in Task B. Our results highlight the effectiveness of few-shot prompting for this task, though we also identify several weaknesses of prompting-based approaches. We compare
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#20197;&#37325;&#26032;&#25490;&#24207;&#20505;&#36873;&#30340;&#26469;&#28304;&#65292;&#23454;&#29616;&#23450;&#20301;&#25991;&#26412;&#25776;&#20889;&#30340;&#28304;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21322;&#30417;&#30563;&#26041;&#27861;&#21487;&#20197;&#20960;&#20046;&#19982;&#23436;&#20840;&#30417;&#30563;&#26041;&#27861;&#19968;&#26679;&#26377;&#25928;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#23545;&#30446;&#26631;&#21644;&#28304;&#25991;&#26723;&#36827;&#34892;&#26114;&#36149;&#30340;&#36328;&#24230;&#32423;&#27880;&#37322;&#12290;</title><link>http://arxiv.org/abs/2306.17322</link><description>&lt;p&gt;
&#24341;&#25991;&#20316;&#20026;&#26597;&#35810;&#65306;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#37325;&#25490;&#24207;&#22120;&#30340;&#28304;&#24402;&#23646;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Citations as Queries: Source Attribution Using Language Models as Rerankers. (arXiv:2306.17322v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17322
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#20197;&#37325;&#26032;&#25490;&#24207;&#20505;&#36873;&#30340;&#26469;&#28304;&#65292;&#23454;&#29616;&#23450;&#20301;&#25991;&#26412;&#25776;&#20889;&#30340;&#28304;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21322;&#30417;&#30563;&#26041;&#27861;&#21487;&#20197;&#20960;&#20046;&#19982;&#23436;&#20840;&#30417;&#30563;&#26041;&#27861;&#19968;&#26679;&#26377;&#25928;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#23545;&#30446;&#26631;&#21644;&#28304;&#25991;&#26723;&#36827;&#34892;&#26114;&#36149;&#30340;&#36328;&#24230;&#32423;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24494;&#35843;&#21508;&#31181;&#35821;&#35328;&#27169;&#22411;&#20197;&#37325;&#26032;&#25490;&#24207;&#20505;&#36873;&#28304;&#65292;&#25506;&#32034;&#20102;&#23450;&#20301;&#25776;&#20889;&#25991;&#26412;&#25152;&#20351;&#29992;&#30340;&#26469;&#28304;&#30340;&#26032;&#26041;&#27861;&#12290;&#22312;&#20351;&#29992;&#22522;&#20934;&#30340;BM25&#26816;&#32034;&#27169;&#22411;&#26816;&#32034;&#20505;&#36873;&#28304;&#20043;&#21518;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#22810;&#31181;&#37325;&#26032;&#25490;&#24207;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;&#23427;&#20204;&#22312;&#28304;&#24402;&#23646;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#20998;&#21035;&#20026;&#33521;&#25991;&#32500;&#22522;&#30334;&#31185;&#21644;&#20013;&#19990;&#32426;&#38463;&#25289;&#20271;&#21382;&#21490;&#25991;&#29486;&#65292;&#24182;&#37319;&#29992;&#20102;&#22810;&#31181;&#22522;&#20110;&#26816;&#32034;&#21644;&#29983;&#25104;&#30340;&#37325;&#26032;&#25490;&#24207;&#27169;&#22411;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35797;&#22270;&#20102;&#35299;&#25152;&#38656;&#30417;&#30563;&#31243;&#24230;&#22914;&#20309;&#24433;&#21709;&#21508;&#31181;&#37325;&#26032;&#25490;&#24207;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21322;&#30417;&#30563;&#26041;&#27861;&#22312;&#36991;&#20813;&#23545;&#30446;&#26631;&#21644;&#28304;&#25991;&#26723;&#36827;&#34892;&#28508;&#22312;&#26114;&#36149;&#30340;&#36328;&#24230;&#32423;&#27880;&#37322;&#30340;&#21516;&#26102;&#65292;&#20960;&#20046;&#21487;&#20197;&#19982;&#23436;&#20840;&#30417;&#30563;&#26041;&#27861;&#19968;&#26679;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores new methods for locating the sources used to write a text, by fine-tuning a variety of language models to rerank candidate sources. After retrieving candidates sources using a baseline BM25 retrieval model, a variety of reranking methods are tested to see how effective they are at the task of source attribution. We conduct experiments on two datasets, English Wikipedia and medieval Arabic historical writing, and employ a variety of retrieval and generation based reranking models. In particular, we seek to understand how the degree of supervision required affects the performance of various reranking models. We find that semisupervised methods can be nearly as effective as fully supervised methods while avoiding potentially costly span-level annotation of the target and source documents.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#25918;&#39046;&#22495;&#30340;&#20027;&#39064;&#20998;&#31867;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#23454;&#26102;&#25509;&#21463;&#29992;&#25143;&#23450;&#20041;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#21547;&#30693;&#35782;&#36827;&#34892;&#38646;&#23556;&#20987;&#20998;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#31995;&#32479;&#22312;&#24320;&#25918;&#39046;&#22495;&#22330;&#26223;&#20013;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#38646;&#23556;&#20987;&#22522;&#32447;&#27169;&#22411;&#65292;&#19982;&#24369;&#30417;&#30563;&#27169;&#22411;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.17290</link><description>&lt;p&gt;
&#23454;&#29616;&#24320;&#25918;&#39046;&#22495;&#30340;&#20027;&#39064;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Towards Open-Domain Topic Classification. (arXiv:2306.17290v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17290
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#25918;&#39046;&#22495;&#30340;&#20027;&#39064;&#20998;&#31867;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#23454;&#26102;&#25509;&#21463;&#29992;&#25143;&#23450;&#20041;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#21547;&#30693;&#35782;&#36827;&#34892;&#38646;&#23556;&#20987;&#20998;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#31995;&#32479;&#22312;&#24320;&#25918;&#39046;&#22495;&#22330;&#26223;&#20013;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#38646;&#23556;&#20987;&#22522;&#32447;&#27169;&#22411;&#65292;&#19982;&#24369;&#30417;&#30563;&#27169;&#22411;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#23454;&#26102;&#25509;&#21463;&#29992;&#25143;&#23450;&#20041;&#20998;&#31867;&#27861;&#30340;&#24320;&#25918;&#39046;&#22495;&#20027;&#39064;&#20998;&#31867;&#31995;&#32479;&#12290;&#29992;&#25143;&#21487;&#20197;&#26681;&#25454;&#20182;&#20204;&#24819;&#35201;&#30340;&#20505;&#36873;&#26631;&#31614;&#23545;&#25991;&#26412;&#29255;&#27573;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#20174;&#25105;&#20204;&#30340;&#32593;&#32476;&#30028;&#38754;&#33719;&#24471;&#21363;&#26102;&#21709;&#24212;&#12290;&#20026;&#20102;&#33719;&#24471;&#36825;&#31181;&#28789;&#27963;&#24615;&#65292;&#25105;&#20204;&#20197;&#38646;&#23556;&#20987;&#30340;&#26041;&#24335;&#26500;&#24314;&#20102;&#21518;&#31471;&#27169;&#22411;&#12290;&#36890;&#36807;&#22312;&#20174;&#32500;&#22522;&#30334;&#31185;&#26500;&#24314;&#30340;&#26032;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#26631;&#31614;&#24863;&#30693;&#25991;&#26412;&#20998;&#31867;&#22120;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38544;&#21547;&#30693;&#35782;&#26469;&#22788;&#29702;&#23427;&#20197;&#21069;&#20174;&#26410;&#35265;&#36807;&#30340;&#26631;&#31614;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20855;&#26377;&#19981;&#21516;&#30340;&#26631;&#31614;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#24320;&#25918;&#39046;&#22495;&#22330;&#26223;&#19979;&#65292;&#35813;&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#38646;&#23556;&#20987;&#22522;&#32447;&#65292;&#24182;&#19982;&#22312;&#22495;&#20869;&#25968;&#25454;&#19978;&#36827;&#34892;&#24369;&#30417;&#30563;&#35757;&#32451;&#30340;&#27169;&#22411;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce an open-domain topic classification system that accepts user-defined taxonomy in real time. Users will be able to classify a text snippet with respect to any candidate labels they want, and get instant response from our web interface. To obtain such flexibility, we build the backend model in a zero-shot way. By training on a new dataset constructed from Wikipedia, our label-aware text classifier can effectively utilize implicit knowledge in the pretrained language model to handle labels it has never seen before. We evaluate our model across four datasets from various domains with different label sets. Experiments show that the model significantly improves over existing zero-shot baselines in open-domain scenarios, and performs competitively with weakly-supervised models trained on in-domain data.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#39044;&#27979;COVID-19&#24739;&#32773;&#20986;&#38498;&#21518;&#22312;&#24613;&#35786;&#23460;&#30340;&#20877;&#35775;&#24773;&#20917;&#65292;&#26089;&#26399;&#35782;&#21035;&#26377;&#21161;&#20110;&#21307;&#29983;&#19987;&#27880;&#20110;&#21361;&#21450;&#29983;&#21629;&#30340;&#30149;&#20363;&#12290;</title><link>http://arxiv.org/abs/2306.17257</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#28304;&#36801;&#31227;&#23398;&#20064;&#39044;&#27979;COVID-19&#24739;&#32773;&#30340;&#24613;&#35786;&#23460;&#20877;&#35775;
&lt;/p&gt;
&lt;p&gt;
Prediction of COVID-19 Patients' Emergency Room Revisit using Multi-Source Transfer Learning. (arXiv:2306.17257v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17257
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#39044;&#27979;COVID-19&#24739;&#32773;&#20986;&#38498;&#21518;&#22312;&#24613;&#35786;&#23460;&#30340;&#20877;&#35775;&#24773;&#20917;&#65292;&#26089;&#26399;&#35782;&#21035;&#26377;&#21161;&#20110;&#21307;&#29983;&#19987;&#27880;&#20110;&#21361;&#21450;&#29983;&#21629;&#30340;&#30149;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2019&#20896;&#29366;&#30149;&#27602;&#30149;&#65288;COVID-19&#65289;&#23548;&#33268;&#20102;&#19968;&#22330;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#20005;&#37325;&#22823;&#27969;&#34892;&#12290;&#38500;&#20102;&#20855;&#26377;&#39640;&#20256;&#26579;&#24615;&#22806;&#65292;COVID-19&#30340;&#20020;&#24202;&#36827;&#23637;&#21487;&#20197;&#26377;&#24456;&#22823;&#24046;&#24322;&#65292;&#20174;&#26080;&#30151;&#29366;&#25658;&#24102;&#32773;&#21040;&#20005;&#37325;&#19988;&#28508;&#22312;&#21361;&#21450;&#29983;&#21629;&#30340;&#20581;&#24247;&#24182;&#21457;&#30151;&#12290;&#35768;&#22810;&#24739;&#32773;&#22312;&#20986;&#38498;&#21518;&#30340;&#30701;&#26102;&#38388;&#20869;&#38656;&#35201;&#20877;&#27425;&#23601;&#35786;&#24613;&#35786;&#23460;&#65288;ER&#65289;&#65292;&#36825;&#26497;&#22823;&#22686;&#21152;&#20102;&#21307;&#21153;&#20154;&#21592;&#30340;&#24037;&#20316;&#36127;&#25285;&#12290;&#21450;&#26089;&#35782;&#21035;&#27492;&#31867;&#24739;&#32773;&#23545;&#20110;&#24110;&#21161;&#21307;&#29983;&#19987;&#27880;&#20110;&#27835;&#30103;&#21361;&#21450;&#29983;&#21629;&#30340;&#30149;&#20363;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#33719;&#21462;&#20102;2020&#24180;3&#26376;&#33267;2021&#24180;1&#26376;&#26399;&#38388;&#21305;&#20857;&#22561;&#22823;&#23398;&#21307;&#23398;&#20013;&#24515;13&#20010;&#38468;&#23646;&#24613;&#35786;&#23460;&#30340;3,210&#20010;&#24739;&#32773;&#23601;&#35786;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#12290;&#25105;&#20204;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;ScispaCy&#25552;&#21462;&#20020;&#24202;&#27010;&#24565;&#65292;&#24182;&#20351;&#29992;&#20986;&#29616;&#26368;&#39057;&#32321;&#30340;1001&#20010;&#27010;&#24565;&#20026;COVID-19&#24739;&#32773;&#22312;&#24613;&#35786;&#23460;&#20013;&#24314;&#31435;&#20102;7&#22825;&#20877;&#35775;&#27169;&#22411;&#12290;&#25105;&#20204;&#20174;13&#20010;&#24613;&#35786;&#23460;&#25910;&#38598;&#30340;&#30740;&#31350;&#25968;&#25454;&#21487;&#33021;&#20855;&#26377;
&lt;/p&gt;
&lt;p&gt;
The coronavirus disease 2019 (COVID-19) has led to a global pandemic of significant severity. In addition to its high level of contagiousness, COVID-19 can have a heterogeneous clinical course, ranging from asymptomatic carriers to severe and potentially life-threatening health complications. Many patients have to revisit the emergency room (ER) within a short time after discharge, which significantly increases the workload for medical staff. Early identification of such patients is crucial for helping physicians focus on treating life-threatening cases. In this study, we obtained Electronic Health Records (EHRs) of 3,210 encounters from 13 affiliated ERs within the University of Pittsburgh Medical Center between March 2020 and January 2021. We leveraged a Natural Language Processing technique, ScispaCy, to extract clinical concepts and used the 1001 most frequent concepts to develop 7-day revisit models for COVID-19 patients in ERs. The research data we collected from 13 ERs may have 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#20010;&#24615;&#21270;&#20919;&#21551;&#21160;&#25512;&#33616;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#23558;&#25512;&#33616;&#36807;&#31243;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#65292;&#25552;&#20379;&#36866;&#29992;&#20110;&#21019;&#19994;&#20225;&#19994;&#21644;&#29992;&#25143;&#21442;&#19982;&#21382;&#21490;&#19981;&#36275;&#30340;&#24179;&#21488;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2306.17256</link><description>&lt;p&gt;
&#20197;&#25552;&#31034;&#20026;&#22522;&#30784;&#30340;&#20010;&#24615;&#21270;&#20919;&#21551;&#21160;&#25512;&#33616;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Personalized Cold-Start Recommendation with Prompts. (arXiv:2306.17256v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#20010;&#24615;&#21270;&#20919;&#21551;&#21160;&#25512;&#33616;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#23558;&#25512;&#33616;&#36807;&#31243;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#65292;&#25552;&#20379;&#36866;&#29992;&#20110;&#21019;&#19994;&#20225;&#19994;&#21644;&#29992;&#25143;&#21442;&#19982;&#21382;&#21490;&#19981;&#36275;&#30340;&#24179;&#21488;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22312;&#26681;&#25454;&#29992;&#25143;&#36807;&#21435;&#30340;&#34892;&#20026;&#24110;&#21161;&#29992;&#25143;&#21457;&#29616;&#19982;&#20854;&#20852;&#36259;&#30456;&#31526;&#30340;&#20449;&#24687;&#26041;&#38754;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#24403;&#29992;&#25143;&#21644;&#29289;&#21697;&#20043;&#38388;&#30340;&#21382;&#21490;&#20132;&#20114;&#35760;&#24405;&#19981;&#21487;&#29992;&#26102;&#65292;&#24320;&#21457;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#36825;&#23601;&#26159;&#25152;&#35859;&#30340;&#31995;&#32479;&#20919;&#21551;&#21160;&#25512;&#33616;&#38382;&#39064;&#12290;&#27492;&#38382;&#39064;&#22312;&#21019;&#19994;&#20225;&#19994;&#25110;&#29992;&#25143;&#21442;&#19982;&#21382;&#21490;&#19981;&#36275;&#30340;&#24179;&#21488;&#20013;&#23588;&#20026;&#31361;&#20986;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#29992;&#25143;&#25110;&#29289;&#21697;&#30340;&#20919;&#21551;&#21160;&#22330;&#26223;&#65292;&#20854;&#20013;&#31995;&#32479;&#20173;&#28982;&#36890;&#36807;&#22312;&#21516;&#19968;&#39046;&#22495;&#20013;&#30340;&#21382;&#21490;&#29992;&#25143;&#21644;&#29289;&#21697;&#20132;&#20114;&#36827;&#34892;&#35757;&#32451;&#26469;&#20026;&#26032;&#29992;&#25143;&#25110;&#29289;&#21697;&#25552;&#20379;&#25512;&#33616;&#65292;&#32780;&#26080;&#27861;&#35299;&#20915;&#25105;&#20204;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#40511;&#27807;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#25512;&#33616;&#36807;&#31243;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#65292;&#20854;&#20013;&#21253;&#21547;&#29992;&#25143;&#36164;&#26009;&#21644;&#29289;&#21697;&#23646;&#24615;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems play a crucial role in helping users discover information that aligns with their interests based on their past behaviors. However, developing personalized recommendation systems becomes challenging when historical records of user-item interactions are unavailable, leading to what is known as the system cold-start recommendation problem. This issue is particularly prominent in start-up businesses or platforms with insufficient user engagement history. Previous studies focus on user or item cold-start scenarios, where systems could make recommendations for new users or items but are still trained with historical user-item interactions in the same domain, which cannot solve our problem. To bridge the gap, our research introduces an innovative and effective approach, capitalizing on the capabilities of pre-trained language models. We transform the recommendation process into sentiment analysis of natural languages containing information of user profiles and item attribu
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31163;&#25955;&#35821;&#38899;&#21333;&#20803;&#32423;&#21035;&#19978;&#36827;&#34892;&#35821;&#38899;&#24773;&#24863;&#20445;&#30041;&#32763;&#35793;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#22810;&#35821;&#35328;&#24773;&#24863;&#23884;&#20837;&#26469;&#39044;&#27979;&#30446;&#26631;&#35821;&#35328;&#20013;&#35821;&#38899;&#21333;&#20803;&#30340;&#38899;&#39640;&#21644;&#25345;&#32493;&#26102;&#38388;&#65292;&#24182;&#25104;&#21151;&#22320;&#20197;&#30456;&#21516;&#30340;&#24773;&#24863;&#20869;&#23481;&#37325;&#26032;&#21512;&#25104;&#28304;&#35821;&#38899;&#20449;&#21495;&#12290;</title><link>http://arxiv.org/abs/2306.17199</link><description>&lt;p&gt;
&#23398;&#20064;&#22810;&#35821;&#35328;&#34920;&#36798;&#24615;&#35821;&#38899;&#34920;&#31034;&#20197;&#23454;&#29616;&#26080;&#38656;&#24179;&#34892;&#25968;&#25454;&#30340;&#38901;&#24459;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Learning Multilingual Expressive Speech Representation for Prosody Prediction without Parallel Data. (arXiv:2306.17199v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17199
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31163;&#25955;&#35821;&#38899;&#21333;&#20803;&#32423;&#21035;&#19978;&#36827;&#34892;&#35821;&#38899;&#24773;&#24863;&#20445;&#30041;&#32763;&#35793;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#22810;&#35821;&#35328;&#24773;&#24863;&#23884;&#20837;&#26469;&#39044;&#27979;&#30446;&#26631;&#35821;&#35328;&#20013;&#35821;&#38899;&#21333;&#20803;&#30340;&#38899;&#39640;&#21644;&#25345;&#32493;&#26102;&#38388;&#65292;&#24182;&#25104;&#21151;&#22320;&#20197;&#30456;&#21516;&#30340;&#24773;&#24863;&#20869;&#23481;&#37325;&#26032;&#21512;&#25104;&#28304;&#35821;&#38899;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31163;&#25955;&#35821;&#38899;&#21333;&#20803;&#32423;&#21035;&#19978;&#36827;&#34892;&#35821;&#38899;&#24773;&#24863;&#20445;&#30041;&#32763;&#35793;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#20351;&#29992;&#22810;&#35821;&#35328;&#24773;&#24863;&#23884;&#20837;&#65292;&#21487;&#20197;&#20197;&#19968;&#31181;&#35821;&#35328;&#26080;&#20851;&#30340;&#26041;&#24335;&#25429;&#25417;&#24773;&#24863;&#20449;&#24687;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#23884;&#20837;&#21487;&#20197;&#29992;&#20110;&#39044;&#27979;&#30446;&#26631;&#35821;&#35328;&#20013;&#35821;&#38899;&#21333;&#20803;&#30340;&#38899;&#39640;&#21644;&#25345;&#32493;&#26102;&#38388;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#29992;&#30456;&#21516;&#30340;&#24773;&#24863;&#20869;&#23481;&#37325;&#26032;&#21512;&#25104;&#28304;&#35821;&#38899;&#20449;&#21495;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#33521;&#35821;&#21644;&#27861;&#35821;&#35821;&#38899;&#20449;&#21495;&#30340;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20248;&#20110;&#19981;&#20351;&#29992;&#24773;&#24863;&#20449;&#24687;&#30340;&#22522;&#32447;&#26041;&#27861;&#65292;&#21363;&#20351;&#24773;&#24863;&#23884;&#20837;&#26159;&#20174;&#19981;&#21516;&#35821;&#35328;&#25552;&#21462;&#30340;&#12290;&#23613;&#31649;&#36825;&#20010;&#21021;&#27493;&#30740;&#31350;&#24182;&#26410;&#30452;&#25509;&#35299;&#20915;&#26426;&#22120;&#32763;&#35793;&#38382;&#39064;&#65292;&#20294;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35821;&#38899;&#37325;&#21512;&#25104;&#30340;&#36328;&#35821;&#35328;&#24773;&#24863;&#20445;&#30041;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a method for speech-to-speech emotionpreserving translation that operates at the level of discrete speech units. Our approach relies on the use of multilingual emotion embedding that can capture affective information in a language-independent manner. We show that this embedding can be used to predict the pitch and duration of speech units in a target language, allowing us to resynthesize the source speech signal with the same emotional content. We evaluate our approach to English and French speech signals and show that it outperforms a baseline method that does not use emotional information, including when the emotion embedding is extracted from a different language. Even if this preliminary study does not address directly the machine translation issue, our results demonstrate the effectiveness of our approach for cross-lingual emotion preservation in the context of speech resynthesis.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#25351;&#20196;&#35843;&#25972;&#25216;&#26415;&#26469;&#25913;&#21464;&#27169;&#22411;&#34892;&#20026;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#25968;&#25454;&#27880;&#20837;&#30340;&#26041;&#27861;AutoPoison&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#23569;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#27602;&#21270;&#65292;&#23545;&#25163;&#33021;&#22815;&#25913;&#21464;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2306.17194</link><description>&lt;p&gt;
&#20851;&#20110;&#25351;&#20196;&#35843;&#25972;&#30340;&#21487;&#21033;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Exploitability of Instruction Tuning. (arXiv:2306.17194v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17194
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#25351;&#20196;&#35843;&#25972;&#25216;&#26415;&#26469;&#25913;&#21464;&#27169;&#22411;&#34892;&#20026;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#25968;&#25454;&#27880;&#20837;&#30340;&#26041;&#27861;AutoPoison&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#23569;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#27602;&#21270;&#65292;&#23545;&#25163;&#33021;&#22815;&#25913;&#21464;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#26159;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#24847;&#22270;&#23545;&#40784;&#30340;&#26377;&#25928;&#25216;&#26415;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#23545;&#25163;&#22914;&#20309;&#36890;&#36807;&#21521;&#35757;&#32451;&#25968;&#25454;&#27880;&#20837;&#29305;&#23450;&#30340;&#25351;&#20196;&#36319;&#38543;&#31034;&#20363;&#26469;&#21033;&#29992;&#25351;&#20196;&#35843;&#25972;&#65292;&#20174;&#32780;&#26377;&#24847;&#25913;&#21464;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#20363;&#22914;&#65292;&#23545;&#25163;&#21487;&#20197;&#36890;&#36807;&#27880;&#20837;&#25552;&#21450;&#30446;&#26631;&#20869;&#23481;&#30340;&#35757;&#32451;&#31034;&#20363;&#65292;&#24182;&#24341;&#35825;&#19979;&#28216;&#27169;&#22411;&#23637;&#31034;&#27492;&#31867;&#34892;&#20026;&#26469;&#23454;&#29616;&#20869;&#23481;&#27880;&#20837;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#25968;&#25454;&#27880;&#20837;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;AutoPoison&#12290;&#23427;&#20351;&#29992;&#20102;&#19968;&#20010;&#39044;&#35328;&#27169;&#22411;&#26469;&#23558;&#22810;&#26679;&#25915;&#20987;&#30446;&#26631;&#33258;&#28982;&#32780;&#36830;&#36143;&#22320;&#27880;&#20837;&#21040;&#27602;&#21270;&#25968;&#25454;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20004;&#20010;&#23454;&#20363;&#25915;&#20987;&#65306;&#20869;&#23481;&#27880;&#20837;&#21644;&#36807;&#24230;&#25298;&#32477;&#25915;&#20987;&#65292;&#27599;&#20010;&#25915;&#20987;&#37117;&#26088;&#22312;&#35825;&#23548;&#29305;&#23450;&#30340;&#21487;&#21033;&#29992;&#34892;&#20026;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#25968;&#25454;&#27880;&#20837;&#26041;&#26696;&#30340;&#24378;&#24230;&#21644;&#38544;&#34109;&#24615;&#36827;&#34892;&#20102;&#37327;&#21270;&#21644;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20165;&#36890;&#36807;&#27602;&#21270;&#23569;&#37327;&#35757;&#32451;&#25968;&#25454;&#65292;AutoPoison&#20801;&#35768;&#23545;&#25163;&#25913;&#21464;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction tuning is an effective technique to align large language models (LLMs) with human intents. In this work, we investigate how an adversary can exploit instruction tuning by injecting specific instruction-following examples into the training data that intentionally changes the model's behavior. For example, an adversary can achieve content injection by injecting training examples that mention target content and eliciting such behavior from downstream models. To achieve this goal, we propose \textit{AutoPoison}, an automated data poisoning pipeline. It naturally and coherently incorporates versatile attack goals into poisoned data with the help of an oracle LLM. We showcase two example attacks: content injection and over-refusal attacks, each aiming to induce a specific exploitable behavior. We quantify and benchmark the strength and the stealthiness of our data poisoning scheme. Our results show that AutoPoison allows an adversary to change a model's behavior by poisoning only
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#22312;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#25104;&#21151;&#65292;&#22312;&#24418;&#24335;&#35821;&#35328;&#29702;&#35770;&#32972;&#26223;&#19979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;&#20160;&#20040;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21040;&#32452;&#21512;&#35268;&#21017;&#30340;&#35299;&#37322;&#65292;&#24182;&#22312;&#19968;&#20010;&#29616;&#23454;&#19990;&#30028;&#30340;&#33521;&#35821;&#21477;&#23376;&#31034;&#20363;&#20013;&#25552;&#20379;&#20102;&#38646;&#38169;&#35823;&#30340;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2306.17184</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#35299;&#20915;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#38382;&#39064;&#65311;&#25968;&#23398;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Why can neural language models solve next-word prediction? A mathematical perspective. (arXiv:2306.17184v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#22312;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#25104;&#21151;&#65292;&#22312;&#24418;&#24335;&#35821;&#35328;&#29702;&#35770;&#32972;&#26223;&#19979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;&#20160;&#20040;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21040;&#32452;&#21512;&#35268;&#21017;&#30340;&#35299;&#37322;&#65292;&#24182;&#22312;&#19968;&#20010;&#29616;&#23454;&#19990;&#30028;&#30340;&#33521;&#35821;&#21477;&#23376;&#31034;&#20363;&#20013;&#25552;&#20379;&#20102;&#38646;&#38169;&#35823;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#65292;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#22312;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#26041;&#38754;&#35777;&#26126;&#20102;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#22312;&#24418;&#24335;&#35821;&#35328;&#29702;&#35770;&#30340;&#32972;&#26223;&#19979;&#65292;&#20851;&#20110;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#22312;&#27492;&#20219;&#21153;&#20013;&#21487;&#20197;&#23398;&#20064;&#21040;&#32452;&#21512;&#35268;&#21017;&#30340;&#25104;&#21151;&#30340;&#20005;&#26684;&#29702;&#35770;&#35299;&#37322;&#23578;&#26410;&#34987;&#25552;&#20986;&#65292;&#22240;&#20026;&#23578;&#19981;&#28165;&#26970;&#20026;&#20160;&#20040;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#21040;&#25511;&#21046;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#20219;&#21153;&#30340;&#32452;&#21512;&#35268;&#21017;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31867;&#21487;&#20197;&#29992;&#26469;&#27169;&#25311;&#33521;&#35821;&#21477;&#23376;&#30340;&#29616;&#23454;&#19990;&#30028;&#31034;&#20363;&#30340;&#24418;&#24335;&#35821;&#35328;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#20219;&#21153;&#65292;&#19988;&#38169;&#35823;&#29575;&#20026;&#38646;&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#31361;&#20986;&#20102;&#23884;&#20837;&#23618;&#21644;&#20840;&#36830;&#25509;&#32452;&#20214;&#22312;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19981;&#21516;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, deep learning has revolutionized the field of natural language processing, with neural language models proving to be very effective for next-word prediction. However, a rigorous theoretical explanation for their success in the context of formal language theory has not yet been developed, as it is unclear why neural language models can learn the combinatorial rules that govern the next-word prediction task. In this paper, we study a class of formal languages that can be used to model real-world examples of English sentences. We construct neural language models can solve the next-word prediction task in this context with zero error. Our proof highlights the different roles of the embedding layer and the fully connected component within the neural language model.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#29983;&#25104;&#36830;&#32493;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#30340;&#26041;&#27861;&#65288;TESGAN&#65289;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;GAN&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#38480;&#21046;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#36830;&#32493;&#30340;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#21462;&#20195;&#31163;&#25955;&#30340;&#26631;&#35760;&#65292;&#20351;&#24471;&#29983;&#25104;&#22120;&#22312;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#26356;&#26032;&#26799;&#24230;&#26102;&#26356;&#21152;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2306.17181</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#26080;&#30417;&#30563;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#29992;&#20110;&#25991;&#26412;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Text Embedding Space Generation Using Generative Adversarial Networks for Text Synthesis. (arXiv:2306.17181v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#29983;&#25104;&#36830;&#32493;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#30340;&#26041;&#27861;&#65288;TESGAN&#65289;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;GAN&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#38480;&#21046;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#36830;&#32493;&#30340;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#21462;&#20195;&#31163;&#25955;&#30340;&#26631;&#35760;&#65292;&#20351;&#24471;&#29983;&#25104;&#22120;&#22312;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#26356;&#26032;&#26799;&#24230;&#26102;&#26356;&#21152;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#25968;&#25454;&#21512;&#25104;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#29983;&#25104;&#22120;&#21644;&#21028;&#21035;&#22120;&#30340;&#31454;&#20105;&#26469;&#21019;&#24314;&#36924;&#30495;&#30340;&#25968;&#25454;&#12290;&#23613;&#31649;GAN&#22312;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#23384;&#22312;&#22266;&#26377;&#30340;&#38480;&#21046;&#12290;&#22240;&#20026;&#33258;&#28982;&#35821;&#35328;&#30001;&#31163;&#25955;&#30340;&#26631;&#35760;&#32452;&#25104;&#65292;&#29983;&#25104;&#22120;&#22312;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#26356;&#26032;&#26799;&#24230;&#26102;&#36935;&#21040;&#22256;&#38590;&#65307;&#22240;&#27492;&#65292;&#22823;&#22810;&#25968;&#25991;&#26412;-GAN&#30740;&#31350;&#20351;&#29992;&#22870;&#21169;&#31995;&#32479;&#20197;&#38543;&#26426;&#26631;&#35760;&#20026;&#22522;&#30784;&#29983;&#25104;&#21477;&#23376;&#12290;&#22240;&#27492;&#65292;&#20808;&#21069;&#30740;&#31350;&#20013;&#30340;&#29983;&#25104;&#22120;&#22312;&#23545;&#25239;&#35757;&#32451;&#20043;&#21069;&#20197;&#33258;&#22238;&#24402;&#26041;&#24335;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#23548;&#33268;&#21512;&#25104;&#30340;&#21477;&#23376;&#37325;&#22797;&#35757;&#32451;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#31867;&#20284;&#21407;&#22987;GAN&#30340;&#26694;&#26550;&#26469;&#21512;&#25104;&#21477;&#23376;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;TESGAN&#65289;&#65292;&#23427;&#29983;&#25104;&#36830;&#32493;&#30340;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#26469;&#35299;&#20915;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Networks (GAN) is a model for data synthesis, which creates plausible data through the competition of generator and discriminator. Although GAN application to image synthesis is extensively studied, it has inherent limitations to natural language generation. Because natural language is composed of discrete tokens, a generator has difficulty updating its gradient through backpropagation; therefore, most text-GAN studies generate sentences starting with a random token based on a reward system. Thus, the generators of previous studies are pre-trained in an autoregressive way before adversarial training, causing data memorization that synthesized sentences reproduce the training data. In this paper, we synthesize sentences using a framework similar to the original GAN. More specifically, we propose Text Embedding Space Generative Adversarial Networks (TESGAN) which generate continuous text embedding spaces instead of discrete tokens to solve the gradient backpropagat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#26495;&#21270;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;NLP&#25216;&#26415;&#36741;&#21161;&#29983;&#25104;&#25918;&#23556;&#23398;&#25253;&#21578;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#22270;&#20687;&#20998;&#31867;&#22120;&#29983;&#25104;&#22270;&#20687;&#26631;&#31614;&#65292;&#28982;&#21518;&#36890;&#36807;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#27169;&#22411;&#29983;&#25104;&#30149;&#29702;&#25551;&#36848;&#65292;&#24182;&#20351;&#29992;BERT&#27169;&#22411;&#26367;&#25442;&#27491;&#24120;&#25253;&#21578;&#27169;&#26495;&#20013;&#30340;&#30456;&#24212;&#37096;&#20998;&#65292;&#26368;&#32456;&#29983;&#25104;&#23436;&#25972;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#12290;</title><link>http://arxiv.org/abs/2306.17180</link><description>&lt;p&gt;
&#26367;&#25442;&#21644;&#25253;&#21578;&#65306;NLP&#36741;&#21161;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Replace and Report: NLP Assisted Radiology Report Generation. (arXiv:2306.17180v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#26495;&#21270;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;NLP&#25216;&#26415;&#36741;&#21161;&#29983;&#25104;&#25918;&#23556;&#23398;&#25253;&#21578;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#22270;&#20687;&#20998;&#31867;&#22120;&#29983;&#25104;&#22270;&#20687;&#26631;&#31614;&#65292;&#28982;&#21518;&#36890;&#36807;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#27169;&#22411;&#29983;&#25104;&#30149;&#29702;&#25551;&#36848;&#65292;&#24182;&#20351;&#29992;BERT&#27169;&#22411;&#26367;&#25442;&#27491;&#24120;&#25253;&#21578;&#27169;&#26495;&#20013;&#30340;&#30456;&#24212;&#37096;&#20998;&#65292;&#26368;&#32456;&#29983;&#25104;&#23436;&#25972;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#23454;&#36341;&#32463;&#24120;&#20351;&#29992;&#21307;&#23398;&#25104;&#20687;&#26469;&#36827;&#34892;&#35786;&#26029;&#21644;&#27835;&#30103;&#12290;&#33258;&#21160;&#29983;&#25104;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#65292;&#25918;&#23556;&#23398;&#25253;&#21578;&#26159;&#30001;&#22810;&#20010;&#21477;&#23376;&#32452;&#25104;&#30340;&#38271;&#31687;&#21465;&#36848;&#65292;&#21253;&#25324;&#24322;&#24120;&#21644;&#27491;&#24120;&#30340;&#21457;&#29616;&#12290;&#22240;&#27492;&#65292;&#23558;&#20256;&#32479;&#30340;&#22270;&#20687;&#26631;&#39064;&#29983;&#25104;&#26041;&#27861;&#24212;&#29992;&#20110;&#29983;&#25104;&#25972;&#20010;&#25253;&#21578;&#26159;&#19981;&#36275;&#22815;&#30340;&#65292;&#22240;&#20026;&#36825;&#20123;&#26041;&#27861;&#26159;&#35774;&#35745;&#29992;&#20110;&#31616;&#35201;&#25551;&#36848;&#22270;&#20687;&#30340;&#30701;&#21477;&#23376;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#26495;&#30340;&#26041;&#27861;&#65292;&#20174;&#25918;&#23556;&#22270;&#20687;&#20013;&#29983;&#25104;&#25918;&#23556;&#23398;&#25253;&#21578;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20197;&#19979;&#27493;&#39588;&#65306;i&#65289;&#20351;&#29992;&#22810;&#26631;&#31614;&#22270;&#20687;&#20998;&#31867;&#22120;&#65292;&#20026;&#36755;&#20837;&#30340;&#25918;&#23556;&#22270;&#29983;&#25104;&#26631;&#31614;&#65307;ii&#65289;&#20351;&#29992;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#27169;&#22411;&#65292;&#26681;&#25454;&#27493;&#39588;&#65288;i&#65289;&#20013;&#29983;&#25104;&#30340;&#26631;&#31614;&#29983;&#25104;&#30149;&#29702;&#25551;&#36848;&#65288;&#25918;&#23556;&#22270;&#20687;&#19978;&#30340;&#24322;&#24120;&#21457;&#29616;&#30340;&#25551;&#36848;&#65289;&#65307;iii&#65289;&#20351;&#29992;&#22522;&#20110;BERT&#30340;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#22120;&#65292;&#25214;&#21040;&#27491;&#24120;&#25253;&#21578;&#27169;&#26495;&#20013;&#35201;&#26367;&#25442;&#20026;&#29983;&#25104;&#30340;&#30149;&#29702;&#25551;&#36848;&#30340;&#37096;&#20998;&#65307;iv&#65289;&#20351;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;&#27169;&#22411;&#26469;&#29983;&#25104;&#26368;&#32456;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical practice frequently uses medical imaging for diagnosis and treatment. A significant challenge for automatic radiology report generation is that the radiology reports are long narratives consisting of multiple sentences for both abnormal and normal findings. Therefore, applying conventional image captioning approaches to generate the whole report proves to be insufficient, as these are designed to briefly describe images with short sentences. We propose a template-based approach to generate radiology reports from radiographs. Our approach involves the following: i) using a multilabel image classifier, produce the tags for the input radiograph; ii) using a transformer-based model, generate pathological descriptions (a description of abnormal findings seen on radiographs) from the tags generated in step (i); iii) using a BERT-based multi-label text classifier, find the spans in the normal report template to replace with the generated pathological descriptions; and iv) using a rul
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;ChatGPT&#20316;&#20026;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#30340;&#25968;&#25454;&#26631;&#27880;&#24037;&#20855;&#65292;&#20811;&#26381;&#20102;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#38656;&#35201;&#20154;&#24037;&#26631;&#27880;&#30340;&#38480;&#21046;&#21644;&#22522;&#20110;&#35789;&#20856;&#30340;&#31639;&#27861;&#26080;&#27861;&#25429;&#25417;&#20840;&#37096;&#24773;&#24863;&#33539;&#22260;&#30340;&#32570;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.17177</link><description>&lt;p&gt;
&#20351;&#29992;ChatGPT&#20316;&#20026;&#25991;&#26412;&#27880;&#37322;&#24037;&#20855;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Leveraging ChatGPT As Text Annotation Tool For Sentiment Analysis. (arXiv:2306.17177v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;ChatGPT&#20316;&#20026;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#30340;&#25968;&#25454;&#26631;&#27880;&#24037;&#20855;&#65292;&#20811;&#26381;&#20102;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#38656;&#35201;&#20154;&#24037;&#26631;&#27880;&#30340;&#38480;&#21046;&#21644;&#22522;&#20110;&#35789;&#20856;&#30340;&#31639;&#27861;&#26080;&#27861;&#25429;&#25417;&#20840;&#37096;&#24773;&#24863;&#33539;&#22260;&#30340;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#20998;&#26512;&#26159;&#19968;&#39033;&#20247;&#25152;&#21608;&#30693;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#28041;&#21450;&#35782;&#21035;&#32473;&#23450;&#25991;&#26412;&#30340;&#24773;&#24863;&#33394;&#24425;&#25110;&#26497;&#24615;&#12290;&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#21644;&#20854;&#20182;&#22312;&#32447;&#24179;&#21488;&#30340;&#22686;&#38271;&#65292;&#24773;&#24863;&#20998;&#26512;&#23545;&#20110;&#23547;&#27714;&#30417;&#25511;&#21644;&#29702;&#35299;&#23458;&#25143;&#21453;&#39304;&#21644;&#24847;&#35265;&#30340;&#20225;&#19994;&#21644;&#32452;&#32455;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#24050;&#32463;&#24191;&#27867;&#29992;&#20110;&#36825;&#20010;&#20219;&#21153;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#20154;&#24037;&#26631;&#27880;&#30340;&#25991;&#26412;&#26469;&#21019;&#24314;&#20998;&#31867;&#22120;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#20351;&#29992;&#20102;&#22522;&#20110;&#35789;&#20856;&#30340;&#24037;&#20855;&#12290;&#22522;&#20110;&#35789;&#20856;&#30340;&#31639;&#27861;&#30340;&#19968;&#20010;&#32570;&#28857;&#26159;&#23427;&#20204;&#20381;&#36182;&#20110;&#39044;&#23450;&#20041;&#30340;&#24773;&#24863;&#35789;&#20856;&#65292;&#36825;&#21487;&#33021;&#26080;&#27861;&#25429;&#25417;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#25152;&#26377;&#24773;&#24863;&#33539;&#22260;&#12290;ChatGPT&#26159;OpenAI&#30340;&#26032;&#20135;&#21697;&#65292;&#24050;&#25104;&#20026;&#26368;&#21463;&#27426;&#36814;&#30340;&#20154;&#24037;&#26234;&#33021;&#20135;&#21697;&#20043;&#19968;&#12290;&#23427;&#21487;&#20197;&#22238;&#31572;&#21508;&#31181;&#20027;&#39064;&#21644;&#20219;&#21153;&#30340;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;ChatGPT&#20316;&#20026;&#19981;&#21516;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#30340;&#25968;&#25454;&#26631;&#27880;&#24037;&#20855;&#12290;&#22312;&#20004;&#20010;&#23454;&#39564;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentiment analysis is a well-known natural language processing task that involves identifying the emotional tone or polarity of a given piece of text. With the growth of social media and other online platforms, sentiment analysis has become increasingly crucial for businesses and organizations seeking to monitor and comprehend customer feedback as well as opinions. Supervised learning algorithms have been popularly employed for this task, but they require human-annotated text to create the classifier. To overcome this challenge, lexicon-based tools have been used. A drawback of lexicon-based algorithms is their reliance on pre-defined sentiment lexicons, which may not capture the full range of sentiments in natural language. ChatGPT is a new product of OpenAI and has emerged as the most popular AI product. It can answer questions on various topics and tasks. This study explores the use of ChatGPT as a tool for data labeling for different sentiment analysis tasks. It is evaluated on two
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#23454;&#39564;&#35780;&#20272;&#20102;ChatGPT 3.5&#12289;ChatGPT 4.0&#12289;Bing AI&#21644;Bard&#22312;&#26032;&#38395;&#20107;&#23454;&#26816;&#26597;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#23427;&#20204;&#30340;&#29087;&#32451;&#31243;&#24230;&#26222;&#36941;&#23621;&#20013;&#65292;&#20854;&#20013;OpenAI&#30340;GPT-4.0&#22312;&#21306;&#20998;&#30495;&#30456;&#21644;&#27450;&#39575;&#26041;&#38754;&#20855;&#26377;&#19968;&#23450;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2306.17176</link><description>&lt;p&gt;
&#26032;&#38395;&#39564;&#35777;&#32773;&#30340;&#23545;&#20915;&#65306;ChatGPT 3.5&#12289;ChatGPT 4.0&#12289;Bing AI&#21644;Bard&#22312;&#26032;&#38395;&#20107;&#23454;&#26816;&#26597;&#20013;&#30340;&#27604;&#36739;&#24615;&#33021;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
News Verifiers Showdown: A Comparative Performance Evaluation of ChatGPT 3.5, ChatGPT 4.0, Bing AI, and Bard in News Fact-Checking. (arXiv:2306.17176v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#23454;&#39564;&#35780;&#20272;&#20102;ChatGPT 3.5&#12289;ChatGPT 4.0&#12289;Bing AI&#21644;Bard&#22312;&#26032;&#38395;&#20107;&#23454;&#26816;&#26597;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#23427;&#20204;&#30340;&#29087;&#32451;&#31243;&#24230;&#26222;&#36941;&#23621;&#20013;&#65292;&#20854;&#20013;OpenAI&#30340;GPT-4.0&#22312;&#21306;&#20998;&#30495;&#30456;&#21644;&#27450;&#39575;&#26041;&#38754;&#20855;&#26377;&#19968;&#23450;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#30693;&#21517;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21253;&#25324;OpenAI&#30340;ChatGPT 3.5&#21644;4.0&#12289;&#35895;&#27468;&#30340;Bard&#65288;LaMDA&#65289;&#21644;&#24494;&#36719;&#30340;Bing AI&#65292;&#22312;&#20351;&#29992;&#40657;&#30418;&#27979;&#35797;&#21306;&#20998;&#26032;&#38395;&#30495;&#23454;&#24615;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#24635;&#20849;&#25552;&#20379;&#20102;100&#26465;&#32463;&#36807;&#20107;&#23454;&#26680;&#26597;&#30340;&#26032;&#38395;&#65292;&#25152;&#26377;&#26032;&#38395;&#22343;&#26469;&#33258;&#29420;&#31435;&#30340;&#20107;&#23454;&#26680;&#26597;&#26426;&#26500;&#65292;&#22312;&#21463;&#25511;&#26465;&#20214;&#19979;&#21521;&#27599;&#20010;LLMs&#25552;&#20379;&#12290;&#23427;&#20204;&#30340;&#22238;&#31572;&#34987;&#24402;&#31867;&#20026;&#19977;&#31867;&#65306;&#30495;&#23454;&#12289;&#38169;&#35823;&#21644;&#37096;&#20998;&#30495;&#23454;/&#38169;&#35823;&#12290;&#22522;&#20110;&#29420;&#31435;&#26426;&#26500;&#25552;&#20379;&#30340;&#26680;&#23454;&#20107;&#23454;&#65292;&#35780;&#20272;&#20102;LLMs&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#26469;&#34913;&#37327;&#20854;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25152;&#26377;&#27169;&#22411;&#30340;&#29087;&#32451;&#31243;&#24230;&#37117;&#23646;&#20110;&#20013;&#31561;&#27700;&#24179;&#65292;&#24179;&#22343;&#24471;&#20998;&#20026;65.25/100&#12290;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#65292;OpenAI&#30340;GPT-4.0&#20197;71&#20998;&#30340;&#24471;&#20998;&#33073;&#39062;&#32780;&#20986;&#65292;&#34920;&#26126;&#36739;&#26032;&#30340;LLMs&#22312;&#21306;&#20998;&#30495;&#30456;&#21644;&#27450;&#39575;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#19982;&#20154;&#31867;&#20107;&#23454;&#26680;&#26597;&#21592;&#30340;&#34920;&#29616;&#30456;&#27604;&#65292;&#23613;&#31649;AI&#27169;&#22411;&#34920;&#29616;&#20986;&#19968;&#23450;&#30340;&#29087;&#32451;&#24230;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study aimed to evaluate the proficiency of prominent Large Language Models (LLMs), namely OpenAI's ChatGPT 3.5 and 4.0, Google's Bard(LaMDA), and Microsoft's Bing AI in discerning the truthfulness of news items using black box testing. A total of 100 fact-checked news items, all sourced from independent fact-checking agencies, were presented to each of these LLMs under controlled conditions. Their responses were classified into one of three categories: True, False, and Partially True/False. The effectiveness of the LLMs was gauged based on the accuracy of their classifications against the verified facts provided by the independent agencies. The results showed a moderate proficiency across all models, with an average score of 65.25 out of 100. Among the models, OpenAI's GPT-4.0 stood out with a score of 71, suggesting an edge in newer LLMs' abilities to differentiate fact from deception. However, when juxtaposed against the performance of human fact-checkers, the AI models, despite
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#21407;&#22987;GP&#31508;&#35760;&#20013;&#25552;&#21462;&#20449;&#24687;&#24182;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#20020;&#24202;&#20915;&#31574;&#36807;&#31243;&#20013;&#29616;&#26377;&#25216;&#26415;&#26080;&#27861;&#22788;&#29702;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.17175</link><description>&lt;p&gt;
&#20174;&#21407;&#22987;&#30340;GP&#31508;&#35760;&#20013;&#25366;&#25496;&#30693;&#35782;&#22270;&#35889;&#65292;&#29992;&#20110;&#36828;&#31243;COVID-19&#21021;&#32423;&#20445;&#20581;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
RECAP-KG: Mining Knowledge Graphs from Raw GP Notes for Remote COVID-19 Assessment in Primary Care. (arXiv:2306.17175v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#21407;&#22987;GP&#31508;&#35760;&#20013;&#25552;&#21462;&#20449;&#24687;&#24182;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#20020;&#24202;&#20915;&#31574;&#36807;&#31243;&#20013;&#29616;&#26377;&#25216;&#26415;&#26080;&#27861;&#22788;&#29702;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#20915;&#31574;&#26159;&#21521;&#24739;&#32773;&#25552;&#20379;&#36866;&#24403;&#25252;&#29702;&#30340;&#22522;&#26412;&#38454;&#27573;&#12290;&#36817;&#24180;&#26469;&#65292;&#20026;&#20102;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#20570;&#20986;&#20915;&#31574;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#20960;&#20010;&#20915;&#31574;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20351;&#29992;&#30340;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#22522;&#20110;&#31616;&#21333;&#30340;&#22238;&#24402;&#27169;&#22411;&#65292;&#21482;&#33021;&#32771;&#34385;&#31616;&#21333;&#30340;&#39044;&#23450;&#20041;&#22810;&#36873;&#29305;&#24449;&#65292;&#22914;&#24739;&#32773;&#24180;&#40836;&#12289;&#26082;&#24448;&#30149;&#21490;&#12289;&#21560;&#28895;&#32773;&#29366;&#20917;&#31561;&#12290;&#20915;&#31574;&#31995;&#32479;&#24403;&#21069;&#26080;&#27861;&#22788;&#29702;&#30340;&#19968;&#20010;&#29305;&#23450;&#24739;&#32773;&#25968;&#25454;&#26469;&#28304;&#26159;&#24739;&#32773;&#20250;&#35786;&#30340;GP&#31508;&#35760;&#30340;&#25910;&#38598;&#12290;&#36825;&#20123;&#31508;&#35760;&#21253;&#21547;&#20102;&#20020;&#24202;&#21307;&#29983;&#29992;&#26469;&#20570;&#20986;&#26368;&#32456;&#20915;&#31574;&#24182;&#23558;&#24739;&#32773;&#24341;&#23548;&#21040;&#36866;&#24403;&#25252;&#29702;&#30340;&#20851;&#38190;&#20307;&#24449;&#21644;&#30151;&#29366;&#12290;&#20174;GP&#31508;&#35760;&#20013;&#25552;&#21462;&#20449;&#24687;&#26159;&#19968;&#20010;&#25216;&#26415;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#24448;&#24448;&#21253;&#21547;&#32553;&#20889;&#12289;&#25171;&#23383;&#38169;&#35823;&#21644;&#19981;&#23436;&#25972;&#30340;&#21477;&#23376;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#36825;&#20010;&#20844;&#24320;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#25191;&#34892;&#20174;&#21407;&#22987;GP&#31508;&#35760;&#20013;&#25552;&#21462;&#20986;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical decision-making is a fundamental stage in delivering appropriate care to patients. In recent years several decision-making systems designed to aid the clinician in this process have been developed. However, technical solutions currently in use are based on simple regression models and are only able to take into account simple pre-defined multiple-choice features, such as patient age, pre-existing conditions, smoker status, etc. One particular source of patient data, that available decision-making systems are incapable of processing is the collection of patient consultation GP notes. These contain crucial signs and symptoms - the information used by clinicians in order to make a final decision and direct the patient to the appropriate care. Extracting information from GP notes is a technically challenging problem, as they tend to include abbreviations, typos, and incomplete sentences.  This paper addresses this open challenge. We present a framework that performs knowledge grap
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22312;&#32447;&#39046;&#22495;&#29983;&#25104;&#38750;&#27491;&#24335;&#25688;&#35201;&#65292;&#24182;&#36890;&#36807;&#35442;&#26041;&#27861;&#22312;&#29992;&#25143;&#20307;&#39564;&#21644;&#36127;&#36733;&#20943;&#36731;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.17174</link><description>&lt;p&gt;
&#22312;&#22312;&#32447;&#39046;&#22495;&#20013;&#24378;&#21270;&#31163;&#32447;&#23398;&#20064;&#30340;&#26041;&#27861;&#65306;&#22686;&#24378;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#31639;&#27861;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Empowering NLG: Offline Reinforcement Learning for Informal Summarization in Online Domains. (arXiv:2306.17174v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17174
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22312;&#32447;&#39046;&#22495;&#29983;&#25104;&#38750;&#27491;&#24335;&#25688;&#35201;&#65292;&#24182;&#36890;&#36807;&#35442;&#26041;&#27861;&#22312;&#29992;&#25143;&#20307;&#39564;&#21644;&#36127;&#36733;&#20943;&#36731;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#26041;&#27861;&#65292;&#26088;&#22312;&#20248;&#21270;&#29992;&#25143;&#20307;&#39564;&#24182;&#20943;&#36731;&#20154;&#24037;&#23458;&#26381;&#20195;&#29702;&#30340;&#24037;&#20316;&#36127;&#25285;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#20026;&#22312;&#32447;&#25991;&#31456;&#21644;&#24086;&#23376;&#29983;&#25104;&#38750;&#27491;&#24335;&#25688;&#35201;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#29616;&#26377;&#30340;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#20840;&#38754;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;&#26550;&#26500;&#35774;&#35745;&#65292;&#21253;&#25324;&#29228;&#34411;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#25991;&#26412;&#29983;&#25104;&#27169;&#22359;&#12290;&#36890;&#36807;&#25552;&#20986;&#36825;&#31181;&#21407;&#21019;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#20026;NLG&#39046;&#22495;&#20316;&#20986;&#20102;&#26377;&#20215;&#20540;&#30340;&#36129;&#29486;&#65292;&#20026;&#22312;&#32447;&#20869;&#23481;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#25688;&#35201;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;&#36890;&#36807;&#23454;&#26045;&#8220;&#22686;&#24378;NLG&#8221;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#22312;&#32447;&#39046;&#22495;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#22238;&#22797;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#24179;&#22343;&#8220;&#21916;&#27426;&#8221;&#35780;&#20998;&#26174;&#33879;&#25552;&#39640;&#65292;&#20174;0.09954378&#22686;&#21152;&#21040;0.5000152&#12290;
&lt;/p&gt;
&lt;p&gt;
Our research introduces an innovative Natural Language Generation (NLG) approach that aims to optimize user experience and alleviate the workload of human customer support agents. Our primary objective is to generate informal summaries for online articles and posts using an offline reinforcement learning technique. In our study, we compare our proposed method with existing approaches to text generation and provide a comprehensive overview of our architectural design, which incorporates crawling, reinforcement learning, and text generation modules. By presenting this original approach, our paper makes a valuable contribution to the field of NLG by offering a fresh perspective on generating natural language summaries for online content. Through the implementation of Empowering NLG, we are able to generate higher-quality replies in the online domain. The experimental results demonstrate a significant improvement in the average "like" score, increasing from 0.09954378 to 0.5000152. This ad
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#31995;&#32479;&#35780;&#20272;&#20102;ChatGPT&#12289;GPT-4&#21644;&#20154;&#31867;&#23548;&#24072;&#22312;&#19981;&#21516;&#30340;&#32534;&#31243;&#25945;&#32946;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;GPT-4&#20248;&#20110;ChatGPT&#65292;&#25509;&#36817;&#20110;&#20154;&#31867;&#23548;&#24072;&#12290;</title><link>http://arxiv.org/abs/2306.17156</link><description>&lt;p&gt;
&#32534;&#31243;&#25945;&#32946;&#30340;&#29983;&#25104;AI&#65306;&#27604;&#36739;ChatGPT&#12289;GPT-4&#21644;&#20154;&#31867;&#23548;&#24072;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Generative AI for Programming Education: Benchmarking ChatGPT, GPT-4, and Human Tutors. (arXiv:2306.17156v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17156
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#31995;&#32479;&#35780;&#20272;&#20102;ChatGPT&#12289;GPT-4&#21644;&#20154;&#31867;&#23548;&#24072;&#22312;&#19981;&#21516;&#30340;&#32534;&#31243;&#25945;&#32946;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;GPT-4&#20248;&#20110;ChatGPT&#65292;&#25509;&#36817;&#20110;&#20154;&#31867;&#23548;&#24072;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;AI&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#39640;&#35745;&#31639;&#26426;&#25945;&#32946;&#26041;&#38754;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#20026;&#21021;&#32423;&#32534;&#31243;&#25552;&#20379;&#19979;&#19968;&#20195;&#25945;&#32946;&#25216;&#26415;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#30740;&#31350;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#19982;&#32534;&#31243;&#25945;&#32946;&#30456;&#20851;&#30340;&#19981;&#21516;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65307;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#30001;&#20110;&#22810;&#31181;&#21407;&#22240;&#32780;&#21463;&#38480;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#32771;&#34385;&#30340;&#26159;&#24050;&#32463;&#36807;&#26102;&#30340;&#27169;&#22411;&#25110;&#20165;&#20165;&#29305;&#23450;&#30340;&#22330;&#26223;&#12290;&#22240;&#27492;&#65292;&#32570;&#20047;&#19968;&#20010;&#31995;&#32479;&#30340;&#30740;&#31350;&#26469;&#23545;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#30340;&#32534;&#31243;&#25945;&#32946;&#22330;&#26223;&#22522;&#20934;&#27979;&#35797;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#20004;&#20010;&#27169;&#22411;&#65292;ChatGPT&#65288;&#22522;&#20110;GPT-3.5&#65289;&#21644;GPT-4&#65292;&#24182;&#23558;&#20854;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#19982;&#20154;&#31867;&#23548;&#24072;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#20351;&#29992;&#20116;&#20010;&#21021;&#32423;Python&#32534;&#31243;&#38382;&#39064;&#21644;&#26469;&#33258;&#22312;&#32447;&#24179;&#21488;&#30340;&#30495;&#23454;&#38169;&#35823;&#31243;&#24207;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#20351;&#29992;&#19987;&#23478;&#35780;&#27880;&#26469;&#35780;&#20272;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-4&#26126;&#26174;&#20248;&#20110;ChatGPT&#65288;&#22522;&#20110;GPT-3.5&#65289;&#65292;&#24182;&#19988;&#25509;&#36817;&#20110;&#20154;&#31867;&#23548;&#24072;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI and large language models hold great promise in enhancing computing education by powering next-generation educational technologies for introductory programming. Recent works have studied these models for different scenarios relevant to programming education; however, these works are limited for several reasons, as they typically consider already outdated models or only specific scenario(s). Consequently, there is a lack of a systematic study that benchmarks state-of-the-art models for a comprehensive set of programming education scenarios. In our work, we systematically evaluate two models, ChatGPT (based on GPT-3.5) and GPT-4, and compare their performance with human tutors for a variety of scenarios. We evaluate using five introductory Python programming problems and real-world buggy programs from an online platform, and assess performance using expert-based annotations. Our results show that GPT-4 drastically outperforms ChatGPT (based on GPT-3.5) and comes close to hu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#23450;&#37327;&#25991;&#26412;&#21644;&#32593;&#32476;&#20998;&#26512;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#26368;&#22823;&#30340;&#40657;&#27934;incels&#35770;&#22363;&#22914;&#20309;&#35752;&#35770;&#36523;&#20221;&#32676;&#20307;&#12290;&#30740;&#31350;&#21457;&#29616;&#35813;&#31038;&#21306;&#20135;&#29983;&#20102;&#35768;&#22810;&#26032;&#30340;&#36523;&#20221;&#26415;&#35821;&#65292;&#23384;&#22312;&#29289;&#36136;&#20027;&#20041;&#30340;&#24847;&#35782;&#24418;&#24577;&#12290;&#23545;&#27492;&#25105;&#20204;&#35752;&#35770;&#20102;&#23545;&#33258;&#21160;&#21270; misogynist hate speech &#26816;&#27979;&#30740;&#31350;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.15745</link><description>&lt;p&gt;
&#12298;&#22312;&#19968;&#20010;&#23545;&#22899;&#24615;&#21388;&#24694;&#30340;incels&#35770;&#22363;&#20013;&#30340;&#36523;&#20221;&#24314;&#26500;&#12299;
&lt;/p&gt;
&lt;p&gt;
Identity Construction in a Misogynist Incels Forum. (arXiv:2306.15745v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15745
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#23450;&#37327;&#25991;&#26412;&#21644;&#32593;&#32476;&#20998;&#26512;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#26368;&#22823;&#30340;&#40657;&#27934;incels&#35770;&#22363;&#22914;&#20309;&#35752;&#35770;&#36523;&#20221;&#32676;&#20307;&#12290;&#30740;&#31350;&#21457;&#29616;&#35813;&#31038;&#21306;&#20135;&#29983;&#20102;&#35768;&#22810;&#26032;&#30340;&#36523;&#20221;&#26415;&#35821;&#65292;&#23384;&#22312;&#29289;&#36136;&#20027;&#20041;&#30340;&#24847;&#35782;&#24418;&#24577;&#12290;&#23545;&#27492;&#25105;&#20204;&#35752;&#35770;&#20102;&#23545;&#33258;&#21160;&#21270; misogynist hate speech &#26816;&#27979;&#30740;&#31350;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#23450;&#37327;&#25991;&#26412;&#21644;&#32593;&#32476;&#20998;&#26512;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;incels.is&#65292;&#21363;&#26368;&#22823;&#30340;&#40657;&#27934;incels&#35770;&#22363;&#22914;&#20309;&#35752;&#35770;&#36523;&#20221;&#32676;&#20307;&#12290;&#25105;&#20204;&#21457;&#29616;&#35813;&#31038;&#21306;&#20135;&#29983;&#20102;&#35768;&#22810;&#26032;&#30340;&#36523;&#20221;&#26415;&#35821;&#65292;&#23613;&#31649;&#22899;&#24615;&#30340;&#26415;&#35821;&#26368;&#24120;&#35265;&#65292;&#20294;&#20854;&#20182;&#23569;&#25968;&#32676;&#20307;&#30340;&#25552;&#21450;&#20063;&#22312;&#22686;&#21152;&#12290;&#23545;&#36523;&#20221;&#32676;&#20307;&#30340;&#20851;&#32852;&#20998;&#26512;&#34920;&#26126;&#65292;&#36825;&#20010;&#31038;&#21306;&#23384;&#22312;&#30528;&#29289;&#36136;&#20027;&#20041;&#30340;&#24847;&#35782;&#24418;&#24577;&#65292;&#20854;&#20013;&#36523;&#20307;&#22806;&#35980;&#12289;&#24615;&#21035;&#21644;&#31181;&#26063;&#31561;&#20915;&#23450;&#20102;&#20154;&#30340;&#20215;&#20540;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#23545;&#33258;&#21160;&#21270; misogynist hate speech &#26816;&#27979;&#30740;&#31350;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online communities of involuntary celibates (incels) are a prominent source of misogynist hate speech. In this paper, we use quantitative text and network analysis approaches to examine how identity groups are discussed on incels.is, the largest black-pilled incels forum. We find that this community produces a wide range of novel identity terms and, while terms for women are most common, mentions of other minoritized identities are increasing. An analysis of the associations made with identity groups suggests an essentialist ideology where physical appearance, as well as gender and racial hierarchies, determine human value. We discuss implications for research into automated misogynist hate speech detection.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24067;&#23616;&#21644;&#20219;&#21153;&#24863;&#30693;&#30340;&#25351;&#23548;&#25552;&#31034;&#27169;&#22411;&#65292;&#31216;&#20026;LATIN-Prompt&#65292;&#36890;&#36807;&#23558;&#25991;&#26723;&#22270;&#20687;&#38382;&#31572;&#23545;&#40784;&#21040;&#29616;&#25104;&#30340;&#25351;&#23548;&#35843;&#20248;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65292;&#21033;&#29992;&#20854;&#38646;&#26679;&#26412;&#33021;&#21147;&#26469;&#25552;&#39640;&#25928;&#26524;&#12290;&#35813;&#27169;&#22411;&#21253;&#25324;&#24067;&#23616;&#24863;&#30693;&#30340;&#25991;&#26723;&#20869;&#23481;&#21644;&#20219;&#21153;&#24863;&#30693;&#30340;&#25551;&#36848;&#65292;&#33021;&#22815;&#24674;&#22797;&#25991;&#26412;&#29255;&#27573;&#20043;&#38388;&#30340;&#24067;&#23616;&#20449;&#24687;&#65292;&#24182;&#29983;&#25104;&#31526;&#21512;&#20219;&#21153;&#38656;&#27714;&#30340;&#31572;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.00526</link><description>&lt;p&gt;
&#24067;&#23616;&#21644;&#20219;&#21153;&#24863;&#30693;&#30340;&#38646;&#26679;&#26412;&#25991;&#26723;&#22270;&#20687;&#38382;&#31572;&#25351;&#23548;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Layout and Task Aware Instruction Prompt for Zero-shot Document Image Question Answering. (arXiv:2306.00526v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00526
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24067;&#23616;&#21644;&#20219;&#21153;&#24863;&#30693;&#30340;&#25351;&#23548;&#25552;&#31034;&#27169;&#22411;&#65292;&#31216;&#20026;LATIN-Prompt&#65292;&#36890;&#36807;&#23558;&#25991;&#26723;&#22270;&#20687;&#38382;&#31572;&#23545;&#40784;&#21040;&#29616;&#25104;&#30340;&#25351;&#23548;&#35843;&#20248;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65292;&#21033;&#29992;&#20854;&#38646;&#26679;&#26412;&#33021;&#21147;&#26469;&#25552;&#39640;&#25928;&#26524;&#12290;&#35813;&#27169;&#22411;&#21253;&#25324;&#24067;&#23616;&#24863;&#30693;&#30340;&#25991;&#26723;&#20869;&#23481;&#21644;&#20219;&#21153;&#24863;&#30693;&#30340;&#25551;&#36848;&#65292;&#33021;&#22815;&#24674;&#22797;&#25991;&#26412;&#29255;&#27573;&#20043;&#38388;&#30340;&#24067;&#23616;&#20449;&#24687;&#65292;&#24182;&#29983;&#25104;&#31526;&#21512;&#20219;&#21153;&#38656;&#27714;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24067;&#23616;&#24863;&#30693;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#22312;&#25991;&#26723;&#22270;&#20687;&#38382;&#31572;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#39046;&#22495;&#39044;&#35757;&#32451;&#21644;&#20219;&#21153;&#24494;&#35843;&#23545;&#20110;&#39069;&#22806;&#30340;&#35270;&#35273;&#12289;&#24067;&#23616;&#21644;&#20219;&#21153;&#27169;&#22359;&#38459;&#27490;&#20102;&#20854;&#30452;&#25509;&#21033;&#29992;&#29616;&#25104;&#30340;&#25351;&#23548;&#35843;&#20248;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65292;&#32780;&#36825;&#20123;&#27169;&#22411;&#26368;&#36817;&#22312;&#38646;&#26679;&#26412;&#23398;&#20064;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#28508;&#21147;&#12290;&#19982;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#25991;&#26723;&#22270;&#20687;&#38382;&#31572;&#39046;&#22495;&#23545;&#40784;&#30456;&#21453;&#65292;&#25105;&#20204;&#23558;&#25991;&#26723;&#22270;&#20687;&#38382;&#31572;&#19982;&#29616;&#25104;&#30340;&#25351;&#23548;&#35843;&#20248;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#23545;&#40784;&#65292;&#21033;&#29992;&#20854;&#38646;&#26679;&#26412;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24067;&#23616;&#21644;&#20219;&#21153;&#24863;&#30693;&#30340;&#25351;&#23548;&#25552;&#31034;&#27169;&#22411;&#65292;&#31216;&#20026;LATIN-Prompt&#65292;&#23427;&#21253;&#25324;&#24067;&#23616;&#24863;&#30693;&#30340;&#25991;&#26723;&#20869;&#23481;&#21644;&#20219;&#21153;&#24863;&#30693;&#30340;&#25551;&#36848;&#12290;&#21069;&#32773;&#36890;&#36807;&#36866;&#24403;&#30340;&#31354;&#26684;&#21644;&#25442;&#34892;&#31526;&#20174;OCR&#24037;&#20855;&#20013;&#24674;&#22797;&#25991;&#26412;&#29255;&#27573;&#20043;&#38388;&#30340;&#24067;&#23616;&#20449;&#24687;&#12290;&#21518;&#32773;&#30830;&#20445;&#27169;&#22411;&#29983;&#25104;&#31526;&#21512;&#20219;&#21153;&#38656;&#27714;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The pre-training-fine-tuning paradigm based on layout-aware multimodal pre-trained models has achieved significant progress on document image question answering. However, domain pre-training and task fine-tuning for additional visual, layout, and task modules prevent them from directly utilizing off-the-shelf instruction-tuning language foundation models, which have recently shown promising potential in zero-shot learning. Contrary to aligning language models to the domain of document image question answering, we align document image question answering to off-the-shell instruction-tuning language foundation models to utilize their zero-shot capability. Specifically, we propose layout and task aware instruction prompt called LATIN-Prompt, which consists of layout-aware document content and task-aware descriptions. The former recovers the layout information among text segments from OCR tools by appropriate spaces and line breaks. The latter ensures that the model generates answers that m
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30495;&#23454;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#27979;&#35797;&#26041;&#27861;&#65306;&#21453;&#20107;&#23454;&#36755;&#20837;&#32534;&#36753;&#22120;&#21644;&#37325;&#24314;&#36755;&#20837;&#27979;&#35797;&#12290;&#36825;&#20123;&#27979;&#35797;&#23545;&#20110;&#35780;&#20272;&#26032;&#20852;&#30340;NLE&#27169;&#22411;&#65292;&#23545;&#24320;&#21457;&#30495;&#23454;&#30340;NLEs&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2305.18029</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#30495;&#23454;&#24615;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Faithfulness Tests for Natural Language Explanations. (arXiv:2305.18029v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18029
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30495;&#23454;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#27979;&#35797;&#26041;&#27861;&#65306;&#21453;&#20107;&#23454;&#36755;&#20837;&#32534;&#36753;&#22120;&#21644;&#37325;&#24314;&#36755;&#20837;&#27979;&#35797;&#12290;&#36825;&#20123;&#27979;&#35797;&#23545;&#20110;&#35780;&#20272;&#26032;&#20852;&#30340;NLE&#27169;&#22411;&#65292;&#23545;&#24320;&#21457;&#30495;&#23454;&#30340;NLEs&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#27169;&#22411;&#30340;&#35299;&#37322;&#26088;&#22312;&#25581;&#31034;&#27169;&#22411;&#39044;&#27979;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#35832;&#22914;&#26174;&#33879;&#24615;&#22320;&#22270;&#25110;&#21453;&#20107;&#23454;&#35299;&#37322;&#31561;&#24403;&#21069;&#30340;&#35299;&#37322;&#26041;&#27861;&#21487;&#33021;&#20250;&#35823;&#23548;&#65292;&#22240;&#20026;&#23427;&#20204;&#23481;&#26131;&#21576;&#29616;&#19982;&#27169;&#22411;&#20869;&#37096;&#26426;&#21046;&#19981;&#19968;&#33268;&#30340;&#21407;&#22240;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#65288;NLEs&#65289;&#30495;&#23454;&#24615;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#27979;&#35797;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;&#20107;&#23454;&#36755;&#20837;&#32534;&#36753;&#22120;&#65292;&#29992;&#20110;&#25554;&#20837;&#23548;&#33268;&#21453;&#20107;&#23454;&#39044;&#27979;&#20294;&#19981;&#34987;NLEs&#21453;&#26144;&#30340;&#21407;&#22240;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#26681;&#25454;&#29983;&#25104;&#30340;NLEs&#20013;&#25152;&#36848;&#30340;&#21407;&#22240;&#37325;&#24314;&#36755;&#20837;&#65292;&#24182;&#26816;&#26597;&#23427;&#20204;&#23548;&#33268;&#30456;&#21516;&#39044;&#27979;&#30340;&#39057;&#29575;&#12290;&#25105;&#20204;&#30340;&#27979;&#35797;&#21487;&#20197;&#35780;&#20272;&#26032;&#20852;&#30340;NLE&#27169;&#22411;&#65292;&#20026;&#24320;&#21457;&#30495;&#23454;&#30340;NLEs&#25552;&#20379;&#20102;&#22522;&#26412;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explanations of neural models aim to reveal a model's decision-making process for its predictions. However, recent work shows that current methods giving explanations such as saliency maps or counterfactuals can be misleading, as they are prone to present reasons that are unfaithful to the model's inner workings. This work explores the challenging question of evaluating the faithfulness of natural language explanations (NLEs). To this end, we present two tests. First, we propose a counterfactual input editor for inserting reasons that lead to counterfactual predictions but are not reflected by the NLEs. Second, we reconstruct inputs from the reasons stated in the generated NLEs and check how often they lead to the same predictions. Our tests can evaluate emerging NLE models, proving a fundamental tool in the development of faithful NLEs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#30740;&#31350;&#20102;&#23427;&#20204;&#22914;&#20309;&#36890;&#36807;&#32763;&#35793;&#25351;&#20196;&#25191;&#34892;&#22810;&#35821;&#35328;&#32763;&#35793;&#20219;&#21153;&#12290;&#30740;&#31350;&#21457;&#29616;&#22810;&#35821;&#35328;LLMs&#20855;&#26377;&#36739;&#24378;&#30340;&#32763;&#35793;&#33021;&#21147;&#65292;&#36825;&#21462;&#20915;&#20110;&#35821;&#35328;&#19982;&#33521;&#35821;&#30340;&#30456;&#20284;&#24615;&#21644;&#39044;&#35757;&#32451;&#38454;&#27573;&#20351;&#29992;&#30340;&#25968;&#25454;&#37327;&#12290;&#27492;&#22806;&#65292;&#25191;&#34892;&#32763;&#35793;&#25351;&#20196;&#30340;&#33021;&#21147;&#20381;&#36182;&#20110;&#23545;&#25351;&#20196;&#30340;&#29702;&#35299;&#21644;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2305.15083</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#35821;&#35328;&#24494;&#35843;&#21644;&#32763;&#35793;&#25351;&#20196;&#35825;&#21457;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#32763;&#35793;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Eliciting the Translation Ability of Large Language Models via Multilingual Finetuning with Translation Instructions. (arXiv:2305.15083v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15083
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#30740;&#31350;&#20102;&#23427;&#20204;&#22914;&#20309;&#36890;&#36807;&#32763;&#35793;&#25351;&#20196;&#25191;&#34892;&#22810;&#35821;&#35328;&#32763;&#35793;&#20219;&#21153;&#12290;&#30740;&#31350;&#21457;&#29616;&#22810;&#35821;&#35328;LLMs&#20855;&#26377;&#36739;&#24378;&#30340;&#32763;&#35793;&#33021;&#21147;&#65292;&#36825;&#21462;&#20915;&#20110;&#35821;&#35328;&#19982;&#33521;&#35821;&#30340;&#30456;&#20284;&#24615;&#21644;&#39044;&#35757;&#32451;&#38454;&#27573;&#20351;&#29992;&#30340;&#25968;&#25454;&#37327;&#12290;&#27492;&#22806;&#65292;&#25191;&#34892;&#32763;&#35793;&#25351;&#20196;&#30340;&#33021;&#21147;&#20381;&#36182;&#20110;&#23545;&#25351;&#20196;&#30340;&#29702;&#35299;&#21644;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20363;&#22914;ChatGPT&#21644;GPT4&#65292;&#23637;&#29616;&#20986;&#22312;&#22810;&#35821;&#35328;&#32763;&#35793;&#26041;&#38754;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#26126;&#30830;&#35757;&#32451;&#24182;&#34892;&#35821;&#26009;&#24211;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#22914;&#20309;&#33719;&#24471;&#20854;&#23545;&#19981;&#21516;&#35821;&#35328;&#36827;&#34892;&#32763;&#35793;&#25351;&#20196;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;XGLM-7B&#36827;&#34892;&#24494;&#35843;&#26469;&#25191;&#34892;&#22810;&#35821;&#35328;&#32763;&#35793;&#20219;&#21153;&#65292;&#24182;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22810;&#35821;&#35328;LLMs&#20855;&#26377;&#27604;&#20808;&#21069;&#23637;&#31034;&#30340;&#26356;&#24378;&#30340;&#32763;&#35793;&#33021;&#21147;&#12290;&#23545;&#20110;&#26576;&#31181;&#35821;&#35328;&#65292;&#20854;&#34920;&#29616;&#21462;&#20915;&#20110;&#20854;&#19982;&#33521;&#35821;&#30340;&#30456;&#20284;&#24615;&#21644;&#39044;&#35757;&#32451;&#38454;&#27573;&#20351;&#29992;&#30340;&#25968;&#25454;&#37327;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#25191;&#34892;&#32763;&#35793;&#25351;&#20196;&#30340;&#33021;&#21147;&#20381;&#36182;&#20110;&#23545;&#32763;&#35793;&#25351;&#20196;&#30340;&#29702;&#35299;&#20197;&#21450;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#36890;&#36807;&#22810;&#35821;&#35328;&#24494;&#35843;&#65292;LLMs&#33021;&#22815;&#23398;&#20064;&#24182;&#22312;&#32763;&#35793;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#33021;&#21147;&#65292;&#21363;&#20351;&#23545;&#20110;&#37027;&#20123;&#35821;&#35328;&#38388;&#24179;&#34892;&#35821;&#26009;&#36739;&#23569;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale Pretrained Language Models (LLMs), such as ChatGPT and GPT4, have shown strong abilities in multilingual translations, without being explicitly trained on parallel corpora. It is interesting how the LLMs obtain their ability to carry out translation instructions for different languages. In this paper, we present a detailed analysis by finetuning a multilingual pretrained language model, XGLM-7B, to perform multilingual translation following given instructions. Firstly, we show that multilingual LLMs have stronger translation abilities than previously demonstrated. For a certain language, the performance depends on its similarity to English and the amount of data used in the pretraining phase. Secondly, we find that LLMs' ability to carry out translation instructions relies on the understanding of translation instructions and the alignment among different languages. With multilingual finetuning, LLMs could learn to perform the translation task well even for those language pa
&lt;/p&gt;</description></item><item><title>UPop&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#35270;&#35273;&#35821;&#35328;Transformer&#21387;&#32553;&#26694;&#26550;&#65292;&#37319;&#29992;&#32479;&#19968;&#21644;&#28176;&#36827;&#24335;&#21098;&#26525;&#26041;&#27861;&#65292;&#21487;&#33258;&#21160;&#20998;&#37197;&#21098;&#26525;&#27604;&#29575;&#65292;&#23454;&#29616;&#26356;&#39640;&#30340;&#21387;&#32553;&#27604;&#29575;&#12290;</title><link>http://arxiv.org/abs/2301.13741</link><description>&lt;p&gt;
UPop&#65306;&#29992;&#20110;&#21387;&#32553;&#35270;&#35273;&#35821;&#35328;Transformer&#27169;&#22411;&#30340;&#32479;&#19968;&#21644;&#28176;&#36827;&#24335;&#21098;&#26525;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
UPop: Unified and Progressive Pruning for Compressing Vision-Language Transformers. (arXiv:2301.13741v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13741
&lt;/p&gt;
&lt;p&gt;
UPop&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#35270;&#35273;&#35821;&#35328;Transformer&#21387;&#32553;&#26694;&#26550;&#65292;&#37319;&#29992;&#32479;&#19968;&#21644;&#28176;&#36827;&#24335;&#21098;&#26525;&#26041;&#27861;&#65292;&#21487;&#33258;&#21160;&#20998;&#37197;&#21098;&#26525;&#27604;&#29575;&#65292;&#23454;&#29616;&#26356;&#39640;&#30340;&#21387;&#32553;&#27604;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#21253;&#21547;&#22823;&#37327;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#20854;&#20013;&#35270;&#35273;&#21644;&#35821;&#35328;&#26159;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#20004;&#31181;&#27169;&#24577;&#12290;&#27492;&#22806;&#65292;&#36234;&#26469;&#36234;&#37325;&#30340;&#27169;&#22411;&#65292;&#20363;&#22914;Transformer&#65292;&#24050;&#32463;&#24341;&#36215;&#20102;&#30740;&#31350;&#20154;&#21592;&#23545;&#27169;&#22411;&#21387;&#32553;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#21387;&#32553;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#35270;&#35273;&#35821;&#35328;Transformer&#65292;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UPop&#30340;&#36890;&#29992;&#35270;&#35273;&#35821;&#35328;Transformer&#21387;&#32553;&#26694;&#26550;&#65292;&#23427;&#21253;&#25324;1&#65289;&#22312;&#21407;&#22987;&#27169;&#22411;&#20013;&#22312;&#36830;&#32493;&#20248;&#21270;&#31354;&#38388;&#20013;&#32479;&#19968;&#25628;&#32034;&#22810;&#27169;&#24577;&#23376;&#32593;&#65292;&#20174;&#32780;&#23454;&#29616;&#21487;&#21387;&#32553;&#27169;&#24577;&#21644;&#32467;&#26500;&#20043;&#38388;&#33258;&#21160;&#20998;&#37197;&#21098;&#26525;&#27604;&#29575;&#65307;2&#65289;&#28176;&#36827;&#24335;&#25628;&#32034;&#21644;&#24494;&#35843;&#23376;&#32593;&#65292;&#20174;&#32780;&#20445;&#25345;&#25628;&#32034;&#21644;&#24494;&#35843;&#20043;&#38388;&#30340;&#25910;&#25947;&#65292;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#21387;&#32553;&#27604;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world data contains a vast amount of multimodal information, among which vision and language are the two most representative modalities. Moreover, increasingly heavier models, \textit{e}.\textit{g}., Transformers, have attracted the attention of researchers to model compression. However, how to compress multimodal models, especially vison-language Transformers, is still under-explored. This paper proposes the \textbf{U}nified and \textbf{P}r\textbf{o}gressive \textbf{P}runing (\textbf{\emph{UPop}}) as a universal vison-language Transformer compression framework, which incorporates 1) unifiedly searching multimodal subnets in a continuous optimization space from the original model, which enables automatic assignment of pruning ratios among compressible modalities and structures; 2) progressively searching and retraining the subnet, which maintains convergence between the search and retrain to attain higher compression ratios. Experiments on various tasks, datasets, and model archit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CONVINSE&#65292;&#19968;&#20010;&#29992;&#20110;&#24322;&#26500;&#25968;&#25454;&#28304;&#19978;&#30340;ConvQA&#30340;&#31471;&#21040;&#31471;&#27969;&#27700;&#32447;&#65292;&#36890;&#36807;&#32852;&#21512;&#25552;&#21462;&#26469;&#33258;&#30693;&#35782;&#24211;&#12289;&#25991;&#26412;&#21644;&#34920;&#26684;&#30340;&#20449;&#24687;&#65292;&#25552;&#21319;&#20102;&#31572;&#26696;&#35206;&#30422;&#29575;&#21644;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2204.11677</link><description>&lt;p&gt;
&#24322;&#26500;&#25968;&#25454;&#28304;&#19978;&#30340;&#23545;&#35805;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Conversational Question Answering on Heterogeneous Sources. (arXiv:2204.11677v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.11677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CONVINSE&#65292;&#19968;&#20010;&#29992;&#20110;&#24322;&#26500;&#25968;&#25454;&#28304;&#19978;&#30340;ConvQA&#30340;&#31471;&#21040;&#31471;&#27969;&#27700;&#32447;&#65292;&#36890;&#36807;&#32852;&#21512;&#25552;&#21462;&#26469;&#33258;&#30693;&#35782;&#24211;&#12289;&#25991;&#26412;&#21644;&#34920;&#26684;&#30340;&#20449;&#24687;&#65292;&#25552;&#21319;&#20102;&#31572;&#26696;&#35206;&#30422;&#29575;&#21644;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35805;&#38382;&#31572;(ConvQA)&#35299;&#20915;&#20102;&#39034;&#24207;&#20449;&#24687;&#38656;&#27714;&#20013;&#30340;&#21518;&#32493;&#38382;&#39064;&#20013;&#19978;&#19979;&#25991;&#38544;&#21547;&#30340;&#38382;&#39064;&#12290;&#24403;&#21069;&#30340;ConvQA&#31995;&#32479;&#21482;&#33021;&#22312;&#21516;&#36136;&#21270;&#30340;&#20449;&#24687;&#28304;&#19978;&#25805;&#20316;&#65292;&#22914;&#30693;&#35782;&#24211;(KB)&#12289;&#25991;&#26412;&#35821;&#26009;&#24211;&#25110;&#34920;&#26684;&#38598;&#21512;&#12290;&#26412;&#25991;&#38024;&#23545;&#30340;&#26159;&#22312;&#36825;&#20123;&#24322;&#36136;&#25968;&#25454;&#28304;&#20013;&#32852;&#21512;&#25552;&#21462;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#21319;&#31572;&#26696;&#35206;&#30422;&#29575;&#21644;&#21487;&#20449;&#24230;&#30340;&#26032;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;CONVINSE&#65292;&#19968;&#20010;&#29992;&#20110;&#24322;&#26500;&#25968;&#25454;&#28304;&#19978;&#30340;ConvQA&#30340;&#31471;&#21040;&#31471;&#27969;&#27700;&#32447;&#65292;&#20998;&#20026;&#19977;&#20010;&#38454;&#27573;&#65306;i&#65289;&#23398;&#20064;&#23545;&#20256;&#20837;&#38382;&#39064;&#21450;&#20854;&#23545;&#35805;&#19978;&#19979;&#25991;&#30340;&#26126;&#30830;&#32467;&#26500;&#21270;&#34920;&#31034;&#65292;ii&#65289;&#21033;&#29992;&#36825;&#20010;&#31867;&#20284;&#26694;&#26550;&#30340;&#34920;&#31034;&#26041;&#24335;&#32479;&#19968;&#22320;&#33719;&#21462;&#21040;&#26469;&#33258;KB&#12289;&#25991;&#26412;&#21644;&#34920;&#26684;&#30340;&#30456;&#20851;&#35777;&#25454;&#65292;iii&#65289;&#36816;&#34892;&#34701;&#21512;&#35299;&#30721;&#27169;&#22411;&#26469;&#29983;&#25104;&#31572;&#26696;&#12290;&#25105;&#20204;&#26500;&#24314;&#24182;&#21457;&#24067;&#20102;&#31532;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;ConvMix&#65292;&#29992;&#20110;&#24322;&#26500;&#25968;&#25454;&#28304;&#19978;&#30340;ConvQA&#65292;&#21253;&#25324;3000&#20010;&#30495;&#23454;&#29992;&#25143;&#23545;&#35805;&#21644;16000&#20010;&#38382;&#39064;&#65292;&#20197;&#21450;&#23454;&#20307;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational question answering (ConvQA) tackles sequential information needs where contexts in follow-up questions are left implicit. Current ConvQA systems operate over homogeneous sources of information: either a knowledge base (KB), or a text corpus, or a collection of tables. This paper addresses the novel issue of jointly tapping into all of these together, this way boosting answer coverage and confidence. We present CONVINSE, an end-to-end pipeline for ConvQA over heterogeneous sources, operating in three stages: i) learning an explicit structured representation of an incoming question and its conversational context, ii) harnessing this frame-like representation to uniformly capture relevant evidences from KB, text, and tables, and iii) running a fusion-in-decoder model to generate the answer. We construct and release the first benchmark, ConvMix, for ConvQA over heterogeneous sources, comprising 3000 real-user conversations with 16000 questions, along with entity annotations,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;GEEP&#65292;&#29992;&#20110;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#21035;&#20844;&#24179;&#24615;&#65292;&#21516;&#26102;&#27809;&#26377;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#36879;&#36807;&#24615;&#21035;&#20013;&#24615;&#25968;&#25454;&#23398;&#20064;&#24615;&#21035;&#30456;&#20851;&#30340;&#25552;&#31034;&#65292;GEEP&#23454;&#29616;&#20102;SOTA&#34920;&#29616;&#24182;&#22312;GLUE&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2110.05367</link><description>&lt;p&gt;
&#22312;&#19981;&#20135;&#29983;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#21035;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Improving Gender Fairness of Pre-Trained Language Models without Catastrophic Forgetting. (arXiv:2110.05367v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.05367
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;GEEP&#65292;&#29992;&#20110;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#21035;&#20844;&#24179;&#24615;&#65292;&#21516;&#26102;&#27809;&#26377;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#36879;&#36807;&#24615;&#21035;&#20013;&#24615;&#25968;&#25454;&#23398;&#20064;&#24615;&#21035;&#30456;&#20851;&#30340;&#25552;&#31034;&#65292;GEEP&#23454;&#29616;&#20102;SOTA&#34920;&#29616;&#24182;&#22312;GLUE&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#35299;&#20915;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24615;&#21035;&#20559;&#35265;&#30340;&#30740;&#31350;&#36890;&#24120;&#24314;&#31435;&#19968;&#20010;&#23567;&#22411;&#30340;&#24615;&#21035;&#20013;&#24615;&#25968;&#25454;&#38598;&#65292;&#28982;&#21518;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#31532;&#20108;&#38454;&#27573;&#30340;&#39044;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#37492;&#20110;&#24615;&#21035;&#20013;&#24615;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#26377;&#38480;&#19988;&#38598;&#20013;&#20851;&#27880;&#65292;&#31532;&#20108;&#38454;&#27573;&#39044;&#35757;&#32451;&#20250;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#24536;&#35760;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20449;&#24687;&#21487;&#33021;&#20250;&#20005;&#37325;&#25439;&#23475;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;GLUE&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#23454;&#35777;&#22320;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#20013;&#20250;&#21457;&#29983;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;GEnder Equality Prompt (GEEP)&#65292;&#20197;&#25913;&#21892;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#21035;&#20844;&#24179;&#24615;&#65292;&#19988;&#36951;&#24536;&#36739;&#23569;&#12290; GEEP&#20250;&#20923;&#32467;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#24615;&#21035;&#20013;&#24615;&#25968;&#25454;&#23398;&#20064;&#19982;&#24615;&#21035;&#30456;&#20851;&#30340;&#25552;&#31034;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#65292;GEEP&#19981;&#20165;&#22312;&#24615;&#21035;&#20844;&#24179;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;SOTA&#34920;&#29616;&#65292;&#32780;&#19988;&#22312;GLUE&#19978;&#36951;&#24536;&#36739;&#23569;&#65292;&#24182;&#21462;&#24471;&#20102;&#26126;&#26174;&#30340;&#24615;&#33021;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing studies addressing gender bias of pre-trained language models, usually build a small gender-neutral data set and conduct a second phase pre-training on the model with such data. However, given the limited size and concentrated focus of the gender-neutral data, catastrophic forgetting would occur during second-phase pre-training. Forgetting information in the original training data may damage the model's downstream performance by a large margin. In this work, we empirically show that catastrophic forgetting occurs in such methods by evaluating them with general NLP tasks in GLUE. Then, we propose a new method, GEnder Equality Prompt (GEEP), to improve gender fairness of pre-trained models with less forgetting. GEEP freezes the pre-trained model and learns gender-related prompts with gender-neutral data. Empirical results show that GEEP not only achieves SOTA performances on gender fairness tasks, but also forgets less and performs better on GLUE by a large margin.
&lt;/p&gt;</description></item></channel></rss>