<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#20107;&#23454;&#24615;&#38382;&#39064;&#65292;&#21253;&#25324;&#20854;&#21487;&#33021;&#24102;&#26469;&#30340;&#21518;&#26524;&#21644;&#25361;&#25112;&#65292;&#20197;&#21450;&#23384;&#20648;&#12289;&#22788;&#29702;&#21644;&#35780;&#20272;&#20107;&#23454;&#30340;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#20123;&#22686;&#24378;LLM&#20107;&#23454;&#24615;&#30340;&#31574;&#30053;&#65292;&#28085;&#30422;&#20102;&#19981;&#21516;&#39046;&#22495;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.07521</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20107;&#23454;&#24615;&#35843;&#26597;&#65306;&#30693;&#35782;&#12289;&#26816;&#32034;&#21644;&#39046;&#22495;&#19987;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity. (arXiv:2310.07521v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07521
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#20107;&#23454;&#24615;&#38382;&#39064;&#65292;&#21253;&#25324;&#20854;&#21487;&#33021;&#24102;&#26469;&#30340;&#21518;&#26524;&#21644;&#25361;&#25112;&#65292;&#20197;&#21450;&#23384;&#20648;&#12289;&#22788;&#29702;&#21644;&#35780;&#20272;&#20107;&#23454;&#30340;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#20123;&#22686;&#24378;LLM&#20107;&#23454;&#24615;&#30340;&#31574;&#30053;&#65292;&#28085;&#30422;&#20102;&#19981;&#21516;&#39046;&#22495;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#21363;&#20107;&#23454;&#24615;&#12290;&#30001;&#20110;LLMs&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#37117;&#26377;&#24212;&#29992;&#65292;&#23427;&#20204;&#30340;&#36755;&#20986;&#30340;&#21487;&#38752;&#24615;&#21644;&#20934;&#30830;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#23558;&#20107;&#23454;&#24615;&#38382;&#39064;&#23450;&#20041;&#20026;LLMs&#20135;&#29983;&#19982;&#24050;&#30830;&#31435;&#20107;&#23454;&#19981;&#19968;&#33268;&#30340;&#20869;&#23481;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#39318;&#20808;&#28145;&#20837;&#25506;&#35752;&#20102;&#36825;&#20123;&#19981;&#20934;&#30830;&#24615;&#30340;&#21547;&#20041;&#65292;&#31361;&#20986;&#20102;&#20107;&#23454;&#38169;&#35823;&#22312;LLMs&#36755;&#20986;&#20013;&#21487;&#33021;&#24102;&#26469;&#30340;&#28508;&#22312;&#21518;&#26524;&#21644;&#25361;&#25112;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;LLMs&#23384;&#20648;&#21644;&#22788;&#29702;&#20107;&#23454;&#30340;&#26426;&#21046;&#65292;&#23547;&#25214;&#20107;&#23454;&#38169;&#35823;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#25105;&#20204;&#30340;&#35752;&#35770;&#38543;&#21518;&#36716;&#21521;&#35780;&#20272;LLM&#20107;&#23454;&#24615;&#30340;&#26041;&#27861;&#35770;&#65292;&#24378;&#35843;&#20851;&#38190;&#25351;&#26631;&#12289;&#22522;&#20934;&#21644;&#30740;&#31350;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#22686;&#24378;LLM&#20107;&#23454;&#24615;&#30340;&#31574;&#30053;&#65292;&#21253;&#25324;&#38024;&#23545;&#29305;&#23450;&#39046;&#22495;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#20004;&#31181;&#20027;&#35201;&#30340;LLM&#37197;&#32622;&#65292;&#29420;&#31435;LLMs&#21644;&#21033;&#29992;&#22806;&#37096;&#25968;&#25454;&#30340;&#26816;&#32034;&#22686;&#24378;LLMs&#65292;&#35814;&#32454;&#20171;&#32461;&#23427;&#20204;&#30340;&#29420;&#29305;&#25361;&#25112;&#21644;&#28508;&#22312;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This survey addresses the crucial issue of factuality in Large Language Models (LLMs). As LLMs find applications across diverse domains, the reliability and accuracy of their outputs become vital. We define the Factuality Issue as the probability of LLMs to produce content inconsistent with established facts. We first delve into the implications of these inaccuracies, highlighting the potential consequences and challenges posed by factual errors in LLM outputs. Subsequently, we analyze the mechanisms through which LLMs store and process facts, seeking the primary causes of factual errors. Our discussion then transitions to methodologies for evaluating LLM factuality, emphasizing key metrics, benchmarks, and studies. We further explore strategies for enhancing LLM factuality, including approaches tailored for specific domains. We focus two primary LLM configurations standalone LLMs and Retrieval-Augmented LLMs that utilizes external data, we detail their unique challenges and potential 
&lt;/p&gt;</description></item><item><title>KwaiYiiMath&#26159;&#19968;&#20010;&#29992;&#20110;&#22686;&#24378;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24212;&#29992;&#30417;&#30563;&#24494;&#35843;&#21644;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#33521;&#35821;&#21644;&#20013;&#25991;&#25968;&#23398;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#22815;&#27491;&#30830;&#35299;&#20915;&#29983;&#25104;&#30340;&#38382;&#39064;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2310.07488</link><description>&lt;p&gt;
KwaiYiiMath: &#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
KwaiYiiMath: Technical Report. (arXiv:2310.07488v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07488
&lt;/p&gt;
&lt;p&gt;
KwaiYiiMath&#26159;&#19968;&#20010;&#29992;&#20110;&#22686;&#24378;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24212;&#29992;&#30417;&#30563;&#24494;&#35843;&#21644;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#33521;&#35821;&#21644;&#20013;&#25991;&#25968;&#23398;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#22815;&#27491;&#30830;&#35299;&#20915;&#29983;&#25104;&#30340;&#38382;&#39064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22788;&#29702;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#19979;&#28216;&#20219;&#21153;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#65292;&#29978;&#33267;&#21487;&#20197;&#22788;&#29702;&#38656;&#35201;&#22810;&#27493;&#25512;&#29702;&#30340;&#25968;&#23398;&#20219;&#21153;&#12290;&#22312;&#26412;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;KwaiYiiMath&#65292;&#36890;&#36807;&#24212;&#29992;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#21644;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#22686;&#24378;&#20102;KwaiYiiBase1&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#21253;&#25324;&#33521;&#35821;&#21644;&#20013;&#25991;&#30340;&#25968;&#23398;&#20219;&#21153;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#26500;&#24314;&#20102;&#19968;&#20010;&#23567;&#35268;&#27169;&#30340;&#20013;&#23567;&#23398;&#25968;&#23398;&#27979;&#35797;&#38598;&#65288;&#21629;&#21517;&#20026;KMath&#65289;&#65292;&#21253;&#21547;188&#20010;&#20363;&#23376;&#65292;&#29992;&#26469;&#35780;&#20272;&#27169;&#22411;&#29983;&#25104;&#30340;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#30340;&#27491;&#30830;&#24615;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#19982;&#31867;&#20284;&#35268;&#27169;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;KwaiYiiMath&#22312;GSM8k&#12289;CMath&#21644;KMath&#19978;&#22343;&#33021;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65288;SOTA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large language models (LLMs) have demonstrated remarkable abilities in handling a variety of natural language processing (NLP) downstream tasks, even on mathematical tasks requiring multi-step reasoning. In this report, we introduce the KwaiYiiMath which enhances the mathematical reasoning abilities of KwaiYiiBase1, by applying Supervised Fine-Tuning (SFT) and Reinforced Learning from Human Feedback (RLHF), including on both English and Chinese mathematical tasks. Meanwhile, we also constructed a small-scale Chinese primary school mathematics test set (named KMath), consisting of 188 examples to evaluate the correctness of the problem-solving process generated by the models. Empirical studies demonstrate that KwaiYiiMath can achieve state-of-the-art (SOTA) performance on GSM8k, CMath, and KMath compared with the similar size models, respectively.
&lt;/p&gt;</description></item><item><title>Cognate Transformer&#26159;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#37325;&#24314;&#21644;&#21516;&#28304;&#21453;&#23556;&#39044;&#27979;&#30340;&#27169;&#22411;&#65292;&#22522;&#20110;&#22810;&#24207;&#21015;&#23545;&#40784;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#35745;&#31639;&#21382;&#21490;&#35821;&#35328;&#23398;&#39046;&#22495;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.07487</link><description>&lt;p&gt;
Cognate Transformer&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#37325;&#24314;&#21644;&#21516;&#28304;&#21453;&#23556;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Cognate Transformer for Automated Phonological Reconstruction and Cognate Reflex Prediction. (arXiv:2310.07487v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07487
&lt;/p&gt;
&lt;p&gt;
Cognate Transformer&#26159;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#37325;&#24314;&#21644;&#21516;&#28304;&#21453;&#23556;&#39044;&#27979;&#30340;&#27169;&#22411;&#65292;&#22522;&#20110;&#22810;&#24207;&#21015;&#23545;&#40784;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#35745;&#31639;&#21382;&#21490;&#35821;&#35328;&#23398;&#39046;&#22495;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#37325;&#24314;&#26159;&#21382;&#21490;&#35821;&#35328;&#23398;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#65292;&#36890;&#36807;&#35266;&#23519;&#23376;&#35821;&#35328;&#30340;&#21516;&#28304;&#35789;&#27719;&#65292;&#30830;&#23450;&#31062;&#20808;&#35821;&#35328;&#30340;&#21407;&#22987;&#35789;&#27719;&#12290;&#35745;&#31639;&#21382;&#21490;&#35821;&#35328;&#23398;&#30340;&#26041;&#27861;&#35797;&#22270;&#36890;&#36807;&#23398;&#20064;&#29616;&#26377;&#35821;&#35328;&#25968;&#25454;&#19978;&#30340;&#27169;&#22411;&#26469;&#33258;&#21160;&#21270;&#36825;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#23558;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;MSA Transformer&#24212;&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#37325;&#24314;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#21629;&#21517;&#20026;Cognate Transformer&#12290;&#25105;&#20204;&#36824;&#23558;&#35813;&#27169;&#22411;&#24212;&#29992;&#20110;&#30456;&#20851;&#30340;&#20219;&#21153;&#65292;&#21363;&#21516;&#28304;&#21453;&#23556;&#39044;&#27979;&#65292;&#26681;&#25454;&#20854;&#20182;&#23376;&#35821;&#35328;&#30340;&#21516;&#28304;&#35789;&#27719;&#26469;&#39044;&#27979;&#23376;&#35821;&#35328;&#20013;&#30340;&#21453;&#23556;&#35789;&#27719;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Phonological reconstruction is one of the central problems in historical linguistics where a proto-word of an ancestral language is determined from the observed cognate words of daughter languages. Computational approaches to historical linguistics attempt to automate the task by learning models on available linguistic data. Several ideas and techniques drawn from computational biology have been successfully applied in the area of computational historical linguistics. Following these lines, we adapt MSA Transformer, a protein language model, to the problem of automated phonological reconstruction. MSA Transformer trains on multiple sequence alignments as input and is, thus, apt for application on aligned cognate words. We, hence, name our model as Cognate Transformer. We also apply the model on another associated task, namely, cognate reflex prediction, where a reflex word in a daughter language is predicted based on cognate words from other daughter languages. We show that our model o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#22810;&#35821;&#35328;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#26377;&#25928;&#22320;&#35843;&#25972;&#36866;&#37197;&#22120;&#26469;&#22788;&#29702;&#20195;&#30721;&#20999;&#25442;&#35821;&#38899;&#65292;&#25552;&#20986;&#20102;&#23558;&#20004;&#20010;&#35821;&#35328;&#36866;&#37197;&#22120;&#30340;&#20449;&#24687;&#34701;&#21512;&#24182;&#23558;&#20195;&#30721;&#20999;&#25442;&#24314;&#27169;&#20026;&#28508;&#22312;&#20108;&#36827;&#21046;&#24207;&#21015;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.07423</link><description>&lt;p&gt;
&#20026;&#22810;&#35821;&#35328;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#20195;&#30721;&#20999;&#25442;&#35843;&#25972;&#36866;&#37197;&#22120;
&lt;/p&gt;
&lt;p&gt;
Adapting the adapters for code-switching in multilingual ASR. (arXiv:2310.07423v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#22810;&#35821;&#35328;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#26377;&#25928;&#22320;&#35843;&#25972;&#36866;&#37197;&#22120;&#26469;&#22788;&#29702;&#20195;&#30721;&#20999;&#25442;&#35821;&#38899;&#65292;&#25552;&#20986;&#20102;&#23558;&#20004;&#20010;&#35821;&#35328;&#36866;&#37197;&#22120;&#30340;&#20449;&#24687;&#34701;&#21512;&#24182;&#23558;&#20195;&#30721;&#20999;&#25442;&#24314;&#27169;&#20026;&#28508;&#22312;&#20108;&#36827;&#21046;&#24207;&#21015;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#35821;&#38899;&#27169;&#22411;&#23637;&#31034;&#20102;&#22312;&#35768;&#22810;&#36164;&#28304;&#21294;&#20047;&#35821;&#35328;&#20013;&#25193;&#23637;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#30340;&#28508;&#21147;&#12290;&#20854;&#20013;&#19968;&#20123;&#27169;&#22411;&#22312;&#20854;&#32452;&#25104;&#20013;&#20351;&#29992;&#35821;&#35328;&#36866;&#37197;&#22120;&#65292;&#36825;&#26377;&#21161;&#20110;&#25552;&#39640;&#21333;&#35821;&#24615;&#33021;&#65292;&#24182;&#36991;&#20813;&#22810;&#35821;&#35328;&#24314;&#27169;&#22312;&#36164;&#28304;&#20016;&#23500;&#30340;&#35821;&#35328;&#20013;&#30340;&#19968;&#20123;&#32570;&#28857;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#32452;&#25104;&#38480;&#21046;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#20195;&#30721;&#20999;&#25442;&#35821;&#38899;&#19978;&#30340;&#21487;&#29992;&#24615;&#65292;&#21363;&#22312;&#21516;&#19968;&#20010;&#35805;&#35821;&#20013;&#28151;&#21512;&#20102;&#20004;&#31181;&#35821;&#35328;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#22320;&#22312;&#20195;&#30721;&#20999;&#25442;&#35821;&#38899;&#19978;&#24494;&#35843;&#36825;&#20123;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#32593;&#32476;&#20013;&#30340;&#27599;&#20010;&#35821;&#35328;&#36866;&#37197;&#28857;&#21516;&#21270;&#26469;&#33258;&#20004;&#20010;&#35821;&#35328;&#36866;&#37197;&#22120;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#36824;&#23558;&#20195;&#30721;&#20999;&#25442;&#24314;&#27169;&#20026;&#19968;&#31995;&#21015;&#21487;&#20197;&#29992;&#20110;&#25351;&#23548;&#27599;&#20010;&#24103;&#32423;&#21035;&#30340;&#35821;&#35328;&#36866;&#37197;&#22120;&#20449;&#24687;&#27969;&#30340;&#28508;&#22312;&#20108;&#36827;&#21046;&#24207;&#21015;&#12290;&#25105;&#20204;&#23545;&#21253;&#25324;&#38463;&#25289;&#20271;&#35821;&#12289;&#26222;&#36890;&#35805;&#21644;&#21360;&#22320;&#35821;&#22312;&#20869;&#30340;&#19977;&#20010;&#20195;&#30721;&#20999;&#25442;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, large pre-trained multilingual speech models have shown potential in scaling Automatic Speech Recognition (ASR) to many low-resource languages. Some of these models employ language adapters in their formulation, which helps to improve monolingual performance and avoids some of the drawbacks of multi-lingual modeling on resource-rich languages. However, this formulation restricts the usability of these models on code-switched speech, where two languages are mixed together in the same utterance. In this work, we propose ways to effectively fine-tune such models on code-switched speech, by assimilating information from both language adapters at each language adaptation point in the network. We also model code-switching as a sequence of latent binary sequences that can be used to guide the flow of information from each language adapter at the frame level. The proposed approaches are evaluated on three code-switched datasets encompassing Arabic, Mandarin, and Hindi languages paire
&lt;/p&gt;</description></item><item><title>DASpeech&#26159;&#19968;&#20010;&#38750;&#33258;&#22238;&#24402;&#30340;&#30452;&#25509;&#35821;&#38899;&#32763;&#35793;&#27169;&#22411;&#65292;&#20351;&#29992;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#26469;&#23454;&#29616;&#24555;&#36895;&#21644;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;&#32763;&#35793;&#12290;</title><link>http://arxiv.org/abs/2310.07403</link><description>&lt;p&gt;
DASpeech&#65306;&#29992;&#20110;&#24555;&#36895;&#39640;&#36136;&#37327;&#35821;&#38899;&#32763;&#35793;&#30340;&#26377;&#21521;&#26080;&#29615;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
DASpeech: Directed Acyclic Transformer for Fast and High-quality Speech-to-Speech Translation. (arXiv:2310.07403v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07403
&lt;/p&gt;
&lt;p&gt;
DASpeech&#26159;&#19968;&#20010;&#38750;&#33258;&#22238;&#24402;&#30340;&#30452;&#25509;&#35821;&#38899;&#32763;&#35793;&#27169;&#22411;&#65292;&#20351;&#29992;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#26469;&#23454;&#29616;&#24555;&#36895;&#21644;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30452;&#25509;&#35821;&#38899;&#32763;&#35793;&#65288;S2ST&#65289;&#20351;&#29992;&#21333;&#20010;&#27169;&#22411;&#23558;&#19968;&#31181;&#35821;&#35328;&#30340;&#35821;&#38899;&#32763;&#35793;&#25104;&#21478;&#19968;&#31181;&#35821;&#35328;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23384;&#22312;&#35821;&#35328;&#21644;&#22768;&#23398;&#22810;&#26679;&#24615;&#65292;&#30446;&#26631;&#35821;&#38899;&#36981;&#24490;&#19968;&#20010;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20998;&#24067;&#65292;&#32473;S2ST&#27169;&#22411;&#23454;&#29616;&#39640;&#36136;&#37327;&#32763;&#35793;&#21644;&#24555;&#36895;&#35299;&#30721;&#36895;&#24230;&#24102;&#26469;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DASpeech&#65292;&#36825;&#26159;&#19968;&#20010;&#38750;&#33258;&#22238;&#24402;&#30340;&#30452;&#25509;S2ST&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#21644;&#39640;&#36136;&#37327;&#30340;S2ST&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#25429;&#25417;&#30446;&#26631;&#35821;&#38899;&#30340;&#22797;&#26434;&#20998;&#24067;&#65292;DASpeech&#37319;&#29992;&#20102;&#20004;&#27493;&#35299;&#30721;&#30340;&#26550;&#26500;&#65292;&#20808;&#30001;&#35821;&#35328;&#35299;&#30721;&#22120;&#29983;&#25104;&#30446;&#26631;&#25991;&#26412;&#65292;&#28982;&#21518;&#30001;&#22768;&#23398;&#35299;&#30721;&#22120;&#26681;&#25454;&#35821;&#35328;&#35299;&#30721;&#22120;&#30340;&#38544;&#34255;&#29366;&#24577;&#29983;&#25104;&#30446;&#26631;&#35821;&#38899;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;DA-Transformer&#30340;&#35299;&#30721;&#22120;&#20316;&#20026;&#35821;&#35328;&#35299;&#30721;&#22120;&#65292;&#23558;FastSpeech 2&#20316;&#20026;&#22768;&#23398;&#35299;&#30721;&#22120;&#12290;DA-Transformer&#20351;&#29992;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#27169;&#25311;&#32763;&#35793;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Direct speech-to-speech translation (S2ST) translates speech from one language into another using a single model. However, due to the presence of linguistic and acoustic diversity, the target speech follows a complex multimodal distribution, posing challenges to achieving both high-quality translations and fast decoding speeds for S2ST models. In this paper, we propose DASpeech, a non-autoregressive direct S2ST model which realizes both fast and high-quality S2ST. To better capture the complex distribution of the target speech, DASpeech adopts the two-pass architecture to decompose the generation process into two steps, where a linguistic decoder first generates the target text, and an acoustic decoder then generates the target speech based on the hidden states of the linguistic decoder. Specifically, we use the decoder of DA-Transformer as the linguistic decoder, and use FastSpeech 2 as the acoustic decoder. DA-Transformer models translations with a directed acyclic graph (DAG). To co
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#25968;&#25454;&#38598;&#31579;&#36873;&#26694;&#26550;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20010;&#24615;&#21270;&#38754;&#21521;&#30446;&#26631;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#21517;&#20026;TopDial&#12290;&#35813;&#25968;&#25454;&#38598;&#36136;&#37327;&#39640;&#65292;&#26377;&#21161;&#20110;&#25506;&#32034;&#20010;&#24615;&#21270;&#30446;&#26631;&#23548;&#21521;&#30340;&#23545;&#35805;&#12290;</title><link>http://arxiv.org/abs/2310.07397</link><description>&lt;p&gt;
&#38754;&#21521;&#30446;&#26631;&#30340;&#20010;&#24615;&#21270;&#20027;&#21160;&#23545;&#35805;&#31995;&#32479;&#65306;&#38382;&#39064;&#24418;&#24335;&#21270;&#19982;&#25968;&#25454;&#38598;&#31579;&#36873;
&lt;/p&gt;
&lt;p&gt;
Target-oriented Proactive Dialogue Systems with Personalization: Problem Formulation and Dataset Curation. (arXiv:2310.07397v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07397
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#25968;&#25454;&#38598;&#31579;&#36873;&#26694;&#26550;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20010;&#24615;&#21270;&#38754;&#21521;&#30446;&#26631;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#21517;&#20026;TopDial&#12290;&#35813;&#25968;&#25454;&#38598;&#36136;&#37327;&#39640;&#65292;&#26377;&#21161;&#20110;&#25506;&#32034;&#20010;&#24615;&#21270;&#30446;&#26631;&#23548;&#21521;&#30340;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#30446;&#26631;&#30340;&#23545;&#35805;&#31995;&#32479;&#26088;&#22312;&#20027;&#21160;&#24341;&#23548;&#23545;&#35805;&#26397;&#21521;&#39044;&#23450;&#30340;&#30446;&#26631;&#25110;&#36798;&#25104;&#29305;&#23450;&#30340;&#31995;&#32479;&#30446;&#26631;&#65292;&#22312;&#23545;&#35805;&#23436;&#25104;&#36807;&#31243;&#20013;&#32771;&#34385;&#20010;&#24615;&#21270;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#65292;&#20174;&#38646;&#24320;&#22987;&#26500;&#24314;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#35282;&#33394;&#25198;&#28436;&#26041;&#27861;&#30340;&#33258;&#21160;&#25968;&#25454;&#38598;&#31579;&#36873;&#26694;&#26550;&#12290;&#22522;&#20110;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20010;&#24615;&#21270;&#38754;&#21521;&#30446;&#26631;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#21517;&#20026;TopDial&#65292;&#21253;&#21547;&#32422;18K&#20010;&#22810;&#36718;&#23545;&#35805;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#25968;&#25454;&#38598;&#20855;&#26377;&#24456;&#39640;&#30340;&#36136;&#37327;&#65292;&#21487;&#20197;&#29992;&#20110;&#25506;&#32034;&#20010;&#24615;&#21270;&#30446;&#26631;&#23548;&#21521;&#30340;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;
Target-oriented dialogue systems, designed to proactively steer conversations toward predefined targets or accomplish specific system-side goals, are an exciting area in conversational AI. In this work, by formulating a &lt;dialogue act, topic&gt; pair as the conversation target, we explore a novel problem of personalized target-oriented dialogue by considering personalization during the target accomplishment process. However, there remains an emergent need for high-quality datasets, and building one from scratch requires tremendous human effort. To address this, we propose an automatic dataset curation framework using a role-playing approach. Based on this framework, we construct a large-scale personalized target-oriented dialogue dataset, TopDial, which comprises about 18K multi-turn dialogues. The experimental results show that this dataset is of high quality and could contribute to exploring personalized target-oriented dialogue.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#20102;&#29983;&#29289;&#23398;&#20013;&#19982;&#35821;&#35328;&#23398;&#23450;&#24459;&#19968;&#33268;&#30340;&#32479;&#35745;&#27169;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27010;&#24565;&#26694;&#26550;&#65292;&#23558;&#19981;&#21516;&#23618;&#27425;&#30340;&#20998;&#26512;&#25972;&#21512;&#36215;&#26469;&#65292;&#25581;&#31034;&#33258;&#28982;&#31995;&#32479;&#30340;&#22522;&#26412;&#32452;&#32455;&#35268;&#24459;&#12290;</title><link>http://arxiv.org/abs/2310.07387</link><description>&lt;p&gt;
&#29983;&#29289;&#23398;&#20013;&#30340;&#35821;&#35328;&#23398;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Linguistic laws in biology. (arXiv:2310.07387v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07387
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#20102;&#29983;&#29289;&#23398;&#20013;&#19982;&#35821;&#35328;&#23398;&#23450;&#24459;&#19968;&#33268;&#30340;&#32479;&#35745;&#27169;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27010;&#24565;&#26694;&#26550;&#65292;&#23558;&#19981;&#21516;&#23618;&#27425;&#30340;&#20998;&#26512;&#25972;&#21512;&#36215;&#26469;&#65292;&#25581;&#31034;&#33258;&#28982;&#31995;&#32479;&#30340;&#22522;&#26412;&#32452;&#32455;&#35268;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#19968;&#20010;&#19990;&#32426;&#20197;&#26469;&#65292;&#23450;&#37327;&#35821;&#35328;&#23398;&#23478;&#19968;&#30452;&#22312;&#30740;&#31350;&#20154;&#31867;&#35821;&#35328;&#30340;&#24120;&#35265;&#32479;&#35745;&#27169;&#24335;&#65292;&#31216;&#20026;&#35821;&#35328;&#23398;&#23450;&#24459;&#12290;&#26368;&#36817;&#65292;&#26469;&#33258;&#19981;&#21516;&#23398;&#31185;&#30340;&#29983;&#29289;&#23398;&#23478;&#24320;&#22987;&#25506;&#32034;&#36825;&#20123;&#23450;&#24459;&#22312;&#35821;&#35328;&#20043;&#22806;&#30340;&#26222;&#36941;&#24615;&#65292;&#21457;&#29616;&#19982;&#35821;&#35328;&#23398;&#23450;&#24459;&#19968;&#33268;&#30340;&#27169;&#24335;&#23384;&#22312;&#20110;&#29983;&#29289;&#32452;&#32455;&#30340;&#22810;&#20010;&#23618;&#27425;&#65292;&#20174;&#20998;&#23376;&#65288;&#22522;&#22240;&#32452;&#12289;&#22522;&#22240;&#21644;&#34507;&#30333;&#36136;&#65289;&#21040;&#20010;&#20307;&#65288;&#21160;&#29289;&#34892;&#20026;&#65289;&#21040;&#29983;&#24577;&#65288;&#31181;&#32676;&#21644;&#29983;&#24577;&#31995;&#32479;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27010;&#24565;&#26694;&#26550;&#65292;&#29992;&#20110;&#30740;&#31350;&#29983;&#29289;&#23398;&#20013;&#30340;&#35821;&#35328;&#23398;&#23450;&#24459;&#65292;&#21253;&#25324;&#21644;&#25972;&#21512;&#19981;&#21516;&#30340;&#20998;&#26512;&#23618;&#27425;&#65292;&#20174;&#25551;&#36848;&#21040;&#39044;&#27979;&#21040;&#29702;&#35770;&#24314;&#31435;&#12290;&#37319;&#29992;&#36825;&#20010;&#26694;&#26550;&#23558;&#20026;&#25581;&#31034;&#33258;&#28982;&#31995;&#32479;&#22522;&#26412;&#32452;&#32455;&#35268;&#24459;&#25552;&#20379;&#20851;&#38190;&#30340;&#26032;&#35265;&#35299;&#65292;&#23558;&#35821;&#35328;&#23398;&#23450;&#24459;&#21644;&#29983;&#29289;&#23398;&#30340;&#26680;&#24515;&#29702;&#35770;&#32479;&#19968;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linguistic laws, the common statistical patterns of human language, have been investigated by quantitative linguists for nearly a century. Recently, biologists from a range of disciplines have started to explore the prevalence of these laws beyond language, finding patterns consistent with linguistic laws across multiple levels of biological organisation, from molecular (genomes, genes, and proteins) to organismal (animal behaviour) to ecological (populations and ecosystems). We propose a new conceptual framework for the study of linguistic laws in biology, comprising and integrating distinct levels of analysis, from description to prediction to theory building. Adopting this framework will provide critical new insights into the fundamental rules of organisation underpinning natural systems, unifying linguistic laws and core theory in biology.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;ELECTRA&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36741;&#21161;&#27169;&#22411;&#65292;&#36890;&#36807;&#28201;&#24230;&#32553;&#25918;&#24179;&#28369;&#20027;&#27169;&#22411;&#30340;&#36755;&#20986;&#20998;&#24067;&#65292;&#36798;&#21040;&#19982;&#26368;&#20808;&#36827;&#30340;ELECTRA&#39044;&#35757;&#32451;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#65292;&#24182;&#26174;&#33879;&#20943;&#23569;&#20102;&#36741;&#21161;&#27169;&#22411;&#20849;&#21516;&#35757;&#32451;&#24102;&#26469;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2310.07347</link><description>&lt;p&gt;
&#39640;&#25928;&#39044;&#35757;&#32451;&#30340;&#24555;&#36895;ELECTRA
&lt;/p&gt;
&lt;p&gt;
Fast-ELECTRA for Efficient Pre-training. (arXiv:2310.07347v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07347
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;ELECTRA&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36741;&#21161;&#27169;&#22411;&#65292;&#36890;&#36807;&#28201;&#24230;&#32553;&#25918;&#24179;&#28369;&#20027;&#27169;&#22411;&#30340;&#36755;&#20986;&#20998;&#24067;&#65292;&#36798;&#21040;&#19982;&#26368;&#20808;&#36827;&#30340;ELECTRA&#39044;&#35757;&#32451;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#65292;&#24182;&#26174;&#33879;&#20943;&#23569;&#20102;&#36741;&#21161;&#27169;&#22411;&#20849;&#21516;&#35757;&#32451;&#24102;&#26469;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ELECTRA&#36890;&#36807;&#26816;&#27979;&#24207;&#21015;&#20013;&#34987;&#36741;&#21161;&#27169;&#22411;&#26367;&#25442;&#30340;&#26631;&#35760;&#26469;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#34429;&#28982;ELECTRA&#22823;&#22823;&#25552;&#39640;&#20102;&#25928;&#29575;&#65292;&#20294;&#20854;&#28508;&#21147;&#21463;&#21040;&#20102;&#36741;&#21161;&#27169;&#22411;&#24102;&#26469;&#30340;&#35757;&#32451;&#25104;&#26412;&#30340;&#38480;&#21046;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#20010;&#19982;&#20027;&#27169;&#22411;&#20849;&#21516;&#35757;&#32451;&#30340;&#27169;&#22411;&#20165;&#29992;&#20110;&#36741;&#21161;&#20027;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#21518;&#34987;&#20002;&#24323;&#12290;&#36825;&#23548;&#33268;&#22823;&#37327;&#30340;&#35757;&#32451;&#25104;&#26412;&#34987;&#30333;&#30333;&#28010;&#36153;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Fast-ELECTRA&#65292;&#23427;&#21033;&#29992;&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36741;&#21161;&#27169;&#22411;&#12290;&#20026;&#20102;&#26500;&#24314;&#20027;&#27169;&#22411;&#30340;&#23398;&#20064;&#35838;&#31243;&#65292;&#25105;&#20204;&#36890;&#36807;&#28201;&#24230;&#32553;&#25918;&#21644;&#36882;&#20943;&#30340;&#26041;&#27861;&#24179;&#28369;&#20854;&#36755;&#20986;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;ELECTRA&#39118;&#26684;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#30456;&#23218;&#32654;&#65292;&#21516;&#26102;&#26174;&#33879;&#20943;&#23569;&#20102;&#36741;&#21161;&#27169;&#22411;&#20849;&#21516;&#35757;&#32451;&#24102;&#26469;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#38477;&#20302;&#20102;&#27169;&#22411;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
ELECTRA pre-trains language models by detecting tokens in a sequence that have been replaced by an auxiliary model. Although ELECTRA offers a significant boost in efficiency, its potential is constrained by the training cost brought by the auxiliary model. Notably, this model, which is jointly trained with the main model, only serves to assist the training of the main model and is discarded post-training. This results in a substantial amount of training cost being expended in vain. To mitigate this issue, we propose Fast-ELECTRA, which leverages an existing language model as the auxiliary model. To construct a learning curriculum for the main model, we smooth its output distribution via temperature scaling following a descending schedule. Our approach rivals the performance of state-of-the-art ELECTRA-style pre-training methods, while significantly eliminating the computation and memory cost brought by the joint training of the auxiliary model. Our method also reduces the sensitivity t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22522;&#20110;&#38899;&#32032;&#30340;&#31070;&#32463;&#21464;&#25442;&#22120;&#24207;&#21015;&#36776;&#21035;&#35757;&#32451;&#20013;&#20351;&#29992;&#19981;&#21516;&#19978;&#19979;&#25991;&#38271;&#24230;&#21644;&#26631;&#31614;&#21333;&#20803;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#35757;&#32451;&#20013;&#20351;&#29992;&#21333;&#35789;&#32423;&#21035;&#30340;&#35821;&#35328;&#27169;&#22411;&#20248;&#20110;&#38899;&#32032;&#32423;&#21035;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#27010;&#29575;&#35745;&#31639;&#20013;&#30340;&#19978;&#19979;&#25991;&#22823;&#23567;&#23545;&#24615;&#33021;&#24433;&#21709;&#26377;&#38480;&#12290;&#30740;&#31350;&#32467;&#26524;&#36824;&#25581;&#31034;&#20102;&#20551;&#35774;&#31354;&#38388;&#36136;&#37327;&#22312;&#24207;&#21015;&#36776;&#21035;&#35757;&#32451;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.07345</link><description>&lt;p&gt;
&#35843;&#26597;&#35821;&#35328;&#27169;&#22411;&#22312;&#31070;&#32463;&#21464;&#25442;&#22120;&#24207;&#21015;&#36776;&#21035;&#35757;&#32451;&#20013;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Investigating the Effect of Language Models in Sequence Discriminative Training for Neural Transducers. (arXiv:2310.07345v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22522;&#20110;&#38899;&#32032;&#30340;&#31070;&#32463;&#21464;&#25442;&#22120;&#24207;&#21015;&#36776;&#21035;&#35757;&#32451;&#20013;&#20351;&#29992;&#19981;&#21516;&#19978;&#19979;&#25991;&#38271;&#24230;&#21644;&#26631;&#31614;&#21333;&#20803;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#35757;&#32451;&#20013;&#20351;&#29992;&#21333;&#35789;&#32423;&#21035;&#30340;&#35821;&#35328;&#27169;&#22411;&#20248;&#20110;&#38899;&#32032;&#32423;&#21035;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#27010;&#29575;&#35745;&#31639;&#20013;&#30340;&#19978;&#19979;&#25991;&#22823;&#23567;&#23545;&#24615;&#33021;&#24433;&#21709;&#26377;&#38480;&#12290;&#30740;&#31350;&#32467;&#26524;&#36824;&#25581;&#31034;&#20102;&#20551;&#35774;&#31354;&#38388;&#36136;&#37327;&#22312;&#24207;&#21015;&#36776;&#21035;&#35757;&#32451;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#22522;&#20110;&#38899;&#32032;&#30340;&#31070;&#32463;&#21464;&#25442;&#22120;&#24207;&#21015;&#36776;&#21035;&#35757;&#32451;&#20013;&#30340;&#19981;&#21516;&#19978;&#19979;&#25991;&#38271;&#24230;&#21644;&#26631;&#31614;&#21333;&#20803;&#65288;&#38899;&#32032; vs. &#21333;&#35789;&#65289;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21516;&#26102;&#32771;&#23519;&#20102;&#26080;&#32593;&#26684;&#21644;N&#26368;&#20339;&#21015;&#34920;&#26041;&#27861;&#12290;&#23545;&#20110;&#20351;&#29992;&#38899;&#32032;&#32423;&#21035;LMs&#30340;&#26080;&#32593;&#26684;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#36817;&#20284;&#19978;&#19979;&#25991;&#21382;&#21490;&#20197;&#21033;&#29992;&#23436;&#20840;&#19978;&#19979;&#25991;&#20381;&#36182;&#30340;LMs&#12290;&#36825;&#31181;&#36817;&#20284;&#21487;&#20197;&#25193;&#23637;&#21040;&#20219;&#24847;&#19978;&#19979;&#25991;&#38271;&#24230;&#65292;&#24182;&#20801;&#35768;&#22312;&#26080;&#32593;&#26684;&#26041;&#27861;&#20013;&#20351;&#29992;&#21333;&#35789;&#32423;&#21035;LMs&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#26080;&#32593;&#26684;&#21644;N&#26368;&#20339;&#21015;&#34920;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#27604;&#36739;&#12290;&#22312;Librispeech&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#35757;&#32451;&#20013;&#20351;&#29992;&#21333;&#35789;&#32423;&#21035;LM&#30340;&#25928;&#26524;&#20248;&#20110;&#38899;&#32032;&#32423;&#21035;LM&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#29992;&#20110;&#27010;&#29575;&#35745;&#31639;&#30340;LM&#30340;&#19978;&#19979;&#25991;&#22823;&#23567;&#23545;&#24615;&#33021;&#24433;&#21709;&#26377;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#24207;&#21015;&#36776;&#21035;&#35757;&#32451;&#20013;&#20551;&#35774;&#31354;&#38388;&#36136;&#37327;&#30340;&#20851;&#38190;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we investigate the effect of language models (LMs) with different context lengths and label units (phoneme vs. word) used in sequence discriminative training for phoneme-based neural transducers. Both lattice-free and N-best-list approaches are examined. For lattice-free methods with phoneme-level LMs, we propose a method to approximate the context history to employ LMs with full-context dependency. This approximation can be extended to arbitrary context length and enables the usage of word-level LMs in lattice-free methods. Moreover, a systematic comparison is conducted across lattice-free and N-best-list-based methods. Experimental results on Librispeech show that using the word-level LM in training outperforms the phoneme-level LM. Besides, we find that the context size of the LM used for probability computation has a limited effect on performance. Moreover, our results reveal the pivotal importance of the hypothesis space quality in sequence discriminative training.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22914;&#20309;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#19981;&#26029;&#21464;&#21270;&#30340;&#19990;&#30028;&#30693;&#35782;&#23545;&#40784;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#35752;&#35770;&#20102;&#30456;&#20851;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2310.07343</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#25429;&#25417;&#19981;&#26029;&#21464;&#21270;&#30340;&#19990;&#30028;&#30693;&#35782;&#65311;&#23545;&#36817;&#26399;&#36827;&#23637;&#30340;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
How Do Large Language Models Capture the Ever-changing World Knowledge? A Review of Recent Advances. (arXiv:2310.07343v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22914;&#20309;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#19981;&#26029;&#21464;&#21270;&#30340;&#19990;&#30028;&#30693;&#35782;&#23545;&#40784;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#35752;&#35770;&#20102;&#30456;&#20851;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#26041;&#38754;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#65292;&#20294;&#23427;&#20204;&#22312;&#37096;&#32626;&#21518;&#24456;&#24555;&#23601;&#20250;&#36807;&#26102;&#12290;&#22914;&#20309;&#20445;&#25345;&#20854;&#26368;&#26032;&#29366;&#24577;&#26159;&#24403;&#21069;&#26102;&#20195;&#30340;&#19968;&#20010;&#32039;&#36843;&#38382;&#39064;&#12290;&#26412;&#25991;&#20840;&#38754;&#35780;&#36848;&#20102;&#23558;LLMs&#19982;&#19981;&#26029;&#21464;&#21270;&#30340;&#19990;&#30028;&#30693;&#35782;&#23545;&#40784;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#23545;&#30740;&#31350;&#24037;&#20316;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#25552;&#20379;&#28145;&#20837;&#30340;&#27604;&#36739;&#21644;&#35752;&#35770;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#29616;&#26377;&#30340;&#25361;&#25112;&#65292;&#24182;&#37325;&#28857;&#20171;&#32461;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#20197;&#20419;&#36827;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#23558;&#35770;&#25991;&#21015;&#34920;&#21457;&#24067;&#22312;https://github.com/hyintell/awesome-refreshing-llms&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although large language models (LLMs) are impressive in solving various tasks, they can quickly be outdated after deployment. Maintaining their up-to-date status is a pressing concern in the current era. This paper provides a comprehensive review of recent advances in aligning LLMs with the ever-changing world knowledge without re-training from scratch. We categorize research works systemically and provide in-depth comparisons and discussion. We also discuss existing challenges and highlight future directions to facilitate research in this field. We release the paper list at https://github.com/hyintell/awesome-refreshing-llms
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36827;&#34892;&#20102;&#23545;&#20013;&#25991;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#23548;&#35843;&#25972;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#25506;&#32034;&#20102;LLM&#22522;&#30784;&#12289;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;&#21644;&#25351;&#20196;&#25968;&#25454;&#31867;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#20182;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#20026;&#23450;&#21046;&#33021;&#26356;&#22909;&#21709;&#24212;&#20013;&#25991;&#25351;&#20196;&#30340;LLM&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.07328</link><description>&lt;p&gt;
&#23545;&#20013;&#25991;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#23548;&#35843;&#25972;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of Instruction-tuning Large Language Models in Chinese. (arXiv:2310.07328v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07328
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36827;&#34892;&#20102;&#23545;&#20013;&#25991;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#23548;&#35843;&#25972;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#25506;&#32034;&#20102;LLM&#22522;&#30784;&#12289;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;&#21644;&#25351;&#20196;&#25968;&#25454;&#31867;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#20182;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#20026;&#23450;&#21046;&#33021;&#26356;&#22909;&#21709;&#24212;&#20013;&#25991;&#25351;&#20196;&#30340;LLM&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#30340;&#25104;&#21151;&#39564;&#35777;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#20013;&#30340;&#28508;&#21147;&#12290;&#38543;&#21518;&#65292;LLM&#30340;&#21457;&#24067;&#24341;&#36215;&#20102;&#24320;&#28304;&#31038;&#21306;&#23545;&#25351;&#23548;&#35843;&#25972;&#30340;&#20852;&#36259;&#65292;&#36825;&#34987;&#35748;&#20026;&#21487;&#20197;&#21152;&#24555;ChatGPT&#30340;&#22797;&#21046;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#20013;&#25991;&#30340;LLM&#25351;&#23548;&#35843;&#25972;&#30340;&#30740;&#31350;&#20173;&#22788;&#20110;&#26089;&#26399;&#38454;&#27573;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#23545;&#20013;&#25991;LLM&#30340;&#25351;&#23548;&#35843;&#25972;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#36825;&#21487;&#20197;&#20316;&#20026;&#19968;&#26412;&#25552;&#20379;&#26377;&#20215;&#20540;&#21457;&#29616;&#30340;&#28921;&#39274;&#20070;&#26469;&#26377;&#25928;&#22320;&#23450;&#21046;LLM&#65292;&#20351;&#20854;&#33021;&#22815;&#26356;&#22909;&#22320;&#21709;&#24212;&#20013;&#25991;&#25351;&#20196;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#25506;&#32034;&#20102;LLM&#22522;&#30784;&#12289;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;&#21644;&#25351;&#20196;&#25968;&#25454;&#31867;&#22411;&#36825;&#19977;&#20010;&#23545;&#25351;&#23548;&#35843;&#25972;&#33267;&#20851;&#37325;&#35201;&#30340;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#23454;&#39564;&#26469;&#30740;&#31350;&#20854;&#20182;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#20363;&#22914;&#24605;&#32500;&#38142;&#25968;&#25454;&#21644;&#20154;&#31867;&#20215;&#20540;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#20010;&#23454;&#35777;&#30740;&#31350;&#33021;&#22815;
&lt;/p&gt;
&lt;p&gt;
The success of ChatGPT validates the potential of large language models (LLMs) in artificial general intelligence (AGI). Subsequently, the release of LLMs has sparked the open-source community's interest in instruction-tuning, which is deemed to accelerate ChatGPT's replication process. However, research on instruction-tuning LLMs in Chinese, the world's most spoken language, is still in its early stages. Therefore, this paper makes an in-depth empirical study of instruction-tuning LLMs in Chinese, which can serve as a cookbook that provides valuable findings for effectively customizing LLMs that can better respond to Chinese instructions. Specifically, we systematically explore the impact of LLM bases, parameter-efficient methods, instruction data types, which are the three most important elements for instruction-tuning. Besides, we also conduct experiment to study the impact of other factors, e.g., chain-of-thought data and human-value alignment. We hope that this empirical study can
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#24503;&#35821;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#23558;&#25968;&#25454;&#22810;&#26679;&#24615;&#32622;&#20110;&#25968;&#25454;&#36136;&#37327;&#20043;&#19978;&#30340;&#20132;&#21449;&#39046;&#22495;&#25968;&#25454;&#38598;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#36229;&#36807;&#20102;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.07321</link><description>&lt;p&gt;
&#20851;&#20110;&#20132;&#21449;&#39046;&#22495;&#25968;&#25454;&#23545;&#24503;&#35821;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
On the Impact of Cross-Domain Data on German Language Models. (arXiv:2310.07321v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#24503;&#35821;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#23558;&#25968;&#25454;&#22810;&#26679;&#24615;&#32622;&#20110;&#25968;&#25454;&#36136;&#37327;&#20043;&#19978;&#30340;&#20132;&#21449;&#39046;&#22495;&#25968;&#25454;&#38598;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#36229;&#36807;&#20102;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35201;&#20040;&#22312;&#36890;&#29992;&#32593;&#32476;&#25235;&#21462;&#25968;&#25454;&#19978;&#35757;&#32451;&#65292;&#35201;&#20040;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#25968;&#25454;&#19978;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#36817;&#25104;&#21151;&#31361;&#26174;&#20102;&#20132;&#21449;&#39046;&#22495;&#25968;&#25454;&#38598;&#30340;&#22909;&#22788;&#12290;&#20026;&#20102;&#32771;&#23519;&#25968;&#25454;&#22810;&#26679;&#24615;&#39640;&#20110;&#36136;&#37327;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#20116;&#20010;&#39046;&#22495;&#25991;&#26412;&#30340;&#24503;&#35821;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#20010;&#26088;&#22312;&#21253;&#21547;&#39640;&#36136;&#37327;&#25968;&#25454;&#30340;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#22312;&#36825;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#21442;&#25968;&#33539;&#22260;&#20174;122M&#21040;750M&#30340;&#19968;&#31995;&#21015;&#27169;&#22411;&#65292;&#25105;&#20204;&#23545;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#20132;&#21449;&#39046;&#22495;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#20248;&#20110;&#20165;&#20351;&#29992;&#36136;&#37327;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#22312;&#20808;&#21069;&#26368;&#20808;&#36827;&#32467;&#26524;&#19978;&#25552;&#20986;&#20102;&#39640;&#36798;4.45%&#30340;&#25913;&#36827;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#22312;https://huggingface.co/ikim-uk-essen&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditionally, large language models have been either trained on general web crawls or domain-specific data. However, recent successes of generative large language models, have shed light on the benefits of cross-domain datasets. To examine the significance of prioritizing data diversity over quality, we present a German dataset comprising texts from five domains, along with another dataset aimed at containing high-quality data. Through training a series of models ranging between 122M and 750M parameters on both datasets, we conduct a comprehensive benchmark on multiple downstream tasks. Our findings demonstrate that the models trained on the cross-domain dataset outperform those trained on quality data alone, leading to improvements up to $4.45\%$ over the previous state-of-the-art. The models are available at https://huggingface.co/ikim-uk-essen
&lt;/p&gt;</description></item><item><title>SNOiC&#27169;&#22411;&#26159;&#19968;&#31181;&#22522;&#20110;&#36719;&#26631;&#31614;&#21644;&#22122;&#22768;&#28151;&#21512;&#30340;&#24320;&#25918;&#24847;&#22270;&#20998;&#31867;&#27169;&#22411;&#65292;&#36890;&#36807;&#20943;&#23569;&#20559;&#24046;&#21644;&#29983;&#25104;&#20266;&#25968;&#25454;&#26469;&#25552;&#39640;&#35782;&#21035;&#24320;&#25918;&#24847;&#22270;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.07306</link><description>&lt;p&gt;
SNOiC: &#22522;&#20110;&#36719;&#26631;&#31614;&#21644;&#22122;&#22768;&#28151;&#21512;&#30340;&#24320;&#25918;&#24847;&#22270;&#20998;&#31867;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SNOiC: Soft Labeling and Noisy Mixup based Open Intent Classification Model. (arXiv:2310.07306v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07306
&lt;/p&gt;
&lt;p&gt;
SNOiC&#27169;&#22411;&#26159;&#19968;&#31181;&#22522;&#20110;&#36719;&#26631;&#31614;&#21644;&#22122;&#22768;&#28151;&#21512;&#30340;&#24320;&#25918;&#24847;&#22270;&#20998;&#31867;&#27169;&#22411;&#65292;&#36890;&#36807;&#20943;&#23569;&#20559;&#24046;&#21644;&#29983;&#25104;&#20266;&#25968;&#25454;&#26469;&#25552;&#39640;&#35782;&#21035;&#24320;&#25918;&#24847;&#22270;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36719;&#26631;&#31614;&#21644;&#22122;&#22768;&#28151;&#21512;&#30340;&#24320;&#25918;&#24847;&#22270;&#20998;&#31867;&#27169;&#22411;&#65288;SNOiC&#65289;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#22823;&#22810;&#20351;&#29992;&#22522;&#20110;&#38408;&#20540;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#24320;&#25918;&#24847;&#22270;&#65292;&#23481;&#26131;&#20986;&#29616;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#24182;&#21487;&#33021;&#20135;&#29983;&#26377;&#20559;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#24320;&#25918;&#24847;&#22270;&#31867;&#21035;&#38656;&#35201;&#26356;&#22810;&#21487;&#29992;&#25968;&#25454;&#30340;&#38656;&#27714;&#20063;&#26159;&#29616;&#26377;&#27169;&#22411;&#30340;&#21478;&#19968;&#20010;&#38480;&#21046;&#12290;SNOiC&#23558;&#36719;&#26631;&#31614;&#21644;&#22122;&#22768;&#28151;&#21512;&#31574;&#30053;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#20943;&#23569;&#20559;&#24046;&#24182;&#20026;&#24320;&#25918;&#24847;&#22270;&#31867;&#21035;&#29983;&#25104;&#20266;&#25968;&#25454;&#12290;&#22312;&#22235;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;SNOiC&#27169;&#22411;&#22312;&#35782;&#21035;&#24320;&#25918;&#24847;&#22270;&#26041;&#38754;&#30340;&#26368;&#20302;&#21644;&#26368;&#39640;&#24615;&#33021;&#20998;&#21035;&#20026;68.72&#65285;&#21644;94.71&#65285;&#12290;&#27492;&#22806;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;SNOiC&#27169;&#22411;&#22312;&#35782;&#21035;&#24320;&#25918;&#24847;&#22270;&#26041;&#38754;&#30340;&#24615;&#33021;&#25552;&#21319;&#20102;0.93&#65285;&#65288;&#26368;&#20302;&#65289;&#21644;12.76&#65285;&#65288;&#26368;&#39640;&#65289;&#12290;&#36890;&#36807;&#20998;&#26512;&#25152;&#25552;&#20986;&#27169;&#22411;&#20013;&#20351;&#29992;&#30340;&#21508;&#31181;&#21442;&#25968;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#36824;&#36827;&#34892;&#20102;&#28040;&#34701;&#30740;&#31350;&#65292;&#20854;&#20013;
&lt;/p&gt;
&lt;p&gt;
This paper presents a Soft Labeling and Noisy Mixup-based open intent classification model (SNOiC). Most of the previous works have used threshold-based methods to identify open intents, which are prone to overfitting and may produce biased predictions. Additionally, the need for more available data for an open intent class presents another limitation for these existing models. SNOiC combines Soft Labeling and Noisy Mixup strategies to reduce the biasing and generate pseudo-data for open intent class. The experimental results on four benchmark datasets show that the SNOiC model achieves a minimum and maximum performance of 68.72\% and 94.71\%, respectively, in identifying open intents. Moreover, compared to state-of-the-art models, the SNOiC model improves the performance of identifying open intents by 0.93\% (minimum) and 12.76\% (maximum). The model's efficacy is further established by analyzing various parameters used in the proposed model. An ablation study is also conducted, which
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Parrot&#65292;&#19968;&#20010;&#39640;&#24230;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#65292;&#36827;&#19968;&#27493;&#23436;&#21892;&#20102;&#22810;&#36718;&#32842;&#22825;&#27169;&#22411;&#22312;&#23545;&#35805;&#20013;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.07301</link><description>&lt;p&gt;
Parrot:&#36890;&#36807;&#23398;&#20064;&#25552;&#38382;&#26469;&#22686;&#24378;&#22810;&#36718;&#32842;&#22825;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Parrot: Enhancing Multi-Turn Chat Models by Learning to Ask Questions. (arXiv:2310.07301v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Parrot&#65292;&#19968;&#20010;&#39640;&#24230;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#65292;&#36827;&#19968;&#27493;&#23436;&#21892;&#20102;&#22810;&#36718;&#32842;&#22825;&#27169;&#22411;&#22312;&#23545;&#35805;&#20013;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32842;&#22825;&#27169;&#22411;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36827;&#23637;&#65307;&#28982;&#32780;&#65292;&#22312;&#24320;&#28304;&#32842;&#22825;&#27169;&#22411;&#65288;&#22914;Alpaca&#21644;Vicuna&#65289;&#19982;&#39046;&#20808;&#30340;&#32842;&#22825;&#27169;&#22411;&#65288;&#22914;ChatGPT&#21644;GPT-4&#65289;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#30340;&#22810;&#36718;&#23545;&#35805;&#28382;&#21518;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#23558;&#28382;&#21518;&#24402;&#22240;&#20110;&#32570;&#20047;&#36275;&#22815;&#39640;&#36136;&#37327;&#30340;&#22810;&#36718;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#12290;&#31038;&#21306;&#25552;&#20379;&#30340;&#35843;&#20248;&#25968;&#25454;&#35201;&#20040;&#26159;&#21333;&#36718;&#20250;&#35805;&#65292;&#35201;&#20040;&#26159;&#23384;&#22312;&#26576;&#20123;&#38382;&#39064;&#30340;&#22810;&#36718;&#20250;&#35805;&#65292;&#20363;&#22914;&#38750;&#20154;&#31867;&#30340;&#25351;&#20196;&#65292;&#21709;&#24212;&#19981;&#22815;&#35814;&#32454;&#65292;&#25110;&#32773;&#24456;&#23569;&#20986;&#29616;&#20027;&#39064;&#36716;&#25442;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;Parrot&#65292;&#19968;&#20010;&#39640;&#24230;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#35813;&#35299;&#20915;&#26041;&#26696;&#26088;&#22312;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#65292;&#28982;&#21518;&#29992;&#20110;&#22686;&#24378;&#22810;&#36718;&#32842;&#22825;&#27169;&#22411;&#22312;&#23545;&#35805;&#20013;&#30340;&#25928;&#26524;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#35757;&#32451;Parrot-Ask&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26088;&#22312;&#27169;&#25311;&#30495;&#23454;&#29992;&#25143;&#29983;&#25104;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
Impressive progress has been made on chat models based on Large Language Models (LLMs) recently; however, there is a noticeable lag in multi-turn conversations between open-source chat models (e.g., Alpaca and Vicuna) and the leading chat models (e.g., ChatGPT and GPT-4). Through a series of analyses, we attribute the lag to the lack of enough high-quality multi-turn instruction-tuning data. The available instruction-tuning data for the community are either single-turn conversations or multi-turn ones with certain issues, such as non-human-like instructions, less detailed responses, or rare topic shifts. In this paper, we address these challenges by introducing Parrot, a highly scalable solution designed to automatically generate high-quality instruction-tuning data, which are then used to enhance the effectiveness of chat models in multi-turn conversations. Specifically, we start by training the Parrot-Ask model, which is designed to emulate real users in generating instructions. We t
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21483;&#20570;RobustGEC&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#31995;&#32479;&#30340;&#19978;&#19979;&#25991;&#40065;&#26834;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;&#20173;&#28982;&#26080;&#27861;&#24456;&#22909;&#22320;&#24212;&#23545;&#19978;&#19979;&#25991;&#25200;&#21160;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.07299</link><description>&lt;p&gt;
RobustGEC: &#40065;&#26834;&#30340;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#31995;&#32479;&#25269;&#25239;&#24494;&#23567;&#30340;&#19978;&#19979;&#25991;&#25200;&#21160;
&lt;/p&gt;
&lt;p&gt;
RobustGEC: Robust Grammatical Error Correction Against Subtle Context Perturbation. (arXiv:2310.07299v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07299
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21483;&#20570;RobustGEC&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#31995;&#32479;&#30340;&#19978;&#19979;&#25991;&#40065;&#26834;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;&#20173;&#28982;&#26080;&#27861;&#24456;&#22909;&#22320;&#24212;&#23545;&#19978;&#19979;&#25991;&#25200;&#21160;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#65288;GEC&#65289;&#31995;&#32479;&#22312;&#24110;&#21161;&#20154;&#20204;&#26085;&#24120;&#20889;&#20316;&#20219;&#21153;&#20013;&#36215;&#21040;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#29992;&#25143;&#26377;&#26102;&#21487;&#33021;&#20250;&#36935;&#21040;&#19968;&#20123;&#21021;&#22987;&#34920;&#29616;&#33391;&#22909;&#20294;&#36935;&#21040;&#36755;&#20837;&#24494;&#23567;&#20462;&#25913;&#26102;&#26080;&#27861;&#20462;&#27491;&#38169;&#35823;&#30340;GEC&#31995;&#32479;&#12290;&#20026;&#20102;&#30830;&#20445;&#29702;&#24819;&#30340;&#29992;&#25143;&#20307;&#39564;&#65292;&#21487;&#38752;&#30340;GEC&#31995;&#32479;&#24212;&#33021;&#22815;&#22312;&#36935;&#21040;&#26080;&#20851;&#19978;&#19979;&#25991;&#25200;&#21160;&#26102;&#25552;&#20379;&#19968;&#33268;&#19988;&#20934;&#30830;&#30340;&#24314;&#35758;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#19978;&#19979;&#25991;&#40065;&#26834;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;RobustGEC&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;GEC&#31995;&#32479;&#19978;&#19979;&#25991;&#40065;&#26834;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;RobustGEC&#21253;&#21547;5,000&#20010;GEC&#26696;&#20363;&#65292;&#27599;&#20010;&#26696;&#20363;&#30001;&#19968;&#21477;&#21407;&#22987;&#30340;&#38169;&#35823;-&#20462;&#27491;&#21477;&#23376;&#23545;&#21644;&#30001;&#20154;&#24037;&#26631;&#27880;&#32773;&#31934;&#24515;&#35774;&#35745;&#30340;&#20116;&#20010;&#21464;&#20307;&#32452;&#25104;&#12290;&#21033;&#29992;RobustGEC&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;GEC&#31995;&#32479;&#22312;&#38754;&#23545;&#19978;&#19979;&#25991;&#25200;&#21160;&#26102;&#20173;&#28982;&#32570;&#20047;&#36275;&#22815;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Grammatical Error Correction (GEC) systems play a vital role in assisting people with their daily writing tasks. However, users may sometimes come across a GEC system that initially performs well but fails to correct errors when the inputs are slightly modified. To ensure an ideal user experience, a reliable GEC system should have the ability to provide consistent and accurate suggestions when encountering irrelevant context perturbations, which we refer to as context robustness. In this paper, we introduce RobustGEC, a benchmark designed to evaluate the context robustness of GEC systems. RobustGEC comprises 5,000 GEC cases, each with one original error-correct sentence pair and five variants carefully devised by human annotators. Utilizing RobustGEC, we reveal that state-of-the-art GEC systems still lack sufficient robustness against context perturbations. In addition, we propose a simple yet effective method for remitting this issue.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#30693;&#35782;&#29983;&#25104;&#22120;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#20171;&#32461;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;CONNER&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29983;&#25104;&#30340;&#30693;&#35782;&#30340;&#20107;&#23454;&#24615;&#19981;&#22914;&#20934;&#30830;&#24615;&#37325;&#35201;&#65292;&#32780;&#29983;&#25104;&#30340;&#30456;&#20851;&#24615;&#21644;&#36830;&#36143;&#24615;&#26356;&#20026;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2310.07289</link><description>&lt;p&gt;
&#36229;&#36234;&#20107;&#23454;&#24615;&#65306;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#30693;&#35782;&#29983;&#25104;&#22120;&#30340;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Beyond Factuality: A Comprehensive Evaluation of Large Language Models as Knowledge Generators. (arXiv:2310.07289v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07289
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#30693;&#35782;&#29983;&#25104;&#22120;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#20171;&#32461;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;CONNER&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29983;&#25104;&#30340;&#30693;&#35782;&#30340;&#20107;&#23454;&#24615;&#19981;&#22914;&#20934;&#30830;&#24615;&#37325;&#35201;&#65292;&#32780;&#29983;&#25104;&#30340;&#30456;&#20851;&#24615;&#21644;&#36830;&#36143;&#24615;&#26356;&#20026;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25552;&#31034;&#29983;&#25104;&#19990;&#30028;&#30693;&#35782;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20248;&#20110;&#20449;&#24687;&#26816;&#32034;&#25216;&#26415;&#65292;&#29992;&#20110;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#31038;&#21306;&#23545;&#20351;&#29992;&#36825;&#31181;&#26410;&#32463;&#23457;&#26597;&#30340;&#30693;&#35782;&#30340;&#20107;&#23454;&#24615;&#21644;&#28508;&#22312;&#24433;&#21709;&#30340;&#25285;&#24551;&#19981;&#32477;&#20110;&#32819;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CONNER&#65292;&#19968;&#31181;&#32508;&#21512;&#30340;&#30693;&#35782;&#35780;&#20272;&#26694;&#26550;&#65292;&#26088;&#22312;&#31995;&#32479;&#22320;&#21644;&#33258;&#21160;&#22320;&#20174;&#20845;&#20010;&#37325;&#35201;&#30340;&#35282;&#24230;&#35780;&#20272;&#29983;&#25104;&#30340;&#30693;&#35782;--&#20107;&#23454;&#24615;&#12289;&#30456;&#20851;&#24615;&#12289;&#36830;&#36143;&#24615;&#12289;&#20449;&#24687;&#24615;&#12289;&#26377;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23545;&#26469;&#33258;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;LLMs&#30340;&#29983;&#25104;&#30693;&#35782;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#36825;&#19977;&#31181;LLMs&#29992;&#20110;&#20004;&#20010;&#24191;&#27867;&#30740;&#31350;&#30340;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#65292;&#21363;&#24320;&#25918;&#22495;&#38382;&#31572;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#23545;&#35805;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#29983;&#25104;&#30340;&#30693;&#35782;&#30340;&#20107;&#23454;&#24615;&#36739;&#20302;&#65292;&#20063;&#19981;&#20250;&#26174;&#33879;&#38459;&#30861;&#19979;&#28216;&#20219;&#21153;&#12290;&#30456;&#21453;&#65292;&#20135;&#20986;&#30340;&#30456;&#20851;&#24615;&#21644;&#36830;&#36143;&#24615;&#27604;&#20934;&#30830;&#24230;&#26356;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) outperform information retrieval techniques for downstream knowledge-intensive tasks when being prompted to generate world knowledge. However, community concerns abound regarding the factuality and potential implications of using this uncensored knowledge. In light of this, we introduce CONNER, a COmpreheNsive kNowledge Evaluation fRamework, designed to systematically and automatically evaluate generated knowledge from six important perspectives -- Factuality, Relevance, Coherence, Informativeness, Helpfulness and Validity. We conduct an extensive empirical analysis of the generated knowledge from three different types of LLMs on two widely studied knowledge-intensive tasks, i.e., open-domain question answering and knowledge-grounded dialogue. Surprisingly, our study reveals that the factuality of generated knowledge, even if lower, does not significantly hinder downstream tasks. Instead, the relevance and coherence of the outputs are more important than sm
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLM-TSE&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#29992;&#25143;&#38190;&#20837;&#30340;&#25991;&#26412;&#36755;&#20837;&#20013;&#25552;&#21462;&#35821;&#20041;&#32447;&#32034;&#65292;&#20197;&#22686;&#24378;&#30446;&#26631;&#35828;&#35805;&#20154;&#25552;&#21462;(TSE)&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#21644;&#21487;&#25511;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.07284</link><description>&lt;p&gt;
&#25171;&#23383;&#20542;&#21548;&#40481;&#23614;&#37202;&#20250;&#65306;&#25991;&#26412;&#24341;&#23548;&#30340;&#30446;&#26631;&#35828;&#35805;&#20154;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Typing to Listen at the Cocktail Party: Text-Guided Target Speaker Extraction. (arXiv:2310.07284v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07284
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLM-TSE&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#29992;&#25143;&#38190;&#20837;&#30340;&#25991;&#26412;&#36755;&#20837;&#20013;&#25552;&#21462;&#35821;&#20041;&#32447;&#32034;&#65292;&#20197;&#22686;&#24378;&#30446;&#26631;&#35828;&#35805;&#20154;&#25552;&#21462;(TSE)&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#21644;&#21487;&#25511;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#25317;&#26377;&#19968;&#31181;&#22312;&#22797;&#26434;&#30340;&#22768;&#23398;&#29615;&#22659;&#20013;&#26377;&#36873;&#25321;&#24615;&#22320;&#19987;&#27880;&#20110;&#24863;&#20852;&#36259;&#30340;&#22768;&#38899;&#28304;&#30340;&#38750;&#20961;&#33021;&#21147;&#65292;&#36890;&#24120;&#31216;&#20026;&#40481;&#23614;&#37202;&#20250;&#22330;&#26223;&#12290;&#20026;&#20102;&#22312;&#26426;&#22120;&#20013;&#22797;&#21046;&#36825;&#31181;&#24341;&#20154;&#27880;&#30446;&#30340;&#21548;&#35273;&#27880;&#24847;&#33021;&#21147;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#30446;&#26631;&#35828;&#35805;&#20154;&#25552;&#21462;(TSE)&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#21033;&#29992;&#30446;&#26631;&#35828;&#35805;&#20154;&#30340;&#39044;&#20808;&#27880;&#20876;&#32447;&#32034;&#26469;&#25552;&#21462;&#24863;&#20852;&#36259;&#30340;&#22768;&#28304;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#24773;&#26223;&#20013;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#20102;&#39044;&#20808;&#27880;&#20876;&#32447;&#32034;&#30340;&#21487;&#33021;&#21464;&#21270;&#29978;&#33267;&#32570;&#22833;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#23558;&#33258;&#28982;&#35821;&#35328;&#25972;&#21512;&#21040;&#29616;&#26377;TSE&#27169;&#22411;&#20013;&#20197;&#22686;&#24378;&#20854;&#28789;&#27963;&#24615;&#21644;&#21487;&#25511;&#24615;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LLM-TSE&#30340;&#27169;&#22411;&#65292;&#20854;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20174;&#29992;&#25143;&#30340;&#38190;&#20837;&#25991;&#26412;&#36755;&#20837;&#20013;&#25552;&#21462;&#26377;&#29992;&#30340;&#35821;&#20041;&#32447;&#32034;&#65292;&#36825;&#20123;&#32447;&#32034;&#21487;&#20197;&#34917;&#20805;&#39044;&#20808;&#27880;&#20876;&#30340;&#32447;&#32034;&#25110;&#29420;&#31435;&#24037;&#20316;&#20197;&#25511;&#21046;TSE&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans possess an extraordinary ability to selectively focus on the sound source of interest amidst complex acoustic environments, commonly referred to as cocktail party scenarios. In an attempt to replicate this remarkable auditory attention capability in machines, target speaker extraction (TSE) models have been developed. These models leverage the pre-registered cues of the target speaker to extract the sound source of interest. However, the effectiveness of these models is hindered in real-world scenarios due to the potential variation or even absence of pre-registered cues. To address this limitation, this study investigates the integration of natural language to enhance the flexibility and controllability of existing TSE models. Specifically, we propose a model named LLM-TSE, wherein a large language model (LLM) to extract useful semantic cues from the user's typed text input, which can complement the pre-registered cues or work independently to control the TSE process. Our exper
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#23588;&#20854;&#26159;BioBERT&#65289;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#24494;&#35843;&#26041;&#27861;&#12290;&#30740;&#31350;&#31361;&#20986;&#20102;BioBERT&#23545;&#20110;&#35299;&#20915;&#19982;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#30456;&#20851;&#20219;&#21153;&#30340;&#29305;&#23450;&#35201;&#27714;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.07282</link><description>&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#26512;&#65306;BioBERT &#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Analysis on Large Language Models in Healthcare: A Case Study of BioBERT. (arXiv:2310.07282v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07282
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#23588;&#20854;&#26159;BioBERT&#65289;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#24494;&#35843;&#26041;&#27861;&#12290;&#30740;&#31350;&#31361;&#20986;&#20102;BioBERT&#23545;&#20110;&#35299;&#20915;&#19982;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#30456;&#20851;&#20219;&#21153;&#30340;&#29305;&#23450;&#35201;&#27714;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#23588;&#20854;&#26159;BioBERT&#65289;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#30740;&#31350;&#12290;&#23427;&#39318;&#20808;&#24443;&#24213;&#26816;&#26597;&#20102;&#20808;&#21069;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#24212;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26041;&#27861;&#30340;&#24773;&#20917;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#38754;&#20020;&#30340;&#23616;&#38480;&#21644;&#25361;&#25112;&#12290;&#38543;&#21518;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;BioBERT&#25972;&#21512;&#21040;&#21307;&#30103;&#20445;&#20581;&#24212;&#29992;&#20013;&#30340;&#36335;&#24452;&#65292;&#31361;&#20986;&#20854;&#36866;&#29992;&#20110;&#35299;&#20915;&#19982;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#30456;&#20851;&#20219;&#21153;&#30340;&#29305;&#23450;&#35201;&#27714;&#12290;&#35813;&#20998;&#26512;&#27010;&#36848;&#20102;&#29992;&#20110;&#38024;&#23545;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#29420;&#29305;&#38656;&#27714;&#24494;&#35843;BioBERT&#30340;&#31995;&#32479;&#26041;&#27861;&#35770;&#12290;&#36825;&#20010;&#26041;&#27861;&#21253;&#25324;&#20174;&#21508;&#31181;&#21307;&#30103;&#20445;&#20581;&#26469;&#28304;&#25910;&#38598;&#25968;&#25454;&#65292;&#20026;&#35782;&#21035;&#21307;&#30103;&#23454;&#20307;&#21644;&#23545;&#20854;&#36827;&#34892;&#20998;&#31867;&#31561;&#20219;&#21153;&#36827;&#34892;&#25968;&#25454;&#27880;&#37322;&#65292;&#24182;&#24212;&#29992;&#19987;&#38376;&#38024;&#23545;&#22788;&#29702;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#20013;&#22797;&#26434;&#24615;&#30340;&#39044;&#22788;&#29702;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper conducts a comprehensive investigation into applying large language models, particularly on BioBERT, in healthcare. It begins with thoroughly examining previous natural language processing (NLP) approaches in healthcare, shedding light on the limitations and challenges these methods face. Following that, this research explores the path that led to the incorporation of BioBERT into healthcare applications, highlighting its suitability for addressing the specific requirements of tasks related to biomedical text mining. The analysis outlines a systematic methodology for fine-tuning BioBERT to meet the unique needs of the healthcare domain. This approach includes various components, including the gathering of data from a wide range of healthcare sources, data annotation for tasks like identifying medical entities and categorizing them, and the application of specialized preprocessing techniques tailored to handle the complexities found in biomedical texts. Additionally, the pape
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#25552;&#39640;&#26080;&#25991;&#26412;&#35821;&#38899;&#32763;&#35793;&#31995;&#32479;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#36716;&#25442;&#65292;&#36890;&#36807;&#36816;&#29992;&#22810;&#35821;&#35328;&#24773;&#24863;&#23884;&#20837;&#26469;&#25429;&#25417;&#35821;&#35328;&#26080;&#20851;&#30340;&#20449;&#24687;&#65292;&#24182;&#26377;&#25928;&#39044;&#27979;&#30446;&#26631;&#35821;&#35328;&#30340;&#38899;&#35843;&#21644;&#26102;&#38271;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#30456;&#23545;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#31995;&#32479;&#22312;&#34920;&#36798;&#33021;&#21147;&#36716;&#25442;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.07279</link><description>&lt;p&gt;
&#25552;&#21319;&#26080;&#25991;&#26412;&#35821;&#38899;&#32763;&#35793;&#31995;&#32479;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Enhancing expressivity transfer in textless speech-to-speech translation. (arXiv:2310.07279v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07279
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#25552;&#39640;&#26080;&#25991;&#26412;&#35821;&#38899;&#32763;&#35793;&#31995;&#32479;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#36716;&#25442;&#65292;&#36890;&#36807;&#36816;&#29992;&#22810;&#35821;&#35328;&#24773;&#24863;&#23884;&#20837;&#26469;&#25429;&#25417;&#35821;&#35328;&#26080;&#20851;&#30340;&#20449;&#24687;&#65292;&#24182;&#26377;&#25928;&#39044;&#27979;&#30446;&#26631;&#35821;&#35328;&#30340;&#38899;&#35843;&#21644;&#26102;&#38271;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#30456;&#23545;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#31995;&#32479;&#22312;&#34920;&#36798;&#33021;&#21147;&#36716;&#25442;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#30340;&#25972;&#21512;&#19979;&#65292;&#26080;&#25991;&#26412;&#35821;&#38899;&#32763;&#35793;&#31995;&#32479;&#27491;&#22312;&#24555;&#36895;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#31995;&#32479;&#22312;&#20934;&#30830;&#25429;&#25417;&#21644;&#36716;&#25442;&#19981;&#21516;&#35821;&#35328;&#30340;&#34920;&#36798;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#34920;&#36798;&#33021;&#21147;&#22312;&#20256;&#36798;&#24773;&#24863;&#12289;&#32454;&#24494;&#24046;&#21035;&#21644;&#25991;&#21270;&#32454;&#24494;&#20043;&#22788;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#20174;&#32780;&#22686;&#24378;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;&#20132;&#27969;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#31163;&#25955;&#35821;&#38899;&#21333;&#20803;&#32423;&#21035;&#19978;&#36816;&#20316;&#65292;&#24182;&#21033;&#29992;&#22810;&#35821;&#35328;&#24773;&#24863;&#23884;&#20837;&#26469;&#25429;&#25417;&#35821;&#35328;&#26080;&#20851;&#30340;&#20449;&#24687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#23884;&#20837;&#26469;&#26377;&#25928;&#39044;&#27979;&#30446;&#26631;&#35821;&#35328;&#30340;&#38899;&#35843;&#21644;&#26102;&#38271;&#12290;&#36890;&#36807;&#23545;&#27861;&#35821;&#21040;&#33521;&#35821;&#32763;&#35793;&#20219;&#21153;&#36827;&#34892;&#30340;&#23458;&#35266;&#21644;&#20027;&#35266;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#31361;&#20986;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#31995;&#32479;&#22312;&#34920;&#36798;&#33021;&#21147;&#36716;&#25442;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Textless speech-to-speech translation systems are rapidly advancing, thanks to the integration of self-supervised learning techniques. However, existing state-of-the-art systems fall short when it comes to capturing and transferring expressivity accurately across different languages. Expressivity plays a vital role in conveying emotions, nuances, and cultural subtleties, thereby enhancing communication across diverse languages. To address this issue this study presents a novel method that operates at the discrete speech unit level and leverages multilingual emotion embeddings to capture language-agnostic information. Specifically, we demonstrate how these embeddings can be used to effectively predict the pitch and duration of speech units in the target language. Through objective and subjective experiments conducted on a French-to-English translation task, our findings highlight the superior expressivity transfer achieved by our approach compared to current state-of-the-art systems.
&lt;/p&gt;</description></item><item><title>BioT5&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#22312;&#29983;&#29289;&#23398;&#20013;&#21033;&#29992;&#21270;&#23398;&#30693;&#35782;&#21644;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#20016;&#23500;&#20102;&#36328;&#27169;&#24577;&#25972;&#21512;&#65292;&#36890;&#36807;&#40065;&#26834;&#30340;&#20998;&#23376;&#34920;&#31034;&#21644;&#19978;&#19979;&#25991;&#30693;&#35782;&#25552;&#21462;&#65292;&#23454;&#29616;&#20102;&#26356;&#26377;&#25928;&#30340;&#20449;&#24687;&#21033;&#29992;&#65292;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.07276</link><description>&lt;p&gt;
BioT5&#65306;&#22312;&#29983;&#29289;&#23398;&#20013;&#21033;&#29992;&#21270;&#23398;&#30693;&#35782;&#21644;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#20016;&#23500;&#36328;&#27169;&#24577;&#25972;&#21512;
&lt;/p&gt;
&lt;p&gt;
BioT5: Enriching Cross-modal Integration in Biology with Chemical Knowledge and Natural Language Associations. (arXiv:2310.07276v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07276
&lt;/p&gt;
&lt;p&gt;
BioT5&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#22312;&#29983;&#29289;&#23398;&#20013;&#21033;&#29992;&#21270;&#23398;&#30693;&#35782;&#21644;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#20016;&#23500;&#20102;&#36328;&#27169;&#24577;&#25972;&#21512;&#65292;&#36890;&#36807;&#40065;&#26834;&#30340;&#20998;&#23376;&#34920;&#31034;&#21644;&#19978;&#19979;&#25991;&#30693;&#35782;&#25552;&#21462;&#65292;&#23454;&#29616;&#20102;&#26356;&#26377;&#25928;&#30340;&#20449;&#24687;&#21033;&#29992;&#65292;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#29983;&#29289;&#30740;&#31350;&#39046;&#22495;&#30340;&#36827;&#23637;&#21033;&#29992;&#20998;&#23376;&#12289;&#34507;&#30333;&#36136;&#21644;&#33258;&#28982;&#35821;&#35328;&#30340;&#25972;&#21512;&#26469;&#22686;&#24378;&#33647;&#29289;&#21457;&#29616;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#27169;&#22411;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#22914;&#29983;&#25104;&#26080;&#25928;&#30340;&#20998;&#23376;SMILES&#12289;&#23545;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#21033;&#29992;&#19981;&#36275;&#20197;&#21450;&#23545;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#30340;&#31561;&#37327;&#22788;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;BioT5&#65292;&#23427;&#36890;&#36807;&#21270;&#23398;&#30693;&#35782;&#21644;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#20016;&#23500;&#20102;&#29983;&#29289;&#23398;&#20013;&#30340;&#36328;&#27169;&#24577;&#25972;&#21512;&#12290;BioT5&#21033;&#29992;SELFIES&#36827;&#34892;100%&#40065;&#26834;&#30340;&#20998;&#23376;&#34920;&#31034;&#65292;&#24182;&#20174;&#38750;&#32467;&#26500;&#21270;&#30340;&#29983;&#29289;&#25991;&#29486;&#20013;&#25552;&#21462;&#29983;&#29289;&#23454;&#20307;&#21608;&#22260;&#19978;&#19979;&#25991;&#30340;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;BioT5&#21306;&#20998;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#20174;&#32780;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#20449;&#24687;&#12290;&#22312;&#24494;&#35843;&#21518;&#65292;BioT5&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#34920;&#26126;&#20854;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in biological research leverage the integration of molecules, proteins, and natural language to enhance drug discovery. However, current models exhibit several limitations, such as the generation of invalid molecular SMILES, underutilization of contextual information, and equal treatment of structured and unstructured knowledge. To address these issues, we propose $\mathbf{BioT5}$, a comprehensive pre-training framework that enriches cross-modal integration in biology with chemical knowledge and natural language associations. $\mathbf{BioT5}$ utilizes SELFIES for $100%$ robust molecular representations and extracts knowledge from the surrounding context of bio-entities in unstructured biological literature. Furthermore, $\mathbf{BioT5}$ distinguishes between structured and unstructured knowledge, leading to more effective utilization of information. After fine-tuning, BioT5 shows superior performance across a wide range of tasks, demonstrating its strong capability 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;LLM&#20013;&#24212;&#29992;&#36890;&#29992;&#20262;&#29702;&#25512;&#29702;&#33021;&#21147;&#32780;&#38750;&#29305;&#23450;&#20262;&#29702;&#21407;&#21017;&#30340;&#26041;&#27861;&#65292;&#20197;&#22788;&#29702;&#20840;&#29699;&#33539;&#22260;&#30340;&#20215;&#20540;&#22810;&#20803;&#24615;&#12290;&#20316;&#32773;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#36947;&#24503;&#22256;&#22659;&#19982;&#19981;&#21516;&#24418;&#24335;&#30340;&#35268;&#33539;&#20262;&#29702;&#20197;&#21450;&#19981;&#21516;&#25277;&#35937;&#23618;&#27425;&#30340;&#36947;&#24503;&#21407;&#21017;&#30456;&#32467;&#21512;&#12290;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;&#23613;&#31649;GPT-4&#20960;&#20046;&#21487;&#20197;&#23436;&#32654;&#22320;&#36827;&#34892;&#20262;&#29702;&#25512;&#29702;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#20173;&#28982;&#23384;&#22312;&#23545;&#35199;&#26041;&#21644;&#20197;&#33521;&#35821;&#20026;&#27597;&#35821;&#30340;&#31038;&#20250;&#36947;&#24503;&#20215;&#20540;&#30340;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2310.07251</link><description>&lt;p&gt;
&#22522;&#20110;&#20262;&#29702;&#25512;&#29702;&#30340;&#36947;&#24503;&#23545;&#40784;&#65306;&#20851;&#20110;&#22312;LLM&#20013;&#19978;&#19979;&#25991;&#20262;&#29702;&#25919;&#31574;&#30340;&#26696;&#20363;&#21644;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Ethical Reasoning over Moral Alignment: A Case and Framework for In-Context Ethical Policies in LLMs. (arXiv:2310.07251v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;LLM&#20013;&#24212;&#29992;&#36890;&#29992;&#20262;&#29702;&#25512;&#29702;&#33021;&#21147;&#32780;&#38750;&#29305;&#23450;&#20262;&#29702;&#21407;&#21017;&#30340;&#26041;&#27861;&#65292;&#20197;&#22788;&#29702;&#20840;&#29699;&#33539;&#22260;&#30340;&#20215;&#20540;&#22810;&#20803;&#24615;&#12290;&#20316;&#32773;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#36947;&#24503;&#22256;&#22659;&#19982;&#19981;&#21516;&#24418;&#24335;&#30340;&#35268;&#33539;&#20262;&#29702;&#20197;&#21450;&#19981;&#21516;&#25277;&#35937;&#23618;&#27425;&#30340;&#36947;&#24503;&#21407;&#21017;&#30456;&#32467;&#21512;&#12290;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;&#23613;&#31649;GPT-4&#20960;&#20046;&#21487;&#20197;&#23436;&#32654;&#22320;&#36827;&#34892;&#20262;&#29702;&#25512;&#29702;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#20173;&#28982;&#23384;&#22312;&#23545;&#35199;&#26041;&#21644;&#20197;&#33521;&#35821;&#20026;&#27597;&#35821;&#30340;&#31038;&#20250;&#36947;&#24503;&#20215;&#20540;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#19982;&#20854;&#23545;LLM&#36827;&#34892;&#36947;&#24503;&#23545;&#40784;&#21040;&#29305;&#23450;&#30340;&#20262;&#29702;&#21407;&#21017;&#65292;&#25105;&#20204;&#24212;&#35813;&#27880;&#20837;&#36890;&#29992;&#30340;&#20262;&#29702;&#25512;&#29702;&#33021;&#21147;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#20840;&#29699;&#33539;&#22260;&#30340;&#20215;&#20540;&#22810;&#20803;&#24615;&#12290;&#24403;&#25552;&#20379;&#20262;&#29702;&#25919;&#31574;&#26102;&#65292;LLM&#24212;&#35813;&#33021;&#22815;&#20570;&#20986;&#19982;&#25919;&#31574;&#19968;&#33268;&#30340;&#20262;&#29702;&#20915;&#31574;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#36947;&#24503;&#22256;&#22659;&#19982;&#19981;&#21516;&#24418;&#24335;&#30340;&#35268;&#33539;&#20262;&#29702;&#20197;&#21450;&#19981;&#21516;&#25277;&#35937;&#23618;&#27425;&#30340;&#36947;&#24503;&#21407;&#21017;&#30456;&#32467;&#21512;&#12290;&#23545;GPT-x&#27169;&#22411;&#30340;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;&#34429;&#28982;GPT-4&#20960;&#20046;&#21487;&#20197;&#23436;&#32654;&#22320;&#36827;&#34892;&#20262;&#29702;&#25512;&#29702;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#20173;&#28982;&#23384;&#22312;&#23545;&#35199;&#26041;&#21644;&#20197;&#33521;&#35821;&#20026;&#27597;&#35821;&#30340;&#31038;&#20250;&#36947;&#24503;&#20215;&#20540;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this position paper, we argue that instead of morally aligning LLMs to specific set of ethical principles, we should infuse generic ethical reasoning capabilities into them so that they can handle value pluralism at a global scale. When provided with an ethical policy, an LLM should be capable of making decisions that are ethically consistent to the policy. We develop a framework that integrates moral dilemmas with moral principles pertaining to different foramlisms of normative ethics, and at different levels of abstractions. Initial experiments with GPT-x models shows that while GPT-4 is a nearly perfect ethical reasoner, the models still have bias towards the moral values of Western and English speaking societies.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35780;&#20272;&#22810;&#31181;&#27969;&#34892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#38382;&#39064;&#26041;&#38754;&#30340;&#30693;&#35782;&#65292;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#23545;&#36825;&#20123;&#27169;&#22411;&#20316;&#20026;&#19968;&#20010;&#32676;&#20307;&#30340;&#21021;&#27493;&#35266;&#23519;&#65292;&#24182;&#25552;&#20986;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07225</link><description>&lt;p&gt;
&#22312;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#20013;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;: &#35266;&#23519;&#21644;&#24320;&#25918;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Exploring the Landscape of Large Language Models In Medical Question Answering: Observations and Open Questions. (arXiv:2310.07225v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07225
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35780;&#20272;&#22810;&#31181;&#27969;&#34892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#38382;&#39064;&#26041;&#38754;&#30340;&#30693;&#35782;&#65292;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#23545;&#36825;&#20123;&#27169;&#22411;&#20316;&#20026;&#19968;&#20010;&#32676;&#20307;&#30340;&#21021;&#27493;&#35266;&#23519;&#65292;&#24182;&#25552;&#20986;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#39046;&#22495;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#36890;&#36807;&#22312;&#26631;&#20934;&#21270;&#32771;&#35797;&#20013;&#21462;&#24471;&#21450;&#26684;&#20998;&#25968;&#65292;&#24182;&#34987;&#35748;&#20026;&#26159;&#25903;&#25345;&#21307;&#30103;&#20445;&#20581;&#24037;&#20316;&#32773;&#30340;&#24037;&#20855;&#12290;&#23558;LLMs&#37096;&#32626;&#21040;&#22914;&#27492;&#39640;&#39118;&#38505;&#30340;&#29615;&#22659;&#20013;&#38656;&#35201;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#38480;&#21046;&#26377;&#28165;&#26224;&#30340;&#29702;&#35299;&#12290;&#38543;&#30528;&#26032;&#30340;LLMs&#30340;&#24555;&#36895;&#21457;&#23637;&#21644;&#21457;&#24067;&#65292;&#35782;&#21035;&#36328;&#27169;&#22411;&#23384;&#22312;&#30340;&#27169;&#24335;&#65292;&#24182;&#22240;&#27492;&#21487;&#33021;&#20986;&#29616;&#22312;&#26032;&#29256;&#26412;&#20013;&#65292;&#29305;&#21035;&#26377;&#20215;&#20540;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22810;&#31181;&#27969;&#34892;LLM&#22312;&#21307;&#23398;&#38382;&#39064;&#26041;&#38754;&#30340;&#30693;&#35782;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#23427;&#20204;&#20316;&#20026;&#19968;&#20010;&#32676;&#20307;&#30340;&#29305;&#24615;&#12290;&#36890;&#36807;&#36825;&#20010;&#27604;&#36739;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21021;&#27493;&#30340;&#35266;&#23519;&#65292;&#24182;&#25552;&#20986;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown promise in medical question answering by achieving passing scores in standardised exams and have been suggested as tools for supporting healthcare workers. Deploying LLMs into such a high-risk context requires a clear understanding of the limitations of these models. With the rapid development and release of new LLMs, it is especially valuable to identify patterns which exist across models and may, therefore, continue to appear in newer versions. In this paper, we evaluate a wide range of popular LLMs on their knowledge of medical questions in order to better understand their properties as a group. From this comparison, we provide preliminary observations and raise open questions for further research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#38376;&#25511;&#30340;&#28151;&#21512;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#31574;&#30053;&#65292;&#36890;&#36807;&#26681;&#25454;&#26631;&#35760;&#30340;&#19987;&#23478;&#27010;&#29575;&#20998;&#24067;&#23558;&#26631;&#35760;&#20998;&#37197;&#32473;&#21464;&#37327;&#25968;&#37327;&#30340;&#19987;&#23478;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#30340;&#31232;&#30095;&#24615;&#21644;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.07188</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#38376;&#25511;&#22312;&#22522;&#20110;&#28151;&#21512;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Adaptive Gating in Mixture-of-Experts based Language Models. (arXiv:2310.07188v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#38376;&#25511;&#30340;&#28151;&#21512;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#31574;&#30053;&#65292;&#36890;&#36807;&#26681;&#25454;&#26631;&#35760;&#30340;&#19987;&#23478;&#27010;&#29575;&#20998;&#24067;&#23558;&#26631;&#35760;&#20998;&#37197;&#32473;&#21464;&#37327;&#25968;&#37327;&#30340;&#19987;&#23478;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#30340;&#31232;&#30095;&#24615;&#21644;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;OpenAI&#30340;ChatGPT&#65289;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;&#31232;&#30095;&#28608;&#27963;&#30340;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#35745;&#31639;&#25805;&#20316;&#25968;&#37327;&#24658;&#23450;&#30340;&#21516;&#26102;&#25193;&#22823;&#27169;&#22411;&#35268;&#27169;&#12290;&#29616;&#26377;&#30340;MoE&#27169;&#22411;&#37319;&#29992;&#20102;&#22266;&#23450;&#30340;&#38376;&#25511;&#32593;&#32476;&#65292;&#27599;&#20010;&#26631;&#35760;&#37117;&#30001;&#30456;&#21516;&#25968;&#37327;&#30340;&#19987;&#23478;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#19982;&#25105;&#20204;&#30340;&#30452;&#35273;&#30456;&#30683;&#30462;&#65292;&#22240;&#20026;&#27599;&#20010;&#24207;&#21015;&#20013;&#30340;&#26631;&#35760;&#22312;&#35821;&#35328;&#22797;&#26434;&#24615;&#26041;&#38754;&#26377;&#25152;&#19981;&#21516;&#65292;&#22240;&#27492;&#38656;&#35201;&#19981;&#21516;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#24456;&#23569;&#35752;&#35770;&#27599;&#20010;&#26631;&#35760;&#30340;&#35745;&#31639;&#21644;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;MoE&#20013;&#20351;&#29992;&#33258;&#36866;&#24212;&#38376;&#25511;&#30340;&#28789;&#27963;&#35757;&#32451;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#21487;&#20197;&#26681;&#25454;&#19987;&#23478;&#27010;&#29575;&#20998;&#24067;&#23558;&#26631;&#35760;&#22788;&#29702;&#20026;&#21487;&#21464;&#25968;&#37327;&#30340;&#19987;&#23478;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#25913;&#36827;&#35757;&#32451;&#25928;&#29575;&#30340;&#21516;&#26102;&#20445;&#25345;&#31232;&#30095;&#24615;&#12290;&#27492;&#22806;&#65292;&#35838;&#31243;&#23398;&#20064;&#20063;&#22312;&#35813;&#26694;&#26550;&#20013;&#24212;&#29992;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models, such as OpenAI's ChatGPT, have demonstrated exceptional language understanding capabilities in various NLP tasks. Sparsely activated mixture-of-experts (MoE) has emerged as a promising solution for scaling models while maintaining a constant number of computational operations. Existing MoE model adopts a fixed gating network where each token is computed by the same number of experts. However, this approach contradicts our intuition that the tokens in each sequence vary in terms of their linguistic complexity and, consequently, require different computational costs. Little is discussed in prior research on the trade-off between computation per token and model performance. This paper introduces adaptive gating in MoE, a flexible training strategy that allows tokens to be processed by a variable number of experts based on expert probability distribution. The proposed framework preserves sparsity while improving training efficiency. Additionally, curriculum learning 
&lt;/p&gt;</description></item><item><title>&#22312;&#32447;&#25512;&#27979;&#35299;&#30721;&#26159;&#36890;&#36807;&#21033;&#29992;&#22810;&#20313;&#35745;&#31639;&#33021;&#21147;&#65292;&#22312;LLM&#26381;&#21153;&#38598;&#32676;&#20013;&#25345;&#32493;&#26356;&#26032;&#33609;&#31295;&#27169;&#22411;&#65292;&#20174;&#32780;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.07177</link><description>&lt;p&gt;
&#22312;&#32447;&#25512;&#27979;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Online Speculative Decoding. (arXiv:2310.07177v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07177
&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#25512;&#27979;&#35299;&#30721;&#26159;&#36890;&#36807;&#21033;&#29992;&#22810;&#20313;&#35745;&#31639;&#33021;&#21147;&#65292;&#22312;LLM&#26381;&#21153;&#38598;&#32676;&#20013;&#25345;&#32493;&#26356;&#26032;&#33609;&#31295;&#27169;&#22411;&#65292;&#20174;&#32780;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#27979;&#35299;&#30721;&#26159;&#36890;&#36807;&#21033;&#29992;&#36739;&#23567;&#30340;&#33609;&#31295;&#27169;&#22411;&#26469;&#39044;&#27979;&#30446;&#26631;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25512;&#29702;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22312;&#38754;&#23545;&#22810;&#26679;&#30340;&#25991;&#26412;&#36755;&#20837;&#21644;&#33609;&#31295;&#27169;&#22411;&#19982;&#30446;&#26631;&#27169;&#22411;&#20043;&#38388;&#30340;&#26174;&#33879;&#33021;&#21147;&#24046;&#36317;&#26102;&#65292;&#20854;&#26377;&#25928;&#24615;&#21487;&#33021;&#21463;&#21040;&#38480;&#21046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22312;&#32447;&#25512;&#27979;&#35299;&#30721;&#65288;OSD&#65289;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#20854;&#20027;&#35201;&#24605;&#24819;&#26159;&#21033;&#29992;LLM&#26381;&#21153;&#38598;&#32676;&#20013;&#20016;&#23500;&#30340;&#22810;&#20313;&#35745;&#31639;&#33021;&#21147;&#65292;&#26681;&#25454;&#35266;&#23519;&#21040;&#30340;&#29992;&#25143;&#26597;&#35810;&#25968;&#25454;&#25345;&#32493;&#26356;&#26032;&#65288;&#22810;&#20010;&#65289;&#33609;&#31295;&#27169;&#22411;&#12290;&#30001;&#20110;LLM&#25512;&#29702;&#21463;&#20869;&#23384;&#38480;&#21046;&#65292;&#20856;&#22411;&#30340;LLM&#26381;&#21153;&#38598;&#32676;&#20013;&#30340;&#21097;&#20313;&#35745;&#31639;&#33021;&#21147;&#21487;&#20197;&#29992;&#20110;&#22312;&#32447;&#37325;&#26032;&#35757;&#32451;&#33609;&#31295;&#27169;&#22411;&#65292;&#20174;&#32780;&#20351;&#35757;&#32451;&#25104;&#26412;&#20445;&#25345;&#20013;&#24615;&#12290;&#30001;&#20110;LLM&#26381;&#21153;&#30340;&#26597;&#35810;&#20998;&#24067;&#30456;&#23545;&#31616;&#21333;&#65292;&#26681;&#25454;&#26597;&#35810;&#20998;&#24067;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#21487;&#20197;&#20351;&#33609;&#31295;&#27169;&#22411;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#30446;&#26631;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speculative decoding is a pivotal technique to accelerate the inference of large language models (LLMs) by employing a smaller draft model to predict the target model's outputs. However, its efficacy can be limited due to the low predictive accuracy of the draft model, particularly when faced with diverse text inputs and a significant capability gap between the draft and target models. We introduce online speculative decoding (OSD) to address this challenge. The main idea is to continually update (multiple) draft model(s) on observed user query data using the abundant excess computational power in an LLM serving cluster. Given that LLM inference is memory-bounded, the surplus computational power in a typical LLM serving cluster can be repurposed for online retraining of draft models, thereby making the training cost-neutral. Since the query distribution of an LLM service is relatively simple, retraining on query distribution enables the draft model to more accurately predict the target
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#38646;&#24320;&#22987;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#23548;&#20247;&#21253;&#24037;&#20316;&#32773;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#22312;&#26500;&#24314;&#26085;&#35821;&#20107;&#20214;&#30693;&#35782;&#22270;&#35889;&#21644;&#35757;&#32451;&#24120;&#35782;&#29983;&#25104;&#27169;&#22411;&#26041;&#38754;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.07170</link><description>&lt;p&gt;
PHALM:&#36890;&#36807;&#24341;&#23548;&#20154;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#20174;&#38646;&#24320;&#22987;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
PHALM: Building a Knowledge Graph from Scratch by Prompting Humans and a Language Model. (arXiv:2310.07170v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07170
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#38646;&#24320;&#22987;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#23548;&#20247;&#21253;&#24037;&#20316;&#32773;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#22312;&#26500;&#24314;&#26085;&#35821;&#20107;&#20214;&#30693;&#35782;&#22270;&#35889;&#21644;&#35757;&#32451;&#24120;&#35782;&#29983;&#25104;&#27169;&#22411;&#26041;&#38754;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#39044;&#35757;&#32451;&#30340;Transformer&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#24448;&#24448;&#26080;&#27861;&#33391;&#22909;&#22788;&#29702;&#24120;&#35782;&#30693;&#35782;&#12290;&#20026;&#20102;&#21019;&#36896;&#24120;&#35782;&#24863;&#30693;&#30340;&#27169;&#22411;&#65292;&#24050;&#32463;&#23581;&#35797;&#20102;&#20174;&#33258;&#21160;&#33719;&#21462;&#21040;&#20247;&#21253;&#33719;&#21462;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20174;&#38646;&#24320;&#22987;&#20197;&#36739;&#20302;&#25104;&#26412;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#30693;&#35782;&#24211;&#26159;&#22256;&#38590;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PHALM&#65292;&#19968;&#31181;&#36890;&#36807;&#24341;&#23548;&#20247;&#21253;&#24037;&#20316;&#32773;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#26469;&#20174;&#38646;&#24320;&#22987;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#26500;&#24314;&#20102;&#19968;&#20010;&#26085;&#35821;&#20107;&#20214;&#30693;&#35782;&#22270;&#35889;&#65292;&#24182;&#35757;&#32451;&#20102;&#26085;&#35821;&#24120;&#35782;&#29983;&#25104;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;&#26500;&#24314;&#30340;&#22270;&#35889;&#21644;&#35757;&#32451;&#27169;&#22411;&#29983;&#25104;&#30340;&#25512;&#29702;&#30340;&#21487;&#25509;&#21463;&#24615;&#12290;&#25105;&#20204;&#36824;&#25253;&#21578;&#20102;&#24341;&#23548;&#20154;&#31867;&#21644;LLM&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#12289;&#25968;&#25454;&#21644;&#27169;&#22411;&#21487;&#20197;&#22312;github.com/nlp-waseda/comet-atomic-ja&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the remarkable progress in natural language understanding with pretrained Transformers, neural language models often do not handle commonsense knowledge well. Toward commonsense-aware models, there have been attempts to obtain knowledge, ranging from automatic acquisition to crowdsourcing. However, it is difficult to obtain a high-quality knowledge base at a low cost, especially from scratch. In this paper, we propose PHALM, a method of building a knowledge graph from scratch, by prompting both crowdworkers and a large language model (LLM). We used this method to build a Japanese event knowledge graph and trained Japanese commonsense generation models. Experimental results revealed the acceptability of the built graph and inferences generated by the trained models. We also report the difference in prompting humans and an LLM. Our code, data, and models are available at github.com/nlp-waseda/comet-atomic-ja.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;VoIP&#36890;&#20449;&#39046;&#22495;&#20013;&#25506;&#32034;&#20102;&#22768;&#23398;&#36716;&#25442;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#24515;&#29702;&#22768;&#23398;&#25351;&#26631;&#65292;&#25581;&#31034;&#20102;&#35821;&#38899;&#22686;&#24378;&#23545;VoIP&#31995;&#32479;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.07161</link><description>&lt;p&gt;
VoIP&#24179;&#21488;&#19978;&#35821;&#38899;&#22686;&#24378;&#30340;&#24515;&#29702;&#22768;&#23398;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Psychoacoustic Challenges Of Speech Enhancement On VoIP Platforms. (arXiv:2310.07161v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07161
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;VoIP&#36890;&#20449;&#39046;&#22495;&#20013;&#25506;&#32034;&#20102;&#22768;&#23398;&#36716;&#25442;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#24515;&#29702;&#22768;&#23398;&#25351;&#26631;&#65292;&#25581;&#31034;&#20102;&#35821;&#38899;&#22686;&#24378;&#23545;VoIP&#31995;&#32479;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;VoIP&#65288;&#20114;&#32852;&#32593;&#35821;&#38899;&#20256;&#36755;&#21327;&#35758;&#65289;&#36890;&#20449;&#20013;&#65292;&#30001;&#22768;&#23398;&#36716;&#25442;&#24341;&#20837;&#30340;&#22797;&#26434;&#24615;&#38656;&#35201;&#36827;&#34892;&#20005;&#26684;&#30340;&#20998;&#26512;&#12290;&#26412;&#30740;&#31350;&#22522;&#20110;&#23545;&#19987;&#26377;&#21457;&#36865;&#31471;&#38477;&#22122;&#25928;&#26524;&#30340;&#25506;&#32034;&#65292;&#23545;Google Meets&#21644;Zoom&#31561;&#24179;&#21488;&#36827;&#34892;&#20102;&#32454;&#33268;&#35780;&#20272;&#12290;&#30740;&#31350;&#21033;&#29992;Deep Noise Suppression&#65288;DNS&#65289;2020&#25968;&#25454;&#38598;&#65292;&#30830;&#20445;&#20102;&#38024;&#23545;&#21508;&#31181;&#38477;&#22122;&#35774;&#32622;&#21644;&#25509;&#25910;&#22120;&#25509;&#21475;&#30340;&#32467;&#26500;&#21270;&#32771;&#23519;&#12290;&#36890;&#36807;&#23558;Oaxaca&#20998;&#35299;&#24341;&#20837;&#21040;&#22768;&#23398;-&#35821;&#38899;&#25200;&#21160;&#20998;&#26512;&#20013;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#35770;&#30340;&#21019;&#26032;&#65292;&#35813;&#20998;&#35299;&#36890;&#24120;&#26159;&#32463;&#27982;&#35745;&#37327;&#23398;&#24037;&#20855;&#65292;&#22312;&#27492;&#22788;&#37325;&#26032;&#29992;&#20110;&#20998;&#26512;VoIP&#31995;&#32479;&#20013;&#30340;&#22768;&#23398;-&#35821;&#38899;&#25200;&#21160;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#30830;&#23450;&#36825;&#20123;&#36716;&#25442;&#30340;&#24433;&#21709;&#65292;&#21033;&#29992;&#24515;&#29702;&#22768;&#23398;&#25351;&#26631;&#65292;&#29305;&#21035;&#26159;PESQ&#21644;STOI&#65292;&#26469;&#25552;&#20379;&#23545;&#35821;&#38899;&#25913;&#21464;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25152;&#33719;&#24471;&#30340;&#35266;&#28857;&#31361;&#20986;&#26174;&#31034;&#20102;VoIP&#24433;&#21709;&#30340;&#22768;&#23398;&#21160;&#21147;&#23398;&#30340;&#22797;&#26434;&#26223;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;
Within the ambit of VoIP (Voice over Internet Protocol) telecommunications, the complexities introduced by acoustic transformations merit rigorous analysis. This research, rooted in the exploration of proprietary sender-side denoising effects, meticulously evaluates platforms such as Google Meets and Zoom. The study draws upon the Deep Noise Suppression (DNS) 2020 dataset, ensuring a structured examination tailored to various denoising settings and receiver interfaces. A methodological novelty is introduced via the Oaxaca decomposition, traditionally an econometric tool, repurposed herein to analyze acoustic-phonetic perturbations within VoIP systems. To further ground the implications of these transformations, psychoacoustic metrics, specifically PESQ and STOI, were harnessed to furnish a comprehensive understanding of speech alterations. Cumulatively, the insights garnered underscore the intricate landscape of VoIP-influenced acoustic dynamics. In addition to the primary findings, a 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#24369;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#27604;&#36739;#BlackLivesMatter&#21644;#BlueLivesMatter&#36816;&#21160;&#30456;&#20851;&#25512;&#25991;&#20013;&#30340;&#35266;&#28857;&#12290;&#36890;&#36807;&#23558;&#25991;&#26412;&#36716;&#25442;&#20026;&#22270;&#65292;&#24182;&#26681;&#25454;&#20316;&#32773;&#30340;&#31038;&#20132;&#32593;&#32476;&#36827;&#34892;&#32467;&#26500;&#21270;&#39044;&#27979;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#32570;&#20047;&#26631;&#27880;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#27169;&#25311;&#36816;&#21160;&#20013;&#30340;&#35266;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.07155</link><description>&lt;p&gt;
"&#20004;&#20010;&#36816;&#21160;&#30340;&#25925;&#20107;": &#20351;&#29992;&#24369;&#30417;&#30563;&#30340;&#22522;&#20110;&#22270;&#30340;&#32467;&#26500;&#21270;&#39044;&#27979;&#26041;&#27861;&#37492;&#21035;&#21644;&#27604;&#36739;#BlackLivesMatter&#21644;#BlueLivesMatter&#36816;&#21160;&#30456;&#20851;&#30340;&#25512;&#25991;&#20013;&#30340;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
"A Tale of Two Movements": Identifying and Comparing Perspectives in #BlackLivesMatter and #BlueLivesMatter Movements-related Tweets using Weakly Supervised Graph-based Structured Prediction. (arXiv:2310.07155v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#24369;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#27604;&#36739;#BlackLivesMatter&#21644;#BlueLivesMatter&#36816;&#21160;&#30456;&#20851;&#25512;&#25991;&#20013;&#30340;&#35266;&#28857;&#12290;&#36890;&#36807;&#23558;&#25991;&#26412;&#36716;&#25442;&#20026;&#22270;&#65292;&#24182;&#26681;&#25454;&#20316;&#32773;&#30340;&#31038;&#20132;&#32593;&#32476;&#36827;&#34892;&#32467;&#26500;&#21270;&#39044;&#27979;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#32570;&#20047;&#26631;&#27880;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#27169;&#25311;&#36816;&#21160;&#20013;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24050;&#25104;&#20026;&#25512;&#21160;&#31038;&#20250;&#21464;&#38761;&#30340;&#37325;&#35201;&#22240;&#32032;&#65292;&#36890;&#36807;&#20419;&#36827;&#22312;&#32447;&#31038;&#20250;&#36816;&#21160;&#30340;&#24418;&#25104;&#12290;&#33258;&#21160;&#29702;&#35299;&#25512;&#21160;&#36816;&#21160;&#21644;&#21453;&#23545;&#36816;&#21160;&#22768;&#38899;&#30340;&#35266;&#28857;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#24456;&#38590;&#33719;&#24471;&#24102;&#26377;&#27880;&#37322;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#30417;&#30563;&#30340;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#65292;&#26174;&#24335;&#22320;&#23545;#BlakcLivesMatter&#30456;&#20851;&#25512;&#25991;&#20013;&#30340;&#35266;&#28857;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#31038;&#20250;-&#35821;&#35328;&#23398;&#30340;&#25968;&#25454;&#34920;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#25991;&#26412;&#20998;&#35299;&#20026;&#32467;&#26500;&#21270;&#20803;&#32032;&#24182;&#19982;&#20316;&#32773;&#30340;&#31038;&#20132;&#32593;&#32476;&#30456;&#36830;&#25509;&#23558;&#20854;&#36716;&#25442;&#20026;&#22270;&#24418;&#65292;&#28982;&#21518;&#20351;&#29992;&#32467;&#26500;&#21270;&#39044;&#27979;&#26469;&#35782;&#21035;&#35266;&#28857;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#20102;&#19968;&#23567;&#32452;&#26631;&#35760;&#31034;&#20363;&#12290;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20154;&#24037;&#35757;&#32451;&#31034;&#20363;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#23558;&#20854;&#19982;&#25163;&#21160;&#27880;&#37322;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;&#23427;&#20204;&#20855;&#26377;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#20154;&#24037;&#27880;&#37322;&#30340;&#27979;&#35797;&#38598;&#36827;&#34892;&#23450;&#37327;&#21644;&#23450;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;...
&lt;/p&gt;
&lt;p&gt;
Social media has become a major driver of social change, by facilitating the formation of online social movements. Automatically understanding the perspectives driving the movement and the voices opposing it, is a challenging task as annotated data is difficult to obtain. We propose a weakly supervised graph-based approach that explicitly models perspectives in #BackLivesMatter-related tweets. Our proposed approach utilizes a social-linguistic representation of the data. We convert the text to a graph by breaking it into structured elements and connect it with the social network of authors, then structured prediction is done over the elements for identifying perspectives. Our approach uses a small seed set of labeled examples. We experiment with large language models for generating artificial training examples, compare them to manual annotation, and find that it achieves comparable performance. We perform quantitative and qualitative analyses using a human-annotated test set. Our model
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;QFT&#26694;&#26550;&#65292;&#21487;&#20197;&#23545;LLMs&#36827;&#34892;&#20869;&#23384;&#39640;&#25928;&#30340;&#20840;&#21442;&#25968;&#24494;&#35843;&#65292;&#32780;&#19981;&#25439;&#23475;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.07147</link><description>&lt;p&gt;
QFT: &#20351;&#29992;&#21487;&#25215;&#25285;&#36164;&#28304;&#23545;LLMs&#36827;&#34892;&#37327;&#21270;&#20840;&#21442;&#25968;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
QFT: Quantized Full-parameter Tuning of LLMs with Affordable Resources. (arXiv:2310.07147v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07147
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;QFT&#26694;&#26550;&#65292;&#21487;&#20197;&#23545;LLMs&#36827;&#34892;&#20869;&#23384;&#39640;&#25928;&#30340;&#20840;&#21442;&#25968;&#24494;&#35843;&#65292;&#32780;&#19981;&#25439;&#23475;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#24433;&#21709;&#12290;&#23545;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#65292;&#20294;&#30001;&#20110;&#20854;&#24040;&#22823;&#30340;&#36164;&#28304;&#38656;&#27714;&#65292;&#36825;&#19968;&#36807;&#31243;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#27492;&#65292;&#29616;&#26377;&#30340;&#21162;&#21147;&#37117;&#38598;&#20013;&#22312;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#19978;&#65292;&#19981;&#24184;&#30340;&#26159;&#65292;&#23427;&#20204;&#27809;&#26377;&#20805;&#20998;&#21457;&#25381;&#20840;&#21442;&#25968;&#24494;&#35843;&#30340;&#28508;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;QFT&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;LLMs&#30340;&#37327;&#21270;&#20840;&#21442;&#25968;&#35843;&#25972;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19981;&#25439;&#23475;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#25928;&#30340;&#20869;&#23384;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#20004;&#20010;&#26032;&#39062;&#30340;&#24605;&#24819;&#65306;&#65288;i&#65289;&#25105;&#20204;&#37319;&#29992;&#39640;&#25928;&#30340;Lion&#20248;&#21270;&#22120;&#65292;&#20165;&#36319;&#36394;&#21160;&#37327;&#24182;&#20855;&#26377;&#27599;&#20010;&#21442;&#25968;&#19968;&#33268;&#30340;&#26356;&#26032;&#24133;&#24230;&#65292;&#36825;&#23545;&#20110;&#31283;&#20581;&#30340;&#37327;&#21270;&#26159;&#19968;&#31181;&#20869;&#22312;&#20248;&#21183;&#65307;&#65288;ii&#65289;&#25105;&#20204;&#23558;&#25152;&#26377;&#27169;&#22411;&#29366;&#24577;&#36827;&#34892;&#37327;&#21270;&#65292;&#24182;&#20197;&#25972;&#25968;&#20540;&#23384;&#20648;&#65292;&#21516;&#26102;&#25552;&#20379;&#26799;&#24230;&#27969;&#21644;&#21442;&#25968;&#26356;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have showcased remarkable impacts across a wide spectrum of natural language processing tasks. Fine-tuning these pre-trained models on downstream datasets provides further significant performance gains, but this process has been challenging due to its extraordinary resource requirements. To this end, existing efforts focus on parameter-efficient fine-tuning, which, unfortunately, fail to capitalize on the powerful potential of full-parameter fine-tuning. In this work, we propose QFT, a novel Quantized Full-parameter Tuning framework for LLMs that enables memory-efficient fine-tuning without harming performance. Our framework incorporates two novel ideas: (i) we adopt the efficient Lion optimizer, which only keeps track of the momentum and has consistent update magnitudes for each parameter, an inherent advantage for robust quantization; and (ii) we quantize all model states and store them as integer values, and present a gradient flow and parameter update s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20511;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24515;&#29702;&#30103;&#27861;&#30340;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#65292;&#22312;&#35748;&#30693;&#22833;&#30495;&#26816;&#27979;&#20219;&#21153;&#20013;&#25552;&#20986;&#20102;&#24605;&#32500;&#35825;&#23548;&#35786;&#26029;&#65288;DoT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#19977;&#20010;&#38454;&#27573;&#23545;&#24739;&#32773;&#30340;&#35328;&#35821;&#36827;&#34892;&#35786;&#26029;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.07146</link><description>&lt;p&gt;
&#20511;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21152;&#24378;&#24515;&#29702;&#30103;&#27861;&#65306;&#36890;&#36807;&#24605;&#32500;&#35825;&#23548;&#36827;&#34892;&#35748;&#30693;&#22833;&#30495;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Empowering Psychotherapy with Large Language Models: Cognitive Distortion Detection through Diagnosis of Thought Prompting. (arXiv:2310.07146v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07146
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20511;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24515;&#29702;&#30103;&#27861;&#30340;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#65292;&#22312;&#35748;&#30693;&#22833;&#30495;&#26816;&#27979;&#20219;&#21153;&#20013;&#25552;&#20986;&#20102;&#24605;&#32500;&#35825;&#23548;&#35786;&#26029;&#65288;DoT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#19977;&#20010;&#38454;&#27573;&#23545;&#24739;&#32773;&#30340;&#35328;&#35821;&#36827;&#34892;&#35786;&#26029;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#19987;&#19994;&#20154;&#21592;&#30340;&#20005;&#37325;&#21294;&#20047;&#21644;&#21487;&#33719;&#24471;&#24615;&#38480;&#21046;&#65292;&#31934;&#31070;&#30142;&#30149;&#20173;&#28982;&#26159;&#25105;&#20204;&#36825;&#20010;&#26102;&#20195;&#26368;&#37325;&#35201;&#30340;&#20844;&#20849;&#21355;&#29983;&#38382;&#39064;&#20043;&#19968;&#12290;&#24515;&#29702;&#30103;&#27861;&#38656;&#35201;&#39640;&#27700;&#24179;&#30340;&#19987;&#38376;&#30693;&#35782;&#65292;&#20197;&#23545;&#24739;&#32773;&#30340;&#35748;&#30693;&#24314;&#27169;&#36827;&#34892;&#28145;&#24230;&#12289;&#22797;&#26434;&#30340;&#25512;&#29702;&#21644;&#20998;&#26512;&#12290;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#20195;&#65292;&#25105;&#20204;&#35748;&#20026;&#29616;&#22312;&#26159;&#24320;&#21457;&#35745;&#31639;&#24515;&#29702;&#30103;&#27861;&#30340;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#30340;&#21512;&#36866;&#26102;&#26426;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#35748;&#30693;&#22833;&#30495;&#26816;&#27979;&#30340;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#24605;&#32500;&#35825;&#23548;&#35786;&#26029;&#65288;DoT&#65289;&#12290;DoT&#36890;&#36807;&#19977;&#20010;&#38454;&#27573;&#23545;&#24739;&#32773;&#30340;&#35328;&#35821;&#36827;&#34892;&#35786;&#26029;&#65306;&#20027;&#35266;&#24615;&#35780;&#20272;&#20197;&#21306;&#20998;&#20107;&#23454;&#21644;&#24605;&#32500;&#65307;&#23545;&#27604;&#25512;&#29702;&#20197;&#24341;&#20986;&#25903;&#25345;&#21644;&#21453;&#39539;&#24605;&#32500;&#30340;&#25512;&#29702;&#36807;&#31243;&#65307;&#27169;&#24335;&#20998;&#26512;&#20197;&#24635;&#32467;&#35748;&#30693;&#27169;&#24335;&#12290;&#36890;&#36807;&#36825;&#19977;&#20010;&#38454;&#27573;&#29983;&#25104;&#30340;&#35786;&#26029;&#35299;&#37322;&#23545;&#36741;&#21161;&#19987;&#19994;&#20154;&#21592;&#38750;&#24120;&#37325;&#35201;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;DoT&#22312;&#35748;&#30693;&#22833;&#30495;&#26041;&#38754;&#30340;&#25928;&#26524;&#26126;&#26174;&#20248;&#20110;ChatGPT&#12290;
&lt;/p&gt;
&lt;p&gt;
Mental illness remains one of the most critical public health issues of our time, due to the severe scarcity and accessibility limit of professionals. Psychotherapy requires high-level expertise to conduct deep, complex reasoning and analysis on the cognition modeling of the patients. In the era of Large Language Models, we believe it is the right time to develop AI assistance for computational psychotherapy. We study the task of cognitive distortion detection and propose the Diagnosis of Thought (DoT) prompting. DoT performs diagnosis on the patient's speech via three stages: subjectivity assessment to separate the facts and the thoughts; contrastive reasoning to elicit the reasoning processes supporting and contradicting the thoughts; and schema analysis to summarize the cognition schemas. The generated diagnosis rationales through the three stages are essential for assisting the professionals. Experiments demonstrate that DoT obtains significant improvements over ChatGPT for cogniti
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#21305;&#37197;&#21644;&#36127;&#26631;&#31614;&#37319;&#26679;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#27169;&#22411; AE-smnsMLC&#65292;&#29992;&#20110;&#35299;&#20915;&#20135;&#21697;&#23646;&#24615;&#20540;&#25552;&#21462;&#30340;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#23558;&#23646;&#24615;&#20540;&#25552;&#21462;&#20219;&#21153;&#36716;&#21270;&#20026;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#21482;&#26377;&#23646;&#24615;&#20540;&#27880;&#37322;&#30340;&#23454;&#38469;&#22330;&#26223;&#65292;&#32780;&#26080;&#38656;&#23646;&#24615;&#20540;&#30340;&#20301;&#32622;&#20449;&#24687;&#27880;&#37322;&#12290;</title><link>http://arxiv.org/abs/2310.07137</link><description>&lt;p&gt;
AE-smnsMLC&#65306;&#22522;&#20110;&#35821;&#20041;&#21305;&#37197;&#21644;&#36127;&#26631;&#31614;&#37319;&#26679;&#30340;&#20135;&#21697;&#23646;&#24615;&#20540;&#25552;&#21462;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
AE-smnsMLC: Multi-Label Classification with Semantic Matching and Negative Label Sampling for Product Attribute Value Extraction. (arXiv:2310.07137v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07137
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#21305;&#37197;&#21644;&#36127;&#26631;&#31614;&#37319;&#26679;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#27169;&#22411; AE-smnsMLC&#65292;&#29992;&#20110;&#35299;&#20915;&#20135;&#21697;&#23646;&#24615;&#20540;&#25552;&#21462;&#30340;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#23558;&#23646;&#24615;&#20540;&#25552;&#21462;&#20219;&#21153;&#36716;&#21270;&#20026;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#21482;&#26377;&#23646;&#24615;&#20540;&#27880;&#37322;&#30340;&#23454;&#38469;&#22330;&#26223;&#65292;&#32780;&#26080;&#38656;&#23646;&#24615;&#20540;&#30340;&#20301;&#32622;&#20449;&#24687;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20135;&#21697;&#23646;&#24615;&#20540;&#25552;&#21462;&#22312;&#30005;&#23376;&#21830;&#21153;&#31561;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#22914;&#20135;&#21697;&#25628;&#32034;&#21644;&#25512;&#33616;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#23558;&#20854;&#35270;&#20026;&#38656;&#35201;&#26356;&#22810;&#27880;&#37322;&#26469;&#26631;&#27880;&#20135;&#21697;&#25991;&#26412;&#20013;&#20540;&#30340;&#24207;&#21015;&#26631;&#35760;&#20219;&#21153;&#12290;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65292;&#20854;&#20013;&#27599;&#20010;&#20135;&#21697;&#21482;&#26377;&#23646;&#24615;&#20540;&#30340;&#24369;&#26631;&#27880;&#65292;&#32780;&#27809;&#26377;&#23427;&#20204;&#30340;&#20301;&#32622;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#21482;&#20351;&#29992;&#20135;&#21697;&#25991;&#26412;&#65288;&#21363;&#20135;&#21697;&#26631;&#39064;&#21644;&#25551;&#36848;&#65289;&#65292;&#32780;&#19981;&#32771;&#34385;&#32473;&#23450;&#20135;&#21697;&#30340;&#22810;&#20010;&#23646;&#24615;&#20540;&#19982;&#20854;&#25991;&#26412;&#20043;&#38388;&#30340;&#35821;&#20041;&#36830;&#25509;&#65292;&#36825;&#21487;&#20197;&#24110;&#21161;&#23646;&#24615;&#20540;&#25552;&#21462;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#19968;&#20010;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#65292;&#21487;&#20197;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#24212;&#29992;&#65292;&#20854;&#20013;&#21482;&#26377;&#23646;&#24615;&#20540;&#30340;&#27880;&#37322;&#21487;&#29992;&#20110;&#35757;&#32451;&#27169;&#22411;&#65288;&#21363;&#27809;&#26377;&#23646;&#24615;&#20540;&#20301;&#32622;&#20449;&#24687;&#30340;&#27880;&#37322;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#35821;&#20041;&#21305;&#37197;&#21644;&#36127;&#26631;&#31614;&#37319;&#26679;&#30340;&#20998;&#31867;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Product attribute value extraction plays an important role for many real-world applications in e-Commerce such as product search and recommendation. Previous methods treat it as a sequence labeling task that needs more annotation for position of values in the product text. This limits their application to real-world scenario in which only attribute values are weakly-annotated for each product without their position. Moreover, these methods only use product text (i.e., product title and description) and do not consider the semantic connection between the multiple attribute values of a given product and its text, which can help attribute value extraction. In this paper, we reformulate this task as a multi-label classification task that can be applied for real-world scenario in which only annotation of attribute values is available to train models (i.e., annotation of positional information of attribute values is not available). We propose a classification model with semantic matching and
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#35299;&#37322;&#26694;&#26550;&#65292;&#20174;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#39118;&#26684;&#24046;&#24322;&#24182;&#27604;&#36739;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;&#39118;&#26684;&#65292;&#21019;&#24314;&#20102;&#20840;&#38754;&#30340;&#22810;&#35821;&#35328;&#31036;&#35980;&#25968;&#25454;&#38598;&#65292;&#25506;&#32034;&#20102;&#31036;&#35980;&#22312;&#22235;&#31181;&#35821;&#35328;&#20013;&#30340;&#21464;&#21270;&#65292;&#20026;&#35780;&#20272;&#35821;&#35328;&#31867;&#21035;&#23545;&#39118;&#26684;&#21464;&#21270;&#30340;&#36129;&#29486;&#21644;&#20102;&#35299;&#19990;&#30028;&#21508;&#22320;&#20154;&#20204;&#30340;&#19981;&#21516;&#27807;&#36890;&#26041;&#24335;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#26041;&#27861;&#21644;&#35299;&#37322;&#27934;&#23519;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.07135</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#39118;&#26684;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Comparing Styles across Languages. (arXiv:2310.07135v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#35299;&#37322;&#26694;&#26550;&#65292;&#20174;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#39118;&#26684;&#24046;&#24322;&#24182;&#27604;&#36739;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;&#39118;&#26684;&#65292;&#21019;&#24314;&#20102;&#20840;&#38754;&#30340;&#22810;&#35821;&#35328;&#31036;&#35980;&#25968;&#25454;&#38598;&#65292;&#25506;&#32034;&#20102;&#31036;&#35980;&#22312;&#22235;&#31181;&#35821;&#35328;&#20013;&#30340;&#21464;&#21270;&#65292;&#20026;&#35780;&#20272;&#35821;&#35328;&#31867;&#21035;&#23545;&#39118;&#26684;&#21464;&#21270;&#30340;&#36129;&#29486;&#21644;&#20102;&#35299;&#19990;&#30028;&#21508;&#22320;&#20154;&#20204;&#30340;&#19981;&#21516;&#27807;&#36890;&#26041;&#24335;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#26041;&#27861;&#21644;&#35299;&#37322;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#36328;&#35821;&#35328;&#39118;&#26684;&#30340;&#24046;&#24322;&#26377;&#21161;&#20110;&#35757;&#32451;&#20154;&#31867;&#21644;&#35745;&#31639;&#26426;&#29983;&#25104;&#31526;&#21512;&#25991;&#21270;&#32972;&#26223;&#30340;&#25991;&#26412;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#35299;&#37322;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#39118;&#26684;&#24046;&#24322;&#65292;&#24182;&#27604;&#36739;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;&#39118;&#26684;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;(1)&#21487;&#20197;&#29983;&#25104;&#20219;&#20309;&#35821;&#35328;&#30340;&#20840;&#38754;&#39118;&#26684;&#35789;&#20856;&#65292;(2)&#23558;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#32479;&#19968;&#20026;&#21487;&#27604;&#36739;&#30340;&#35789;&#27719;&#31867;&#21035;&#12290;&#25105;&#20204;&#24212;&#29992;&#35813;&#26694;&#26550;&#27604;&#36739;&#20102;&#31036;&#35980;&#35821;&#35328;&#65292;&#21019;&#24314;&#20102;&#31532;&#19968;&#20010;&#20840;&#38754;&#30340;&#22810;&#35821;&#35328;&#31036;&#35980;&#25968;&#25454;&#38598;&#65292;&#24182;&#25506;&#32034;&#20102;&#31036;&#35980;&#22312;&#22235;&#31181;&#35821;&#35328;&#20013;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35780;&#20272;&#19981;&#21516;&#35821;&#35328;&#31867;&#21035;&#23545;&#39118;&#26684;&#21464;&#21270;&#30340;&#36129;&#29486;&#65292;&#24182;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#27934;&#23519;&#21147;&#65292;&#20102;&#35299;&#19990;&#30028;&#21508;&#22320;&#20154;&#20204;&#30340;&#19981;&#21516;&#27807;&#36890;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding how styles differ across languages is advantageous for training both humans and computers to generate culturally appropriate text. We introduce an explanation framework to extract stylistic differences from multilingual LMs and compare styles across languages. Our framework (1) generates comprehensive style lexica in any language and (2) consolidates feature importances from LMs into comparable lexical categories. We apply this framework to compare politeness, creating the first holistic multilingual politeness dataset and exploring how politeness varies across four languages. Our approach enables an effective evaluation of how distinct linguistic categories contribute to stylistic variations and provides interpretable insights into how people communicate differently around the world.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#28145;&#24230;&#35821;&#35328;&#27169;&#22411;(DLMs)&#30340;&#20998;&#23618;&#32467;&#26500;&#19982;&#20154;&#33041;&#35821;&#35328;&#29702;&#35299;&#30340;&#26102;&#38388;&#21160;&#24577;&#20043;&#38388;&#23384;&#22312;&#24378;&#30456;&#20851;&#24615;&#65292;&#36890;&#36807;&#37319;&#29992;&#30005;&#23376;&#30382;&#23618;&#22270;(ECoG)&#25968;&#25454;&#26469;&#20248;&#21270;&#26102;&#38388;&#20998;&#36776;&#29575;&#65292;&#20026;&#28145;&#20837;&#20102;&#35299;&#20154;&#33041;&#35821;&#35328;&#22788;&#29702;&#26426;&#21046;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2310.07106</link><description>&lt;p&gt;
&#12298;&#20154;&#33041;&#20013;&#35821;&#35328;&#22788;&#29702;&#30340;&#26102;&#24577;&#32467;&#26500;&#19982;&#28145;&#24230;&#35821;&#35328;&#27169;&#22411;&#30340;&#23618;&#27425;&#32467;&#26500;&#30456;&#31526;&#12299;
&lt;/p&gt;
&lt;p&gt;
The Temporal Structure of Language Processing in the Human Brain Corresponds to The Layered Hierarchy of Deep Language Models. (arXiv:2310.07106v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07106
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#28145;&#24230;&#35821;&#35328;&#27169;&#22411;(DLMs)&#30340;&#20998;&#23618;&#32467;&#26500;&#19982;&#20154;&#33041;&#35821;&#35328;&#29702;&#35299;&#30340;&#26102;&#38388;&#21160;&#24577;&#20043;&#38388;&#23384;&#22312;&#24378;&#30456;&#20851;&#24615;&#65292;&#36890;&#36807;&#37319;&#29992;&#30005;&#23376;&#30382;&#23618;&#22270;(ECoG)&#25968;&#25454;&#26469;&#20248;&#21270;&#26102;&#38388;&#20998;&#36776;&#29575;&#65292;&#20026;&#28145;&#20837;&#20102;&#35299;&#20154;&#33041;&#35821;&#35328;&#22788;&#29702;&#26426;&#21046;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#35821;&#35328;&#27169;&#22411;(DLMs)&#25552;&#20379;&#20102;&#19968;&#31181;&#29702;&#35299;&#20154;&#33041;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26426;&#21046;&#30340;&#26032;&#30340;&#35745;&#31639;&#33539;&#24335;&#12290;&#19982;&#20256;&#32479;&#30340;&#24515;&#29702;&#35821;&#35328;&#23398;&#27169;&#22411;&#19981;&#21516;&#65292;DLMs&#20351;&#29992;&#20998;&#23618;&#30340;&#36830;&#32493;&#25968;&#20540;&#21521;&#37327;&#24207;&#21015;&#26469;&#34920;&#31034;&#21333;&#35789;&#21644;&#19978;&#19979;&#25991;&#65292;&#20351;&#24471;&#35832;&#22810;&#26032;&#30340;&#24212;&#29992;&#25104;&#20026;&#21487;&#33021;&#65292;&#22914;&#31867;&#20154;&#29983;&#25104;&#25991;&#26412;&#12290;&#26412;&#25991;&#36890;&#36807;&#23637;&#31034;DLMs&#30340;&#20998;&#23618;&#32467;&#26500;&#21487;&#33021;&#29992;&#20110;&#27169;&#25311;&#22823;&#33041;&#35821;&#35328;&#29702;&#35299;&#30340;&#26102;&#38388;&#21160;&#24577;&#65292;&#35777;&#26126;&#20102;DLM&#23618;&#27425;&#28145;&#24230;&#19982;&#26368;&#33021;&#39044;&#27979;&#20154;&#33041;&#30340;&#23618;&#27425;&#26102;&#38388;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#20043;&#25152;&#20197;&#33021;&#22815;&#22312;&#26102;&#38388;&#19978;&#35299;&#26512;&#20986;&#27599;&#20010;&#23618;&#27425;&#30340;&#20248;&#21183;&#22312;&#20110;&#25105;&#20204;&#37319;&#29992;&#20102;&#30005;&#23376;&#30382;&#36136;&#22270;(ECoG)&#25968;&#25454;&#65292;&#20854;&#26102;&#38388;&#20998;&#36776;&#29575;&#27604;&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;(fMRI)&#31561;&#26080;&#25439;&#27979;&#37327;&#26041;&#27861;&#26356;&#39640;&#12290;&#25105;&#20204;&#21033;&#29992;ECoG&#20174;&#21442;&#19982;&#32773;&#21548;30&#20998;&#38047;&#21465;&#36848;&#26102;&#35760;&#24405;&#31070;&#32463;&#27963;&#21160;&#65292;&#21516;&#26102;&#23558;&#30456;&#21516;&#21465;&#36848;&#25552;&#20379;&#32473;&#39640;&#24615;&#33021;DLM(GPT2-XL)&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Language Models (DLMs) provide a novel computational paradigm for understanding the mechanisms of natural language processing in the human brain. Unlike traditional psycholinguistic models, DLMs use layered sequences of continuous numerical vectors to represent words and context, allowing a plethora of emerging applications such as human-like text generation. In this paper we show evidence that the layered hierarchy of DLMs may be used to model the temporal dynamics of language comprehension in the brain by demonstrating a strong correlation between DLM layer depth and the time at which layers are most predictive of the human brain. Our ability to temporally resolve individual layers benefits from our use of electrocorticography (ECoG) data, which has a much higher temporal resolution than noninvasive methods like fMRI. Using ECoG, we record neural activity from participants listening to a 30-minute narrative while also feeding the same narrative to a high-performing DLM (GPT2-XL)
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#31232;&#30095;&#36890;&#29992;Transformer&#65288;SUT&#65289;&#65292;&#23427;&#36890;&#36807;&#21033;&#29992;&#31232;&#30095;&#28151;&#21512;&#19987;&#23478;&#65288;SMoE&#65289;&#21644;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20999;&#26829;&#27861;&#30340;&#21160;&#24577;&#20572;&#27490;&#26426;&#21046;&#26469;&#20943;&#23569;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#21442;&#25968;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;SUT&#22312;&#24418;&#24335;&#35821;&#35328;&#20219;&#21153;&#19978;&#20855;&#26377;&#19982;&#24378;&#22522;&#32447;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#33021;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#36164;&#28304;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.07096</link><description>&lt;p&gt;
&#31232;&#30095;&#36890;&#29992;Transformer
&lt;/p&gt;
&lt;p&gt;
Sparse Universal Transformer. (arXiv:2310.07096v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31232;&#30095;&#36890;&#29992;Transformer&#65288;SUT&#65289;&#65292;&#23427;&#36890;&#36807;&#21033;&#29992;&#31232;&#30095;&#28151;&#21512;&#19987;&#23478;&#65288;SMoE&#65289;&#21644;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20999;&#26829;&#27861;&#30340;&#21160;&#24577;&#20572;&#27490;&#26426;&#21046;&#26469;&#20943;&#23569;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#21442;&#25968;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;SUT&#22312;&#24418;&#24335;&#35821;&#35328;&#20219;&#21153;&#19978;&#20855;&#26377;&#19982;&#24378;&#22522;&#32447;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#33021;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#36164;&#28304;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;Transformer&#65288;UT&#65289;&#26159;Transformer&#30340;&#19968;&#31181;&#21464;&#20307;&#65292;&#20854;&#22312;&#21508;&#23618;&#20043;&#38388;&#20849;&#20139;&#21442;&#25968;&#12290;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;&#65292;&#22312;&#24418;&#24335;&#35821;&#35328;&#20219;&#21153;&#20013;&#65292;UT&#27604;Vanilla Transformer&#65288;VT&#65289;&#20855;&#26377;&#26356;&#22909;&#30340;&#32452;&#21512;&#27867;&#21270;&#33021;&#21147;&#12290;&#21442;&#25968;&#20849;&#20139;&#36824;&#20351;&#20854;&#20855;&#26377;&#27604;VT&#26356;&#22909;&#30340;&#21442;&#25968;&#25928;&#29575;&#12290;&#23613;&#31649;&#20855;&#26377;&#35768;&#22810;&#20248;&#28857;&#65292;&#20294;&#25193;&#23637;UT&#21442;&#25968;&#27604;&#25193;&#23637;VT&#26356;&#38656;&#35201;&#35745;&#31639;&#21644;&#20869;&#23384;&#36164;&#28304;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31232;&#30095;&#36890;&#29992;Transformer&#65288;SUT&#65289;&#65292;&#23427;&#21033;&#29992;&#31232;&#30095;&#28151;&#21512;&#19987;&#23478;&#65288;SMoE&#65289;&#21644;&#22522;&#20110;&#20999;&#26829;&#27861;&#30340;&#21160;&#24577;&#20572;&#27490;&#26426;&#21046;&#26469;&#20943;&#23569;UT&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#21442;&#25968;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;SUT&#22312;WMT'14&#19978;&#20165;&#20351;&#29992;&#19968;&#21322;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#21442;&#25968;&#23601;&#33021;&#36798;&#21040;&#19982;&#24378;&#22522;&#32447;&#27169;&#22411;&#30456;&#21516;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#24418;&#24335;&#35821;&#35328;&#20219;&#21153;&#65288;&#36923;&#36753;&#25512;&#29702;&#21644;CFQ&#65289;&#19978;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#26032;&#30340;&#20572;&#27490;&#26426;&#21046;&#36824;&#33021;&#20351;&#35745;&#31639;&#36164;&#28304;&#20943;&#23569;&#32422;50&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Universal Transformer (UT) is a variant of the Transformer that shares parameters across its layers. Empirical evidence shows that UTs have better compositional generalization than Vanilla Transformers (VTs) in formal language tasks. The parameter-sharing also affords it better parameter efficiency than VTs. Despite its many advantages, scaling UT parameters is much more compute and memory intensive than scaling up a VT. This paper proposes the Sparse Universal Transformer (SUT), which leverages Sparse Mixture of Experts (SMoE) and a new stick-breaking-based dynamic halting mechanism to reduce UT's computation complexity while retaining its parameter efficiency and generalization ability. Experiments show that SUT achieves the same performance as strong baseline models while only using half computation and parameters on WMT'14 and strong generalization results on formal language tasks (Logical inference and CFQ). The new halting mechanism also enables around 50\% reduction in compu
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#24615;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#22810;&#27169;&#24577;&#31435;&#22330;&#39044;&#27979;&#20013;&#65292;&#24494;&#35843;&#30340;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#38598;&#21512;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#22810;&#27169;&#24577;&#27169;&#22411;&#26356;&#36866;&#21512;&#23558;&#22270;&#20687;&#20869;&#23481;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#25688;&#35201;&#65292;&#20351;&#29992;&#19978;&#19979;&#25991;&#31034;&#20363;&#21487;&#20197;&#25552;&#39640;LLM&#30340;&#23569;&#26679;&#26412;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.07093</link><description>&lt;p&gt;
&#35770;&#36777;&#31435;&#22330;&#39044;&#27979;&#65306;&#22810;&#27169;&#24577;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Argumentative Stance Prediction: An Exploratory Study on Multimodality and Few-Shot Learning. (arXiv:2310.07093v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07093
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#24615;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#22810;&#27169;&#24577;&#31435;&#22330;&#39044;&#27979;&#20013;&#65292;&#24494;&#35843;&#30340;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#38598;&#21512;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#22810;&#27169;&#24577;&#27169;&#22411;&#26356;&#36866;&#21512;&#23558;&#22270;&#20687;&#20869;&#23481;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#25688;&#35201;&#65292;&#20351;&#29992;&#19978;&#19979;&#25991;&#31034;&#20363;&#21487;&#20197;&#25552;&#39640;LLM&#30340;&#23569;&#26679;&#26412;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25512;&#36827;&#20316;&#20026;&#22810;&#27169;&#24577;&#38382;&#39064;&#30340;&#35770;&#36777;&#31435;&#22330;&#39044;&#27979;&#65292;&#22810;&#27169;&#24577;&#35770;&#35777;&#25366;&#25496;&#30340;&#39318;&#20010;&#20849;&#20139;&#20219;&#21153;&#20027;&#25345;&#20102;&#20851;&#20110;&#26538;&#25903;&#25511;&#21046;&#21644;&#22549;&#32974;&#31561;&#20851;&#38190;&#31038;&#20250;&#35758;&#39064;&#30340;&#31435;&#22330;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#22270;&#20687;&#22312;&#25512;&#25991;&#20013;&#29992;&#20110;&#31435;&#22330;&#39044;&#27979;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#27604;&#36739;&#24320;&#31665;&#21363;&#29992;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#23569;&#26679;&#26412;&#29615;&#22659;&#19979;&#19982;&#24494;&#35843;&#30340;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#24494;&#35843;&#30340;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#38598;&#21512;&#65288;0.817 F1&#20998;&#25968;&#65289;&#20248;&#20110;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;0.677 F1&#20998;&#25968;&#65289;&#21644;&#22522;&#20110;&#25991;&#26412;&#30340;&#23569;&#26679;&#26412;&#39044;&#27979;&#20351;&#29992;&#26368;&#26032;&#30340;LLM&#65288;0.550 F1&#20998;&#25968;&#65289;&#12290;&#38500;&#20102;&#24615;&#33021;&#24046;&#24322;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#23558;&#22270;&#20687;&#20869;&#23481;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#25688;&#35201;&#26102;&#34920;&#29616;&#26356;&#22909;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#21407;&#22987;&#20687;&#32032;&#32467;&#26500;&#65292;&#24182;&#19988;&#22312;&#19978;&#19979;&#25991;&#31034;&#20363;&#19979;&#20351;&#29992;&#21487;&#20197;&#25552;&#39640;LLM&#30340;&#23569;&#26679;&#26412;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
To advance argumentative stance prediction as a multimodal problem, the First Shared Task in Multimodal Argument Mining hosted stance prediction in crucial social topics of gun control and abortion. Our exploratory study attempts to evaluate the necessity of images for stance prediction in tweets and compare out-of-the-box text-based large-language models (LLM) in few-shot settings against fine-tuned unimodal and multimodal models. Our work suggests an ensemble of fine-tuned text-based language models (0.817 F1-score) outperforms both the multimodal (0.677 F1-score) and text-based few-shot prediction using a recent state-of-the-art LLM (0.550 F1-score). In addition to the differences in performance, our findings suggest that the multimodal models tend to perform better when image content is summarized as natural language over their native pixel structure and, using in-context examples improves few-shot performance of LLMs.
&lt;/p&gt;</description></item><item><title>Jaeger&#26159;&#19968;&#31181;&#22522;&#20110;&#36830;&#25509;&#30340;&#22810;&#21464;&#25442;&#22120;VQA&#27169;&#22411;&#65292;&#21033;&#29992;RoBERTa large&#21644;GPT2-xl&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#36890;&#36807;&#24182;&#34892;&#32771;&#34385;&#22810;&#28304;&#20449;&#24687;&#26469;&#22686;&#24378;&#27169;&#22411;&#34920;&#24449;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.07091</link><description>&lt;p&gt;
Jaeger:&#19968;&#31181;&#22522;&#20110;&#36830;&#25509;&#30340;&#22810;&#21464;&#25442;&#22120;VQA&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jaeger: A Concatenation-Based Multi-Transformer VQA Model. (arXiv:2310.07091v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07091
&lt;/p&gt;
&lt;p&gt;
Jaeger&#26159;&#19968;&#31181;&#22522;&#20110;&#36830;&#25509;&#30340;&#22810;&#21464;&#25442;&#22120;VQA&#27169;&#22411;&#65292;&#21033;&#29992;RoBERTa large&#21644;GPT2-xl&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#36890;&#36807;&#24182;&#34892;&#32771;&#34385;&#22810;&#28304;&#20449;&#24687;&#26469;&#22686;&#24378;&#27169;&#22411;&#34920;&#24449;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25991;&#26723;&#30340;&#35270;&#35273;&#38382;&#31572;&#22312;&#35821;&#35328;&#24847;&#20041;&#28040;&#27495;&#21644;&#32454;&#31890;&#24230;&#22810;&#27169;&#24577;&#26816;&#32034;&#20043;&#38388;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;&#30001;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#21644;&#24320;&#25918;&#19990;&#30028;&#20808;&#39564;&#27169;&#22411;&#30340;&#21033;&#29992;&#65292;&#25991;&#26723;&#38382;&#31572;&#21462;&#24471;&#20102;&#40723;&#33310;&#20154;&#24515;&#30340;&#36827;&#23637;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#65292;&#21253;&#25324;&#21709;&#24212;&#26102;&#38388;&#24310;&#38271;&#12289;&#25512;&#26029;&#25345;&#32493;&#26102;&#38388;&#24310;&#38271;&#21644;&#21305;&#37197;&#19981;&#20934;&#30830;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36830;&#25509;&#30340;&#22810;&#21464;&#25442;&#22120;VQA&#27169;&#22411;Jaegar&#12290;&#20026;&#20102;&#25552;&#21462;&#38382;&#39064;&#29305;&#24449;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;RoBERTa large&#21644;GPT2-xl&#31561;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24378;&#22823;&#33021;&#21147;&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23558;&#20004;&#31181;&#27169;&#22411;&#30340;&#36755;&#20986;&#36827;&#34892;&#36830;&#25509;&#25805;&#20316;&#12290;&#36825;&#20010;&#25805;&#20316;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#21516;&#26102;&#32771;&#34385;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#20449;&#24687;&#65292;&#22686;&#24378;&#20102;&#20854;&#34920;&#24449;&#33021;&#21147;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#21487;&#33021;&#22686;&#24378;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document-based Visual Question Answering poses a challenging task between linguistic sense disambiguation and fine-grained multimodal retrieval. Although there has been encouraging progress in document-based question answering due to the utilization of large language and open-world prior models\cite{1}, several challenges persist, including prolonged response times, extended inference durations, and imprecision in matching. In order to overcome these challenges, we propose Jaegar, a concatenation-based multi-transformer VQA model. To derive question features, we leverage the exceptional capabilities of RoBERTa large\cite{2} and GPT2-xl\cite{3} as feature extractors. Subsequently, we subject the outputs from both models to a concatenation process. This operation allows the model to consider information from diverse sources concurrently, strengthening its representational capability. By leveraging pre-trained models for feature extraction, our approach has the potential to amplify the pe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#21464;&#36755;&#20837;&#25552;&#31034;&#26469;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#21892;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#12290;&#36825;&#31181;&#26041;&#27861;&#33258;&#21160;&#37319;&#38598;&#27169;&#22411;&#21453;&#39304;&#65292;&#29983;&#25104;&#36866;&#21512;&#38382;&#39064;&#30340;&#22810;&#26679;&#21270;&#25552;&#31034;&#65292;&#24182;&#36890;&#36807;&#22810;&#27425;&#25512;&#29702;&#35843;&#29992;&#26469;&#38598;&#25104;&#36825;&#20123;&#22810;&#26679;&#21270;&#30340;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2310.07088</link><description>&lt;p&gt;
&#24605;&#32500;&#22810;&#26679;&#24615;&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Diversity of Thought Improves Reasoning Abilities of Large Language Models. (arXiv:2310.07088v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#21464;&#36755;&#20837;&#25552;&#31034;&#26469;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#21892;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#12290;&#36825;&#31181;&#26041;&#27861;&#33258;&#21160;&#37319;&#38598;&#27169;&#22411;&#21453;&#39304;&#65292;&#29983;&#25104;&#36866;&#21512;&#38382;&#39064;&#30340;&#22810;&#26679;&#21270;&#25552;&#31034;&#65292;&#24182;&#36890;&#36807;&#22810;&#27425;&#25512;&#29702;&#35843;&#29992;&#26469;&#38598;&#25104;&#36825;&#20123;&#22810;&#26679;&#21270;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#38656;&#35201;&#22797;&#26434;&#25512;&#29702;&#30340;&#29615;&#22659;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#28982;&#32780;&#65292;&#23558;&#27169;&#22411;&#25351;&#23548;&#20998;&#35299;&#38382;&#39064;&#20026;&#26356;&#23567;&#30340;&#25512;&#29702;&#27493;&#39588;&#25110;&#36890;&#36807;&#20462;&#25913;&#35299;&#30721;&#27493;&#39588;&#20351;&#21508;&#31181;&#29983;&#25104;&#32467;&#26524;&#21512;&#24182;&#21487;&#20197;&#25552;&#21319;&#24615;&#33021;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#37117;&#20551;&#35774;&#36755;&#20837;&#25552;&#31034;&#26159;&#22266;&#23450;&#30340;&#65292;&#24182;&#26399;&#26395;&#35299;&#30721;&#31574;&#30053;&#24341;&#20837;&#25152;&#38656;&#30340;&#22810;&#26679;&#24615;&#12290;&#26412;&#25991;&#25918;&#26494;&#20102;&#36825;&#20010;&#20551;&#35774;&#65292;&#24182;&#35752;&#35770;&#20102;&#22914;&#20309;&#36890;&#36807;&#21019;&#24314;&#21644;&#21033;&#29992;&#36755;&#20837;&#25552;&#31034;&#30340;&#21464;&#21270;&#26469;&#25552;&#21319;&#24605;&#32500;&#22810;&#26679;&#24615;&#20197;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#21521;&#35821;&#35328;&#27169;&#22411;&#24449;&#27714;&#21453;&#39304;&#26469;&#26500;&#24605;&#36866;&#21512;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#33258;&#21160;&#25552;&#39640;&#25552;&#31034;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;DIV-SE (DIVerse reasoning path Self-Ensemble)&#20013;&#23545;&#22810;&#26679;&#30340;&#25552;&#31034;&#36827;&#34892;&#21512;&#25104;&#65292;&#36890;&#36807;&#22810;&#27425;&#25512;&#29702;&#35843;&#29992;&#23454;&#29616;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#32463;&#27982;&#39640;&#25928;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#21363;&#22312;&#19968;&#20010;&#25512;&#29702;&#20013;&#20351;&#29992;&#22810;&#26679;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are documented to struggle in settings that require complex reasoning. Nevertheless, instructing the model to break down the problem into smaller reasoning steps (Wei et al., 2022), or ensembling various generations through modifying decoding steps (Wang et al., 2023) boosts performance. Current methods assume that the input prompt is fixed and expect the decoding strategies to introduce the diversity needed for ensembling. In this work, we relax this assumption and discuss how one can create and leverage variations of the input prompt as a means to diversity of thought to improve model performance. We propose a method that automatically improves prompt diversity by soliciting feedback from the LLM to ideate approaches that fit for the problem. We then ensemble the diverse prompts in our method DIV-SE (DIVerse reasoning path Self-Ensemble) across multiple inference calls. We also propose a cost-effective alternative where diverse prompts are used within a s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#26816;&#32034;&#22686;&#24378;&#21644;&#25439;&#22833;&#21152;&#26435;&#20004;&#31181;&#26041;&#27861;&#65292;&#25104;&#21151;&#25913;&#36827;&#20102;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#23545;&#20110;&#24815;&#29992;&#34920;&#36798;&#30340;&#32763;&#35793;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.07081</link><description>&lt;p&gt;
&#36328;&#36234;&#38376;&#27099;&#65306;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#21644;&#25439;&#22833;&#21152;&#26435;&#23454;&#29616;&#24815;&#29992;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Crossing the Threshold: Idiomatic Machine Translation through Retrieval Augmentation and Loss Weighting. (arXiv:2310.07081v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#26816;&#32034;&#22686;&#24378;&#21644;&#25439;&#22833;&#21152;&#26435;&#20004;&#31181;&#26041;&#27861;&#65292;&#25104;&#21151;&#25913;&#36827;&#20102;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#23545;&#20110;&#24815;&#29992;&#34920;&#36798;&#30340;&#32763;&#35793;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24815;&#29992;&#35821;&#22312;&#26085;&#24120;&#35821;&#35328;&#20013;&#24456;&#24120;&#35265;&#65292;&#20294;&#23545;&#20110;&#32763;&#35793;&#20154;&#21592;&#26469;&#35828;&#24120;&#24120;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#24847;&#20041;&#19981;&#26159;&#30001;&#23427;&#20204;&#30340;&#37096;&#20998;&#30340;&#24847;&#20041;&#25512;&#23548;&#20986;&#26469;&#30340;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#20173;&#28982;&#38590;&#20197;&#32763;&#35793;&#24815;&#29992;&#34920;&#36798;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#24815;&#29992;&#32763;&#35793;&#21644;&#30456;&#20851;&#38382;&#39064;&#30340;&#31616;&#21333;&#25551;&#36848;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#36827;&#34892;&#19968;&#20010;&#32508;&#21512;&#23454;&#39564;&#65292;&#25581;&#31034;&#20102;&#22522;&#20110;transformer&#30340;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#22312;&#27491;&#30830;&#22320;&#40664;&#35748;&#37319;&#29992;&#24815;&#29992;&#32763;&#35793;&#26102;&#30340;&#20020;&#30028;&#28857;&#12290;&#20026;&#20102;&#25193;&#22823;&#22810;&#35821;&#36164;&#28304;&#65292;&#25105;&#20204;&#32534;&#21046;&#20102;&#19968;&#20010;&#21253;&#21547;&#27861;&#35821;&#12289;&#33452;&#20848;&#35821;&#21644;&#26085;&#35821;&#20013;&#21253;&#21547;&#24815;&#29992;&#34920;&#36798;&#30340;&#22823;&#32422;4k&#20010;&#33258;&#28982;&#21477;&#23376;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#25913;&#36827;&#23545;&#33258;&#28982;&#24815;&#29992;&#35821;&#30340;&#32763;&#35793;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#25216;&#26415;&#65306;&#22312;&#21487;&#33021;&#20855;&#26377;&#24815;&#29992;&#24615;&#30340;&#21477;&#23376;&#19978;&#31574;&#30053;&#24615;&#22320;&#21152;&#26435;&#35757;&#32451;&#25439;&#22833;&#65292;&#24182;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#12290;&#36825;&#19981;&#20165;&#20351;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#22312;&#24815;&#29992;&#21477;&#23376;&#19978;&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;&#26368;&#22810;13%&#65292;&#36824;&#25913;&#36827;&#20102;&#32763;&#35793;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Idioms are common in everyday language, but often pose a challenge to translators because their meanings do not follow from the meanings of their parts. Despite significant advances, machine translation systems still struggle to translate idiomatic expressions. We provide a simple characterization of idiomatic translation and related issues. This allows us to conduct a synthetic experiment revealing a tipping point at which transformer-based machine translation models correctly default to idiomatic translations. To expand multilingual resources, we compile a dataset of ~4k natural sentences containing idiomatic expressions in French, Finnish, and Japanese. To improve translation of natural idioms, we introduce two straightforward yet effective techniques: the strategic upweighting of training loss on potentially idiomatic sentences, and using retrieval-augmented models. This not only improves the accuracy of a strong pretrained MT model on idiomatic sentences by up to 13% in absolute a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25299;&#23637;&#20102;&#23545;COVID-19&#34394;&#20551;&#20449;&#24687;&#25968;&#25454;&#38598;&#36827;&#34892;&#23457;&#35745;&#21644;&#24378;&#21270;&#30340;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#21457;&#29616;&#22312;&#23567;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#19987;&#19994;&#20998;&#31867;&#22120;&#22312;&#37326;&#22806;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#26377;&#38480;&#12290;&#20854;&#27425;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#20154;&#24037;&#27880;&#37322;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25239;&#20869;&#23481;&#26469;&#22686;&#24378;&#20998;&#31867;&#22120;&#12290;</title><link>http://arxiv.org/abs/2310.07078</link><description>&lt;p&gt;
&#36890;&#36807;&#25239;&#20869;&#23481;&#37319;&#26679;&#23545;COVID-19&#34394;&#20551;&#20449;&#24687;&#25968;&#25454;&#38598;&#36827;&#34892;&#23457;&#35745;&#21644;&#24378;&#21270;
&lt;/p&gt;
&lt;p&gt;
Auditing and Robustifying COVID-19 Misinformation Datasets via Anticontent Sampling. (arXiv:2310.07078v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07078
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25299;&#23637;&#20102;&#23545;COVID-19&#34394;&#20551;&#20449;&#24687;&#25968;&#25454;&#38598;&#36827;&#34892;&#23457;&#35745;&#21644;&#24378;&#21270;&#30340;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#21457;&#29616;&#22312;&#23567;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#19987;&#19994;&#20998;&#31867;&#22120;&#22312;&#37326;&#22806;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#26377;&#38480;&#12290;&#20854;&#27425;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#20154;&#24037;&#27880;&#37322;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25239;&#20869;&#23481;&#26469;&#22686;&#24378;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26377;&#20004;&#20010;&#20851;&#38190;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#23427;&#35748;&#20026;&#22312;&#23567;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#39640;&#24230;&#19987;&#19994;&#30340;&#32597;&#35265;&#20869;&#23481;&#20998;&#31867;&#22120;&#36890;&#24120;&#23545;&#37326;&#22806;&#35266;&#23519;&#21040;&#30340;&#36127;&#38754;&#31867;&#21035;&#30340;&#20016;&#23500;&#24615;&#21644;&#35805;&#39064;&#22810;&#26679;&#24615;&#65288;&#31216;&#20026;&#25239;&#20869;&#23481;&#65289;&#26377;&#38480;&#26292;&#38706;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#20998;&#31867;&#22120;&#22312;&#27979;&#35797;&#38598;&#19978;&#35266;&#23519;&#21040;&#30340;&#24378;&#22823;&#24615;&#33021;&#21487;&#33021;&#26080;&#27861;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#29615;&#22659;&#20013;&#26377;&#25928;&#36716;&#21270;&#12290;&#22312;COVID-19&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#37326;&#22806;&#23457;&#35745;&#65292;&#24182;&#35777;&#26126;&#20102;&#20351;&#29992;&#20960;&#20010;&#37325;&#35201;&#24341;&#29992;&#30340;&#26368;&#36817;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#37326;&#22806;&#35780;&#20272;&#26102;&#23481;&#26131;&#21463;&#21040;&#25239;&#20869;&#23481;&#30340;&#25915;&#20987;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#21160;&#23398;&#20064;&#27969;&#31243;&#65292;&#23427;&#19981;&#38656;&#35201;&#20219;&#20309;&#20154;&#24037;&#27880;&#37322;&#65292;&#24182;&#36890;&#36807;&#25361;&#25112;&#24615;&#30340;&#25239;&#20869;&#23481;&#19981;&#26029;&#22686;&#24378;&#35757;&#32451;&#25968;&#25454;&#65292;&#20174;&#32780;&#24378;&#21270;&#36825;&#20123;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper makes two key contributions. First, it argues that highly specialized rare content classifiers trained on small data typically have limited exposure to the richness and topical diversity of the negative class (dubbed anticontent) as observed in the wild. As a result, these classifiers' strong performance observed on the test set may not translate into real-world settings. In the context of COVID-19 misinformation detection, we conduct an in-the-wild audit of multiple datasets and demonstrate that models trained with several prominently cited recent datasets are vulnerable to anticontent when evaluated in the wild. Second, we present a novel active learning pipeline that requires zero manual annotation and iteratively augments the training data with challenging anticontent, robustifying these classifiers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#35821;&#27861;&#38169;&#35823;&#19988;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#30340;LLM&#24037;&#20855;&#20351;&#29992;&#26041;&#27861;ToolDec&#65292;&#36890;&#36807;&#26377;&#38480;&#29366;&#24577;&#35299;&#30721;&#31639;&#27861;&#28040;&#38500;&#20102;&#24037;&#20855;&#30456;&#20851;&#38169;&#35823;&#65292;&#20351;LLM&#33021;&#22815;&#26377;&#25928;&#36873;&#25321;&#24037;&#20855;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#25110;&#19978;&#19979;&#25991;&#25991;&#26723;&#12290;</title><link>http://arxiv.org/abs/2310.07075</link><description>&lt;p&gt;
&#36890;&#36807;&#26377;&#38480;&#29366;&#24577;&#35299;&#30721;&#23454;&#29616;&#26080;&#35821;&#27861;&#38169;&#35823;&#21644;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#30340;LLM&#24037;&#20855;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
Syntax Error-Free and Generalizable Tool Use for LLMs via Finite-State Decoding. (arXiv:2310.07075v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#35821;&#27861;&#38169;&#35823;&#19988;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#30340;LLM&#24037;&#20855;&#20351;&#29992;&#26041;&#27861;ToolDec&#65292;&#36890;&#36807;&#26377;&#38480;&#29366;&#24577;&#35299;&#30721;&#31639;&#27861;&#28040;&#38500;&#20102;&#24037;&#20855;&#30456;&#20851;&#38169;&#35823;&#65292;&#20351;LLM&#33021;&#22815;&#26377;&#25928;&#36873;&#25321;&#24037;&#20855;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#25110;&#19978;&#19979;&#25991;&#25991;&#26723;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#23637;&#31034;&#20986;&#20351;&#29992;&#22806;&#37096;&#24037;&#20855;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#26377;&#24076;&#26395;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#28041;&#21450;&#23545;&#24037;&#20855;&#28436;&#31034;&#36827;&#34892;&#24494;&#35843;&#65292;&#36825;&#26679;&#22312;&#27809;&#26377;&#39069;&#22806;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#26080;&#27861;&#25512;&#24191;&#21040;&#26032;&#30340;&#24037;&#20855;&#65292;&#35201;&#20040;&#22312;&#19978;&#19979;&#25991;&#20013;&#25552;&#20379;&#24037;&#20855;&#25991;&#26723;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#24037;&#20855;&#25968;&#37327;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#24120;&#24120;&#20135;&#29983;&#35821;&#27861;&#26080;&#25928;&#30340;&#24037;&#20855;&#35843;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ToolDec&#65292;&#19968;&#31181;&#26377;&#38480;&#29366;&#24577;&#26426;&#24341;&#23548;&#30340;&#35299;&#30721;&#31639;&#27861;&#65292;&#29992;&#20110;&#24037;&#20855;&#22686;&#24378;&#30340;LLMs&#12290;ToolDec&#36890;&#36807;&#30830;&#20445;&#26377;&#25928;&#30340;&#24037;&#20855;&#21517;&#31216;&#21644;&#31867;&#22411;&#19968;&#33268;&#30340;&#21442;&#25968;&#65292;&#28040;&#38500;&#20102;&#20219;&#20309;&#24037;&#20855;&#22686;&#24378;&#30340;LLMs&#20013;&#30340;&#24037;&#20855;&#30456;&#20851;&#38169;&#35823;&#12290;&#27492;&#22806;&#65292;ToolDec&#20351;LLM&#33021;&#22815;&#20165;&#20165;&#20351;&#29992;&#23427;&#20204;&#30340;&#21517;&#31216;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#26377;&#25928;&#22320;&#36873;&#25321;&#24037;&#20855;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#25110;&#19978;&#19979;&#25991;&#25991;&#26723;&#12290;&#25105;&#20204;&#22312;&#28041;&#21450;&#25968;&#23398;&#20989;&#25968;&#12289;&#30693;&#35782;&#22270;&#35889;&#20851;&#31995;&#21644;&#22797;&#26434;&#30340;&#29616;&#23454;&#19990;&#30028;RESTful API&#30340;&#21508;&#31181;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#22810;&#31181;&#20808;&#21069;&#30340;&#26041;&#27861;&#21450;&#20854;ToolDec&#22686;&#24378;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown promising capabilities in using external tools to solve complex problems. However, existing approaches either involve fine-tuning on tool demonstrations, which do not generalize to new tools without additional training, or providing tool documentation in context, limiting the number of tools. Both approaches often generate syntactically invalid tool calls. In this paper, we propose ToolDec, a finite-state machine-guided decoding algorithm for tool-augmented LLMs. ToolDec eliminates tool-related errors for any tool-augmented LLMs by ensuring valid tool names and type-conforming arguments. Furthermore, ToolDec enables LLM to effectively select tools using only the information contained in their names, with no need for fine-tuning or in-context documentation. We evaluated multiple prior methods and their ToolDec-enhanced versions on a variety of tasks involving tools like math functions, knowledge graph relations, and complex real-world RESTful APIs
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#25552;&#39640;&#25552;&#31034;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Hypotheses-to-Theories (HtT)&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;LLMs&#25512;&#29702;&#30340;&#35268;&#21017;&#24211;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#25552;&#31034;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.07064</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
Large Language Models can Learn Rules. (arXiv:2310.07064v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07064
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#25552;&#39640;&#25552;&#31034;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Hypotheses-to-Theories (HtT)&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;LLMs&#25512;&#29702;&#30340;&#35268;&#21017;&#24211;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#25552;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#32473;&#20986;&#19968;&#20123;&#31034;&#20363;&#21644;&#20013;&#38388;&#27493;&#39588;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20381;&#36182;LLM&#20013;&#30340;&#38544;&#24335;&#30693;&#35782;&#30340;&#25552;&#31034;&#26041;&#27861;&#22312;&#38544;&#24335;&#30693;&#35782;&#38169;&#35823;&#25110;&#19982;&#20219;&#21153;&#19981;&#19968;&#33268;&#26102;&#24448;&#24448;&#20250;&#20135;&#29983;&#38169;&#35823;&#30340;&#31572;&#26696;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;"&#20551;&#35774;&#21040;&#29702;&#35770;" (HtT) &#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;LLMs&#25512;&#29702;&#30340;&#35268;&#21017;&#24211;&#12290;HtT&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65292;&#24402;&#32435;&#38454;&#27573;&#21644;&#28436;&#32462;&#38454;&#27573;&#12290;&#22312;&#24402;&#32435;&#38454;&#27573;&#65292;&#39318;&#20808;&#35201;&#27714;LLM&#26681;&#25454;&#19968;&#32452;&#35757;&#32451;&#31034;&#20363;&#29983;&#25104;&#21644;&#39564;&#35777;&#35268;&#21017;&#12290;&#20986;&#29616;&#24182;&#23548;&#33268;&#27491;&#30830;&#31572;&#26696;&#30340;&#35268;&#21017;&#23558;&#34987;&#25910;&#38598;&#24418;&#25104;&#19968;&#20010;&#35268;&#21017;&#24211;&#12290;&#22312;&#28436;&#32462;&#38454;&#27573;&#65292;&#28982;&#21518;&#35201;&#27714;LLM&#20351;&#29992;&#23398;&#20064;&#30340;&#35268;&#21017;&#24211;&#36827;&#34892;&#25512;&#29702;&#20197;&#22238;&#31572;&#27979;&#35797;&#38382;&#39064;&#12290;&#22312;&#25968;&#20540;&#25512;&#29702;&#21644;&#20851;&#31995;&#25512;&#29702;&#38382;&#39064;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;HtT&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#20351;&#20854;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
When prompted with a few examples and intermediate steps, large language models (LLMs) have demonstrated impressive performance in various reasoning tasks. However, prompting methods that rely on implicit knowledge in an LLM often hallucinate incorrect answers when the implicit knowledge is wrong or inconsistent with the task. To tackle this problem, we present Hypotheses-to-Theories (HtT), a framework that learns a rule library for reasoning with LLMs. HtT contains two stages, an induction stage and a deduction stage. In the induction stage, an LLM is first asked to generate and verify rules over a set of training examples. Rules that appear and lead to correct answers sufficiently often are collected to form a rule library. In the deduction stage, the LLM is then prompted to employ the learned rule library to perform reasoning to answer test questions. Experiments on both numerical reasoning and relational reasoning problems show that HtT improves existing prompting methods, with an 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DKEC&#65292;&#19968;&#31181;&#39046;&#22495;&#30693;&#35782;&#22686;&#24378;&#30340;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#21307;&#23398;&#35786;&#26029;&#39044;&#27979;&#12290;&#23427;&#21033;&#29992;&#26631;&#31614;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#32452;&#20869;&#35757;&#32451;&#26041;&#27861;&#26469;&#25429;&#25417;&#21307;&#23398;&#23454;&#20307;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#24182;&#22686;&#21152;&#32597;&#35265;&#31867;&#21035;&#30340;&#26679;&#26412;&#25968;&#37327;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.07059</link><description>&lt;p&gt;
DKEC: &#39046;&#22495;&#30693;&#35782;&#22686;&#24378;&#30340;&#30005;&#23376;&#30149;&#21382;&#22810;&#26631;&#31614;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
DKEC: Domain Knowledge Enhanced Multi-Label Classification for Electronic Health Records. (arXiv:2310.07059v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DKEC&#65292;&#19968;&#31181;&#39046;&#22495;&#30693;&#35782;&#22686;&#24378;&#30340;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#21307;&#23398;&#35786;&#26029;&#39044;&#27979;&#12290;&#23427;&#21033;&#29992;&#26631;&#31614;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#32452;&#20869;&#35757;&#32451;&#26041;&#27861;&#26469;&#25429;&#25417;&#21307;&#23398;&#23454;&#20307;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#24182;&#22686;&#21152;&#32597;&#35265;&#31867;&#21035;&#30340;&#26679;&#26412;&#25968;&#37327;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#39046;&#22495;&#30340;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#32463;&#24120;&#38754;&#20020;&#38271;&#23614;&#26631;&#31614;&#20998;&#24067;&#65292;&#21363;&#32597;&#35265;&#31867;&#21035;&#30340;&#35757;&#32451;&#26679;&#26412;&#23569;&#20110;&#39057;&#32321;&#31867;&#21035;&#12290;&#34429;&#28982;&#20043;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#23618;&#27425;&#21270;&#26631;&#31614;&#32467;&#26500;&#26469;&#25214;&#21040;&#37325;&#35201;&#29305;&#24449;&#65292;&#20294;&#22823;&#22810;&#25968;&#24573;&#30053;&#20102;&#20174;&#21307;&#23398;&#25351;&#21335;&#20013;&#34701;&#20837;&#39046;&#22495;&#30693;&#35782;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DKEC&#65292;&#19968;&#31181;&#22686;&#24378;&#21307;&#23398;&#35786;&#26029;&#39044;&#27979;&#30340;&#39046;&#22495;&#30693;&#35782;&#22686;&#24378;&#20998;&#31867;&#22120;&#65292;&#20854;&#20013;&#21253;&#25324;&#20004;&#20010;&#21019;&#26032;&#28857;&#65306;&#65288;1&#65289;&#19968;&#20010;&#22522;&#20110;&#26631;&#31614;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#32467;&#21512;&#24322;&#26500;&#22270;&#21644;&#39046;&#22495;&#26412;&#20307;&#26469;&#25429;&#25417;&#21307;&#23398;&#23454;&#20307;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#65288;2&#65289;&#19968;&#31181;&#22522;&#20110;&#26631;&#31614;&#30456;&#20284;&#24615;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#32452;&#20869;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#21152;&#32597;&#35265;&#31867;&#21035;&#30340;&#26679;&#26412;&#25968;&#37327;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#30495;&#23454;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;DKEC&#65306;RAA&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#26469;&#33258;&#24613;&#25937;&#26381;&#21153;&#65288;EMS&#65289;&#20107;&#20214;&#30340;4,417&#20010;&#24739;&#32773;&#25252;&#29702;&#25253;&#21578;&#30340;&#25910;&#38598;&#65292;&#21644;&#26469;&#33258;53898&#25253;&#21578;&#30340;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-label text classification (MLTC) tasks in the medical domain often face long-tail label distribution, where rare classes have fewer training samples than frequent classes. Although previous works have explored different model architectures and hierarchical label structures to find important features, most of them neglect to incorporate the domain knowledge from medical guidelines. In this paper, we present DKEC, Domain Knowledge Enhanced Classifier for medical diagnosis prediction with two innovations: (1) a label-wise attention mechanism that incorporates a heterogeneous graph and domain ontologies to capture the semantic relationships between medical entities, (2) a simple yet effective group-wise training method based on similarity of labels to increase samples of rare classes. We evaluate DKEC on two real-world medical datasets: the RAA dataset, a collection of 4,417 patient care reports from emergency medical services (EMS) incidents, and a subset of 53,898 reports from the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#31227;&#21160;&#20132;&#20114;&#36712;&#36857;&#20013;&#33258;&#21160;&#25552;&#21462;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#23439;&#20219;&#21153;&#12290;&#36890;&#36807;&#22810;&#39033;&#30740;&#31350;&#21644;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#25552;&#21462;&#30340;&#23439;&#20219;&#21153;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.07023</link><description>&lt;p&gt;
&#20174;&#22823;&#35268;&#27169;&#20132;&#20114;&#36712;&#36857;&#20013;&#33258;&#21160;&#25366;&#25496;&#23439;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Automatic Macro Mining from Interaction Traces at Scale. (arXiv:2310.07023v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#31227;&#21160;&#20132;&#20114;&#36712;&#36857;&#20013;&#33258;&#21160;&#25552;&#21462;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#23439;&#20219;&#21153;&#12290;&#36890;&#36807;&#22810;&#39033;&#30740;&#31350;&#21644;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#25552;&#21462;&#30340;&#23439;&#20219;&#21153;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23439;&#20219;&#21153;&#26159;&#25105;&#20204;&#26085;&#24120;&#25163;&#26426;&#27963;&#21160;&#30340;&#26500;&#24314;&#22359;&#20219;&#21153;&#65288;&#20363;&#22914;&#65292;&#8220;&#30331;&#24405;&#8221;&#25110;&#8220;&#39044;&#23450;&#33322;&#29677;&#8221;&#65289;&#12290;&#26377;&#25928;&#22320;&#25552;&#21462;&#23439;&#20219;&#21153;&#23545;&#20110;&#29702;&#35299;&#31227;&#21160;&#20132;&#20114;&#21644;&#23454;&#29616;&#20219;&#21153;&#33258;&#21160;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#23439;&#20219;&#21153;&#22312;&#22823;&#35268;&#27169;&#24773;&#20917;&#19979;&#24456;&#38590;&#25552;&#21462;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#30001;&#22810;&#20010;&#27493;&#39588;&#32452;&#25104;&#65292;&#21516;&#26102;&#21448;&#38544;&#34255;&#22312;&#24212;&#29992;&#30340;&#32534;&#31243;&#32452;&#20214;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#33258;&#21160;&#20174;&#38543;&#26426;&#21644;&#29992;&#25143;&#31574;&#21010;&#30340;&#31227;&#21160;&#20132;&#20114;&#36712;&#36857;&#20013;&#25552;&#21462;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#23439;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20135;&#29983;&#30340;&#23439;&#20219;&#21153;&#33258;&#21160;&#26631;&#35760;&#20102;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#24182;&#19988;&#21487;&#20197;&#23436;&#20840;&#25191;&#34892;&#12290;&#20026;&#20102;&#26816;&#39564;&#25552;&#21462;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#39033;&#30740;&#31350;&#65292;&#21253;&#25324;&#29992;&#25143;&#35780;&#20272;&#12289;&#19982;&#20154;&#24037;&#31574;&#21010;&#20219;&#21153;&#30340;&#27604;&#36739;&#20998;&#26512;&#20197;&#21450;&#23545;&#36825;&#20123;&#23439;&#20219;&#21153;&#30340;&#33258;&#21160;&#25191;&#34892;&#12290;&#36825;&#20123;&#23454;&#39564;&#21644;&#20998;&#26512;&#26174;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#25552;&#21462;&#30340;&#23439;&#20219;&#21153;&#22312;&#19981;&#21516;&#30340;&#20219;&#21153;&#20013;&#30340;&#26377;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Macros are building block tasks of our everyday smartphone activity (e.g., "login", or "booking a flight"). Effectively extracting macros is important for understanding mobile interaction and enabling task automation. These macros are however difficult to extract at scale as they can be comprised of multiple steps yet hidden within programmatic components of the app. In this paper, we introduce a novel approach based on Large Language Models (LLMs) to automatically extract semantically meaningful macros from both random and user-curated mobile interaction traces. The macros produced by our approach are automatically tagged with natural language descriptions and are fully executable. To examine the quality of extraction, we conduct multiple studies, including user evaluation, comparative analysis against human-curated tasks, and automatic execution of these macros. These experiments and analyses show the effectiveness of our approach and the usefulness of extracted macros in various dow
&lt;/p&gt;</description></item><item><title>NEWTON&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29289;&#29702;&#25512;&#29702;&#33021;&#21147;&#30340;&#20179;&#24211;&#21644;&#22522;&#20934;&#65292;&#21253;&#21547;2800&#20010;&#29289;&#20307;-&#23646;&#24615;&#23545;&#21644;160K&#20010;&#38382;&#31572;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07018</link><description>&lt;p&gt;
NEWTON: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#29289;&#29702;&#25512;&#29702;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
NEWTON: Are Large Language Models Capable of Physical Reasoning?. (arXiv:2310.07018v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07018
&lt;/p&gt;
&lt;p&gt;
NEWTON&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29289;&#29702;&#25512;&#29702;&#33021;&#21147;&#30340;&#20179;&#24211;&#21644;&#22522;&#20934;&#65292;&#21253;&#21547;2800&#20010;&#29289;&#20307;-&#23646;&#24615;&#23545;&#21644;160K&#20010;&#38382;&#31572;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20854;&#35821;&#22659;&#21270;&#34920;&#31034;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#34987;&#23454;&#35777;&#22320;&#35777;&#26126;&#21253;&#21547;&#21477;&#27861;&#12289;&#35821;&#20041;&#12289;&#35789;&#20041;&#21644;&#24120;&#35782;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#23427;&#20204;&#22312;&#29289;&#29702;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#25506;&#32034;&#36824;&#26377;&#19968;&#23450;&#38480;&#21046;&#65292;&#29305;&#21035;&#26159;&#28041;&#21450;&#29702;&#35299;&#26085;&#24120;&#29289;&#20307;&#30340;&#20851;&#38190;&#23646;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;NEWTON&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLM&#30340;&#29289;&#29702;&#25512;&#29702;&#33021;&#21147;&#30340;&#20179;&#24211;&#21644;&#22522;&#20934;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#23454;&#29616;&#27492;&#22522;&#20934;&#30340;&#39046;&#22495;&#29305;&#23450;&#36866;&#24212;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#27969;&#31243;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#29983;&#25104;&#19968;&#20010;&#26681;&#25454;&#20182;&#20204;&#24212;&#29992;&#31243;&#24207;&#30456;&#20851;&#29289;&#20307;&#21644;&#23646;&#24615;&#23450;&#21046;&#30340;&#22522;&#20934;&#21464;&#20307;&#12290;NEWTON&#20179;&#24211;&#21253;&#25324;2800&#20010;&#29289;&#20307;-&#23646;&#24615;&#23545;&#30340;&#38598;&#21512;&#65292;&#20026;&#29983;&#25104;&#26080;&#38480;&#35268;&#27169;&#30340;&#35780;&#20272;&#27169;&#26495;&#22880;&#23450;&#22522;&#30784;&#12290;NEWTON&#22522;&#20934;&#21253;&#21547;160K&#20010;&#38382;&#31572;&#38382;&#39064;&#65292;&#20351;&#29992;NEWTON&#20179;&#24211;&#31574;&#21010;&#65292;&#20197;&#35843;&#26597;&#29289;&#29702;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), through their contextualized representations, have been empirically proven to encapsulate syntactic, semantic, word sense, and common-sense knowledge. However, there has been limited exploration of their physical reasoning abilities, specifically concerning the crucial attributes for comprehending everyday objects. To address this gap, we introduce NEWTON, a repository and benchmark for evaluating the physics reasoning skills of LLMs. Further, to enable domain-specific adaptation of this benchmark, we present a pipeline to enable researchers to generate a variant of this benchmark that has been customized to the objects and attributes relevant for their application. The NEWTON repository comprises a collection of 2800 object-attribute pairs, providing the foundation for generating infinite-scale assessment templates. The NEWTON benchmark consists of 160K QA questions, curated using the NEWTON repository to investigate the physical reasoning capabilities of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#25991;&#26412;&#38382;&#31572;&#31995;&#32479;&#29983;&#25104;&#30340;&#20505;&#36873;&#31572;&#26696;&#22522;&#20110;&#20854;&#31867;&#22411;&#36827;&#34892;&#36807;&#28388;&#21644;&#37325;&#26032;&#25490;&#24207;&#65292;&#20197;&#35299;&#20915;&#22312;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#20219;&#21153;&#20013;&#65292;&#27169;&#22411;&#23481;&#37327;&#26377;&#38480;&#19988;&#23545;&#20110;&#21547;&#26377;&#19981;&#22826;&#27969;&#34892;&#23454;&#20307;&#30340;&#38382;&#39064;&#36136;&#37327;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07008</link><description>&lt;p&gt;
&#31572;&#26696;&#20505;&#36873;&#31867;&#22411;&#36873;&#25321;&#65306;&#38381;&#20070;&#38382;&#31572;&#20013;&#30340;&#25991;&#26412;&#21040;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#28385;&#36275;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
Answer Candidate Type Selection: Text-to-Text Language Model for Closed Book Question Answering Meets Knowledge Graphs. (arXiv:2310.07008v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07008
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#25991;&#26412;&#38382;&#31572;&#31995;&#32479;&#29983;&#25104;&#30340;&#20505;&#36873;&#31572;&#26696;&#22522;&#20110;&#20854;&#31867;&#22411;&#36827;&#34892;&#36807;&#28388;&#21644;&#37325;&#26032;&#25490;&#24207;&#65292;&#20197;&#35299;&#20915;&#22312;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#20219;&#21153;&#20013;&#65292;&#27169;&#22411;&#23481;&#37327;&#26377;&#38480;&#19988;&#23545;&#20110;&#21547;&#26377;&#19981;&#22826;&#27969;&#34892;&#23454;&#20307;&#30340;&#38382;&#39064;&#36136;&#37327;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;T5&#25110;BART&#65289;&#22312;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#65288;KGQA&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#30340;&#23481;&#37327;&#26377;&#38480;&#65292;&#23545;&#20110;&#21253;&#21547;&#19981;&#22826;&#27969;&#34892;&#23454;&#20307;&#30340;&#38382;&#39064;&#65292;&#36136;&#37327;&#19979;&#38477;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#25991;&#26412;&#38382;&#31572;&#31995;&#32479;&#30340;&#22522;&#30784;&#19978;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26681;&#25454;&#20505;&#36873;&#31572;&#26696;&#30340;&#31867;&#22411;&#65288;&#26469;&#33258;Wikidata&#30340;"instance_of"&#23646;&#24615;&#65289;&#36827;&#34892;&#31579;&#36873;&#21644;&#37325;&#26032;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Text-to-Text Language Models (LMs), such as T5 or BART yield promising results in the Knowledge Graph Question Answering (KGQA) task. However, the capacity of the models is limited and the quality decreases for questions with less popular entities. In this paper, we present a novel approach which works on top of the pre-trained Text-to-Text QA system to address this issue. Our simple yet effective method performs filtering and re-ranking of generated candidates based on their types derived from Wikidata "instance_of" property.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29983;&#25104;&#25915;&#20987;&#25216;&#26415;&#65292;&#36890;&#36807;&#25805;&#32437;&#35299;&#30721;&#26041;&#27861;&#30340;&#21464;&#21270;&#65292;&#21487;&#20197;&#23548;&#33268;&#24320;&#28304;LLMs&#30340;&#28798;&#38590;&#24615;&#36234;&#29425;&#65292;&#23558;&#38169;&#20301;&#29575;&#20174;0%&#25552;&#39640;&#21040;&#20102;&#36229;&#36807;95%&#12290;&#36825;&#39033;&#25915;&#20987;&#26041;&#27861;&#31616;&#21333;&#32780;&#26377;&#25928;&#65292;&#22312;11&#20010;&#35821;&#35328;&#27169;&#22411;&#20013;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#19988;&#35745;&#31639;&#37327;&#38477;&#20302;&#20102;30&#20493;&#12290;</title><link>http://arxiv.org/abs/2310.06987</link><description>&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#25216;&#26415;&#23454;&#26045;&#24320;&#28304;LLM&#30340;&#28798;&#38590;&#24615;&#36234;&#29425;
&lt;/p&gt;
&lt;p&gt;
Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation. (arXiv:2310.06987v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29983;&#25104;&#25915;&#20987;&#25216;&#26415;&#65292;&#36890;&#36807;&#25805;&#32437;&#35299;&#30721;&#26041;&#27861;&#30340;&#21464;&#21270;&#65292;&#21487;&#20197;&#23548;&#33268;&#24320;&#28304;LLMs&#30340;&#28798;&#38590;&#24615;&#36234;&#29425;&#65292;&#23558;&#38169;&#20301;&#29575;&#20174;0%&#25552;&#39640;&#21040;&#20102;&#36229;&#36807;95%&#12290;&#36825;&#39033;&#25915;&#20987;&#26041;&#27861;&#31616;&#21333;&#32780;&#26377;&#25928;&#65292;&#22312;11&#20010;&#35821;&#35328;&#27169;&#22411;&#20013;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#19988;&#35745;&#31639;&#37327;&#38477;&#20302;&#20102;30&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#28304;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#26497;&#22823;&#22320;&#25512;&#36827;&#20102;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#12290;&#22312;&#21457;&#24067;&#27169;&#22411;&#20043;&#21069;&#65292;&#20154;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#21162;&#21147;&#65292;&#20197;&#30830;&#20445;&#27169;&#22411;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#19968;&#33268;&#24615;&#65292;&#20027;&#35201;&#30446;&#26631;&#26159;&#30830;&#20445;&#20854;&#26377;&#30410;&#19988;&#26080;&#23475;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#32463;&#36807;&#31934;&#24515;&#23545;&#40784;&#30340;&#27169;&#22411;&#20063;&#21487;&#33021;&#34987;&#24694;&#24847;&#25805;&#32437;&#65292;&#23548;&#33268;&#24847;&#22806;&#34892;&#20026;&#65292;&#21363;&#25152;&#35859;&#30340;&#8220;&#36234;&#29425;&#8221;&#12290;&#36825;&#20123;&#36234;&#29425;&#36890;&#24120;&#30001;&#29305;&#23450;&#30340;&#25991;&#26412;&#36755;&#20837;&#35302;&#21457;&#65292;&#36890;&#24120;&#31216;&#20026;&#23545;&#25239;&#24615;&#25552;&#31034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#25915;&#20987;&#25216;&#26415;&#65292;&#36825;&#26159;&#19968;&#31181;&#26497;&#20854;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#20165;&#36890;&#36807;&#25805;&#32437;&#35299;&#30721;&#26041;&#27861;&#30340;&#21464;&#21270;&#26469;&#25200;&#20081;&#27169;&#22411;&#30340;&#23545;&#40784;&#12290;&#36890;&#36807;&#21033;&#29992;&#19981;&#21516;&#30340;&#29983;&#25104;&#31574;&#30053;&#65292;&#21253;&#25324;&#21464;&#21270;&#30340;&#35299;&#30721;&#36229;&#21442;&#25968;&#21644;&#37319;&#26679;&#26041;&#27861;&#65292;&#25105;&#20204;&#23558;11&#20010;&#35821;&#35328;&#27169;&#22411;&#65292;&#21253;&#25324;LLaMA2&#12289;Vicuna&#12289;Falcon&#21644;MPT&#23478;&#26063;&#30340;&#38169;&#20301;&#29575;&#20174;0%&#25552;&#39640;&#21040;&#20102;95%&#20197;&#19978;&#65292;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#35745;&#31639;&#37327;&#38477;&#20302;&#20102;30&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid progress in open-source large language models (LLMs) is significantly advancing AI development. Extensive efforts have been made before model release to align their behavior with human values, with the primary goal of ensuring their helpfulness and harmlessness. However, even carefully aligned models can be manipulated maliciously, leading to unintended behaviors, known as "jailbreaks". These jailbreaks are typically triggered by specific text inputs, often referred to as adversarial prompts. In this work, we propose the generation exploitation attack, an extremely simple approach that disrupts model alignment by only manipulating variations of decoding methods. By exploiting different generation strategies, including varying decoding hyper-parameters and sampling methods, we increase the misalignment rate from 0% to more than 95% across 11 language models including LLaMA2, Vicuna, Falcon, and MPT families, outperforming state-of-the-art attacks with $30\times$ lower computat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#36829;&#21453;&#26399;&#26395;&#26426;&#21046;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#38477;&#20302;&#29992;&#25143;&#39044;&#27979;&#35823;&#24046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20803;&#35748;&#30693;&#25552;&#31034;&#26694;&#26550;&#65292;&#24182;&#21457;&#29616;&#23384;&#20648;&#21644;&#26816;&#32034;&#36829;&#21453;&#29992;&#25143;&#26399;&#26395;&#30340;&#20107;&#23454;&#21487;&#20197;&#20351;&#27169;&#22411;&#20197;&#31867;&#20284;&#20154;&#31867;&#23398;&#20064;&#29702;&#35770;&#30340;&#26041;&#24335;&#20102;&#35299;&#29992;&#25143;&#12290;</title><link>http://arxiv.org/abs/2310.06983</link><description>&lt;p&gt;
&#36890;&#36807;&#20803;&#35748;&#30693;&#25552;&#31034;&#36829;&#21453;&#26399;&#26395;&#38477;&#20302;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24515;&#26234;&#29702;&#35770;&#39044;&#27979;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
Violation of Expectation via Metacognitive Prompting Reduces Theory of Mind Prediction Error in Large Language Models. (arXiv:2310.06983v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#36829;&#21453;&#26399;&#26395;&#26426;&#21046;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#38477;&#20302;&#29992;&#25143;&#39044;&#27979;&#35823;&#24046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20803;&#35748;&#30693;&#25552;&#31034;&#26694;&#26550;&#65292;&#24182;&#21457;&#29616;&#23384;&#20648;&#21644;&#26816;&#32034;&#36829;&#21453;&#29992;&#25143;&#26399;&#26395;&#30340;&#20107;&#23454;&#21487;&#20197;&#20351;&#27169;&#22411;&#20197;&#31867;&#20284;&#20154;&#31867;&#23398;&#20064;&#29702;&#35770;&#30340;&#26041;&#24335;&#20102;&#35299;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#24515;&#26234;&#29702;&#35770;(ToM)&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#20449;&#26381;&#30340;&#27700;&#24179;&#12290;&#23558;&#19981;&#21487;&#35266;&#23519;&#30340;&#24515;&#29702;&#29366;&#24577;&#24402;&#22240;&#20110;&#20182;&#20154;&#23545;&#20110;&#20154;&#31867;&#31038;&#20250;&#35748;&#30693;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#19988;&#22312;&#20010;&#20307;&#20154;&#31867;&#19982;&#20154;&#24037;&#26234;&#33021;(AIs)&#20043;&#38388;&#30340;&#22996;&#25176;-&#20195;&#29702;&#20851;&#31995;&#20013;&#21487;&#33021;&#21516;&#26679;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#22312;&#21457;&#23637;&#24515;&#29702;&#23398;&#20013;&#30740;&#31350;&#30340;&#26426;&#21046;&#65292;&#21363;&#36829;&#21453;&#26399;&#26395;(VoE)&#65292;&#22914;&#20309;&#23454;&#29616;&#20197;&#36890;&#36807;&#21033;&#29992;&#26032;&#29983;&#30340;ToM&#21151;&#33021;&#26469;&#38477;&#20302;LLM&#23545;&#29992;&#25143;&#30340;&#39044;&#27979;&#35823;&#24046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#8220;&#20803;&#35748;&#30693;&#25552;&#31034;&#8221;&#26694;&#26550;&#65292;&#20197;&#22312;AI&#36741;&#23548;&#21592;&#30340;&#24773;&#22659;&#20013;&#24212;&#29992;VoE&#12290;&#36890;&#36807;&#23384;&#20648;&#21644;&#26816;&#32034;&#22312;LLM&#23545;&#29992;&#25143;&#26399;&#26395;&#34987;&#36829;&#21453;&#30340;&#24773;&#20917;&#19979;&#24471;&#21040;&#30340;&#20107;&#23454;&#65292;&#25105;&#20204;&#21457;&#29616;LLM&#33021;&#22815;&#20197;&#19982;&#20154;&#31867;&#23398;&#20064;&#29702;&#35770;&#30456;&#31526;&#30340;&#26041;&#24335;&#20102;&#35299;&#29992;&#25143;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#24314;&#27169;&#29992;&#25143;&#24515;&#29702;&#30340;&#28508;&#22312;&#21361;&#38505;&#21644;&#22686;&#24378;&#26426;&#20250;&#65292;&#24182;&#25552;&#20986;&#20102;&#25511;&#21046;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research shows that Large Language Models (LLMs) exhibit a compelling level of proficiency in Theory of Mind (ToM) tasks. This ability to impute unobservable mental states to others is vital to human social cognition and may prove equally important in principal-agent relations between individual humans and Artificial Intelligences (AIs). In this paper, we explore how a mechanism studied in developmental psychology known as Violation of Expectation (VoE) can be implemented to reduce errors in LLM prediction about users by leveraging emergent ToM affordances. And we introduce a \textit{metacognitive prompting} framework to apply VoE in the context of an AI tutor. By storing and retrieving facts derived in cases where LLM expectation about the user was violated, we find that LLMs are able to learn about users in ways that echo theories of human learning. Finally, we discuss latent hazards and augmentative opportunities associated with modeling user psychology and propose ways to mi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;Transformer&#23884;&#20837;&#30340;&#32447;&#24615;&#20998;&#35299;&#26041;&#27861;&#22312;&#26426;&#22120;&#32763;&#35793;&#35299;&#30721;&#22120;&#20013;&#30340;&#24212;&#29992;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#34429;&#28982;&#20998;&#35299;&#25351;&#26631;&#19982;&#27169;&#22411;&#34920;&#29616;&#26377;&#25928;&#30456;&#20851;&#65292;&#20294;&#19981;&#21516;&#36816;&#34892;&#20043;&#38388;&#30340;&#21464;&#24322;&#24615;&#34920;&#26126;&#23545;&#20110;&#25968;&#23398;&#37325;&#36848;&#26159;&#21542;&#22312;&#23454;&#35777;&#19978;&#20855;&#26377;&#24847;&#20041;&#36824;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2310.06977</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#22312;&#20960;&#20309;&#23398;&#20013;&#35201;&#36153;&#24515;&#65311;&#20851;&#20110;Transformer&#23884;&#20837;&#30340;&#32447;&#24615;&#20998;&#35299;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Why bother with geometry? On the relevance of linear decompositions of Transformer embeddings. (arXiv:2310.06977v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06977
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;Transformer&#23884;&#20837;&#30340;&#32447;&#24615;&#20998;&#35299;&#26041;&#27861;&#22312;&#26426;&#22120;&#32763;&#35793;&#35299;&#30721;&#22120;&#20013;&#30340;&#24212;&#29992;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#34429;&#28982;&#20998;&#35299;&#25351;&#26631;&#19982;&#27169;&#22411;&#34920;&#29616;&#26377;&#25928;&#30456;&#20851;&#65292;&#20294;&#19981;&#21516;&#36816;&#34892;&#20043;&#38388;&#30340;&#21464;&#24322;&#24615;&#34920;&#26126;&#23545;&#20110;&#25968;&#23398;&#37325;&#36848;&#26159;&#21542;&#22312;&#23454;&#35777;&#19978;&#20855;&#26377;&#24847;&#20041;&#36824;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#34920;&#26126;&#65292;Transformer&#23884;&#20837;&#21487;&#20197;&#32447;&#24615;&#20998;&#35299;&#20026;&#26126;&#30830;&#23450;&#20041;&#30340;&#22240;&#23376;&#20043;&#21644;&#65292;&#36825;&#20123;&#22240;&#23376;&#21487;&#20197;&#19982;&#29305;&#23450;&#30340;&#32593;&#32476;&#36755;&#20837;&#25110;&#32452;&#20214;&#30456;&#20851;&#32852;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#32570;&#20047;&#30740;&#31350;&#36825;&#20123;&#25968;&#23398;&#37325;&#36848;&#26159;&#21542;&#22312;&#23454;&#35777;&#19978;&#20855;&#26377;&#24847;&#20041;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20004;&#31181;&#23884;&#20837;&#20998;&#35299;&#26041;&#27861;&#30740;&#31350;&#20102;&#26426;&#22120;&#32763;&#35793;&#35299;&#30721;&#22120;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#20998;&#35299;&#23548;&#20986;&#30340;&#25351;&#26631;&#19982;&#27169;&#22411;&#24615;&#33021;&#26377;&#25928;&#30456;&#20851;&#65292;&#20294;&#19981;&#21516;&#36816;&#34892;&#20043;&#38388;&#30340;&#21464;&#24322;&#24615;&#34920;&#26126;&#23545;&#36825;&#20010;&#38382;&#39064;&#38656;&#35201;&#26356;&#21152;&#32454;&#33268;&#30340;&#32771;&#34385;&#12290;&#25105;&#20204;&#27979;&#37327;&#30340;&#39640;&#24230;&#21464;&#24322;&#24615;&#34920;&#26126;&#65292;&#20960;&#20309;&#21453;&#26144;&#30340;&#26159;&#27169;&#22411;&#29305;&#23450;&#30340;&#29305;&#24449;&#65292;&#32780;&#19981;&#26159;&#29305;&#23450;&#20110;&#21477;&#23376;&#30340;&#35745;&#31639;&#65292;&#32780;&#30456;&#20284;&#30340;&#35757;&#32451;&#26465;&#20214;&#24182;&#19981;&#33021;&#20445;&#35777;&#30456;&#20284;&#30340;&#21521;&#37327;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
A recent body of work has demonstrated that Transformer embeddings can be linearly decomposed into well-defined sums of factors, that can in turn be related to specific network inputs or components. There is however still a dearth of work studying whether these mathematical reformulations are empirically meaningful. In the present work, we study representations from machine-translation decoders using two of such embedding decomposition methods. Our results indicate that, while decomposition-derived indicators effectively correlate with model performance, variation across different runs suggests a more nuanced take on this question. The high variability of our measurements indicate that geometry reflects model-specific characteristics more than it does sentence-specific computations, and that similar training conditions do not guarantee similar vector spaces.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25991;&#26723;&#32423;&#30417;&#30563;&#36827;&#34892;&#22810;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#26080;&#38656;&#32454;&#31890;&#24230;&#26631;&#31614;&#65292;&#21487;&#20197;&#26816;&#27979;&#25991;&#26723;&#20013;&#30340;&#22810;&#20010;&#26041;&#38754;&#65292;&#24182;&#33021;&#29702;&#35299;&#36890;&#36807;&#22810;&#20010;&#26041;&#38754;&#34920;&#36798;&#30340;&#24773;&#24863;&#12290;</title><link>http://arxiv.org/abs/2310.06940</link><description>&lt;p&gt;
&#26080;&#38656;&#32454;&#31890;&#24230;&#26631;&#31614;&#30340;&#22810;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#30340;&#25991;&#26723;&#32423;&#30417;&#30563;
&lt;/p&gt;
&lt;p&gt;
Document-Level Supervision for Multi-Aspect Sentiment Analysis Without Fine-grained Labels. (arXiv:2310.06940v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25991;&#26723;&#32423;&#30417;&#30563;&#36827;&#34892;&#22810;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#26080;&#38656;&#32454;&#31890;&#24230;&#26631;&#31614;&#65292;&#21487;&#20197;&#26816;&#27979;&#25991;&#26723;&#20013;&#30340;&#22810;&#20010;&#26041;&#38754;&#65292;&#24182;&#33021;&#29702;&#35299;&#36890;&#36807;&#22810;&#20010;&#26041;&#38754;&#34920;&#36798;&#30340;&#24773;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#65288;ABSA&#65289;&#26159;&#19968;&#20010;&#24191;&#27867;&#30740;&#31350;&#30340;&#20027;&#39064;&#65292;&#36890;&#24120;&#36890;&#36807;&#20154;&#24037;&#26631;&#27880;&#30340;&#24847;&#35265;&#25991;&#26412;&#36827;&#34892;&#30417;&#30563;&#35757;&#32451;&#12290;&#36825;&#20123;&#32454;&#31890;&#24230;&#30340;&#26631;&#27880;&#21253;&#25324;&#35782;&#21035;&#29992;&#25143;&#34920;&#36798;&#24773;&#24863;&#30340;&#26041;&#38754;&#20197;&#21450;&#23427;&#20204;&#30340;&#26497;&#24615;&#65288;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32454;&#31890;&#24230;&#30340;&#26631;&#27880;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#29615;&#22659;&#20013;&#24448;&#24448;&#26114;&#36149;&#19988;&#38590;&#20197;&#23454;&#29616;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#29992;&#25143;&#29983;&#25104;&#30340;&#25991;&#26412;&#21253;&#21547;&#20102;&#25972;&#20307;&#24773;&#24863;&#65292;&#20363;&#22914;&#29992;&#25143;&#35780;&#35770;&#25110;&#29992;&#25143;&#21453;&#39304;&#20013;&#30340;1-5&#35780;&#20998;&#65292;&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#26469;&#36827;&#34892;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;VAE&#20027;&#39064;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#25991;&#26723;&#32423;&#30417;&#30563;&#36827;&#34892;ABSA&#65292;&#26080;&#38656;&#23545;&#26041;&#38754;&#25110;&#24773;&#24863;&#36827;&#34892;&#32454;&#31890;&#24230;&#26631;&#31614;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#22312;&#19968;&#20010;&#25991;&#26723;&#20013;&#26816;&#27979;&#22810;&#20010;&#26041;&#38754;&#65292;&#20174;&#32780;&#21487;&#20197;&#29702;&#35299;&#36890;&#36807;&#22810;&#20010;&#26041;&#38754;&#34920;&#36798;&#30340;&#24773;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aspect-based sentiment analysis (ABSA) is a widely studied topic, most often trained through supervision from human annotations of opinionated texts. These fine-grained annotations include identifying aspects towards which a user expresses their sentiment, and their associated polarities (aspect-based sentiments). Such fine-grained annotations can be expensive and often infeasible to obtain in real-world settings. There is, however, an abundance of scenarios where user-generated text contains an overall sentiment, such as a rating of 1-5 in user reviews or user-generated feedback, which may be leveraged for this task. In this paper, we propose a VAE-based topic modeling approach that performs ABSA using document-level supervision and without requiring fine-grained labels for either aspects or sentiments. Our approach allows for the detection of multiple aspects in a document, thereby allowing for the possibility of reasoning about how sentiment expressed through multiple aspects comes 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20934;&#30830;&#31232;&#30095;&#24494;&#35843;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;L2&#33539;&#25968;&#30340;&#33976;&#39311;&#26041;&#27861;SquareHead&#65292;&#21487;&#20197;&#22312;&#39640;&#31232;&#30095;&#24615;&#19979;&#23454;&#29616;&#20934;&#30830;&#30340;&#24674;&#22797;&#65307;&#21516;&#26102;&#23637;&#31034;&#20102;&#31232;&#30095;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#38469;&#25928;&#29575;&#65292;&#21487;&#22312;CPU&#21644;GPU&#36816;&#34892;&#26102;&#23454;&#29616;&#21152;&#36895;&#65292;&#24182;&#19988;&#35266;&#23519;&#21040;&#22312;&#21463;&#20869;&#23384;&#38480;&#21046;&#30340;&#27169;&#22411;&#20013;&#65292;&#31232;&#30095;&#24615;&#20063;&#21487;&#29992;&#20110;&#20943;&#23569;&#20869;&#23384;&#24102;&#23485;&#12290;</title><link>http://arxiv.org/abs/2310.06927</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31232;&#30095;&#24494;&#35843;&#30340;&#25512;&#29702;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
Sparse Finetuning for Inference Acceleration of Large Language Models. (arXiv:2310.06927v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06927
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20934;&#30830;&#31232;&#30095;&#24494;&#35843;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;L2&#33539;&#25968;&#30340;&#33976;&#39311;&#26041;&#27861;SquareHead&#65292;&#21487;&#20197;&#22312;&#39640;&#31232;&#30095;&#24615;&#19979;&#23454;&#29616;&#20934;&#30830;&#30340;&#24674;&#22797;&#65307;&#21516;&#26102;&#23637;&#31034;&#20102;&#31232;&#30095;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#38469;&#25928;&#29575;&#65292;&#21487;&#22312;CPU&#21644;GPU&#36816;&#34892;&#26102;&#23454;&#29616;&#21152;&#36895;&#65292;&#24182;&#19988;&#35266;&#23519;&#21040;&#22312;&#21463;&#20869;&#23384;&#38480;&#21046;&#30340;&#27169;&#22411;&#20013;&#65292;&#31232;&#30095;&#24615;&#20063;&#21487;&#29992;&#20110;&#20943;&#23569;&#20869;&#23384;&#24102;&#23485;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#35757;&#32451;&#36807;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#31934;&#30830;&#30340;&#31232;&#30095;&#24494;&#35843;&#65292;&#21363;&#22312;&#19987;&#38376;&#20219;&#21153;&#19978;&#23545;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#21516;&#26102;&#22312;&#26435;&#37325;&#19978;&#24341;&#20837;&#31232;&#30095;&#24615;&#12290;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22522;&#20110;&#25439;&#22833;&#30340;&#26631;&#20934;&#24494;&#35843;&#21487;&#33021;&#26080;&#27861;&#24674;&#22797;&#20934;&#30830;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#31232;&#30095;&#24773;&#20917;&#19979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23545;&#33976;&#39311;&#31867;&#22411;&#30340;&#25439;&#22833;&#36827;&#34892;&#20102;&#35814;&#32454;&#30740;&#31350;&#65292;&#30830;&#23450;&#20102;&#19968;&#31181;&#22522;&#20110;L2&#33539;&#25968;&#30340;&#33976;&#39311;&#26041;&#27861;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;SquareHead&#65292;&#21363;&#20351;&#22312;&#26356;&#39640;&#30340;&#31232;&#30095;&#24615;&#19979;&#65292;&#23427;&#20063;&#33021;&#23454;&#29616;&#20934;&#30830;&#30340;&#24674;&#22797;&#65292;&#36866;&#29992;&#20110;&#25152;&#26377;&#27169;&#22411;&#31867;&#22411;&#12290;&#22312;&#23454;&#38469;&#25928;&#29575;&#26041;&#38754;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31232;&#30095;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#31232;&#30095;&#24615;&#22312;CPU&#21644;GPU&#36816;&#34892;&#26102;&#23454;&#29616;&#21152;&#36895;&#12290;&#34429;&#28982;&#26631;&#20934;&#26041;&#27861;&#26159;&#21033;&#29992;&#31232;&#30095;&#24615;&#36827;&#34892;&#35745;&#31639;&#20943;&#23569;&#65292;&#20294;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#21463;&#20869;&#23384;&#38480;&#21046;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#31232;&#30095;&#24615;&#20063;&#21487;&#20197;&#29992;&#20110;&#20943;&#23569;&#20869;&#23384;&#24102;&#23485;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#30001;&#20110;&#31232;&#30095;&#24615;&#23548;&#33268;&#30340;&#36895;&#24230;&#25552;&#21319;&#20197;&#21450;&#24674;&#22797;&#20934;&#30830;&#24615;&#30340;&#31471;&#21040;&#31471;&#32467;&#26524;&#65292;&#24212;&#29992;&#20110;T5 (&#35821;&#35328;&#32763;&#35793;)&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of accurate sparse finetuning of large language models (LLMs), that is, finetuning pretrained LLMs on specialized tasks, while inducing sparsity in their weights. On the accuracy side, we observe that standard loss-based finetuning may fail to recover accuracy, especially at high sparsities. To address this, we perform a detailed study of distillation-type losses, determining an L2-based distillation approach we term SquareHead which enables accurate recovery even at higher sparsities, across all model types. On the practical efficiency side, we show that sparse LLMs can be executed with speedups by taking advantage of sparsity, for both CPU and GPU runtimes. While the standard approach is to leverage sparsity for computational reduction, we observe that in the case of memory-bound LLMs sparsity can also be leveraged for reducing memory bandwidth. We exhibit end-to-end results showing speedups due to sparsity, while recovering accuracy, on T5 (language translati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32858;&#28966;-&#20449;&#24687;&#29109;&#20989;&#25968;&#65292;&#21033;&#29992;&#30828;&#36127;&#26679;&#26412;&#25366;&#25496;&#25913;&#36827;&#20102;&#23545;&#27604;&#23398;&#20064;&#21477;&#23376;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#20854;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.06918</link><description>&lt;p&gt;
&#29992;&#32858;&#28966;-&#20449;&#24687;&#29109;&#25913;&#36827;&#23545;&#27604;&#23398;&#20064;&#21477;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Improving Contrastive Learning of Sentence Embeddings with Focal-InfoNCE. (arXiv:2310.06918v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32858;&#28966;-&#20449;&#24687;&#29109;&#20989;&#25968;&#65292;&#21033;&#29992;&#30828;&#36127;&#26679;&#26412;&#25366;&#25496;&#25913;&#36827;&#20102;&#23545;&#27604;&#23398;&#20064;&#21477;&#23376;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#20854;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
SimCSE&#30340;&#26368;&#26032;&#25104;&#21151;&#26497;&#22823;&#22320;&#25512;&#36827;&#20102;&#21477;&#23376;&#34920;&#31034;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;SimCSE&#30340;&#21407;&#22987;&#34920;&#36798;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#20013;&#30828;&#36127;&#26679;&#26412;&#30340;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;SimCSE&#19982;&#30828;&#36127;&#26679;&#26412;&#25366;&#25496;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#25552;&#39640;&#21477;&#23376;&#23884;&#20837;&#30340;&#36136;&#37327;&#12290;&#25152;&#25552;&#20986;&#30340;&#32858;&#28966;-&#20449;&#24687;&#29109;&#20989;&#25968;&#22312;&#23545;&#27604;&#30446;&#26631;&#20013;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#35843;&#33410;&#39033;&#65292;&#38477;&#20302;&#19982;&#26131;&#36127;&#26679;&#26412;&#30456;&#20851;&#30340;&#25439;&#22833;&#65292;&#24182;&#40723;&#21169;&#27169;&#22411;&#20851;&#27880;&#20110;&#22256;&#38590;&#36127;&#26679;&#26412;&#12290;&#22312;&#21508;&#31181;STS&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26031;&#30382;&#23572;&#26364;&#30456;&#20851;&#31995;&#25968;&#12289;&#34920;&#31034;&#23545;&#40784;&#24615;&#21644;&#19968;&#33268;&#24615;&#26041;&#38754;&#25913;&#36827;&#20102;&#21477;&#23376;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent success of SimCSE has greatly advanced state-of-the-art sentence representations. However, the original formulation of SimCSE does not fully exploit the potential of hard negative samples in contrastive learning. This study introduces an unsupervised contrastive learning framework that combines SimCSE with hard negative mining, aiming to enhance the quality of sentence embeddings. The proposed focal-InfoNCE function introduces self-paced modulation terms in the contrastive objective, downweighting the loss associated with easy negatives and encouraging the model focusing on hard negatives. Experimentation on various STS benchmarks shows that our method improves sentence embeddings in terms of Spearman's correlation and representation alignment and uniformity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#25991;&#26412;&#34920;&#31034;&#25216;&#26415;&#22312;&#32570;&#38519;&#20998;&#37197;&#20013;&#30340;&#24212;&#29992;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#26032;&#25216;&#26415;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#24494;&#22937;&#30340;&#25991;&#26412;&#27169;&#24335;&#65292;&#25552;&#39640;&#33258;&#21160;&#21270;&#32570;&#38519;&#20998;&#37197;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.06913</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#25991;&#26412;&#34920;&#31034;&#25216;&#26415;&#22312;&#32570;&#38519;&#20998;&#37197;&#20013;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comparative Study of Transformer-based Neural Text Representation Techniques on Bug Triaging. (arXiv:2310.06913v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#25991;&#26412;&#34920;&#31034;&#25216;&#26415;&#22312;&#32570;&#38519;&#20998;&#37197;&#20013;&#30340;&#24212;&#29992;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#26032;&#25216;&#26415;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#24494;&#22937;&#30340;&#25991;&#26412;&#27169;&#24335;&#65292;&#25552;&#39640;&#33258;&#21160;&#21270;&#32570;&#38519;&#20998;&#37197;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31649;&#29702;&#32570;&#38519;&#25253;&#21578;&#26102;&#65292;&#36890;&#24120;&#31532;&#19968;&#27493;&#26159;&#23558;&#32570;&#38519;&#20998;&#37197;&#32473;&#26368;&#36866;&#21512;&#29702;&#35299;&#12289;&#23450;&#20301;&#21644;&#20462;&#22797;&#30446;&#26631;&#32570;&#38519;&#30340;&#24320;&#21457;&#20154;&#21592;&#12290;&#27492;&#22806;&#65292;&#23558;&#32473;&#23450;&#30340;&#32570;&#38519;&#20998;&#37197;&#32473;&#36719;&#20214;&#39033;&#30446;&#30340;&#29305;&#23450;&#37096;&#20998;&#21487;&#20197;&#21152;&#24555;&#20462;&#22797;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#36825;&#20123;&#27963;&#21160;&#30340;&#37325;&#35201;&#24615;&#65292;&#20294;&#22312;&#25163;&#21160;&#20998;&#37197;&#30340;&#36807;&#31243;&#20013;&#21487;&#33021;&#38656;&#35201;&#33457;&#36153;&#20960;&#22825;&#30340;&#26102;&#38388;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#23581;&#35797;&#21033;&#29992;&#26377;&#38480;&#30340;&#25991;&#26412;&#25968;&#25454;&#35757;&#32451;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#26469;&#33258;&#21160;&#21270;&#36825;&#20010;&#36807;&#31243;&#65292;&#20294;&#24471;&#21040;&#30340;&#25104;&#21151;&#31243;&#24230;&#21442;&#24046;&#19981;&#40784;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#24037;&#20316;&#20013;&#20351;&#29992;&#30340;&#25991;&#26412;&#34920;&#31034;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21463;&#21040;&#20854;&#34920;&#36798;&#33021;&#21147;&#30340;&#38480;&#21046;&#65292;&#24448;&#24448;&#26080;&#27861;&#25429;&#25417;&#21040;&#21487;&#20197;&#24110;&#21161;&#32570;&#38519;&#20998;&#37197;&#36807;&#31243;&#30340;&#24494;&#22937;&#25991;&#26412;&#27169;&#24335;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#39044;&#35757;&#32451;&#31070;&#32463;&#25991;&#26412;&#34920;&#31034;&#25216;&#26415;&#65288;&#22914;BERT&#65289;&#22312;&#20960;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Often, the first step in managing bug reports is related to triaging a bug to the appropriate developer who is best suited to understand, localize, and fix the target bug. Additionally, assigning a given bug to a particular part of a software project can help to expedite the fixing process. However, despite the importance of these activities, they are quite challenging, where days can be spent on the manual triaging process. Past studies have attempted to leverage the limited textual data of bug reports to train text classification models that automate this process -- to varying degrees of success. However, the textual representations and machine learning models used in prior work are limited by their expressiveness, often failing to capture nuanced textual patterns that might otherwise aid in the triaging process. Recently, large, transformer-based, pre-trained neural text representation techniques such as BERT have achieved greater performance in several natural language processing t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35748;&#30693;&#26234;&#33021;&#20307;&#30340;&#20219;&#21153;&#30693;&#35782;&#26469;&#28304;&#65292;&#25506;&#32034;&#20102;&#23558;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#22806;&#37096;&#30693;&#35782;&#28304;&#29992;&#20110;&#35748;&#30693;&#31995;&#32479;&#30340;&#25361;&#25112;&#21644;&#26426;&#20250;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#36807;&#25972;&#21512;&#30693;&#35782;&#25552;&#21462;&#21644;&#35748;&#30693;&#26550;&#26500;&#33021;&#21147;&#26469;&#25552;&#39640;&#30693;&#35782;&#25552;&#21462;&#25928;&#26524;&#30340;&#21487;&#33021;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.06846</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35748;&#30693;&#26234;&#33021;&#20307;&#30340;&#30693;&#35782;&#26469;&#28304;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploiting Language Models as a Source of Knowledge for Cognitive Agents. (arXiv:2310.06846v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35748;&#30693;&#26234;&#33021;&#20307;&#30340;&#20219;&#21153;&#30693;&#35782;&#26469;&#28304;&#65292;&#25506;&#32034;&#20102;&#23558;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#22806;&#37096;&#30693;&#35782;&#28304;&#29992;&#20110;&#35748;&#30693;&#31995;&#32479;&#30340;&#25361;&#25112;&#21644;&#26426;&#20250;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#36807;&#25972;&#21512;&#30693;&#35782;&#25552;&#21462;&#21644;&#35748;&#30693;&#26550;&#26500;&#33021;&#21147;&#26469;&#25552;&#39640;&#30693;&#35782;&#25552;&#21462;&#25928;&#26524;&#30340;&#21487;&#33021;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#19981;&#20165;&#21487;&#20197;&#23436;&#25104;&#21477;&#23376;&#34917;&#20840;&#65292;&#36824;&#21487;&#20197;&#36827;&#34892;&#38382;&#31572;&#12289;&#25688;&#35201;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#31561;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35748;&#30693;&#26234;&#33021;&#20307;&#30340;&#20219;&#21153;&#30693;&#35782;&#26469;&#28304;&#65292;&#21363;&#36890;&#36807;&#35748;&#30693;&#26550;&#26500;&#23454;&#29616;&#30340;&#26234;&#33021;&#20307;&#12290;&#25105;&#20204;&#35782;&#21035;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35748;&#30693;&#31995;&#32479;&#22806;&#37096;&#30693;&#35782;&#28304;&#30340;&#25361;&#25112;&#21644;&#26426;&#20250;&#65292;&#20197;&#21450;&#36890;&#36807;&#23558;&#30693;&#35782;&#25552;&#21462;&#19982;&#35748;&#30693;&#26550;&#26500;&#33021;&#21147;&#25972;&#21512;&#26469;&#25552;&#39640;&#30693;&#35782;&#25552;&#21462;&#25928;&#26524;&#30340;&#21487;&#33021;&#26041;&#27861;&#65292;&#24182;&#20030;&#20363;&#20171;&#32461;&#25105;&#20204;&#26368;&#36817;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) provide capabilities far beyond sentence completion, including question answering, summarization, and natural-language inference. While many of these capabilities have potential application to cognitive systems, our research is exploiting language models as a source of task knowledge for cognitive agents, that is, agents realized via a cognitive architecture. We identify challenges and opportunities for using language models as an external knowledge source for cognitive systems and possible ways to improve the effectiveness of knowledge extraction by integrating extraction with cognitive architecture capabilities, highlighting with examples from our recent work in this area.
&lt;/p&gt;</description></item><item><title>Meta-CoT&#26159;&#19968;&#31181;&#22312;&#28151;&#21512;&#20219;&#21153;&#22330;&#26223;&#20013;&#33021;&#22815;&#36890;&#29992;&#24605;&#32500;&#38142;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#22312;&#21313;&#20010;&#20844;&#20849;&#22522;&#20934;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#21644;&#20248;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.06692</link><description>&lt;p&gt;
Meta-CoT:&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#28151;&#21512;&#20219;&#21153;&#22330;&#26223;&#20013;&#30340;&#36890;&#29992;&#24605;&#32500;&#38142;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Meta-CoT: Generalizable Chain-of-Thought Prompting in Mixed-task Scenarios with Large Language Models. (arXiv:2310.06692v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06692
&lt;/p&gt;
&lt;p&gt;
Meta-CoT&#26159;&#19968;&#31181;&#22312;&#28151;&#21512;&#20219;&#21153;&#22330;&#26223;&#20013;&#33021;&#22815;&#36890;&#29992;&#24605;&#32500;&#38142;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#22312;&#21313;&#20010;&#20844;&#20849;&#22522;&#20934;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#21644;&#20248;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#21033;&#29992;&#38142;&#24335;&#24605;&#32500;&#25552;&#31034;&#23637;&#31034;&#20986;&#20102;&#21331;&#36234;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#36825;&#31181;&#25552;&#31034;&#29983;&#25104;&#20013;&#38388;&#25512;&#29702;&#38142;&#20197;&#20316;&#20026;&#24471;&#20986;&#31572;&#26696;&#30340;&#22522;&#26412;&#29702;&#30001;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;CoT&#26041;&#27861;&#35201;&#20040;&#20165;&#20165;&#20351;&#29992;&#31867;&#20284;&#8220;&#35753;&#25105;&#20204;&#36880;&#27493;&#24605;&#32771;&#8221;&#30340;&#36890;&#29992;&#25552;&#31034;&#65292;&#35201;&#20040;&#36807;&#20110;&#20381;&#36182;&#25163;&#24037;&#35774;&#35745;&#30340;&#20219;&#21153;&#29305;&#23450;&#28436;&#31034;&#26469;&#36798;&#21040;&#29702;&#24819;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#23548;&#33268;&#24615;&#33021;&#21644;&#27867;&#21270;&#20043;&#38388;&#30340;&#19981;&#21487;&#36991;&#20813;&#30340;&#40511;&#27807;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#40511;&#27807;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Meta-CoT&#65292;&#19968;&#31181;&#22312;&#26410;&#30693;&#36755;&#20837;&#38382;&#39064;&#31867;&#22411;&#30340;&#28151;&#21512;&#20219;&#21153;&#22330;&#26223;&#20013;&#20855;&#26377;&#36890;&#29992;&#24615;&#30340;CoT&#25552;&#31034;&#26041;&#27861;&#12290;Meta-CoT&#39318;&#20808;&#26681;&#25454;&#36755;&#20837;&#38382;&#39064;&#23545;&#22330;&#26223;&#36827;&#34892;&#20998;&#31867;&#65292;&#28982;&#21518;&#20197;&#33258;&#21160;&#27169;&#24335;&#20174;&#30456;&#24212;&#30340;&#25968;&#25454;&#27744;&#20013;&#26500;&#24314;&#22810;&#26679;&#30340;&#28436;&#31034;&#12290;Meta-CoT&#22312;&#21313;&#20010;&#20844;&#20849;&#22522;&#20934;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#21644;&#20248;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;Meta-CoT&#23454;&#29616;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have unveiled remarkable reasoning capabilities by exploiting chain-of-thought (CoT) prompting, which generates intermediate reasoning chains to serve as the rationale for deriving the answer. However, current CoT methods either simply employ general prompts such as Let's think step by step, or heavily rely on handcrafted task-specific demonstrations to attain preferable performances, thereby engendering an inescapable gap between performance and generalization. To bridge this gap, we propose Meta-CoT, a generalizable CoT prompting method in mixed-task scenarios where the type of input questions is unknown. Meta-CoT firstly categorizes the scenario based on the input question and subsequently constructs diverse demonstrations from the corresponding data pool in an automatic pattern. Meta-CoT simultaneously enjoys remarkable performances on ten public benchmark reasoning tasks and superior generalization capabilities. Notably, Meta-CoT achieves the state-of-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#24314;&#26500;&#24615;&#21644;&#22810;&#26679;&#21270;&#21453;&#39304;&#65288;CDF&#65289;&#65292;&#29992;&#20110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23545;&#40784;&#25928;&#26524;&#12290;&#25105;&#20204;&#36890;&#36807;&#25910;&#38598;&#19981;&#21516;&#31867;&#22411;&#30340;&#21453;&#39304;&#65292;&#24182;&#26681;&#25454;&#38382;&#39064;&#30340;&#38590;&#24230;&#32423;&#21035;&#36827;&#34892;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.06450</link><description>&lt;p&gt;
&#29992;&#22810;&#26679;&#21270;&#21453;&#39304;&#26500;&#24314;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Constructive Large Language Models Alignment with Diverse Feedback. (arXiv:2310.06450v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#24314;&#26500;&#24615;&#21644;&#22810;&#26679;&#21270;&#21453;&#39304;&#65288;CDF&#65289;&#65292;&#29992;&#20110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23545;&#40784;&#25928;&#26524;&#12290;&#25105;&#20204;&#36890;&#36807;&#25910;&#38598;&#19981;&#21516;&#31867;&#22411;&#30340;&#21453;&#39304;&#65292;&#24182;&#26681;&#25454;&#38382;&#39064;&#30340;&#38590;&#24230;&#32423;&#21035;&#36827;&#34892;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30740;&#31350;&#20013;&#65292;&#23545;&#20854;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#23545;&#40784;&#36234;&#26469;&#36234;&#37325;&#35270;&#65292;&#20197;&#20943;&#23569;&#26377;&#23475;&#20869;&#23481;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#23545;&#40784;&#26041;&#27861;&#36890;&#24120;&#20165;&#20381;&#36182;&#20110;&#20154;&#31867;&#21453;&#39304;&#30340;&#21333;&#19968;&#24418;&#24335;&#65292;&#22914;&#20559;&#22909;&#12289;&#27880;&#37322;&#26631;&#31614;&#25110;&#33258;&#28982;&#35821;&#35328;&#25209;&#35780;&#65292;&#24573;&#35270;&#20102;&#32467;&#21512;&#36825;&#20123;&#21453;&#39304;&#31867;&#22411;&#30340;&#28508;&#22312;&#20248;&#21183;&#12290;&#36825;&#31181;&#38480;&#21046;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#65292;&#21363;&#20351;&#26377;&#20016;&#23500;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#24314;&#26500;&#24615;&#21644;&#22810;&#26679;&#21270;&#21453;&#39304;&#65288;CDF&#65289;&#20316;&#20026;&#22686;&#24378;LLM&#23545;&#40784;&#30340;&#26032;&#26041;&#27861;&#65292;&#21463;&#24314;&#26500;&#23398;&#20064;&#29702;&#35770;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#25910;&#38598;&#36866;&#29992;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#19981;&#21516;&#38590;&#24230;&#38382;&#39064;&#30340;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#21453;&#39304;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#25209;&#35780;&#21453;&#39304;&#35299;&#20915;&#31616;&#21333;&#38382;&#39064;&#65292;&#21033;&#29992;&#25913;&#36827;&#21453;&#39304;&#35299;&#20915;&#20013;&#31561;&#38382;&#39064;&#65292;&#21033;&#29992;&#20559;&#22909;&#21453;&#39304;&#35299;&#20915;&#22256;&#38590;&#38382;&#39064;&#12290;&#36890;&#36807;&#29992;&#36825;&#31181;&#22810;&#26679;&#21270;&#21453;&#39304;&#35757;&#32451;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent research on large language models (LLMs), there has been a growing emphasis on aligning these models with human values to reduce the impact of harmful content. However, current alignment methods often rely solely on singular forms of human feedback, such as preferences, annotated labels, or natural language critiques, overlooking the potential advantages of combining these feedback types. This limitation leads to suboptimal performance, even when ample training data is available. In this paper, we introduce Constructive and Diverse Feedback (CDF) as a novel method to enhance LLM alignment, inspired by constructivist learning theory. Our approach involves collecting three distinct types of feedback tailored to problems of varying difficulty levels within the training dataset. Specifically, we exploit critique feedback for easy problems, refinement feedback for medium problems, and preference feedback for hard problems. By training our model with this diversified feedback, we a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#33258;&#21160;&#31070;&#32463;&#20803;&#35299;&#37322;&#20013;&#21363;&#26102;&#35843;&#20248;&#30340;&#37325;&#35201;&#24615;&#65292;&#36890;&#36807;&#37325;&#26032;&#26684;&#24335;&#21270;&#35299;&#37322;&#25552;&#31034;&#65292;&#25105;&#20204;&#26174;&#33879;&#25552;&#39640;&#20102;&#35299;&#37322;&#36136;&#37327;&#24182;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#20110;&#26356;&#28145;&#20837;&#29702;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20316;&#21407;&#29702;&#21644;&#23433;&#20840;&#24615;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.06200</link><description>&lt;p&gt;
&#33258;&#21160;&#31070;&#32463;&#20803;&#35299;&#37322;&#30340;&#21450;&#26102;&#35843;&#20248;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Importance of Prompt Tuning for Automated Neuron Explanations. (arXiv:2310.06200v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#33258;&#21160;&#31070;&#32463;&#20803;&#35299;&#37322;&#20013;&#21363;&#26102;&#35843;&#20248;&#30340;&#37325;&#35201;&#24615;&#65292;&#36890;&#36807;&#37325;&#26032;&#26684;&#24335;&#21270;&#35299;&#37322;&#25552;&#31034;&#65292;&#25105;&#20204;&#26174;&#33879;&#25552;&#39640;&#20102;&#35299;&#37322;&#36136;&#37327;&#24182;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#20110;&#26356;&#28145;&#20837;&#29702;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20316;&#21407;&#29702;&#21644;&#23433;&#20840;&#24615;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#36827;&#23637;&#26497;&#22823;&#22320;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#33021;&#21147;&#65292;&#20294;&#25105;&#20204;&#23545;&#36825;&#20123;&#27169;&#22411;&#21450;&#20854;&#23433;&#20840;&#24615;&#30340;&#29702;&#35299;&#24182;&#27809;&#26377;&#21516;&#27493;&#36827;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#30740;&#31350;&#23427;&#20204;&#30340;&#20010;&#20307;&#31070;&#32463;&#20803;&#26469;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;LLM&#12290;&#25105;&#20204;&#22312;&#21069;&#20154;&#30740;&#31350;&#30340;&#22522;&#30784;&#19978;&#65292;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;GPT-4&#65292;&#22914;&#20309;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#20013;&#27599;&#20010;&#31070;&#32463;&#20803;&#30340;&#21151;&#33021;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#29983;&#25104;&#35299;&#37322;&#25152;&#20351;&#29992;&#30340;&#25552;&#31034;&#30340;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#20197;&#26356;&#33258;&#28982;&#30340;&#26041;&#24335;&#37325;&#26032;&#26684;&#24335;&#21270;&#35299;&#37322;&#25552;&#31034;&#22914;&#20309;&#26174;&#33879;&#25552;&#39640;&#31070;&#32463;&#20803;&#35299;&#37322;&#30340;&#36136;&#37327;&#65292;&#24182;&#22823;&#24133;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#36890;&#36807;&#19977;&#31181;&#19981;&#21516;&#30340;&#26041;&#24335;&#28436;&#31034;&#20102;&#25105;&#20204;&#26032;&#25552;&#31034;&#30340;&#25928;&#26524;&#65292;&#21253;&#25324;&#33258;&#21160;&#21270;&#35780;&#20272;&#21644;&#20154;&#24037;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances have greatly increased the capabilities of large language models (LLMs), but our understanding of the models and their safety has not progressed as fast. In this paper we aim to understand LLMs deeper by studying their individual neurons. We build upon previous work showing large language models such as GPT-4 can be useful in explaining what each neuron in a language model does. Specifically, we analyze the effect of the prompt used to generate explanations and show that reformatting the explanation prompt in a more natural way can significantly improve neuron explanation quality and greatly reduce computational cost. We demonstrate the effects of our new prompts in three different ways, incorporating both automated and human evaluations.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35299;&#37322;&#20854;&#20182;&#39044;&#27979;&#27169;&#22411;&#26377;&#25928;&#24615;&#30340;&#26694;&#26550;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#22810;&#20010;&#25552;&#31034;&#31574;&#30053;&#65292;&#22635;&#34917;&#20102;&#24403;&#21069;&#23545;&#20110;LLMs&#22312;&#35299;&#37322;&#20854;&#20182;&#27169;&#22411;&#34892;&#20026;&#26041;&#38754;&#30340;&#32570;&#22833;&#12290;</title><link>http://arxiv.org/abs/2310.05797</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#20107;&#21518;&#35299;&#37322;&#22120;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Large Language Models Post Hoc Explainers?. (arXiv:2310.05797v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05797
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35299;&#37322;&#20854;&#20182;&#39044;&#27979;&#27169;&#22411;&#26377;&#25928;&#24615;&#30340;&#26694;&#26550;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#22810;&#20010;&#25552;&#31034;&#31574;&#30053;&#65292;&#22635;&#34917;&#20102;&#24403;&#21069;&#23545;&#20110;LLMs&#22312;&#35299;&#37322;&#20854;&#20182;&#27169;&#22411;&#34892;&#20026;&#26041;&#38754;&#30340;&#32570;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36234;&#26469;&#36234;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24212;&#29992;&#20013;&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#21019;&#26032;&#65292;&#21363;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#20351;&#24471;LLM&#33021;&#22815;&#22312;&#25512;&#29702;&#38454;&#27573;&#36890;&#36807;&#22312;&#25552;&#31034;&#20013;&#25552;&#20379;&#23569;&#37327;&#31034;&#20363;&#26469;&#23398;&#20064;&#26032;&#20219;&#21153;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#27169;&#22411;&#24494;&#35843;&#30340;&#38656;&#35201;&#12290;&#34429;&#28982;LLM&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#22810;&#20010;&#39046;&#22495;&#65292;&#20294;&#20854;&#22312;&#35299;&#37322;&#20854;&#20182;&#27169;&#22411;&#34892;&#20026;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#20173;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#12290;&#23613;&#31649;&#23384;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#26032;&#35299;&#37322;&#25216;&#26415;&#65292;&#20294;&#24456;&#22810;&#25216;&#26415;&#35201;&#27714;&#23545;&#27169;&#22411;&#20855;&#26377;&#30333;&#30418;&#35775;&#38382;&#26435;&#38480;&#21644;/&#25110;&#35745;&#31639;&#25104;&#26412;&#36739;&#39640;&#65292;&#20984;&#26174;&#20102;&#19979;&#19968;&#20195;&#20107;&#21518;&#35299;&#37322;&#22120;&#30340;&#38656;&#27714;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#30740;&#31350;LLM&#35299;&#37322;&#20854;&#20182;&#39044;&#27979;&#27169;&#22411;&#26377;&#25928;&#24615;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#22810;&#31181;&#25552;&#31034;&#31574;&#30053;&#30340;&#26032;&#39062;&#26694;&#26550;&#65306;i&#65289;&#22522;&#20110;&#25200;&#21160;&#30340;ICL&#65292;ii&#65289;&#22522;&#20110;&#39044;&#27979;&#30340;ICL&#65292;iii&#65289;&#22522;&#20110;&#25351;&#20196;&#30340;ICL&#65292;&#21644;iv&#65289;&#22522;&#20110;&#35299;&#37322;&#30340;ICL&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are increasingly used as powerful tools for a plethora of natural language processing (NLP) applications. A recent innovation, in-context learning (ICL), enables LLMs to learn new tasks by supplying a few examples in the prompt during inference time, thereby eliminating the need for model fine-tuning. While LLMs have been utilized in several applications, their applicability in explaining the behavior of other models remains relatively unexplored. Despite the growing number of new explanation techniques, many require white-box access to the model and/or are computationally expensive, highlighting a need for next-generation post hoc explainers. In this work, we present the first framework to study the effectiveness of LLMs in explaining other predictive models. More specifically, we propose a novel framework encompassing multiple prompting strategies: i) Perturbation-based ICL, ii) Prediction-based ICL, iii) Instruction-based ICL, and iv) Explanation-based I
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#20154;&#26684;&#20559;&#35265;&#23545;&#31038;&#20132;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#26469;&#34913;&#37327;&#19981;&#21516;&#20154;&#26684;&#37319;&#29992;&#19979;&#30340;&#20559;&#35265;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.05280</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#38543;&#26426;&#40550;&#40521;&#26356;&#21361;&#38505;&#21527;&#65311;&#35780;&#20272;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#20154;&#26684;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Are Personalized Stochastic Parrots More Dangerous? Evaluating Persona Biases in Dialogue Systems. (arXiv:2310.05280v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05280
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#20154;&#26684;&#20559;&#35265;&#23545;&#31038;&#20132;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#26469;&#34913;&#37327;&#19981;&#21516;&#20154;&#26684;&#37319;&#29992;&#19979;&#30340;&#20559;&#35265;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#20351;&#20854;&#33021;&#22815;&#25353;&#29031;&#33258;&#30001;&#24418;&#24335;&#30340;&#25351;&#20196;&#36827;&#34892;&#25805;&#20316;&#65292;&#21253;&#25324;&#22312;&#23545;&#35805;&#20013;&#27169;&#20223;&#36890;&#29992;&#25110;&#29305;&#23450;&#20154;&#21475;&#32676;&#20307;&#30340;&#20154;&#26684;&#12290;&#36890;&#29992;&#20154;&#26684;&#25351;&#30340;&#26159;&#26469;&#33258;&#26576;&#19968;&#20154;&#21475;&#32676;&#20307;&#30340;&#20010;&#20307;&#65288;&#20363;&#22914;&#20122;&#27954;&#20154;&#65289;&#65292;&#32780;&#29305;&#23450;&#20154;&#26684;&#21487;&#20197;&#26159;&#21382;&#21490;&#20154;&#29289;&#30340;&#23454;&#38469;&#22995;&#21517;&#12290;&#34429;&#28982;&#37319;&#29992;&#20154;&#26684;&#20351;&#23545;&#35805;&#31995;&#32479;&#26356;&#20855;&#21560;&#24341;&#21147;&#21644;&#20146;&#21644;&#21147;&#65292;&#20294;&#20063;&#23384;&#22312;&#28508;&#22312;&#39118;&#38505;&#65292;&#21487;&#33021;&#36890;&#36807;&#19982;&#29992;&#25143;&#30340;&#20132;&#20114;&#32780;&#21152;&#21095;&#31038;&#20250;&#20559;&#35265;&#65292;&#36827;&#19968;&#27493;&#36896;&#25104;&#31038;&#20250;&#20260;&#23475;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#8220;&#20154;&#26684;&#20559;&#35265;&#8221;&#65292;&#25105;&#20204;&#23558;&#20854;&#23450;&#20041;&#20026;&#26377;&#23475;&#23545;&#35805;&#27169;&#22411;&#34892;&#20026;&#23545;&#19981;&#21516;&#20154;&#26684;&#37319;&#29992;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#23558;&#20154;&#26684;&#20559;&#35265;&#20998;&#20026;&#26377;&#23475;&#34920;&#36798;&#21644;&#26377;&#23475;&#35748;&#21516;&#20004;&#31867;&#65292;&#21516;&#26102;&#24314;&#31435;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#20197;&#34913;&#37327;&#20116;&#20010;&#26041;&#38754;&#30340;&#20154;&#26684;&#20559;&#35265;&#65306;&#20882;&#29359;&#24615;&#12289;&#26377;&#27602;&#24310;&#32493;&#12289;&#20851;&#24576;&#12289;&#21051;&#26495;&#21360;&#35937;&#30340;&#35748;&#21516;&#20197;&#21450;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Large Language Models empower them to follow freeform instructions, including imitating generic or specific demographic personas in conversations. Generic personas refer to an individual from a demographic group (e.g. an Asian person), whereas specific personas can be actual names of historical figures. While the adoption of personas allows dialogue systems to be more engaging and approachable to users, it also carries the potential risk of exacerbating social biases in model responses, further causing societal harms through interactions with users. In this paper, we systematically study "persona biases", which we define to be the sensitivity of harmful dialogue model behaviors to different persona adoptions. We categorize persona biases into biases in harmful expression and harmful agreement, as well as establish a comprehensive evaluation framework to measure persona biases in five aspects: Offensiveness, Toxic Continuation, Regard, Stereotype Agreement, and To
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#21517;&#20026;BRAINTEASER&#30340;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#65292;&#26088;&#22312;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#27178;&#21521;&#24605;&#32500;&#21644;&#36829;&#21453;&#40664;&#35748;&#24120;&#35782;&#32852;&#31995;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#27178;&#21521;&#24605;&#32500;&#22522;&#20934;&#65292;&#20016;&#23500;&#38382;&#39064;&#30340;&#35821;&#20041;&#21644;&#19978;&#19979;&#25991;&#37325;&#24314;&#65292;&#23454;&#39564;&#35777;&#26126;&#27169;&#22411;&#19982;&#20154;&#31867;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2310.05057</link><description>&lt;p&gt;
BRAINTEASER&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27178;&#21521;&#24605;&#32500;&#38590;&#39064;
&lt;/p&gt;
&lt;p&gt;
BRAINTEASER: Lateral Thinking Puzzles for Large Language Models. (arXiv:2310.05057v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05057
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#21517;&#20026;BRAINTEASER&#30340;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#65292;&#26088;&#22312;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#27178;&#21521;&#24605;&#32500;&#21644;&#36829;&#21453;&#40664;&#35748;&#24120;&#35782;&#32852;&#31995;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#27178;&#21521;&#24605;&#32500;&#22522;&#20934;&#65292;&#20016;&#23500;&#38382;&#39064;&#30340;&#35821;&#20041;&#21644;&#19978;&#19979;&#25991;&#37325;&#24314;&#65292;&#23454;&#39564;&#35777;&#26126;&#27169;&#22411;&#19982;&#20154;&#31867;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#25104;&#21151;&#28608;&#21169;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31038;&#21306;&#20851;&#27880;&#38656;&#35201;&#38544;&#21547;&#21644;&#22797;&#26434;&#25512;&#29702;&#30340;&#20219;&#21153;&#65292;&#20381;&#36182;&#20110;&#31867;&#20154;&#30340;&#24120;&#35782;&#26426;&#21046;&#12290;&#34429;&#28982;&#36825;&#20123;&#22402;&#30452;&#24605;&#32500;&#20219;&#21153;&#30456;&#23545;&#36739;&#21463;&#27426;&#36814;&#65292;&#20294;&#27178;&#21521;&#24605;&#32500;&#38590;&#39064;&#21364;&#40092;&#26377;&#20851;&#27880;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;BRAINTEASER&#65306;&#19968;&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#65292;&#26088;&#22312;&#27979;&#35797;&#27169;&#22411;&#34920;&#29616;&#20986;&#27178;&#21521;&#24605;&#32500;&#21644;&#36829;&#21453;&#40664;&#35748;&#24120;&#35782;&#32852;&#31995;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#19977;&#27493;&#39588;&#30340;&#31243;&#24207;&#26469;&#21019;&#24314;&#31532;&#19968;&#20010;&#27178;&#21521;&#24605;&#32500;&#22522;&#20934;&#65292;&#21253;&#25324;&#25968;&#25454;&#25910;&#38598;&#12289;&#24178;&#25200;&#39033;&#29983;&#25104;&#21644;&#23545;&#25239;&#24615;&#26679;&#26412;&#29983;&#25104;&#65292;&#20849;&#26377;1,100&#20010;&#20855;&#26377;&#39640;&#36136;&#37327;&#27880;&#37322;&#30340;&#38590;&#39064;&#12290;&#20026;&#20102;&#35780;&#20272;&#27169;&#22411;&#30340;&#27178;&#21521;&#25512;&#29702;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#22522;&#20110;&#38382;&#39064;&#30340;&#35821;&#20041;&#21644;&#19978;&#19979;&#25991;&#37325;&#24314;&#26469;&#20016;&#23500;BRAINTEASER&#12290;&#25105;&#20204;&#23545;&#26368;&#20808;&#36827;&#30340;&#25351;&#23548;&#24615;&#21644;&#24120;&#35782;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#20154;&#31867;&#21644;&#27169;&#22411;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of language models has inspired the NLP community to attend to tasks that require implicit and complex reasoning, relying on human-like commonsense mechanisms. While such vertical thinking tasks have been relatively popular, lateral thinking puzzles have received little attention. To bridge this gap, we devise BRAINTEASER: a multiple-choice Question Answering task designed to test the model's ability to exhibit lateral thinking and defy default commonsense associations. We design a three-step procedure for creating the first lateral thinking benchmark, consisting of data collection, distractor generation, and generation of adversarial examples, leading to 1,100 puzzles with high-quality annotations. To assess the consistency of lateral reasoning by models, we enrich BRAINTEASER based on a semantic and contextual reconstruction of its questions. Our experiments with state-of-the-art instruction- and commonsense language models reveal a significant gap between human and model
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#38646;-shot&#20851;&#31995;&#25277;&#21462;&#22120;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#36807;&#24635;&#32467;&#21644;&#25552;&#38382;(\textsc{SumAsk})&#25552;&#31034;&#26041;&#27861;&#26469;&#25913;&#36827;&#38646;-shot&#20851;&#31995;&#25277;&#21462;&#12290;&#23454;&#39564;&#35777;&#26126;LLMs&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.05028</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;-shot&#20851;&#31995;&#25277;&#21462;&#22120;
&lt;/p&gt;
&lt;p&gt;
Revisiting Large Language Models as Zero-shot Relation Extractors. (arXiv:2310.05028v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#38646;-shot&#20851;&#31995;&#25277;&#21462;&#22120;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#36807;&#24635;&#32467;&#21644;&#25552;&#38382;(\textsc{SumAsk})&#25552;&#31034;&#26041;&#27861;&#26469;&#25913;&#36827;&#38646;-shot&#20851;&#31995;&#25277;&#21462;&#12290;&#23454;&#39564;&#35777;&#26126;LLMs&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25277;&#21462;(RE)&#21363;&#20351;&#22312;&#38646;-shot&#35774;&#23450;&#19979;&#65292;&#19968;&#30452;&#28041;&#21450;&#19968;&#23450;&#31243;&#24230;&#30340;&#26631;&#35760;&#25110;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#22815;&#22312;&#32473;&#23450;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#26080;&#38656;&#20219;&#20309;&#25968;&#25454;&#21644;&#21442;&#25968;&#35843;&#25972;&#65292;&#33258;&#21160;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#36825;&#20026;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#20851;&#31995;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#12290;&#26412;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#23558;LLMs&#65292;&#22914;ChatGPT&#65292;&#20316;&#20026;&#38646;-shot&#20851;&#31995;&#25277;&#21462;&#22120;&#30340;&#30740;&#31350;&#12290;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#29616;&#26377;RE&#25552;&#31034;&#30340;&#32570;&#28857;&#65292;&#24182;&#23581;&#35797;&#23558;&#26368;&#36817;&#30340;&#25552;&#31034;&#25216;&#26415;&#65292;&#22914;CoT&#65292;&#32435;&#20837;&#20854;&#20013;&#20197;&#25552;&#39640;&#38646;-shot&#20851;&#31995;&#25277;&#21462;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24635;&#32467;&#21644;&#25552;&#38382;(\textsc{SumAsk})&#25552;&#31034;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#25552;&#31034;&#65292;&#36890;&#36807;&#36882;&#24402;&#20351;&#29992;LLMs&#23558;RE&#36755;&#20837;&#36716;&#25442;&#20026;&#26377;&#25928;&#30340;&#38382;&#31572;(QA)&#26684;&#24335;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#23545;&#21508;&#31181;&#22522;&#20934;&#21644;&#35774;&#32622;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#20197;&#35843;&#26597;LLMs&#22312;&#38646;-shot&#20851;&#31995;&#25277;&#21462;&#19978;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26377;&#20197;&#19979;&#30340;followi
&lt;/p&gt;
&lt;p&gt;
Relation extraction (RE) consistently involves a certain degree of labeled or unlabeled data even if under zero-shot setting. Recent studies have shown that large language models (LLMs) transfer well to new tasks out-of-the-box simply given a natural language prompt, which provides the possibility of extracting relations from text without any data and parameter tuning. This work focuses on the study of exploring LLMs, such as ChatGPT, as zero-shot relation extractors. On the one hand, we analyze the drawbacks of existing RE prompts and attempt to incorporate recent prompt techniques such as chain-of-thought (CoT) to improve zero-shot RE. We propose the summarize-and-ask (\textsc{SumAsk}) prompting, a simple prompt recursively using LLMs to transform RE inputs to the effective question answering (QA) format. On the other hand, we conduct comprehensive experiments on various benchmarks and settings to investigate the capabilities of LLMs on zero-shot RE. Specifically, we have the followi
&lt;/p&gt;</description></item><item><title>Hermes&#26159;&#19968;&#20010;&#33258;&#21160;&#29983;&#25104;&#26377;&#38480;&#29366;&#24577;&#26426;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#38145;&#31227;&#21160;&#32593;&#32476;&#21327;&#35758;&#30340;&#23433;&#20840;&#20998;&#26512;&#12290;&#36890;&#36807;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#35268;&#33539;&#24182;&#29983;&#25104;&#36923;&#36753;&#20844;&#24335;&#65292;Hermes&#33021;&#22815;&#21457;&#29616;&#26032;&#30340;&#28431;&#27934;&#21644;&#25915;&#20987;&#65292;&#24182;&#23545;&#29616;&#26377;&#35268;&#33539;&#21644;&#21830;&#19994;&#22522;&#24102;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2310.04381</link><description>&lt;p&gt;
Hermes&#65306;&#36890;&#36807;&#20174;&#33258;&#28982;&#35821;&#35328;&#35268;&#33539;&#21512;&#25104;&#26377;&#38480;&#29366;&#24577;&#26426;&#26469;&#35299;&#38145;&#31227;&#21160;&#32593;&#32476;&#21327;&#35758;&#30340;&#23433;&#20840;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Hermes: Unlocking Security Analysis of Cellular Network Protocols by Synthesizing Finite State Machines from Natural Language Specifications. (arXiv:2310.04381v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04381
&lt;/p&gt;
&lt;p&gt;
Hermes&#26159;&#19968;&#20010;&#33258;&#21160;&#29983;&#25104;&#26377;&#38480;&#29366;&#24577;&#26426;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#38145;&#31227;&#21160;&#32593;&#32476;&#21327;&#35758;&#30340;&#23433;&#20840;&#20998;&#26512;&#12290;&#36890;&#36807;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#35268;&#33539;&#24182;&#29983;&#25104;&#36923;&#36753;&#20844;&#24335;&#65292;Hermes&#33021;&#22815;&#21457;&#29616;&#26032;&#30340;&#28431;&#27934;&#21644;&#25915;&#20987;&#65292;&#24182;&#23545;&#29616;&#26377;&#35268;&#33539;&#21644;&#21830;&#19994;&#22522;&#24102;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Hermes&#65292;&#19968;&#20010;&#33258;&#21160;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#31227;&#21160;&#35268;&#33539;&#30340;&#24418;&#24335;&#34920;&#36798;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#12290;&#25105;&#20204;&#39318;&#20808;&#24320;&#21457;&#20102;&#31070;&#32463;&#32452;&#25104;&#20998;&#26512;&#22120;NEUTREX&#65292;&#29992;&#20110;&#22788;&#29702;&#19982;&#36716;&#25442;&#30456;&#20851;&#30340;&#25991;&#26412;&#24182;&#25552;&#21462;&#36716;&#25442;&#32452;&#20214;&#65288;&#21363;&#29366;&#24577;&#12289;&#26465;&#20214;&#21644;&#21160;&#20316;&#65289;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#65292;&#36890;&#36807;&#21033;&#29992;&#20381;&#23384;&#35299;&#26512;&#26641;&#23558;&#36825;&#20123;&#36716;&#25442;&#32452;&#20214;&#36716;&#21270;&#25104;&#36923;&#36753;&#20844;&#24335;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#36923;&#36753;&#20844;&#24335;&#32534;&#35793;&#25104;&#36716;&#25442;&#21644;&#21019;&#24314;&#24418;&#24335;&#27169;&#22411;&#20316;&#20026;&#26377;&#38480;&#29366;&#24577;&#26426;&#12290;&#20026;&#20102;&#35777;&#26126;Hermes&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;4G NAS&#12289;5G NAS&#21644;5G RRC&#35268;&#33539;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#33719;&#24471;&#20102;81-87%&#30340;&#24635;&#20307;&#20934;&#30830;&#29575;&#65292;&#36825;&#26159;&#23545;&#29616;&#26377;&#25216;&#26415;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;&#25105;&#20204;&#23545;&#25552;&#21462;&#30340;&#27169;&#22411;&#36827;&#34892;&#30340;&#23433;&#20840;&#20998;&#26512;&#25581;&#31034;&#20986;&#20102;3&#20010;&#26032;&#30340;&#28431;&#27934;&#12289;&#21457;&#29616;&#20102;19&#20010;&#20043;&#21069;&#30340;&#25915;&#20987;4G&#21644;5G&#35268;&#33539;&#65292;&#20197;&#21450;7&#20010;&#21830;&#19994;4G&#22522;&#24102;&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present Hermes, an end-to-end framework to automatically generate formal representations from natural language cellular specifications. We first develop a neural constituency parser, NEUTREX, to process transition-relevant texts and extract transition components (i.e., states, conditions, and actions). We also design a domain-specific language to translate these transition components to logical formulas by leveraging dependency parse trees. Finally, we compile these logical formulas to generate transitions and create the formal model as finite state machines. To demonstrate the effectiveness of Hermes, we evaluate it on 4G NAS, 5G NAS, and 5G RRC specifications and obtain an overall accuracy of 81-87%, which is a substantial improvement over the state-of-the-art. Our security analysis of the extracted models uncovers 3 new vulnerabilities and identifies 19 previous attacks in 4G and 5G specifications, and 7 deviations in commercial 4G basebands.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#26368;&#26032;&#30340;GPT-4V(ision)&#27169;&#22411;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21457;&#29616;&#20854;&#20855;&#26377;&#22788;&#29702;&#22810;&#27169;&#24577;&#36755;&#20837;&#12289;&#39640;&#36890;&#29992;&#24615;&#21644;&#29702;&#35299;&#24102;&#26377;&#35270;&#35273;&#26631;&#35760;&#30340;&#33021;&#21147;&#65292;&#20351;&#20854;&#25104;&#20026;&#24378;&#22823;&#30340;&#22810;&#27169;&#24577;&#36890;&#29992;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2309.17421</link><description>&lt;p&gt;
LMM&#30340;&#40654;&#26126;&#65306;&#19982;GPT-4V(ision)&#30340;&#21021;&#27493;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision). (arXiv:2309.17421v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#26368;&#26032;&#30340;GPT-4V(ision)&#27169;&#22411;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21457;&#29616;&#20854;&#20855;&#26377;&#22788;&#29702;&#22810;&#27169;&#24577;&#36755;&#20837;&#12289;&#39640;&#36890;&#29992;&#24615;&#21644;&#29702;&#35299;&#24102;&#26377;&#35270;&#35273;&#26631;&#35760;&#30340;&#33021;&#21147;&#65292;&#20351;&#20854;&#25104;&#20026;&#24378;&#22823;&#30340;&#22810;&#27169;&#24577;&#36890;&#29992;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#36890;&#36807;&#35270;&#35273;&#29702;&#35299;&#31561;&#22810;&#24863;&#30693;&#33021;&#21147;&#26469;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20197;&#23454;&#29616;&#26356;&#24378;&#22823;&#30340;&#36890;&#29992;&#26234;&#33021;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#26368;&#26032;&#30340;&#27169;&#22411;GPT-4V(ision)&#65292;&#20197;&#28145;&#21270;&#23545;LMMs&#30340;&#29702;&#35299;&#12290;&#20998;&#26512;&#37325;&#28857;&#26159;GPT-4V&#33021;&#22815;&#25191;&#34892;&#30340;&#24341;&#20154;&#27880;&#30446;&#20219;&#21153;&#65292;&#21253;&#25324;&#27979;&#35797;&#26679;&#26412;&#26469;&#25506;&#27979;GPT-4V&#33021;&#21147;&#30340;&#36136;&#37327;&#21644;&#36890;&#29992;&#24615;&#65292;&#20854;&#25903;&#25345;&#30340;&#36755;&#20837;&#21644;&#24037;&#20316;&#27169;&#24335;&#65292;&#20197;&#21450;&#28608;&#21457;&#27169;&#22411;&#30340;&#26377;&#25928;&#26041;&#24335;&#12290;&#22312;&#25506;&#32034;GPT-4V&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#20102;&#19968;&#31995;&#21015;&#28085;&#30422;&#21508;&#31181;&#39046;&#22495;&#21644;&#20219;&#21153;&#30340;&#23450;&#24615;&#26679;&#26412;&#12290;&#36825;&#20123;&#26679;&#26412;&#30340;&#35266;&#23519;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-4V&#22312;&#22788;&#29702;&#20219;&#24847;&#20132;&#38169;&#30340;&#22810;&#27169;&#24577;&#36755;&#20837;&#21644;&#20854;&#33021;&#21147;&#30340;&#36890;&#29992;&#24615;&#26041;&#38754;&#20855;&#26377;&#21069;&#25152;&#26410;&#26377;&#30340;&#33021;&#21147;&#65292;&#20351;&#20854;&#25104;&#20026;&#24378;&#22823;&#30340;&#22810;&#27169;&#24577;&#20840;&#33021;&#31995;&#32479;&#12290;&#27492;&#22806;&#65292;GPT-4V&#29420;&#29305;&#30340;&#33021;&#21147;&#21487;&#20197;&#29702;&#35299;&#32472;&#21046;&#22312;&#36755;&#20837;&#22270;&#20687;&#19978;&#30340;&#35270;&#35273;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large multimodal models (LMMs) extend large language models (LLMs) with multi-sensory skills, such as visual understanding, to achieve stronger generic intelligence. In this paper, we analyze the latest model, GPT-4V(ision), to deepen the understanding of LMMs. The analysis focuses on the intriguing tasks that GPT-4V can perform, containing test samples to probe the quality and genericity of GPT-4V's capabilities, its supported inputs and working modes, and the effective ways to prompt the model. In our approach to exploring GPT-4V, we curate and organize a collection of carefully designed qualitative samples spanning a variety of domains and tasks. Observations from these samples demonstrate that GPT-4V's unprecedented ability in processing arbitrarily interleaved multimodal inputs and the genericity of its capabilities together make GPT-4V a powerful multimodal generalist system. Furthermore, GPT-4V's unique capability of understanding visual markers drawn on input images can give ri
&lt;/p&gt;</description></item><item><title>InstructionGPT-4&#36890;&#36807;&#20165;&#20351;&#29992;200&#20010;&#20363;&#23376;&#36827;&#34892;&#24494;&#35843;&#65292;&#22312;&#22810;&#27169;&#24335;&#25351;&#20196;&#25968;&#25454;&#36136;&#37327;&#24230;&#37327;&#21644;&#36873;&#25321;&#22120;&#30340;&#24110;&#21161;&#19979;&#65292;&#22312;&#21508;&#31181;&#35780;&#20272;&#20219;&#21153;&#20013;&#20248;&#20110;&#21407;&#22987;&#30340;MiniGPT-4&#12290;</title><link>http://arxiv.org/abs/2308.12067</link><description>&lt;p&gt;
InstructionGPT-4: &#19968;&#20010;200&#25351;&#20196;&#33539;&#24335;&#29992;&#20110;&#24494;&#35843;MiniGPT-4
&lt;/p&gt;
&lt;p&gt;
InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4. (arXiv:2308.12067v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12067
&lt;/p&gt;
&lt;p&gt;
InstructionGPT-4&#36890;&#36807;&#20165;&#20351;&#29992;200&#20010;&#20363;&#23376;&#36827;&#34892;&#24494;&#35843;&#65292;&#22312;&#22810;&#27169;&#24335;&#25351;&#20196;&#25968;&#25454;&#36136;&#37327;&#24230;&#37327;&#21644;&#36873;&#25321;&#22120;&#30340;&#24110;&#21161;&#19979;&#65292;&#22312;&#21508;&#31181;&#35780;&#20272;&#20219;&#21153;&#20013;&#20248;&#20110;&#21407;&#22987;&#30340;MiniGPT-4&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#20004;&#38454;&#27573;&#30340;&#35757;&#32451;&#36807;&#31243;&#33719;&#21462;&#20854;&#36981;&#24490;&#25351;&#20196;&#30340;&#33021;&#21147;&#65306;&#22312;&#22270;&#20687;-&#25991;&#26412;&#23545;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#30417;&#30563;&#24335;&#35270;&#35273;-&#35821;&#35328;&#25351;&#20196;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#21482;&#26377;&#26377;&#38480;&#37327;&#30340;&#39640;&#36136;&#37327;&#36981;&#24490;&#25351;&#20196;&#25968;&#25454;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20063;&#33021;&#21462;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;InstructionGPT-4&#65292;&#23427;&#32463;&#36807;&#24494;&#35843;&#30340;&#25968;&#25454;&#38598;&#21482;&#21253;&#21547;200&#20010;&#20363;&#23376;&#65292;&#32422;&#21344;MiniGPT-4&#23545;&#40784;&#25968;&#25454;&#38598;&#20013;&#20351;&#29992;&#30340;&#36981;&#24490;&#25351;&#20196;&#25968;&#25454;&#30340;6%&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#20960;&#20010;&#29992;&#20110;&#35780;&#20272;&#22810;&#27169;&#24335;&#25351;&#20196;&#25968;&#25454;&#36136;&#37327;&#30340;&#24230;&#37327;&#25351;&#26631;&#12290;&#22522;&#20110;&#36825;&#20123;&#24230;&#37327;&#25351;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25968;&#25454;&#36873;&#25321;&#22120;&#65292;&#33258;&#21160;&#35782;&#21035;&#21644;&#36807;&#28388;&#20302;&#36136;&#37327;&#30340;&#35270;&#35273;-&#35821;&#35328;&#25968;&#25454;&#12290;&#36890;&#36807;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;InstructionGPT-4&#22312;&#21508;&#31181;&#35780;&#20272;&#65288;&#22914;&#35270;&#35273;&#38382;&#31572;&#12289;GPT-4&#20559;&#22909;&#65289;&#19978;&#20248;&#20110;&#21407;&#22987;&#30340;MiniGPT-4&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;...
&lt;/p&gt;
&lt;p&gt;
Multimodal large language models acquire their instruction-following capabilities through a two-stage training process: pre-training on image-text pairs and fine-tuning on supervised vision-language instruction data. Recent studies have shown that large language models can achieve satisfactory results even with a limited amount of high-quality instruction-following data. In this paper, we introduce InstructionGPT-4, which is fine-tuned on a small dataset comprising only 200 examples, amounting to approximately 6% of the instruction-following data used in the alignment dataset for MiniGPT-4. We first propose several metrics to access the quality of multimodal instruction data. Based on these metrics, we present a simple and effective data selector to automatically identify and filter low-quality vision-language data. By employing this method, InstructionGPT-4 outperforms the original MiniGPT-4 on various evaluations (e.g., visual question answering, GPT-4 preference). Overall, our findi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ChatReport&#30340;&#22522;&#20110;LLM&#30340;&#31995;&#32479;&#65292;&#23427;&#36890;&#36807;&#23454;&#29616;&#21487;&#36861;&#28335;&#30340;&#31572;&#26696;&#21644;&#35299;&#20915;&#39046;&#22495;&#19987;&#23478;&#21442;&#19982;&#20302;&#25928;&#24615;&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#36890;&#36807;&#33258;&#21160;&#20998;&#26512;&#20225;&#19994;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#65292;&#23454;&#29616;&#21487;&#25345;&#32493;&#24615;&#25259;&#38706;&#20998;&#26512;&#27665;&#20027;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.15770</link><description>&lt;p&gt;
CHATREPORT&#65306;&#36890;&#36807;&#22522;&#20110;LLM&#24037;&#20855;&#23454;&#29616;&#21487;&#25345;&#32493;&#24615;&#25259;&#38706;&#20998;&#26512;&#30340;&#27665;&#20027;&#21270;
&lt;/p&gt;
&lt;p&gt;
CHATREPORT: Democratizing Sustainability Disclosure Analysis through LLM-based Tools. (arXiv:2307.15770v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ChatReport&#30340;&#22522;&#20110;LLM&#30340;&#31995;&#32479;&#65292;&#23427;&#36890;&#36807;&#23454;&#29616;&#21487;&#36861;&#28335;&#30340;&#31572;&#26696;&#21644;&#35299;&#20915;&#39046;&#22495;&#19987;&#23478;&#21442;&#19982;&#20302;&#25928;&#24615;&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#36890;&#36807;&#33258;&#21160;&#20998;&#26512;&#20225;&#19994;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#65292;&#23454;&#29616;&#21487;&#25345;&#32493;&#24615;&#25259;&#38706;&#20998;&#26512;&#27665;&#20027;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#23545;&#27668;&#20505;&#21464;&#21270;&#65292;&#20844;&#21496;&#30495;&#30340;&#22312;&#26397;&#30528;&#26356;&#21487;&#25345;&#32493;&#32463;&#33829;&#36808;&#20986;&#23454;&#36136;&#24615;&#30340;&#27493;&#20240;&#21527;&#65311;&#19968;&#20010;&#20840;&#38754;&#30340;&#31572;&#26696;&#21487;&#20197;&#22312;&#20225;&#19994;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#30340;&#23494;&#38598;&#20449;&#24687;&#20013;&#25214;&#21040;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25253;&#21578;&#30340;&#25968;&#37327;&#21644;&#22797;&#26434;&#24615;&#20351;&#20154;&#24037;&#20998;&#26512;&#25104;&#26412;&#38750;&#24120;&#39640;&#26114;&#12290;&#22240;&#27492;&#65292;&#21482;&#26377;&#23569;&#25968;&#30340;&#26426;&#26500;&#25317;&#26377;&#36164;&#28304;&#33021;&#22815;&#22823;&#35268;&#27169;&#20998;&#26512;&#36825;&#20123;&#25253;&#21578;&#65292;&#36825;&#23548;&#33268;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#32570;&#20047;&#36879;&#26126;&#24230;&#12290;&#36890;&#36807;&#22522;&#20110;LLM&#33258;&#21160;&#20998;&#26512;&#24037;&#20855;&#36171;&#33021;&#21033;&#30410;&#30456;&#20851;&#32773;&#21487;&#33021;&#26159;&#23454;&#29616;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#20998;&#26512;&#27665;&#20027;&#21270;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#24320;&#21457;&#36825;&#26679;&#30340;&#24037;&#20855;&#38754;&#20020;&#25361;&#25112;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;LLM&#30340;&#24187;&#35273;&#38382;&#39064;&#21644;&#23558;&#39046;&#22495;&#19987;&#23478;&#24341;&#20837;AI&#24320;&#21457;&#36807;&#31243;&#30340;&#20302;&#25928;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ChatReport&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#26032;&#22411;&#31995;&#32479;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#20998;&#26512;&#20225;&#19994;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#65292;&#36890;&#36807;&#20351;&#31572;&#26696;&#21487;&#36861;&#28335;&#26469;&#20943;&#23569;&#24187;&#35273;&#30340;&#21361;&#23475;&#65292;&#24182;&#35299;&#20915;&#39046;&#22495;&#19987;&#23478;&#21442;&#19982;AI&#24320;&#21457;&#36807;&#31243;&#30340;&#20302;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the face of climate change, are companies really taking substantial steps toward more sustainable operations? A comprehensive answer lies in the dense, information-rich landscape of corporate sustainability reports. However, the sheer volume and complexity of these reports make human analysis very costly. Therefore, only a few entities worldwide have the resources to analyze these reports at scale, which leads to a lack of transparency in sustainability reporting. Empowering stakeholders with LLM-based automatic analysis tools can be a promising way to democratize sustainability report analysis. However, developing such tools is challenging due to (1) the hallucination of LLMs and (2) the inefficiency of bringing domain experts into the AI development loop. In this paper, we ChatReport, a novel LLM-based system to automate the analysis of corporate sustainability reports, addressing existing challenges by (1) making the answers traceable to reduce the harm of hallucination and (2) a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#23545;&#25239;&#24615;&#23884;&#20837;&#31354;&#38388;&#25915;&#20987;&#65292;&#25506;&#35752;&#20102;&#23558;&#29616;&#25104;&#32452;&#20214;&#25554;&#20837;&#22810;&#27169;&#22411;&#27169;&#22411;&#20013;&#30340;&#28431;&#27934;&#21644;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#22810;&#27169;&#22411;&#31995;&#32479;&#26435;&#37325;&#21644;&#21442;&#25968;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#23547;&#25214;&#20301;&#20110;&#39044;&#35757;&#32451;&#32452;&#20214;&#23884;&#20837;&#31354;&#38388;&#21361;&#38505;&#21306;&#22495;&#30340;&#36755;&#20837;&#22270;&#20687;&#36827;&#34892;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2307.14539</link><description>&lt;p&gt;
&#25554;&#20837;&#24182;&#31048;&#31095;&#65306;&#21033;&#29992;&#22810;&#27169;&#22411;&#27169;&#22411;&#30340;&#29616;&#25104;&#32452;&#20214;&#36827;&#34892;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Plug and Pray: Exploiting off-the-shelf components of Multi-Modal Models. (arXiv:2307.14539v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#23545;&#25239;&#24615;&#23884;&#20837;&#31354;&#38388;&#25915;&#20987;&#65292;&#25506;&#35752;&#20102;&#23558;&#29616;&#25104;&#32452;&#20214;&#25554;&#20837;&#22810;&#27169;&#22411;&#27169;&#22411;&#20013;&#30340;&#28431;&#27934;&#21644;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#22810;&#27169;&#22411;&#31995;&#32479;&#26435;&#37325;&#21644;&#21442;&#25968;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#23547;&#25214;&#20301;&#20110;&#39044;&#35757;&#32451;&#32452;&#20214;&#23884;&#20837;&#31354;&#38388;&#21361;&#38505;&#21306;&#22495;&#30340;&#36755;&#20837;&#22270;&#20687;&#36827;&#34892;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#39069;&#22806;&#30340;&#27169;&#24577;&#65288;&#22914;&#35270;&#35273;&#65289;&#21152;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24555;&#36895;&#22686;&#38271;&#21644;&#26085;&#30410;&#21463;&#27426;&#36814;&#24341;&#36215;&#20102;&#20005;&#37325;&#30340;&#23433;&#20840;&#38382;&#39064;&#12290;&#36825;&#31181;&#27169;&#24577;&#30340;&#25193;&#23637;&#31867;&#20284;&#20110;&#22312;&#25151;&#23376;&#19978;&#22686;&#21152;&#26356;&#22810;&#30340;&#38376;&#65292;&#26080;&#24847;&#20013;&#20026;&#23545;&#25239;&#24615;&#25915;&#20987;&#21019;&#24314;&#20102;&#22810;&#20010;&#35775;&#38382;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#23545;&#25239;&#24615;&#23884;&#20837;&#31354;&#38388;&#25915;&#20987;&#65292;&#25105;&#20204;&#24378;&#35843;&#22810;&#27169;&#22411;&#31995;&#32479;&#20013;&#30340;&#28431;&#27934;&#65292;&#36825;&#20123;&#28431;&#27934;&#28304;&#20110;&#20197;&#25554;&#25300;&#26041;&#24335;&#23558;&#29616;&#25104;&#32452;&#20214;&#65288;&#22914;&#20844;&#20849;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#65289;&#24341;&#20837;&#36825;&#20123;&#31995;&#32479;&#12290;&#19982;&#29616;&#26377;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#35775;&#38382;&#22810;&#27169;&#22411;&#31995;&#32479;&#30340;&#26435;&#37325;&#25110;&#21442;&#25968;&#65292;&#32780;&#26159;&#20381;&#36182;&#20110;&#36825;&#20123;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#30340;&#24222;&#22823;&#19988;&#26410;&#23436;&#20840;&#24320;&#21457;&#30340;&#23884;&#20837;&#31354;&#38388;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#23884;&#20837;&#31354;&#38388;&#25915;&#20987;&#26159;&#36890;&#36807;&#23547;&#25214;&#20301;&#20110;&#36825;&#20123;&#39044;&#35757;&#32451;&#32452;&#20214;&#30340;&#24191;&#27867;&#23884;&#20837;&#31354;&#38388;&#30340;&#21361;&#38505;&#25110;&#30446;&#26631;&#21306;&#22495;&#30340;&#36755;&#20837;&#22270;&#20687;&#26469;&#36827;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid growth and increasing popularity of incorporating additional modalities (e.g., vision) into large language models (LLMs) has raised significant security concerns. This expansion of modality, akin to adding more doors to a house, unintentionally creates multiple access points for adversarial attacks. In this paper, by introducing adversarial embedding space attacks, we emphasize the vulnerabilities present in multi-modal systems that originate from incorporating off-the-shelf components like public pre-trained encoders in a plug-and-play manner into these systems. In contrast to existing work, our approach does not require access to the multi-modal system's weights or parameters but instead relies on the huge under-explored embedding space of such pre-trained encoders. Our proposed embedding space attacks involve seeking input images that reside within the dangerous or targeted regions of the extensive embedding space of these pre-trained components. These crafted adversarial 
&lt;/p&gt;</description></item><item><title>&#39564;&#35777;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#36755;&#20986;&#26159;&#19968;&#20010;&#26032;&#20852;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#20998;&#26512;&#22810;&#27169;&#24577;&#25968;&#25454;&#28246;&#30340;&#24213;&#23618;&#25968;&#25454;&#65292;&#35780;&#20272;&#20854;&#36136;&#37327;&#21644;&#19968;&#33268;&#24615;&#65292;&#26469;&#24314;&#31435;&#35780;&#20272;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#36755;&#20986;&#30340;&#26356;&#22362;&#23454;&#22522;&#30784;&#65292;&#24182;&#35299;&#20915;&#38169;&#35823;&#20449;&#24687;&#20256;&#25773;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.02796</link><description>&lt;p&gt;
VerifAI&#65306;&#39564;&#35777;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
VerifAI: Verified Generative AI. (arXiv:2307.02796v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02796
&lt;/p&gt;
&lt;p&gt;
&#39564;&#35777;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#36755;&#20986;&#26159;&#19968;&#20010;&#26032;&#20852;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#20998;&#26512;&#22810;&#27169;&#24577;&#25968;&#25454;&#28246;&#30340;&#24213;&#23618;&#25968;&#25454;&#65292;&#35780;&#20272;&#20854;&#36136;&#37327;&#21644;&#19968;&#33268;&#24615;&#65292;&#26469;&#24314;&#31435;&#35780;&#20272;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#36755;&#20986;&#30340;&#26356;&#22362;&#23454;&#22522;&#30784;&#65292;&#24182;&#35299;&#20915;&#38169;&#35823;&#20449;&#24687;&#20256;&#25773;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#35201;&#30340;&#36827;&#23637;&#65292;&#20294;&#26159;&#23545;&#20110;&#20854;&#36755;&#20986;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#30340;&#25285;&#24551;&#20173;&#22312;&#22686;&#38271;&#12290;&#36825;&#31181;&#19981;&#20934;&#30830;&#24615;&#21487;&#33021;&#20135;&#29983;&#20005;&#37325;&#21518;&#26524;&#65292;&#22914;&#38169;&#35823;&#20915;&#31574;&#65292;&#20256;&#25773;&#34394;&#20551;&#20449;&#24687;&#65292;&#20405;&#29359;&#38544;&#31169;&#65292;&#27861;&#24459;&#36131;&#20219;&#31561;&#12290;&#34429;&#28982;&#24050;&#32463;&#22312;&#36827;&#34892;&#24212;&#23545;&#36825;&#20123;&#39118;&#38505;&#30340;&#21162;&#21147;&#65292;&#21253;&#25324;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#21644;&#36127;&#36131;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#23454;&#36341;&#65292;&#22914;&#36879;&#26126;&#24230;&#65292;&#38544;&#31169;&#20445;&#25252;&#65292;&#20559;&#35265;&#32531;&#35299;&#20197;&#21450;&#31038;&#20250;&#21644;&#29615;&#22659;&#36131;&#20219;&#31561;&#65292;&#20294;&#30001;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24341;&#36215;&#30340;&#38169;&#35823;&#20449;&#24687;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#65292;&#20174;&#25968;&#25454;&#31649;&#29702;&#30340;&#35282;&#24230;&#39564;&#35777;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#36755;&#20986;&#26159;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#20010;&#26032;&#20852;&#38382;&#39064;&#12290;&#36825;&#21253;&#25324;&#20998;&#26512;&#26469;&#33258;&#22810;&#27169;&#24577;&#25968;&#25454;&#28246;&#30340;&#24213;&#23618;&#25968;&#25454;&#65292;&#21253;&#25324;&#25991;&#26412;&#25991;&#20214;&#65292;&#34920;&#26684;&#21644;&#30693;&#35782;&#22270;&#35889;&#65292;&#24182;&#35780;&#20272;&#20854;&#36136;&#37327;&#21644;&#19968;&#33268;&#24615;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#21487;&#20197;&#20026;&#35780;&#20272;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#36755;&#20986;&#22880;&#23450;&#26356;&#22362;&#23454;&#30340;&#22522;&#30784;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#24110;&#21161;&#35299;&#20915;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#36755;&#20986;&#39564;&#35777;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI has made significant strides, yet concerns about the accuracy and reliability of its outputs continue to grow. Such inaccuracies can have serious consequences such as inaccurate decision-making, the spread of false information, privacy violations, legal liabilities, and more. Although efforts to address these risks are underway, including explainable AI and responsible AI practices such as transparency, privacy protection, bias mitigation, and social and environmental responsibility, misinformation caused by generative AI will remain a significant challenge. We propose that verifying the outputs of generative AI from a data management perspective is an emerging issue for generative AI. This involves analyzing the underlying data from multi-modal data lakes, including text files, tables, and knowledge graphs, and assessing its quality and consistency. By doing so, we can establish a stronger foundation for evaluating the outputs of generative AI models. Such an approach ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;CamChoice&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#22810;&#39033;&#36873;&#25321;&#29702;&#35299;&#38382;&#39064;&#21644;&#30495;&#23454;&#20505;&#36873;&#31572;&#26696;&#36873;&#39033;&#20998;&#24067;&#65292;&#20026;&#20505;&#36873;&#20154;&#20998;&#24067;&#21305;&#37197;&#20219;&#21153;&#25552;&#20379;&#20102;&#33258;&#21160;&#35780;&#20272;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2306.13047</link><description>&lt;p&gt;
CamChoice&#65306;&#19968;&#20221;&#21253;&#21547;&#22810;&#39033;&#36873;&#25321;&#39064;&#21644;&#20505;&#36873;&#31572;&#26696;&#20998;&#24067;&#30340;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
CamChoice: A Corpus of Multiple Choice Questions and Candidate Response Distributions. (arXiv:2306.13047v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;CamChoice&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#22810;&#39033;&#36873;&#25321;&#29702;&#35299;&#38382;&#39064;&#21644;&#30495;&#23454;&#20505;&#36873;&#31572;&#26696;&#36873;&#39033;&#20998;&#24067;&#65292;&#20026;&#20505;&#36873;&#20154;&#20998;&#24067;&#21305;&#37197;&#20219;&#21153;&#25552;&#20379;&#20102;&#33258;&#21160;&#35780;&#20272;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#39033;&#36873;&#25321;&#39064;&#26159;&#29992;&#20110;&#34913;&#37327;&#20505;&#36873;&#20154;&#22312;&#21508;&#31181;&#39046;&#22495;&#21644;&#20219;&#21153;&#20013;&#33021;&#21147;&#30340;&#26222;&#36941;&#35780;&#20272;&#24418;&#24335;&#12290;&#25552;&#20986;&#30340;&#38382;&#39064;&#30340;&#36136;&#37327;&#23545;&#20110;&#27979;&#35797;&#35774;&#35745;&#20154;&#21592;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#27492;&#26032;&#25552;&#20986;&#30340;&#38382;&#39064;&#22312;&#37096;&#32626;&#21040;&#23454;&#38469;&#32771;&#35797;&#20043;&#21069;&#38656;&#35201;&#32463;&#36807;&#20960;&#20010;&#39044;&#27979;&#35797;&#35780;&#20272;&#38454;&#27573;&#12290;&#30446;&#21069;&#65292;&#36825;&#20010;&#36807;&#31243;&#26159;&#30456;&#24403;&#25163;&#21160;&#21270;&#30340;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#38382;&#39064;&#24320;&#21457;&#21608;&#26399;&#30340;&#26102;&#38388;&#28382;&#21518;&#12290;&#33258;&#21160;&#21270;&#27492;&#36807;&#31243;&#23558;&#22823;&#22823;&#25552;&#39640;&#25928;&#29575;&#65292;&#28982;&#32780;&#30446;&#21069;&#30340;&#25968;&#25454;&#38598;&#19981;&#21253;&#21547;&#36275;&#22815;&#30340;&#39044;&#27979;&#35797;&#20998;&#26512;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CamChoice&#65306;&#19968;&#20221;&#21253;&#21547;&#19981;&#21516;&#30446;&#26631;&#32423;&#21035;&#38382;&#39064;&#21644;&#30495;&#23454;&#20505;&#36873;&#31572;&#26696;&#36873;&#39033;&#20998;&#24067;&#30340;&#22810;&#39033;&#36873;&#25321;&#29702;&#35299;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20505;&#36873;&#20154;&#20998;&#24067;&#21305;&#37197;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#20960;&#31181;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;RACE++&#19978;&#35757;&#32451;&#30340;&#33258;&#21160;&#31995;&#32479;&#21487;&#20197;&#23454;&#29616;&#35813;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiple Choice examinations are a ubiquitous form of assessment that is used to measure the ability of candidates across various domains and tasks. Maintaining the quality of proposed questions is of great importance to test designers, and therefore newly proposed questions go through several pre-test evaluation stages before they can be deployed into real-world exams. This process is currently quite manual, which can lead to time lags in the question development cycle. Automating this process would lead to a large improvement in efficiency, however, current datasets do not contain sufficient pre-test analysis information. In this paper, we introduce CamChoice; a multiple-choice comprehension dataset with questions at different target levels, where questions have the true candidate selected options distributions. We introduce the task of candidate distribution matching, propose several evaluation metrics for the task, and demonstrate that automatic systems trained on RACE++ can be lev
&lt;/p&gt;</description></item><item><title>VisoGender&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#32844;&#19994;&#30456;&#20851;&#24615;&#21035;&#20559;&#35265;&#30340;&#25968;&#25454;&#38598;&#12290;&#30740;&#31350;&#26174;&#31034;&#65292;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#32570;&#20047;&#27491;&#30830;&#35299;&#26512;&#22797;&#26434;&#22330;&#26223;&#20013;&#24615;&#21035;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#29983;&#25104;&#23383;&#24149;&#30340;&#27169;&#22411;&#36890;&#24120;&#27604;&#31867;&#20284;CLIP&#30340;&#27169;&#22411;&#26356;&#31934;&#30830;&#21644;&#26356;&#23569;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2306.12424</link><description>&lt;p&gt;
VisoGender&#65306;&#19968;&#20221;&#29992;&#20110;&#35780;&#20272;&#22270;&#20687;-&#25991;&#26412;&#20195;&#35789;&#35299;&#26512;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
VisoGender: A dataset for benchmarking gender bias in image-text pronoun resolution. (arXiv:2306.12424v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12424
&lt;/p&gt;
&lt;p&gt;
VisoGender&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#32844;&#19994;&#30456;&#20851;&#24615;&#21035;&#20559;&#35265;&#30340;&#25968;&#25454;&#38598;&#12290;&#30740;&#31350;&#26174;&#31034;&#65292;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#32570;&#20047;&#27491;&#30830;&#35299;&#26512;&#22797;&#26434;&#22330;&#26223;&#20013;&#24615;&#21035;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#29983;&#25104;&#23383;&#24149;&#30340;&#27169;&#22411;&#36890;&#24120;&#27604;&#31867;&#20284;CLIP&#30340;&#27169;&#22411;&#26356;&#31934;&#30830;&#21644;&#26356;&#23569;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;VisoGender&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#32844;&#19994;&#30456;&#20851;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#21463;Winograd&#21644;Winogender&#27169;&#24335;&#30340;&#21551;&#21457;&#65292;&#20854;&#20013;&#27599;&#20010;&#22270;&#20687;&#37117;&#19982;&#21253;&#21547;&#22330;&#26223;&#20013;&#20027;&#35821;&#21644;&#23486;&#35821;&#20195;&#35789;&#20851;&#31995;&#30340;&#26631;&#39064;&#30456;&#20851;&#32852;&#12290;VisoGender&#22312;&#32844;&#19994;&#35282;&#33394;&#20013;&#24179;&#34913;&#20102;&#24615;&#21035;&#20195;&#34920;&#65292;&#25903;&#25345;&#20004;&#31181;&#20559;&#35265;&#35780;&#20272;&#26041;&#24335;&#65306;i&#65289;&#35299;&#20915;&#20559;&#35265;&#65292;&#25105;&#20204;&#35780;&#20272;&#30007;&#24615;&#21644;&#22899;&#24615;&#35299;&#20915;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#24046;&#24322;&#65307;ii&#65289;&#26816;&#32034;&#20559;&#35265;&#65292;&#25105;&#20204;&#27604;&#36739;&#22312;&#24615;&#21035;&#20013;&#31435;&#30340;&#25628;&#32034;&#26597;&#35810;&#20013;&#26816;&#32034;&#21040;&#30340;&#30007;&#24615;&#21644;&#22899;&#24615;&#19987;&#19994;&#20154;&#21592;&#30340;&#27604;&#20363;&#12290;&#25105;&#20204;&#23545;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#32570;&#20047;&#27491;&#30830;&#35299;&#26512;&#22797;&#26434;&#22330;&#26223;&#20013;&#24615;&#21035;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#34429;&#28982;&#24615;&#21035;&#20559;&#35265;&#30340;&#26041;&#21521;&#21644;&#24133;&#24230;&#21462;&#20915;&#20110;&#20219;&#21153;&#21644;&#35780;&#20272;&#30340;&#27169;&#22411;&#65292;&#20294;&#29983;&#25104;&#23383;&#24149;&#30340;&#27169;&#22411;&#36890;&#24120;&#27604;&#31867;&#20284;CLIP&#30340;&#27169;&#22411;&#26356;&#31934;&#30830;&#21644;&#26356;&#23569;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce VisoGender, a novel dataset for benchmarking gender bias in vision-language models. We focus on occupation-related gender biases, inspired by Winograd and Winogender schemas, where each image is associated with a caption containing a pronoun relationship of subjects and objects in the scene. VisoGender is balanced by gender representation in professional roles, supporting bias evaluation in two ways: i) resolution bias, where we evaluate the difference between gender resolution accuracies for men and women and ii) retrieval bias, where we compare ratios of male and female professionals retrieved for a gender-neutral search query. We benchmark several state-of-the-art vision-language models and find that they lack the reasoning abilities to correctly resolve gender in complex scenes. While the direction and magnitude of gender bias depends on the task and the model being evaluated, captioning models generally are more accurate and less biased than CLIP-like models. Dataset 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32418;&#38431;&#34892;&#21160;&#65292;&#36890;&#36807;&#20174;&#39640;&#23618;&#27425;&#12289;&#25277;&#35937;&#30340;&#35268;&#33539;&#20986;&#21457;&#26469;&#32771;&#34385;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#20197;&#25506;&#31350;&#27169;&#22411;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2306.09442</link><description>&lt;p&gt;
&#20174;&#38646;&#24320;&#22987;&#23454;&#29616;&#32418;&#38431;&#23545;&#25239;&#35821;&#35328;&#27169;&#22411;&#30340;&#25506;&#32034;&#19982;&#24314;&#31435;
&lt;/p&gt;
&lt;p&gt;
Explore, Establish, Exploit: Red Teaming Language Models from Scratch. (arXiv:2306.09442v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32418;&#38431;&#34892;&#21160;&#65292;&#36890;&#36807;&#20174;&#39640;&#23618;&#27425;&#12289;&#25277;&#35937;&#30340;&#35268;&#33539;&#20986;&#21457;&#26469;&#32771;&#34385;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#20197;&#25506;&#31350;&#27169;&#22411;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#33021;&#20250;&#20135;&#29983;&#26377;&#23475;&#36755;&#20986;&#65292;&#20363;&#22914;&#26377;&#27602;&#25110;&#19981;&#35802;&#23454;&#38472;&#36848;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#24341;&#20837;&#20102;&#24037;&#20855;&#20197;&#35843;&#26597;&#26377;&#23475;&#36755;&#20986;&#65292;&#20197;&#35782;&#21035;&#21644;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;&#34429;&#28982;&#36825;&#26159;&#30830;&#20445;&#35821;&#35328;&#27169;&#22411;&#23433;&#20840;&#30340;&#26377;&#20215;&#20540;&#27493;&#39588;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#29616;&#26377;&#30340;&#38024;&#23545;&#19981;&#24076;&#26395;&#30340;&#36755;&#20986;&#30340;&#20998;&#31867;&#22120;&#12290;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#21482;&#26377;&#39044;&#20808;&#30693;&#36947;&#26377;&#23475;&#34892;&#20026;&#31867;&#22411;&#30340;&#24773;&#20917;&#19979;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#36339;&#36807;&#20102;&#32418;&#38431;&#34892;&#21160;&#30340;&#26680;&#24515;&#25361;&#25112;&#65306;&#24320;&#21457;&#27169;&#22411;&#21487;&#33021;&#23637;&#31034;&#30340;&#34892;&#20026;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#24403;&#36825;&#26679;&#30340;&#20998;&#31867;&#22120;&#24050;&#32463;&#23384;&#22312;&#26102;&#65292;&#32418;&#38431;&#34892;&#21160;&#30340;&#36793;&#38469;&#20215;&#20540;&#26377;&#38480;&#65292;&#22240;&#20026;&#20998;&#31867;&#22120;&#21487;&#20197;&#29992;&#20110;&#36807;&#28388;&#35757;&#32451;&#25968;&#25454;&#25110;&#27169;&#22411;&#36755;&#20986;&#12290;&#26412;&#25991;&#32771;&#34385;&#22312;&#20551;&#35774;&#23545;&#25163;&#20174;&#39640;&#32423;&#12289;&#25277;&#35937;&#30340;&#19981;&#33391;&#34892;&#20026;&#35268;&#33539;&#20986;&#21457;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#32418;&#38431;&#34892;&#21160;&#12290;&#32418;&#38431;&#24212;&#35813;&#22312;&#31934;&#21270;/&#25193;&#23637;&#27492;&#35268;&#33539;&#30340;&#21516;&#26102;&#23545;&#25239;&#35813;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deploying Large language models (LLMs) can pose hazards from harmful outputs such as toxic or dishonest speech. Prior work has introduced tools that elicit harmful outputs in order to identify and mitigate these risks. While this is a valuable step toward securing language models, these approaches typically rely on a pre-existing classifier for undesired outputs. This limits their application to situations where the type of harmful behavior is known with precision beforehand. However, this skips a central challenge of red teaming: developing a contextual understanding of the behaviors that a model can exhibit. Furthermore, when such a classifier already exists, red teaming has limited marginal value because the classifier could simply be used to filter training data or model outputs. In this work, we consider red teaming under the assumption that the adversary is working from a high-level, abstract specification of undesired behavior. The red team is expected to refine/extend this spec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#27573;&#24490;&#29615;Transformer&#65288;SRformer&#65289;&#26469;&#20943;&#23569;&#35745;&#31639;/&#20869;&#23384;&#25104;&#26412;&#65292;&#24182;&#20351;&#29992;RAF&#23618;&#22788;&#29702;&#36328;&#27573;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#24207;&#21015;&#22788;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.16340</link><description>&lt;p&gt;
&#20998;&#27573;&#24490;&#29615;Transformer:&#19968;&#31181;&#39640;&#25928;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Segmented Recurrent Transformer: An Efficient Sequence-to-Sequence Model. (arXiv:2305.16340v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#27573;&#24490;&#29615;Transformer&#65288;SRformer&#65289;&#26469;&#20943;&#23569;&#35745;&#31639;/&#20869;&#23384;&#25104;&#26412;&#65292;&#24182;&#20351;&#29992;RAF&#23618;&#22788;&#29702;&#36328;&#27573;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#24207;&#21015;&#22788;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#35821;&#35328;&#21644;&#35270;&#35273;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#24207;&#21015;&#38271;&#24230;&#30340;&#22686;&#21152;&#65292;&#23427;&#20204;&#30340;&#35745;&#31639;&#25104;&#26412;&#21576;&#20108;&#27425;&#22686;&#38271;&#65292;&#20351;&#24471;&#23427;&#20204;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24212;&#29992;&#20013;&#20351;&#29992;&#25104;&#20026;&#19981;&#21487;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#23558;&#25972;&#20010;&#24207;&#21015;&#21010;&#20998;&#25104;&#33509;&#24178;&#27573;&#12290;&#28982;&#21518;&#20351;&#29992;&#20855;&#26377;&#24490;&#29615;&#32467;&#26500;&#30340;&#31070;&#32463;&#20803;&#26469;&#32858;&#21512;&#36328;&#27573;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#20855;&#26377;&#36739;&#20302;&#35745;&#31639;/&#20869;&#23384;&#25104;&#26412;&#30340;&#24207;&#21015;&#22788;&#29702;&#33021;&#21147;&#27169;&#22411;&#12290;&#20026;&#20102;&#39564;&#35777;&#36825;&#20010;&#24819;&#27861;&#65292;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#20351;&#29992;&#23616;&#37096;Attention&#26426;&#21046;&#23545;&#21333;&#20010;&#27573;&#30340;&#24433;&#21709;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#27573;&#24490;&#29615;Transformer&#65288;SRformer&#65289;&#65292;&#23427;&#23558;&#20998;&#27573;Attention&#21644;&#24490;&#29615;Attention&#30456;&#32467;&#21512;&#12290;&#23427;&#20351;&#29992;&#24490;&#29615;accumulate and fire&#65288;RAF&#65289;&#23618;&#22312;&#30456;&#37051;&#27573;&#20043;&#38388;&#22788;&#29702;&#20449;&#24687;&#12290;&#36890;&#36807;&#26356;&#26032;key&#30340;&#20135;&#21697;&#26469;&#34917;&#20607;&#20943;&#23569;Attention&#31383;&#21475;&#38271;&#24230;&#20135;&#29983;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have shown dominant performance across a range of domains including language and vision. However, their computational cost grows quadratically with the sequence length, making their usage prohibitive for resource-constrained applications. To counter this, our approach is to divide the whole sequence into segments. The information across segments can then be aggregated using neurons with recurrence leveraging their inherent memory. Such an approach leads to models with sequential processing capability at a lower computation/memory cost. To investigate this idea, first, we examine the effects of using local attention mechanism on the individual segments. Then we propose a segmented recurrent transformer (SRformer) that combines segmented attention with recurrent attention. It uses recurrent accumulate and fire (RAF) layers to process information between consecutive segments. The loss caused by reducing the attention window length is compensated by updating the product of key
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;JEEBench&#65292;&#19968;&#20010;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;&#36890;&#36807;&#35780;&#20272;&#21508;&#31181;&#27169;&#22411;&#65292;&#32467;&#26524;&#26174;&#31034;&#30446;&#21069;&#26368;&#22909;&#30340;&#27169;&#22411;&#22312;&#35299;&#20915;&#38382;&#39064;&#26102;&#23384;&#22312;&#20195;&#25968;&#25805;&#20316;&#38169;&#35823;&#12289;&#25277;&#35937;&#27010;&#24565;&#36716;&#21270;&#19981;&#20934;&#30830;&#21644;&#38590;&#20197;&#26816;&#32034;&#30456;&#20851;&#27010;&#24565;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.15074</link><description>&lt;p&gt;
LLM&#20204;&#36827;&#27493;&#21040;&#20102;&#20160;&#20040;&#31243;&#24230;&#65311;&#19968;&#20010;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#35299;&#20915;&#22522;&#20934;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models. (arXiv:2305.15074v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15074
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;JEEBench&#65292;&#19968;&#20010;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;&#36890;&#36807;&#35780;&#20272;&#21508;&#31181;&#27169;&#22411;&#65292;&#32467;&#26524;&#26174;&#31034;&#30446;&#21069;&#26368;&#22909;&#30340;&#27169;&#22411;&#22312;&#35299;&#20915;&#38382;&#39064;&#26102;&#23384;&#22312;&#20195;&#25968;&#25805;&#20316;&#38169;&#35823;&#12289;&#25277;&#35937;&#27010;&#24565;&#36716;&#21270;&#19981;&#20934;&#30830;&#21644;&#38590;&#20197;&#26816;&#32034;&#30456;&#20851;&#27010;&#24565;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#37324;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29616;&#26377;&#30340;&#25512;&#29702;&#22522;&#20934;&#19978;&#30340;&#24615;&#33021;&#26174;&#33879;&#25552;&#39640;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;JEEBench&#65292;&#19968;&#20010;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;&#25105;&#20204;&#20174;&#39640;&#31454;&#20105;&#30340;&#21360;&#24230;&#29702;&#24037;&#23398;&#38498;&#65288;IIT&#65289;JEE-Advanced&#32771;&#35797;&#20013;&#31934;&#36873;&#20986;&#20102;515&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#39044;&#24037;&#31243;&#25968;&#23398;&#12289;&#29289;&#29702;&#21644;&#21270;&#23398;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#22522;&#20934;&#20013;&#65292;&#38271;&#26399;&#25512;&#29702;&#21644;&#28145;&#20837;&#39046;&#22495;&#30693;&#35782;&#30340;&#36816;&#29992;&#23545;&#38382;&#39064;&#30340;&#35299;&#20915;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#23545;&#21508;&#31181;&#24320;&#28304;&#21644;&#19987;&#26377;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#21363;&#20351;&#20351;&#29992;&#20102;&#33258;&#19968;&#33268;&#24615;&#12289;&#33258;&#25105;&#23436;&#21892;&#21644;&#24605;&#32500;&#38142;&#25552;&#31034;&#31561;&#25216;&#26415;&#65292;&#26368;&#39640;&#24615;&#33021;&#20063;&#19981;&#21040;40\%&#12290;&#26368;&#22909;&#30340;&#27169;&#22411;GPT-4&#30340;&#20856;&#22411;&#22833;&#36133;&#27169;&#24335;&#21253;&#25324;&#20195;&#25968;&#25805;&#20316;&#38169;&#35823;&#12289;&#23558;&#25277;&#35937;&#27010;&#24565;&#20934;&#30830;&#22320;&#36716;&#21270;&#20026;&#25968;&#23398;&#26041;&#31243;&#20197;&#21450;&#26080;&#27861;&#26816;&#32034;&#30456;&#20851;&#30340;&#39046;&#22495;&#29305;&#23450;&#27010;&#24565;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#65292;&#20165;&#20165;&#36890;&#36807;&#36755;&#20837;&#25552;&#31034;&#19981;&#33021;&#35753;&#27169;&#22411;&#25104;&#21151;&#35299;&#20915;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The performance of large language models (LLMs) on existing reasoning benchmarks has significantly improved over the past years. In response, we present JEEBench, a considerably more challenging benchmark dataset for evaluating the problem solving abilities of LLMs. We curate 515 challenging pre-engineering mathematics, physics and chemistry problems from the highly competitive IIT JEE-Advanced exam. Long-horizon reasoning on top of deep in-domain knowledge is essential for solving problems in this benchmark. Our evaluation on various open-source and proprietary models reveals that the highest performance, even after using techniques like self-consistency, self-refinement and chain-of-thought prompting, is less than 40\%. The typical failure modes of GPT-4, the best model, are errors in algebraic manipulation, difficulty in grounding abstract concepts into mathematical equations accurately and failure in retrieving relevant domain-specific concepts. We also observe that by mere prompti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;UniChart&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22270;&#34920;&#29702;&#35299;&#21644;&#25512;&#29702;&#30340;&#36890;&#29992;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;UniChart&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#25324;&#21508;&#31181;&#19981;&#21516;&#20027;&#39064;&#21644;&#35270;&#35273;&#39118;&#26684;&#30340;&#22823;&#37327;&#22270;&#34920;&#35821;&#26009;&#24211;&#65292;&#28982;&#21518;&#20351;&#29992;&#22270;&#34920;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#26469;&#32534;&#30721;&#22270;&#34920;&#65292;&#24182;&#22312;&#20960;&#20010;&#22270;&#34920;&#29702;&#35299;&#21644;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2305.14761</link><description>&lt;p&gt;
UniChart&#65306;&#38754;&#21521;&#22270;&#34920;&#29702;&#35299;&#21644;&#25512;&#29702;&#30340;&#36890;&#29992;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
UniChart: A Universal Vision-language Pretrained Model for Chart Comprehension and Reasoning. (arXiv:2305.14761v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;UniChart&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22270;&#34920;&#29702;&#35299;&#21644;&#25512;&#29702;&#30340;&#36890;&#29992;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;UniChart&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#25324;&#21508;&#31181;&#19981;&#21516;&#20027;&#39064;&#21644;&#35270;&#35273;&#39118;&#26684;&#30340;&#22823;&#37327;&#22270;&#34920;&#35821;&#26009;&#24211;&#65292;&#28982;&#21518;&#20351;&#29992;&#22270;&#34920;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#26469;&#32534;&#30721;&#22270;&#34920;&#65292;&#24182;&#22312;&#20960;&#20010;&#22270;&#34920;&#29702;&#35299;&#21644;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#22312;&#25968;&#25454;&#20998;&#26512;&#12289;&#21487;&#35270;&#21270;&#37325;&#35201;&#35265;&#35299;&#21644;&#22238;&#31572;&#25968;&#25454;&#30340;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#26041;&#38754;&#38750;&#24120;&#27969;&#34892;&#12290;&#20026;&#20102;&#26041;&#20415;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#22522;&#20110;&#22270;&#34920;&#30340;&#25968;&#25454;&#20998;&#26512;&#65292;&#26368;&#36817;&#24341;&#20837;&#20102;&#20960;&#20010;&#19979;&#28216;&#20219;&#21153;&#65292;&#20363;&#22914;&#22270;&#34920;&#38382;&#31572;&#21644;&#22270;&#34920;&#24635;&#32467;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#35299;&#20915;&#36825;&#20123;&#20219;&#21153;&#30340;&#26041;&#27861;&#37117;&#20351;&#29992;&#35821;&#35328;&#25110;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#65292;&#32780;&#19981;&#35797;&#22270;&#26126;&#30830;&#24314;&#27169;&#22270;&#34920;&#30340;&#32467;&#26500;&#65288;&#20363;&#22914;&#65292;&#22914;&#20309;&#35270;&#35273;&#32534;&#30721;&#25968;&#25454;&#20197;&#21450;&#22914;&#20309;&#23558;&#22270;&#34920;&#20803;&#32032;&#30456;&#20114;&#20851;&#32852;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#25324;&#21508;&#31181;&#19981;&#21516;&#20027;&#39064;&#21644;&#35270;&#35273;&#39118;&#26684;&#30340;&#22823;&#37327;&#22270;&#34920;&#35821;&#26009;&#24211;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;UniChart&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22270;&#34920;&#29702;&#35299;&#21644;&#25512;&#29702;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;UniChart&#23545;&#22270;&#34920;&#30340;&#30456;&#20851;&#25991;&#26412;&#12289;&#25968;&#25454;&#21644;&#35270;&#35273;&#20803;&#32032;&#36827;&#34892;&#32534;&#30721;&#65292;&#28982;&#21518;&#20351;&#29992;&#22522;&#20110;&#22270;&#34920;&#30340;&#25991;&#26412;&#35299;&#30721;&#22120;&#20197;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#39044;&#26399;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#38754;&#21521;&#22270;&#34920;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#21253;&#25324;&#65306;&#65288;i&#65289;&#20302;&#23618;&#27425;&#35270;&#35273;&#32534;&#30721;&#39044;&#27979;&#65292;&#65288;ii&#65289;&#22270;&#34920;&#20803;&#32032;&#20851;&#31995;&#39044;&#27979;&#21644;&#65288;iii&#65289;&#22270;&#34920;&#38382;&#39064;&#22238;&#31572;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#26174;&#31034;&#65292;UniChart&#22312;&#20960;&#20010;&#22270;&#34920;&#29702;&#35299;&#21644;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#20110;&#24378;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Charts are very popular for analyzing data, visualizing key insights and answering complex reasoning questions about data. To facilitate chart-based data analysis using natural language, several downstream tasks have been introduced recently such as chart question answering and chart summarization. However, most of the methods that solve these tasks use pretraining on language or vision-language tasks that do not attempt to explicitly model the structure of the charts (e.g., how data is visually encoded and how chart elements are related to each other). To address this, we first build a large corpus of charts covering a wide variety of topics and visual styles. We then present UniChart, a pretrained model for chart comprehension and reasoning. UniChart encodes the relevant text, data, and visual elements of charts and then uses a chart-grounded text decoder to generate the expected output in natural language. We propose several chart-specific pretraining tasks that include: (i) low-lev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#35757;&#32451;&#31639;&#27861;Left-over Lunch RL &#65288;LoL-RL&#65289;&#65292;&#20351;&#29992;&#31163;&#32447;&#31574;&#30053;&#26799;&#24230;&#23398;&#20064;&#20219;&#20309;&#24207;&#21015;&#21040;&#24207;&#21015;&#25968;&#25454;&#65292;&#20174;&#32780;&#23454;&#29616;&#20248;&#21270;LM&#25928;&#29992;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.14718</link><description>&lt;p&gt;
&#21033;&#29992;&#22522;&#20110;&#20248;&#21183;&#30340;&#31163;&#32447;&#31574;&#30053;&#26799;&#24230;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving Language Models with Advantage-based Offline Policy Gradients. (arXiv:2305.14718v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14718
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#35757;&#32451;&#31639;&#27861;Left-over Lunch RL &#65288;LoL-RL&#65289;&#65292;&#20351;&#29992;&#31163;&#32447;&#31574;&#30053;&#26799;&#24230;&#23398;&#20064;&#20219;&#20309;&#24207;&#21015;&#21040;&#24207;&#21015;&#25968;&#25454;&#65292;&#20174;&#32780;&#23454;&#29616;&#20248;&#21270;LM&#25928;&#29992;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26681;&#25454;&#29992;&#25143;&#23450;&#20041;&#30340;&#36136;&#37327;&#25110;&#39118;&#26684;&#38480;&#21046;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20856;&#22411;&#30340;&#26041;&#27861;&#21253;&#25324;&#23398;&#20064;&#39069;&#22806;&#30340;&#20154;&#24037;&#32534;&#20889;&#25968;&#25454;&#65292;&#20351;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#36807;&#28388;&#8220;&#20302;&#36136;&#37327;&#8221;&#25968;&#25454;&#21644;/&#25110;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#20307;&#21453;&#39304;&#65288;RLHF&#65289;&#12290;&#28982;&#32780;&#65292;&#36807;&#28388;&#20250;&#21024;&#38500;&#26377;&#20215;&#20540;&#30340;&#35757;&#32451;&#20449;&#21495;&#65292;&#32780;&#25968;&#25454;&#25910;&#38598;&#21644;RLHF&#19981;&#26029;&#38656;&#35201;&#39069;&#22806;&#30340;&#20154;&#24037;&#32534;&#20889;&#25110;LM&#25506;&#32034;&#25968;&#25454;&#65292;&#36825;&#21487;&#33021;&#25104;&#26412;&#39640;&#12290;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#26159;&#8220;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;RL&#26469;&#20248;&#21270;&#29616;&#26377;&#30340;&#20247;&#21253;&#21644;&#20114;&#32852;&#32593;&#25968;&#25454;&#19978;&#30340;LM&#25928;&#29992;&#21527;&#65311;&#8221;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21097;&#20313;&#21320;&#39184;&#24378;&#21270;&#23398;&#20064;&#65288;LoL-RL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#20351;&#29992;&#31163;&#32447;&#31574;&#30053;&#26799;&#24230;&#26469;&#23398;&#20064;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20316;&#20026;1&#27493;RL&#28216;&#25103;&#12290; LoL-RL&#21487;&#20197;&#24494;&#35843;LM&#65292;&#20197;&#20248;&#21270;&#20219;&#24847;&#22522;&#20110;&#20998;&#31867;&#22120;&#25110;&#20154;&#23450;&#20041;&#30340;&#25928;&#29992;&#20989;&#25968;&#30340;&#20219;&#20309;&#24207;&#21015;&#21040;&#24207;&#21015;&#25968;&#25454;&#12290;&#20351;&#29992;&#19981;&#21516;&#22823;&#23567;&#27169;&#22411;&#30340;&#20116;&#20010;&#19981;&#21516;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#30340;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Improving language model generations according to some user-defined quality or style constraints is challenging. Typical approaches include learning on additional human-written data, filtering ``low-quality'' data using heuristics and/or using reinforcement learning with human feedback (RLHF). However, filtering can remove valuable training signals, whereas data collection and RLHF constantly require additional human-written or LM exploration data which can be costly to obtain. A natural question to ask is ``Can we leverage RL to optimize LM utility on existing crowd-sourced and internet data?''  To this end, we present Left-over Lunch RL (LoL-RL), a simple training algorithm that uses offline policy gradients for learning language generation tasks as a 1-step RL game. LoL-RL can finetune LMs to optimize arbitrary classifier-based or human-defined utility functions on any sequence-to-sequence data. Experiments with five different language generation tasks using models of varying sizes 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;FACTSCORE&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23558;&#29983;&#25104;&#30340;&#20869;&#23481;&#20998;&#35299;&#20026;&#21407;&#23376;&#20107;&#23454;&#65292;&#24182;&#35745;&#31639;&#34987;&#21487;&#38752;&#30693;&#35782;&#28304;&#25903;&#25345;&#30340;&#27604;&#20363;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#38271;&#25991;&#26412;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#20154;&#24037;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#21830;&#19994;&#35821;&#35328;&#27169;&#22411;&#20013;&#20165;&#26377;58%&#30340;ChatGPT&#20256;&#35760;&#36798;&#21040;&#20102;&#39640;&#27700;&#24179;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#27169;&#22411;&#65292;&#21033;&#29992;&#26816;&#32034;&#21644;&#24378;&#35821;&#35328;&#27169;&#22411;&#20272;&#35745;FACTSCORE&#65292;&#35823;&#24046;&#29575;&#20302;&#20110;2%&#12290;</title><link>http://arxiv.org/abs/2305.14251</link><description>&lt;p&gt;
FActScore: &#23545;&#38271;&#25991;&#26412;&#29983;&#25104;&#20013;&#20107;&#23454;&#20934;&#30830;&#24615;&#30340;&#32454;&#31890;&#24230;&#21407;&#23376;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation. (arXiv:2305.14251v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;FACTSCORE&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23558;&#29983;&#25104;&#30340;&#20869;&#23481;&#20998;&#35299;&#20026;&#21407;&#23376;&#20107;&#23454;&#65292;&#24182;&#35745;&#31639;&#34987;&#21487;&#38752;&#30693;&#35782;&#28304;&#25903;&#25345;&#30340;&#27604;&#20363;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#38271;&#25991;&#26412;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#20154;&#24037;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#21830;&#19994;&#35821;&#35328;&#27169;&#22411;&#20013;&#20165;&#26377;58%&#30340;ChatGPT&#20256;&#35760;&#36798;&#21040;&#20102;&#39640;&#27700;&#24179;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#27169;&#22411;&#65292;&#21033;&#29992;&#26816;&#32034;&#21644;&#24378;&#35821;&#35328;&#27169;&#22411;&#20272;&#35745;FACTSCORE&#65292;&#35823;&#24046;&#29575;&#20302;&#20110;2%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#38271;&#25991;&#26412;&#30340;&#20107;&#23454;&#24615;&#26159;&#19968;&#39033;&#26840;&#25163;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#65288;1&#65289;&#29983;&#25104;&#30340;&#20869;&#23481;&#36890;&#24120;&#21253;&#21547;&#25903;&#25345;&#21644;&#19981;&#25903;&#25345;&#30340;&#20449;&#24687;&#65292;&#20351;&#24471;&#20108;&#20803;&#21028;&#26029;&#36136;&#37327;&#19981;&#36275;&#65292;&#65288;2&#65289;&#20154;&#24037;&#35780;&#20272;&#32791;&#26102;&#19988;&#25104;&#26412;&#39640;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;FACTSCORE&#65292;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#23427;&#23558;&#29983;&#25104;&#20869;&#23481;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#21407;&#23376;&#20107;&#23454;&#65292;&#24182;&#35745;&#31639;&#34987;&#21487;&#38752;&#30693;&#35782;&#28304;&#25903;&#25345;&#30340;&#21407;&#23376;&#20107;&#23454;&#30340;&#30334;&#20998;&#27604;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#20154;&#24037;&#35780;&#20272;&#65292;&#24471;&#20986;&#20102;&#20960;&#20010;&#26368;&#20808;&#36827;&#21830;&#19994;&#35821;&#35328;&#27169;&#22411;&#65288;InstructGPT&#12289;ChatGPT&#21644;&#22686;&#24378;&#25552;&#21462;PerplexityAI&#65289;&#29983;&#25104;&#30340;&#20154;&#29289;&#20256;&#35760;&#30340;FACTSCORE&#65292;&#24182;&#25253;&#36947;&#20102;&#26032;&#30340;&#20998;&#26512;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#23545;&#20110;&#36825;&#26679;&#30340;&#32454;&#31890;&#24230;&#35780;&#20998;&#30340;&#38656;&#27714;&#65288;&#20363;&#22914;&#65292;ChatGPT&#21482;&#36798;&#21040;58%&#65289;&#12290;&#30001;&#20110;&#20154;&#24037;&#35780;&#20272;&#36153;&#26102;&#36153;&#21147;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#26816;&#32034;&#21644;&#24378;&#35821;&#35328;&#27169;&#22411;&#20272;&#35745;FACTSCORE&#30340;&#33258;&#21160;&#21270;&#27169;&#22411;&#65292;&#35823;&#24046;&#29575;&#19981;&#36229;&#36807;2%&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating the factuality of long-form text generated by large language models (LMs) is non-trivial because (1) generations often contain a mixture of supported and unsupported pieces of information, making binary judgments of quality inadequate, and (2) human evaluation is time-consuming and costly. In this paper, we introduce FACTSCORE, a new evaluation that breaks a generation into a series of atomic facts and computes the percentage of atomic facts supported by a reliable knowledge source. We conduct an extensive human evaluation to obtain FACTSCOREs of people biographies generated by several state-of-the-art commercial LMs -- InstructGPT, ChatGPT, and the retrieval-augmented PerplexityAI -- and report new analysis demonstrating the need for such a fine-grained score (e.g., ChatGPT only achieves 58%). Since human evaluation is costly, we also introduce an automated model that estimates FACTSCORE using retrieval and a strong language model, with less than a 2% error rate. Finally, w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#32534;&#36753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#12289;&#26041;&#27861;&#21644;&#26426;&#20250;&#65292;&#25552;&#20379;&#20102;&#20219;&#21153;&#23450;&#20041;&#21644;&#25361;&#25112;&#30340;&#27010;&#36848;&#12289;&#20808;&#36827;&#26041;&#27861;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#20197;&#21450;&#26500;&#24314;&#20102;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#36825;&#20123;&#32467;&#26524;&#26377;&#21161;&#20110;&#25913;&#36827;LLMs&#30340;&#32534;&#36753;&#25216;&#26415;&#65292;&#25552;&#39640;&#20854;&#25928;&#26524;&#21644;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13172</link><description>&lt;p&gt;
&#32534;&#36753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#38382;&#39064;&#12289;&#26041;&#27861;&#21644;&#26426;&#20250;
&lt;/p&gt;
&lt;p&gt;
Editing Large Language Models: Problems, Methods, and Opportunities. (arXiv:2305.13172v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#32534;&#36753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#12289;&#26041;&#27861;&#21644;&#26426;&#20250;&#65292;&#25552;&#20379;&#20102;&#20219;&#21153;&#23450;&#20041;&#21644;&#25361;&#25112;&#30340;&#27010;&#36848;&#12289;&#20808;&#36827;&#26041;&#27861;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#20197;&#21450;&#26500;&#24314;&#20102;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#36825;&#20123;&#32467;&#26524;&#26377;&#21161;&#20110;&#25913;&#36827;LLMs&#30340;&#32534;&#36753;&#25216;&#26415;&#65292;&#25552;&#39640;&#20854;&#25928;&#26524;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33021;&#22815;&#35757;&#32451;&#20986;&#34920;&#29616;&#20248;&#31168;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20294;&#20854;&#20445;&#25345;&#30456;&#20851;&#24615;&#21644;&#32416;&#27491;&#38169;&#35823;&#30340;&#26041;&#27861;&#20173;&#28982;&#38590;&#20197;&#30830;&#23450;&#12290;&#20026;&#27492;&#65292;&#26368;&#36817;&#20960;&#24180;&#20986;&#29616;&#20102;&#35768;&#22810;&#32534;&#36753;LLMs&#30340;&#25216;&#26415;&#65292;&#20854;&#30446;&#26631;&#26159;&#22312;&#29305;&#23450;&#39046;&#22495;&#20869;&#39640;&#25928;&#22320;&#25913;&#21464;LLMs&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#19981;&#23545;&#20854;&#20182;&#36755;&#20837;&#30340;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#19982;LLMs&#27169;&#22411;&#32534;&#36753;&#30456;&#20851;&#30340;&#38382;&#39064;&#12289;&#26041;&#27861;&#21644;&#26426;&#20250;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#27169;&#22411;&#32534;&#36753;&#20219;&#21153;&#23450;&#20041;&#21644;&#30456;&#20851;&#25361;&#25112;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#20197;&#21450;&#23545;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30340;&#28145;&#20837;&#23454;&#35777;&#20998;&#26512;&#12290;&#25105;&#20204;&#36824;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20197;&#20419;&#36827;&#26356;&#24378;&#22823;&#30340;&#35780;&#20272;&#65292;&#24182;&#25351;&#20986;&#29616;&#26377;&#25216;&#26415;&#22266;&#26377;&#30340;&#25345;&#20037;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20026;&#27599;&#31181;&#32534;&#36753;&#25216;&#26415;&#30340;&#25928;&#26524;&#21644;&#21487;&#34892;&#24615;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#20174;&#32780;&#24110;&#21161;&#31038;&#21306;&#22312;LLMs&#30340;&#31649;&#29702;&#20013;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the ability to train capable LLMs, the methodology for maintaining their relevancy and rectifying errors remains elusive. To this end, the past few years have witnessed a surge in techniques for editing LLMs, the objective of which is to efficiently alter the behavior of LLMs within a specific domain without negatively impacting performance across other inputs. This paper embarks on a deep exploration of the problems, methods, and opportunities related to model editing for LLMs. In particular, we provide an exhaustive overview of the task definition and challenges associated with model editing, along with an in-depth empirical analysis of the most progressive methods currently at our disposal. We also build a new benchmark dataset to facilitate a more robust evaluation and pinpoint enduring issues intrinsic to existing techniques. Our objective is to provide valuable insights into the effectiveness and feasibility of each editing technique, thereby assisting the community in ma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;$\lambda$-\textit{equitune}&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;\textit{&#37325;&#35201;&#24615;&#26435;&#37325;}$\lambda$&#23545;&#29305;&#24449;&#36827;&#34892;&#24179;&#22343;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#31561;&#21464;&#23567;&#26679;&#26412;&#23398;&#20064;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.09900</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#31561;&#21464;&#23567;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Equivariant Few-Shot Learning from Pretrained Models. (arXiv:2305.09900v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;$\lambda$-\textit{equitune}&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;\textit{&#37325;&#35201;&#24615;&#26435;&#37325;}$\lambda$&#23545;&#29305;&#24449;&#36827;&#34892;&#24179;&#22343;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#31561;&#21464;&#23567;&#26679;&#26412;&#23398;&#20064;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;&#26159;&#22522;&#30784;&#27169;&#22411;&#22312;&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#21462;&#24471;&#25104;&#21151;&#30340;&#20851;&#38190;&#12290;&#26368;&#36817;&#30340;&#20316;&#21697; \cite{basu2022equi} &#21644; \cite{kaba2022equivariance} &#20998;&#21035;&#25552;&#20986;&#20102;&#20351;&#29992;&#20174;&#32676;&#21464;&#25442;&#36755;&#20837;&#24471;&#21040;&#30340;&#29305;&#24449;&#30340;&#32676;&#24179;&#22343;&#20540;&#65288;\textit{equitune}&#65289;&#21644;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#26469;&#20174;&#19981;&#31561;&#21464;&#30340;&#31070;&#32463;&#32593;&#32476;&#33719;&#21462;&#31561;&#21464;&#36755;&#20986;&#12290;&#34429;&#28982; \cite{kaba2022equivariance} &#21482;&#20851;&#27880;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#21363;&#20351;&#22312;&#33391;&#22909;&#30340;&#24494;&#35843;&#32467;&#26524;&#19979;&#65292;\textit{equitune} &#22312;&#31561;&#21464;&#38646;&#26679;&#26412;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#22240;&#20026;&#39044;&#35757;&#32451;&#27169;&#22411;&#20026;&#26576;&#20123;&#36716;&#25442;&#25552;&#20379;&#20102;&#26356;&#39640;&#36136;&#37327;&#30340;&#29305;&#24449;&#65292;&#32780;&#23545;&#20854;&#36827;&#34892;&#31616;&#21333;&#24179;&#22343;&#20250;&#20135;&#29983;&#19981;&#33391;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;\textit{&#37325;&#35201;&#24615;&#26435;&#37325;}$\lambda$&#23545;&#29305;&#24449;&#36827;&#34892;&#24179;&#22343;&#30340;$\lambda$-\textit{equitune} &#26041;&#27861;&#12290;&#36825;&#20123;&#26435;&#37325;&#26159;&#20351;&#29992;&#19968;&#20010;&#23567;&#22411;&#31070;&#32463;&#32593;&#32476;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#65292;&#20174;&#32780;&#23548;&#33268;&#20986;&#33394;&#30340;&#38646;&#26679;&#26412;&#21644;&#24494;&#35843;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient transfer learning algorithms are key to the success of foundation models on diverse downstream tasks even with limited data. Recent works of \cite{basu2022equi} and \cite{kaba2022equivariance} propose group averaging (\textit{equitune}) and optimization-based methods, respectively, over features from group-transformed inputs to obtain equivariant outputs from non-equivariant neural networks. While \cite{kaba2022equivariance} are only concerned with training from scratch, we find that equitune performs poorly on equivariant zero-shot tasks despite good finetuning results. We hypothesize that this is because pretrained models provide better quality features for certain transformations than others and simply averaging them is deleterious. Hence, we propose $\lambda$-\textit{equitune} that averages the features using \textit{importance weights}, $\lambda$s. These weights are learned directly from the data using a small neural network, leading to excellent zero-shot and finetuned 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30693;&#35782;&#21453;&#24605;&#30340;&#26032;&#33539;&#24335;&#65292;&#26088;&#22312;&#24110;&#21161;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#24050;&#32463;&#32534;&#30721;&#22312;&#20854;&#39044;&#35757;&#32451;&#21442;&#25968;&#20013;&#30340;&#30456;&#20851;&#28508;&#22312;&#30693;&#35782;&#65292;&#32780;&#19981;&#38656;&#35201;&#20174;&#22806;&#37096;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#22312;&#27169;&#22411;&#20013;&#28155;&#21152;&#25552;&#31034;&#65292;&#24182;&#23558;&#30456;&#20851;&#30693;&#35782;&#27880;&#20837;&#27169;&#22411;&#36827;&#34892;&#25972;&#21512;&#65292;&#21462;&#24471;&#20102;&#22312;&#24120;&#35782;&#25512;&#29702;&#20219;&#21153;&#21644;GLUE&#22522;&#20934;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.08732</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#21453;&#24605;
&lt;/p&gt;
&lt;p&gt;
Knowledge Rumination for Pre-trained Language Models. (arXiv:2305.08732v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30693;&#35782;&#21453;&#24605;&#30340;&#26032;&#33539;&#24335;&#65292;&#26088;&#22312;&#24110;&#21161;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#24050;&#32463;&#32534;&#30721;&#22312;&#20854;&#39044;&#35757;&#32451;&#21442;&#25968;&#20013;&#30340;&#30456;&#20851;&#28508;&#22312;&#30693;&#35782;&#65292;&#32780;&#19981;&#38656;&#35201;&#20174;&#22806;&#37096;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#22312;&#27169;&#22411;&#20013;&#28155;&#21152;&#25552;&#31034;&#65292;&#24182;&#23558;&#30456;&#20851;&#30693;&#35782;&#27880;&#20837;&#27169;&#22411;&#36827;&#34892;&#25972;&#21512;&#65292;&#21462;&#24471;&#20102;&#22312;&#24120;&#35782;&#25512;&#29702;&#20219;&#21153;&#21644;GLUE&#22522;&#20934;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#26222;&#36890;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21333;&#29420;&#22788;&#29702;&#30693;&#35782;&#23494;&#38598;&#22411;NLP&#20219;&#21153;&#30340;&#33021;&#21147;&#19981;&#36275;&#65292;&#22240;&#27492;&#65292;&#19968;&#20123;&#24037;&#20316;&#23581;&#35797;&#23558;&#22806;&#37096;&#30693;&#35782;&#38598;&#25104;&#21040;PLMs&#20013;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26377;&#30528;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#65292;&#20294;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#35266;&#23519;&#21040;&#65292;PLM&#21487;&#33021;&#24050;&#32463;&#22312;&#20854;&#39044;&#35757;&#32451;&#21442;&#25968;&#20013;&#32534;&#30721;&#20102;&#20016;&#23500;&#30340;&#30693;&#35782;&#65292;&#20294;&#22312;&#24212;&#29992;&#21040;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#26102;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#23427;&#20204;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30693;&#35782;&#21453;&#24605;&#30340;&#26032;&#33539;&#24335;&#65292;&#20197;&#24110;&#21161;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#30456;&#20851;&#30340;&#28508;&#22312;&#30693;&#35782;&#65292;&#32780;&#19981;&#38656;&#35201;&#20174;&#22806;&#37096;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#23427;&#20204;&#12290;&#36890;&#36807;&#31616;&#21333;&#22320;&#22312;PLMs&#20013;&#28155;&#21152;&#19968;&#20010;&#22914;&#8220;&#25454;&#25105;&#25152;&#30693;&#8221;&#30340;&#25552;&#31034;&#65292;&#25105;&#20204;&#35797;&#22270;&#22238;&#39038;&#30456;&#20851;&#30340;&#28508;&#22312;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#27880;&#20837;&#27169;&#22411;&#20197;&#36827;&#34892;&#30693;&#35782;&#25972;&#21512;&#12290;&#25105;&#20204;&#23558;&#25552;&#20986;&#30340;&#30693;&#35782;&#21453;&#24605;&#24212;&#29992;&#20110;&#21508;&#31181;&#35821;&#35328;&#27169;&#22411;&#65292;&#21253;&#25324;RoBERTa&#12289;DeBERTa&#21644;GPT-3&#12290;&#22312;&#20845;&#20010;&#24120;&#35782;&#25512;&#29702;&#20219;&#21153;&#21644;GLUE&#22522;&#20934;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;.....
&lt;/p&gt;
&lt;p&gt;
Previous studies have revealed that vanilla pre-trained language models (PLMs) lack the capacity to handle knowledge-intensive NLP tasks alone; thus, several works have attempted to integrate external knowledge into PLMs. However, despite the promising outcome, we empirically observe that PLMs may have already encoded rich knowledge in their pre-trained parameters but fail to fully utilize them when applying them to knowledge-intensive tasks. In this paper, we propose a new paradigm dubbed Knowledge Rumination to help the pre-trained language model utilize that related latent knowledge without retrieving it from the external corpus. By simply adding a prompt like "As far as I know" to the PLMs, we try to review related latent knowledge and inject them back into the model for knowledge consolidation. We apply the proposed knowledge rumination to various language models, including RoBERTa, DeBERTa, and GPT-3. Experimental results on six commonsense reasoning tasks and GLUE benchmarks dem
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;ChatGPT&#21644;GPT-4&#22312;&#37329;&#34701;&#25991;&#26412;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#23427;&#20204;&#22312;&#25968;&#20540;&#25512;&#29702;&#19978;&#34920;&#29616;&#20986;&#33394;&#20294;&#22312;&#38656;&#35201;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;</title><link>http://arxiv.org/abs/2305.05862</link><description>&lt;p&gt;
ChatGPT&#21644;GPT-4&#26159;&#21542;&#26159;&#37329;&#34701;&#25991;&#26412;&#20998;&#26512;&#30340;&#36890;&#29992;&#27714;&#35299;&#22120;&#65311;&#23545;&#20960;&#31181;&#20856;&#22411;&#20219;&#21153;&#36827;&#34892;&#26816;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are ChatGPT and GPT-4 General-Purpose Solvers for Financial Text Analytics? An Examination on Several Typical Tasks. (arXiv:2305.05862v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;ChatGPT&#21644;GPT-4&#22312;&#37329;&#34701;&#25991;&#26412;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#23427;&#20204;&#22312;&#25968;&#20540;&#25512;&#29702;&#19978;&#34920;&#29616;&#20986;&#33394;&#20294;&#22312;&#38656;&#35201;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;ChatGPT&#21644;GPT-4&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#23545;&#35805;&#22238;&#24212;&#12290;&#23613;&#31649;ChatGPT&#21644;GPT-4&#22312;&#36890;&#29992;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#32463;&#36807;&#20102;&#24191;&#27867;&#30340;&#27979;&#35797;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#36824;&#27809;&#26377;&#23545;&#37329;&#34701;&#35821;&#26009;&#24211;&#36827;&#34892;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#22312;&#38646;&#26679;&#26412;&#25110;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#32771;&#23519;ChatGPT&#21644;GPT-4&#20316;&#20026;&#20856;&#22411;&#37329;&#34701;&#25991;&#26412;&#20998;&#26512;&#38382;&#39064;&#27714;&#35299;&#22120;&#30340;&#28508;&#21147;&#26469;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#20116;&#20010;&#19981;&#21516;&#30340;&#37329;&#34701;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22235;&#39033;&#20195;&#34920;&#24615;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#21021;&#27493;&#30740;&#31350;&#34920;&#26126;&#65292;ChatGPT&#21644;GPT-4&#22312;&#38656;&#35201;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#37329;&#34701;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#21644;&#24773;&#24863;&#20998;&#26512;&#31561;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;&#22312;&#25968;&#20540;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#24403;&#21069;&#29256;&#26412;ChatGPT&#21644;GPT-4&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#29616;&#26377;&#25216;&#26415;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
The most recent large language models such as ChatGPT and GPT-4 have garnered significant attention, as they are capable of generating high-quality responses to human input. Despite the extensive testing of ChatGPT and GPT-4 on generic text corpora, showcasing their impressive capabilities, a study focusing on financial corpora has not been conducted. In this study, we aim to bridge this gap by examining the potential of ChatGPT and GPT-4 as a solver for typical financial text analytic problems in the zero-shot or few-shot setting. Specifically, we assess their capabilities on four representative tasks over five distinct financial textual datasets. The preliminary study shows that ChatGPT and GPT-4 struggle on tasks such as financial named entity recognition (NER) and sentiment analysis, where domain-specific knowledge is required, while they excel in numerical reasoning tasks. We report both the strengths and limitations of the current versions of ChatGPT and GPT-4, comparing them to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26426;&#22120;&#20154;&#36827;&#34892;&#23478;&#24237;&#28165;&#25195;&#30340;&#20010;&#24615;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23569;&#26679;&#26412;&#25688;&#35201;&#33021;&#21147;&#65292;&#26426;&#22120;&#20154;&#21487;&#20197;&#23398;&#20064;&#29992;&#25143;&#30340;&#20559;&#22909;&#24182;&#23558;&#20854;&#25512;&#24191;&#21040;&#26410;&#26469;&#30340;&#22330;&#26223;&#20013;&#65292;&#20174;&#32780;&#23454;&#29616;&#24555;&#36895;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2305.05658</link><description>&lt;p&gt;
TidyBot: &#24212;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20010;&#24615;&#21270;&#26426;&#22120;&#20154;&#29289;&#29702;&#36741;&#21161;
&lt;/p&gt;
&lt;p&gt;
TidyBot: Personalized Robot Assistance with Large Language Models. (arXiv:2305.05658v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26426;&#22120;&#20154;&#36827;&#34892;&#23478;&#24237;&#28165;&#25195;&#30340;&#20010;&#24615;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23569;&#26679;&#26412;&#25688;&#35201;&#33021;&#21147;&#65292;&#26426;&#22120;&#20154;&#21487;&#20197;&#23398;&#20064;&#29992;&#25143;&#30340;&#20559;&#22909;&#24182;&#23558;&#20854;&#25512;&#24191;&#21040;&#26410;&#26469;&#30340;&#22330;&#26223;&#20013;&#65292;&#20174;&#32780;&#23454;&#29616;&#24555;&#36895;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#26377;&#25928;&#20010;&#24615;&#21270;&#22320;&#25552;&#20379;&#29289;&#29702;&#36741;&#21161;&#65292;&#23427;&#24517;&#39035;&#23398;&#20064;&#29992;&#25143;&#30340;&#20010;&#20154;&#21916;&#22909;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#26410;&#26469;&#30340;&#22330;&#26223;&#20013;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26426;&#22120;&#20154;&#36827;&#34892;&#23478;&#24237;&#28165;&#25195;&#30340;&#20010;&#24615;&#21270;&#38382;&#39064;&#65292;&#36825;&#20123;&#26426;&#22120;&#20154;&#33021;&#22815;&#36890;&#36807;&#25441;&#36215;&#29289;&#21697;&#24182;&#23558;&#20854;&#25918;&#22238;&#21407;&#22788;&#26469;&#25972;&#29702;&#25151;&#38388;&#12290;&#19968;&#20010;&#20851;&#38190;&#30340;&#25361;&#25112;&#26159;&#30830;&#23450;&#27599;&#20010;&#29289;&#21697;&#30340;&#27491;&#30830;&#20301;&#32622;&#65292;&#22240;&#20026;&#20154;&#20204;&#30340;&#21916;&#22909;&#21487;&#20197;&#22240;&#20010;&#20154;&#21697;&#21619;&#25110;&#25991;&#21270;&#32972;&#26223;&#32780;&#22823;&#19981;&#30456;&#21516;&#12290;&#20363;&#22914;&#65292;&#19968;&#20010;&#20154;&#21487;&#33021;&#21916;&#27426;&#25226;&#34924;&#34923;&#25918;&#22312;&#25277;&#23625;&#37324;&#65292;&#32780;&#21478;&#19968;&#20010;&#20154;&#21487;&#33021;&#21916;&#27426;&#25226;&#34924;&#34923;&#25918;&#22312;&#26550;&#23376;&#19978;&#12290;&#25105;&#20204;&#26088;&#22312;&#24314;&#31435;&#31995;&#32479;&#65292;&#36825;&#20123;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#19982;&#29305;&#23450;&#20154;&#30340;&#20808;&#21069;&#20132;&#20114;&#23398;&#20064;&#36825;&#26679;&#30340;&#21916;&#22909;&#65292;&#32780;&#21482;&#38656;&#35201;&#20960;&#20010;&#31034;&#20363;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26426;&#22120;&#20154;&#21487;&#20197;&#23558;&#22522;&#20110;&#35821;&#35328;&#30340;&#35268;&#21010;&#21644;&#24863;&#30693;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#23569;&#26679;&#26412;&#25688;&#35201;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#20174;&#32780;&#25512;&#26029;&#20986;&#24191;&#27867;&#36866;&#29992;&#20110;&#26410;&#26469;&#20132;&#20114;&#30340;&#29992;&#25143;&#20559;&#22909;&#12290;&#36825;&#31181;&#26041;&#27861;&#23454;&#29616;&#20102;&#24555;&#36895;&#36866;&#24212;&#65292;&#24182;&#21462;&#24471;&#20102;91.2%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
For a robot to personalize physical assistance effectively, it must learn user preferences that can be generally reapplied to future scenarios. In this work, we investigate personalization of household cleanup with robots that can tidy up rooms by picking up objects and putting them away. A key challenge is determining the proper place to put each object, as people's preferences can vary greatly depending on personal taste or cultural background. For instance, one person may prefer storing shirts in the drawer, while another may prefer them on the shelf. We aim to build systems that can learn such preferences from just a handful of examples via prior interactions with a particular person. We show that robots can combine language-based planning and perception with the few-shot summarization capabilities of large language models (LLMs) to infer generalized user preferences that are broadly applicable to future interactions. This approach enables fast adaptation and achieves 91.2% accurac
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#34701;&#21512;&#22312;&#19981;&#21516;&#27169;&#24577;&#19978;&#35757;&#32451;&#30340;transformer&#36827;&#34892;&#22810;&#27169;&#24577;&#27169;&#22411;&#34701;&#21512;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#21442;&#25968;&#26377;&#25928;&#30340;&#27169;&#24577;&#19981;&#21487;&#30693;&#26550;&#26500;&#65292;&#24418;&#25104;&#26377;&#25928;&#30340;&#35757;&#32451;&#37197;&#26041;&#12290;</title><link>http://arxiv.org/abs/2304.14933</link><description>&lt;p&gt;
&#19968;&#39033;&#22810;&#27169;&#24577;&#27169;&#22411;&#34701;&#21512;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of Multimodal Model Merging. (arXiv:2304.14933v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#34701;&#21512;&#22312;&#19981;&#21516;&#27169;&#24577;&#19978;&#35757;&#32451;&#30340;transformer&#36827;&#34892;&#22810;&#27169;&#24577;&#27169;&#22411;&#34701;&#21512;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#21442;&#25968;&#26377;&#25928;&#30340;&#27169;&#24577;&#19981;&#21487;&#30693;&#26550;&#26500;&#65292;&#24418;&#25104;&#26377;&#25928;&#30340;&#35757;&#32451;&#37197;&#26041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#34701;&#21512;&#65288;&#20363;&#22914;&#25554;&#20540;&#25110;&#20219;&#21153;&#31639;&#26415;&#65289;&#23558;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#22810;&#20010;&#27169;&#22411;&#21512;&#24182;&#20197;&#29983;&#25104;&#22810;&#20219;&#21153;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#25216;&#26415;&#22312;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#24050;&#32463;&#34987;&#35777;&#26126;&#25104;&#21151;&#65292;&#20854;&#20013;&#27169;&#22411;&#26159;&#22312;&#30456;&#20284;&#30340;&#20219;&#21153;&#21644;&#30456;&#21516;&#30340;&#21021;&#22987;&#21270;&#19979;&#35757;&#32451;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#22312;&#19981;&#21516;&#27169;&#24577;&#19978;&#35757;&#32451;&#30340;transformer&#36827;&#34892;&#34701;&#21512;&#65292;&#23558;&#27492;&#27010;&#24565;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#35774;&#32622;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#38024;&#23545;&#19968;&#20010;&#26032;&#39062;&#30340;&#30446;&#26631;&#36827;&#34892;&#30740;&#31350;&#65292;&#22312;&#35813;&#30446;&#26631;&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#36328;&#27169;&#24577;&#30340;transformer&#21512;&#24182;&#21040;&#29305;&#23450;&#27169;&#24577;&#30340;&#26550;&#26500;&#20013;&#65292;&#20197;&#21019;&#24314;&#19968;&#20010;&#21442;&#25968;&#26377;&#25928;&#30340;&#27169;&#24577;&#19981;&#21487;&#30693;&#26550;&#26500;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#24433;&#21709;&#27169;&#22411;&#34701;&#21512;&#21518;&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#21253;&#25324;&#21021;&#22987;&#21270;&#12289;&#34701;&#21512;&#26426;&#21046;&#21644;&#27169;&#22411;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#24471;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#35757;&#32451;&#37197;&#26041;&#65292;&#21487;&#20197;&#36890;&#36807;&#27169;&#22411;&#34701;&#21512;&#26469;&#21305;&#37197;&#27169;&#24577;&#19981;&#21487;&#30693;&#22522;&#32447;&#30340;&#24615;&#33021;&#65288;&#21363;&#20174;&#22836;&#24320;&#22987;&#39044;&#35757;&#32451;&#65289;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#20379;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model merging (e.g., via interpolation or task arithmetic) fuses multiple models trained on different tasks to generate a multi-task solution. The technique has been proven successful in previous studies, where the models are trained on similar tasks and with the same initialization. In this paper, we expand on this concept to a multimodal setup by merging transformers trained on different modalities. Furthermore, we conduct our study for a novel goal where we can merge vision, language, and cross-modal transformers of a modality-specific architecture to create a parameter-efficient modality-agnostic architecture. Through comprehensive experiments, we systematically investigate the key factors impacting model performance after merging, including initialization, merging mechanisms, and model architectures. Our analysis leads to an effective training recipe for matching the performance of the modality-agnostic baseline (i.e. pre-trained from scratch) via model merging. Our code is availa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#29702;&#35770;&#26041;&#27861;&#8212;&#8212;&#34892;&#20026;&#26399;&#26395;&#36793;&#30028;&#65288;BEB&#65289;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#40784;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#24182;&#35777;&#26126;&#20219;&#20309;&#23545;&#40784;&#36807;&#31243;&#37117;&#26080;&#27861;&#26681;&#38500;&#19981;&#24076;&#26395;&#30340;&#34892;&#20026;&#65292;&#36825;&#23545;&#20110;&#38450;&#27490;&#24694;&#24847;&#25915;&#20987;&#26159;&#19981;&#23433;&#20840;&#30340;&#12290;</title><link>http://arxiv.org/abs/2304.11082</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#30340;&#22522;&#26412;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Fundamental Limitations of Alignment in Large Language Models. (arXiv:2304.11082v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#29702;&#35770;&#26041;&#27861;&#8212;&#8212;&#34892;&#20026;&#26399;&#26395;&#36793;&#30028;&#65288;BEB&#65289;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#40784;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#24182;&#35777;&#26126;&#20219;&#20309;&#23545;&#40784;&#36807;&#31243;&#37117;&#26080;&#27861;&#26681;&#38500;&#19981;&#24076;&#26395;&#30340;&#34892;&#20026;&#65292;&#36825;&#23545;&#20110;&#38450;&#27490;&#24694;&#24847;&#25915;&#20987;&#26159;&#19981;&#23433;&#20840;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#19982;&#20154;&#20132;&#20114;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#26041;&#38754;&#26159;&#23545;&#40784;&#20854;&#34892;&#20026;&#65292;&#20351;&#20854;&#23545;&#20854;&#20154;&#31867;&#29992;&#25143;&#26377;&#29992;&#19988;&#26080;&#23475;&#12290;&#36825;&#36890;&#24120;&#36890;&#36807;&#35843;&#25972;&#27169;&#22411;&#30340;&#26041;&#24335;&#26469;&#23454;&#29616;&#65292;&#20197;&#22686;&#24378;&#25152;&#38656;&#30340;&#34892;&#20026;&#24182;&#25233;&#21046;&#19981;&#24076;&#26395;&#30340;&#34892;&#20026;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#34892;&#20026;&#26399;&#26395;&#36793;&#30028;(BEB)&#30340;&#29702;&#35770;&#26041;&#27861;&#65292;&#23427;&#20801;&#35768;&#25105;&#20204;&#27491;&#24335;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20960;&#20010;&#20869;&#22312;&#29305;&#24449;&#21644;&#23545;&#40784;&#30340;&#38480;&#21046;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#23545;&#20110;&#20219;&#20309;&#20855;&#26377;&#34987;&#35813;&#27169;&#22411;&#34920;&#29616;&#20986;&#30340;&#26377;&#38480;&#27010;&#29575;&#30340;&#34892;&#20026;&#65292;&#37117;&#23384;&#22312;&#21487;&#20197;&#35302;&#21457;&#27169;&#22411;&#36755;&#20986;&#27492;&#34892;&#20026;&#30340;&#25552;&#31034;&#65292;&#20854;&#27010;&#29575;&#38543;&#25552;&#31034;&#30340;&#38271;&#24230;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;&#36825;&#24847;&#21619;&#30528;&#20219;&#20309;&#20943;&#24369;&#19981;&#24076;&#26395;&#30340;&#34892;&#20026;&#20294;&#26410;&#23558;&#20854;&#23436;&#20840;&#28040;&#38500;&#30340;&#23545;&#40784;&#36807;&#31243;&#37117;&#26080;&#27861;&#25269;&#24481;&#38024;&#23545;&#24615;&#25915;&#20987;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#31034;&#20102;&#39046;&#20808;&#30340;
&lt;/p&gt;
&lt;p&gt;
An important aspect in developing language models that interact with humans is aligning their behavior to be useful and unharmful for their human users. This is usually achieved by tuning the model in a way that enhances desired behaviors and inhibits undesired ones, a process referred to as alignment. In this paper, we propose a theoretical approach called Behavior Expectation Bounds (BEB) which allows us to formally investigate several inherent characteristics and limitations of alignment in large language models. Importantly, we prove that for any behavior that has a finite probability of being exhibited by the model, there exist prompts that can trigger the model into outputting this behavior, with probability that increases with the length of the prompt. This implies that any alignment process that attenuates undesired behavior but does not remove it altogether, is not safe against adversarial prompting attacks. Furthermore, our framework hints at the mechanism by which leading al
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#35780;&#20272;&#20102;ChatGPT&#22312;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#21644;&#24773;&#24863;&#25512;&#29702;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#20197;&#21450;&#19981;&#21516;&#25552;&#31034;&#31574;&#30053;&#21644;&#24773;&#24863;&#20449;&#24687;&#23545;&#20854;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#22312;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#21152;&#20837;&#24773;&#24863;&#22686;&#24378;&#25552;&#31034;&#23545;&#26576;&#20123;&#20219;&#21153;&#25928;&#26524;&#26174;&#33879;&#12290;</title><link>http://arxiv.org/abs/2304.03347</link><description>&lt;p&gt;
&#35770;ChatGPT&#21644;&#24773;&#24863;&#22686;&#24378;&#25552;&#31034;&#22312;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#20013;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
On the Evaluations of ChatGPT and Emotion-enhanced Prompting for Mental Health Analysis. (arXiv:2304.03347v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#35780;&#20272;&#20102;ChatGPT&#22312;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#21644;&#24773;&#24863;&#25512;&#29702;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#20197;&#21450;&#19981;&#21516;&#25552;&#31034;&#31574;&#30053;&#21644;&#24773;&#24863;&#20449;&#24687;&#23545;&#20854;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#22312;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#21152;&#20837;&#24773;&#24863;&#22686;&#24378;&#25552;&#31034;&#23545;&#26576;&#20123;&#20219;&#21153;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#26174;&#31034;&#20986;&#25552;&#39640;&#24515;&#29702;&#20581;&#24247;&#25252;&#29702;&#25928;&#29575;&#21644;&#21487;&#35775;&#38382;&#24615;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#32780;&#26368;&#36817;&#30340;&#20027;&#27969;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;(PLMs)&#20316;&#20026;&#39592;&#24178;&#65292;&#24182;&#34701;&#20837;&#24773;&#24863;&#20449;&#24687;&#12290;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#65292;&#22914;ChatGPT&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;ChatGPT&#38646;-shot&#24615;&#33021;&#30740;&#31350;&#22312;&#19981;&#20805;&#20998;&#30340;&#35780;&#20272;&#12289;&#24773;&#24863;&#20449;&#24687;&#21033;&#29992;&#21644;&#26041;&#27861;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#20840;&#38754;&#35780;&#20272;&#20102;ChatGPT&#22312;11&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#21644;&#24773;&#24863;&#25512;&#29702;&#33021;&#21147;&#65292;&#28085;&#30422;&#20102;5&#20010;&#20219;&#21153;&#65292;&#21253;&#25324;&#20108;&#36827;&#21046;&#21644;&#22810;&#31867;&#24515;&#29702;&#20581;&#24247;&#29366;&#20917;&#26816;&#27979;&#12289;&#24515;&#29702;&#20581;&#24247;&#29366;&#20917;&#30340;&#21407;&#22240;/&#22240;&#32032;&#26816;&#27979;&#12289;&#23545;&#35805;&#20013;&#30340;&#24773;&#32490;&#35782;&#21035;&#21644;&#22240;&#26524;&#24773;&#24863;&#34164;&#21547;&#12290;&#25105;&#20204;&#22312;&#23454;&#35777;&#20998;&#26512;&#20013;&#25506;&#31350;&#20102;&#19981;&#21516;&#25552;&#31034;&#31574;&#30053;&#20197;&#21450;&#24773;&#24863;&#22686;&#24378;&#25552;&#31034;&#23545;ChatGPT&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#22312;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#20013;&#26377;&#30528;&#33391;&#22909;&#30340;&#34920;&#29616;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#20219;&#21153;&#20013;&#21152;&#20837;&#24773;&#24863;&#22686;&#24378;&#25552;&#31034;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated mental health analysis shows great potential for enhancing the efficiency and accessibility of mental health care, whereas the recent dominant methods utilized pre-trained language models (PLMs) as the backbone and incorporated emotional information. The latest large language models (LLMs), such as ChatGPT, exhibit dramatic capabilities on diverse natural language processing tasks. However, existing studies on ChatGPT's zero-shot performance for mental health analysis have limitations in inadequate evaluation, utilization of emotional information, and explainability of methods. In this work, we comprehensively evaluate the mental health analysis and emotional reasoning ability of ChatGPT on 11 datasets across 5 tasks, including binary and multi-class mental health condition detection, cause/factor detection of mental health conditions, emotion recognition in conversations, and causal emotion entailment. We empirically analyze the impact of different prompting strategies with 
&lt;/p&gt;</description></item><item><title>SelfCheckGPT&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20197;&#38646;&#36164;&#28304;&#30340;&#26041;&#24335;&#26816;&#26597;&#40657;&#30418;&#27169;&#22411;&#30340;&#24187;&#35273;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2303.08896</link><description>&lt;p&gt;
SelfCheckGPT: &#38646;&#36164;&#28304;&#40657;&#30418;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#29992;&#20110;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models. (arXiv:2303.08896v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08896
&lt;/p&gt;
&lt;p&gt;
SelfCheckGPT&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20197;&#38646;&#36164;&#28304;&#30340;&#26041;&#24335;&#26816;&#26597;&#40657;&#30418;&#27169;&#22411;&#30340;&#24187;&#35273;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20363;&#22914;GPT-3&#65292;&#33021;&#22815;&#23545;&#21508;&#31181;&#29992;&#25143;&#25552;&#31034;&#36827;&#34892;&#39640;&#24230;&#27969;&#30021;&#30340;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;LLM&#24050;&#30693;&#20250;&#20135;&#29983;&#24187;&#35273;&#20107;&#23454;&#21644;&#38750;&#20107;&#23454;&#38472;&#36848;&#65292;&#36825;&#21487;&#33021;&#20250;&#21066;&#24369;&#23545;&#23427;&#20204;&#30340;&#36755;&#20986;&#30340;&#20449;&#20219;&#12290;&#29616;&#26377;&#30340;&#20107;&#23454;&#26816;&#26597;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#35775;&#38382;&#20196;&#29260;&#32423;&#36755;&#20986;&#27010;&#29575;&#20998;&#24067;&#65288;&#36825;&#21487;&#33021;&#23545;&#20110;ChatGPT&#31561;&#31995;&#32479;&#26469;&#35828;&#19981;&#21487;&#29992;&#65289;&#65292;&#35201;&#20040;&#38656;&#35201;&#36890;&#36807;&#21333;&#29420;&#30340;&#36890;&#24120;&#22797;&#26434;&#30340;&#27169;&#22359;&#25509;&#21475;&#30340;&#22806;&#37096;&#25968;&#25454;&#24211;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;SelfCheckGPT&#8221;&#65292;&#21487;&#20197;&#20197;&#38646;&#36164;&#28304;&#30340;&#26041;&#24335;&#26816;&#26597;&#40657;&#30418;&#27169;&#22411;&#65292;&#21363;&#19981;&#38656;&#35201;&#22806;&#37096;&#25968;&#25454;&#24211;&#12290; SelfCheckGPT&#21033;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#24605;&#24819;&#65306;&#22914;&#26524;LLM&#20855;&#26377;&#29305;&#23450;&#27010;&#24565;&#30340;&#30693;&#35782;&#65292;&#21017;&#37319;&#26679;&#30340;&#21709;&#24212;&#21487;&#33021;&#31867;&#20284;&#24182;&#21253;&#21547;&#19968;&#33268;&#30340;&#20107;&#23454;&#12290;&#20294;&#26159;&#65292;&#23545;&#20110;&#24187;&#35273;&#30340;&#20107;&#23454;&#65292;&#38543;&#26426;&#37319;&#26679;&#30340;&#21709;&#24212;&#21487;&#33021;&#20250;&#21457;&#25955;&#24182;&#30456;&#20114;&#30683;&#30462;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;GP-T-3&#27169;&#22411;&#20026;&#20363;&#26469;&#30740;&#31350;&#27492;&#26041;&#27861;&#65292;&#24182;&#22312;&#24120;&#35265;&#20219;&#21153;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;SelfCheckGPT&#33021;&#22815;&#26377;&#25928;&#22320;&#26816;&#27979;&#27169;&#22411;&#30340;&#24187;&#35273;&#29616;&#35937;&#65292;&#19988;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20445;&#25345;&#33391;&#22909;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Large Language Models (LLMs) such as GPT-3 are capable of generating highly fluent responses to a wide variety of user prompts. However, LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output. Existing fact-checking approaches either require access to token-level output probability distribution (which may not be available for systems such as ChatGPT) or external databases that are interfaced via separate, often complex, modules. In this work, we propose "SelfCheckGPT", a simple sampling-based approach that can be used to fact-check black-box models in a zero-resource fashion, i.e. without an external database. SelfCheckGPT leverages the simple idea that if a LLM has knowledge of a given concept, sampled responses are likely to be similar and contain consistent facts. However, for hallucinated facts, stochastically sampled responses are likely to diverge and contradict one another. We investigate this approach by using GP
&lt;/p&gt;</description></item><item><title>UPRISE&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#26816;&#32034;&#22120;&#65292;&#21487;&#33258;&#21160;&#20026;&#32473;&#23450;&#30340;&#38646;&#26679;&#26412;&#20219;&#21153;&#36755;&#20837;&#26816;&#32034;&#25552;&#31034;&#65292;&#20174;&#32780;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#35780;&#20272;&#12290;&#23427;&#36890;&#36807;&#36328;&#20219;&#21153;&#21644;&#36328;&#27169;&#22411;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;&#20854;&#36890;&#29992;&#24615;&#21644;&#28508;&#21147;&#65292;&#21516;&#26102;&#34920;&#26126;&#20855;&#26377;&#20943;&#36731;&#24187;&#35273;&#38382;&#39064;&#21644;&#25552;&#39640;LLM&#24615;&#33021;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.08518</link><description>&lt;p&gt;
UPRISE: &#36890;&#29992;&#25552;&#31034;&#26816;&#32034;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation. (arXiv:2303.08518v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08518
&lt;/p&gt;
&lt;p&gt;
UPRISE&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#26816;&#32034;&#22120;&#65292;&#21487;&#33258;&#21160;&#20026;&#32473;&#23450;&#30340;&#38646;&#26679;&#26412;&#20219;&#21153;&#36755;&#20837;&#26816;&#32034;&#25552;&#31034;&#65292;&#20174;&#32780;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#35780;&#20272;&#12290;&#23427;&#36890;&#36807;&#36328;&#20219;&#21153;&#21644;&#36328;&#27169;&#22411;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;&#20854;&#36890;&#29992;&#24615;&#21644;&#28508;&#21147;&#65292;&#21516;&#26102;&#34920;&#26126;&#20855;&#26377;&#20943;&#36731;&#24187;&#35273;&#38382;&#39064;&#21644;&#25552;&#39640;LLM&#24615;&#33021;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22240;&#20854;&#20986;&#33394;&#30340;&#33021;&#21147;&#32780;&#21463;&#27426;&#36814;&#65292;&#20294;&#38656;&#35201;&#29305;&#23450;&#27169;&#22411;&#30340;&#24494;&#35843;&#25110;&#20219;&#21153;&#29305;&#23450;&#25552;&#31034;&#24037;&#31243;&#21487;&#33021;&#20250;&#38459;&#30861;&#20854;&#19968;&#33324;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;UPRISE&#65288;&#36890;&#29992;&#25552;&#31034;&#26816;&#32034;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#35780;&#20272;&#65289;&#65292;&#35813;&#26041;&#27861;&#35843;&#25972;&#20102;&#36731;&#37327;&#32423;&#21644;&#22810;&#21151;&#33021;&#30340;&#26816;&#32034;&#22120;&#65292;&#20197;&#33258;&#21160;&#26816;&#32034;&#32473;&#23450;&#38646;&#26679;&#26412;&#20219;&#21153;&#36755;&#20837;&#30340;&#25552;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#36328;&#20219;&#21153;&#21644;&#36328;&#27169;&#22411;&#26041;&#26696;&#20013;&#23637;&#31034;&#20102;&#36890;&#29992;&#24615;&#65306;&#26816;&#32034;&#22120;&#38024;&#23545;&#21508;&#31181;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#65292;&#20294;&#22312;&#30475;&#19981;&#35265;&#30340;&#20219;&#21153;&#31867;&#22411;&#19978;&#36827;&#34892;&#27979;&#35797;&#65307;&#25105;&#20204;&#22312;&#19968;&#20010;&#23567;&#22411;&#20923;&#32467;LLM&#8212;&#8212;GPT-Neo-2.7B&#19978;&#35843;&#25972;&#26816;&#32034;&#22120;&#65292;&#20294;&#22312;&#19981;&#21516;&#35268;&#27169;&#30340;LLM&#19978;&#27979;&#35797;&#26816;&#32034;&#22120;&#65292;&#20363;&#22914;BLOOM-7.1B&#12289;OPT-66B&#21644;GPT3-175B&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;UPRISE&#22312;&#25105;&#20204;&#19982;ChatGPT&#30340;&#23454;&#39564;&#20013;&#20943;&#36731;&#20102;&#24187;&#35273;&#38382;&#39064;&#65292;&#34920;&#26126;&#23427;&#26377;&#28508;&#21147;&#25913;&#36827;&#29978;&#33267;&#26159;&#26368;&#24378;&#30340;LLM&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are popular for their impressive abilities, but the need for model-specific fine-tuning or task-specific prompt engineering can hinder their generalization. We propose UPRISE (Universal Prompt Retrieval for Improving zero-Shot Evaluation), which tunes a lightweight and versatile retriever that automatically retrieves prompts for a given zero-shot task input. Specifically, we demonstrate universality in a cross-task and cross-model scenario: the retriever is tuned on a diverse set of tasks, but tested on unseen task types; we use a small frozen LLM, GPT-Neo-2.7B, for tuning the retriever, but test the retriever on different LLMs of much larger scales, such as BLOOM-7.1B, OPT-66B and GPT3-175B. Additionally, we show that UPRISE mitigates the hallucination problem in our experiments with ChatGPT, suggesting its potential to improve even the strongest LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;query2doc&#30340;&#26597;&#35810;&#25193;&#23637;&#26041;&#27861;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20266;&#25991;&#26723;&#26469;&#25913;&#21892;&#31232;&#30095;&#21644;&#23494;&#38598;&#26816;&#32034;&#31995;&#32479;&#65292;&#21462;&#24471;&#20102;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#25552;&#39640; BM25 &#24615;&#33021;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.07678</link><description>&lt;p&gt;
Query2doc: &#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26597;&#35810;&#25193;&#23637;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Query2doc: Query Expansion with Large Language Models. (arXiv:2303.07678v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;query2doc&#30340;&#26597;&#35810;&#25193;&#23637;&#26041;&#27861;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20266;&#25991;&#26723;&#26469;&#25913;&#21892;&#31232;&#30095;&#21644;&#23494;&#38598;&#26816;&#32034;&#31995;&#32479;&#65292;&#21462;&#24471;&#20102;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#25552;&#39640; BM25 &#24615;&#33021;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26597;&#35810;&#25193;&#23637;&#26041;&#27861;&#65292;&#31216;&#20026;query2doc&#65292;&#21487;&#25913;&#21892;&#31232;&#30095;&#21644;&#23494;&#38598;&#26816;&#32034;&#31995;&#32479;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#21033;&#29992;&#23567;&#25209;&#37327;&#25552;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20266;&#25991;&#26723;&#65292;&#28982;&#21518;&#20351;&#29992;&#29983;&#25104;&#30340;&#20266;&#25991;&#26723;&#25193;&#23637;&#26597;&#35810;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32463;&#36807;&#35757;&#32451;&#65292;&#33021;&#22815;&#35760;&#24518;&#30693;&#35782;&#65292;&#20174;&#32780;&#29983;&#25104;&#30340;&#20266;&#25991;&#26723;&#36890;&#24120;&#21253;&#21547;&#39640;&#24230;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#26377;&#21161;&#20110;&#26597;&#35810;&#28040;&#23696;&#21644;&#25351;&#23548;&#26816;&#32034;&#22120;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19981;&#36827;&#34892;&#20219;&#20309;&#27169;&#22411;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;query2doc &#22312; MS-MARCO &#21644; TREC DL &#31561; ad-hoc IR &#25968;&#25454;&#38598;&#19978;&#23558; BM25 &#30340;&#24615;&#33021;&#25552;&#39640;&#20102; 3% &#21040; 15%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#22312;&#39046;&#22495;&#20869;&#21644;&#39046;&#22495;&#22806;&#30340;&#32467;&#26524;&#26041;&#38754;&#21463;&#30410;&#20110;&#26368;&#20808;&#36827;&#30340;&#23494;&#38598;&#26816;&#32034;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a simple yet effective query expansion approach, denoted as query2doc, to improve both sparse and dense retrieval systems. The proposed method first generates pseudo-documents by few-shot prompting large language models (LLMs), and then expands the query with generated pseudo-documents. LLMs are trained on web-scale text corpora and are adept at knowledge memorization. The pseudo-documents from LLMs often contain highly relevant information that can aid in query disambiguation and guide the retrievers. Experimental results demonstrate that query2doc boosts the performance of BM25 by 3% to 15% on ad-hoc IR datasets, such as MS-MARCO and TREC DL, without any model fine-tuning. Furthermore, our method also benefits state-of-the-art dense retrievers in terms of both in-domain and out-of-domain results.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35821;&#35328;&#35821;&#20041;&#35299;&#26512;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;(AL-MSP), &#36873;&#25321;&#19968;&#20010;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#23376;&#38598;&#36827;&#34892;&#32763;&#35793;&#65292;&#36890;&#36807;&#20248;&#20808;&#36873;&#25321;&#22810;&#26679;&#21270;&#36923;&#36753;&#24418;&#24335;&#32467;&#26500;&#21644;&#26356;&#22810;&#35789;&#27719;&#36873;&#25321;&#30340;&#31034;&#20363;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#30340;&#26080;&#38656;&#39069;&#22806;&#27880;&#37322;&#25104;&#26412;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#26041;&#27861;&#65292;&#26377;&#25928;&#20943;&#23569;&#20102;&#32763;&#35793;&#25104;&#26412;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#22522;&#32447;&#26356;&#22909;&#30340;&#35299;&#26512;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.12920</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#35821;&#20041;&#35299;&#26512;&#30340;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Active Learning for Multilingual Semantic Parser. (arXiv:2301.12920v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12920
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35821;&#35328;&#35821;&#20041;&#35299;&#26512;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;(AL-MSP), &#36873;&#25321;&#19968;&#20010;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#23376;&#38598;&#36827;&#34892;&#32763;&#35793;&#65292;&#36890;&#36807;&#20248;&#20808;&#36873;&#25321;&#22810;&#26679;&#21270;&#36923;&#36753;&#24418;&#24335;&#32467;&#26500;&#21644;&#26356;&#22810;&#35789;&#27719;&#36873;&#25321;&#30340;&#31034;&#20363;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#30340;&#26080;&#38656;&#39069;&#22806;&#27880;&#37322;&#25104;&#26412;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#26041;&#27861;&#65292;&#26377;&#25928;&#20943;&#23569;&#20102;&#32763;&#35793;&#25104;&#26412;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#22522;&#32447;&#26356;&#22909;&#30340;&#35299;&#26512;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22810;&#35821;&#35328;&#35821;&#20041;&#35299;&#26512;(MSP) &#25968;&#25454;&#38598;&#20960;&#20046;&#37117;&#26159;&#36890;&#36807;&#23558;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#30340;&#35805;&#35821;&#20174;&#36164;&#28304;&#20016;&#23500;&#30340;&#35821;&#35328;&#32763;&#35793;&#21040;&#30446;&#26631;&#35821;&#35328;&#32780;&#25910;&#38598;&#30340;&#12290;&#20294;&#26159;&#65292;&#25163;&#21160;&#32763;&#35793;&#25104;&#26412;&#39640;&#26114;&#12290;&#20026;&#20102;&#20943;&#23569;&#32763;&#35793;&#24037;&#20316;&#37327;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;MSP&#30340;&#20027;&#21160;&#23398;&#20064;&#36807;&#31243;(AL-MSP)&#12290;AL-MSP&#21482;&#36873;&#25321;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#23376;&#38598;&#36827;&#34892;&#32763;&#35793;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36873;&#25321;&#26041;&#27861;&#65292;&#20248;&#20808;&#36873;&#25321;&#22810;&#26679;&#21270;&#36923;&#36753;&#24418;&#24335;&#32467;&#26500;&#21644;&#26356;&#22810;&#35789;&#27719;&#36873;&#25321;&#30340;&#31034;&#20363;&#65292;&#20197;&#21450;&#19968;&#31181;&#26080;&#38656;&#39069;&#22806;&#27880;&#37322;&#25104;&#26412;&#30340;&#26032;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36890;&#36807;&#29702;&#24819;&#30340;&#36873;&#25321;&#26041;&#27861;&#65292;AL-MSP&#26174;&#33879;&#20943;&#23569;&#20102;&#32763;&#35793;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#36873;&#25321;&#26041;&#27861;&#19982;&#36866;&#24403;&#30340;&#36229;&#21442;&#25968;&#21487;&#20197;&#22312;&#20004;&#20010;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#27604;&#20854;&#20182;&#22522;&#32447;&#26356;&#22909;&#30340;&#35299;&#26512;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current multilingual semantic parsing (MSP) datasets are almost all collected by translating the utterances in the existing datasets from the resource-rich language to the target language. However, manual translation is costly. To reduce the translation effort, this paper proposes the first active learning procedure for MSP (AL-MSP). AL-MSP selects only a subset from the existing datasets to be translated. We also propose a novel selection method that prioritizes the examples diversifying the logical form structures with more lexical choices, and a novel hyperparameter tuning method that needs no extra annotation cost. Our experiments show that AL-MSP significantly reduces translation costs with ideal selection methods. Our selection method with proper hyperparameters yields better parsing performance than the other baselines on two multilingual datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#30693;&#35782;&#26174;&#33879;&#36328;&#24230;&#25513;&#30721;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20840;&#33258;&#30417;&#30563;&#23398;&#20064;&#24110;&#21161;&#35821;&#35328;&#27169;&#22411;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#33719;&#21462;&#26356;&#22810;&#30340;&#30693;&#35782;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2204.07994</link><description>&lt;p&gt;
&#30693;&#35782;&#26174;&#33879;&#36328;&#24230;&#25513;&#30721;&#65306;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#30693;&#35782;&#24211;
&lt;/p&gt;
&lt;p&gt;
Knowledgeable Salient Span Mask for Enhancing Language Models as Knowledge Base. (arXiv:2204.07994v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.07994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#30693;&#35782;&#26174;&#33879;&#36328;&#24230;&#25513;&#30721;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20840;&#33258;&#30417;&#30563;&#23398;&#20064;&#24110;&#21161;&#35821;&#35328;&#27169;&#22411;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#33719;&#21462;&#26356;&#22810;&#30340;&#30693;&#35782;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22914;BERT&#22312;&#21508;&#31181;&#19979;&#28216;NLP&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#35201;&#27714;&#27169;&#22411;&#36827;&#34892;&#22635;&#31354;&#24335;&#27979;&#35797;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;PLMs&#22312;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#33719;&#21462;&#30693;&#35782;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#20026;&#20102;&#20102;&#35299;PLMs&#22312;&#26816;&#32034;&#30693;&#35782;&#26102;&#30340;&#20869;&#37096;&#34892;&#20026;&#65292;&#25105;&#20204;&#39318;&#20808;&#20026;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#23450;&#20041;&#20102;&#30693;&#35782;&#21487;&#35265;&#65288;K-B&#65289;&#26631;&#35760;&#21644;&#26080;&#30693;&#35782;&#65288;K-F&#65289;&#26631;&#35760;&#65292;&#24182;&#35831;&#19987;&#19994;&#26631;&#27880;&#21592;&#25163;&#21160;&#26631;&#35760;&#20102;&#19968;&#20123;&#26679;&#26412;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;PLMs&#26356;&#26377;&#21487;&#33021;&#23545;K-B&#26631;&#35760;&#32473;&#20986;&#38169;&#35823;&#39044;&#27979;&#65292;&#24182;&#19988;&#22312;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#20869;&#37096;&#23545;&#36825;&#20123;&#26631;&#35760;&#20851;&#27880;&#30340;&#26356;&#23569;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#20197;&#20840;&#33258;&#30417;&#30563;&#30340;&#26041;&#24335;&#24320;&#21457;&#20102;&#20004;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#24110;&#21161;&#27169;&#22411;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#23398;&#20064;&#26356;&#22810;&#30340;&#30693;&#35782;&#12290;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#25506;&#32034;&#20840;&#33258;&#30417;&#30563;&#23398;&#20064;&#25345;&#32493;&#39044;&#35757;&#32451;&#30693;&#35782;&#30340;&#30740;&#31350;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models (PLMs) like BERT have made significant progress in various downstream NLP tasks. However, by asking models to do cloze-style tests, recent work finds that PLMs are short in acquiring knowledge from unstructured text. To understand the internal behaviour of PLMs in retrieving knowledge, we first define knowledge-baring (K-B) tokens and knowledge-free (K-F) tokens for unstructured text and ask professional annotators to label some samples manually. Then, we find that PLMs are more likely to give wrong predictions on K-B tokens and attend less attention to those tokens inside the self-attention module. Based on these observations, we develop two solutions to help the model learn more knowledge from unstructured text in a fully self-supervised manner. Experiments on knowledge-intensive tasks show the effectiveness of the proposed methods. To our best knowledge, we are the first to explore fully self-supervised learning of knowledge in continual pre-training.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;UNIQORN&#30340;&#38382;&#31572;&#31995;&#32479;&#65292;&#23427;&#33021;&#22815;&#26080;&#32541;&#22320;&#22788;&#29702;RDF&#25968;&#25454;&#21644;&#25991;&#26412;&#65292;&#20351;&#29992;fine-tuned BERT&#27169;&#22411;&#20026;&#38382;&#39064;&#26500;&#24314;&#19978;&#19979;&#25991;&#22270;&#65292;&#24182;&#20351;&#29992;&#22270;&#31639;&#27861;&#30830;&#23450;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#23376;&#22270;&#26469;&#22238;&#31572;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2108.08614</link><description>&lt;p&gt;
UNIQORN&#65306;&#32479;&#19968;&#30340;RDF&#30693;&#35782;&#22270;&#35889;&#19982;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
UNIQORN: Unified Question Answering over RDF Knowledge Graphs and Natural Language Text. (arXiv:2108.08614v5 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.08614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;UNIQORN&#30340;&#38382;&#31572;&#31995;&#32479;&#65292;&#23427;&#33021;&#22815;&#26080;&#32541;&#22320;&#22788;&#29702;RDF&#25968;&#25454;&#21644;&#25991;&#26412;&#65292;&#20351;&#29992;fine-tuned BERT&#27169;&#22411;&#20026;&#38382;&#39064;&#26500;&#24314;&#19978;&#19979;&#25991;&#22270;&#65292;&#24182;&#20351;&#29992;&#22270;&#31639;&#27861;&#30830;&#23450;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#23376;&#22270;&#26469;&#22238;&#31572;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#39064;&#22238;&#31572;&#22312;&#30693;&#35782;&#22270;&#35889;&#21644;&#20854;&#20182;RDF&#25968;&#25454;&#19978;&#24050;&#32463;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#35768;&#22810;&#20248;&#31168;&#30340;&#31995;&#32479;&#21487;&#20197;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#25110;&#30005;&#25253;&#26597;&#35810;&#25552;&#20379;&#28165;&#26224;&#30340;&#31572;&#26696;&#12290;&#20854;&#20013;&#19968;&#20123;&#31995;&#32479;&#23558;&#25991;&#26412;&#28304;&#20316;&#20026;&#38468;&#21152;&#35777;&#25454;&#32435;&#20837;&#22238;&#31572;&#36807;&#31243;&#65292;&#20294;&#19981;&#33021;&#35745;&#31639;&#20165;&#23384;&#22312;&#20110;&#25991;&#26412;&#20013;&#30340;&#31572;&#26696;&#12290;&#30456;&#21453;&#65292;IR&#21644;NLP&#31038;&#21306;&#30340;&#31995;&#32479;&#24050;&#32463;&#35299;&#20915;&#20102;&#26377;&#20851;&#25991;&#26412;&#30340;QA&#38382;&#39064;&#65292;&#20294;&#26159;&#36825;&#20123;&#31995;&#32479;&#20960;&#20046;&#19981;&#21033;&#29992;&#35821;&#20041;&#25968;&#25454;&#21644;&#30693;&#35782;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21487;&#20197;&#26080;&#32541;&#25805;&#20316;&#28151;&#21512;RDF&#25968;&#25454;&#38598;&#21644;&#25991;&#26412;&#35821;&#26009;&#24211;&#25110;&#21333;&#20010;&#26469;&#28304;&#30340;&#22797;&#26434;&#38382;&#39064;&#30340;&#31995;&#32479;&#65292;&#22312;&#32479;&#19968;&#26694;&#26550;&#20013;&#36827;&#34892;&#25805;&#20316;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;UNIQORN&#65292;&#36890;&#36807;&#20351;&#29992;&#32463;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;BERT&#27169;&#22411;&#20174;RDF&#25968;&#25454;&#21644;/&#25110;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#35777;&#25454;&#26469;&#21160;&#24577;&#26500;&#24314;&#19978;&#19979;&#25991;&#22270;&#12290;&#32467;&#26524;&#22270;&#36890;&#24120;&#38750;&#24120;&#20016;&#23500;&#20294;&#39640;&#24230;&#22024;&#26434;&#12290;UNIQORN&#36890;&#36807;&#29992;&#20110;&#32452;Steiner&#26641;&#30340;&#22270;&#31639;&#27861;&#26469;&#22788;&#29702;&#36825;&#20010;&#36755;&#20837;&#65292;&#20174;&#32780;&#30830;&#23450;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#23376;&#22270;&#65292;&#36827;&#32780;&#22238;&#31572;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question answering over knowledge graphs and other RDF data has been greatly advanced, with a number of good systems providing crisp answers for natural language questions or telegraphic queries. Some of these systems incorporate textual sources as additional evidence for the answering process, but cannot compute answers that are present in text alone. Conversely, systems from the IR and NLP communities have addressed QA over text, but such systems barely utilize semantic data and knowledge. This paper presents the first system for complex questions that can seamlessly operate over a mixture of RDF datasets and text corpora, or individual sources, in a unified framework. Our method, called UNIQORN, builds a context graph on-the-fly, by retrieving question-relevant evidences from the RDF data and/or a text corpus, using fine-tuned BERT models. The resulting graph is typically rich but highly noisy. UNIQORN copes with this input by a graph algorithm for Group Steiner Trees, that identifi
&lt;/p&gt;</description></item></channel></rss>