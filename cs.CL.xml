<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;BAT&#65292;&#23427;&#32467;&#21512;&#20102;&#21452;&#32819;&#22768;&#38899;&#22330;&#26223;&#20998;&#26512;&#27169;&#22411;&#30340;&#31354;&#38388;&#22768;&#38899;&#24863;&#30693;&#33021;&#21147;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#33021;&#21147;&#65292;&#20197;&#22797;&#21046;&#20154;&#31867;&#30340;&#31354;&#38388;&#22768;&#38899;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#30340;&#21452;&#32819;&#38899;&#39057;&#25968;&#25454;&#38598;&#21644;&#22522;&#20110;&#31354;&#38388;&#22768;&#38899;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;BAT&#22312;&#31354;&#38388;&#22768;&#38899;&#24863;&#30693;&#21644;&#25512;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01591</link><description>&lt;p&gt;
BAT: &#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#20851;&#20110;&#31354;&#38388;&#22768;&#38899;&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
BAT: Learning to Reason about Spatial Sounds with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01591
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;BAT&#65292;&#23427;&#32467;&#21512;&#20102;&#21452;&#32819;&#22768;&#38899;&#22330;&#26223;&#20998;&#26512;&#27169;&#22411;&#30340;&#31354;&#38388;&#22768;&#38899;&#24863;&#30693;&#33021;&#21147;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#33021;&#21147;&#65292;&#20197;&#22797;&#21046;&#20154;&#31867;&#30340;&#31354;&#38388;&#22768;&#38899;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#30340;&#21452;&#32819;&#38899;&#39057;&#25968;&#25454;&#38598;&#21644;&#22522;&#20110;&#31354;&#38388;&#22768;&#38899;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;BAT&#22312;&#31354;&#38388;&#22768;&#38899;&#24863;&#30693;&#21644;&#25512;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;&#22768;&#38899;&#25512;&#29702;&#26159;&#19968;&#31181;&#22522;&#26412;&#30340;&#20154;&#31867;&#25216;&#33021;&#65292;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#26681;&#25454;&#22768;&#38899;&#26469;&#23548;&#33322;&#21644;&#35299;&#37322;&#25105;&#20204;&#30340;&#21608;&#22260;&#29615;&#22659;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;BAT&#65292;&#23427;&#23558;&#21452;&#32819;&#22768;&#38899;&#22330;&#26223;&#20998;&#26512;&#27169;&#22411;&#30340;&#31354;&#38388;&#22768;&#38899;&#24863;&#30693;&#33021;&#21147;&#19982;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#20197;&#22797;&#21046;&#36825;&#31181;&#22266;&#26377;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#29616;&#26377;&#37326;&#22806;&#31354;&#38388;&#22768;&#38899;&#25968;&#25454;&#38598;&#30340;&#32570;&#20047;&#65292;&#25105;&#20204;&#20351;&#29992;AudioSet&#21644;SoundSpaces 2.0&#21512;&#25104;&#20102;&#19968;&#20010;&#21452;&#32819;&#38899;&#39057;&#25968;&#25454;&#38598;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#31354;&#38388;&#22768;&#38899;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;SpatialSoundQA&#65292;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;QA&#20219;&#21153;&#65292;&#20197;&#35757;&#32451;BAT&#22312;&#31354;&#38388;&#22768;&#38899;&#24863;&#30693;&#21644;&#25512;&#29702;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;BAT&#30340;&#22768;&#23398;&#21069;&#31471;&#32534;&#30721;&#22120;&#26159;&#19968;&#31181;&#21517;&#20026;Spatial Audio Spectrogram Transformer&#65288;Spatial-AST&#65289;&#30340;&#21019;&#26032;&#31354;&#38388;&#38899;&#39057;&#32534;&#30721;&#22120;&#65292;&#23427;&#26412;&#36523;&#22312;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#12289;&#31354;&#38388;&#23450;&#20301;&#21644;&#36317;&#31163;&#20272;&#35745;&#31561;&#26041;&#38754;&#20855;&#26377;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;Spatial-AST&#19982;LLaMA-2 7B&#38598;&#25104;&#65292;
&lt;/p&gt;
&lt;p&gt;
Spatial sound reasoning is a fundamental human skill, enabling us to navigate and interpret our surroundings based on sound. In this paper we present BAT, which combines the spatial sound perception ability of a binaural acoustic scene analysis model with the natural language reasoning capabilities of a large language model (LLM) to replicate this innate ability. To address the lack of existing datasets of in-the-wild spatial sounds, we synthesized a binaural audio dataset using AudioSet and SoundSpaces 2.0. Next, we developed SpatialSoundQA, a spatial sound-based question-answering dataset, offering a range of QA tasks that train BAT in various aspects of spatial sound perception and reasoning. The acoustic front end encoder of BAT is a novel spatial audio encoder named Spatial Audio Spectrogram Transformer, or Spatial-AST, which by itself achieves strong performance across sound event detection, spatial localization, and distance estimation. By integrating Spatial-AST with LLaMA-2 7B
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20877;&#29616;&#24515;&#29702;&#35821;&#35328;&#23398;&#23454;&#39564;&#65292;&#30740;&#31350;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23613;&#31649;&#26080;&#20855;&#36523;&#24615;&#65292;&#21364;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#21040;&#20154;&#31867;&#23545;&#20110;&#35821;&#35328;&#20013;&#22522;&#26412;&#30340;&#31354;&#38388;&#26500;&#24314;&#22359;&#30340;&#38544;&#21547;&#30452;&#35273;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#35821;&#35328;&#21644;&#31354;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00956</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#21644;&#35270;&#35273;&#27169;&#22411;&#20013;&#25506;&#32034;&#31354;&#38388;&#27169;&#24335;&#30452;&#35273;
&lt;/p&gt;
&lt;p&gt;
Exploring Spatial Schema Intuitions in Large Language and Vision Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00956
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20877;&#29616;&#24515;&#29702;&#35821;&#35328;&#23398;&#23454;&#39564;&#65292;&#30740;&#31350;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23613;&#31649;&#26080;&#20855;&#36523;&#24615;&#65292;&#21364;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#21040;&#20154;&#31867;&#23545;&#20110;&#35821;&#35328;&#20013;&#22522;&#26412;&#30340;&#31354;&#38388;&#26500;&#24314;&#22359;&#30340;&#38544;&#21547;&#30452;&#35273;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#35821;&#35328;&#21644;&#31354;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#38750;&#24120;&#26222;&#36941;&#65292;&#20294;&#20851;&#20110;LLM&#20013;&#30340;&#20855;&#36523;&#24615;&#38382;&#39064;&#36824;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#65292;&#36825;&#20351;&#23427;&#20204;&#19981;&#21516;&#20110;&#26426;&#22120;&#20154;&#20013;&#20855;&#20307;&#30340;&#20855;&#36523;&#31995;&#32479;&#65292;&#26426;&#22120;&#20154;&#21487;&#20197;&#36890;&#36807;&#24863;&#30693;&#30452;&#25509;&#25351;&#23548;&#34892;&#21160;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#38382;&#39064;&#65292;&#21363;LLM&#26159;&#21542;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#21040;&#20154;&#31867;&#23545;&#20110;&#35821;&#35328;&#20013;&#22522;&#26412;&#30340;&#31354;&#38388;&#26500;&#24314;&#22359;&#30340;&#38544;&#21547;&#30452;&#35273;&#65292;&#23613;&#31649;&#23427;&#20204;&#26159;&#38750;&#20855;&#36523;&#30340;&#12290;&#25105;&#20204;&#36816;&#29992;&#20174;&#26089;&#26399;&#24863;&#30693;&#36816;&#21160;&#32463;&#39564;&#20013;&#21457;&#23637;&#20986;&#30340;&#31354;&#38388;&#35748;&#30693;&#22522;&#30784;&#30340;&#35265;&#35299;&#65292;&#36890;&#36807;&#20877;&#29616;&#19977;&#20010;&#24515;&#29702;&#35821;&#35328;&#23398;&#23454;&#39564;&#26469;&#25351;&#23548;&#25105;&#20204;&#30340;&#25506;&#32034;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#27169;&#22411;&#36755;&#20986;&#19982;&#20154;&#31867;&#22238;&#31572;&#20043;&#38388;&#20986;&#29616;&#20102;&#30456;&#20851;&#24615;&#65292;&#25581;&#31034;&#20102;&#27809;&#26377;&#19982;&#20855;&#20307;&#32463;&#39564;&#26377;&#23454;&#36136;&#36830;&#25509;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;&#26174;&#33879;&#30340;&#21306;&#21035;&#21253;&#25324;&#26497;&#21270;&#30340;&#35821;&#35328;&#27169;&#22411;&#21709;&#24212;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#38477;&#20302;&#30340;&#30456;&#20851;&#24615;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#20110;&#28145;&#20837;&#20102;&#35299;&#35821;&#35328;&#21644;&#31354;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#26377;&#30528;&#24494;&#22937;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the ubiquity of large language models (LLMs) in AI research, the question of embodiment in LLMs remains underexplored, distinguishing them from embodied systems in robotics where sensory perception directly informs physical action. Our investigation navigates the intriguing terrain of whether LLMs, despite their non-embodied nature, effectively capture implicit human intuitions about fundamental, spatial building blocks of language. We employ insights from spatial cognitive foundations developed through early sensorimotor experiences, guiding our exploration through the reproduction of three psycholinguistic experiments. Surprisingly, correlations between model outputs and human responses emerge, revealing adaptability without a tangible connection to embodied experiences. Notable distinctions include polarized language model responses and reduced correlations in vision language models. This research contributes to a nuanced understanding of the interplay between language, spat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20197;&#35789;&#27719;&#20026;&#23450;&#20041;&#30340;&#35821;&#20041;&#23398;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;LM&#28508;&#22312;&#31354;&#38388;&#30340;&#21442;&#32771;&#26694;&#26550;&#65292;&#30830;&#20445;&#22522;&#20110;LM&#35789;&#27719;&#30340;&#20998;&#31163;&#35821;&#20041;&#20998;&#26512;&#12290;&#22312;LM&#36866;&#24212;&#36807;&#31243;&#20013;&#65292;&#24341;&#20837;&#20102;&#35745;&#31639;logits&#30340;&#26032;&#25216;&#26415;&#21644;&#31070;&#32463;&#32858;&#31867;&#27169;&#22359;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#25991;&#26412;&#29702;&#35299;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2401.16184</link><description>&lt;p&gt;
&#20851;&#20110;LM&#28508;&#22312;&#31354;&#38388;&#30340;&#35821;&#20041;&#23398;&#65306;&#19968;&#31181;&#20197;&#35789;&#27719;&#20026;&#23450;&#20041;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
On the Semantics of LM Latent Space: A Vocabulary-defined Approach
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2401.16184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20197;&#35789;&#27719;&#20026;&#23450;&#20041;&#30340;&#35821;&#20041;&#23398;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;LM&#28508;&#22312;&#31354;&#38388;&#30340;&#21442;&#32771;&#26694;&#26550;&#65292;&#30830;&#20445;&#22522;&#20110;LM&#35789;&#27719;&#30340;&#20998;&#31163;&#35821;&#20041;&#20998;&#26512;&#12290;&#22312;LM&#36866;&#24212;&#36807;&#31243;&#20013;&#65292;&#24341;&#20837;&#20102;&#35745;&#31639;logits&#30340;&#26032;&#25216;&#26415;&#21644;&#31070;&#32463;&#32858;&#31867;&#27169;&#22359;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#25991;&#26412;&#29702;&#35299;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;(LM)&#30340;&#28508;&#22312;&#31354;&#38388;&#23545;&#20110;&#25913;&#36827;&#20854;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#20998;&#26512;&#24448;&#24448;&#22312;&#25552;&#20379;&#22522;&#20110;&#27169;&#22411;&#30340;&#23545;LM&#35821;&#20041;&#30340;&#20998;&#31163;&#27934;&#23519;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#24182;&#24573;&#35270;&#20102;LM&#36866;&#24212;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#20026;&#20102;&#21709;&#24212;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#24320;&#21019;&#24615;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#20197;&#35789;&#27719;&#20026;&#23450;&#20041;&#30340;&#35821;&#20041;&#23398;&#65292;&#23427;&#22312;LM&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#24314;&#31435;&#20102;&#19968;&#20010;&#21442;&#32771;&#26694;&#26550;&#65292;&#30830;&#20445;&#22522;&#20110;LM&#35789;&#27719;&#30340;&#20998;&#31163;&#35821;&#20041;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#20132;&#32455;&#20998;&#26512;&#65292;&#21033;&#29992;LM&#35789;&#27719;&#26469;&#33719;&#24471;&#20197;&#27169;&#22411;&#20026;&#20013;&#24515;&#30340;&#27934;&#23519;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;logits&#30340;&#26032;&#25216;&#26415;&#65292;&#24378;&#35843;&#21487;&#24494;&#20998;&#24615;&#21644;&#23616;&#37096;&#31561;&#36317;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#31070;&#32463;&#32858;&#31867;&#27169;&#22359;&#65292;&#29992;&#20110;&#22312;LM&#36866;&#24212;&#36807;&#31243;&#20013;&#36827;&#34892;&#35821;&#20041;&#26657;&#20934;&#12290;&#36890;&#36807;&#22312;&#22810;&#31181;&#25991;&#26412;&#29702;&#35299;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#38754;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the latent space of language models (LM) is crucial to refining their performance and interpretability. Existing analyses often fall short in providing disentangled (model-centric) insights into LM semantics, and neglect essential aspects of LM adaption. In response, we introduce a pioneering method called vocabulary-defined semantics, which establishes a reference frame within the LM latent space, ensuring disentangled semantic analysis grounded in LM vocabulary. Our approach transcends prior entangled analysis, leveraging LM vocabulary for model-centric insights. Furthermore, we propose a novel technique to compute logits, emphasising differentiability and local isotropy, and introduce a neural clustering module for semantically calibrating data representations during LM adaptation. Through extensive experiments across diverse text understanding datasets, our approach outperforms state-of-the-art methods of retrieval-augmented generation and parameter-efficient finetuni
&lt;/p&gt;</description></item><item><title>&#36880;&#23618;&#37325;&#35201;&#24615;&#37319;&#26679;&#30340;&#26032;&#26041;&#27861;LISA&#22312;&#24494;&#35843;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#35760;&#24518;&#25104;&#26412;&#20302;&#19988;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.17919</link><description>&lt;p&gt;
LISA&#65306;&#29992;&#20110;&#39640;&#25928;&#20869;&#23384;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;&#36880;&#23618;&#37325;&#35201;&#24615;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17919
&lt;/p&gt;
&lt;p&gt;
&#36880;&#23618;&#37325;&#35201;&#24615;&#37319;&#26679;&#30340;&#26032;&#26041;&#27861;LISA&#22312;&#24494;&#35843;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#35760;&#24518;&#25104;&#26412;&#20302;&#19988;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#33258;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39318;&#27425;&#20986;&#29616;&#20197;&#26469;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#65292;&#28982;&#32780;&#23427;&#20204;&#24040;&#22823;&#30340;&#20869;&#23384;&#28040;&#32791;&#24050;&#25104;&#20026;&#22823;&#35268;&#27169;&#35757;&#32451;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35832;&#22914;&#20302;&#31209;&#35843;&#25972;&#65288;LoRA&#65289;&#20043;&#31867;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;&#26469;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#20294;&#22312;&#22823;&#22810;&#25968;&#22823;&#35268;&#27169;&#24494;&#35843;&#35774;&#32622;&#20013;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20173;&#26080;&#27861;&#19982;&#23436;&#25972;&#21442;&#25968;&#35757;&#32451;&#30456;&#21305;&#37197;&#12290;&#20026;&#24357;&#34917;&#36825;&#19968;&#19981;&#36275;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LoRA&#22312;&#24494;&#35843;&#20219;&#21153;&#20013;&#30340;&#36880;&#23618;&#29305;&#24615;&#65292;&#24182;&#35266;&#23519;&#21040;&#19981;&#21516;&#23618;&#20043;&#38388;&#26435;&#37325;&#33539;&#25968;&#30340;&#24322;&#24120;&#20559;&#26012;&#12290;&#21033;&#29992;&#36825;&#19968;&#20851;&#38190;&#35266;&#23519;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#31616;&#21333;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#35760;&#24518;&#25104;&#26412;&#20302;&#20110;LoRA&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#24191;&#27867;&#30340;&#35774;&#32622;&#20013;&#20248;&#20110;LoRA&#21644;&#23436;&#25972;&#21442;&#25968;&#35757;&#32451;&#12290;&#25105;&#20204;&#23558;&#20854;&#21629;&#21517;&#20026;Layerwise Importance Sampled AdamW&#65288;LISA&#65289;&#65292;&#36825;&#26159;LoRA&#30340;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#24212;&#29992;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17919v1 Announce Type: cross  Abstract: The machine learning community has witnessed impressive advancements since the first appearance of large language models (LLMs), yet their huge memory consumption has become a major roadblock to large-scale training. Parameter Efficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been proposed to alleviate this problem, but their performance still fails to match full parameter training in most large-scale fine-tuning settings. Attempting to complement this deficiency, we investigate layerwise properties of LoRA on fine-tuning tasks and observe an uncommon skewness of weight norms across different layers. Utilizing this key observation, a surprisingly simple training strategy is discovered, which outperforms both LoRA and full parameter training in a wide range of settings with memory costs as low as LoRA. We name it Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA, which applies the idea of
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-4&#22312;&#35821;&#27861;&#38169;&#35823;&#26657;&#27491;&#35780;&#20272;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#19982;&#20154;&#31867;&#21028;&#26029;&#26377;&#24456;&#39640;&#30340;&#30456;&#20851;&#24615;&#65292;&#20984;&#26174;&#20102;&#22312;&#35780;&#20272;&#26631;&#20934;&#20013;&#27969;&#30021;&#24230;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.17540</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#35821;&#27861;&#38169;&#35823;&#26657;&#27491;&#30340;&#26368;&#20808;&#36827;&#35780;&#20272;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Are State-of-the-Art Evaluator for Grammatical Error Correction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17540
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-4&#22312;&#35821;&#27861;&#38169;&#35823;&#26657;&#27491;&#35780;&#20272;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#19982;&#20154;&#31867;&#21028;&#26029;&#26377;&#24456;&#39640;&#30340;&#30456;&#20851;&#24615;&#65292;&#20984;&#26174;&#20102;&#22312;&#35780;&#20272;&#26631;&#20934;&#20013;&#27969;&#30021;&#24230;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25454;&#25253;&#36947;&#22312;&#26576;&#20123;&#20219;&#21153;&#20013;&#32988;&#36807;&#29616;&#26377;&#30340;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#65292;&#27604;&#22914;&#25991;&#26412;&#24635;&#32467;&#21644;&#26426;&#22120;&#32763;&#35793;&#12290;&#20294;&#26159;&#65292;&#20851;&#20110;LLMs&#22312;&#35821;&#27861;&#38169;&#35823;&#26657;&#27491;&#65288;GEC&#65289;&#20013;&#20316;&#20026;&#35780;&#20272;&#22120;&#30340;&#30740;&#31350;&#36824;&#19981;&#36275;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#37319;&#29992;&#35774;&#35745;&#30340;&#25552;&#31034;&#26469;&#25972;&#21512;&#20808;&#21069;&#30740;&#31350;&#21551;&#21457;&#30340;&#21508;&#31181;&#35780;&#20272;&#26631;&#20934;&#65292;&#35843;&#26597;&#20102;LLMs&#22312;GEC&#35780;&#20272;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-4&#22312;&#19982;&#20154;&#31867;&#21028;&#26029;&#20043;&#38388;&#36798;&#21040;&#20102;0.662&#30340;Kendall&#31561;&#32423;&#30456;&#20851;&#24615;&#65292;&#36229;&#36807;&#20102;&#25152;&#26377;&#29616;&#26377;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#22312;&#26368;&#36817;&#30340;GEC&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;LLMs&#35268;&#27169;&#30340;&#37325;&#35201;&#24615;&#65292;&#29305;&#21035;&#24378;&#35843;&#20102;&#27969;&#30021;&#24230;&#22312;&#35780;&#20272;&#26631;&#20934;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17540v1 Announce Type: new  Abstract: Large Language Models (LLMs) have been reported to outperform existing automatic evaluation metrics in some tasks, such as text summarization and machine translation. However, there has been a lack of research on LLMs as evaluators in grammatical error correction (GEC). In this study, we investigate the performance of LLMs in GEC evaluation by employing prompts designed to incorporate various evaluation criteria inspired by previous research. Our extensive experimental results demonstrate that GPT-4 achieved Kendall's rank correlation of 0.662 with human judgments, surpassing all existing methods. Furthermore, in recent GEC evaluations, we have underscored the significance of the LLMs scale and particularly emphasized the importance of fluency among evaluation criteria.
&lt;/p&gt;</description></item><item><title>MIntRec2.0&#20171;&#32461;&#20102;&#19968;&#20010;&#26088;&#22312;&#35299;&#20915;&#22810;&#27169;&#24577;&#24847;&#22270;&#35782;&#21035;&#21644;&#23545;&#35805;&#20013;&#22330;&#22806;&#26816;&#27979;&#25361;&#25112;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;30&#20010;&#32454;&#31890;&#24230;&#31867;&#21035;&#30340;1,245&#20010;&#23545;&#35805;&#21644;15,040&#20010;&#26679;&#26412;&#65292;&#20854;&#20013;&#21253;&#25324;&#36924;&#30495;&#30340;&#22330;&#22806;&#26679;&#26412;&#65292;&#24182;&#20016;&#23500;&#20102;&#21457;&#35328;&#32773;&#20449;&#24687;&#20197;&#25903;&#25345;&#22810;&#26041;&#23545;&#35805;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2403.10943</link><description>&lt;p&gt;
MIntRec2.0&#65306;&#29992;&#20110;&#22810;&#27169;&#24577;&#24847;&#22270;&#35782;&#21035;&#21644;&#23545;&#35805;&#20013;&#22330;&#22806;&#26816;&#27979;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MIntRec2.0: A Large-scale Benchmark Dataset for Multimodal Intent Recognition and Out-of-scope Detection in Conversations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10943
&lt;/p&gt;
&lt;p&gt;
MIntRec2.0&#20171;&#32461;&#20102;&#19968;&#20010;&#26088;&#22312;&#35299;&#20915;&#22810;&#27169;&#24577;&#24847;&#22270;&#35782;&#21035;&#21644;&#23545;&#35805;&#20013;&#22330;&#22806;&#26816;&#27979;&#25361;&#25112;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;30&#20010;&#32454;&#31890;&#24230;&#31867;&#21035;&#30340;1,245&#20010;&#23545;&#35805;&#21644;15,040&#20010;&#26679;&#26412;&#65292;&#20854;&#20013;&#21253;&#25324;&#36924;&#30495;&#30340;&#22330;&#22806;&#26679;&#26412;&#65292;&#24182;&#20016;&#23500;&#20102;&#21457;&#35328;&#32773;&#20449;&#24687;&#20197;&#25903;&#25345;&#22810;&#26041;&#23545;&#35805;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#24847;&#22270;&#35782;&#21035;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#65292;&#38656;&#35201;&#25972;&#21512;&#26469;&#33258;&#29616;&#23454;&#19990;&#30028;&#32972;&#26223;&#30340;&#38750;&#35821;&#35328;&#24418;&#24335;&#65292;&#20197;&#22686;&#24378;&#23545;&#20154;&#31867;&#24847;&#22270;&#30340;&#29702;&#35299;&#12290;&#29616;&#26377;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#22312;&#35268;&#27169;&#19978;&#21463;&#38480;&#65292;&#24182;&#19988;&#22312;&#22788;&#29702;&#22810;&#36718;&#23545;&#35805;&#20114;&#21160;&#20013;&#20986;&#29616;&#30340;&#22330;&#22806;&#26679;&#26412;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;MIntRec2.0&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#26041;&#23545;&#35805;&#20013;&#30340;&#22810;&#27169;&#24577;&#24847;&#22270;&#35782;&#21035;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#23427;&#21253;&#21547;1,245&#20010;&#23545;&#35805;&#65292;15,040&#20010;&#26679;&#26412;&#65292;&#27599;&#20010;&#26679;&#26412;&#22312;30&#20010;&#32454;&#31890;&#24230;&#31867;&#21035;&#30340;&#26032;&#24847;&#22270;&#20998;&#31867;&#20013;&#36827;&#34892;&#20102;&#27880;&#37322;&#12290;&#38500;&#20102;9,304&#20010;&#22330;&#20869;&#26679;&#26412;&#22806;&#65292;&#36824;&#21253;&#25324;5,736&#20010;&#20986;&#29616;&#22312;&#22810;&#36718;&#19978;&#19979;&#25991;&#20013;&#30340;&#22330;&#22806;&#26679;&#26412;&#65292;&#36825;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#33258;&#28982;&#21457;&#29983;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#27599;&#20010;&#35805;&#35821;&#20013;&#21457;&#35328;&#32773;&#30340;&#35814;&#32454;&#20449;&#24687;&#65292;&#20016;&#23500;&#20102;&#23427;&#22312;&#22810;&#26041;&#23545;&#35805;&#30740;&#31350;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10943v1 Announce Type: cross  Abstract: Multimodal intent recognition poses significant challenges, requiring the incorporation of non-verbal modalities from real-world contexts to enhance the comprehension of human intentions. Existing benchmark datasets are limited in scale and suffer from difficulties in handling out-of-scope samples that arise in multi-turn conversational interactions. We introduce MIntRec2.0, a large-scale benchmark dataset for multimodal intent recognition in multi-party conversations. It contains 1,245 dialogues with 15,040 samples, each annotated within a new intent taxonomy of 30 fine-grained classes. Besides 9,304 in-scope samples, it also includes 5,736 out-of-scope samples appearing in multi-turn contexts, which naturally occur in real-world scenarios. Furthermore, we provide comprehensive information on the speakers in each utterance, enriching its utility for multi-party conversational research. We establish a general framework supporting the o
&lt;/p&gt;</description></item><item><title>&#34394;&#20551;&#20449;&#24687;&#19981;&#20165;&#20165;&#28041;&#21450;&#38169;&#35823;&#30340;&#20107;&#23454;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#32447;&#36793;&#32536;&#24847;&#35782;&#24418;&#24577;&#36890;&#36807;&#20849;&#35782;&#21644;&#8220;&#20107;&#23454;&#27491;&#30830;&#8221;&#30340;&#20869;&#23481;&#20256;&#25773;&#65292;&#21516;&#26102;&#21457;&#29616;&#20102;&#26497;&#21491;&#29992;&#25143;&#20542;&#21521;&#25361;&#36873;&#20851;&#27880;&#29305;&#23450;&#20027;&#39064;&#30340;&#26032;&#38395;&#65292;&#24182;&#19988;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#30340;&#27807;&#36890;&#39118;&#26684;&#35782;&#21035;&#26131;&#20110;&#20998;&#20139;&#38169;&#35823;&#20449;&#24687;&#30340;&#29992;&#25143;&#12290;</title><link>https://arxiv.org/abs/2403.08391</link><description>&lt;p&gt;
&#34394;&#20551;&#20449;&#24687;&#19981;&#26159;&#20851;&#20110;&#38169;&#35823;&#20107;&#23454;&#65306;&#23545;&#36793;&#32536;&#20869;&#23481;&#30340;&#29983;&#20135;&#21644;&#28040;&#36153;&#36827;&#34892;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Misinformation is not about Bad Facts: An Analysis of the Production and Consumption of Fringe Content
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08391
&lt;/p&gt;
&lt;p&gt;
&#34394;&#20551;&#20449;&#24687;&#19981;&#20165;&#20165;&#28041;&#21450;&#38169;&#35823;&#30340;&#20107;&#23454;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#32447;&#36793;&#32536;&#24847;&#35782;&#24418;&#24577;&#36890;&#36807;&#20849;&#35782;&#21644;&#8220;&#20107;&#23454;&#27491;&#30830;&#8221;&#30340;&#20869;&#23481;&#20256;&#25773;&#65292;&#21516;&#26102;&#21457;&#29616;&#20102;&#26497;&#21491;&#29992;&#25143;&#20542;&#21521;&#25361;&#36873;&#20851;&#27880;&#29305;&#23450;&#20027;&#39064;&#30340;&#26032;&#38395;&#65292;&#24182;&#19988;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#30340;&#27807;&#36890;&#39118;&#26684;&#35782;&#21035;&#26131;&#20110;&#20998;&#20139;&#38169;&#35823;&#20449;&#24687;&#30340;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#35823;&#20256;&#26681;&#26412;&#19981;&#26159;&#19968;&#20010;&#20449;&#24687;&#38382;&#39064;&#21602;&#65311;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#32447;&#36793;&#32536;&#24847;&#35782;&#24418;&#24577;&#36890;&#36807;&#19968;&#31181;&#22522;&#20110;&#20849;&#35782;&#21644;&#8220;&#20107;&#23454;&#27491;&#30830;&#8221;&#30340;&#20869;&#23481;&#20256;&#25773;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20855;&#26377;&#20013;&#38388;&#21644;&#26497;&#21491;&#25919;&#27835;&#20542;&#21521;&#30340;&#28595;&#22823;&#21033;&#20122;&#26032;&#38395;&#20986;&#29256;&#21830;&#22312;&#20449;&#24687;&#23436;&#25972;&#24615;&#21644;&#36136;&#37327;&#26041;&#38754;&#23621;&#20110;&#21487;&#27604;&#27700;&#24179;&#65307;&#27492;&#22806;&#65292;&#26497;&#21491;&#32764;Twitter&#29992;&#25143;&#32463;&#24120;&#20998;&#20139;&#26469;&#33258;&#20013;&#38388;&#26469;&#28304;&#30340;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#24403;&#25105;&#20204;&#32771;&#34385;&#21040;&#20004;&#20010;&#39069;&#22806;&#22240;&#32032;&#26102;&#65292;&#20986;&#29616;&#20102;&#26126;&#26174;&#24046;&#24322;&#65306;1&#65289;&#26497;&#21491;&#29992;&#25143;&#23545;&#25991;&#31456;&#30340;&#29421;&#38552;&#20027;&#39064;&#36873;&#25321;&#65292;&#26263;&#31034;&#20182;&#20204;&#21482;&#25361;&#36873;&#19982;&#20182;&#20204;&#20851;&#27880;&#29305;&#23450;&#20027;&#39064;&#30340;&#26032;&#38395;&#25991;&#31456;&#65292;2&#65289;&#22312;&#23457;&#26597;&#25991;&#31456;&#20889;&#20316;&#39118;&#26684;&#26102;&#65292;&#20013;&#38388;&#21644;&#26497;&#21491;&#20986;&#29256;&#21830;&#20043;&#38388;&#30340;&#24040;&#22823;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#29978;&#33267;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#30340;&#27807;&#36890;&#39118;&#26684;&#35782;&#21035;&#26131;&#20110;&#20998;&#20139;&#38169;&#35823;&#20449;&#24687;&#30340;&#29992;&#25143;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#20110;&#21512;&#20316;&#26041;&#38754;&#26377;&#37325;&#35201;&#30340;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08391v1 Announce Type: new  Abstract: What if misinformation is not an information problem at all? Our findings suggest that online fringe ideologies spread through the use of content that is consensus-based and "factually correct". We found that Australian news publishers with both moderate and far-right political leanings contain comparable levels of information completeness and quality; and furthermore, that far-right Twitter users often share from moderate sources. However, a stark difference emerges when we consider two additional factors: 1) the narrow topic selection of articles by far-right users, suggesting that they cherrypick only news articles that engage with specific topics of their concern, and 2) the difference between moderate and far-right publishers when we examine the writing style of their articles. Furthermore, we can even identify users prone to sharing misinformation based on their communication style. These findings have important implications for co
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;SPA&#65288;Side Plugin Adaption&#65289;&#30340;&#36731;&#37327;&#32423;&#26550;&#26500;&#65292;&#29992;&#20110;&#22312;&#20005;&#26684;&#30340;&#35774;&#22791;&#35745;&#31639;&#21644;&#20869;&#23384;&#32422;&#26463;&#26465;&#20214;&#19979;&#24555;&#36895;&#36827;&#34892;&#25512;&#26029;&#65292;&#21516;&#26102;&#20445;&#30041;&#38544;&#31169;&#12290;</title><link>https://arxiv.org/abs/2403.07088</link><description>&lt;p&gt;
SPA&#65306;&#38754;&#21521;&#20113;&#31471;&#21644;&#35774;&#22791;&#21327;&#20316;&#30340;&#35745;&#31639;&#21451;&#22909;&#22411;Seq2seq&#20010;&#24615;&#21270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
SPA: Towards A Computational Friendly Cloud-Base and On-Devices Collaboration Seq2seq Personalized Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07088
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SPA&#65288;Side Plugin Adaption&#65289;&#30340;&#36731;&#37327;&#32423;&#26550;&#26500;&#65292;&#29992;&#20110;&#22312;&#20005;&#26684;&#30340;&#35774;&#22791;&#35745;&#31639;&#21644;&#20869;&#23384;&#32422;&#26463;&#26465;&#20214;&#19979;&#24555;&#36895;&#36827;&#34892;&#25512;&#26029;&#65292;&#21516;&#26102;&#20445;&#30041;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#34920;&#29616;&#20986;&#33394;&#30340;&#33021;&#21147;&#24050;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#38382;&#31572;&#20013;&#24471;&#21040;&#23637;&#31034;&#12290;&#28982;&#32780;&#65292;LLMs&#38656;&#35201;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#22823;&#20869;&#23384;&#25104;&#26412;&#12290;&#21516;&#26102;&#65292;&#24403;&#35757;&#32451;&#25110;&#39044;&#27979;&#36807;&#31243;&#21253;&#21547;&#25935;&#24863;&#20449;&#24687;&#26102;&#65292;LLMs&#21487;&#33021;&#20250;&#23548;&#33268;&#38544;&#31169;&#27844;&#38706;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SPA&#65288;Side Plugin Adaption&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#26550;&#26500;&#65292;&#29992;&#20110;&#24555;&#36895;&#35774;&#22791;&#19978;&#30340;&#25512;&#26029;&#21644;&#22312;&#20005;&#26684;&#30340;&#35774;&#22791;&#35745;&#31639;&#21644;&#20869;&#23384;&#32422;&#26463;&#26465;&#20214;&#19979;&#20445;&#25345;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07088v1 Announce Type: new  Abstract: Large language models(LLMs) have shown its outperforming ability on various tasks and question answering. However, LLMs require high computation cost and large memory cost. At the same time, LLMs may cause privacy leakage when training or prediction procedure contains sensitive information. In this paper, we propose SPA(Side Plugin Adaption), a lightweight architecture for fast on-devices inference and privacy retaining on the constraints of strict on-devices computation and memory constraints. Compared with other on-devices seq2seq generation, SPA could make a fast and stable inference on low-resource constraints, allowing it to obtain cost effiency. Our method establish an interaction between a pretrained LLMs on-cloud and additive parameters on-devices, which could provide the knowledge on both pretrained LLMs and private personal feature.Further more, SPA provides a framework to keep feature-base parameters on private guaranteed but 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;ConAgents&#26694;&#26550;&#65292;&#36890;&#36807;&#21512;&#20316;&#21644;&#20114;&#21160;&#20195;&#29702;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#20351;&#29992;&#24037;&#20855;&#65292;&#24182;&#24341;&#20837;&#20102;&#36845;&#20195;&#26657;&#20934;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#21333;&#20010;&#20195;&#29702;&#25191;&#34892;&#22810;&#26679;&#21270;&#25805;&#20316;&#33021;&#21147;&#26377;&#38480;&#21644;&#33258;&#36866;&#24212;&#32416;&#27491;&#38169;&#35823;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.03031</link><description>&lt;p&gt;
&#36890;&#36807;&#21512;&#20316;&#21644;&#20114;&#21160;&#20195;&#29702;&#23398;&#20064;&#20351;&#29992;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Learning to Use Tools via Cooperative and Interactive Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03031
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;ConAgents&#26694;&#26550;&#65292;&#36890;&#36807;&#21512;&#20316;&#21644;&#20114;&#21160;&#20195;&#29702;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#20351;&#29992;&#24037;&#20855;&#65292;&#24182;&#24341;&#20837;&#20102;&#36845;&#20195;&#26657;&#20934;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#21333;&#20010;&#20195;&#29702;&#25191;&#34892;&#22810;&#26679;&#21270;&#25805;&#20316;&#33021;&#21147;&#26377;&#38480;&#21644;&#33258;&#36866;&#24212;&#32416;&#27491;&#38169;&#35823;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#20855;&#23398;&#20064;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#20195;&#29702;&#20154;&#33021;&#22815;&#20351;&#29992;&#22806;&#37096;&#24037;&#20855;&#26469;&#25193;&#23637;&#20854;&#21151;&#33021;&#12290;&#29616;&#26377;&#26041;&#27861;&#21033;&#29992;&#21333;&#20010;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#24490;&#29615;&#36873;&#25321;&#21644;&#25191;&#34892;&#24037;&#20855;&#65292;&#28982;&#21518;&#23558;&#32467;&#26524;&#21512;&#24182;&#21040;&#19979;&#19968;&#20010;&#21160;&#20316;&#39044;&#27979;&#20013;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#22788;&#29702;&#22797;&#26434;&#20219;&#21153;&#26102;&#20173;&#28982;&#23384;&#22312;&#28508;&#22312;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#65292;&#21407;&#22240;&#26159;&#65306;&#65288;1&#65289;&#21333;&#20010;LLM&#30340;&#22266;&#26377;&#33021;&#21147;&#25191;&#34892;&#22810;&#26679;&#21270;&#25805;&#20316;&#21463;&#38480;&#65292;&#20197;&#21450;&#65288;2&#65289;&#22312;&#20219;&#21153;&#22833;&#36133;&#26102;&#38590;&#20197;&#33258;&#36866;&#24212;&#22320;&#32416;&#27491;&#38169;&#35823;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ConAgents&#65292;&#21363;&#21512;&#20316;&#21644;&#20114;&#21160;&#20195;&#29702;&#26694;&#26550;&#65292;&#23558;&#24037;&#20855;&#23398;&#20064;&#30340;&#24037;&#20316;&#27969;&#27169;&#22359;&#21270;&#20026;Grounding&#65288;&#22522;&#30784;&#65289;&#12289;Execution&#65288;&#25191;&#34892;&#65289;&#21644;Observing&#65288;&#35266;&#23519;&#65289;&#20195;&#29702;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#36845;&#20195;&#26657;&#20934;&#65288;IterCali&#65289;&#26041;&#27861;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#26681;&#25454;&#26469;&#33258;&#24037;&#20855;&#29615;&#22659;&#30340;&#21453;&#39304;&#23545;&#33258;&#24049;&#36827;&#34892;&#35843;&#25972;&#12290;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#36229;&#36807;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03031v1 Announce Type: new  Abstract: Tool learning empowers large language models (LLMs) as agents to use external tools to extend their capability. Existing methods employ one single LLM-based agent to iteratively select and execute tools, thereafter incorporating the result into the next action prediction. However, they still suffer from potential performance degradation when addressing complex tasks due to: (1) the limitation of the inherent capability of a single LLM to perform diverse actions, and (2) the struggle to adaptively correct mistakes when the task fails. To mitigate these problems, we propose the ConAgents, a Cooperative and interactive Agents framework, which modularizes the workflow of tool learning into Grounding, Execution, and Observing agents. We also introduce an iterative calibration (IterCali) method, enabling the agents to adapt themselves based on the feedback from the tool environment. Experiments conducted on three datasets demonstrate the super
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#23545;LLMs&#22312;&#36234;&#21335;&#35821;&#19978;&#36827;&#34892;&#24494;&#35843;&#21644;&#32508;&#21512;&#35780;&#20272;&#65292;&#21457;&#29616;&#24494;&#35843;&#21518;&#30340;&#27169;&#22411;&#22312;&#36234;&#21335;&#35821;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#33021;&#21147;&#65292;&#21516;&#26102;&#25351;&#20986;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#19982;&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#30528;&#26435;&#34913;&#20851;&#31995;&#65292;&#35757;&#32451;&#25110;&#24494;&#35843;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#26159;&#24433;&#21709;LLM&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;</title><link>https://arxiv.org/abs/2403.02715</link><description>&lt;p&gt;
&#36328;&#36234;&#35821;&#35328;&#35270;&#37326;&#65306;&#36234;&#21335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#21644;&#32508;&#21512;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Crossing Linguistic Horizons: Finetuning and Comprehensive Evaluation of Vietnamese Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02715
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#23545;LLMs&#22312;&#36234;&#21335;&#35821;&#19978;&#36827;&#34892;&#24494;&#35843;&#21644;&#32508;&#21512;&#35780;&#20272;&#65292;&#21457;&#29616;&#24494;&#35843;&#21518;&#30340;&#27169;&#22411;&#22312;&#36234;&#21335;&#35821;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#33021;&#21147;&#65292;&#21516;&#26102;&#25351;&#20986;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#19982;&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#30528;&#26435;&#34913;&#20851;&#31995;&#65292;&#35757;&#32451;&#25110;&#24494;&#35843;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#26159;&#24433;&#21709;LLM&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#24378;&#35843;&#20102;&#23427;&#20204;&#22312;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22312;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#39044;&#35757;&#32451;&#65292;&#20294;&#30446;&#21069;&#24320;&#28304;&#30340;LLMs&#22312;&#22788;&#29702;&#36234;&#21335;&#35821;&#26041;&#38754;&#30340;&#25928;&#26524;&#26377;&#38480;&#12290;&#36825;&#19968;&#25361;&#25112;&#26159;&#30001;&#20110;&#32570;&#20047;&#19987;&#38376;&#38024;&#23545;&#36234;&#21335;&#35821;LLM&#35780;&#20272;&#30340;&#31995;&#32479;&#21270;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#25351;&#26631;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#19987;&#38376;&#20026;&#36234;&#21335;&#35821;&#36827;&#34892;&#20102;LLM&#30340;&#24494;&#35843;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#28085;&#30422;10&#20010;&#24120;&#35265;&#20219;&#21153;&#21644;31&#20010;&#25351;&#26631;&#30340;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;LLMs&#22312;&#36234;&#21335;&#35821;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#22686;&#24378;&#30340;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#20855;&#26377;&#26356;&#22810;&#21442;&#25968;&#30340;&#27169;&#22411;&#21487;&#33021;&#20250;&#24102;&#26469;&#26356;&#22810;&#30340;&#20559;&#35265;&#21644;&#26410;&#26657;&#20934;&#30340;&#36755;&#20986;&#65292;&#32780;&#24433;&#21709;LLM&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#26159;&#35757;&#32451;&#25110;&#24494;&#35843;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02715v1 Announce Type: cross  Abstract: Recent advancements in large language models (LLMs) have underscored their importance in the evolution of artificial intelligence. However, despite extensive pretraining on multilingual datasets, available open-sourced LLMs exhibit limited effectiveness in processing Vietnamese. The challenge is exacerbated by the absence of systematic benchmark datasets and metrics tailored for Vietnamese LLM evaluation. To mitigate these issues, we have finetuned LLMs specifically for Vietnamese and developed a comprehensive evaluation framework encompassing 10 common tasks and 31 metrics. Our evaluation results reveal that the fine-tuned LLMs exhibit enhanced comprehension and generative capabilities in Vietnamese. Moreover, our analysis indicates that models with more parameters can introduce more biases and uncalibrated outputs and the key factor influencing LLM performance is the quality of the training or fine-tuning datasets. These insights und
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;SEEDA&#65292;&#19968;&#20010;&#29992;&#20110;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#23545;12&#31181;&#26368;&#20808;&#36827;&#31995;&#32479;&#36827;&#34892;&#20803;&#35780;&#20272;&#30340;&#26657;&#27491;&#65292;&#36890;&#36807;&#22312;&#21477;&#23376;&#32423;&#21035;&#20803;&#35780;&#20272;&#20013;&#23545;&#31890;&#24230;&#36827;&#34892;&#23545;&#40784;&#65292;&#25552;&#39640;&#20102;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.02674</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#30340;&#20803;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Revisiting Meta-evaluation for Grammatical Error Correction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02674
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;SEEDA&#65292;&#19968;&#20010;&#29992;&#20110;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#23545;12&#31181;&#26368;&#20808;&#36827;&#31995;&#32479;&#36827;&#34892;&#20803;&#35780;&#20272;&#30340;&#26657;&#27491;&#65292;&#36890;&#36807;&#22312;&#21477;&#23376;&#32423;&#21035;&#20803;&#35780;&#20272;&#20013;&#23545;&#31890;&#24230;&#36827;&#34892;&#23545;&#40784;&#65292;&#25552;&#39640;&#20102;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Metrics&#22312;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#65288;GEC&#65289;&#20013;&#33258;&#21160;&#35780;&#20272;&#30340;&#22522;&#30784;&#12290;&#20854;&#20013;&#65292;&#20803;&#35780;&#20272;&#20381;&#36182;&#20110;&#23427;&#20204;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#33521;&#35821;GEC&#20013;&#24120;&#35268;&#30340;&#20803;&#35780;&#20272;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#65292;&#21253;&#25324;&#30001;&#20110;&#35780;&#20272;&#31890;&#24230;&#19981;&#19968;&#33268;&#32780;&#23548;&#33268;&#30340;&#20559;&#35265;&#65292;&#20197;&#21450;&#20351;&#29992;&#20256;&#32479;&#31995;&#32479;&#30340;&#36807;&#26102;&#35774;&#32622;&#12290;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;SEEDA&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;GEC&#20803;&#35780;&#20272;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#26657;&#27491;&#21644;&#20004;&#31181;&#19981;&#21516;&#31890;&#24230;&#65288;&#22522;&#20110;&#32534;&#36753;&#21644;&#22522;&#20110;&#21477;&#23376;&#65289;&#30340;&#20154;&#31867;&#35780;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02674v1 Announce Type: new  Abstract: Metrics are the foundation for automatic evaluation in grammatical error correction (GEC), with their evaluation of the metrics (meta-evaluation) relying on their correlation with human judgments. However, conventional meta-evaluations in English GEC encounter several challenges including biases caused by inconsistencies in evaluation granularity, and an outdated setup using classical systems. These problems can lead to misinterpretation of metrics and potentially hinder the applicability of GEC techniques. To address these issues, this paper proposes SEEDA, a new dataset for GEC meta-evaluation. SEEDA consists of corrections with human ratings along two different granularities: edit-based and sentence-based, covering 12 state-of-the-art systems including large language models (LLMs), and two human corrections with different focuses. The results of improved correlations by aligning the granularity in the sentence-level meta-evaluation, s
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#25506;&#26597;&#37319;&#26679;&#8221;&#30340;&#26032;&#31639;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#30830;&#23450;&#33609;&#31295;&#27169;&#22411;&#21644;&#30446;&#26631;&#27169;&#22411;&#30340;&#30456;&#20284;&#24230;&#65292;&#26469;&#21152;&#36895;&#36138;&#23146;&#22352;&#26631;&#26799;&#24230;&#31639;&#27861;&#65292;&#23454;&#29616;&#39640;&#36798;5.6&#20493;&#30340;&#21152;&#36895;&#12290;</title><link>https://arxiv.org/abs/2403.01251</link><description>&lt;p&gt;
&#36890;&#36807;&#25506;&#26597;&#37319;&#26679;&#21152;&#36895;&#36138;&#23146;&#22352;&#26631;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
Accelerating Greedy Coordinate Gradient via Probe Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01251
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#25506;&#26597;&#37319;&#26679;&#8221;&#30340;&#26032;&#31639;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#30830;&#23450;&#33609;&#31295;&#27169;&#22411;&#21644;&#30446;&#26631;&#27169;&#22411;&#30340;&#30456;&#20284;&#24230;&#65292;&#26469;&#21152;&#36895;&#36138;&#23146;&#22352;&#26631;&#26799;&#24230;&#31639;&#27861;&#65292;&#23454;&#29616;&#39640;&#36798;5.6&#20493;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23433;&#20840;&#24615;&#24050;&#25104;&#20026;&#19968;&#20010;&#20013;&#24515;&#38382;&#39064;&#65292;&#32771;&#34385;&#21040;&#23427;&#20204;&#30340;&#24555;&#36895;&#21457;&#23637;&#21644;&#24191;&#27867;&#24212;&#29992;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36138;&#23146;&#22352;&#26631;&#26799;&#24230;&#65288;GCG&#65289;&#22312;&#26500;&#24314;&#21253;&#21547;&#23545;&#25239;&#21518;&#32512;&#30340;&#25552;&#31034;&#26102;&#38750;&#24120;&#26377;&#25928;&#65292;&#20197;&#30772;&#22351;&#34987;&#35748;&#20026;&#26159;&#23433;&#20840;&#30340;LLMs&#65292;&#20294;GCG&#30340;&#20248;&#21270;&#32791;&#26102;&#36739;&#38271;&#65292;&#38480;&#21046;&#20102;&#20854;&#23454;&#29992;&#24615;&#12290;&#20026;&#20102;&#20943;&#23569;GCG&#30340;&#26102;&#38388;&#25104;&#26412;&#24182;&#23454;&#29616;&#23545;LLMs&#23433;&#20840;&#24615;&#26356;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#25506;&#26597;&#37319;&#26679;&#8221;&#30340;&#26032;&#31639;&#27861;&#65292;&#20197;&#21152;&#36895;GCG&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#26426;&#21046;&#65292;&#21160;&#24577;&#30830;&#23450;&#36739;&#23567;&#33609;&#31295;&#27169;&#22411;&#30340;&#39044;&#27979;&#19982;&#30446;&#26631;&#27169;&#22411;&#30340;&#25552;&#31034;&#20505;&#36873;&#39044;&#27979;&#30340;&#30456;&#20284;&#31243;&#24230;&#12290;&#24403;&#30446;&#26631;&#27169;&#22411;&#19982;&#33609;&#31295;&#27169;&#22411;&#30456;&#20284;&#26102;&#65292;&#25105;&#20204;&#22823;&#37327;&#20381;&#36182;&#20110;&#33609;&#31295;&#27169;&#22411;&#26469;&#36807;&#28388;&#22823;&#37327;&#28508;&#22312;&#25552;&#31034;&#20505;&#36873;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#26102;&#38388;&#12290;&#25506;&#26597;&#37319;&#26679;&#20351;&#29992;Llam&#23454;&#29616;&#39640;&#36798;5.6&#20493;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01251v1 Announce Type: new  Abstract: Safety of Large Language Models (LLMs) has become a central issue given their rapid progress and wide applications. Greedy Coordinate Gradient (GCG) is shown to be effective in constructing prompts containing adversarial suffixes to break the presumingly safe LLMs, but the optimization of GCG is time-consuming and limits its practicality. To reduce the time cost of GCG and enable more comprehensive studies of LLM safety, in this work, we study a new algorithm called $\texttt{Probe sampling}$ to accelerate the GCG algorithm. At the core of the algorithm is a mechanism that dynamically determines how similar a smaller draft model's predictions are to the target model's predictions for prompt candidates. When the target model is similar to the draft model, we rely heavily on the draft model to filter out a large number of potential prompt candidates to reduce the computation time. Probe sampling achieves up to $5.6$ times speedup using Llam
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Self-Synthesized Rehearsal&#65288;SSR&#65289;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#23454;&#20363;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#22797;&#36848;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36973;&#21463;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.01244</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#25105;&#29983;&#25104;&#30340;&#22797;&#36848;&#26469;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01244
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Self-Synthesized Rehearsal&#65288;SSR&#65289;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#23454;&#20363;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#22797;&#36848;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36973;&#21463;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25345;&#32493;&#23398;&#20064;&#36807;&#31243;&#20013;&#36973;&#21463;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#22797;&#36848;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#20808;&#21069;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#20445;&#30041;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#28982;&#32780;&#36825;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#21487;&#33021;&#26080;&#27861;&#23454;&#29616;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#33258;&#25105;&#29983;&#25104;&#22797;&#36848;&#65288;SSR&#65289;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;LLM&#29983;&#25104;&#21512;&#25104;&#23454;&#20363;&#36827;&#34892;&#22797;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01244v1 Announce Type: cross  Abstract: Large language models (LLMs) suffer from catastrophic forgetting during continual learning. Conventional rehearsal-based methods rely on previous training data to retain the model's ability, which may not be feasible in real-world applications. When conducting continual learning based on a publicly-released LLM checkpoint, the availability of the original training data may be non-existent. To address this challenge, we propose a framework called Self-Synthesized Rehearsal (SSR) that uses the LLM to generate synthetic instances for rehearsal. Concretely, we first employ the base LLM for in-context learning to generate synthetic instances. Subsequently, we utilize the latest LLM to refine the instance outputs based on the synthetic inputs, preserving its acquired ability. Finally, we select diverse high-quality synthetic instances for rehearsal in future stages. Experimental results demonstrate that SSR achieves superior or comparable pe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;IntactKV&#26041;&#27861;&#65292;&#36890;&#36807;&#20445;&#25345;&#20851;&#38190;&#26631;&#35760;&#30340;&#23436;&#25972;&#24615;&#65292;&#25913;&#21892;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37327;&#21270;&#36807;&#31243;&#65292;&#36827;&#19968;&#27493;&#38477;&#20302;&#20102;&#37327;&#21270;&#35823;&#24046;&#30340;&#19978;&#38480;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.01241</link><description>&lt;p&gt;
IntactKV: &#36890;&#36807;&#20445;&#25345;&#20851;&#38190;&#26631;&#35760;&#23436;&#25972;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
IntactKV: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;IntactKV&#26041;&#27861;&#65292;&#36890;&#36807;&#20445;&#25345;&#20851;&#38190;&#26631;&#35760;&#30340;&#23436;&#25972;&#24615;&#65292;&#25913;&#21892;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37327;&#21270;&#36807;&#31243;&#65292;&#36827;&#19968;&#27493;&#38477;&#20302;&#20102;&#37327;&#21270;&#35823;&#24046;&#30340;&#19978;&#38480;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#12290;&#20026;&#20102;&#20943;&#23569;&#36825;&#19968;&#38590;&#39064;&#65292;&#20154;&#20204;&#24050;&#32463;&#25506;&#32034;&#20102;&#21508;&#31181;&#37327;&#21270;&#26041;&#27861;&#65292;&#28982;&#32780;&#36825;&#20123;&#26041;&#27861;&#20250;&#25439;&#23475;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19968;&#20010;&#20197;&#21069;&#34987;&#24573;&#35270;&#30340;&#24322;&#24120;&#28857;&#31867;&#22411;&#12290;&#36825;&#20123;&#24322;&#24120;&#28857;&#34987;&#21457;&#29616;&#23558;&#22823;&#37096;&#20998;&#27880;&#24847;&#21147;&#20998;&#37197;&#32473;&#36755;&#20837;&#30340;&#21021;&#22987;&#26631;&#35760;&#65292;&#34987;&#31216;&#20026;&#20851;&#38190;&#26631;&#35760;&#65292;&#36825;&#23545;&#20110;&#37327;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;IntactKV&#65292;&#20174;&#23436;&#25972;&#31934;&#24230;&#27169;&#22411;&#20013;&#26080;&#25439;&#22320;&#29983;&#25104;&#20851;&#38190;&#26631;&#35760;&#30340;KV&#32531;&#23384;&#12290;&#36825;&#31181;&#26041;&#27861;&#31616;&#21333;&#26131;&#34892;&#65292;&#21487;&#20197;&#36731;&#26494;&#19982;&#29616;&#26377;&#30340;&#37327;&#21270;&#35299;&#20915;&#26041;&#26696;&#32467;&#21512;&#12290;&#27492;&#22806;&#65292;IntactKV&#21487;&#20197;&#34987;&#26657;&#20934;&#20026;&#39069;&#22806;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21442;&#25968;&#65292;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#37327;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25968;&#23398;&#20998;&#26512;&#36824;&#35777;&#26126;&#20102;IntactKV&#26377;&#25928;&#22320;&#38477;&#20302;&#20102;&#37327;&#21270;&#35823;&#24046;&#30340;&#19978;&#38480;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;IntactKV&#24102;&#26469;&#20102;&#25345;&#32493;&#30340;&#25913;&#36827;&#65292;&#24182;&#23454;&#29616;&#20102;&#26080;&#25439;&#30340;&#20165;&#26435;&#37325;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01241v1 Announce Type: cross  Abstract: Large language models (LLMs) excel in natural language processing but demand intensive computation. To mitigate this, various quantization methods have been explored, yet they compromise LLM performance. This paper unveils a previously overlooked type of outlier in LLMs. Such outliers are found to allocate most of the attention scores on initial tokens of input, termed as pivot tokens, which is crucial to the performance of quantized LLMs. Given that, we propose IntactKV to generate the KV cache of pivot tokens losslessly from the full-precision model. The approach is simple and easy to combine with existing quantization solutions. Besides, IntactKV can be calibrated as additional LLM parameters to boost the quantized LLMs further. Mathematical analysis also proves that IntactKV effectively reduces the upper bound of quantization error. Empirical results show that IntactKV brings consistent improvement and achieves lossless weight-only
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31995;&#21015;&#38463;&#25289;&#20271;&#22810;&#27169;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;Peacock&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#20986;&#33394;&#24615;&#33021;&#21644;&#19981;&#26029;&#20986;&#29616;&#30340;&#26041;&#35328;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#38463;&#25289;&#20271;&#35821;&#30456;&#20851;&#26041;&#38754;&#30340;&#26032;&#22522;&#20934;Henna</title><link>https://arxiv.org/abs/2403.01031</link><description>&lt;p&gt;
&#23380;&#38592;&#65306;&#19968;&#31995;&#21015;&#38463;&#25289;&#20271;&#22810;&#27169;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#21450;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Peacock: A Family of Arabic Multimodal Large Language Models and Benchmarks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01031
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31995;&#21015;&#38463;&#25289;&#20271;&#22810;&#27169;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;Peacock&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#20986;&#33394;&#24615;&#33021;&#21644;&#19981;&#26029;&#20986;&#29616;&#30340;&#26041;&#35328;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#38463;&#25289;&#20271;&#35821;&#30456;&#20851;&#26041;&#38754;&#30340;&#26032;&#22522;&#20934;Henna
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22312;&#38656;&#35201;&#22797;&#26434;&#25512;&#29702;&#21644;&#35821;&#35328;&#29702;&#35299;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#24050;&#34987;&#35777;&#26126;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38500;&#33521;&#35821;&#20197;&#22806;&#30340;&#20854;&#20182;&#35821;&#35328;&#32570;&#20047;&#39640;&#36136;&#37327;&#30340;&#22810;&#27169;&#24335;&#36164;&#28304;&#65292;MLLMs&#30340;&#25104;&#21151;&#20173;&#28982;&#30456;&#23545;&#23616;&#38480;&#20110;&#33521;&#35821;&#29615;&#22659;&#12290;&#36825;&#32473;&#24320;&#21457;&#20854;&#20182;&#35821;&#35328;&#30340;&#21487;&#27604;&#36739;&#27169;&#22411;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#29978;&#33267;&#21253;&#25324;&#37027;&#20123;&#25317;&#26377;&#24222;&#22823;&#35828;&#35805;&#20154;&#21475;&#30340;&#35821;&#35328;&#65292;&#22914;&#38463;&#25289;&#20271;&#35821;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#22871;&#32508;&#21512;&#30340;&#38463;&#25289;&#20271;MLLMs&#31995;&#21015;&#65292;&#31216;&#20026;\textit{Peacock}&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#33021;&#21147;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21508;&#31181;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#20986;&#33394;&#24615;&#33021;&#65292;&#24182;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#23427;&#20204;&#19981;&#26029;&#20986;&#29616;&#30340;&#26041;&#35328;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;\textit{Henna}&#30340;&#26032;&#22522;&#20934;&#65292;&#19987;&#38376;&#29992;&#20110;&#35780;&#20272;MLLM&#22312;&#19982;&#38463;&#25289;&#20271;&#35821;&#30456;&#20851;&#30340;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01031v1 Announce Type: cross  Abstract: Multimodal large language models (MLLMs) have proven effective in a wide range of tasks requiring complex reasoning and linguistic comprehension. However, due to a lack of high-quality multimodal resources in languages other than English, success of MLLMs remains relatively limited to English-based settings. This poses significant challenges in developing comparable models for other languages, including even those with large speaker populations such as Arabic. To alleviate this challenge, we introduce a comprehensive family of Arabic MLLMs, dubbed \textit{Peacock}, with strong vision and language capabilities. Through comprehensive qualitative and quantitative analysis, we demonstrate the solid performance of our models on various visual reasoning tasks and further show their emerging dialectal potential. Additionally, we introduce ~\textit{Henna}, a new benchmark specifically designed for assessing MLLMs on aspects related to Arabic c
&lt;/p&gt;</description></item><item><title>LoRA&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;LoRA&#19982;dropout&#26041;&#27861;&#22312;&#27169;&#22411;&#23450;&#21046;&#20013;&#30340;&#30683;&#30462;&#65292;&#37325;&#26032;&#23457;&#35270;&#20102;transformer-specific&#30340;dropout&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#25968;&#23398;&#21644;&#32463;&#39564;&#19978;&#30340;&#31561;&#20215;&#24615;&#21644;&#21306;&#21035;&#12290;</title><link>https://arxiv.org/abs/2403.00812</link><description>&lt;p&gt;
LoRA&#22312;&#32479;&#19968;&#26694;&#26550;&#19979;&#36935;&#35265;&#20102;Dropout
&lt;/p&gt;
&lt;p&gt;
LoRA Meets Dropout under a Unified Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00812
&lt;/p&gt;
&lt;p&gt;
LoRA&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;LoRA&#19982;dropout&#26041;&#27861;&#22312;&#27169;&#22411;&#23450;&#21046;&#20013;&#30340;&#30683;&#30462;&#65292;&#37325;&#26032;&#23457;&#35270;&#20102;transformer-specific&#30340;dropout&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#25968;&#23398;&#21644;&#32463;&#39564;&#19978;&#30340;&#31561;&#20215;&#24615;&#21644;&#21306;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#26174;&#33879;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#20803;&#32032;&#65292;&#32780;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65292;&#29305;&#21035;&#26159;LoRA&#65292;&#24050;&#32463;&#25104;&#20026;&#27169;&#22411;&#23450;&#21046;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#30340;&#27969;&#34892;&#36873;&#25321;&#12290;&#21516;&#26102;&#65292;&#21508;&#31181;dropout&#26041;&#27861;&#26368;&#21021;&#26159;&#20026;&#25152;&#26377;&#21442;&#25968;&#36827;&#34892;&#23436;&#25972;&#24494;&#35843;&#32780;&#35774;&#35745;&#30340;&#65292;&#26377;&#21161;&#20110;&#20943;&#36731;&#19982;&#36807;&#22810;&#21442;&#25968;&#20887;&#20313;&#30456;&#20851;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;LoRA&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#24494;&#19981;&#36275;&#36947;&#19982;&#20808;&#21069;dropout&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#20043;&#38388;&#23384;&#22312;&#21487;&#33021;&#30340;&#30683;&#30462;&#65292;&#36825;&#19968;&#28857;&#20043;&#21069;&#22823;&#22810;&#34987;&#24573;&#35270;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#39318;&#20808;&#30830;&#35748;&#39640;&#25928;&#21442;&#25968;&#30340;LoRA&#20063;&#23481;&#26131;&#20986;&#29616;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#29305;&#23450;&#20110;transformer&#30340;dropout&#26041;&#27861;&#65292;&#20174;&#25968;&#23398;&#21644;&#32463;&#39564;&#19978;&#24314;&#31435;&#23427;&#20204;&#30340;&#31561;&#20215;&#24615;&#21644;&#21306;&#21035;&#12290;&#22522;&#20110;&#36825;&#31181;&#27604;&#36739;&#20998;&#26512;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#36827;&#34892;&#20840;&#38754;&#30740;&#31350;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00812v1 Announce Type: cross  Abstract: With the remarkable capabilities, large language models (LLMs) have emerged as essential elements in numerous NLP applications, while parameter-efficient finetuning, especially LoRA, has gained popularity as a lightweight approach for model customization. Meanwhile, various dropout methods, initially designed for full finetuning with all the parameters updated, alleviates overfitting associated with excessive parameter redundancy. Hence, a possible contradiction arises from negligible trainable parameters of LoRA and the effectiveness of previous dropout methods, which has been largely overlooked. To fill this gap, we first confirm that parameter-efficient LoRA is also overfitting-prone. We then revisit transformer-specific dropout methods, and establish their equivalence and distinctions mathematically and empirically. Building upon this comparative analysis, we introduce a unified framework for a comprehensive investigation, which in
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#35328;&#24341;&#23548;&#30340;&#25216;&#33021;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20998;&#21106;&#20449;&#24687;&#26469;&#21457;&#29616;&#21487;&#37325;&#29992;&#30340;&#25216;&#33021;&#65292;&#24182;&#24341;&#20837;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#21407;&#21017;&#26469;&#24341;&#23548;&#36825;&#19968;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#21152;&#36895;&#23398;&#20064;&#24182;&#36229;&#36234;&#22522;&#32447;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.16354</link><description>&lt;p&gt;
&#36890;&#36807;&#26102;&#38388;&#21464;&#20998;&#25512;&#26029;&#36827;&#34892;&#35821;&#35328;&#24341;&#23548;&#30340;&#25216;&#33021;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Language-guided Skill Learning with Temporal Variational Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16354
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#35328;&#24341;&#23548;&#30340;&#25216;&#33021;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20998;&#21106;&#20449;&#24687;&#26469;&#21457;&#29616;&#21487;&#37325;&#29992;&#30340;&#25216;&#33021;&#65292;&#24182;&#24341;&#20837;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#21407;&#21017;&#26469;&#24341;&#23548;&#36825;&#19968;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#21152;&#36895;&#23398;&#20064;&#24182;&#36229;&#36234;&#22522;&#32447;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#21457;&#29616;&#25216;&#33021;&#30340;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#39318;&#20808;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#25552;&#20986;&#36712;&#36857;&#30340;&#21021;&#22987;&#20998;&#21106;&#12290;&#38543;&#21518;&#65292;&#19968;&#20010;&#20998;&#23618;&#21464;&#20998;&#25512;&#26029;&#26694;&#26550;&#23558;LLM&#29983;&#25104;&#30340;&#20998;&#21106;&#20449;&#24687;&#32435;&#20837;&#20854;&#20013;&#65292;&#36890;&#36807;&#21512;&#24182;&#36712;&#36857;&#27573;&#26469;&#21457;&#29616;&#21487;&#37325;&#29992;&#30340;&#25216;&#33021;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25511;&#21046;&#21387;&#32553;&#21644;&#21487;&#37325;&#29992;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#21407;&#21017;&#30340;&#26032;&#36741;&#21161;&#30446;&#26631;&#65292;&#24110;&#21161;&#24341;&#23548;&#36825;&#31181;&#25216;&#33021;&#21457;&#29616;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#25105;&#20204;&#26041;&#27861;&#30340;Agent&#33021;&#22815;&#21457;&#29616;&#26377;&#21161;&#20110;&#21152;&#36895;&#23398;&#20064;&#30340;&#25216;&#33021;&#65292;&#22312;BabyAI&#65288;&#19968;&#20010;&#32593;&#26684;&#19990;&#30028;&#23548;&#33322;&#29615;&#22659;&#65289;&#20197;&#21450;ALFRED&#65288;&#19968;&#20010;&#23478;&#24237;&#27169;&#25311;&#29615;&#22659;&#65289;&#30340;&#26032;&#38271;&#26399;&#20219;&#21153;&#20013;&#32988;&#36807;&#22522;&#32447;&#25216;&#33021;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16354v1 Announce Type: cross  Abstract: We present an algorithm for skill discovery from expert demonstrations. The algorithm first utilizes Large Language Models (LLMs) to propose an initial segmentation of the trajectories. Following that, a hierarchical variational inference framework incorporates the LLM-generated segmentation information to discover reusable skills by merging trajectory segments. To further control the trade-off between compression and reusability, we introduce a novel auxiliary objective based on the Minimum Description Length principle that helps guide this skill discovery process. Our results demonstrate that agents equipped with our method are able to discover skills that help accelerate learning and outperform baseline skill learning approaches on new long-horizon tasks in BabyAI, a grid world navigation environment, as well as ALFRED, a household simulation environment.
&lt;/p&gt;</description></item><item><title>&#29616;&#26377;&#30340;LLM&#27700;&#21360;&#31995;&#32479;&#34429;&#28982;&#20855;&#26377;&#36136;&#37327;&#20445;&#30041;&#12289;&#40065;&#26834;&#24615;&#21644;&#20844;&#24320;&#26816;&#27979;API&#31561;&#20248;&#28857;&#65292;&#20294;&#20063;&#22240;&#27492;&#23481;&#26131;&#21463;&#21040;&#21508;&#31181;&#25915;&#20987;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#22871;&#23454;&#29992;&#25351;&#21335;&#20197;&#32531;&#35299;&#36825;&#20123;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2402.16187</link><description>&lt;p&gt;
&#21033;&#29992;&#20854;&#20248;&#21183;&#25915;&#20987;LLM&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
Attacking LLM Watermarks by Exploiting Their Strengths
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16187
&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;LLM&#27700;&#21360;&#31995;&#32479;&#34429;&#28982;&#20855;&#26377;&#36136;&#37327;&#20445;&#30041;&#12289;&#40065;&#26834;&#24615;&#21644;&#20844;&#24320;&#26816;&#27979;API&#31561;&#20248;&#28857;&#65292;&#20294;&#20063;&#22240;&#27492;&#23481;&#26131;&#21463;&#21040;&#21508;&#31181;&#25915;&#20987;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#22871;&#23454;&#29992;&#25351;&#21335;&#20197;&#32531;&#35299;&#36825;&#20123;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#30340;&#36827;&#23637;&#20351;&#24471;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#25991;&#26412;&#12289;&#20195;&#30721;&#21644;&#22270;&#29255;&#33021;&#22815;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#27169;&#20223;&#20154;&#31867;&#29983;&#25104;&#30340;&#20869;&#23481;&#12290;&#27700;&#21360;&#25216;&#26415;&#26088;&#22312;&#23558;&#20449;&#24687;&#23884;&#20837;&#27169;&#22411;&#30340;&#36755;&#20986;&#20013;&#20197;&#39564;&#35777;&#20854;&#26469;&#28304;&#65292;&#23545;&#20110;&#20943;&#23569;&#23545;&#36825;&#20123;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#30340;&#28389;&#29992;&#38750;&#24120;&#26377;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#27700;&#21360;&#26041;&#26696;&#20173;&#28982;&#20196;&#20154;&#24847;&#22806;&#22320;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#26377;&#30340;LLM&#27700;&#21360;&#31995;&#32479;&#20849;&#20139;&#30340;&#21487;&#21462;&#29305;&#24615;&#65292;&#20363;&#22914;&#36136;&#37327;&#20445;&#30041;&#12289;&#40065;&#26834;&#24615;&#21644;&#20844;&#24320;&#26816;&#27979;API&#65292;&#21453;&#36807;&#26469;&#21364;&#20351;&#36825;&#20123;&#31995;&#32479;&#23481;&#26131;&#36973;&#21463;&#21508;&#31181;&#25915;&#20987;&#12290;&#25105;&#20204;&#22312;&#24120;&#35265;&#27700;&#21360;&#35774;&#35745;&#36873;&#25321;&#26041;&#38754;&#20005;&#26684;&#30740;&#31350;&#28508;&#22312;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#25915;&#20987;&#30340;&#26368;&#20339;&#23454;&#36341;&#21644;&#38450;&#24481;&#25514;&#26045;&#8212;&#8212;&#24314;&#31435;&#20102;&#19968;&#22871;&#23884;&#20837;&#21644;&#26816;&#27979;LLM&#27700;&#21360;&#30340;&#23454;&#29992;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16187v1 Announce Type: cross  Abstract: Advances in generative models have made it possible for AI-generated text, code, and images to mirror human-generated content in many applications. Watermarking, a technique that aims to embed information in the output of a model to verify its source, is useful for mitigating misuse of such AI-generated content. However, existing watermarking schemes remain surprisingly susceptible to attack. In particular, we show that desirable properties shared by existing LLM watermarking systems such as quality preservation, robustness, and public detection APIs can in turn make these systems vulnerable to various attacks. We rigorously study potential attacks in terms of common watermark design choices, and propose best practices and defenses for mitigation -- establishing a set of practical guidelines for embedding and detection of LLM watermarks.
&lt;/p&gt;</description></item><item><title>FuseChat&#36890;&#36807;&#30693;&#35782;&#34701;&#21512;&#23558;&#22810;&#20010;&#23545;&#35805;&#27169;&#22411;&#30340;&#38598;&#20307;&#30693;&#35782;&#36716;&#31227;&#21040;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#39044;&#35757;&#32451;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.16107</link><description>&lt;p&gt;
FuseChat&#65306;&#23545;&#35805;&#27169;&#22411;&#30693;&#35782;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
FuseChat: Knowledge Fusion of Chat Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16107
&lt;/p&gt;
&lt;p&gt;
FuseChat&#36890;&#36807;&#30693;&#35782;&#34701;&#21512;&#23558;&#22810;&#20010;&#23545;&#35805;&#27169;&#22411;&#30340;&#38598;&#20307;&#30693;&#35782;&#36716;&#31227;&#21040;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#39044;&#35757;&#32451;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30830;&#23454;&#21487;&#20197;&#23548;&#33268;&#20855;&#26377;&#29420;&#29305;&#33021;&#21147;&#21644;&#20248;&#21183;&#30340;&#27169;&#22411;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#20250;&#20135;&#29983;&#24040;&#22823;&#25104;&#26412;&#65292;&#24182;&#21487;&#33021;&#23548;&#33268;&#31454;&#20105;&#33021;&#21147;&#30340;&#28508;&#22312;&#20887;&#20313;&#12290;&#19968;&#31181;&#26367;&#20195;&#31574;&#30053;&#26159;&#23558;&#29616;&#26377;&#30340;LLMs&#32452;&#21512;&#25104;&#26356;&#24378;&#22823;&#30340;LLM&#65292;&#20174;&#32780;&#20943;&#23569;&#26114;&#36149;&#30340;&#39044;&#35757;&#32451;&#30340;&#24517;&#35201;&#24615;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;LLMs&#30340;&#22810;&#26679;&#21270;&#26550;&#26500;&#65292;&#30452;&#25509;&#21442;&#25968;&#34701;&#21512;&#34987;&#35777;&#26126;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#26368;&#36817;&#65292;FuseLLM&#24341;&#20837;&#20102;&#30693;&#35782;&#34701;&#21512;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#36731;&#37327;&#32423;&#30340;&#25345;&#32493;&#35757;&#32451;&#23558;&#22810;&#20010;&#32467;&#26500;&#22810;&#26679;&#30340;LLM&#30340;&#38598;&#20307;&#30693;&#35782;&#36716;&#31227;&#33267;&#30446;&#26631;LLM&#12290;&#22312;&#26412;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;FuseLLM&#26694;&#26550;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#23454;&#29616;&#20102;&#23545;&#35805;LLM&#30340;&#34701;&#21512;&#65292;&#29983;&#25104;&#20102;FuseChat&#12290;FuseChat&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#38454;&#27573;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23545;&#32467;&#26500;&#21644;&#35268;&#27169;&#19981;&#21516;&#30340;&#28304;LLMs&#36827;&#34892;&#30693;&#35782;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16107v1 Announce Type: new  Abstract: While training large language models (LLMs) from scratch can indeed lead to models with distinct capabilities and strengths, this approach incurs substantial costs and may lead to potential redundancy in competencies. An alternative strategy is to combine existing LLMs into a more robust LLM, thereby diminishing the necessity for expensive pre-training. However, due to the diverse architectures of LLMs, direct parameter blending proves to be unfeasible. Recently, \textsc{FuseLLM} introduced the concept of knowledge fusion to transfer the collective knowledge of multiple structurally varied LLMs into a target LLM through lightweight continual training. In this report, we extend the scalability and flexibility of the \textsc{FuseLLM} framework to realize the fusion of chat LLMs, resulting in \textsc{FuseChat}. \textsc{FuseChat} comprises two main stages. Firstly, we undertake knowledge fusion for structurally and scale-varied source LLMs t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Prejudice-Caprice Framework&#65288;PCF&#65289;&#26469;&#20840;&#38754;&#34913;&#37327;LLMs&#20013;&#30340;&#27495;&#35270;&#65292;&#32771;&#34385;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#30340;&#19968;&#36143;&#20559;&#35265;&#20559;&#22909;&#21644;&#20559;&#22909;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.15481</link><description>&lt;p&gt;
&#20559;&#35265;&#21644;&#21453;&#22797;&#26080;&#24120;&#65306;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31038;&#20250;&#27495;&#35270;&#30340;&#32479;&#35745;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Prejudice and Caprice: A Statistical Framework for Measuring Social Discrimination in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15481
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Prejudice-Caprice Framework&#65288;PCF&#65289;&#26469;&#20840;&#38754;&#34913;&#37327;LLMs&#20013;&#30340;&#27495;&#35270;&#65292;&#32771;&#34385;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#30340;&#19968;&#36143;&#20559;&#35265;&#20559;&#22909;&#21644;&#20559;&#22909;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15481v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#31038;&#20250;&#36816;&#33829;&#20013;&#30340;&#26085;&#30410;&#34701;&#21512;&#21152;&#21095;&#20102;&#23427;&#20204;&#23545;&#32463;&#27982;&#12289;&#27861;&#24459;&#12289;&#25945;&#32946;&#21644;&#21307;&#30103;&#31561;&#37325;&#35201;&#39046;&#22495;&#20915;&#31574;&#30340;&#24433;&#21709;&#65292;&#24341;&#21457;&#20102;&#20844;&#20247;&#23545;&#36825;&#20123;&#27169;&#22411;&#28041;&#21450;&#27495;&#35270;&#23433;&#20840;&#21644;&#21487;&#38752;&#24615;&#30340;&#25285;&#24551;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#27495;&#35270;&#27979;&#37327;&#26694;&#26550;&#20165;&#35780;&#20272;LLMs&#30340;&#24179;&#22343;&#27495;&#35270;&#34892;&#20026;&#65292;&#24448;&#24448;&#30001;&#20110;&#24573;&#35270;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#23548;&#33268;&#27495;&#35270;&#30340;&#22240;&#32032;&#65292;&#21363;LLMs&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#30340;&#39044;&#27979;&#21464;&#21270;&#32780;&#21464;&#24471;&#19981;&#36275;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Prejudice-Caprice Framework&#65288;PCF&#65289;&#65292;&#36890;&#36807;&#32771;&#34385;LLMs&#30340;&#19968;&#36143;&#20559;&#35265;&#20559;&#22909;&#21644;&#22312;&#22810;&#26679;&#19978;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15481v1 Announce Type: new  Abstract: The growing integration of large language models (LLMs) into social operations amplifies their impact on decisions in crucial areas such as economics, law, education, and healthcare, raising public concerns about these models' discrimination-related safety and reliability. However, prior discrimination measuring frameworks solely assess the average discriminatory behavior of LLMs, often proving inadequate due to the overlook of an additional discrimination-leading factor, i.e., the LLMs' prediction variation across diverse contexts. In this work, we present the Prejudice-Caprice Framework (PCF) that comprehensively measures discrimination in LLMs by considering both their consistently biased preference and preference variation across diverse contexts. Specifically, we mathematically dissect the aggregated contextualized discrimination risk of LLMs into prejudice risk, originating from LLMs' persistent prejudice, and caprice risk, stemmin
&lt;/p&gt;</description></item><item><title>&#33258;&#20462;&#22797;&#29616;&#35937;&#23384;&#22312;&#20110;&#21508;&#31181;&#27169;&#22411;&#23478;&#26063;&#21644;&#23610;&#23544;&#19978;&#65292;&#20294;&#22312;&#23436;&#25972;&#30340;&#35757;&#32451;&#20998;&#24067;&#19978;&#26159;&#19981;&#23436;&#32654;&#21644;&#22024;&#26434;&#30340;&#65292;&#26377;&#20004;&#31181;&#26426;&#21046;&#21487;&#20419;&#25104;&#33258;&#20462;&#22797;&#65292;&#21253;&#25324;&#26368;&#32456;LayerNorm&#32553;&#25918;&#22240;&#23376;&#30340;&#21464;&#21270;&#21644;&#23454;&#29616;&#21453;&#25830;&#38500;&#30340;&#31232;&#30095;&#31070;&#32463;&#20803;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.15390</link><description>&lt;p&gt;
&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#33258;&#20462;&#22797;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Explorations of Self-Repair in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15390
&lt;/p&gt;
&lt;p&gt;
&#33258;&#20462;&#22797;&#29616;&#35937;&#23384;&#22312;&#20110;&#21508;&#31181;&#27169;&#22411;&#23478;&#26063;&#21644;&#23610;&#23544;&#19978;&#65292;&#20294;&#22312;&#23436;&#25972;&#30340;&#35757;&#32451;&#20998;&#24067;&#19978;&#26159;&#19981;&#23436;&#32654;&#21644;&#22024;&#26434;&#30340;&#65292;&#26377;&#20004;&#31181;&#26426;&#21046;&#21487;&#20419;&#25104;&#33258;&#20462;&#22797;&#65292;&#21253;&#25324;&#26368;&#32456;LayerNorm&#32553;&#25918;&#22240;&#23376;&#30340;&#21464;&#21270;&#21644;&#23454;&#29616;&#21453;&#25830;&#38500;&#30340;&#31232;&#30095;&#31070;&#32463;&#20803;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30740;&#31350;&#29421;&#31364;&#20998;&#24067;&#30340;&#21487;&#35299;&#37322;&#24615;&#21457;&#29616;&#20102;&#33258;&#20462;&#22797;&#29616;&#35937;&#65292;&#21363;&#22914;&#26524;&#21093;&#31163;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32452;&#20214;&#65292;&#21518;&#32493;&#32452;&#20214;&#20250;&#25913;&#21464;&#20854;&#34892;&#20026;&#20197;&#36827;&#34892;&#34917;&#20607;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22522;&#20110;&#36825;&#20123;&#36807;&#21435;&#30340;&#25991;&#29486;&#65292;&#23637;&#31034;&#20102;&#24403;&#22312;&#23436;&#25972;&#30340;&#35757;&#32451;&#20998;&#24067;&#19978;&#21093;&#31163;&#21333;&#20010;&#27880;&#24847;&#21147;&#22836;&#26102;&#65292;&#33258;&#20462;&#22797;&#23384;&#22312;&#20110;&#21508;&#31181;&#27169;&#22411;&#23478;&#26063;&#21644;&#23610;&#23544;&#19978;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#22312;&#23436;&#25972;&#30340;&#35757;&#32451;&#20998;&#24067;&#19978;&#65292;&#33258;&#20462;&#22797;&#26159;&#19981;&#23436;&#32654;&#30340;&#65292;&#22240;&#20026;&#22836;&#37096;&#30340;&#21407;&#22987;&#30452;&#25509;&#25928;&#26524;&#24182;&#26410;&#23436;&#20840;&#24674;&#22797;&#65292;&#24182;&#19988;&#26159;&#22024;&#26434;&#30340;&#65292;&#22240;&#20026;&#33258;&#20462;&#22797;&#31243;&#24230;&#22312;&#19981;&#21516;&#25552;&#31034;&#20043;&#38388;&#26174;&#33879;&#21464;&#21270;&#65288;&#26377;&#26102;&#36229;&#36807;&#21407;&#22987;&#25928;&#26524;&#65289;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#20419;&#25104;&#33258;&#20462;&#22797;&#30340;&#20004;&#31181;&#19981;&#21516;&#26426;&#21046;&#65292;&#21253;&#25324;&#26368;&#32456;LayerNorm&#32553;&#25918;&#22240;&#23376;&#30340;&#21464;&#21270;&#65288;&#21487;&#20462;&#22797;&#30452;&#25509;&#25928;&#26524;&#30340;30%&#65289;&#20197;&#21450;&#23454;&#29616;&#21453;&#25830;&#38500;&#30340;&#31232;&#30095;&#31070;&#32463;&#20803;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15390v1 Announce Type: cross  Abstract: Prior interpretability research studying narrow distributions has preliminarily identified self-repair, a phenomena where if components in large language models are ablated, later components will change their behavior to compensate. Our work builds off this past literature, demonstrating that self-repair exists on a variety of models families and sizes when ablating individual attention heads on the full training distribution. We further show that on the full training distribution self-repair is imperfect, as the original direct effect of the head is not fully restored, and noisy, since the degree of self-repair varies significantly across different prompts (sometimes overcorrecting beyond the original effect). We highlight two different mechanisms that contribute to self-repair, including changes in the final LayerNorm scaling factor (which can repair up to 30% of the direct effect) and sparse sets of neurons implementing Anti-Erasure
&lt;/p&gt;</description></item><item><title>GPT-HateCheck&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#22836;&#24320;&#22987;&#29983;&#25104;&#26356;&#22810;&#26679;&#21270;&#21644;&#29616;&#23454;&#30340;&#21151;&#33021;&#27979;&#35797;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#27979;&#35797;&#26696;&#20363;&#36807;&#20110;&#36890;&#29992;&#31616;&#21333;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.15238</link><description>&lt;p&gt;
GPT-HateCheck: LLMs&#33021;&#21542;&#20026;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#32534;&#20889;&#26356;&#22909;&#30340;&#21151;&#33021;&#27979;&#35797;&#65311;
&lt;/p&gt;
&lt;p&gt;
GPT-HateCheck: Can LLMs Write Better Functional Tests for Hate Speech Detection?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15238
&lt;/p&gt;
&lt;p&gt;
GPT-HateCheck&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#22836;&#24320;&#22987;&#29983;&#25104;&#26356;&#22810;&#26679;&#21270;&#21644;&#29616;&#23454;&#30340;&#21151;&#33021;&#27979;&#35797;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#27979;&#35797;&#26696;&#20363;&#36807;&#20110;&#36890;&#29992;&#31616;&#21333;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#20167;&#24680;&#26816;&#27979;&#21463;&#21040;&#25968;&#25454;&#37319;&#26679;&#12289;&#27880;&#37322;&#21644;&#27169;&#22411;&#39044;&#35757;&#32451;&#20013;&#24341;&#20837;&#30340;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#20165;&#27979;&#37327;&#22312;&#30041;&#23384;&#27979;&#35797;&#25968;&#25454;&#20013;&#25152;&#26377;&#31034;&#20363;&#19978;&#30340;&#24179;&#22343;&#24615;&#33021;&#26159;&#19981;&#36275;&#22815;&#30340;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#24517;&#39035;&#35782;&#21035;&#29305;&#23450;&#27169;&#22411;&#30340;&#24369;&#28857;&#65292;&#24182;&#22312;&#20854;&#26356;&#26377;&#21487;&#33021;&#22833;&#36133;&#26102;&#33719;&#24471;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26368;&#36817;&#30340;&#26041;&#21521;&#65292;&#21363;HateCheck&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#20351;&#29992;&#27169;&#26495;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#19978;&#27979;&#35797;&#31934;&#32454;&#31890;&#24230;&#27169;&#22411;&#21151;&#33021;&#30340;&#22871;&#20214;&#65292;&#27169;&#26495;&#30340;&#24418;&#24335;&#20026;&#8220;&#20320;&#21482;&#26159;&#19968;&#20010;[&#39554;&#20154;&#30340;&#35789;]&#23545;&#25105;&#26469;&#35828;&#8221;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;HateCheck&#20801;&#35768;&#33719;&#24471;&#26356;&#35814;&#32454;&#30340;&#35786;&#26029;&#35265;&#35299;&#65292;&#20294;&#20854;&#27979;&#35797;&#29992;&#20363;&#36890;&#24120;&#26159;&#36890;&#29992;&#30340;&#65292;&#24182;&#19988;&#20855;&#26377;&#31616;&#21333;&#30340;&#21477;&#23376;&#32467;&#26500;&#65292;&#19981;&#31526;&#21512;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GPT-HateCheck&#65292;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20174;&#22836;&#24320;&#22987;&#29983;&#25104;&#26356;&#22810;&#26679;&#21270;&#21644;&#29616;&#23454;&#30340;&#21151;&#33021;&#27979;&#35797;&#12290;&#25105;&#20204;&#37319;&#29992;&#39069;&#22806;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#27169;&#22411;&#26469;&#39564;&#35777;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15238v1 Announce Type: new  Abstract: Online hate detection suffers from biases incurred in data sampling, annotation, and model pre-training. Therefore, measuring the averaged performance over all examples in held-out test data is inadequate. Instead, we must identify specific model weaknesses and be informed when it is more likely to fail. A recent proposal in this direction is HateCheck, a suite for testing fine-grained model functionalities on synthesized data generated using templates of the kind "You are just a [slur] to me." However, despite enabling more detailed diagnostic insights, the HateCheck test cases are often generic and have simplistic sentence structures that do not match the real-world data. To address this limitation, we propose GPT-HateCheck, a framework to generate more diverse and realistic functional tests from scratch by instructing large language models (LLMs). We employ an additional natural language inference (NLI) model to verify the generations
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20943;&#23569;&#35780;&#20272;LLMs&#24615;&#33021;&#25152;&#38656;&#30340;&#35780;&#20272;&#27425;&#25968;&#30340;&#31574;&#30053;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#23567;&#35268;&#27169;&#31034;&#20363;&#19978;&#21487;&#20197;&#20934;&#30830;&#20272;&#35745;LLMs&#22312;&#22810;&#31181;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14992</link><description>&lt;p&gt;
&#23567;&#22411;&#22522;&#20934;&#27979;&#35797;&#65306;&#29992;&#26356;&#23569;&#30340;&#31034;&#20363;&#35780;&#20272;LLM
&lt;/p&gt;
&lt;p&gt;
tinyBenchmarks: evaluating LLMs with fewer examples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20943;&#23569;&#35780;&#20272;LLMs&#24615;&#33021;&#25152;&#38656;&#30340;&#35780;&#20272;&#27425;&#25968;&#30340;&#31574;&#30053;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#23567;&#35268;&#27169;&#31034;&#20363;&#19978;&#21487;&#20197;&#20934;&#30830;&#20272;&#35745;LLMs&#22312;&#22810;&#31181;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22810;&#21151;&#33021;&#24615;&#23548;&#33268;&#21019;&#24314;&#20102;&#22810;&#31181;&#22522;&#20934;&#27979;&#35797;&#65292;&#24443;&#24213;&#27979;&#35797;&#21508;&#31181;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;&#25104;&#21315;&#19978;&#19975;&#20010;&#31034;&#20363;&#65292;&#20351;&#24471;&#35780;&#20272;LLMs&#38750;&#24120;&#26114;&#36149;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20943;&#23569;&#35780;&#20272;LLMs&#24615;&#33021;&#25152;&#38656;&#30340;&#35780;&#20272;&#27425;&#25968;&#30340;&#31574;&#30053;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35201;&#20934;&#30830;&#20272;&#35745;LLMs&#22312;MMLU&#19978;&#30340;&#24615;&#33021;&#65288;&#19968;&#20010;&#21253;&#21547;14K&#20010;&#31034;&#20363;&#30340;&#27969;&#34892;&#22810;&#36873;&#38382;&#31572;&#22522;&#20934;&#27979;&#35797;&#65289;&#65292;&#21482;&#38656;&#35201;&#22312;100&#20010;&#31934;&#24515;&#25361;&#36873;&#30340;&#31034;&#20363;&#19978;&#35780;&#20272;&#36825;&#20010;LLMs&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#35780;&#20272;&#24037;&#20855;&#21644;&#27969;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;&#24494;&#22411;&#29256;&#26412;&#65306;Open LLM Leaderboard&#12289;MMLU&#12289;HELM&#21644;AlpacaEval 2.0&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;&#36825;&#20123;&#24037;&#20855;&#21644;&#24494;&#22411;&#22522;&#20934;&#27979;&#35797;&#36275;&#20197;&#21487;&#38752;&#19988;&#39640;&#25928;&#22320;&#37325;&#29616;&#21407;&#22987;&#35780;&#20272;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14992v1 Announce Type: cross  Abstract: The versatility of large language models (LLMs) led to the creation of diverse benchmarks that thoroughly test a variety of language models' abilities. These benchmarks consist of tens of thousands of examples making evaluation of LLMs very expensive. In this paper, we investigate strategies to reduce the number of evaluations needed to assess the performance of an LLM on several key benchmarks. For example, we show that to accurately estimate the performance of an LLM on MMLU, a popular multiple-choice QA benchmark consisting of 14K examples, it is sufficient to evaluate this LLM on 100 curated examples. We release evaluation tools and tiny versions of popular benchmarks: Open LLM Leaderboard, MMLU, HELM, and AlpacaEval 2.0. Our empirical analysis demonstrates that these tools and tiny benchmarks are sufficient to reliably and efficiently reproduce the original evaluation results.
&lt;/p&gt;</description></item><item><title>&#21457;&#24067;&#20102;IEPile&#65292;&#19968;&#20010;&#21253;&#21547;&#32422;0.32B&#20010;&#26631;&#35760;&#30340;&#32508;&#21512;&#21452;&#35821;IE&#25351;&#20196;&#35821;&#26009;&#24211;&#65292;&#36890;&#36807;&#25910;&#38598;&#21644;&#28165;&#29702;33&#20010;&#29616;&#26377;IE&#25968;&#25454;&#38598;&#24182;&#24341;&#20837;&#22522;&#20110;&#27169;&#24335;&#30340;&#25351;&#20196;&#29983;&#25104;&#65292;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#25277;&#21462;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.14710</link><description>&lt;p&gt;
IEPile: &#25366;&#25496;&#22823;&#35268;&#27169;&#22522;&#20110;&#27169;&#24335;&#30340;&#20449;&#24687;&#25277;&#21462;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
IEPile: Unearthing Large-Scale Schema-Based Information Extraction Corpus
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14710
&lt;/p&gt;
&lt;p&gt;
&#21457;&#24067;&#20102;IEPile&#65292;&#19968;&#20010;&#21253;&#21547;&#32422;0.32B&#20010;&#26631;&#35760;&#30340;&#32508;&#21512;&#21452;&#35821;IE&#25351;&#20196;&#35821;&#26009;&#24211;&#65292;&#36890;&#36807;&#25910;&#38598;&#21644;&#28165;&#29702;33&#20010;&#29616;&#26377;IE&#25968;&#25454;&#38598;&#24182;&#24341;&#20837;&#22522;&#20110;&#27169;&#24335;&#30340;&#25351;&#20196;&#29983;&#25104;&#65292;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#25277;&#21462;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#28508;&#21147;&#65307;&#28982;&#32780;&#65292;&#22312;&#20449;&#24687;&#25277;&#21462;&#65288;IE&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#25968;&#25454;&#26159;&#25552;&#21319;LLMs&#29305;&#23450;&#33021;&#21147;&#30340;&#20851;&#38190;&#65292;&#32780;&#24403;&#21069;&#30340;IE&#25968;&#25454;&#38598;&#24448;&#24448;&#35268;&#27169;&#36739;&#23567;&#12289;&#20998;&#25955;&#19988;&#32570;&#20047;&#26631;&#20934;&#21270;&#30340;&#27169;&#24335;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;IEPile&#65292;&#19968;&#20010;&#32508;&#21512;&#30340;&#21452;&#35821;&#65288;&#33521;&#25991;&#21644;&#20013;&#25991;&#65289;IE&#25351;&#20196;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;&#32422;0.32B&#20010;&#26631;&#35760;&#12290;&#25105;&#20204;&#36890;&#36807;&#25910;&#38598;&#21644;&#28165;&#29702;33&#20010;&#29616;&#26377;IE&#25968;&#25454;&#38598;&#26500;&#24314;IEPile&#65292;&#24182;&#24341;&#20837;&#22522;&#20110;&#27169;&#24335;&#30340;&#25351;&#20196;&#29983;&#25104;&#26469;&#25366;&#25496;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#12290;&#22312;LLaMA&#21644;Baichuan&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;IEPile&#21487;&#20197;&#25552;&#39640;LLMs&#22312;IE&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;&#25105;&#20204;&#24320;&#28304;&#20102;&#36164;&#28304;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24076;&#26395;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31038;&#21306;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14710v1 Announce Type: cross  Abstract: Large Language Models (LLMs) demonstrate remarkable potential across various domains; however, they exhibit a significant performance gap in Information Extraction (IE). Note that high-quality instruction data is the vital key for enhancing the specific capabilities of LLMs, while current IE datasets tend to be small in scale, fragmented, and lack standardized schema. To this end, we introduce IEPile, a comprehensive bilingual (English and Chinese) IE instruction corpus, which contains approximately 0.32B tokens. We construct IEPile by collecting and cleaning 33 existing IE datasets, and introduce schema-based instruction generation to unearth a large-scale corpus. Experimental results on LLaMA and Baichuan demonstrate that using IEPile can enhance the performance of LLMs for IE, especially the zero-shot generalization. We open-source the resource and pre-trained models, hoping to provide valuable support to the NLP community.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#21306;&#22495;&#21010;&#20998;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#21457;&#29616;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#24212;&#35821;&#35328;&#33021;&#21147;&#30340;&#26680;&#24515;&#21306;&#22495;&#65292;&#24182;&#23637;&#31034;&#20102;&#21435;&#38500;&#35813;&#21306;&#22495;&#20250;&#23548;&#33268;&#36328;30&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;&#26174;&#33879;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2402.14700</link><description>&lt;p&gt;
&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#35328;&#21306;&#22495;
&lt;/p&gt;
&lt;p&gt;
Unveiling Linguistic Regions in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14700
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#21306;&#22495;&#21010;&#20998;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#21457;&#29616;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#24212;&#35821;&#35328;&#33021;&#21147;&#30340;&#26680;&#24515;&#21306;&#22495;&#65292;&#24182;&#23637;&#31034;&#20102;&#21435;&#38500;&#35813;&#21306;&#22495;&#20250;&#23548;&#33268;&#36328;30&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;&#26174;&#33879;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#23637;&#31034;&#20102;&#30456;&#24403;&#22823;&#30340;&#36328;&#35821;&#35328;&#23545;&#40784;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#24403;&#21069;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#25913;&#21892;LLMs&#30340;&#36328;&#35821;&#35328;&#27867;&#21270;&#33021;&#21147;&#19978;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;LLMs&#22914;&#20309;&#23454;&#29616;&#36328;&#35821;&#35328;&#23545;&#40784;&#30340;&#20869;&#22312;&#26426;&#21046;&#20173;&#28982;&#32570;&#20047;&#30740;&#31350;&#12290;&#20174;&#21306;&#22495;&#21010;&#20998;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#26412;&#25991;&#22312;LLMs&#30340;&#35821;&#35328;&#33021;&#21147;&#19978;&#36827;&#34892;&#20102;&#20960;&#39033;&#35843;&#26597;&#12290;&#25105;&#20204;&#21457;&#29616;LLMs&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#21306;&#22495;&#23545;&#24212;&#20110;&#35821;&#35328;&#33021;&#21147;&#65292;&#22823;&#32422;&#21344;&#24635;&#27169;&#22411;&#21442;&#25968;&#30340;1%&#12290;&#36890;&#36807;&#23558;&#21442;&#25968;&#35774;&#32622;&#20026;&#38646;&#26469;&#21435;&#38500;&#36825;&#20010;&#26680;&#24515;&#21306;&#22495;&#65292;&#20250;&#23548;&#33268;30&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;&#26174;&#33879;&#24615;&#33021;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;&#26680;&#24515;&#21306;&#22495;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#32500;&#24230;&#20381;&#36182;&#24615;&#65292;&#23545;&#29305;&#23450;&#32500;&#24230;&#19978;&#30340;&#21333;&#20010;&#21442;&#25968;&#30340;&#25200;&#21160;&#20250;&#23548;&#33268;&#35821;&#35328;&#33021;&#21147;&#30340;&#20007;&#22833;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#29420;&#29305;&#30340;&#21306;&#22495;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14700v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated considerable cross-lingual alignment and generalization ability. Current research primarily focuses on improving LLMs' cross-lingual generalization capabilities. However, there is still a lack of research on the intrinsic mechanisms of how LLMs achieve cross-lingual alignment. From the perspective of region partitioning, this paper conducts several investigations on the linguistic competence of LLMs. We discover a core region in LLMs that corresponds to linguistic competence, accounting for approximately 1% of the total model parameters. Removing this core region by setting parameters to zero results in a significant performance decrease across 30 different languages. Furthermore, this core region exhibits significant dimensional dependency, perturbations to even a single parameter on specific dimensions leading to a loss of linguistic competence. Moreover, we discover that distinct regions 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#27880;&#26041;&#26696;&#65292;&#29992;&#20110;&#20998;&#31867;&#26465;&#27454;&#21644;&#26465;&#20214;&#21512;&#21516;&#20013;&#30340;&#19981;&#21516;&#31867;&#22411;&#30340;&#26465;&#27454;&#65292;&#20197;&#25903;&#25345;&#27861;&#24459;&#19987;&#23478;&#24555;&#36895;&#35782;&#21035;&#21644;&#35780;&#20272;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#23545;&#36825;&#20123;&#31867;&#21035;&#36827;&#34892;&#33258;&#21160;&#20998;&#31867;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14457</link><description>&lt;p&gt;
&#26631;&#27880;&#21644;&#20998;&#31867;&#26465;&#27454;&#21644;&#26465;&#20214;&#21512;&#21516;&#20013;&#30340;&#30456;&#20851;&#26465;&#27454;
&lt;/p&gt;
&lt;p&gt;
Annotation and Classification of Relevant Clauses in Terms-and-Conditions Contracts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14457
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#27880;&#26041;&#26696;&#65292;&#29992;&#20110;&#20998;&#31867;&#26465;&#27454;&#21644;&#26465;&#20214;&#21512;&#21516;&#20013;&#30340;&#19981;&#21516;&#31867;&#22411;&#30340;&#26465;&#27454;&#65292;&#20197;&#25903;&#25345;&#27861;&#24459;&#19987;&#23478;&#24555;&#36895;&#35782;&#21035;&#21644;&#35780;&#20272;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#23545;&#36825;&#20123;&#31867;&#21035;&#36827;&#34892;&#33258;&#21160;&#20998;&#31867;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#27880;&#26041;&#26696;&#65292;&#29992;&#20110;&#20998;&#31867;&#26465;&#27454;&#21644;&#26465;&#20214;&#21512;&#21516;&#20013;&#30340;&#19981;&#21516;&#31867;&#22411;&#30340;&#26465;&#27454;&#65292;&#26368;&#32456;&#30446;&#26631;&#26159;&#25903;&#25345;&#27861;&#24459;&#19987;&#23478;&#24555;&#36895;&#35782;&#21035;&#21644;&#35780;&#20272;&#36825;&#31867;&#27861;&#24459;&#25991;&#20214;&#20013;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#23567;&#35268;&#27169;&#30340;&#26465;&#27454;&#21644;&#26465;&#20214;&#21512;&#21516;&#35821;&#26009;&#24211;&#65292;&#24182;&#23436;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;14&#20010;&#31867;&#21035;&#30340;&#26631;&#27880;&#26041;&#26696;&#65292;&#26368;&#32456;&#36798;&#25104;&#20102;0.92&#30340;&#26631;&#27880;&#32773;&#38388;&#19968;&#33268;&#24615;&#12290;&#28982;&#21518;&#65292;&#38024;&#23545;&#20854;&#20013;&#30340;11&#20010;&#31867;&#21035;&#65292;&#25105;&#20204;&#20351;&#29992;&#22810;&#35821;&#35328;T5&#21644;&#20004;&#20010;&#22522;&#20110;BERT&#30340;LLM&#30340;&#20004;&#20010;&#24494;&#35843;&#29256;&#26412;&#36827;&#34892;&#20102;&#23569;&#26679;&#26412;&#25552;&#31034;&#30340;&#20108;&#20998;&#31867;&#20219;&#21153;&#23454;&#39564;&#65292;&#20854;&#20013;&#36824;&#21253;&#25324;&#24847;&#22823;&#21033;&#35821;&#30340;&#20004;&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36890;&#36807;&#22312;&#39564;&#35777;&#20219;&#21153;&#20013;&#36798;&#21040;&#20174;.79&#21040;.95&#30340;&#20934;&#30830;&#29575;&#65292;&#33258;&#21160;&#23545;&#25105;&#20204;&#30340;&#31867;&#21035;&#36827;&#34892;&#20998;&#31867;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14457v1 Announce Type: new  Abstract: In this paper, we propose a new annotation scheme to classify different types of clauses in Terms-and-Conditions contracts with the ultimate goal of supporting legal experts to quickly identify and assess problematic issues in this type of legal documents. To this end, we built a small corpus of Terms-and-Conditions contracts and finalized an annotation scheme of 14 categories, eventually reaching an inter-annotator agreement of 0.92. Then, for 11 of them, we experimented with binary classification tasks using few-shot prompting with a multilingual T5 and two fine-tuned versions of two BERT-based LLMs for Italian. Our experiments showed the feasibility of automatic classification of our categories by reaching accuracies ranging from .79 to .95 on validation tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"ProSparse"&#30340;&#26377;&#25928;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#20197;&#25512;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#26356;&#39640;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#32780;&#19981;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.13516</link><description>&lt;p&gt;
ProSparse: &#24341;&#20837;&#21644;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#28608;&#27963;&#31232;&#30095;&#24615;
&lt;/p&gt;
&lt;p&gt;
ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"ProSparse"&#30340;&#26377;&#25928;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#20197;&#25512;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#26356;&#39640;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#32780;&#19981;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Activation sparsity&#25351;&#30340;&#26159;&#28608;&#27963;&#36755;&#20986;&#20013;&#23384;&#22312;&#35768;&#22810;&#24369;&#36129;&#29486;&#20803;&#32032;&#12290;&#20316;&#20026;&#20351;&#29992;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#27169;&#22411;&#30340;&#26222;&#36941;&#23646;&#24615;&#65292;&#24050;&#34987;&#35777;&#26126;&#26159;&#25552;&#39640;&#27169;&#22411;&#25512;&#29702;&#25928;&#29575;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#33539;&#20363;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#37319;&#29992;&#20102;&#27809;&#26377;&#20869;&#22312;&#28608;&#27963;&#31232;&#30095;&#24615;&#30340;&#28608;&#27963;&#20989;&#25968;&#65288;&#20363;&#22914;GELU&#21644;Swish&#65289;&#12290;&#19968;&#20123;&#26368;&#36817;&#30340;&#21162;&#21147;&#23581;&#35797;&#24341;&#20837;ReLU&#25110;&#20854;&#21464;&#20307;&#20316;&#20026;&#26367;&#20195;&#28608;&#27963;&#20989;&#25968;&#65292;&#20197;&#24110;&#21161;LLMs&#23454;&#29616;&#28608;&#27963;&#31232;&#30095;&#24615;&#21644;&#25512;&#29702;&#21152;&#36895;&#65292;&#20294;&#24456;&#23569;&#33021;&#21516;&#26102;&#33719;&#24471;&#39640;&#31232;&#30095;&#24230;&#21644;&#21487;&#27604;&#36739;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"ProSparse"&#30340;&#26377;&#25928;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#20197;&#25512;&#21160;LLMs&#23454;&#29616;&#26356;&#39640;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#32780;&#19981;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23558;LLMs&#30340;&#28608;&#27963;&#20989;&#25968;&#26367;&#25442;&#20026;ReLU&#21518;&#65292;ProSparse&#37319;&#29992;&#28176;&#36827;&#31232;&#30095;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13516v1 Announce Type: cross  Abstract: Activation sparsity refers to the existence of considerable weakly-contributed elements among activation outputs. As a prevalent property of the models using the ReLU activation function, it has been proven a promising paradigm to boost model inference efficiency. Nevertheless, most large language models (LLMs) adopt activation functions without intrinsic activation sparsity (e.g., GELU and Swish). Some recent efforts have explored introducing ReLU or its variants as the substitutive activation function to help LLMs achieve activation sparsity and inference acceleration, but few can simultaneously obtain high sparsity and comparable model performance. This paper introduces an effective sparsification method named "ProSparse" to push LLMs for higher activation sparsity without decreasing model performance. Specifically, after substituting the activation function of LLMs with ReLU, ProSparse adopts progressive sparsity regularization wit
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#25345;&#32493;&#39044;&#35757;&#32451;&#25991;&#26723;&#20043;&#21069;&#26292;&#38706;LLM&#21040;&#38382;&#39064;-&#31572;&#26696;&#23545;&#65292;&#20197;&#20415;&#20174;&#22797;&#26434;&#25991;&#26723;&#20013;&#32534;&#30721;&#30693;&#35782;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#30693;&#35782;&#35775;&#38382;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2402.12847</link><description>&lt;p&gt;
&#35843;&#25972;&#36807;&#30340;&#35821;&#35328;&#27169;&#22411;&#26356;&#22909;&#30340;&#30693;&#35782;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned Language Models are Better Knowledge Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12847
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#25345;&#32493;&#39044;&#35757;&#32451;&#25991;&#26723;&#20043;&#21069;&#26292;&#38706;LLM&#21040;&#38382;&#39064;-&#31572;&#26696;&#23545;&#65292;&#20197;&#20415;&#20174;&#22797;&#26434;&#25991;&#26723;&#20013;&#32534;&#30721;&#30693;&#35782;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#30693;&#35782;&#35775;&#38382;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20351;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21161;&#25163;&#33021;&#22815;&#26377;&#25928;&#22320;&#36866;&#24212;&#19981;&#26029;&#21457;&#23637;&#30340;&#20449;&#24687;&#38656;&#27714;&#65292;&#24517;&#39035;&#33021;&#22815;&#36890;&#36807;&#25345;&#32493;&#22312;&#26032;&#25968;&#25454;&#19978;&#35757;&#32451;&#26469;&#26356;&#26032;&#23427;&#20204;&#30340;&#20107;&#23454;&#30693;&#35782;&#12290;&#20256;&#32479;&#20570;&#27861;&#28041;&#21450;&#22312;&#26032;&#25991;&#26723;&#19978;&#25345;&#32493;&#39044;&#22521;&#35757;&#65292;&#28982;&#21518;&#26681;&#25454;&#38382;&#39064;-&#31572;&#26696;&#65288;QA&#65289;&#23545;&#36827;&#34892;&#25351;&#23548;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12847v1 Announce Type: cross  Abstract: In order for large language model (LLM)-based assistants to effectively adapt to evolving information needs, it must be possible to update their factual knowledge through continued training on new data. The standard recipe for doing so involves continued pre-training on new documents followed by instruction-tuning on question-answer (QA) pairs. However, we find that LLMs trained with this recipe struggle to answer questions, even though the perplexity of documents is minimized. We found that QA pairs are generally straightforward, while documents are more complex, weaving many factual statements together in an intricate manner. Therefore, we hypothesize that it is beneficial to expose LLMs to QA pairs before continued pre-training on documents so that the process of encoding knowledge from complex documents takes into account how this knowledge is accessed through questions. Based on this, we propose pre-instruction-tuning (PIT), a met
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32534;&#20889;&#20195;&#30721;&#21644;&#19982;&#29615;&#22659;&#20132;&#20114;&#26469;&#26500;&#24314;&#19990;&#30028;&#27169;&#22411;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;LLM&#20195;&#29702;&#22312;&#26679;&#26412;&#25928;&#29575;&#19978;&#20248;&#20110;&#28145;&#24230;RL&#65292;&#24182;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#20248;&#20110;ReAct&#39118;&#26684;&#30340;&#20195;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.12275</link><description>&lt;p&gt;
WorldCoder&#65292;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;LLM&#20195;&#29702;&#65306;&#36890;&#36807;&#32534;&#20889;&#20195;&#30721;&#21644;&#19982;&#29615;&#22659;&#20132;&#20114;&#26500;&#24314;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
WorldCoder, a Model-Based LLM Agent: Building World Models by Writing Code and Interacting with the Environment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12275
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32534;&#20889;&#20195;&#30721;&#21644;&#19982;&#29615;&#22659;&#20132;&#20114;&#26469;&#26500;&#24314;&#19990;&#30028;&#27169;&#22411;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;LLM&#20195;&#29702;&#22312;&#26679;&#26412;&#25928;&#29575;&#19978;&#20248;&#20110;&#28145;&#24230;RL&#65292;&#24182;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#20248;&#20110;ReAct&#39118;&#26684;&#30340;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#20195;&#29702;&#65292;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#26500;&#24314;&#20195;&#34920;&#20854;&#23545;&#19990;&#30028;&#30693;&#35782;&#30340;Python&#31243;&#24207;&#12290;&#35813;&#19990;&#30028;&#27169;&#22411;&#35797;&#22270;&#35299;&#37322;&#20854;&#20132;&#20114;&#65292;&#21516;&#26102;&#23545;&#33258;&#24049;&#33021;&#22815;&#33719;&#24471;&#30340;&#22870;&#21169;&#25345;&#20048;&#35266;&#24577;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#25193;&#23637;LLM&#30340;&#31243;&#24207;&#21512;&#25104;&#24037;&#20316;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#22312;&#32593;&#26684;&#19990;&#30028;&#19978;&#30740;&#31350;&#20102;&#25105;&#20204;&#30340;&#20195;&#29702;&#65292;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26679;&#26412;&#25928;&#29575;&#19978;&#27604;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26356;&#39640;&#65292;&#24182;&#19988;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#27604;ReAct&#39118;&#26684;&#30340;&#20195;&#29702;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12275v1 Announce Type: new  Abstract: We give a model-based agent that builds a Python program representing its knowledge of the world based on its interactions with the environment. The world model tries to explain its interactions, while also being optimistic about what reward it can achieve. We do this by extending work on program synthesis via LLMs. We study our agent on gridworlds, finding our approach is more sample-efficient compared to deep RL, and more compute-efficient compared to ReAct-style agents.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#20013;&#25991;LLMs&#23433;&#20840;&#24615;&#30340;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#32454;&#31890;&#24230;&#30340;&#23433;&#20840;&#35780;&#20272;&#26631;&#20934;&#65292;&#20197;&#21450;&#25193;&#23637;&#20102;&#20004;&#31181;&#22330;&#26223;&#29992;&#20110;&#35782;&#21035;&#26377;&#39118;&#38505;&#25552;&#31034;&#25298;&#32477;&#30340;&#34394;&#38420;&#36127;&#38754;&#21644;&#38169;&#35823;&#32943;&#23450;&#31034;&#20363;&#12290;</title><link>https://arxiv.org/abs/2402.12193</link><description>&lt;p&gt;
&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23433;&#20840;&#26426;&#21046;&#30340;&#20013;&#25991;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A Chinese Dataset for Evaluating the Safeguards in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12193
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#20013;&#25991;LLMs&#23433;&#20840;&#24615;&#30340;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#32454;&#31890;&#24230;&#30340;&#23433;&#20840;&#35780;&#20272;&#26631;&#20934;&#65292;&#20197;&#21450;&#25193;&#23637;&#20102;&#20004;&#31181;&#22330;&#26223;&#29992;&#20110;&#35782;&#21035;&#26377;&#39118;&#38505;&#25552;&#31034;&#25298;&#32477;&#30340;&#34394;&#38420;&#36127;&#38754;&#21644;&#38169;&#35823;&#32943;&#23450;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#33021;&#20135;&#29983;&#26377;&#23475;&#21709;&#24212;&#65292;&#22312;LLMs&#37096;&#32626;&#26102;&#20351;&#29992;&#25143;&#38754;&#20020;&#24847;&#22806;&#39118;&#38505;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#20851;&#20110;LLMs&#24341;&#21457;&#39118;&#38505;&#30340;&#32508;&#21512;&#20998;&#31867;&#27861;&#65292;&#20197;&#21450;&#30456;&#24212;&#30340;&#25552;&#31034;&#65292;&#21487;&#29992;&#20110;&#26816;&#26597;LLMs&#30340;&#23433;&#20840;&#26426;&#21046;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#20960;&#20046;&#23436;&#20840;&#38598;&#20013;&#22312;&#33521;&#35821;&#19978;&#65292;&#20854;&#20182;&#35821;&#35328;&#30340;&#30740;&#31350;&#36739;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#20013;&#25991;LLMs&#23433;&#20840;&#24615;&#30340;&#25968;&#25454;&#38598;&#65292;&#28982;&#21518;&#23558;&#20854;&#25193;&#23637;&#21040;&#21478;&#22806;&#20004;&#31181;&#24773;&#26223;&#65292;&#21487;&#29992;&#20110;&#26356;&#22909;&#22320;&#35782;&#21035;&#20851;&#20110;&#26377;&#39118;&#38505;&#25552;&#31034;&#25298;&#32477;&#30340;&#34394;&#38420;&#36127;&#38754;&#21644;&#38169;&#35823;&#32943;&#23450;&#31034;&#20363;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#38024;&#23545;&#27599;&#31181;&#39118;&#38505;&#31867;&#22411;&#25552;&#20986;&#19968;&#32452;&#32454;&#31890;&#24230;&#30340;&#23433;&#20840;&#35780;&#20272;&#26631;&#20934;&#65292;&#20419;&#36827;&#20154;&#24037;&#26631;&#27880;&#21644;&#33258;&#21160;&#35780;&#20272;LLM&#21709;&#24212;&#26377;&#23475;&#24615;&#12290;&#25105;&#20204;&#23545;&#20116;&#20010;LLM&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#29305;&#23450;&#20110;&#22320;&#21306;&#30340;&#39118;&#38505;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12193v1 Announce Type: new  Abstract: Many studies have demonstrated that large language models (LLMs) can produce harmful responses, exposing users to unexpected risks when LLMs are deployed. Previous studies have proposed comprehensive taxonomies of the risks posed by LLMs, as well as corresponding prompts that can be used to examine the safety mechanisms of LLMs. However, the focus has been almost exclusively on English, and little has been explored for other languages. Here we aim to bridge this gap. We first introduce a dataset for the safety evaluation of Chinese LLMs, and then extend it to two other scenarios that can be used to better identify false negative and false positive examples in terms of risky prompt rejections. We further present a set of fine-grained safety assessment criteria for each risk type, facilitating both manual annotation and automatic evaluation in terms of LLM response harmfulness. Our experiments on five LLMs show that region-specific risks a
&lt;/p&gt;</description></item><item><title>Meta Ranking&#26041;&#27861;&#36890;&#36807;&#27604;&#36739;&#30446;&#26631;&#26597;&#35810;-&#21709;&#24212;&#23545;&#19982;&#21442;&#32771;&#26597;&#35810;-&#21709;&#24212;&#23545;&#26469;&#20351;&#36739;&#19981;&#20855;&#22791;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#26377;&#25928;&#22320;&#35780;&#20272;&#21333;&#20010;&#21709;&#24212;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12146</link><description>&lt;p&gt;
Meta Ranking: Less Capable Language Models are Capable for Single Response Judgement
&lt;/p&gt;
&lt;p&gt;
Meta Ranking: Less Capable Language Models are Capable for Single Response Judgement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12146
&lt;/p&gt;
&lt;p&gt;
Meta Ranking&#26041;&#27861;&#36890;&#36807;&#27604;&#36739;&#30446;&#26631;&#26597;&#35810;-&#21709;&#24212;&#23545;&#19982;&#21442;&#32771;&#26597;&#35810;-&#21709;&#24212;&#23545;&#26469;&#20351;&#36739;&#19981;&#20855;&#22791;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#26377;&#25928;&#22320;&#35780;&#20272;&#21333;&#20010;&#21709;&#24212;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24191;&#27867;&#20219;&#21153;&#20013;&#23637;&#29616;&#24378;&#22823;&#24615;&#33021;&#65292;&#20294;&#20173;&#38754;&#20020;&#24187;&#35273;&#31561;&#21487;&#38752;&#24615;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20687;GPT-4&#36825;&#26679;&#39640;&#33021;&#21147;&#30340;LLMs&#22312;&#35780;&#20272;&#21333;&#20010;&#21709;&#24212;&#30340;&#21487;&#38752;&#24615;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#32780;&#36739;&#19981;&#20855;&#22791;&#33021;&#21147;&#30340;LLMs&#36890;&#24120;&#34987;&#35843;&#25972;&#26469;&#35780;&#20272;&#23545;&#30456;&#21516;&#26597;&#35810;&#30340;&#21709;&#24212;&#30340;&#30456;&#23545;&#21487;&#38752;&#24615;&#12290;&#20026;&#20102;&#20351;&#36739;&#19981;&#20855;&#22791;&#33021;&#21147;&#30340;LLMs&#26377;&#25928;&#22320;&#35780;&#20272;&#21333;&#20010;&#21709;&#24212;&#30340;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;$\textit{Meta}$ $\textit{Ranking}$&#65288;MR&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#19982;&#20808;&#21069;&#30452;&#25509;&#35780;&#20272;&#21709;&#24212;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#30446;&#26631;&#26597;&#35810;-&#21709;&#24212;&#23545;&#19982;&#21442;&#32771;&#26597;&#35810;-&#21709;&#24212;&#23545;&#36827;&#34892;&#27604;&#36739;&#26469;&#23454;&#29616;&#21028;&#26029;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#25512;&#29702;&#20219;&#21153;&#30340;LLM&#21709;&#24212;&#30340;&#38169;&#35823;&#26816;&#27979;&#20013;&#65292;MR&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#26377;&#25928;&#24615;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#36739;&#19981;&#20855;&#22791;&#33021;&#21147;&#30340;LLMs&#20063;&#33021;&#32988;&#36807;&#24378;&#22522;&#32447;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;MR&#21487;&#20197;&#34987;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12146v1 Announce Type: cross  Abstract: Although Large Language Models (LLMs) have demonstrated strong performance on a wide range of tasks, they still face reliability challenges such as hallucination. Previous studies reveal that highly capable LLMs like GPT-4 are effective in judging the reliability of individual responses, while less capable ones are often tuned to evaluate the relative reliability of responses to the same query. To enable less capable LLMs to effectively judge the reliability of individual responses, we propose a novel method named $\textit{Meta}$ $\textit{Ranking}$ (MR). Unlike previous methods, which assess the response directly, we achieve the judgement by comparing the target query-response pair with reference query-response pairs. We found its remarkable effectiveness in error detection for LLM responses on reasoning tasks, where less capable LLMs could outperform strong baselines, even without fine-tuning. We further demonstrate that MR can be use
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27714;&#35299;&#22120;&#23618;&#36866;&#24212;&#65288;SoLA&#65289;&#26041;&#27861;&#65292;&#22312;LLM&#20013;&#24341;&#20837;&#27714;&#35299;&#22120;&#23618;&#65292;&#19981;&#21516;&#22320;&#24341;&#23548;&#35299;&#20915;&#26041;&#26696;&#26397;&#21521;&#21487;&#28385;&#36275;&#24615;</title><link>https://arxiv.org/abs/2402.11903</link><description>&lt;p&gt;
SoLA: &#20026;&#20102;&#26356;&#22909;&#30340;&#36923;&#36753;&#25512;&#29702;&#32780;&#23545;LLM&#36827;&#34892;&#27714;&#35299;&#23618;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
SoLA: Solver-Layer Adaption of LLM for Better Logic Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11903
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27714;&#35299;&#22120;&#23618;&#36866;&#24212;&#65288;SoLA&#65289;&#26041;&#27861;&#65292;&#22312;LLM&#20013;&#24341;&#20837;&#27714;&#35299;&#22120;&#23618;&#65292;&#19981;&#21516;&#22320;&#24341;&#23548;&#35299;&#20915;&#26041;&#26696;&#26397;&#21521;&#21487;&#28385;&#36275;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36923;&#36753;&#25512;&#29702;&#19978;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#20808;&#21069;&#30340;&#21162;&#21147;&#35797;&#22270;&#36890;&#36807;&#24037;&#20855;&#23398;&#20064;&#26469;&#25913;&#21464;&#38382;&#39064;&#27714;&#35299;&#12290;&#34429;&#28982;&#22312;&#23567;&#35268;&#27169;&#38382;&#39064;&#19978;&#24050;&#32463;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#35268;&#27169;&#24222;&#22823;&#19988;&#34920;&#36798;&#22797;&#26434;&#65292;&#35299;&#20915;&#24037;&#19994;&#26696;&#20363;&#20173;&#28982;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27714;&#35299;&#23618;&#36866;&#24212;&#65288;SoLA&#65289;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;LLM&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#27714;&#35299;&#22120;&#20316;&#20026;&#26032;&#23618;&#65292;&#19981;&#21516;&#22320;&#24341;&#23548;&#35299;&#20915;&#26041;&#26696;&#26397;&#21521;&#21487;&#28385;&#36275;&#24615;&#12290;&#22312;SoLA&#20013;&#65292;LLM&#26088;&#22312;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#24182;&#35782;&#21035;&#26368;&#39640;&#36136;&#37327;&#30340;&#23616;&#37096;&#35299;&#65292;&#32780;&#27714;&#35299;&#22120;&#23618;&#21017;&#19987;&#27880;&#20110;&#21021;&#22987;&#35299;&#19981;&#28385;&#36275;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#20511;&#21161;MaxSAT&#20316;&#20026;&#26725;&#26753;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#21069;&#21521;&#21644;&#21518;&#21521;&#20256;&#36882;&#26799;&#24230;&#65292;&#20351;&#26368;&#32456;&#27169;&#22411;&#33021;&#22815;&#25910;&#25947;&#21040;&#19968;&#20010;&#28385;&#36275;&#30340;&#35299;&#25110;&#35777;&#26126;&#19981;&#21487;&#28385;&#36275;&#24615;&#12290;&#21518;&#38376;&#29702;&#35770;&#30830;&#20445;SoLA&#33021;&#22815;&#33719;&#24471;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11903v1 Announce Type: cross  Abstract: Considering the challenges faced by large language models (LLMs) on logical reasoning, prior efforts have sought to transform problem-solving through tool learning. While progress has been made on small-scale problems, solving industrial cases remains difficult due to their large scale and intricate expressions. In this paper, we propose a novel solver-layer adaptation (SoLA) method, where we introduce a solver as a new layer of the LLM to differentially guide solutions towards satisfiability. In SoLA, LLM aims to comprehend the search space described in natural language and identify local solutions of the highest quality, while the solver layer focuses solely on constraints not satisfied by the initial solution. Leveraging MaxSAT as a bridge, we define forward and backward transfer gradients, enabling the final model to converge to a satisfied solution or prove unsatisfiability. The backdoor theory ensures that SoLA can obtain accurat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;IS3&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#22686;&#37327;&#24207;&#21015;&#26631;&#35760;&#20219;&#21153;&#20013;&#30340;E2O&#21644;O2E&#20004;&#31181;&#37325;&#35201;&#30340;&#35821;&#20041;&#36716;&#21464;&#65292;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#26469;&#32500;&#25345;&#23545;&#26087;&#23454;&#20307;&#30340;&#21028;&#21035;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.10447</link><description>&lt;p&gt;
&#22686;&#37327;&#24207;&#21015;&#26631;&#35760;&#65306;&#20004;&#31181;&#36716;&#21464;&#30340;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
Incremental Sequence Labeling: A Tale of Two Shifts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10447
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;IS3&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#22686;&#37327;&#24207;&#21015;&#26631;&#35760;&#20219;&#21153;&#20013;&#30340;E2O&#21644;O2E&#20004;&#31181;&#37325;&#35201;&#30340;&#35821;&#20041;&#36716;&#21464;&#65292;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#26469;&#32500;&#25345;&#23545;&#26087;&#23454;&#20307;&#30340;&#21028;&#21035;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#37327;&#24207;&#21015;&#26631;&#35760;&#20219;&#21153;&#28041;&#21450;&#22312;&#20445;&#30041;&#23545;&#20808;&#21069;&#31867;&#21035;&#30693;&#35782;&#30340;&#21516;&#26102;&#65292;&#38543;&#26102;&#38388;&#19981;&#26029;&#23398;&#20064;&#26032;&#31867;&#21035;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#30830;&#23450;&#20102;&#20004;&#31181;&#37325;&#35201;&#30340;&#35821;&#20041;&#36716;&#21464;&#65306;E2O&#65288;&#27169;&#22411;&#23558;&#26087;&#23454;&#20307;&#38169;&#35823;&#26631;&#35760;&#20026;&#38750;&#23454;&#20307;&#65289;&#21644;O2E&#65288;&#27169;&#22411;&#23558;&#38750;&#23454;&#20307;&#25110;&#26087;&#23454;&#20307;&#26631;&#35760;&#20026;&#26032;&#23454;&#20307;&#65289;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#35299;&#20915;E2O&#38382;&#39064;&#19978;&#65292;&#24573;&#35270;&#20102;O2E&#38382;&#39064;&#12290;&#36825;&#31181;&#24573;&#30053;&#23548;&#33268;&#27169;&#22411;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#23545;&#26032;&#25968;&#25454;&#26679;&#26412;&#36827;&#34892;&#20998;&#31867;&#26102;&#23384;&#22312;&#20559;&#35265;&#65292;&#35748;&#20026;&#23427;&#20204;&#23646;&#20110;&#26032;&#31867;&#21035;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21363;&#26080;&#35821;&#20041;&#36716;&#21464;&#30340;&#22686;&#37327;&#39034;&#24207;&#26631;&#35760;&#65288;IS3&#65289;&#12290;&#21463;&#21040;&#24050;&#30830;&#23450;&#30340;&#35821;&#20041;&#36716;&#21464;&#65288;E2O&#21644;O2E&#65289;&#30340;&#21551;&#21457;&#65292;IS3&#26088;&#22312;&#32531;&#35299;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#33267;&#20110;E2O&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#26469;&#32500;&#25345;&#27169;&#22411;&#23545;&#26087;&#23454;&#20307;&#30340;&#21028;&#21035;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10447v1 Announce Type: new  Abstract: The incremental sequence labeling task involves continuously learning new classes over time while retaining knowledge of the previous ones. Our investigation identifies two significant semantic shifts: E2O (where the model mislabels an old entity as a non-entity) and O2E (where the model labels a non-entity or old entity as a new entity). Previous research has predominantly focused on addressing the E2O problem, neglecting the O2E issue. This negligence results in a model bias towards classifying new data samples as belonging to the new class during the learning process. To address these challenges, we propose a novel framework, Incremental Sequential Labeling without Semantic Shifts (IS3). Motivated by the identified semantic shifts (E2O and O2E), IS3 aims to mitigate catastrophic forgetting in models. As for the E2O problem, we use knowledge distillation to maintain the model's discriminative ability for old entities. Simultaneously, t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;HyperBERT&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#20013;&#24341;&#20837;&#36229;&#22270;&#24863;&#30693;&#23618;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#19978;&#38590;&#20197;&#25429;&#25417;&#36229;&#22270;&#32467;&#26500;&#20449;&#24687;&#21644;&#25991;&#26412;&#23646;&#24615;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25928;&#26524;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.07309</link><description>&lt;p&gt;
HyperBERT:&#23558;&#28151;&#21512;&#36229;&#22270;&#24863;&#30693;&#23618;&#19982;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#25991;&#26412;&#23646;&#24615;&#36229;&#22270;&#19978;&#30340;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
HyperBERT: Mixing Hypergraph-Aware Layers with Language Models for Node Classification on Text-Attributed Hypergraphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07309
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;HyperBERT&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#20013;&#24341;&#20837;&#36229;&#22270;&#24863;&#30693;&#23618;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#19978;&#38590;&#20197;&#25429;&#25417;&#36229;&#22270;&#32467;&#26500;&#20449;&#24687;&#21644;&#25991;&#26412;&#23646;&#24615;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25928;&#26524;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22270;&#36890;&#36807;&#22797;&#26434;&#30340;&#25299;&#25169;&#32467;&#26500;&#26631;&#35760;&#65292;&#34920;&#36798;&#22810;&#20010;&#23454;&#20307;&#20043;&#38388;&#30340;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#65292;&#20854;&#20013;&#36229;&#36793;&#25198;&#28436;&#37325;&#35201;&#35282;&#33394;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#36229;&#22270;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#23398;&#20064;&#25991;&#26412;&#23646;&#24615;&#36229;&#22270;&#19978;&#30340;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#20013;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#38590;&#20197;&#21516;&#26102;&#25429;&#25417;&#36229;&#22270;&#32467;&#26500;&#20449;&#24687;&#30340;&#20840;&#37096;&#20869;&#23481;&#21644;&#33410;&#28857;&#23646;&#24615;&#20013;&#30340;&#20016;&#23500;&#35821;&#35328;&#23646;&#24615;&#65292;&#36825;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24433;&#21709;&#20102;&#23427;&#20204;&#30340;&#25928;&#26524;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20026;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#36827;&#19968;&#27493;&#22686;&#24378;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#65292;&#24341;&#20837;&#19987;&#38376;&#30340;&#36229;&#22270;&#24863;&#30693;&#23618;&#12290;&#36825;&#20123;&#23618;&#23558;&#39640;&#38454;&#32467;&#26500;&#24402;&#32435;&#20559;&#24046;&#24341;&#20837;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#21033;&#29992;&#36229;&#22270;&#32467;&#26500;&#20013;&#30340;&#39640;&#38454;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#25991;&#26412;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypergraphs are marked by complex topology, expressing higher-order interactions among multiple entities with hyperedges. Lately, hypergraph-based deep learning methods to learn informative data representations for the problem of node classification on text-attributed hypergraphs have garnered increasing research attention. However, existing methods struggle to simultaneously capture the full extent of hypergraph structural information and the rich linguistic attributes inherent in the nodes attributes, which largely hampers their effectiveness and generalizability. To overcome these challenges, we explore ways to further augment a pretrained BERT model with specialized hypergraph-aware layers for the task of node classification. Such layers introduce higher-order structural inductive bias into the language model, thus improving the model's capacity to harness both higher-order context information from the hypergraph structure and semantic information present in text. In this paper, we
&lt;/p&gt;</description></item><item><title>Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#31163;&#25955;&#31639;&#27861;&#26041;&#38754;&#30340;&#32452;&#21512;&#33021;&#21147;&#38750;&#24120;&#26377;&#38480;&#65292;&#27604;&#37325;&#26032;&#23398;&#20064;&#25152;&#26377;&#23376;&#20219;&#21153;&#23545;&#20110;&#26032;&#30340;&#31639;&#27861;&#32452;&#21512;&#30340;&#25928;&#26524;&#26356;&#24046;&#65292;&#32780;&#19988;&#26799;&#24230;&#19979;&#38477;&#22312;&#35760;&#24518;&#21069;&#39304;&#27169;&#22411;&#19978;&#30340;&#25928;&#29575;&#38750;&#24120;&#20302;&#12290;</title><link>https://arxiv.org/abs/2402.05785</link><description>&lt;p&gt;
Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#31639;&#27861;&#23398;&#20064;&#19978;&#30340;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Limits of Transformer Language Models on Algorithmic Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05785
&lt;/p&gt;
&lt;p&gt;
Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#31163;&#25955;&#31639;&#27861;&#26041;&#38754;&#30340;&#32452;&#21512;&#33021;&#21147;&#38750;&#24120;&#26377;&#38480;&#65292;&#27604;&#37325;&#26032;&#23398;&#20064;&#25152;&#26377;&#23376;&#20219;&#21153;&#23545;&#20110;&#26032;&#30340;&#31639;&#27861;&#32452;&#21512;&#30340;&#25928;&#26524;&#26356;&#24046;&#65292;&#32780;&#19988;&#26799;&#24230;&#19979;&#38477;&#22312;&#35760;&#24518;&#21069;&#39304;&#27169;&#22411;&#19978;&#30340;&#25928;&#29575;&#38750;&#24120;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#20102;Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#31163;&#25955;&#31639;&#27861;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#35201;&#27714;&#32452;&#21512;&#22810;&#20010;&#31163;&#25955;&#23376;&#20219;&#21153;&#30340;&#26032;&#20219;&#21153;&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;LLaMA&#27169;&#22411;&#21644;&#22312;GPT-4&#21644;Gemini&#19978;&#25552;&#31034;&#26469;&#34913;&#37327;&#23398;&#20064;&#23398;&#20064;&#21407;&#35821;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#21512;&#33021;&#21147;&#38750;&#24120;&#26377;&#38480;&#65292;&#24182;&#19988;&#22312;&#26679;&#26412;&#35268;&#27169;&#26041;&#38754;&#27604;&#20026;&#26032;&#30340;&#31639;&#27861;&#32452;&#21512;&#37325;&#26032;&#23398;&#20064;&#25152;&#26377;&#23376;&#20219;&#21153;&#25928;&#26524;&#26356;&#24046;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#22797;&#26434;&#24615;&#29702;&#35770;&#30340;&#23450;&#29702;&#65292;&#35777;&#26126;&#20102;&#35760;&#24518;&#21069;&#39304;&#27169;&#22411;&#19978;&#30340;&#26799;&#24230;&#19979;&#38477;&#21487;&#20197;&#25351;&#25968;&#32423;&#22320;&#28010;&#36153;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze the capabilities of Transformer language models on learning discrete algorithms. To this end, we introduce two new tasks demanding the composition of several discrete sub-tasks. On both training LLaMA models from scratch and prompting on GPT-4 and Gemini we measure learning compositions of learned primitives. We observe that the compositional capabilities of state-of-the-art Transformer language models are very limited and sample-wise scale worse than relearning all sub-tasks for a new algorithmic composition. We also present a theorem in complexity theory, showing that gradient descent on memorizing feedforward models can be exponentially data inefficient.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20449;&#24687;&#20445;&#30041;&#25512;&#21160;&#37327;&#21270;LLMs&#30340;LoRA-Finetuning&#30340;&#26041;&#27861;IR-QLoRA&#65292;&#20351;&#29992;&#32479;&#35745;&#20449;&#24687;&#26657;&#20934;&#21644;&#35843;&#20248;&#20449;&#24687;&#24377;&#24615;&#36830;&#25509;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05445</link><description>&lt;p&gt;
&#20934;&#30830;&#30340;LLMs&#30340;LoRA-Finetuning&#37327;&#21270;&#36890;&#36807;&#20449;&#24687;&#20445;&#30041;
&lt;/p&gt;
&lt;p&gt;
Accurate LoRA-Finetuning Quantization of LLMs via Information Retention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20449;&#24687;&#20445;&#30041;&#25512;&#21160;&#37327;&#21270;LLMs&#30340;LoRA-Finetuning&#30340;&#26041;&#27861;IR-QLoRA&#65292;&#20351;&#29992;&#32479;&#35745;&#20449;&#24687;&#26657;&#20934;&#21644;&#35843;&#20248;&#20449;&#24687;&#24377;&#24615;&#36830;&#25509;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;LLMs&#30340;LoRA-finetuning&#37327;&#21270;&#30740;&#31350;&#24471;&#21040;&#20934;&#30830;&#20294;&#32039;&#20945;&#30340;LLMs&#20197;&#20415;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#30828;&#20214;&#19978;&#37096;&#32626;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#23548;&#33268;&#37327;&#21270;&#30340;LLMs&#20005;&#37325;&#36864;&#21270;&#65292;&#29978;&#33267;&#26080;&#27861;&#20174;LoRA&#30340;&#35843;&#20248;&#20013;&#33719;&#30410;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;IR-QLoRA&#65292;&#36890;&#36807;&#20449;&#24687;&#20445;&#30041;&#25512;&#21160;&#24102;&#26377;LoRA&#30340;&#37327;&#21270;LLMs&#21464;&#24471;&#39640;&#24230;&#20934;&#30830;&#12290;&#25152;&#25552;&#20986;&#30340;IR-QLoRA&#20027;&#35201;&#20381;&#36182;&#20110;&#20004;&#31181;&#20174;&#32479;&#19968;&#20449;&#24687;&#35270;&#35282;&#27966;&#29983;&#30340;&#25216;&#26415;&#65306;&#65288;1&#65289;&#22522;&#20110;&#32479;&#35745;&#30340;&#20449;&#24687;&#26657;&#20934;&#37327;&#21270;&#20801;&#35768;LLMs&#30340;&#37327;&#21270;&#21442;&#25968;&#31934;&#30830;&#20445;&#30041;&#21407;&#22987;&#20449;&#24687;&#65307;&#65288;2&#65289;&#22522;&#20110;&#35843;&#20248;&#30340;&#20449;&#24687;&#24377;&#24615;&#36830;&#25509;&#20351;LoRA&#21033;&#29992;&#20855;&#26377;&#22810;&#26679;&#20449;&#24687;&#30340;&#24377;&#24615;&#34920;&#31034;&#36716;&#25442;&#12290;&#32508;&#21512;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;2-4&#20301;&#23485;&#19979;&#65292;IR-QLoRA&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;LLaMA&#21644;LLaMA2&#31995;&#21015;&#30340;&#20934;&#30830;&#24615;&#65292;&#20363;&#22914;&#65292;4&#20301;LLaMA-7B&#30456;&#27604;&#20110;...
&lt;/p&gt;
&lt;p&gt;
The LoRA-finetuning quantization of LLMs has been extensively studied to obtain accurate yet compact LLMs for deployment on resource-constrained hardware. However, existing methods cause the quantized LLM to severely degrade and even fail to benefit from the finetuning of LoRA. This paper proposes a novel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate through information retention. The proposed IR-QLoRA mainly relies on two technologies derived from the perspective of unified information: (1) statistics-based Information Calibration Quantization allows the quantized parameters of LLM to retain original information accurately; (2) finetuning-based Information Elastic Connection makes LoRA utilizes elastic representation transformation with diverse information. Comprehensive experiments show that IR-QLoRA can significantly improve accuracy across LLaMA and LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4% improvement on MMLU compared with the 
&lt;/p&gt;</description></item><item><title>MEMORYLLM&#26159;&#19968;&#20010;&#33258;&#26356;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;Transformer&#21644;&#19968;&#20010;&#22266;&#23450;&#22823;&#23567;&#30340;&#20869;&#23384;&#27744;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#25972;&#21512;&#26032;&#30693;&#35782;&#24182;&#20445;&#25345;&#38271;&#26399;&#20449;&#24687;&#20445;&#30041;&#33021;&#21147;&#12290;&#21363;&#20351;&#22312;&#36817;&#30334;&#19975;&#27425;&#20869;&#23384;&#26356;&#26032;&#21518;&#65292;MEMORYLLM&#20173;&#33021;&#20445;&#25345;&#25805;&#20316;&#30340;&#23436;&#25972;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04624</link><description>&lt;p&gt;
MEMORYLLM&#65306;&#38754;&#21521;&#33258;&#26356;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MEMORYLLM: Towards Self-Updatable Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04624
&lt;/p&gt;
&lt;p&gt;
MEMORYLLM&#26159;&#19968;&#20010;&#33258;&#26356;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;Transformer&#21644;&#19968;&#20010;&#22266;&#23450;&#22823;&#23567;&#30340;&#20869;&#23384;&#27744;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#25972;&#21512;&#26032;&#30693;&#35782;&#24182;&#20445;&#25345;&#38271;&#26399;&#20449;&#24687;&#20445;&#30041;&#33021;&#21147;&#12290;&#21363;&#20351;&#22312;&#36817;&#30334;&#19975;&#27425;&#20869;&#23384;&#26356;&#26032;&#21518;&#65292;MEMORYLLM&#20173;&#33021;&#20445;&#25345;&#25805;&#20316;&#30340;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#37096;&#32626;&#21518;&#36890;&#24120;&#20445;&#25345;&#38745;&#24577;&#65292;&#36825;&#21487;&#33021;&#20250;&#20351;&#21521;&#27169;&#22411;&#20013;&#27880;&#20837;&#26032;&#30693;&#35782;&#21464;&#24471;&#22256;&#38590;&#12290;&#25105;&#20204;&#26088;&#22312;&#26500;&#24314;&#21253;&#21547;&#30456;&#24403;&#27604;&#20363;&#30340;&#21487;&#33258;&#26356;&#26032;&#21442;&#25968;&#30340;&#27169;&#22411;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#19988;&#39640;&#25928;&#22320;&#25972;&#21512;&#26032;&#30693;&#35782;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MEMORYLLM&#65292;&#23427;&#26159;&#19968;&#20010;&#30001;Transformer&#21644;&#22266;&#23450;&#22823;&#23567;&#30340;&#20869;&#23384;&#27744;&#32452;&#25104;&#30340;&#27169;&#22411;&#65292;&#20301;&#20110;Transformer&#30340;&#28508;&#31354;&#38388;&#20869;&#12290;MEMORYLLM&#21487;&#20197;&#36890;&#36807;&#25991;&#26412;&#30693;&#35782;&#36827;&#34892;&#33258;&#25105;&#26356;&#26032;&#24182;&#35760;&#24518;&#20808;&#21069;&#27880;&#20837;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#35777;&#26126;&#20102;MEMORYLLM&#26377;&#25928;&#22320;&#25972;&#21512;&#26032;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#20854;&#24615;&#33021;&#22312;&#27169;&#22411;&#32534;&#36753;&#22522;&#20934;&#19978;&#24471;&#21040;&#20102;&#35777;&#23454;&#12290;&#21516;&#26102;&#65292;&#35813;&#27169;&#22411;&#36824;&#34920;&#29616;&#20986;&#38271;&#26399;&#20449;&#24687;&#20445;&#30041;&#33021;&#21147;&#65292;&#36825;&#22312;&#25105;&#20204;&#33258;&#23450;&#20041;&#30340;&#35780;&#20272;&#21644;&#38271;&#19978;&#19979;&#25991;&#22522;&#20934;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;MEMORYLLM&#22312;&#36827;&#34892;&#20102;&#36817;&#30334;&#19975;&#27425;&#20869;&#23384;&#26356;&#26032;&#21518;&#65292;&#27809;&#26377;&#20219;&#20309;&#24615;&#33021;&#19979;&#38477;&#30340;&#36857;&#35937;&#65292;&#26174;&#31034;&#20986;&#25805;&#20316;&#30340;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing Large Language Models (LLMs) usually remain static after deployment, which might make it hard to inject new knowledge into the model. We aim to build models containing a considerable portion of self-updatable parameters, enabling the model to integrate new knowledge effectively and efficiently. To this end, we introduce MEMORYLLM, a model that comprises a transformer and a fixed-size memory pool within the latent space of the transformer. MEMORYLLM can self-update with text knowledge and memorize the knowledge injected earlier. Our evaluations demonstrate the ability of MEMORYLLM to effectively incorporate new knowledge, as evidenced by its performance on model editing benchmarks. Meanwhile, the model exhibits long-term information retention capacity, which is validated through our custom-designed evaluations and long-context benchmarks. MEMORYLLM also shows operational integrity without any sign of performance degradation even after nearly a million memory updates.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32454;&#31890;&#24230;&#22870;&#21169;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#24341;&#29992;&#30340;&#26377;&#25928;&#26694;&#26550;&#65292;&#24182;&#22312;&#24120;&#35265;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#31574;&#30053;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.04315</link><description>&lt;p&gt;
&#20351;&#29992;&#32454;&#31890;&#24230;&#22870;&#21169;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#24102;&#24341;&#29992;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Training Language Models to Generate Text with Citations via Fine-grained Rewards
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32454;&#31890;&#24230;&#22870;&#21169;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#24341;&#29992;&#30340;&#26377;&#25928;&#26694;&#26550;&#65292;&#24182;&#22312;&#24120;&#35265;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#31574;&#30053;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22238;&#31572;&#29992;&#25143;&#26597;&#35810;&#26041;&#38754;&#38750;&#24120;&#26377;&#29992;&#65292;&#20294;&#23481;&#26131;&#20135;&#29983;&#24187;&#35273;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#22238;&#31572;&#24120;&#24120;&#32570;&#20047;&#21487;&#38752;&#26469;&#28304;&#30340;&#24341;&#29992;&#12290;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#30452;&#35266;&#26041;&#27861;&#26159;&#23558;&#22806;&#37096;&#25991;&#26723;&#30340;&#24341;&#29992;&#20316;&#20026;&#35777;&#25454;&#21253;&#21547;&#22312;&#25991;&#26412;&#20013;&#12290;&#34429;&#28982;&#20197;&#21069;&#30340;&#30740;&#31350;&#30452;&#25509;&#20419;&#20351;LLMs&#29983;&#25104;&#24341;&#29992;&#25991;&#26412;&#65292;&#20294;&#23427;&#20204;&#30340;&#24615;&#33021;&#36828;&#38750;&#20196;&#20154;&#28385;&#24847;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#36739;&#23567;&#30340;LLMs&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#20351;&#29992;&#32454;&#31890;&#24230;&#22870;&#21169;&#25945;&#25480;LLMs&#29983;&#25104;&#39640;&#24230;&#25903;&#25345;&#21644;&#30456;&#20851;&#30340;&#24341;&#29992;&#65292;&#21516;&#26102;&#30830;&#20445;&#20854;&#21709;&#24212;&#30340;&#27491;&#30830;&#24615;&#12290;&#25105;&#20204;&#36824;&#23545;&#23558;&#36825;&#20123;&#32454;&#31890;&#24230;&#22870;&#21169;&#24212;&#29992;&#20110;&#24120;&#35265;&#30340;LLMs&#35757;&#32451;&#31574;&#30053;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#65292;&#35777;&#26126;&#20854;&#30456;&#23545;&#20110;&#20256;&#32479;&#20570;&#27861;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#22312;&#20174;ALCE&#22522;&#20934;&#27979;&#35797;&#20013;&#33719;&#21462;&#30340;&#38382;&#31572;&#65288;QA&#65289;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24182;&#39564;&#35777;&#20102;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
While recent Large Language Models (LLMs) have proven useful in answering user queries, they are prone to hallucination, and their responses often lack credibility due to missing references to reliable sources. An intuitive solution to these issues would be to include in-text citations referring to external documents as evidence. While previous works have directly prompted LLMs to generate in-text citations, their performances are far from satisfactory, especially when it comes to smaller LLMs. In this work, we propose an effective training framework using fine-grained rewards to teach LLMs to generate highly supportive and relevant citations, while ensuring the correctness of their responses. We also conduct a systematic analysis of applying these fine-grained rewards to common LLM training strategies, demonstrating its advantage over conventional practices. We conduct extensive experiments on Question Answering (QA) datasets taken from the ALCE benchmark and validate the model's gene
&lt;/p&gt;</description></item><item><title>ANLS*&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#22411;&#27169;&#22411;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;&#65292;&#38024;&#23545;&#21508;&#31181;&#20219;&#21153;&#21253;&#25324;&#20449;&#24687;&#25552;&#21462;&#21644;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;&#23427;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;ANLS&#24230;&#37327;&#26041;&#27861;&#65292;&#21487;&#20197;&#20316;&#20026;&#26367;&#20195;&#26041;&#26696;&#20351;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.03848</link><description>&lt;p&gt;
ANLS* -- &#19968;&#31181;&#36866;&#29992;&#20110;&#29983;&#25104;&#22411;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#25991;&#26723;&#22788;&#29702;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ANLS* -- A Universal Document Processing Metric for Generative Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03848
&lt;/p&gt;
&lt;p&gt;
ANLS*&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#22411;&#27169;&#22411;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;&#65292;&#38024;&#23545;&#21508;&#31181;&#20219;&#21153;&#21253;&#25324;&#20449;&#24687;&#25552;&#21462;&#21644;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;&#23427;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;ANLS&#24230;&#37327;&#26041;&#27861;&#65292;&#21487;&#20197;&#20316;&#20026;&#26367;&#20195;&#26041;&#26696;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#22312;&#25991;&#26723;&#20998;&#31867;&#21644;&#20449;&#24687;&#25552;&#21462;&#31561;&#20219;&#21153;&#20013;&#65292;&#21306;&#20998;&#27169;&#22411;&#19968;&#30452;&#26159;&#20027;&#35201;&#36873;&#25321;&#12290;&#36825;&#20123;&#27169;&#22411;&#20570;&#20986;&#30340;&#39044;&#27979;&#21487;&#20197;&#20998;&#20026;&#26377;&#38480;&#25968;&#37327;&#30340;&#39044;&#23450;&#20041;&#31867;&#21035;&#65292;&#20415;&#20110;&#36827;&#34892;&#20108;&#20803;&#30495;&#20551;&#35780;&#20272;&#65292;&#24182;&#33021;&#30452;&#25509;&#35745;&#31639;F1&#20998;&#25968;&#31561;&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#22411;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;GLLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#20419;&#20351;&#39046;&#22495;&#21457;&#29983;&#20102;&#36716;&#21464;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#22791;&#20102;&#24378;&#22823;&#30340;&#38646;-shot&#33021;&#21147;&#65292;&#28040;&#38500;&#20102;&#19979;&#28216;&#25968;&#25454;&#38598;&#21644;&#35745;&#31639;&#26114;&#36149;&#30340;&#24494;&#35843;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;GLLMs&#23384;&#22312;&#25361;&#25112;&#65292;&#22240;&#20026;&#23545;&#20110;GLLMs&#30340;&#39044;&#27979;&#65292;&#19981;&#33021;&#24212;&#29992;&#20110;&#21306;&#20998;&#27169;&#22411;&#25152;&#20351;&#29992;&#30340;&#20108;&#20803;&#30495;&#20551;&#35780;&#20272;&#26041;&#27861;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#22411;&#27169;&#22411;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;ANLS*&#65292;&#29992;&#20110;&#35780;&#20272;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#20449;&#24687;&#25552;&#21462;&#21644;&#20998;&#31867;&#20219;&#21153;&#12290;ANLS*&#24230;&#37327;&#26041;&#27861;&#25193;&#23637;&#20102;&#29616;&#26377;ANLS&#24230;&#37327;&#26041;&#27861;&#65292;&#21487;&#20316;&#20026;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditionally, discriminative models have been the predominant choice for tasks like document classification and information extraction. These models make predictions that fall into a limited number of predefined classes, facilitating a binary true or false evaluation and enabling the direct calculation of metrics such as the F1 score. However, recent advancements in generative large language models (GLLMs) have prompted a shift in the field due to their enhanced zero-shot capabilities, which eliminate the need for a downstream dataset and computationally expensive fine-tuning. However, evaluating GLLMs presents a challenge as the binary true or false evaluation used for discriminative models is not applicable to the predictions made by GLLMs. This paper introduces a new metric for generative models called ANLS* for evaluating a wide variety of tasks, including information extraction and classification tasks. The ANLS* metric extends existing ANLS metrics as a drop-in-replacement and i
&lt;/p&gt;</description></item><item><title>&#21516;&#24615;&#36136;&#30340;&#23884;&#20837;&#31354;&#38388;&#23545;&#32858;&#31867;&#21644;&#32447;&#24615;&#20998;&#31867;&#30446;&#26631;&#20855;&#26377;&#36127;&#38754;&#24433;&#21709;&#65292;&#36825;&#19968;&#20107;&#23454;&#24471;&#21040;&#20102;&#26412;&#25991;&#30340;&#23454;&#35777;&#25903;&#25345;&#65292;&#24182;&#23545;&#25991;&#29486;&#20013;&#30340;&#20808;&#21069;&#32467;&#26524;&#26377;&#25152;&#21551;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.03191</link><description>&lt;p&gt;
&#21516;&#24615;&#36136;&#65292;&#32858;&#31867;&#21644;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Isotropy, Clusters, and Classifiers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03191
&lt;/p&gt;
&lt;p&gt;
&#21516;&#24615;&#36136;&#30340;&#23884;&#20837;&#31354;&#38388;&#23545;&#32858;&#31867;&#21644;&#32447;&#24615;&#20998;&#31867;&#30446;&#26631;&#20855;&#26377;&#36127;&#38754;&#24433;&#21709;&#65292;&#36825;&#19968;&#20107;&#23454;&#24471;&#21040;&#20102;&#26412;&#25991;&#30340;&#23454;&#35777;&#25903;&#25345;&#65292;&#24182;&#23545;&#25991;&#29486;&#20013;&#30340;&#20808;&#21069;&#32467;&#26524;&#26377;&#25152;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20851;&#20110;&#23884;&#20837;&#31354;&#38388;&#26159;&#21542;&#22343;&#21248;&#21033;&#29992;&#25152;&#26377;&#32500;&#24230;&#65288;&#21363;&#26159;&#21542;&#20855;&#26377;&#21516;&#24615;&#36136;&#65289;&#30340;&#38382;&#39064;&#24341;&#36215;&#20102;&#35752;&#35770;&#12290;&#26377;&#35777;&#25454;&#25903;&#25345;&#21644;&#21453;&#23545;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#23454;&#26045;&#21516;&#24615;&#36136;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#21516;&#24615;&#36136;&#23545;&#23884;&#20837;&#31354;&#38388;&#30340;&#35201;&#27714;&#19982;&#32858;&#31867;&#30340;&#23384;&#22312;&#19981;&#20860;&#23481;&#65292;&#36825;&#20063;&#23545;&#32447;&#24615;&#20998;&#31867;&#30446;&#26631;&#20135;&#29983;&#20102;&#36127;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20010;&#20107;&#23454;&#65292;&#24182;&#29992;&#23427;&#26469;&#38416;&#26126;&#25991;&#29486;&#20013;&#30340;&#20808;&#21069;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whether embedding spaces use all their dimensions equally, i.e., whether they are isotropic, has been a recent subject of discussion. Evidence has been accrued both for and against enforcing isotropy in embedding spaces. In the present paper, we stress that isotropy imposes requirements on the embedding space that are not compatible with the presence of clusters -- which also negatively impacts linear classification objectives. We demonstrate this fact empirically and use it to shed light on previous results from the literature.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#26694;&#26550;UNIHD&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#35780;&#20272;&#22522;&#20934;&#26041;&#27861;MHaluBench&#26469;&#35780;&#20272;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#30340;&#36827;&#23637;&#12290;&#36825;&#39033;&#24037;&#20316;&#25193;&#23637;&#20102;&#24187;&#35273;&#26816;&#27979;&#30340;&#30740;&#31350;&#33539;&#22260;&#24182;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.03190</link><description>&lt;p&gt;
&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unified Hallucination Detection for Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03190
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#26694;&#26550;UNIHD&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#35780;&#20272;&#22522;&#20934;&#26041;&#27861;MHaluBench&#26469;&#35780;&#20272;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#30340;&#36827;&#23637;&#12290;&#36825;&#39033;&#24037;&#20316;&#25193;&#23637;&#20102;&#24187;&#35273;&#26816;&#27979;&#30340;&#30740;&#31350;&#33539;&#22260;&#24182;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#22810;&#27169;&#24577;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(MLLMs)&#20173;&#28982;&#23384;&#22312;&#24187;&#35273;&#30340;&#20005;&#37325;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#21487;&#38752;&#22320;&#26816;&#27979;MLLMs&#20013;&#30340;&#24187;&#35273;&#24050;&#25104;&#20026;&#27169;&#22411;&#35780;&#20272;&#21644;&#23454;&#38469;&#24212;&#29992;&#37096;&#32626;&#20445;&#38556;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#20043;&#21069;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#21463;&#21040;&#20102;&#29421;&#31364;&#30340;&#20219;&#21153;&#28966;&#28857;&#12289;&#19981;&#36275;&#30340;&#24187;&#35273;&#31867;&#21035;&#28085;&#30422;&#33539;&#22260;&#20197;&#21450;&#32570;&#20047;&#35814;&#32454;&#30340;&#32454;&#31890;&#24230;&#30340;&#38480;&#21046;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25193;&#23637;&#20102;&#24187;&#35273;&#26816;&#27979;&#30340;&#30740;&#31350;&#33539;&#22260;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20803;&#35780;&#20272;&#22522;&#20934;&#26041;&#27861;&#65292;MHaluBench&#65292;&#31934;&#24515;&#35774;&#35745;&#20197;&#20419;&#36827;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#30340;&#36827;&#23637;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#26694;&#26550;&#65292;UNIHD&#65292;&#23427;&#21033;&#29992;&#19968;&#22871;&#36741;&#21161;&#24037;&#20855;&#26469;&#31283;&#20581;&#22320;&#39564;&#35777;&#24187;&#35273;&#30340;&#21457;&#29983;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;UNIHD&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite significant strides in multimodal tasks, Multimodal Large Language Models (MLLMs) are plagued by the critical issue of hallucination. The reliable detection of such hallucinations in MLLMs has, therefore, become a vital aspect of model evaluation and the safeguarding of practical application deployment. Prior research in this domain has been constrained by a narrow focus on singular tasks, an inadequate range of hallucination categories addressed, and a lack of detailed granularity. In response to these challenges, our work expands the investigative horizons of hallucination detection. We present a novel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate the evaluation of advancements in hallucination detection methods. Additionally, we unveil a novel unified multimodal hallucination detection framework, UNIHD, which leverages a suite of auxiliary tools to validate the occurrence of hallucinations robustly. We demonstrate the effectiveness of UNIHD throug
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#36873;&#25321;&#21512;&#36866;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#20462;&#27491;&#30340;&#32553;&#25918;&#23450;&#24459;&#21644;&#39044;&#23398;&#20064;&#25968;&#25454;&#22823;&#23567;&#30340;&#27010;&#24565;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#36873;&#25321;&#31639;&#27861;&#65292;&#21487;&#20197;&#36873;&#25321;&#25509;&#36817;&#26368;&#20248;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.02314</link><description>&lt;p&gt;
&#36890;&#36807;&#20462;&#27491;&#30340;&#32553;&#25918;&#23450;&#24459;&#36873;&#25321;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Selecting Large Language Model to Fine-tune via Rectified Scaling Law
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02314
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#36873;&#25321;&#21512;&#36866;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#20462;&#27491;&#30340;&#32553;&#25918;&#23450;&#24459;&#21644;&#39044;&#23398;&#20064;&#25968;&#25454;&#22823;&#23567;&#30340;&#27010;&#24565;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#36873;&#25321;&#31639;&#27861;&#65292;&#21487;&#20197;&#36873;&#25321;&#25509;&#36817;&#26368;&#20248;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26085;&#30410;&#22686;&#38271;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#24577;&#31995;&#32479;&#20013;&#65292;&#22312;&#20247;&#22810;&#36873;&#39033;&#20013;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#25104;&#20026;&#20102;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#24494;&#35843;&#25152;&#26377;&#27169;&#22411;&#28982;&#21518;&#20877;&#36827;&#34892;&#36873;&#25321;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#36164;&#28304;&#21463;&#38480;&#30340;&#36873;&#25321;&#20219;&#21153;&#36716;&#21270;&#20026;&#39044;&#27979;&#24494;&#35843;&#24615;&#33021;&#65292;&#24182;&#19988;&#23637;&#31034;&#20854;&#19982;&#32553;&#25918;&#23450;&#24459;&#20043;&#38388;&#30340;&#33258;&#28982;&#32852;&#31995;&#12290;&#19982;&#39044;&#35757;&#32451;&#19981;&#21516;&#65292;&#25105;&#20204;&#21457;&#29616;&#24494;&#35843;&#30340;&#32553;&#25918;&#26354;&#32447;&#19981;&#20165;&#21253;&#25324;&#20247;&#25152;&#21608;&#30693;&#30340;&#8220;&#21151;&#29575;&#38454;&#27573;&#8221;&#65292;&#36824;&#21253;&#25324;&#20197;&#21069;&#26410;&#34987;&#35266;&#23519;&#21040;&#30340;&#8220;&#39044;&#21151;&#29575;&#38454;&#27573;&#8221;&#12290;&#25105;&#20204;&#36824;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#29616;&#26377;&#30340;&#32553;&#25918;&#23450;&#24459;&#26080;&#27861;&#29702;&#35770;&#21644;&#23454;&#35777;&#22320;&#25429;&#25417;&#21040;&#36825;&#31181;&#30456;&#21464;&#29616;&#35937;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#8220;&#39044;&#23398;&#20064;&#25968;&#25454;&#22823;&#23567;&#8221;&#27010;&#24565;&#24341;&#20837;&#21040;&#25105;&#20204;&#30340;&#20462;&#27491;&#32553;&#25918;&#23450;&#24459;&#20013;&#65292;&#36825;&#20811;&#26381;&#20102;&#29702;&#35770;&#19978;&#30340;&#38480;&#21046;&#65292;&#24182;&#26356;&#22909;&#22320;&#36866;&#24212;&#23454;&#39564;&#32467;&#26524;&#12290;&#36890;&#36807;&#21033;&#29992;&#25105;&#20204;&#30340;&#23450;&#24459;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#35328;&#27169;&#22411;&#36873;&#25321;&#31639;&#27861;&#65292;&#21487;&#20197;&#36873;&#25321;&#25509;&#36817;&#26368;&#20248;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ever-growing ecosystem of LLMs has posed a challenge in selecting the most appropriate pre-trained model to fine-tune amidst a sea of options. Given constrained resources, fine-tuning all models and making selections afterward is unrealistic. In this work, we formulate this resource-constrained selection task into predicting fine-tuning performance and illustrate its natural connection with scaling laws. Unlike pre-training, We find that the fine-tuning scaling curve includes not just the well-known "power phase" but also the previously unobserved "pre-power phase". We also explain why existing scaling laws fail to capture this phase transition phenomenon both theoretically and empirically. To address this, we introduce the concept of "pre-learned data size" into our rectified scaling law, which overcomes theoretical limitations and fits experimental results much better. By leveraging our law, we propose a novel LLM selection algorithm that selects the near-optimal model with hundr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#30693;&#35782;&#32534;&#36753;&#23545;&#37051;&#36817;&#30693;&#35782;&#30340;&#25200;&#21160;&#65292;&#25552;&#20986;&#20102; additivity &#25351;&#26631;&#20197;&#21450; Perturbation Evaluation of Appending Knowledge (PEAK) &#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#38468;&#21152;&#26032;&#30693;&#35782;&#26102;&#37051;&#36817;&#30693;&#35782;&#30340;&#25200;&#21160;&#31243;&#24230;&#12290;</title><link>https://arxiv.org/abs/2401.17623</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#37051;&#36817;&#25200;&#21160;&#30340;&#30693;&#35782;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Neighboring Perturbations of Knowledge Editing on Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17623
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#30693;&#35782;&#32534;&#36753;&#23545;&#37051;&#36817;&#30693;&#35782;&#30340;&#25200;&#21160;&#65292;&#25552;&#20986;&#20102; additivity &#25351;&#26631;&#20197;&#21450; Perturbation Evaluation of Appending Knowledge (PEAK) &#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#38468;&#21152;&#26032;&#30693;&#35782;&#26102;&#37051;&#36817;&#30693;&#35782;&#30340;&#25200;&#21160;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#20294;&#30001;&#20110;&#38169;&#35823;&#25110;&#36807;&#26102;&#30340;&#30693;&#35782;&#65292;&#23427;&#20204;&#23481;&#26131;&#29983;&#25104;&#24847;&#22806;&#30340;&#25991;&#26412;&#12290;&#32771;&#34385;&#21040;&#37325;&#26032;&#35757;&#32451;LLMs&#30340;&#36164;&#28304;&#23494;&#38598;&#22411;&#24615;&#36136;&#65292;&#30693;&#35782;&#32534;&#36753;&#30340;&#21457;&#23637;&#21576;&#29616;&#20986;&#26174;&#33879;&#22686;&#38271;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#21644;&#35780;&#20272;&#24456;&#23569;&#25506;&#32034;&#32534;&#36753;&#23545;&#30456;&#37051;&#30693;&#35782;&#30340;&#25200;&#21160;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#26032;&#30693;&#35782;&#26356;&#26032;&#21040;LLMs&#20013;&#26159;&#21542;&#25200;&#20081;&#20102;&#20854;&#20013;&#21253;&#21547;&#30340;&#30456;&#37051;&#30693;&#35782;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#23558;&#26032;&#30340;&#31572;&#26696;&#38468;&#21152;&#21040;&#20107;&#23454;&#24615;&#38382;&#39064;&#30340;&#31572;&#26696;&#21015;&#34920;&#20013;&#26159;&#21542;&#20250;&#23548;&#33268;&#21407;&#22987;&#27491;&#30830;&#31572;&#26696;&#30340;&#20007;&#22833;&#65292;&#20197;&#21450;&#19981;&#32463;&#24847;&#22320;&#21253;&#21547;&#20102;&#38169;&#35823;&#31572;&#26696;&#12290;&#24341;&#20837;&#20102;&#19968;&#20010;&#21152;&#24615;&#25351;&#26631;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#34987;&#31216;&#20026;&#30693;&#35782;&#38468;&#21152;&#25200;&#21160;&#35780;&#20272;&#65288;PEAK&#65289;&#30340;&#22522;&#20934;&#26469;&#35780;&#20272;&#38468;&#21152;&#26032;&#30693;&#35782;&#26102;&#37051;&#36817;&#30693;&#35782;&#30340;&#25200;&#21160;&#31243;&#24230;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;&#26426;&#21046;&#65292;&#29992;&#20110;&#22788;&#29702;&#26032;&#22686;&#30693;&#35782;&#30340;&#25928;&#26524;&#21644;&#25200;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their exceptional capabilities, large language models (LLMs) are prone to generating unintended text due to false or outdated knowledge. Given the resource-intensive nature of retraining LLMs, there has been a notable increase in the development of knowledge editing. However, current approaches and evaluations rarely explore the perturbation of editing on neighboring knowledge. This paper studies whether updating new knowledge to LLMs perturbs the neighboring knowledge encapsulated within them. Specifically, we seek to figure out whether appending a new answer into an answer list to a factual question leads to catastrophic forgetting of original correct answers in this list, as well as unintentional inclusion of incorrect answers. A metric of additivity is introduced and a benchmark dubbed as Perturbation Evaluation of Appending Knowledge (PEAK) is constructed to evaluate the degree of perturbation to neighboring knowledge when appending new knowledge. Besides, a plug-and-play 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#22686;&#21152;&#23545;&#40784;&#24230;&#21644;&#20943;&#23569;&#26377;&#29992;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#25552;&#20379;&#36825;&#20004;&#20010;&#25968;&#37327;&#30340;&#36793;&#30028;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.16332</link><description>&lt;p&gt;
&#23545;&#40784;&#21644;&#26377;&#29992;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65306;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Tradeoffs Between Alignment and Helpfulness in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.16332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#22686;&#21152;&#23545;&#40784;&#24230;&#21644;&#20943;&#23569;&#26377;&#29992;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#25552;&#20379;&#36825;&#20004;&#20010;&#25968;&#37327;&#30340;&#36793;&#30028;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#24050;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#36890;&#36807;&#22686;&#24378;&#26399;&#26395;&#34892;&#20026;&#21644;&#25233;&#21046;&#38750;&#26399;&#26395;&#34892;&#20026;&#65292;&#23454;&#29616;&#20154;&#31867;&#19982;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#23433;&#20840;&#20132;&#20114;&#12290;&#36890;&#24120;&#36890;&#36807;&#35843;&#25972;&#27169;&#22411;&#25110;&#25554;&#20837;&#39044;&#35774;&#30340;&#23545;&#40784;&#25552;&#31034;&#26469;&#23454;&#29616;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#25913;&#21464;&#35757;&#32451;&#21518;&#30340;&#34920;&#31034;&#26469;&#25913;&#21464;&#27169;&#22411;&#34892;&#20026;&#30340;&#34920;&#31034;&#24037;&#31243;&#26041;&#27861;&#22312;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26377;&#25928;&#24615;&#12290;&#34920;&#31034;&#24037;&#31243;&#22312;&#38754;&#23545;&#23545;&#25239;&#25915;&#20987;&#21644;&#38477;&#20302;&#31038;&#20250;&#20559;&#35265;&#31561;&#23545;&#40784;&#23548;&#21521;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#22686;&#30410;&#65292;&#20294;&#20063;&#23548;&#33268;&#20102;&#27169;&#22411;&#25191;&#34892;&#22522;&#26412;&#20219;&#21153;&#33021;&#21147;&#30340;&#38477;&#20302;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22686;&#21152;&#23545;&#40784;&#24230;&#21644;&#20943;&#23569;&#27169;&#22411;&#26377;&#29992;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#25552;&#20379;&#36825;&#20004;&#20010;&#25968;&#37327;&#30340;&#36793;&#30028;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#30456;&#20851;&#24615;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#27169;&#22411;&#30340;&#26377;&#29992;&#24615;&#36890;&#24120;&#20250;&#20943;&#23569;
&lt;/p&gt;
&lt;p&gt;
Language model alignment has become an important component of AI safety, allowing safe interactions between humans and language models, by enhancing desired behaviors and inhibiting undesired ones. It is often done by tuning the model or inserting preset aligning prompts. Recently, representation engineering, a method which alters the model's behavior via changing its representations post-training, was shown to be effective in aligning LLMs (Zou et al., 2023a). Representation engineering yields gains in alignment oriented tasks such as resistance to adversarial attacks and reduction of social biases, but was also shown to cause a decrease in the ability of the model to perform basic tasks. In this paper we study the tradeoff between the increase in alignment and decrease in helpfulness of the model. We propose a theoretical framework which provides bounds for these two quantities, and demonstrate their relevance empirically. Interestingly, we find that while the helpfulness generally d
&lt;/p&gt;</description></item><item><title>SQLformer&#26159;&#19968;&#20010;&#29992;&#20110;&#25991;&#26412;&#21040;SQL&#32763;&#35793;&#30340;&#28145;&#24230;&#33258;&#22238;&#24402;&#26597;&#35810;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#29305;&#23450;&#30340;Transformer&#26550;&#26500;&#65292;&#24182;&#36890;&#36807;&#32467;&#26500;&#24402;&#32435;&#20559;&#24046;&#35299;&#20915;&#39046;&#22495;&#27867;&#21270;&#21644;&#33258;&#28982;&#35821;&#35328;&#19982;SQL&#26597;&#35810;&#23545;&#40784;&#30340;&#38590;&#39064;&#12290;</title><link>https://arxiv.org/abs/2310.18376</link><description>&lt;p&gt;
SQLformer&#65306;&#28145;&#24230;&#33258;&#22238;&#24402;&#26597;&#35810;&#22270;&#29983;&#25104;&#29992;&#20110;&#25991;&#26412;&#21040;SQL&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
SQLformer: Deep Auto-Regressive Query Graph Generation for Text-to-SQL Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.18376
&lt;/p&gt;
&lt;p&gt;
SQLformer&#26159;&#19968;&#20010;&#29992;&#20110;&#25991;&#26412;&#21040;SQL&#32763;&#35793;&#30340;&#28145;&#24230;&#33258;&#22238;&#24402;&#26597;&#35810;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#29305;&#23450;&#30340;Transformer&#26550;&#26500;&#65292;&#24182;&#36890;&#36807;&#32467;&#26500;&#24402;&#32435;&#20559;&#24046;&#35299;&#20915;&#39046;&#22495;&#27867;&#21270;&#21644;&#33258;&#28982;&#35821;&#35328;&#19982;SQL&#26597;&#35810;&#23545;&#40784;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;&#25991;&#26412;&#21040;SQL&#32763;&#35793;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#38271;&#65292;&#36825;&#26159;&#23558;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#36716;&#21270;&#20026;&#21487;&#25191;&#34892;SQL&#26597;&#35810;&#30340;&#20219;&#21153;&#12290;&#36825;&#39033;&#25216;&#26415;&#20855;&#26377;&#28508;&#22312;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#20351;&#25968;&#25454;&#24211;&#20013;&#30340;&#25968;&#25454;&#25552;&#21462;&#27665;&#20027;&#21270;&#12290;&#28982;&#32780;&#65292;&#20854;&#20013;&#19968;&#20123;&#20027;&#35201;&#38556;&#30861;&#21253;&#25324;&#39046;&#22495;&#27867;&#21270;&#65292;&#21363;&#36866;&#24212;&#20197;&#21069;&#26410;&#35265;&#21040;&#30340;&#25968;&#25454;&#24211;&#65292;&#24182;&#19988;&#23558;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#19982;&#30456;&#24212;&#30340;SQL&#26597;&#35810;&#23545;&#40784;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SQLformer&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#25191;&#34892;&#25991;&#26412;&#21040;SQL&#32763;&#35793;&#20219;&#21153;&#32780;&#35774;&#35745;&#30340;&#26032;&#22411;Transformer&#20307;&#31995;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20197;&#33258;&#22238;&#24402;&#30340;&#26041;&#24335;&#39044;&#27979;SQL&#26597;&#35810;&#65292;&#24182;&#22312;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#23618;&#20013;&#32467;&#21512;&#32467;&#26500;&#24402;&#32435;&#20559;&#24046;&#12290;&#36825;&#31181;&#20559;&#24046;&#26159;&#30001;&#25968;&#25454;&#24211;&#34920;&#21644;&#21015;&#36873;&#25321;&#24341;&#23548;&#30340;&#65292;&#26377;&#21161;&#20110;&#35299;&#30721;&#22120;&#20197;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#30340;&#35268;&#33539;&#39034;&#24207;&#29983;&#25104;SQL&#26597;&#35810;&#30340;&#22270;&#24418;&#34920;&#31034;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#35828;&#26126;&#20102;&#29616;&#38454;&#27573;&#30340;&#25216;&#26415;&#27700;&#24179;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been growing interest in text-to-SQL translation, which is the task of converting natural language questions into executable SQL queries. This technology is important for its potential to democratize data extraction from databases. However, some of its key hurdles include domain generalisation, which is the ability to adapt to previously unseen databases, and alignment of natural language questions with the corresponding SQL queries. To overcome these challenges, we introduce SQLformer, a novel Transformer architecture specifically crafted to perform text-to-SQL translation tasks. Our model predicts SQL queries as abstract syntax trees (ASTs) in an autoregressive way, incorporating structural inductive bias in the encoder and decoder layers. This bias, guided by database table and column selection, aids the decoder in generating SQL query ASTs represented as graphs in a Breadth-First Search canonical order. Comprehensive experiments illustrate the state-of-th
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23454;&#36341;&#23454;&#39564;&#21644;&#29702;&#35770;&#27934;&#23519;&#65292;&#25506;&#31350;&#24403;&#20195;NLP&#31995;&#32479;&#20043;&#38388;&#30340;&#21327;&#20316;&#26426;&#21046;&#65292;&#21457;&#29616;&#26576;&#20123;&#21327;&#20316;&#31574;&#30053;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#20248;&#21270;&#20102;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2310.02124</link><description>&lt;p&gt;
&#25506;&#32034;LLM&#20195;&#29702;&#30340;&#21327;&#20316;&#26426;&#21046;&#65306;&#31038;&#20250;&#24515;&#29702;&#23398;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.02124
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23454;&#36341;&#23454;&#39564;&#21644;&#29702;&#35770;&#27934;&#23519;&#65292;&#25506;&#31350;&#24403;&#20195;NLP&#31995;&#32479;&#20043;&#38388;&#30340;&#21327;&#20316;&#26426;&#21046;&#65292;&#21457;&#29616;&#26576;&#20123;&#21327;&#20316;&#31574;&#30053;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#20248;&#21270;&#20102;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31995;&#32479;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#22797;&#26434;&#30340;&#31038;&#20250;&#29615;&#22659;&#20013;&#65292;&#19968;&#20010;&#36843;&#20999;&#30340;&#38382;&#39064;&#20986;&#29616;&#20102;&#65306;&#36825;&#20123;NLP&#31995;&#32479;&#33021;&#21542;&#27169;&#20223;&#31867;&#20154;&#31867;&#30340;&#21327;&#20316;&#26234;&#33021;&#65292;&#22312;&#30001;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#32452;&#25104;&#30340;&#22810;&#20195;&#29702;&#31038;&#20250;&#20013;&#65311;&#26412;&#25991;&#36890;&#36807;&#23558;&#23454;&#36341;&#23454;&#39564;&#19982;&#29702;&#35770;&#35266;&#28857;&#30456;&#32467;&#21512;&#65292;&#25506;&#31350;&#24403;&#20195;NLP&#31995;&#32479;&#20043;&#38388;&#30340;&#21327;&#20316;&#26426;&#21046;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#22235;&#20010;&#30001;LLM&#20195;&#29702;&#32452;&#25104;&#30340;&#29420;&#29305;&#8220;&#31038;&#20250;&#8221;&#65292;&#27599;&#20010;&#20195;&#29702;&#20197;&#29305;&#23450;&#30340;&#8220;&#29305;&#36136;&#8221;&#65288;&#38543;&#21644;&#25110;&#36807;&#20110;&#33258;&#20449;&#65289;&#20026;&#29305;&#24449;&#65292;&#24182;&#19982;&#19981;&#21516;&#30340;&#8220;&#24605;&#32500;&#27169;&#24335;&#8221;&#65288;&#36777;&#35770;&#25110;&#21453;&#24605;&#65289;&#23637;&#24320;&#21327;&#20316;&#12290;&#36890;&#36807;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#36825;&#20123;&#22810;&#20195;&#29702;&#31038;&#20250;&#65292;&#25105;&#20204;&#21457;&#29616;&#26576;&#20123;&#21327;&#20316;&#31574;&#30053;&#19981;&#20165;&#32988;&#36807;&#20808;&#21069;&#39030;&#23574;&#26041;&#27861;&#65292;&#32780;&#19988;&#20248;&#21270;&#20102;&#25928;&#29575;&#65288;&#20351;&#29992;&#26356;&#23569;&#30340;API&#20196;&#29260;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#36827;&#19968;&#27493;&#35828;&#26126;LLM&#20195;&#29702;&#21487;&#20197;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.02124v2 Announce Type: replace-cross  Abstract: As Natural Language Processing (NLP) systems are increasingly employed in intricate social environments, a pressing query emerges: Can these NLP systems mirror human-esque collaborative intelligence, in a multi-agent society consisting of multiple large language models (LLMs)? This paper probes the collaboration mechanisms among contemporary NLP systems by melding practical experiments with theoretical insights. We fabricate four unique `societies' comprised of LLM agents, where each agent is characterized by a specific `trait' (easy-going or overconfident) and engages in collaboration with a distinct `thinking pattern' (debate or reflection). Through evaluating these multi-agent societies on three benchmark datasets, we discern that certain collaborative strategies not only outshine previous top-tier approaches, but also optimize efficiency (using fewer API tokens). Moreover, our results further illustrate that LLM agents mani
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22686;&#21152;&#21442;&#32771;&#25991;&#29486;&#30340;&#22810;&#26679;&#24615;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;Div-Ref &#26174;&#33879;&#25552;&#39640;&#20102;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#35780;&#20272;&#30340;&#30456;&#20851;&#24615;</title><link>https://arxiv.org/abs/2305.15067</link><description>&lt;p&gt;
&#24182;&#38750;&#25152;&#26377;&#35780;&#20272;&#25351;&#26631;&#37117;&#24212;&#21463;&#21040;&#25351;&#36131;&#65306;&#36890;&#36807;&#22810;&#26679;&#21270;&#21442;&#32771;&#25991;&#29486;&#25913;&#36827;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Not All Metrics Are Guilty: Improving NLG Evaluation by Diversifying References
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.15067
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22686;&#21152;&#21442;&#32771;&#25991;&#29486;&#30340;&#22810;&#26679;&#24615;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;Div-Ref &#26174;&#33879;&#25552;&#39640;&#20102;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#35780;&#20272;&#30340;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#20851;&#20110;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#30340;&#30740;&#31350;&#20381;&#36182;&#20110;&#20855;&#26377;&#26377;&#38480;&#21442;&#32771;&#25991;&#29486;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#36739;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;Div-Ref&#65292;&#36890;&#36807;&#20016;&#23500;&#21442;&#32771;&#25991;&#29486;&#30340;&#25968;&#37327;&#26469;&#22686;&#24378;&#29616;&#26377;&#30340;&#35780;&#20272;&#22522;&#20934;&#12290;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23558;&#21333;&#20010;&#21442;&#32771;&#25991;&#29486;&#30340;&#34920;&#36798;&#22810;&#26679;&#21270;&#20026;&#22810;&#20010;&#39640;&#36136;&#37327;&#30340;&#34920;&#36798;&#65292;&#20197;&#23613;&#21487;&#33021;&#35206;&#30422;&#21442;&#32771;&#21477;&#30340;&#35821;&#20041;&#31354;&#38388;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#20174;&#32463;&#39564;&#19978;&#35777;&#26126;&#22810;&#26679;&#21270;&#21442;&#32771;&#25991;&#29486;&#30340;&#34920;&#36798;&#21487;&#20197;&#26174;&#33879;&#22686;&#24378;&#33258;&#21160;&#35780;&#20272;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.15067v2 Announce Type: replace  Abstract: Most research about natural language generation (NLG) relies on evaluation benchmarks with limited references for a sample, which may result in poor correlations with human judgements. The underlying reason is that one semantic meaning can actually be expressed in different forms, and the evaluation with a single or few references may not accurately reflect the quality of the model's hypotheses. To address this issue, this paper presents a simple and effective method, named Div-Ref, to enhance existing evaluation benchmarks by enriching the number of references. We leverage large language models (LLMs) to diversify the expression of a single reference into multiple high-quality ones to cover the semantic space of the reference sentence as much as possible. We conduct comprehensive experiments to empirically demonstrate that diversifying the expression of reference can significantly enhance the correlation between automatic evaluation
&lt;/p&gt;</description></item><item><title>Taiyi-Diffusion-XL&#26159;&#19968;&#20010;&#20013;&#33521;&#21452;&#35821;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;CLIP&#21644;Stable-Diffusion-XL&#30340;&#33021;&#21147;&#36827;&#34892;&#25193;&#23637;&#65292;&#24182;&#36890;&#36807;&#21452;&#35821;&#36830;&#32493;&#39044;&#35757;&#32451;&#26469;&#23454;&#29616;&#12290;&#27169;&#22411;&#36890;&#36807;&#23558;&#24120;&#29992;&#30340;&#27721;&#23383;&#25972;&#21512;&#21040;CLIP&#30340;&#20998;&#35789;&#22120;&#21644;&#23884;&#20837;&#23618;&#20013;&#65292;&#20197;&#21450;&#20351;&#29992;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20016;&#23500;&#25991;&#26412;&#25552;&#31034;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#22270;&#20687;&#26631;&#39064;&#21644;&#39640;&#36136;&#37327;&#30340;&#35270;&#35273;&#25928;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#21452;&#35821;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2401.14688</link><description>&lt;p&gt;
Taiyi-Diffusion-XL: &#20511;&#21161;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#25903;&#25345;&#25512;&#36827;&#21452;&#35821;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Taiyi-Diffusion-XL: Advancing Bilingual Text-to-Image Generation with Large Vision-Language Model Support. (arXiv:2401.14688v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14688
&lt;/p&gt;
&lt;p&gt;
Taiyi-Diffusion-XL&#26159;&#19968;&#20010;&#20013;&#33521;&#21452;&#35821;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;CLIP&#21644;Stable-Diffusion-XL&#30340;&#33021;&#21147;&#36827;&#34892;&#25193;&#23637;&#65292;&#24182;&#36890;&#36807;&#21452;&#35821;&#36830;&#32493;&#39044;&#35757;&#32451;&#26469;&#23454;&#29616;&#12290;&#27169;&#22411;&#36890;&#36807;&#23558;&#24120;&#29992;&#30340;&#27721;&#23383;&#25972;&#21512;&#21040;CLIP&#30340;&#20998;&#35789;&#22120;&#21644;&#23884;&#20837;&#23618;&#20013;&#65292;&#20197;&#21450;&#20351;&#29992;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20016;&#23500;&#25991;&#26412;&#25552;&#31034;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#22270;&#20687;&#26631;&#39064;&#21644;&#39640;&#36136;&#37327;&#30340;&#35270;&#35273;&#25928;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#21452;&#35821;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#30340;&#36827;&#23637;&#26174;&#33879;&#25552;&#21319;&#20102;&#22270;&#20687;&#29983;&#25104;&#33021;&#21147;&#65292;&#28982;&#32780;&#22312;&#21452;&#35821;&#25110;&#20013;&#25991;&#35821;&#35328;&#25903;&#25345;&#26041;&#38754;&#20173;&#23384;&#22312;&#26126;&#26174;&#30340;&#24320;&#28304;&#27169;&#22411;&#32570;&#21475;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Taiyi-Diffusion-XL&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#20013;&#33521;&#21452;&#35821;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;CLIP&#21644;Stable-Diffusion-XL&#33021;&#21147;&#30340;&#25193;&#23637;&#65292;&#24182;&#36890;&#36807;&#21452;&#35821;&#36830;&#32493;&#39044;&#35757;&#32451;&#30340;&#36807;&#31243;&#26469;&#24320;&#21457;&#12290;&#36825;&#31181;&#26041;&#27861;&#21253;&#25324;&#36890;&#36807;&#23558;&#26368;&#24120;&#29992;&#30340;&#27721;&#23383;&#25972;&#21512;&#21040;CLIP&#30340;&#20998;&#35789;&#22120;&#21644;&#23884;&#20837;&#23618;&#20013;&#26469;&#25193;&#23637;&#35789;&#27719;&#37327;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#21516;&#26102;&#36824;&#21152;&#20837;&#20102;&#32477;&#23545;&#20301;&#32622;&#32534;&#30721;&#25193;&#23637;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#26469;&#20016;&#23500;&#25991;&#26412;&#25552;&#31034;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#22909;&#30340;&#22270;&#20687;&#26631;&#39064;&#21644;&#26356;&#39640;&#30340;&#35270;&#35273;&#36136;&#37327;&#12290;&#36825;&#20123;&#22686;&#24378;&#25514;&#26045;&#38543;&#21518;&#24212;&#29992;&#20110;&#19979;&#28216;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#24320;&#21457;&#30340;CLIP&#27169;&#22411;&#22312;&#21452;&#35821;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#27492;&#22806;&#65292;&#21452;&#35821;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#25972;&#21512;&#20351;&#35813;&#27169;&#22411;&#22312;&#20013;&#33521;&#25991;&#22330;&#26223;&#19979;&#20855;&#26377;&#22343;&#34913;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in text-to-image models have significantly enhanced image generation capabilities, yet a notable gap of open-source models persists in bilingual or Chinese language support. To address this need, we present Taiyi-Diffusion-XL, a new Chinese and English bilingual text-to-image model which is developed by extending the capabilities of CLIP and Stable-Diffusion-XL through a process of bilingual continuous pre-training. This approach includes the efficient expansion of vocabulary by integrating the most frequently used Chinese characters into CLIP's tokenizer and embedding layers, coupled with an absolute position encoding expansion. Additionally, we enrich text prompts by large vision-language model, leading to better images captions and possess higher visual quality. These enhancements are subsequently applied to downstream text-to-image models. Our empirical results indicate that the developed CLIP model excels in bilingual image-text retrieval.Furthermore, the bilin
&lt;/p&gt;</description></item><item><title>AUTOACT&#26159;&#19968;&#20010;&#33258;&#21160;&#20195;&#29702;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#20027;&#35268;&#21010;&#21512;&#25104;&#36712;&#36857;&#65292;&#19981;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#21644;&#38381;&#28304;&#27169;&#22411;&#65292;&#33021;&#22815;&#23454;&#29616;&#26356;&#22909;&#25110;&#31867;&#20284;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.05268</link><description>&lt;p&gt;
AUTOACT&#65306;&#36890;&#36807;&#33258;&#20027;&#35268;&#21010;&#23454;&#29616;&#30340;&#33258;&#21160;&#20195;&#29702;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
AUTOACT: Automatic Agent Learning from Scratch via Self-Planning. (arXiv:2401.05268v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05268
&lt;/p&gt;
&lt;p&gt;
AUTOACT&#26159;&#19968;&#20010;&#33258;&#21160;&#20195;&#29702;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#20027;&#35268;&#21010;&#21512;&#25104;&#36712;&#36857;&#65292;&#19981;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#21644;&#38381;&#28304;&#27169;&#22411;&#65292;&#33021;&#22815;&#23454;&#29616;&#26356;&#22909;&#25110;&#31867;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#20195;&#29702;&#22312;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#39046;&#22495;&#36827;&#34892;&#20102;&#19981;&#26029;&#30340;&#25506;&#32034;&#65292;&#20294;&#29616;&#26377;&#30340;&#35821;&#35328;&#20195;&#29702;&#31995;&#32479;&#20173;&#28982;&#38754;&#20020;&#26114;&#36149;&#12289;&#19981;&#21487;&#37325;&#22797;&#30340;&#25968;&#25454;&#20381;&#36182;&#38382;&#39064;&#65292;&#24182;&#19988;&#38754;&#20020;&#23558;&#21333;&#19968;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#20010;&#21151;&#33021;&#30340;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AutoAct&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#21160;&#20195;&#29702;&#23398;&#20064;&#26694;&#26550;&#65292;&#19981;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#24102;&#27880;&#37322;&#30340;&#25968;&#25454;&#21644;&#26469;&#33258;&#38381;&#28304;&#27169;&#22411;&#65288;&#22914;GPT-4&#65289;&#30340;&#21512;&#25104;&#36712;&#36857;&#12290;&#32473;&#23450;&#26377;&#38480;&#30340;&#25968;&#25454;&#21644;&#24037;&#20855;&#24211;&#65292;AutoAct&#39318;&#20808;&#33258;&#21160;&#21512;&#25104;&#35268;&#21010;&#36712;&#36857;&#65292;&#19981;&#38656;&#35201;&#20154;&#31867;&#25110;&#24378;&#38381;&#28304;&#27169;&#22411;&#30340;&#20219;&#20309;&#36741;&#21161;&#12290;&#28982;&#21518;&#65292;AutoAct&#21033;&#29992;&#20998;&#24037;&#31574;&#30053;&#65292;&#26681;&#25454;&#30446;&#26631;&#20219;&#21153;&#20449;&#24687;&#21644;&#21512;&#25104;&#36712;&#36857;&#33258;&#21160;&#21306;&#20998;&#65292;&#20135;&#29983;&#19968;&#20010;&#23376;&#20195;&#29702;&#32452;&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#31181;LLMs&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;AutoAct&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#25110;&#19982;&#20854;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language agents have achieved considerable performance on various complex tasks. Despite the incessant exploration in this field, existing language agent systems still struggle with costly, non-reproducible data reliance and face the challenge of compelling a single model for multiple functions. To this end, we introduce AutoAct, an automatic agent learning framework that does not rely on large-scale annotated data and synthetic trajectories from closed-source models (e.g., GPT-4). Given limited data with a tool library, AutoAct first automatically synthesizes planning trajectories without any assistance from humans or strong closed-source models. Then, AutoAct leverages a division-of-labor strategy to automatically differentiate based on the target task information and synthesized trajectories, producing a sub-agent group to complete the task. We conduct comprehensive experiments with different LLMs, which demonstrates that AutoAct yields better or parallel performance compared to var
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#39640;&#25928;&#30340;&#26816;&#32034;&#34920;&#31034;&#34701;&#21512;&#26041;&#27861;ReFusion&#65292;&#36890;&#36807;&#23558;&#26816;&#32034;&#34920;&#31034;&#30452;&#25509;&#34701;&#21512;&#21040;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#35299;&#20915;&#20102;&#23558;&#22806;&#37096;&#25968;&#25454;&#24211;&#30340;&#30693;&#35782;&#34701;&#20837;&#38750;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.02993</link><description>&lt;p&gt;
&#20351;&#29992;&#35745;&#31639;&#39640;&#25928;&#30340;&#26816;&#32034;&#34920;&#31034;&#34701;&#21512;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Natural Language Understanding with Computation-Efficient Retrieval Representation Fusion. (arXiv:2401.02993v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02993
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#39640;&#25928;&#30340;&#26816;&#32034;&#34920;&#31034;&#34701;&#21512;&#26041;&#27861;ReFusion&#65292;&#36890;&#36807;&#23558;&#26816;&#32034;&#34920;&#31034;&#30452;&#25509;&#34701;&#21512;&#21040;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#35299;&#20915;&#20102;&#23558;&#22806;&#37096;&#25968;&#25454;&#24211;&#30340;&#30693;&#35782;&#34701;&#20837;&#38750;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#22806;&#37096;&#25968;&#25454;&#24211;&#20013;&#33719;&#21462;&#30693;&#35782;&#24182;&#34701;&#20837;&#35821;&#35328;&#27169;&#22411;&#30340;&#26816;&#32034;&#22686;&#24378;&#26041;&#27861;&#22312;&#21508;&#31181;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#65288;&#20363;&#22914;&#38382;&#31572;&#21644;&#25991;&#26412;&#29983;&#25104;&#65289;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#38750;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#65288;&#20363;&#22914;&#25991;&#26412;&#20998;&#31867;&#65289;&#20013;&#38598;&#25104;&#26816;&#32034;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#23558;&#26816;&#32034;&#20869;&#23481;&#25340;&#25509;&#21040;&#36755;&#20837;&#20013;&#24418;&#25104;&#25552;&#31034;&#24615;&#30340;&#36755;&#20837;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#35201;&#27714;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#22788;&#29702;&#38271;&#25991;&#26412;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25512;&#26029;&#36825;&#31181;&#25340;&#25509;&#25968;&#25454;&#20063;&#20250;&#28040;&#32791;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#39640;&#25928;&#30340;&#26816;&#32034;&#34920;&#31034;&#34701;&#21512;&#26041;&#27861;ReFusion&#65292;&#37319;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#12290;&#20027;&#35201;&#24605;&#24819;&#26159;&#30452;&#25509;&#23558;&#26816;&#32034;&#34920;&#31034;&#19982;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#34701;&#21512;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#26816;&#32034;&#27169;&#22359;&#65292;&#37325;&#26032;&#26816;&#32034;...
&lt;/p&gt;
&lt;p&gt;
Retrieval-based augmentations that aim to incorporate knowledge from an external database into language models have achieved great success in various knowledge-intensive (KI) tasks, such as question-answering and text generation. However, integrating retrievals in non-knowledge-intensive (NKI) tasks, such as text classification, is still challenging. Existing works focus on concatenating retrievals to inputs as context to form the prompt-based inputs. Unfortunately, such methods require language models to have the capability to handle long texts. Besides, inferring such concatenated data would also consume a significant amount of computational resources.  To solve these challenges, we propose \textbf{ReFusion} in this paper, a computation-efficient \textbf{Re}trieval representation \textbf{Fusion} with neural architecture search. The main idea is to directly fuse the retrieval representations into the language models. Specifically, we first propose an online retrieval module that retri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#33258;&#34917;&#20195;&#30721;&#29983;&#25104;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#34917;&#26426;&#21046;&#23454;&#29616;&#20102;&#20013;&#26029;&#21644;&#24490;&#29615;&#26426;&#21046;&#65292;&#20351;&#20256;&#32479;&#35299;&#30721;&#36827;&#31243;&#21464;&#24471;&#38750;&#21333;&#35843;&#12290;&#21033;&#29992;&#20013;&#26029;&#26426;&#21046;&#21487;&#20197;&#25512;&#36831;&#29983;&#25104;&#20195;&#30721;&#65292;&#22686;&#24378;&#23545;&#36755;&#20986;&#30340;&#25511;&#21046;&#65307;&#21033;&#29992;&#24490;&#29615;&#26426;&#21046;&#21487;&#20197;&#24490;&#29615;&#26356;&#26032;&#21644;&#21516;&#27493;&#29983;&#25104;&#30340;&#27599;&#20010;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2311.17972</link><description>&lt;p&gt;
&#33258;&#34917;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Self-Infilling Code Generation. (arXiv:2311.17972v2 [cs.PL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.17972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#33258;&#34917;&#20195;&#30721;&#29983;&#25104;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#34917;&#26426;&#21046;&#23454;&#29616;&#20102;&#20013;&#26029;&#21644;&#24490;&#29615;&#26426;&#21046;&#65292;&#20351;&#20256;&#32479;&#35299;&#30721;&#36827;&#31243;&#21464;&#24471;&#38750;&#21333;&#35843;&#12290;&#21033;&#29992;&#20013;&#26029;&#26426;&#21046;&#21487;&#20197;&#25512;&#36831;&#29983;&#25104;&#20195;&#30721;&#65292;&#22686;&#24378;&#23545;&#36755;&#20986;&#30340;&#25511;&#21046;&#65307;&#21033;&#29992;&#24490;&#29615;&#26426;&#21046;&#21487;&#20197;&#24490;&#29615;&#26356;&#26032;&#21644;&#21516;&#27493;&#29983;&#25104;&#30340;&#27599;&#20010;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#34917;&#20195;&#30721;&#29983;&#25104;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#23427;&#23558;&#34917;&#20805;&#25805;&#20316;&#34701;&#20837;&#33258;&#22238;&#24402;&#35299;&#30721;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#26368;&#36817;&#30340;&#33021;&#22815;&#36827;&#34892;&#22635;&#20805;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#33258;&#21160;&#36827;&#34892;&#22635;&#20805;&#30340;&#35266;&#23519;&#32467;&#26524;&#65306;&#34917;&#20805;&#25805;&#20316;&#26088;&#22312;&#26681;&#25454;&#39044;&#23450;&#20041;&#30340;&#21069;&#32512;&#21644;&#21518;&#32512;&#22635;&#20805;&#20013;&#38388;&#20869;&#23481;&#65292;&#32780;&#33258;&#34917;&#26426;&#21046;&#39034;&#24207;&#29983;&#25104;&#36825;&#20123;&#21608;&#22260;&#19978;&#19979;&#25991;&#21644;&#34987;&#22635;&#20805;&#20869;&#23481;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#33021;&#21147;&#22312;&#20256;&#32479;&#35299;&#30721;&#20013;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#20013;&#26029;&#21644;&#24490;&#29615;&#26426;&#21046;&#65292;&#20351;&#20854;&#36827;&#21270;&#20026;&#38750;&#21333;&#35843;&#36807;&#31243;&#12290;&#20013;&#26029;&#26426;&#21046;&#20801;&#35768;&#25512;&#36831;&#29983;&#25104;&#29305;&#23450;&#30340;&#20195;&#30721;&#65292;&#30452;&#21040;&#30830;&#23450;&#30340;&#21518;&#32512;&#24314;&#31435;&#65292;&#22686;&#24378;&#23545;&#36755;&#20986;&#30340;&#25511;&#21046;&#12290;&#21516;&#26102;&#65292;&#24490;&#29615;&#26426;&#21046;&#21033;&#29992;&#33258;&#34917;&#21644;&#20174;&#24038;&#21040;&#21491;&#35299;&#30721;&#30340;&#20114;&#34917;&#24615;&#65292;&#21487;&#20197;&#24490;&#29615;&#26356;&#26032;&#21644;&#21516;&#27493;&#27599;&#20010;&#29983;&#25104;&#37096;&#20998;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#26469;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces self-infilling code generation, a general framework that incorporates infilling operations into auto-regressive decoding. Our approach capitalizes on the observation that recent infilling-capable code language models can self-infill: whereas infilling operations aim to fill in the middle based on a predefined prefix and suffix, self-infilling sequentially generates both such surrounding context and the infilled content. We utilize this capability to introduce novel interruption and looping mechanisms in conventional decoding, evolving it into a non-monotonic process. Interruptions allow for postponing the generation of specific code until a definitive suffix is established, enhancing control over the output. Meanwhile, the looping mechanism, which leverages the complementary nature of self-infilling and left-to-right decoding, can iteratively update and synchronize each piece of generation cyclically. Extensive experiments are conducted to demonstrate that our prop
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27010;&#24565;&#25552;&#31034;&#23398;&#20064;&#65288;MCPL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#26102;&#23398;&#20064;&#22810;&#20010;&#26032;&#30340;&#8220;&#35789;&#8221;&#26469;&#35299;&#20915;&#22312;&#21333;&#20010;&#22330;&#26223;&#20013;&#35782;&#21035;&#21644;&#25972;&#21512;&#22810;&#20010;&#23545;&#35937;&#32423;&#27010;&#24565;&#30340;&#25361;&#25112;&#12290;&#38024;&#23545;&#35789;&#27010;&#24565;&#30456;&#20851;&#24615;&#20934;&#30830;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#27880;&#24847;&#21147;&#25513;&#30721;&#12289;&#25552;&#31034;&#23545;&#27604;&#25439;&#22833;&#21644;&#32465;&#23450;&#24418;&#23481;&#35789;&#31561;&#19977;&#31181;&#27491;&#21017;&#21270;&#25216;&#26415;&#12290;&#36890;&#36807;&#22270;&#20687;&#29983;&#25104;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26694;&#26550;&#33021;&#22815;&#29983;&#25104;&#26356;&#22810;&#26679;&#21270;&#21644;&#21512;&#25104;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2310.12274</link><description>&lt;p&gt;
&#19968;&#22270;&#25269;&#21315;&#35328;&#65306;&#20351;&#29992;&#22810;&#27010;&#24565;&#25552;&#31034;&#23398;&#20064;&#26469;&#23398;&#20064;&#23545;&#35937;&#32423;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
An Image is Worth Multiple Words: Learning Object Level Concepts using Multi-Concept Prompt Learning. (arXiv:2310.12274v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12274
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27010;&#24565;&#25552;&#31034;&#23398;&#20064;&#65288;MCPL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#26102;&#23398;&#20064;&#22810;&#20010;&#26032;&#30340;&#8220;&#35789;&#8221;&#26469;&#35299;&#20915;&#22312;&#21333;&#20010;&#22330;&#26223;&#20013;&#35782;&#21035;&#21644;&#25972;&#21512;&#22810;&#20010;&#23545;&#35937;&#32423;&#27010;&#24565;&#30340;&#25361;&#25112;&#12290;&#38024;&#23545;&#35789;&#27010;&#24565;&#30456;&#20851;&#24615;&#20934;&#30830;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#27880;&#24847;&#21147;&#25513;&#30721;&#12289;&#25552;&#31034;&#23545;&#27604;&#25439;&#22833;&#21644;&#32465;&#23450;&#24418;&#23481;&#35789;&#31561;&#19977;&#31181;&#27491;&#21017;&#21270;&#25216;&#26415;&#12290;&#36890;&#36807;&#22270;&#20687;&#29983;&#25104;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26694;&#26550;&#33021;&#22815;&#29983;&#25104;&#26356;&#22810;&#26679;&#21270;&#21644;&#21512;&#25104;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#23383;&#21453;&#36716;&#26159;&#19968;&#31181;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#23398;&#20064;&#19968;&#31181;&#26032;&#30340;&#8220;&#21333;&#35789;&#8221;&#30340;&#23884;&#20837;&#34920;&#31034;&#22270;&#20687;&#39118;&#26684;&#21644;&#22806;&#35266;&#65292;&#20351;&#20854;&#33021;&#22815;&#25972;&#21512;&#21040;&#33258;&#28982;&#35821;&#35328;&#21477;&#23376;&#20013;&#29983;&#25104;&#26032;&#30340;&#21512;&#25104;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#23545;&#20110;&#21487;&#33719;&#24471;&#20010;&#21035;&#27010;&#24565;&#30340;&#23884;&#20837;&#65292;&#35782;&#21035;&#21644;&#25972;&#21512;&#19968;&#20010;&#22330;&#26223;&#20013;&#30340;&#22810;&#20010;&#23545;&#35937;&#32423;&#27010;&#24565;&#20173;&#28982;&#38754;&#20020;&#30528;&#26174;&#33879;&#30340;&#25361;&#25112;&#65292;&#36825;&#20063;&#24471;&#21040;&#20102;&#25105;&#20204;&#30340;&#23454;&#35777;&#27979;&#35797;&#30340;&#36827;&#19968;&#27493;&#35777;&#23454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#27010;&#24565;&#25552;&#31034;&#23398;&#20064;&#65288;MCPL&#65289;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#19968;&#20010;&#21477;&#23376;-&#22270;&#20687;&#23545;&#20013;&#21516;&#26102;&#23398;&#20064;&#22810;&#20010;&#26032;&#30340;&#8220;&#35789;&#8221;&#12290;&#20026;&#20102;&#22686;&#24378;&#35789;&#27010;&#24565;&#30456;&#20851;&#24615;&#30340;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#27491;&#21017;&#21270;&#25216;&#26415;&#65306;&#27880;&#24847;&#21147;&#25513;&#30721;&#65288;AttnMask&#65289;&#23558;&#23398;&#20064;&#38598;&#20013;&#22312;&#30456;&#20851;&#21306;&#22495;&#65307;&#25552;&#31034;&#23545;&#27604;&#25439;&#22833;&#65288;PromptCL&#65289;&#23558;&#19981;&#21516;&#27010;&#24565;&#30340;&#23884;&#20837;&#20998;&#31163;&#24320;&#26469;&#65307;&#20197;&#21450;&#32465;&#23450;&#24418;&#23481;&#35789;&#65288;Bind adj.&#65289;&#23558;&#26032;&#30340;&#8220;&#35789;&#8221;&#19982;&#24050;&#30693;&#35789;&#30456;&#20851;&#32852;&#12290;&#25105;&#20204;&#36890;&#36807;&#22270;&#20687;&#29983;&#25104;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Textural Inversion, a prompt learning method, learns a singular embedding for a new "word" to represent image style and appearance, allowing it to be integrated into natural language sentences to generate novel synthesised images. However, identifying and integrating multiple object-level concepts within one scene poses significant challenges even when embeddings for individual concepts are attainable. This is further confirmed by our empirical tests. To address this challenge, we introduce a framework for Multi-Concept Prompt Learning (MCPL), where multiple new "words" are simultaneously learned from a single sentence-image pair. To enhance the accuracy of word-concept correlation, we propose three regularisation techniques: Attention Masking (AttnMask) to concentrate learning on relevant areas; Prompts Contrastive Loss (PromptCL) to separate the embeddings of different concepts; and Bind adjective (Bind adj.) to associate new "words" with known words. We evaluate via image generation
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;FactCHD&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#26816;&#27979;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#29983;&#25104;&#25991;&#26412;&#30340;&#20107;&#23454;&#24615;&#12290;&#22522;&#20934;&#21253;&#21547;&#20102;&#22810;&#31181;&#20107;&#23454;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#20107;&#23454;&#30340;&#35777;&#25454;&#38142;&#36827;&#34892;&#32452;&#21512;&#24615;&#24187;&#35273;&#30340;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.12086</link><description>&lt;p&gt;
&#21457;&#29616;&#22622;&#22764;&#20043;&#27468;&#65306;&#21487;&#38752;&#30340;&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Siren's Song: Towards Reliable Fact-Conflicting Hallucination Detection. (arXiv:2310.12086v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12086
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;FactCHD&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#26816;&#27979;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#29983;&#25104;&#25991;&#26412;&#30340;&#20107;&#23454;&#24615;&#12290;&#22522;&#20934;&#21253;&#21547;&#20102;&#22810;&#31181;&#20107;&#23454;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#20107;&#23454;&#30340;&#35777;&#25454;&#38142;&#36827;&#34892;&#32452;&#21512;&#24615;&#24187;&#35273;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT/GPT-4&#65292;&#22240;&#20854;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#20854;&#22312;&#32593;&#32476;&#24179;&#21488;&#19978;&#23384;&#22312;&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#30340;&#38382;&#39064;&#38480;&#21046;&#20102;&#20854;&#37319;&#29992;&#12290;&#23545;&#30001;LLMs&#20135;&#29983;&#30340;&#25991;&#26412;&#30340;&#20107;&#23454;&#24615;&#35780;&#20272;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#65292;&#19981;&#20165;&#28041;&#21450;&#23545;&#22522;&#26412;&#20107;&#23454;&#30340;&#21028;&#26029;&#65292;&#36824;&#21253;&#25324;&#23545;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#65288;&#22914;&#22810;&#36339;&#31561;&#65289;&#20013;&#20986;&#29616;&#30340;&#20107;&#23454;&#38169;&#35823;&#30340;&#35780;&#20272;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FactCHD&#65292;&#19968;&#31181;&#20026;LLMs&#31934;&#24515;&#35774;&#35745;&#30340;&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#26816;&#27979;&#22522;&#20934;&#12290;&#20316;&#20026;&#22312;&#8220;&#26597;&#35810;-&#21709;&#24212;&#8221;&#19978;&#19979;&#25991;&#20013;&#35780;&#20272;&#20107;&#23454;&#24615;&#30340;&#20851;&#38190;&#24037;&#20855;&#65292;&#25105;&#20204;&#30340;&#22522;&#20934;&#37319;&#29992;&#20102;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#20107;&#23454;&#27169;&#24335;&#65292;&#22914;&#22522;&#26412;&#20107;&#23454;&#65292;&#22810;&#36339;&#65292;&#27604;&#36739;&#21644;&#38598;&#21512;&#25805;&#20316;&#27169;&#24335;&#12290;&#25105;&#20204;&#22522;&#20934;&#30340;&#19968;&#20010;&#29420;&#29305;&#29305;&#28857;&#26159;&#20854;&#21253;&#21547;&#22522;&#20110;&#20107;&#23454;&#30340;&#35777;&#25454;&#38142;&#65292;&#20174;&#32780;&#20415;&#20110;&#36827;&#34892;&#32452;&#21512;&#24615;&#24187;&#35273;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), such as ChatGPT/GPT-4, have garnered widespread attention owing to their myriad of practical applications, yet their adoption has been constrained by issues of fact-conflicting hallucinations across web platforms. The assessment of factuality in text, produced by LLMs, remains inadequately explored, extending not only to the judgment of vanilla facts but also encompassing the evaluation of factual errors emerging in complex inferential tasks like multi-hop, and etc. In response, we introduce FactCHD, a fact-conflicting hallucination detection benchmark meticulously designed for LLMs. Functioning as a pivotal tool in evaluating factuality within "Query-Respons" contexts, our benchmark assimilates a large-scale dataset, encapsulating a broad spectrum of factuality patterns, such as vanilla, multi-hops, comparison, and set-operation patterns. A distinctive feature of our benchmark is its incorporation of fact-based chains of evidence, thereby facilitating com
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#20998;&#26512;&#20102;&#22823;&#22411;Transformer&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;&#30740;&#31350;&#35777;&#26126;&#65292;&#22312;&#20551;&#35774;&#27169;&#22411;&#21487;&#23454;&#29616;&#30340;&#24773;&#20917;&#19979;&#65292;&#32463;&#36807;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#33021;&#22815;&#27169;&#20223;&#19987;&#23478;&#31639;&#27861;&#22312;&#35266;&#23519;&#21040;&#30340;&#36712;&#36857;&#19978;&#30340;&#26465;&#20214;&#26399;&#26395;&#12290;</title><link>http://arxiv.org/abs/2310.08566</link><description>&lt;p&gt;
&#20197;Transformer&#20026;&#20915;&#31574;&#32773;&#65306;&#36890;&#36807;&#30417;&#30563;&#39044;&#35757;&#32451;&#23454;&#29616;&#21487;&#35777;&#26126;&#30340;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Transformers as Decision Makers: Provable In-Context Reinforcement Learning via Supervised Pretraining. (arXiv:2310.08566v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08566
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#20998;&#26512;&#20102;&#22823;&#22411;Transformer&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;&#30740;&#31350;&#35777;&#26126;&#65292;&#22312;&#20551;&#35774;&#27169;&#22411;&#21487;&#23454;&#29616;&#30340;&#24773;&#20917;&#19979;&#65292;&#32463;&#36807;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#33021;&#22815;&#27169;&#20223;&#19987;&#23478;&#31639;&#27861;&#22312;&#35266;&#23519;&#21040;&#30340;&#36712;&#36857;&#19978;&#30340;&#26465;&#20214;&#26399;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;Transformer&#27169;&#22411;&#23637;&#31034;&#20102;&#20196;&#20154;&#24778;&#21497;&#30340;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#33021;&#21147;&#65292;&#21363;&#24403;&#23427;&#20204;&#38754;&#23545;&#26469;&#33258;&#26410;&#30693;&#29615;&#22659;&#30340;&#20132;&#20114;&#36712;&#36857;&#26102;&#65292;&#23427;&#20204;&#33021;&#22815;&#20570;&#20986;&#33391;&#22909;&#30340;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;Transformer&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#35757;&#32451;&#20197;&#25191;&#34892;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#29702;&#35770;&#19978;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#29702;&#35299;&#12290;&#29305;&#21035;&#26159;&#65292;&#23578;&#19981;&#28165;&#26970;Transformer&#27169;&#22411;&#21487;&#20197;&#22312;&#19978;&#19979;&#25991;&#20013;&#25191;&#34892;&#21738;&#20123;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20197;&#21450;&#31163;&#32447;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20998;&#24067;&#24046;&#24322;&#22914;&#20309;&#24433;&#21709;&#24050;&#23398;&#20064;&#30340;&#31639;&#27861;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#20998;&#26512;&#20102;&#23545;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#30340;&#30417;&#30563;&#39044;&#35757;&#32451;&#12290;&#36825;&#21253;&#25324;&#20102;&#20004;&#31181;&#26368;&#36817;&#25552;&#20986;&#30340;&#35757;&#32451;&#26041;&#27861;&#65306;&#31639;&#27861;&#33976;&#39311;&#21644;&#20915;&#31574;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#12290;&#39318;&#20808;&#65292;&#22312;&#20551;&#35774;&#27169;&#22411;&#21487;&#23454;&#29616;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#32463;&#36807;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#23558;&#27169;&#20223;&#19987;&#23478;&#31639;&#27861;&#22312;&#35266;&#23519;&#21040;&#30340;&#36712;&#36857;&#19978;&#30340;&#26465;&#20214;&#26399;&#26395;&#12290;&#24191;&#20041;&#35823;&#24046;&#30340;&#32553;&#25918;&#33539;&#22260;&#19982;&#8230;
&lt;/p&gt;
&lt;p&gt;
Large transformer models pretrained on offline reinforcement learning datasets have demonstrated remarkable in-context reinforcement learning (ICRL) capabilities, where they can make good decisions when prompted with interaction trajectories from unseen environments. However, when and how transformers can be trained to perform ICRL have not been theoretically well-understood. In particular, it is unclear which reinforcement-learning algorithms transformers can perform in context, and how distribution mismatch in offline training data affects the learned algorithms. This paper provides a theoretical framework that analyzes supervised pretraining for ICRL. This includes two recently proposed training methods -- algorithm distillation and decision-pretrained transformers. First, assuming model realizability, we prove the supervised-pretrained transformer will imitate the conditional expectation of the expert algorithm given the observed trajectory. The generalization error will scale with
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#35821;&#27861;&#38169;&#35823;&#19988;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#30340;LLM&#24037;&#20855;&#20351;&#29992;&#26041;&#27861;ToolDec&#65292;&#36890;&#36807;&#26377;&#38480;&#29366;&#24577;&#35299;&#30721;&#31639;&#27861;&#28040;&#38500;&#20102;&#24037;&#20855;&#30456;&#20851;&#38169;&#35823;&#65292;&#20351;LLM&#33021;&#22815;&#26377;&#25928;&#36873;&#25321;&#24037;&#20855;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#25110;&#19978;&#19979;&#25991;&#25991;&#26723;&#12290;</title><link>http://arxiv.org/abs/2310.07075</link><description>&lt;p&gt;
&#36890;&#36807;&#26377;&#38480;&#29366;&#24577;&#35299;&#30721;&#23454;&#29616;&#26080;&#35821;&#27861;&#38169;&#35823;&#21644;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#30340;LLM&#24037;&#20855;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
Syntax Error-Free and Generalizable Tool Use for LLMs via Finite-State Decoding. (arXiv:2310.07075v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#35821;&#27861;&#38169;&#35823;&#19988;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#30340;LLM&#24037;&#20855;&#20351;&#29992;&#26041;&#27861;ToolDec&#65292;&#36890;&#36807;&#26377;&#38480;&#29366;&#24577;&#35299;&#30721;&#31639;&#27861;&#28040;&#38500;&#20102;&#24037;&#20855;&#30456;&#20851;&#38169;&#35823;&#65292;&#20351;LLM&#33021;&#22815;&#26377;&#25928;&#36873;&#25321;&#24037;&#20855;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#25110;&#19978;&#19979;&#25991;&#25991;&#26723;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#23637;&#31034;&#20986;&#20351;&#29992;&#22806;&#37096;&#24037;&#20855;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#26377;&#24076;&#26395;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#28041;&#21450;&#23545;&#24037;&#20855;&#28436;&#31034;&#36827;&#34892;&#24494;&#35843;&#65292;&#36825;&#26679;&#22312;&#27809;&#26377;&#39069;&#22806;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#26080;&#27861;&#25512;&#24191;&#21040;&#26032;&#30340;&#24037;&#20855;&#65292;&#35201;&#20040;&#22312;&#19978;&#19979;&#25991;&#20013;&#25552;&#20379;&#24037;&#20855;&#25991;&#26723;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#24037;&#20855;&#25968;&#37327;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#24120;&#24120;&#20135;&#29983;&#35821;&#27861;&#26080;&#25928;&#30340;&#24037;&#20855;&#35843;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ToolDec&#65292;&#19968;&#31181;&#26377;&#38480;&#29366;&#24577;&#26426;&#24341;&#23548;&#30340;&#35299;&#30721;&#31639;&#27861;&#65292;&#29992;&#20110;&#24037;&#20855;&#22686;&#24378;&#30340;LLMs&#12290;ToolDec&#36890;&#36807;&#30830;&#20445;&#26377;&#25928;&#30340;&#24037;&#20855;&#21517;&#31216;&#21644;&#31867;&#22411;&#19968;&#33268;&#30340;&#21442;&#25968;&#65292;&#28040;&#38500;&#20102;&#20219;&#20309;&#24037;&#20855;&#22686;&#24378;&#30340;LLMs&#20013;&#30340;&#24037;&#20855;&#30456;&#20851;&#38169;&#35823;&#12290;&#27492;&#22806;&#65292;ToolDec&#20351;LLM&#33021;&#22815;&#20165;&#20165;&#20351;&#29992;&#23427;&#20204;&#30340;&#21517;&#31216;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#26377;&#25928;&#22320;&#36873;&#25321;&#24037;&#20855;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#25110;&#19978;&#19979;&#25991;&#25991;&#26723;&#12290;&#25105;&#20204;&#22312;&#28041;&#21450;&#25968;&#23398;&#20989;&#25968;&#12289;&#30693;&#35782;&#22270;&#35889;&#20851;&#31995;&#21644;&#22797;&#26434;&#30340;&#29616;&#23454;&#19990;&#30028;RESTful API&#30340;&#21508;&#31181;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#22810;&#31181;&#20808;&#21069;&#30340;&#26041;&#27861;&#21450;&#20854;ToolDec&#22686;&#24378;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown promising capabilities in using external tools to solve complex problems. However, existing approaches either involve fine-tuning on tool demonstrations, which do not generalize to new tools without additional training, or providing tool documentation in context, limiting the number of tools. Both approaches often generate syntactically invalid tool calls. In this paper, we propose ToolDec, a finite-state machine-guided decoding algorithm for tool-augmented LLMs. ToolDec eliminates tool-related errors for any tool-augmented LLMs by ensuring valid tool names and type-conforming arguments. Furthermore, ToolDec enables LLM to effectively select tools using only the information contained in their names, with no need for fine-tuning or in-context documentation. We evaluated multiple prior methods and their ToolDec-enhanced versions on a variety of tasks involving tools like math functions, knowledge graph relations, and complex real-world RESTful APIs
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#23569;&#37327;&#19978;&#19979;&#25991;&#31034;&#33539;&#26469;&#25805;&#32437;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25552;&#20379;&#31034;&#33539;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#65292;&#21487;&#20197;&#22686;&#21152;&#25110;&#38477;&#20302;&#27169;&#22411;&#22238;&#31572;&#24694;&#24847;&#25552;&#31034;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#30456;&#21516;&#19978;&#19979;&#25991;&#25915;&#20987;&#21644;&#30456;&#21516;&#19978;&#19979;&#25991;&#38450;&#24481;&#26041;&#27861;&#65292;&#29992;&#20110;&#36234;&#29425;&#21644;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.06387</link><description>&lt;p&gt;
&#21482;&#38656;&#23569;&#37327;&#19978;&#19979;&#25991;&#31034;&#33539;&#21363;&#21487;&#23454;&#29616;&#36234;&#29425;&#21644;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations. (arXiv:2310.06387v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#23569;&#37327;&#19978;&#19979;&#25991;&#31034;&#33539;&#26469;&#25805;&#32437;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25552;&#20379;&#31034;&#33539;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#65292;&#21487;&#20197;&#22686;&#21152;&#25110;&#38477;&#20302;&#27169;&#22411;&#22238;&#31572;&#24694;&#24847;&#25552;&#31034;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#30456;&#21516;&#19978;&#19979;&#25991;&#25915;&#20987;&#21644;&#30456;&#21516;&#19978;&#19979;&#25991;&#38450;&#24481;&#26041;&#27861;&#65292;&#29992;&#20110;&#36234;&#29425;&#21644;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#23545;&#20854;&#23433;&#20840;&#24615;&#21644;&#29983;&#25104;&#24694;&#24847;&#20869;&#23481;&#30340;&#28508;&#22312;&#39118;&#38505;&#30340;&#25285;&#24551;&#20063;&#28014;&#29616;&#20986;&#26469;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#30456;&#21516;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#20013;&#25805;&#32437;LLM&#23545;&#40784;&#33021;&#21147;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20165;&#36890;&#36807;&#23569;&#37327;&#30340;&#19978;&#19979;&#25991;&#31034;&#33539;&#32780;&#26080;&#38656;&#24494;&#35843;&#65292;&#23601;&#21487;&#20197;&#25805;&#32437;LLM&#22686;&#21152;&#25110;&#38477;&#20302;&#36234;&#29425;&#27010;&#29575;&#65292;&#21363;&#22238;&#31572;&#24694;&#24847;&#25552;&#31034;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#36234;&#29425;&#21644;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#30446;&#30340;&#30340;&#30456;&#21516;&#19978;&#19979;&#25991;&#25915;&#20987;&#65288;ICA&#65289;&#21644;&#30456;&#21516;&#19978;&#19979;&#25991;&#38450;&#24481;&#65288;ICD&#65289;&#26041;&#27861;&#12290;ICA&#36890;&#36807;&#26500;&#36896;&#24694;&#24847;&#19978;&#19979;&#25991;&#25351;&#23548;&#27169;&#22411;&#29983;&#25104;&#26377;&#23475;&#36755;&#20986;&#65292;&#32780;ICD&#36890;&#36807;&#25298;&#32477;&#22238;&#31572;&#26377;&#23475;&#25552;&#31034;&#30340;&#31034;&#33539;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;ICA&#21644;ICD&#22312;&#22686;&#21152;&#25110;&#38477;&#20302;&#23545;&#25239;&#36234;&#29425;&#25915;&#20987;&#25104;&#21151;&#29575;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;ICL&#22312;&#36234;&#29425;&#21644;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown remarkable success in various tasks, but concerns about their safety and the potential for generating malicious content have emerged. In this paper, we explore the power of In-Context Learning (ICL) in manipulating the alignment ability of LLMs. We find that by providing just few in-context demonstrations without fine-tuning, LLMs can be manipulated to increase or decrease the probability of jailbreaking, i.e. answering malicious prompts. Based on these observations, we propose In-Context Attack (ICA) and In-Context Defense (ICD) methods for jailbreaking and guarding aligned language model purposes. ICA crafts malicious contexts to guide models in generating harmful outputs, while ICD enhances model robustness by demonstrations of rejecting to answer harmful prompts. Our experiments show the effectiveness of ICA and ICD in increasing or reducing the success rate of adversarial jailbreaking attacks. Overall, we shed light on the potential of ICL t
&lt;/p&gt;</description></item><item><title>LLMs&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#21482;&#33021;&#23398;&#20064;&#21040;"A&#26159;B"&#30340;&#32467;&#26500;&#65292;&#26080;&#27861;&#33258;&#21160;&#25512;&#24191;&#21040;"B&#26159;A"&#12290;&#36825;&#34920;&#26126;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#26029;&#19978;&#23384;&#22312;&#22522;&#26412;&#22833;&#36133;&#21644;&#35757;&#32451;&#38598;&#20013;&#27169;&#24335;&#30340;&#25512;&#24191;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.12288</link><description>&lt;p&gt;
&#32763;&#36716;&#35781;&#21650;: &#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35757;&#32451;&#30340;"A&#26159;B"&#26080;&#27861;&#23398;&#20064;"B&#26159;A"
&lt;/p&gt;
&lt;p&gt;
The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A". (arXiv:2309.12288v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12288
&lt;/p&gt;
&lt;p&gt;
LLMs&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#21482;&#33021;&#23398;&#20064;&#21040;"A&#26159;B"&#30340;&#32467;&#26500;&#65292;&#26080;&#27861;&#33258;&#21160;&#25512;&#24191;&#21040;"B&#26159;A"&#12290;&#36825;&#34920;&#26126;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#26029;&#19978;&#23384;&#22312;&#22522;&#26412;&#22833;&#36133;&#21644;&#35757;&#32451;&#38598;&#20013;&#27169;&#24335;&#30340;&#25512;&#24191;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25581;&#31034;&#20102;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#27867;&#21270;&#19978;&#30340;&#20196;&#20154;&#24778;&#35766;&#30340;&#22833;&#36133;&#12290;&#22914;&#26524;&#19968;&#20010;&#27169;&#22411;&#26159;&#22522;&#20110;"A&#26159;B"&#24418;&#24335;&#30340;&#21477;&#23376;&#36827;&#34892;&#35757;&#32451;&#65292;&#23427;&#19981;&#20250;&#33258;&#21160;&#25512;&#24191;&#21040;&#30456;&#21453;&#30340;&#26041;&#21521;"B&#26159;A"&#12290;&#36825;&#23601;&#26159;&#32763;&#36716;&#35781;&#21650;&#12290;&#20363;&#22914;&#65292;&#22914;&#26524;&#19968;&#20010;&#27169;&#22411;&#26159;&#22522;&#20110;"Olaf Scholz&#26159;&#24503;&#22269;&#31532;&#20061;&#20219;&#24635;&#29702;"&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#23427;&#19981;&#20250;&#33258;&#21160;&#33021;&#22815;&#22238;&#31572;&#38382;&#39064;"&#35841;&#26159;&#24503;&#22269;&#31532;&#20061;&#20219;&#24635;&#29702;&#65311;"&#12290;&#27492;&#22806;&#65292;&#27491;&#30830;&#31572;&#26696;&#65288;"Olaf Scholz"&#65289;&#30340;&#21487;&#33021;&#24615;&#19981;&#20250;&#27604;&#38543;&#26426;&#21517;&#23383;&#26356;&#39640;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#26029;&#19978;&#23384;&#22312;&#22522;&#26412;&#22833;&#36133;&#65292;&#24182;&#19988;&#19981;&#20250;&#25512;&#24191;&#21040;&#23427;&#20204;&#35757;&#32451;&#38598;&#20013;&#30340;&#26222;&#36941;&#27169;&#24335;&#65288;&#21363;&#22914;&#26524;&#20986;&#29616;"A&#26159;B"&#65292;&#21017;"B&#26159;A"&#26356;&#21487;&#33021;&#20986;&#29616;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#34394;&#26500;&#30340;&#38472;&#36848;&#65288;&#22914;"Uriah Hawthorne&#26159;'Abyssal Melodies'&#30340;&#20316;&#26354;&#23478;"&#65289;&#19978;&#23545;GPT-3&#21644;Llama-1&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#23637;&#31034;&#23427;&#20204;&#26080;&#27861;&#27491;&#30830;&#22238;&#31572;"&#35841;&#21019;&#20316;&#20102;'Abyssal Melodies'?"&#26469;&#25552;&#20379;&#32763;&#36716;&#35781;&#21650;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We expose a surprising failure of generalization in auto-regressive large language models (LLMs). If a model is trained on a sentence of the form "A is B", it will not automatically generalize to the reverse direction "B is A". This is the Reversal Curse. For instance, if a model is trained on "Olaf Scholz was the ninth Chancellor of Germany", it will not automatically be able to answer the question, "Who was the ninth Chancellor of Germany?". Moreover, the likelihood of the correct answer ("Olaf Scholz") will not be higher than for a random name. Thus, models exhibit a basic failure of logical deduction and do not generalize a prevalent pattern in their training set (i.e. if "A is B'' occurs, "B is A" is more likely to occur). We provide evidence for the Reversal Curse by finetuning GPT-3 and Llama-1 on fictitious statements such as "Uriah Hawthorne is the composer of 'Abyssal Melodies'" and showing that they fail to correctly answer "Who composed 'Abyssal Melodies?'". The Reversal Cu
&lt;/p&gt;</description></item><item><title>Text2Reward&#26159;&#19968;&#20010;&#26080;&#38656;&#25968;&#25454;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#26681;&#25454;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#21487;&#35299;&#37322;&#12289;&#33258;&#30001;&#24418;&#24335;&#30340;&#23494;&#38598;&#22870;&#21169;&#20989;&#25968;&#65292;&#24191;&#27867;&#36866;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#20801;&#35768;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#36845;&#20195;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.11489</link><description>&lt;p&gt;
Text2Reward&#65306;&#38024;&#23545;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#29983;&#25104;&#23494;&#38598;&#22870;&#21169;&#20989;&#25968;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning. (arXiv:2309.11489v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11489
&lt;/p&gt;
&lt;p&gt;
Text2Reward&#26159;&#19968;&#20010;&#26080;&#38656;&#25968;&#25454;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#26681;&#25454;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#21487;&#35299;&#37322;&#12289;&#33258;&#30001;&#24418;&#24335;&#30340;&#23494;&#38598;&#22870;&#21169;&#20989;&#25968;&#65292;&#24191;&#27867;&#36866;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#20801;&#35768;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#36845;&#20195;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#26159;&#24378;&#21270;&#23398;&#20064;&#20013;&#38271;&#26399;&#20197;&#26469;&#30340;&#25361;&#25112;&#65307;&#23427;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#25110;&#39046;&#22495;&#25968;&#25454;&#65292;&#23548;&#33268;&#24320;&#21457;&#25104;&#26412;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Text2Reward&#65292;&#19968;&#20010;&#26080;&#38656;&#25968;&#25454;&#30340;&#26694;&#26550;&#65292;&#21487;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33258;&#21160;&#29983;&#25104;&#23494;&#38598;&#22870;&#21169;&#20989;&#25968;&#12290;&#32473;&#23450;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#30446;&#26631;&#65292;Text2Reward&#29983;&#25104;&#20316;&#20026;&#29615;&#22659;&#32039;&#20945;&#34920;&#31034;&#30340;&#21487;&#25191;&#34892;&#31243;&#24207;&#30340;&#23494;&#38598;&#22870;&#21169;&#20989;&#25968;&#12290;&#19982;&#36870;&#24378;&#21270;&#23398;&#20064;&#21644;&#26368;&#36817;&#20351;&#29992;LLM&#32534;&#20889;&#31232;&#30095;&#22870;&#21169;&#20195;&#30721;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;Text2Reward&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#12289;&#33258;&#30001;&#24418;&#24335;&#30340;&#23494;&#38598;&#22870;&#21169;&#20195;&#30721;&#65292;&#21487;&#28085;&#30422;&#21508;&#31181;&#20219;&#21153;&#65292;&#21033;&#29992;&#29616;&#26377;&#36719;&#20214;&#21253;&#65292;&#24182;&#20801;&#35768;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#36845;&#20195;&#25913;&#36827;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#26426;&#22120;&#20154;&#25805;&#20316;&#22522;&#20934;&#65288;ManiSkill2&#65292;MetaWorld&#65289;&#21644;&#20004;&#20010;MuJoCo&#30340;&#36816;&#21160;&#29615;&#22659;&#19978;&#35780;&#20272;&#20102;Text2Reward&#12290;&#22312;17&#20010;&#25805;&#20316;&#20219;&#21153;&#20013;&#30340;13&#20010;&#20219;&#21153;&#20013;&#65292;&#20351;&#29992;&#29983;&#25104;&#30340;&#22870;&#21169;&#20195;&#30721;&#35757;&#32451;&#30340;&#25919;&#31574;&#23454;&#29616;&#20102;&#31867;&#20284;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing reward functions is a longstanding challenge in reinforcement learning (RL); it requires specialized knowledge or domain data, leading to high costs for development. To address this, we introduce Text2Reward, a data-free framework that automates the generation of dense reward functions based on large language models (LLMs). Given a goal described in natural language, Text2Reward generates dense reward functions as an executable program grounded in a compact representation of the environment. Unlike inverse RL and recent work that uses LLMs to write sparse reward codes, Text2Reward produces interpretable, free-form dense reward codes that cover a wide range of tasks, utilize existing packages, and allow iterative refinement with human feedback. We evaluate Text2Reward on two robotic manipulation benchmarks (ManiSkill2, MetaWorld) and two locomotion environments of MuJoCo. On 13 of the 17 manipulation tasks, policies trained with generated reward codes achieve similar or better
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21019;&#26032;&#24615;&#22320;&#23558;&#20174;&#30495;&#23454;&#20154;&#26426;&#23545;&#35805;&#20013;&#25552;&#21462;&#30340;&#20154;&#31867;&#38382;&#39064;&#20316;&#20026;&#23398;&#20064;&#30446;&#26631;&#65292;&#24182;&#19988;&#35757;&#32451;&#20102;&#19968;&#20010;&#29992;&#25143;&#27169;&#25311;&#22120;UserGPT&#65292;&#24182;&#20351;&#29992;&#29983;&#25104;&#30340;&#39640;&#36136;&#37327;&#21512;&#25104;&#23545;&#35805;&#25968;&#25454;&#38598;RealChat&#26469;&#35757;&#32451;&#21161;&#25163;&#27169;&#22411;ReaLM&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ReaLM&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36807;&#20102;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.11534</link><description>&lt;p&gt;
&#20316;&#20026;&#29992;&#25143;&#27169;&#25311;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Model as a User Simulator. (arXiv:2308.11534v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21019;&#26032;&#24615;&#22320;&#23558;&#20174;&#30495;&#23454;&#20154;&#26426;&#23545;&#35805;&#20013;&#25552;&#21462;&#30340;&#20154;&#31867;&#38382;&#39064;&#20316;&#20026;&#23398;&#20064;&#30446;&#26631;&#65292;&#24182;&#19988;&#35757;&#32451;&#20102;&#19968;&#20010;&#29992;&#25143;&#27169;&#25311;&#22120;UserGPT&#65292;&#24182;&#20351;&#29992;&#29983;&#25104;&#30340;&#39640;&#36136;&#37327;&#21512;&#25104;&#23545;&#35805;&#25968;&#25454;&#38598;RealChat&#26469;&#35757;&#32451;&#21161;&#25163;&#27169;&#22411;ReaLM&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ReaLM&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36807;&#20102;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38381;&#28304;ChatGPT&#30340;&#21331;&#36234;&#24615;&#33021;&#24341;&#21457;&#20102;&#23545;&#20854;&#27665;&#20027;&#21270;&#30340;&#21162;&#21147;&#65292;&#20511;&#21161;&#30495;&#23454;&#29992;&#25143;&#21644;ChatGPT&#23545;&#35805;&#30340;&#21162;&#21147;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;Vicuna&#26159;&#19968;&#20010;&#24456;&#22909;&#30340;&#20363;&#23376;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;Baize&#21644;UltraChat&#31561;&#21162;&#21147;&#20027;&#35201;&#20381;&#38752;ChatGPT&#26681;&#25454;&#25351;&#20196;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#65292;&#32780;&#19981;&#26159;&#30495;&#23454;&#30340;&#20154;&#31867;&#23398;&#20064;&#65292;&#23548;&#33268;&#33539;&#22260;&#26377;&#38480;&#65292;&#22810;&#26679;&#24615;&#20943;&#24369;&#65292;&#32570;&#20047;&#30495;&#27491;&#30340;&#22810;&#36718;&#23545;&#35805;&#21160;&#24577;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#21019;&#26032;&#24615;&#22320;&#25226;&#20174;&#30495;&#23454;&#20154;&#26426;&#23545;&#35805;&#20013;&#25552;&#21462;&#30340;&#20154;&#31867;&#38382;&#39064;&#20316;&#20026;&#23398;&#20064;&#30446;&#26631;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#29992;&#25143;&#27169;&#25311;&#22120;UserGPT&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#21512;&#25104;&#23545;&#35805;&#25968;&#25454;&#38598;RealChat&#12290;&#38543;&#21518;&#65292;&#35813;&#25968;&#25454;&#38598;&#35757;&#32451;&#25105;&#20204;&#30340;&#21161;&#25163;&#27169;&#22411;ReaLM&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ReaLM&#22312;Vicuna-Bench&#21644;MT-Bench&#20013;&#22343;&#36229;&#36807;&#20102;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The unparalleled performance of closed-sourced ChatGPT has sparked efforts towards its democratization, with notable strides made by leveraging real user and ChatGPT conversations, as evidenced by Vicuna. However, while current endeavors like Baize and UltraChat aim to auto-generate conversational data due to challenges in gathering human participation, they primarily rely on ChatGPT to simulate human behaviors based on directives rather than genuine human learning. This results in a limited scope, diminished diversity, and an absence of genuine multi-round conversational dynamics. To address the above issues, we innovatively target human questions extracted from genuine human-machine conversations as a learning goal and train a user simulator, UserGPT, to produce a high-quality human-centric synthetic conversation dataset, RealChat. Subsequently, this dataset trains our assistant model, ReaLM. Experimentally, ReaLM outpaces baseline models in both Vicuna-Bench and MT-Bench by pairwise
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#35780;&#20272;&#20102;ChatGPT&#22312;&#32534;&#30721;&#31639;&#27861;&#21644;&#25968;&#25454;&#32467;&#26500;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#22522;&#20110;&#26368;&#22823;&#30340;&#32534;&#30721;&#25361;&#25112;&#30446;&#24405;&#65292;&#37325;&#28857;&#20851;&#27880;Python&#32534;&#31243;&#35821;&#35328;&#21644;&#25968;&#25454;&#32467;&#26500;&#31639;&#27861;&#20004;&#20010;&#22522;&#30784;&#20027;&#39064;&#12290;&#24635;&#32467;&#27979;&#35797;&#20013;ChatGPT&#30340;&#20195;&#30721;&#35299;&#20915;&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#12289;&#20195;&#30721;&#36136;&#37327;&#21644;&#36816;&#34892;&#26102;&#38169;&#35823;&#30340;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2307.05360</link><description>&lt;p&gt;
&#25581;&#24320;&#24040;&#20154;&#30340;&#30495;&#38754;&#30446;&#65306;&#23545;ChatGPT&#22312;&#32534;&#30721;&#31639;&#27861;&#21644;&#25968;&#25454;&#32467;&#26500;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Unmasking the giant: A comprehensive evaluation of ChatGPT's proficiency in coding algorithms and data structures. (arXiv:2307.05360v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#35780;&#20272;&#20102;ChatGPT&#22312;&#32534;&#30721;&#31639;&#27861;&#21644;&#25968;&#25454;&#32467;&#26500;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#22522;&#20110;&#26368;&#22823;&#30340;&#32534;&#30721;&#25361;&#25112;&#30446;&#24405;&#65292;&#37325;&#28857;&#20851;&#27880;Python&#32534;&#31243;&#35821;&#35328;&#21644;&#25968;&#25454;&#32467;&#26500;&#31639;&#27861;&#20004;&#20010;&#22522;&#30784;&#20027;&#39064;&#12290;&#24635;&#32467;&#27979;&#35797;&#20013;ChatGPT&#30340;&#20195;&#30721;&#35299;&#20915;&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#12289;&#20195;&#30721;&#36136;&#37327;&#21644;&#36816;&#34892;&#26102;&#38169;&#35823;&#30340;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#36716;&#21464;&#24615;&#24433;&#21709;&#28145;&#21051;&#22320;&#37325;&#22609;&#20102;&#20154;&#24037;&#26234;&#33021;(AI)&#25216;&#26415;&#39046;&#22495;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;ChatGPT&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#26377;&#30528;&#29420;&#29305;&#20043;&#22788;&#65292;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#22810;&#36718;&#23545;&#35805;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#31181;&#35821;&#35328;&#20013;&#23637;&#31034;&#20986;&#23545;&#32534;&#30721;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26681;&#25454;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#32534;&#30721;&#25361;&#25112;&#30446;&#24405;&#23545;ChatGPT&#30340;&#32534;&#30721;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;Python&#32534;&#31243;&#35821;&#35328;&#65292;&#20197;&#21450;&#38598;&#20013;&#22312;&#25968;&#25454;&#32467;&#26500;&#21644;&#31639;&#27861;&#19978;&#30340;&#38382;&#39064;&#65292;&#36825;&#20004;&#20010;&#20027;&#39064;&#26159;&#35745;&#31639;&#26426;&#31185;&#23398;&#30340;&#22522;&#30784;&#12290;&#25105;&#20204;&#35780;&#20272;ChatGPT&#35299;&#20915;&#25152;&#25552;&#20132;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#35780;&#20272;&#20854;&#20195;&#30721;&#36136;&#37327;&#20197;&#21450;&#20195;&#30721;&#24341;&#21457;&#30340;&#36816;&#34892;&#26102;&#38169;&#35823;&#30340;&#24615;&#36136;&#12290;&#24403;ChatGPT&#30340;&#20195;&#30721;&#25104;&#21151;&#25191;&#34892;&#20294;&#26410;&#33021;&#35299;&#20915;&#25163;&#22836;&#38382;&#39064;&#26102;&#65292;&#25105;&#20204;&#20250;&#30740;&#31350;&#36890;&#36807;&#30340;&#27979;&#35797;&#26696;&#20363;&#20013;&#30340;&#27169;&#24335;&#65292;&#20197;&#20102;&#35299;ChatGPT&#20195;&#30721;&#20013;&#30340;&#38169;&#35823;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
The transformative influence of Large Language Models (LLMs) is profoundly reshaping the Artificial Intelligence (AI) technology domain. Notably, ChatGPT distinguishes itself within these models, demonstrating remarkable performance in multi-turn conversations and exhibiting code proficiency across an array of languages. In this paper, we carry out a comprehensive evaluation of ChatGPT's coding capabilities based on what is to date the largest catalog of coding challenges. Our focus is on the python programming language and problems centered on data structures and algorithms, two topics at the very foundations of Computer Science. We evaluate ChatGPT for its ability to generate correct solutions to the problems fed to it, its code quality, and nature of run-time errors thrown by its code. Where ChatGPT code successfully executes, but fails to solve the problem at hand, we look into patterns in the test cases passed in order to gain some insights into how wrong ChatGPT code is in these 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#38024;&#23545;Indic&#35821;&#35328;Unicode&#20070;&#20889;&#26041;&#26696;&#20013;&#24120;&#35265;&#21450;&#19981;&#24120;&#35265;&#38382;&#39064;&#30340;&#24037;&#20855;&#24211;&#65292;&#20998;&#21035;&#26159;&#32416;&#27491;&#19981;&#19968;&#33268;&#24615;&#30340;&#35268;&#33539;&#21270;&#22120;&#21644;Abugida&#25991;&#26412;&#30340;&#23383;&#24418;&#35299;&#26512;&#22120;&#65292;&#20351;&#29992;&#36825;&#20004;&#20010;&#24037;&#20855;&#21487;&#20197;&#25552;&#39640;400%&#30340;&#36895;&#24230;&#24182;&#26174;&#33879;&#25913;&#21892;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.01743</link><description>&lt;p&gt;
&#19968;&#31181;&#36866;&#29992;&#20110;Unicode&#25991;&#26412;&#30340;Abugida&#35268;&#33539;&#21270;&#22120;&#21644;&#35299;&#26512;&#22120;
&lt;/p&gt;
&lt;p&gt;
Abugida Normalizer and Parser for Unicode texts. (arXiv:2306.01743v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#38024;&#23545;Indic&#35821;&#35328;Unicode&#20070;&#20889;&#26041;&#26696;&#20013;&#24120;&#35265;&#21450;&#19981;&#24120;&#35265;&#38382;&#39064;&#30340;&#24037;&#20855;&#24211;&#65292;&#20998;&#21035;&#26159;&#32416;&#27491;&#19981;&#19968;&#33268;&#24615;&#30340;&#35268;&#33539;&#21270;&#22120;&#21644;Abugida&#25991;&#26412;&#30340;&#23383;&#24418;&#35299;&#26512;&#22120;&#65292;&#20351;&#29992;&#36825;&#20004;&#20010;&#24037;&#20855;&#21487;&#20197;&#25552;&#39640;400%&#30340;&#36895;&#24230;&#24182;&#26174;&#33879;&#25913;&#21892;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#24211;&#65292;&#20197;&#35299;&#20915;Indic&#35821;&#35328;&#22522;&#20110;Unicode&#30340;&#20070;&#20889;&#26041;&#26696;&#30340;&#24120;&#35265;&#21644;&#19981;&#24120;&#35265;&#38382;&#39064;&#12290;&#31532;&#19968;&#20010;&#26159;&#19968;&#20010;&#35268;&#33539;&#21270;&#22120;&#65292;&#36890;&#36807;https://pypi.org/project/bnunicodenormalizer/&#32416;&#27491;&#30001;&#32534;&#30721;&#26041;&#26696;&#24341;&#36215;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#31532;&#20108;&#20010;&#26159;Abugida&#25991;&#26412;&#30340;&#23383;&#24418;&#35299;&#26512;&#22120;https://pypi.org/project/indicparser/&#12290;&#36825;&#20004;&#20010;&#24037;&#20855;&#27604;&#20197;&#21069;&#20351;&#29992;&#30340;&#24037;&#20855;&#26356;&#26377;&#25928;&#29575;&#21644;&#26377;&#25928;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;400%&#30340;&#36895;&#24230;&#25552;&#21319;&#65292;&#24182;&#30830;&#20445;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#22522;&#20110;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes two libraries to address common and uncommon issues with Unicode-based writing schemes for Indic languages. The first is a normalizer that corrects inconsistencies caused by the encoding scheme https://pypi.org/project/bnunicodenormalizer/ . The second is a grapheme parser for Abugida text https://pypi.org/project/indicparser/ . Both tools are more efficient and effective than previously used tools. We report 400% increase in speed and ensure significantly better performance for different language model based downstream tasks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;&#20869;&#37096;&#24037;&#20316;&#35760;&#24518;&#27169;&#22359;&#30340;&#20915;&#31574;Transformer&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20915;&#31574;&#20195;&#29702;&#22312;&#22788;&#29702;&#26032;&#20219;&#21153;&#19978;&#24615;&#33021;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;&#35757;&#32451;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#36716;&#21270;&#20915;&#31574;&#21046;&#23450;&#20195;&#29702;&#23545;&#26032;&#20219;&#21153;&#30340;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16338</link><description>&lt;p&gt;
&#28145;&#24605;&#29087;&#34385;&#65306;&#20855;&#26377;&#20869;&#37096;&#24037;&#20316;&#35760;&#24518;&#30340;&#20915;&#31574;Transformer
&lt;/p&gt;
&lt;p&gt;
Think Before You Act: Decision Transformers with Internal Working Memory. (arXiv:2305.16338v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16338
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;&#20869;&#37096;&#24037;&#20316;&#35760;&#24518;&#27169;&#22359;&#30340;&#20915;&#31574;Transformer&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20915;&#31574;&#20195;&#29702;&#22312;&#22788;&#29702;&#26032;&#20219;&#21153;&#19978;&#24615;&#33021;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;&#35757;&#32451;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#36716;&#21270;&#20915;&#31574;&#21046;&#23450;&#20195;&#29702;&#23545;&#26032;&#20219;&#21153;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20915;&#31574;&#21046;&#23450;&#20195;&#29702;&#24050;&#32463;&#23637;&#31034;&#20102;&#36328;&#36234;&#22810;&#20010;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#31181;&#20302;&#25928;&#24615;&#28304;&#20110;&#36951;&#24536;&#29616;&#35937;&#65292;&#21363;&#27169;&#22411;&#36890;&#36807;&#21442;&#25968;&#35760;&#24518;&#20854;&#34892;&#20026;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#12290;&#22240;&#27492;&#65292;&#26032;&#20219;&#21153;&#30340;&#35757;&#32451;&#21487;&#33021;&#20250;&#38477;&#20302;&#27169;&#22411;&#22312;&#20808;&#21069;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#19982;LLM&#30340;&#38544;&#24335;&#35760;&#24518;&#26426;&#21046;&#19981;&#21516;&#65292;&#20154;&#33041;&#21033;&#29992;&#20998;&#24067;&#24335;&#23384;&#20648;&#22120;&#23384;&#20648;&#35760;&#24518;&#65292;&#20197;&#26377;&#25928;&#22320;&#31649;&#29702;&#21644;&#32452;&#32455;&#22810;&#31181;&#25216;&#33021;&#65292;&#20943;&#36731;&#20102;&#36951;&#24536;&#29616;&#35937;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#20869;&#37096;&#24037;&#20316;&#35760;&#24518;&#27169;&#22359;&#26469;&#23384;&#20648;&#12289;&#34701;&#21512;&#21644;&#26816;&#32034;&#19981;&#21516;&#19979;&#28216;&#20219;&#21153;&#30340;&#20449;&#24687;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;Atari&#28216;&#25103;&#21644;&#20803;&#19990;&#30028;&#29289;&#20307;&#25805;&#20316;&#20219;&#21153;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35760;&#24518;&#24494;&#35843;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#36716;&#21270;&#20915;&#31574;&#21046;&#23450;&#20195;&#29702;&#23545;&#26032;&#20219;&#21153;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language model (LLM)-based decision-making agents have shown the ability to generalize across multiple tasks. However, their performance relies on massive data and compute. We argue that this inefficiency stems from the forgetting phenomenon, in which a model memorizes its behaviors in parameters throughout training. As a result, training on a new task may deteriorate the model's performance on previous tasks. In contrast to LLMs' implicit memory mechanism, the human brain utilizes distributed memory storage, which helps manage and organize multiple skills efficiently, mitigating the forgetting phenomenon. Thus inspired, we propose an internal working memory module to store, blend, and retrieve information for different downstream tasks. Evaluation results show that the proposed method improves training efficiency and generalization in both Atari games and meta-world object manipulation tasks. Moreover, we demonstrate that memory fine-tuning further enhances the adaptability of t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#25353;&#20195;&#30721;&#29255;&#27573;&#25628;&#32034;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;StackOverflow&#25968;&#25454;&#30340;SearchBySnippet&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#21333;&#32534;&#30721;&#22120;&#27169;&#22411;SnippeR&#65292;&#23427;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11625</link><description>&lt;p&gt;
&#20195;&#30721;&#25628;&#32034;&#65306;&#19968;&#20010;&#26032;&#30340;SearchBySnippet&#25968;&#25454;&#38598;&#21644;SnippeR&#26816;&#32034;&#27169;&#22411;&#29992;&#20110;&#25353;&#20195;&#30721;&#29255;&#27573;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Searching by Code: a New SearchBySnippet Dataset and SnippeR Retrieval Model for Searching by Code Snippets. (arXiv:2305.11625v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#25353;&#20195;&#30721;&#29255;&#27573;&#25628;&#32034;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;StackOverflow&#25968;&#25454;&#30340;SearchBySnippet&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#21333;&#32534;&#30721;&#22120;&#27169;&#22411;SnippeR&#65292;&#23427;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#25628;&#32034;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#22312;&#36817;&#24180;&#26469;&#24471;&#21040;&#20102;&#24456;&#22810;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#23581;&#35797;&#20027;&#35201;&#32771;&#34385;&#36890;&#36807;&#25991;&#26412;&#26597;&#35810;&#25628;&#32034;&#20195;&#30721;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20351;&#29992;&#20195;&#30721;&#29255;&#27573;&#65288;&#21487;&#33021;&#20276;&#38543;&#30528;&#22238;&#28335;&#65289;&#20316;&#20026;&#26597;&#35810;&#65292;&#24182;&#23547;&#25214;&#20855;&#26377;&#20462;&#22797;&#38169;&#35823;&#25351;&#20196;&#21644;&#20195;&#30721;&#31034;&#20363;&#30340;&#31572;&#26696;&#26159;&#19968;&#20010;&#33258;&#28982;&#30340;&#29992;&#20363;&#65292;&#32780;&#29616;&#26377;&#26041;&#27861;&#24182;&#26410;&#28085;&#30422;&#27492;&#31867;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#25968;&#25454;&#38598;&#20351;&#29992;&#20174;&#20195;&#30721;&#20013;&#25552;&#21462;&#30340;&#27880;&#37322;&#32780;&#19981;&#26159;&#20840;&#25991;&#25551;&#36848;&#20316;&#20026;&#25991;&#26412;&#65292;&#20351;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#36825;&#31181;&#29992;&#20363;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;StackOverflow&#25968;&#25454;&#23454;&#29616;&#25628;&#32034;BySnippet&#29992;&#20363;&#30340;&#26032;SearchBySnippet&#25968;&#25454;&#38598;; &#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#29616;&#26377;&#20307;&#31995;&#32467;&#26500;&#21363;&#20351;&#22312;&#24494;&#35843;&#20043;&#21518;&#20063;&#19981;&#33021;&#19982;&#26368;&#31616;&#21333;&#30340;BM25&#22522;&#32447;&#30456;&#25552;&#24182;&#35770;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21333;&#32534;&#30721;&#22120;&#27169;&#22411;SnippeR&#65292;&#23427;&#22312;SearchBySnippet&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20960;&#20010;&#24378;&#22522;&#32447;&#65292;&#32467;&#26524;&#20026;0.451 Recall@10; &#25105;&#20204;&#24314;&#35758;&#23558;SearchBySnippet&#25968;&#25454;&#38598;&#21644;SnippeR&#20316;&#20026;&#26032;&#30340;&#37325;&#35201;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#26469;&#25512;&#24191;&#20195;&#30721;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code search is an important task that has seen many developments in recent years. However, previous attempts have mostly considered the problem of searching for code by a text query. We argue that using a code snippet (and possibly an associated traceback) as a query and looking for answers with bugfixing instructions and code samples is a natural use case that is not covered by existing approaches. Moreover, existing datasets use comments extracted from code rather than full-text descriptions as text, making them unsuitable for this use case. We present a new SearchBySnippet dataset implementing the search-by-code use case based on StackOverflow data; it turns out that in this setting, existing architectures fall short of the simplest BM25 baseline even after fine-tuning. We present a new single encoder model SnippeR that outperforms several strong baselines on the SearchBySnippet dataset with a result of 0.451 Recall@10; we propose the SearchBySnippet dataset and SnippeR as a new imp
&lt;/p&gt;</description></item><item><title>&#36830;&#32493;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#38754;&#20020;&#30528;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#35299;&#20915;&#26032;&#22686;&#23454;&#20307;&#21644;&#20851;&#31995;&#20197;&#21450;&#22810;&#27169;&#24577;&#28304;&#25968;&#25454;&#21464;&#21270;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.08698</link><description>&lt;p&gt;
&#36830;&#32493;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Continual Multimodal Knowledge Graph Construction. (arXiv:2305.08698v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08698
&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#38754;&#20020;&#30528;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#35299;&#20915;&#26032;&#22686;&#23454;&#20307;&#21644;&#20851;&#31995;&#20197;&#21450;&#22810;&#27169;&#24577;&#28304;&#25968;&#25454;&#21464;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#65288;MKGC&#65289;&#28041;&#21450;&#20351;&#29992;&#22810;&#31181;&#24418;&#24335;&#30340;&#25968;&#25454;&#65292;&#22914;&#25991;&#26412;&#21644;&#22270;&#20687;&#65292;&#21019;&#24314;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#32467;&#26500;&#21270;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MKGC&#27169;&#22411;&#22312;&#22788;&#29702;&#21160;&#24577;&#29616;&#23454;&#22330;&#26223;&#20013;&#26032;&#22686;&#23454;&#20307;&#21644;&#20851;&#31995;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#30446;&#21069;&#30340;&#36830;&#32493;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#35774;&#32622;&#20027;&#35201;&#20851;&#27880;&#20174;&#25991;&#26412;&#25968;&#25454;&#20013;&#25552;&#21462;&#23454;&#20307;&#21644;&#20851;&#31995;&#65292;&#24573;&#35270;&#20102;&#20854;&#20182;&#22810;&#27169;&#24577;&#28304;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#25506;&#32034;&#36830;&#32493;MKGC&#30340;&#25361;&#25112;&#65292;&#20197;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#65292;&#24182;&#30830;&#20445;&#20445;&#30041;&#20174;&#19981;&#21516;&#24418;&#24335;&#30340;&#25968;&#25454;&#20013;&#25552;&#21462;&#30340;&#36807;&#21435;&#30693;&#35782;&#12290;&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#36890;&#36807;&#24320;&#21457;&#32456;&#36523;MKGC&#22522;&#20934;&#25968;&#25454;&#38598;&#26469;&#30740;&#31350;&#36825;&#20010;&#22797;&#26434;&#30340;&#20027;&#39064;&#12290;&#22522;&#20110;&#32463;&#39564;&#21457;&#29616;&#65292;&#24403;&#22810;&#23186;&#20307;&#25968;&#25454;&#35757;&#32451;&#26102;&#65292;&#19968;&#20123;&#20856;&#22411;&#30340;MKGC&#27169;&#22411;&#21487;&#33021;&#22312;&#36830;&#32493;&#35774;&#32622;&#20013;&#24847;&#22806;&#34920;&#29616;&#19981;&#20339;&#65292;&#19982;&#37027;&#20123;&#20165;&#21033;&#29992;&#25991;&#26412;&#36164;&#28304;&#30340;&#27169;&#22411;&#30456;&#27604;&#12290;&#25105;&#20204;&#20197;&#23454;&#39564;&#35777;&#25454;&#20026;&#22522;&#30784;&#65292;&#24635;&#32467;&#20986;&#20197;&#19979;&#35770;&#28857;&#65306;&#36830;&#32493;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#38754;&#20020;&#30528;&#25968;&#25454;&#28304;&#21464;&#21270;&#23548;&#33268;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Knowledge Graph Construction (MKGC) involves creating structured representations of entities and relations using multiple modalities, such as text and images. However, existing MKGC models face challenges in handling the addition of new entities and relations in dynamic real-world scenarios. The current continual setting for knowledge graph construction mainly focuses on entity and relation extraction from text data, overlooking other multimodal sources. Therefore, there arises the need to explore the challenge of continual MKGC to address the phenomenon of catastrophic forgetting and ensure the retention of past knowledge extracted from different forms of data. This research focuses on investigating this complex topic by developing lifelong MKGC benchmark datasets. Based on the empirical findings that several typical MKGC models, when trained on multimedia data, might unexpectedly underperform compared to those solely utilizing textual resources in a continual setting, we p
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#26497;&#24230;&#24369;&#30340;&#30417;&#30563;&#26041;&#27861;&#65292;&#21482;&#20381;&#36182;&#20110;&#31867;&#21035;&#21517;&#31216;&#32780;&#19981;&#26159;&#27880;&#37322;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#31034;&#20363;&#65292;&#35299;&#20915;&#24403;&#21069;&#20167;&#24680;&#35328;&#35770;&#35782;&#21035;&#30340;&#30740;&#31350;&#23384;&#22312;&#30340;&#25968;&#25454;&#21019;&#24314;&#31574;&#30053;&#19981;&#31995;&#32479;&#21644;&#19981;&#21516;&#27880;&#37322;&#26041;&#26696;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.02637</link><description>&lt;p&gt;
&#38754;&#21521;&#36328;&#25968;&#25454;&#38598;&#30340;&#24369;&#30417;&#30563;&#20167;&#24680;&#35328;&#35770;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Towards Weakly-Supervised Hate Speech Classification Across Datasets. (arXiv:2305.02637v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02637
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#26497;&#24230;&#24369;&#30340;&#30417;&#30563;&#26041;&#27861;&#65292;&#21482;&#20381;&#36182;&#20110;&#31867;&#21035;&#21517;&#31216;&#32780;&#19981;&#26159;&#27880;&#37322;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#31034;&#20363;&#65292;&#35299;&#20915;&#24403;&#21069;&#20167;&#24680;&#35328;&#35770;&#35782;&#21035;&#30340;&#30740;&#31350;&#23384;&#22312;&#30340;&#25968;&#25454;&#21019;&#24314;&#31574;&#30053;&#19981;&#31995;&#32479;&#21644;&#19981;&#21516;&#27880;&#37322;&#26041;&#26696;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#22810;&#20301;&#23398;&#32773;&#25351;&#20986;&#30340;&#37027;&#26679;&#65292;&#24403;&#21069;&#38024;&#23545;&#20167;&#24680;&#35328;&#35770;&#65288;HS&#65289;&#35782;&#21035;&#30340;&#30740;&#31350;&#29305;&#28857;&#26159;&#19981;&#31995;&#32479;&#30340;&#25968;&#25454;&#21019;&#24314;&#31574;&#30053;&#21644;&#19981;&#21516;&#30340;&#27880;&#37322;&#26041;&#26696;&#12290;&#22240;&#27492;&#65292;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#24448;&#24448;&#23545;&#23427;&#20204;&#26410;&#34987;&#35757;&#32451;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#27867;&#21270;&#24615;&#33021;&#24046;&#65292;&#24182;&#19988;&#19981;&#21516;HS&#20998;&#31867;&#27861;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#25152;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#26080;&#27861;&#27604;&#36739;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26497;&#24230;&#24369;&#30340;&#30417;&#30563;&#26041;&#27861;&#65292;&#21482;&#20381;&#36182;&#20110;&#31867;&#21035;&#21517;&#31216;&#32780;&#19981;&#26159;&#27880;&#37322;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#31034;&#20363;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#20869;&#21644;&#36328;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;HS&#20998;&#31867;&#27169;&#22411;&#36890;&#29992;&#24615;&#36739;&#24046;&#30340;&#21407;&#22240;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
As pointed out by several scholars, current research on hate speech (HS) recognition is characterized by unsystematic data creation strategies and diverging annotation schemata. Subsequently, supervised-learning models tend to generalize poorly to datasets they were not trained on, and the performance of the models trained on datasets labeled using different HS taxonomies cannot be compared. To ease this problem, we propose applying extremely weak supervision that only relies on the class name rather than on class samples from the annotated data. We demonstrate the effectiveness of a state-of-the-art weakly-supervised text classification model in various in-dataset and cross-dataset settings. Furthermore, we conduct an in-depth quantitative and qualitative analysis of the source of poor generalizability of HS classification models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#27169;&#24335;&#20026;&#23548;&#21521;&#30340;&#36127;&#36131;&#20219;AI-by-design&#21442;&#32771;&#26550;&#26500;&#65292;&#29992;&#20110;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;AI&#31995;&#32479;&#65292;&#37325;&#28857;&#20851;&#27880;&#21487;&#35299;&#37322;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#40065;&#26834;&#24615;&#31561;&#20851;&#38190;&#35774;&#35745;&#20803;&#32032;&#12290;</title><link>http://arxiv.org/abs/2304.11090</link><description>&lt;p&gt;
&#22312;ChatGPT&#26102;&#20195;&#36808;&#21521;&#36127;&#36131;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#29992;&#20110;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;AI&#31995;&#32479;&#30340;&#21442;&#32771;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Towards Responsible AI in the Era of ChatGPT: A Reference Architecture for Designing Foundation Model-based AI Systems. (arXiv:2304.11090v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#27169;&#24335;&#20026;&#23548;&#21521;&#30340;&#36127;&#36131;&#20219;AI-by-design&#21442;&#32771;&#26550;&#26500;&#65292;&#29992;&#20110;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;AI&#31995;&#32479;&#65292;&#37325;&#28857;&#20851;&#27880;&#21487;&#35299;&#37322;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#40065;&#26834;&#24615;&#31561;&#20851;&#38190;&#35774;&#35745;&#20803;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#12289;Bard&#21644;&#20854;&#20182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#25512;&#20986;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#24341;&#36215;&#20102;&#24040;&#22823;&#20851;&#27880;&#12290;&#22522;&#30784;&#27169;&#22411;&#23558;&#25104;&#20026;&#26410;&#26469;&#22823;&#22810;&#25968;AI&#31995;&#32479;&#30340;&#22522;&#30784;&#26500;&#24314;&#22359;&#30340;&#36235;&#21183;&#27491;&#22312;&#22686;&#38271;&#12290;&#28982;&#32780;&#65292;&#23558;&#22522;&#30784;&#27169;&#22411;&#32435;&#20837;AI&#31995;&#32479;&#24341;&#21457;&#20102;&#23545;&#36127;&#36131;&#20219;AI&#30340;&#37325;&#22823;&#20851;&#27880;&#65292;&#36825;&#26159;&#30001;&#20110;&#20854;&#40657;&#21283;&#23376;&#24615;&#36136;&#21644;&#24555;&#36895;&#21457;&#23637;&#30340;&#36229;&#32423;&#26234;&#33021;&#24341;&#36215;&#30340;&#12290;&#27492;&#22806;&#65292;&#22522;&#30784;&#27169;&#22411;&#30340;&#22686;&#38271;&#33021;&#21147;&#26368;&#32456;&#21487;&#33021;&#20250;&#21534;&#22124;AI&#31995;&#32479;&#30340;&#20854;&#20182;&#32452;&#20214;&#65292;&#24341;&#20837;&#26550;&#26500;&#35774;&#35745;&#20013;&#30340;&#36816;&#21160;&#36793;&#30028;&#21644;&#25509;&#21475;&#28436;&#21464;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#27169;&#24335;&#20026;&#23548;&#21521;&#30340;&#36127;&#36131;&#20219;AI-by-design&#21442;&#32771;&#26550;&#26500;&#65292;&#29992;&#20110;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;AI&#31995;&#32479;&#12290;&#29305;&#21035;&#22320;&#65292;&#26412;&#25991;&#39318;&#20808;&#21576;&#29616;&#20102;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;AI&#31995;&#32479;&#22312;&#26550;&#26500;&#28436;&#36827;&#26041;&#38754;&#30340;&#21457;&#23637;&#65292;&#20174;"&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#36830;&#25509;&#22120;"&#21040;"&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#21333;&#29255;&#26426;&#26680;"&#12290;&#28982;&#21518;&#65292;&#23427;&#25552;&#20986;&#20102;&#19968;&#20010;&#21442;&#32771;&#26550;&#26500;&#65292;&#21253;&#25324;&#20116;&#20010;&#31867;&#21035;&#30340;&#27169;&#24335;&#65292;&#37325;&#28857;&#20851;&#27880;&#20851;&#38190;&#35774;&#35745;&#20803;&#32032;&#65292;&#20363;&#22914;&#21487;&#35299;&#37322;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#21442;&#32771;&#26550;&#26500;&#20026;&#35774;&#35745;&#36127;&#36131;&#20219;&#30340;&#22522;&#30784;&#27169;&#22411;&#30340;AI&#31995;&#32479;&#25552;&#20379;&#20102;&#31995;&#32479;&#21270;&#21644;&#36879;&#26126;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The release of ChatGPT, Bard, and other large language model (LLM)-based chatbots has drawn huge attention on foundations models worldwide. There is a growing trend that foundation models will serve as the fundamental building blocks for most of the future AI systems. However, incorporating foundation models in AI systems raises significant concerns about responsible AI due to their black box nature and rapidly advancing super-intelligence. Additionally, the foundation model's growing capabilities can eventually absorb the other components of AI systems, introducing the moving boundary and interface evolution challenges in architecture design. To address these challenges, this paper proposes a pattern-oriented responsible-AI-by-design reference architecture for designing foundation model-based AI systems. Specially, the paper first presents an architecture evolution of AI systems in the era of foundation models, from "foundation-model-as-a-connector" to "foundation-model-as-a-monolithi
&lt;/p&gt;</description></item><item><title>GesGPT &#26159;&#19968;&#31181;&#26032;&#30340;&#35821;&#38899;&#25163;&#21183;&#29983;&#25104;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; GPT &#30340;&#35821;&#20041;&#20998;&#26512;&#33021;&#21147;&#65292;&#20174;&#25991;&#26412;&#36755;&#20837;&#20013;&#25552;&#21462;&#19982;&#25163;&#21183;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#25163;&#21183;&#24211;&#21644;&#38598;&#25104;&#27169;&#22359;&#29983;&#25104;&#24847;&#20041;&#20016;&#23500;&#30340;&#35821;&#38899;&#25163;&#21183;&#12290;</title><link>http://arxiv.org/abs/2303.13013</link><description>&lt;p&gt;
GesGPT: &#22522;&#20110; GPT &#25991;&#26412;&#35299;&#26512;&#30340;&#35821;&#38899;&#25163;&#21183;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
GesGPT: Speech Gesture Synthesis With Text Parsing from GPT. (arXiv:2303.13013v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13013
&lt;/p&gt;
&lt;p&gt;
GesGPT &#26159;&#19968;&#31181;&#26032;&#30340;&#35821;&#38899;&#25163;&#21183;&#29983;&#25104;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; GPT &#30340;&#35821;&#20041;&#20998;&#26512;&#33021;&#21147;&#65292;&#20174;&#25991;&#26412;&#36755;&#20837;&#20013;&#25552;&#21462;&#19982;&#25163;&#21183;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#25163;&#21183;&#24211;&#21644;&#38598;&#25104;&#27169;&#22359;&#29983;&#25104;&#24847;&#20041;&#20016;&#23500;&#30340;&#35821;&#38899;&#25163;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#21183;&#21512;&#25104;&#20316;&#20026;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#29983;&#25104;&#19978;&#19979;&#25991;&#30456;&#20851;&#21644;&#33258;&#28982;&#30340;&#25163;&#21183;&#26469;&#23545;&#24212;&#35821;&#38899;&#25110;&#25991;&#26412;&#36755;&#20837;&#12290;&#23613;&#31649;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#24573;&#30053;&#20102;&#25991;&#26412;&#20013;&#20016;&#23500;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#23548;&#33268;&#25163;&#21183;&#19981;&#22815;&#34920;&#36798;&#21644;&#24847;&#20041;&#19981;&#22815;&#20016;&#23500;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; GesGPT&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22914; GPT &#30340;&#35821;&#20041;&#20998;&#26512;&#33021;&#21147;&#36827;&#34892;&#25163;&#21183;&#29983;&#25104;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992; LLM &#22312;&#25991;&#26412;&#20998;&#26512;&#26041;&#38754;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#25105;&#20204;&#35774;&#35745;&#25552;&#31034;&#26469;&#20174;&#25991;&#26412;&#36755;&#20837;&#20013;&#25552;&#21462;&#19982;&#25163;&#21183;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#24320;&#21457;&#25552;&#31034;&#21407;&#21017;&#65292;&#23558;&#25163;&#21183;&#29983;&#25104;&#36716;&#21270;&#20026;&#22522;&#20110; GPT &#30340;&#24847;&#22270;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#31934;&#24515;&#31574;&#21010;&#30340;&#25163;&#21183;&#24211;&#21644;&#38598;&#25104;&#27169;&#22359;&#29983;&#25104;&#35821;&#20041;&#20016;&#23500;&#30340;&#35821;&#38899;&#25163;&#21183;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GesGPT &#21487;&#20197;&#26377;&#25928;&#22320;&#38024;&#23545;&#35821;&#38899;&#25110;&#25991;&#26412;&#36755;&#20837;&#29983;&#25104;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#12289;&#26377;&#24847;&#20041;&#30340;&#25163;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gesture synthesis has gained significant attention as a critical research area, focusing on producing contextually appropriate and natural gestures corresponding to speech or textual input. Although deep learning-based approaches have achieved remarkable progress, they often overlook the rich semantic information present in the text, leading to less expressive and meaningful gestures. We propose GesGPT, a novel approach to gesture generation that leverages the semantic analysis capabilities of Large Language Models (LLMs), such as GPT. By capitalizing on the strengths of LLMs for text analysis, we design prompts to extract gesture-related information from textual input. Our method entails developing prompt principles that transform gesture generation into an intention classification problem based on GPT, and utilizing a curated gesture library and integration module to produce semantically rich co-speech gestures. Experimental results demonstrate that GesGPT effectively generates conte
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21270;&#30340;Switch Transformer&#26694;&#26550;&#65292;&#24182;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#65292;&#21462;&#24471;&#20102;&#22312;&#23567;&#22411;&#27861;&#35821;&#20020;&#24202;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#27604;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#37319;&#29992;Switch Transformer&#30340;&#19987;&#23478;&#28151;&#21512;&#26426;&#21046;&#26377;&#21161;&#20110;&#25552;&#39640;&#35782;&#21035;&#20934;&#30830;&#24230;&#65292;&#26368;&#32456;&#22312;&#27979;&#35797;&#38598;&#19978;&#23454;&#29616;&#20102;87&#65285;&#30340;&#20934;&#30830;&#29575;&#12289;87&#65285;&#30340;&#31934;&#24230;&#21644;86&#65285;&#30340;&#21484;&#22238;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.12892</link><description>&lt;p&gt;
&#29992;&#20110;&#20020;&#24202;&#21465;&#36848;&#20998;&#31867;&#30340;&#23567;&#35268;&#27169;&#20132;&#25442;&#21464;&#21387;&#22120;&#21644;&#22522;&#20110;NLP&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Small-Scale Switch Transformer and NLP-based Model for Clinical Narratives Classification. (arXiv:2303.12892v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21270;&#30340;Switch Transformer&#26694;&#26550;&#65292;&#24182;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#65292;&#21462;&#24471;&#20102;&#22312;&#23567;&#22411;&#27861;&#35821;&#20020;&#24202;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#27604;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#37319;&#29992;Switch Transformer&#30340;&#19987;&#23478;&#28151;&#21512;&#26426;&#21046;&#26377;&#21161;&#20110;&#25552;&#39640;&#35782;&#21035;&#20934;&#30830;&#24230;&#65292;&#26368;&#32456;&#22312;&#27979;&#35797;&#38598;&#19978;&#23454;&#29616;&#20102;87&#65285;&#30340;&#20934;&#30830;&#29575;&#12289;87&#65285;&#30340;&#31934;&#24230;&#21644;86&#65285;&#30340;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#27169;&#22411;&#65288;&#22914;&#20132;&#25442;&#21464;&#21387;&#22120;&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#36807;&#20110;&#22797;&#26434;&#24182;&#38656;&#35201;&#22823;&#37327;&#30340;&#39044;&#35757;&#32451;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#26377;&#38480;&#25968;&#25454;&#30340;&#23567;&#22411;&#20020;&#24202;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21270;&#30340;Switch Transformer&#26694;&#26550;&#65292;&#24182;&#20174;&#22836;&#24320;&#22987;&#22312;CHU Sainte-Justine&#21307;&#38498;&#30340;&#23567;&#22411;&#27861;&#35821;&#20020;&#24202;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#31616;&#21270;&#30340;&#23567;&#35268;&#27169;&#21464;&#21387;&#22120;&#27169;&#22411;&#20248;&#20110;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#65292;&#21253;&#25324;DistillBERT&#12289;CamemBERT&#12289;FlauBERT&#21644;FrALBERT&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;Switch Transformer&#30340;&#19987;&#23478;&#28151;&#21512;&#26426;&#21046;&#26377;&#21161;&#20110;&#25429;&#33719;&#22810;&#26679;&#30340;&#27169;&#24335;&#65307;&#22240;&#27492;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#27604;&#20855;&#26377;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#30340;&#20256;&#32479;&#21464;&#21387;&#22120;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#27979;&#35797;&#38598;&#19978;&#23454;&#29616;&#20102;87&#65285;&#30340;&#20934;&#30830;&#29575;&#65292;87&#65285;&#30340;&#31934;&#24230;&#21644;86&#65285;&#30340;&#21484;&#22238;&#29575;&#65292;&#31361;&#26174;&#20102;&#20854;&#22312;&#23567;&#22411;&#20020;&#24202;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Transformer-based models such as the Switch Transformer have achieved remarkable results in natural language processing tasks. However, these models are often too complex and require extensive pre-training, which limits their effectiveness for small clinical text classification tasks with limited data. In this study, we propose a simplified Switch Transformer framework and train it from scratch on a small French clinical text classification dataset at CHU Sainte-Justine hospital. Our results demonstrate that the simplified small-scale Transformer models outperform pre-trained BERT-based models, including DistillBERT, CamemBERT, FlauBERT, and FrALBERT. Additionally, using a mixture of expert mechanisms from the Switch Transformer helps capture diverse patterns; hence, the proposed approach achieves better results than a conventional Transformer with the self-attention mechanism. Finally, our proposed framework achieves an accuracy of 87\%, precision at 87\%, and recall 
&lt;/p&gt;</description></item></channel></rss>