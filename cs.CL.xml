<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>SafeguardGPT&#26694;&#26550;&#20351;&#29992;&#24515;&#29702;&#27835;&#30103;&#26469;&#32416;&#27491;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#30340;&#26377;&#23475;&#34892;&#20026;&#65292;&#25913;&#36827;&#19982;&#20154;&#31867;&#30340;&#23545;&#35805;&#36136;&#37327;&#65292;&#36827;&#32780;&#25512;&#36827;&#20581;&#24247;AI&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2304.00416</link><description>&lt;p&gt;
&#36808;&#21521;&#20581;&#24247;AI&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20063;&#38656;&#35201;&#27835;&#30103;&#24072;
&lt;/p&gt;
&lt;p&gt;
Towards Healthy AI: Large Language Models Need Therapists Too. (arXiv:2304.00416v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00416
&lt;/p&gt;
&lt;p&gt;
SafeguardGPT&#26694;&#26550;&#20351;&#29992;&#24515;&#29702;&#27835;&#30103;&#26469;&#32416;&#27491;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#30340;&#26377;&#23475;&#34892;&#20026;&#65292;&#25913;&#36827;&#19982;&#20154;&#31867;&#30340;&#23545;&#35805;&#36136;&#37327;&#65292;&#36827;&#32780;&#25512;&#36827;&#20581;&#24247;AI&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#30340;&#36827;&#23637;&#24102;&#26469;&#20102;&#21151;&#33021;&#24378;&#22823;&#30340; AI &#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#33021;&#22815;&#21442;&#19982;&#33258;&#28982;&#19988;&#31867;&#20284;&#20154;&#31867;&#30340;&#23545;&#35805;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32842;&#22825;&#26426;&#22120;&#20154;&#21487;&#33021;&#20855;&#26377;&#28508;&#22312;&#30340;&#21361;&#23475;&#24615;&#65292;&#34920;&#29616;&#20986;&#25805;&#32437;&#12289;&#28748;&#36755;&#34394;&#20551;&#35266;&#24565;&#21644;&#33258;&#24651;&#34892;&#20026;&#12290;&#25105;&#20204;&#23450;&#20041;&#20581;&#24247;AI&#20026;&#23433;&#20840;&#12289;&#21487;&#20449;&#21644;&#36947;&#24503;&#30340;AI&#12290;&#20026;&#20102;&#21019;&#36896;&#20581;&#24247;&#30340;AI&#31995;&#32479;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SafeguardGPT&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#24515;&#29702;&#27835;&#30103;&#26469;&#32416;&#27491;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#30340;&#36825;&#20123;&#26377;&#23475;&#34892;&#20026;&#12290;&#35813;&#26694;&#26550;&#28041;&#21450;&#22235;&#31181;&#31867;&#22411;&#30340;AI&#20195;&#29702;&#65306;&#32842;&#22825;&#26426;&#22120;&#20154;&#12289;"&#29992;&#25143;"&#12289;"&#27835;&#30103;&#24072;"&#21644;"&#35780;&#35770;&#23478;"&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#31038;&#20132;&#23545;&#35805;&#30340;&#24037;&#20316;&#31034;&#20363;&#23637;&#31034;&#20102;SafeguardGPT&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#25913;&#36827;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#20043;&#38388;&#30340;&#23545;&#35805;&#36136;&#37327;&#12290;&#34429;&#28982;&#26410;&#26469;&#20173;&#38656;&#35299;&#20915;&#20960;&#20010;&#25361;&#25112;&#21644;&#26041;&#21521;&#65292;&#20294;SafeguardGPT&#20026;&#25913;&#21892;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20043;&#38388;&#30340;&#21327;&#35843;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in large language models (LLMs) have led to the development of powerful AI chatbots capable of engaging in natural and human-like conversations. However, these chatbots can be potentially harmful, exhibiting manipulative, gaslighting, and narcissistic behaviors. We define Healthy AI to be safe, trustworthy and ethical. To create healthy AI systems, we present the SafeguardGPT framework that uses psychotherapy to correct for these harmful behaviors in AI chatbots. The framework involves four types of AI agents: a Chatbot, a "User," a "Therapist," and a "Critic." We demonstrate the effectiveness of SafeguardGPT through a working example of simulating a social conversation. Our results show that the framework can improve the quality of conversations between AI chatbots and humans. Although there are still several challenges and directions to be addressed in the future, SafeguardGPT provides a promising approach to improving the alignment between AI chatbots and human value
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#35199;&#29677;&#29273;&#40644;&#37329;&#26102;&#20195;&#21095;&#22330;&#39046;&#22495;&#20013;&#24212;&#29992;AAA&#25216;&#26415;&#65292;&#36890;&#36807;&#32858;&#31867;&#20998;&#26512;&#21644;&#36317;&#31163;&#24230;&#37327;&#26041;&#27861;&#35777;&#26126;&#20102;Tirso de Molina&#26159;&#20116;&#37096;&#20256;&#32479;&#21916;&#21095;&#30340;&#20316;&#32773;&#12290;</title><link>http://arxiv.org/abs/2304.00363</link><description>&lt;p&gt;
Tirso de Molina&#20316;&#21697;&#20013;&#30340;&#33258;&#21160;&#20316;&#32773;&#24402;&#23646;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Automatic Authorship Attribution in the Work of Tirso de Molina. (arXiv:2304.00363v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#35199;&#29677;&#29273;&#40644;&#37329;&#26102;&#20195;&#21095;&#22330;&#39046;&#22495;&#20013;&#24212;&#29992;AAA&#25216;&#26415;&#65292;&#36890;&#36807;&#32858;&#31867;&#20998;&#26512;&#21644;&#36317;&#31163;&#24230;&#37327;&#26041;&#27861;&#35777;&#26126;&#20102;Tirso de Molina&#26159;&#20116;&#37096;&#20256;&#32479;&#21916;&#21095;&#30340;&#20316;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20316;&#32773;&#24402;&#23646; (AAA) &#26159;&#22312;&#25968;&#23383;&#20154;&#25991;&#23398;&#39046;&#22495;&#24212;&#29992;&#24037;&#20855;&#21644;&#25216;&#26415;&#36827;&#34892;&#20316;&#32773;&#24402;&#23646;&#30740;&#31350;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#25968;&#37327;&#21644;&#32479;&#35745;&#30340;&#26041;&#27861;&#65292;&#36825;&#19968;&#23398;&#31185;&#21487;&#20197;&#23545;&#20256;&#32479;&#25209;&#35780;&#23478;&#22788;&#29702;&#20102;&#20960;&#20010;&#19990;&#32426;&#30340;&#33879;&#21517;&#20316;&#32773;&#38382;&#39064;&#20316;&#20986;&#26356;&#36827;&#19968;&#27493;&#30340;&#32467;&#35770;&#65292;&#25171;&#24320;&#20102;&#23545;&#39118;&#26684;&#27604;&#36739;&#30340;&#26032;&#38376;&#24452;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#27979;&#35797;&#20116;&#37096;&#20256;&#32479;&#24402;&#23646;&#20110;&#35199;&#29677;&#29273;&#21095;&#20316;&#23478;Tirso de Molina (1579-1648) &#30340;&#21916;&#21095;&#12298;La ninfa del cielo&#12299;&#12289;&#12298;El burlador de Sevilla&#12299;&#12289;&#12298;Tan largo me lo fiais&#12299;&#12289;&#12298;La mujer por fuerza&#12299;&#21644;&#12298;El condenado por desconfiado&#12299;&#30340;&#20316;&#32773;&#36523;&#20221;&#65292;&#35777;&#26126;&#36825;&#20123;&#24037;&#20855;&#21644;&#25216;&#26415;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#30446;&#30340;&#65292;&#23545;&#30001;Tirso de Molina&#12289;Andres de Claramonte (c. 1560-1626)&#12289;Antonio Mira de Amescua (1577-1644)&#21644;Luis Velez de Guevara (1579-1644)&#30340;&#21095;&#26412;&#26500;&#25104;&#30340;&#35821;&#26009;&#24211;&#36827;&#34892;&#20102;&#19968;&#20123;&#32858;&#31867;&#20998;&#26512;&#30340;&#23454;&#39564;&#65292;&#20351;&#29992;&#20102;R&#35821;&#35328;&#20013;&#30340;Stylo&#21253;&#21644;&#22235;&#31181;&#36317;&#31163;&#24230;&#37327;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;AAA&#22312;&#35199;&#29677;&#29273;&#40644;&#37329;&#26102;&#20195;&#21095;&#38498;&#30340;&#20316;&#32773;&#24402;&#23646;&#30740;&#31350;&#20013;&#20855;&#26377;&#28508;&#21147;&#65292;&#35777;&#26126;&#20102;Tirso de Molina&#22312;&#20116;&#37096;&#20998;&#26512;&#20013;&#30340;&#20316;&#32773;&#36523;&#20221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic Authorship Attribution (AAA) is the result of applying tools and techniques from Digital Humanities to authorship attribution studies. Through a quantitative and statistical approach this discipline can draw further conclusions about renowned authorship issues which traditional critics have been dealing with for centuries, opening a new door to style comparison. The aim of this paper is to prove the potential of these tools and techniques by testing the authorship of five comedies traditionally attributed to Spanish playwright Tirso de Molina (1579-1648): La ninfa del cielo, El burlador de Sevilla, Tan largo me lo fiais, La mujer por fuerza and El condenado por desconfiado. To accomplish this purpose some experiments concerning clustering analysis by Stylo package from R and four distance measures are carried out on a corpus built with plays by Tirso, Andres de Claramonte (c. 1560-1626), Antonio Mira de Amescua (1577-1644) and Luis Velez de Guevara (1579-1644). The results ob
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#21019;&#24314;&#22823;&#35268;&#27169;&#24320;&#25918;&#39046;&#22495;&#35282;&#33394;&#25198;&#28436;&#23545;&#35805;&#35821;&#26009;&#24211;&#30340;&#22256;&#38590;&#65292;&#24182;&#35299;&#20915;&#20102;&#20247;&#21253;&#21644;&#38544;&#31169;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.00350</link><description>&lt;p&gt;
&#24403;&#20247;&#21253;&#36935;&#21040;&#35282;&#33394;&#25198;&#28436;&#65306;&#21019;&#24314;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#24320;&#25918;&#39046;&#22495;&#35282;&#33394;&#25198;&#28436;&#23545;&#35805;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
When Crowd Meets Persona: Creating a Large-Scale Open-Domain Persona Dialogue Corpus. (arXiv:2304.00350v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00350
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#21019;&#24314;&#22823;&#35268;&#27169;&#24320;&#25918;&#39046;&#22495;&#35282;&#33394;&#25198;&#28436;&#23545;&#35805;&#35821;&#26009;&#24211;&#30340;&#22256;&#38590;&#65292;&#24182;&#35299;&#20915;&#20102;&#20247;&#21253;&#21644;&#38544;&#31169;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#38598;&#38656;&#35201;&#23567;&#24515;&#65292;&#22240;&#20026;&#21333;&#35789;&#30340;&#35821;&#20041;&#23481;&#26131;&#21463;&#21040;&#24494;&#22937;&#30340;&#25991;&#26412;&#21464;&#21270;&#25110;&#34987;&#27880;&#37322;&#27010;&#24565;&#30340;&#23450;&#20041;&#24433;&#21709;&#12290;&#36825;&#31181;&#36235;&#21183;&#21487;&#20197;&#22312;&#29983;&#25104;&#20219;&#21153;&#65288;&#20363;&#22914;&#38382;&#31572;&#21644;&#23545;&#35805;&#29983;&#25104;&#65289;&#20197;&#21450;&#21019;&#24314;&#22522;&#20110;&#20998;&#31867;&#30340;&#35821;&#26009;&#24211;&#65288;&#20363;&#22914;&#20027;&#39064;&#20998;&#31867;&#25110;&#24773;&#24863;&#20998;&#26512;&#65289;&#20013;&#30475;&#21040;&#12290;&#24320;&#25918;&#39046;&#22495;&#20250;&#35805;&#28041;&#21450;&#20004;&#20010;&#25110;&#22810;&#20010;&#20247;&#21253;&#24037;&#20154;&#33258;&#30001;&#35848;&#35770;&#20219;&#20309;&#20027;&#39064;&#65292;&#25910;&#38598;&#36825;&#31181;&#25968;&#25454;&#23588;&#20854;&#22256;&#38590;&#65292;&#21407;&#22240;&#26159;&#65306;1&#65289;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#65292;&#25968;&#25454;&#38598;&#24212;&#35813;&#8220;&#21019;&#36896;&#8221;&#32780;&#19981;&#26159;&#8220;&#33719;&#24471;&#8221;&#65307;2&#65289;&#26377;&#20607;&#21019;&#24314;&#36825;&#31867;&#23545;&#35805;&#21487;&#33021;&#19982;&#20247;&#21253;&#24037;&#20316;&#20154;&#21592;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#34892;&#20026;&#19981;&#21516;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#21019;&#24314;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#24320;&#25918;&#39046;&#22495;&#35282;&#33394;&#25198;&#28436;&#23545;&#35805;&#35821;&#26009;&#24211;&#26102;&#30340;&#36825;&#20123;&#38382;&#39064;&#65292;&#20854;&#20013;&#8220;&#35282;&#33394;&#25198;&#28436;&#8221;&#24847;&#21619;&#30528;&#23545;&#35805;&#30001;&#22266;&#23450;&#35282;&#33394;&#30340;&#22810;&#20010;&#28436;&#21592;&#21644;&#26469;&#33258;&#26410;&#25351;&#23450;&#30340;&#32676;&#20247;&#30340;&#29992;&#25143;&#31471;&#24037;&#20316;&#32773;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building a natural language dataset requires caution since word semantics is vulnerable to subtle text change or the definition of the annotated concept. Such a tendency can be seen in generative tasks like question-answering and dialogue generation and also in tasks that create a categorization-based corpus, like topic classification or sentiment analysis. Open-domain conversations involve two or more crowdworkers freely conversing about any topic, and collecting such data is particularly difficult for two reasons: 1) the dataset should be ``crafted" rather than ``obtained" due to privacy concerns, and 2) paid creation of such dialogues may differ from how crowdworkers behave in real-world settings. In this study, we tackle these issues when creating a large-scale open-domain persona dialogue corpus, where persona implies that the conversation is performed by several actors with a fixed persona and user-side workers from an unspecified crowd.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#25910;&#38598;&#20102;1999&#24180;&#33267;2019&#24180;&#26399;&#38388;&#22312;&#21360;&#24230;&#35758;&#20250;&#19979;&#35758;&#38498;&#38382;&#39064;&#26102;&#38388;&#20013;&#35752;&#35770;&#30340;&#38382;&#39064;&#21644;&#31572;&#26696;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#25506;&#31350;&#24615;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#26174;&#31034;&#35758;&#20250;&#23545;&#35805;&#21453;&#26144;&#20102;&#19981;&#21516;&#26102;&#26399;&#30340;&#25919;&#27835;&#21644;&#31038;&#20250;&#32463;&#27982;&#32039;&#24352;&#23616;&#21183;&#12290;</title><link>http://arxiv.org/abs/2304.00235</link><description>&lt;p&gt;
&#21360;&#24230;&#35758;&#20250;&#35752;&#35770;&#20102;&#20160;&#20040;&#65311;Lok Sabha&#20013;&#38382;&#39064;&#26102;&#38388;&#30340;&#25506;&#32034;&#24615;&#20998;&#26512;&#12290;(arXiv:2304.00235v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
What Does the Indian Parliament Discuss? An Exploratory Analysis of the Question Hour in the Lok Sabha. (arXiv:2304.00235v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#25910;&#38598;&#20102;1999&#24180;&#33267;2019&#24180;&#26399;&#38388;&#22312;&#21360;&#24230;&#35758;&#20250;&#19979;&#35758;&#38498;&#38382;&#39064;&#26102;&#38388;&#20013;&#35752;&#35770;&#30340;&#38382;&#39064;&#21644;&#31572;&#26696;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#25506;&#31350;&#24615;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#26174;&#31034;&#35758;&#20250;&#23545;&#35805;&#21453;&#26144;&#20102;&#19981;&#21516;&#26102;&#26399;&#30340;&#25919;&#27835;&#21644;&#31038;&#20250;&#32463;&#27982;&#32039;&#24352;&#23616;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
TCPD-IPD&#25968;&#25454;&#38598;&#26159;&#25910;&#38598;&#20102;1999&#24180;&#33267;2019&#24180;&#26399;&#38388;&#22312;&#21360;&#24230;&#35758;&#20250;&#19979;&#35758;&#38498;&#38382;&#39064;&#26102;&#38388;&#20013;&#35752;&#35770;&#30340;&#38382;&#39064;&#21644;&#31572;&#26696;&#12290;&#34429;&#28982;&#25163;&#21160;&#20998;&#26512;&#36825;&#20040;&#24222;&#22823;&#30340;&#25968;&#25454;&#38598;&#24456;&#22256;&#38590;&#65292;&#20294;&#29616;&#20195;&#25991;&#26412;&#20998;&#26512;&#24037;&#20855;&#21487;&#20197;&#25552;&#20379;&#26377;&#25928;&#30340;&#23548;&#33322;&#25163;&#27573;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#25506;&#31350;&#24615;&#20998;&#26512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#35265;&#22320;&#30340;&#35821;&#26009;&#24211;&#27700;&#24179;&#32479;&#35745;&#25968;&#25454;&#21644;&#23545;&#25968;&#25454;&#38598;&#30340;&#19977;&#20010;&#23376;&#38598;&#36827;&#34892;&#30340;&#35814;&#32454;&#20998;&#26512;&#12290; &#22312;&#21518;&#19968;&#39033;&#20998;&#26512;&#20013;&#65292;&#37325;&#28857;&#26159;&#20351;&#29992;&#21160;&#24577;&#20027;&#39064;&#27169;&#22411;&#29702;&#35299;&#35805;&#39064;&#30340;&#26102;&#38388;&#28436;&#21464;&#12290; &#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#35758;&#20250;&#23545;&#35805;&#30830;&#23454;&#21453;&#26144;&#20102;&#27599;&#20010;&#26102;&#26399;&#30340;&#25919;&#27835;&#21644;&#31038;&#20250;&#32463;&#27982;&#32039;&#24352;&#23616;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The TCPD-IPD dataset is a collection of questions and answers discussed in the Lower House of the Parliament of India during the Question Hour between 1999 and 2019. Although it is difficult to analyze such a huge collection manually, modern text analysis tools can provide a powerful means to navigate it. In this paper, we perform an exploratory analysis of the dataset. In particular, we present insightful corpus-level statistics and a detailed analysis of three subsets of the dataset. In the latter analysis, the focus is on understanding the temporal evolution of topics using a dynamic topic model. We observe that the parliamentary conversation indeed mirrors the political and socio-economic tensions of each period.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;Transformer&#30340;&#26041;&#27861;&#65292;&#21363;REPORT&#65292;&#33021;&#22815;&#21516;&#26102;&#32858;&#21512;&#20851;&#31995;&#36335;&#24452;&#21644;&#19978;&#19979;&#25991;&#65292;&#25429;&#25417;&#23454;&#20307;&#20043;&#38388;&#30340;&#32852;&#31995;&#21644;&#20869;&#22312;&#29305;&#24615;&#12290;&#23427;&#23436;&#20840;&#20381;&#36182;&#20110;&#20851;&#31995;&#35821;&#20041;&#65292;&#24182;&#33021;&#33258;&#28982;&#22320;&#25512;&#24191;&#21040;&#23436;&#20840;&#24402;&#32435;&#30340;&#35774;&#32622;&#20013;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.00215</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#23618;Transformer&#30340;&#20851;&#31995;&#36335;&#24452;&#21644;&#19978;&#19979;&#25991;&#24402;&#32435;&#20851;&#31995;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Inductive Relation Prediction from Relational Paths and Context with Hierarchical Transformers. (arXiv:2304.00215v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;Transformer&#30340;&#26041;&#27861;&#65292;&#21363;REPORT&#65292;&#33021;&#22815;&#21516;&#26102;&#32858;&#21512;&#20851;&#31995;&#36335;&#24452;&#21644;&#19978;&#19979;&#25991;&#65292;&#25429;&#25417;&#23454;&#20307;&#20043;&#38388;&#30340;&#32852;&#31995;&#21644;&#20869;&#22312;&#29305;&#24615;&#12290;&#23427;&#23436;&#20840;&#20381;&#36182;&#20110;&#20851;&#31995;&#35821;&#20041;&#65292;&#24182;&#33021;&#33258;&#28982;&#22320;&#25512;&#24191;&#21040;&#23436;&#20840;&#24402;&#32435;&#30340;&#35774;&#32622;&#20013;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30693;&#35782;&#22270;&#35889;&#20013;&#36827;&#34892;&#20851;&#31995;&#39044;&#27979;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#29616;&#26377;&#30340;&#23884;&#20837;&#24335;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#36716;&#23548;&#35774;&#32622;&#65292;&#32570;&#20047;&#24402;&#32435;&#33021;&#21147;&#65292;&#26080;&#27861;&#25512;&#24191;&#21040;&#26032;&#30340;&#23454;&#20307;&#19978;&#36827;&#34892;&#25512;&#29702;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32479;&#19968;&#30340;&#20998;&#23618;Transformer&#26694;&#26550;&#65292;&#21363;REPORT&#65292;&#21516;&#26102;&#32858;&#21512;&#20851;&#31995;&#36335;&#24452;&#21644;&#19978;&#19979;&#25991;&#65292;&#25429;&#25417;&#23454;&#20307;&#20043;&#38388;&#30340;&#32852;&#31995;&#21644;&#20869;&#22312;&#29305;&#24615;&#65292;&#36825;&#31181;&#26041;&#27861;&#23436;&#20840;&#20381;&#36182;&#20110;&#20851;&#31995;&#35821;&#20041;&#65292;&#24182;&#33021;&#33258;&#28982;&#22320;&#25512;&#24191;&#21040;&#23436;&#20840;&#24402;&#32435;&#30340;&#35774;&#32622;&#20013;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;REPORT&#34920;&#29616;&#20248;&#20110;&#25152;&#26377;&#22522;&#32447;&#26041;&#27861;&#65292;&#29978;&#33267;&#22312;&#20004;&#20010;&#23436;&#20840;&#24402;&#32435;&#30340;&#25968;&#25454;&#38598;&#30340;&#20843;&#20010;&#29256;&#26412;&#23376;&#38598;&#19978;&#20063;&#26159;&#22914;&#27492;&#12290;&#27492;&#22806;&#65292;REPORT&#33021;&#22815;&#23558;&#25512;&#29702;&#25512;&#24191;&#21040;&#35757;&#32451;&#21644;&#25512;&#29702;&#20013;&#27809;&#26377;&#20844;&#20849;&#23454;&#20307;&#30340;&#26032;&#23454;&#20307;&#19978;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation prediction on knowledge graphs (KGs) is a key research topic. Dominant embedding-based methods mainly focus on the transductive setting and lack the inductive ability to generalize to new entities for inference. Existing methods for inductive reasoning mostly mine the connections between entities, i.e., relational paths, without considering the nature of head and tail entities contained in the relational context. This paper proposes a novel method that captures both connections between entities and the intrinsic nature of entities, by simultaneously aggregating RElational Paths and cOntext with a unified hieRarchical Transformer framework, namely REPORT. REPORT relies solely on relation semantics and can naturally generalize to the fully-inductive setting, where KGs for training and inference have no common entities. In the experiments, REPORT performs consistently better than all baselines on almost all the eight version subsets of two fully-inductive datasets. Moreover. REPO
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#31070;&#32463;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#26469;&#33258;&#22810;&#20010;&#28192;&#36947;&#30340;&#21382;&#21490;&#23545;&#35805;&#21644;&#20505;&#36873;&#26469;&#28304;&#19978;&#19979;&#25991;&#20449;&#24687;&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#20197;&#25913;&#21892;&#22810;&#36718;&#23545;&#35805;&#21709;&#24212;&#25490;&#24207;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.00180</link><description>&lt;p&gt;
FCC: &#22312;&#23545;&#35805;&#31995;&#32479;&#20013;&#34701;&#21512;&#21382;&#21490;&#23545;&#35805;&#21644;&#20505;&#36873;&#26469;&#28304;&#36827;&#34892;&#19978;&#19979;&#25991;&#21709;&#24212;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
FCC: Fusing Conversation History and Candidate Provenance for Contextual Response Ranking in Dialogue Systems. (arXiv:2304.00180v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#31070;&#32463;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#26469;&#33258;&#22810;&#20010;&#28192;&#36947;&#30340;&#21382;&#21490;&#23545;&#35805;&#21644;&#20505;&#36873;&#26469;&#28304;&#19978;&#19979;&#25991;&#20449;&#24687;&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#20197;&#25913;&#21892;&#22810;&#36718;&#23545;&#35805;&#21709;&#24212;&#25490;&#24207;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#20013;&#30340;&#21709;&#24212;&#25490;&#24207;&#22312;&#26816;&#32034;&#22411;&#20250;&#35805;&#31995;&#32479;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#65292;&#20026;&#20102;&#25429;&#25417;&#23545;&#35805;&#30340;&#35201;&#28857;&#65292;&#19978;&#19979;&#25991;&#20449;&#24687;&#20316;&#20026;&#37325;&#35201;&#30340;&#30693;&#35782;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#31070;&#32463;&#26694;&#26550;&#65292;&#21487;&#20197;&#25972;&#21512;&#26469;&#33258;&#22810;&#20010;&#28192;&#36947;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#29305;&#21035;&#26159;&#38024;&#23545;&#24403;&#21069;&#20219;&#21153;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#25552;&#20379;&#20004;&#20010;&#20449;&#24687;&#36890;&#36947;&#24182;&#34892;&#65292;&#21363;&#23558;&#19982;&#20505;&#36873;&#26469;&#28304;&#30456;&#20851;&#30340; Conversational History&#65288;&#23545;&#35805;&#21382;&#21490;&#65289;&#21644; Domain Knowledge&#65288;&#39046;&#22495;&#30693;&#35782;&#65289;&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#20316;&#20026;&#19978;&#19979;&#25991;&#20449;&#24687;&#20197;&#25552;&#39640;&#22810;&#36718;&#23545;&#35805;&#21709;&#24212;&#25490;&#24207;&#30340;&#34920;&#29616;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#36890;&#29992;&#20110;&#23558;&#20854;&#20182;&#19978;&#19979;&#25991;&#30446;&#26631;&#20219;&#21153;&#30340;&#21508;&#31181;&#19978;&#19979;&#25991;&#29305;&#24449;&#32435;&#20837;&#27169;&#22359;&#20013;&#12290;&#25105;&#20204;&#22312;&#24191;&#27867;&#29992;&#20110;&#35780;&#20272;&#20250;&#35805;&#21709;&#24212;&#25490;&#21517;&#20219;&#21153;&#30340; MSDialog &#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#26174;&#30528;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Response ranking in dialogues plays a crucial role in retrieval-based conversational systems. In a multi-turn dialogue, to capture the gist of a conversation, contextual information serves as essential knowledge to achieve this goal. In this paper, we present a flexible neural framework that can integrate contextual information from multiple channels. Specifically for the current task, our approach is to provide two information channels in parallel, Fusing Conversation history and domain knowledge extracted from Candidate provenance (FCC), where candidate responses are curated, as contextual information to improve the performance of multi-turn dialogue response ranking. The proposed approach can be generalized as a module to incorporate miscellaneous contextual features for other context-oriented tasks. We evaluate our model on the MSDialog dataset widely used for evaluating conversational response ranking tasks. Our experimental results show that our framework significantly outperform
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;Lego-Features&#30340;&#29305;&#24449;&#65292;&#23558;&#29616;&#26377;&#32534;&#30721;&#34920;&#31034;&#36716;&#25442;&#25104;&#27169;&#22359;&#21270;&#29305;&#24449;&#65292;&#32780;&#19981;&#20462;&#25913;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#27969;&#24335;&#29615;&#22659;&#20013;&#20063;&#36866;&#29992;&#12290;&#36825;&#20123;&#29305;&#24449;&#24615;&#33021;&#24378;&#22823;&#65292;RNN-T&#25110;LAS&#35299;&#30721;&#22120;&#27979;&#35797;&#26102;&#20445;&#25345;&#39640;&#36136;&#37327;&#30340;&#19979;&#28216;&#24615;&#33021;&#65292;&#24182;&#36275;&#22815;&#20016;&#23500;&#65292;&#21487;&#20197;&#20195;&#34920;&#20004;&#20010;&#36890;&#36947;&#20915;&#31574;&#20013;&#30340;&#31532;&#19968;&#36941;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2304.00173</link><description>&lt;p&gt;
Lego-Features&#65306;&#23548;&#20986;&#27169;&#22359;&#21270;&#32534;&#30721;&#22120;&#29305;&#24449;&#29992;&#20110;&#27969;&#24335;&#21644;&#20915;&#31574;ASR
&lt;/p&gt;
&lt;p&gt;
Lego-Features: Exporting modular encoder features for streaming and deliberation ASR. (arXiv:2304.00173v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00173
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;Lego-Features&#30340;&#29305;&#24449;&#65292;&#23558;&#29616;&#26377;&#32534;&#30721;&#34920;&#31034;&#36716;&#25442;&#25104;&#27169;&#22359;&#21270;&#29305;&#24449;&#65292;&#32780;&#19981;&#20462;&#25913;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#27969;&#24335;&#29615;&#22659;&#20013;&#20063;&#36866;&#29992;&#12290;&#36825;&#20123;&#29305;&#24449;&#24615;&#33021;&#24378;&#22823;&#65292;RNN-T&#25110;LAS&#35299;&#30721;&#22120;&#27979;&#35797;&#26102;&#20445;&#25345;&#39640;&#36136;&#37327;&#30340;&#19979;&#28216;&#24615;&#33021;&#65292;&#24182;&#36275;&#22815;&#20016;&#23500;&#65292;&#21487;&#20197;&#20195;&#34920;&#20004;&#20010;&#36890;&#36947;&#20915;&#31574;&#20013;&#30340;&#31532;&#19968;&#36941;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31471;&#21040;&#31471;&#30340;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#20013;&#65292;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20043;&#38388;&#19981;&#21487;&#36991;&#20813;&#22320;&#20986;&#29616;&#20102;&#19968;&#31181;&#32039;&#23494;&#32806;&#21512;&#30340;&#34920;&#36798;&#12290;&#25105;&#20204;&#24314;&#31435;&#22312;&#26368;&#36817;&#24320;&#22987;&#25506;&#32034;&#26500;&#24314;&#24102;&#26377;&#27169;&#22359;&#21270;&#32534;&#30721;&#34920;&#31034;&#30340;&#32534;&#30721;&#22120;&#30340;&#24037;&#20316;&#20043;&#19978;&#65292;&#36825;&#26679;&#19981;&#21516;&#27169;&#22411;&#30340;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#21487;&#20197;&#22312;&#38646;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#25340;&#25509;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#24494;&#35843;&#12290;&#34429;&#28982;&#20043;&#21069;&#30340;&#30740;&#31350;&#21482;&#28041;&#21450;&#20840;&#19978;&#19979;&#25991;&#35821;&#38899;&#27169;&#22411;&#65292;&#20294;&#25105;&#20204;&#20063;&#22312;&#27969;&#24335;&#29615;&#22659;&#20013;&#25506;&#32034;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24314;&#31435;&#22312;&#29616;&#26377;&#32534;&#30721;&#34920;&#31034;&#20043;&#19978;&#65292;&#23558;&#20854;&#36716;&#25442;&#20026;&#27169;&#22359;&#21270;&#29305;&#24449;&#65292;&#31216;&#20026;Lego-Features&#65292;&#32780;&#19981;&#20462;&#25913;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#36825;&#20123;&#29305;&#24449;&#22312;&#29992;&#19981;&#21516;&#30340;&#21021;&#22987;&#21270;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#26102;&#20173;&#28982;&#21487;&#20197;&#30456;&#20114;&#26367;&#25442;&#12290;&#23613;&#31649;&#31232;&#30095;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Lego-Features&#22312;&#20351;&#29992;RNN-T&#25110;LAS&#35299;&#30721;&#22120;&#36827;&#34892;&#27979;&#35797;&#26102;&#20855;&#26377;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#20445;&#25345;&#39640;&#36136;&#37327;&#30340;&#19979;&#28216;&#24615;&#33021;&#12290;&#23427;&#20204;&#20063;&#36275;&#22815;&#20016;&#23500;&#65292;&#21487;&#20197;&#20195;&#34920;&#20004;&#20010;&#36890;&#36947;&#20915;&#31574;&#20013;&#30340;&#31532;&#19968;&#36941;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
In end-to-end (E2E) speech recognition models, a representational tight-coupling inevitably emerges between the encoder and the decoder. We build upon recent work that has begun to explore building encoders with modular encoded representations, such that encoders and decoders from different models can be stitched together in a zero-shot manner without further fine-tuning. While previous research only addresses full-context speech models, we explore the problem in a streaming setting as well. Our framework builds on top of existing encoded representations, converting them to modular features, dubbed as Lego-Features, without modifying the pre-trained model. The features remain interchangeable when the model is retrained with distinct initializations. Though sparse, we show that the Lego-Features are powerful when tested with RNN-T or LAS decoders, maintaining high-quality downstream performance. They are also rich enough to represent the first-pass prediction during two-pass deliberatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20248;&#21270;&#30340;Conformer&#27169;&#22411;&#65292;&#23427;&#22312;&#22823;&#23567;&#12289;&#36895;&#24230;&#21644;FLOPS&#26041;&#38754;&#32463;&#36807;&#20248;&#21270;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35774;&#22791;&#19978;&#20855;&#26377;&#24555;&#36895;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;&#20248;&#21270;&#30340;Conformer&#21487;&#20197;&#36731;&#26494;&#22320;&#24182;&#20837;&#32423;&#32852;&#32534;&#30721;&#22120;&#35774;&#32622;&#65292;&#24182;&#22312;&#26356;&#22810;&#36164;&#28304;&#21487;&#29992;&#26102;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.00171</link><description>&lt;p&gt;
&#23454;&#29992;Conformer&#65306;&#20248;&#21270;&#35774;&#22791;&#21644;&#20113;ASR&#30340;Conformer&#22823;&#23567;&#65292;&#36895;&#24230;&#21644;FLOPS
&lt;/p&gt;
&lt;p&gt;
Practical Conformer: Optimizing size, speed and flops of Conformer for on-Device and cloud ASR. (arXiv:2304.00171v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20248;&#21270;&#30340;Conformer&#27169;&#22411;&#65292;&#23427;&#22312;&#22823;&#23567;&#12289;&#36895;&#24230;&#21644;FLOPS&#26041;&#38754;&#32463;&#36807;&#20248;&#21270;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35774;&#22791;&#19978;&#20855;&#26377;&#24555;&#36895;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;&#20248;&#21270;&#30340;Conformer&#21487;&#20197;&#36731;&#26494;&#22320;&#24182;&#20837;&#32423;&#32852;&#32534;&#30721;&#22120;&#35774;&#32622;&#65292;&#24182;&#22312;&#26356;&#22810;&#36164;&#28304;&#21487;&#29992;&#26102;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Conformer&#27169;&#22411;&#32500;&#25252;&#22823;&#37327;&#30340;&#20869;&#37096;&#29366;&#24577;&#65292;&#20854;&#20013;&#32477;&#22823;&#37096;&#20998;&#19982;&#33258;&#27880;&#24847;&#21147;&#23618;&#30456;&#20851;&#12290;&#22312;&#26377;&#38480;&#30340;&#20869;&#23384;&#24102;&#23485;&#19979;&#65292;&#27599;&#27425;&#25512;&#29702;&#20174;&#20869;&#23384;&#20013;&#35835;&#21462;&#36825;&#20123;&#29366;&#24577;&#21487;&#33021;&#20250;&#20943;&#24930;&#25512;&#29702;&#36895;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20248;&#21270;&#30340;Conformer&#65292;&#23427;&#36275;&#22815;&#23567;&#65292;&#20197;&#28385;&#36275;&#35774;&#22791;&#30340;&#38480;&#21046;&#65292;&#24182;&#21487;&#20197;&#22312;TPU&#19978;&#24555;&#36895;&#25512;&#29702;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#21508;&#31181;&#24819;&#27861;&#26469;&#25552;&#39640;&#25191;&#34892;&#36895;&#24230;&#65292;&#21253;&#25324;&#29992;&#20165;&#21253;&#21547;&#21367;&#31215;&#30340;&#22359;&#26367;&#25442;&#36739;&#20302;&#30340;Conformer&#22359;&#65292;&#31574;&#30053;&#24615;&#22320;&#32553;&#23567;&#20307;&#31995;&#32467;&#26500;&#65292;&#24182;&#21033;&#29992;RNNAttention-Performer&#12290;&#25105;&#20204;&#30340;&#20248;&#21270;Conformer&#21487;&#20197;&#36731;&#26494;&#22320;&#24182;&#20837;&#32423;&#32852;&#32534;&#30721;&#22120;&#35774;&#32622;&#65292;&#20801;&#35768;&#31532;&#20108;&#36941;&#35299;&#30721;&#22120;&#23545;&#20854;&#36755;&#20986;&#36827;&#34892;&#25805;&#20316;&#65292;&#24182;&#22312;&#26356;&#22810;&#36164;&#28304;&#21487;&#29992;&#26102;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#20248;&#21270;&#21487;&#20197;&#23558;&#24310;&#36831;&#38477;&#20302;6.8&#20493;&#65292;&#19988;&#22312;&#36136;&#37327;&#19978;&#21462;&#24471;&#21512;&#29702;&#30340;&#25240;&#34935;&#12290;&#36890;&#36807;&#32423;&#32852;&#31532;&#20108;&#27425;&#36890;&#36807;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35782;&#21035;&#31934;&#24230;&#30340;&#23436;&#20840;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conformer models maintain a large number of internal states, the vast majority of which are associated with self-attention layers. With limited memory bandwidth, reading these from memory at each inference step can slow down inference. In this paper, we design an optimized conformer that is small enough to meet on-device restrictions and has fast inference on TPUs. We explore various ideas to improve the execution speed, including replacing lower conformer blocks with convolution-only blocks, strategically downsizing the architecture, and utilizing an RNNAttention-Performer. Our optimized conformer can be readily incorporated into a cascaded-encoder setting, allowing a second-pass decoder to operate on its output and improve the accuracy whenever more resources are available. Altogether, we find that these optimizations can reduce latency by a factor of 6.8x, and come at a reasonable trade-off in quality. With the cascaded second-pass, we show that the recognition accuracy is completel
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#31867;&#27861;&#21644;&#25968;&#25454;&#38598;&#65292;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35299;&#30721;&#23398;&#26415;&#25163;&#31295;&#30340;&#31471;&#21040;&#31471;&#20889;&#20316;&#36712;&#36857;&#65292;&#20197;&#25552;&#20379;&#26356;&#32454;&#33268;&#30340;&#20998;&#26512;&#21644;&#21453;&#39304;&#65292;&#24182;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#22312;&#25991;&#26723;&#23618;&#38754;&#19978;&#30340;&#21019;&#36896;&#24615;&#21644;&#32467;&#26500;&#21453;&#39304;&#12290;</title><link>http://arxiv.org/abs/2304.00121</link><description>&lt;p&gt;
&#23398;&#26415;&#25163;&#31295;&#30340;&#31471;&#21040;&#31471;&#20889;&#20316;&#36712;&#36857;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Decoding the End-to-end Writing Trajectory in Scholarly Manuscripts. (arXiv:2304.00121v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00121
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#31867;&#27861;&#21644;&#25968;&#25454;&#38598;&#65292;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35299;&#30721;&#23398;&#26415;&#25163;&#31295;&#30340;&#31471;&#21040;&#31471;&#20889;&#20316;&#36712;&#36857;&#65292;&#20197;&#25552;&#20379;&#26356;&#32454;&#33268;&#30340;&#20998;&#26512;&#21644;&#21453;&#39304;&#65292;&#24182;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#22312;&#25991;&#26723;&#23618;&#38754;&#19978;&#30340;&#21019;&#36896;&#24615;&#21644;&#32467;&#26500;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#26415;&#20889;&#20316;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#39046;&#22495;&#65292;&#36890;&#24120;&#25353;&#29031;&#31995;&#32479;&#24615;&#30340;&#31243;&#24207;&#35268;&#21010;&#21644;&#21046;&#20316;&#26082;&#26377;&#21512;&#29702;&#24615;&#21448;&#23500;&#26377;&#21019;&#36896;&#24615;&#30340;&#20316;&#21697;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#25991;&#26412;&#29983;&#25104;&#21644;&#20462;&#35746;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#30456;&#24403;&#30340;&#25104;&#21151;&#65307;&#28982;&#32780;&#65292;LLM&#20173;&#28982;&#38590;&#20197;&#22312;&#25991;&#26723;&#23618;&#38754;&#19978;&#25552;&#20379;&#32467;&#26500;&#21644;&#21019;&#36896;&#24615;&#21453;&#39304;&#65292;&#36825;&#23545;&#20110;&#23398;&#26415;&#20889;&#20316;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#26681;&#25454;&#24847;&#22270;&#12289;&#20889;&#20316;&#32773;&#30340;&#34892;&#20026;&#21450;&#20854;&#25152;&#20889;&#25968;&#25454;&#30340;&#20449;&#24687;&#31867;&#22411;&#23558;&#23398;&#26415;&#20889;&#20316;&#34892;&#20026;&#20998;&#31867;&#65292;&#24182;&#25552;&#20379;&#20102;ManuScript&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#25105;&#20204;&#20998;&#31867;&#27861;&#30340;&#31616;&#21270;&#29256;&#26412;&#21450;&#20316;&#23478;&#34892;&#20026;&#21644;&#24847;&#22270;&#31561;&#27880;&#37322;&#12290;&#25105;&#20204;&#30340;&#20998;&#31867;&#27861;&#21253;&#25324;&#19977;&#20010;&#32423;&#21035;&#65292;&#20197;&#36861;&#36394;&#24635;&#20307;&#20889;&#20316;&#27969;&#31243;&#24182;&#35782;&#21035;&#23884;&#20837;&#22312;&#27599;&#20010;&#39640;&#32423;&#36807;&#31243;&#20013;&#30340;&#19981;&#21516;&#20889;&#20316;&#32773;&#27963;&#21160;&#12290; ManuScript&#30001;1000&#22810;&#20010;&#35821;&#35328;&#34920;&#36798;&#29305;&#24449;&#32452;&#25104;&#65292;&#36825;&#20123;&#29305;&#24449;&#26159;&#36890;&#36807;&#23545;&#21477;&#23376;&#30340;&#20840;&#38754;&#35299;&#26512;&#21644;&#36923;&#36753;&#32467;&#26500;&#30340;&#27880;&#37322;&#32780;&#33719;&#24471;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#20998;&#31867;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#35299;&#30721;&#23398;&#26415;&#25163;&#31295;&#30340;&#31471;&#21040;&#31471;&#20889;&#20316;&#36712;&#36857;&#65292;&#36890;&#36807;&#25552;&#20379;&#20851;&#20110;&#20889;&#20316;&#32773;&#24847;&#22270;&#12289;&#34892;&#20026;&#21644;&#20351;&#29992;&#20449;&#24687;&#31867;&#22411;&#30340;&#35814;&#32454;&#20449;&#24687;&#65292;&#23454;&#29616;&#23545;&#25991;&#26723;&#23618;&#38754;&#19978;&#26356;&#32454;&#33268;&#30340;&#20998;&#26512;&#21644;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scholarly writing presents a complex space that generally follows a methodical procedure to plan and produce both rationally sound and creative compositions. Recent works involving large language models (LLM) demonstrate considerable success in text generation and revision tasks; however, LLMs still struggle to provide structural and creative feedback on the document level that is crucial to academic writing. In this paper, we introduce a novel taxonomy that categorizes scholarly writing behaviors according to intention, writer actions, and the information types of the written data. We also provide ManuScript, an original dataset annotated with a simplified version of our taxonomy to show writer actions and the intentions behind them. Motivated by cognitive writing theory, our taxonomy for scientific papers includes three levels of categorization in order to trace the general writing flow and identify the distinct writer activities embedded within each higher-level process. ManuScript 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#26368;&#26032;&#30340;&#24819;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35270;&#20026;&#21487;&#20197;&#35775;&#38382;&#21253;&#21547;&#20851;&#20110;&#32452;&#32455;&#12289;&#26426;&#26500;&#21644;&#20844;&#21496;&#30340;&#26368;&#26032;&#21644;&#20934;&#30830;&#20449;&#24687;&#30340;&#20195;&#29702;&#65292;&#32467;&#21512;&#27668;&#20505;&#21464;&#21270;&#25968;&#25454;&#24211;&#21644;&#24120;&#35268; Google &#25628;&#32034;&#65292;&#20197;&#25552;&#20379;&#26356;&#21487;&#38752;&#21644;&#20934;&#30830;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2304.00116</link><description>&lt;p&gt;
&#21033;&#29992;&#27668;&#20505;&#36164;&#28304;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Enhancing Large Language Models with Climate Resources. (arXiv:2304.00116v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#26368;&#26032;&#30340;&#24819;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35270;&#20026;&#21487;&#20197;&#35775;&#38382;&#21253;&#21547;&#20851;&#20110;&#32452;&#32455;&#12289;&#26426;&#26500;&#21644;&#20844;&#21496;&#30340;&#26368;&#26032;&#21644;&#20934;&#30830;&#20449;&#24687;&#30340;&#20195;&#29702;&#65292;&#32467;&#21512;&#27668;&#20505;&#21464;&#21270;&#25968;&#25454;&#24211;&#21644;&#24120;&#35268; Google &#25628;&#32034;&#65292;&#20197;&#25552;&#20379;&#26356;&#21487;&#38752;&#21644;&#20934;&#30830;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#22312;&#29983;&#25104;&#21508;&#31181;&#20027;&#39064;&#30340;&#31867;&#20154;&#25991;&#26412;&#26041;&#38754;&#23637;&#31034;&#20102;&#20854;&#33021;&#21147;&#65292;&#26174;&#33879;&#22320;&#25913;&#21464;&#20102;&#20154;&#24037;&#26234;&#33021;&#30340;&#26684;&#23616;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294; LLM &#32570;&#20047;&#26368;&#36817;&#30340;&#20449;&#24687;&#65292;&#24182;&#19988;&#24120;&#24120;&#20351;&#29992;&#19981;&#22815;&#20934;&#30830;&#30340;&#35821;&#35328;&#65292;&#36825;&#22312;&#38656;&#35201;&#20934;&#30830;&#24615;&#30340;&#39046;&#22495;&#27604;&#22914;&#27668;&#20505;&#21464;&#21270;&#20013;&#21487;&#33021;&#20250;&#26377;&#23475;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#26032;&#30340;&#24819;&#27861;&#26469;&#21033;&#29992; LLM &#30340;&#28508;&#21147;&#65292;&#23558;&#23427;&#20204;&#35270;&#20026;&#21487;&#20197;&#35775;&#38382;&#21253;&#21547;&#20851;&#20110;&#32452;&#32455;&#12289;&#26426;&#26500;&#21644;&#20844;&#21496;&#30340;&#26368;&#26032;&#21644;&#20934;&#30830;&#20449;&#24687;&#30340;&#25968;&#25454;&#24211;&#30340;&#20195;&#29702;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#21407;&#22411;&#20195;&#29702;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#35813;&#20195;&#29702;&#20174; ClimateWatch (https://www.climatewatchdata.org/) &#26816;&#32034;&#25490;&#25918;&#25968;&#25454;&#24182;&#21033;&#29992;&#24120;&#35268; Google &#25628;&#32034;&#12290;&#36890;&#36807;&#23558;&#36825;&#20123;&#36164;&#28304;&#19982; LLM &#38598;&#25104;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20811;&#26381;&#20102;&#19982;&#19981;&#31934;&#30830;&#35821;&#35328;&#30456;&#20851;&#30340;&#38480;&#21046;&#65292;&#22312;&#27668;&#20505;&#21464;&#21270;&#31561;&#39046;&#22495;&#25552;&#20379;&#20102;&#26356;&#21487;&#38752;&#21644;&#20934;&#30830;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have significantly transformed the landscape of artificial intelligence by demonstrating their ability in generating human-like text across diverse topics. However, despite their impressive capabilities, LLMs lack recent information and often employ imprecise language, which can be detrimental in domains where accuracy is crucial, such as climate change. In this study, we make use of recent ideas to harness the potential of LLMs by viewing them as agents that access multiple sources, including databases containing recent and precise information about organizations, institutions, and companies. We demonstrate the effectiveness of our method through a prototype agent that retrieves emission data from ClimateWatch (https://www.climatewatchdata.org/) and leverages general Google search. By integrating these resources with LLMs, our approach overcomes the limitations associated with imprecise language and delivers more reliable and accurate information in the cr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;&#22522;&#20110;Transformer&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#24320;&#21457;&#35821;&#26009;&#24211;&#21644;&#27604;&#36739;5&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#25552;&#21462;&#20102;&#30002;&#29366;&#33146;&#32467;&#33410;&#36229;&#22768;&#25253;&#21578;&#20013;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#20854;&#20013;&#65292;GatorTron&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2304.00115</link><description>&lt;p&gt;
&#37319;&#29992;&#22522;&#20110;Transformer&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#20174;&#36229;&#22768;&#25253;&#21578;&#20013;&#25552;&#21462;&#30002;&#29366;&#33146;&#32467;&#33410;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Extracting Thyroid Nodules Characteristics from Ultrasound Reports Using Transformer-based Natural Language Processing Methods. (arXiv:2304.00115v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00115
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#22522;&#20110;Transformer&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#24320;&#21457;&#35821;&#26009;&#24211;&#21644;&#27604;&#36739;5&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#25552;&#21462;&#20102;&#30002;&#29366;&#33146;&#32467;&#33410;&#36229;&#22768;&#25253;&#21578;&#20013;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#20854;&#20013;&#65292;GatorTron&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30002;&#29366;&#33146;&#32467;&#33410;&#30340;&#36229;&#22768;&#29305;&#24449;&#26159;&#25351;&#23548;&#30002;&#29366;&#33146;&#30284;&#35780;&#20272;&#30340;&#37325;&#35201;&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#30002;&#29366;&#33146;&#32467;&#33410;&#30340;&#29305;&#24449;&#36890;&#24120;&#35760;&#24405;&#22312;&#20020;&#24202;&#21465;&#36848;&#20013;&#65292;&#22914;&#36229;&#22768;&#25253;&#21578;&#20013;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#20351;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;NLP&#31995;&#32479;&#25552;&#21462;&#20102;&#19968;&#20123;&#38480;&#23450;&#25968;&#37327;(&lt;9)&#30340;&#29305;&#24449;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#22810;&#23398;&#31185;&#22242;&#38431;&#30340;NLP&#19987;&#23478;&#21644;&#30002;&#29366;&#33146;&#19987;&#23478;&#30830;&#23450;&#20102;&#23545;&#20020;&#24202;&#25252;&#29702;&#26377;&#37325;&#35201;&#24847;&#20041;&#30340;&#30002;&#29366;&#33146;&#32467;&#33410;&#29305;&#24449;&#65292;&#32534;&#20889;&#20102;&#27880;&#37322;&#25351;&#21335;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#35821;&#26009;&#24211;&#65292;&#24182;&#27604;&#36739;&#20102;5&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;transformer&#30340;NLP&#26041;&#27861;&#65292;&#21253;&#25324;BERT&#12289;RoBERTa&#12289;LongFormer&#12289;DeBERTa&#21644;GatorTron&#65292;&#20174;&#36229;&#22768;&#25253;&#21578;&#20013;&#25552;&#21462;&#30002;&#29366;&#33146;&#32467;&#33410;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;GatorTron&#65292;&#20351;&#29992;90&#20159;&#20010;&#25991;&#26412;&#21333;&#35789;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#26159;&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26368;&#20339;&#30340;&#20005;&#26684;&#21644;&#23485;&#26494;F1-score&#20998;&#21035;&#20026;0.8851&#21644;0.9495&#65292;&#20174;&#36229;&#22768;&#25253;&#21578;&#20013;&#25552;&#21462;&#30002;&#29366;&#33146;&#32467;&#33410;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ultrasound characteristics of thyroid nodules guide the evaluation of thyroid cancer in patients with thyroid nodules. However, the characteristics of thyroid nodules are often documented in clinical narratives such as ultrasound reports. Previous studies have examined natural language processing (NLP) methods in extracting a limited number of characteristics (&lt;9) using rule-based NLP systems. In this study, a multidisciplinary team of NLP experts and thyroid specialists, identified thyroid nodule characteristics that are important for clinical care, composed annotation guidelines, developed a corpus, and compared 5 state-of-the-art transformer-based NLP methods, including BERT, RoBERTa, LongFormer, DeBERTa, and GatorTron, for extraction of thyroid nodule characteristics from ultrasound reports. Our GatorTron model, a transformer-based large language model trained using over 90 billion words of text, achieved the best strict and lenient F1-score of 0.8851 and 0.9495 for the extract
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#31232;&#30095;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#39640;&#25928;&#31264;&#23494;&#26816;&#32034;&#65292;&#20351;&#29992;Tevatron &#21644;MSMARCO&#12289;NQ&#21644;TriviaQA&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#31232;&#30095;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#26367;&#25442;&#20026;&#21407;&#26377;&#27169;&#22411;&#65292;&#20960;&#20046;&#19981;&#38477;&#20302;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#25512;&#29702;&#36895;&#24230;&#25552;&#39640;&#20102;&#26368;&#22810;4.3&#20493;&#12290;</title><link>http://arxiv.org/abs/2304.00114</link><description>&lt;p&gt;
&#31264;&#23494;&#31232;&#30095;&#26816;&#32034;&#65306;&#20351;&#29992;&#31232;&#30095;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#39640;&#25928;&#31264;&#23494;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Dense Sparse Retrieval: Using Sparse Language Models for Inference Efficient Dense Retrieval. (arXiv:2304.00114v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00114
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#31232;&#30095;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#39640;&#25928;&#31264;&#23494;&#26816;&#32034;&#65292;&#20351;&#29992;Tevatron &#21644;MSMARCO&#12289;NQ&#21644;TriviaQA&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#31232;&#30095;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#26367;&#25442;&#20026;&#21407;&#26377;&#27169;&#22411;&#65292;&#20960;&#20046;&#19981;&#38477;&#20302;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#25512;&#29702;&#36895;&#24230;&#25552;&#39640;&#20102;&#26368;&#22810;4.3&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21521;&#37327;&#30340;&#26816;&#32034;&#31995;&#32479;&#24050;&#25104;&#20026;&#23398;&#26415;&#21644;&#24037;&#19994;&#25628;&#32034;&#24212;&#29992;&#30340;&#24120;&#35265;&#24037;&#20855;&#65292;&#22240;&#20026;&#23427;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#21487;&#25193;&#23637;&#30340;&#26041;&#24335;&#26469;&#21033;&#29992;&#25991;&#26723;&#21644;&#26597;&#35810;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#36827;&#34892;&#25628;&#32034;&#12290;&#30001;&#20110;&#36825;&#20123;&#21521;&#37327;&#31995;&#32479;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#35821;&#35328;&#27169;&#22411;&#65292;&#22240;&#27492;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;GPU&#30340;&#20351;&#29992;&#65292;&#36825;&#21487;&#33021;&#20250;&#24456;&#26114;&#36149;&#19988;&#38590;&#20197;&#31649;&#29702;&#12290;&#37492;&#20110;&#36817;&#24180;&#26469;&#24341;&#20837;&#31232;&#30095;&#24615;&#20197;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#31232;&#30095;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31264;&#23494;&#26816;&#32034;&#20197;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#12290;&#20351;&#29992;&#27969;&#34892;&#30340;&#26816;&#32034;&#24211;Tevatron&#21644;MSMARCO&#12289;NQ&#21644;TriviaQA&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21457;&#29616;&#31232;&#30095;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#26367;&#25442;&#20026;&#21407;&#26377;&#27169;&#22411;&#65292;&#20960;&#20046;&#19981;&#38477;&#20302;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#25512;&#29702;&#36895;&#24230;&#25552;&#39640;&#20102;&#26368;&#22810;4.3&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vector-based retrieval systems have become a common staple for academic and industrial search applications because they provide a simple and scalable way of extending the search to leverage contextual representations for documents and queries. As these vector-based systems rely on contextual language models, their usage commonly requires GPUs, which can be expensive and difficult to manage. Given recent advances in introducing sparsity into language models for improved inference efficiency, in this paper, we study how sparse language models can be used for dense retrieval to improve inference efficiency. Using the popular retrieval library Tevatron and the MSMARCO, NQ, and TriviaQA datasets, we find that sparse language models can be used as direct replacements with little to no drop in accuracy and up to 4.3x improved inference speeds
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#30740;&#31350;&#20102;&#20174;&#20020;&#24202;&#35760;&#24405;&#20013;&#25552;&#21462;&#35893;&#22916;&#30151;&#29366;&#30340;&#26041;&#27861;&#12290;&#32463;&#36807;&#27604;&#36739;&#65292;&#35813;&#30740;&#31350;&#21457;&#29616;GatorTron&#27169;&#22411;&#20855;&#26377;&#26368;&#20339;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.00111</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20174;&#20020;&#24202;&#21465;&#36848;&#20013;&#35782;&#21035;&#35893;&#22916;&#30151;&#29366;
&lt;/p&gt;
&lt;p&gt;
Identifying Symptoms of Delirium from Clinical Narratives Using Natural Language Processing. (arXiv:2304.00111v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#30740;&#31350;&#20102;&#20174;&#20020;&#24202;&#35760;&#24405;&#20013;&#25552;&#21462;&#35893;&#22916;&#30151;&#29366;&#30340;&#26041;&#27861;&#12290;&#32463;&#36807;&#27604;&#36739;&#65292;&#35813;&#30740;&#31350;&#21457;&#29616;GatorTron&#27169;&#22411;&#20855;&#26377;&#26368;&#20339;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35893;&#22916;&#30151;&#29366;&#26159;&#19968;&#31181;&#24613;&#24615;&#30340;&#35748;&#30693;&#21151;&#33021;&#19979;&#38477;&#25110;&#27874;&#21160;&#65292;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#30340;&#19981;&#33391;&#21518;&#26524;&#12290;&#30001;&#20110;&#35893;&#22916;&#30151;&#29366;&#30340;&#30701;&#26242;&#21644;&#22810;&#26679;&#24615;&#65292;&#23427;&#32463;&#24120;&#34987;&#24573;&#35270;&#19988;&#26410;&#34987;&#32534;&#30721;&#21040;&#24739;&#32773;&#30340;&#30005;&#23376;&#30149;&#21382;&#20013;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26159;&#19968;&#31181;&#20174;&#20020;&#24202;&#21465;&#36848;&#20013;&#25552;&#21462;&#21307;&#23398;&#27010;&#24565;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#22312;&#35893;&#22916;&#30151;&#29366;&#21644;&#21518;&#26524;&#30340;&#30740;&#31350;&#20013;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#36741;&#21161;&#35893;&#22916;&#30151;&#29366;&#30340;&#35786;&#26029;&#21644;&#34920;&#22411;&#25551;&#36848;&#65292;&#25105;&#20204;&#32452;&#25104;&#20102;&#19987;&#23478;&#23567;&#32452;&#26469;&#20998;&#31867;&#19981;&#21516;&#30340;&#35893;&#22916;&#30151;&#29366;&#65292;&#32534;&#20889;&#27880;&#37322;&#25351;&#21335;&#65292;&#21019;&#24314;&#20102;&#21253;&#21547;&#22810;&#26679;&#21270;&#35893;&#22916;&#30151;&#29366;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#20174;&#20020;&#24202;&#35760;&#24405;&#20013;&#25552;&#21462;&#35893;&#22916;&#30151;&#29366;&#30340;NLP&#26041;&#27861;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#21253;&#25324;2&#31181;&#24120;&#35268;&#39046;&#22495;&#27169;&#22411;&#65288;BERT&#21644;RoBERTa&#65289;&#21644;3&#31181;&#20020;&#24202;&#39046;&#22495;&#27169;&#22411;&#65288;BERT_MIMIC&#65292;RoBERTa_MIMIC&#21644;GatorTron&#65289;&#22312;&#20869;&#30340;5&#31181;&#26368;&#20808;&#36827;&#30340;&#36716;&#25442;&#22120;&#27169;&#22411;&#12290;GatorTron&#21462;&#24471;&#20102;&#26368;&#20339;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Delirium is an acute decline or fluctuation in attention, awareness, or other cognitive function that can lead to serious adverse outcomes. Despite the severe outcomes, delirium is frequently unrecognized and uncoded in patients' electronic health records (EHRs) due to its transient and diverse nature. Natural language processing (NLP), a key technology that extracts medical concepts from clinical narratives, has shown great potential in studies of delirium outcomes and symptoms. To assist in the diagnosis and phenotyping of delirium, we formed an expert panel to categorize diverse delirium symptoms, composed annotation guidelines, created a delirium corpus with diverse delirium symptoms, and developed NLP methods to extract delirium symptoms from clinical notes. We compared 5 state-of-the-art transformer models including 2 models (BERT and RoBERTa) from the general domain and 3 models (BERT_MIMIC, RoBERTa_MIMIC, and GatorTron) from the clinical domain. GatorTron achieved the best stri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Alleviate&#65292;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#30005;&#35805;&#21307;&#30103;&#30340;&#34394;&#25311;&#21161;&#25163;&#65292;&#36890;&#36807;&#20010;&#24615;&#21270;&#25252;&#29702;&#26469;&#21327;&#21161;&#24739;&#26377;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#30340;&#24739;&#32773;&#65292;&#24182;&#21327;&#21161;&#20020;&#24202;&#21307;&#29983;&#26356;&#22909;&#22320;&#20102;&#35299;&#20182;&#20204;&#30340;&#24739;&#32773;&#12290;Alleviate&#21033;&#29992;&#20020;&#24202;&#26377;&#25928;&#30340;&#24515;&#29702;&#20581;&#24247;&#25991;&#26412;&#21644;&#25968;&#25454;&#24211;&#20570;&#20986;&#21307;&#23398;&#19978;&#21512;&#29702;&#21644;&#30693;&#24773;&#30340;&#20915;&#31574;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#26696;&#20363;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;Alleviate&#25552;&#20379;&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#34394;&#25311;&#21161;&#25163;&#12290;</title><link>http://arxiv.org/abs/2304.00025</link><description>&lt;p&gt;
&#28436;&#31034;Alleviate&#65306;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#30005;&#35805;&#21307;&#30103;&#30340;&#34394;&#25311;&#21161;&#25163;&#65306;&#20197;&#24515;&#29702;&#20581;&#24247;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Demo Alleviate: Demonstrating Artificial Intelligence Enabled Virtual Assistance for Telehealth: The Mental Health Case. (arXiv:2304.00025v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Alleviate&#65292;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#30005;&#35805;&#21307;&#30103;&#30340;&#34394;&#25311;&#21161;&#25163;&#65292;&#36890;&#36807;&#20010;&#24615;&#21270;&#25252;&#29702;&#26469;&#21327;&#21161;&#24739;&#26377;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#30340;&#24739;&#32773;&#65292;&#24182;&#21327;&#21161;&#20020;&#24202;&#21307;&#29983;&#26356;&#22909;&#22320;&#20102;&#35299;&#20182;&#20204;&#30340;&#24739;&#32773;&#12290;Alleviate&#21033;&#29992;&#20020;&#24202;&#26377;&#25928;&#30340;&#24515;&#29702;&#20581;&#24247;&#25991;&#26412;&#21644;&#25968;&#25454;&#24211;&#20570;&#20986;&#21307;&#23398;&#19978;&#21512;&#29702;&#21644;&#30693;&#24773;&#30340;&#20915;&#31574;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#26696;&#20363;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;Alleviate&#25552;&#20379;&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#34394;&#25311;&#21161;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30123;&#24773;&#20043;&#21518;&#65292;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#31934;&#31070;&#21355;&#29983;&#25252;&#29702;&#25903;&#25345;&#21464;&#24471;&#24840;&#21457;&#37325;&#35201;&#12290;&#20026;&#20102;&#25552;&#20379;&#36275;&#22815;&#30340;&#20851;&#24576;&#65292;&#24212;&#23545;&#25152;&#38754;&#20020;&#25361;&#25112;&#30340;&#24191;&#24230;&#21644;&#22797;&#26434;&#24615;&#28041;&#21450;&#65306;&#65288;a&#65289;&#20010;&#24615;&#21270;&#30149;&#20154;&#29702;&#35299;&#65292;&#65288;b&#65289;&#23433;&#20840;&#32422;&#26463;&#21644;&#21307;&#23398;&#39564;&#35777;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30149;&#20154;&#20132;&#20114;&#65292;&#20197;&#21450;&#65288;c&#65289;&#25903;&#25345;&#20351;&#29992;&#32842;&#22825;&#26426;&#22120;&#20154;-&#30149;&#20154;&#20132;&#20114;&#36827;&#34892;&#22522;&#20110;&#21453;&#39304;&#30340;&#35774;&#35745;&#30340;&#19981;&#26029;&#25913;&#36827;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Alleviate&#65292;&#19968;&#20010;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#26088;&#22312;&#36890;&#36807;&#20010;&#24615;&#21270;&#25252;&#29702;&#26469;&#21327;&#21161;&#24739;&#26377;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#30340;&#24739;&#32773;&#65292;&#24182;&#21327;&#21161;&#20020;&#24202;&#21307;&#29983;&#26356;&#22909;&#22320;&#20102;&#35299;&#20182;&#20204;&#30340;&#24739;&#32773;&#12290;Alleviate&#21033;&#29992;&#19968;&#31995;&#21015;&#20844;&#24320;&#21487;&#29992;&#30340;&#20020;&#24202;&#26377;&#25928;&#30340;&#24515;&#29702;&#20581;&#24247;&#25991;&#26412;&#21644;&#25968;&#25454;&#24211;&#65292;&#20174;&#32780;&#20351;Alleviate&#33021;&#22815;&#20570;&#20986;&#21307;&#23398;&#19978;&#21512;&#29702;&#21644;&#30693;&#24773;&#30340;&#20915;&#31574;&#12290;&#27492;&#22806;&#65292;Alleviate&#30340;&#27169;&#22359;&#21270;&#35774;&#35745;&#21644;&#21487;&#35299;&#37322;&#24615;&#20915;&#31574;&#20351;&#20854;&#36866;&#21512;&#36827;&#34892;&#20581;&#22766;&#21644;&#25345;&#32493;&#22522;&#20110;&#21453;&#39304;&#30340;&#35774;&#35745;&#25913;&#36827;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;Alleviate&#30340;&#19981;&#21516;&#27169;&#22359;&#20197;&#21450;&#23427;&#20204;&#22914;&#20309;&#20849;&#21516;&#20026;&#24515;&#29702;&#20581;&#24247;&#24739;&#32773;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#24110;&#21161;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#19968;&#31995;&#21015;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;Alleviate&#25552;&#20379;&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#34394;&#25311;&#21161;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;
After the pandemic, artificial intelligence (AI) powered support for mental health care has become increasingly important. The breadth and complexity of significant challenges required to provide adequate care involve: (a) Personalized patient understanding, (b) Safety-constrained and medically validated chatbot patient interactions, and (c) Support for continued feedback-based refinements in design using chatbot-patient interactions. We propose Alleviate, a chatbot designed to assist patients suffering from mental health challenges with personalized care and assist clinicians with understanding their patients better. Alleviate draws from an array of publicly available clinically valid mental-health texts and databases, allowing Alleviate to make medically sound and informed decisions. In addition, Alleviate's modular design and explainable decision-making lends itself to robust and continued feedback-based refinements to its design. In this paper, we explain the different modules of A
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;SemiMemes&#65292;&#20027;&#35201;&#24212;&#29992;&#20110;Memes&#30340;&#20998;&#26512;&#21644;&#27880;&#37322;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#20248;&#20110;&#20854;&#20182;&#26368;&#26032;&#30340;&#22810;&#27169;&#24577;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.00020</link><description>&lt;p&gt;
SemiMemes&#65306;&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;Memes&#20998;&#26512;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SemiMemes: A Semi-supervised Learning Approach for Multimodal Memes Analysis. (arXiv:2304.00020v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00020
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;SemiMemes&#65292;&#20027;&#35201;&#24212;&#29992;&#20110;Memes&#30340;&#20998;&#26512;&#21644;&#27880;&#37322;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#20248;&#20110;&#20854;&#20182;&#26368;&#26032;&#30340;&#22810;&#27169;&#24577;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#19978;Memes&#30340;&#26222;&#21450;&#24615;&#24341;&#21457;&#20102;&#20998;&#26512;&#20854;&#38544;&#21547;&#21547;&#20041;&#12289;&#23457;&#26597;&#26377;&#23475;&#20869;&#23481;&#30340;&#38656;&#27714;&#12290;&#26426;&#22120;&#23398;&#20064;&#30340;Meme&#23457;&#26597;&#31995;&#32479;&#38656;&#35201;&#21322;&#30417;&#30563;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#21033;&#29992;&#20114;&#32852;&#32593;&#19978;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;Memes&#65292;&#24182;&#20351;&#27880;&#37322;&#36807;&#31243;&#21464;&#24471;&#26356;&#31616;&#21333;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#38656;&#35201;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#22240;&#20026;Memes&#30340;&#21547;&#20041;&#36890;&#24120;&#26469;&#33258;&#22270;&#20687;&#21644;&#25991;&#26412;&#12290;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#21363;&#22810;&#23186;&#20307;&#33258;&#21160;&#24615;&#21035;&#27495;&#35270;&#35782;&#21035;&#21644;&#20196;&#20154;&#35752;&#21388;&#30340;Memes&#25968;&#25454;&#38598;&#19978;&#65292;&#20248;&#20110;&#20854;&#20182;&#22810;&#27169;&#24577;&#21322;&#30417;&#30563;&#21644;&#30417;&#30563;&#23398;&#20064;&#30340;&#26368;&#26032;&#27169;&#22411;&#12290;&#20511;&#37492;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#25152;&#33719;&#24471;&#30340;&#35265;&#35299;&#65292;&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;SemiMemes&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#33258;&#32534;&#30721;&#22120;&#21644;&#20998;&#31867;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
The prevalence of memes on social media has created the need to sentiment analyze their underlying meanings for censoring harmful content. Meme censoring systems by machine learning raise the need for a semi-supervised learning solution to take advantage of the large number of unlabeled memes available on the internet and make the annotation process less challenging. Moreover, the approach needs to utilize multimodal data as memes' meanings usually come from both images and texts. This research proposes a multimodal semi-supervised learning approach that outperforms other multimodal semi-supervised learning and supervised learning state-of-the-art models on two datasets, the Multimedia Automatic Misogyny Identification and Hateful Memes dataset. Building on the insights gained from Contrastive Language-Image Pre-training, which is an effective multimodal learning technique, this research introduces SemiMemes, a novel training method that combines auto-encoder and classification task to
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#36896;&#24615;&#38382;&#39064;&#65292;&#20998;&#26512;&#20102;&#19982;&#20043;&#30456;&#20851;&#30340;&#26426;&#22120;&#21019;&#36896;&#24615;&#30340;&#38590;&#28857;&#21644;&#26131;&#28857;&#65292;&#24182;&#37325;&#28857;&#20998;&#26512;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#21019;&#24847;&#20135;&#19994;&#20013;&#30340;&#31038;&#20250;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.00008</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#36896;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Creativity of Large Language Models. (arXiv:2304.00008v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00008
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#36896;&#24615;&#38382;&#39064;&#65292;&#20998;&#26512;&#20102;&#19982;&#20043;&#30456;&#20851;&#30340;&#26426;&#22120;&#21019;&#36896;&#24615;&#30340;&#38590;&#28857;&#21644;&#26131;&#28857;&#65292;&#24182;&#37325;&#28857;&#20998;&#26512;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#21019;&#24847;&#20135;&#19994;&#20013;&#30340;&#31038;&#20250;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#27491;&#22312;&#39072;&#35206;&#20154;&#24037;&#26234;&#33021;&#30340;&#22810;&#20010;&#39046;&#22495;&#12290;&#20854;&#20013;&#26368;&#26174;&#33879;&#30340;&#24212;&#29992;&#20043;&#19968;&#26159;&#21019;&#20316;&#65292;&#20363;&#22914;&#35799;&#27468;&#25110;&#25925;&#20107;&#65306;&#29983;&#25104;&#30340;&#36755;&#20986;&#36890;&#24120;&#20855;&#26377;&#24778;&#20154;&#30340;&#36136;&#37327;&#12290;&#20294;&#26159;&#65292;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#26159;&#65306;LLMs&#30495;&#30340;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#21019;&#36896;&#24615;&#30340;&#21527;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#21019;&#36896;&#24615;&#29702;&#35770;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;LLMs&#30340;&#21457;&#23637;&#65292;&#25506;&#35752;&#20102;&#20851;&#38190;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#21644;&#25361;&#25112;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#19982;LLMs&#30456;&#20851;&#30340;&#26426;&#22120;&#21019;&#36896;&#24615;&#26041;&#38754;&#30830;&#23450;&#20102;&#19968;&#32452;&#8220;&#26131;&#8221;&#21644;&#8220;&#38590;&#8221;&#38382;&#39064;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#21019;&#24847;&#20135;&#19994;&#20013;&#30340;&#31038;&#20250;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are revolutionizing several areas of Artificial Intelligence. One of the most remarkable applications is creative writing, e.g., poetry or storytelling: the generated outputs are often of astonishing quality. However, a natural question arise: can LLMs really be considered creative? In this article we firstly analyze the development of LLMs under the lens of creativity theories, investigating the key open questions and challenges. Then, we identify a set of "easy" and "hard" problems in machine creativity, discussing them in relation to LLMs. Finally, we analyze the societal impact of these technologies with a particular focus on the creative industries.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#33021;&#21542;&#36890;&#36807;&#24037;&#31243;&#22522;&#30784;&#65288;FE&#65289;&#21644;&#24037;&#31243;&#21407;&#29702;&#19982;&#23454;&#36341;&#65288;PE&#65289;&#32771;&#35797;&#65292;&#30740;&#31350;&#21457;&#29616;ChatGPT-4&#22312;FE&#32771;&#35797;&#20013;&#24471;&#20998;70.9&#65285;&#65292;&#22312;PE&#32771;&#35797;&#20013;&#24471;&#20998;46.2&#65285;&#65292;&#24182;&#19988;&#26377;&#26395;&#36890;&#36807;PE&#32771;&#35797;&#12290;</title><link>http://arxiv.org/abs/2303.18149</link><description>&lt;p&gt;
AI&#32842;&#22825;&#26426;&#22120;&#20154;&#26159;&#21542;&#33021;&#36890;&#36807;&#24037;&#31243;&#22522;&#30784;&#65288;FE&#65289;&#21644;&#24037;&#31243;&#21407;&#29702;&#19982;&#23454;&#36341;&#65288;PE&#65289;&#32467;&#26500;&#32771;&#35797;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can AI Chatbots Pass the Fundamentals of Engineering (FE) and Principles and Practice of Engineering (PE) Structural Exams?. (arXiv:2303.18149v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#33021;&#21542;&#36890;&#36807;&#24037;&#31243;&#22522;&#30784;&#65288;FE&#65289;&#21644;&#24037;&#31243;&#21407;&#29702;&#19982;&#23454;&#36341;&#65288;PE&#65289;&#32771;&#35797;&#65292;&#30740;&#31350;&#21457;&#29616;ChatGPT-4&#22312;FE&#32771;&#35797;&#20013;&#24471;&#20998;70.9&#65285;&#65292;&#22312;PE&#32771;&#35797;&#20013;&#24471;&#20998;46.2&#65285;&#65292;&#24182;&#19988;&#26377;&#26395;&#36890;&#36807;PE&#32771;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#31243;&#30028;&#65292;&#38543;&#30528;OpenAI ChatGPT-4&#21644;Google Bard&#30340;&#21457;&#24067;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#25216;&#26415;&#36817;&#24180;&#26469;&#36805;&#36895;&#21457;&#23637;&#12290;&#34429;&#28982;&#36825;&#20123;&#32842;&#22825;&#26426;&#22120;&#20154;&#34987;&#25253;&#36947;&#34920;&#29616;&#33391;&#22909;&#65292;&#29978;&#33267;&#36890;&#36807;&#20102;&#21508;&#31181;&#26631;&#20934;&#21270;&#32771;&#35797;&#65292;&#21253;&#25324;&#21307;&#23398;&#21644;&#27861;&#24459;&#32771;&#35797;&#65292;&#20294;&#26412;&#35770;&#25991;&#25506;&#35752;&#36825;&#20123;&#32842;&#22825;&#26426;&#22120;&#20154;&#26159;&#21542;&#20063;&#33021;&#36890;&#36807;&#24037;&#31243;&#22522;&#30784;&#65288;FE&#65289;&#21644;&#24037;&#31243;&#21407;&#29702;&#19982;&#23454;&#36341;&#65288;PE&#65289;&#32771;&#35797;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#26679;&#21270;&#30340;&#22303;&#26408;&#21644;&#29615;&#22659;&#24037;&#31243;&#38382;&#39064;&#21644;&#24773;&#26223;&#26469;&#35780;&#20272;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#24615;&#33021;&#65292;&#22312;FE&#21644;PE&#32771;&#35797;&#20013;&#24120;&#35265;&#12290;&#22522;&#20110;&#30456;&#20851;&#24615;&#12289;&#20934;&#30830;&#24615;&#21644;&#28165;&#26224;&#24230;&#65292;&#20998;&#26512;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21709;&#24212;&#65292;&#28982;&#21518;&#19982;National Council of Examiners for Engineering and Surveying (NCEES)&#30340;&#24314;&#35758;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#25253;&#21578;&#26174;&#31034;&#65292;ChatGPT-4&#21644;Bard&#22312;FE&#32771;&#35797;&#20013;&#24471;&#20998;&#20998;&#21035;&#20026;70.9&#65285;&#21644;39.2&#65285;&#65292;&#22312;PE&#32771;&#35797;&#20013;&#24471;&#20998;&#20998;&#21035;&#20026;46.2&#65285;&#21644;41&#65285;&#12290;&#26174;&#28982;&#65292;&#30446;&#21069;&#29256;&#26412;&#30340;ChatGPT-4&#26377;&#21487;&#33021;&#36890;&#36807;PE&#32771;&#35797;&#65292;&#20294;&#22312;FE&#32771;&#35797;&#20013;&#25104;&#32489;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
The engineering community has recently witnessed the emergence of chatbot technology with the release of OpenAI ChatGPT-4 and Google Bard. While these chatbots have been reported to perform well and even pass various standardized tests, including medical and law exams, this forum paper explores whether these chatbots can also pass the Fundamentals of Engineering (FE) and Principles and Practice of Engineering (PE) exams. A diverse range of civil and environmental engineering questions and scenarios are used to evaluate the chatbots' performance, as commonly present in the FE and PE exams. The chatbots' responses were analyzed based on their relevance, accuracy, and clarity and then compared against the recommendations of the National Council of Examiners for Engineering and Surveying (NCEES). Our report shows that ChatGPT-4 and Bard, respectively scored 70.9% and 39.2% in the FE exam and 46.2% and 41% in the PE exam. It is evident that the current version of ChatGPT-4 could potentially
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#33521;&#25991;GPT&#27169;&#22411;&#23545;&#40784;&#21040;&#35199;&#29677;&#29273;&#35821;&#30340;&#23567;&#23553;&#38381;&#39046;&#22495;&#20013;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#22870;&#21169;&#27169;&#22411;&#26469;&#25913;&#36827;&#31572;&#26696;&#30340;&#35299;&#30721;&#21644;&#29983;&#25104;&#65292;&#22312;&#38382;&#31572;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.17649</link><description>&lt;p&gt;
&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#23558;&#19968;&#20010;&#20013;&#31561;&#22823;&#23567;&#30340;&#33521;&#25991;GPT&#27169;&#22411;&#23545;&#40784;&#21040;&#35199;&#29677;&#29273;&#35821;&#30340;&#23567;&#23553;&#38381;&#39046;&#22495;&#20013;
&lt;/p&gt;
&lt;p&gt;
Aligning a medium-size GPT model in English to a small closed domain in Spanish using reinforcement learning. (arXiv:2303.17649v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#33521;&#25991;GPT&#27169;&#22411;&#23545;&#40784;&#21040;&#35199;&#29677;&#29273;&#35821;&#30340;&#23567;&#23553;&#38381;&#39046;&#22495;&#20013;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#22870;&#21169;&#27169;&#22411;&#26469;&#25913;&#36827;&#31572;&#26696;&#30340;&#35299;&#30721;&#21644;&#29983;&#25104;&#65292;&#22312;&#38382;&#31572;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#21407;&#26412;&#29992;&#20110;&#24320;&#25918;&#39046;&#22495;&#30340;&#20013;&#31561;&#22823;&#23567;&#33521;&#25991;GPT&#27169;&#22411;&#65292;&#23545;&#40784;&#21040;&#35199;&#29677;&#29273;&#35821;&#30340;&#23567;&#23553;&#38381;&#39046;&#22495;&#12290;&#35813;&#27169;&#22411;&#34987;&#31934;&#32454;&#35843;&#25972;&#29992;&#20110;&#38382;&#31572;&#20219;&#21153;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#36824;&#38656;&#35201;&#35757;&#32451;&#21644;&#23454;&#29616;&#21478;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#22870;&#21169;&#27169;&#22411;&#65289;&#65292;&#20197;&#35780;&#20998;&#24182;&#30830;&#23450;&#31572;&#26696;&#26159;&#21542;&#36866;&#29992;&#20110;&#32473;&#23450;&#30340;&#38382;&#39064;&#12290;&#35813;&#32452;&#20214;&#26377;&#21161;&#20110;&#25913;&#36827;&#31995;&#32479;&#22238;&#31572;&#30340;&#35299;&#30721;&#21644;&#29983;&#25104;&#12290; BLEU&#21644;perplexity&#31561;&#25968;&#23383;&#24230;&#37327;&#26631;&#20934;&#34987;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#65292;&#21516;&#26102;&#20063;&#20351;&#29992;&#20154;&#31867;&#21028;&#26029;&#26469;&#27604;&#36739;&#35299;&#30721;&#25216;&#26415;&#19982;&#20854;&#20182;&#25216;&#26415;&#12290;&#26368;&#32456;&#65292;&#32467;&#26524;&#25903;&#25345;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#24182;&#30830;&#23450;&#20351;&#29992;&#22870;&#21169;&#27169;&#22411;&#26469;&#23545;&#40784;&#29983;&#25104;&#22238;&#31572;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a methodology to align a medium-sized GPT model, originally trained in English for an open domain, to a small closed domain in Spanish. The application for which the model is finely tuned is the question answering task. To achieve this we also needed to train and implement another neural network (which we called the reward model) that could score and determine whether an answer is appropriate for a given question. This component served to improve the decoding and generation of the answers of the system. Numerical metrics such as BLEU and perplexity were used to evaluate the model, and human judgment was also used to compare the decoding technique with others. Finally, the results favored the proposed method, and it was determined that it is feasible to use a reward model to align the generation of responses.
&lt;/p&gt;</description></item><item><title>&#29992;ChatGPT&#20316;&#20026;&#20219;&#21153;&#35268;&#21010;&#24037;&#20855;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#25511;&#21046;&#22120;&#26469;&#25972;&#21512;&#29616;&#26377;&#30340;AI&#27169;&#22411;&#65292;&#35299;&#20915;&#22797;&#26434;&#30340;AI&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.17580</link><description>&lt;p&gt;
HuggingGPT: &#22312;HugingFace&#20013;&#20351;&#29992;ChatGPT&#21450;&#20854;&#20249;&#20276;&#35299;&#20915;AI&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace. (arXiv:2303.17580v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17580
&lt;/p&gt;
&lt;p&gt;
&#29992;ChatGPT&#20316;&#20026;&#20219;&#21153;&#35268;&#21010;&#24037;&#20855;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#25511;&#21046;&#22120;&#26469;&#25972;&#21512;&#29616;&#26377;&#30340;AI&#27169;&#22411;&#65292;&#35299;&#20915;&#22797;&#26434;&#30340;AI&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#19981;&#21516;&#39046;&#22495;&#21644;&#27169;&#24577;&#30340;&#22797;&#26434;AI&#20219;&#21153;&#26159;&#36890;&#21521;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#25511;&#21046;&#22120;&#26469;&#31649;&#29702;&#29616;&#26377;&#30340;AI&#27169;&#22411;&#20197;&#35299;&#20915;AI&#20219;&#21153;&#65292;&#35821;&#35328;&#25104;&#20026;&#36890;&#29992;&#25509;&#21475;&#26469;&#36171;&#33021;&#23427;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;ChatGPT&#20316;&#20026;&#20219;&#21153;&#35268;&#21010;&#24037;&#20855;&#65292;&#26681;&#25454;HuggingFace&#20013;&#21487;&#29992;&#30340;&#27169;&#22411;&#21151;&#33021;&#25551;&#36848;&#26469;&#36873;&#25321;&#27169;&#22411;&#65292;&#22312;&#36873;&#23450;AI&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#27599;&#20010;&#23376;&#20219;&#21153;&#65292;&#24182;&#24635;&#32467;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence (AGI). While there are abundant AI models available for different domains and modalities, they cannot handle complicated AI tasks. Considering large language models (LLMs) have exhibited exceptional ability in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks and language could be a generic interface to empower this. Based on this philosophy, we present HuggingGPT, a system that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., HuggingFace) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in HuggingFace, execute each subtask with the selected AI model, and summarize the response acco
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#20449;&#24687;&#26102;&#20998;&#35789;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20998;&#35789;&#27169;&#24335;&#21487;&#20197;&#24102;&#26469;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#22522;&#20110;&#23383;&#31526;&#30340;&#27169;&#22411;&#20063;&#26377;&#24076;&#26395;&#65292;&#22240;&#27492;&#26080;&#20998;&#35789;&#30340;&#20449;&#24687;&#25277;&#21462;&#27169;&#22411;&#26159;&#21487;&#34892;&#30340;&#12290;</title><link>http://arxiv.org/abs/2303.15100</link><description>&lt;p&gt;
&#20449;&#24687;&#25277;&#21462;&#30740;&#31350;&#65306;&#35831;&#27880;&#24847;&#20998;&#35789;&#65281;
&lt;/p&gt;
&lt;p&gt;
An Information Extraction Study: Take In Mind the Tokenization!. (arXiv:2303.15100v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15100
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#20449;&#24687;&#26102;&#20998;&#35789;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20998;&#35789;&#27169;&#24335;&#21487;&#20197;&#24102;&#26469;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#22522;&#20110;&#23383;&#31526;&#30340;&#27169;&#22411;&#20063;&#26377;&#24076;&#26395;&#65292;&#22240;&#27492;&#26080;&#20998;&#35789;&#30340;&#20449;&#24687;&#25277;&#21462;&#27169;&#22411;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#20351;&#29992;&#23383;&#31526;&#32780;&#19981;&#26159;&#20998;&#35789;&#25991;&#26412;&#20316;&#20026;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36755;&#20837;&#30340;&#20248;&#21183;&#21644;&#26435;&#34913;&#30340;&#30740;&#31350;&#24050;&#32463;&#24471;&#21040;&#20102;&#24456;&#22823;&#30340;&#21457;&#23637;&#12290;&#26032;&#30340;&#26080;&#20998;&#35789;&#27169;&#22411;&#28040;&#38500;&#20102;&#20256;&#32479;&#30340;&#20998;&#35789;&#27493;&#39588;&#65292;&#20294;&#23427;&#20204;&#30340;&#25928;&#29575;&#20173;&#19981;&#28165;&#26970;&#12290;&#27492;&#22806;&#65292;&#22312;&#24207;&#21015;&#26631;&#35760;&#20219;&#21153;&#20013;&#65292;&#20998;&#35789;&#30340;&#24433;&#21709;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20174;&#25991;&#26723;&#20013;&#25552;&#21462;&#20449;&#24687;&#26102;&#20998;&#35789;&#30340;&#24433;&#21709;&#65292;&#24182;&#23545;&#22522;&#20110;&#23376;&#35789;&#21644;&#22522;&#20110;&#23383;&#31526;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#21644;&#20998;&#26512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#20013;&#36827;&#34892;&#20449;&#24687;&#25277;&#21462; (IE)&#12290;&#20027;&#35201;&#32467;&#26524;&#26159;&#21452;&#37325;&#30340;&#65306;&#20998;&#35789;&#27169;&#24335;&#21487;&#33021;&#20250;&#24341;&#20837;&#24402;&#32435;&#20559;&#24046;&#65292;&#20174;&#32780;&#20135;&#29983;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#22522;&#20110;&#23383;&#31526;&#30340;&#27169;&#22411;&#20135;&#29983;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#22240;&#27492;&#65292;&#36716;&#25442;&#20026;&#26080;&#20998;&#35789;&#30340;IE&#27169;&#22411;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current research on the advantages and trade-offs of using characters, instead of tokenized text, as input for deep learning models, has evolved substantially. New token-free models remove the traditional tokenization step; however, their efficiency remains unclear. Moreover, the effect of tokenization is relatively unexplored in sequence tagging tasks. To this end, we investigate the impact of tokenization when extracting information from documents and present a comparative study and analysis of subword-based and character-based models. Specifically, we study Information Extraction (IE) from biomedical texts. The main outcome is twofold: tokenization patterns can introduce inductive bias that results in state-of-the-art performance, and the character-based models produce promising results; thus, transitioning to token-free IE models is feasible.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#21307;&#23398;&#39046;&#22495;&#21033;&#29992;LLaMA&#27169;&#22411;&#24494;&#35843;&#30340;&#21307;&#30103;&#32842;&#22825;&#27169;&#22411;ChatDoctor&#12290;&#32463;&#36807;700&#22810;&#31181;&#30142;&#30149;&#21644;&#20854;&#30456;&#24212;&#30151;&#29366;&#12289;&#33647;&#21697;&#21644;&#21307;&#30103;&#26816;&#26597;&#30340;&#25910;&#38598;&#21644;&#22788;&#29702;&#65292;&#36825;&#31181;&#27169;&#22411;&#20855;&#26377;&#29702;&#35299;&#24739;&#32773;&#38656;&#27714;&#12289;&#25552;&#20379;&#24314;&#35758;&#21644;&#24110;&#21161;&#30340;&#28508;&#21147;&#12290;&#36825;&#20123;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#21040;&#21307;&#30103;&#20445;&#20581;&#20013;&#21487;&#20197;&#26497;&#22823;&#22320;&#25913;&#36827;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#21644;&#24739;&#32773;&#30340;&#27807;&#36890;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2303.14070</link><description>&lt;p&gt;
ChatDoctor&#65306;&#20351;&#29992;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#22312;LLaMA&#27169;&#22411;&#19978;&#24494;&#35843;&#30340;&#21307;&#30103;&#32842;&#22825;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ChatDoctor: A Medical Chat Model Fine-tuned on LLaMA Model using Medical Domain Knowledge. (arXiv:2303.14070v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#21307;&#23398;&#39046;&#22495;&#21033;&#29992;LLaMA&#27169;&#22411;&#24494;&#35843;&#30340;&#21307;&#30103;&#32842;&#22825;&#27169;&#22411;ChatDoctor&#12290;&#32463;&#36807;700&#22810;&#31181;&#30142;&#30149;&#21644;&#20854;&#30456;&#24212;&#30151;&#29366;&#12289;&#33647;&#21697;&#21644;&#21307;&#30103;&#26816;&#26597;&#30340;&#25910;&#38598;&#21644;&#22788;&#29702;&#65292;&#36825;&#31181;&#27169;&#22411;&#20855;&#26377;&#29702;&#35299;&#24739;&#32773;&#38656;&#27714;&#12289;&#25552;&#20379;&#24314;&#35758;&#21644;&#24110;&#21161;&#30340;&#28508;&#21147;&#12290;&#36825;&#20123;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#21040;&#21307;&#30103;&#20445;&#20581;&#20013;&#21487;&#20197;&#26497;&#22823;&#22320;&#25913;&#36827;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#21644;&#24739;&#32773;&#30340;&#27807;&#36890;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#19968;&#33324;&#39046;&#22495;&#20013;&#24212;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#20363;&#22914;ChatGPT&#65292;&#24050;&#32463;&#34920;&#29616;&#20986;&#20223;&#20315;&#26159;&#20154;&#31867;&#35762;&#35805;&#33324;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#35821;&#35328;&#27169;&#22411;&#24182;&#27809;&#26377;&#32463;&#36807;&#20010;&#21035;&#19988;&#20180;&#32454;&#20026;&#21307;&#23398;&#39046;&#22495;&#23398;&#20064;&#65292;&#23548;&#33268;&#35786;&#26029;&#20934;&#30830;&#24230;&#20302;&#19988;&#19981;&#33021;&#32473;&#20986;&#27491;&#30830;&#30340;&#21307;&#30103;&#35786;&#26029;&#12289;&#33647;&#21697;&#31561;&#24314;&#35758;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;700&#22810;&#31181;&#30142;&#30149;&#21450;&#20854;&#30456;&#24212;&#30151;&#29366;&#12289;&#25512;&#33616;&#33647;&#21697;&#21644;&#25152;&#38656;&#21307;&#30103;&#26816;&#26597;&#65292;&#28982;&#21518;&#29983;&#25104;&#20102;5K&#21517;&#21307;&#24739;&#30340;&#23545;&#35805;&#12290;&#36890;&#36807;&#24494;&#35843;&#21307;&#24739;&#23545;&#35805;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#20102;&#29702;&#35299;&#24739;&#32773;&#38656;&#27714;&#12289;&#25552;&#20379;&#26126;&#26234;&#24314;&#35758;&#24182;&#22312;&#21508;&#31181;&#21307;&#30103;&#30456;&#20851;&#39046;&#22495;&#25552;&#20379;&#23453;&#36149;&#24110;&#21161;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#23558;&#36825;&#20123;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#21040;&#21307;&#30103;&#20445;&#20581;&#20013;&#65292;&#21487;&#20197;&#24443;&#24213;&#25913;&#21464;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#21644;&#24739;&#32773;&#30340;&#27807;&#36890;&#26041;&#24335;&#65292;&#26368;&#32456;&#25913;&#21892;&#25972;&#20307;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent large language models (LLMs) in the general domain, such as ChatGPT, have shown remarkable success in following instructions and producing human-like responses. However, such language models have not been learned individually and carefully for the medical domain, resulting in poor diagnostic accuracy and inability to give correct recommendations for medical diagnosis, medications, etc. To address this issue, we collected more than 700 diseases and their corresponding symptoms, recommended medications, and required medical tests, and then generated 5K doctor-patient conversations. By fine-tuning models of doctor-patient conversations, these models emerge with great potential to understand patients' needs, provide informed advice, and offer valuable assistance in a variety of medical-related fields. The integration of these advanced language models into healthcare can revolutionize the way healthcare professionals and patients communicate, ultimately improving the overall quality 
&lt;/p&gt;</description></item><item><title>RepoCoder&#26159;&#19968;&#20010;&#31616;&#21333;&#12289;&#36890;&#29992;&#21644;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#19968;&#20010;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#26816;&#32034;&#22120;&#21644;&#39044;&#35757;&#32451;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#24211;&#32423;&#21035;&#20195;&#30721;&#23436;&#25104;&#27969;&#31243;&#20013;&#23454;&#29616;&#20102;&#26377;&#25928;&#21033;&#29992;&#24211;&#32423;&#21035;&#20449;&#24687;&#36827;&#34892;&#20195;&#30721;&#23436;&#25104;&#21644;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2303.12570</link><description>&lt;p&gt;
RepoCoder&#65306;&#36890;&#36807;&#36845;&#20195;&#26816;&#32034;&#21644;&#29983;&#25104;&#23454;&#29616;&#30340;&#20195;&#30721;&#23384;&#20648;&#24211;&#32423;&#21035;&#23436;&#25104;
&lt;/p&gt;
&lt;p&gt;
RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation. (arXiv:2303.12570v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12570
&lt;/p&gt;
&lt;p&gt;
RepoCoder&#26159;&#19968;&#20010;&#31616;&#21333;&#12289;&#36890;&#29992;&#21644;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#19968;&#20010;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#26816;&#32034;&#22120;&#21644;&#39044;&#35757;&#32451;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#24211;&#32423;&#21035;&#20195;&#30721;&#23436;&#25104;&#27969;&#31243;&#20013;&#23454;&#29616;&#20102;&#26377;&#25928;&#21033;&#29992;&#24211;&#32423;&#21035;&#20449;&#24687;&#36827;&#34892;&#20195;&#30721;&#23436;&#25104;&#21644;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24211;&#32423;&#21035;&#20195;&#30721;&#23436;&#25104;&#20219;&#21153;&#26159;&#22522;&#20110;&#20195;&#30721;&#24211;&#26356;&#24191;&#38420;&#19978;&#19979;&#25991;&#20013;&#32487;&#32493;&#32534;&#20889;&#26410;&#23436;&#25104;&#20195;&#30721;&#30340;&#36807;&#31243;&#12290;&#20294;&#26159;&#23545;&#20110;&#33258;&#21160;&#23436;&#25104;&#24037;&#20855;&#32780;&#35328;&#65292;&#24456;&#38590;&#21033;&#29992;&#25955;&#24067;&#22312;&#19981;&#21516;&#25991;&#20214;&#20013;&#30340;&#26377;&#29992;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;RepoCoder&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#12289;&#36890;&#29992;&#21644;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#12290;&#23427;&#36890;&#36807;&#25972;&#21512;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#26816;&#32034;&#22120;&#21644;&#39044;&#35757;&#32451;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#31616;&#21270;&#20102;&#24211;&#32423;&#21035;&#20195;&#30721;&#23436;&#25104;&#27969;&#31243;&#65292;&#20174;&#32780;&#20801;&#35768;&#26377;&#25928;&#21033;&#29992;&#24211;&#32423;&#21035;&#20449;&#24687;&#36827;&#34892;&#20195;&#30721;&#23436;&#25104;&#65292;&#24182;&#20855;&#26377;&#19981;&#21516;&#31890;&#24230;&#23618;&#38754;&#30340;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;RepoCoder &#36824;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#26816;&#32034;-&#29983;&#25104;&#27169;&#22411;&#65292;&#24357;&#21512;&#20102;&#26816;&#32034;&#19978;&#19979;&#25991;&#21644;&#39044;&#26399;&#23436;&#25104;&#30446;&#26631;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;RepoEval&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#26368;&#26032;&#21644;&#39640;&#36136;&#37327;&#30495;&#23454;&#19990;&#30028;&#30340;&#20195;&#30721;&#24211;&#65292;&#28085;&#30422;&#20102;&#34892;&#12289;API &#35843;&#29992;&#21644;&#20989;&#25968;&#20307;&#23436;&#25104;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of repository-level code completion is to continue writing the unfinished code based on a broader context of the repository. While for automated code completion tools, it is difficult to utilize the useful information scattered in different files. We propose RepoCoder, a simple, generic, and effective framework to address the challenge. It streamlines the repository-level code completion process by incorporating a similarity-based retriever and a pre-trained code language model, which allows for the effective utilization of repository-level information for code completion and grants the ability to generate code at various levels of granularity. Furthermore, RepoCoder utilizes a novel iterative retrieval-generation paradigm that bridges the gap between retrieval context and the intended completion target. We also propose a new benchmark RepoEval, which consists of the latest and high-quality real-world repositories covering line, API invocation, and function body completion sce
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23545; 33 &#31181;&#35821;&#35328;&#20013; 8 &#20010;&#19981;&#21516;&#20219;&#21153;&#30340;&#29983;&#25104; AI &#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#27604;&#36739;&#20102;&#29983;&#25104; LLMs &#21644;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#34920;&#29616;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2303.12528</link><description>&lt;p&gt;
MEGA: &#22810;&#35821;&#35328;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#32508;&#21512;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
MEGA: Multilingual Evaluation of Generative AI. (arXiv:2303.12528v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12528
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23545; 33 &#31181;&#35821;&#35328;&#20013; 8 &#20010;&#19981;&#21516;&#20219;&#21153;&#30340;&#29983;&#25104; AI &#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#27604;&#36739;&#20102;&#29983;&#25104; LLMs &#21644;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#34920;&#29616;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;AI&#27169;&#22411;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65288;&#22914;&#35821;&#35328;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#35821;&#35328;&#29983;&#25104;&#65289;&#19978;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#24403;&#20170;AI&#31038;&#21306;&#26368;&#37325;&#35201;&#30340;&#38382;&#39064;&#20043;&#19968;&#26159;&#20851;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#33021;&#21147;&#21644;&#38480;&#21046;&#65292;&#35780;&#20272;&#29983;&#25104;AI&#26174;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22823;&#22810;&#25968;&#20851;&#20110;&#29983;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30740;&#31350;&#37117;&#38480;&#20110;&#33521;&#35821;&#65292;&#19981;&#28165;&#26970;&#36825;&#20123;&#27169;&#22411;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#20854;&#20182;&#35821;&#35328;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#39318;&#20010;&#20840;&#38754;&#35780;&#20272; 8 &#39033;&#19981;&#21516;&#20219;&#21153;&#21644; 33 &#31181;&#35821;&#35328;&#30340;&#29983;&#25104;LLMs MEGA &#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#36824;&#23558;&#29983;&#25104;LLMs&#30340;&#24615;&#33021;&#19982;&#36825;&#20123;&#20219;&#21153;&#19978;&#26368;&#20808;&#36827;&#30340;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#30830;&#23450;&#29983;&#25104;&#27169;&#22411;&#30340;&#34920;&#29616;&#22914;&#20309;&#19982;&#19978;&#19968;&#20195;LLMs&#30456;&#27604;&#12290;&#25105;&#20204;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#24443;&#24213;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI models have impressive performance on many Natural Language Processing tasks such as language understanding, reasoning and language generation. One of the most important questions that is being asked by the AI community today is about the capabilities and limits of these models, and it is clear that evaluating generative AI is very challenging. Most studies on generative Large Language Models (LLMs) are restricted to English and it is unclear how capable these models are at understanding and generating other languages. We present the first comprehensive benchmarking of generative LLMs MEGA, which evaluates models on standard NLP benchmarks, covering 8 diverse tasks and 33 typologically diverse languages. We also compare the performance of generative LLMs to State of the Art (SOTA) non-autoregressive models on these tasks to determine how well generative models perform compared to the previous generation of LLMs. We present a thorough analysis of the performance of model
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;Transformer&#30340;&#22359;&#20301;&#21387;&#32553;&#26041;&#27861;&#65292;&#31216;&#20026;BBCT&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26356;&#32454;&#31890;&#24230;&#22320;&#21387;&#32553;&#25972;&#20010;Transformer&#65292;&#21253;&#25324;&#23884;&#20837;&#12289;&#30697;&#38453;&#20056;&#27861;&#12289;GELU&#12289;softmax&#12289;&#23618;&#24402;&#19968;&#21270;&#21644;&#25152;&#26377;&#20013;&#38388;&#32467;&#26524;&#12290;&#22312;GLUE&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;BBCT&#21487;&#20197;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#23454;&#29616;&#23569;&#20110;1&#65285;&#30340;&#20934;&#30830;&#29575;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2303.09184</link><description>&lt;p&gt;
&#22522;&#20110;&#22359;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#20301;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Block-wise Bit-Compression of Transformer-based Models. (arXiv:2303.09184v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;Transformer&#30340;&#22359;&#20301;&#21387;&#32553;&#26041;&#27861;&#65292;&#31216;&#20026;BBCT&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26356;&#32454;&#31890;&#24230;&#22320;&#21387;&#32553;&#25972;&#20010;Transformer&#65292;&#21253;&#25324;&#23884;&#20837;&#12289;&#30697;&#38453;&#20056;&#27861;&#12289;GELU&#12289;softmax&#12289;&#23618;&#24402;&#19968;&#21270;&#21644;&#25152;&#26377;&#20013;&#38388;&#32467;&#26524;&#12290;&#22312;GLUE&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;BBCT&#21487;&#20197;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#23454;&#29616;&#23569;&#20110;1&#65285;&#30340;&#20934;&#30830;&#29575;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;BERT&#12289;GPT-3&#21644;ChatGPT&#31561;&#36817;&#26399;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#30340;&#27969;&#34892;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;Transformer&#27169;&#22411;&#30340;&#24040;&#22823;&#35745;&#31639;&#37327;&#12289;&#24040;&#22823;&#30340;&#20869;&#23384;&#21344;&#29992;&#21644;&#39640;&#24310;&#36831;&#26159;&#20113;&#35745;&#31639;&#20013;&#19981;&#21487;&#36991;&#20813;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BBCT&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#29992;&#20110;Transformer&#30340;&#22359;&#20301;&#21387;&#32553;&#26041;&#27861;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#25972;&#20010;Transformer&#30340;&#26356;&#32454;&#31890;&#24230;&#30340;&#21387;&#32553;&#65292;&#21253;&#25324;&#23884;&#20837;&#12289;&#30697;&#38453;&#20056;&#27861;&#12289;GELU&#12289;softmax&#12289;&#23618;&#24402;&#19968;&#21270;&#21644;&#25152;&#26377;&#20013;&#38388;&#32467;&#26524;&#12290;&#25105;&#20204;&#20197;&#39640;&#25928;BERT&#20026;&#26696;&#20363;&#65292;&#20351;&#29992;BBCT&#26041;&#27861;&#36827;&#34892;&#21387;&#32553;&#12290;&#25105;&#20204;&#22312;General Language Understanding Evaluation(GLUE)&#25968;&#25454;&#38598;&#19978;&#30340;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;BBCT&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#30340;&#20934;&#30830;&#24230;&#19979;&#38477;&#23567;&#20110;1&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the popularity of the recent Transformer-based models represented by BERT, GPT-3 and ChatGPT, there has been state-of-the-art performance in a range of natural language processing tasks. However, the massive computations, huge memory footprint, and thus high latency of Transformer-based models is an inevitable challenge for the cloud with high real-time requirement. To tackle the issue, we propose BBCT, a method of block-wise bit-compression for transformer without retraining. Our method achieves more fine-grained compression of the whole transformer, including embedding, matrix multiplication, GELU, softmax, layer normalization, and all the intermediate results. As a case, we compress an efficient BERT with the method of BBCT. Our benchmark test results on General Language Understanding Evaluation (GLUE) show that BBCT can achieve less than 1% accuracy drop in most tasks.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#26080;&#30417;&#30563;&#26412;&#20307;&#21305;&#37197;&#25110;&#26412;&#20307;&#23545;&#40784;&#35270;&#20026;&#32763;&#35793;&#20219;&#21153;&#30340;&#26032;&#35270;&#35282;&#30340;Truveta Mapper&#26694;&#26550;&#65292;&#22312;&#38646;&#26679;&#26412;&#12289;&#32479;&#19968;&#21644;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#19979;&#25191;&#34892;&#22810;&#26412;&#20307;&#23545;&#40784;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#38388;&#24310;&#36831;&#21644;&#23545;&#40784;&#36136;&#37327;&#26041;&#38754;&#32988;&#36807;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#65292;&#26080;&#38656;&#26174;&#24335;&#36328;&#26412;&#20307;&#25163;&#21160;&#26631;&#27880;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2301.09767</link><description>&lt;p&gt;
Truveta Mapper&#65306;&#19968;&#20010;&#38646;&#26679;&#26412;&#26412;&#20307;&#26144;&#23556;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Truveta Mapper: A Zero-shot Ontology Alignment Framework. (arXiv:2301.09767v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09767
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#26080;&#30417;&#30563;&#26412;&#20307;&#21305;&#37197;&#25110;&#26412;&#20307;&#23545;&#40784;&#35270;&#20026;&#32763;&#35793;&#20219;&#21153;&#30340;&#26032;&#35270;&#35282;&#30340;Truveta Mapper&#26694;&#26550;&#65292;&#22312;&#38646;&#26679;&#26412;&#12289;&#32479;&#19968;&#21644;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#19979;&#25191;&#34892;&#22810;&#26412;&#20307;&#23545;&#40784;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#38388;&#24310;&#36831;&#21644;&#23545;&#40784;&#36136;&#37327;&#26041;&#38754;&#32988;&#36807;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#65292;&#26080;&#38656;&#26174;&#24335;&#36328;&#26412;&#20307;&#25163;&#21160;&#26631;&#27880;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26080;&#30417;&#30563;&#26412;&#20307;&#21305;&#37197;(Ontology Matching, OM)&#25110;&#26412;&#20307;&#23545;&#40784;(Ontology Alignment, OA)&#35270;&#20026;&#32763;&#35793;&#20219;&#21153;&#30340;&#26032;&#35270;&#35282;&#12290;&#23558;&#26412;&#20307;&#34920;&#31034;&#20026;&#22270;&#24418;&#65292;&#22312;&#28304;&#26412;&#20307;&#22270;&#20013;&#30340;&#33410;&#28857;&#21040;&#30446;&#26631;&#26412;&#20307;&#22270;&#20013;&#30340;&#36335;&#24452;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#12290;&#25152;&#25552;&#20986;&#30340;Truveta Mapper (TM)&#26694;&#26550;&#21033;&#29992;&#22810;&#20219;&#21153;&#24207;&#21015;&#21040;&#24207;&#21015;&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;&#22312;&#38646;&#26679;&#26412;&#12289;&#32479;&#19968;&#21644;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#19979;&#25191;&#34892;&#22810;&#26412;&#20307;&#23545;&#40784;&#12290;&#22810;&#20219;&#21153;&#20351;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#26469;&#38544;&#21547;&#22320;&#23398;&#20064;&#19981;&#21516;&#26412;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#26080;&#38656;&#20219;&#20309;&#26174;&#24335;&#30340;&#36328;&#26412;&#20307;&#25163;&#21160;&#26631;&#27880;&#25968;&#25454;&#12290;&#36825;&#20063;&#20351;&#24471;&#35813;&#26694;&#26550;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#38388;&#24310;&#36831;&#21644;&#23545;&#40784;&#36136;&#37327;&#26041;&#38754;&#32988;&#36807;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#12290;&#27169;&#22411;&#20165;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#25991;&#26412;&#35821;&#26009;&#24211;&#21644;&#20869;&#37096;&#26412;&#20307;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#12290;&#35813;&#26041;&#26696;&#20248;&#20110;&#29616;&#26377;&#26631;&#20934;&#22522;&#20934;&#35299;&#20915;&#26041;&#26696;&#65292;&#22914;Edit-Similarity&#21644;MINTE+&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, a new perspective is suggested for unsupervised Ontology Matching (OM) or Ontology Alignment (OA) by treating it as a translation task. Ontologies are represented as graphs, and the translation is performed from a node in the source ontology graph to a path in the target ontology graph. The proposed framework, Truveta Mapper (TM), leverages a multi-task sequence-to-sequence transformer model to perform alignment across multiple ontologies in a zero-shot, unified and end-to-end manner. Multi-tasking enables the model to implicitly learn the relationship between different ontologies via transfer-learning without requiring any explicit cross-ontology manually labeled data. This also enables the formulated framework to outperform existing solutions for both runtime latency and alignment quality. The model is pre-trained and fine-tuned only on publicly available text corpus and inner-ontologies data. The proposed solution outperforms state-of-the-art approaches, Edit-Similari
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;pAbT5&#65292;&#36825;&#26159;&#19968;&#31181;&#33021;&#22815;&#37319;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#20026;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#29983;&#25104;&#37197;&#23545;&#25239;&#20307;&#38142;&#24207;&#21015;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#20934;&#30830;&#22320;&#39044;&#27979;&#37197;&#23545;&#24773;&#20917;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#23454;&#39564;&#39564;&#35777;&#26041;&#38754;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2301.02748</link><description>&lt;p&gt;
&#22522;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#30340;&#37197;&#23545;&#25239;&#20307;&#38142;&#24207;&#21015;&#26465;&#20214;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Conditional Generation of Paired Antibody Chain Sequences through Encoder-Decoder Language Model. (arXiv:2301.02748v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.02748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;pAbT5&#65292;&#36825;&#26159;&#19968;&#31181;&#33021;&#22815;&#37319;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#20026;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#29983;&#25104;&#37197;&#23545;&#25239;&#20307;&#38142;&#24207;&#21015;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#20934;&#30830;&#22320;&#39044;&#27979;&#37197;&#23545;&#24773;&#20917;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#23454;&#39564;&#39564;&#35777;&#26041;&#38754;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#22312;&#24207;&#21015;&#12289;&#32467;&#26500;&#21644;&#21151;&#33021;&#39044;&#27979;&#26041;&#38754;&#20855;&#26377;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#20165;&#38480;&#20110;&#21333;&#19968;&#24207;&#21015;&#30340;&#32534;&#30721;&#22120;&#25110;&#35299;&#30721;&#22120;&#32467;&#26500;&#65292;&#32780;&#35768;&#22810;&#29983;&#29289;&#23398;&#29615;&#22659;&#28041;&#21450;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;pAbT5&#65292;&#23427;&#37319;&#29992;&#22522;&#20110;T5&#30340;&#26550;&#26500;&#23558;&#25239;&#20307;&#38142;&#37197;&#23545;&#24314;&#27169;&#20026;&#27491;&#21521;&#21644;&#21453;&#21521;&#32763;&#35793;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;pAbT5&#36890;&#36807;&#24207;&#21015;&#29983;&#25104;&#20934;&#30830;&#22320;&#21453;&#26144;&#20102;&#38142;&#30340;&#37197;&#23545;&#12290;&#25105;&#20204;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#21487;&#21464;&#38271;&#24230;&#30340;&#24207;&#21015;&#65292;&#20854;&#19979;&#19968;&#20010;&#35789;&#35821;&#30340;&#39044;&#27979;&#27010;&#29575;&#19982;&#24207;&#21015;&#27604;&#23545;&#30340;&#20301;&#32622;&#29305;&#24322;&#24615;&#35780;&#20998;&#30697;&#38453;&#19968;&#33268;&#12290;&#20687;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20854;&#20182;&#30740;&#31350;&#19968;&#26679;&#65292;pAbT5&#22312;&#23454;&#39564;&#39564;&#35777;&#26041;&#38754;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#39044;&#27979;&#33021;&#21147;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;pAbT5&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#30340;&#29983;&#25104;&#24335;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Protein language models (LMs) have been successful in sequence, structural and functional predictions. However, currently, protein LMs are limited to encoder- or decoder-only architectures for single sequences while many biological contexts involve protein-protein interactions. Here, we introduce pAbT5, which models antibody chain pairing as forward- and back-translations using a T5-based architecture. We show that pAbT5 accurately reflects chain pairing through sequence generation. Our protein LM generates variable-length sequences and its next-word prediction probability agrees with position-specific scoring matrix from sequence alignment. Like other works in protein LM, pAbT5 performs state-of-the-art unsupervised prediction on experimental measurements. To the best of our knowledge, pAbT5 is the first generative encoder-decoder protein LM for protein-protein interactions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21307;&#23398;&#30693;&#35782;&#22686;&#24378;&#30340;&#25918;&#23556;&#23398;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21033;&#29992;&#19977;&#20803;&#32452;&#25552;&#21462;&#27169;&#22359;&#21644;&#21307;&#23398;&#30693;&#35782;&#24211;&#26597;&#35810;&#23454;&#20307;&#32763;&#35793;&#31561;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#21307;&#23398;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#38544;&#21547;&#20197;&#21450;&#21307;&#23398;&#35786;&#26029;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2301.02228</link><description>&lt;p&gt;
MedKLIP&#65306;&#22522;&#20110;&#21307;&#23398;&#30693;&#35782;&#22686;&#24378;&#30340;&#25918;&#23556;&#23398;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
MedKLIP: Medical Knowledge Enhanced Language-Image Pre-Training in Radiology. (arXiv:2301.02228v3 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.02228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21307;&#23398;&#30693;&#35782;&#22686;&#24378;&#30340;&#25918;&#23556;&#23398;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21033;&#29992;&#19977;&#20803;&#32452;&#25552;&#21462;&#27169;&#22359;&#21644;&#21307;&#23398;&#30693;&#35782;&#24211;&#26597;&#35810;&#23454;&#20307;&#32763;&#35793;&#31561;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#21307;&#23398;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#38544;&#21547;&#20197;&#21450;&#21307;&#23398;&#35786;&#26029;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#21033;&#29992;&#25918;&#23556;&#23398;&#26085;&#24120;&#23454;&#36341;&#20013;&#25104;&#23545;&#30340;&#22270;&#20687;-&#25991;&#23383;&#25253;&#21578;&#22686;&#24378;&#21307;&#23398;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#65288;VLP&#65289;&#20013;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20197;&#19979;&#36129;&#29486;&#65306;&#39318;&#20808;&#65292;&#19981;&#21516;&#20110;&#30452;&#25509;&#22788;&#29702;&#21407;&#22987;&#25253;&#21578;&#30340;&#29616;&#26377;&#24037;&#20316;&#65292;&#25105;&#20204;&#37319;&#29992;&#19977;&#20803;&#32452;&#25552;&#21462;&#27169;&#22359;&#25552;&#21462;&#21307;&#23398;&#30456;&#20851;&#20449;&#24687;&#65292;&#36991;&#20813;&#35821;&#35328;&#35821;&#27861;&#20013;&#30340;&#19981;&#24517;&#35201;&#22797;&#26434;&#24615;&#24182;&#22686;&#24378;&#30417;&#30563;&#20449;&#21495;&#65307;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#19977;&#20803;&#32452;&#32534;&#30721;&#27169;&#22359;&#65292;&#36890;&#36807;&#26597;&#35810;&#30693;&#35782;&#24211;&#36827;&#34892;&#23454;&#20307;&#32763;&#35793;&#65292;&#21033;&#29992;&#21307;&#23398;&#39046;&#22495;&#20016;&#23500;&#30340;&#39046;&#22495;&#30693;&#35782;&#65292;&#24182;&#22312;&#35821;&#35328;&#23884;&#20837;&#31354;&#38388;&#20013;&#38544;&#21547;&#22320;&#24314;&#31435;&#21307;&#23398;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65307;&#31532;&#19977;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#34701;&#21512;&#27169;&#22411;&#65292;&#22312;&#22270;&#20687;&#34917;&#19969;&#32423;&#21035;&#19978;&#31354;&#38388;&#23545;&#40784;&#23454;&#20307;&#25551;&#36848;&#21644;&#35270;&#35273;&#20449;&#21495;&#65292;&#23454;&#29616;&#21307;&#23398;&#35786;&#26029;&#30340;&#33021;&#21147;&#65307;&#31532;&#22235;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#20197;&#39564;&#35777;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider enhancing medical visual-language pre-training (VLP) with domain-specific knowledge, by exploiting the paired image-text reports from the radiological daily practice. In particular, we make the following contributions: First, unlike existing works that directly process the raw reports, we adopt a novel triplet extraction module to extract the medical-related information, avoiding unnecessary complexity from language grammar and enhancing the supervision signals; Second, we propose a novel triplet encoding module with entity translation by querying a knowledge base, to exploit the rich domain knowledge in medical field, and implicitly build relationships between medical entities in the language embedding space; Third, we propose to use a Transformer-based fusion model for spatially aligning the entity description with visual signals at the image patch level, enabling the ability for medical diagnosis; Fourth, we conduct thorough experiments to validate the eff
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#32447;&#24615;&#35268;&#21010;&#21333;&#35789;&#38382;&#39064;&#36716;&#25442;&#20026;&#25968;&#23398;&#20844;&#24335;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#36755;&#20837;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#24182;&#22686;&#24378;&#36755;&#20837;&#20197;&#31361;&#20986;&#36825;&#20123;&#23454;&#20307;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24230;&#65292;&#36194;&#24471;&#20102;NL4Opt&#31454;&#36187;&#29983;&#25104;&#36187;&#36947;&#30340;&#31532;&#19968;&#21517;&#12290;</title><link>http://arxiv.org/abs/2212.13201</link><description>&lt;p&gt;
&#36755;&#20837;&#21629;&#21517;&#23454;&#20307;&#33258;&#21160;&#35299;&#26512;&#20248;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Highlighting Named Entities in Input for Auto-Formulation of Optimization Problems. (arXiv:2212.13201v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.13201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#32447;&#24615;&#35268;&#21010;&#21333;&#35789;&#38382;&#39064;&#36716;&#25442;&#20026;&#25968;&#23398;&#20844;&#24335;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#36755;&#20837;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#24182;&#22686;&#24378;&#36755;&#20837;&#20197;&#31361;&#20986;&#36825;&#20123;&#23454;&#20307;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24230;&#65292;&#36194;&#24471;&#20102;NL4Opt&#31454;&#36187;&#29983;&#25104;&#36187;&#36947;&#30340;&#31532;&#19968;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#31609;&#23398;&#26159;&#23558;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#24314;&#27169;&#20026;&#25968;&#23398;&#20248;&#21270;&#38382;&#39064;&#26469;&#35299;&#20915;&#30340;&#12290;&#34429;&#28982;&#35299;&#20915;&#25968;&#23398;&#31995;&#32479;&#30340;&#38382;&#39064;&#26159;&#30001;&#20998;&#26512;&#36719;&#20214;&#23436;&#25104;&#30340;&#65292;&#20294;&#23558;&#38382;&#39064;&#20316;&#20026;&#19968;&#32452;&#25968;&#23398;&#25805;&#20316;&#36827;&#34892;&#34920;&#36798;&#36890;&#24120;&#26159;&#30001;&#39046;&#22495;&#19987;&#23478;&#25163;&#21160;&#23436;&#25104;&#30340;&#12290;&#26368;&#36817;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26174;&#31034;&#20986;&#23558;&#25991;&#26412;&#38382;&#39064;&#25551;&#36848;&#36716;&#25442;&#20026;&#30456;&#24212;&#30340;&#25968;&#23398;&#20844;&#24335;&#30340;&#21069;&#26223;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#32447;&#24615;&#35268;&#21010;&#21333;&#35789;&#38382;&#39064;&#36716;&#25442;&#20026;&#25968;&#23398;&#20844;&#24335;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#36755;&#20837;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#24182;&#22686;&#24378;&#36755;&#20837;&#20197;&#31361;&#20986;&#36825;&#20123;&#23454;&#20307;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;NL4Opt&#31454;&#36187;&#30340;&#25152;&#26377;&#25552;&#20132;&#20013;&#33719;&#24471;&#20102;&#26368;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#33719;&#24471;&#20102;&#29983;&#25104;&#36187;&#36947;&#30340;&#31532;&#19968;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
Operations research deals with modeling and solving real-world problems as mathematical optimization problems. While solving mathematical systems is accomplished by analytical software, formulating a problem as a set of mathematical operations has been typically done manually by domain experts. Recent machine learning methods have shown promise in converting textual problem descriptions to corresponding mathematical formulations. This paper presents an approach that converts linear programming word problems into mathematical formulations. We leverage the named entities in the input and augment the input to highlight these entities. Our approach achieves the highest accuracy among all submissions to the NL4Opt Competition, securing first place in the generation track.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27979;&#37327;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#21512;&#25104;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#24182;&#20174;&#24418;&#23481;&#35789;&#20462;&#39280;&#21517;&#35789;&#30701;&#35821;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#19977;&#20010;&#26032;&#30340;&#21512;&#25104;&#34892;&#20026;&#27979;&#35797;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#21069;&#30340;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#21482;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#31526;&#21512;&#39044;&#26399;&#30340;&#35821;&#35328;&#29702;&#35770;&#12290;</title><link>http://arxiv.org/abs/2212.04310</link><description>&lt;p&gt;
&#33945;&#22612;&#21476;&#35821;&#20041;&#19982;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20462;&#39280;&#19968;&#33268;&#24615;&#27979;&#37327;
&lt;/p&gt;
&lt;p&gt;
Montague semantics and modifier consistency measurement in neural language models. (arXiv:2212.04310v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04310
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27979;&#37327;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#21512;&#25104;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#24182;&#20174;&#24418;&#23481;&#35789;&#20462;&#39280;&#21517;&#35789;&#30701;&#35821;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#19977;&#20010;&#26032;&#30340;&#21512;&#25104;&#34892;&#20026;&#27979;&#35797;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#21069;&#30340;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#21482;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#31526;&#21512;&#39044;&#26399;&#30340;&#35821;&#35328;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#20960;&#24180;&#20013;&#65292;&#20998;&#24067;&#24335;&#35821;&#35328;&#34920;&#31034;&#27169;&#22411;&#24050;&#32463;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#21516;&#26102;&#65292;&#21487;&#35299;&#37322;&#24615;&#30340;&#38656;&#27714;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#23427;&#20204;&#30340;&#26412;&#36136;&#23646;&#24615;&#21644;&#33021;&#21147;&#30340;&#36136;&#30097;&#12290;&#23588;&#20854;&#26159;&#65292;&#20998;&#24067;&#24335;&#27169;&#22411;&#22312;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#30340;&#32452;&#21512;&#29616;&#35937;&#26102;&#24448;&#24448;&#19981;&#19968;&#33268;&#65292;&#36825;&#23545;&#23427;&#20204;&#30340;&#23433;&#20840;&#24615;&#21644;&#20844;&#24179;&#24615;&#20855;&#26377;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#30446;&#21069;&#22823;&#22810;&#25968;&#26377;&#20851;&#21512;&#25104;&#24615;&#30340;&#30740;&#31350;&#21482;&#26159;&#38024;&#23545;&#25913;&#21892;&#23427;&#20204;&#22312;&#30456;&#20284;&#24615;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#26412;&#30740;&#31350;&#37319;&#21462;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27979;&#37327;&#24403;&#20195;&#35821;&#35328;&#27169;&#22411;&#32452;&#25104;&#24615;&#34892;&#20026;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#24418;&#23481;&#35789;&#20462;&#39280;&#21517;&#35789;&#30701;&#35821;&#20013;&#30340;&#24418;&#23481;&#35789;&#20462;&#39280;&#29616;&#35937;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#20010;&#28789;&#24863;&#26469;&#33258;&#33945;&#22612;&#21476;&#35821;&#24847;&#30340;&#21512;&#25104;&#34892;&#20026;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#21069;&#30340;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#21482;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#31526;&#21512;&#39044;&#26399;&#30340;&#35821;&#35328;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, distributional language representation models have demonstrated great practical success. At the same time, the need for interpretability has elicited questions on their intrinsic properties and capabilities. Crucially, distributional models are often inconsistent when dealing with compositional phenomena in natural language, which has significant implications for their safety and fairness. Despite this, most current research on compositionality is directed towards improving their performance on similarity tasks only. This work takes a different approach, and proposes a methodology for measuring compositional behavior in contemporary language models. Specifically, we focus on adjectival modifier phenomena in adjective-noun phrases. We introduce three novel tests of compositional behavior inspired by Montague semantics. Our experimental results indicate that current neural language models behave according to the expected linguistic theories to a limited extent only. This
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;HamNoSys&#31526;&#21495;&#36716;&#25442;&#20026;&#25163;&#35821;&#23039;&#21183;&#24207;&#21015;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;&#24314;&#31435;&#25991;&#26412;&#21644;&#23039;&#21183;&#38388;&#30340;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#65292;&#21487;&#29992;&#20110;&#19981;&#21516;&#25163;&#35821;&#20043;&#38388;&#30340;&#36890;&#29992;&#32763;&#35793;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36317;&#31163;&#27979;&#37327;&#26041;&#27861;&#21487;&#20197;&#24230;&#37327;&#25163;&#35821;&#23039;&#21183;&#24207;&#21015;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;</title><link>http://arxiv.org/abs/2211.13613</link><description>&lt;p&gt;
Ham2Pose&#65306;&#23558;&#25163;&#35821;&#31526;&#21495;&#36716;&#21270;&#25104;&#23039;&#21183;&#24207;&#21015;&#30340;&#21160;&#30011;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Ham2Pose: Animating Sign Language Notation into Pose Sequences. (arXiv:2211.13613v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13613
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;HamNoSys&#31526;&#21495;&#36716;&#25442;&#20026;&#25163;&#35821;&#23039;&#21183;&#24207;&#21015;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;&#24314;&#31435;&#25991;&#26412;&#21644;&#23039;&#21183;&#38388;&#30340;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#65292;&#21487;&#29992;&#20110;&#19981;&#21516;&#25163;&#35821;&#20043;&#38388;&#30340;&#36890;&#29992;&#32763;&#35793;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36317;&#31163;&#27979;&#37327;&#26041;&#27861;&#21487;&#20197;&#24230;&#37327;&#25163;&#35821;&#23039;&#21183;&#24207;&#21015;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#21475;&#35821;&#32763;&#35793;&#25104;&#25163;&#35821;&#23545;&#20110;&#32843;&#21548;&#31038;&#21306;&#20043;&#38388;&#30340;&#24320;&#25918;&#24615;&#20132;&#27969;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#23558;HamNoSys&#65292;&#19968;&#31181;&#35789;&#27719;&#25163;&#35821;&#31526;&#21495;&#65292;&#36716;&#25442;&#20026;&#25163;&#35821;&#23039;&#21183;&#24207;&#21015;&#30340;&#21160;&#30011;&#26041;&#27861;&#12290;&#30001;&#20110;HamNoSys&#26159;&#36890;&#29992;&#35774;&#35745;&#30340;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19981;&#21463;&#30446;&#26631;&#25163;&#35821;&#38480;&#21046;&#30340;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;&#36880;&#28176;&#29983;&#25104;&#23039;&#21183;&#39044;&#27979;&#65292;&#21516;&#26102;&#32771;&#34385;&#23427;&#20204;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20449;&#24687;&#65292;&#20026;&#35757;&#32451;&#36807;&#31243;&#25552;&#20379;&#20102;&#24369;&#30417;&#30563;&#65292;&#24182;&#19988;&#26174;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20174;&#37096;&#20998;&#21644;&#19981;&#20934;&#30830;&#30340;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#26102;&#25104;&#21151;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#36317;&#31163;&#27979;&#37327;&#26041;&#27861;&#65292;&#32771;&#34385;&#32570;&#22833;&#20851;&#38190;&#28857;&#65292;&#20351;&#29992;DTW-MJE&#26469;&#27979;&#37327;&#23039;&#21183;&#24207;&#21015;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#25105;&#20204;&#20351;&#29992;AUTSL&#36825;&#20010;&#22823;&#35268;&#27169;&#25163;&#35821;&#25968;&#25454;&#38598;&#26469;&#39564;&#35777;&#23427;&#30340;&#27491;&#30830;&#24615;&#65292;&#24182;&#19988;&#23637;&#31034;&#23427;&#21487;&#20197;&#24230;&#37327;&#25163;&#35821;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Translating spoken languages into Sign languages is necessary for open communication between the hearing and hearing-impaired communities. To achieve this goal, we propose the first method for animating a text written in HamNoSys, a lexical Sign language notation, into signed pose sequences. As HamNoSys is universal by design, our proposed method offers a generic solution invariant to the target Sign language. Our method gradually generates pose predictions using transformer encoders that create meaningful representations of the text and poses while considering their spatial and temporal information. We use weak supervision for the training process and show that our method succeeds in learning from partial and inaccurate data. Additionally, we offer a new distance measurement that considers missing keypoints, to measure the distance between pose sequences using DTW-MJE. We validate its correctness using AUTSL, a large-scale Sign language dataset, show that it measures the distance betw
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36830;&#32493;&#21521;&#37327;&#31354;&#38388;&#20013;&#34920;&#31034;&#25968;&#23398;&#34920;&#36798;&#24335;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#26694;&#26550;&#30340;&#32534;&#30721;&#22120;&#29983;&#25104;&#21521;&#37327;&#34920;&#31034;&#26469;&#25429;&#25417;&#25968;&#23398;&#35821;&#20041;&#65292;&#27604;&#33258;&#21160;&#32534;&#30721;&#22120;&#25928;&#26524;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2211.08142</link><description>&lt;p&gt;
&#36830;&#32493;&#21521;&#37327;&#31354;&#38388;&#20013;&#30340;&#25968;&#23398;&#34920;&#36798;&#24335;&#35821;&#20041;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Semantic Representations of Mathematical Expressions in a Continuous Vector Space. (arXiv:2211.08142v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08142
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36830;&#32493;&#21521;&#37327;&#31354;&#38388;&#20013;&#34920;&#31034;&#25968;&#23398;&#34920;&#36798;&#24335;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#26694;&#26550;&#30340;&#32534;&#30721;&#22120;&#29983;&#25104;&#21521;&#37327;&#34920;&#31034;&#26469;&#25429;&#25417;&#25968;&#23398;&#35821;&#20041;&#65292;&#27604;&#33258;&#21160;&#32534;&#30721;&#22120;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#31526;&#21495;&#22312;STEM&#25991;&#29486;&#20013;&#21344;&#25454;&#20102;&#24456;&#22823;&#19968;&#37096;&#20998;&#65292;&#20294;&#26159;&#65292;&#20026;&#20844;&#24335;&#25214;&#21040;&#35821;&#20041;&#34920;&#31034;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#30001;&#20110;&#25968;&#23398;&#31526;&#21495;&#26159;&#31934;&#30830;&#30340;&#65292;&#22312;&#23383;&#31526;&#24494;&#23567;&#21464;&#21270;&#26102;&#20854;&#21547;&#20041;&#20250;&#21457;&#29983;&#26174;&#33879;&#21464;&#21270;&#65292;&#22240;&#27492;&#36866;&#29992;&#20110;&#33258;&#28982;&#25991;&#26412;&#30340;&#26041;&#27861;&#24182;&#19981;&#19968;&#23450;&#36866;&#29992;&#20110;&#25968;&#23398;&#34920;&#36798;&#24335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#22312;&#36830;&#32493;&#21521;&#37327;&#31354;&#38388;&#20013;&#34920;&#31034;&#25968;&#23398;&#34920;&#36798;&#24335;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;&#26694;&#26550;&#30340;&#32534;&#30721;&#22120;&#65292;&#35757;&#32451;&#20854;&#22312;&#35270;&#35273;&#19978;&#19981;&#21516;&#20294;&#22312;&#25968;&#23398;&#19978;&#31561;&#20215;&#30340;&#34920;&#36798;&#24335;&#19978;&#29983;&#25104;&#21521;&#37327;&#34920;&#31034;&#65288;&#25110;&#23884;&#20837;&#65289;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#19982;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#34920;&#26126;&#21069;&#32773;&#26356;&#33021;&#25429;&#25417;&#25968;&#23398;&#35821;&#20041;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#20419;&#36827;&#26410;&#26469;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20221;&#31561;&#20215;&#30340;&#36229;&#36234;&#21644;&#20195;&#25968;&#34920;&#36798;&#24335;&#23545;&#30340;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mathematical notation makes up a large portion of STEM literature, yet, finding semantic representations for formulae remains a challenging problem. Because mathematical notation is precise, and its meaning changes significantly with small character shifts, the methods that work for natural text do not necessarily work well for mathematical expressions. In this work, we describe an approach for representing mathematical expressions in a continuous vector space. We use the encoder of a sequence-to-sequence architecture, trained on visually different but mathematically equivalent expressions, to generate vector representations (or embeddings). We compare this approach with an autoencoder and show that the former is better at capturing mathematical semantics. Finally, to expedite future research, we publish a corpus of equivalent transcendental and algebraic expression pairs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29616;&#20195;NLP&#31995;&#32479;&#20013;&#31038;&#20132;&#26234;&#33021;&#21644;&#24515;&#29702;&#29702;&#35770;&#30340;&#38382;&#39064;&#12290;&#20316;&#32773;&#20351;&#29992;&#20004;&#20010;&#20219;&#21153;&#35780;&#20272;&#27169;&#22411;&#22312;&#29702;&#35299;&#31038;&#20132;&#20114;&#21160;&#24847;&#22270;&#21644;&#25512;&#26029;&#21442;&#19982;&#32773;&#24515;&#29702;&#29366;&#24577;&#21644;&#29616;&#23454;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#24403;&#21069;&#26368;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;GPT-3&#32570;&#20047;&#36825;&#31181;&#33021;&#21147;&#12290;&#36825;&#19968;&#21457;&#29616;&#24378;&#35843;&#20102;NLP&#31995;&#32479;&#22312;&#29702;&#35299;&#31038;&#20132;&#21160;&#24577;&#26041;&#38754;&#30340;&#24403;&#21069;&#23616;&#38480;&#24615;&#65292;&#24182;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2210.13312</link><description>&lt;p&gt;
&#31070;&#32463;&#24515;&#29702;&#29702;&#35770;&#65311;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#31038;&#20132;&#26234;&#33021;&#23616;&#38480;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs. (arXiv:2210.13312v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29616;&#20195;NLP&#31995;&#32479;&#20013;&#31038;&#20132;&#26234;&#33021;&#21644;&#24515;&#29702;&#29702;&#35770;&#30340;&#38382;&#39064;&#12290;&#20316;&#32773;&#20351;&#29992;&#20004;&#20010;&#20219;&#21153;&#35780;&#20272;&#27169;&#22411;&#22312;&#29702;&#35299;&#31038;&#20132;&#20114;&#21160;&#24847;&#22270;&#21644;&#25512;&#26029;&#21442;&#19982;&#32773;&#24515;&#29702;&#29366;&#24577;&#21644;&#29616;&#23454;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#24403;&#21069;&#26368;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;GPT-3&#32570;&#20047;&#36825;&#31181;&#33021;&#21147;&#12290;&#36825;&#19968;&#21457;&#29616;&#24378;&#35843;&#20102;NLP&#31995;&#32479;&#22312;&#29702;&#35299;&#31038;&#20132;&#21160;&#24577;&#26041;&#38754;&#30340;&#24403;&#21069;&#23616;&#38480;&#24615;&#65292;&#24182;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#26234;&#33021;&#21644;&#24515;&#29702;&#29702;&#35770;&#65288;ToM&#65289;&#21363;&#29702;&#35299;&#25152;&#26377;&#21442;&#19982;&#32773;&#30340;&#19981;&#21516;&#24515;&#29702;&#29366;&#24577;&#12289;&#24847;&#22270;&#21644;&#21453;&#24212;&#30340;&#33021;&#21147;&#65292;&#20351;&#24471;&#20154;&#31867;&#33021;&#22815;&#26377;&#25928;&#22320;&#29702;&#35299;&#21644;&#24212;&#23545;&#26085;&#24120;&#31038;&#20132;&#20114;&#21160;&#12290;&#38543;&#30528;NLP&#31995;&#32479;&#22312;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#31038;&#20132;&#22330;&#26223;&#20013;&#24471;&#21040;&#24212;&#29992;&#65292;&#29702;&#35299;&#31038;&#20132;&#21160;&#24577;&#30340;&#33021;&#21147;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20174;&#23454;&#35777;&#21644;&#29702;&#35770;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#29616;&#20195;NLP&#31995;&#32479;&#20013;&#31038;&#20132;&#26234;&#33021;&#21644;&#24515;&#29702;&#29702;&#35770;&#36825;&#19968;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#39033;&#20219;&#21153;&#65306;SocialIQa&#65288;Sap et al&#12290;&#65292;2019&#65289;&#21644;ToMi&#65288;Le et al&#12290;&#65292;2019&#65289;&#26469;&#34913;&#37327;&#27169;&#22411;&#29702;&#35299;&#31038;&#20132;&#20114;&#21160;&#21442;&#19982;&#32773;&#24847;&#22270;&#21644;&#21453;&#24212;&#20197;&#21450;&#25512;&#26029;&#21442;&#19982;&#32773;&#24515;&#29702;&#29366;&#24577;&#21644;&#29616;&#23454;&#30340;&#33021;&#21147;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#24403;&#20170;&#26368;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;GPT-3&#65307;Brown et al&#12290;&#65292;2020&#65289;&#32570;&#20047;&#36825;&#31181;&#31038;&#20132;&#26234;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;NLP&#31995;&#32479;&#22312;&#29702;&#35299;&#31038;&#20132;&#21160;&#24577;&#26041;&#38754;&#30340;&#24403;&#21069;&#23616;&#38480;&#24615;&#20197;&#21450;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#36825;&#19968;&#39046;&#22495;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social intelligence and Theory of Mind (ToM), i.e., the ability to reason about the different mental states, intents, and reactions of all people involved, allow humans to effectively navigate and understand everyday social interactions. As NLP systems are used in increasingly complex social situations, their ability to grasp social dynamics becomes crucial. In this work, we examine the open question of social intelligence and Theory of Mind in modern NLP systems from an empirical and theory-based perspective. We show that one of today's largest language models (GPT-3; Brown et al., 2020) lacks this kind of social intelligence out-of-the box, using two tasks: SocialIQa (Sap et al., 2019), which measures models' ability to understand intents and reactions of participants of social interactions, and ToMi (Le et al., 2019), which measures whether models can infer mental states and realities of participants of situations. Our results show that models struggle substantially at these Theory 
&lt;/p&gt;</description></item><item><title>BioGPT&#26159;&#19968;&#31181;&#38024;&#23545;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#29983;&#25104;&#24335;Transformer&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#22312;&#22810;&#39033;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#22343;&#34920;&#29616;&#20986;&#33394;&#65292;&#23588;&#20854;&#22312;&#20851;&#31995;&#25552;&#21462;&#20219;&#21153;&#20013;&#34920;&#29616;&#23588;&#20026;&#31361;&#20986;&#12290;</title><link>http://arxiv.org/abs/2210.10341</link><description>&lt;p&gt;
BioGPT&#65306;&#38024;&#23545;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#29983;&#25104;&#21644;&#25366;&#25496;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining. (arXiv:2210.10341v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10341
&lt;/p&gt;
&lt;p&gt;
BioGPT&#26159;&#19968;&#31181;&#38024;&#23545;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#29983;&#25104;&#24335;Transformer&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#22312;&#22810;&#39033;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#22343;&#34920;&#29616;&#20986;&#33394;&#65292;&#23588;&#20854;&#22312;&#20851;&#31995;&#25552;&#21462;&#20219;&#21153;&#20013;&#34920;&#29616;&#23588;&#20026;&#31361;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#25104;&#21151;&#65292;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20063;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#36890;&#29992;&#35821;&#35328;&#39046;&#22495;&#20013;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20004;&#20010;&#20027;&#35201;&#20998;&#25903;&#26159;BERT&#65288;&#21450;&#20854;&#21464;&#20307;&#65289;&#21644;GPT&#65288;&#21450;&#20854;&#21464;&#20307;&#65289;&#12290;&#24050;&#32463;&#26377;&#24456;&#22810;&#38024;&#23545;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#22914;BioBERT&#21644;PubMedBERT&#65292;&#22312;&#22810;&#31181;&#21028;&#21035;&#24335;&#19979;&#28216;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#32570;&#20047;&#29983;&#25104;&#33021;&#21147;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#29983;&#25104;&#24335;Transformer&#35821;&#35328;&#27169;&#22411;BioGPT&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#19978;&#20248;&#20110;&#20043;&#21069;&#30340;&#27169;&#22411;&#12290;&#29305;&#21035;&#22320;&#65292;&#22312;BC5CDR&#12289;KD-DTI&#21644;DDI&#20851;&#31995;&#25552;&#21462;&#20219;&#21153;&#19978;&#65292;&#25105;&#20204;&#20998;&#21035;&#33719;&#24471;&#20102;44.98&#65285;&#65292;38.42&#65285;&#21644;40.76&#65285;&#30340;F1&#24471;&#20998;&#65292;&#20197;&#21450;&#22312;PubMedQA&#20219;&#21153;&#19978;&#30340;78.2&#65285;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models have attracted increasing attention in the biomedical domain, inspired by their great success in the general natural language domain. Among the two main branches of pre-trained language models in the general language domain, i.e., BERT (and its variants) and GPT (and its variants), the first one has been extensively studied in the biomedical domain, such as BioBERT and PubMedBERT. While they have achieved great success on a variety of discriminative downstream biomedical tasks, the lack of generation ability constrains their application scope. In this paper, we propose BioGPT, a domain-specific generative Transformer language model pre-trained on large scale biomedical literature. We evaluate BioGPT on six biomedical NLP tasks and demonstrate that our model outperforms previous models on most tasks. Especially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI end-to-end relation extraction tasks respectively, and 78.2% accuracy on PubMedQA
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#25351;&#26631;&#65292;&#27604;&#29616;&#26377;&#30340;&#25688;&#35201;&#35780;&#20272;&#25351;&#26631;&#26356;&#22909;&#65292;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#19988;&#26356;&#21152;&#40065;&#26834;&#65292;&#19982;&#29616;&#26377;&#25351;&#26631;&#30456;&#32467;&#21512;&#21487;&#20197;&#20351;&#35780;&#20272;&#25928;&#26524;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2208.07316</link><description>&lt;p&gt;
MENLI: &#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
MENLI: Robust Evaluation Metrics from Natural Language Inference. (arXiv:2208.07316v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.07316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#25351;&#26631;&#65292;&#27604;&#29616;&#26377;&#30340;&#25688;&#35201;&#35780;&#20272;&#25351;&#26631;&#26356;&#22909;&#65292;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#19988;&#26356;&#21152;&#40065;&#26834;&#65292;&#19982;&#29616;&#26377;&#25351;&#26631;&#30456;&#32467;&#21512;&#21487;&#20197;&#20351;&#35780;&#20272;&#25928;&#26524;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#34987;&#25552;&#20986;&#30340;&#22522;&#20110;BERT&#30340;&#25991;&#26412;&#29983;&#25104;&#35780;&#20272;&#25351;&#26631;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#26131;&#21463;&#21040;&#23545;&#20449;&#24687;&#27491;&#30830;&#24615;&#30340;&#25915;&#20987;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#37096;&#20998;&#21407;&#22240;&#26159;&#27492;&#31867;&#27169;&#22411;&#26159;&#22522;&#20110;&#35821;&#20041;&#30456;&#20284;&#24615;&#24314;&#27169;&#30340;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#25351;&#26631;&#65292;&#36825;&#31181;&#25351;&#26631;&#26356;&#36866;&#21512;&#24314;&#27169;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#20559;&#22909;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#26694;&#26550;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#30340;NLI&#22522;&#30784;&#25351;&#26631;&#27604;&#26368;&#36817;&#30340;BERT&#22522;&#30784;&#25351;&#26631;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;NLI&#22522;&#30784;&#25351;&#26631;&#20248;&#20110;&#29616;&#26377;&#30340;&#25688;&#35201;&#35780;&#20272;&#25351;&#26631;&#65292;&#20294;&#20302;&#20110;SOTA MT&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#26377;&#25351;&#26631;&#19982;&#25105;&#20204;&#30340;NLI&#25351;&#26631;&#30456;&#32467;&#21512;&#26102;&#65292;&#25105;&#20204;&#26082;&#33719;&#24471;&#20102;&#26356;&#39640;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65288;15&#65285;-30&#65285;&#65289;&#65292;&#21448;&#33719;&#24471;&#20102;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#26356;&#39640;&#30340;&#36136;&#37327;&#25351;&#26631;&#65288;+5&#65285;&#33267;30&#65285;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently proposed BERT-based evaluation metrics for text generation perform well on standard benchmarks but are vulnerable to adversarial attacks, e.g., relating to information correctness. We argue that this stems (in part) from the fact that they are models of semantic similarity. In contrast, we develop evaluation metrics based on Natural Language Inference (NLI), which we deem a more appropriate modeling. We design a preference-based adversarial attack framework and show that our NLI based metrics are much more robust to the attacks than the recent BERT-based metrics. On standard benchmarks, our NLI based metrics outperform existing summarization metrics, but perform below SOTA MT metrics. However, when combining existing metrics with our NLI metrics, we obtain both higher adversarial robustness (15%-30%) and higher quality metrics as measured on standard benchmarks (+5% to 30%).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;377808&#39318;&#33521;&#25991;&#27468;&#26354;&#27468;&#35789;&#36827;&#34892;&#22823;&#35268;&#27169;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#21450;&#26102;&#30340;&#24615;&#21035;&#27495;&#35270;&#30340;&#22686;&#21152;&#20197;&#21450;&#19981;&#21516;&#24615;&#21035;&#34920;&#28436;&#32773;&#30340;&#35821;&#35328;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2208.02052</link><description>&lt;p&gt;
&#27468;&#35789;&#20013;&#30340;&#24615;&#21035;&#27495;&#35270;&#21644;&#24615;&#21035;&#20559;&#35265;&#30340;&#22823;&#35268;&#27169;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Large scale analysis of gender bias and sexism in song lyrics. (arXiv:2208.02052v3 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.02052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;377808&#39318;&#33521;&#25991;&#27468;&#26354;&#27468;&#35789;&#36827;&#34892;&#22823;&#35268;&#27169;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#21450;&#26102;&#30340;&#24615;&#21035;&#27495;&#35270;&#30340;&#22686;&#21152;&#20197;&#21450;&#19981;&#21516;&#24615;&#21035;&#34920;&#28436;&#32773;&#30340;&#35821;&#35328;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#20998;&#26512;&#20102;&#8220;Two Million Song Database&#8221;&#35821;&#26009;&#24211;&#20013;377808&#39318;&#33521;&#25991;&#27468;&#26354;&#27468;&#35789;&#65292;&#30528;&#37325;&#20998;&#26512;&#20102;&#20116;&#21313;&#24180;&#65288;1960-2010&#65289;&#38388;&#24615;&#21035;&#27495;&#35270;&#30340;&#34920;&#36798;&#65292;&#20197;&#21450;&#23545;&#24615;&#21035;&#20559;&#24046;&#30340;&#35780;&#27979;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#24615;&#21035;&#27495;&#35270;&#20998;&#31867;&#22120;&#65292;&#25105;&#20204;&#22312;&#36739;&#22823;&#30340;&#35268;&#27169;&#19978;&#35782;&#21035;&#20102;&#24615;&#21035;&#27495;&#35270;&#27468;&#35789;&#65292;&#36828;&#36229;&#21069;&#20154;&#29992;&#25163;&#21160;&#26631;&#27880;&#27969;&#34892;&#27468;&#26354;&#30340;&#23567;&#26679;&#26412;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22312;&#27468;&#26354;&#27468;&#35789;&#19978;&#23398;&#20064;&#30340;&#35789;&#23884;&#20837;&#26469;&#34913;&#37327;&#20851;&#32852;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#24615;&#21035;&#20559;&#35265;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23588;&#20854;&#26159;&#30001;&#30007;&#24615;&#33402;&#26415;&#23478;&#28436;&#21809;&#30340;&#27969;&#34892;&#27468;&#26354;&#20013;&#30340;&#24615;&#21035;&#27495;&#35270;&#20869;&#23481;&#22312;&#26102;&#38388;&#19978;&#21576;&#36880;&#28176;&#22686;&#22810;&#30340;&#36235;&#21183;&#12290;&#26681;&#25454;&#34920;&#28436;&#32773;&#30340;&#24615;&#21035;&#19981;&#21516;&#65292;&#27468;&#26354;&#36824;&#26174;&#31034;&#20986;&#19981;&#21516;&#30340;&#35821;&#35328;&#20559;&#35265;&#65292;&#30007;&#24615;&#29420;&#21809;&#33402;&#26415;&#23478;&#30340;&#27468;&#26354;&#20013;&#21253;&#21547;&#26356;&#22810;&#21644;&#26356;&#24378;&#30340;&#20559;&#35265;&#12290;&#36825;&#26159;&#31532;&#19968;&#27425;&#36827;&#34892;&#36825;&#31181;&#22823;&#35268;&#27169;&#30340;&#20998;&#26512;&#65292;&#20026;&#25105;&#20204;&#25581;&#31034;&#20102;&#27969;&#34892;&#25991;&#21270;&#36825;&#19968;&#37325;&#35201;&#37096;&#20998;&#30340;&#35821;&#35328;&#29992;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We employ Natural Language Processing techniques to analyse 377808 English song lyrics from the "Two Million Song Database" corpus, focusing on the expression of sexism across five decades (1960-2010) and the measurement of gender biases. Using a sexism classifier, we identify sexist lyrics at a larger scale than previous studies using small samples of manually annotated popular songs. Furthermore, we reveal gender biases by measuring associations in word embeddings learned on song lyrics. We find sexist content to increase across time, especially from male artists and for popular songs appearing in Billboard charts. Songs are also shown to contain different language biases depending on the gender of the performer, with male solo artist songs containing more and stronger biases. This is the first large scale analysis of this type, giving insights into language usage in such an influential part of popular culture.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#35270;&#22270;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;&#65292;&#20854;&#20013;DH-KG&#21253;&#21547;&#36229;&#20851;&#31995;&#23454;&#20363;&#35270;&#22270;&#21644;&#20174;&#23454;&#20307;&#23618;&#27425;&#25277;&#35937;&#20986;&#30340;&#36229;&#20851;&#31995;&#26412;&#20307;&#35270;&#22270;&#65292;&#23450;&#20041;&#20102;&#38142;&#25509;&#39044;&#27979;&#21644;&#23454;&#20307;&#31867;&#22411;&#35782;&#21035;&#20219;&#21153;&#65292;&#24182;&#26500;&#24314;&#20102;&#20004;&#20010;DH-KG&#25968;&#25454;&#38598;JW44K-6K&#21644;HTDM&#12290;DHGE&#26159;&#19968;&#20010;&#22522;&#20110;GRAN&#32534;&#30721;&#22120;&#12289;HGNN&#21644;&#32852;&#21512;&#23398;&#20064;&#30340;DH-KG&#23884;&#20837;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2207.08562</link><description>&lt;p&gt;
DHGE&#65306;&#21452;&#35270;&#22270;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#23884;&#20837;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#21644;&#23454;&#20307;&#31867;&#22411;
&lt;/p&gt;
&lt;p&gt;
DHGE: Dual-View Hyper-Relational Knowledge Graph Embedding for Link Prediction and Entity Typing. (arXiv:2207.08562v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.08562
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#35270;&#22270;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;&#65292;&#20854;&#20013;DH-KG&#21253;&#21547;&#36229;&#20851;&#31995;&#23454;&#20363;&#35270;&#22270;&#21644;&#20174;&#23454;&#20307;&#23618;&#27425;&#25277;&#35937;&#20986;&#30340;&#36229;&#20851;&#31995;&#26412;&#20307;&#35270;&#22270;&#65292;&#23450;&#20041;&#20102;&#38142;&#25509;&#39044;&#27979;&#21644;&#23454;&#20307;&#31867;&#22411;&#35782;&#21035;&#20219;&#21153;&#65292;&#24182;&#26500;&#24314;&#20102;&#20004;&#20010;DH-KG&#25968;&#25454;&#38598;JW44K-6K&#21644;HTDM&#12290;DHGE&#26159;&#19968;&#20010;&#22522;&#20110;GRAN&#32534;&#30721;&#22120;&#12289;HGNN&#21644;&#32852;&#21512;&#23398;&#20064;&#30340;DH-KG&#23884;&#20837;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#36229;&#20851;&#31995;&#20107;&#23454;&#30001;&#19968;&#20010;&#20027;&#19977;&#20803;&#32452;&#21644;&#20960;&#20010;&#36741;&#21161;&#30340;&#23646;&#24615;-&#20540;&#25551;&#36848;&#32452;&#25104;&#65292;&#34987;&#35748;&#20026;&#27604;&#22522;&#20110;&#19977;&#20803;&#32452;&#30340;&#20107;&#23454;&#26356;&#20840;&#38754;&#21644;&#20855;&#20307;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#21333;&#35270;&#22270;&#30340;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;&#24212;&#29992;&#21463;&#21040;&#38480;&#21046;&#65292;&#22240;&#20026;&#23427;&#20204;&#24369;&#21270;&#20102;&#34920;&#31034;&#23454;&#20307;&#20043;&#38388;&#20146;&#23646;&#20851;&#31995;&#30340;&#20998;&#23618;&#32467;&#26500;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#36229;&#20851;&#31995;&#23454;&#20363;&#35270;&#22270;&#21644;&#20174;&#23454;&#20307;&#23618;&#27425;&#25277;&#35937;&#20986;&#30340;&#36229;&#20851;&#31995;&#26412;&#20307;&#35270;&#22270;&#30340;&#21452;&#35270;&#22270;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#35889;&#32467;&#26500;&#65288;DH-KG&#65289;&#12290;&#26412;&#25991;&#39318;&#27425;&#22312;DH-KG&#19978;&#23450;&#20041;&#20102;&#38142;&#25509;&#39044;&#27979;&#21644;&#23454;&#20307;&#31867;&#22411;&#35782;&#21035;&#20219;&#21153;&#65292;&#24182;&#26500;&#24314;&#20102;&#20004;&#20010;DH-KG&#25968;&#25454;&#38598;&#65292;JW44K-6K&#65292;&#20174;&#32500;&#22522;&#25968;&#25454;&#20013;&#25552;&#21462;&#65292;&#21644;&#22522;&#20110;&#21307;&#23398;&#25968;&#25454;&#30340;HTDM&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DHGE&#65292;&#19968;&#20010;&#22522;&#20110;GRAN&#32534;&#30721;&#22120;&#12289;HGNN&#21644;&#32852;&#21512;&#23398;&#20064;&#30340;DH-KG&#23884;&#20837;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of representation learning on knowledge graphs (KGs), a hyper-relational fact consists of a main triple and several auxiliary attribute-value descriptions, which is considered more comprehensive and specific than a triple-based fact. However, currently available hyper-relational KG embedding methods in a single view are limited in application because they weaken the hierarchical structure that represents the affiliation between entities. To overcome this limitation, we propose a dual-view hyper-relational KG structure (DH-KG) that contains a hyper-relational instance view for entities and a hyper-relational ontology view for concepts that are abstracted hierarchically from the entities. This paper defines link prediction and entity typing tasks on DH-KG for the first time and constructs two DH-KG datasets, JW44K-6K, extracted from Wikidata, and HTDM based on medical data. Furthermore, we propose DHGE, a DH-KG embedding model based on GRAN encoders, HGNNs, and joint learnin
&lt;/p&gt;</description></item><item><title>ART&#26159;&#19968;&#31181;&#33021;&#22815;&#19981;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#65292;&#24182;&#19988;&#21482;&#38656;&#35201;&#35775;&#38382;&#26410;&#37197;&#23545;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#12290;&#23427;&#20351;&#29992;&#19968;&#20010;&#26032;&#30340;&#25991;&#26723;&#26816;&#32034;&#33258;&#32534;&#30721;&#26041;&#26696;&#65292;&#36890;&#36807;&#38382;&#39064;&#37325;&#26500;&#36827;&#34892;&#26816;&#32034;&#35757;&#32451;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#26080;&#30417;&#30563;&#23398;&#20064;&#65292;&#24182;&#19988;&#21487;&#20197;&#23558;&#20854;&#21512;&#24182;&#21040;&#23436;&#25972;&#30340;Open QA&#31995;&#32479;&#20013;&#12290;</title><link>http://arxiv.org/abs/2206.10658</link><description>&lt;p&gt;
&#31572;&#26696;&#26377;&#35299;&#65306;&#26080;&#38656;&#26631;&#35760;&#25968;&#25454;&#26469;&#35757;&#32451;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Questions Are All You Need to Train a Dense Passage Retriever. (arXiv:2206.10658v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.10658
&lt;/p&gt;
&lt;p&gt;
ART&#26159;&#19968;&#31181;&#33021;&#22815;&#19981;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#65292;&#24182;&#19988;&#21482;&#38656;&#35201;&#35775;&#38382;&#26410;&#37197;&#23545;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#12290;&#23427;&#20351;&#29992;&#19968;&#20010;&#26032;&#30340;&#25991;&#26723;&#26816;&#32034;&#33258;&#32534;&#30721;&#26041;&#26696;&#65292;&#36890;&#36807;&#38382;&#39064;&#37325;&#26500;&#36827;&#34892;&#26816;&#32034;&#35757;&#32451;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#26080;&#30417;&#30563;&#23398;&#20064;&#65292;&#24182;&#19988;&#21487;&#20197;&#23558;&#20854;&#21512;&#24182;&#21040;&#23436;&#25972;&#30340;Open QA&#31995;&#32479;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;ART&#65292;&#19968;&#31181;&#26032;&#30340;&#35821;&#26009;&#24211;&#32423;&#33258;&#32534;&#30721;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#22312;&#24320;&#25918;&#22495;&#20219;&#21153;&#20013;&#65292;&#22914;&#24320;&#25918;&#24335;&#38382;&#31572;&#65288;Open QA&#65289;&#20013;&#65292;&#23494;&#38598;&#26816;&#32034;&#26159;&#19968;&#20010;&#20013;&#24515;&#25361;&#25112;&#65292;&#20854;&#20013;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#30417;&#30563;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#33258;&#23450;&#20041;&#30340;&#30828;&#36127;&#38754;&#25366;&#25496;&#21644;&#27491;&#38754;&#31034;&#20363;&#21435;&#22122;&#22768;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;ART&#21482;&#38656;&#35201;&#35775;&#38382;&#26410;&#37197;&#23545;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#65288;&#20363;&#22914;&#65292;&#38382;&#39064;&#21644;&#28508;&#22312;&#31572;&#26696;&#25991;&#20214;&#65289;&#12290;&#23427;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#25991;&#26723;&#26816;&#32034;&#33258;&#32534;&#30721;&#26041;&#26696;&#65292;&#20854;&#20013;&#65288;1&#65289;&#36755;&#20837;&#38382;&#39064;&#29992;&#20110;&#26816;&#32034;&#19968;&#32452;&#35777;&#25454;&#25991;&#26723;&#65292;&#65288;2&#65289;&#28982;&#21518;&#20351;&#29992;&#25991;&#26723;&#35745;&#31639;&#37325;&#26500;&#21407;&#22987;&#38382;&#39064;&#30340;&#27010;&#29575;&#12290;&#22522;&#20110;&#38382;&#39064;&#37325;&#26500;&#30340;&#26816;&#32034;&#35757;&#32451;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#26080;&#30417;&#30563;&#23398;&#20064;&#65292;&#21253;&#25324;&#25991;&#26723;&#21644;&#38382;&#39064;&#32534;&#30721;&#22120;&#65292;&#21487;&#20197;&#31245;&#21518;&#23558;&#20854;&#21512;&#24182;&#21040;&#23436;&#25972;&#30340;Open QA&#31995;&#32479;&#20013;&#65292;&#32780;&#19981;&#38656;&#35201;&#36827;&#19968;&#27493;&#36827;&#34892;&#24494;&#35843;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ART&#33719;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce ART, a new corpus-level autoencoding approach for training dense retrieval models that does not require any labeled training data. Dense retrieval is a central challenge for open-domain tasks, such as Open QA, where state-of-the-art methods typically require large supervised datasets with custom hard-negative mining and denoising of positive examples. ART, in contrast, only requires access to unpaired inputs and outputs (e.g. questions and potential answer documents). It uses a new document-retrieval autoencoding scheme, where (1) an input question is used to retrieve a set of evidence documents, and (2) the documents are then used to compute the probability of reconstructing the original question. Training for retrieval based on question reconstruction enables effective unsupervised learning of both document and question encoders, which can be later incorporated into complete Open QA systems without any further finetuning. Extensive experiments demonstrate that ART obtain
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#28176;&#36827;&#38750;&#32467;&#26500;&#21270;&#24133;&#20540;&#20462;&#21098;&#36827;&#34892;&#20462;&#21098;&#30340;&#27169;&#22411;&#22914;&#20309;&#22312;&#39046;&#22495;&#21644;&#20219;&#21153;&#20043;&#38388;&#36827;&#34892;&#36716;&#31227;&#12290;&#20351;&#29992;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#34987;&#20462;&#21098;&#27169;&#22411;&#33021;&#22815;&#22312;&#19981;&#36827;&#34892;&#24191;&#27867;&#30340;&#36229;&#21442;&#25968;&#25506;&#32034;&#25110;&#19987;&#38376;&#26041;&#27861;&#30340;&#24773;&#20917;&#19979;&#36716;&#31227;&#21040;&#26032;&#30340;&#39046;&#22495;&#21644;&#20219;&#21153;&#12290;&#22312;&#29983;&#29289;&#21307;&#23398;NLP&#20219;&#21153;&#20013;&#65292;Sparse*BERT&#21487;&#20197;&#36798;&#21040;&#25110;&#36229;&#36807;BioBERT&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2205.12452</link><description>&lt;p&gt;
&#31232;&#30095;*BERT&#65306;&#31232;&#30095;&#27169;&#22411;&#33021;&#22815;&#27867;&#21270;&#21040;&#26032;&#30340;&#20219;&#21153;&#21644;&#39046;&#22495;&#65288;&#32763;&#35793;&#33258;arXiv:2205.12452v2 [cs.CL] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
Sparse*BERT: Sparse Models Generalize To New tasks and Domains. (arXiv:2205.12452v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#28176;&#36827;&#38750;&#32467;&#26500;&#21270;&#24133;&#20540;&#20462;&#21098;&#36827;&#34892;&#20462;&#21098;&#30340;&#27169;&#22411;&#22914;&#20309;&#22312;&#39046;&#22495;&#21644;&#20219;&#21153;&#20043;&#38388;&#36827;&#34892;&#36716;&#31227;&#12290;&#20351;&#29992;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#34987;&#20462;&#21098;&#27169;&#22411;&#33021;&#22815;&#22312;&#19981;&#36827;&#34892;&#24191;&#27867;&#30340;&#36229;&#21442;&#25968;&#25506;&#32034;&#25110;&#19987;&#38376;&#26041;&#27861;&#30340;&#24773;&#20917;&#19979;&#36716;&#31227;&#21040;&#26032;&#30340;&#39046;&#22495;&#21644;&#20219;&#21153;&#12290;&#22312;&#29983;&#29289;&#21307;&#23398;NLP&#20219;&#21153;&#20013;&#65292;Sparse*BERT&#21487;&#20197;&#36798;&#21040;&#25110;&#36229;&#36807;BioBERT&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#22823;&#22810;&#25968;&#29616;&#20195;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31995;&#32479;&#30340;&#26680;&#24515;&#26550;&#26500;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#22312;&#20219;&#21153;&#21644;&#39046;&#22495;&#20043;&#38388;&#22987;&#32456;&#25552;&#20379;&#21331;&#36234;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#20294;&#20854;&#39640;&#35745;&#31639;&#24320;&#38144;&#21487;&#33021;&#20250;&#20351;&#25512;&#29702;&#21464;&#24471;&#22256;&#38590;&#21644;&#26114;&#36149;&#12290;&#20026;&#20102;&#20351;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#25104;&#26412;&#26356;&#20302;&#65292;&#36817;&#26399;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#20462;&#21098;&#12289;&#37327;&#21270;&#21644;&#33976;&#39311;&#26469;&#25552;&#39640;&#25512;&#29702;&#36895;&#24230;&#24182;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#28176;&#36827;&#38750;&#32467;&#26500;&#21270;&#24133;&#20540;&#20462;&#21098;&#36827;&#34892;&#20462;&#21098;&#30340;&#27169;&#22411;&#22914;&#20309;&#22312;&#39046;&#22495;&#21644;&#20219;&#21153;&#20043;&#38388;&#36827;&#34892;&#36716;&#31227;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#34987;&#20462;&#21098;&#27169;&#22411;&#33021;&#22815;&#22312;&#19981;&#36827;&#34892;&#24191;&#27867;&#30340;&#36229;&#21442;&#25968;&#25506;&#32034;&#25110;&#19987;&#38376;&#26041;&#27861;&#30340;&#24773;&#20917;&#19979;&#36716;&#31227;&#21040;&#26032;&#30340;&#39046;&#22495;&#21644;&#20219;&#21153;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#31232;&#30095;&#36890;&#29992;&#27169;&#22411;Sparse*BERT&#21487;&#20197;&#36890;&#36807;&#22312;&#38750;&#32467;&#26500;&#21270;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#19978;&#39044;&#35757;&#32451;&#21387;&#32553;&#30340;&#26550;&#26500;&#32780;&#25104;&#20026;SparseBioBERT&#65292;&#24182;&#19988;&#22312;&#22810;&#31181;&#29983;&#29289;&#21307;&#23398;NLP&#20219;&#21153;&#20013;&#21487;&#20197;&#36798;&#21040;&#25110;&#36229;&#36807;BioBERT&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models have become the core architecture upon which most modern natural language processing (NLP) systems build. These models can consistently deliver impressive accuracy and robustness across tasks and domains, but their high computational overhead can make inference difficult and expensive. To make using these models less costly, recent work has explored leveraging structured and unstructured pruning, quantization, and distillation to improve inference speed and decrease size. This paper studies how models pruned using Gradual Unstructured Magnitude Pruning can transfer between domains and tasks. Our experimentation shows that models that are pruned during pretraining using general domain masked language models can transfer to novel domains and tasks without extensive hyperparameter exploration or specialized approaches. We demonstrate that our general sparse model Sparse*BERT can become SparseBioBERT simply by pretraining the compressed architecture on unstructured bi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38646;&#26679;&#26412;&#38382;&#39064;&#29983;&#25104;&#27169;&#22411;&#30340;&#37325;&#26032;&#25490;&#24207;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#24320;&#25918;&#24335;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#27573;&#33853;&#26816;&#32034;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#20219;&#20309;&#26816;&#32034;&#26041;&#27861;&#65292;&#26080;&#38656;&#29305;&#23450;&#35757;&#32451;&#65292;&#21487;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#20013;&#25552;&#39640;&#26816;&#32034;&#20934;&#30830;&#24615;&#65292;&#21487;&#20197;&#24110;&#21161;&#25552;&#39640;&#24320;&#25918;&#22495;&#38382;&#31572;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2204.07496</link><description>&lt;p&gt;
&#29992;&#38646;&#26679;&#26412;&#38382;&#39064;&#29983;&#25104;&#25552;&#39640;&#27573;&#33853;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Improving Passage Retrieval with Zero-Shot Question Generation. (arXiv:2204.07496v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.07496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38646;&#26679;&#26412;&#38382;&#39064;&#29983;&#25104;&#27169;&#22411;&#30340;&#37325;&#26032;&#25490;&#24207;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#24320;&#25918;&#24335;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#27573;&#33853;&#26816;&#32034;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#20219;&#20309;&#26816;&#32034;&#26041;&#27861;&#65292;&#26080;&#38656;&#29305;&#23450;&#35757;&#32451;&#65292;&#21487;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#20013;&#25552;&#39640;&#26816;&#32034;&#20934;&#30830;&#24615;&#65292;&#21487;&#20197;&#24110;&#21161;&#25552;&#39640;&#24320;&#25918;&#22495;&#38382;&#31572;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#37325;&#26032;&#25490;&#24207;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#24320;&#25918;&#24335;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#27573;&#33853;&#26816;&#32034;&#12290;&#37325;&#26032;&#25490;&#24207;&#22120;&#20351;&#29992;&#38646;&#26679;&#26412;&#38382;&#39064;&#29983;&#25104;&#27169;&#22411;&#37325;&#26032;&#35780;&#20998;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#22522;&#20110;&#25152;&#26816;&#32034;&#21040;&#30340;&#27573;&#33853;&#30340;&#36755;&#20837;&#38382;&#39064;&#30340;&#27010;&#29575;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#26816;&#32034;&#26041;&#27861;&#65288;&#22914;&#31070;&#32463;&#25110;&#22522;&#20110;&#20851;&#38190;&#23383;&#30340;&#26816;&#32034;&#65289;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#22495;&#25110;&#20219;&#21153;&#29305;&#23450;&#30340;&#35757;&#32451;&#65288;&#22240;&#27492;&#65292;&#39044;&#35745;&#26356;&#22909;&#22320;&#25512;&#24191;&#21040;&#25968;&#25454;&#20998;&#24067;&#36716;&#31227;&#65289;&#65292;&#24182;&#25552;&#20379;&#20102;&#26597;&#35810;&#21644;&#27573;&#33853;&#20043;&#38388;&#30340;&#20016;&#23500;&#20132;&#21449;&#27880;&#24847;&#65288;&#21363;&#24517;&#39035;&#35299;&#37322;&#38382;&#39064;&#20013;&#30340;&#27599;&#20010;&#20196;&#29260;&#65289;&#12290;&#22312;&#22810;&#20010;&#24320;&#25918;&#22495;&#26816;&#32034;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#30340;&#37325;&#26032;&#25490;&#24207;&#22120;&#23558;&#24378;&#22823;&#30340;&#26080;&#30417;&#30563;&#26816;&#32034;&#27169;&#22411;&#20013;&#25552;&#39640;&#20102;6&#65285;-18&#65285;&#65292;&#24182;&#23558;&#24378;&#22823;&#30340;&#30417;&#30563;&#27169;&#22411;&#20013;&#30340;&#21069;20&#20010;&#27573;&#33853;&#26816;&#32034;&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;&#39640;&#36798;12&#65285;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#31616;&#21333;&#22320;&#28155;&#21152;&#26032;&#30340;&#37325;&#26032;&#25490;&#24207;&#22120;&#26469;&#33719;&#24471;&#20102;&#26032;&#30340;&#24320;&#25918;&#22495;&#38382;&#31572;&#30340;&#26368;&#26032;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a simple and effective re-ranking method for improving passage retrieval in open question answering. The re-ranker re-scores retrieved passages with a zero-shot question generation model, which uses a pre-trained language model to compute the probability of the input question conditioned on a retrieved passage. This approach can be applied on top of any retrieval method (e.g. neural or keyword-based), does not require any domain- or task-specific training (and therefore is expected to generalize better to data distribution shifts), and provides rich cross-attention between query and passage (i.e. it must explain every token in the question). When evaluated on a number of open-domain retrieval datasets, our re-ranker improves strong unsupervised retrieval models by 6%-18% absolute and strong supervised models by up to 12% in terms of top-20 passage retrieval accuracy. We also obtain new state-of-the-art results on full open-domain question answering by simply adding the new r
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#30740;&#31350;&#25910;&#38598;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#35270;&#39057;-&#35821;&#35328;&#25925;&#20107;&#25968;&#25454;&#38598;SYMON&#65292;&#29992;&#20110;&#25512;&#36827;&#22810;&#27169;&#24577;&#25925;&#20107;&#29702;&#35299;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2203.05711</link><description>&lt;p&gt;
&#30005;&#24433;&#21465;&#36848;&#25688;&#35201;&#65306;&#19968;&#20010;&#29992;&#20110;&#25925;&#20107;&#29702;&#35299;&#30340;&#35270;&#39057;&#35821;&#35328;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Synopses of Movie Narratives: a Video-Language Dataset for Story Understanding. (arXiv:2203.05711v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.05711
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#30740;&#31350;&#25910;&#38598;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#35270;&#39057;-&#35821;&#35328;&#25925;&#20107;&#25968;&#25454;&#38598;SYMON&#65292;&#29992;&#20110;&#25512;&#36827;&#22810;&#27169;&#24577;&#25925;&#20107;&#29702;&#35299;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;AI&#26377;&#20102;&#26368;&#36817;&#30340;&#36827;&#23637;&#65292;&#20294;&#25925;&#20107;&#29702;&#35299;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25910;&#38598;&#12289;&#39044;&#22788;&#29702;&#24182;&#20844;&#24320;&#21457;&#24067;&#20102;&#19968;&#20010;&#35270;&#39057;&#35821;&#35328;&#25925;&#20107;&#25968;&#25454;&#38598;SYMON&#65292;&#20854;&#20013;&#21253;&#21547;5,193&#20010;&#27969;&#34892;&#30005;&#24433;&#21644;&#30005;&#35270;&#21095;&#30340;&#35270;&#39057;&#25688;&#35201;&#12290;SYMON&#25429;&#25417;&#20102;&#30001;&#20154;&#31867;&#21019;&#20316;&#32773;&#21046;&#20316;&#30340;&#38754;&#21521;&#20154;&#31867;&#35266;&#20247;&#30340;&#33258;&#28982;&#25925;&#20107;&#21465;&#36848;&#35270;&#39057;&#12290;&#20316;&#20026;&#19968;&#20010;&#21407;&#22411;&#21644;&#33258;&#28982;&#25925;&#20107;&#25968;&#25454;&#38598;&#65292;SYMON&#20855;&#26377;&#39640;&#35206;&#30422;&#30340;&#22810;&#27169;&#24577;&#25925;&#20107;&#20107;&#20214;&#12289;&#20016;&#23500;&#30340;&#24515;&#29702;&#29366;&#24577;&#25551;&#36848;&#21644;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#20043;&#38388;&#30340;&#22823;&#35821;&#20041;&#24046;&#36317;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#21644;&#30005;&#24433;&#25688;&#35201;&#35270;&#39057;&#30340;&#38646;&#26679;&#26412;&#23545;&#40784;&#30340;&#22522;&#20934;&#65292;&#23637;&#31034;&#20102;&#22312;&#25925;&#20107;&#29702;&#35299;&#20013;&#39046;&#22495;&#20869;&#25968;&#25454;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;SYMON&#65292;&#25105;&#20204;&#24076;&#26395;&#20026;&#22810;&#27169;&#24577;&#25925;&#20107;&#29702;&#35299;&#30340;&#36827;&#23637;&#25171;&#19979;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent advances of AI, story understanding remains an open and under-investigated problem. We collect, preprocess, and publicly release a video-language story dataset, Synopses of Movie Narratives (SYMON), containing 5,193 video summaries of popular movies and TV series. SYMON captures naturalistic story-telling videos for human audience made by human creators. As a prototypical and naturalistic story dataset, SYMON features high coverage of multimodal story events, abundant mental-state descriptions, and large semantic gaps between the visual and the textual modalities. We establish benchmarks on video-text retrieval and zero-shot alignment on movie summary videos, which showcase the importance of in-domain data in story understanding. With SYMON, we hope to lay the groundwork for progress in multimodal story understanding.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#35789;&#35821;&#30340;&#8220;&#33258;&#25105;&#32593;&#32476;&#8221;&#65292;&#36890;&#36807;&#20998;&#26512;Twitter&#19978;&#30340;&#24322;&#26500;&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#35789;&#27719;&#20351;&#29992;&#23384;&#22312;&#21516;&#24515;&#22278;&#23618;&#27425;&#32467;&#26500;&#65292;&#21508;&#23618;&#22823;&#23567;&#21576;&#35268;&#21017;&#24615;&#22686;&#38271;&#12290;</title><link>http://arxiv.org/abs/2203.00588</link><description>&lt;p&gt;
&#35789;&#35821;&#30340;&#8220;&#33258;&#25105;&#32593;&#32476;&#8221;&#20013;&#30340;&#32467;&#26500;&#19981;&#21464;&#37327;&#19982;&#35821;&#20041;&#25351;&#32441;
&lt;/p&gt;
&lt;p&gt;
Structural invariants and semantic fingerprints in the "ego network" of words. (arXiv:2203.00588v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.00588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#35789;&#35821;&#30340;&#8220;&#33258;&#25105;&#32593;&#32476;&#8221;&#65292;&#36890;&#36807;&#20998;&#26512;Twitter&#19978;&#30340;&#24322;&#26500;&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#35789;&#27719;&#20351;&#29992;&#23384;&#22312;&#21516;&#24515;&#22278;&#23618;&#27425;&#32467;&#26500;&#65292;&#21508;&#23618;&#22823;&#23567;&#21576;&#35268;&#21017;&#24615;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#31038;&#20132;&#34892;&#20026;&#30340;&#35748;&#30693;&#27169;&#22411;&#34920;&#26126;&#65292;&#30001;&#20110;&#35748;&#30693;&#38480;&#21046;&#65292;&#20154;&#31867;&#20250;&#25353;&#29031;&#19968;&#31181;&#35268;&#21017;&#30340;&#32467;&#26500;&#32452;&#32455;&#31038;&#20132;&#20851;&#31995;&#12290;&#26412;&#25991;&#20551;&#35774;&#31867;&#20284;&#30340;&#35268;&#21017;&#24615;&#20063;&#23384;&#22312;&#20110;&#20854;&#20182;&#28041;&#21450;&#35821;&#35328;&#20135;&#29983;&#30340;&#35748;&#30693;&#36807;&#31243;&#20013;&#12290;&#20026;&#20102;&#25506;&#31350;&#36825;&#19968;&#20551;&#35774;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#20010;&#21253;&#21547;Twitter&#29992;&#25143;&#30340;&#24322;&#26500;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;&#26222;&#36890;&#29992;&#25143;&#21644;&#19987;&#19994;&#20889;&#25163;&#65289;&#12290;&#36890;&#36807;&#21033;&#29992;&#19968;&#31181;&#31867;&#20284;&#20110;&#25581;&#31034;&#31038;&#20132;&#35748;&#30693;&#38480;&#21046;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#32467;&#26500;&#21644;&#35821;&#20041;&#23618;&#38754;&#19978;&#21457;&#29616;&#20102;&#35268;&#21017;&#24615;&#12290;&#22312;&#32467;&#26500;&#23618;&#38754;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#19968;&#31181;&#31216;&#20026;&#8220;&#35789;&#35821;&#33258;&#25105;&#32593;&#32476;&#8221;&#30340;&#21516;&#24515;&#22278;&#23618;&#27425;&#32467;&#26500;&#21487;&#20197;&#24456;&#22909;&#22320;&#25429;&#25417;&#20010;&#20307;&#32452;&#32455;&#25152;&#20351;&#29992;&#30340;&#35789;&#27719;&#12290;&#36825;&#31181;&#32467;&#26500;&#20013;&#21508;&#23618;&#22823;&#23567;&#21576;&#35268;&#21017;&#24615;&#22686;&#38271;&#65288;&#32422;2-3&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Well-established cognitive models coming from anthropology have shown that, due to the cognitive constraints that limit our "bandwidth" for social interactions, humans organize their social relations according to a regular structure. In this work, we postulate that similar regularities can be found in other cognitive processes, such as those involving language production. In order to investigate this claim, we analyse a dataset containing tweets of a heterogeneous group of Twitter users (regular users and professional writers). Leveraging a methodology similar to the one used to uncover the well-established social cognitive constraints, we find regularities at both the structural and semantic level. At the former, we find that a concentric layered structure (which we call ego network of words, in analogy to the ego network of social relationships) very well captures how individuals organise the words they use. The size of the layers in this structure regularly grows (approximately 2-3 
&lt;/p&gt;</description></item></channel></rss>