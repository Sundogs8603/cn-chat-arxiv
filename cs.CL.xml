<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#23545;&#27604;&#25439;&#22833;&#26041;&#27861;&#21487;&#20197;&#36798;&#21040;&#21644;&#35789;&#23884;&#20837;&#27169;&#22411;&#30456;&#21516;&#30340;&#31867;&#27604;&#24674;&#22797;&#25928;&#26524;&#65292;&#19988;&#22312;&#35757;&#32451;&#26102;&#38388;&#19978;&#26377;&#22823;&#24133;&#24230;&#30340;&#25552;&#21319;&#65307;&#24182;&#35777;&#26126;&#20102;&#23545;&#27604;&#25439;&#22833;&#33021;&#22815;&#21019;&#36896;&#20986;&#20855;&#26377;&#24179;&#34892;&#32467;&#26500;&#30340;&#35789;&#23884;&#20837;&#12290;</title><link>http://arxiv.org/abs/2306.08221</link><description>&lt;p&gt;
&#23545;&#27604;&#25439;&#22833;&#23545;&#20110;&#24674;&#22797;&#31867;&#27604;&#20851;&#31995;&#20026;&#24179;&#34892;&#32447;&#23601;&#36275;&#22815;&#20102;
&lt;/p&gt;
&lt;p&gt;
Contrastive Loss is All You Need to Recover Analogies as Parallel Lines. (arXiv:2306.08221v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08221
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#23545;&#27604;&#25439;&#22833;&#26041;&#27861;&#21487;&#20197;&#36798;&#21040;&#21644;&#35789;&#23884;&#20837;&#27169;&#22411;&#30456;&#21516;&#30340;&#31867;&#27604;&#24674;&#22797;&#25928;&#26524;&#65292;&#19988;&#22312;&#35757;&#32451;&#26102;&#38388;&#19978;&#26377;&#22823;&#24133;&#24230;&#30340;&#25552;&#21319;&#65307;&#24182;&#35777;&#26126;&#20102;&#23545;&#27604;&#25439;&#22833;&#33021;&#22815;&#21019;&#36896;&#20986;&#20855;&#26377;&#24179;&#34892;&#32467;&#26500;&#30340;&#35789;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#38745;&#24577;&#35789;&#23884;&#20837;&#27169;&#22411;&#24050;&#30693;&#23558;&#35821;&#35328;&#31867;&#27604;&#20851;&#31995;&#34920;&#31034;&#20026;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#24179;&#34892;&#32447;&#65292;&#20294;&#20026;&#20160;&#20040;&#23427;&#20204;&#23548;&#33268;&#36825;&#26679;&#30340;&#20960;&#20309;&#32467;&#26500;&#30340;&#22522;&#26412;&#26426;&#21046;&#20173;&#28982;&#19981;&#26126;&#30830;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#20998;&#24067;&#20449;&#24687;&#19978;&#24212;&#29992;&#22522;&#26412;&#23545;&#27604;&#26679;&#24335;&#30340;&#26041;&#27861;&#19982;&#27969;&#34892;&#30340;&#35789;&#23884;&#20837;&#27169;&#22411;&#22312;&#31867;&#27604;&#24674;&#22797;&#20219;&#21153;&#19978;&#34920;&#29616;&#30456;&#24403;&#65292;&#21516;&#26102;&#22312;&#35757;&#32451;&#26102;&#38388;&#19978;&#23454;&#29616;&#20102;&#24040;&#22823;&#30340;&#21152;&#36895;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#23545;&#27604;&#25439;&#22833;&#36275;&#20197;&#22312;&#35789;&#23884;&#20837;&#20013;&#21019;&#24314;&#36825;&#20123;&#24179;&#34892;&#32467;&#26500;&#65292;&#24182;&#24314;&#31435;&#36215;&#20849;&#29616;&#32479;&#35745;&#25968;&#25454;&#19982;&#25152;&#24471;&#21040;&#30340;&#35789;&#23884;&#20837;&#30340;&#20960;&#20309;&#32467;&#26500;&#20043;&#38388;&#30340;&#31934;&#30830;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
While static word embedding models are known to represent linguistic analogies as parallel lines in high-dimensional space, the underlying mechanism as to why they result in such geometric structures remains obscure. We find that an elementary contrastive-style method employed over distributional information performs competitively with popular word embedding models on analogy recovery tasks, while achieving dramatic speedups in training time. Further, we demonstrate that a contrastive loss is sufficient to create these parallel structures in word embeddings, and establish a precise relationship between the co-occurrence statistics and the geometric structure of the resulting word embeddings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#25506;&#27979;&#20998;&#31867;&#22120;&#26469;&#35780;&#20272;&#32452;&#20214;&#26159;&#21542;&#34920;&#31034;&#23646;&#24615;&#65292;&#22635;&#34917;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20851;&#20110;&#8220;&#34920;&#31034;&#8221;&#30340;&#21746;&#23398;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2306.08193</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#34920;&#31034;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Operationalising Representation in Natural Language Processing. (arXiv:2306.08193v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#25506;&#27979;&#20998;&#31867;&#22120;&#26469;&#35780;&#20272;&#32452;&#20214;&#26159;&#21542;&#34920;&#31034;&#23646;&#24615;&#65292;&#22635;&#34917;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20851;&#20110;&#8220;&#34920;&#31034;&#8221;&#30340;&#21746;&#23398;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#8220;&#34920;&#31034;&#8221;&#22312;&#35748;&#30693;&#31185;&#23398;&#21746;&#23398;&#20013;&#20855;&#26377;&#26680;&#24515;&#22320;&#20301;&#65292;&#20294;&#22312;&#24403;&#20195;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#23454;&#36341;&#20013;&#65292;&#20960;&#20046;&#27809;&#26377;&#21746;&#23398;&#39046;&#22495;&#30340;&#20808;&#21069;&#30740;&#31350;&#19982;&#20043;&#28041;&#21450;&#12290;&#26412;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65306;&#32467;&#21512;&#35748;&#30693;&#31185;&#23398;&#30340;&#24605;&#24819;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35780;&#20272;&#31070;&#32463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#32452;&#20214;&#25152;&#20316;&#20986;&#30340;&#34920;&#31034;&#24615;&#22768;&#26126;&#65292;&#24182;&#25552;&#20986;&#19977;&#20010;&#35780;&#20272;&#32452;&#20214;&#26159;&#21542;&#34920;&#31034;&#23646;&#24615;&#30340;&#26631;&#20934;&#65292;&#24182;&#20351;&#29992;&#25506;&#27979;&#20998;&#31867;&#22120;&#26469;&#23454;&#29616;&#36825;&#20123;&#26631;&#20934;&#30340;&#25805;&#20316;&#21270;&#65292;&#25506;&#27979;&#20998;&#31867;&#22120;&#26159;NLP&#65288;&#21644;&#26356;&#24191;&#27867;&#30340;&#28145;&#24230;&#23398;&#20064;&#65289;&#20013;&#27969;&#34892;&#30340;&#20998;&#26512;&#25216;&#26415;&#12290;&#25805;&#20316;&#21270;&#19968;&#20010;&#22312;&#21746;&#23398;&#19978;&#21463;&#21040;&#21551;&#21457;&#30340;&#8220;&#34920;&#31034;&#8221;&#27010;&#24565;&#30340;&#39033;&#30446;&#24212;&#35813;&#24341;&#36215;&#31185;&#23398;&#21746;&#23398;&#23478;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#23454;&#36341;&#32773;&#30340;&#20852;&#36259;&#12290;&#23545;&#20110;&#21746;&#23398;&#23478;&#26469;&#35828;&#65292;&#36825;&#25552;&#20379;&#20102;&#19968;&#20010;&#27979;&#35797;&#26377;&#20851;&#34920;&#31034;&#30340;&#26412;&#36136;&#30340;&#35770;&#25454;&#30340;&#26032;&#39062;&#22330;&#22320;&#65292;&#24182;&#24110;&#21161;NLPers&#32452;&#32455;&#26377;&#20851;&#25506;&#27979;&#23454;&#39564;&#30340;&#22823;&#37327;&#25991;&#29486;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#32463;&#39564;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite its centrality in the philosophy of cognitive science, there has been little prior philosophical work engaging with the notion of representation in contemporary NLP practice. This paper attempts to fill that lacuna: drawing on ideas from cognitive science, I introduce a framework for evaluating the representational claims made about components of neural NLP models, proposing three criteria with which to evaluate whether a component of a model represents a property and operationalising these criteria using probing classifiers, a popular analysis technique in NLP (and deep learning more broadly).  The project of operationalising a philosophically-informed notion of representation should be of interest to both philosophers of science and NLP practitioners. It affords philosophers a novel testing-ground for claims about the nature of representation, and helps NLPers organise the large literature on probing experiments, suggesting novel avenues for empirical research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;GPT-3&#27169;&#22411;&#22312;LIAR&#25968;&#25454;&#38598;&#19978;&#26816;&#27979;&#34394;&#20551;&#25919;&#27835;&#38472;&#36848;&#30340;&#25928;&#26524;&#65292;&#24182;&#35777;&#26126;&#20854;&#20934;&#30830;&#24615;&#39640;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#19988;&#20351;&#29992;&#38646;&#26679;&#26412;&#23398;&#20064;&#21487;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.08190</link><description>&lt;p&gt;
GPT-3&#22312;LIAR&#25968;&#25454;&#38598;&#20013;&#26816;&#27979;&#34394;&#20551;&#25919;&#27835;&#38472;&#36848;&#30340;&#25928;&#26524;&#35780;&#20272;&#65306;&#19968;&#39033;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Assessing the Effectiveness of GPT-3 in Detecting False Political Statements: A Case Study on the LIAR Dataset. (arXiv:2306.08190v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;GPT-3&#27169;&#22411;&#22312;LIAR&#25968;&#25454;&#38598;&#19978;&#26816;&#27979;&#34394;&#20551;&#25919;&#27835;&#38472;&#36848;&#30340;&#25928;&#26524;&#65292;&#24182;&#35777;&#26126;&#20854;&#20934;&#30830;&#24615;&#39640;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#19988;&#20351;&#29992;&#38646;&#26679;&#26412;&#23398;&#20064;&#21487;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#25919;&#27835;&#34394;&#20551;&#38472;&#36848;&#23545;&#20110;&#32500;&#25252;&#20449;&#24687;&#23436;&#25972;&#24615;&#21644;&#38450;&#27490;&#20449;&#24687;&#35823;&#20256;&#22312;&#31038;&#20250;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#21382;&#21490;&#19978;&#65292;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#37319;&#29992;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#26816;&#27979;&#27450;&#39575;&#24615;&#38472;&#36848;&#65292;&#21253;&#25324;&#20351;&#29992;&#20803;&#25968;&#25454;&#12289;n-gram&#20998;&#26512;&#20197;&#21450;&#35821;&#35328;&#23398;&#21644;&#39118;&#26684;&#23398;&#29305;&#24449;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#27604;&#22914;GPT-3&#65292;&#24050;&#32463;&#22312;&#24191;&#27867;&#30340;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22312;LIAR&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;GPT-3&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#33719;&#24471;&#20102;&#27604;&#26368;&#20808;&#36827;&#27169;&#22411;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#32780;&#19981;&#20351;&#29992;&#20219;&#20309;&#38468;&#21152;&#30340;&#20803;&#25968;&#25454;&#25110;&#35821;&#35328;&#23398;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#36827;&#34892;&#20102;&#38646;&#26679;&#26412;&#23398;&#20064;&#65292;&#24182;&#23454;&#29616;&#20102;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#20010;&#20248;&#28857;&#26159;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#35299;&#37322;&#25253;&#21578;&#65292;&#35828;&#26126;&#23427;&#26159;&#22914;&#20309;&#24471;&#20986;&#39044;&#27979;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The detection of political fake statements is crucial for maintaining information integrity and preventing the spread of misinformation in society. Historically, state-of-the-art machine learning models employed various methods for detecting deceptive statements. These methods include the use of metadata (W. Wang et al., 2018), n-grams analysis (Singh et al., 2021), and linguistic (Wu et al., 2022) and stylometric (Islam et al., 2020) features. Recent advancements in large language models, such as GPT-3 (Brown et al., 2020) have achieved state-of-the-art performance on a wide range of tasks. In this study, we conducted experiments with GPT-3 on the LIAR dataset (W. Wang et al., 2018) and achieved higher accuracy than state-of-the-art models without using any additional meta or linguistic features. Additionally, we experimented with zero-shot learning using a carefully designed prompt and achieved near state-of-the-art performance. An advantage of this approach is that the model provide
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#31995;&#32479;&#35780;&#20272;&#20102;&#24403;&#21069;&#19968;&#20195;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22788;&#29702;&#21542;&#23450;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;LLMs&#23384;&#22312;&#26080;&#27861;&#25429;&#25417;&#21542;&#23450;&#35789;&#27719;&#35821;&#20041;&#12289;&#19981;&#33021;&#25512;&#29702;&#30340;&#38382;&#39064;&#31561;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2306.08189</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#19981;&#26159;&#21542;&#23450;&#32773;&#65306;&#23545;&#21542;&#23450;&#22522;&#20934;&#27979;&#35797;&#20013;&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models are not naysayers: An analysis of language models on negation benchmarks. (arXiv:2306.08189v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#31995;&#32479;&#35780;&#20272;&#20102;&#24403;&#21069;&#19968;&#20195;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22788;&#29702;&#21542;&#23450;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;LLMs&#23384;&#22312;&#26080;&#27861;&#25429;&#25417;&#21542;&#23450;&#35789;&#27719;&#35821;&#20041;&#12289;&#19981;&#33021;&#25512;&#29702;&#30340;&#38382;&#39064;&#31561;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21542;&#23450;&#24050;&#34987;&#35777;&#26126;&#26159;&#34987;&#25513;&#34109;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT&#65289;&#30340;&#20027;&#35201;&#29942;&#39048;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#24182;&#27809;&#26377;&#20840;&#38754;&#30740;&#31350;&#22823;&#22411;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#65288;&#8220;LLMs&#8221;&#65289;&#26159;&#21542;&#20173;&#28982;&#23384;&#22312;&#36825;&#19968;&#21457;&#29616;&#12290;&#38543;&#30528;&#30740;&#31350;&#21644;&#24212;&#29992;LLMs&#20307;&#31215;&#30340;&#26085;&#30410;&#22686;&#38271;&#65292;&#25105;&#20204;&#36864;&#21518;&#19968;&#27493;&#65292;&#35780;&#20272;&#24403;&#21069;&#19968;&#20195;LLMs&#22788;&#29702;&#21542;&#23450;&#30340;&#33021;&#21147;&#65292;&#32780;&#36825;&#26159;&#23545;&#20110;&#35821;&#35328;&#29702;&#35299;&#33267;&#20851;&#37325;&#35201;&#30340;&#22522;&#26412;&#35821;&#35328;&#29616;&#35937;&#12290;&#25105;&#20204;&#35780;&#20272;&#19981;&#21516;&#30340;LLMs-&#21253;&#25324;&#24320;&#28304;&#30340;GPT-NEO&#12289;GPT-3&#21644;InstructGPT-&#23545;&#19968;&#31995;&#21015;&#21542;&#23450;&#22522;&#20934;&#27979;&#35797;&#36827;&#34892;&#35780;&#20272;&#12290;&#36890;&#36807;&#23545;&#27169;&#22411;&#22823;&#23567;&#21644;&#25552;&#31034;&#36827;&#34892;&#31995;&#32479;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#23384;&#22312;&#20960;&#20010;&#38480;&#21046;&#65292;&#21253;&#25324;&#23545;&#21542;&#23450;&#23384;&#22312;&#30340;&#19981;&#25935;&#24863;&#24615;&#12289;&#26080;&#27861;&#25429;&#25417;&#21542;&#23450;&#30340;&#35789;&#27719;&#35821;&#20041;&#20197;&#21450;&#26080;&#27861;&#22312;&#21542;&#23450;&#24773;&#20917;&#19979;&#36827;&#34892;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Negation has been shown to be a major bottleneck for masked language models, such as BERT. However, whether this finding still holds for larger-sized auto-regressive language models (``LLMs'') has not been studied comprehensively. With the ever-increasing volume of research and applications of LLMs, we take a step back to evaluate the ability of current-generation LLMs to handle negation, a fundamental linguistic phenomenon that is central to language understanding. We evaluate different LLMs -- including the open-source GPT-neo, GPT-3, and InstructGPT -- against a wide range of negation benchmarks. Through systematic experimentation with varying model sizes and prompts, we show that LLMs have several limitations including insensitivity to the presence of negation, an inability to capture the lexical semantics of negation, and a failure to reason under negation.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20302;&#31209;&#33258;&#36866;&#24212;&#32416;&#38169;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#21487;&#20197;&#26174;&#33879;&#22320;&#20943;&#23569;&#31934;&#32454;&#35843;&#25972;VRAM&#38656;&#27714;&#65292;&#24182;&#32416;&#27491;&#37327;&#21270;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#37327;&#21270;&#35823;&#24046;&#65292;&#20351;&#28040;&#36153;&#32773;&#31508;&#35760;&#26412;&#30005;&#33041;&#21487;&#20197;&#23545;70&#20159;&#20010;&#21442;&#25968;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#65292;&#29983;&#25104;&#36830;&#36143;&#30340;&#33521;&#25991;&#25991;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.08162</link><description>&lt;p&gt;
INT2.1&#65306;&#36890;&#36807;&#20302;&#31209;&#33258;&#36866;&#24212;&#32416;&#38169;&#23454;&#29616;&#31934;&#32454;&#21487;&#35843;&#30340;&#37327;&#21270;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
INT2.1: Towards Fine-Tunable Quantized Large Language Models with Error Correction through Low-Rank Adaptation. (arXiv:2306.08162v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08162
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20302;&#31209;&#33258;&#36866;&#24212;&#32416;&#38169;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#21487;&#20197;&#26174;&#33879;&#22320;&#20943;&#23569;&#31934;&#32454;&#35843;&#25972;VRAM&#38656;&#27714;&#65292;&#24182;&#32416;&#27491;&#37327;&#21270;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#37327;&#21270;&#35823;&#24046;&#65292;&#20351;&#28040;&#36153;&#32773;&#31508;&#35760;&#26412;&#30005;&#33041;&#21487;&#20197;&#23545;70&#20159;&#20010;&#21442;&#25968;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#65292;&#29983;&#25104;&#36830;&#36143;&#30340;&#33521;&#25991;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#22320;&#20943;&#23569;&#31934;&#32454;&#35843;&#25972;VRAM&#38656;&#27714;&#65292;&#24182;&#32416;&#27491;&#37327;&#21270;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#37327;&#21270;&#35823;&#24046;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#20302;&#31209;&#33258;&#36866;&#24212;&#65288;LoRA&#65289;&#24320;&#21457;&#20102;&#19968;&#31181;&#26497;&#20854;&#20869;&#23384;&#39640;&#25928;&#30340;&#37327;&#21270;&#27169;&#22411;&#31934;&#32454;&#35843;&#25972;&#26041;&#27861;&#65288;EMEF&#65289;&#65292;&#24182;&#26681;&#25454;&#23427;&#26500;&#24314;&#20102;&#19968;&#20010;&#38169;&#35823;&#20462;&#27491;&#31639;&#27861;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#37327;&#21270;&#36807;&#31243;&#20013;&#24341;&#36215;&#30340;&#35823;&#24046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23558;&#20869;&#23384;&#35201;&#27714;&#38477;&#20302;&#22810;&#36798;5.6&#20493;&#65292;&#20174;&#32780;&#20351;&#28040;&#36153;&#32773;&#31508;&#35760;&#26412;&#30005;&#33041;&#21487;&#20197;&#23545;70&#20159;&#20010;&#21442;&#25968;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#31209;&#32416;&#38169;&#65288;LREC&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#22686;&#21152;&#30340;LoRA&#23618;&#26469;&#25913;&#21892;&#37327;&#21270;&#27169;&#22411;&#19982;&#20854;&#28014;&#28857;&#25968;&#23545;&#24212;&#29289;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#32416;&#38169;&#26694;&#26550;&#21487;&#20197;&#29983;&#25104;&#36830;&#36143;&#30340;&#33521;&#25991;&#25991;&#26412;&#65292;&#23454;&#29616;&#20102;&#23436;&#20840;&#21151;&#33021;&#30340;INT2&#37327;&#21270;&#22823;&#35821;&#35328;&#27169;&#22411;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#36798;&#21040;&#36825;&#31181;&#24615;&#33021;&#30340;INT2&#22823;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a method that dramatically reduces fine-tuning VRAM requirements and rectifies quantization errors in quantized Large Language Models. First, we develop an extremely memory-efficient fine-tuning (EMEF) method for quantized models using Low-Rank Adaptation (LoRA), and drawing upon it, we construct an error-correcting algorithm designed to minimize errors induced by the quantization process. Our method reduces the memory requirements by up to 5.6 times, which enables fine-tuning a 7 billion parameter Large Language Model (LLM) on consumer laptops. At the same time, we propose a Low-Rank Error Correction (LREC) method that exploits the added LoRA layers to ameliorate the gap between the quantized model and its float point counterpart. Our error correction framework leads to a fully functional INT2 quantized LLM with the capacity to generate coherent English text. To the best of our knowledge, this is the first INT2 Large Language Model that has been able to reach such a perfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;h2oGPT&#65292;&#36825;&#26159;&#19968;&#22871;&#24320;&#28304;&#20195;&#30721;&#24211;&#65292;&#29992;&#20110;&#21019;&#24314;&#21644;&#20351;&#29992;&#22522;&#20110;GPTs&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21253;&#25324;100&#65285;&#31169;&#26377;&#25991;&#26723;&#25628;&#32034;&#12290;&#30446;&#26631;&#26159;&#21019;&#24314;&#30495;&#27491;&#24320;&#28304;&#30340;&#26367;&#20195;&#23553;&#38381;&#28304;GPTs&#65292;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#30340;&#24320;&#21457;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.08161</link><description>&lt;p&gt;
h2oGPT&#65306;&#27665;&#20027;&#21270;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
h2oGPT: Democratizing Large Language Models. (arXiv:2306.08161v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08161
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;h2oGPT&#65292;&#36825;&#26159;&#19968;&#22871;&#24320;&#28304;&#20195;&#30721;&#24211;&#65292;&#29992;&#20110;&#21019;&#24314;&#21644;&#20351;&#29992;&#22522;&#20110;GPTs&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21253;&#25324;100&#65285;&#31169;&#26377;&#25991;&#26723;&#25628;&#32034;&#12290;&#30446;&#26631;&#26159;&#21019;&#24314;&#30495;&#27491;&#24320;&#28304;&#30340;&#26367;&#20195;&#23553;&#38381;&#28304;GPTs&#65292;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#30340;&#24320;&#21457;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPTs&#65289;&#65292;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-4&#22240;&#20854;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#29616;&#23454;&#24212;&#29992;&#32780;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#38761;&#21629;&#30340;&#19968;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20063;&#24102;&#26469;&#20102;&#35768;&#22810;&#37325;&#22823;&#30340;&#39118;&#38505;&#65292;&#22914;&#23384;&#22312;&#26377;&#20559;&#35265;&#12289;&#31169;&#20154;&#25110;&#26377;&#23475;&#25991;&#26412;&#21644;&#26410;&#32463;&#25480;&#26435;&#30340;&#29256;&#26435;&#26448;&#26009;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;h2oGPT&#65292;&#36825;&#26159;&#19968;&#22871;&#24320;&#28304;&#20195;&#30721;&#24211;&#65292;&#29992;&#20110;&#21019;&#24314;&#21644;&#20351;&#29992;&#22522;&#20110;GPTs&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#35813;&#39033;&#30446;&#30340;&#30446;&#26631;&#26159;&#21019;&#24314;&#19990;&#30028;&#19978;&#26368;&#22909;&#30340;&#30495;&#27491;&#24320;&#28304;&#30340;&#26367;&#20195;&#23553;&#38381;&#28304;GPTs&#12290;&#19982;&#24320;&#28304;&#31038;&#21306;&#21512;&#20316;&#65292;&#20316;&#20026;&#20854;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#24320;&#28304;&#20102;&#20960;&#20010;LLM&#65292;&#20854;&#21442;&#25968;&#20174;7&#20159;&#21040;400&#20159;&#65292;&#21487;&#22312;&#23436;&#20840;&#33258;&#30001;&#30340;Apache 2.0&#35768;&#21487;&#19979;&#21830;&#29992;&#12290;&#25105;&#20204;&#30340;&#21457;&#24067;&#21253;&#25324;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#30340;100&#65285;&#31169;&#26377;&#25991;&#26723;&#25628;&#32034;&#12290;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#26377;&#21161;&#20110;&#20419;&#36827;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#24182;&#20351;&#20854;&#26356;&#21152;&#21487;&#38752;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation Large Language Models (LLMs) such as GPT-4 represent a revolution in AI due to their real-world applications though natural language processing. However, they also pose many significant risks such as the presence of biased, private, or harmful text, and the unauthorized inclusion of copyrighted material.  We introduce h2oGPT, a suite of open-source code repositories for the creation and use of Large Language Models (LLMs) based on Generative Pretrained Transformers (GPTs). The goal of this project is to create the world's best truly open-source alternative to closed-source GPTs. In collaboration with and as part of the incredible and unstoppable open-source community, we open-source several fine-tuned h2oGPT models from 7 to 40 Billion parameters, ready for commercial use under fully permissive Apache 2.0 licenses. Included in our release is 100% private document search using natural language.  Open-source language models help boost AI development and make it more accessible
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;209&#31687;&#20851;&#20110;NLP&#27169;&#22411;&#20559;&#35265;&#30340;&#35770;&#25991;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#28041;&#21450;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#23450;&#20041;&#65292;&#24182;&#30830;&#23450;&#20102;NLP&#20559;&#35265;&#30740;&#31350;&#30340;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#12290;&#24403;&#21069;&#21435;&#20559;&#35265;&#25216;&#26415;&#21482;&#26159;&#38544;&#34255;&#20102;&#20559;&#35265;&#32780;&#19981;&#26159;&#30495;&#27491;&#21435;&#38500;&#23427;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.08158</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Survey on Sociodemographic Bias in Natural Language Processing. (arXiv:2306.08158v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;209&#31687;&#20851;&#20110;NLP&#27169;&#22411;&#20559;&#35265;&#30340;&#35770;&#25991;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#28041;&#21450;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#23450;&#20041;&#65292;&#24182;&#30830;&#23450;&#20102;NLP&#20559;&#35265;&#30740;&#31350;&#30340;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#12290;&#24403;&#21069;&#21435;&#20559;&#35265;&#25216;&#26415;&#21482;&#26159;&#38544;&#34255;&#20102;&#20559;&#35265;&#32780;&#19981;&#26159;&#30495;&#27491;&#21435;&#38500;&#23427;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24448;&#24448;&#20250;&#23398;&#20064;&#21040;&#38750;&#39044;&#26399;&#30340;&#20559;&#35265;&#65292;&#36825;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#33021;&#20250;&#20135;&#29983;&#26377;&#23475;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#23545;209&#31687;&#20851;&#20110;NLP&#27169;&#22411;&#20013;&#20559;&#35265;&#30340;&#35770;&#25991;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#35770;&#25991;&#28041;&#21450;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#20559;&#35265;&#19982;&#30495;&#23454;&#19990;&#30028;&#30340;&#21361;&#23475;&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#25105;&#20204;&#20511;&#37492;&#24515;&#29702;&#23398;&#21644;&#34892;&#20026;&#32463;&#27982;&#23398;&#30340;&#24605;&#24819;&#65292;&#25552;&#20986;&#20102;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#23450;&#20041;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;NLP&#20559;&#35265;&#30740;&#31350;&#30340;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#65306;&#20559;&#35265;&#31867;&#22411;&#12289;&#37327;&#21270;&#20559;&#35265;&#21644;&#21435;&#20559;&#35265;&#12290;&#25105;&#20204;&#35748;&#20026;&#24403;&#21069;&#23545;&#20110;&#37327;&#21270;&#20559;&#35265;&#30340;&#26041;&#27861;&#23384;&#22312;&#21487;&#38752;&#24615;&#38382;&#39064;&#65292;&#35768;&#22810;&#20559;&#35265;&#24230;&#37327;&#24182;&#19981;&#28041;&#21450;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#20559;&#35265;&#65292;&#24403;&#21069;&#30340;&#21435;&#20559;&#35265;&#25216;&#26415;&#26159;&#34920;&#38754;&#30340;&#65292;&#21482;&#26159;&#38544;&#34255;&#20102;&#20559;&#35265;&#65292;&#32780;&#19981;&#26159;&#30495;&#27491;&#21435;&#38500;&#23427;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks often learn unintended biases during training, which might have harmful effects when deployed in real-world settings. This paper surveys 209 papers on bias in NLP models, most of which address sociodemographic bias. To better understand the distinction between bias and real-world harm, we turn to ideas from psychology and behavioral economics to propose a definition for sociodemographic bias. We identify three main categories of NLP bias research: types of bias, quantifying bias, and debiasing. We conclude that current approaches on quantifying bias face reliability issues, that many of the bias metrics do not relate to real-world biases, and that current debiasing techniques are superficial and hide bias rather than removing it. Finally, we provide recommendations for future work.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23545;&#38271;&#35270;&#39057;ASR&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#19982;&#26368;&#22823;&#29109;&#22522;&#32447;&#30456;&#27604;&#65292;&#20351;&#29992;LLM&#33021;&#22815;&#26368;&#22810;&#20943;&#23569;8&#65285;&#30340;Word Error Rate&#21644;30&#65285;&#30340;Salient Term Error Rate&#12290;&#32463;&#36807;&#25913;&#36827;&#30340;&#26684;&#22788;&#29702;&#21644;&#25658;&#24102;&#19978;&#19979;&#25991;&#30340;&#32452;&#21512;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.08133</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#38271;&#24418;&#24335;&#25968;&#25454;&#37325;&#26032;&#35780;&#20998;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Large-scale Language Model Rescoring on Long-form Data. (arXiv:2306.08133v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23545;&#38271;&#35270;&#39057;ASR&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#19982;&#26368;&#22823;&#29109;&#22522;&#32447;&#30456;&#27604;&#65292;&#20351;&#29992;LLM&#33021;&#22815;&#26368;&#22810;&#20943;&#23569;8&#65285;&#30340;Word Error Rate&#21644;30&#65285;&#30340;Salient Term Error Rate&#12290;&#32463;&#36807;&#25913;&#36827;&#30340;&#26684;&#22788;&#29702;&#21644;&#25658;&#24102;&#19978;&#19979;&#25991;&#30340;&#32452;&#21512;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23545;YouTube&#35270;&#39057;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#35270;&#39057;&#34987;&#29992;&#20316;&#38271;&#24418;&#24335;ASR&#30340;&#28304;&#12290;&#25105;&#20204;&#35777;&#26126;&#22312;&#32654;&#22269;&#33521;&#35821;&#65288;en-us&#65289;&#21644;&#21360;&#24230;&#33521;&#35821;&#65288;en-in&#65289;&#38271;&#24418;&#24335;ASR&#27979;&#35797;&#38598;&#19978;&#65292;&#30456;&#23545;&#20110;&#22522;&#20110;&#26368;&#22823;&#29109;&#30340;&#35821;&#35328;&#27169;&#22411;&#24378;&#19968;&#27425;&#36890;&#36807;&#22522;&#32447;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#39640;&#36798;8&#65285;&#30340;&#30456;&#23545;Word Error Rate&#65288;WER&#65289;&#38477;&#20302;&#21644;&#39640;&#36798;30&#65285;&#30340;&#30456;&#23545;Salient Term Error Rate&#65288;STER&#65289;&#38477;&#20302;&#12290;&#32463;&#36807;&#25913;&#36827;&#30340;&#26684;&#22788;&#29702;&#23548;&#33268;&#24102;&#26377;&#27491;&#30830;&#65288;&#38750;&#26641;&#24418;&#65289;&#26377;&#21521;&#22270;&#25299;&#25169;&#21644;&#25658;&#24102;&#21069;&#19968;&#27573;&#26368;&#20339;&#20551;&#35774;&#30340;&#19978;&#19979;&#25991;&#30340;&#26684;&#30340;&#26174;&#30528;&#33719;&#32988;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#22522;&#20110;&#22823;&#37327;&#21487;&#29992;&#25968;&#25454;&#65288;&#22914;C4&#65289;&#30340;LLMs&#21644;&#20256;&#32479;&#31070;&#32463;LMs&#30340;&#32452;&#21512;&#30340;&#24615;&#33021;&#25552;&#21319;&#26159;&#32047;&#21152;&#30340;&#65292;&#24182;&#19988;&#26174;&#30528;&#20248;&#20110;&#20855;&#26377;&#26368;&#22823;&#29109;LM&#30340;&#24378;&#19968;&#27425;&#36890;&#36807;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we study the impact of Large-scale Language Models (LLM) on Automated Speech Recognition (ASR) of YouTube videos, which we use as a source for long-form ASR. We demonstrate up to 8\% relative reduction in Word Error Eate (WER) on US English (en-us) and code-switched Indian English (en-in) long-form ASR test sets and a reduction of up to 30\% relative on Salient Term Error Rate (STER) over a strong first-pass baseline that uses a maximum-entropy based language model. Improved lattice processing that results in a lattice with a proper (non-tree) digraph topology and carrying context from the 1-best hypothesis of the previous segment(s) results in significant wins in rescoring with LLMs. We also find that the gains in performance from the combination of LLMs trained on vast quantities of available data (such as C4) and conventional neural LMs is additive and significantly outperforms a strong first-pass baseline with a maximum entropy LM.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#20449;&#24687;&#26816;&#32034;&#35270;&#35273;&#38382;&#31572;&#26694;&#26550;AVIS&#65292;&#21487;&#20197;&#35299;&#20915;&#35270;&#35273;&#38382;&#39064;&#25152;&#38656;&#30340;&#22806;&#37096;&#30693;&#35782;&#33719;&#21462;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.08129</link><description>&lt;p&gt;
AVIS:&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#35270;&#35273;&#20449;&#24687;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
AVIS: Autonomous Visual Information Seeking with Large Language Models. (arXiv:2306.08129v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#20449;&#24687;&#26816;&#32034;&#35270;&#35273;&#38382;&#31572;&#26694;&#26550;AVIS&#65292;&#21487;&#20197;&#35299;&#20915;&#35270;&#35273;&#38382;&#39064;&#25152;&#38656;&#30340;&#22806;&#37096;&#30693;&#35782;&#33719;&#21462;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23454;&#29616;&#33258;&#20027;&#20449;&#24687;&#26816;&#32034;&#30340;&#35270;&#35273;&#38382;&#31572;&#26694;&#26550;AVIS&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;LLM&#21160;&#24577;&#22320;&#21046;&#23450;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#30340;&#31574;&#30053;&#65292;&#24182;&#35843;&#26597;&#23427;&#20204;&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#33719;&#21462;&#25552;&#20379;&#25152;&#25552;&#20986;&#38382;&#39064;&#25152;&#38656;&#30340;&#19981;&#21487;&#25110;&#32570;&#30340;&#30693;&#35782;&#12290;&#22238;&#31572;&#38656;&#35201;&#22806;&#37096;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#39064;&#65292;&#22914;&#8220;&#36825;&#24133;&#22270;&#20687;&#25152;&#25551;&#32472;&#30340;&#24314;&#31569;&#29289;&#26159;&#20026;&#20102;&#32426;&#24565;&#21738;&#20010;&#20107;&#20214;&#65311;&#8221;&#65292;&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#36825;&#20010;&#20219;&#21153;&#21576;&#29616;&#20986;&#19968;&#20010;&#32452;&#21512;&#25628;&#32034;&#31354;&#38388;&#65292;&#38656;&#35201;&#19968;&#31995;&#21015;&#34892;&#21160;&#65292;&#21253;&#25324;&#35843;&#29992;API&#12289;&#20998;&#26512;&#23427;&#20204;&#30340;&#21709;&#24212;&#24182;&#20570;&#20986;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#29992;&#25143;&#30740;&#31350;&#65292;&#25910;&#38598;&#20102;&#20154;&#31867;&#38754;&#23545;&#36825;&#20010;&#20219;&#21153;&#26102;&#21508;&#31181;&#21508;&#26679;&#30340;&#20915;&#31574;&#23454;&#20363;&#12290;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#35774;&#35745;&#20102;&#19968;&#20010;&#30001;&#19977;&#20010;&#32452;&#20214;&#32452;&#25104;&#30340;&#31995;&#32479;&#65306;&#19968;&#20010;&#30001;LLM&#39537;&#21160;&#30340;&#35268;&#21010;&#22120;&#65292;&#21160;&#24577;&#30830;&#23450;&#19979;&#19968;&#20010;&#35201;&#20351;&#29992;&#30340;&#24037;&#20855;&#65307;&#19968;&#20010;&#30001;LLM&#39537;&#21160;&#30340;&#25512;&#29702;&#22120;&#65292;&#20998;&#26512;&#24182;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose an autonomous information seeking visual question answering framework, AVIS. Our method leverages a Large Language Model (LLM) to dynamically strategize the utilization of external tools and to investigate their outputs, thereby acquiring the indispensable knowledge needed to provide answers to the posed questions. Responding to visual questions that necessitate external knowledge, such as "What event is commemorated by the building depicted in this image?", is a complex task. This task presents a combinatorial search space that demands a sequence of actions, including invoking APIs, analyzing their responses, and making informed decisions. We conduct a user study to collect a variety of instances of human decision-making when faced with this task. This data is then used to design a system comprised of three components: an LLM-powered planner that dynamically determines which tool to use next, an LLM-powered reasoner that analyzes and extracts key information 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;PersonaPKT&#65292;&#19968;&#31181;&#36731;&#37327;&#32423;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#26126;&#30830;&#20010;&#24615;&#25551;&#36848;&#30340;&#24773;&#20917;&#19979;&#26500;&#24314;&#31526;&#21512;&#35282;&#33394;&#30340;&#23545;&#35805;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#27599;&#20010;&#20010;&#24615;&#34920;&#31034;&#20026;&#19968;&#20010;&#36830;&#32493;&#21521;&#37327;&#65292;&#30452;&#25509;&#20174;&#21516;&#19968;&#35282;&#33394;&#20135;&#29983;&#30340;&#23569;&#37327;&#23545;&#35805;&#26679;&#26412;&#20013;&#23398;&#20064;&#38544;&#21547;&#30340;&#20010;&#24615;&#29305;&#23450;&#29305;&#24449;&#65292;&#24182;&#33021;&#22815;&#39640;&#25928;&#26500;&#24314;&#20010;&#24615;&#21270;&#23545;&#35805;&#20195;&#29702;&#24182;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2306.08126</link><description>&lt;p&gt;
PersonaPKT&#65306;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#30340;&#30693;&#35782;&#36801;&#31227;&#26500;&#24314;&#20010;&#24615;&#21270;&#23545;&#35805;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
PersonaPKT: Building Personalized Dialogue Agents via Parameter-efficient Knowledge Transfer. (arXiv:2306.08126v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;PersonaPKT&#65292;&#19968;&#31181;&#36731;&#37327;&#32423;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#26126;&#30830;&#20010;&#24615;&#25551;&#36848;&#30340;&#24773;&#20917;&#19979;&#26500;&#24314;&#31526;&#21512;&#35282;&#33394;&#30340;&#23545;&#35805;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#27599;&#20010;&#20010;&#24615;&#34920;&#31034;&#20026;&#19968;&#20010;&#36830;&#32493;&#21521;&#37327;&#65292;&#30452;&#25509;&#20174;&#21516;&#19968;&#35282;&#33394;&#20135;&#29983;&#30340;&#23569;&#37327;&#23545;&#35805;&#26679;&#26412;&#20013;&#23398;&#20064;&#38544;&#21547;&#30340;&#20010;&#24615;&#29305;&#23450;&#29305;&#24449;&#65292;&#24182;&#33021;&#22815;&#39640;&#25928;&#26500;&#24314;&#20010;&#24615;&#21270;&#23545;&#35805;&#20195;&#29702;&#24182;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#39537;&#21160;&#30340;&#20010;&#24615;&#21270;&#23545;&#35805;&#20195;&#29702;&#65288;DA&#65289;&#36890;&#24120;&#20381;&#36182;&#20110;&#26126;&#30830;&#30340;&#35282;&#33394;&#25551;&#36848;&#26469;&#20445;&#25345;&#20010;&#24615;&#30340;&#19968;&#33268;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25551;&#36848;&#21487;&#33021;&#24182;&#19981;&#24635;&#26159;&#21487;&#29992;&#25110;&#21487;&#33021;&#23384;&#22312;&#38544;&#31169;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;PersonaPKT&#65292;&#36825;&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#26126;&#30830;&#20010;&#24615;&#25551;&#36848;&#30340;&#24773;&#20917;&#19979;&#26500;&#24314;&#31526;&#21512;&#35282;&#33394;&#30340;&#23545;&#35805;&#27169;&#22411;&#12290;&#36890;&#36807;&#23558;&#27599;&#20010;&#20010;&#24615;&#34920;&#31034;&#20026;&#19968;&#20010;&#36830;&#32493;&#21521;&#37327;&#65292;PersonaPKT&#30452;&#25509;&#20174;&#21516;&#19968;&#35282;&#33394;&#20135;&#29983;&#30340;&#23569;&#37327;&#23545;&#35805;&#26679;&#26412;&#20013;&#23398;&#20064;&#38544;&#21547;&#30340;&#20010;&#24615;&#29305;&#23450;&#29305;&#24449;&#65292;&#24182;&#22312;PLM&#39592;&#24178;&#19978;&#22686;&#21152;&#23569;&#20110;0.1&#65285;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;PersonaPKT&#21487;&#20197;&#39640;&#25928;&#22320;&#26500;&#24314;&#20010;&#24615;&#21270;DA&#65292;&#24182;&#22312;&#20445;&#25345;&#33391;&#22909;&#30340;&#21709;&#24212;&#29983;&#25104;&#36136;&#37327;&#30340;&#21516;&#26102;&#65292;&#22312;&#35282;&#33394;&#30340;&#19968;&#33268;&#24615;&#26041;&#38754;&#20248;&#20110;&#21508;&#31181;&#22522;&#32447;&#12290;&#27492;&#22806;&#65292;&#23427;&#36890;&#36807;&#36991;&#20813;&#26126;&#30830;&#30340;&#20010;&#24615;&#25551;&#36848;&#26469;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized dialogue agents (DAs) powered by large pre-trained language models (PLMs) often rely on explicit persona descriptions to maintain personality consistency. However, such descriptions may not always be available or may pose privacy concerns. To tackle this bottleneck, we introduce PersonaPKT, a lightweight transfer learning approach that can build persona-consistent dialogue models without explicit persona descriptions. By representing each persona as a continuous vector, PersonaPKT learns implicit persona-specific features directly from a small number of dialogue samples produced by the same persona, adding less than 0.1% trainable parameters for each persona on top of the PLM backbone. Empirical results demonstrate that PersonaPKT effectively builds personalized DAs with high storage efficiency, outperforming various baselines in terms of persona consistency while maintaining good response generation quality. In addition, it enhances privacy protection by avoiding explicit
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#65292;&#21487;&#23545;&#23398;&#26415;&#20889;&#20316;&#20013;&#30340;&#25220;&#34989;&#34892;&#20026;&#36827;&#34892;&#26377;&#25928;&#26816;&#27979;&#65292;&#19981;&#20165;&#22312;&#21477;&#23376;&#32423;&#21035;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#36824;&#22312;&#25991;&#26723;&#32423;&#21035;&#19978;&#25552;&#20379;&#21487;&#37327;&#21270;&#25351;&#26631;&#12290;&#35813;&#26041;&#27861;&#30340;&#20934;&#30830;&#29575;&#39640;&#36798;94&#65285;&#65292;&#20855;&#26377;&#36739;&#24378;&#30340;&#36866;&#24212;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#33021;&#22815;&#19981;&#26029;&#38543;&#30528;LLM&#25216;&#26415;&#30340;&#21457;&#23637;&#32780;&#19981;&#26029;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.08122</link><description>&lt;p&gt;
&#20174;&#21477;&#23376;&#21040;&#25991;&#26723;&#23618;&#38754;&#30340;AI&#29983;&#25104;&#25220;&#34989;&#26816;&#27979;&#65306;&#36229;&#36234;&#40657;&#21283;&#23376;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Beyond Black Box AI-Generated Plagiarism Detection: From Sentence to Document Level. (arXiv:2306.08122v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08122
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#65292;&#21487;&#23545;&#23398;&#26415;&#20889;&#20316;&#20013;&#30340;&#25220;&#34989;&#34892;&#20026;&#36827;&#34892;&#26377;&#25928;&#26816;&#27979;&#65292;&#19981;&#20165;&#22312;&#21477;&#23376;&#32423;&#21035;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#36824;&#22312;&#25991;&#26723;&#32423;&#21035;&#19978;&#25552;&#20379;&#21487;&#37327;&#21270;&#25351;&#26631;&#12290;&#35813;&#26041;&#27861;&#30340;&#20934;&#30830;&#29575;&#39640;&#36798;94&#65285;&#65292;&#20855;&#26377;&#36739;&#24378;&#30340;&#36866;&#24212;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#33021;&#22815;&#19981;&#26029;&#38543;&#30528;LLM&#25216;&#26415;&#30340;&#21457;&#23637;&#32780;&#19981;&#26029;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#26415;&#20889;&#20316;&#20013;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#20381;&#36182;&#23548;&#33268;&#20102;&#25220;&#34989;&#29616;&#35937;&#30340;&#22686;&#21152;&#12290;&#29616;&#26377;&#30340;AI&#29983;&#25104;&#25991;&#26412;&#20998;&#31867;&#22120;&#20934;&#30830;&#24615;&#26377;&#38480;&#65292;&#24448;&#24448;&#20135;&#29983;&#35823;&#25253;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25216;&#26415;&#65292;&#25552;&#20379;&#21477;&#23376;&#21644;&#25991;&#26723;&#32423;&#21035;&#30340;&#21487;&#37327;&#21270;&#25351;&#26631;&#65292;&#26041;&#20415;&#20154;&#31867;&#35780;&#20272;&#32773;&#36827;&#34892;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#22810;&#26041;&#38754;&#30340;&#26041;&#27861;&#65292;&#29983;&#25104;&#32473;&#23450;&#38382;&#39064;&#30340;&#22810;&#20010;&#37322;&#20041;&#29256;&#26412;&#65292;&#23558;&#23427;&#20204;&#36755;&#20837;LLM&#20197;&#29983;&#25104;&#31572;&#26696;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#20313;&#24358;&#30456;&#20284;&#24230;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#65292;&#23558;&#29983;&#25104;&#30340;&#21477;&#23376;&#19982;&#23398;&#29983;&#22238;&#31572;&#20013;&#30340;&#21477;&#23376;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20998;&#31867;&#20154;&#31867;&#21644;AI&#25991;&#26412;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#36798;&#21040;&#20102;94&#65285;&#65292;&#20026;&#23398;&#26415;&#29615;&#22659;&#20013;&#30340;&#25220;&#34989;&#26816;&#27979;&#25552;&#20379;&#20102;&#24378;&#22823;&#32780;&#36866;&#24212;&#24615;&#24378;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#26041;&#27861;&#38543;&#30528;LLM&#25216;&#26415;&#30340;&#21457;&#23637;&#32780;&#19981;&#26029;&#25913;&#36827;&#65292;&#20943;&#23569;&#20102;&#26032;&#27169;&#22411;&#35757;&#32451;&#25110;&#37325;&#26032;&#37197;&#32622;&#30340;&#38656;&#27714;&#65292;&#24182;&#25552;&#20379;&#20102;&#27604;&#20256;&#32479;&#40657;&#21283;&#23376;&#26041;&#27861;&#26356;&#36879;&#26126;&#30340;&#35780;&#20272;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing reliance on large language models (LLMs) in academic writing has led to a rise in plagiarism. Existing AI-generated text classifiers have limited accuracy and often produce false positives. We propose a novel approach using natural language processing (NLP) techniques, offering quantifiable metrics at both sentence and document levels for easier interpretation by human evaluators. Our method employs a multi-faceted approach, generating multiple paraphrased versions of a given question and inputting them into the LLM to generate answers. By using a contrastive loss function based on cosine similarity, we match generated sentences with those from the student's response. Our approach achieves up to 94% accuracy in classifying human and AI text, providing a robust and adaptable solution for plagiarism detection in academic settings. This method improves with LLM advancements, reducing the need for new model training or reconfiguration, and offers a more transparent way of ev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#35299;&#23494;&#20219;&#21153;&#20316;&#20026;&#20998;&#31867;&#38382;&#39064;&#26469;&#35299;&#20915;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#21508;&#31181;&#31867;&#22411;&#30340;&#23494;&#30721;&#25968;&#25454;&#38598;&#65292;&#26368;&#32456;&#35780;&#20272;&#20102;&#21508;&#31181;Tokenizer-Model&#32452;&#21512;&#22312;&#27492;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.08116</link><description>&lt;p&gt;
CipherSniffer: &#20998;&#31867;&#23494;&#30721;&#31867;&#22411;
&lt;/p&gt;
&lt;p&gt;
CipherSniffer: Classifying Cipher Types. (arXiv:2306.08116v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#35299;&#23494;&#20219;&#21153;&#20316;&#20026;&#20998;&#31867;&#38382;&#39064;&#26469;&#35299;&#20915;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#21508;&#31181;&#31867;&#22411;&#30340;&#23494;&#30721;&#25968;&#25454;&#38598;&#65292;&#26368;&#32456;&#35780;&#20272;&#20102;&#21508;&#31181;Tokenizer-Model&#32452;&#21512;&#22312;&#27492;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23494;&#30721;&#26159;&#21152;&#23494;&#36890;&#20449;&#30340;&#24378;&#26377;&#21147;&#24037;&#20855;&#12290;&#26377;&#24456;&#22810;&#19981;&#21516;&#30340;&#23494;&#30721;&#31867;&#22411;&#65292;&#36825;&#20351;&#24471;&#20351;&#29992;&#26292;&#21147;&#30772;&#35299;&#26469;&#35299;&#23494;&#23494;&#30721;&#30340;&#35745;&#31639;&#36153;&#29992;&#26114;&#36149;&#12290;&#26412;&#25991;&#23558;&#35299;&#23494;&#20219;&#21153;&#20316;&#20026;&#20998;&#31867;&#38382;&#39064;&#26469;&#26694;&#26550;&#21270;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#32622;&#25442;&#12289;&#26367;&#25442;&#12289;&#25991;&#26412;&#21453;&#36716;&#12289;&#21333;&#35789;&#21453;&#36716;&#12289;&#21477;&#23376;&#31227;&#20301;&#21644;&#26410;&#21152;&#23494;&#25991;&#26412;&#30340;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181;Tokenizer-Model&#32452;&#21512;&#22312;&#27492;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ciphers are a powerful tool for encrypting communication. There are many different cipher types, which makes it computationally expensive to solve a cipher using brute force. In this paper, we frame the decryption task as a classification problem. We first create a dataset of transpositions, substitutions, text reversals, word reversals, sentence shifts, and unencrypted text. Then, we evaluate the performance of various tokenizer-model combinations on this task.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25506;&#35752;&#20102;AutoML&#21644;LLMs&#20043;&#38388;&#30340;&#20849;&#29983;&#20851;&#31995;&#65292;&#24182;&#25351;&#20986;&#36825;&#20004;&#20010;&#39046;&#22495;&#30340;&#34701;&#21512;&#26377;&#26395;&#39072;&#35206;NLP&#21644;AutoML&#20004;&#20010;&#39046;&#22495;&#65292;&#21516;&#26102;&#20063;&#23384;&#22312;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2306.08107</link><description>&lt;p&gt;
&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;AutoML&#65306;&#24403;&#21069;&#25361;&#25112;&#65292;&#26410;&#26469;&#26426;&#36935;&#21644;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
AutoML in the Age of Large Language Models: Current Challenges, Future Opportunities and Risks. (arXiv:2306.08107v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08107
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25506;&#35752;&#20102;AutoML&#21644;LLMs&#20043;&#38388;&#30340;&#20849;&#29983;&#20851;&#31995;&#65292;&#24182;&#25351;&#20986;&#36825;&#20004;&#20010;&#39046;&#22495;&#30340;&#34701;&#21512;&#26377;&#26395;&#39072;&#35206;NLP&#21644;AutoML&#20004;&#20010;&#39046;&#22495;&#65292;&#21516;&#26102;&#20063;&#23384;&#22312;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;&#29305;&#21035;&#26159;&#22312;NLP&#39046;&#22495;&#65292;&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26368;&#36817;&#32463;&#21382;&#20102;&#19968;&#31995;&#21015;&#31361;&#30772;&#12290;&#25105;&#20204;&#35774;&#24819;&#65292;&#20004;&#20010;&#39046;&#22495;&#36890;&#36807;&#32039;&#23494;&#30340;&#34701;&#21512;&#21487;&#20197;&#24444;&#27492;&#25512;&#21160;&#26497;&#38480;&#12290;&#20026;&#20102;&#23637;&#31034;&#36825;&#19968;&#24895;&#26223;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;AutoML&#21644;LLMs&#20043;&#38388;&#30340;&#20849;&#29983;&#20851;&#31995;&#28508;&#21147;&#65292;&#30528;&#37325;&#25506;&#35752;&#20102;&#23427;&#20204;&#22914;&#20309;&#20114;&#30456;&#21463;&#30410;&#12290;&#25105;&#20204;&#29305;&#21035;&#30740;&#31350;&#20102;&#20174;&#19981;&#21516;&#35282;&#24230;&#22686;&#24378;LLMs&#30340;AutoML&#26041;&#27861;&#30340;&#26426;&#20250;&#20197;&#21450;&#21033;&#29992;AutoML&#36827;&#19968;&#27493;&#25913;&#36827;LLMs&#30340;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#29616;&#26377;&#24037;&#20316;&#65292;&#24182;&#23545;&#20854;&#20013;&#30340;&#39118;&#38505;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#35780;&#20272;&#12290;&#25105;&#20204;&#22362;&#20449;&#65292;&#20004;&#20010;&#39046;&#22495;&#30340;&#34701;&#21512;&#26377;&#21487;&#33021;&#39072;&#35206;NLP&#21644;AutoML&#20004;&#20010;&#39046;&#22495;&#12290;&#36890;&#36807;&#24378;&#35843;&#21487;&#24819;&#35937;&#30340;&#21327;&#21516;&#20316;&#29992;&#21644;&#39118;&#38505;&#65292;&#25105;&#20204;&#26088;&#22312;&#20419;&#36827;&#22312;&#20132;&#21449;&#28857;&#30340;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The fields of both Natural Language Processing (NLP) and Automated Machine Learning (AutoML) have achieved remarkable results over the past years. In NLP, especially Large Language Models (LLMs) have experienced a rapid series of breakthroughs very recently. We envision that the two fields can radically push the boundaries of each other through tight integration. To showcase this vision, we explore the potential of a symbiotic relationship between AutoML and LLMs, shedding light on how they can benefit each other. In particular, we investigate both the opportunities to enhance AutoML approaches with LLMs from different perspectives and the challenges of leveraging AutoML to further improve LLMs. To this end, we survey existing work, and we critically assess risks. We strongly believe that the integration of the two fields has the potential to disrupt both fields, NLP and AutoML. By highlighting conceivable synergies, but also risks, we aim to foster further exploration at the intersect
&lt;/p&gt;</description></item><item><title>FLamE&#26159;&#19968;&#20010;&#21033;&#29992;GPT-3&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#29992;&#20110;RoBERTa&#24494;&#35843;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#20154;&#29983;&#25104;&#30340;&#35299;&#37322;&#22823;&#22810;&#19981;&#33021;&#20805;&#20998;&#22320;&#35777;&#26126;&#20998;&#31867;&#20915;&#31574;&#65292;&#26631;&#31614;&#29305;&#23450;&#32447;&#32034;&#22312;&#35299;&#37322;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.08042</link><description>&lt;p&gt;
FLamE: &#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#19979;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FLamE: Few-shot Learning from Natural Language Explanations. (arXiv:2306.08042v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08042
&lt;/p&gt;
&lt;p&gt;
FLamE&#26159;&#19968;&#20010;&#21033;&#29992;GPT-3&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#29992;&#20110;RoBERTa&#24494;&#35843;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#20154;&#29983;&#25104;&#30340;&#35299;&#37322;&#22823;&#22810;&#19981;&#33021;&#20805;&#20998;&#22320;&#35777;&#26126;&#20998;&#31867;&#20915;&#31574;&#65292;&#26631;&#31614;&#29305;&#23450;&#32447;&#32034;&#22312;&#35299;&#37322;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#22312;&#29702;&#35770;&#19978;&#20855;&#26377;&#20026;&#27169;&#22411;&#25512;&#29702;&#25552;&#20379;&#20016;&#23500;&#20449;&#24687;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;Lampinen&#31561;&#20154;&#30340;&#26368;&#26032;&#30740;&#31350;&#34920;&#26126;&#65292;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#22312;&#25552;&#39640;&#20998;&#31867;&#26041;&#38754;&#30340;&#25928;&#29992;&#26377;&#38480;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#20174;&#35299;&#37322;&#20013;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FLamE&#65292;&#36825;&#26159;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;&#65292;&#39318;&#20808;&#20351;&#29992;GPT-3&#29983;&#25104;&#35299;&#37322;&#65292;&#28982;&#21518;&#20351;&#29992;&#29983;&#25104;&#30340;&#35299;&#37322;&#23545;&#36739;&#23567;&#30340;&#27169;&#22411;&#65288;&#20363;&#22914;RoBERTa&#65289;&#36827;&#34892;&#24494;&#35843;&#12290;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#24378;&#22522;&#32447;&#30456;&#27604;&#65292;FLamE&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#65292;&#22312;e-SNLI&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#24615;&#27604;GPT-3 Babbage&#25552;&#39640;&#20102;17.6&#65285;&#65292;&#27604;GPT-3 Davinci&#25552;&#39640;&#20102;5.7&#65285;&#12290;&#23613;&#31649;&#25552;&#39640;&#20102;&#20998;&#31867;&#24615;&#33021;&#65292;&#20294;&#26159;&#20154;&#31867;&#35780;&#20272;&#20986;&#20154;&#29983;&#25104;&#30340;&#22823;&#37096;&#20998;&#35299;&#37322;&#37117;&#19981;&#33021;&#20805;&#20998;&#22320;&#35777;&#26126;&#20998;&#31867;&#20915;&#31574;&#12290;&#39069;&#22806;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#26631;&#31614;&#29305;&#23450;&#32447;&#32034;&#65288;&#22914;&#20013;&#31435;&#26631;&#31614;&#30340;&#8220;&#19981;&#30693;&#36947;&#8221;&#65289;&#22312;&#29983;&#25104;&#35299;&#37322;&#20013;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language explanations have the potential to provide rich information that in principle guides model reasoning. Yet, recent work by Lampinen et al. (2022) has shown limited utility of natural language explanations in improving classification. To effectively learn from explanations, we present FLamE, a two-stage few-shot learning framework that first generates explanations using GPT-3, and then finetunes a smaller model (e.g., RoBERTa) with generated explanations. Our experiments on natural language inference demonstrate effectiveness over strong baselines, increasing accuracy by 17.6% over GPT-3 Babbage and 5.7% over GPT-3 Davinci in e-SNLI. Despite improving classification performance, human evaluation surprisingly reveals that the majority of generated explanations does not adequately justify classification decisions. Additional analyses point to the important role of label-specific cues (e.g., "not know" for the neutral label) in generated explanations.
&lt;/p&gt;</description></item><item><title>Curatr&#26159;&#19968;&#20010;&#22312;&#32447;&#24179;&#21488;&#65292;&#21487;&#20197;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#25903;&#25345;&#30340;&#35821;&#20041;&#25628;&#32034;&#26469;&#36827;&#34892;&#21382;&#21490;&#25991;&#23398;&#25991;&#26412;&#30340;&#31579;&#36873;&#65292;&#25552;&#20379;&#20027;&#39064;&#35789;&#20856;&#30340;&#29983;&#25104;&#65292;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#24555;&#36895;&#20174;&#22823;&#37327;&#30340;&#25968;&#23383;&#21270;&#25991;&#26412;&#20013;&#25361;&#36873;&#20986;&#30456;&#20851;&#30340;&#23376;&#35821;&#26009;&#24211;&#12290;</title><link>http://arxiv.org/abs/2306.08020</link><description>&lt;p&gt;
Curatr&#65306;&#21382;&#21490;&#25991;&#23398;&#25991;&#26412;&#35821;&#20041;&#20998;&#26512;&#19982;&#31579;&#36873;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
Curatr: A Platform for Semantic Analysis and Curation of Historical Literary Texts. (arXiv:2306.08020v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08020
&lt;/p&gt;
&lt;p&gt;
Curatr&#26159;&#19968;&#20010;&#22312;&#32447;&#24179;&#21488;&#65292;&#21487;&#20197;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#25903;&#25345;&#30340;&#35821;&#20041;&#25628;&#32034;&#26469;&#36827;&#34892;&#21382;&#21490;&#25991;&#23398;&#25991;&#26412;&#30340;&#31579;&#36873;&#65292;&#25552;&#20379;&#20027;&#39064;&#35789;&#20856;&#30340;&#29983;&#25104;&#65292;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#24555;&#36895;&#20174;&#22823;&#37327;&#30340;&#25968;&#23383;&#21270;&#25991;&#26412;&#20013;&#25361;&#36873;&#20986;&#30456;&#20851;&#30340;&#23376;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#21270;&#21382;&#21490;&#21644;&#29616;&#20195;&#25991;&#23398;&#25991;&#26412;&#30340;&#25345;&#32493;&#22686;&#21152;&#25552;&#20379;&#20102;&#20154;&#25991;&#23398;&#31185;&#26032;&#30740;&#31350;&#30340;&#20016;&#23500;&#21487;&#33021;&#24615;&#65292;&#28982;&#32780;&#36825;&#20123;&#38598;&#21512;&#30340;&#35268;&#27169;&#21644;&#22810;&#26679;&#24615;&#65292;&#20063;&#38754;&#20020;&#30528;&#29305;&#27530;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Curatr&#65292;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#25903;&#25345;&#30340;&#35821;&#20041;&#25628;&#32034;&#22312;&#32447;&#24179;&#21488;&#65292;&#26088;&#22312;&#20026;&#25968;&#23383;&#20154;&#25991;&#23398;&#31185;&#23398;&#32773;&#25552;&#20379;&#25991;&#23398;&#25991;&#26412;&#30340;&#25506;&#32034;&#21644;&#31579;&#36873;&#12290;&#35813;&#24179;&#21488;&#25552;&#20379;&#20102;&#19968;&#20010;&#25991;&#26412;&#25366;&#25496;&#24037;&#20316;&#27969;&#65292;&#36890;&#36807;&#23558;&#31070;&#32463;&#35789;&#23884;&#20837;&#19982;&#19987;&#19994;&#39046;&#22495;&#30693;&#35782;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#20027;&#39064;&#35789;&#20856;&#30340;&#29983;&#25104;&#65292;&#21487;&#20197;&#35753;&#30740;&#31350;&#20154;&#21592;&#20174;&#22823;&#37327;&#30340;18&#19990;&#32426;&#21644;19&#19990;&#32426;&#25968;&#23383;&#21270;&#25991;&#26412;&#20013;&#31579;&#36873;&#30456;&#20851;&#23376;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing availability of digital collections of historical and contemporary literature presents a wealth of possibilities for new research in the humanities. The scale and diversity of such collections however, presents particular challenges in identifying and extracting relevant content. This paper presents Curatr, an online platform for the exploration and curation of literature with machine learning-supported semantic search, designed within the context of digital humanities scholarship. The platform provides a text mining workflow that combines neural word embeddings with expert domain knowledge to enable the generation of thematic lexicons, allowing researches to curate relevant sub-corpora from a large corpus of 18th and 19th century digitised texts.
&lt;/p&gt;</description></item><item><title>Mol-Instructions&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#35774;&#35745;&#30340;&#32508;&#21512;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#39046;&#22495;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#35748;&#30693;&#25935;&#38160;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.08018</link><description>&lt;p&gt;
Mol-Instructions: &#19968;&#20010;&#22823;&#35268;&#27169;&#29983;&#29289;&#20998;&#23376;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#20026;&#22823;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models. (arXiv:2306.08018v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08018
&lt;/p&gt;
&lt;p&gt;
Mol-Instructions&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#35774;&#35745;&#30340;&#32508;&#21512;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#39046;&#22495;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#35748;&#30693;&#25935;&#38160;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20197;&#20854;&#21331;&#36234;&#30340;&#20219;&#21153;&#22788;&#29702;&#33021;&#21147;&#21644;&#21019;&#26032;&#30340;&#36755;&#20986;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#25512;&#21160;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#29983;&#29289;&#20998;&#23376;&#30740;&#31350;&#31561;&#19987;&#19994;&#39046;&#22495;&#30340;&#29087;&#32451;&#24212;&#29992;&#36824;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Mol-Instructions&#65292;&#36825;&#26159;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#12289;&#19987;&#38376;&#38024;&#23545;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#35774;&#35745;&#30340;&#32508;&#21512;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;Mol-Instructions&#30001;&#19977;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#65306;&#20998;&#23376;&#23548;&#21521;&#25351;&#20196;&#12289;&#34507;&#30333;&#36136;&#23548;&#21521;&#25351;&#20196;&#21644;&#29983;&#29289;&#20998;&#23376;&#25991;&#26412;&#25351;&#20196;&#65292;&#27599;&#20010;&#37096;&#20998;&#37117;&#34987;&#31574;&#21010;&#29992;&#20110;&#22686;&#24378;LLM&#23545;&#29983;&#29289;&#20998;&#23376;&#29305;&#24615;&#21644;&#34892;&#20026;&#30340;&#29702;&#35299;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#20195;&#34920;&#24615;LLM&#30340;&#24191;&#27867;&#25351;&#20196;&#35843;&#25972;&#23454;&#39564;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;Mol-Instructions&#22312;&#22686;&#24378;&#22823;&#27169;&#22411;&#22312;&#29983;&#29289;&#20998;&#23376;&#30740;&#31350;&#22797;&#26434;&#39046;&#22495;&#20869;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#35748;&#30693;&#25935;&#38160;&#24230;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20174;&#32780;&#20419;&#36827;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), with their remarkable task-handling capabilities and innovative outputs, have catalyzed significant advancements across a spectrum of fields. However, their proficiency within specialized domains such as biomolecular studies remains limited. To address this challenge, we introduce Mol-Instructions, a meticulously curated, comprehensive instruction dataset expressly designed for the biomolecular realm. Mol-Instructions is composed of three pivotal components: molecule-oriented instructions, protein-oriented instructions, and biomolecular text instructions, each curated to enhance the understanding and prediction capabilities of LLMs concerning biomolecular features and behaviors. Through extensive instruction tuning experiments on the representative LLM, we underscore the potency of Mol-Instructions to enhance the adaptability and cognitive acuity of large models within the complex sphere of biomolecular studies, thereby promoting advancements in the biomol
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;DeepSpeech&#38899;&#39057;&#21040;&#23383;&#27597;&#35782;&#21035;&#24341;&#25806;&#30340;&#26032;&#26041;&#26696;&#26469;&#20998;&#31867;&#26391;&#35835;&#21644;&#33258;&#21457;&#35821;&#38899;&#12290;&#21033;&#29992;&#35813;&#26041;&#27861;&#21487;&#20197;&#38750;&#24120;&#26377;&#25928;&#22320;&#35782;&#21035;&#36825;&#20004;&#31181;&#31867;&#22411;&#30340;&#35821;&#38899;&#12290;</title><link>http://arxiv.org/abs/2306.08012</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#20998;&#31867;&#26391;&#35835;&#21644;&#33258;&#21457;&#35821;&#38899;&#30340;&#26032;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
A Novel Scheme to classify Read and Spontaneous Speech. (arXiv:2306.08012v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;DeepSpeech&#38899;&#39057;&#21040;&#23383;&#27597;&#35782;&#21035;&#24341;&#25806;&#30340;&#26032;&#26041;&#26696;&#26469;&#20998;&#31867;&#26391;&#35835;&#21644;&#33258;&#21457;&#35821;&#38899;&#12290;&#21033;&#29992;&#35813;&#26041;&#27861;&#21487;&#20197;&#38750;&#24120;&#26377;&#25928;&#22320;&#35782;&#21035;&#36825;&#20004;&#31181;&#31867;&#22411;&#30340;&#35821;&#38899;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19&#30123;&#24773;&#23548;&#33268;&#36828;&#31243;&#30005;&#35805;&#37319;&#35775;&#30340;&#22686;&#21152;&#65292;&#22240;&#27492;&#38656;&#35201;&#21306;&#20998;&#38899;&#39057;&#35760;&#24405;&#20013;&#30340;&#26391;&#35835;&#21644;&#33258;&#21457;&#35821;&#38899;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#26696;&#26469;&#35782;&#21035;&#26391;&#35835;&#21644;&#33258;&#21457;&#35821;&#38899;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;DeepSpeech&#38899;&#39057;&#21040;&#23383;&#27597;&#35782;&#21035;&#24341;&#25806;&#20174;&#38899;&#39057;&#20013;&#29983;&#25104;&#23383;&#27597;&#24207;&#21015;&#65292;&#24182;&#20174;&#20013;&#25552;&#21462;&#29305;&#24449;&#26469;&#21306;&#20998;&#26391;&#35835;&#21644;&#33258;&#21457;&#35821;&#38899;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#19968;&#23567;&#32452;&#33258;&#35299;&#37322;&#30340;&#29305;&#24449;&#20063;&#21487;&#20197;&#38750;&#24120;&#26377;&#25928;&#22320;&#20998;&#31867;&#36825;&#20004;&#31181;&#31867;&#22411;&#30340;&#35821;&#38899;&#12290;
&lt;/p&gt;
&lt;p&gt;
The COVID-19 pandemic has led to an increased use of remote telephonic interviews, making it important to distinguish between scripted and spontaneous speech in audio recordings. In this paper, we propose a novel scheme for identifying read and spontaneous speech. Our approach uses a pre-trained DeepSpeech audio-to-alphabet recognition engine to generate a sequence of alphabets from the audio. From these alphabets, we derive features that allow us to discriminate between read and spontaneous speech. Our experimental results show that even a small set of self-explanatory features can effectively classify the two types of speech very effectively.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#39046;&#22495;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;CX-BERT&#12289;BlueBERT&#21644;ClinicalBERT&#25552;&#39640;CLIP-like&#27169;&#22411;&#23545;&#20302;&#24739;&#30149;&#29575;&#33016;&#37096;&#30149;&#30151;&#30340;&#38646;&#26679;&#26412;&#26816;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#22612;&#23545;&#20110;&#20302;&#24739;&#30149;&#29575;&#30142;&#30149;&#30340;&#26816;&#27979;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#36825;&#25552;&#31034;&#20102;&#26410;&#26469;&#21487;&#20351;&#29992;&#19981;&#21516;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#38598;&#25104;&#27169;&#22411;&#36827;&#34892;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2306.08000</link><description>&lt;p&gt;
&#21033;&#29992;&#39046;&#22495;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#20302;&#24739;&#30149;&#29575;&#33016;&#37096;&#30149;&#30151;&#30340;&#38646;&#26679;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Improving Zero-Shot Detection of Low Prevalence Chest Pathologies using Domain Pre-trained Language Models. (arXiv:2306.08000v1 [physics.med-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08000
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#39046;&#22495;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;CX-BERT&#12289;BlueBERT&#21644;ClinicalBERT&#25552;&#39640;CLIP-like&#27169;&#22411;&#23545;&#20302;&#24739;&#30149;&#29575;&#33016;&#37096;&#30149;&#30151;&#30340;&#38646;&#26679;&#26412;&#26816;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#22612;&#23545;&#20110;&#20302;&#24739;&#30149;&#29575;&#30142;&#30149;&#30340;&#26816;&#27979;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#36825;&#25552;&#31034;&#20102;&#26410;&#26469;&#21487;&#20351;&#29992;&#19981;&#21516;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#38598;&#25104;&#27169;&#22411;&#36827;&#34892;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#20351;&#24471;&#21487;&#20197;&#21033;&#29992;&#25104;&#23545;&#30340;&#22270;&#20687;&#35782;&#21035;&#26631;&#31614;&#25968;&#25454;&#26367;&#20195;&#32467;&#26500;&#21270;&#26631;&#31614;&#65292;&#28040;&#38500;&#20102;&#23545;&#19987;&#23478;&#27880;&#37322;&#25968;&#25454;&#38598;&#30340;&#38656;&#27714;&#12290;&#20687;CLIP-based CheXzero&#36825;&#26679;&#30340;&#27169;&#22411;&#21033;&#29992;&#20102;&#36825;&#20123;&#22312;&#33016;&#37096;X&#23556;&#32447;&#35299;&#37322;&#39046;&#22495;&#30340;&#36827;&#27493;&#12290;&#25105;&#20204;&#20551;&#35774;&#65292;&#20351;&#29992;CX-BERT&#12289;BlueBERT&#21644;ClinicalBERT&#31561;&#39046;&#22495;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#26367;&#25442;BERT&#26435;&#37325;&#26469;&#22686;&#21152;&#29305;&#23450;&#39046;&#22495;&#30693;&#35782;&#65292;&#26377;&#21487;&#33021;&#25552;&#39640;&#31867;&#20284;&#20110;CLIP&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20294;&#20195;&#20215;&#26159;&#25171;&#30772;&#21407;&#22987;&#27169;&#22411;&#30340;&#23545;&#40784;&#24615;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20855;&#26377;&#29305;&#23450;&#39046;&#22495;&#39044;&#35757;&#32451;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#27169;&#22411;&#22312;&#26816;&#27979;&#20302;&#24739;&#30149;&#29575;&#30149;&#29702;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#26367;&#25442;&#21407;&#22987;CLIP-BERT&#26435;&#37325;&#20250;&#38477;&#20302;&#27169;&#22411;&#22312;&#24120;&#35265;&#30149;&#29702;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#39044;&#35757;&#32451;&#25991;&#26412;&#22612;&#22312;&#20302;&#24739;&#30149;&#29575;&#30142;&#30149;&#30340;&#26816;&#27979;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#28608;&#21457;&#20102;&#26410;&#26469;&#20351;&#29992;&#19981;&#21516;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#32452;&#21512;&#30340;&#38598;&#25104;&#27169;&#22411;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in zero-shot learning have enabled the use of paired image-text data to replace structured labels, replacing the need for expert annotated datasets. Models such as CLIP-based CheXzero utilize these advancements in the domain of chest X-ray interpretation. We hypothesize that domain pre-trained models such as CXR-BERT, BlueBERT, and ClinicalBERT offer the potential to improve the performance of CLIP-like models with specific domain knowledge by replacing BERT weights at the cost of breaking the original model's alignment. We evaluate the performance of zero-shot classification models with domain-specific pre-training for detecting low-prevalence pathologies. Even though replacing the weights of the original CLIP-BERT degrades model performance on commonly found pathologies, we show that pre-trained text towers perform exceptionally better on low-prevalence diseases. This motivates future ensemble models with a combination of differently trained language models for maxima
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#35789;&#27719;&#20998;&#37197;&#21333;&#29420;&#30340;&#39118;&#26684;&#21521;&#37327;&#26469;&#23545;&#25991;&#26412;&#36827;&#34892;&#39118;&#26684;&#36716;&#25442;&#65292;&#24182;&#24341;&#20837;&#22522;&#20110;&#25945;&#24072;-&#23398;&#29983;&#23398;&#20064;&#30340;&#23545;&#25239;&#24615;&#22521;&#35757;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;&#22521;&#35757;&#31283;&#23450;&#24615;&#65292;&#23454;&#29616;&#20102;&#21452;&#39118;&#26684;&#36716;&#25442;&#21644;&#22810;&#39118;&#26684;&#36716;&#25442;&#20004;&#31181;&#24773;&#20917;&#19979;&#30340;&#26126;&#26174;&#25552;&#39640;&#30340;&#39118;&#26684;&#36716;&#25442;&#20934;&#30830;&#24615;&#21644;&#20869;&#23481;&#20445;&#30041;&#12290;</title><link>http://arxiv.org/abs/2306.07994</link><description>&lt;p&gt;
MSSRNet: &#26080;&#30417;&#30563;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;&#20013;&#30340;&#39034;&#24207;&#39118;&#26684;&#34920;&#31034;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
MSSRNet: Manipulating Sequential Style Representation for Unsupervised Text Style Transfer. (arXiv:2306.07994v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#35789;&#27719;&#20998;&#37197;&#21333;&#29420;&#30340;&#39118;&#26684;&#21521;&#37327;&#26469;&#23545;&#25991;&#26412;&#36827;&#34892;&#39118;&#26684;&#36716;&#25442;&#65292;&#24182;&#24341;&#20837;&#22522;&#20110;&#25945;&#24072;-&#23398;&#29983;&#23398;&#20064;&#30340;&#23545;&#25239;&#24615;&#22521;&#35757;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;&#22521;&#35757;&#31283;&#23450;&#24615;&#65292;&#23454;&#29616;&#20102;&#21452;&#39118;&#26684;&#36716;&#25442;&#21644;&#22810;&#39118;&#26684;&#36716;&#25442;&#20004;&#31181;&#24773;&#20917;&#19979;&#30340;&#26126;&#26174;&#25552;&#39640;&#30340;&#39118;&#26684;&#36716;&#25442;&#20934;&#30830;&#24615;&#21644;&#20869;&#23481;&#20445;&#30041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;&#20219;&#21153;&#26088;&#22312;&#23558;&#25991;&#26412;&#37325;&#20889;&#20026;&#30446;&#26631;&#39118;&#26684;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#20027;&#35201;&#20869;&#23481;&#12290;&#20256;&#32479;&#26041;&#27861;&#20381;&#36182;&#20110;&#20351;&#29992;&#22266;&#23450;&#22823;&#23567;&#30340;&#21521;&#37327;&#26469;&#35843;&#33410;&#25991;&#26412;&#39118;&#26684;&#65292;&#36825;&#24456;&#38590;&#20934;&#30830;&#20256;&#36798;&#27599;&#20010;&#21333;&#29420;&#20196;&#29260;&#30340;&#39118;&#26684;&#24378;&#24230;&#12290;&#20107;&#23454;&#19978;&#65292;&#25991;&#26412;&#30340;&#27599;&#20010;&#20196;&#29260;&#37117;&#21253;&#21547;&#19981;&#21516;&#30340;&#39118;&#26684;&#24378;&#24230;&#65292;&#24182;&#23545;&#25972;&#20307;&#39118;&#26684;&#20135;&#29983;&#19981;&#21516;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#20026;&#25991;&#26412;&#20013;&#30340;&#27599;&#20010;&#20196;&#29260;&#20998;&#37197;&#21333;&#29420;&#30340;&#39118;&#26684;&#21521;&#37327;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20801;&#35768;&#23545;&#39118;&#26684;&#24378;&#24230;&#36827;&#34892;&#32454;&#31890;&#24230;&#30340;&#25511;&#21046;&#21644;&#25805;&#20316;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#25945;&#24072;-&#23398;&#29983;&#23398;&#20064;&#30340;&#23545;&#25239;&#24615;&#22521;&#35757;&#26694;&#26550;&#65292;&#20197;&#22686;&#24378;&#22521;&#35757;&#31283;&#23450;&#24615;&#24182;&#20943;&#36731;&#39640;&#32500;&#20248;&#21270;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#23454;&#39564;&#30340;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21452;&#39118;&#26684;&#36716;&#25442;&#21644;&#22810;&#39118;&#26684;&#36716;&#25442;&#35774;&#32622;&#20013;&#65292;&#20855;&#26377;&#26126;&#26174;&#25552;&#39640;&#30340;&#39118;&#26684;&#36716;&#25442;&#20934;&#30830;&#24615;&#21644;&#20869;&#23481;&#20445;&#30041;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised text style transfer task aims to rewrite a text into target style while preserving its main content. Traditional methods rely on the use of a fixed-sized vector to regulate text style, which is difficult to accurately convey the style strength for each individual token. In fact, each token of a text contains different style intensity and makes different contribution to the overall style. Our proposed method addresses this issue by assigning individual style vector to each token in a text, allowing for fine-grained control and manipulation of the style strength. Additionally, an adversarial training framework integrated with teacher-student learning is introduced to enhance training stability and reduce the complexity of high-dimensional optimization. The results of our experiments demonstrate the efficacy of our method in terms of clearly improved style transfer accuracy and content preservation in both two-style transfer and multi-style transfer settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#36870;&#25991;&#26723;&#39057;&#29575;&#65288;IDF&#65289;&#21644;&#28508;&#22312;&#29380;&#21033;&#20811;&#38647;&#20998;&#37197;&#65288;LDA&#65289;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20197;&#26356;&#22909;&#22320;&#24212;&#23545;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#30340;&#19981;&#21516;&#23646;&#24615;&#65292;&#36890;&#36807;&#22522;&#20110;&#28857;&#36190;&#25968;&#12289;&#35780;&#35770;&#25968;&#21644;&#36716;&#21457;&#25968;&#31561;&#23646;&#24615;&#23545;&#27599;&#20010;&#25991;&#26723;&#30340;&#37325;&#35201;&#24615;&#36827;&#34892;&#21152;&#26435;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#36807;&#28388;&#22122;&#22768;&#24182;&#35782;&#21035;&#26368;&#30456;&#20851;&#30340;&#20851;&#38190;&#35789;&#12290;</title><link>http://arxiv.org/abs/2306.07978</link><description>&lt;p&gt;
&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#23646;&#24615;&#22686;&#24378;&#20851;&#38190;&#35789;&#26816;&#27979;&#65306;&#22522;&#20110;IDF-LDA&#27169;&#22411;&#24212;&#29992;&#20110;&#26032;&#28010;&#24494;&#21338;
&lt;/p&gt;
&lt;p&gt;
Utilizing Social Media Attributes for Enhanced Keyword Detection: An IDF-LDA Model Applied to Sina Weibo. (arXiv:2306.07978v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#36870;&#25991;&#26723;&#39057;&#29575;&#65288;IDF&#65289;&#21644;&#28508;&#22312;&#29380;&#21033;&#20811;&#38647;&#20998;&#37197;&#65288;LDA&#65289;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20197;&#26356;&#22909;&#22320;&#24212;&#23545;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#30340;&#19981;&#21516;&#23646;&#24615;&#65292;&#36890;&#36807;&#22522;&#20110;&#28857;&#36190;&#25968;&#12289;&#35780;&#35770;&#25968;&#21644;&#36716;&#21457;&#25968;&#31561;&#23646;&#24615;&#23545;&#27599;&#20010;&#25991;&#26723;&#30340;&#37325;&#35201;&#24615;&#36827;&#34892;&#21152;&#26435;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#36807;&#28388;&#22122;&#22768;&#24182;&#35782;&#21035;&#26368;&#30456;&#20851;&#30340;&#20851;&#38190;&#35789;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;Twitter&#21644;&#24494;&#21338;&#31561;&#31038;&#20132;&#23186;&#20307;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20174;&#22823;&#37327;&#23454;&#26102;&#25991;&#26412;&#25968;&#25454;&#27969;&#20013;&#26816;&#27979;&#20851;&#38190;&#35789;&#24050;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#30340;&#38382;&#39064;&#12290;&#20851;&#38190;&#35789;&#26816;&#27979;&#38382;&#39064;&#26088;&#22312;&#20174;&#28023;&#37327;&#25991;&#26412;&#25968;&#25454;&#20013;&#25628;&#32034;&#37325;&#35201;&#20449;&#24687;&#20197;&#21453;&#26144;&#26368;&#37325;&#35201;&#30340;&#20107;&#20214;&#25110;&#35805;&#39064;&#12290;&#28982;&#32780;&#65292;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#36890;&#24120;&#20855;&#26377;&#29420;&#29305;&#30340;&#29305;&#28857;&#65306;&#25991;&#26723;&#36890;&#24120;&#24456;&#30701;&#65292;&#35821;&#35328;&#21475;&#35821;&#21270;&#65292;&#24182;&#19988;&#25968;&#25454;&#24456;&#21487;&#33021;&#20855;&#26377;&#37325;&#35201;&#30340;&#26102;&#38388;&#27169;&#24335;&#12290;&#22240;&#27492;&#65292;&#20174;&#36825;&#20123;&#25991;&#26412;&#27969;&#20013;&#21457;&#29616;&#20851;&#38190;&#20449;&#24687;&#21487;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#20851;&#38190;&#35789;&#26816;&#27979;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#23558;&#36870;&#25991;&#26723;&#39057;&#29575;&#65288;IDF&#65289;&#21644;&#28508;&#22312;&#29380;&#21033;&#20811;&#38647;&#20998;&#37197;&#65288;LDA&#65289;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#26356;&#22909;&#22320;&#24212;&#23545;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#30340;&#19981;&#21516;&#23646;&#24615;&#65292;&#22914;&#28857;&#36190;&#25968;&#12289;&#35780;&#35770;&#25968;&#21644;&#36716;&#21457;&#25968;&#12290;&#36890;&#36807;&#22522;&#20110;&#36825;&#20123;&#23646;&#24615;&#23545;&#27599;&#20010;&#25991;&#26723;&#30340;&#37325;&#35201;&#24615;&#36827;&#34892;&#21152;&#26435;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#36807;&#28388;&#22122;&#22768;&#24182;&#35782;&#21035;&#26368;&#30456;&#20851;&#30340;&#20851;&#38190;&#35789;&#12290;&#25105;&#20204;&#22312;&#20013;&#22269;&#27969;&#34892;&#30340;&#24494;&#21338;&#24179;&#21488;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#24471;&#20998;&#26041;&#38754;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid development of social media such as Twitter and Weibo, detecting keywords from a huge volume of text data streams in real-time has become a critical problem. The keyword detection problem aims at searching important information from massive text data to reflect the most important events or topics. However, social media data usually has unique features: the documents are usually short, the language is colloquial, and the data is likely to have significant temporal patterns. Therefore, it could be challenging to discover critical information from these text streams. In this paper, we propose a novel method to address the keyword detection problem in social media. Our model combines the Inverse Document Frequency (IDF) and Latent Dirichlet Allocation (LDA) models to better cope with the distinct attributes of social media data, such as the number of likes, comments, and retweets. By weighting the importance of each document based on these attributes, our method can effectiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21019;&#24615;&#22320;&#30740;&#31350;&#20102;&#22522;&#20110; prompt &#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411; API &#30340;&#29305;&#27931;&#20234;&#26131;&#24863;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#40657;&#30418;&#26694;&#26550;&#8212;&#8212;TrojPrompt&#65292;&#29992;&#20110;&#29983;&#25104;&#36890;&#29992;&#21644;&#38544;&#34109;&#30340;&#35302;&#21457;&#22120;&#65292;&#24182;&#23558;&#29305;&#27931;&#20234;&#26408;&#39532;&#25554;&#20837;&#30828;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2306.06815</link><description>&lt;p&gt;
TrojPrompt&#65306;&#22522;&#20110;&#40657;&#30418;&#26041;&#24335;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26408;&#39532;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
TrojPrompt: A Black-box Trojan Attack on Pre-trained Language Models. (arXiv:2306.06815v1 [cs.CR] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21019;&#24615;&#22320;&#30740;&#31350;&#20102;&#22522;&#20110; prompt &#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411; API &#30340;&#29305;&#27931;&#20234;&#26131;&#24863;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#40657;&#30418;&#26694;&#26550;&#8212;&#8212;TrojPrompt&#65292;&#29992;&#20110;&#29983;&#25104;&#36890;&#29992;&#21644;&#38544;&#34109;&#30340;&#35302;&#21457;&#22120;&#65292;&#24182;&#23558;&#29305;&#27931;&#20234;&#26408;&#39532;&#25554;&#20837;&#30828;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt&#23398;&#20064;&#34987;&#35777;&#26126;&#22312;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#36866;&#24212;&#24615;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#24494;&#35843;&#33539;&#24335;&#65292;&#24182;&#22312;&#19987;&#20026;&#23569;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#37327;&#36523;&#23450;&#21046;&#30340;&#24212;&#29992;&#31243;&#24207;&#21644;API&#20013;&#23637;&#29616;&#20102;&#26480;&#20986;&#30340;&#21069;&#26223;&#12290;&#20294;&#26159;&#65292;&#23613;&#31649;prompt&#23398;&#20064;&#30340;API&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#23427;&#20204;&#30340;&#23433;&#20840;&#38382;&#39064;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#22312;prompt&#23398;&#20064;&#30340;PLM API&#30340;&#29305;&#27931;&#20234;&#26131;&#24863;&#24615;&#26041;&#38754;&#36827;&#34892;&#20102;&#24320;&#21019;&#24615;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#31163;&#25955;&#25552;&#31034;&#65292;&#23569;&#26679;&#26412;&#21644;&#40657;&#30418;&#35774;&#32622;&#26159;&#20960;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#38480;&#21046;&#20102;&#29616;&#26377;&#21518;&#38376;&#25915;&#20987;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TrojPrompt&#65292;&#36825;&#26159;&#19968;&#31181;&#33258;&#21160;&#30340;&#40657;&#30418;&#26694;&#26550;&#65292;&#21487;&#26377;&#25928;&#29983;&#25104;&#36890;&#29992;&#30340;&#21644;&#38544;&#31192;&#30340;&#35302;&#21457;&#22120;&#65292;&#24182;&#23558;&#29305;&#27931;&#20234;&#26408;&#39532;&#25554;&#20837;&#30828;&#25552;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;API&#39537;&#21160;&#30340;&#36890;&#29992;&#35302;&#21457;&#22120;&#21457;&#29616;&#31639;&#27861;&#65292;&#36890;&#36807;&#26597;&#35810;&#21463;&#23475;&#32773;PLM API&#65292;&#20026;&#21508;&#31181;&#36755;&#20837;&#29983;&#25104;&#36890;&#29992;&#35302;&#21457;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt learning has been proven to be highly effective in improving pre-trained language model (PLM) adaptability, surpassing conventional fine-tuning paradigms, and showing exceptional promise in an ever-growing landscape of applications and APIs tailored for few-shot learning scenarios. Despite the growing prominence of prompt learning-based APIs, their security concerns remain underexplored. In this paper, we undertake a pioneering study on the Trojan susceptibility of prompt-learning PLM APIs. We identified several key challenges, including discrete-prompt, few-shot, and black-box settings, which limit the applicability of existing backdoor attacks. To address these challenges, we propose TrojPrompt, an automatic and black-box framework to effectively generate universal and stealthy triggers and insert Trojans into hard prompts. Specifically, we propose a universal API-driven trigger discovery algorithm for generating universal triggers for various inputs by querying victim PLM API
&lt;/p&gt;</description></item><item><title>Mind2Web&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#24320;&#21457;&#21644;&#35780;&#20272;&#36890;&#29992;Web&#20195;&#29702;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#25353;&#29031;&#35821;&#35328;&#25351;&#20196;&#23436;&#25104;&#20219;&#24847;&#32593;&#31449;&#19978;&#30340;&#22797;&#26434;&#20219;&#21153;&#12290;&#27492;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#19977;&#20010;&#24517;&#35201;&#20803;&#32032;&#65306;1&#65289;&#22810;&#26679;&#30340;&#39046;&#22495;&#12289;&#32593;&#31449;&#21644;&#20219;&#21153;&#65292;2&#65289;&#20351;&#29992;&#30495;&#23454;&#32593;&#31449;&#32780;&#38750;&#27169;&#25311;&#21644;&#31616;&#21270;&#32593;&#31449;&#65292;3&#65289;&#24191;&#27867;&#30340;&#29992;&#25143;&#20132;&#20114;&#27169;&#24335;&#12290;&#30740;&#31350;&#32773;&#20351;&#29992;Mind2Web&#36827;&#34892;&#20102;&#21021;&#27493;&#25506;&#32034;&#65292;&#20351;&#29992;&#23567;&#22411;LM&#36807;&#28388;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#20219;&#21153;&#23436;&#25104;&#29575;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.06070</link><description>&lt;p&gt;
Mind2Web&#65306;&#38754;&#21521;Web&#30340;&#36890;&#29992;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Mind2Web: Towards a Generalist Agent for the Web. (arXiv:2306.06070v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06070
&lt;/p&gt;
&lt;p&gt;
Mind2Web&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#24320;&#21457;&#21644;&#35780;&#20272;&#36890;&#29992;Web&#20195;&#29702;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#25353;&#29031;&#35821;&#35328;&#25351;&#20196;&#23436;&#25104;&#20219;&#24847;&#32593;&#31449;&#19978;&#30340;&#22797;&#26434;&#20219;&#21153;&#12290;&#27492;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#19977;&#20010;&#24517;&#35201;&#20803;&#32032;&#65306;1&#65289;&#22810;&#26679;&#30340;&#39046;&#22495;&#12289;&#32593;&#31449;&#21644;&#20219;&#21153;&#65292;2&#65289;&#20351;&#29992;&#30495;&#23454;&#32593;&#31449;&#32780;&#38750;&#27169;&#25311;&#21644;&#31616;&#21270;&#32593;&#31449;&#65292;3&#65289;&#24191;&#27867;&#30340;&#29992;&#25143;&#20132;&#20114;&#27169;&#24335;&#12290;&#30740;&#31350;&#32773;&#20351;&#29992;Mind2Web&#36827;&#34892;&#20102;&#21021;&#27493;&#25506;&#32034;&#65292;&#20351;&#29992;&#23567;&#22411;LM&#36807;&#28388;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#20219;&#21153;&#23436;&#25104;&#29575;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Mind2Web&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#24320;&#21457;&#21644;&#35780;&#20272;&#36890;&#29992;Web&#20195;&#29702;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#25353;&#29031;&#35821;&#35328;&#25351;&#20196;&#23436;&#25104;&#20219;&#24847;&#32593;&#31449;&#19978;&#30340;&#22797;&#26434;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;Web&#20195;&#29702;&#25968;&#25454;&#38598;&#35201;&#20040;&#20351;&#29992;&#27169;&#25311;&#32593;&#31449;&#65292;&#35201;&#20040;&#20165;&#35206;&#30422;&#26377;&#38480;&#30340;&#32593;&#31449;&#21644;&#20219;&#21153;&#65292;&#22240;&#27492;&#19981;&#36866;&#21512;&#36890;&#29992;Web&#20195;&#29702;&#12290;&#36890;&#36807;&#25910;&#38598;&#26469;&#33258;31&#20010;&#39046;&#22495;&#12289;137&#20010;&#32593;&#31449;&#30340;&#36229;&#36807;2,000&#20010;&#24320;&#25918;&#24335;&#20219;&#21153;&#21644;&#20219;&#21153;&#30340;&#20247;&#21253;&#25805;&#20316;&#24207;&#21015;&#65292;Mind2Web&#20026;&#26500;&#24314;&#36890;&#29992;Web&#20195;&#29702;&#25552;&#20379;&#20102;&#19977;&#20010;&#24517;&#35201;&#20803;&#32032;&#65306;1&#65289;&#22810;&#26679;&#30340;&#39046;&#22495;&#12289;&#32593;&#31449;&#21644;&#20219;&#21153;&#65292;2&#65289;&#20351;&#29992;&#30495;&#23454;&#32593;&#31449;&#32780;&#38750;&#27169;&#25311;&#21644;&#31616;&#21270;&#32593;&#31449;&#65292;3&#65289;&#24191;&#27867;&#30340;&#29992;&#25143;&#20132;&#20114;&#27169;&#24335;&#12290;&#22522;&#20110;Mind2Web&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26500;&#24314;&#36890;&#29992;Web&#20195;&#29702;&#30340;&#21021;&#27493;&#25506;&#32034;&#12290;&#34429;&#28982;&#29616;&#23454;&#19990;&#30028;&#32593;&#31449;&#30340;&#21407;&#22987;HTML&#24448;&#24448;&#22826;&#22823;&#32780;&#26080;&#27861;&#25552;&#20379;&#32473;LLMs&#65292;&#20294;&#25105;&#20204;&#23637;&#31034;&#20102;&#39318;&#20808;&#29992;&#23567;&#22411;LM&#36807;&#28388;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#20219;&#21153;&#23436;&#25104;&#29575;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Mind2Web, the first dataset for developing and evaluating generalist agents for the web that can follow language instructions to complete complex tasks on any website. Existing datasets for web agents either use simulated websites or only cover a limited set of websites and tasks, thus not suitable for generalist web agents. With over 2,000 open-ended tasks collected from 137 websites spanning 31 domains and crowdsourced action sequences for the tasks, Mind2Web provides three necessary ingredients for building generalist web agents: 1) diverse domains, websites, and tasks, 2) use of real-world websites instead of simulated and simplified ones, and 3) a broad spectrum of user interaction patterns. Based on Mind2Web, we conduct an initial exploration of using large language models (LLMs) for building generalist web agents. While the raw HTML of real-world websites are often too large to be fed to LLMs, we show that first filtering it with a small LM significantly improves th
&lt;/p&gt;</description></item><item><title>Xiezhi&#26159;&#19968;&#31181;&#20840;&#38754;&#32508;&#21512;&#30340;&#35780;&#20272;&#22871;&#20214;&#65292;&#35774;&#26377;516&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65292;&#35206;&#30422;&#20102;&#20174;13&#20010;&#19981;&#21516;&#23398;&#31185;&#36328;&#36234;&#30340;15&#20010;&#19987;&#19994;&#39046;&#22495;&#65292;&#24182;&#23545;47&#20010;&#20808;&#36827;&#30340;LLMs&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;LLMs&#22312;&#22823;&#22810;&#25968;&#39046;&#22495;&#36229;&#36234;&#20154;&#31867;&#65292;&#20294;&#22312;&#19968;&#20123;&#39046;&#22495;&#34920;&#29616;&#19981;&#20339;&#12290;</title><link>http://arxiv.org/abs/2306.05783</link><description>&lt;p&gt;
Xiezhi&#65306;&#19968;&#31181;&#20840;&#38754;&#26356;&#26032;&#30340;&#32508;&#21512;&#39046;&#22495;&#30693;&#35782;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation. (arXiv:2306.05783v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05783
&lt;/p&gt;
&lt;p&gt;
Xiezhi&#26159;&#19968;&#31181;&#20840;&#38754;&#32508;&#21512;&#30340;&#35780;&#20272;&#22871;&#20214;&#65292;&#35774;&#26377;516&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65292;&#35206;&#30422;&#20102;&#20174;13&#20010;&#19981;&#21516;&#23398;&#31185;&#36328;&#36234;&#30340;15&#20010;&#19987;&#19994;&#39046;&#22495;&#65292;&#24182;&#23545;47&#20010;&#20808;&#36827;&#30340;LLMs&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;LLMs&#22312;&#22823;&#22810;&#25968;&#39046;&#22495;&#36229;&#36234;&#20154;&#31867;&#65292;&#20294;&#22312;&#19968;&#20123;&#39046;&#22495;&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#24613;&#38656;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#22522;&#20934;&#26469;&#23454;&#29616;&#23545;&#40784;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Xiezhi&#65292;&#36825;&#26159;&#19968;&#20010;&#26368;&#20840;&#38754;&#30340;&#35780;&#20272;&#22871;&#20214;&#65292;&#26088;&#22312;&#35780;&#20272;&#32508;&#21512;&#39046;&#22495;&#30693;&#35782;&#12290;Xiezhi&#21253;&#25324;&#36328;&#36234;13&#20010;&#19981;&#21516;&#23398;&#31185;&#30340;516&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65292;&#21253;&#25324;22&#19975;&#20010;&#38382;&#39064;&#65292;&#24182;&#19988;&#38468;&#24102;Xiezhi-Specialty&#21644;Xiezhi-Interdiscipline&#65292;&#22343;&#26377;15,000&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;47&#20010;&#20808;&#36827;&#30340;LLM&#22312;Xiezhi&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;LLM&#22312;&#31185;&#23398;&#12289;&#24037;&#31243;&#12289;&#20892;&#23398;&#12289;&#21307;&#23398;&#21644;&#33402;&#26415;&#26041;&#38754;&#36229;&#36807;&#20102;&#20154;&#31867;&#30340;&#24179;&#22343;&#34920;&#29616;&#65292;&#20294;&#22312;&#32463;&#27982;&#23398;&#12289;&#27861;&#23398;&#12289;&#25945;&#32946;&#23398;&#12289;&#25991;&#23398;&#12289;&#21382;&#21490;&#21644;&#31649;&#29702;&#26041;&#38754;&#21017;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#26399;&#26395;Xiezhi&#23558;&#26377;&#21161;&#20110;&#20998;&#26512;LLM&#30340;&#37325;&#35201;&#20248;&#28857;&#21644;&#19981;&#36275;&#20043;&#22788;&#65292;&#35813;&#22522;&#20934;&#24050;&#22312;https://github.com/MikeGu721/XiezhiBenchmark&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
New Natural Langauge Process~(NLP) benchmarks are urgently needed to align with the rapid development of large language models (LLMs). We present Xiezhi, the most comprehensive evaluation suite designed to assess holistic domain knowledge. Xiezhi comprises multiple-choice questions across 516 diverse disciplines ranging from 13 different subjects with 220,000 questions and accompanied by Xiezhi-Specialty and Xiezhi-Interdiscipline, both with 15k questions. We conduct evaluation of the 47 cutting-edge LLMs on Xiezhi. Results indicate that LLMs exceed average performance of humans in science, engineering, agronomy, medicine, and art, but fall short in economics, jurisprudence, pedagogy, literature, history, and management. We anticipate Xiezhi will help analyze important strengths and shortcomings of LLMs, and the benchmark is released in https://github.com/MikeGu721/XiezhiBenchmark .
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#36138;&#24515;&#23545;&#25239;&#25915;&#20987;&#65292;&#38024;&#23545;&#22522;&#20110;&#25552;&#31034;&#30340;&#27169;&#26495;&#22312;PLMs&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#28431;&#27934;&#65292;&#36890;&#36807;&#23383;&#31526;&#32423;&#21644;&#21333;&#35789;&#32423;&#30340;&#30772;&#22351;&#26041;&#27861;&#36827;&#34892;&#25915;&#20987;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.05659</link><description>&lt;p&gt;
COVER&#65306;&#19968;&#31181;&#21551;&#21457;&#24335;&#36138;&#24515;&#23545;&#25239;&#25915;&#20987;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22522;&#20110;&#25552;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
COVER: A Heuristic Greedy Adversarial Attack on Prompt-based Learning in Language Models. (arXiv:2306.05659v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#36138;&#24515;&#23545;&#25239;&#25915;&#20987;&#65292;&#38024;&#23545;&#22522;&#20110;&#25552;&#31034;&#30340;&#27169;&#26495;&#22312;PLMs&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#28431;&#27934;&#65292;&#36890;&#36807;&#23383;&#31526;&#32423;&#21644;&#21333;&#35789;&#32423;&#30340;&#30772;&#22351;&#26041;&#27861;&#36827;&#34892;&#25915;&#20987;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#24050;&#34987;&#35777;&#26126;&#26159;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#20013;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#24335;&#65292;&#29305;&#21035;&#26159;&#22312;&#20687;&#23569;&#37327;&#26679;&#26412;&#22330;&#26223;&#36825;&#26679;&#30340;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#12290;&#28982;&#32780;&#65292;PLMs&#30340;&#21487;&#20449;&#24230;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#19988;&#22312;&#22522;&#20110;&#27169;&#26495;&#30340;&#25552;&#31034;&#20013;&#24050;&#32463;&#26174;&#31034;&#20986;&#20102;&#28508;&#22312;&#30340;&#28431;&#27934;&#65292;&#21487;&#33021;&#20250;&#35823;&#23548;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#24341;&#36215;&#20005;&#37325;&#30340;&#23433;&#20840;&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#40657;&#30418;&#22330;&#26223;&#20013;&#25552;&#20986;&#22522;&#20110;&#25552;&#31034;&#30340;&#23545;&#25239;&#25915;&#20987;&#25163;&#27573;&#65292;&#25581;&#31034;&#20102;PLMs&#30340;&#19968;&#20123;&#28431;&#27934;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#23383;&#31526;&#32423;&#21035;&#21644;&#21333;&#35789;&#32423;&#21035;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#30772;&#22351;&#25163;&#21160;&#27169;&#26495;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;&#19978;&#36848;&#21551;&#21457;&#24335;&#30772;&#22351;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#36138;&#24515;&#31639;&#27861;&#36827;&#34892;&#25915;&#20987;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;BERT&#31995;&#21015;&#27169;&#22411;&#30340;&#19977;&#20010;&#21464;&#31181;&#21644;&#20843;&#20010;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#20219;&#21153;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25915;&#20987;&#25104;&#21151;&#29575;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt-based learning has been proved to be an effective way in pre-trained language models (PLMs), especially in low-resource scenarios like few-shot settings. However, the trustworthiness of PLMs is of paramount significance and potential vulnerabilities have been shown in prompt-based templates that could mislead the predictions of language models, causing serious security concerns. In this paper, we will shed light on some vulnerabilities of PLMs, by proposing a prompt-based adversarial attack on manual templates in black box scenarios. First of all, we design character-level and word-level heuristic approaches to break manual templates separately. Then we present a greedy algorithm for the attack based on the above heuristic destructive approaches. Finally, we evaluate our approach with the classification tasks on three variants of BERT series models and eight datasets. And comprehensive experimental results justify the effectiveness of our approach in terms of attack success rate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20026;IWSLT 2023&#22810;&#35821;&#35328;&#32763;&#35793;&#36129;&#29486;&#30340;&#32763;&#35793;&#31995;&#32479;&#65292;&#20854;&#37325;&#28857;&#22312;&#20110;&#32763;&#35793;&#31185;&#23398;&#20250;&#35758;&#28436;&#35762;&#12290;&#29992;&#8220;&#26816;&#32034;&#24335;&#26041;&#27861;&#8221;&#65288;kNN-MT&#65289;&#36827;&#34892;&#26377;&#25928;&#30340;&#36866;&#24212;&#65292;&#35813;&#31995;&#32479;&#37319;&#29992;&#36866;&#37197;&#22120;&#36731;&#26494;&#38598;&#25104;&#26469;&#33258;&#25968;&#25454;&#22686;&#24378;&#30340;&#22686;&#37327;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#23637;&#31034;&#32423;&#32852;&#31995;&#32479;&#26356;&#23481;&#26131;&#36866;&#24212;&#29305;&#23450;&#30446;&#26631;&#39046;&#22495;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2306.05320</link><description>&lt;p&gt;
KIT&#30340;&#22810;&#35821;&#35328;&#28436;&#35762;&#32763;&#35793;&#31995;&#32479;&#22312;IWSLT 2023&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
KIT's Multilingual Speech Translation System for IWSLT 2023. (arXiv:2306.05320v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20026;IWSLT 2023&#22810;&#35821;&#35328;&#32763;&#35793;&#36129;&#29486;&#30340;&#32763;&#35793;&#31995;&#32479;&#65292;&#20854;&#37325;&#28857;&#22312;&#20110;&#32763;&#35793;&#31185;&#23398;&#20250;&#35758;&#28436;&#35762;&#12290;&#29992;&#8220;&#26816;&#32034;&#24335;&#26041;&#27861;&#8221;&#65288;kNN-MT&#65289;&#36827;&#34892;&#26377;&#25928;&#30340;&#36866;&#24212;&#65292;&#35813;&#31995;&#32479;&#37319;&#29992;&#36866;&#37197;&#22120;&#36731;&#26494;&#38598;&#25104;&#26469;&#33258;&#25968;&#25454;&#22686;&#24378;&#30340;&#22686;&#37327;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#23637;&#31034;&#32423;&#32852;&#31995;&#32479;&#26356;&#23481;&#26131;&#36866;&#24212;&#29305;&#23450;&#30446;&#26631;&#39046;&#22495;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#26377;&#30340;&#35821;&#38899;&#32763;&#35793;&#22522;&#20934;&#27979;&#35797;&#37117;&#38024;&#23545;&#39640;&#21697;&#36136;&#24405;&#38899;&#26465;&#20214;&#19979;&#30340;&#20197;&#33521;&#35821;&#20026;&#27597;&#35821;&#30340;&#35821;&#38899;&#65292;&#36825;&#36890;&#24120;&#19982;&#23454;&#38469;&#20351;&#29992;&#24773;&#20917;&#20013;&#30340;&#26465;&#20214;&#19981;&#31526;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#25105;&#20204;&#20026;IWSLT 2023&#22810;&#35821;&#35328;&#36712;&#36947;&#35774;&#35745;&#30340;&#35821;&#38899;&#32763;&#35793;&#31995;&#32479;&#65292;&#37325;&#28857;&#32763;&#35793;&#31185;&#23398;&#20250;&#35758;&#28436;&#35762;&#12290;&#27979;&#35797;&#26465;&#20214;&#21253;&#25324;&#21475;&#38899;&#37325;&#30340;&#36755;&#20837;&#35821;&#38899;&#21644;&#26415;&#35821;&#23494;&#38598;&#30340;&#20869;&#23481;&#65292;&#24182;&#19988;&#38656;&#35201;&#32763;&#35793;&#25104;10&#31181;&#36164;&#28304;&#25968;&#37327;&#19981;&#21516;&#30340;&#35821;&#35328;&#12290;&#22312;&#27809;&#26377;&#26469;&#33258;&#30446;&#26631;&#39046;&#22495;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#26816;&#32034;&#24335;&#26041;&#27861;&#65288;kNN-MT&#65289;&#36827;&#34892;&#26377;&#25928;&#30340;&#36866;&#24212;&#65288;&#35821;&#38899;&#32763;&#35793;+0.8 BLEU&#65289;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#36866;&#37197;&#22120;&#36731;&#26494;&#38598;&#25104;&#26469;&#33258;&#25968;&#25454;&#22686;&#24378;&#30340;&#22686;&#37327;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#23637;&#31034;&#20854;&#19982;&#37325;&#26032;&#35757;&#32451;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#32423;&#32852;&#31995;&#32479;&#26356;&#23481;&#26131;&#36866;&#24212;&#29305;&#23450;&#30446;&#26631;&#39046;&#22495;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;&#30001;&#22810;&#20010;&#29420;&#31435;&#27169;&#22359;&#32452;&#25104;&#30340;&#12290;&#25105;&#20204;&#30340;&#32423;&#32852;&#35821;&#38899;&#31995;&#32479;&#36828;&#36828;&#20248;&#20110;&#20854;&#31471;&#21040;&#31471;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many existing speech translation benchmarks focus on native-English speech in high-quality recording conditions, which often do not match the conditions in real-life use-cases. In this paper, we describe our speech translation system for the multilingual track of IWSLT 2023, which focuses on the translation of scientific conference talks. The test condition features accented input speech and terminology-dense contents. The tasks requires translation into 10 languages of varying amounts of resources. In absence of training data from the target domain, we use a retrieval-based approach (kNN-MT) for effective adaptation (+0.8 BLEU for speech translation). We also use adapters to easily integrate incremental training data from data augmentation, and show that it matches the performance of re-training. We observe that cascaded systems are more easily adaptable towards specific target domains, due to their separate modules. Our cascaded speech system substantially outperforms its end-to-end 
&lt;/p&gt;</description></item><item><title>INSTRUCTEVAL&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#32508;&#21512;&#22871;&#20214;&#65292;&#23427;&#37319;&#21462;&#20102;&#20840;&#38754;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#35299;&#20915;&#38382;&#39064;&#12289;&#20889;&#20316;&#33021;&#21147;&#21644;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#19968;&#33268;&#24615;&#31561;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2306.04757</link><description>&lt;p&gt;
INSTRUCTEVAL&#65306;&#38754;&#21521;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25972;&#20307;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large Language Models. (arXiv:2306.04757v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04757
&lt;/p&gt;
&lt;p&gt;
INSTRUCTEVAL&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#32508;&#21512;&#22871;&#20214;&#65292;&#23427;&#37319;&#21462;&#20102;&#20840;&#38754;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#35299;&#20915;&#38382;&#39064;&#12289;&#20889;&#20316;&#33021;&#21147;&#21644;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#19968;&#33268;&#24615;&#31561;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#20174;&#26681;&#26412;&#19978;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#24050;&#32463;&#22312;&#35832;&#22914;&#23545;&#35805;&#20195;&#29702;&#31561;&#24212;&#29992;&#20013;&#26174;&#31034;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#36825;&#20123;&#27169;&#22411;&#65292;&#22914;GPT-4&#65292;&#19981;&#20165;&#33021;&#22815;&#25484;&#25569;&#35821;&#35328;&#65292;&#32780;&#19988;&#21487;&#20197;&#35299;&#20915;&#25968;&#23398;&#12289;&#32534;&#30721;&#12289;&#21307;&#23398;&#21644;&#27861;&#24459;&#31561;&#39046;&#22495;&#30340;&#22797;&#26434;&#20219;&#21153;&#12290;&#23613;&#31649;&#23427;&#20204;&#20855;&#26377;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#20294;&#30001;&#20110;&#35768;&#22810;&#27169;&#22411;&#30340;&#40657;&#30418;&#24615;&#36136;&#21644;&#32570;&#20047;&#20840;&#38754;&#30340;&#35780;&#20272;&#30740;&#31350;&#65292;&#23545;&#23427;&#20204;&#30340;&#20840;&#37096;&#28508;&#21147;&#20173;&#28982;&#32570;&#20047;&#20840;&#38754;&#30340;&#29702;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;INSTRUCTEVAL&#65292;&#19968;&#20010;&#26356;&#20840;&#38754;&#30340;&#35780;&#20272;&#22871;&#20214;&#65292;&#19987;&#38376;&#38024;&#23545;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#19982;&#20197;&#24448;&#30340;&#20316;&#21697;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#35780;&#20272;&#21253;&#25324;&#23545;&#27169;&#22411;&#22522;&#20110;&#35299;&#20915;&#38382;&#39064;&#12289;&#20889;&#20316;&#33021;&#21147;&#21644;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#19968;&#33268;&#24615;&#30340;&#20005;&#26684;&#35780;&#20272;&#12290;&#25105;&#20204;&#37319;&#21462;&#20102;&#20840;&#38754;&#30340;&#26041;&#27861;&#26469;&#20998;&#26512;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#30340;&#21508;&#31181;&#22240;&#32032;&#65292;&#21253;&#25324;&#39044;&#35757;&#32451;&#22522;&#30784;&#12289;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#21644;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned large language models have revolutionized natural language processing and have shown great potential in applications such as conversational agents. These models, such as GPT-4, can not only master language but also solve complex tasks in areas like mathematics, coding, medicine, and law. Despite their impressive capabilities, there is still a lack of comprehensive understanding regarding their full potential, primarily due to the black-box nature of many models and the absence of holistic evaluation studies. To address these challenges, we present INSTRUCTEVAL, a more comprehensive evaluation suite designed specifically for instruction-tuned large language models. Unlike previous works, our evaluation involves a rigorous assessment of models based on problem-solving, writing ability, and alignment to human values. We take a holistic approach to analyze various factors affecting model performance, including the pretraining foundation, instruction-tuning data, and train
&lt;/p&gt;</description></item><item><title>Zambezi Voice&#26159;&#38024;&#23545;&#36190;&#27604;&#20122;&#35821;&#30340;&#22810;&#35821;&#31181;&#35821;&#38899;&#35821;&#26009;&#24211;&#65292;&#21547;&#26377;160&#23567;&#26102;&#26080;&#26631;&#31614;&#38899;&#39057;&#21644;80&#23567;&#26102;&#26377;&#26631;&#31614;&#25968;&#25454;&#65292;&#21487;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#21644;&#22810;&#35821;&#31181;&#35821;&#38899;&#22788;&#29702;&#30740;&#31350;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#20026;&#36190;&#27604;&#20122;&#35821;&#21019;&#36896;&#30340;&#31532;&#19968;&#20010;&#22810;&#35821;&#31181;&#35821;&#38899;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.04428</link><description>&lt;p&gt;
Zambezi Voice: &#19968;&#31181;&#36190;&#27604;&#20122;&#35821;&#30340;&#22810;&#35821;&#31181;&#35821;&#38899;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
Zambezi Voice: A Multilingual Speech Corpus for Zambian Languages. (arXiv:2306.04428v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04428
&lt;/p&gt;
&lt;p&gt;
Zambezi Voice&#26159;&#38024;&#23545;&#36190;&#27604;&#20122;&#35821;&#30340;&#22810;&#35821;&#31181;&#35821;&#38899;&#35821;&#26009;&#24211;&#65292;&#21547;&#26377;160&#23567;&#26102;&#26080;&#26631;&#31614;&#38899;&#39057;&#21644;80&#23567;&#26102;&#26377;&#26631;&#31614;&#25968;&#25454;&#65292;&#21487;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#21644;&#22810;&#35821;&#31181;&#35821;&#38899;&#22788;&#29702;&#30740;&#31350;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#20026;&#36190;&#27604;&#20122;&#35821;&#21019;&#36896;&#30340;&#31532;&#19968;&#20010;&#22810;&#35821;&#31181;&#35821;&#38899;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; Zambezi Voice&#65292;&#23427;&#26159;&#19968;&#31181;&#38024;&#23545;&#36190;&#27604;&#20122;&#35821;&#30340;&#24320;&#28304;&#22810;&#35821;&#31181;&#35821;&#38899;&#36164;&#28304;&#12290;&#23427;&#21253;&#21547;&#20004;&#20010;&#25968;&#25454;&#38598;&#21512;&#65306;&#19968;&#32452;&#26080;&#26631;&#31614;&#30340;&#25910;&#38899;&#26426;&#26032;&#38395;&#21644;&#35848;&#35805;&#33410;&#30446;&#30340;&#38899;&#39057;&#24405;&#38899;&#65288;160&#23567;&#26102;&#65289;&#65292;&#20197;&#21450;&#19968;&#32452;&#34987;&#26631;&#27880;&#30340;&#25968;&#25454;&#65288;&#36229;&#36807;80&#23567;&#26102;&#65289;&#65292;&#20854;&#20013;&#21253;&#25324;&#20174;&#20844;&#20849;&#21487;&#24471;&#30340;&#25991;&#23398;&#20070;&#31821;&#20013;&#28304;&#33258;&#30340;&#25991;&#26412;&#30340;&#26391;&#35835;&#35821;&#38899;&#12290;&#35813;&#25968;&#25454;&#38598;&#34987;&#21019;&#24314;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#65292;&#20294;&#21487;&#20197;&#25193;&#23637;&#21040;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;&#22810;&#35821;&#31181;&#35821;&#38899;&#22788;&#29702;&#30740;&#31350;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#20026;&#36190;&#27604;&#20122;&#35821;&#21019;&#36896;&#30340;&#31532;&#19968;&#20010;&#22810;&#35821;&#31181;&#35821;&#38899;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#24494;&#35843;Wav2Vec2.0&#22823;&#35268;&#27169;&#22810;&#35821;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#21033;&#29992;&#39044;&#35757;&#32451;&#21644;&#36328;&#35821;&#35328;&#36716;&#31227;&#23398;&#20064;&#26469;&#26500;&#24314;&#22522;&#32447;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#65288;E2E&#65289;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#12290;&#35813;&#25968;&#25454;&#38598;&#22312;&#20844;&#20849;&#39046;&#22495;&#19979;&#21457;&#24067;&#65292;&#24182;&#21487;&#36890;&#36807;&#39033;&#30446;&#20195;&#30721;&#24211;&#36827;&#34892;&#35775;&#38382;&#12290;&#35831;&#21442;&#35265; https://github.com/unza-speech-lab/zambezi-v&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces Zambezi Voice, an open-source multilingual speech resource for Zambian languages. It contains two collections of datasets: unlabelled audio recordings of radio news and talk shows programs (160 hours) and labelled data (over 80 hours) consisting of read speech recorded from text sourced from publicly available literature books. The dataset is created for speech recognition but can be extended to multilingual speech processing research for both supervised and unsupervised learning approaches. To our knowledge, this is the first multilingual speech dataset created for Zambian languages. We exploit pretraining and cross-lingual transfer learning by finetuning the Wav2Vec2.0 large-scale multilingual pre-trained model to build end-to-end (E2E) speech recognition models for our baseline models. The dataset is released publicly under a Creative Commons BY-NC-ND 4.0 license and can be accessed through the project repository. See https://github.com/unza-speech-lab/zambezi-v
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#30340;&#21518;&#35757;&#32451;&#25216;&#26415;Dial-MAE&#65292;&#21033;&#29992;&#29983;&#25104;&#26041;&#27861;&#26356;&#22909;&#22320;&#21387;&#32553;&#23545;&#35805;&#35821;&#20041;&#33267;&#23494;&#38598;&#21521;&#37327;&#65292;&#24182;&#25552;&#39640;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04357</link><description>&lt;p&gt;
&#29992;&#20110;&#22522;&#20110;&#26816;&#32034;&#30340;&#23545;&#35805;&#31995;&#32479;&#30340;&#19978;&#19979;&#25991;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
ConTextual Masked Auto-Encoder for Retrieval-based Dialogue Systems. (arXiv:2306.04357v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#30340;&#21518;&#35757;&#32451;&#25216;&#26415;Dial-MAE&#65292;&#21033;&#29992;&#29983;&#25104;&#26041;&#27861;&#26356;&#22909;&#22320;&#21387;&#32553;&#23545;&#35805;&#35821;&#20041;&#33267;&#23494;&#38598;&#21521;&#37327;&#65292;&#24182;&#25552;&#39640;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#26088;&#22312;&#26681;&#25454;&#32473;&#23450;&#30340;&#29992;&#25143;&#21644;&#31995;&#32479;&#35805;&#35821;&#21382;&#21490;&#35760;&#24405;&#20174;&#20960;&#20010;&#20505;&#36873;&#21709;&#24212;&#20013;&#36873;&#25321;&#36866;&#24403;&#30340;&#21709;&#24212;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#21518;&#35757;&#32451;&#22823;&#22810;&#20381;&#36182;&#20110;&#21333;&#32431;&#30340;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#26469;&#25552;&#39640;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#30340;&#20934;&#30830;&#24615;&#12290;&#20294;&#26159;&#65292;&#26368;&#36817;&#24320;&#21457;&#30340;&#29983;&#25104;&#26041;&#27861;&#22312;IR&#31038;&#21306;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#25991;&#26412;&#34920;&#31034;&#33021;&#21147;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#26356;&#22909;&#30340;&#23545;&#35805;&#35821;&#20041;&#24314;&#27169;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986; Dial-MAE&#65288;&#23545;&#35805;&#19978;&#19979;&#25991;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#38024;&#23545;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#30340;&#21518;&#35757;&#32451;&#25216;&#26415;&#12290; Dial-MAE&#20351;&#29992;&#19968;&#20010;&#19981;&#23545;&#31216;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#23398;&#20064;&#23558;&#23545;&#35805;&#30340;&#35821;&#20041;&#26356;&#22909;&#22320;&#21387;&#32553;&#21040;&#23494;&#38598;&#21521;&#37327;&#20013;&#12290; Dial-MAE&#30340;&#36807;&#31243;&#21253;&#25324;&#30001;&#28145;&#24230;&#32534;&#30721;&#22120;&#21019;&#24314;&#24102;&#26377;&#25513;&#30721;&#23545;&#35805;&#19978;&#19979;&#25991;&#30340;&#23545;&#35805;&#23884;&#20837;&#65292;&#28982;&#21518;&#26159;&#27973;&#35299;&#30721;&#22120;&#65292;&#35813;&#35299;&#30721;&#22120;&#20351;&#29992;&#27492;&#23884;&#20837;&#20197;&#21450;&#19978;&#19979;&#25991;&#21521;&#37327;&#26469;&#29983;&#25104;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue response selection aims to select an appropriate response from several candidates based on a given user and system utterance history. Recent studies have been improving the accuracy of dialogue response selection through post-training, mostly relying on naive masked language modeling methods. However, the recently developed generative methods have shown promising text representation capabilities in IR community, which could potentially lead to better dialogue semantics modeling. Thus, in this paper, we propose Dial-MAE (Dialogue Contextual Masking Auto-encoder), a straightforward yet effective post-training technique tailored for dialogue response selection. Dial-MAE uses an asymmetric encoder-decoder architecture that learns to better compress the semantics of the dialogue into dialogue-dense vectors. The process of Dial-MAE involves a deep encoder creating a dialogue embedding with the masked dialogue context, followed by a shallow decoder that uses this embedding along with
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;MidMed&#65292;&#19968;&#20010;&#38754;&#21521;&#21307;&#30103;&#21672;&#35810;&#30340;&#28151;&#21512;&#31867;&#22411;&#23545;&#35805;&#31995;&#32479;&#65292;&#28085;&#30422;&#20116;&#31181;&#23545;&#35805;&#31867;&#22411;&#21644;&#22235;&#20010;&#37096;&#38376;&#12290;&#25552;&#20986;&#25351;&#23548;&#24335;&#30340;&#21307;&#23398;&#23545;&#35805;&#29983;&#25104;&#26694;&#26550;InsMed&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22312;&#22810;&#20010;&#35780;&#20272;&#25351;&#26631;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.02923</link><description>&lt;p&gt;
MidMed&#65306;&#38754;&#21521;&#21307;&#30103;&#21672;&#35810;&#30340;&#28151;&#21512;&#31867;&#22411;&#23545;&#35805;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
MidMed: Towards Mixed-Type Dialogues for Medical Consultation. (arXiv:2306.02923v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;MidMed&#65292;&#19968;&#20010;&#38754;&#21521;&#21307;&#30103;&#21672;&#35810;&#30340;&#28151;&#21512;&#31867;&#22411;&#23545;&#35805;&#31995;&#32479;&#65292;&#28085;&#30422;&#20116;&#31181;&#23545;&#35805;&#31867;&#22411;&#21644;&#22235;&#20010;&#37096;&#38376;&#12290;&#25552;&#20986;&#25351;&#23548;&#24335;&#30340;&#21307;&#23398;&#23545;&#35805;&#29983;&#25104;&#26694;&#26550;InsMed&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22312;&#22810;&#20010;&#35780;&#20272;&#25351;&#26631;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#21307;&#30103;&#23545;&#35805;&#31995;&#32479;&#20551;&#35774;&#22312;&#21307;&#23398;&#21672;&#35810;&#20043;&#21069;&#65292;&#24739;&#32773;&#37117;&#26377;&#26126;&#30830;&#30340;&#30446;&#26631;&#65288;&#33647;&#29289;&#26597;&#35810;&#12289;&#25163;&#26415;&#26597;&#35810;&#31561;&#65289;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#32570;&#20047;&#21307;&#23398;&#30693;&#35782;&#65292;&#24739;&#32773;&#36890;&#24120;&#24456;&#38590;&#30830;&#23450;&#25152;&#38656;&#20449;&#24687;&#24182;&#21046;&#23450;&#26126;&#30830;&#30340;&#30446;&#26631;&#12290;&#26412;&#25991;&#23558;&#36825;&#19968;&#25361;&#25112;&#35270;&#20026;&#22914;&#20309;&#26500;&#24314;&#21307;&#23398;&#21672;&#35810;&#23545;&#35805;&#31995;&#32479;&#20197;&#24110;&#21161;&#24739;&#32773;&#28548;&#28165;&#30446;&#26631;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#20154;&#23545;&#20154;&#30340;&#28151;&#21512;&#31867;&#22411;&#21307;&#23398;&#21672;&#35810;&#23545;&#35805;&#35821;&#26009;&#24211;&#65288;MidMed&#65289;&#65292;&#21253;&#25324;&#35786;&#26029;&#12289;&#24314;&#35758;&#12289;&#30693;&#35782;&#24341;&#23548;&#12289;&#38382;&#31572;&#21644;&#38386;&#32842;&#31561;&#20116;&#31181;&#23545;&#35805;&#31867;&#22411;&#12290;MidMed&#28085;&#30422;&#20102;&#22235;&#20010;&#37096;&#38376;&#65288;&#32819;&#40763;&#21897;&#31185;&#12289;&#30524;&#31185;&#12289;&#30382;&#32932;&#31185;&#21644;&#28040;&#21270;&#31995;&#32479;&#65289;&#65292;&#20849;&#26377;8175&#27573;&#23545;&#35805;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;MidMed&#19978;&#24314;&#31435;&#20102;&#22522;&#32447;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25351;&#23548;&#24335;&#30340;&#21307;&#23398;&#23545;&#35805;&#29983;&#25104;&#26694;&#26550;&#65288;InsMed&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#22810;&#20010;&#35780;&#20272;&#25351;&#26631;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most medical dialogue systems assume that patients have clear goals (medicine querying, surgical operation querying, etc.) before medical consultation. However, in many real scenarios, due to the lack of medical knowledge, it is usually difficult for patients to determine clear goals with all necessary slots. In this paper, we identify this challenge as how to construct medical consultation dialogue systems to help patients clarify their goals. To mitigate this challenge, we propose a novel task and create a human-to-human mixed-type medical consultation dialogue corpus, termed MidMed, covering five dialogue types: task-oriented dialogue for diagnosis, recommendation, knowledge-grounded dialogue, QA, and chitchat. MidMed covers four departments (otorhinolaryngology, ophthalmology, skin, and digestive system), with 8,175 dialogues. Furthermore, we build baselines on MidMed and propose an instruction-guiding medical dialogue generation framework, termed InsMed, to address this task. Expe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#24050;&#26377;&#30340;&#20363;&#23376;&#32451;&#20064;&#29983;&#25104;&#26032;&#30340;&#21306;&#21035;&#24863;&#30693;&#22411;&#22635;&#31354;&#32451;&#20064;&#65292;&#26080;&#38656;&#28145;&#24230;&#26631;&#27880;&#65292;&#29305;&#21035;&#38024;&#23545;&#35821;&#35328;&#23398;&#20064;&#20013;&#30340;&#35821;&#27861;&#32451;&#20064;&#65307;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24182;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#27861;&#35821;&#35821;&#27861;&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#22312;&#36825;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#31454;&#20105;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2306.01584</link><description>&lt;p&gt;
&#20174;&#37096;&#20998;&#26631;&#27880;&#25968;&#25454;&#20013;&#23398;&#20064;&#65306;&#38754;&#21521;&#35821;&#35328;&#23398;&#20064;&#30340;&#21306;&#21035;&#24863;&#30693;&#22411;&#22635;&#31354;&#32451;&#20064;&#33258;&#21160;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Learning from Partially Annotated Data: Example-aware Creation of Gap-filling Exercises for Language Learning. (arXiv:2306.01584v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01584
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#24050;&#26377;&#30340;&#20363;&#23376;&#32451;&#20064;&#29983;&#25104;&#26032;&#30340;&#21306;&#21035;&#24863;&#30693;&#22411;&#22635;&#31354;&#32451;&#20064;&#65292;&#26080;&#38656;&#28145;&#24230;&#26631;&#27880;&#65292;&#29305;&#21035;&#38024;&#23545;&#35821;&#35328;&#23398;&#20064;&#20013;&#30340;&#35821;&#27861;&#32451;&#20064;&#65307;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24182;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#27861;&#35821;&#35821;&#27861;&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#22312;&#36825;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#31454;&#20105;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20570;&#32451;&#20064;&#65288;&#21253;&#25324;&#32451;&#20064;&#27979;&#35797;&#65289;&#26159;&#23398;&#20064;&#30340;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#21019;&#24314;&#36825;&#26679;&#30340;&#32451;&#20064;&#38656;&#35201;&#25945;&#24072;&#20184;&#20986;&#38750;&#24120;&#22823;&#30340;&#21162;&#21147;&#12290;&#22312;&#25968;&#23383;&#21270;&#25945;&#32946;&#24037;&#20855;&#20013;&#33258;&#21160;&#29983;&#25104;&#32451;&#20064;&#20855;&#26377;&#24456;&#22823;&#30340;&#20215;&#20540;&#12290;&#26412;&#25991;&#29305;&#21035;&#20851;&#27880;&#20110;&#26080;&#38656;&#28145;&#24230;&#26631;&#27880;&#26448;&#26009;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#31034;&#20363;&#32451;&#20064;&#33258;&#21160;&#21019;&#24314;&#35821;&#35328;&#23398;&#20064;&#20013;&#30340;&#22635;&#31354;&#32451;&#20064;&#65292;&#23588;&#20854;&#26159;&#35821;&#27861;&#32451;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#19987;&#38376;&#29992;&#20110;Gap-filling exercise generation&#20219;&#21153;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#19968;&#20010;&#27861;&#35821;&#35821;&#27861;&#30340;&#29616;&#23454;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#27604;&#31454;&#20105;&#22522;&#20934;&#22312;&#27861;&#35821;&#35821;&#27861;Gap-filling exercise generation&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#21516;&#26102;&#21482;&#38656;&#23545;&#37096;&#20998;&#25968;&#25454;&#36827;&#34892;&#26631;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since performing exercises (including, e.g., practice tests) forms a crucial component of learning, and creating such exercises requires non-trivial effort from the teacher. There is a great value in automatic exercise generation in digital tools in education. In this paper, we particularly focus on automatic creation of gapfilling exercises for language learning, specifically grammar exercises. Since providing any annotation in this domain requires human expert effort, we aim to avoid it entirely and explore the task of converting existing texts into new gap-filling exercises, purely based on an example exercise, without explicit instruction or detailed annotation of the intended grammar topics. We contribute (i) a novel neural network architecture specifically designed for aforementioned gap-filling exercise generation task, and (ii) a real-world benchmark dataset for French grammar. We show that our model for this French grammar gap-filling exercise generation outperforms a competit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;PassGPT&#36827;&#34892;&#23494;&#30721;&#24314;&#27169;&#21644;&#29983;&#25104;&#65292;&#35813;&#27169;&#22411;&#27604;&#22522;&#20110;GAN&#30340;&#29616;&#26377;&#26041;&#27861;&#26356;&#20934;&#30830;&#65292;&#33021;&#29983;&#25104;&#31526;&#21512;&#20219;&#24847;&#38480;&#21046;&#30340;&#23494;&#30721;&#65292;&#20026;&#25552;&#39640;&#23494;&#30721;&#24378;&#24230;&#20272;&#35745;&#22120;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2306.01545</link><description>&lt;p&gt;
PassGPT: &#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23494;&#30721;&#24314;&#27169;&#21644;&#65288;&#24341;&#23548;&#24335;&#65289;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
PassGPT: Password Modeling and (Guided) Generation with Large Language Models. (arXiv:2306.01545v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01545
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;PassGPT&#36827;&#34892;&#23494;&#30721;&#24314;&#27169;&#21644;&#29983;&#25104;&#65292;&#35813;&#27169;&#22411;&#27604;&#22522;&#20110;GAN&#30340;&#29616;&#26377;&#26041;&#27861;&#26356;&#20934;&#30830;&#65292;&#33021;&#29983;&#25104;&#31526;&#21512;&#20219;&#24847;&#38480;&#21046;&#30340;&#23494;&#30721;&#65292;&#20026;&#25552;&#39640;&#23494;&#30721;&#24378;&#24230;&#20272;&#35745;&#22120;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#25104;&#21151;&#22320;&#27169;&#25311;&#33258;&#28982;&#35821;&#35328;&#65292;&#26080;&#38656;&#26126;&#30830;&#30340;&#30417;&#30563;&#65292;&#20165;&#36890;&#36807;&#22823;&#37327;&#30340;&#25991;&#26412;&#36827;&#34892;&#35757;&#32451;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#22312;&#24314;&#27169;&#23494;&#30721;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;PassGPT&#65292;&#23427;&#26159;&#19968;&#20010;&#22312;&#23494;&#30721;&#27844;&#38706;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;LLM&#65292;&#29992;&#20110;&#29983;&#25104;&#23494;&#30721;&#12290;PassGPT&#36890;&#36807;&#29468;&#27979;&#20004;&#20493;&#20110;&#22522;&#20110;&#29983;&#25104;&#24615;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#23494;&#30721;&#32780;&#32988;&#36807;&#20854;&#23427;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#24341;&#23548;&#24335;&#23494;&#30721;&#29983;&#25104;&#30340;&#27010;&#24565;&#65292;&#21033;&#29992;PassGPT&#30340;&#25277;&#26679;&#36807;&#31243;&#29983;&#25104;&#31526;&#21512;&#20219;&#24847;&#38480;&#21046;&#30340;&#23494;&#30721;&#65292;&#36825;&#22312;&#24403;&#21069;&#22522;&#20110;GAN&#30340;&#31574;&#30053;&#20013;&#26159;&#32570;&#20047;&#30340;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;PassGPT&#23545;&#23494;&#30721;&#23450;&#20041;&#30340;&#29109;&#21644;&#27010;&#29575;&#20998;&#24067;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#22312;&#22686;&#24378;&#29616;&#26377;&#23494;&#30721;&#24378;&#24230;&#20272;&#35745;&#22120;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) successfully model natural language from vast amounts of text without the need for explicit supervision. In this paper, we investigate the efficacy of LLMs in modeling passwords. We present PassGPT, a LLM trained on password leaks for password generation. PassGPT outperforms existing methods based on generative adversarial networks (GAN) by guessing twice as many previously unseen passwords. Furthermore, we introduce the concept of guided password generation, where we leverage PassGPT sampling procedure to generate passwords matching arbitrary constraints, a feat lacking in current GAN-based strategies. Lastly, we conduct an in-depth analysis of the entropy and probability distribution that PassGPT defines over passwords and discuss their use in enhancing existing password strength estimators.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#26159;&#21542;&#21487;&#20197;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25277;&#35937;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#33258;&#21160;&#29983;&#25104;&#27861;&#24459;&#26696;&#20363;&#21028;&#20915;&#30340;&#25688;&#35201;&#65292;&#24182;&#22312;&#21360;&#24230;&#30340;&#27861;&#24237;&#26696;&#20363;&#21028;&#20915;&#20013;&#36827;&#34892;&#20102;&#30456;&#20851;&#23454;&#39564;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2306.01248</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#25277;&#35937;&#27169;&#22411;&#21644;LLMs&#22312;&#27861;&#24459;&#26696;&#20363;&#21028;&#20915;&#25688;&#35201;&#20013;&#30340;&#24212;&#29992;&#20934;&#22791;&#24773;&#20917;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Ready are Pre-trained Abstractive Models and LLMs for Legal Case Judgement Summarization?. (arXiv:2306.01248v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01248
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#26159;&#21542;&#21487;&#20197;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25277;&#35937;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#33258;&#21160;&#29983;&#25104;&#27861;&#24459;&#26696;&#20363;&#21028;&#20915;&#30340;&#25688;&#35201;&#65292;&#24182;&#22312;&#21360;&#24230;&#30340;&#27861;&#24237;&#26696;&#20363;&#21028;&#20915;&#20013;&#36827;&#34892;&#20102;&#30456;&#20851;&#23454;&#39564;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#25688;&#35201;&#27861;&#24459;&#26696;&#20363;&#21028;&#20915;&#19968;&#30452;&#26159;&#37319;&#29992;&#25277;&#21462;&#24335;&#25688;&#35201;&#26041;&#27861;&#23581;&#35797;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36817;&#24180;&#26469;&#65292;&#20855;&#26377;&#29983;&#25104;&#26356;&#33258;&#28982;&#21644;&#36830;&#36143;&#25688;&#35201;&#33021;&#21147;&#30340;&#25277;&#35937;&#25688;&#35201;&#27169;&#22411;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#29616;&#22312;&#24050;&#32463;&#26377;&#20102;&#19987;&#38376;&#29992;&#20110;&#27861;&#24459;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#25277;&#35937;&#25688;&#35201;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;&#22914;ChatGPT&#36825;&#26679;&#30340;&#36890;&#29992;&#39046;&#22495;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#65292;&#24182;&#20855;&#26377;&#25991;&#26412;&#25688;&#35201;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#20540;&#24471;&#38382;&#30340;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#24050;&#20934;&#22791;&#22909;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#26696;&#20363;&#21028;&#20915;&#30340;&#25277;&#35937;&#25688;&#35201;&#12290;&#20026;&#20102;&#25506;&#35752;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#39046;&#22495;&#29305;&#23450;&#30340;&#25277;&#35937;&#24615;&#25688;&#35201;&#27169;&#22411;&#21644;&#36890;&#29992;&#39046;&#22495;&#30340;LLMs&#24212;&#29992;&#20110;&#21360;&#24230;&#27861;&#24237;&#26696;&#20363;&#21028;&#20915;&#20013;&#65292;&#24182;&#26816;&#26597;&#25152;&#29983;&#25104;&#25688;&#35201;&#30340;&#36136;&#37327;&#12290;&#38500;&#20102;&#25688;&#35201;&#36136;&#37327;&#30340;&#26631;&#20934;&#24230;&#37327;&#65292;&#25105;&#20204;&#36824;&#26816;&#26597;&#20102;&#29983;&#25104;&#30340;&#25688;&#35201;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#19981;&#19968;&#33268;&#24615;&#21644;&#34394;&#26500;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic summarization of legal case judgements has traditionally been attempted by using extractive summarization methods. However, in recent years, abstractive summarization models are gaining popularity since they can generate more natural and coherent summaries. Legal domain-specific pre-trained abstractive summarization models are now available. Moreover, general-domain pre-trained Large Language Models (LLMs), such as ChatGPT, are known to generate high-quality text and have the capacity for text summarization. Hence it is natural to ask if these models are ready for off-the-shelf application to automatically generate abstractive summaries for case judgements. To explore this question, we apply several state-of-the-art domain-specific abstractive summarization models and general-domain LLMs on Indian court case judgements, and check the quality of the generated summaries. In addition to standard metrics for summary quality, we check for inconsistencies and hallucinations in the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;GOTHate&#25968;&#25454;&#38598;&#65292;&#24182;&#35814;&#32454;&#27604;&#36739;&#20102;&#35813;&#25968;&#25454;&#38598;&#19982;&#29616;&#26377;&#30340;&#20167;&#24680;&#35328;&#35770;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#20102;&#27169;&#22411;&#22914;&#20309;&#25429;&#25417;&#20013;&#31435;&#30340;&#24694;&#24847;&#20869;&#23481;&#20013;&#30340;&#20167;&#24680;&#20449;&#21495;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;GOTHate&#24456;&#38590;&#22312;&#32431;&#25991;&#26412;&#29615;&#22659;&#19979;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#20171;&#32461;&#20102;&#22914;&#20309;&#36890;&#36807;&#28155;&#21152;&#20869;&#29983;&#20449;&#21495;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01105</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#20167;&#24680;&#35328;&#35770;&#22522;&#20934;&#65306;&#20174;&#25968;&#25454;&#31579;&#36873;&#21040;&#31995;&#32479;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Revisiting Hate Speech Benchmarks: From Data Curation to System Deployment. (arXiv:2306.01105v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01105
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;GOTHate&#25968;&#25454;&#38598;&#65292;&#24182;&#35814;&#32454;&#27604;&#36739;&#20102;&#35813;&#25968;&#25454;&#38598;&#19982;&#29616;&#26377;&#30340;&#20167;&#24680;&#35328;&#35770;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#20102;&#27169;&#22411;&#22914;&#20309;&#25429;&#25417;&#20013;&#31435;&#30340;&#24694;&#24847;&#20869;&#23481;&#20013;&#30340;&#20167;&#24680;&#20449;&#21495;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;GOTHate&#24456;&#38590;&#22312;&#32431;&#25991;&#26412;&#29615;&#22659;&#19979;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#20171;&#32461;&#20102;&#22914;&#20309;&#36890;&#36807;&#28155;&#21152;&#20869;&#29983;&#20449;&#21495;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#19978;&#20805;&#26021;&#30528;&#20805;&#28385;&#20167;&#24680;&#30340;&#20869;&#23481;&#65292;&#20854;&#20013;&#24456;&#22810;&#32463;&#24120;&#20276;&#38543;&#30528;&#35821;&#35328;&#21644;&#20027;&#39064;&#30340;&#22810;&#26679;&#24615;&#12290;&#29992;&#20110;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#27809;&#26377;&#32771;&#34385;&#21040;&#36825;&#31181;&#32972;&#31163;&#65292;&#22240;&#20026;&#23427;&#20204;&#20027;&#35201;&#26159;&#20351;&#29992;&#20167;&#24680;&#35789;&#27719;&#32534;&#21046;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#20013;&#31435;&#30340;&#24694;&#24847;&#20869;&#23481;&#20013;&#25429;&#25417;&#20167;&#24680;&#20449;&#21495;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#35774;&#35745;&#27169;&#25311;&#20167;&#24680;&#29616;&#23454;&#19990;&#30028;&#21464;&#24322;&#24615;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GOTHate&#65292;&#36825;&#26159;&#19968;&#20010;&#32508;&#21512;&#20102;&#19981;&#21516;&#35821;&#35328;&#21644;&#20027;&#39064;&#30340;&#22823;&#35268;&#27169;&#20195;&#30721;&#28151;&#21512;&#20247;&#21253;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20174;Twitter&#20013;&#26816;&#27979;&#20167;&#24680;&#35328;&#35770;&#65292;&#21253;&#25324;&#32422;51k&#20010;&#24086;&#23376;&#12290;&#25105;&#20204;&#35814;&#32454;&#27604;&#36739;&#20102;GOTHate&#21644;&#29616;&#26377;&#20167;&#24680;&#35328;&#35770;&#25968;&#25454;&#38598;&#65292;&#31361;&#20986;&#20102;&#23427;&#30340;&#26032;&#39062;&#24615;&#65292;&#24182;&#20351;&#29992;10&#20010;&#26368;&#36817;&#30340;&#22522;&#20934;&#32447;&#23545;&#20854;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#35777;&#21644;&#22522;&#20934;&#27979;&#35797;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#32431;&#25991;&#26412;&#29615;&#22659;&#19979;&#65292;&#24456;&#38590;&#23545;GOTHate&#36827;&#34892;&#20998;&#31867;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#28155;&#21152;&#20869;&#29983;&#20449;&#21495;&#20197;&#22686;&#24378;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social media is awash with hateful content, much of which is often veiled with linguistic and topical diversity. The benchmark datasets used for hate speech detection do not account for such divagation as they are predominantly compiled using hate lexicons. However, capturing hate signals becomes challenging in neutrally-seeded malicious content. Thus, designing models and datasets that mimic the real-world variability of hate warrants further investigation.  To this end, we present GOTHate, a large-scale code-mixed crowdsourced dataset of around 51k posts for hate speech detection from Twitter. GOTHate is neutrally seeded, encompassing different languages and topics. We conduct detailed comparisons of GOTHate with the existing hate speech datasets, highlighting its novelty. We benchmark it with 10 recent baselines. Our extensive empirical and benchmarking experiments suggest that GOTHate is hard to classify in a text-only setup. Thus, we investigate how adding endogenous signals enhan
&lt;/p&gt;</description></item><item><title>&#24247;&#31185;&#36842;&#20122;&#26694;&#26550;&#26159;&#19968;&#20010;&#25903;&#25345;&#21508;&#31181;&#27010;&#29575;&#29702;&#35770;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#25216;&#26415;&#30340;&#38480;&#21046;&#30340;&#24182;&#34892;&#30340;&#31070;&#32463;&#31526;&#21495;&#32467;&#26500;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#38598;&#21512;&#27963;&#21160;&#26816;&#27979;&#12289;&#23454;&#20307;&#38142;&#25509;&#21644;&#25512;&#33616;&#20219;&#21153;&#65292;&#25552;&#39640;&#20102;&#26368;&#26032;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.00480</link><description>&lt;p&gt;
&#19982;&#24247;&#31185;&#36842;&#20122;&#24182;&#34892;&#30340;&#31070;&#32463;&#31526;&#21495;&#19968;&#20307;&#21270;
&lt;/p&gt;
&lt;p&gt;
Parallel Neurosymbolic Integration with Concordia. (arXiv:2306.00480v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00480
&lt;/p&gt;
&lt;p&gt;
&#24247;&#31185;&#36842;&#20122;&#26694;&#26550;&#26159;&#19968;&#20010;&#25903;&#25345;&#21508;&#31181;&#27010;&#29575;&#29702;&#35770;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#25216;&#26415;&#30340;&#38480;&#21046;&#30340;&#24182;&#34892;&#30340;&#31070;&#32463;&#31526;&#21495;&#32467;&#26500;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#38598;&#21512;&#27963;&#21160;&#26816;&#27979;&#12289;&#23454;&#20307;&#38142;&#25509;&#21644;&#25512;&#33616;&#20219;&#21153;&#65292;&#25552;&#39640;&#20102;&#26368;&#26032;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24182;&#34892;&#30340;&#31070;&#32463;&#31526;&#21495;&#32467;&#26500;&#36890;&#36807;&#23558;&#36923;&#36753;&#29702;&#35770;&#30340;&#30693;&#35782;&#25552;&#21462;&#21040;&#28145;&#24230;&#27169;&#22411;&#20013;&#65292;&#22312;NLP&#20013;&#24471;&#21040;&#26377;&#25928;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25216;&#26415;&#20173;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#21253;&#25324;&#25903;&#25345;&#21463;&#38480;&#24418;&#24335;&#30340;&#36923;&#36753;&#29702;&#35770;&#65292;&#24182;&#20381;&#36182;&#20110;&#36923;&#36753;&#21644;&#28145;&#24230;&#32593;&#32476;&#20043;&#38388;&#30340;&#29420;&#31435;&#24615;&#20551;&#35774;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#24247;&#31185;&#36842;&#20122;&#26694;&#26550;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#25216;&#26415;&#30340;&#38480;&#21046;&#12290;&#24247;&#31185;&#36842;&#20122;&#26694;&#26550;&#26082;&#19981;&#20851;&#27880;&#28145;&#24230;&#32593;&#32476;&#65292;&#20063;&#19981;&#20851;&#27880;&#36923;&#36753;&#29702;&#35770;&#65292;&#25903;&#25345;&#21508;&#31181;&#27010;&#29575;&#29702;&#35770;&#12290;&#26694;&#26550;&#21487;&#20197;&#25903;&#25345;&#20004;&#20010;&#32452;&#20214;&#30340;&#30417;&#30563;&#35757;&#32451;&#21644;&#31070;&#32463;&#32452;&#20214;&#30340;&#26080;&#30417;&#30563;&#35757;&#32451;&#12290;&#24247;&#31185;&#36842;&#20122;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;NLP&#21644;&#25968;&#25454;&#20998;&#31867;&#20197;&#22806;&#30340;&#20219;&#21153;&#65292;&#25552;&#39640;&#20102;&#38598;&#21512;&#27963;&#21160;&#26816;&#27979;&#12289;&#23454;&#20307;&#38142;&#25509;&#21644;&#25512;&#33616;&#20219;&#21153;&#30340;&#26368;&#26032;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parallel neurosymbolic architectures have been applied effectively in NLP by distilling knowledge from a logic theory into a deep model.However, prior art faces several limitations including supporting restricted forms of logic theories and relying on the assumption of independence between the logic and the deep network. We present Concordia, a framework overcoming the limitations of prior art. Concordia is agnostic both to the deep network and the logic theory offering support for a wide range of probabilistic theories. Our framework can support supervised training of both components and unsupervised training of the neural component. Concordia has been successfully applied to tasks beyond NLP and data classification, improving the accuracy of state-of-the-art on collective activity detection, entity linking and recommendation tasks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#27169;&#22411;&#36827;&#34892;&#34701;&#21512;&#65292;&#33021;&#29983;&#25104;&#20855;&#26377;&#36830;&#36143;&#24615;&#30340;&#22270;&#20687;&#36755;&#20986;&#65292;&#21516;&#26102;&#20063;&#33021;&#36827;&#34892;&#22270;&#20687;&#26816;&#32034;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#12290;</title><link>http://arxiv.org/abs/2305.17216</link><description>&lt;p&gt;
&#29992;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22270;&#29255;
&lt;/p&gt;
&lt;p&gt;
Generating Images with Multimodal Language Models. (arXiv:2305.17216v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17216
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#27169;&#22411;&#36827;&#34892;&#34701;&#21512;&#65292;&#33021;&#29983;&#25104;&#20855;&#26377;&#36830;&#36143;&#24615;&#30340;&#22270;&#20687;&#36755;&#20986;&#65292;&#21516;&#26102;&#20063;&#33021;&#36827;&#34892;&#22270;&#20687;&#26816;&#32034;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#20165;&#21253;&#21547;&#25991;&#26412;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#27169;&#22411;&#36827;&#34892;&#34701;&#21512;&#65292;&#36890;&#36807;&#26144;&#23556;&#23427;&#20204;&#30340;&#23884;&#20837;&#31354;&#38388;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#23637;&#31034;&#20102;&#24191;&#27867;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#65306;&#22270;&#20687;&#26816;&#32034;&#12289;&#26032;&#39062;&#22270;&#20687;&#29983;&#25104;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#12290;&#36825;&#26159;&#31532;&#19968;&#31181;&#33021;&#22815;&#22312;&#20219;&#24847;&#20132;&#38169;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#20043;&#38388;&#36827;&#34892;&#26465;&#20214;&#35843;&#33410;&#65292;&#29983;&#25104;&#36830;&#36143;&#22270;&#20687;&#65288;&#21644;&#25991;&#26412;&#65289;&#36755;&#20986;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#22312;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26144;&#23556;&#32593;&#32476;&#65292;&#23558;LLM&#22522;&#20110;&#29616;&#25104;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#23558;&#25991;&#26412;&#30340;&#38544;&#34255;&#34920;&#31034;&#36716;&#25442;&#20026;&#35270;&#35273;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#65292;&#21033;&#29992;LLM&#24378;&#22823;&#30340;&#25991;&#26412;&#34920;&#31034;&#26469;&#29983;&#25104;&#35270;&#35273;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38271;&#19988;&#22797;&#26434;&#35821;&#35328;&#30340;&#20219;&#21153;&#19978;&#20248;&#20110;&#22522;&#20934;&#29983;&#25104;&#27169;&#22411;&#12290;&#38500;&#20102;&#26032;&#39062;&#22270;&#20687;&#29983;&#25104;&#20043;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#33021;&#22815;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#26816;&#32034;&#22270;&#20687;&#65292;&#24182;&#36827;&#34892;&#22810;&#27169;&#24577;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;LIVE&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35270;&#35273;&#22686;&#24378;&#23398;&#20064;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#30340;&#24819;&#35937;&#65292;&#24182;&#38024;&#23545;&#27599;&#20010;&#21477;&#23376;&#36827;&#34892;&#21160;&#24577;&#21512;&#25104;&#65292;&#22823;&#37327;&#23454;&#39564;&#27979;&#35797;&#34920;&#26126;&#23427;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#29983;&#25104;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.16944</link><description>&lt;p&gt;
&#23398;&#20064;&#24819;&#35937;&#65306;&#35270;&#35273;&#22686;&#24378;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Learning to Imagine: Visually-Augmented Natural Language Generation. (arXiv:2305.16944v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;LIVE&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35270;&#35273;&#22686;&#24378;&#23398;&#20064;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#30340;&#24819;&#35937;&#65292;&#24182;&#38024;&#23545;&#27599;&#20010;&#21477;&#23376;&#36827;&#34892;&#21160;&#24577;&#21512;&#25104;&#65292;&#22823;&#37327;&#23454;&#39564;&#27979;&#35797;&#34920;&#26126;&#23427;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#24448;&#24448;&#20250;&#24819;&#35937;&#30456;&#20851;&#22330;&#26223;&#26469;&#24110;&#21161;&#20889;&#20316;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#35270;&#35273;&#20449;&#24687;&#20197;&#19982;&#20154;&#31867;&#30456;&#21516;&#30340;&#26041;&#24335;&#36827;&#34892;&#21019;&#20316;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26041;&#27861;LIVE&#65292;&#20351;&#24471;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#33021;&#22815;&#23398;&#20064;&#36890;&#36807;&#35270;&#35273;&#22686;&#24378;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#30340;&#24819;&#35937; &#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22522;&#20110;&#25991;&#26412;&#24819;&#35937;&#22330;&#26223;&#65306;&#25105;&#20204;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#20381;&#25454;&#36755;&#20837;&#25991;&#26412;&#21512;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;CLIP&#30830;&#23450;&#25991;&#26412;&#26159;&#21542;&#33021;&#20197;&#21518;&#39564;&#26041;&#24335;&#21796;&#36215;&#24819;&#35937;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#24819;&#35937;&#26159;&#21160;&#24577;&#30340;&#65292;&#25105;&#20204;&#20250;&#38024;&#23545;&#27599;&#20010;&#21477;&#23376;&#36827;&#34892;&#21512;&#25104;&#65292;&#32780;&#19981;&#26159;&#38024;&#23545;&#25972;&#20010;&#27573;&#33853;&#29983;&#25104;&#19968;&#24352;&#22270;&#20687;&#12290;&#22312;&#25216;&#26415;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21363;&#25554;&#21363;&#29992;&#34701;&#21512;&#23618;&#65292;&#20197;&#33719;&#21462;&#27599;&#20010;&#25991;&#26412;&#30340;&#35270;&#35273;&#22686;&#24378;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#35270;&#35273;-&#25991;&#26412;&#34701;&#21512;&#23618;&#19982;Transformer-based&#26550;&#26500;&#20860;&#23481;&#12290;&#25105;&#20204;&#20351;&#29992;BART&#21644;T5&#36827;&#34892;&#20102;&#22235;&#20010;&#29983;&#25104;&#20219;&#21153;&#30340;&#22823;&#37327;&#23454;&#39564;&#27979;&#35797;&#65292;&#33258;&#21160;&#32467;&#26524;&#21644;&#20154;&#31867;&#35780;&#20272;&#37117;&#34920;&#26126;LIVE&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
People often imagine relevant scenes to aid in the writing process. In this work, we aim to utilize visual information for composition in the same manner as humans. We propose a method, LIVE, that makes pre-trained language models (PLMs) Learn to Imagine for Visuallyaugmented natural language gEneration. First, we imagine the scene based on the text: we use a diffusion model to synthesize high-quality images conditioned on the input texts. Second, we use CLIP to determine whether the text can evoke the imagination in a posterior way. Finally, our imagination is dynamic, and we conduct synthesis for each sentence rather than generate only one image for an entire paragraph. Technically, we propose a novel plug-and-play fusion layer to obtain visually-augmented representations for each text. Our vision-text fusion layer is compatible with Transformerbased architecture. We have conducted extensive experiments on four generation tasks using BART and T5, and the automatic results and human e
&lt;/p&gt;</description></item><item><title>GDA&#26159;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#20851;&#31995;&#25991;&#26412;&#22686;&#24378;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#37319;&#29992;&#20004;&#20010;&#20114;&#34917;&#27169;&#22359;&#65292;&#20445;&#25345;&#35821;&#20041;&#21644;&#35821;&#27861;&#32467;&#26500;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#20351;&#29992;&#23454;&#20307;&#25552;&#31034;&#25193;&#23637;&#19978;&#19979;&#25991;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;GDA&#36229;&#36234;&#20102;&#29616;&#26377;&#22686;&#24378;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16663</link><description>&lt;p&gt;
GDA: &#29992;&#20110;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#30340;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
GDA: Generative Data Augmentation Techniques for Relation Extraction Tasks. (arXiv:2305.16663v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16663
&lt;/p&gt;
&lt;p&gt;
GDA&#26159;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#20851;&#31995;&#25991;&#26412;&#22686;&#24378;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#37319;&#29992;&#20004;&#20010;&#20114;&#34917;&#27169;&#22359;&#65292;&#20445;&#25345;&#35821;&#20041;&#21644;&#35821;&#27861;&#32467;&#26500;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#20351;&#29992;&#23454;&#20307;&#25552;&#31034;&#25193;&#23637;&#19978;&#19979;&#25991;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;GDA&#36229;&#36234;&#20102;&#29616;&#26377;&#22686;&#24378;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32473;&#23450;&#36275;&#22815;&#30340;&#35757;&#32451;&#26631;&#27880;&#26102;&#65292;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#21487;&#20197;&#22312;&#21477;&#23376;&#20013;&#25552;&#21462;&#20986;&#20004;&#20010;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#22312;&#23454;&#36341;&#20013;&#33719;&#24471;&#36825;&#31181;&#26631;&#27880;&#26159;&#36153;&#21147;&#30340;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#37319;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#22312;&#38480;&#21046;&#30340;&#26631;&#27880;&#33539;&#22260;&#20043;&#22806;&#29983;&#25104;&#20266;&#26631;&#27880;&#21477;&#23376;&#12290;&#24403;&#37319;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;&#22686;&#24378;&#26102;&#65292;&#36825;&#20123;&#25216;&#26415;&#26080;&#27861;&#20445;&#25345;&#21407;&#22987;&#21477;&#23376;&#30340;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#24182;&#19988;&#22312;&#20351;&#29992;seq2seq&#27169;&#22411;&#34920;&#36798;&#20851;&#31995;&#26102;&#26080;&#27861;&#20445;&#25345;&#21477;&#23376;&#30340;&#35821;&#27861;&#32467;&#26500;&#65292; resulting in less diverse augmentations &#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#20851;&#31995;&#25991;&#26412;&#30340;&#22686;&#24378;&#25216;&#26415;&#65292;&#31216;&#20026;GDA&#65292;&#23427;&#20351;&#29992;&#20004;&#20010;&#20114;&#34917;&#27169;&#22359;&#26469;&#20445;&#25345;&#35821;&#20041;&#19968;&#33268;&#24615;&#21644;&#35821;&#27861;&#32467;&#26500;&#12290;&#25105;&#20204;&#37319;&#29992;&#29983;&#25104;&#24335;&#20844;&#24335;&#65292;&#24182;&#35774;&#35745;&#19968;&#20010;&#22810;&#20219;&#21153;&#35299;&#20915;&#26041;&#26696;&#20197;&#23454;&#29616;&#21327;&#21516;&#25928;&#24212;&#12290;&#27492;&#22806;&#65292;GDA&#37319;&#29992;&#23454;&#20307;&#25552;&#31034;&#20316;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#23558;&#23454;&#20307;&#30340;&#19978;&#19979;&#25991;&#25193;&#23637;&#21040;&#21477;&#23376;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GDA&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;NYT10&#21644;SemEval2010 Task 8&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation extraction (RE) tasks show promising performance in extracting relations from two entities mentioned in sentences, given sufficient annotations available during training. Such annotations would be labor-intensive to obtain in practice. Existing work adopts data augmentation techniques to generate pseudo-annotated sentences beyond limited annotations. These techniques neither preserve the semantic consistency of the original sentences when rule-based augmentations are adopted, nor preserve the syntax structure of sentences when expressing relations using seq2seq models, resulting in less diverse augmentations. In this work, we propose a dedicated augmentation technique for relational texts, named GDA, which uses two complementary modules to preserve both semantic consistency and syntax structures. We adopt a generative formulation and design a multi-tasking solution to achieve synergies. Furthermore, GDA adopts entity hints as the prior knowledge of the generative model to augm
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20154;&#21592;&#30740;&#31350;&#20102;&#22312;&#25968;&#25454;&#21463;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#32553;&#25918;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#26368;&#20248;&#24615;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#32771;&#34385;&#21040;&#37325;&#22797;&#20196;&#29260;&#21644;&#36807;&#37327;&#21442;&#25968;&#30340;&#20215;&#20540;&#36882;&#20943;&#12290;</title><link>http://arxiv.org/abs/2305.16264</link><description>&lt;p&gt;
&#32553;&#25918;&#25968;&#25454;&#21463;&#38480;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Scaling Data-Constrained Language Models. (arXiv:2305.16264v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16264
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#30740;&#31350;&#20102;&#22312;&#25968;&#25454;&#21463;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#32553;&#25918;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#26368;&#20248;&#24615;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#32771;&#34385;&#21040;&#37325;&#22797;&#20196;&#29260;&#21644;&#36807;&#37327;&#21442;&#25968;&#30340;&#20215;&#20540;&#36882;&#20943;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#25193;&#23637;&#35821;&#35328;&#27169;&#22411;&#30340;&#36235;&#21183;&#28041;&#21450;&#22686;&#21152;&#21442;&#25968;&#35745;&#25968;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#12290;&#25512;&#26029;&#36825;&#20010;&#36235;&#21183;&#34920;&#26126;&#65292;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#21487;&#33021;&#24456;&#24555;&#23601;&#20250;&#21463;&#21040;&#20114;&#32852;&#32593;&#19978;&#21487;&#29992;&#25991;&#26412;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;&#20986;&#20110;&#27492;&#38480;&#21046;&#30340;&#21160;&#26426;&#65292;&#25105;&#20204;&#30740;&#31350;&#22312;&#25968;&#25454;&#21463;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#32553;&#25918;&#35821;&#35328;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36816;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#65292;&#21464;&#21270;&#25968;&#25454;&#37325;&#22797;&#31243;&#24230;&#21644;&#35745;&#31639;&#39044;&#31639;&#65292;&#33539;&#22260;&#36798;&#21040;&#20102;9000&#20159;&#20010;&#35757;&#32451;&#20196;&#29260;&#21644;9&#20159;&#21442;&#25968;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#26377;&#38480;&#30340;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#39640;&#36798;4&#27425;&#37325;&#22797;&#25968;&#25454;&#30340;&#35757;&#32451;&#19982;&#20351;&#29992;&#21807;&#19968;&#25968;&#25454;&#30456;&#27604;&#23545;&#25439;&#22833;&#30340;&#36129;&#29486;&#24494;&#19981;&#36275;&#36947;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#26356;&#22810;&#30340;&#37325;&#22797;&#25968;&#25454;&#65292;&#28155;&#21152;&#35745;&#31639;&#30340;&#20215;&#20540;&#26368;&#32456;&#20250;&#34928;&#20943;&#20026;&#38646;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#32463;&#39564;&#35777;&#20102;&#19968;&#20010;&#35745;&#31639;&#26368;&#20248;&#24615;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#32771;&#34385;&#21040;&#37325;&#22797;&#20196;&#29260;&#21644;&#36807;&#37327;&#21442;&#25968;&#30340;&#20215;&#20540;&#36882;&#20943;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#32531;&#35299;&#25968;&#25454;&#31232;&#32570;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current trend of scaling language models involves increasing both parameter count and training dataset size. Extrapolating this trend suggests that training dataset size may soon be limited by the amount of text data available on the internet. Motivated by this limit, we investigate scaling language models in data-constrained regimes. Specifically, we run a large set of experiments varying the extent of data repetition and compute budget, ranging up to 900 billion training tokens and 9 billion parameter models. We find that with constrained data for a fixed compute budget, training with up to 4 epochs of repeated data yields negligible changes to loss compared to having unique data. However, with more repetition, the value of adding compute eventually decays to zero. We propose and empirically validate a scaling law for compute optimality that accounts for the decreasing value of repeated tokens and excess parameters. Finally, we experiment with approaches mitigating data scarcity,
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;22&#31181;&#21360;&#24230;&#23466;&#27861;&#20013;&#21015;&#20986;&#30340;&#25152;&#26377;21&#31181;&#26412;&#22303;&#25991;&#23383;&#21644;&#32599;&#39532;&#23383;&#27597;&#30340;&#20844;&#24320;&#35821;&#35328;&#37492;&#21035;&#65288;LID&#65289;&#25968;&#25454;&#21644;&#27169;&#22411;&#12290;IndicLID&#26159;&#19978;&#36848;&#35821;&#35328;&#30340;&#26412;&#22303;&#21644;&#32599;&#39532;&#21270;&#33050;&#26412;&#30340;&#35821;&#35328;&#37492;&#21035;&#22120;&#65292;&#36824;&#25552;&#20986;&#20102;&#35299;&#20915;&#32599;&#39532;&#21270;&#25991;&#26412;&#30340;LID&#38382;&#39064;&#30340;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.15814</link><description>&lt;p&gt;
Bhasha-Abhijnaanam&#65306;22&#31181;&#21360;&#24230;&#25991;&#23383;&#21644;&#32599;&#39532;&#25340;&#38899;&#35821;&#35328;&#37492;&#21035;&#12290; (arXiv&#65306;2305.15814v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
Bhasha-Abhijnaanam: Native-script and romanized Language Identification for 22 Indic languages. (arXiv:2305.15814v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15814
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;22&#31181;&#21360;&#24230;&#23466;&#27861;&#20013;&#21015;&#20986;&#30340;&#25152;&#26377;21&#31181;&#26412;&#22303;&#25991;&#23383;&#21644;&#32599;&#39532;&#23383;&#27597;&#30340;&#20844;&#24320;&#35821;&#35328;&#37492;&#21035;&#65288;LID&#65289;&#25968;&#25454;&#21644;&#27169;&#22411;&#12290;IndicLID&#26159;&#19978;&#36848;&#35821;&#35328;&#30340;&#26412;&#22303;&#21644;&#32599;&#39532;&#21270;&#33050;&#26412;&#30340;&#35821;&#35328;&#37492;&#21035;&#22120;&#65292;&#36824;&#25552;&#20986;&#20102;&#35299;&#20915;&#32599;&#39532;&#21270;&#25991;&#26412;&#30340;LID&#38382;&#39064;&#30340;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;22&#20010;&#21360;&#24230;&#23466;&#27861;&#20013;&#21015;&#20986;&#30340;&#25152;&#26377;21&#31181;&#26412;&#22303;&#25991;&#23383;&#21644;&#32599;&#39532;&#23383;&#27597;&#30340;&#20844;&#24320;&#35821;&#35328;&#37492;&#21035;&#65288;LID&#65289;&#25968;&#25454;&#21644;&#27169;&#22411;&#12290;&#19982;&#29616;&#26377;&#30340;LID&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;Bhasha-Abhijnaanam&#22312;&#26412;&#22303;&#25991;&#23383;&#25991;&#26412;&#30340;&#35821;&#35328;&#28085;&#30422;&#33539;&#22260;&#26041;&#38754;&#26356;&#20026;&#24191;&#27867;&#65292;&#24182;&#20855;&#26377;&#31454;&#20105;&#21147;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;IndicLID&#26159;&#19978;&#36848;&#35821;&#35328;&#30340;&#26412;&#22303;&#21644;&#32599;&#39532;&#21270;&#33050;&#26412;&#30340;&#35821;&#35328;&#37492;&#21035;&#22120;&#12290;&#23545;&#20110;&#32599;&#39532;&#21270;&#25991;&#26412;&#30340;LID&#65292;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#21644;&#24403;&#35821;&#35328;&#30456;&#20284;&#26102;&#65292;&#20302;LID&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#31616;&#21333;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#22312;&#20219;&#20309;&#35821;&#35328;&#20013;&#65292;&#32599;&#39532;&#21270;&#25991;&#26412;&#30340;&#30740;&#31350;&#37117;&#24456;&#26377;&#38480;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#23545;&#38656;&#35201;&#32599;&#39532;&#21270;&#35821;&#35328;&#37492;&#21035;&#30340;&#20854;&#20182;&#35821;&#35328;&#20063;&#20855;&#26377;&#21442;&#32771;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
We create publicly available language identification (LID) datasets and models in all 22 Indian languages listed in the Indian constitution in both native-script and romanized text. First, we create Bhasha-Abhijnaanam, a language identification test set for native-script as well as romanized text which spans all 22 Indic languages. We also train IndicLID, a language identifier for all the above-mentioned languages in both native and romanized script. For native-script text, it has better language coverage than existing LIDs and is competitive or better than other LIDs. IndicLID is the first LID for romanized text in Indian languages. Two major challenges for romanized text LID are the lack of training data and low-LID performance when languages are similar. We provide simple and effective solutions to these problems. In general, there has been limited work on romanized text in any language, and our findings are relevant to other languages that need romanized language identification. Ou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#32508;&#36848;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;NLP&#20013;&#30340;&#21508;&#31181;&#24212;&#29992;&#65292;&#27604;&#36739;&#20102;&#20854;&#19982;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#20248;&#21155;&#65292;&#25351;&#20986;&#25193;&#25955;&#27169;&#22411;&#22312;&#24182;&#34892;&#29983;&#25104;&#12289;&#25991;&#26412;&#25554;&#20540;&#12289;&#35789;&#32423;&#21035;&#25511;&#21046;&#31561;&#26041;&#38754;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.14671</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models in NLP: A Survey. (arXiv:2305.14671v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#32508;&#36848;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;NLP&#20013;&#30340;&#21508;&#31181;&#24212;&#29992;&#65292;&#27604;&#36739;&#20102;&#20854;&#19982;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#20248;&#21155;&#65292;&#25351;&#20986;&#25193;&#25955;&#27169;&#22411;&#22312;&#24182;&#34892;&#29983;&#25104;&#12289;&#25991;&#26412;&#25554;&#20540;&#12289;&#35789;&#32423;&#21035;&#25511;&#21046;&#31561;&#26041;&#38754;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35770;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20013;&#30340;&#24212;&#29992;&#12290;&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31867;&#25968;&#23398;&#27169;&#22411;&#65292;&#26088;&#22312;&#25429;&#25417;&#20449;&#24687;&#25110;&#20449;&#21495;&#22312;&#32593;&#32476;&#25110;&#27969;&#24418;&#19978;&#30340;&#25193;&#25955;&#12290;&#22312;NLP&#20013;&#65292;&#25193;&#25955;&#27169;&#22411;&#24050;&#34987;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#65292;&#22914;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#12289;&#24773;&#24863;&#20998;&#26512;&#12289;&#20027;&#39064;&#24314;&#27169;&#21644;&#26426;&#22120;&#32763;&#35793;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;NLP&#20013;&#20351;&#29992;&#30340;&#19981;&#21516;&#25193;&#25955;&#27169;&#22411;&#30340;&#20844;&#24335;&#12289;&#20248;&#21155;&#28857;&#21644;&#24212;&#29992;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#23545;&#25193;&#25955;&#27169;&#22411;&#19982;&#20854;&#20182;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#65292;&#29305;&#21035;&#26159;&#33258;&#22238;&#24402;(AR)&#27169;&#22411;&#65292;&#36824;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;Transformer&#19982;&#25193;&#25955;&#27169;&#22411;&#32467;&#21512;&#20197;&#23454;&#29616;&#22810;&#26679;&#21270;&#30340;&#26550;&#26500;&#12290;&#30456;&#27604;AR&#27169;&#22411;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#24182;&#34892;&#29983;&#25104;&#12289;&#25991;&#26412;&#25554;&#20540;&#12289;&#35789;&#32423;&#21035;&#25511;&#21046;(&#22914;&#21477;&#27861;&#32467;&#26500;)&#31561;&#26041;&#38754;&#20855;&#26377;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
This survey paper provides a comprehensive review of the use of diffusion models in natural language processing (NLP). Diffusion models are a class of mathematical models that aim to capture the diffusion of information or signals across a network or manifold. In NLP, diffusion models have been used in a variety of applications, such as natural language generation, sentiment analysis, topic modeling, and machine translation. This paper discusses the different formulations of diffusion models used in NLP, their strengths and limitations, and their applications. We also perform a thorough comparison between diffusion models and alternative generative models, specifically highlighting the autoregressive (AR) models, while also examining how diverse architectures incorporate the Transformer in conjunction with diffusion models. Compared to AR models, diffusion models have significant advantages for parallel generation, text interpolation, token-level controls such as syntactic structures a
&lt;/p&gt;</description></item><item><title>WebIE&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#12289;&#23454;&#20307;&#20851;&#32852;&#38381;&#21512;&#30340;IE&#25968;&#25454;&#38598;&#65292;&#20197;&#26356;&#22909;&#22320;&#21453;&#26144;&#32593;&#32476;&#25968;&#25454;&#65292;&#24182;&#35780;&#20272;&#20102;&#29983;&#25104;&#24335;IE&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.14293</link><description>&lt;p&gt;
WebIE&#65306;&#22522;&#20110;&#32593;&#32476;&#30340;&#20449;&#24687;&#25552;&#21462;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
WebIE: Faithful and Robust Information Extraction on the Web. (arXiv:2305.14293v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14293
&lt;/p&gt;
&lt;p&gt;
WebIE&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#12289;&#23454;&#20307;&#20851;&#32852;&#38381;&#21512;&#30340;IE&#25968;&#25454;&#38598;&#65292;&#20197;&#26356;&#22909;&#22320;&#21453;&#26144;&#32593;&#32476;&#25968;&#25454;&#65292;&#24182;&#35780;&#20272;&#20102;&#29983;&#25104;&#24335;IE&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#21407;&#22987;&#25991;&#26412;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#21644;&#26377;&#23454;&#38469;&#24847;&#20041;&#30340;&#19977;&#20803;&#32452;&#26159;&#20449;&#24687;&#25552;&#21462;&#65288;IE&#65289;&#20013;&#30340;&#19968;&#39033;&#22522;&#26412;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;IE&#25968;&#25454;&#38598;&#36890;&#24120;&#26159;&#20174;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;&#20013;&#25910;&#38598;&#30340;&#65292;&#20351;&#29992;&#36229;&#38142;&#25509;&#23558;&#23454;&#20307;&#19982;Wikidata&#30693;&#35782;&#24211;&#38142;&#25509;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;&#20165;&#22312;&#32500;&#22522;&#30334;&#31185;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#24212;&#29992;&#20110;&#32593;&#39029;&#39046;&#22495;&#26102;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#22240;&#20026;&#36825;&#20123;&#39046;&#22495;&#32463;&#24120;&#21253;&#21547;&#22024;&#26434;&#25110;&#27809;&#26377;&#20219;&#20309;&#23454;&#38469;&#20449;&#24687;&#30340;&#25991;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;WebIE&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#12289;&#23454;&#20307;&#20851;&#32852;&#38381;&#21512;&#30340;IE&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20174;&#33521;&#35821;Common Crawl&#35821;&#26009;&#24211;&#20013;&#33258;&#21160;&#25910;&#38598;&#30340;160&#19975;&#20010;&#21477;&#23376;&#12290;WebIE&#36824;&#21253;&#25324;&#36127;&#20363;&#65292;&#21363;&#27809;&#26377;&#20107;&#23454;&#19977;&#20803;&#32452;&#30340;&#21477;&#23376;&#65292;&#20197;&#26356;&#22909;&#22320;&#21453;&#26144;&#32593;&#32476;&#25968;&#25454;&#12290;&#25105;&#20204;&#36890;&#36807;&#20247;&#21253;&#23545;WebIE&#36827;&#34892;&#20102;&#22823;&#32422;21000&#20010;&#19977;&#20803;&#32452;&#30340;&#27880;&#37322;&#65292;&#24182;&#20171;&#32461;&#20102;mWebIE&#65292;&#36825;&#26159;&#27880;&#37322;&#38598;&#22312;&#27861;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#12289;&#33889;&#33796;&#29273;&#35821;&#21644;&#21360;&#22320;&#35821;&#20013;&#30340;&#32763;&#35793;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#29983;&#25104;&#24335;IE&#27169;&#22411;&#22312;&#22495;&#20869;&#12289;&#22495;&#22806;&#21644;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#22312;WebIE&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33391;&#22909;
&lt;/p&gt;
&lt;p&gt;
Extracting structured and grounded fact triples from raw text is a fundamental task in Information Extraction (IE). Existing IE datasets are typically collected from Wikipedia articles, using hyperlinks to link entities to the Wikidata knowledge base. However, models trained only on Wikipedia have limitations when applied to web domains, which often contain noisy text or text that does not have any factual information. We present WebIE, the first large-scale, entity-linked closed IE dataset consisting of 1.6M sentences automatically collected from the English Common Crawl corpus. WebIE also includes negative examples, i.e. sentences without fact triples, to better reflect the data on the web. We annotate ~21K triples from WebIE through crowdsourcing and introduce mWebIE, a translation of the annotated set in four other languages: French, Spanish, Portuguese, and Hindi. We evaluate the in-domain, out-of-domain, and zero-shot cross-lingual performance of generative IE models and find mod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21517;&#20026;LLM-Pruner&#65292;&#37319;&#29992;&#32467;&#26500;&#20462;&#21098;&#30340;&#26041;&#24335;&#22312;&#20445;&#30041;&#22823;&#22810;&#25968;&#21151;&#33021;&#30340;&#21516;&#26102;&#65292;&#21387;&#32553;LLM&#30340;&#32467;&#26500;&#65292;&#20197;&#20943;&#23569;LLM&#22312;&#37096;&#32626;&#12289;&#25512;&#29702;&#21644;&#35757;&#32451;&#38454;&#27573;&#20013;&#30340;&#22823;&#23567;&#21644;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.11627</link><description>&lt;p&gt;
LLM-Pruner: &#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#26500;&#20462;&#21098;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
LLM-Pruner: On the Structural Pruning of Large Language Models. (arXiv:2305.11627v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21517;&#20026;LLM-Pruner&#65292;&#37319;&#29992;&#32467;&#26500;&#20462;&#21098;&#30340;&#26041;&#24335;&#22312;&#20445;&#30041;&#22823;&#22810;&#25968;&#21151;&#33021;&#30340;&#21516;&#26102;&#65292;&#21387;&#32553;LLM&#30340;&#32467;&#26500;&#65292;&#20197;&#20943;&#23569;LLM&#22312;&#37096;&#32626;&#12289;&#25512;&#29702;&#21644;&#35757;&#32451;&#38454;&#27573;&#20013;&#30340;&#22823;&#23567;&#21644;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20986;&#22312;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#20196;&#20154;&#24778;&#35766;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33021;&#21147;&#36890;&#24120;&#20276;&#38543;&#30528;&#24040;&#22823;&#30340;&#27169;&#22411;&#22823;&#23567;&#65292;&#36825;&#22312;&#37096;&#32626;&#12289;&#25512;&#29702;&#21644;&#35757;&#32451;&#38454;&#27573;&#37117;&#23384;&#22312;&#26174;&#33879;&#25361;&#25112;&#12290;&#25105;&#20204;&#20197;&#20219;&#21153;&#26080;&#20851;&#30340;&#26041;&#24335;&#25506;&#32034;&#20102;LLM&#30340;&#21387;&#32553;&#26041;&#27861;&#65292;&#26088;&#22312;&#20445;&#30041;&#21407;&#22987;LLM&#30340;&#22810;&#20219;&#21153;&#35299;&#20915;&#21644;&#35821;&#35328;&#29983;&#25104;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#32467;&#26500;&#20462;&#21098;&#65292;&#22312;&#26799;&#24230;&#20449;&#24687;&#30340;&#25903;&#25345;&#19979;&#36873;&#25321;&#24615;&#22320;&#31227;&#38500;&#38750;&#20851;&#38190;&#30340;&#32806;&#21512;&#32467;&#26500;&#65292;&#26368;&#22823;&#38480;&#24230;&#22320;&#20445;&#30041;&#20102;&#22823;&#22810;&#25968;LLM&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown remarkable capabilities in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges in both the deployment, inference, and training stages. With LLM being a general-purpose task solver, we explore its compression in a task-agnostic manner, which aims to preserve the multi-task solving and language generation ability of the original LLM. One challenge to achieving this is the enormous size of the training corpus of LLM, which makes both data transfer and model post-training over-burdensome. Thus, we tackle the compression of LLMs within the bound of two constraints: being task-agnostic and minimizing the reliance on the original training dataset. Our method, named LLM-Pruner, adopts structural pruning that selectively removes non-critical coupled structures based on gradient information, maximally preserving the majority of the LLM's functionalit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#19968;&#31181;&#20132;&#20114;&#26426;&#21046;&#65292;&#20801;&#35768;&#29992;&#25143;&#30452;&#25509;&#32534;&#36753;&#19968;&#27493;&#27493;&#35299;&#37322;&#38169;&#35823;SQL&#20197;&#20462;&#22797;SQL&#38169;&#35823;&#65292;&#23454;&#39564;&#35777;&#26126;&#26041;&#27861;&#25552;&#39640;&#20102;31.6&#65285;&#30340;&#25191;&#34892;&#20934;&#30830;&#24615;&#12290;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#24110;&#21161;&#29992;&#25143;&#20197;&#26356;&#23569;&#30340;&#26102;&#38388;&#21644;&#26356;&#39640;&#30340;&#20449;&#24515;&#35299;&#20915;&#20102;&#26356;&#22810;&#30340;SQL&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.07372</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#32534;&#36753;&#30340;&#36880;&#27493;&#35299;&#37322;&#23454;&#29616;&#20132;&#20114;&#24335;&#25991;&#26412;&#36716;SQL
&lt;/p&gt;
&lt;p&gt;
Interactive Text-to-SQL Generation via Editable Step-by-Step Explanations. (arXiv:2305.07372v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07372
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#19968;&#31181;&#20132;&#20114;&#26426;&#21046;&#65292;&#20801;&#35768;&#29992;&#25143;&#30452;&#25509;&#32534;&#36753;&#19968;&#27493;&#27493;&#35299;&#37322;&#38169;&#35823;SQL&#20197;&#20462;&#22797;SQL&#38169;&#35823;&#65292;&#23454;&#39564;&#35777;&#26126;&#26041;&#27861;&#25552;&#39640;&#20102;31.6&#65285;&#30340;&#25191;&#34892;&#20934;&#30830;&#24615;&#12290;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#24110;&#21161;&#29992;&#25143;&#20197;&#26356;&#23569;&#30340;&#26102;&#38388;&#21644;&#26356;&#39640;&#30340;&#20449;&#24515;&#35299;&#20915;&#20102;&#26356;&#22810;&#30340;SQL&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25968;&#25454;&#24211;&#22312;&#22823;&#25968;&#25454;&#26102;&#20195;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#28982;&#32780;&#65292;&#38750;&#19987;&#23478;&#24456;&#38590;&#23436;&#20840;&#37322;&#25918;&#20851;&#31995;&#25968;&#25454;&#24211;&#30340;&#20998;&#26512;&#33021;&#21147;&#65292;&#22240;&#20026;&#20182;&#20204;&#19981;&#29087;&#24713;SQL&#31561;&#25968;&#25454;&#24211;&#35821;&#35328;&#12290;&#35768;&#22810;&#25216;&#26415;&#24050;&#34987;&#25552;&#20986;&#33258;&#28982;&#35821;&#35328;&#33258;&#21160;&#29983;&#25104;SQL&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#20197;&#19979;&#20004;&#20010;&#38382;&#39064;&#65306;&#65288;1&#65289;&#23545;&#20110;&#22797;&#26434;&#26597;&#35810;&#23427;&#20204;&#20173;&#20250;&#29359;&#24456;&#22810;&#38169;&#35823;&#65292;&#65288;2&#65289;&#23427;&#20204;&#19981;&#25552;&#20379;&#19968;&#31181;&#28789;&#27963;&#30340;&#26041;&#24335;&#65292;&#35753;&#38750;&#19987;&#23478;&#29992;&#25143;&#39564;&#35777;&#21644;&#25913;&#36827;&#19981;&#27491;&#30830;&#30340;&#26597;&#35810;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20132;&#20114;&#26426;&#21046;&#65292;&#20801;&#35768;&#29992;&#25143;&#30452;&#25509;&#32534;&#36753;&#19968;&#27493;&#27493;&#35299;&#37322;&#38169;&#35823;SQL&#20197;&#20462;&#22797;SQL&#38169;&#35823;&#12290;&#22312;Spider&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33267;&#23569;&#27604;&#19977;&#31181;&#26368;&#20808;&#36827;&#26041;&#27861;&#22312;&#25191;&#34892;&#20934;&#30830;&#24615;&#26041;&#38754;&#25552;&#39640;&#20102;31.6&#65285;&#12290;24&#21517;&#21442;&#19982;&#32773;&#30340;&#29992;&#25143;&#30740;&#31350;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24110;&#21161;&#29992;&#25143;&#20197;&#26356;&#23569;&#30340;&#26102;&#38388;&#21644;&#26356;&#39640;&#30340;&#20449;&#24515;&#35299;&#20915;&#20102;&#26356;&#22810;&#30340;SQL&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relational databases play an important role in this Big Data era. However, it is challenging for non-experts to fully unleash the analytical power of relational databases, since they are not familiar with database languages such as SQL. Many techniques have been proposed to automatically generate SQL from natural language, but they suffer from two issues: (1) they still make many mistakes, particularly for complex queries, and (2) they do not provide a flexible way for non-expert users to validate and refine the incorrect queries. To address these issues, we introduce a new interaction mechanism that allows users directly edit a step-by-step explanation of an incorrect SQL to fix SQL errors. Experiments on the Spider benchmark show that our approach outperforms three SOTA approaches by at least 31.6% in terms of execution accuracy. A user study with 24 participants further shows that our approach helped users solve significantly more SQL tasks with less time and higher confidence, demo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#35843;&#26597;&#20102;&#20855;&#36523;&#21270;&#31574;&#30053;&#23545;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#27604;&#21947;&#24615;&#35821;&#35328;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#26356;&#22823;&#30340;&#27169;&#22411;&#22312;&#22788;&#29702;&#34892;&#20026;&#26356;&#20855;&#20307;&#21270;&#30340;&#38544;&#21947;&#24615;&#21477;&#23376;&#26102;&#34920;&#29616;&#26356;&#20339;&#12290;</title><link>http://arxiv.org/abs/2305.03445</link><description>&lt;p&gt;
LMs&#22266;&#23432;&#38453;&#22320;&#65306;&#25506;&#31350;&#20855;&#36523;&#21270;&#23545;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#27604;&#21947;&#24615;&#35821;&#35328;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
LMs stand their Ground: Investigating the Effect of Embodiment in Figurative Language Interpretation by Language Models. (arXiv:2305.03445v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#35843;&#26597;&#20102;&#20855;&#36523;&#21270;&#31574;&#30053;&#23545;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#27604;&#21947;&#24615;&#35821;&#35328;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#26356;&#22823;&#30340;&#27169;&#22411;&#22312;&#22788;&#29702;&#34892;&#20026;&#26356;&#20855;&#20307;&#21270;&#30340;&#38544;&#21947;&#24615;&#21477;&#23376;&#26102;&#34920;&#29616;&#26356;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27604;&#21947;&#35821;&#35328;&#26159;&#35821;&#35328;&#27169;&#22411;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#20854;&#35299;&#37322;&#22522;&#20110;&#21333;&#35789;&#30340;&#20351;&#29992;&#26041;&#24335;&#20559;&#31163;&#20102;&#23427;&#20204;&#30340;&#24120;&#35268;&#39034;&#24207;&#21644;&#21547;&#20041;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#21487;&#20197;&#36731;&#26494;&#29702;&#35299;&#21644;&#35808;&#37322;&#38544;&#21947;&#12289;&#27604;&#21947;&#25110;&#20064;&#35821;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#20174;&#20855;&#36523;&#38544;&#21947;&#20013;&#25512;&#23548;&#20986;&#26469;&#12290;&#35821;&#35328;&#26159;&#20855;&#36523;&#21270;&#30340;&#20195;&#29702;&#65292;&#22914;&#26524;&#38544;&#21947;&#26159;&#20256;&#32479;&#30340;&#21644;&#35789;&#27719;&#21270;&#30340;&#65292;&#37027;&#20040;&#19968;&#20010;&#27809;&#26377;&#36523;&#20307;&#30340;&#31995;&#32479;&#23601;&#26356;&#23481;&#26131;&#29702;&#35299;&#20855;&#36523;&#27010;&#24565;&#12290;&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#27604;&#21947;&#24615;&#21477;&#23376;&#30340;&#34892;&#21160;&#26356;&#20855;&#20307;&#21270;&#26102;&#65292;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#37322;&#38544;&#21947;&#21477;&#23376;&#26102;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#25490;&#38500;&#20102;&#19982;&#20854;&#20182;&#29305;&#24449;&#65288;&#20363;&#22914;&#21333;&#35789;&#38271;&#24230;&#25110;&#20855;&#20307;&#24615;&#65289;&#30340;&#22810;&#37325;&#20849;&#32447;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Figurative language is a challenge for language models since its interpretation is based on the use of words in a way that deviates from their conventional order and meaning. Yet, humans can easily understand and interpret metaphors, similes or idioms as they can be derived from embodied metaphors. Language is a proxy for embodiment and if a metaphor is conventional and lexicalised, it becomes easier for a system without a body to make sense of embodied concepts. Yet, the intricate relation between embodiment and features such as concreteness or age of acquisition has not been studied in the context of figurative language interpretation concerning language models. Hence, the presented study shows how larger language models perform better at interpreting metaphoric sentences when the action of the metaphorical sentence is more embodied. The analysis rules out multicollinearity with other features (e.g. word length or concreteness) and provides initial evidence that larger language model
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35821;&#35328;&#25552;&#31034;&#19979;&#30340;&#22870;&#21169;&#26102;&#38388;&#20559;&#22909;&#65292;&#24182;&#21457;&#29616;GPT&#22312;&#20855;&#26377;&#36739;&#24369;&#26410;&#26469;&#26102;&#24577;&#30340;&#35821;&#35328;&#19979;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#32784;&#24515;&#65292;&#36825;&#19982;&#20351;&#29992;&#35813;&#35821;&#35328;&#30340;&#20154;&#31867;&#30340;&#20559;&#22909;&#30456;&#20284;&#12290;</title><link>http://arxiv.org/abs/2305.02531</link><description>&lt;p&gt;
&#35821;&#35328;&#12289;&#26102;&#38388;&#20559;&#22909;&#21644;&#28040;&#36153;&#34892;&#20026;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Language, Time Preferences, and Consumer Behavior: Evidence from Large Language Models. (arXiv:2305.02531v1 [econ.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35821;&#35328;&#25552;&#31034;&#19979;&#30340;&#22870;&#21169;&#26102;&#38388;&#20559;&#22909;&#65292;&#24182;&#21457;&#29616;GPT&#22312;&#20855;&#26377;&#36739;&#24369;&#26410;&#26469;&#26102;&#24577;&#30340;&#35821;&#35328;&#19979;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#32784;&#24515;&#65292;&#36825;&#19982;&#20351;&#29992;&#35813;&#35821;&#35328;&#30340;&#20154;&#31867;&#30340;&#20559;&#22909;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#23545;&#25105;&#20204;&#23545;&#26102;&#38388;&#21644;&#22870;&#21169;&#30340;&#24863;&#30693;&#26377;&#24456;&#22823;&#30340;&#24433;&#21709;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#24403;&#20197;&#19981;&#21516;&#30340;&#35821;&#35328;&#35810;&#38382;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#23427;&#20204;&#26159;&#21542;&#26174;&#31034;&#20986;&#19981;&#21516;&#30340;&#22870;&#21169;&#26102;&#38388;&#20559;&#22909;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#36873;&#25321;&#26159;&#21542;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#36873;&#25321;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;GPT-3.5&#65288;&#20197;&#19979;&#31616;&#31216;GPT&#65289;&#22312;&#22810;&#31181;&#35821;&#35328;&#25552;&#31034;&#19979;&#30340;&#21709;&#24212;&#65292;&#25506;&#32034;&#20102;&#36739;&#23567;&#12289;&#36739;&#26089;&#30340;&#22870;&#21169;&#21644;&#36739;&#22823;&#12289;&#36739;&#26202;&#30340;&#22870;&#21169;&#20043;&#38388;&#30340;&#20559;&#22909;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#24403;&#20197;&#35821;&#20041;&#21547;&#20041;&#36739;&#24369;&#30340;&#26410;&#26469;&#26102;&#24577;&#21442;&#32771;&#65288;FTR&#65289;&#65292;&#22914;&#24503;&#35821;&#21644;&#27721;&#35821;&#65292;&#20026;&#25552;&#31034;&#35821;&#26102;&#65292;GPT&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#32784;&#24515;&#65292;&#30456;&#27604;&#33521;&#35821;&#21644;&#27861;&#35821;&#31561;&#20855;&#26377;&#24378;&#22823;FTR&#30340;&#35821;&#35328;&#12290;&#36825;&#20123;&#21457;&#29616;&#19982;&#29616;&#26377;&#25991;&#29486;&#19968;&#33268;&#65292;&#24182;&#34920;&#26126;&#20102;GPT&#30340;&#36873;&#25321;&#19982;&#36825;&#20123;&#35821;&#35328;&#30340;&#20351;&#29992;&#32773;&#30340;&#20559;&#22909;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#28982;&#32780;&#65292;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#36739;&#26089;&#25110;&#36739;&#26202;&#22870;&#21169;&#30340;&#20559;&#22909;&#24182;&#27809;&#26377;&#38543;&#30528;&#22870;&#21169;&#24046;&#24322;&#31995;&#32479;&#22320;&#25913;&#21464;&#65292;&#36825;&#34920;&#26126;&#20102;&#19968;&#31181;&#35789;&#20856;&#24207;&#20248;&#20808;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language has a strong influence on our perceptions of time and rewards. This raises the question of whether large language models, when asked in different languages, show different preferences for rewards over time and if their choices are similar to those of humans. In this study, we analyze the responses of GPT-3.5 (hereafter referred to as GPT) to prompts in multiple languages, exploring preferences between smaller, sooner rewards and larger, later rewards. Our results show that GPT displays greater patience when prompted in languages with weak future tense references (FTR), such as German and Mandarin, compared to languages with strong FTR, like English and French. These findings are consistent with existing literature and suggest a correlation between GPT's choices and the preferences of speakers of these languages. However, further analysis reveals that the preference for earlier or later rewards does not systematically change with reward gaps, indicating a lexicographic preferen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;GPTutor&#30340;ChatGPT&#21160;&#21147;&#32534;&#31243;&#24037;&#20855;&#65292;&#23427;&#26159;&#19968;&#20010;&#20351;&#29992;ChatGPT API&#30340;Visual Studio Code&#25193;&#23637;&#65292;&#36890;&#36807;&#35774;&#35745;&#25552;&#31034;&#35789;&#65292;&#21487;&#20197;&#23545;&#25152;&#36873;&#20195;&#30721;&#36827;&#34892;&#31934;&#31616;&#12289;&#20934;&#30830;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2305.01863</link><description>&lt;p&gt;
GPTutor: &#19968;&#31181;&#30001;ChatGPT&#39537;&#21160;&#30340;&#32534;&#31243;&#24037;&#20855;&#65292;&#29992;&#20110;&#31243;&#24207;&#20195;&#30721;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
GPTutor: a ChatGPT-powered programming tool for code explanation. (arXiv:2305.01863v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;GPTutor&#30340;ChatGPT&#21160;&#21147;&#32534;&#31243;&#24037;&#20855;&#65292;&#23427;&#26159;&#19968;&#20010;&#20351;&#29992;ChatGPT API&#30340;Visual Studio Code&#25193;&#23637;&#65292;&#36890;&#36807;&#35774;&#35745;&#25552;&#31034;&#35789;&#65292;&#21487;&#20197;&#23545;&#25152;&#36873;&#20195;&#30721;&#36827;&#34892;&#31934;&#31616;&#12289;&#20934;&#30830;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#26032;&#30340;&#32534;&#31243;&#25216;&#33021;&#38656;&#35201;&#20010;&#24615;&#21270;&#25351;&#23548;&#12290;&#38543;&#30528;ChatGPT API&#31561;&#39640;&#32423;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#29616;&#22312;&#26377;&#21487;&#33021;&#21019;&#24314;&#19968;&#20010;&#26041;&#20415;&#30340;&#12289;&#20010;&#24615;&#21270;&#30340;AI&#32534;&#31243;&#25945;&#32946;&#36741;&#23548;&#31995;&#32479;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;GPTutor&#30340;ChatGPT&#21160;&#21147;&#32534;&#31243;&#24037;&#20855;&#65292;&#23427;&#26159;&#19968;&#20010;&#20351;&#29992;ChatGPT API&#30340;Visual Studio Code&#25193;&#23637;&#65292;&#29992;&#20110;&#25552;&#20379;&#32534;&#31243;&#20195;&#30721;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning new programming skills requires tailored guidance. With the emergence of advanced Natural Language Generation models like the ChatGPT API, there is now a possibility of creating a convenient and personalized tutoring system with AI for computer science education. This paper presents GPTutor, a ChatGPT-powered programming tool, which is a Visual Studio Code extension using the ChatGPT API to provide programming code explanations. By integrating Visual Studio Code API, GPTutor can comprehensively analyze the provided code by referencing the relevant source codes. As a result, GPTutor can use designed prompts to explain the selected code with a pop-up message. GPTutor is now published at the Visual Studio Code Extension Marketplace, and its source code is openly accessible on GitHub. Preliminary evaluation indicates that GPTutor delivers the most concise and accurate explanations compared to vanilla ChatGPT and GitHub Copilot. Moreover, the feedback from students and teachers ind
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#22810;&#35821;&#31181;&#29992;&#25143;&#26597;&#35810;&#25340;&#20889;&#26816;&#26597;&#22120;&#65292;&#23427;&#38750;&#24120;&#24555;&#36895;&#12289;&#21487;&#25193;&#23637;&#65292;&#24182;&#26681;&#25454;&#29305;&#23450;&#20135;&#21697;&#30340;&#38656;&#27714;&#35843;&#25972;&#20854;&#35789;&#27719;&#34920;&#21644;&#25340;&#20889;&#36755;&#20986;&#65292;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2305.01082</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#22810;&#35821;&#31181;&#29992;&#25143;&#26597;&#35810;&#25340;&#20889;&#26816;&#26597;&#22120;
&lt;/p&gt;
&lt;p&gt;
Contextual Multilingual Spellchecker for User Queries. (arXiv:2305.01082v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#22810;&#35821;&#31181;&#29992;&#25143;&#26597;&#35810;&#25340;&#20889;&#26816;&#26597;&#22120;&#65292;&#23427;&#38750;&#24120;&#24555;&#36895;&#12289;&#21487;&#25193;&#23637;&#65292;&#24182;&#26681;&#25454;&#29305;&#23450;&#20135;&#21697;&#30340;&#38656;&#27714;&#35843;&#25972;&#20854;&#35789;&#27719;&#34920;&#21644;&#25340;&#20889;&#36755;&#20986;&#65292;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25340;&#20889;&#26816;&#26597;&#26159;&#26368;&#22522;&#26412;&#21644;&#24191;&#27867;&#20351;&#29992;&#30340;&#25628;&#32034;&#21151;&#33021;&#20043;&#19968;&#12290;&#32416;&#27491;&#25340;&#20889;&#38169;&#35823;&#30340;&#29992;&#25143;&#26597;&#35810;&#19981;&#20165;&#22686;&#24378;&#20102;&#29992;&#25143;&#20307;&#39564;&#65292;&#32780;&#19988;&#29992;&#25143;&#20063;&#26399;&#26395;&#33021;&#22815;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24191;&#27867;&#21487;&#29992;&#30340;&#25340;&#20889;&#26816;&#26597;&#35299;&#20915;&#26041;&#26696;&#35201;&#20040;&#27604;&#26368;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#31934;&#24230;&#20302;&#65292;&#35201;&#20040;&#36895;&#24230;&#22826;&#24930;&#65292;&#26080;&#27861;&#29992;&#20110;&#24310;&#36831;&#26159;&#20851;&#38190;&#35201;&#27714;&#30340;&#25628;&#32034;&#29992;&#20363;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#26368;&#26032;&#30340;&#21019;&#26032;&#26550;&#26500;&#38598;&#20013;&#22312;&#33521;&#35821;&#19978;&#65292;&#24182;&#19988;&#27809;&#26377;&#20197;&#22810;&#35821;&#35328;&#26041;&#24335;&#36827;&#34892;&#22521;&#35757;&#65292;&#24182;&#19988;&#26159;&#38024;&#23545;&#36739;&#38271;&#25991;&#26412;&#30340;&#25340;&#20889;&#32416;&#27491;&#36827;&#34892;&#22521;&#35757;&#65292;&#36825;&#26159;&#19982;&#23545;&#29992;&#25143;&#26597;&#35810;&#30340;&#25340;&#20889;&#32416;&#27491;&#19981;&#21516;&#30340;&#33539;&#24335;&#65292;&#20854;&#20013;&#19978;&#19979;&#25991;&#24456;&#23569;(&#22823;&#22810;&#25968;&#26597;&#35810;&#21482;&#26377;1-2&#20010;&#21333;&#35789;)&#12290;&#26368;&#21518;&#65292;&#30001;&#20110;&#22823;&#22810;&#25968;&#20225;&#19994;&#26377;&#29420;&#29305;&#30340;&#35789;&#27719;&#65292;&#20363;&#22914;&#20135;&#21697;&#21517;&#31216;&#65292;&#29616;&#25104;&#30340;&#25340;&#20889;&#35299;&#20915;&#26041;&#26696;&#26080;&#27861;&#28385;&#36275;&#29992;&#25143;&#30340;&#38656;&#27714;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#25340;&#20889;&#26816;&#26597;&#22120;&#65292;&#23427;&#38750;&#24120;&#24555;&#36895;&#21644;&#21487;&#25193;&#23637;&#65292;&#24182;&#26681;&#25454;&#29305;&#23450;&#20135;&#21697;&#30340;&#38656;&#27714;&#35843;&#25972;&#20854;&#35789;&#27719;&#34920;&#21644;&#25340;&#20889;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spellchecking is one of the most fundamental and widely used search features. Correcting incorrectly spelled user queries not only enhances the user experience but is expected by the user. However, most widely available spellchecking solutions are either lower accuracy than state-of-the-art solutions or too slow to be used for search use cases where latency is a key requirement. Furthermore, most innovative recent architectures focus on English and are not trained in a multilingual fashion and are trained for spell correction in longer text, which is a different paradigm from spell correction for user queries, where context is sparse (most queries are 1-2 words long). Finally, since most enterprises have unique vocabularies such as product names, off-the-shelf spelling solutions fall short of users' needs. In this work, we build a multilingual spellchecker that is extremely fast and scalable and that adapts its vocabulary and hence speller output based on a specific product's needs. Fu
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#24037;&#20855;&#23398;&#20064;&#32467;&#21512;&#20102;&#19987;&#29992;&#24037;&#20855;&#21644;&#22522;&#30784;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#38382;&#39064;&#35299;&#20915;&#30340;&#22686;&#24378;&#31934;&#24230;&#12289;&#25928;&#29575;&#21644;&#33258;&#21160;&#21270;&#12290;&#26412;&#25991;&#23545;&#24037;&#20855;&#23398;&#20064;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#28085;&#30422;&#20004;&#31181;&#31867;&#22411;&#23398;&#20064;&#30340;&#36890;&#29992;&#24037;&#20855;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#29420;&#29305;&#25361;&#25112;&#12289;&#26426;&#20250;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2304.08354</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#24037;&#20855;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Tool Learning with Foundation Models. (arXiv:2304.08354v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08354
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#24037;&#20855;&#23398;&#20064;&#32467;&#21512;&#20102;&#19987;&#29992;&#24037;&#20855;&#21644;&#22522;&#30784;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#38382;&#39064;&#35299;&#20915;&#30340;&#22686;&#24378;&#31934;&#24230;&#12289;&#25928;&#29575;&#21644;&#33258;&#21160;&#21270;&#12290;&#26412;&#25991;&#23545;&#24037;&#20855;&#23398;&#20064;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#28085;&#30422;&#20004;&#31181;&#31867;&#22411;&#23398;&#20064;&#30340;&#36890;&#29992;&#24037;&#20855;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#29420;&#29305;&#25361;&#25112;&#12289;&#26426;&#20250;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#25317;&#26377;&#38750;&#20961;&#30340;&#21019;&#36896;&#21644;&#21033;&#29992;&#24037;&#20855;&#30340;&#33021;&#21147;&#65292;&#20351;&#24471;&#20182;&#20204;&#33021;&#22815;&#20811;&#26381;&#29289;&#29702;&#38480;&#21046;&#24182;&#25506;&#32034;&#26032;&#30340;&#39046;&#22495;&#12290;&#38543;&#30528;&#22522;&#30784;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;AI&#31995;&#32479;&#26377;&#26395;&#20687;&#20154;&#31867;&#19968;&#26679;&#29087;&#32451;&#22320;&#20351;&#29992;&#24037;&#20855;&#12290;&#36825;&#31181;&#33539;&#24335;&#21363;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#24037;&#20855;&#23398;&#20064;&#65292;&#32467;&#21512;&#20102;&#19987;&#29992;&#24037;&#20855;&#21644;&#22522;&#30784;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#38382;&#39064;&#35299;&#20915;&#30340;&#22686;&#24378;&#31934;&#24230;&#12289;&#25928;&#29575;&#21644;&#33258;&#21160;&#21270;&#12290;&#23613;&#31649;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#35813;&#39046;&#22495;&#20173;&#32570;&#20047;&#23545;&#20851;&#38190;&#25361;&#25112;&#12289;&#26426;&#20250;&#21644;&#26410;&#26469;&#21457;&#23637;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#23545;&#24037;&#20855;&#23398;&#20064;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#12290;&#39318;&#20808;&#20171;&#32461;&#20102;&#24037;&#20855;&#23398;&#20064;&#30340;&#32972;&#26223;&#65292;&#21253;&#25324;&#20854;&#35748;&#30693;&#36215;&#28304;&#12289;&#22522;&#30784;&#27169;&#22411;&#30340;&#33539;&#24335;&#36716;&#25442;&#21644;&#24037;&#20855;&#21644;&#27169;&#22411;&#30340;&#20114;&#34917;&#20316;&#29992;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#29616;&#26377;&#30340;&#24037;&#20855;&#23398;&#20064;&#30740;&#31350;&#65292;&#21253;&#25324;&#22522;&#20110;&#24037;&#20855;&#21644;&#38754;&#21521;&#24037;&#20855;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#28085;&#30422;&#20004;&#31181;&#31867;&#22411;&#23398;&#20064;&#30340;&#36890;&#29992;&#24037;&#20855;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#29420;&#29305;&#25361;&#25112;&#12289;&#26426;&#20250;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;&#25105;&#20204;&#39044;&#35745;&#36825;&#31181;&#31995;&#32479;&#30340;&#25506;&#32034;&#23558;&#20026;&#26410;&#26469;&#24320;&#21457;&#20855;&#26377;&#22797;&#26434;&#24037;&#20855;&#23398;&#20064;&#33021;&#21147;&#30340;AI&#31995;&#32479;&#25552;&#20379;&#19968;&#20010;&#36339;&#26495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans possess an extraordinary ability to create and utilize tools, allowing them to overcome physical limitations and explore new frontiers. With the advent of foundation models, AI systems have the potential to be equally adept in tool use as humans. This paradigm, i.e., tool learning with foundation models, combines the strengths of specialized tools and foundation models to achieve enhanced accuracy, efficiency, and automation in problem-solving. Despite its immense potential, there is still a lack of a comprehensive understanding of key challenges, opportunities, and future endeavors in this field. To this end, we present a systematic investigation of tool learning in this paper. We first introduce the background of tool learning, including its cognitive origins, the paradigm shift of foundation models, and the complementary roles of tools and models. Then we recapitulate existing tool learning research into tool-augmented and tool-oriented learning. We formulate a general tool l
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;LLaMA&#29616;&#26377;&#30340;&#35789;&#27719;&#34920;&#65292;&#22686;&#21152;&#20102;20,000&#20010;&#20013;&#25991;&#26631;&#35760;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#32534;&#30721;&#25928;&#29575;&#21644;&#23545;&#27721;&#35821;&#35821;&#20041;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#22312;&#20013;&#25991;&#25968;&#25454;&#19978;&#36827;&#34892;&#20108;&#27425;&#39044;&#35757;&#32451;&#21644;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#65292;&#20197;&#25913;&#21892;LLaMA&#23545;&#20013;&#25991;&#30340;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.08177</link><description>&lt;p&gt;
&#20013;&#25991;LLaMA&#21644;Alpaca&#30340;&#39640;&#25928;&#26377;&#25928;&#30340;&#25991;&#26412;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca. (arXiv:2304.08177v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08177
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;LLaMA&#29616;&#26377;&#30340;&#35789;&#27719;&#34920;&#65292;&#22686;&#21152;&#20102;20,000&#20010;&#20013;&#25991;&#26631;&#35760;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#32534;&#30721;&#25928;&#29575;&#21644;&#23545;&#27721;&#35821;&#35821;&#20041;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#22312;&#20013;&#25991;&#25968;&#25454;&#19978;&#36827;&#34892;&#20108;&#27425;&#39044;&#35757;&#32451;&#21644;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#65292;&#20197;&#25913;&#21892;LLaMA&#23545;&#20013;&#25991;&#30340;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#65292;&#24182;&#26174;&#31034;&#20986;&#26397;&#30528;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#26377;&#24076;&#26395;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#21644;&#37096;&#32626;LLM&#30340;&#39640;&#25104;&#26412;&#23545;&#36879;&#26126;&#12289;&#21487;&#35775;&#38382;&#30340;&#23398;&#26415;&#30740;&#31350;&#26500;&#25104;&#20102;&#37325;&#22823;&#38556;&#30861;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;LLaMA&#29616;&#26377;&#30340;&#35789;&#27719;&#34920;&#65292;&#22686;&#21152;&#20102;20,000&#20010;&#20013;&#25991;&#26631;&#35760;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#32534;&#30721;&#25928;&#29575;&#21644;&#23545;&#27721;&#35821;&#35821;&#20041;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#22312;&#20013;&#25991;&#25968;&#25454;&#19978;&#36827;&#34892;&#20108;&#27425;&#39044;&#35757;&#32451;&#21644;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#29983;&#25104;&#20013;&#25991;&#25991;&#26412;&#21450;&#20854;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), such as ChatGPT and GPT-4, have dramatically transformed natural language processing research and shown promising strides towards Artificial General Intelligence (AGI). Nonetheless, the high costs associated with training and deploying LLMs present substantial obstacles to transparent, accessible academic research. While several large language models, such as LLaMA, have been open-sourced by the community, these predominantly focus on English corpora, limiting their usefulness for other languages. In this paper, we propose a method to augment LLaMA with capabilities for understanding and generating Chinese text and its ability to follow instructions. We achieve this by extending LLaMA's existing vocabulary with an additional 20,000 Chinese tokens, thereby improving its encoding efficiency and semantic understanding of Chinese. We further incorporate secondary pre-training using Chinese data and fine-tune the model with Chinese instruction datasets, signifi
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;OpenAGI&#24179;&#21488;&#36890;&#36807;&#25972;&#21512;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#24418;&#24335;&#65292;&#23454;&#29616;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2304.04370</link><description>&lt;p&gt;
OpenAGI&#65306;&#24403;LLM&#36935;&#21040;&#39046;&#22495;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
OpenAGI: When LLM Meets Domain Experts. (arXiv:2304.04370v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04370
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;OpenAGI&#24179;&#21488;&#36890;&#36807;&#25972;&#21512;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#24418;&#24335;&#65292;&#23454;&#29616;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20855;&#26377;&#23558;&#22522;&#26412;&#25216;&#33021;&#32452;&#21512;&#25104;&#22797;&#26434;&#25216;&#33021;&#20197;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#36825;&#31181;&#33021;&#21147;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#21516;&#26679;&#37325;&#35201;&#65292;&#22240;&#27492;&#65292;&#25105;&#20204;&#26029;&#35328;&#65292;&#38500;&#20102;&#24320;&#21457;&#22823;&#22411;&#32508;&#21512;&#26234;&#33021;&#27169;&#22411;&#22806;&#65292;&#23558;&#19981;&#21516;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#24212;&#29992;&#20110;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#33021;&#21147;&#21516;&#26679;&#20851;&#38190;&#65292;&#20197;&#22312;&#20154;&#24037;&#26234;&#33021;&#36890;&#29992;&#26234;&#33021;&#30340;&#36861;&#27714;&#20013;&#20351;&#20854;&#20855;&#22791;&#36825;&#31181;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#35777;&#26126;&#20854;&#20855;&#26377;&#20986;&#33394;&#30340;&#23398;&#20064;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#20351;&#23427;&#20204;&#25104;&#20026;&#36873;&#25321;&#12289;&#32508;&#21512;&#21644;&#25191;&#34892;&#22806;&#37096;&#27169;&#22411;&#20197;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#25511;&#21046;&#22120;&#30340;&#26377;&#21069;&#36884;&#30340;&#36873;&#25321;&#12290;&#22312;&#36825;&#20010;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;OpenAGI&#30340;&#24320;&#28304;AGI&#30740;&#31350;&#24179;&#21488;&#65292;&#19987;&#38376;&#35774;&#35745;&#20026;&#25552;&#20379;&#22797;&#26434;&#30340;&#22810;&#27493;&#39588;&#20219;&#21153;&#65292;&#24182;&#37197;&#26377;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#21508;&#31181;&#21487;&#25193;&#23637;&#27169;&#22411;&#12290;OpenAGI&#23558;&#22797;&#26434;&#20219;&#21153;&#38416;&#37322;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#65292;&#26088;&#22312;&#20419;&#36827;&#39046;&#22495;&#19987;&#23478;&#21644;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human intelligence has the remarkable ability to assemble basic skills into complex ones so as to solve complex tasks. This ability is equally important for Artificial Intelligence (AI), and thus, we assert that in addition to the development of large, comprehensive intelligent models, it is equally crucial to equip such models with the capability to harness various domain-specific expert models for complex task-solving in the pursuit of Artificial General Intelligence (AGI). Recent developments in Large Language Models (LLMs) have demonstrated remarkable learning and reasoning abilities, making them promising as a controller to select, synthesize, and execute external models to solve complex tasks. In this project, we develop OpenAGI, an open-source AGI research platform, specifically designed to offer complex, multi-step tasks and accompanied by task-specific datasets, evaluation metrics, and a diverse range of extensible models. OpenAGI formulates complex tasks as natural language q
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36866;&#24212;&#25552;&#31034;&#21644;&#38646;&#21021;&#22987;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#36731;&#37327;&#32423;&#35821;&#35328;&#27169;&#22411;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#39640;&#25928;&#24494;&#35843;LLaMA&#20026;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#65292;&#20855;&#26377;&#27604;Alpaca&#26356;&#30701;&#30340;&#24494;&#35843;&#26102;&#38388;&#24182;&#20855;&#26377;&#36817;&#20284;&#30340;&#21709;&#24212;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.16199</link><description>&lt;p&gt;
LLaMA-Adapter: &#38646;&#21021;&#22987;&#21270;&#27880;&#24847;&#21147;&#19979;&#30340;&#35821;&#35328;&#27169;&#22411;&#31934;&#32454;&#35843;&#25972;&#30340;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention. (arXiv:2303.16199v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16199
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36866;&#24212;&#25552;&#31034;&#21644;&#38646;&#21021;&#22987;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#36731;&#37327;&#32423;&#35821;&#35328;&#27169;&#22411;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#39640;&#25928;&#24494;&#35843;LLaMA&#20026;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#65292;&#20855;&#26377;&#27604;Alpaca&#26356;&#30701;&#30340;&#24494;&#35843;&#26102;&#38388;&#24182;&#20855;&#26377;&#36817;&#20284;&#30340;&#21709;&#24212;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;LLaMA-Adapter&#36825;&#19968;&#36731;&#37327;&#32423;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;LLaMA&#39640;&#25928;&#22320;&#24494;&#35843;&#20026;&#19968;&#20010;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#12290;&#21033;&#29992;52K&#20010;&#33258;&#25105;&#25351;&#23548;&#31034;&#33539;&#65292;LLaMA-Adapter&#20165;&#22312;&#20923;&#32467;&#30340;LLaMA 7B&#27169;&#22411;&#19978;&#24341;&#20837;&#20102;1.2M&#20010;&#21487;&#23398;&#20064;&#21442;&#25968;&#65292;&#24182;&#19988;&#22312;8&#20010;A100 GPU&#19978;&#20165;&#32791;&#26102;&#19981;&#21040;&#19968;&#20010;&#23567;&#26102;&#36827;&#34892;&#24494;&#35843;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#32452;&#21487;&#23398;&#20064;&#30340;&#36866;&#24212;&#25552;&#31034;&#65292;&#24182;&#22312;&#36739;&#39640;&#30340;&#21464;&#21387;&#22120;&#23618;&#20013;&#23558;&#23427;&#20204;&#39044;&#32622;&#20110;&#36755;&#20837;&#25991;&#26412;&#20196;&#29260;&#20043;&#21069;&#12290;&#28982;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#21021;&#22987;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#38646;&#38376;&#25511;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#23558;&#26032;&#30340;&#25351;&#20196;&#25552;&#31034;&#27880;&#20837;LLaMA&#65292;&#24182;&#26377;&#25928;&#22320;&#20445;&#30041;&#20102;&#20854;&#39044;&#20808;&#35757;&#32451;&#30340;&#30693;&#35782;&#12290;&#36890;&#36807;&#39640;&#25928;&#35757;&#32451;&#65292;LLaMA-Adapter&#33021;&#22815;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#21709;&#24212;&#65292;&#19982;&#23436;&#20840;&#24494;&#35843;&#30340;7B&#21442;&#25968;&#30340;Alpaca&#30456;&#20284;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#21487;&#20197;&#31616;&#21333;&#22320;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#36755;&#20837;&#65292;&#20363;&#22914;&#22270;&#20687;&#65292;&#29992;&#20110;&#22270;&#20687;&#30456;&#20851;&#30340;LLaMA&#65292;&#22312;ScienceQA&#19978;&#23454;&#29616;&#20102;&#26356;&#24378;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;https://github.com/ZrrSkywalker/LLaMA-Adapt&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present LLaMA-Adapter, a lightweight adaption method to efficiently fine-tune LLaMA into an instruction-following model. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon the frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8 A100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and prepend them to the input text tokens at higher transformer layers. Then, a zero-init attention mechanism with zero gating is proposed, which adaptively injects the new instructional cues into LLaMA, while effectively preserves its pre-trained knowledge. With efficient training, LLaMA-Adapter generates high-quality responses, comparable to Alpaca with fully fine-tuned 7B parameters. Furthermore, our approach can be simply extended to multi-modal input, e.g., images, for image-conditioned LLaMA, which achieves superior reasoning capacity on ScienceQA. We release our code at https://github.com/ZrrSkywalker/LLaMA-Adapt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;RNN seq2seq&#27169;&#22411;&#22312;&#23398;&#20064;&#22235;&#31181;&#36716;&#25442;&#20219;&#21153;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#21482;&#33021;&#36924;&#36817;&#31526;&#21512;&#35757;&#32451;&#25110;&#20998;&#24067;&#20869;&#25968;&#25454;&#30340;&#26144;&#23556;&#65292;&#19981;&#33021;&#23398;&#20064;&#24213;&#23618;&#20989;&#25968;&#65307;&#25991;&#31456;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#22797;&#26434;&#24615;&#23618;&#27425;&#32467;&#26500;&#65292;&#29992;&#20110;&#26080;&#27880;&#24847;&#21147;RNN seq2seq&#27169;&#22411;&#65292;&#32780;&#19981;&#26159;&#23383;&#31526;&#20018;&#36716;&#25442;&#30340;&#22797;&#26434;&#24615;&#23618;&#27425;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2303.06841</link><description>&lt;p&gt;
&#20351;&#29992;RNN&#27169;&#22411;&#23398;&#20064;&#36716;&#25442;&#21644;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Learning Transductions and Alignments with RNN Seq2seq Models. (arXiv:2303.06841v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;RNN seq2seq&#27169;&#22411;&#22312;&#23398;&#20064;&#22235;&#31181;&#36716;&#25442;&#20219;&#21153;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#21482;&#33021;&#36924;&#36817;&#31526;&#21512;&#35757;&#32451;&#25110;&#20998;&#24067;&#20869;&#25968;&#25454;&#30340;&#26144;&#23556;&#65292;&#19981;&#33021;&#23398;&#20064;&#24213;&#23618;&#20989;&#25968;&#65307;&#25991;&#31456;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#22797;&#26434;&#24615;&#23618;&#27425;&#32467;&#26500;&#65292;&#29992;&#20110;&#26080;&#27880;&#24847;&#21147;RNN seq2seq&#27169;&#22411;&#65292;&#32780;&#19981;&#26159;&#23383;&#31526;&#20018;&#36716;&#25442;&#30340;&#22797;&#26434;&#24615;&#23618;&#27425;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#24207;&#21015;&#21040;&#24207;&#21015;(RNN seq2seq)&#27169;&#22411;&#22312;&#23398;&#20064;&#22235;&#31181;&#36716;&#25442;&#20219;&#21153;&#65306;&#24658;&#31561;&#12289;&#21453;&#36716;&#12289;&#23436;&#20840;&#37325;&#22797;&#21644;&#20108;&#27425;&#22797;&#21046;&#12290;&#36825;&#20123;&#36716;&#25442;&#22312;&#26377;&#38480;&#29366;&#24577;&#36716;&#25442;&#22120;&#19979;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#24182;&#20855;&#26377;&#36880;&#28176;&#22686;&#21152;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;RNN seq2seq&#27169;&#22411;&#21482;&#33021;&#36924;&#36817;&#31526;&#21512;&#35757;&#32451;&#25110;&#20998;&#24067;&#20869;&#25968;&#25454;&#30340;&#26144;&#23556;&#65292;&#32780;&#19981;&#33021;&#23398;&#20064;&#24213;&#23618;&#20989;&#25968;&#12290;&#23613;&#31649;&#27880;&#24847;&#21147;&#26426;&#21046;&#20351;&#23398;&#20064;&#26356;&#21152;&#39640;&#25928;&#21644;&#40065;&#26834;&#65292;&#20294;&#23427;&#24182;&#19981;&#33021;&#20811;&#26381;&#20998;&#24067;&#22806;&#30340;&#27867;&#21270;&#38480;&#21046;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#22797;&#26434;&#24615;&#23618;&#27425;&#32467;&#26500;&#26469;&#23398;&#20064;&#36825;&#22235;&#20010;&#20219;&#21153;&#30340;&#26080;&#27880;&#24847;&#21147;RNN seq2seq&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#29992;&#27491;&#24335;&#35821;&#35328;&#30340;&#22797;&#26434;&#24615;&#23618;&#27425;&#32467;&#26500;&#26469;&#35299;&#37322;&#65292;&#32780;&#19981;&#26159;&#23383;&#31526;&#20018;&#36716;&#25442;&#30340;&#22797;&#26434;&#24615;&#23618;&#27425;&#32467;&#26500;&#12290;RNN&#30340;&#21464;&#31181;&#20063;&#22312;&#32467;&#26524;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#31616;&#21333;&#30340;RNN seq2seq&#27169;&#22411;&#26080;&#27861;&#35745;&#31639;&#36755;&#20837;&#38271;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper studies the capabilities of Recurrent-Neural-Network sequence to sequence (RNN seq2seq) models in learning four transduction tasks: identity, reversal, total reduplication, and quadratic copying. These transductions are traditionally well studied under finite state transducers and attributed with increasing complexity. We find that RNN seq2seq models are only able to approximate a mapping that fits the training or in-distribution data, instead of learning the underlying functions. Although attention makes learning more efficient and robust, it does not overcome the out-of-distribution generalization limitation. We establish a novel complexity hierarchy for learning the four tasks for attention-less RNN seq2seq models, which may be understood in terms of the complexity hierarchy of formal languages, instead of string transductions. RNN variants also play a role in the results. In particular, we show that Simple RNN seq2seq models cannot count the input length.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#29992;&#20154;&#31867;&#21453;&#39304;&#26367;&#20195;&#20256;&#32479;&#20114;&#32852;&#32593;&#25991;&#26412;&#26469;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#26465;&#20214;&#35757;&#32451;&#26159;&#26368;&#20248;&#21644;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#21487;&#23558;&#19981;&#33391;&#20869;&#23481;&#30340;&#29983;&#25104;&#36895;&#29575;&#38477;&#20302;&#19968;&#20010;&#25968;&#37327;&#32423;&#65292;&#21516;&#26102;&#20445;&#25345;&#35821;&#35328;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.08582</link><description>&lt;p&gt;
&#29992;&#20154;&#31867;&#20559;&#22909;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Pretraining Language Models with Human Preferences. (arXiv:2302.08582v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#29992;&#20154;&#31867;&#21453;&#39304;&#26367;&#20195;&#20256;&#32479;&#20114;&#32852;&#32593;&#25991;&#26412;&#26469;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#26465;&#20214;&#35757;&#32451;&#26159;&#26368;&#20248;&#21644;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#21487;&#23558;&#19981;&#33391;&#20869;&#23481;&#30340;&#29983;&#25104;&#36895;&#29575;&#38477;&#20302;&#19968;&#20010;&#25968;&#37327;&#32423;&#65292;&#21516;&#26102;&#20445;&#25345;&#35821;&#35328;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#39044;&#35757;&#32451;&#26159;&#20026;&#20102;&#27169;&#20223;&#20114;&#32852;&#32593;&#25991;&#26412;&#65292;&#20854;&#20013;&#21253;&#25324;&#22914;&#26524;&#30001;LMs&#29983;&#25104;&#32780;&#36829;&#21453;&#20154;&#31867;&#20559;&#22909;&#30340;&#20869;&#23481;:&#34394;&#20551;&#20449;&#24687;&#65292;&#20882;&#29359;&#24615;&#35780;&#35770;&#65292;&#20010;&#20154;&#21487;&#35782;&#21035;&#20449;&#24687;&#65292;&#36136;&#37327;&#36739;&#20302;&#25110;&#26377;&#32570;&#38519;&#30340;&#20195;&#30721;&#31561;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;LMs&#30340;&#22791;&#36873;&#30446;&#26631;&#65292;&#20197;&#24341;&#23548;&#23427;&#20204;&#29983;&#25104;&#19982;&#20154;&#31867;&#20559;&#22909;&#19968;&#33268;&#30340;&#25991;&#26412;&#12290;&#25105;&#20204;&#22312;&#19977;&#39033;&#20219;&#21153;&#20013;&#38024;&#23545;&#20154;&#31867;&#21453;&#39304;&#23545;&#20116;&#20010;&#39044;&#35757;&#32451;&#30446;&#26631;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#30740;&#31350;&#23427;&#20204;&#22914;&#20309;&#24433;&#21709;&#39044;&#35757;&#32451;LMs&#30340;&#19968;&#33268;&#24615;&#21644;&#33021;&#21147;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#25105;&#20204;&#25506;&#32034;&#30340;&#26041;&#27861;&#20013;&#26377;&#19968;&#31181;&#24085;&#32047;&#25176;&#26368;&#20248;&#19988;&#31616;&#21333;&#30340;&#26041;&#27861;&#65306;&#26465;&#20214;&#35757;&#32451;&#65292;&#25110;&#23398;&#20064;&#22312;&#22870;&#21169;&#27169;&#22411;&#32473;&#20986;&#30340;&#20154;&#31867;&#20559;&#22909;&#24471;&#20998;&#26465;&#20214;&#19979;&#30340;&#20196;&#29260;&#20998;&#24067;&#12290;&#26465;&#20214;&#35757;&#32451;&#23558;&#19981;&#33391;&#20869;&#23481;&#30340;&#29983;&#25104;&#36895;&#29575;&#38477;&#20302;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;&#65292;&#26080;&#35770;&#26159;&#22312;&#27809;&#26377;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#36824;&#26159;&#22312;&#23545;&#25239;&#36873;&#25321;&#30340;&#25552;&#31034;&#19979;&#29983;&#25104;&#65292;&#37117;&#22914;&#27492;&#12290;&#27492;&#22806;&#65292;&#26465;&#20214;&#35757;&#32451;&#20445;&#25345;&#20102;LMs&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#65292;&#34920;&#26126;&#23427;&#26159;&#39044;&#35757;&#32451;LMs&#29983;&#25104;&#19982;&#20154;&#31867;&#20559;&#22909;&#19968;&#33268;&#30340;&#25991;&#26412;&#30340;&#21487;&#34892;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) are pretrained to imitate internet text, including content that would violate human preferences if generated by an LM: falsehoods, offensive comments, personally identifiable information, low-quality or buggy code, and more. Here, we explore alternative objectives for pretraining LMs in a way that also guides them to generate text aligned with human preferences. We benchmark five objectives for pretraining with human feedback across three tasks and study how they affect the trade-off between alignment and capabilities of pretrained LMs. We find a Pareto-optimal and simple approach among those we explored: conditional training, or learning distribution over tokens conditional on their human preference scores given by a reward model. Conditional training reduces the rate of undesirable content by up to an order of magnitude, both when generating without a prompt and with an adversarially-chosen prompt. Moreover, conditional training maintains the downstream task per
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#39044;&#27979;&#20154;&#31867;&#22312;&#20845;&#20010;&#24863;&#23448;&#27169;&#24577;&#19979;&#30340;&#24863;&#30693;&#35780;&#21028;&#65292;&#24182;&#33021;&#25552;&#20379;&#20174;&#35821;&#35328;&#20013;&#25552;&#21462;&#24863;&#30693;&#20449;&#24687;&#30340;&#19979;&#38480;&#12290;</title><link>http://arxiv.org/abs/2302.01308</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#39044;&#27979;&#20154;&#31867;&#22312;&#20845;&#20010;&#24863;&#23448;&#27169;&#24577;&#19979;&#30340;&#24863;&#30693;&#35780;&#21028;
&lt;/p&gt;
&lt;p&gt;
Large language models predict human sensory judgments across six modalities. (arXiv:2302.01308v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#39044;&#27979;&#20154;&#31867;&#22312;&#20845;&#20010;&#24863;&#23448;&#27169;&#24577;&#19979;&#30340;&#24863;&#30693;&#35780;&#21028;&#65292;&#24182;&#33021;&#25552;&#20379;&#20174;&#35821;&#35328;&#20013;&#25552;&#21462;&#24863;&#30693;&#20449;&#24687;&#30340;&#19979;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#23450;&#20174;&#35821;&#35328;&#20013;&#21487;&#20197;&#24674;&#22797;&#24863;&#30693;&#19990;&#30028;&#30340;&#31243;&#24230;&#26159;&#21746;&#23398;&#21644;&#35748;&#30693;&#31185;&#23398;&#20013;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#65292;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#25552;&#20379;&#20174;&#35821;&#35328;&#20013;&#25552;&#21462;&#24863;&#30693;&#20449;&#24687;&#30340;&#19979;&#38480;&#65292;&#21487;&#20197;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#25552;&#20379;&#26032;&#30340;&#35265;&#35299;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;GPT&#27169;&#22411;&#20013;&#24341;&#20986;&#20102;&#20845;&#20010;&#24515;&#29702;&#29289;&#29702;&#25968;&#25454;&#38598;&#30340;&#25104;&#23545;&#30456;&#20284;&#24230;&#35780;&#20272;&#32467;&#26524;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#35780;&#20272;&#32467;&#26524;&#22312;&#25152;&#26377;&#39046;&#22495;&#20013;&#22343;&#19982;&#20154;&#31867;&#25968;&#25454;&#26174;&#33879;&#30456;&#20851;&#65292;&#22238;&#22797;&#20102;&#20247;&#25152;&#21608;&#30693;&#30340;&#34920;&#29616;&#65292;&#22914;&#39068;&#33394;&#29615;&#21644;&#38899;&#39640;&#34746;&#26059;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#19968;&#20010;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#19978;&#20849;&#21516;&#35757;&#32451;&#30340;&#27169;&#22411;&#65288;GPT-4&#65289;&#24182;&#19981;&#19968;&#23450;&#20250;&#23548;&#33268;&#23545;&#35270;&#35273;&#27169;&#24577;&#30340;&#29305;&#23450;&#25913;&#36827;&#12290;&#20026;&#20102;&#30740;&#31350;&#29305;&#23450;&#35821;&#35328;&#23545;&#24863;&#30693;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#36824;&#23558;&#36825;&#20123;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#35821;&#35328;&#39068;&#33394;&#21629;&#21517;&#20219;&#21153;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;GPT-4&#22312;&#33521;&#35821;&#21644;&#20420;&#35821;&#20013;&#22797;&#21046;&#20102;&#36328;&#35821;&#35328;&#24046;&#24322;&#65292;&#38416;&#26126;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Determining the extent to which the perceptual world can be recovered from language is a longstanding problem in philosophy and cognitive science. We show that state-of-the-art large language models can unlock new insights into this problem by providing a lower bound on the amount of perceptual information that can be extracted from language. Specifically, we elicit pairwise similarity judgments from GPT models across six psychophysical datasets. We show that the judgments are significantly correlated with human data across all domains, recovering well-known representations like the color wheel and pitch spiral. Surprisingly, we find that a model (GPT-4) co-trained on vision and language does not necessarily lead to improvements specific to the visual modality. To study the influence of specific languages on perception, we also apply the models to a multilingual color-naming task. We find that GPT-4 replicates cross-linguistic variation in English and Russian illuminating the interacti
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23558;&#20165;&#22788;&#29702;&#25991;&#26412;&#30340;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#20687;&#36827;&#34892;&#32852;&#31995;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#20132;&#38169;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#65292;&#24182;&#29983;&#25104;&#19982;&#26816;&#32034;&#22270;&#20687;&#20132;&#38169;&#30340;&#33258;&#30001;&#24418;&#24335;&#25991;&#26412;&#12290;&#35813;&#26041;&#27861;&#22312;&#29615;&#22659;&#30456;&#20851;&#30340;&#22270;&#20687;&#26816;&#32034;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#21313;&#20998;&#20248;&#24322;&#65292;&#26159;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#35270;&#35273;&#22330;&#26223;&#19979;&#20132;&#20114;&#38382;&#39064;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2301.13823</link><description>&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#20687;&#36827;&#34892;&#32852;&#31995;&#20197;&#22788;&#29702;&#22810;&#27169;&#24577;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Grounding Language Models to Images for Multimodal Inputs and Outputs. (arXiv:2301.13823v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13823
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23558;&#20165;&#22788;&#29702;&#25991;&#26412;&#30340;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#20687;&#36827;&#34892;&#32852;&#31995;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#20132;&#38169;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#65292;&#24182;&#29983;&#25104;&#19982;&#26816;&#32034;&#22270;&#20687;&#20132;&#38169;&#30340;&#33258;&#30001;&#24418;&#24335;&#25991;&#26412;&#12290;&#35813;&#26041;&#27861;&#22312;&#29615;&#22659;&#30456;&#20851;&#30340;&#22270;&#20687;&#26816;&#32034;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#21313;&#20998;&#20248;&#24322;&#65292;&#26159;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#35270;&#35273;&#22330;&#26223;&#19979;&#20132;&#20114;&#38382;&#39064;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;&#20165;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#19982;&#35270;&#35273;&#39046;&#22495;&#32852;&#31995;&#36215;&#26469;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#20132;&#38169;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#65292;&#24182;&#29983;&#25104;&#19982;&#26816;&#32034;&#22270;&#20687;&#20132;&#38169;&#30340;&#25991;&#26412;&#12290;&#25105;&#20204;&#21033;&#29992;&#20174;&#22823;&#35268;&#27169;&#25991;&#26412;&#39044;&#35757;&#32451;&#20013;&#23398;&#21040;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#20363;&#22914;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#33258;&#30001;&#24418;&#24335;&#25991;&#26412;&#29983;&#25104;&#12290;&#25105;&#20204;&#20445;&#25345;&#35821;&#35328;&#27169;&#22411;&#20923;&#32467;&#65292;&#24182;&#24494;&#35843;&#36755;&#20837;&#21644;&#36755;&#20986;&#32447;&#24615;&#23618;&#20197;&#23454;&#29616;&#36328;&#27169;&#24577;&#20132;&#20114;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#20132;&#38169;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#65292;&#24182;&#29983;&#25104;&#19982;&#26816;&#32034;&#22270;&#20687;&#20132;&#38169;&#30340;&#33258;&#30001;&#24418;&#24335;&#25991;&#26412;&#12290;&#25105;&#20204;&#22312;&#29615;&#22659;&#30456;&#20851;&#30340;&#22270;&#20687;&#26816;&#32034;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#31561;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#38646;-shot&#34920;&#29616;&#65292;&#24182;&#23637;&#31034;&#20102;&#24341;&#20154;&#20837;&#32988;&#30340;&#20132;&#20114;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#20219;&#20309;&#29616;&#25104;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20026;&#22312;&#35270;&#35273;&#22330;&#26223;&#19979;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#25928;&#19988;&#36890;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an efficient method to ground pretrained text-only language models to the visual domain, enabling them to process arbitrarily interleaved image-and-text data, and generate text interleaved with retrieved images. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and finetune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text interleaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contextual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained language models in visually grounded settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#21508;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#23545;&#19981;&#21516;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#24847;&#22270;&#20998;&#31867;&#20219;&#21153;&#20013;&#26816;&#27979;&#36229;&#20998;&#24067;&#65288;OOD&#65289;&#30340;&#33021;&#21147;&#65292;&#26088;&#22312;&#20026;&#35821;&#35328;&#27169;&#22411;&#30340;&#36229;&#20998;&#24067;&#40065;&#26834;&#24615;&#25552;&#20379;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2301.11660</link><description>&lt;p&gt;
&#20351;&#29992;&#21442;&#25968;&#39640;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#36229;&#20998;&#24067;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Probing Out-of-Distribution Robustness of Language Models with Parameter-Efficient Transfer Learning. (arXiv:2301.11660v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#21508;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#23545;&#19981;&#21516;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#24847;&#22270;&#20998;&#31867;&#20219;&#21153;&#20013;&#26816;&#27979;&#36229;&#20998;&#24067;&#65288;OOD&#65289;&#30340;&#33021;&#21147;&#65292;&#26088;&#22312;&#20026;&#35821;&#35328;&#27169;&#22411;&#30340;&#36229;&#20998;&#24067;&#40065;&#26834;&#24615;&#25552;&#20379;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#30340;&#35268;&#27169;&#19981;&#26029;&#22686;&#21152;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#35768;&#22810;&#21442;&#25968;&#39640;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#24357;&#34917;&#24494;&#35843;&#25104;&#26412;&#30340;&#24040;&#22823;&#20195;&#20215;&#12290;&#23613;&#31649;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21644;&#21508;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#65288;PETL&#65289;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#26159;&#21542;&#33021;&#26377;&#25928;&#22320;&#22788;&#29702;&#24050;&#20998;&#24067;&#25913;&#21464;&#30340;&#36755;&#20837;&#20173;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#25506;&#35752;&#20102;&#38543;&#30528;PLM&#22823;&#23567;&#22686;&#38271;&#25110;&#25913;&#21464;&#20256;&#36755;&#26041;&#27861;&#65292;&#26816;&#27979;&#36229;&#20998;&#24067;&#65288;OOD&#65289;&#30340;&#33021;&#21147;&#22914;&#20309;&#25913;&#21464;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181;PETL&#25216;&#26415;&#65292;&#21253;&#25324;&#24494;&#35843;&#12289;Adapter&#12289;LoRA&#21644;&#21069;&#32512;&#35843;&#25972;&#65292;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#24847;&#22270;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#27599;&#20010;&#20219;&#21153;&#37117;&#20351;&#29992;&#19981;&#21516;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the size of the pre-trained language model (PLM) continues to increase, numerous parameter-efficient transfer learning methods have been proposed recently to compensate for the tremendous cost of fine-tuning. Despite the impressive results achieved by large pre-trained language models (PLMs) and various parameter-efficient transfer learning (PETL) methods on sundry benchmarks, it remains unclear if they can handle inputs that have been distributionally shifted effectively. In this study, we systematically explore how the ability to detect out-of-distribution (OOD) changes as the size of the PLM grows or the transfer methods are altered. Specifically, we evaluated various PETL techniques, including fine-tuning, Adapter, LoRA, and prefix-tuning, on three different intention classification tasks, each utilizing various language models with different scales.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#32447;&#24615;&#25506;&#27979;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#20004;&#32773;&#30340;&#20248;&#28857;&#65292;&#26088;&#22312;&#25552;&#39640;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2212.10873</link><description>&lt;p&gt;
&#25552;&#21319;&#32447;&#24615;&#25506;&#27979;&#65306;&#36229;&#36234;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Prompt-Augmented Linear Probing: Scaling beyond the Limit of Few-shot In-Context Learners. (arXiv:2212.10873v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#32447;&#24615;&#25506;&#27979;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#20004;&#32773;&#30340;&#20248;&#28857;&#65292;&#26088;&#22312;&#25552;&#39640;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#39640;&#25928;&#22320;&#36827;&#34892;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#39069;&#22806;&#30340;&#27169;&#22411;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24213;&#23618;&#35821;&#35328;&#27169;&#22411;&#30340;&#22266;&#26377;&#36755;&#20837;&#38271;&#24230;&#38480;&#21046;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24615;&#33021;&#22312;&#21487;&#29992;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#19978;&#24182;&#19981;&#20855;&#26377;&#21487;&#20280;&#32553;&#24615;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#35768;&#22810;&#30740;&#31350;&#34920;&#26126;&#35821;&#35328;&#27169;&#22411;&#20063;&#26159;&#24378;&#22823;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#20351;&#20854;&#33021;&#22815;&#20197;&#40657;&#21283;&#23376;&#30340;&#26041;&#24335;&#20351;&#29992;&#65292;&#24182;&#23454;&#29616;&#32447;&#24615;&#25506;&#27979;&#33539;&#24335;&#65292;&#21363;&#22312;&#39044;&#20808;&#25552;&#21462;&#30340;&#36755;&#20837;&#34920;&#31034;&#20043;&#19978;&#35757;&#32451;&#36731;&#37327;&#32423;&#37492;&#21035;&#22120;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;prompt-augmented linear probing&#65288;PALP&#65289;&#65292;&#23427;&#26159;&#32447;&#24615;&#25506;&#27979;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#28151;&#21512;&#20307;&#65292;&#20860;&#20855;&#20108;&#32773;&#30340;&#20248;&#28857;&#12290;PALP&#32487;&#25215;&#20102;&#32447;&#24615;&#25506;&#27979;&#30340;&#21487;&#20280;&#32553;&#24615;&#21644;&#20351;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#23558;&#36755;&#20837;&#23450;&#21046;&#20026;&#26356;&#26131;&#29702;&#35299;&#30340;&#24418;&#24335;&#26469;&#27966;&#29983;&#26356;&#26377;&#24847;&#20041;&#34920;&#31034;&#30340;&#33021;&#21147;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#30340;&#28145;&#20837;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;PALP&#22312;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#22343;&#32988;&#36807;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Through in-context learning (ICL), large-scale language models are effective few-shot learners without additional model fine-tuning. However, the ICL performance does not scale well with the number of available training samples as it is limited by the inherent input length constraint of the underlying language model. Meanwhile, many studies have revealed that language models are also powerful feature extractors, allowing them to be utilized in a black-box manner and enabling the linear probing paradigm, where lightweight discriminators are trained on top of the pre-extracted input representations. This paper proposes prompt-augmented linear probing (PALP), a hybrid of linear probing and ICL, which leverages the best of both worlds. PALP inherits the scalability of linear probing and the capability of enforcing language models to derive more meaningful representations via tailoring input into a more conceivable form. Throughout in-depth investigations on various datasets, we verified th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#21644;&#20998;&#26512;&#65292;&#35780;&#20272;&#20102;GPT-3&#20316;&#20026;&#25968;&#25454;&#27880;&#37322;&#22120;&#30340;&#24615;&#33021;&#65292;&#25506;&#35752;&#20102;&#20854;&#20316;&#20026;&#36890;&#29992;NLP&#25968;&#25454;&#27880;&#37322;&#22120;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.10450</link><description>&lt;p&gt;
GPT-3&#26159;&#21542;&#26159;&#19968;&#20010;&#22909;&#30340;&#25968;&#25454;&#27880;&#37322;&#22120;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is GPT-3 a Good Data Annotator?. (arXiv:2212.10450v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#21644;&#20998;&#26512;&#65292;&#35780;&#20272;&#20102;GPT-3&#20316;&#20026;&#25968;&#25454;&#27880;&#37322;&#22120;&#30340;&#24615;&#33021;&#65292;&#25506;&#35752;&#20102;&#20854;&#20316;&#20026;&#36890;&#29992;NLP&#25968;&#25454;&#27880;&#37322;&#22120;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#27880;&#37322;&#26159;&#26631;&#35760;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25968;&#25454;&#30340;&#36807;&#31243;&#12290;&#39640;&#36136;&#37327;&#30340;&#27880;&#37322;&#23545;&#20110;&#27169;&#22411;&#23398;&#20064;&#36755;&#20837;&#25968;&#25454;&#19982;&#26399;&#26395;&#36755;&#20986;&#20043;&#38388;&#30340;&#20851;&#31995;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;OpenAI&#24320;&#21457;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;GPT-3&#22312;&#24191;&#27867;&#30340;NLP&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38646;&#27425;&#21644;&#23569;&#27425;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#33258;&#28982;&#32780;&#28982;&#22320;&#24819;&#30693;&#36947;&#23427;&#26159;&#21542;&#21487;&#20197;&#29992;&#20110;&#26377;&#25928;&#22320;&#20026;NLP&#20219;&#21153;&#27880;&#37322;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19982;&#20256;&#32479;&#25968;&#25454;&#27880;&#37322;&#26041;&#27861;&#30340;&#27604;&#36739;&#20197;&#21450;&#20998;&#26512;&#20854;&#22312;&#19968;&#31995;&#21015;&#20219;&#21153;&#19978;&#30340;&#36755;&#20986;&#65292;&#35780;&#20272;GPT-3&#20316;&#20026;&#25968;&#25454;&#27880;&#37322;&#22120;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#36825;&#20010;&#20998;&#26512;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#20851;&#20110;GPT-3&#20316;&#20026;NLP&#36890;&#29992;&#25968;&#25454;&#27880;&#37322;&#22120;&#28508;&#21147;&#30340;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data annotation is the process of labeling data that could be used to train machine learning models. Having high-quality annotation is crucial, as it allows the model to learn the relationship between the input data and the desired output. GPT-3, a large-scale language model developed by OpenAI, has demonstrated impressive zero- and few-shot performance on a wide range of NLP tasks. It is therefore natural to wonder whether it can be used to effectively annotate data for NLP tasks. In this paper, we evaluate the performance of GPT-3 as a data annotator by comparing it with traditional data annotation methods and analyzing its output on a range of tasks. Through this analysis, we aim to provide insight into the potential of GPT-3 as a general-purpose data annotator in NLP.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;MTurk&#19978;&#39640;&#19968;&#33268;&#24615;&#24037;&#20316;&#32773;&#23545;&#33258;&#21160;&#25688;&#35201;&#30340;&#35780;&#20272;&#65292;&#36890;&#36807;&#20004;&#27493;&#31579;&#36873;&#25214;&#21040;&#39640;&#36136;&#37327;&#30340;&#24037;&#20316;&#32773;&#65292;&#20026;&#20854;&#20182;&#20855;&#26377;&#25361;&#25112;&#24615;&#27880;&#37322;&#20219;&#21153;&#30340;&#25307;&#21215;&#25552;&#20379;&#20102;&#26368;&#20339;&#23454;&#36341;&#12290;</title><link>http://arxiv.org/abs/2212.10397</link><description>&lt;p&gt;
&#24323;&#30707;&#25104;&#37329;&#65306;MTurk&#20013;&#39640;&#19968;&#33268;&#24615;&#24037;&#20316;&#32773;&#23545;&#33258;&#21160;&#25688;&#35201;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Needle in a Haystack: An Analysis of High-Agreement Workers on MTurk for Summarization. (arXiv:2212.10397v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;MTurk&#19978;&#39640;&#19968;&#33268;&#24615;&#24037;&#20316;&#32773;&#23545;&#33258;&#21160;&#25688;&#35201;&#30340;&#35780;&#20272;&#65292;&#36890;&#36807;&#20004;&#27493;&#31579;&#36873;&#25214;&#21040;&#39640;&#36136;&#37327;&#30340;&#24037;&#20316;&#32773;&#65292;&#20026;&#20854;&#20182;&#20855;&#26377;&#25361;&#25112;&#24615;&#27880;&#37322;&#20219;&#21153;&#30340;&#25307;&#21215;&#25552;&#20379;&#20102;&#26368;&#20339;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#36991;&#20813;&#22312;&#20302;&#36136;&#37327;&#27880;&#37322;&#19978;&#28010;&#36153;&#26114;&#36149;&#32780;&#20302;&#25928;&#30340;&#36164;&#28304;&#65292;&#25105;&#20204;&#38656;&#35201;&#19968;&#31181;&#26041;&#27861;&#26469;&#21019;&#24314;&#19968;&#32452;&#21487;&#38752;&#30340;&#27880;&#37322;&#32773;&#65292;&#20182;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#23436;&#25104;&#22256;&#38590;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#35780;&#20272;&#33258;&#21160;&#25688;&#35201;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#20004;&#27493;&#27969;&#31243;&#35843;&#26597;&#25307;&#21215;&#39640;&#36136;&#37327;&#30340;Amazon Mechanical Turk&#24037;&#20316;&#32773;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#20182;&#20204;&#36827;&#34892;&#35780;&#20272;&#20043;&#21069;&#25104;&#21151;&#22320;&#36807;&#28388;&#25481;&#21155;&#36136;&#30340;&#24037;&#20316;&#32773;&#65292;&#24182;&#22312;&#31867;&#20284;&#36164;&#28304;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#39640;&#24230;&#19968;&#33268;&#30340;&#27880;&#37322;&#12290;&#23613;&#31649;&#25105;&#20204;&#30340;&#24037;&#20316;&#32773;&#24444;&#27492;&#20043;&#38388;&#20197;&#21450;CloudResearch&#24037;&#20316;&#32773;&#20043;&#38388;&#34920;&#29616;&#20986;&#24378;&#28872;&#30340;&#20849;&#35782;&#65292;&#20294;&#20182;&#20204;&#19982;&#19987;&#23478;&#23545;&#25968;&#25454;&#23376;&#38598;&#30340;&#21028;&#26029;&#19968;&#33268;&#24615;&#24182;&#19981;&#20687;&#39044;&#26399;&#37027;&#26679;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#22312;&#27491;&#30830;&#24615;&#26041;&#38754;&#36827;&#34892;&#22521;&#35757;&#12290;&#26412;&#25991;&#20173;&#28982;&#20316;&#20026;&#22312;&#20854;&#20182;&#20855;&#26377;&#25361;&#25112;&#24615;&#27880;&#37322;&#20219;&#21153;&#20013;&#25307;&#21215;&#21512;&#26684;&#30340;&#27880;&#37322;&#32773;&#30340;&#26368;&#20339;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;
To prevent the costly and inefficient use of resources on low-quality annotations, we want a method for creating a pool of dependable annotators who can effectively complete difficult tasks, such as evaluating automatic summarization. Thus, we investigate the recruitment of high-quality Amazon Mechanical Turk workers via a two-step pipeline. We show that we can successfully filter out subpar workers before they carry out the evaluations and obtain high-agreement annotations with similar constraints on resources. Although our workers demonstrate a strong consensus among themselves and CloudResearch workers, their alignment with expert judgments on a subset of the data is not as expected and needs further training in correctness. This paper still serves as a best practice for the recruitment of qualified annotators in other challenging annotation tasks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; Detailed Outline Control(DOC) &#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35814;&#32454;&#22823;&#32434;&#21644;&#35814;&#32454;&#25511;&#21046;&#22120;&#26469;&#25552;&#39640;&#29983;&#25104;&#38271;&#31687;&#25925;&#20107;&#26102;&#30340;&#24773;&#33410;&#36830;&#36143;&#24615;&#21644;&#22823;&#32434;&#30456;&#20851;&#24615;&#65292;&#20154;&#31867;&#35780;&#20272;&#35777;&#23454;&#35813;&#26041;&#27861;&#22312;&#36825;&#20123;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#65292;&#24182;&#19988;&#26356;&#36866;&#29992;&#20110;&#20132;&#20114;&#29983;&#25104;&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2212.10077</link><description>&lt;p&gt;
&#36890;&#36807;&#35814;&#32454;&#30340;&#22823;&#32434;&#25511;&#21046;&#25552;&#21319;&#38271;&#31687;&#25925;&#20107;&#36830;&#36143;&#24615;
&lt;/p&gt;
&lt;p&gt;
DOC: Improving Long Story Coherence With Detailed Outline Control. (arXiv:2212.10077v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10077
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; Detailed Outline Control(DOC) &#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35814;&#32454;&#22823;&#32434;&#21644;&#35814;&#32454;&#25511;&#21046;&#22120;&#26469;&#25552;&#39640;&#29983;&#25104;&#38271;&#31687;&#25925;&#20107;&#26102;&#30340;&#24773;&#33410;&#36830;&#36143;&#24615;&#21644;&#22823;&#32434;&#30456;&#20851;&#24615;&#65292;&#20154;&#31867;&#35780;&#20272;&#35777;&#23454;&#35813;&#26041;&#27861;&#22312;&#36825;&#20123;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#65292;&#24182;&#19988;&#26356;&#36866;&#29992;&#20110;&#20132;&#20114;&#29983;&#25104;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026; Detailed Outline Control(DOC)&#30340;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;&#29983;&#25104;&#25968;&#21315;&#23383;&#38271;&#30340;&#25925;&#20107;&#26102;&#30340;&#38271;&#31243;&#24773;&#33410;&#36830;&#36143;&#24615;&#12290;DOC&#30001;&#20004;&#20010;&#20114;&#34917;&#32452;&#20214;&#32452;&#25104;&#65306;&#35814;&#32454;&#22823;&#32434;&#21644;&#35814;&#32454;&#25511;&#21046;&#22120;&#12290;&#35814;&#32454;&#22823;&#32434;&#21019;&#24314;&#19968;&#20010;&#26356;&#35814;&#32454;&#12289;&#23618;&#27425;&#21270;&#30340;&#22823;&#32434;&#65292;&#23558;&#21019;&#36896;&#24615;&#36127;&#25285;&#20174;&#20027;&#35201;&#36215;&#33609;&#36807;&#31243;&#36716;&#31227;&#21040;&#35268;&#21010;&#38454;&#27573;&#12290;&#35814;&#32454;&#25511;&#21046;&#22120;&#36890;&#36807;&#25511;&#21046;&#25925;&#20107;&#27573;&#33853;&#19982;&#22823;&#32434;&#32454;&#33410;&#23545;&#40784;&#65292;&#30830;&#20445;&#26356;&#35814;&#32454;&#30340;&#22823;&#32434;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#20173;&#28982;&#34987;&#23562;&#37325;&#12290;&#22312;&#33258;&#21160;&#29983;&#25104;&#30340;&#25925;&#20107;&#30340;&#20154;&#31867;&#35780;&#20272;&#20013;&#65292;DOC&#22312;&#24773;&#33410;&#36830;&#36143;&#24615;(22.5% &#32477;&#23545;&#22686;&#30410;)&#12289;&#22823;&#32434;&#30456;&#20851;&#24615;(28.2%)&#21644;&#36259;&#21619;&#24615;(20.7%)&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#24378;&#22823;&#30340;Re3&#22522;&#32447;(Yang&#31561;&#20154;&#65292;2022)&#12290;&#20154;&#20204;&#36824;&#35780;&#20215;DOC&#22312;&#20132;&#20114;&#29983;&#25104;&#35774;&#32622;&#26041;&#38754;&#26356;&#26131;&#20110;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the Detailed Outline Control (DOC) framework for improving long-range plot coherence when automatically generating several-thousand-word-long stories. DOC consists of two complementary components: a detailed outliner and a detailed controller. The detailed outliner creates a more detailed, hierarchically structured outline, shifting creative burden from the main drafting procedure to the planning stage. The detailed controller ensures the more detailed outline is still respected during generation by controlling story passages to align with outline details. In human evaluations of automatically generated stories, DOC substantially outperforms a strong Re3 baseline (Yang et al., 2022) on plot coherence (22.5% absolute gain), outline relevance (28.2%), and interestingness (20.7%). Humans also judged DOC to be much more controllable in an interactive generation setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#21270;&#30446;&#26631;&#34920;&#31034;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#39640;&#25928;&#24615;&#25913;&#36827;&#26041;&#27861;&#65292;&#31216;&#20026;data2vec 2.0&#65292;&#23427;&#22312;&#22810;&#39033;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#21644;&#20854;&#20182;&#31639;&#27861;&#30456;&#24403;&#30340;&#20934;&#30830;&#29575;&#65292;&#20294;&#38656;&#35201;&#30340;&#39044;&#35757;&#32451;&#26102;&#38388;&#36739;&#30701;&#12290;</title><link>http://arxiv.org/abs/2212.07525</link><description>&lt;p&gt;
&#38754;&#21521;&#35270;&#35273;&#12289;&#35821;&#38899;&#21644;&#35821;&#35328;&#30340;&#19978;&#19979;&#25991;&#21270;&#30446;&#26631;&#34920;&#31034;&#33258;&#30417;&#30563;&#23398;&#20064;&#39640;&#25928;&#24615;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Efficient Self-supervised Learning with Contextualized Target Representations for Vision, Speech and Language. (arXiv:2212.07525v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#21270;&#30446;&#26631;&#34920;&#31034;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#39640;&#25928;&#24615;&#25913;&#36827;&#26041;&#27861;&#65292;&#31216;&#20026;data2vec 2.0&#65292;&#23427;&#22312;&#22810;&#39033;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#21644;&#20854;&#20182;&#31639;&#27861;&#30456;&#24403;&#30340;&#20934;&#30830;&#29575;&#65292;&#20294;&#38656;&#35201;&#30340;&#39044;&#35757;&#32451;&#26102;&#38388;&#36739;&#30701;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#26159;&#27169;&#24577;&#29305;&#23450;&#30340;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#39640;&#20102;data2vec&#30340;&#35757;&#32451;&#25928;&#29575;&#65292;&#35813;&#23398;&#20064;&#30446;&#26631;&#21487;&#20197;&#25512;&#24191;&#21040;&#22810;&#31181;&#27169;&#24577;&#12290;&#25105;&#20204;&#19981;&#23545;&#25513;&#34109;&#26631;&#35760;&#36827;&#34892;&#32534;&#30721;&#65292;&#20351;&#29992;&#24555;&#36895;&#21367;&#31215;&#35299;&#30721;&#22120;&#65292;&#24182;&#20998;&#25674;&#26500;&#24314;&#25945;&#24072;&#34920;&#31034;&#30340;&#24037;&#20316;&#12290;data2vec 2.0&#21463;&#21040;data2vec&#24341;&#20837;&#30340;&#20016;&#23500;&#19978;&#19979;&#25991;&#21270;&#30446;&#26631;&#34920;&#31034;&#30340;&#30410;&#22788;&#65292;&#21487;&#20197;&#23454;&#29616;&#24555;&#36895;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;&#22312;ImageNet-1K&#22270;&#20687;&#20998;&#31867;&#23454;&#39564;&#20013;&#65292;data2vec 2.0&#22312;&#20302;16.4&#20493;&#30340;&#39044;&#35757;&#32451;&#26102;&#38388;&#20869;&#19982;&#33945;&#29256;&#33258;&#32534;&#30721;&#22120;&#30340;&#20934;&#30830;&#29575;&#30456;&#21305;&#37197;&#65292;&#22312;Librispeech&#35821;&#38899;&#35782;&#21035;&#20013;&#65292;&#23427;&#30340;&#34920;&#29616;&#19982;wav2vec 2.0&#30456;&#24403;&#65292;&#26102;&#38388;&#23569;10.6&#20493;&#65292;&#22312;GLUE&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#65292;&#23427;&#19982;&#37325;&#26032;&#35757;&#32451;&#30340;RoBERTa&#27169;&#22411;&#30340;&#26102;&#38388;&#30456;&#27604;&#20943;&#21322;&#12290;&#22312;&#29306;&#29298;&#19968;&#23450;&#30340;&#36895;&#24230;&#20197;&#25442;&#21462;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#35757;&#32451;&#20102;150&#20010;epochs&#30340;ViT-L&#27169;&#22411;&#21487;&#20197;&#24471;&#21040;86.8\%&#30340;ImageNet-1K top-1&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current self-supervised learning algorithms are often modality-specific and require large amounts of computational resources. To address these issues, we increase the training efficiency of data2vec, a learning objective that generalizes across several modalities. We do not encode masked tokens, use a fast convolutional decoder and amortize the effort to build teacher representations. data2vec 2.0 benefits from the rich contextualized target representations introduced in data2vec which enable a fast self-supervised learner. Experiments on ImageNet-1K image classification show that data2vec 2.0 matches the accuracy of Masked Autoencoders in 16.4x lower pre-training time, on Librispeech speech recognition it performs as well as wav2vec 2.0 in 10.6x less time, and on GLUE natural language understanding it matches a retrained RoBERTa model in half the time. Trading some speed for accuracy results in ImageNet-1K top-1 accuracy of 86.8\% with a ViT-L model trained for 150 epochs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#19968;&#33268;&#24615;&#25490;&#21517;&#30340;&#33258;&#21160;&#26080;&#22122;&#22768;&#26631;&#31614;&#26816;&#27979;&#25216;&#26415;&#65292;&#23558;&#20854;&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASV&#65289;&#20219;&#21153;&#20316;&#20026;&#27010;&#24565;&#39564;&#35777;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#25552;&#39640;&#22823;&#35268;&#27169;&#35828;&#35805;&#20154;&#35782;&#21035;&#25968;&#25454;&#38598;&#30340;&#39640;&#25928;&#28165;&#27927;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.00239</link><description>&lt;p&gt;
&#22522;&#20110;&#19981;&#19968;&#33268;&#24615;&#25490;&#21517;&#30340;&#39640;&#36136;&#37327;&#26080;&#22122;&#22768;&#26631;&#31614;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Inconsistency Ranking-based Noisy Label Detection for High-quality Data. (arXiv:2212.00239v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#19968;&#33268;&#24615;&#25490;&#21517;&#30340;&#33258;&#21160;&#26080;&#22122;&#22768;&#26631;&#31614;&#26816;&#27979;&#25216;&#26415;&#65292;&#23558;&#20854;&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASV&#65289;&#20219;&#21153;&#20316;&#20026;&#27010;&#24565;&#39564;&#35777;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#25552;&#39640;&#22823;&#35268;&#27169;&#35828;&#35805;&#20154;&#35782;&#21035;&#25968;&#25454;&#38598;&#30340;&#39640;&#25928;&#28165;&#27927;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#25104;&#21151;&#38656;&#35201;&#39640;&#36136;&#37327;&#30340;&#27880;&#37322;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#65292;&#28982;&#32780;&#23454;&#38469;&#20013;&#65292;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#21644;&#36136;&#37327;&#24448;&#24448;&#23384;&#22312;&#19968;&#20010;&#26435;&#34913;&#65292;&#22240;&#20026;&#25968;&#25454;&#30340;&#25910;&#38598;&#21644;&#28165;&#27927;&#36153;&#29992;&#39640;&#26114;&#19988;&#32791;&#26102;&#12290;&#22312;&#20351;&#29992;&#20247;&#21253;&#25968;&#25454;&#38598;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#29305;&#21035;&#26159;&#38656;&#35201;&#25490;&#38500;&#22122;&#22768;&#26631;&#31614;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#19968;&#33268;&#24615;&#25490;&#21517;&#30340;&#33258;&#21160;&#26080;&#22122;&#22768;&#26631;&#31614;&#26816;&#27979;&#25216;&#26415;&#65292;&#23558;&#20854;&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASV&#65289;&#20219;&#21153;&#20316;&#20026;&#27010;&#24565;&#39564;&#35777;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#31867;&#38388;&#21644;&#31867;&#20869;&#19981;&#19968;&#33268;&#24615;&#25490;&#21517;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#22122;&#22768;&#35774;&#32622;&#19979;&#27604;&#36739;&#20102;&#20960;&#31181;&#24230;&#37327;&#23398;&#20064;&#25439;&#22833;&#20989;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#25552;&#39640;&#22823;&#35268;&#27169;&#35828;&#35805;&#20154;&#35782;&#21035;&#25968;&#25454;&#38598;&#30340;&#39640;&#25928;&#28165;&#27927;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of deep learning requires high-quality annotated and massive data. However, the size and the quality of a dataset are usually a trade-off in practice, as data collection and cleaning are expensive and time-consuming. In real-world applications, especially those using crowdsourcing datasets, it is important to exclude noisy labels. To address this, this paper proposes an automatic noisy label detection (NLD) technique with inconsistency ranking for high-quality data. We apply this technique to the automatic speaker verification (ASV) task as a proof of concept. We investigate both inter-class and intra-class inconsistency ranking and compare several metric learning loss functions under different noise settings. Experimental results confirm that the proposed solution could increase both the efficient and effective cleaning of large-scale speaker recognition datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#30740;&#31350;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#24187;&#35273;&#30340;&#20135;&#29983;&#21407;&#22240;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#38477;&#20302;&#24187;&#35273;&#30340;&#20135;&#29983;&#24182;&#19988;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2211.09878</link><description>&lt;p&gt;
&#29992;&#29305;&#24449;&#24402;&#22240;&#38477;&#20302;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Reducing Hallucinations in Neural Machine Translation with Feature Attribution. (arXiv:2211.09878v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09878
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#30740;&#31350;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#24187;&#35273;&#30340;&#20135;&#29983;&#21407;&#22240;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#38477;&#20302;&#24187;&#35273;&#30340;&#20135;&#29983;&#24182;&#19988;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#26465;&#20214;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#39046;&#22495;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25104;&#26524;&#65292;&#20294;&#26159;&#39640;&#24230;&#20381;&#36182;&#20110;&#24179;&#34892;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#12290;&#24403;&#35757;&#32451;&#25968;&#25454;&#38598;&#36136;&#37327;&#36739;&#20302;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#23481;&#26131;&#20986;&#29616;&#21508;&#31181;&#38169;&#35823;&#31867;&#22411;&#65292;&#21253;&#25324;&#24187;&#35273;&#65292;&#21363;&#27969;&#30021;&#20294;&#19982;&#28304;&#35821;&#35328;&#21477;&#23376;&#26080;&#20851;&#30340;&#36755;&#20986;&#12290;&#36825;&#20123;&#38169;&#35823;&#23588;&#20854;&#21361;&#38505;&#65292;&#22240;&#20026;&#22312;&#34920;&#38754;&#19978;&#65292;&#32763;&#35793;&#21487;&#33021;&#34987;&#35748;&#20026;&#26159;&#27491;&#30830;&#30340;&#36755;&#20986;&#65292;&#29305;&#21035;&#26159;&#22914;&#26524;&#35835;&#32773;&#19981;&#29702;&#35299;&#28304;&#35821;&#35328;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#29992;&#20110;&#38477;&#20302;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#24187;&#35273;&#30340;&#27169;&#22411;&#29702;&#35299;&#21644;&#35268;&#33539;&#21270;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#30740;&#31350;&#20135;&#29983;&#24187;&#35273;&#30340;NMT&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#22823;&#22823;&#24110;&#21161;&#20943;&#23569;&#24187;&#35273;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20174;&#22836;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural conditional language generation models achieve the state-of-the-art in Neural Machine Translation (NMT) but are highly dependent on the quality of parallel training dataset. When trained on low-quality datasets, these models are prone to various error types, including hallucinations, i.e. outputs that are fluent, but unrelated to the source sentences. These errors are particularly dangerous, because on the surface the translation can be perceived as a correct output, especially if the reader does not understand the source language. We present a case study focusing on model understanding and regularisation to reduce hallucinations in NMT. We first use feature attribution methods to study the behaviour of an NMT model that produces hallucinations. We then leverage these methods to propose a novel loss function that substantially helps reduce hallucinations and does not require retraining the model from scratch.
&lt;/p&gt;</description></item><item><title>ATCO2&#35821;&#26009;&#24211;&#26159;&#19968;&#20010;&#26088;&#22312;&#20419;&#36827;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#36890;&#20449;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#21644;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30740;&#31350;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#25968;&#25454;&#25910;&#38598;&#21644;&#39044;&#22788;&#29702;&#12289;&#35821;&#38899;&#25968;&#25454;&#30340;&#20266;&#27880;&#37322;&#65292;&#20197;&#21450;&#25552;&#21462;&#19982;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#30456;&#20851;&#30340;&#21629;&#21517;&#23454;&#20307;&#12290;</title><link>http://arxiv.org/abs/2211.04054</link><description>&lt;p&gt;
ATCO2&#35821;&#26009;&#24211;&#65306;&#29992;&#20110;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#36890;&#20449;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#21644;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30740;&#31350;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ATCO2 corpus: A Large-Scale Dataset for Research on Automatic Speech Recognition and Natural Language Understanding of Air Traffic Control Communications. (arXiv:2211.04054v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04054
&lt;/p&gt;
&lt;p&gt;
ATCO2&#35821;&#26009;&#24211;&#26159;&#19968;&#20010;&#26088;&#22312;&#20419;&#36827;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#36890;&#20449;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#21644;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30740;&#31350;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#25968;&#25454;&#25910;&#38598;&#21644;&#39044;&#22788;&#29702;&#12289;&#35821;&#38899;&#25968;&#25454;&#30340;&#20266;&#27880;&#37322;&#65292;&#20197;&#21450;&#25552;&#21462;&#19982;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#30456;&#20851;&#30340;&#21629;&#21517;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20114;&#32852;&#30340;&#25968;&#23383;&#19990;&#30028;&#20013;&#65292;&#20010;&#20154;&#21161;&#25163;&#12289;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#22120;&#21644;&#23545;&#35805;&#29702;&#35299;&#31995;&#32479;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#33322;&#31354;&#20132;&#36890;&#31649;&#21046;&#36890;&#20449;&#23601;&#26159;&#20854;&#20013;&#19968;&#20010;&#26126;&#26174;&#30340;&#20363;&#23376;&#12290;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#26088;&#22312;&#20197;&#23433;&#20840;&#21644;&#26368;&#20248;&#30340;&#26041;&#24335;&#25351;&#23548;&#39134;&#34892;&#22120;&#21644;&#25511;&#21046;&#31354;&#22495;&#12290;&#36825;&#20123;&#22522;&#20110;&#35821;&#38899;&#30340;&#23545;&#35805;&#26159;&#36890;&#36807;&#36229;&#39640;&#39057;&#26080;&#32447;&#30005;&#39057;&#36947;&#22312;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#21592;&#65288;ATCO&#65289;&#21644;&#39134;&#34892;&#21592;&#20043;&#38388;&#36827;&#34892;&#30340;&#12290;&#20026;&#20102;&#23558;&#36825;&#20123;&#26032;&#25216;&#26415;&#32435;&#20837;ATC&#65288;&#20302;&#36164;&#28304;&#39046;&#22495;&#65289;&#65292;&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#27880;&#37322;&#25968;&#25454;&#38598;&#26469;&#24320;&#21457;&#25968;&#25454;&#39537;&#21160;&#30340;AI&#31995;&#32479;&#65292;&#20854;&#20013;&#21253;&#25324;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ATCO2&#35821;&#26009;&#24211;&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#20419;&#36827;ATC&#39046;&#22495;&#30740;&#31350;&#30340;&#25968;&#25454;&#38598;&#65292;&#30001;&#20110;&#32570;&#20047;&#27880;&#37322;&#30340;&#25968;&#25454;&#65292;&#35813;&#39046;&#22495;&#28382;&#21518;&#24456;&#22810;&#12290;ATCO2&#35821;&#26009;&#24211;&#21253;&#25324;1&#65289;&#25968;&#25454;&#25910;&#38598;&#21644;&#39044;&#22788;&#29702;&#65292;2&#65289;&#35821;&#38899;&#25968;&#25454;&#30340;&#20266;&#27880;&#37322;&#65292;&#21644;3&#65289;&#25552;&#21462;&#19982;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#30456;&#20851;&#30340;&#21629;&#21517;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personal assistants, automatic speech recognizers and dialogue understanding systems are becoming more critical in our interconnected digital world. A clear example is air traffic control (ATC) communications. ATC aims at guiding aircraft and controlling the airspace in a safe and optimal manner. These voice-based dialogues are carried between an air traffic controller (ATCO) and pilots via very-high frequency radio channels. In order to incorporate these novel technologies into ATC (low-resource domain), large-scale annotated datasets are required to develop the data-driven AI systems. Two examples are automatic speech recognition (ASR) and natural language understanding (NLU). In this paper, we introduce the ATCO2 corpus, a dataset that aims at fostering research on the challenging ATC field, which has lagged behind due to lack of annotated data. The ATCO2 corpus covers 1) data collection and pre-processing, 2) pseudo-annotations of speech data, and 3) extraction of ATC-related named
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33539;&#24335;&#8212;&#8212;&#22522;&#20110;&#25552;&#31034;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;PromptCast&#65289;&#65292;&#23558;&#25968;&#23383;&#36755;&#20837;&#21644;&#36755;&#20986;&#36716;&#21270;&#20026;&#25552;&#31034;&#65292;&#24182;&#20197;&#21477;&#23376;&#21040;&#21477;&#23376;&#30340;&#26041;&#24335;&#25552;&#20986;&#39044;&#27979;&#20219;&#21153;&#65292;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2210.08964</link><description>&lt;p&gt;
PromptCast&#65306;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
PromptCast: A New Prompt-based Learning Paradigm for Time Series Forecasting. (arXiv:2210.08964v3 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08964
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33539;&#24335;&#8212;&#8212;&#22522;&#20110;&#25552;&#31034;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;PromptCast&#65289;&#65292;&#23558;&#25968;&#23383;&#36755;&#20837;&#21644;&#36755;&#20986;&#36716;&#21270;&#20026;&#25552;&#31034;&#65292;&#24182;&#20197;&#21477;&#23376;&#21040;&#21477;&#23376;&#30340;&#26041;&#24335;&#25552;&#20986;&#39044;&#27979;&#20219;&#21153;&#65292;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33539;&#24335;&#8212;&#8212;&#22522;&#20110;&#25552;&#31034;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;PromptCast&#65289;&#12290;&#22312;&#36825;&#31181;&#26032;&#30340;&#20219;&#21153;&#20013;&#65292;&#23558;&#21407;&#26469;&#30340;&#25968;&#23383;&#36755;&#20837;&#21644;&#36755;&#20986;&#36716;&#21270;&#20026;&#25552;&#31034;&#65292;&#24182;&#20197;&#21477;&#23376;&#21040;&#21477;&#23376;&#30340;&#26041;&#24335;&#25552;&#20986;&#39044;&#27979;&#20219;&#21153;&#65292;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#39044;&#27979;&#30340;&#30446;&#30340;&#12290;&#20026;&#20102;&#25903;&#25345;&#21644;&#20419;&#36827;&#36825;&#20010;&#20219;&#21153;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#65288;PISA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new perspective on time series forecasting. In existing time series forecasting methods, the models take a sequence of numerical values as input and yield numerical values as output. The existing SOTA models are largely based on the Transformer architecture, modified with multiple encoding mechanisms to incorporate the context and semantics around the historical data. Inspired by the successes of pre-trained language foundation models, we pose a question about whether these models can also be adapted to solve time-series forecasting. Thus, we propose a new forecasting paradigm: prompt-based time series forecasting (PromptCast). In this novel task, the numerical input and output are transformed into prompts and the forecasting task is framed in a sentence-to-sentence manner, making it possible to directly apply language models for forecasting purposes. To support and facilitate the research of this task, we also present a large-scale dataset (PISA) that includes th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#25513;&#33180;&#22810;&#27169;&#24577;&#24314;&#27169;&#26041;&#27861;&#65292;&#20197;&#23398;&#20064;&#32454;&#31890;&#24230;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#65292;&#36890;&#36807;&#38544;&#24335;&#21644;&#26174;&#24335;&#30446;&#26631;&#26469;&#24674;&#22797;&#32852;&#21512;&#25513;&#33180;&#20449;&#21495;&#20197;&#25552;&#39640;&#32454;&#21270;&#30340;&#22270;&#20687;-&#25991;&#26412;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2210.04183</link><description>&lt;p&gt;
MAMO&#65306;&#38754;&#21521;&#32454;&#31890;&#24230;&#35270;&#35273;-&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;&#30340;&#25513;&#33180;&#22810;&#27169;&#24577;&#24314;&#27169;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MAMO: Masked Multimodal Modeling for Fine-Grained Vision-Language Representation Learning. (arXiv:2210.04183v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#25513;&#33180;&#22810;&#27169;&#24577;&#24314;&#27169;&#26041;&#27861;&#65292;&#20197;&#23398;&#20064;&#32454;&#31890;&#24230;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#65292;&#36890;&#36807;&#38544;&#24335;&#21644;&#26174;&#24335;&#30446;&#26631;&#26469;&#24674;&#22797;&#32852;&#21512;&#25513;&#33180;&#20449;&#21495;&#20197;&#25552;&#39640;&#32454;&#21270;&#30340;&#22270;&#20687;-&#25991;&#26412;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#22312;&#21508;&#31181;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#22823;&#37096;&#20998;&#26041;&#27861;&#37117;&#20027;&#35201;&#33268;&#21147;&#20110;&#24314;&#31435;&#20840;&#23616;&#32423;&#21035;&#30340;&#22270;&#20687;&#19982;&#35821;&#35328;&#23545;&#40784;&#65292;&#32570;&#20047;&#26377;&#25928;&#30340;&#32454;&#31890;&#24230;&#22270;&#20687;-&#25991;&#26412;&#20132;&#20114;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#25513;&#33180;&#22810;&#27169;&#24577;&#24314;&#27169;&#26041;&#27861;&#65292;&#20197;&#23398;&#20064;&#32454;&#31890;&#24230;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#22270;&#20687;-&#25991;&#26412;&#36755;&#20837;&#36827;&#34892;&#32852;&#21512;&#25513;&#33180;&#65292;&#24182;&#38598;&#25104;&#20102;&#26174;&#24335;&#21644;&#38544;&#24335;&#30446;&#26631;&#26469;&#24674;&#22797;&#25513;&#33180;&#20449;&#21495;&#12290;&#20854;&#20013;&#65292;&#38544;&#24335;&#30446;&#26631;&#20026;&#35270;&#35273;&#21644;&#35821;&#35328;&#25552;&#20379;&#32479;&#19968;&#19988;&#26080;&#20559;&#24046;&#30340;&#30446;&#26631;&#65292;&#27169;&#22411;&#39044;&#27979;&#26410;&#25513;&#33180;&#36755;&#20837;&#30340;&#28508;&#22312;&#22810;&#27169;&#24577;&#34920;&#31034;&#65307;&#26174;&#24335;&#30446;&#26631;&#21017;&#36890;&#36807;&#24674;&#22797;&#22270;&#20687;&#22359;&#30340;&#21160;&#37327;&#35270;&#35273;&#29305;&#24449;&#21644;&#21333;&#35789;&#26631;&#35760;&#30340;&#27010;&#24565;&#65292;&#36827;&#19968;&#27493;&#20016;&#23500;&#22810;&#27169;&#24577;&#34920;&#31034;&#12290;&#36890;&#36807;&#36825;&#26679;&#30340;&#25513;&#33180;&#24314;&#27169;&#36807;&#31243;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#23398;&#20064;&#21040;&#32454;&#31890;&#24230;&#30340;&#22810;&#27169;&#24577;&#20132;&#20114;&#65292;&#36824;&#33021;&#23398;&#20064;&#21040;&#26377;&#24847;&#20041;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal representation learning has shown promising improvements on various vision-language tasks. Most existing methods excel at building global-level alignment between vision and language while lacking effective fine-grained image-text interaction. In this paper, we propose a jointly masked multimodal modeling method to learn fine-grained multimodal representations. Our method performs joint masking on image-text input and integrates both implicit and explicit targets for the masked signals to recover. The implicit target provides a unified and debiased objective for vision and language, where the model predicts latent multimodal representations of the unmasked input. The explicit target further enriches the multimodal representations by recovering high-level and semantically meaningful information: momentum visual features of image patches and concepts of word tokens. Through such a masked modeling process, our model not only learns fine-grained multimodal interaction, but also a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#36866;&#24212;&#30340;&#21160;&#24577;&#19978;&#19979;&#25991;&#35789;&#23884;&#20837;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#24863;&#30693;&#26102;&#38388;&#30340;&#27169;&#26495;&#29983;&#25104;&#25552;&#31034;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;MLM&#24471;&#21040;DCWE&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22256;&#24785;&#24230;&#21644;&#19979;&#28216;&#20219;&#21153;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2208.10734</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#26495;&#21270;&#26102;&#38388;&#36866;&#24212;&#30340;&#21160;&#24577;&#19978;&#19979;&#25991;&#35789;&#23884;&#20837;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning Dynamic Contextualised Word Embeddings via Template-based Temporal Adaptation. (arXiv:2208.10734v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10734
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#36866;&#24212;&#30340;&#21160;&#24577;&#19978;&#19979;&#25991;&#35789;&#23884;&#20837;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#24863;&#30693;&#26102;&#38388;&#30340;&#27169;&#26495;&#29983;&#25104;&#25552;&#31034;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;MLM&#24471;&#21040;DCWE&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22256;&#24785;&#24230;&#21644;&#19979;&#28216;&#20219;&#21153;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#19978;&#19979;&#25991;&#35789;&#23884;&#20837;&#65288;DCWE&#65289;&#34920;&#31034;&#21333;&#35789;&#30340;&#35821;&#20041;&#38543;&#26102;&#38388;&#30340;&#21464;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24863;&#30693;&#26102;&#38388;&#30340;&#27169;&#26495;&#23398;&#20064;&#39044;&#35757;&#32451;&#25513;&#34109;&#35821;&#35328;&#27169;&#22411;&#65288;MLM&#65289;&#24471;&#21040;DCWE&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#19968;&#31181;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#19982;$C_1$&#21644;$C_2$&#30456;&#20851;&#30340;&#8220;&#26530;&#36724;&#8221;&#65288;pivot&#65289;&#26415;&#35821;&#21644;&#22312;&#21508;&#20010;&#24555;&#29031;&#20013;&#19982;&#29305;&#23450;&#26530;&#36724;&#26415;&#35821;&#30456;&#20851;&#30340;&#8220;&#38170;&#8221;&#65288;anchor&#65289;&#26415;&#35821;&#26469;&#29983;&#25104;&#25552;&#31034;&#12290;&#28982;&#21518;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#23398;&#20064;&#26102;&#38388;&#25935;&#24863;&#30340;&#27169;&#26495;&#30340;&#26041;&#27861;&#65292;&#20174;$C_1$&#21644;$C_2$&#20013;&#36827;&#34892;&#23398;&#20064;&#65292;&#26080;&#38656;&#20219;&#20309;&#20154;&#24037;&#30417;&#30563;&#12290;&#25509;&#19979;&#26469;&#65292;&#26412;&#25991;&#20351;&#29992;&#36825;&#20123;&#29983;&#25104;&#30340;&#25552;&#31034;&#36890;&#36807;&#24494;&#35843;&#26469;&#36866;&#24212;&#39044;&#35757;&#32451;&#30340;MLM&#21040;$T_2$&#12290;&#22810;&#20010;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36229;&#36807;&#24378;&#22522;&#32447;9.2&#28857;&#30340;&#21516;&#26102;&#38477;&#20302;&#20102;$C_1$&#27979;&#35797;&#21477;&#23376;&#30340;&#22256;&#24785;&#24230;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#21160;&#24577;&#19978;&#19979;&#25991;&#35789;&#23884;&#20837;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#24773;&#24863;&#20998;&#26512;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31561;&#19979;&#28216;&#20219;&#21153;&#26159;&#26377;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic contextualised word embeddings (DCWEs) represent the temporal semantic variations of words. We propose a method for learning DCWEs by time-adapting a pretrained Masked Language Model (MLM) using time-sensitive templates. Given two snapshots $C_1$ and $C_2$ of a corpus taken respectively at two distinct timestamps $T_1$ and $T_2$, we first propose an unsupervised method to select (a) \emph{pivot} terms related to both $C_1$ and $C_2$, and (b) \emph{anchor} terms that are associated with a specific pivot term in each individual snapshot. We then generate prompts by filling manually compiled templates using the extracted pivot and anchor terms. Moreover, we propose an automatic method to learn time-sensitive templates from $C_1$ and $C_2$, without requiring any human supervision. Next, we use the generated prompts to adapt a pretrained MLM to $T_2$ by fine-tuning using those prompts. Multiple experiments show that our proposed method reduces the perplexity of test sentences in $C_
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;PLAtE&#30340;&#22823;&#35268;&#27169;&#21015;&#34920;&#39029;&#32593;&#32476;&#25277;&#21462;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20174;&#20135;&#21697;&#35780;&#35770;&#39029;&#38754;&#20013;&#25552;&#21462;&#21830;&#21697;&#21015;&#34920;&#21644;&#20135;&#21697;&#23646;&#24615;&#12290;&#25968;&#25454;&#38598;&#30001;52,898&#20010;&#39033;&#30446;&#21644;156,014&#20010;&#23646;&#24615;&#32452;&#25104;&#65292;&#26159;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#21015;&#34920;&#39029;&#32593;&#32476;&#25277;&#21462;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2205.12386</link><description>&lt;p&gt;
PLAtE: &#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#21015;&#34920;&#39029;&#32593;&#32476;&#25277;&#21462;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
PLAtE: A Large-scale Dataset for List Page Web Extraction. (arXiv:2205.12386v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12386
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;PLAtE&#30340;&#22823;&#35268;&#27169;&#21015;&#34920;&#39029;&#32593;&#32476;&#25277;&#21462;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20174;&#20135;&#21697;&#35780;&#35770;&#39029;&#38754;&#20013;&#25552;&#21462;&#21830;&#21697;&#21015;&#34920;&#21644;&#20135;&#21697;&#23646;&#24615;&#12290;&#25968;&#25454;&#38598;&#30001;52,898&#20010;&#39033;&#30446;&#21644;156,014&#20010;&#23646;&#24615;&#32452;&#25104;&#65292;&#26159;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#21015;&#34920;&#39029;&#32593;&#32476;&#25277;&#21462;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#31070;&#32463;&#27169;&#22411;&#34987;&#21033;&#29992;&#26469;&#26174;&#33879;&#25552;&#39640;&#20174;&#21322;&#32467;&#26500;&#21270;&#32593;&#31449;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#32487;&#32493;&#36827;&#27493;&#30340;&#38556;&#30861;&#26159;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#30340;&#25968;&#25454;&#38598;&#25968;&#37327;&#22826;&#23569;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; PLAtE &#65288;Pages of Lists Attribute Extraction&#65289;&#22522;&#20934;&#25968;&#25454;&#38598;&#20316;&#20026;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26032;&#32593;&#32476;&#25277;&#21462;&#20219;&#21153;&#12290;PLAtE &#20027;&#35201;&#20851;&#27880;&#36141;&#29289;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#20174;&#21253;&#21547;&#22810;&#20010;&#39033;&#30446;&#30340;&#20135;&#21697;&#35780;&#35770;&#39029;&#38754;&#20013;&#25552;&#21462;&#65292;&#21253;&#21547;&#20004;&#20010;&#20219;&#21153;&#65306;&#65288;1&#65289;&#26597;&#25214;&#20135;&#21697;&#21015;&#34920;&#20998;&#21106;&#36793;&#30028;&#21644;&#65288;2&#65289;&#25552;&#21462;&#27599;&#20010;&#20135;&#21697;&#30340;&#23646;&#24615;&#12290;PLAtE&#30001;&#26469;&#33258;6,694&#20010;&#39029;&#38754;&#30340;52,898&#20010;&#39033;&#30446;&#21644;156,014&#20010;&#23646;&#24615;&#32452;&#25104;&#65292;&#26159;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#21015;&#34920;&#39029;&#32593;&#32476;&#25277;&#21462;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#38454;&#27573;&#26041;&#27861;&#26469;&#25910;&#38598;&#21644;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#24182;&#23558;&#19977;&#20010;&#26368;&#20808;&#36827;&#30340;&#32593;&#32476;&#25277;&#21462;&#27169;&#22411;&#36866;&#24212;&#20110;&#20004;&#20010;&#20219;&#21153;&#65292;&#23450;&#37327;&#21644;&#23450;&#24615;&#27604;&#36739;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, neural models have been leveraged to significantly improve the performance of information extraction from semi-structured websites. However, a barrier for continued progress is the small number of datasets large enough to train these models. In this work, we introduce the PLAtE (Pages of Lists Attribute Extraction) benchmark dataset as a challenging new web extraction task. PLAtE focuses on shopping data, specifically extractions from product review pages with multiple items encompassing the tasks of: (1) finding product-list segmentation boundaries and (2) extracting attributes for each product. PLAtE is composed of 52, 898 items collected from 6, 694 pages and 156, 014 attributes, making it the first largescale list page web extraction dataset. We use a multi-stage approach to collect and annotate the dataset and adapt three state-of-the-art web extraction models to the two tasks comparing their strengths and weaknesses both quantitatively and qualitatively.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#20026;&#26399;3&#20010;&#26376;&#30340;&#27665;&#26063;&#23398;&#30740;&#31350;&#65292;&#30830;&#23450;&#20102;&#32593;&#32476;&#31038;&#21306;&#20013;&#20351;&#29992;&#30340;&#20845;&#31181;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#25552;&#31034;&#20462;&#39280;&#22120;&#30340;&#20998;&#31867;&#23398;&#65292;&#20026;&#30740;&#31350;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#23454;&#36341;&#25552;&#20379;&#20102;&#19968;&#20010;&#27010;&#24565;&#36215;&#28857;&#65292;&#24182;&#25552;&#20379;&#20102;AI&#29983;&#25104;&#33402;&#26415;&#30340;&#23454;&#36341;&#32773;&#25913;&#36827;&#22270;&#20687;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2204.13988</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#25552;&#31034;&#20462;&#39280;&#20998;&#31867;&#23398;
&lt;/p&gt;
&lt;p&gt;
A Taxonomy of Prompt Modifiers for Text-To-Image Generation. (arXiv:2204.13988v3 [cs.MM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.13988
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#20026;&#26399;3&#20010;&#26376;&#30340;&#27665;&#26063;&#23398;&#30740;&#31350;&#65292;&#30830;&#23450;&#20102;&#32593;&#32476;&#31038;&#21306;&#20013;&#20351;&#29992;&#30340;&#20845;&#31181;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#25552;&#31034;&#20462;&#39280;&#22120;&#30340;&#20998;&#31867;&#23398;&#65292;&#20026;&#30740;&#31350;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#23454;&#36341;&#25552;&#20379;&#20102;&#19968;&#20010;&#27010;&#24565;&#36215;&#28857;&#65292;&#24182;&#25552;&#20379;&#20102;AI&#29983;&#25104;&#33402;&#26415;&#30340;&#23454;&#36341;&#32773;&#25913;&#36827;&#22270;&#20687;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;2021&#24180;&#20197;&#26469;&#65292;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#29983;&#25104;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#26497;&#22823;&#20851;&#27880;&#12290;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#20174;&#25991;&#23383;&#36755;&#20837;&#65288;&#8220;&#25552;&#31034;&#8221;&#65289;&#20013;&#21512;&#25104;&#32654;&#20029;&#21644;&#24341;&#20154;&#20837;&#32988;&#30340;&#25968;&#23383;&#22270;&#20687;&#21644;&#33402;&#26415;&#21697;&#12290;&#22312;&#32447;&#31038;&#21306;&#22260;&#32469;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#21644;AI&#29983;&#25104;&#33402;&#26415;&#36805;&#36895;&#20986;&#29616;&#12290;&#26412;&#25991;&#22522;&#20110;&#20026;&#26399;3&#20010;&#26376;&#30340;&#27665;&#26063;&#23398;&#30740;&#31350;&#65292;&#30830;&#23450;&#20102;&#32593;&#32476;&#31038;&#21306;&#20013;&#23454;&#36341;&#32773;&#20351;&#29992;&#30340;&#20845;&#31181;&#25552;&#31034;&#20462;&#25913;&#22120;&#12290;&#36825;&#20010;&#26032;&#39062;&#30340;&#25552;&#31034;&#20462;&#39280;&#20998;&#31867;&#23398;&#20026;&#30740;&#31350;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#23454;&#36341;&#25552;&#20379;&#20102;&#19968;&#20010;&#27010;&#24565;&#36215;&#28857;&#65292;&#21516;&#26102;&#20063;&#21487;&#33021;&#24110;&#21161;AI&#29983;&#25104;&#33402;&#26415;&#30340;&#23454;&#36341;&#32773;&#25913;&#36827;&#20182;&#20204;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#27010;&#36848;&#20102;&#22914;&#20309;&#22312;&#8220;&#25552;&#31034;&#24037;&#31243;&#8221;&#23454;&#36341;&#20013;&#24212;&#29992;&#25552;&#31034;&#20462;&#39280;&#22120;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#31181;&#26032;&#21019;&#24847;&#23454;&#36341;&#22312;&#20154;&#26426;&#20132;&#20114;&#39046;&#22495;&#30340;&#30740;&#31350;&#26426;&#20250;&#12290;&#25991;&#31456;&#22312;&#20174;&#20154;&#24037;&#26234;&#33021;&#20132;&#20114;&#30340;&#35282;&#24230;&#35752;&#35770;&#25552;&#31034;&#24037;&#31243;&#30340;&#26356;&#24191;&#27867;&#24433;&#21709;&#21518;&#32467;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image generation has seen an explosion of interest since 2021. Today, beautiful and intriguing digital images and artworks can be synthesized from textual inputs ("prompts") with deep generative models. Online communities around text-to-image generation and AI generated art have quickly emerged. This paper identifies six types of prompt modifiers used by practitioners in the online community based on a 3-month ethnographic study. The novel taxonomy of prompt modifiers provides researchers a conceptual starting point for investigating the practice of text-to-image generation, but may also help practitioners of AI generated art improve their images. We further outline how prompt modifiers are applied in the practice of "prompt engineering." We discuss research opportunities of this novel creative practice in the field of Human-Computer Interaction (HCI). The paper concludes with a discussion of broader implications of prompt engineering from the perspective of Human-AI Interactio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#24320;&#21457;&#20986;&#19968;&#31181;&#26816;&#27979;&#24613;&#35786;&#31185;&#20998;&#35786;&#21069;&#36133;&#34880;&#30151;&#30340;&#27169;&#22411;&#65292;&#20854;&#24615;&#33021;&#20248;&#20110;&#26631;&#20934;&#36133;&#34880;&#30151;&#31579;&#26597;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2204.07657</link><description>&lt;p&gt;
&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#22312;&#24613;&#35786;&#31185;&#20998;&#35786;&#20013;&#26816;&#27979;&#36133;&#34880;&#30151;
&lt;/p&gt;
&lt;p&gt;
Detection of sepsis during emergency department triage using machine learning. (arXiv:2204.07657v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.07657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#24320;&#21457;&#20986;&#19968;&#31181;&#26816;&#27979;&#24613;&#35786;&#31185;&#20998;&#35786;&#21069;&#36133;&#34880;&#30151;&#30340;&#27169;&#22411;&#65292;&#20854;&#24615;&#33021;&#20248;&#20110;&#26631;&#20934;&#36133;&#34880;&#30151;&#31579;&#26597;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36523;&#20307;&#22120;&#23448;&#21151;&#33021;&#38556;&#30861;&#30340;&#36133;&#34880;&#30151;&#26159;&#20840;&#29699;&#27515;&#20129;&#21644;&#21361;&#37325;&#30142;&#30149;&#30340;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#27604;&#36739;&#26631;&#20934;&#36133;&#34880;&#30151;&#31579;&#26597;&#31639;&#27861;&#21644;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20998;&#35786;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#24613;&#35786;&#31185;&#20998;&#35786;&#21069;&#65288;&#26410;&#20351;&#29992;&#23454;&#39564;&#23460;&#35786;&#26029;&#65289;&#30340;&#36133;&#34880;&#30151;&#26816;&#27979;&#24615;&#33021;&#12290;&#30740;&#31350;&#24471;&#20986;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340; AUC &#20026; 0.9423&#65292;&#25935;&#24863;&#24615;&#20026; 71.09%&#12290;
&lt;/p&gt;
&lt;p&gt;
Sepsis is a life-threatening condition with organ dysfunction and is a leading cause of death and critical illness worldwide. Even a few hours of delay in the treatment of sepsis results in increased mortality. Early detection of sepsis during emergency department triage would allow early initiation of lab analysis, antibiotic administration, and other sepsis treatment protocols. The purpose of this study was to compare sepsis detection performance at ED triage (prior to the use of laboratory diagnostics) of the standard sepsis screening algorithm (SIRS with source of infection) and a machine learning algorithm trained on EHR triage data. A machine learning model (KATE Sepsis) was developed using patient encounters with triage data from 16participating hospitals. KATE Sepsis and standard screening were retrospectively evaluated on the adult population of 512,949 medical records. KATE Sepsis demonstrates an AUC of 0.9423 (0.9401 - 0.9441) with sensitivity of 71.09% (70.12% - 71.98%) and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20960;&#20010;AL&#21477;&#23376;&#26597;&#35810;&#35780;&#20272;&#20989;&#25968;&#65292;&#20851;&#27880;&#28508;&#22312;&#27491;&#38754;&#26631;&#35760;&#65292;&#24182;&#20351;&#29992;&#26356;&#22909;&#30340;&#25968;&#25454;&#39537;&#21160;&#30340;&#27491;&#24120;&#21270;&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#21270;NER&#27880;&#37322;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2111.03837</link><description>&lt;p&gt;
&#38598;&#20013;&#20851;&#27880;&#28508;&#22312;&#21629;&#21517;&#23454;&#20307;&#30340;&#20027;&#21160;&#26631;&#27880;&#33719;&#21462;
&lt;/p&gt;
&lt;p&gt;
Focusing on Potential Named Entities During Active Label Acquisition. (arXiv:2111.03837v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.03837
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20960;&#20010;AL&#21477;&#23376;&#26597;&#35810;&#35780;&#20272;&#20989;&#25968;&#65292;&#20851;&#27880;&#28508;&#22312;&#27491;&#38754;&#26631;&#35760;&#65292;&#24182;&#20351;&#29992;&#26356;&#22909;&#30340;&#25968;&#25454;&#39537;&#21160;&#30340;&#27491;&#24120;&#21270;&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#21270;NER&#27880;&#37322;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;(NER)&#26088;&#22312;&#35782;&#21035;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#21629;&#21517;&#23454;&#20307;&#30340;&#25552;&#21450;&#24182;&#23558;&#20854;&#20998;&#31867;&#21040;&#39044;&#23450;&#20041;&#30340;&#21629;&#21517;&#23454;&#20307;&#31867;&#21035;&#20013;&#12290;&#34429;&#28982;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26377;&#21161;&#20110;&#22312;NER&#20013;&#23454;&#29616;&#33391;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20294;&#35768;&#22810;&#29305;&#23450;&#39046;&#22495;&#30340;NER&#24212;&#29992;&#20173;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#12290;&#20027;&#21160;&#23398;&#20064;(AL)&#26159;&#35299;&#20915;&#26631;&#31614;&#33719;&#21462;&#38382;&#39064;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#24050;&#29992;&#20110;NER&#20219;&#21153;&#65292;&#20197;&#26368;&#23567;&#21270;&#27880;&#37322;&#25104;&#26412;&#32780;&#19981;&#29306;&#29298;&#27169;&#22411;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#26631;&#35760;&#30340;&#20005;&#37325;&#19981;&#22343;&#21248;&#31867;&#20998;&#24067;&#24341;&#20837;&#20102;&#35774;&#35745;&#26377;&#25928;&#30340;NER&#20027;&#21160;&#23398;&#20064;&#26597;&#35810;&#26041;&#27861;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;AL&#21477;&#23376;&#26597;&#35810;&#35780;&#20272;&#20989;&#25968;&#65292;&#26356;&#22810;&#20851;&#27880;&#28508;&#22312;&#30340;&#27491;&#38754;&#26631;&#35760;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#21477;&#23376;&#21644;&#26631;&#35760;&#25104;&#26412;&#35780;&#20272;&#31574;&#30053;&#26469;&#35780;&#20272;&#36825;&#20123;&#25552;&#35758;&#30340;&#20989;&#25968;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#26356;&#22909;&#30340;&#25968;&#25454;&#39537;&#21160;&#30340;&#27491;&#24120;&#21270;&#26041;&#27861;&#65292;&#20197;&#24809;&#32602;&#36807;&#38271;&#25110;&#36807;&#30701;&#30340;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Named entity recognition (NER) aims to identify mentions of named entities in an unstructured text and classify them into predefined named entity classes. While deep learning-based pre-trained language models help to achieve good predictive performances in NER, many domain-specific NER applications still call for a substantial amount of labeled data. Active learning (AL), a general framework for the label acquisition problem, has been used for NER tasks to minimize the annotation cost without sacrificing model performance. However, the heavily imbalanced class distribution of tokens introduces challenges in designing effective AL querying methods for NER. We propose several AL sentence query evaluation functions that pay more attention to potential positive tokens, and evaluate these proposed functions with both sentence-based and token-based cost evaluation strategies. We also propose a better data-driven normalization approach to penalize sentences that are too long or too short. Our
&lt;/p&gt;</description></item></channel></rss>