<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>EAGLE&#26159;&#19968;&#20010;&#26080;&#25439;&#21152;&#36895;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#27425;&#39030;&#23618;&#29305;&#24449;&#23618;&#38754;&#19978;&#33258;&#22238;&#24402;&#25512;&#29702;&#65292;&#24182;&#35299;&#20915;&#37319;&#26679;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#24555;3&#20493;&#30340;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.15077</link><description>&lt;p&gt;
EAGLE: &#25512;&#27979;&#37319;&#26679;&#38656;&#35201;&#37325;&#26032;&#24605;&#32771;&#29305;&#24449;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty. (arXiv:2401.15077v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15077
&lt;/p&gt;
&lt;p&gt;
EAGLE&#26159;&#19968;&#20010;&#26080;&#25439;&#21152;&#36895;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#27425;&#39030;&#23618;&#29305;&#24449;&#23618;&#38754;&#19978;&#33258;&#22238;&#24402;&#25512;&#29702;&#65292;&#24182;&#35299;&#20915;&#37319;&#26679;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#24555;3&#20493;&#30340;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#22238;&#24402;&#35299;&#30721;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#21464;&#24471;&#32791;&#26102;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;EAGLE&#65288;&#29992;&#20110;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#25928;&#29575;&#30340;&#22806;&#25512;&#31639;&#27861;&#65289;&#65292;&#23454;&#29616;&#20102;&#26080;&#25439;&#21152;&#36895;&#12290;&#19982;&#20256;&#32479;&#30340;&#25512;&#27979;&#37319;&#26679;&#26041;&#27861;&#19981;&#21516;&#65292;EAGLE&#22312;&#26356;&#35268;&#24459;&#30340;&#65288;&#27425;&#39030;&#23618;&#65289;&#29305;&#24449;&#23618;&#38754;&#19978;&#33258;&#22238;&#24402;&#36827;&#34892;&#32534;&#20889;&#65292;&#24182;&#36890;&#36807;&#25972;&#21512;&#25552;&#21069;&#19968;&#20010;&#26102;&#38388;&#27493;&#30340;&#26631;&#35760;&#26469;&#35299;&#20915;&#19979;&#19968;&#20010;&#29305;&#24449;&#39044;&#27979;&#38382;&#39064;&#20013;&#30340;&#37319;&#26679;&#19981;&#30830;&#23450;&#24615;&#12290;EAGLE&#25152;&#25552;&#20379;&#30340;&#21152;&#36895;&#26159;&#26080;&#25439;&#30340;&#65306;&#23427;&#19981;&#38656;&#35201;&#24494;&#35843;&#30446;&#26631;LLM&#65292;&#24182;&#19988;&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#21407;&#22987;&#30340;&#33258;&#22238;&#24402;&#35299;&#30721;&#30340;&#20998;&#24067;&#30456;&#21516;&#12290;&#25130;&#33267;&#26412;&#25991;&#25552;&#20132;&#26102;&#65292;EAGLE&#26159;&#24050;&#30693;&#25512;&#27979;&#37319;&#26679;&#23478;&#26063;&#20013;&#36895;&#24230;&#26368;&#24555;&#30340;&#26694;&#26550;&#12290;&#22312;MT-bench&#19978;&#65292;EAGLE&#27604;&#21407;&#22987;&#35299;&#30721;&#24555;3&#20493;&#65292;&#27604;Lookahead&#24555;2&#20493;&#65292;&#27604;Medusa&#24555;1.6&#20493;&#12290;&#20351;&#29992;gpt-fast&#65292;EAGLE&#24179;&#22343;&#27599;&#31186;&#36798;&#21040;160&#20010;&#26631;&#35760;&#19982;LLaMA2-Chat&#25645;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Auto-regressive decoding makes the inference of Large Language Models (LLMs) time-consuming. We propose a simple framework, EAGLE (Extrapolation Algorithm for Greater Language-model Efficiency), for lossless acceleration. Unlike traditional speculative sampling methods, EAGLE operates the drafting process auto-regressively at the more regular (second-top-layer) feature level and addresses the sampling uncertainty issues in the next-feature prediction problems by integrating tokens from one time step ahead. The acceleration provided by EAGLE is lossless: it involves no fine-tuning of the target LLM, and the generated text maintains the same distribution as that of vanilla auto-regressive decoding. As of the submission of this paper, EAGLE is the fastest known framework within the speculative sampling family. On MT-bench, EAGLE is 3x faster than vanilla decoding, 2x faster than Lookahead, and 1.6x faster than Medusa. Using gpt-fast, EAGLE attains on average 160 tokens/s with LLaMA2-Chat 
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#37197;&#23545;19&#19990;&#32426;&#32654;&#22269;&#25991;&#23398;&#20316;&#21697;&#20013;&#30340;&#27491;&#23383;&#24322;&#24418;&#35789;&#27719;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#32452;&#31070;&#32463;&#32534;&#36753;&#36317;&#31163;&#27169;&#22411;&#65292;&#24182;&#19982;&#22312;L2&#33521;&#35821;&#23398;&#20064;&#32773;&#30340;&#27491;&#23383;&#38169;&#35823;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;&#25991;&#23398;&#27491;&#23383;&#21464;&#24322;&#23545;&#23383;&#31526;&#20018;&#37197;&#23545;&#26041;&#27861;&#24102;&#26469;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.15068</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#32534;&#36753;&#36317;&#31163;&#27169;&#22411;&#23558;&#27491;&#23383;&#24322;&#24418;&#25991;&#23398;&#35789;&#27719;&#19982;&#26631;&#20934;&#23545;&#24212;&#35789;&#37197;&#23545;
&lt;/p&gt;
&lt;p&gt;
Pairing Orthographically Variant Literary Words to Standard Equivalents Using Neural Edit Distance Models. (arXiv:2401.15068v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15068
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#37197;&#23545;19&#19990;&#32426;&#32654;&#22269;&#25991;&#23398;&#20316;&#21697;&#20013;&#30340;&#27491;&#23383;&#24322;&#24418;&#35789;&#27719;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#32452;&#31070;&#32463;&#32534;&#36753;&#36317;&#31163;&#27169;&#22411;&#65292;&#24182;&#19982;&#22312;L2&#33521;&#35821;&#23398;&#20064;&#32773;&#30340;&#27491;&#23383;&#38169;&#35823;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;&#25991;&#23398;&#27491;&#23383;&#21464;&#24322;&#23545;&#23383;&#31526;&#20018;&#37197;&#23545;&#26041;&#27861;&#24102;&#26469;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35821;&#26009;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;19&#19990;&#32426;&#32654;&#22269;&#25991;&#23398;&#20316;&#21697;&#20013;&#30340;&#27491;&#23383;&#24322;&#24418;&#35789;&#27719;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#23545;&#24212;&#30340;&#8220;&#26631;&#20934;&#8221;&#35789;&#23545;&#30340;&#26631;&#27880;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#32452;&#31070;&#32463;&#32534;&#36753;&#36317;&#31163;&#27169;&#22411;&#65292;&#23558;&#36825;&#20123;&#21464;&#20307;&#19982;&#23427;&#20204;&#30340;&#26631;&#20934;&#24418;&#24335;&#36827;&#34892;&#37197;&#23545;&#65292;&#24182;&#23558;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#35757;&#32451;&#22312;L2&#33521;&#35821;&#23398;&#20064;&#32773;&#30340;&#27491;&#23383;&#38169;&#35823;&#35821;&#26009;&#24211;&#19978;&#30340;&#19968;&#32452;&#31070;&#32463;&#32534;&#36753;&#36317;&#31163;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#27604;&#36739;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#19981;&#21516;&#36127;&#35757;&#32451;&#26679;&#26412;&#29983;&#25104;&#31574;&#30053;&#19979;&#30340;&#30456;&#23545;&#24615;&#33021;&#65292;&#24182;&#23545;&#25991;&#23398;&#27491;&#23383;&#21464;&#24322;&#23545;&#23383;&#31526;&#20018;&#37197;&#23545;&#26041;&#27861;&#24102;&#26469;&#30340;&#29420;&#29305;&#25361;&#25112;&#25552;&#20986;&#20102;&#32467;&#35770;&#24615;&#30340;&#24847;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel corpus consisting of orthographically variant words found in works of 19th century U.S. literature annotated with their corresponding "standard" word pair. We train a set of neural edit distance models to pair these variants with their standard forms, and compare the performance of these models to the performance of a set of neural edit distance models trained on a corpus of orthographic errors made by L2 English learners. Finally, we analyze the relative performance of these models in the light of different negative training sample generation strategies, and offer concluding remarks on the unique challenge literary orthographic variation poses to string pairing methodologies.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30058;&#33540;&#25104;&#29087;&#30417;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#26816;&#27979;&#25104;&#29087;&#30058;&#33540;&#24182;&#36827;&#34892;&#37319;&#25688;&#12290;</title><link>http://arxiv.org/abs/2401.15055</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22797;&#26434;&#22330;&#26223;&#20013;&#30058;&#33540;&#20998;&#31867;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep learning-based approach for tomato classification in complex scenes. (arXiv:2401.15055v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15055
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30058;&#33540;&#25104;&#29087;&#30417;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#26816;&#27979;&#25104;&#29087;&#30058;&#33540;&#24182;&#36827;&#34892;&#37319;&#25688;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36861;&#36394;&#30058;&#33540;&#25104;&#29087;&#26159;&#19968;&#39033;&#32791;&#26102;&#19988;&#21171;&#21160;&#23494;&#38598;&#30340;&#20219;&#21153;&#12290;&#32467;&#21512;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#21487;&#20197;&#24110;&#21161;&#29992;&#25143;&#20248;&#21270;&#30417;&#27979;&#26893;&#29289;&#25104;&#29087;&#29366;&#24577;&#30340;&#36807;&#31243;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30058;&#33540;&#25104;&#29087;&#30417;&#27979;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22797;&#26434;&#22330;&#26223;&#12290;&#30446;&#26631;&#26159;&#21450;&#26102;&#26816;&#27979;&#25104;&#29087;&#30058;&#33540;&#24182;&#36827;&#34892;&#37319;&#25688;&#12290;&#35813;&#26041;&#27861;&#20998;&#20026;&#20004;&#20010;&#37096;&#20998;&#12290;&#39318;&#20808;&#65292;&#22330;&#26223;&#22270;&#20687;&#34987;&#20256;&#36755;&#21040;&#39044;&#22788;&#29702;&#23618;&#65292;&#35813;&#36807;&#31243;&#21487;&#26816;&#27979;&#21040;&#24863;&#20852;&#36259;&#21306;&#22495;&#65288;&#21253;&#21547;&#30058;&#33540;&#30340;&#22270;&#20687;&#21306;&#22495;&#65289;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#22270;&#20687;&#34987;&#29992;&#20316;&#25104;&#29087;&#24230;&#26816;&#27979;&#23618;&#30340;&#36755;&#20837;&#12290;&#35813;&#23618;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#31639;&#27861;&#65292;&#23545;&#20854;&#25552;&#20379;&#30340;&#30058;&#33540;&#32553;&#30053;&#22270;&#36827;&#34892;&#20998;&#31867;&#65292;&#20849;&#20998;&#20026;&#20116;&#20010;&#31867;&#21035;&#65306;&#32511;&#33394;&#12289;&#26131;&#30862;&#12289;&#31881;&#33394;&#12289;&#28129;&#32418;&#12289;&#25104;&#29087;&#32418;&#12290;&#23454;&#39564;&#22522;&#20110;&#20174;&#20114;&#32852;&#32593;&#19978;&#25910;&#38598;&#30340;&#22270;&#20687;&#65292;&#36890;&#36807;&#20351;&#29992;&#30058;&#33540;&#20851;&#38190;&#35789;&#36827;&#34892;&#25628;&#32034;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tracking ripening tomatoes is time consuming and labor intensive. Artificial intelligence technologies combined with those of computer vision can help users optimize the process of monitoring the ripening status of plants. To this end, we have proposed a tomato ripening monitoring approach based on deep learning in complex scenes. The objective is to detect mature tomatoes and harvest them in a timely manner. The proposed approach is declined in two parts. Firstly, the images of the scene are transmitted to the pre-processing layer. This process allows the detection of areas of interest (area of the image containing tomatoes). Then, these images are used as input to the maturity detection layer. This layer, based on a deep neural network learning algorithm, classifies the tomato thumbnails provided to it in one of the following five categories: green, brittle, pink, pale red, mature red. The experiments are based on images collected from the internet gathered through searches using tom
&lt;/p&gt;</description></item><item><title>LongFin&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#25991;&#26723;&#29702;&#35299;&#27169;&#22411;&#65292;&#33021;&#22815;&#22788;&#29702;&#38271;&#37329;&#34701;&#39046;&#22495;&#25991;&#26723;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#22312;&#22788;&#29702;&#38271;&#25991;&#26723;&#26102;&#30340;&#38480;&#21046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.15050</link><description>&lt;p&gt;
LongFin&#65306;&#19968;&#31181;&#38754;&#21521;&#38271;&#37329;&#34701;&#39046;&#22495;&#25991;&#26723;&#30340;&#22810;&#27169;&#24577;&#25991;&#26723;&#29702;&#35299;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LongFin: A Multimodal Document Understanding Model for Long Financial Domain Documents. (arXiv:2401.15050v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15050
&lt;/p&gt;
&lt;p&gt;
LongFin&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#25991;&#26723;&#29702;&#35299;&#27169;&#22411;&#65292;&#33021;&#22815;&#22788;&#29702;&#38271;&#37329;&#34701;&#39046;&#22495;&#25991;&#26723;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#22312;&#22788;&#29702;&#38271;&#25991;&#26723;&#26102;&#30340;&#38480;&#21046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;AI&#26159;&#19968;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#20854;&#19987;&#27880;&#20110;&#29702;&#35299;&#21644;&#20174;&#25195;&#25551;&#21644;&#25968;&#23383;&#25991;&#26723;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#20197;&#20351;&#26085;&#24120;&#19994;&#21153;&#25805;&#20316;&#26356;&#21152;&#39640;&#25928;&#12290;&#34429;&#28982;&#24050;&#32463;&#24341;&#20837;&#20102;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#26469;&#20419;&#36827;&#35757;&#32451;&#33021;&#22815;&#35299;&#26512;&#21644;&#25552;&#21462;&#21508;&#31181;&#25991;&#26723;&#31867;&#22411;&#65288;&#22914;&#25910;&#25454;&#21644;&#25195;&#25551;&#34920;&#21333;&#65289;&#20449;&#24687;&#30340;AI&#27169;&#22411;&#65292;&#20294;&#26159;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#37117;&#26080;&#27861;&#35299;&#20915;&#24037;&#19994;&#29615;&#22659;&#20013;&#20986;&#29616;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#29616;&#26377;&#25968;&#25454;&#38598;&#20027;&#35201;&#30001;&#21333;&#39029;&#32452;&#25104;&#30340;&#30701;&#25991;&#26723;&#26500;&#25104;&#65292;&#32780;&#29616;&#26377;&#27169;&#22411;&#21463;&#38480;&#20110;&#26368;&#22823;&#38271;&#24230;&#30340;&#38480;&#21046;&#65292;&#36890;&#24120;&#38480;&#21046;&#22312;512&#20010;&#26631;&#35760;&#12290;&#22240;&#27492;&#65292;&#22312;&#37329;&#34701;&#26381;&#21153;&#20013;&#23454;&#38469;&#24212;&#29992;&#36825;&#20123;&#26041;&#27861;&#20250;&#21463;&#21040;&#20005;&#37325;&#38480;&#21046;&#65292;&#22240;&#20026;&#25991;&#26723;&#21487;&#33021;&#36328;&#36234;&#22810;&#20010;&#39029;&#38754;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LongFin&#65292;&#19968;&#31181;&#33021;&#22815;&#32534;&#30721;&#22810;&#36798;4K&#20010;&#26631;&#35760;&#30340;&#22810;&#27169;&#24577;&#25991;&#26723;AI&#27169;&#22411;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;LongForms
&lt;/p&gt;
&lt;p&gt;
Document AI is a growing research field that focuses on the comprehension and extraction of information from scanned and digital documents to make everyday business operations more efficient. Numerous downstream tasks and datasets have been introduced to facilitate the training of AI models capable of parsing and extracting information from various document types such as receipts and scanned forms. Despite these advancements, both existing datasets and models fail to address critical challenges that arise in industrial contexts. Existing datasets primarily comprise short documents consisting of a single page, while existing models are constrained by a limited maximum length, often set at 512 tokens. Consequently, the practical application of these methods in financial services, where documents can span multiple pages, is severely impeded. To overcome these challenges, we introduce LongFin, a multimodal document AI model capable of encoding up to 4K tokens. We also propose the LongForms
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#20581;&#24247;&#25991;&#26412;&#31616;&#21270;&#30740;&#31350;&#30340;&#28040;&#21270;&#30284;&#30151;&#25945;&#32946;&#26448;&#26009;&#30340;&#27880;&#37322;&#35821;&#26009;&#24211;&#65292;&#24182;&#25506;&#32034;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21270;&#26041;&#27861;&#65292;&#21253;&#25324;&#24494;&#35843;&#12289;&#22686;&#24378;&#23398;&#20064;&#12289;&#22686;&#24378;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#12289;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.15043</link><description>&lt;p&gt;
&#20581;&#24247;&#25991;&#26412;&#31616;&#21270;&#65306;&#28040;&#21270;&#30284;&#30151;&#25945;&#32946;&#30340;&#27880;&#37322;&#35821;&#26009;&#24211;&#21644;&#22686;&#24378;&#23398;&#20064;&#30340;&#26032;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Health Text Simplification: An Annotated Corpus for Digestive Cancer Education and Novel Strategies for Reinforcement Learning. (arXiv:2401.15043v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15043
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#20581;&#24247;&#25991;&#26412;&#31616;&#21270;&#30740;&#31350;&#30340;&#28040;&#21270;&#30284;&#30151;&#25945;&#32946;&#26448;&#26009;&#30340;&#27880;&#37322;&#35821;&#26009;&#24211;&#65292;&#24182;&#25506;&#32034;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21270;&#26041;&#27861;&#65292;&#21253;&#25324;&#24494;&#35843;&#12289;&#22686;&#24378;&#23398;&#20064;&#12289;&#22686;&#24378;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#12289;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#20581;&#24247;&#25945;&#32946;&#26448;&#26009;&#30340;&#38405;&#35835;&#27700;&#24179;&#26174;&#33879;&#24433;&#21709;&#20449;&#24687;&#30340;&#21487;&#29702;&#35299;&#24615;&#21644;&#21487;&#25509;&#35302;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23569;&#25968;&#26063;&#35028;&#20154;&#32676;&#12290;&#35768;&#22810;&#24739;&#32773;&#25945;&#32946;&#36164;&#28304;&#36229;&#36807;&#20102;&#24191;&#27867;&#25509;&#21463;&#30340;&#26631;&#20934;&#30340;&#38405;&#35835;&#27700;&#24179;&#21644;&#22797;&#26434;&#24615;&#12290;&#22312;&#20581;&#24247;&#20449;&#24687;&#20013;&#65292;&#24613;&#38656;&#39640;&#24615;&#33021;&#30340;&#25991;&#26412;&#31616;&#21270;&#27169;&#22411;&#20197;&#22686;&#24378;&#20256;&#25773;&#21644;&#35782;&#23383;&#33021;&#21147;&#12290;&#36825;&#31181;&#38656;&#35201;&#22312;&#30284;&#30151;&#25945;&#32946;&#20013;&#23588;&#20026;&#36843;&#20999;&#65292;&#26377;&#25928;&#30340;&#39044;&#38450;&#21644;&#31579;&#26597;&#25945;&#32946;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#21457;&#30149;&#29575;&#21644;&#27515;&#20129;&#29575;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#24341;&#20837;&#20102;&#31616;&#21270;&#30340;&#28040;&#21270;&#30284;&#30151;&#65288;SimpleDC&#65289;&#24182;&#34892;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#20581;&#24247;&#25991;&#26412;&#31616;&#21270;&#30740;&#31350;&#12290;&#21033;&#29992;SimpleDC&#21644;&#29616;&#26377;&#30340;Med-EASi&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#31616;&#21270;&#26041;&#27861;&#65292;&#21253;&#25324;&#24494;&#35843;&#12289;&#22686;&#24378;&#23398;&#20064;&#65288;RL&#65289;&#12289;&#22686;&#24378;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#12289;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: The reading level of health educational materials significantly influences information understandability and accessibility, particularly for minoritized populations. Many patient educational resources surpass the reading level and complexity of widely accepted standards. There is a critical need for high-performing text simplification models in health information to enhance dissemination and literacy. This need is particularly acute in cancer education, where effective prevention and screening education can substantially reduce morbidity and mortality.  Methods: We introduce Simplified Digestive Cancer (SimpleDC), a parallel corpus of cancer education materials tailored for health text simplification research. Utilizing SimpleDC alongside the existing Med-EASi corpus, we explore Large Language Model (LLM)-based simplification methods, including fine-tuning, reinforcement learning (RL), reinforcement learning with human feedback (RLHF), domain adaptation, and prompt-based app
&lt;/p&gt;</description></item><item><title>PROXYQA&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38271;&#31687;&#25991;&#26412;&#29983;&#25104;&#30340;&#26367;&#20195;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#35814;&#23613;&#30340;&#20869;&#23481;&#65292;&#24182;&#21033;&#29992;&#35780;&#20272;&#22120;&#21644;&#29983;&#25104;&#20869;&#23481;&#20316;&#20026;&#32972;&#26223;&#29615;&#22659;&#65292;&#26681;&#25454;&#35780;&#20272;&#22120;&#22238;&#31572;&#20195;&#29702;&#38382;&#39064;&#30340;&#34920;&#29616;&#26469;&#35780;&#20272;&#29983;&#25104;&#20869;&#23481;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.15042</link><description>&lt;p&gt;
PROXYQA&#65306;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38271;&#31687;&#25991;&#26412;&#29983;&#25104;&#30340;&#26367;&#20195;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
PROXYQA: An Alternative Framework for Evaluating Long-Form Text Generation with Large Language Models. (arXiv:2401.15042v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15042
&lt;/p&gt;
&lt;p&gt;
PROXYQA&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38271;&#31687;&#25991;&#26412;&#29983;&#25104;&#30340;&#26367;&#20195;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#35814;&#23613;&#30340;&#20869;&#23481;&#65292;&#24182;&#21033;&#29992;&#35780;&#20272;&#22120;&#21644;&#29983;&#25104;&#20869;&#23481;&#20316;&#20026;&#32972;&#26223;&#29615;&#22659;&#65292;&#26681;&#25454;&#35780;&#20272;&#22120;&#22238;&#31572;&#20195;&#29702;&#38382;&#39064;&#30340;&#34920;&#29616;&#26469;&#35780;&#20272;&#29983;&#25104;&#20869;&#23481;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#38271;&#31687;&#25991;&#26412;&#29702;&#35299;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#29983;&#25104;&#38271;&#31687;&#20869;&#23481;&#65288;&#22914;&#25253;&#21578;&#21644;&#25991;&#31456;&#65289;&#30340;&#33021;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#24403;&#21069;&#30340;&#22522;&#20934;&#19981;&#36275;&#20197;&#20805;&#20998;&#35780;&#20272;LLMs&#29983;&#25104;&#20449;&#24687;&#20016;&#23500;&#19988;&#20840;&#38754;&#30340;&#20869;&#23481;&#65292;&#22240;&#27492;&#38656;&#35201;&#19968;&#31181;&#26356;&#20005;&#26684;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;\textsc{ProxyQA}&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#38271;&#31687;&#25991;&#26412;&#29983;&#25104;&#65292;&#21253;&#25324;&#28145;&#20837;&#20154;&#24037;&#31574;&#21010;&#30340;&#28085;&#30422;&#22810;&#20010;&#39046;&#22495;&#30340;&#8220;&#20803;&#38382;&#39064;&#8221;&#12290;&#27599;&#20010;&#20803;&#38382;&#39064;&#37117;&#21253;&#21547;&#30456;&#24212;&#30340;&#24102;&#27880;&#37322;&#31572;&#26696;&#30340;&#8220;&#20195;&#29702;&#38382;&#39064;&#8221;&#12290;LLMs&#34987;&#35201;&#27714;&#26681;&#25454;&#36825;&#20123;&#20803;&#38382;&#39064;&#29983;&#25104;&#35814;&#23613;&#30340;&#20869;&#23481;&#12290;&#21033;&#29992;&#35780;&#20272;&#22120;&#24182;&#23558;&#29983;&#25104;&#30340;&#20869;&#23481;&#20316;&#20026;&#32972;&#26223;&#29615;&#22659;&#65292;\textsc{ProxyQA}&#26681;&#25454;&#35780;&#20272;&#22120;&#22238;&#31572;&#8220;&#20195;&#29702;&#38382;&#39064;&#8221;&#30340;&#34920;&#29616;&#35780;&#20272;&#29983;&#25104;&#20869;&#23481;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#26816;&#39564;&#20102;&#22810;&#20010;LLMs&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have exhibited remarkable success in long-form context comprehension tasks. However, their capacity to generate long contents, such as reports and articles, remains insufficiently explored. Current benchmarks do not adequately assess LLMs' ability to produce informative and comprehensive content, necessitating a more rigorous evaluation approach. In this study, we introduce \textsc{ProxyQA}, a framework for evaluating long-form text generation, comprising in-depth human-curated \textit{meta-questions} spanning various domains. Each meta-question contains corresponding \textit{proxy-questions} with annotated answers. LLMs are prompted to generate extensive content in response to these meta-questions. Utilizing an evaluator and incorporating generated content as background context, \textsc{ProxyQA} evaluates the quality of generated content based on the evaluator's performance in answering the \textit{proxy-questions}. We examine multiple LLMs, emphasizing \t
&lt;/p&gt;</description></item><item><title>SliceGPT&#26159;&#19968;&#31181;&#26032;&#30340;&#20107;&#21518;&#35757;&#32451;&#31232;&#30095;&#21270;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#26435;&#37325;&#30697;&#38453;&#26367;&#25442;&#20026;&#36739;&#23567;&#30340;&#30697;&#38453;&#20197;&#20943;&#23567;&#32593;&#32476;&#30340;&#32500;&#24230;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#39640;&#20219;&#21153;&#24615;&#33021;&#30340;&#21516;&#26102;&#20943;&#23569;&#27169;&#22411;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2401.15024</link><description>&lt;p&gt;
SliceGPT: &#36890;&#36807;&#21024;&#38500;&#34892;&#21644;&#21015;&#26469;&#21387;&#32553;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SliceGPT: Compress Large Language Models by Deleting Rows and Columns. (arXiv:2401.15024v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15024
&lt;/p&gt;
&lt;p&gt;
SliceGPT&#26159;&#19968;&#31181;&#26032;&#30340;&#20107;&#21518;&#35757;&#32451;&#31232;&#30095;&#21270;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#26435;&#37325;&#30697;&#38453;&#26367;&#25442;&#20026;&#36739;&#23567;&#30340;&#30697;&#38453;&#20197;&#20943;&#23567;&#32593;&#32476;&#30340;&#32500;&#24230;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#39640;&#20219;&#21153;&#24615;&#33021;&#30340;&#21516;&#26102;&#20943;&#23569;&#27169;&#22411;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#22522;&#30707;&#65292;&#20294;&#20351;&#29992;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#21644;&#20869;&#23384;&#36164;&#28304;&#12290;&#31232;&#30095;&#21270;&#26041;&#27861;&#21487;&#20197;&#32531;&#35299;&#36825;&#20123;&#36164;&#28304;&#38480;&#21046;&#65292;&#24182;&#19988;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#21487;&#20197;&#36827;&#34892;&#20107;&#21518;&#30340;&#31232;&#30095;&#21270;&#22788;&#29702;&#12290;&#29616;&#26377;&#30340;&#31232;&#30095;&#21270;&#25216;&#26415;&#38754;&#20020;&#30528;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#39069;&#22806;&#30340;&#25968;&#25454;&#32467;&#26500;&#65292;&#24182;&#19988;&#22312;&#24403;&#21069;&#30828;&#20214;&#19978;&#36895;&#24230;&#21463;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20107;&#21518;&#35757;&#32451;&#31232;&#30095;&#21270;&#26041;&#26696;SliceGPT&#65292;&#35813;&#26041;&#26696;&#29992;&#36739;&#23567;&#30340;&#65288;&#31264;&#23494;&#30340;&#65289;&#30697;&#38453;&#26367;&#25442;&#27599;&#20010;&#26435;&#37325;&#30697;&#38453;&#65292;&#20174;&#32780;&#20943;&#23567;&#32593;&#32476;&#30340;&#23884;&#20837;&#32500;&#24230;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SliceGPT&#22312;&#20445;&#25345;&#30456;&#24212;&#31264;&#23494;&#27169;&#22411;&#30340;99%&#12289;99%&#21644;90%&#30340;&#38646;-shot&#20219;&#21153;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#21487;&#20197;&#31227;&#38500;LLAMA2-70B&#12289;OPT 66B&#21644;Phi-2&#27169;&#22411;&#20013;&#22810;&#36798;25%&#30340;&#27169;&#22411;&#21442;&#25968;&#65288;&#21253;&#25324;&#23884;&#20837;&#65289;&#12290;&#25105;&#20204;&#30340;&#20999;&#29255;&#27169;&#22411;&#22312;&#36739;&#23569;&#30340;GPU&#19978;&#36816;&#34892;&#24182;&#19988;&#26356;&#24555;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#20195;&#30721;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have become the cornerstone of natural language processing, but their use comes with substantial costs in terms of compute and memory resources. Sparsification provides a solution to alleviate these resource constraints, and recent works have shown that trained models can be sparsified post-hoc. Existing sparsification techniques face challenges as they need additional data structures and offer constrained speedup with current hardware. In this paper we present SliceGPT, a new post-training sparsification scheme which replaces each weight matrix with a smaller (dense) matrix, reducing the embedding dimension of the network. Through extensive experimentation, we show that SliceGPT can remove up to 25% of the model parameters (including embeddings) for LLAMA2-70B, OPT 66B and Phi-2 models while maintaining 99%, 99% and 90% zero-shot task performance of the dense model respectively. Our sliced models run on fewer GPUs and run faster without any additional code optimi
&lt;/p&gt;</description></item><item><title>"Airavata"&#26159;&#19968;&#20010;&#38024;&#23545;&#21360;&#22320;&#35821;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#30340;LLM&#65292;&#36890;&#36807;&#24494;&#35843;OpenHathi&#21644;IndicInstruct&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#26356;&#22909;&#30340;&#21327;&#21161;&#20219;&#21153;&#24615;&#33021;&#65292;&#24182;&#35745;&#21010;&#25193;&#23637;&#21040;&#25152;&#26377;22&#31181;&#35745;&#21010;Indic&#35821;&#35328;&#12290;</title><link>http://arxiv.org/abs/2401.15006</link><description>&lt;p&gt;
Airavata: &#24341;&#20837;&#38024;&#23545;&#21360;&#22320;&#35821;&#25351;&#20196;&#35843;&#25972;&#30340;LLM
&lt;/p&gt;
&lt;p&gt;
Airavata: Introducing Hindi Instruction-tuned LLM. (arXiv:2401.15006v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15006
&lt;/p&gt;
&lt;p&gt;
"Airavata"&#26159;&#19968;&#20010;&#38024;&#23545;&#21360;&#22320;&#35821;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#30340;LLM&#65292;&#36890;&#36807;&#24494;&#35843;OpenHathi&#21644;IndicInstruct&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#26356;&#22909;&#30340;&#21327;&#21161;&#20219;&#21153;&#24615;&#33021;&#65292;&#24182;&#35745;&#21010;&#25193;&#23637;&#21040;&#25152;&#26377;22&#31181;&#35745;&#21010;Indic&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23459;&#24067;&#39318;&#27425;&#21457;&#24067;&#20102;"Airavata"&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#21360;&#22320;&#35821;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#30340;LLM&#12290;&#36890;&#36807;&#23558;OpenHathi&#19982;&#21508;&#31181;&#25351;&#20196;&#35843;&#25972;&#30340;&#21360;&#22320;&#35821;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;Airavata&#26356;&#36866;&#21512;&#36741;&#21161;&#20219;&#21153;&#12290;&#38500;&#20102;&#27169;&#22411;&#22806;&#65292;&#25105;&#20204;&#36824;&#20998;&#20139;&#20102;IndicInstruct&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#32452;&#29992;&#20110;&#36827;&#19968;&#27493;&#30740;&#31350;Indic LLM&#30340;&#22810;&#26679;&#21270;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#35780;&#20272;&#22522;&#20934;&#21644;&#35780;&#20272;&#26694;&#26550;&#65292;&#20197;&#35780;&#20272;LLM&#22312;&#21360;&#22320;&#35821;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#30446;&#21069;&#65292;Airavata&#25903;&#25345;&#21360;&#22320;&#35821;&#65292;&#20294;&#25105;&#20204;&#35745;&#21010;&#23558;&#20854;&#25193;&#23637;&#21040;&#25152;&#26377;22&#31181;&#35745;&#21010;Indic&#35821;&#35328;&#12290;&#24744;&#21487;&#20197;&#22312;https://ai4bharat.github.io/airavata&#19978;&#35775;&#38382;&#25152;&#26377;&#24037;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
We announce the initial release of "Airavata," an instruction-tuned LLM for Hindi. Airavata was created by fine-tuning OpenHathi with diverse, instruction-tuning Hindi datasets to make it better suited for assistive tasks. Along with the model, we also share the IndicInstruct dataset, which is a collection of diverse instruction-tuning datasets to enable further research for Indic LLMs. Additionally, we present evaluation benchmarks and a framework for assessing LLM performance across tasks in Hindi. Currently, Airavata supports Hindi, but we plan to expand this to all 22 scheduled Indic languages. You can access all artifacts at https://ai4bharat.github.io/airavata.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;JetBrains IDE&#20013;&#23454;&#26045;&#30340;&#22522;&#20110;&#23884;&#20837;&#24335;&#30340;&#25628;&#32034;&#21151;&#33021;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#25552;&#39640;&#20102;&#25628;&#32034;&#39033;&#30340;&#21487;&#21457;&#29616;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.14975</link><description>&lt;p&gt;
&#22522;&#20110;&#23884;&#20837;&#24335;&#30340;JetBrains IDE&#30340;&#25628;&#32034;&#21151;&#33021;
&lt;/p&gt;
&lt;p&gt;
Embedding-based search in JetBrains IDEs. (arXiv:2401.14975v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14975
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;JetBrains IDE&#20013;&#23454;&#26045;&#30340;&#22522;&#20110;&#23884;&#20837;&#24335;&#30340;&#25628;&#32034;&#21151;&#33021;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#25552;&#39640;&#20102;&#25628;&#32034;&#39033;&#30340;&#21487;&#21457;&#29616;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#20195;&#38598;&#25104;&#24320;&#21457;&#29615;&#22659;&#65288;IDE&#65289;&#21644;&#20195;&#30721;&#32534;&#36753;&#22120;&#37117;&#20855;&#26377;&#36890;&#36807;&#21487;&#29992;&#21151;&#33021;&#21644;&#39033;&#30446;&#20013;&#30340;&#39033;&#36827;&#34892;&#25628;&#32034;&#30340;&#21151;&#33021;&#12290;&#22312;JetBrains IDE&#20013;&#65292;&#36825;&#20010;&#21151;&#33021;&#31216;&#20026;"&#25628;&#32034;&#20840;&#37096;"&#65306;&#23427;&#20801;&#35768;&#29992;&#25143;&#20174;&#19968;&#20010;&#20837;&#21475;&#28857;&#25628;&#32034;&#25991;&#20214;&#12289;&#25805;&#20316;&#12289;&#31867;&#12289;&#31526;&#21495;&#12289;&#35774;&#32622;&#20197;&#21450;&#29256;&#26412;&#25511;&#21046;&#31995;&#32479;(VCS)&#21382;&#21490;&#35760;&#24405;&#20013;&#30340;&#20219;&#20309;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#23427;&#20165;&#36890;&#36807;&#19981;&#32771;&#34385;&#35821;&#20041;&#30340;&#31639;&#27861;&#33719;&#21462;&#20505;&#36873;&#39033;&#65292;&#27604;&#22914;&#21516;&#20041;&#35789;&#12289;&#22797;&#26434;&#30340;&#21333;&#35789;&#25490;&#21015;&#12289;&#35789;&#24615;&#20462;&#25913;&#21644;&#25340;&#20889;&#38169;&#35823;&#31561;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#25105;&#20204;&#23454;&#26045;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#25628;&#32034;&#39033;&#30340;&#21487;&#21457;&#29616;&#24615;&#12290;&#25105;&#20204;&#36824;&#20998;&#20139;&#20102;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#38556;&#30861;&#20197;&#21450;&#22914;&#20309;&#20811;&#26381;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most modern Integrated Development Environments (IDEs) and code editors have a feature to search across available functionality and items in an open project. In JetBrains IDEs, this feature is called Search Everywhere: it allows users to search for files, actions, classes, symbols, settings, and anything from VCS history from a single entry point. However, it works with the candidates obtained by algorithms that don't account for semantics, e.g., synonyms, complex word permutations, part of the speech modifications, and typos. In this work, we describe the machine learning approach we implemented to improve the discoverability of search items. We also share the obstacles encountered during this process and how we overcame them.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#29992;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#35760;&#24518;&#20102;&#24050;&#30693;&#26412;&#20307;&#35770;&#30340;&#20449;&#24687;&#20197;&#21450;&#35760;&#24518;&#30340;&#31243;&#24230;&#65292;&#32467;&#26524;&#26174;&#31034;LLMs&#37096;&#20998;&#22320;&#20102;&#35299;&#26412;&#20307;&#35770;&#30340;&#27010;&#24565;&#65292;&#35760;&#24518;&#31243;&#24230;&#19982;&#20854;&#22312;Web&#19978;&#30340;&#27969;&#34892;&#31243;&#24230;&#25104;&#27491;&#27604;&#12290;</title><link>http://arxiv.org/abs/2401.14931</link><description>&lt;p&gt;
LLM&#26159;&#21542;&#33021;&#35760;&#24518;&#26412;&#20307;&#35770;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do LLMs Dream of Ontologies?. (arXiv:2401.14931v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14931
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#29992;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#35760;&#24518;&#20102;&#24050;&#30693;&#26412;&#20307;&#35770;&#30340;&#20449;&#24687;&#20197;&#21450;&#35760;&#24518;&#30340;&#31243;&#24230;&#65292;&#32467;&#26524;&#26174;&#31034;LLMs&#37096;&#20998;&#22320;&#20102;&#35299;&#26412;&#20307;&#35770;&#30340;&#27010;&#24565;&#65292;&#35760;&#24518;&#31243;&#24230;&#19982;&#20854;&#22312;Web&#19978;&#30340;&#27969;&#34892;&#31243;&#24230;&#25104;&#27491;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26368;&#36817;&#22312;&#33258;&#21160;&#25991;&#26412;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#36827;&#23637;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#20381;&#36182;&#20110;&#24213;&#23618;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#30340;&#21442;&#25968;&#25968;&#37327;&#65292;&#36825;&#20351;&#24471;LLMs&#33021;&#22815;&#35760;&#24518;&#35757;&#32451;&#36807;&#31243;&#20013;&#25509;&#35302;&#21040;&#30340;&#22823;&#37327;&#25968;&#25454;&#30340;&#19968;&#37096;&#20998;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#29992;&#39044;&#35757;&#32451;LLMs&#26159;&#21542;&#35760;&#24518;&#20102;&#24050;&#30693;&#26412;&#20307;&#35770;&#30340;&#20449;&#24687;&#20197;&#21450;&#35760;&#24518;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#37096;&#20998;&#22320;&#20102;&#35299;&#26412;&#20307;&#35770;&#65306;&#23427;&#20204;&#21487;&#20197;&#35760;&#24518;&#25991;&#26412;&#20013;&#25552;&#21040;&#30340;&#26412;&#20307;&#35770;&#27010;&#24565;&#65292;&#20294;&#20854;&#23545;&#27010;&#24565;&#30340;&#35760;&#24518;&#31243;&#24230;&#20284;&#20046;&#19982;&#20854;&#22312;Web&#19978;&#30340;&#27969;&#34892;&#31243;&#24230;&#25104;&#27604;&#20363;&#21464;&#21270;&#65292;&#22240;&#20026;Web&#26159;&#23427;&#20204;&#35757;&#32451;&#26448;&#26009;&#30340;&#20027;&#35201;&#26469;&#28304;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#36890;&#36807;&#27979;&#37327;&#19981;&#21516;&#25552;&#31034;&#37325;&#22797;&#12289;&#26597;&#35810;&#35821;&#35328;&#21644;&#30830;&#23450;&#24230;&#30340;&#36755;&#20986;&#19968;&#33268;&#24615;&#26469;&#20272;&#35745;LLMs&#23545;&#26412;&#20307;&#35770;&#20449;&#24687;&#30340;&#35760;&#24518;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently revolutionized automated text understanding and generation. The performance of these models relies on the high number of parameters of the underlying neural architectures, which allows LLMs to memorize part of the vast quantity of data seen during the training. This paper investigates whether and to what extent general-purpose pre-trained LLMs have memorized information from known ontologies. Our results show that LLMs partially know ontologies: they can, and do indeed, memorize concepts from ontologies mentioned in the text, but the level of memorization of their concepts seems to vary proportionally to their popularity on the Web, the primary source of their training material. We additionally propose new metrics to estimate the degree of memorization of ontological information in LLMs by measuring the consistency of the output produced across different prompt repetitions, query languages, and degrees of determinism.
&lt;/p&gt;</description></item><item><title>&#20420;&#32599;&#26031;&#35821;&#21644;&#33521;&#35821;&#30340;&#20803;&#38899;&#38899;&#32032;&#21442;&#25968;&#36827;&#34892;&#27604;&#36739;&#65292;&#20026;&#22810;&#35821;&#31181;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#25552;&#20379;&#36890;&#29992;&#27169;&#22411;&#65292;&#20934;&#30830;&#35782;&#21035;&#19981;&#30830;&#23450;&#35821;&#35328;&#30340;&#35821;&#38899;&#12290;</title><link>http://arxiv.org/abs/2401.14890</link><description>&lt;p&gt;
&#20420;&#32599;&#26031;&#35821;&#21644;&#33521;&#35821;&#20803;&#38899;&#38899;&#32032;&#21442;&#25968;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Comparison of parameters of vowel sounds of russian and english languages. (arXiv:2401.14890v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14890
&lt;/p&gt;
&lt;p&gt;
&#20420;&#32599;&#26031;&#35821;&#21644;&#33521;&#35821;&#30340;&#20803;&#38899;&#38899;&#32032;&#21442;&#25968;&#36827;&#34892;&#27604;&#36739;&#65292;&#20026;&#22810;&#35821;&#31181;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#25552;&#20379;&#36890;&#29992;&#27169;&#22411;&#65292;&#20934;&#30830;&#35782;&#21035;&#19981;&#30830;&#23450;&#35821;&#35328;&#30340;&#35821;&#38899;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#35821;&#31181;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20013;&#65292;&#36890;&#24120;&#20250;&#20986;&#29616;&#19968;&#31181;&#24773;&#20917;&#65292;&#21363;&#35821;&#35328;&#20107;&#20808;&#19981;&#30693;&#36947;&#65292;&#20294;&#20449;&#21495;&#24050;&#32463;&#25509;&#25910;&#24182;&#27491;&#22312;&#22788;&#29702;&#20013;&#12290;&#38024;&#23545;&#36825;&#31181;&#24773;&#20917;&#65292;&#38656;&#35201;&#19968;&#31181;&#24191;&#20041;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#23545;&#35821;&#38899;&#30340;&#38899;&#32032;&#24046;&#24322;&#20316;&#20986;&#21709;&#24212;&#65292;&#24182;&#26681;&#25454;&#36825;&#20123;&#24046;&#24322;&#27491;&#30830;&#22320;&#35782;&#21035;&#25152;&#38656;&#35821;&#35328;&#30340;&#35821;&#38899;&#12290;&#20026;&#20102;&#26500;&#24314;&#36825;&#26679;&#19968;&#20010;&#27169;&#22411;&#65292;&#38656;&#35201;&#35774;&#23450;&#38899;&#32032;&#21442;&#25968;&#30340;&#20540;&#65292;&#28982;&#21518;&#27604;&#36739;&#31867;&#20284;&#30340;&#38899;&#32032;&#65292;&#30830;&#23450;&#37325;&#35201;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
In multilingual speech recognition systems, a situation can often arise when the language is not known in advance, but the signal has already been received and is being processed. For such cases, some generalized model is needed that will be able to respond to phonetic differences and, depending on them, correctly recog-nize speech in the desired language. To build such a model, it is necessary to set the values of phonetic parameters, and then compare similar sounds, establishing significant differences.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#21644;&#35780;&#20272;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#31995;&#32479;&#20013;&#30340;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#32452;&#20214;&#65292;&#22635;&#34917;&#20102;&#30446;&#21069;&#30740;&#31350;&#20013;&#24573;&#35270;&#30340;&#39046;&#22495;&#65292;&#22312;&#26377;&#25928;&#30340;RAG&#30340;&#25552;&#31034;&#34920;&#36848;&#20013;&#65292;&#19981;&#30456;&#20851;&#25991;&#26723;&#30340;&#21253;&#21547;&#21487;&#33021;&#20250;&#23545;&#31995;&#32479;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.14887</link><description>&lt;p&gt;
&#22122;&#22768;&#30340;&#21147;&#37327;&#65306;&#37325;&#26032;&#23450;&#20041;RAG&#31995;&#32479;&#30340;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
The Power of Noise: Redefining Retrieval for RAG Systems. (arXiv:2401.14887v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#21644;&#35780;&#20272;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#31995;&#32479;&#20013;&#30340;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#32452;&#20214;&#65292;&#22635;&#34917;&#20102;&#30446;&#21069;&#30740;&#31350;&#20013;&#24573;&#35270;&#30340;&#39046;&#22495;&#65292;&#22312;&#26377;&#25928;&#30340;RAG&#30340;&#25552;&#31034;&#34920;&#36848;&#20013;&#65292;&#19981;&#30456;&#20851;&#25991;&#26723;&#30340;&#21253;&#21547;&#21487;&#33021;&#20250;&#23545;&#31995;&#32479;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#31995;&#32479;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20195;&#34920;&#20102;&#19968;&#20010;&#37325;&#22823;&#36827;&#27493;&#12290;RAG&#31995;&#32479;&#36890;&#36807;&#25972;&#21512;&#36890;&#36807;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#38454;&#27573;&#26816;&#32034;&#30340;&#22806;&#37096;&#25968;&#25454;&#26469;&#22686;&#24378;&#20854;&#29983;&#25104;&#33021;&#21147;&#65292;&#20811;&#26381;&#20102;&#26631;&#20934;LLMs&#30340;&#38480;&#21046;&#65292;&#21518;&#32773;&#20165;&#38480;&#20110;&#20854;&#39044;&#20808;&#35757;&#32451;&#30340;&#30693;&#35782;&#21644;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#12290;&#36825;&#20010;&#39046;&#22495;&#30340;&#22823;&#37096;&#20998;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;RAG&#31995;&#32479;&#20869;LLMs&#30340;&#29983;&#25104;&#26041;&#38754;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#65292;&#36890;&#36807;&#20840;&#38754;&#32780;&#25209;&#21028;&#24615;&#22320;&#20998;&#26512;IR&#32452;&#20214;&#23545;RAG&#31995;&#32479;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#19968;&#20010;&#26816;&#32034;&#22120;&#22312;&#26377;&#25928;&#30340;RAG&#30340;&#25552;&#31034;&#34920;&#36848;&#20013;&#24212;&#35813;&#20855;&#22791;&#30340;&#29305;&#24449;&#65292;&#37325;&#28857;&#20851;&#27880;&#24212;&#35813;&#26816;&#32034;&#21738;&#31181;&#31867;&#22411;&#30340;&#25991;&#26723;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181;&#22240;&#32032;&#65292;&#22914;&#25991;&#26723;&#19982;&#25552;&#31034;&#30340;&#30456;&#20851;&#24615;&#65292;&#23427;&#20204;&#30340;&#20301;&#32622;&#20197;&#21450;&#19978;&#19979;&#25991;&#20013;&#21253;&#21547;&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20986;&#65292;&#21253;&#21547;&#19981;&#30456;&#20851;&#30340;&#25991;&#26723;&#21487;&#33021;&#20250;&#8230;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Augmented Generation (RAG) systems represent a significant advancement over traditional Large Language Models (LLMs). RAG systems enhance their generation ability by incorporating external data retrieved through an Information Retrieval (IR) phase, overcoming the limitations of standard LLMs, which are restricted to their pre-trained knowledge and limited context window. Most research in this area has predominantly concentrated on the generative aspect of LLMs within RAG systems. Our study fills this gap by thoroughly and critically analyzing the influence of IR components on RAG systems. This paper analyzes which characteristics a retriever should possess for an effective RAG's prompt formulation, focusing on the type of documents that should be retrieved. We evaluate various elements, such as the relevance of the documents to the prompt, their position, and the number included in the context. Our findings reveal, among other insights, that including irrelevant documents can
&lt;/p&gt;</description></item><item><title>F-Eval&#26159;&#19968;&#20010;&#21452;&#35821;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#26412;&#33021;&#21147;&#65292;&#21253;&#25324;&#34920;&#36798;&#12289;&#24120;&#35782;&#21644;&#36923;&#36753;&#12290;&#23427;&#37319;&#29992;&#22810;&#31181;&#20219;&#21153;&#24418;&#24335;&#36827;&#34892;&#35780;&#20272;&#65292;&#21253;&#25324;&#23458;&#35266;&#20219;&#21153;&#21644;&#20027;&#35266;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#26469;&#35299;&#20915;&#26080;&#21442;&#32771;&#30340;&#20027;&#35266;&#20219;&#21153;&#35780;&#20272;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.14869</link><description>&lt;p&gt;
F-Eval:&#20351;&#29992;&#20248;&#21270;&#30340;&#35780;&#20272;&#26041;&#27861;&#35780;&#20272;&#22522;&#26412;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
F-Eval: Asssessing Fundamental Abilities with Refined Evaluation Methods. (arXiv:2401.14869v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14869
&lt;/p&gt;
&lt;p&gt;
F-Eval&#26159;&#19968;&#20010;&#21452;&#35821;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#26412;&#33021;&#21147;&#65292;&#21253;&#25324;&#34920;&#36798;&#12289;&#24120;&#35782;&#21644;&#36923;&#36753;&#12290;&#23427;&#37319;&#29992;&#22810;&#31181;&#20219;&#21153;&#24418;&#24335;&#36827;&#34892;&#35780;&#20272;&#65292;&#21253;&#25324;&#23458;&#35266;&#20219;&#21153;&#21644;&#20027;&#35266;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#26469;&#35299;&#20915;&#26080;&#21442;&#32771;&#30340;&#20027;&#35266;&#20219;&#21153;&#35780;&#20272;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22240;&#20854;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#65292;&#23548;&#33268;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#35780;&#20272;LLMs&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35780;&#20272;&#22522;&#20934;&#20165;&#38480;&#20110;&#35780;&#20272;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#65292;&#24573;&#35270;&#20102;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#20986;&#29616;&#30340;&#22522;&#26412;&#33021;&#21147;&#12290;&#20808;&#21069;&#30340;&#20027;&#35266;&#35780;&#20272;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#30001;API&#27169;&#22411;&#35780;&#20998;&#12290;&#28982;&#32780;&#65292;&#22312;&#27809;&#26377;&#21442;&#32771;&#25991;&#29486;&#30340;&#24773;&#20917;&#19979;&#65292;&#22823;&#27169;&#22411;&#26174;&#31034;&#20986;&#26377;&#38480;&#30340;&#33021;&#21147;&#26469;&#21306;&#20998;&#32454;&#24494;&#24046;&#24322;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;F-Eval&#65292;&#19968;&#20010;&#21452;&#35821;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22522;&#26412;&#33021;&#21147;&#65292;&#21253;&#25324;&#34920;&#36798;&#12289;&#24120;&#35782;&#21644;&#36923;&#36753;&#12290;F-Eval&#20013;&#30340;&#20219;&#21153;&#21253;&#25324;&#22810;&#39033;&#36873;&#25321;&#23458;&#35266;&#20219;&#21153;&#12289;&#24320;&#25918;&#24335;&#23458;&#35266;&#20219;&#21153;&#12289;&#22522;&#20110;&#21442;&#32771;&#30340;&#20027;&#35266;&#20219;&#21153;&#21644;&#26080;&#21442;&#32771;&#30340;&#20027;&#35266;&#20219;&#21153;&#12290;&#23545;&#20110;&#26080;&#21442;&#32771;&#30340;&#20027;&#35266;&#20219;&#21153;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20316;&#20026;&#26367;&#20195;API&#27169;&#22411;&#35780;&#20998;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23545;13&#20010;&#20808;&#36827;&#30340;LLMs&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) garner significant attention for their unprecedented performance, leading to an increasing number of researches evaluating LLMs. However, these evaluation benchmarks are limited to assessing the instruction-following capabilities, overlooking the fundamental abilities that emerge during the pre-training stage. Previous subjective evaluation methods mainly reply on scoring by API models. However, in the absence of references, large models have shown limited ability to discern subtle differences. To bridge the gap, we propose F-Eval, a bilingual evaluation benchmark to evaluate the fundamental abilities, including expression, commonsense and logic. The tasks in F-Eval include multi-choice objective tasks, open-ended objective tasks, reference-based subjective tasks and reference-free subjective tasks. For reference-free subjective tasks, we devise new evaluation methods, serving as alternatives to scoring by API models. We conduct evaluations on 13 advanced L
&lt;/p&gt;</description></item><item><title>ChemDFM&#26159;&#39318;&#20010;&#38754;&#21521;&#21270;&#23398;&#26234;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#23545;&#21270;&#23398;&#25991;&#29486;&#21644;&#25968;&#25454;&#30340;&#35757;&#32451;&#65292;&#20855;&#22791;&#20102;&#23384;&#20648;&#12289;&#29702;&#35299;&#21644;&#25512;&#29702;&#21270;&#23398;&#30693;&#35782;&#21644;&#35821;&#35328;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#21270;&#23398;&#39046;&#22495;&#30340;&#24615;&#33021;&#19978;&#20248;&#20110;&#20854;&#20182;&#24320;&#28304;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.14818</link><description>&lt;p&gt;
ChemDFM: &#21270;&#23398;&#39046;&#22495;&#23545;&#35805;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ChemDFM: Dialogue Foundation Model for Chemistry. (arXiv:2401.14818v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14818
&lt;/p&gt;
&lt;p&gt;
ChemDFM&#26159;&#39318;&#20010;&#38754;&#21521;&#21270;&#23398;&#26234;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#23545;&#21270;&#23398;&#25991;&#29486;&#21644;&#25968;&#25454;&#30340;&#35757;&#32451;&#65292;&#20855;&#22791;&#20102;&#23384;&#20648;&#12289;&#29702;&#35299;&#21644;&#25512;&#29702;&#21270;&#23398;&#30693;&#35782;&#21644;&#35821;&#35328;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#21270;&#23398;&#39046;&#22495;&#30340;&#24615;&#33021;&#19978;&#20248;&#20110;&#20854;&#20182;&#24320;&#28304;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#19968;&#33324;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#23427;&#20204;&#30340;&#20219;&#21153;&#27010;&#25324;&#21644;&#33258;&#30001;&#23545;&#35805;&#33021;&#21147;&#21487;&#20197;&#26497;&#22823;&#22320;&#24110;&#21161;&#35774;&#35745;&#21270;&#23398;&#26234;&#33021;(CGI)&#65292;&#20197;&#21327;&#21161;&#21270;&#23398;&#39046;&#22495;&#30340;&#23454;&#38469;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#22312;&#21270;&#23398;&#39046;&#22495;&#20013;&#23384;&#22312;&#19987;&#19994;&#35821;&#35328;&#21644;&#30693;&#35782;&#65292;&#22914;&#39640;&#24230;&#20449;&#24687;&#21270;&#30340;SMILES&#31526;&#21495;&#34920;&#31034;&#27861;&#65292;&#38459;&#30861;&#20102;&#19968;&#33324;&#39046;&#22495;LLMs&#22312;&#21270;&#23398;&#39046;&#22495;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;ChemDFM&#65292;&#36825;&#26159;&#39318;&#20010;&#38754;&#21521;CGI&#30340;LLM&#12290;ChemDFM-13B&#26159;&#22312;&#21270;&#23398;&#25991;&#29486;&#12289;&#25945;&#31185;&#20070;&#12289;&#35828;&#26126;&#20070;&#20197;&#21450;&#21508;&#31181;&#19968;&#33324;&#39046;&#22495;&#30340;&#25968;&#25454;&#20013;&#35757;&#32451;&#30340;34B&#20196;&#29260;&#12290;&#22240;&#27492;&#65292;&#23427;&#21487;&#20197;&#23384;&#20648;&#12289;&#29702;&#35299;&#21644;&#25512;&#29702;&#21270;&#23398;&#30693;&#35782;&#21644;&#35821;&#35328;&#65292;&#21516;&#26102;&#20855;&#26377;&#20808;&#36827;&#30340;&#33258;&#30001;&#24418;&#24335;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;&#24191;&#27867;&#30340;&#23450;&#37327;&#35780;&#20272;&#34920;&#26126;&#65292;ChemDFM&#21487;&#20197;&#26126;&#26174;&#20248;&#20110;&#20195;&#34920;&#24615;&#30340;&#24320;&#28304;LLMs&#12290;&#27492;&#22806;&#65292;ChemDFM&#36824;&#21487;&#20197;...
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have established great success in the general domain of natural language processing. Their emerging task generalization and free-form dialogue capabilities can greatly help to design Chemical General Intelligence (CGI) to assist real-world research in chemistry. However, the existence of specialized language and knowledge in the field of chemistry, such as the highly informative SMILES notation, hinders the performance of general-domain LLMs in chemistry. To this end, we develop ChemDFM, the first LLM towards CGI. ChemDFM-13B is trained on 34B tokens from chemical literature, textbooks, and instructions as well as various data from the general domain. Therefore, it can store, understand, and reason over chemical knowledge and languages while still possessing advanced free-form language comprehension capabilities. Extensive quantitative evaluation shows that ChemDFM can significantly outperform the representative open-sourced LLMs. Moreover, ChemDFM can also
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#37329;&#34701;&#39046;&#22495;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#26041;&#27861;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#12290;&#36890;&#36807;&#22312;&#37329;&#34701;&#25991;&#20214;&#21644;&#35828;&#26126;&#20070;&#19978;&#36827;&#34892;&#31934;&#32454;&#35843;&#20248;&#65292;&#23637;&#31034;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#30446;&#26631;&#39046;&#22495;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.14777</link><description>&lt;p&gt;
&#29992;&#20110;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Adaptation for Financial Sentiment Analysis. (arXiv:2401.14777v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#37329;&#34701;&#39046;&#22495;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#26041;&#27861;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#12290;&#36890;&#36807;&#22312;&#37329;&#34701;&#25991;&#20214;&#21644;&#35828;&#26126;&#20070;&#19978;&#36827;&#34892;&#31934;&#32454;&#35843;&#20248;&#65292;&#23637;&#31034;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#30446;&#26631;&#39046;&#22495;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#22312;&#37329;&#34701;&#26426;&#26500;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#25552;&#20379;&#26377;&#20851;&#20844;&#21496;&#21644;&#24066;&#22330;&#30340;&#37329;&#34701;&#25991;&#20214;&#30340;&#39640;&#24230;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#37329;&#34701;&#39046;&#22495;&#30340;&#26223;&#35266;&#23545;NLP&#25552;&#20986;&#20102;&#39069;&#22806;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#25991;&#26412;&#30340;&#22797;&#26434;&#24615;&#21644;&#29305;&#23450;&#26415;&#35821;&#30340;&#20351;&#29992;&#12290;&#21363;&#20351;&#20351;&#29992;&#20855;&#26377;&#20986;&#33394;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#22312;&#19987;&#38376;&#38024;&#23545;&#37329;&#34701;&#30340;&#20219;&#21153;&#19978;&#20063;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#20851;&#20110;&#20197;&#37329;&#34701;&#39046;&#22495;&#20026;&#30446;&#26631;&#30340;LLM&#36866;&#24212;&#26041;&#27861;&#30740;&#31350;&#65292;&#24182;&#39640;&#24230;&#20851;&#27880;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#21508;&#31181;&#24191;&#27867;&#30340;&#31574;&#30053;&#26469;&#36866;&#24212;&#20004;&#20010;&#21442;&#25968;&#20302;&#20110;15&#20159;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#22312;&#37329;&#34701;&#25991;&#26723;&#21644;&#35828;&#26126;&#20070;&#19978;&#36827;&#34892;&#31934;&#32454;&#35843;&#20248;&#65292;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#21487;&#20197;&#36866;&#24212;&#30446;&#26631;&#39046;&#22495;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#23567;&#22411;LLM&#20855;&#26377;&#30456;&#20284;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language processing (NLP) has recently gained relevance within financial institutions by providing highly valuable insights into companies and markets' financial documents. However, the landscape of the financial domain presents extra challenges for NLP, due to the complexity of the texts and the use of specific terminology. Generalist language models tend to fall short in tasks specifically tailored for finance, even when using large language models (LLMs) with great natural language understanding and generative capabilities. This paper presents a study on LLM adaptation methods targeted at the financial domain and with high emphasis on financial sentiment analysis. To this purpose, two foundation models with less than 1.5B parameters have been adapted using a wide range of strategies. We show that through careful fine-tuning on both financial documents and instructions, these foundation models can be adapted to the target domain. Moreover, we observe that small LLMs have comp
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22768;&#23398;&#21644;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20132;&#26367;&#21644;&#22238;&#24212;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#36825;&#20004;&#31181;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#21475;&#35821;&#23545;&#35805;&#20013;&#23454;&#29616;&#26356;&#21152;&#33258;&#28982;&#21644;&#23545;&#35805;&#24335;&#30340;&#20114;&#21160;&#12290;</title><link>http://arxiv.org/abs/2401.14717</link><description>&lt;p&gt;
&#29992;&#22768;&#23398;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34701;&#21512;&#36827;&#34892;&#20132;&#26367;&#21644;&#22238;&#24212;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Turn-taking and Backchannel Prediction with Acoustic and Large Language Model Fusion. (arXiv:2401.14717v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14717
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22768;&#23398;&#21644;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20132;&#26367;&#21644;&#22238;&#24212;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#36825;&#20004;&#31181;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#21475;&#35821;&#23545;&#35805;&#20013;&#23454;&#29616;&#26356;&#21152;&#33258;&#28982;&#21644;&#23545;&#35805;&#24335;&#30340;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#39044;&#27979;&#21475;&#35821;&#23545;&#35805;&#20013;&#20132;&#26367;&#21644;&#22238;&#24212;&#20301;&#32622;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#31070;&#32463;&#22768;&#23398;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#22312;Switchboard&#20154;&#38469;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;&#21333;&#27169;&#24577;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20219;&#21153;&#25351;&#20196;&#24494;&#35843;&#31574;&#30053;&#65292;&#20197;&#36827;&#19968;&#27493;&#20174;LLM&#32534;&#30721;&#30340;&#30693;&#35782;&#20013;&#21463;&#30410;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#20351;&#29992;&#32452;&#21512;&#30340;LLM&#21644;&#22768;&#23398;&#27169;&#22411;&#22312;&#20154;&#31867;&#21644;&#35821;&#38899;AI&#20195;&#29702;&#20043;&#38388;&#23454;&#29616;&#26356;&#33258;&#28982;&#21644;&#23545;&#35805;&#24335;&#20114;&#21160;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an approach for continuous prediction of turn-taking and backchanneling locations in spoken dialogue by fusing a neural acoustic model with a large language model (LLM). Experiments on the Switchboard human-human conversation dataset demonstrate that our approach consistently outperforms the baseline models with single modality. We also develop a novel multi-task instruction fine-tuning strategy to further benefit from LLM-encoded knowledge for understanding the tasks and conversational contexts, leading to additional improvements. Our approach demonstrates the potential of combined LLMs and acoustic models for a more natural and conversational interaction between humans and speech-enabled AI agents.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26159;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#30340;&#20154;&#24037;&#25968;&#25454;&#30340;&#36861;&#36394;&#30740;&#31350;&#65292;&#23558;&#21508;&#31181;&#31867;&#22411;&#30340;LLM&#29983;&#25104;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#20102;&#27719;&#24635;&#21644;&#27979;&#35797;&#65292;&#24182;&#25581;&#31034;&#20102;&#38544;&#34255;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#38382;&#39064;&#12290;&#36825;&#26159;&#31532;&#19968;&#27425;&#23545;LLM&#29983;&#25104;&#25968;&#25454;&#36827;&#34892;&#32508;&#21512;&#20998;&#26512;&#21644;&#27604;&#36739;&#65292;&#24182;&#24341;&#21457;&#20102;&#23545;&#20154;&#24037;&#25968;&#25454;&#36136;&#37327;&#30340;&#20851;&#27880;&#12290;</title><link>http://arxiv.org/abs/2401.14698</link><description>&lt;p&gt;
&#21453;&#24605;LLM&#29983;&#25104;&#25968;&#25454;&#30340;&#30495;&#23454;&#24615;&#65306;&#23545;LLM&#29983;&#25104;&#25968;&#25454;&#30340;&#36861;&#36394;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Under the Surface: Tracking the Artifactuality of LLM-Generated Data. (arXiv:2401.14698v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14698
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#30340;&#20154;&#24037;&#25968;&#25454;&#30340;&#36861;&#36394;&#30740;&#31350;&#65292;&#23558;&#21508;&#31181;&#31867;&#22411;&#30340;LLM&#29983;&#25104;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#20102;&#27719;&#24635;&#21644;&#27979;&#35797;&#65292;&#24182;&#25581;&#31034;&#20102;&#38544;&#34255;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#38382;&#39064;&#12290;&#36825;&#26159;&#31532;&#19968;&#27425;&#23545;LLM&#29983;&#25104;&#25968;&#25454;&#36827;&#34892;&#32508;&#21512;&#20998;&#26512;&#21644;&#27604;&#36739;&#65292;&#24182;&#24341;&#21457;&#20102;&#23545;&#20154;&#24037;&#25968;&#25454;&#36136;&#37327;&#30340;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29983;&#25104;&#20154;&#24037;&#25968;&#25454;&#26041;&#38754;&#30340;&#19981;&#26029;&#25193;&#22823;&#30340;&#20316;&#29992;&#12290;LLM&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#29983;&#25104;&#22810;&#31181;&#36755;&#20986;&#65292;&#21253;&#25324;&#27880;&#37322;&#12289;&#20559;&#22909;&#12289;&#25351;&#20196;&#25552;&#31034;&#12289;&#27169;&#25311;&#23545;&#35805;&#21644;&#33258;&#30001;&#25991;&#26412;&#12290;&#30001;&#20110;&#36825;&#20123;LLM&#29983;&#25104;&#25968;&#25454;&#24418;&#24335;&#22312;&#24212;&#29992;&#20013;&#32463;&#24120;&#20132;&#21449;&#65292;&#23427;&#20204;&#30456;&#20114;&#24433;&#21709;&#65292;&#24182;&#24341;&#21457;&#20102;&#23545;&#35757;&#32451;&#24490;&#29615;&#20013;&#21512;&#24182;&#30340;&#20154;&#24037;&#25968;&#25454;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#30340;&#37325;&#22823;&#20851;&#27880;&#65292;&#24418;&#25104;&#20102;&#19968;&#20010;&#20154;&#24037;&#25968;&#25454;&#29983;&#24577;&#31995;&#32479;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#39033;&#30740;&#31350;&#23558;&#21508;&#31181;&#31867;&#22411;&#30340;LLM&#29983;&#25104;&#25991;&#26412;&#25968;&#25454;&#27719;&#24635;&#36215;&#26469;&#65292;&#20174;&#26356;&#20005;&#26684;&#21463;&#38480;&#30340;&#25968;&#25454;&#22914;&#8220;&#20219;&#21153;&#26631;&#31614;&#8221;&#21040;&#26356;&#33258;&#30001;&#30340;&#8220;&#33258;&#30001;&#25991;&#26412;&#8221;&#12290;&#28982;&#21518;&#25105;&#20204;&#23545;LLM&#29983;&#25104;&#30340;&#20154;&#24037;&#25968;&#25454;&#30340;&#36136;&#37327;&#21644;&#24433;&#21709;&#36827;&#34892;&#20102;&#21387;&#21147;&#27979;&#35797;&#65292;&#24182;&#19982;&#20154;&#24037;&#25968;&#25454;&#22312;&#21508;&#31181;&#29616;&#26377;&#22522;&#20934;&#19978;&#36827;&#34892;&#27604;&#36739;&#12290;&#23613;&#31649;&#20154;&#24037;&#25968;&#25454;&#33021;&#22815;&#21305;&#37197;&#20154;&#31867;&#34920;&#29616;&#65292;&#20294;&#26412;&#25991;&#25581;&#31034;&#20102;&#38544;&#34255;&#30340;&#24040;&#22823;&#38544;&#24739;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work delves into the expanding role of large language models (LLMs) in generating artificial data. LLMs are increasingly employed to create a variety of outputs, including annotations, preferences, instruction prompts, simulated dialogues, and free text. As these forms of LLM-generated data often intersect in their application, they exert mutual influence on each other and raise significant concerns about the quality and diversity of the artificial data incorporated into training cycles, leading to an artificial data ecosystem. To the best of our knowledge, this is the first study to aggregate various types of LLM-generated text data, from more tightly constrained data like "task labels" to more lightly constrained "free-form text". We then stress test the quality and implications of LLM-generated artificial data, comparing it with human data across various existing benchmarks. Despite artificial data's capability to match human performance, this paper reveals significant hidden d
&lt;/p&gt;</description></item><item><title>Taiyi-Diffusion-XL&#26159;&#19968;&#20010;&#20013;&#33521;&#21452;&#35821;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;CLIP&#21644;Stable-Diffusion-XL&#30340;&#33021;&#21147;&#36827;&#34892;&#25193;&#23637;&#65292;&#24182;&#36890;&#36807;&#21452;&#35821;&#36830;&#32493;&#39044;&#35757;&#32451;&#26469;&#23454;&#29616;&#12290;&#27169;&#22411;&#36890;&#36807;&#23558;&#24120;&#29992;&#30340;&#27721;&#23383;&#25972;&#21512;&#21040;CLIP&#30340;&#20998;&#35789;&#22120;&#21644;&#23884;&#20837;&#23618;&#20013;&#65292;&#20197;&#21450;&#20351;&#29992;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20016;&#23500;&#25991;&#26412;&#25552;&#31034;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#22270;&#20687;&#26631;&#39064;&#21644;&#39640;&#36136;&#37327;&#30340;&#35270;&#35273;&#25928;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#21452;&#35821;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2401.14688</link><description>&lt;p&gt;
Taiyi-Diffusion-XL: &#20511;&#21161;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#25903;&#25345;&#25512;&#36827;&#21452;&#35821;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Taiyi-Diffusion-XL: Advancing Bilingual Text-to-Image Generation with Large Vision-Language Model Support. (arXiv:2401.14688v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14688
&lt;/p&gt;
&lt;p&gt;
Taiyi-Diffusion-XL&#26159;&#19968;&#20010;&#20013;&#33521;&#21452;&#35821;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;CLIP&#21644;Stable-Diffusion-XL&#30340;&#33021;&#21147;&#36827;&#34892;&#25193;&#23637;&#65292;&#24182;&#36890;&#36807;&#21452;&#35821;&#36830;&#32493;&#39044;&#35757;&#32451;&#26469;&#23454;&#29616;&#12290;&#27169;&#22411;&#36890;&#36807;&#23558;&#24120;&#29992;&#30340;&#27721;&#23383;&#25972;&#21512;&#21040;CLIP&#30340;&#20998;&#35789;&#22120;&#21644;&#23884;&#20837;&#23618;&#20013;&#65292;&#20197;&#21450;&#20351;&#29992;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20016;&#23500;&#25991;&#26412;&#25552;&#31034;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#22270;&#20687;&#26631;&#39064;&#21644;&#39640;&#36136;&#37327;&#30340;&#35270;&#35273;&#25928;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#21452;&#35821;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#30340;&#36827;&#23637;&#26174;&#33879;&#25552;&#21319;&#20102;&#22270;&#20687;&#29983;&#25104;&#33021;&#21147;&#65292;&#28982;&#32780;&#22312;&#21452;&#35821;&#25110;&#20013;&#25991;&#35821;&#35328;&#25903;&#25345;&#26041;&#38754;&#20173;&#23384;&#22312;&#26126;&#26174;&#30340;&#24320;&#28304;&#27169;&#22411;&#32570;&#21475;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Taiyi-Diffusion-XL&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#20013;&#33521;&#21452;&#35821;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;CLIP&#21644;Stable-Diffusion-XL&#33021;&#21147;&#30340;&#25193;&#23637;&#65292;&#24182;&#36890;&#36807;&#21452;&#35821;&#36830;&#32493;&#39044;&#35757;&#32451;&#30340;&#36807;&#31243;&#26469;&#24320;&#21457;&#12290;&#36825;&#31181;&#26041;&#27861;&#21253;&#25324;&#36890;&#36807;&#23558;&#26368;&#24120;&#29992;&#30340;&#27721;&#23383;&#25972;&#21512;&#21040;CLIP&#30340;&#20998;&#35789;&#22120;&#21644;&#23884;&#20837;&#23618;&#20013;&#26469;&#25193;&#23637;&#35789;&#27719;&#37327;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#21516;&#26102;&#36824;&#21152;&#20837;&#20102;&#32477;&#23545;&#20301;&#32622;&#32534;&#30721;&#25193;&#23637;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#26469;&#20016;&#23500;&#25991;&#26412;&#25552;&#31034;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#22909;&#30340;&#22270;&#20687;&#26631;&#39064;&#21644;&#26356;&#39640;&#30340;&#35270;&#35273;&#36136;&#37327;&#12290;&#36825;&#20123;&#22686;&#24378;&#25514;&#26045;&#38543;&#21518;&#24212;&#29992;&#20110;&#19979;&#28216;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#24320;&#21457;&#30340;CLIP&#27169;&#22411;&#22312;&#21452;&#35821;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#27492;&#22806;&#65292;&#21452;&#35821;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#25972;&#21512;&#20351;&#35813;&#27169;&#22411;&#22312;&#20013;&#33521;&#25991;&#22330;&#26223;&#19979;&#20855;&#26377;&#22343;&#34913;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in text-to-image models have significantly enhanced image generation capabilities, yet a notable gap of open-source models persists in bilingual or Chinese language support. To address this need, we present Taiyi-Diffusion-XL, a new Chinese and English bilingual text-to-image model which is developed by extending the capabilities of CLIP and Stable-Diffusion-XL through a process of bilingual continuous pre-training. This approach includes the efficient expansion of vocabulary by integrating the most frequently used Chinese characters into CLIP's tokenizer and embedding layers, coupled with an absolute position encoding expansion. Additionally, we enrich text prompts by large vision-language model, leading to better images captions and possess higher visual quality. These enhancements are subsequently applied to downstream text-to-image models. Our empirical results indicate that the developed CLIP model excels in bilingual image-text retrieval.Furthermore, the bilin
&lt;/p&gt;</description></item><item><title>MasonTigers@LT-EDI-2024&#22242;&#38431;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#21313;&#31181;&#35821;&#35328;&#20013;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#35780;&#35770;&#20013;&#30340;&#24656;&#21516;&#21644;&#24694;&#24847;&#24615;&#21035;&#35748;&#30693;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.14681</link><description>&lt;p&gt;
MasonTigers@LT-EDI-2024&#65306;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#35780;&#35770;&#20013;&#30340;&#24656;&#21516;&#21644;&#24694;&#24847;&#24615;&#21035;&#35748;&#30693;&#30340;&#38598;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MasonTigers@LT-EDI-2024: An Ensemble Approach towards Detecting Homophobia and Transphobia in Social Media Comments. (arXiv:2401.14681v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14681
&lt;/p&gt;
&lt;p&gt;
MasonTigers@LT-EDI-2024&#22242;&#38431;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#21313;&#31181;&#35821;&#35328;&#20013;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#35780;&#35770;&#20013;&#30340;&#24656;&#21516;&#21644;&#24694;&#24847;&#24615;&#21035;&#35748;&#30693;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#22312;LT-EDI 2024&#30740;&#35752;&#20250;&#30340;&#20219;&#21153;2&#20013;&#38024;&#23545;&#21313;&#31181;&#35821;&#35328;&#26816;&#27979;&#24656;&#21516;&#21644;/&#25110;&#24694;&#24847;&#24615;&#21035;&#35748;&#30693;&#30340;&#26041;&#27861;&#21644;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#21333;&#35821;&#35328;transformer&#21644;&#38598;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;&#27599;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#38598;&#25104;&#27169;&#22411;&#34920;&#29616;&#33391;&#22909;&#65292;&#25105;&#20204;&#30340;&#22242;&#38431;MasonTigers&#22312;&#21313;&#31181;&#35821;&#35328;&#20013;&#30340;&#20843;&#31181;&#20013;&#37117;&#25490;&#21517;&#21069;&#20116;&#65292;&#20197;&#23439;&#35266;F1&#20998;&#25968;&#34913;&#37327;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24378;&#35843;&#20102;&#38598;&#25104;&#26041;&#27861;&#22312;&#22810;&#35821;&#35328;&#24773;&#22659;&#19979;&#30340;&#26377;&#25928;&#24615;&#65292;&#35299;&#20915;&#20102;&#35821;&#35328;&#29305;&#23450;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we describe our approaches and results for Task 2 of the LT-EDI 2024 Workshop, aimed at detecting homophobia and/or transphobia across ten languages. Our methodologies include monolingual transformers and ensemble methods, capitalizing on the strengths of each to enhance the performance of the models. The ensemble models worked well, placing our team, MasonTigers, in the top five for eight of the ten languages, as measured by the macro F1 score. Our work emphasizes the efficacy of ensemble methods in multilingual scenarios, addressing the complexities of language-specific tasks.
&lt;/p&gt;</description></item><item><title>MaLLaM&#26159;&#39532;&#26469;&#35199;&#20122;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#25968;&#25454;&#38598;&#21644;&#39044;&#35757;&#32451;&#30340;BPE&#20998;&#35789;&#22120;&#65292;&#22312;&#39532;&#26469;&#35821;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#31454;&#20105;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#25429;&#25417;&#21644;&#29702;&#35299;&#39532;&#26469;&#35199;&#20122;&#35821;&#35328;&#32454;&#24494;&#24046;&#24322;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#20026;&#25552;&#21319;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#25171;&#19979;&#20102;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2401.14680</link><description>&lt;p&gt;
MaLLaM -- &#39532;&#26469;&#35199;&#20122;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MaLLaM -- Malaysia Large Language Model. (arXiv:2401.14680v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14680
&lt;/p&gt;
&lt;p&gt;
MaLLaM&#26159;&#39532;&#26469;&#35199;&#20122;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#25968;&#25454;&#38598;&#21644;&#39044;&#35757;&#32451;&#30340;BPE&#20998;&#35789;&#22120;&#65292;&#22312;&#39532;&#26469;&#35821;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#31454;&#20105;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#25429;&#25417;&#21644;&#29702;&#35299;&#39532;&#26469;&#35199;&#20122;&#35821;&#35328;&#32454;&#24494;&#24046;&#24322;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#20026;&#25552;&#21319;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#25171;&#19979;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#39532;&#26469;&#35199;&#20122;&#35821;&#22659;&#19979;&#20174;&#22836;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#36275;&#65292;&#25105;&#20204;&#20351;&#29992;349GB&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65288;&#30456;&#24403;&#20110;90&#20159;&#20010;&#26631;&#35760;&#65289;&#65292;&#20351;&#29992;&#25105;&#20204;&#39044;&#35757;&#32451;&#30340;&#23383;&#33410;&#23545;&#32534;&#30721;&#65288;BPE&#65289;&#20998;&#35789;&#22120;&#65292;&#22312;1.1&#20159;&#12289;30&#20159;&#21644;50&#20159;&#21442;&#25968;&#19978;&#35757;&#32451;&#20102;&#27169;&#22411;&#12290;MaLLaM&#22312;&#39532;&#26469;&#35821;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#23613;&#31649;&#21482;&#26159;&#20351;&#29992;&#20102;90&#20159;&#20010;&#26631;&#35760;&#30340;&#36739;&#23567;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#25105;&#20204;&#32463;&#36807;&#25351;&#20196;&#35843;&#25972;&#30340;MaLLaM&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#31454;&#20105;&#21147;&#12290;&#19982;ChatGPT3.5&#21644;Malaysian Mistral&#30456;&#27604;&#65292;MaLLaM&#30340;&#25351;&#20196;&#35843;&#25972;&#27169;&#22411;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#29087;&#32451;&#24230;&#65292;&#31361;&#20986;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#25429;&#25417;&#21644;&#29702;&#35299;&#39532;&#26469;&#35199;&#20122;&#35821;&#35328;&#32454;&#24494;&#24046;&#24322;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;MaLLaM&#27169;&#22411;&#22312;&#35813;&#39046;&#22495;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#65292;&#25552;&#20379;&#20102;&#19982;&#39532;&#26469;&#35199;&#20122;&#35821;&#22659;&#32039;&#23494;&#32852;&#31995;&#30340;&#20840;&#38754;&#35821;&#35328;&#34920;&#31034;&#12290;&#36825;&#39033;&#21162;&#21147;&#26088;&#22312;&#20026;&#25552;&#21319;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#38138;&#24179;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Addressing the gap in Large Language Model pretrained from scratch with Malaysian context, We trained models with 1.1 billion, 3 billion, and 5 billion parameters on a substantial 349GB dataset, equivalent to 90 billion tokens based on our pretrained Byte Pair Encoding (BPE) tokenizer for a single epoch. MaLLaM contributes to enhanced natural language understanding and generation tasks in the Malay language. Although trained on a smaller dataset of 90 billion tokens, our instruction-tuned MaLLaM models perform competitively. When compared to ChatGPT3.5 and Malaysian Mistral, MaLLaM's instruction-tuned models demonstrate notable proficiency, underscoring the effectiveness of our approach in capturing and understanding the nuances of the Malaysian language. MaLLaM models mark a significant contribution to the field, providing comprehensive language representations grounded in Malaysian context. This endeavor aims to pave the way for enhanced natural language understanding and generation 
&lt;/p&gt;</description></item><item><title>UNIT-DSR&#31995;&#32479;&#26159;&#19968;&#31181;&#20351;&#29992;&#35821;&#38899;&#21333;&#20803;&#26631;&#20934;&#21270;&#30340;&#21457;&#38899;&#38556;&#30861;&#35821;&#38899;&#37325;&#24314;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#35821;&#38899;&#21333;&#20803;&#32422;&#26463;&#21457;&#38899;&#38556;&#30861;&#20869;&#23481;&#24674;&#22797;&#65292;&#25552;&#21319;&#20102;&#35757;&#32451;&#25928;&#29575;&#21644;&#37325;&#24314;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.14664</link><description>&lt;p&gt;
UNIT-DSR: &#20351;&#29992;&#35821;&#38899;&#21333;&#20803;&#26631;&#20934;&#21270;&#30340;&#21457;&#38899;&#38556;&#30861;&#35821;&#38899;&#37325;&#24314;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
UNIT-DSR: Dysarthric Speech Reconstruction System Using Speech Unit Normalization. (arXiv:2401.14664v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14664
&lt;/p&gt;
&lt;p&gt;
UNIT-DSR&#31995;&#32479;&#26159;&#19968;&#31181;&#20351;&#29992;&#35821;&#38899;&#21333;&#20803;&#26631;&#20934;&#21270;&#30340;&#21457;&#38899;&#38556;&#30861;&#35821;&#38899;&#37325;&#24314;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#35821;&#38899;&#21333;&#20803;&#32422;&#26463;&#21457;&#38899;&#38556;&#30861;&#20869;&#23481;&#24674;&#22797;&#65292;&#25552;&#21319;&#20102;&#35757;&#32451;&#25928;&#29575;&#21644;&#37325;&#24314;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#38899;&#38556;&#30861;&#35821;&#38899;&#37325;&#24314;&#65288;DSR&#65289;&#31995;&#32479;&#26088;&#22312;&#23558;&#21457;&#38899;&#38556;&#30861;&#30340;&#35821;&#38899;&#33258;&#21160;&#36716;&#25442;&#20026;&#21548;&#36215;&#26469;&#27491;&#24120;&#30340;&#35821;&#38899;&#12290;&#35813;&#25216;&#26415;&#33021;&#22815;&#25913;&#21892;&#21463;&#31070;&#32463;&#36816;&#21160;&#38556;&#30861;&#24433;&#21709;&#30340;&#28436;&#35762;&#32773;&#19982;&#20154;&#20204;&#30340;&#20132;&#27969;&#65292;&#24182;&#25552;&#21319;&#20182;&#20204;&#30340;&#31038;&#20132;&#34701;&#20837;&#24230;&#12290;&#22522;&#20110;NED&#65288;&#31070;&#32463;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#65289;&#30340;&#31995;&#32479;&#30456;&#36739;&#20110;&#22522;&#20110;GAN&#65288;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65289;&#30340;&#26041;&#27861;&#22312;&#37325;&#24314;&#35821;&#38899;&#30340;&#21487;&#25026;&#24230;&#26041;&#38754;&#26377;&#20102;&#26174;&#33879;&#25552;&#39640;&#65292;&#20294;&#35813;&#26041;&#27861;&#20173;&#21463;&#21040;&#32423;&#32852;&#27969;&#31243;&#21644;&#20869;&#23481;&#32534;&#30721;&#22120;&#30340;&#36741;&#21161;&#20219;&#21153;&#23548;&#33268;&#30340;&#35757;&#32451;&#25928;&#29575;&#19981;&#39640;&#30340;&#38480;&#21046;&#65292;&#32780;&#36825;&#21487;&#33021;&#20250;&#24433;&#21709;&#37325;&#24314;&#30340;&#36136;&#37327;&#12290;&#21463;&#21040;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#21644;&#31163;&#25955;&#35821;&#38899;&#21333;&#20803;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Unit-DSR&#31995;&#32479;&#65292;&#21033;&#29992;HuBERT&#30340;&#24378;&#22823;&#39046;&#22495;&#33258;&#36866;&#24212;&#33021;&#21147;&#26469;&#25552;&#21319;&#35757;&#32451;&#25928;&#29575;&#65292;&#24182;&#21033;&#29992;&#35821;&#38899;&#21333;&#20803;&#22312;&#31163;&#25955;&#35821;&#35328;&#31354;&#38388;&#20013;&#32422;&#26463;&#21457;&#38899;&#38556;&#30861;&#20869;&#23481;&#30340;&#24674;&#22797;&#12290;&#19982;NED&#26041;&#27861;&#30456;&#27604;&#65292;Unit-DSR&#31995;&#32479;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dysarthric speech reconstruction (DSR) systems aim to automatically convert dysarthric speech into normal-sounding speech. The technology eases communication with speakers affected by the neuromotor disorder and enhances their social inclusion. NED-based (Neural Encoder-Decoder) systems have significantly improved the intelligibility of the reconstructed speech as compared with GAN-based (Generative Adversarial Network) approaches, but the approach is still limited by training inefficiency caused by the cascaded pipeline and auxiliary tasks of the content encoder, which may in turn affect the quality of reconstruction. Inspired by self-supervised speech representation learning and discrete speech units, we propose a Unit-DSR system, which harnesses the powerful domain-adaptation capacity of HuBERT for training efficiency improvement and utilizes speech units to constrain the dysarthric content restoration in a discrete linguistic space. Compared with NED approaches, the Unit-DSR system
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#31185;&#23398;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#21644;&#21270;&#23398;&#39046;&#22495;&#30340;&#32508;&#36848;&#65292;&#20998;&#26512;&#20102;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#31185;&#23398;LLMs&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.14656</link><description>&lt;p&gt;
&#31185;&#23398;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#29983;&#29289;&#21644;&#21270;&#23398;&#39046;&#22495;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Scientific Large Language Models: A Survey on Biological &amp; Chemical Domains. (arXiv:2401.14656v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14656
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#31185;&#23398;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#21644;&#21270;&#23398;&#39046;&#22495;&#30340;&#32508;&#36848;&#65292;&#20998;&#26512;&#20102;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#31185;&#23398;LLMs&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#25552;&#21319;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#19968;&#31181;&#21464;&#38761;&#24615;&#21147;&#37327;&#65292;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#36808;&#20986;&#20102;&#37325;&#35201;&#30340;&#19968;&#27493;&#12290;LLMs&#30340;&#24212;&#29992;&#24050;&#36229;&#36234;&#20256;&#32479;&#30340;&#35821;&#35328;&#30028;&#38480;&#65292;&#21253;&#25324;&#22312;&#21508;&#31181;&#31185;&#23398;&#23398;&#31185;&#20869;&#24320;&#21457;&#30340;&#19987;&#38376;&#35821;&#35328;&#31995;&#32479;&#12290;&#36825;&#31181;&#19981;&#26029;&#22686;&#38271;&#30340;&#20852;&#36259;&#23548;&#33268;&#20102;&#31185;&#23398;LLMs&#30340;&#20986;&#29616;&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#38376;&#20026;&#20419;&#36827;&#31185;&#23398;&#21457;&#29616;&#32780;&#35774;&#35745;&#30340;&#26032;&#22411;&#23376;&#31867;&#12290;&#20316;&#20026;&#20154;&#24037;&#26234;&#33021;&#31185;&#23398;&#31038;&#21306;&#20013;&#30340;&#26032;&#20852;&#39046;&#22495;&#65292;&#31185;&#23398;LLMs&#20540;&#24471;&#20840;&#38754;&#25506;&#32034;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#32570;&#20047;&#31995;&#32479;&#19988;&#26368;&#26032;&#30340;&#32508;&#36848;&#20171;&#32461;&#23427;&#20204;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#31995;&#32479;&#22320;&#21246;&#30011;&#8220;&#31185;&#23398;&#35821;&#35328;&#8221;&#30340;&#27010;&#24565;&#65292;&#24182;&#20840;&#38754;&#23457;&#26597;&#31185;&#23398;LLMs&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#32771;&#34385;&#21040;&#31185;&#23398;&#23398;&#31185;&#30340;&#24191;&#27867;&#39046;&#22495;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#37319;&#29992;&#20102;&#19968;&#31181;&#32858;&#28966;&#30340;&#35270;&#35282;&#65292;&#19987;&#27880;&#20110;&#29983;&#29289;&#21644;&#21270;&#23398;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have emerged as a transformative power in enhancing natural language comprehension, representing a significant stride toward artificial general intelligence. The application of LLMs extends beyond conventional linguistic boundaries, encompassing specialized linguistic systems developed within various scientific disciplines. This growing interest has led to the advent of scientific LLMs, a novel subclass specifically engineered for facilitating scientific discovery. As a burgeoning area in the community of AI for Science, scientific LLMs warrant comprehensive exploration. However, a systematic and up-to-date survey introducing them is currently lacking. In this paper, we endeavor to methodically delineate the concept of "scientific language", whilst providing a thorough review of the latest advancements in scientific LLMs. Given the expansive realm of scientific disciplines, our analysis adopts a focused lens, concentrating on the biological and chemical dom
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38754;&#21521;&#20445;&#38505;&#20105;&#35758;&#30340;&#38889;&#22269;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#21477;&#23376;&#36716;&#25442;&#22120;&#24494;&#35843;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#19982;&#22823;&#22411;&#25968;&#25454;&#38598;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.14654</link><description>&lt;p&gt;
&#38754;&#21521;&#20445;&#38505;&#20105;&#35758;&#30340;&#38889;&#22269;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A Korean Legal Judgment Prediction Dataset for Insurance Disputes. (arXiv:2401.14654v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14654
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38754;&#21521;&#20445;&#38505;&#20105;&#35758;&#30340;&#38889;&#22269;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#21477;&#23376;&#36716;&#25442;&#22120;&#24494;&#35843;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#19982;&#22823;&#22411;&#25968;&#25454;&#38598;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38754;&#21521;&#20445;&#38505;&#20105;&#35758;&#30340;&#38889;&#22269;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#65288;LJP&#65289;&#25968;&#25454;&#38598;&#12290;&#25104;&#21151;&#39044;&#27979;&#20445;&#38505;&#20105;&#35758;&#30340;LJP&#27169;&#22411;&#21487;&#20197;&#20351;&#20445;&#38505;&#20844;&#21496;&#21450;&#20854;&#23458;&#25143;&#21463;&#30410;&#12290;&#23427;&#21487;&#20197;&#36890;&#36807;&#39044;&#27979;&#22914;&#26524;&#36827;&#34892;&#20105;&#35758;&#35843;&#35299;&#36807;&#31243;&#65292;&#32467;&#26524;&#23558;&#22914;&#20309;&#20986;&#29616;&#26469;&#33410;&#30465;&#21452;&#26041;&#30340;&#26102;&#38388;&#21644;&#37329;&#38065;&#12290;&#27491;&#22914;&#20302;&#36164;&#28304;&#35821;&#35328;&#32463;&#24120;&#38754;&#20020;&#30340;&#24773;&#20917;&#19968;&#26679;&#65292;&#35813;&#29305;&#23450;&#20219;&#21153;&#30340;&#21487;&#29992;&#25968;&#25454;&#37327;&#26377;&#38480;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21477;&#23376;&#36716;&#25442;&#22120;&#24494;&#35843;&#65288;SetFit&#65289;&#65288;Tunstall&#31561;&#65292;2022&#65289;&#26159;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#30340;&#26631;&#20934;&#24494;&#35843;&#30340;&#33391;&#22909;&#26367;&#20195;&#26041;&#27861;&#12290;&#20351;&#29992;SetFit&#26041;&#27861;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#19982;&#38889;&#22269;LJP&#22522;&#20934;&#27169;&#22411;&#65288;Hwang&#31561;&#65292;2022&#65289;&#22312;&#24615;&#33021;&#19978;&#26174;&#31034;&#20986;&#30456;&#20284;&#30340;&#34920;&#29616;&#65292;&#23613;&#31649;&#25968;&#25454;&#35268;&#27169;&#35201;&#23567;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a Korean legal judgment prediction (LJP) dataset for insurance disputes. Successful LJP models on insurance disputes can benefit insurance companies and their customers. It can save both sides' time and money by allowing them to predict how the result would come out if they proceed to the dispute mediation process. As is often the case with low-resource languages, there is a limitation on the amount of data available for this specific task. To mitigate this issue, we investigate how one can achieve a good performance despite the limitation in data. In our experiment, we demonstrate that Sentence Transformer Fine-tuning (SetFit, Tunstall et al., 2022) is a good alternative to standard fine-tuning when training data are limited. The models fine-tuned with the SetFit approach on our data show similar performance to the Korean LJP benchmark models (Hwang et al., 2022) despite the much smaller data size.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#38382;&#39064;&#22238;&#31572;&#24402;&#22240;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#35780;&#20272;&#22120;&#22312;&#32454;&#31890;&#24230;&#30340;&#24402;&#22240;&#35774;&#32622;&#19979;&#34920;&#29616;&#19981;&#20339;&#65292;&#21516;&#26102;&#22312;&#22797;&#26434;&#30340;&#24341;&#25991;-&#38472;&#36848;&#25512;&#29702;&#20013;&#20063;&#23384;&#22312;&#24369;&#28857;&#12290;</title><link>http://arxiv.org/abs/2401.14640</link><description>&lt;p&gt;
&#22312;&#22797;&#26434;&#38382;&#39064;&#22238;&#31572;&#24402;&#22240;&#20013;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Large Language Models in Complex Question Answering Attribution using Knowledge Graphs. (arXiv:2401.14640v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14640
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#38382;&#39064;&#22238;&#31572;&#24402;&#22240;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#35780;&#20272;&#22120;&#22312;&#32454;&#31890;&#24230;&#30340;&#24402;&#22240;&#35774;&#32622;&#19979;&#34920;&#29616;&#19981;&#20339;&#65292;&#21516;&#26102;&#22312;&#22797;&#26434;&#30340;&#24341;&#25991;-&#38472;&#36848;&#25512;&#29702;&#20013;&#20063;&#23384;&#22312;&#24369;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#39064;&#22238;&#31572;&#30340;&#24402;&#22240;&#26159;&#20026;&#29983;&#25104;&#30340;&#38472;&#36848;&#25552;&#20379;&#24341;&#29992;, &#24182;&#19988;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#20851;&#27880;&#12290;&#30446;&#21069;&#30340;&#33258;&#21160;&#35780;&#20272;&#24402;&#22240;&#30340;&#26041;&#27861;&#24448;&#24448;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM), &#20294;&#20173;&#28982;&#19981;&#36275;, &#29305;&#21035;&#26159;&#22312;&#35782;&#21035;&#24402;&#22240;&#20043;&#38388;&#32454;&#24494;&#24046;&#21035;&#21644;&#24341;&#29992;&#19982;&#38472;&#36848;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#26041;&#38754;&#12290;&#20026;&#20102;&#27604;&#36739;&#36825;&#20123;&#24402;&#22240;&#35780;&#20272;&#26041;&#27861;&#24182;&#24320;&#21457;&#26032;&#30340;&#26041;&#27861;, &#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#32452;&#32454;&#31890;&#24230;&#30340;&#31867;&#21035;(&#21363;&#25903;&#25345;, &#19981;&#36275;, &#30683;&#30462;&#21644;&#26080;&#20851;), &#29992;&#20110;&#34913;&#37327;&#24402;&#22240;, &#24182;&#36890;&#36807;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;(KG)&#20026;&#38382;&#39064;-&#22238;&#31572;&#23545;&#33258;&#21160;&#29983;&#25104;&#19981;&#21516;&#31867;&#21035;&#30340;&#24402;&#22240;, &#24320;&#21457;&#20102;&#19968;&#20010;&#22797;&#26434;&#30340;&#24402;&#22240;&#38382;&#39064;&#22238;&#31572;(CAQA)&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#26174;&#31034;, &#29616;&#26377;&#30340;&#35780;&#20272;&#22120;&#22312;&#32454;&#31890;&#24230;&#30340;&#24402;&#22240;&#35774;&#32622;&#19979;&#34920;&#29616;&#19981;&#20339;, &#22312;&#22797;&#26434;&#30340;&#24341;&#25991;-&#38472;&#36848;&#25512;&#29702;&#20013;&#23384;&#22312;&#24369;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The attribution of question answering is to provide citations for supporting generated statements, and has attracted wide research attention. The current methods for automatically evaluating the attribution, which are often based on Large Language Models (LLMs), are still inadequate, particularly in recognizing subtle differences between attributions, and complex relationships between citations and statements. To compare these attribution evaluation methods and develop new ones, we introduce a set of fine-grained categories (i.e., supportive, insufficient, contradictory and irrelevant) for measuring the attribution, and develop a Complex Attributed Question Answering (CAQA) benchmark by leveraging knowledge graphs (KGs) for automatically generating attributions of different categories to question-answer pairs. Our analysis reveals that existing evaluators perform poorly under fine-grained attribution settings and exhibit weaknesses in complex citation-statement reasoning. Our CAQA benc
&lt;/p&gt;</description></item><item><title>T-Rex&#26159;&#19968;&#31181;&#21033;&#29992;&#25991;&#26412;&#20449;&#24687;&#36741;&#21161;&#21453;&#21521;&#21512;&#25104;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#22522;&#20110;&#22270;&#32467;&#26500;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.14637</link><description>&lt;p&gt;
T-Rex: &#25991;&#26412;&#36741;&#21161;&#30340;&#21453;&#21521;&#21512;&#25104;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
T-Rex: Text-assisted Retrosynthesis Prediction. (arXiv:2401.14637v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14637
&lt;/p&gt;
&lt;p&gt;
T-Rex&#26159;&#19968;&#31181;&#21033;&#29992;&#25991;&#26412;&#20449;&#24687;&#36741;&#21161;&#21453;&#21521;&#21512;&#25104;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#22522;&#20110;&#22270;&#32467;&#26500;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#35745;&#31639;&#21270;&#23398;&#20013;&#30340;&#22522;&#30784;&#20219;&#21153;&#65292;&#21453;&#21521;&#21512;&#25104;&#39044;&#27979;&#26088;&#22312;&#35782;&#21035;&#19968;&#32452;&#21453;&#24212;&#29289;&#26469;&#21512;&#25104;&#30446;&#26631;&#20998;&#23376;&#12290;&#29616;&#26377;&#30340;&#26080;&#27169;&#26495;&#26041;&#27861;&#21482;&#32771;&#34385;&#30446;&#26631;&#20998;&#23376;&#30340;&#22270;&#32467;&#26500;&#65292;&#24448;&#24448;&#19981;&#33021;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#32597;&#35265;&#30340;&#21453;&#24212;&#31867;&#22411;&#21644;&#22823;&#20998;&#23376;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;T-Rex&#65292;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#26469;&#36741;&#21161;&#29983;&#25104;&#21453;&#24212;&#29289;&#30340;&#21453;&#21521;&#21512;&#25104;&#39044;&#27979;&#26041;&#27861;&#12290;T-Rex&#39318;&#20808;&#21033;&#29992;ChatGPT&#20026;&#30446;&#26631;&#20998;&#23376;&#29983;&#25104;&#25551;&#36848;&#65292;&#24182;&#22522;&#20110;&#25551;&#36848;&#21644;&#20998;&#23376;&#22270;&#23545;&#20505;&#36873;&#21453;&#24212;&#20013;&#24515;&#36827;&#34892;&#25490;&#21517;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#26597;&#35810;&#27599;&#20010;&#21453;&#24212;&#29289;&#30340;&#25551;&#36848;&#65292;&#24182;&#26816;&#26597;&#21738;&#32452;&#21453;&#24212;&#29289;&#26368;&#33021;&#21512;&#25104;&#30446;&#26631;&#20998;&#23376;&#65292;&#37325;&#26032;&#23545;&#36825;&#20123;&#20505;&#36873;&#36827;&#34892;&#25490;&#21517;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;T-Rex&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#22522;&#20110;&#22270;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#36825;&#34920;&#26126;&#32771;&#34385;&#25991;&#26412;&#20449;&#24687;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a fundamental task in computational chemistry, retrosynthesis prediction aims to identify a set of reactants to synthesize a target molecule. Existing template-free approaches only consider the graph structures of the target molecule, which often cannot generalize well to rare reaction types and large molecules. Here, we propose T-Rex, a text-assisted retrosynthesis prediction approach that exploits pre-trained text language models, such as ChatGPT, to assist the generation of reactants. T-Rex first exploits ChatGPT to generate a description for the target molecule and rank candidate reaction centers based both the description and the molecular graph. It then re-ranks these candidates by querying the descriptions for each reactants and examines which group of reactants can best synthesize the target molecule. We observed that T-Rex substantially outperformed graph-based state-of-the-art approaches on two datasets, indicating the effectiveness of considering text information. We furt
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#26500;&#24314;&#39046;&#22495;&#29305;&#23450;&#26415;&#35821;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#23545;&#21508;&#31181;&#20856;&#22411;&#30340;&#20013;&#25991;&#25340;&#20889;&#26816;&#26597;&#65288;CSC&#65289;&#27169;&#22411;&#30340;&#39046;&#22495;&#36866;&#24212;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#26032;&#39046;&#22495;&#20013;&#30340;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2401.14630</link><description>&lt;p&gt;
&#12298;&#23545;&#20013;&#25991;&#25340;&#20889;&#26816;&#26597;&#27169;&#22411;&#30340;&#39046;&#22495;&#36866;&#24212;&#33021;&#21147;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#12299;
&lt;/p&gt;
&lt;p&gt;
An Empirical Investigation of Domain Adaptation Ability for Chinese Spelling Check Models. (arXiv:2401.14630v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14630
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#26500;&#24314;&#39046;&#22495;&#29305;&#23450;&#26415;&#35821;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#23545;&#21508;&#31181;&#20856;&#22411;&#30340;&#20013;&#25991;&#25340;&#20889;&#26816;&#26597;&#65288;CSC&#65289;&#27169;&#22411;&#30340;&#39046;&#22495;&#36866;&#24212;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#26032;&#39046;&#22495;&#20013;&#30340;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20013;&#25991;&#25340;&#20889;&#26816;&#26597;&#65288;CSC&#65289;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#20013;&#19968;&#39033;&#26377;&#24847;&#20041;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#26816;&#27979;&#20013;&#25991;&#25991;&#26412;&#20013;&#30340;&#25340;&#20889;&#38169;&#35823;&#65292;&#28982;&#21518;&#23545;&#36825;&#20123;&#38169;&#35823;&#36827;&#34892;&#32416;&#27491;&#12290;&#28982;&#32780;&#65292;CSC&#27169;&#22411;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#19968;&#33324;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#24403;&#38754;&#20020;&#28041;&#21450;&#29305;&#23450;&#39046;&#22495;&#26415;&#35821;&#30340;&#19979;&#28216;&#20219;&#21153;&#26102;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#19979;&#38477;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#19977;&#20010;&#21253;&#21547;&#36130;&#32463;&#12289;&#21307;&#30103;&#21644;&#27861;&#24459;&#39046;&#22495;&#20016;&#23500;&#39046;&#22495;&#29305;&#23450;&#26415;&#35821;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#23545;&#21508;&#31181;&#20856;&#22411;&#30340;CSC&#27169;&#22411;&#30340;&#39046;&#22495;&#36866;&#24212;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#30456;&#24212;&#39046;&#22495;&#29305;&#23450;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#20197;&#30830;&#23450;&#20960;&#31181;&#20856;&#22411;CSC&#27169;&#22411;&#30340;&#36328;&#22495;&#36866;&#24212;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#27979;&#35797;&#20102;&#27969;&#34892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGPT&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#26032;&#39046;&#22495;&#20013;&#65292;CSC&#27169;&#22411;&#30340;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chinese Spelling Check (CSC) is a meaningful task in the area of Natural Language Processing (NLP) which aims at detecting spelling errors in Chinese texts and then correcting these errors. However, CSC models are based on pretrained language models, which are trained on a general corpus. Consequently, their performance may drop when confronted with downstream tasks involving domain-specific terms. In this paper, we conduct a thorough evaluation about the domain adaption ability of various typical CSC models by building three new datasets encompassing rich domain-specific terms from the financial, medical, and legal domains. Then we conduct empirical investigations in the corresponding domain-specific test datasets to ascertain the cross-domain adaptation ability of several typical CSC models. We also test the performance of the popular large language model ChatGPT. As shown in our experiments, the performances of the CSC models drop significantly in the new domains.
&lt;/p&gt;</description></item><item><title>&#20256;&#32479;ASR&#35780;&#20272;&#26041;&#27861;&#26080;&#27861;&#20840;&#38754;&#20102;&#35299;&#29305;&#23450;&#33030;&#24369;&#24615;&#65292;&#21516;&#26102;&#21518;&#22788;&#29702;&#38454;&#27573;&#20063;&#32570;&#20047;&#35814;&#32454;&#20449;&#24687;&#65292;&#36825;&#23545;&#29992;&#25143;&#21451;&#22909;&#24615;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#38656;&#35201;&#32508;&#21512;&#32771;&#34385;&#35821;&#38899;&#23618;&#38754;&#21644;&#25991;&#26412;&#23618;&#38754;&#65292;&#24182;&#25552;&#20986;&#20102;&#38169;&#35823;&#21487;&#35299;&#37322;&#24615;&#22522;&#20934;&#25351;&#21335;&#12290;</title><link>http://arxiv.org/abs/2401.14625</link><description>&lt;p&gt;
&#36808;&#21521;&#23454;&#29992;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#21644;&#21518;&#22788;&#29702;&#65306;&#20851;&#20110;&#21487;&#35299;&#37322;&#38169;&#35823;&#22522;&#20934;&#25351;&#21335;&#30340;&#21628;&#21505;
&lt;/p&gt;
&lt;p&gt;
Toward Practical Automatic Speech Recognition and Post-Processing: a Call for Explainable Error Benchmark Guideline. (arXiv:2401.14625v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14625
&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;ASR&#35780;&#20272;&#26041;&#27861;&#26080;&#27861;&#20840;&#38754;&#20102;&#35299;&#29305;&#23450;&#33030;&#24369;&#24615;&#65292;&#21516;&#26102;&#21518;&#22788;&#29702;&#38454;&#27573;&#20063;&#32570;&#20047;&#35814;&#32454;&#20449;&#24687;&#65292;&#36825;&#23545;&#29992;&#25143;&#21451;&#22909;&#24615;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#38656;&#35201;&#32508;&#21512;&#32771;&#34385;&#35821;&#38899;&#23618;&#38754;&#21644;&#25991;&#26412;&#23618;&#38754;&#65292;&#24182;&#25552;&#20986;&#20102;&#38169;&#35823;&#21487;&#35299;&#37322;&#24615;&#22522;&#20934;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#30340;&#32467;&#26524;&#20316;&#20026;&#19979;&#28216;&#20219;&#21153;&#30340;&#36755;&#20837;&#65292;&#26497;&#22823;&#22320;&#24433;&#21709;&#30528;&#26368;&#32456;&#29992;&#25143;&#30340;&#28385;&#24847;&#24230;&#12290;&#22240;&#27492;&#65292;&#35786;&#26029;&#21644;&#22686;&#24378;ASR&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#24369;&#28857;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;ASR&#31995;&#32479;&#35780;&#20272;&#26041;&#27861;&#29983;&#25104;&#19968;&#20010;&#21333;&#19968;&#30340;&#12289;&#32508;&#21512;&#30340;&#23450;&#37327;&#25351;&#26631;&#65292;&#26080;&#27861;&#25552;&#20379;&#23545;&#29305;&#23450;&#24369;&#28857;&#30340;&#20840;&#38754;&#27934;&#23519;&#12290;&#36825;&#31181;&#32570;&#20047;&#32454;&#33410;&#20063;&#24310;&#20280;&#21040;&#21518;&#22788;&#29702;&#38454;&#27573;&#65292;&#36827;&#19968;&#27493;&#28151;&#28102;&#20102;&#28508;&#22312;&#30340;&#24369;&#28857;&#12290;&#23613;&#31649;ASR&#27169;&#22411;&#26377;&#33021;&#21147;&#20934;&#30830;&#35782;&#21035;&#35805;&#35821;&#65292;&#20294;&#35835;&#32773;&#24863;&#30693;&#24230;&#36739;&#24046;&#21487;&#33021;&#23545;&#29992;&#25143;&#28385;&#24847;&#24230;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#20174;&#32780;&#22312;&#35782;&#21035;&#20934;&#30830;&#24230;&#21644;&#29992;&#25143;&#21451;&#22909;&#24615;&#20043;&#38388;&#20135;&#29983;&#19968;&#31181;&#26435;&#34913;&#12290;&#20026;&#20102;&#26377;&#25928;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24517;&#39035;&#21516;&#26102;&#32771;&#34385;&#21040;&#35821;&#38899;&#23618;&#38754;&#65288;&#23545;&#20110;&#35782;&#21035;&#20934;&#30830;&#24615;&#33267;&#20851;&#37325;&#35201;&#65289;&#21644;&#25991;&#26412;&#23618;&#38754;&#65288;&#23545;&#20110;&#29992;&#25143;&#21451;&#22909;&#24615;&#33267;&#20851;&#37325;&#35201;&#65289;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#35758;&#24320;&#21457;&#19968;&#20010;&#38169;&#35823;&#21487;&#35299;&#37322;&#24615;&#22522;&#20934;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic speech recognition (ASR) outcomes serve as input for downstream tasks, substantially impacting the satisfaction level of end-users. Hence, the diagnosis and enhancement of the vulnerabilities present in the ASR model bear significant importance. However, traditional evaluation methodologies of ASR systems generate a singular, composite quantitative metric, which fails to provide comprehensive insight into specific vulnerabilities. This lack of detail extends to the post-processing stage, resulting in further obfuscation of potential weaknesses. Despite an ASR model's ability to recognize utterances accurately, subpar readability can negatively affect user satisfaction, giving rise to a trade-off between recognition accuracy and user-friendliness. To effectively address this, it is imperative to consider both the speech-level, crucial for recognition accuracy, and the text-level, critical for user-friendliness. Consequently, we propose the development of an Error Explainable B
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25910;&#38598;&#29305;&#23450;&#39046;&#22495;&#30693;&#35782;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#35813;&#26041;&#27861;&#26500;&#24314;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#21517;&#20026;&#8220;Knowledge Pile&#8221;&#30340;&#25968;&#25454;&#38598;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26174;&#33879;&#25913;&#21892;&#20102;&#29305;&#23450;&#39046;&#22495;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.14624</link><description>&lt;p&gt;
CC&#26597;&#35810;&#65306;&#20174;&#20844;&#24320;&#25991;&#29486;&#20013;&#21457;&#29616;&#22823;&#35268;&#27169;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Query of CC: Unearthing Large Scale Domain-Specific Knowledge from Public Corpora. (arXiv:2401.14624v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14624
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25910;&#38598;&#29305;&#23450;&#39046;&#22495;&#30693;&#35782;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#35813;&#26041;&#27861;&#26500;&#24314;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#21517;&#20026;&#8220;Knowledge Pile&#8221;&#30340;&#25968;&#25454;&#38598;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26174;&#33879;&#25913;&#21892;&#20102;&#29305;&#23450;&#39046;&#22495;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#28508;&#21147;&#65292;&#28982;&#32780;&#29305;&#23450;&#39046;&#22495;&#30340;&#24320;&#28304;&#27169;&#22411;&#21644;&#25968;&#25454;&#20173;&#28982;&#38750;&#24120;&#31232;&#32570;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#25163;&#21160;&#25351;&#23450;&#36164;&#28304;&#21644;&#25910;&#38598;&#29305;&#23450;&#39046;&#22495;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#65292;&#36825;&#28040;&#32791;&#20102;&#22823;&#37327;&#26102;&#38388;&#21644;&#31934;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#8220;CC&#26597;&#35810;&#8221;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#31181;&#23376;&#20449;&#24687;&#65292;&#24182;&#20174;&#20844;&#24320;&#25991;&#29486;&#20013;&#26816;&#32034;&#30456;&#20851;&#25968;&#25454;&#12290;&#23427;&#19981;&#20165;&#25910;&#38598;&#20102;&#29305;&#23450;&#39046;&#22495;&#30340;&#30693;&#35782;&#30456;&#20851;&#25968;&#25454;&#65292;&#36824;&#25581;&#31034;&#20102;&#28508;&#22312;&#30340;&#25512;&#29702;&#36807;&#31243;&#25968;&#25454;&#12290;&#36890;&#36807;&#24212;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;Knowledge Pile&#8221;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#21253;&#25324;STEM&#31185;&#23398;&#21644;&#20154;&#25991;&#31185;&#23398;&#22312;&#20869;&#30340;&#22235;&#20010;&#20027;&#35201;&#39046;&#22495;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#8220;Knowledge Pile&#8221;&#26174;&#33879;&#25913;&#21892;&#20102;
&lt;/p&gt;
&lt;p&gt;
Large language models have demonstrated remarkable potential in various tasks, however, there remains a significant scarcity of open-source models and data for specific domains. Previous works have primarily focused on manually specifying resources and collecting high-quality data on specific domains, which significantly consume time and effort. To address this limitation, we propose an efficient data collection method~\textit{Query of CC} based on large language models. This method bootstraps seed information through a large language model and retrieves related data from public corpora. It not only collects knowledge-related data for specific domains but unearths the data with potential reasoning procedures. Through the application of this method, we have curated a high-quality dataset called~\textsc{Knowledge Pile}, encompassing four major domains, including stem and humanities sciences, among others. Experimental results demonstrate that~\textsc{Knowledge Pile} significantly improve
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#8220;&#26367;&#20195;&#24615;&#35328;&#35770;&#8221;&#20316;&#20026;&#30452;&#25509;&#23545;&#25239;&#20167;&#24680;&#35328;&#35770;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#26500;&#24314;&#25152;&#38656;&#25968;&#25454;&#38598;&#30340;&#25351;&#23548;&#12290;&#26367;&#20195;&#24615;&#35328;&#35770;&#36890;&#36807;&#25552;&#20379;&#23454;&#38469;&#21487;&#34892;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#32771;&#34385;&#29615;&#22659;&#22240;&#32032;&#24182;&#20419;&#20351;&#28436;&#35762;&#32773;&#25913;&#21464;&#65292;&#20026;&#35299;&#20915;&#31038;&#20250;&#38382;&#39064;&#25552;&#20379;&#26377;&#29992;&#24037;&#20855;&#12290;&#23558;&#26367;&#20195;&#24615;&#35328;&#35770;&#19982;&#23545;&#25239;&#21465;&#20107;&#32467;&#21512;&#20351;&#29992;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#23545;&#25239;&#20167;&#24680;&#35328;&#35770;&#65292;&#34917;&#20805;&#20102;&#23545;&#25239;&#21465;&#20107;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.14616</link><description>&lt;p&gt;
&#26367;&#20195;&#24615;&#35328;&#35770;&#65306;&#34917;&#20805;&#23545;&#25239;&#21465;&#20107;&#30340;&#26041;&#27861;&#20197;&#25913;&#21892;&#35752;&#35770;&#65288;arXiv:2401.14616v1 [cs.CL]&#65289;
&lt;/p&gt;
&lt;p&gt;
Alternative Speech: Complementary Method to Counter-Narrative for Better Discourse. (arXiv:2401.14616v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14616
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#8220;&#26367;&#20195;&#24615;&#35328;&#35770;&#8221;&#20316;&#20026;&#30452;&#25509;&#23545;&#25239;&#20167;&#24680;&#35328;&#35770;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#26500;&#24314;&#25152;&#38656;&#25968;&#25454;&#38598;&#30340;&#25351;&#23548;&#12290;&#26367;&#20195;&#24615;&#35328;&#35770;&#36890;&#36807;&#25552;&#20379;&#23454;&#38469;&#21487;&#34892;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#32771;&#34385;&#29615;&#22659;&#22240;&#32032;&#24182;&#20419;&#20351;&#28436;&#35762;&#32773;&#25913;&#21464;&#65292;&#20026;&#35299;&#20915;&#31038;&#20250;&#38382;&#39064;&#25552;&#20379;&#26377;&#29992;&#24037;&#20855;&#12290;&#23558;&#26367;&#20195;&#24615;&#35328;&#35770;&#19982;&#23545;&#25239;&#21465;&#20107;&#32467;&#21512;&#20351;&#29992;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#23545;&#25239;&#20167;&#24680;&#35328;&#35770;&#65292;&#34917;&#20805;&#20102;&#23545;&#25239;&#21465;&#20107;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#26367;&#20195;&#24615;&#35328;&#35770;&#8221;&#30340;&#27010;&#24565;&#65292;&#20316;&#20026;&#30452;&#25509;&#23545;&#25239;&#20167;&#24680;&#35328;&#35770;&#21644;&#34917;&#20805;&#21465;&#20107;&#38480;&#21046;&#30340;&#26032;&#26041;&#24335;&#12290;&#26367;&#20195;&#24615;&#35328;&#35770;&#36890;&#36807;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#25552;&#20379;&#35328;&#35770;&#32423;&#21035;&#30340;&#20462;&#27491;&#65292;&#32771;&#34385;&#21608;&#22260;&#29615;&#22659;&#24182;&#20419;&#20351;&#28436;&#35762;&#32773;&#25913;&#21464;&#65292;&#20026;&#20167;&#24680;&#35328;&#35770;&#25552;&#20379;&#23454;&#38469;&#21487;&#34892;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#26367;&#20195;&#24615;&#35328;&#35770;&#21487;&#20197;&#19982;&#23545;&#25239;&#21465;&#20107;&#19968;&#36215;&#23545;&#25239;&#20167;&#24680;&#35328;&#35770;&#65292;&#20026;&#35299;&#20915;&#31181;&#26063;&#27495;&#35270;&#21644;&#24615;&#21035;&#19981;&#24179;&#31561;&#31561;&#31038;&#20250;&#38382;&#39064;&#25552;&#20379;&#26377;&#29992;&#24037;&#20855;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36825;&#20010;&#26032;&#27010;&#24565;&#65292;&#24182;&#25552;&#20379;&#20102;&#26500;&#24314;&#25152;&#38656;&#25968;&#25454;&#38598;&#30340;&#35814;&#32454;&#25351;&#23548;&#12290;&#36890;&#36807;&#35752;&#35770;&#65292;&#25105;&#20204;&#35777;&#26126;&#23558;&#26367;&#20195;&#24615;&#35328;&#35770;&#19982;&#23545;&#25239;&#21465;&#20107;&#30456;&#32467;&#21512;&#21487;&#20197;&#26159;&#23545;&#25239;&#20167;&#24680;&#35328;&#35770;&#26356;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#34917;&#20805;&#20102;&#23545;&#25239;&#21465;&#20107;&#30340;&#20855;&#20307;&#24615;&#21644;&#24341;&#23548;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22788;&#29702;&#20167;&#24680;&#35328;&#35770;&#30340;&#21478;&#19968;&#31181;&#35270;&#35282;&#65292;&#25552;&#20379;&#20102;&#21487;&#34892;&#30340;&#34917;&#25937;&#25514;&#26045;&#26469;&#34917;&#20805;&#24403;&#21069;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the concept of "Alternative Speech" as a new way to directly combat hate speech and complement the limitations of counter-narrative. An alternative speech provides practical alternatives to hate speech in real-world scenarios by offering speech-level corrections to speakers while considering the surrounding context and promoting speakers to reform. Further, an alternative speech can combat hate speech alongside counter-narratives, offering a useful tool to address social issues such as racial discrimination and gender inequality. We propose the new concept and provide detailed guidelines for constructing the necessary dataset. Through discussion, we demonstrate that combining alternative speech and counter-narrative can be a more effective strategy for combating hate speech by complementing specificity and guiding capacity of counter-narrative. This paper presents another perspective for dealing with hate speech, offering viable remedies to complement the constraints of cu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#26234;&#33021;&#20307;&#23545;&#35805;&#30340;&#26041;&#24335;&#26469;&#20943;&#36731;&#20020;&#24202;&#20915;&#31574;&#20013;&#30340;&#35748;&#30693;&#20559;&#24046;&#65292;&#24182;&#35780;&#20272;&#20854;&#23545;&#25552;&#39640;&#35786;&#26029;&#20934;&#30830;&#24615;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.14589</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#23545;&#35805;&#25552;&#39640;&#35786;&#26029;&#20934;&#30830;&#24230;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20943;&#23569;&#35748;&#30693;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Enhancing Diagnostic Accuracy through Multi-Agent Conversations: Using Large Language Models to Mitigate Cognitive Bias. (arXiv:2401.14589v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#26234;&#33021;&#20307;&#23545;&#35805;&#30340;&#26041;&#24335;&#26469;&#20943;&#36731;&#20020;&#24202;&#20915;&#31574;&#20013;&#30340;&#35748;&#30693;&#20559;&#24046;&#65292;&#24182;&#35780;&#20272;&#20854;&#23545;&#25552;&#39640;&#35786;&#26029;&#20934;&#30830;&#24615;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#20020;&#24202;&#20915;&#31574;&#20013;&#30340;&#35748;&#30693;&#20559;&#24046;&#26174;&#33879;&#23548;&#33268;&#35786;&#26029;&#38169;&#35823;&#21644;&#27425;&#20248;&#24739;&#32773;&#32467;&#26524;&#12290;&#35299;&#20915;&#36825;&#20123;&#20559;&#24046;&#38382;&#39064;&#22312;&#21307;&#30103;&#39046;&#22495;&#38754;&#20020;&#24040;&#22823;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22810;&#26234;&#33021;&#20307;&#26694;&#26550;&#20013;&#20943;&#36731;&#36825;&#20123;&#20559;&#24046;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#23545;&#35805;&#27169;&#25311;&#20020;&#24202;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#35780;&#20272;&#20854;&#23545;&#25913;&#21892;&#35786;&#26029;&#20934;&#30830;&#24615;&#30340;&#26377;&#25928;&#24615;&#12290;&#26041;&#27861;&#65306;&#20174;&#25991;&#29486;&#20013;&#25214;&#21040;&#20102;&#24635;&#20849;16&#20010;&#24050;&#21457;&#34920;&#21644;&#26410;&#21457;&#34920;&#30340;&#30149;&#20363;&#25253;&#21578;&#65292;&#20854;&#20013;&#35748;&#30693;&#20559;&#24046;&#23548;&#33268;&#35823;&#35786;&#12290;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992; GPT-4 Turbo &#20419;&#36827;&#22235;&#20010;&#27169;&#25311;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#20197;&#22797;&#21046;&#20020;&#24202;&#22242;&#38431;&#21160;&#24577;&#12290;&#27599;&#20010;&#26234;&#33021;&#20307;&#37117;&#26377;&#29420;&#29305;&#30340;&#35282;&#33394;&#65306;1) &#22312;&#32771;&#34385;&#35752;&#35770;&#21518;&#36827;&#34892;&#21021;&#27493;&#21644;&#26368;&#32456;&#35786;&#26029;&#12290;2) &#20805;&#24403;&#39764;&#39740;&#30340;&#20195;&#35328;&#20154;&#65292;&#20197;&#32416;&#27491;&#30830;&#35748;&#20559;&#24046;&#21644;&#38170;&#23450;&#20559;&#24046;&#12290;3) &#20805;&#24403;&#23548;&#24072;&#21644;&#20419;&#36827;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: Cognitive biases in clinical decision-making significantly contribute to errors in diagnosis and suboptimal patient outcomes. Addressing these biases presents a formidable challenge in the medical field. This study explores the role of large language models (LLMs) in mitigating these biases through the utilization of a multi-agent framework. We simulate the clinical decision-making processes through multi-agent conversation and evaluate its efficacy in improving diagnostic accuracy. Methods: A total of 16 published and unpublished case reports where cognitive biases have resulted in misdiagnoses were identified from the literature. In the multi-agent system, we leveraged GPT-4 Turbo to facilitate interactions among four simulated agents to replicate clinical team dynamics. Each agent has a distinct role: 1) To make the initial and final diagnosis after considering the discussions, 2) The devil's advocate and correct confirmation and anchoring bias, 3) The tutor and facilita
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#35821;&#35328;&#35782;&#21035;&#21644;&#20613;&#37324;&#21494;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#21382;&#21490;&#25991;&#29486;&#20013;&#20122;&#32654;&#23612;&#20122;&#22303;&#32819;&#20854;&#35821;&#30340;&#32467;&#26500;&#21270;&#35821;&#35328;&#20132;&#26367;&#12290;</title><link>http://arxiv.org/abs/2401.14569</link><description>&lt;p&gt;
&#32467;&#21512;&#35821;&#35328;&#35782;&#21035;&#21644;&#20613;&#37324;&#21494;&#20998;&#26512;&#65292;&#22312;&#21382;&#21490;&#25991;&#29486;&#20013;&#26816;&#27979;&#32467;&#26500;&#21270;&#35821;&#35328;&#20132;&#26367;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Detecting Structured Language Alternations in Historical Documents by Combining Language Identification with Fourier Analysis. (arXiv:2401.14569v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#35821;&#35328;&#35782;&#21035;&#21644;&#20613;&#37324;&#21494;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#21382;&#21490;&#25991;&#29486;&#20013;&#20122;&#32654;&#23612;&#20122;&#22303;&#32819;&#20854;&#35821;&#30340;&#32467;&#26500;&#21270;&#35821;&#35328;&#20132;&#26367;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#29992;&#20110;&#35782;&#21035;&#38750;&#26631;&#20934;&#35821;&#35328;&#21644;&#20070;&#20889;&#32452;&#21512;&#65288;&#20122;&#32654;&#23612;&#20122;&#22303;&#32819;&#20854;&#35821;&#65289;&#30340;&#21382;&#21490;&#35821;&#35328;&#25991;&#29486;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#26816;&#27979;&#25991;&#29486;&#20013;&#32467;&#26500;&#21270;&#35821;&#35328;&#20132;&#26367;&#39057;&#29575;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we present a generalizable workflow to identify documents in a historic language with a nonstandard language and script combination, Armeno-Turkish. We introduce the task of detecting distinct patterns of multilinguality based on the frequency of structured language alternations within a document.
&lt;/p&gt;</description></item><item><title>&#33258;&#36866;&#24212;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26469;&#22312;&#29305;&#23450;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#12289;&#25913;&#36827;&#32763;&#35793;&#36136;&#37327;&#65292;&#35299;&#20915;&#20102;&#22312;&#22495;&#36866;&#24212;&#20013;&#22240;&#32570;&#20047;&#22495;&#20869;&#25968;&#25454;&#32780;&#23548;&#33268;&#32763;&#35793;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.14559</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#26426;&#22120;&#32763;&#35793;&#30340;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Language Modelling Approaches to Adaptive Machine Translation. (arXiv:2401.14559v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14559
&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26469;&#22312;&#29305;&#23450;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#12289;&#25913;&#36827;&#32763;&#35793;&#36136;&#37327;&#65292;&#35299;&#20915;&#20102;&#22312;&#22495;&#36866;&#24212;&#20013;&#22240;&#32570;&#20047;&#22495;&#20869;&#25968;&#25454;&#32780;&#23548;&#33268;&#32763;&#35793;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#33268;&#24615;&#26159;&#39640;&#36136;&#37327;&#32763;&#35793;&#30340;&#20851;&#38190;&#35201;&#27714;&#12290;&#22312;&#29305;&#23450;&#39046;&#22495;&#39033;&#30446;&#20013;&#65292;&#36981;&#24490;&#39044;&#23450;&#20041;&#30340;&#26415;&#35821;&#21644;&#36866;&#24212;&#26356;&#27491;&#21518;&#30340;&#32763;&#35793;&#23588;&#20026;&#37325;&#35201;&#12290;&#26426;&#22120;&#32763;&#35793;&#22312;&#22495;&#36866;&#24212;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#32763;&#35793;&#29615;&#22659;&#20013;&#65292;&#30001;&#20110;&#32570;&#20047;&#19987;&#38376;&#30340;&#25968;&#25454;&#38598;&#21644;&#26415;&#35821;&#65292;&#25110;&#32773;&#21487;&#29992;&#30340;&#22495;&#20869;&#32763;&#35793;&#19981;&#19968;&#33268;&#19988;&#19981;&#20934;&#30830;&#65292;&#23548;&#33268;&#22495;&#20869;&#25968;&#25454;&#31232;&#32570;&#29616;&#35937;&#26222;&#36941;&#23384;&#22312;&#12290;&#22312;&#27809;&#26377;&#36275;&#22815;&#22495;&#20869;&#25968;&#25454;&#29992;&#20110;&#24494;&#35843;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#29983;&#25104;&#19982;&#30456;&#20851;&#19978;&#19979;&#25991;&#19968;&#33268;&#30340;&#32763;&#35793;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#34429;&#28982;&#23454;&#26102;&#36866;&#24212;&#21487;&#20197;&#21033;&#29992;&#36739;&#23569;&#37327;&#30340;&#22495;&#20869;&#25968;&#25454;&#23454;&#26102;&#25913;&#36827;&#32763;&#35793;&#36136;&#37327;&#65292;&#20294;&#30001;&#20110;&#21463;&#21040;&#19978;&#19979;&#25991;&#38480;&#21046;&#21644;&#25928;&#29575;&#32422;&#26463;&#65292;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#30340;&#26377;&#36259;&#33021;&#21147;&#65292;&#36890;&#36807;&#23398;&#20064;&#22797;&#21046;&#29305;&#23450;&#30340;&#36755;&#20837;&#36755;&#20986;&#26469;&#25913;&#36827;&#32763;&#35793;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consistency is a key requirement of high-quality translation. It is especially important to adhere to pre-approved terminology and adapt to corrected translations in domain-specific projects. Machine translation (MT) has achieved significant progress in the area of domain adaptation. However, in-domain data scarcity is common in translation settings, due to the lack of specialised datasets and terminology, or inconsistency and inaccuracy of available in-domain translations. In such scenarios where there is insufficient in-domain data to fine-tune MT models, producing translations that are consistent with the relevant context is challenging. While real-time adaptation can make use of smaller amounts of in-domain data to improve the translation on the fly, it remains challenging due to supported context limitations and efficiency constraints. Large language models (LLMs) have recently shown interesting capabilities of in-context learning, where they learn to replicate certain input-outpu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#35299;&#30721;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24207;&#21015;&#26631;&#27880;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#25506;&#32034;&#20102;&#25552;&#39640;&#23427;&#20204;&#24615;&#33021;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2401.14556</link><description>&lt;p&gt;
&#19981;&#19968;&#23450;&#24635;&#26159;&#21521;&#21491;&#30475;&#65306;&#30740;&#31350;&#22522;&#20110;&#35299;&#30721;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24207;&#21015;&#26631;&#27880;&#20013;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Do Not (Always) Look Right: Investigating the Capabilities of Decoder-Based Large Language Models for Sequence Labeling. (arXiv:2401.14556v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14556
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#35299;&#30721;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24207;&#21015;&#26631;&#27880;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#25506;&#32034;&#20102;&#25552;&#39640;&#23427;&#20204;&#24615;&#33021;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#30446;&#26631;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#34429;&#28982;&#19982;&#30456;&#20284;&#35268;&#27169;&#30340;&#22240;&#26524;&#35821;&#35328;&#24314;&#27169;&#35299;&#30721;&#22120;&#30456;&#27604;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;MLM-based&#32534;&#30721;&#22120;&#22987;&#32456;&#34920;&#29616;&#20248;&#24322;&#65292;&#20294;&#26368;&#36817;&#23558;&#35299;&#30721;&#22120;&#27169;&#22411;&#25193;&#23637;&#33267;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#36235;&#21183;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#19982;MLM-based&#32534;&#30721;&#22120;&#30456;&#25239;&#34913;&#12290;&#23613;&#31649;&#35268;&#27169;&#25193;&#22823;&#20102;&#23427;&#20204;&#22312;NLU&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#20294;LLMs&#22312;&#20449;&#24687;&#25552;&#21462;&#65288;IE&#65289;&#20219;&#21153;&#20013;&#65292;&#23588;&#20854;&#26159;&#24207;&#21015;&#26631;&#27880;&#65288;SL&#65289;&#20219;&#21153;&#26041;&#38754;&#20173;&#28982;&#33853;&#21518;&#20110;SOTA&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;LLMs&#30340;SL&#24615;&#33021;&#26159;&#30001;&#20854;&#22266;&#26377;&#30340;&#38480;&#21046;&#20915;&#23450;&#30340;&#36824;&#26159;&#21487;&#20197;&#25913;&#36827;&#30340;&#65292;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#25913;&#36827;"&#24320;&#25918;&#24335;"LLMs&#65288;Llama2&#21644;Mistral&#65289;&#22312;IE&#20219;&#21153;&#20013;&#30340;SL&#24615;&#33021;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#35299;&#30721;&#22120;&#22359;&#32452;&#20869;&#30340;&#21452;&#21521;&#20449;&#24687;&#27969;&#65292;&#24212;&#29992;&#20102;&#23618;&#27425;&#36880;&#23618;&#31227;&#38500;&#25110;&#21551;&#29992;&#22240;&#26524;&#25513;&#30721;&#65288;CM&#65289;&#36827;&#26469;LLM&#24494;&#35843;&#12290;&#36825;&#31181;&#26041;&#27861;&#20135;&#29983;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models based on masked language modeling (MLM) objective excel in natural language understanding (NLU) tasks. While fine-tuned MLM-based encoders consistently outperform causal language modeling decoders of comparable size, a recent trend of scaling decoder models to multiple billion parameters resulted in large language models (LLMs), making them competitive with MLM-based encoders. Although scale amplifies their prowess in NLU tasks, LLMs fall short of SOTA results in information extraction (IE) tasks, many framed as sequence labeling (SL). However, whether this is an intrinsic limitation of LLMs or whether their SL performance can be improved remains unclear. To address this, we explore strategies to enhance the SL performance of "open" LLMs (Llama2 and Mistral) on IE tasks. We investigate bidirectional information flow within groups of decoder blocks, applying layer-wise removal or enforcement of the causal mask (CM) during LLM fine-tuning. This approach yields
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20570;&#36873;&#25321;&#26102;&#34920;&#29616;&#20986;&#20102;&#19982;&#20154;&#31867;&#21644;&#21160;&#29289;&#30456;&#20284;&#30340;&#30456;&#23545;&#20215;&#20540;&#20559;&#24046;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#20154;&#31867;&#36873;&#25321;&#20013;&#30340;&#32972;&#26223;&#20381;&#36182;&#24615;&#26426;&#21046;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2401.14530</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30456;&#23545;&#20215;&#20540;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Relative Value Biases in Large Language Models. (arXiv:2401.14530v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14530
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20570;&#36873;&#25321;&#26102;&#34920;&#29616;&#20986;&#20102;&#19982;&#20154;&#31867;&#21644;&#21160;&#29289;&#30456;&#20284;&#30340;&#30456;&#23545;&#20215;&#20540;&#20559;&#24046;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#20154;&#31867;&#36873;&#25321;&#20013;&#30340;&#32972;&#26223;&#20381;&#36182;&#24615;&#26426;&#21046;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21644;&#21160;&#29289;&#22312;&#24378;&#21270;&#23398;&#20064;&#26041;&#38754;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#37027;&#20123;&#36873;&#39033;&#19982;&#36739;&#20302;&#30340;&#32477;&#23545;&#22870;&#21169;&#30456;&#20851;&#65292;&#20182;&#20204;&#26356;&#20542;&#21521;&#20110;&#36873;&#25321;&#36807;&#21435;&#30456;&#23545;&#26356;&#22909;&#32467;&#26524;&#30340;&#36873;&#39033;&#12290;&#26412;&#30740;&#31350;&#27979;&#35797;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20250;&#34920;&#29616;&#20986;&#31867;&#20284;&#30340;&#20559;&#24046;&#12290;&#25105;&#20204;&#35753;gpt-4-1106-preview(GPT-4 Turbo)&#21644;Llama-2-70B&#22312;&#26368;&#22823;&#21270;&#22238;&#25253;&#30340;&#30446;&#26631;&#19979;&#21453;&#22797;&#22312;&#36873;&#39033;&#23545;&#20043;&#38388;&#36827;&#34892;&#36873;&#25321;&#12290;&#27599;&#20010;&#25552;&#31034;&#20013;&#37117;&#21253;&#21547;&#20102;&#20808;&#21069;&#32467;&#26524;&#30340;&#23436;&#25972;&#35760;&#24405;&#12290;&#20004;&#20010;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#19982;&#20154;&#31867;&#21644;&#21160;&#29289;&#35266;&#23519;&#21040;&#30340;&#30456;&#23545;&#20215;&#20540;&#20915;&#31574;&#20559;&#24046;&#31867;&#20284;&#30340;&#34892;&#20026;&#12290;&#26356;&#26126;&#30830;&#22320;&#36827;&#34892;&#32467;&#26524;&#20043;&#38388;&#30340;&#30456;&#23545;&#27604;&#36739;&#20250;&#25918;&#22823;&#36825;&#31181;&#20559;&#24046;&#65292;&#32780;&#20419;&#20351;&#27169;&#22411;&#20272;&#35745;&#39044;&#26399;&#32467;&#26524;&#20250;&#20351;&#20559;&#24046;&#28040;&#22833;&#12290;&#36825;&#20123;&#32467;&#26524;&#23545;&#20110;&#20102;&#35299;&#20154;&#31867;&#36873;&#25321;&#20013;&#36129;&#29486;&#21040;&#32972;&#26223;&#20381;&#36182;&#24615;&#30340;&#28508;&#22312;&#26426;&#21046;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Studies of reinforcement learning in humans and animals have demonstrated a preference for options that yielded relatively better outcomes in the past, even when those options are associated with lower absolute reward. The present study tested whether large language models would exhibit a similar bias. We had gpt-4-1106-preview (GPT-4 Turbo) and Llama-2-70B make repeated choices between pairs of options with the goal of maximizing payoffs. A complete record of previous outcomes was included in each prompt. Both models exhibited relative value decision biases similar to those observed in humans and animals. Making relative comparisons among outcomes more explicit magnified the bias, whereas prompting the models to estimate expected outcomes caused the bias to disappear. These results have implications for the potential mechanisms that contribute to context-dependent choice in human agents.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#36328;&#22810;&#35821;&#35328;&#30340;&#22996;&#23113;&#35821;&#22788;&#29702;&#65292;&#24182;&#36890;&#36807;&#22810;&#35821;&#35328;&#36716;&#25442;&#22120;&#27169;&#22411;&#25104;&#21151;&#28040;&#27495;&#28508;&#22312;&#22996;&#23113;&#26415;&#35821;&#12290;&#32467;&#26524;&#26174;&#31034;&#22810;&#35821;&#35328;&#27169;&#22411;&#22312;&#35813;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#21333;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#35777;&#26126;&#22810;&#35821;&#35328;&#25968;&#25454;&#23545;&#20110;&#36328;&#35821;&#35328;&#22996;&#23113;&#35821;&#30340;&#35745;&#31639;&#29305;&#24615;&#20855;&#26377;&#39069;&#22806;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.14526</link><description>&lt;p&gt;
MEDs for PETs: &#22810;&#35821;&#35328;&#22996;&#23113;&#35821;&#28040;&#27495;&#20026;&#28508;&#22312;&#22996;&#23113;&#26415;&#35821;
&lt;/p&gt;
&lt;p&gt;
MEDs for PETs: Multilingual Euphemism Disambiguation for Potentially Euphemistic Terms. (arXiv:2401.14526v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14526
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#36328;&#22810;&#35821;&#35328;&#30340;&#22996;&#23113;&#35821;&#22788;&#29702;&#65292;&#24182;&#36890;&#36807;&#22810;&#35821;&#35328;&#36716;&#25442;&#22120;&#27169;&#22411;&#25104;&#21151;&#28040;&#27495;&#28508;&#22312;&#22996;&#23113;&#26415;&#35821;&#12290;&#32467;&#26524;&#26174;&#31034;&#22810;&#35821;&#35328;&#27169;&#22411;&#22312;&#35813;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#21333;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#35777;&#26126;&#22810;&#35821;&#35328;&#25968;&#25454;&#23545;&#20110;&#36328;&#35821;&#35328;&#22996;&#23113;&#35821;&#30340;&#35745;&#31639;&#29305;&#24615;&#20855;&#26377;&#39069;&#22806;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#36328;&#22810;&#35821;&#35328;&#30340;&#22996;&#23113;&#35821;&#35745;&#31639;&#22788;&#29702;&#65292;&#22996;&#23113;&#35821;&#26159;&#19968;&#31181;&#26222;&#36941;&#30340;&#35821;&#35328;&#29616;&#35937;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#22810;&#35821;&#35328;&#36716;&#25442;&#22120;&#27169;&#22411;&#65288;XLM-RoBERTa&#65289;&#26469;&#28040;&#38500;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#29615;&#22659;&#20013;&#30340;&#28508;&#22312;&#22996;&#23113;&#26415;&#35821;&#65288;PETs&#65289;&#30340;&#27495;&#20041;&#12290;&#31526;&#21512;&#24403;&#21069;&#36235;&#21183;&#65292;&#25105;&#20204;&#35777;&#26126;&#36328;&#35821;&#35328;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#26159;&#21487;&#33021;&#30340;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#32479;&#35745;&#19978;&#23384;&#22312;&#26174;&#33879;&#20248;&#21183;&#30340;&#24773;&#20917;&#19979;&#65292;&#22810;&#35821;&#35328;&#27169;&#22411;&#22312;&#35813;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#27604;&#21333;&#35821;&#35328;&#27169;&#22411;&#26356;&#22909;&#65292;&#36825;&#34920;&#26126;&#22810;&#35821;&#35328;&#25968;&#25454;&#20026;&#27169;&#22411;&#23398;&#20064;&#20851;&#20110;&#36328;&#35821;&#35328;&#22996;&#23113;&#35821;&#35745;&#31639;&#24615;&#36136;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#26426;&#20250;&#12290;&#22312;&#21518;&#32493;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#30528;&#37325;&#32771;&#23519;&#20102;&#21253;&#25324;&#27515;&#20129;&#21644;&#36523;&#20307;&#21151;&#33021;&#22312;&#20869;&#30340;&#36890;&#29992;&#22996;&#23113;&#26415;&#35821;&#31867;&#21035;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#36328;&#35821;&#35328;&#25968;&#25454;&#26159;&#21542;&#27604;&#21516;&#19968;&#39046;&#22495;&#30340;&#35821;&#35328;&#20869;&#25968;&#25454;&#26356;&#37325;&#35201;&#65292;&#20197;&#36827;&#19968;&#27493;&#20102;&#35299;&#36328;&#35821;&#35328;&#36716;&#31227;&#30340;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates the computational processing of euphemisms, a universal linguistic phenomenon, across multiple languages. We train a multilingual transformer model (XLM-RoBERTa) to disambiguate potentially euphemistic terms (PETs) in multilingual and cross-lingual settings. In line with current trends, we demonstrate that zero-shot learning across languages takes place. We also show cases where multilingual models perform better on the task compared to monolingual models by a statistically significant margin, indicating that multilingual data presents additional opportunities for models to learn about cross-lingual, computational properties of euphemisms. In a follow-up analysis, we focus on universal euphemistic "categories" such as death and bodily functions among others. We test to see whether cross-lingual data of the same domain is more important than within-language data of other domains to further understand the nature of the cross-lingual transfer.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;GPT-3.5&#27169;&#22411;&#65292;&#22312;&#22810;&#22269;&#23466;&#27861;&#25991;&#26412;&#20013;&#36827;&#34892;&#23453;&#36149;&#30340;&#25688;&#35201;&#24635;&#32467;&#65292;&#29305;&#21035;&#20851;&#27880;&#20110;&#19982;&#20844;&#27665;&#26435;&#21033;&#21644;&#20041;&#21153;&#30456;&#20851;&#30340;&#27431;&#27954;&#22269;&#23478;&#23466;&#27861;&#27573;&#33853;&#65292;&#32467;&#26524;&#26174;&#31034;&#20854;&#33021;&#22815;&#20934;&#30830;&#12289;&#36830;&#36143;&#19988;&#24544;&#23454;&#22320;&#25429;&#25417;&#21040;RD&#20027;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.14524</link><description>&lt;p&gt;
&#35780;&#20272;GPT-3.5&#22312;&#20849;&#20139;&#20027;&#39064;&#30340;&#27431;&#27954;&#23466;&#27861;&#25991;&#26412;&#20013;&#30340;&#24847;&#35782;&#21644;&#25688;&#35201;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating GPT-3.5's Awareness and Summarization Abilities for European Constitutional Texts with Shared Topics. (arXiv:2401.14524v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;GPT-3.5&#27169;&#22411;&#65292;&#22312;&#22810;&#22269;&#23466;&#27861;&#25991;&#26412;&#20013;&#36827;&#34892;&#23453;&#36149;&#30340;&#25688;&#35201;&#24635;&#32467;&#65292;&#29305;&#21035;&#20851;&#27880;&#20110;&#19982;&#20844;&#27665;&#26435;&#21033;&#21644;&#20041;&#21153;&#30456;&#20851;&#30340;&#27431;&#27954;&#22269;&#23478;&#23466;&#27861;&#27573;&#33853;&#65292;&#32467;&#26524;&#26174;&#31034;&#20854;&#33021;&#22815;&#20934;&#30830;&#12289;&#36830;&#36143;&#19988;&#24544;&#23454;&#22320;&#25429;&#25417;&#21040;RD&#20027;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23466;&#27861;&#26159;&#25903;&#25745;&#25919;&#24220;&#21644;&#31038;&#20250;&#32467;&#26500;&#30340;&#22522;&#30784;&#27861;&#24459;&#25991;&#20214;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#19981;&#20165;&#26159;&#22269;&#23478;&#25991;&#21270;&#21644;&#31038;&#20250;&#29420;&#29305;&#24615;&#30340;&#21453;&#26144;&#65292;&#36824;&#26377;&#21161;&#20110;&#30830;&#23450;&#26222;&#36941;&#37325;&#35201;&#30340;&#20027;&#39064;&#65292;&#22914;&#20844;&#27665;&#30340;&#26435;&#21033;&#21644;&#20041;&#21153;&#65288;RD&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#33879;&#21517;&#30340;GPT-3.5&#65292;&#21033;&#29992;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#29702;&#35299;&#36229;&#36234;&#22269;&#30028;&#30340;&#23466;&#27861;&#27573;&#33853;&#12290;&#25105;&#20204;&#30740;&#31350;&#30340;&#19968;&#20010;&#37325;&#35201;&#36129;&#29486;&#26159;&#25277;&#35937;&#25688;&#35201;&#22312;&#22810;&#28304;&#23466;&#27861;&#25991;&#26412;&#38598;&#21512;&#19978;&#30340;&#26032;&#24212;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#19982;RD&#20027;&#39064;&#30456;&#20851;&#30340;&#27431;&#27954;&#22269;&#23478;&#23466;&#27861;&#27573;&#33853;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-3.5&#20855;&#26377;&#24847;&#20041;&#65292;&#33021;&#22815;&#20135;&#29983;&#28085;&#30422;&#27431;&#27954;&#22269;&#23478;RD&#20027;&#39064;&#30340;&#20449;&#24687;&#20016;&#23500;&#12289;&#36830;&#36143;&#21644;&#24544;&#23454;&#30340;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Constitutions are foundational legal documents that underpin the governmental and societal structures. As such, they are a reflection of a nation's cultural and social uniqueness, but also contribute to establish topics of universal importance, like citizens' rights and duties (RD). In this work, using the renowned GPT-3.5, we leverage generative large language models to understand constitutional passages that transcend national boundaries. A key contribution of our study is the introduction of a novel application of abstractive summarization on a multi-source collection of constitutional texts, with a focus on European countries' constitution passages related to RD topics. Our results show the meaningfulness of GPT-3.5 to produce informative, coherent and faithful summaries capturing RD topics across European countries.
&lt;/p&gt;</description></item><item><title>LLMs&#26082;&#26377;&#33021;&#21147;&#24402;&#22240;&#20110;&#20449;&#24565;&#12289;&#27442;&#26395;&#12289;&#24847;&#22270;&#21644;&#24773;&#24863;&#65292;&#20063;&#33021;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#19981;&#26029;&#25552;&#39640;&#65292;&#20294;&#20182;&#20204;&#26080;&#27861;&#36890;&#36807;&#20849;&#24773;&#26041;&#27861;&#26469;&#23562;&#37325;&#20010;&#20307;&#25104;&#20026;&#20363;&#22806;&#30340;&#26435;&#21033;&#12290;&#20182;&#20204;&#20165;&#36890;&#36807;&#35782;&#21035;&#35821;&#35328;&#27169;&#24335;&#21028;&#26029;&#26696;&#20214;&#30456;&#20284;&#24615;&#65292;&#32780;&#26080;&#27861;&#32771;&#34385;&#20010;&#20307;&#30340;&#20869;&#37096;&#24515;&#29702;&#29366;&#24577;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20849;&#24773;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.14523</link><description>&lt;p&gt;
&#20849;&#24773;&#19982;&#25104;&#20026;&#19968;&#20010;&#20363;&#22806;&#30340;&#26435;&#21033;&#65306;LLMs&#21487;&#20197;&#20570;&#20160;&#20040;&#65292;&#19981;&#33021;&#20570;&#20160;&#20040;
&lt;/p&gt;
&lt;p&gt;
Empathy and the Right to Be an Exception: What LLMs Can and Cannot Do. (arXiv:2401.14523v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14523
&lt;/p&gt;
&lt;p&gt;
LLMs&#26082;&#26377;&#33021;&#21147;&#24402;&#22240;&#20110;&#20449;&#24565;&#12289;&#27442;&#26395;&#12289;&#24847;&#22270;&#21644;&#24773;&#24863;&#65292;&#20063;&#33021;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#19981;&#26029;&#25552;&#39640;&#65292;&#20294;&#20182;&#20204;&#26080;&#27861;&#36890;&#36807;&#20849;&#24773;&#26041;&#27861;&#26469;&#23562;&#37325;&#20010;&#20307;&#25104;&#20026;&#20363;&#22806;&#30340;&#26435;&#21033;&#12290;&#20182;&#20204;&#20165;&#36890;&#36807;&#35782;&#21035;&#35821;&#35328;&#27169;&#24335;&#21028;&#26029;&#26696;&#20214;&#30456;&#20284;&#24615;&#65292;&#32780;&#26080;&#27861;&#32771;&#34385;&#20010;&#20307;&#30340;&#20869;&#37096;&#24515;&#29702;&#29366;&#24577;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20849;&#24773;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24615;&#33021;&#30340;&#36827;&#27493;&#23548;&#33268;&#19968;&#20123;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#20013;&#29702;&#35770;&#24515;&#26234;&#65288;ToM&#65289;&#30340;&#20986;&#29616;&#12290;LLMs&#33021;&#22815;&#24402;&#22240;&#20110;&#20449;&#24565;&#12289;&#27442;&#26395;&#12289;&#24847;&#22270;&#21644;&#24773;&#24863;&#65292;&#24182;&#19988;&#23427;&#20204;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#20250;&#26377;&#25152;&#25552;&#39640;&#12290;&#19982;&#20154;&#31867;&#29305;&#26377;&#30340;&#20849;&#24773;&#26041;&#27861;&#19981;&#21516;&#65292;&#23427;&#20204;&#36890;&#36807;&#35782;&#21035;&#25968;&#25454;&#38598;&#20013;&#36890;&#24120;&#19981;&#21253;&#25324;&#30340;&#35821;&#35328;&#27169;&#24335;&#26469;&#23398;&#20064;&#24402;&#22240;&#24515;&#29702;&#29366;&#24577;&#12290;&#25105;&#20204;&#38382;LLMs&#30340;&#26080;&#27861;&#20849;&#24773;&#26159;&#21542;&#20250;&#22952;&#30861;&#23427;&#20204;&#23562;&#37325;&#20010;&#20307;&#25104;&#20026;&#19968;&#20010;&#20363;&#22806;&#30340;&#26435;&#21033;&#65292;&#21363;&#26159;&#21542;&#20250;&#22952;&#30861;&#23427;&#20204;&#22522;&#20110;&#23545;&#20010;&#20307;&#30340;&#20010;&#24615;&#25935;&#24863;&#24615;&#36827;&#34892;&#24615;&#26684;&#35780;&#20272;&#21644;&#34892;&#20026;&#39044;&#27979;&#12290;LLMs&#33021;&#21542;&#35748;&#30495;&#32771;&#34385;&#20010;&#20307;&#30340;&#20027;&#24352;&#65292;&#21363;&#20182;&#20204;&#30340;&#24773;&#20917;&#26159;&#22522;&#20110;&#20449;&#24565;&#12289;&#27442;&#26395;&#21644;&#24847;&#22270;&#31561;&#20869;&#37096;&#24515;&#29702;&#29366;&#24577;&#32780;&#19981;&#21516;&#65292;&#36824;&#26159;&#20165;&#38480;&#20110;&#22522;&#20110;&#20854;&#19982;&#20182;&#20154;&#30340;&#30456;&#20284;&#20043;&#22788;&#26469;&#21028;&#26029;&#35813;&#26696;&#20214;&#65311;&#25105;&#20204;&#25552;&#20986;&#20849;&#24773;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Advances in the performance of large language models (LLMs) have led some researchers to propose the emergence of theory of mind (ToM) in artificial intelligence (AI). LLMs can attribute beliefs, desires, intentions, and emotions, and they will improve in their accuracy. Rather than employing the characteristically human method of empathy, they learn to attribute mental states by recognizing linguistic patterns in a dataset that typically do not include that individual. We ask whether LLMs' inability to empathize precludes them from honoring an individual's right to be an exception, that is, from making assessments of character and predictions of behavior that reflect appropriate sensitivity to a person's individuality. Can LLMs seriously consider an individual's claim that their case is different based on internal mental states like beliefs, desires, and intentions, or are they limited to judging that case based on its similarities to others? We propose that the method of empathy has 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;K-QA&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;1212&#20010;&#30495;&#23454;&#19990;&#30028;&#21307;&#30103;&#23545;&#35805;&#20013;&#30340;&#24739;&#32773;&#38382;&#39064;&#65292;&#24182;&#32856;&#35831;&#20869;&#37096;&#21307;&#29983;&#22238;&#31572;&#21644;&#20998;&#35299;&#12290;&#30740;&#31350;&#36824;&#21046;&#23450;&#20102;&#20004;&#20010;&#22522;&#20110;NLI&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#30340;&#21484;&#22238;&#29575;&#21644;&#31934;&#30830;&#24230;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#20110;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#29615;&#22659;&#19979;&#30340;&#20934;&#30830;&#24615;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2401.14493</link><description>&lt;p&gt;
K-QA&#65306;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#21307;&#30103;&#38382;&#31572;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
K-QA: A Real-World Medical Q&amp;A Benchmark. (arXiv:2401.14493v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14493
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;K-QA&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;1212&#20010;&#30495;&#23454;&#19990;&#30028;&#21307;&#30103;&#23545;&#35805;&#20013;&#30340;&#24739;&#32773;&#38382;&#39064;&#65292;&#24182;&#32856;&#35831;&#20869;&#37096;&#21307;&#29983;&#22238;&#31572;&#21644;&#20998;&#35299;&#12290;&#30740;&#31350;&#36824;&#21046;&#23450;&#20102;&#20004;&#20010;&#22522;&#20110;NLI&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#30340;&#21484;&#22238;&#29575;&#21644;&#31934;&#30830;&#24230;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#20110;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#29615;&#22659;&#19979;&#30340;&#20934;&#30830;&#24615;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#20445;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#30340;&#22238;&#31572;&#20934;&#30830;&#24615;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#65292;&#38169;&#35823;&#30340;&#20449;&#24687;&#21487;&#33021;&#30452;&#25509;&#24433;&#21709;&#24739;&#32773;&#20581;&#24247;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;K-QA&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;1212&#20010;&#30001;K Health&#65288;&#19968;&#23478;AI&#39537;&#21160;&#30340;&#20020;&#24202;&#24179;&#21488;&#65289;&#19978;&#30340;&#30495;&#23454;&#23545;&#35805;&#20013;&#30340;&#24739;&#32773;&#38382;&#39064;&#12290;&#25105;&#20204;&#32856;&#35831;&#19968;&#32452;&#20869;&#37096;&#21307;&#29983;&#26469;&#22238;&#31572;&#24182;&#25163;&#21160;&#20998;&#35299;K-QA&#30340;&#23376;&#38598;&#20026;&#33258;&#21253;&#21547;&#30340;&#38472;&#36848;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#20004;&#20010;&#22522;&#20110;NLI&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#36817;&#20284;&#20110;&#21484;&#22238;&#29575;&#21644;&#31934;&#30830;&#24230;&#65306;&#65288;1&#65289;&#20840;&#38754;&#24615;&#65292;&#34913;&#37327;&#29983;&#25104;&#22238;&#31572;&#20013;&#25152;&#21547;&#30340;&#22522;&#26412;&#20020;&#24202;&#20449;&#24687;&#30340;&#30334;&#20998;&#27604;&#65292;&#65288;2&#65289;&#34394;&#26500;&#29575;&#65292;&#34913;&#37327;LLM&#22238;&#31572;&#25152;&#30683;&#30462;&#30340;&#21307;&#29983;&#31574;&#21010;&#22238;&#22797;&#20013;&#30340;&#38472;&#36848;&#25968;&#37327;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;K-QA&#21644;&#36825;&#20123;&#25351;&#26631;&#26469;&#35780;&#20272;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#20197;&#21450;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#21307;&#23398;&#23548;&#21521;&#22686;&#24378;&#26816;&#32034;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensuring the accuracy of responses provided by large language models (LLMs) is crucial, particularly in clinical settings where incorrect information may directly impact patient health. To address this challenge, we construct K-QA, a dataset containing 1,212 patient questions originating from real-world conversations held on K Health (an AI-driven clinical platform). We employ a panel of in-house physicians to answer and manually decompose a subset of K-QA into self-contained statements. Additionally, we formulate two NLI-based evaluation metrics approximating recall and precision: (1) comprehensiveness, measuring the percentage of essential clinical information in the generated answer and (2) hallucination rate, measuring the number of statements from the physician-curated response contradicted by the LLM answer. Finally, we use K-QA along with these metrics to evaluate several state-of-the-art models, as well as the effect of in-context learning and medically-oriented augmented retri
&lt;/p&gt;</description></item><item><title>LongHealth&#26159;&#19968;&#20010;&#20855;&#26377;&#38271;&#31687;&#20020;&#24202;&#25991;&#26723;&#30340;&#38382;&#31572;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#38271;&#31687;&#20020;&#24202;&#25968;&#25454;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#26368;&#39640;&#20934;&#30830;&#24615;&#35266;&#23519;&#22312;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#20013;&#65292;Mixtral-8x7B-Instruct-v0.1&#22312;&#20174;&#21333;&#20010;&#21644;&#22810;&#20010;&#30149;&#24739;&#25991;&#26723;&#20013;&#26816;&#32034;&#20449;&#24687;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#26368;&#22909;&#12290;</title><link>http://arxiv.org/abs/2401.14490</link><description>&lt;p&gt;
LongHealth:&#19968;&#20221;&#20855;&#26377;&#38271;&#31687;&#20020;&#24202;&#25991;&#26723;&#30340;&#38382;&#31572;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
LongHealth: A Question Answering Benchmark with Long Clinical Documents. (arXiv:2401.14490v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14490
&lt;/p&gt;
&lt;p&gt;
LongHealth&#26159;&#19968;&#20010;&#20855;&#26377;&#38271;&#31687;&#20020;&#24202;&#25991;&#26723;&#30340;&#38382;&#31572;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#38271;&#31687;&#20020;&#24202;&#25968;&#25454;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#26368;&#39640;&#20934;&#30830;&#24615;&#35266;&#23519;&#22312;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#20013;&#65292;Mixtral-8x7B-Instruct-v0.1&#22312;&#20174;&#21333;&#20010;&#21644;&#22810;&#20010;&#30149;&#24739;&#25991;&#26723;&#20013;&#26816;&#32034;&#20449;&#24687;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#26377;&#28508;&#22312;&#30340;&#22909;&#22788;&#65292;&#23588;&#20854;&#26159;&#22312;&#22788;&#29702;&#22823;&#37327;&#30340;&#30149;&#24739;&#35760;&#24405;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20934;&#26410;&#33021;&#20805;&#20998;&#35780;&#20272;LLMs&#22312;&#22788;&#29702;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#38271;&#31687;&#20020;&#24202;&#25968;&#25454;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#25552;&#20986;&#20102;LongHealth&#22522;&#20934;&#65292;&#21253;&#25324;20&#20010;&#35814;&#32454;&#30340;&#34394;&#26500;&#30149;&#20363;&#65292;&#28085;&#30422;&#21508;&#31181;&#30142;&#30149;&#65292;&#27599;&#20010;&#26696;&#20363;&#21253;&#21547;5090&#33267;6754&#20010;&#23383;&#12290;&#35813;&#22522;&#20934;&#36890;&#36807;&#19977;&#20010;&#31867;&#21035;&#30340;400&#20010;&#22810;&#39033;&#36873;&#25321;&#39064;&#65292;&#25361;&#25112;LLMs&#20174;&#22823;&#22411;&#20020;&#24202;&#25991;&#26723;&#20013;&#25552;&#21462;&#24182;&#35299;&#37322;&#20449;&#24687;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#35780;&#20272;&#20102;&#20061;&#31181;&#24320;&#28304;LLMs&#65292;&#20854;&#20013;&#26368;&#23569;&#26377;16,000&#20010;&#26631;&#35760;&#65292;&#24182;&#19988;&#36824;&#21253;&#25324;&#20102;OpenAI&#30340;&#19987;&#26377;&#21644;&#25104;&#26412;&#25928;&#30410;&#30340;GPT-3.5 Turbo&#36827;&#34892;&#27604;&#36739;&#12290; Mixtral-8x7B-Instruct-v0.1&#22312;&#20174;&#21333;&#20010;&#21644;&#22810;&#20010;&#30149;&#24739;&#25991;&#26723;&#20013;&#26816;&#32034;&#20449;&#24687;&#30340;&#20219;&#21153;&#20013;&#35266;&#23519;&#21040;&#26368;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#30340;LLMs&#22312;&#22788;&#29702;&#26377;&#20851;&#21542;&#23450;&#21644;&#25490;&#24207;&#30340;&#20219;&#21153;&#26102;&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: Recent advancements in large language models (LLMs) offer potential benefits in healthcare, particularly in processing extensive patient records. However, existing benchmarks do not fully assess LLMs' capability in handling real-world, lengthy clinical data.  Methods: We present the LongHealth benchmark, comprising 20 detailed fictional patient cases across various diseases, with each case containing 5,090 to 6,754 words. The benchmark challenges LLMs with 400 multiple-choice questions in three categories: information extraction, negation, and sorting, challenging LLMs to extract and interpret information from large clinical documents.  Results: We evaluated nine open-source LLMs with a minimum of 16,000 tokens and also included OpenAI's proprietary and cost-efficient GPT-3.5 Turbo for comparison. The highest accuracy was observed for Mixtral-8x7B-Instruct-v0.1, particularly in tasks focused on information retrieval from single and multiple patient documents. However, all m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Wordflow&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#31038;&#20132;&#25552;&#31034;&#24037;&#31243;&#30340;&#26041;&#24335;&#35753;&#38750;&#19987;&#23478;&#29992;&#25143;&#26356;&#22909;&#22320;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#24182;&#21487;&#20197;&#36731;&#26494;&#21019;&#24314;&#12289;&#36816;&#34892;&#12289;&#20849;&#20139;&#21644;&#21457;&#29616;LLM&#25552;&#31034;&#12290;&#36890;&#36807;&#21033;&#29992;&#29616;&#20195;&#32593;&#32476;&#25216;&#26415;&#65292;Wordflow&#20801;&#35768;&#29992;&#25143;&#22312;&#27983;&#35272;&#22120;&#20013;&#26412;&#22320;&#21644;&#31169;&#19979;&#36816;&#34892;LLM&#12290;</title><link>http://arxiv.org/abs/2401.14447</link><description>&lt;p&gt;
Wordflow: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31038;&#20132;&#25552;&#31034;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
Wordflow: Social Prompt Engineering for Large Language Models. (arXiv:2401.14447v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Wordflow&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#31038;&#20132;&#25552;&#31034;&#24037;&#31243;&#30340;&#26041;&#24335;&#35753;&#38750;&#19987;&#23478;&#29992;&#25143;&#26356;&#22909;&#22320;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#24182;&#21487;&#20197;&#36731;&#26494;&#21019;&#24314;&#12289;&#36816;&#34892;&#12289;&#20849;&#20139;&#21644;&#21457;&#29616;LLM&#25552;&#31034;&#12290;&#36890;&#36807;&#21033;&#29992;&#29616;&#20195;&#32593;&#32476;&#25216;&#26415;&#65292;Wordflow&#20801;&#35768;&#29992;&#25143;&#22312;&#27983;&#35272;&#22120;&#20013;&#26412;&#22320;&#21644;&#31169;&#19979;&#36816;&#34892;LLM&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38656;&#35201;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#25165;&#33021;&#26377;&#25928;&#20351;&#29992;&#12290;&#23545;&#20110;&#38750;&#19987;&#23478;&#26469;&#35828;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36807;&#31243;&#65292;&#22240;&#20026;&#20182;&#20204;&#23545;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#19981;&#37027;&#20040;&#29087;&#24713;&#12290;&#34429;&#28982;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19968;&#20123;&#25216;&#26415;&#21644;&#24037;&#20855;&#26469;&#24110;&#21161;LLM&#29992;&#25143;&#35774;&#35745;&#25552;&#31034;&#65292;&#20294;&#36825;&#20123;&#20316;&#21697;&#20027;&#35201;&#38024;&#23545;&#30340;&#26159;AI&#24212;&#29992;&#24320;&#21457;&#32773;&#32780;&#19981;&#26159;&#38750;&#19987;&#23478;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31038;&#20132;&#25552;&#31034;&#24037;&#31243;&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#31038;&#20132;&#35745;&#31639;&#25216;&#26415;&#20419;&#36827;&#21327;&#20316;&#25552;&#31034;&#35774;&#35745;&#30340;&#26032;&#33539;&#24335;&#12290;&#20026;&#20102;&#30740;&#31350;&#31038;&#20132;&#25552;&#31034;&#24037;&#31243;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Wordflow&#65292;&#19968;&#20010;&#24320;&#28304;&#30340;&#31038;&#20132;&#25991;&#26412;&#32534;&#36753;&#22120;&#65292;&#20351;&#26222;&#36890;&#29992;&#25143;&#21487;&#20197;&#36731;&#26494;&#21019;&#24314;&#12289;&#36816;&#34892;&#12289;&#20849;&#20139;&#21644;&#21457;&#29616;LLM&#25552;&#31034;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#21033;&#29992;&#29616;&#20195;&#32593;&#32476;&#25216;&#26415;&#65292;Wordflow&#20801;&#35768;&#29992;&#25143;&#22312;&#20854;&#27983;&#35272;&#22120;&#20013;&#26412;&#22320;&#21644;&#31169;&#19979;&#36816;&#34892;LLM&#12290;&#20004;&#20010;&#20351;&#29992;&#22330;&#26223;&#31361;&#20986;&#20102;&#31038;&#20132;&#25552;&#31034;&#24037;&#31243;&#21644;&#25105;&#20204;&#30340;&#24037;&#20855;&#22914;&#20309;&#22686;&#24378;&#26222;&#36890;&#20154;&#19982;LLM&#30340;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) require well-crafted prompts for effective use. Prompt engineering, the process of designing prompts, is challenging, particularly for non-experts who are less familiar with AI technologies. While researchers have proposed techniques and tools to assist LLM users in prompt design, these works primarily target AI application developers rather than non-experts. To address this research gap, we propose social prompt engineering, a novel paradigm that leverages social computing techniques to facilitate collaborative prompt design. To investigate social prompt engineering, we introduce Wordflow, an open-source and social text editor that enables everyday users to easily create, run, share, and discover LLM prompts. Additionally, by leveraging modern web technologies, Wordflow allows users to run LLMs locally and privately in their browsers. Two usage scenarios highlight how social prompt engineering and our tool can enhance laypeople's interaction with LLMs. Wor
&lt;/p&gt;</description></item><item><title>&#36825;&#20221;&#35770;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#26368;&#20808;&#36827;&#30340;NLI&#27169;&#22411;&#23545;&#24494;&#23567;&#30340;&#35821;&#20041;&#20445;&#25345;&#34920;&#38754;&#24418;&#24335;&#21464;&#21270;&#38750;&#24120;&#25935;&#24863;&#65292;&#23548;&#33268;&#25512;&#26029;&#32467;&#26524;&#19981;&#19968;&#33268;&#12290;&#20854;&#34892;&#20026;&#19982;&#23545;&#32452;&#21512;&#35821;&#20041;&#30340;&#26377;&#25928;&#29702;&#35299;&#19981;&#21516;&#65292;&#36825;&#23545;&#24403;&#21069;NLI&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.14440</link><description>&lt;p&gt;
&#35821;&#20041;&#25935;&#24863;&#24615;&#21644;&#19981;&#19968;&#33268;&#30340;&#39044;&#27979;&#65306;&#34913;&#37327;NLI&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
Semantic Sensitivities and Inconsistent Predictions: Measuring the Fragility of NLI Models. (arXiv:2401.14440v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14440
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#35770;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#26368;&#20808;&#36827;&#30340;NLI&#27169;&#22411;&#23545;&#24494;&#23567;&#30340;&#35821;&#20041;&#20445;&#25345;&#34920;&#38754;&#24418;&#24335;&#21464;&#21270;&#38750;&#24120;&#25935;&#24863;&#65292;&#23548;&#33268;&#25512;&#26029;&#32467;&#26524;&#19981;&#19968;&#33268;&#12290;&#20854;&#34892;&#20026;&#19982;&#23545;&#32452;&#21512;&#35821;&#20041;&#30340;&#26377;&#25928;&#29702;&#35299;&#19981;&#21516;&#65292;&#36825;&#23545;&#24403;&#21069;NLI&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#22522;&#20110;transformer&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#27169;&#22411;&#30340;&#26032;&#33021;&#21147;&#36827;&#34892;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23427;&#20204;&#20855;&#22791;&#23545;&#35789;&#27719;&#21644;&#32452;&#21512;&#35821;&#20041;&#30340;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#36825;&#20123;&#35828;&#27861;&#24212;&#35813;&#25345;&#20445;&#30041;&#24577;&#24230;&#65306;&#25105;&#20204;&#21457;&#29616;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#27169;&#22411;&#23545;&#24494;&#23567;&#30340;&#20445;&#30041;&#35821;&#20041;&#30340;&#34920;&#38754;&#24418;&#24335;&#21464;&#21270;&#25935;&#24863;&#65292;&#36825;&#23548;&#33268;&#25512;&#26029;&#36807;&#31243;&#20013;&#20986;&#29616;&#22823;&#37327;&#19981;&#19968;&#33268;&#30340;&#27169;&#22411;&#20915;&#31574;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#31181;&#34892;&#20026;&#19982;&#23545;&#32452;&#21512;&#35821;&#20041;&#30340;&#26377;&#25928;&#21644;&#28145;&#20837;&#29702;&#35299;&#19981;&#21516;&#65292;&#32780;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#35780;&#20272;&#27169;&#22411;&#20934;&#30830;&#24230;&#25110;&#25506;&#31350;&#21477;&#27861;&#12289;&#21333;&#35843;&#24615;&#21644;&#36923;&#36753;&#40065;&#26834;&#24615;&#25512;&#29702;&#26102;&#22343;&#19981;&#20250;&#20986;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#26469;&#34913;&#37327;&#35821;&#20041;&#25935;&#24863;&#24615;&#30340;&#31243;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#21547;&#26377;&#24494;&#23567;&#20445;&#30041;&#35821;&#20041;&#30340;&#34920;&#38754;&#24418;&#24335;&#36755;&#20837;&#22122;&#22768;&#30340;&#23545;&#25239;&#29983;&#25104;&#26679;&#20363;&#26469;&#35780;&#20272;NLI&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies of the emergent capabilities of transformer-based Natural Language Understanding (NLU) models have indicated that they have an understanding of lexical and compositional semantics. We provide evidence that suggests these claims should be taken with a grain of salt: we find that state-of-the-art Natural Language Inference (NLI) models are sensitive towards minor semantics preserving surface-form variations, which lead to sizable inconsistent model decisions during inference. Notably, this behaviour differs from valid and in-depth comprehension of compositional semantics, however does neither emerge when evaluating model accuracy on standard benchmarks nor when probing for syntactic, monotonic, and logically robust reasoning. We propose a novel framework to measure the extent of semantic sensitivity. To this end, we evaluate NLI models on adversarially generated examples containing minor semantics-preserving surface-form input noise. This is achieved using conditional text
&lt;/p&gt;</description></item><item><title>DeepSeek-Coder&#26159;&#19968;&#31995;&#21015;&#24320;&#28304;&#20195;&#30721;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#39640;&#36136;&#37327;&#39033;&#30446;&#32423;&#20195;&#30721;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#37319;&#29992;&#22635;&#31354;&#20219;&#21153;&#21644;16K&#31383;&#21475;&#26469;&#22686;&#24378;&#20195;&#30721;&#29983;&#25104;&#21644;&#22635;&#20805;&#65292;&#19981;&#20165;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#19982;&#24320;&#28304;&#20195;&#30721;&#27169;&#22411;&#21516;&#26679;&#30340;&#26368;&#26032;&#34920;&#29616;&#65292;&#32780;&#19988;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#38381;&#28304;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.14196</link><description>&lt;p&gt;
DeepSeek-Coder: &#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#32534;&#31243;&#30456;&#36935;&#30340;&#26102;&#20505;--&#20195;&#30721;&#26234;&#33021;&#30340;&#23835;&#36215;
&lt;/p&gt;
&lt;p&gt;
DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence. (arXiv:2401.14196v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14196
&lt;/p&gt;
&lt;p&gt;
DeepSeek-Coder&#26159;&#19968;&#31995;&#21015;&#24320;&#28304;&#20195;&#30721;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#39640;&#36136;&#37327;&#39033;&#30446;&#32423;&#20195;&#30721;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#37319;&#29992;&#22635;&#31354;&#20219;&#21153;&#21644;16K&#31383;&#21475;&#26469;&#22686;&#24378;&#20195;&#30721;&#29983;&#25104;&#21644;&#22635;&#20805;&#65292;&#19981;&#20165;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#19982;&#24320;&#28304;&#20195;&#30721;&#27169;&#22411;&#21516;&#26679;&#30340;&#26368;&#26032;&#34920;&#29616;&#65292;&#32780;&#19988;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#38381;&#28304;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#21457;&#23637;&#20026;&#36719;&#20214;&#24320;&#21457;&#20013;&#30340;&#20195;&#30721;&#26234;&#33021;&#24102;&#26469;&#20102;&#38761;&#21629;&#12290;&#28982;&#32780;&#65292;&#38381;&#28304;&#27169;&#22411;&#30340;&#20027;&#23548;&#22320;&#20301;&#38480;&#21046;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DeepSeek-Coder&#31995;&#21015;&#65292;&#36825;&#26159;&#19968;&#31995;&#21015;&#24320;&#28304;&#20195;&#30721;&#27169;&#22411;&#65292;&#22823;&#23567;&#20174;1.3B&#21040;33B&#65292;&#20174;&#22836;&#24320;&#22987;&#22312;2&#19975;&#20159;&#20010;&#26631;&#35760;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#39640;&#36136;&#37327;&#39033;&#30446;&#32423;&#20195;&#30721;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#24182;&#37319;&#29992;&#22635;&#31354;&#20219;&#21153;&#21644;16K&#31383;&#21475;&#26469;&#22686;&#24378;&#20195;&#30721;&#29983;&#25104;&#21644;&#22635;&#20805;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;DeepSeek-Coder&#19981;&#20165;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#19982;&#24320;&#28304;&#20195;&#30721;&#27169;&#22411;&#21516;&#26679;&#30340;&#26368;&#26032;&#34920;&#29616;&#65292;&#32780;&#19988;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;Codex&#21644;GPT-3.5&#31561;&#38381;&#28304;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;DeepSeek-Coder&#27169;&#22411;&#37319;&#29992;&#20102;&#23485;&#26494;&#30340;&#35768;&#21487;&#35777;&#65292;&#26082;&#20801;&#35768;&#30740;&#31350;&#65292;&#20063;&#20801;&#35768;&#26080;&#38480;&#21046;&#30340;&#21830;&#19994;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid development of large language models has revolutionized code intelligence in software development. However, the predominance of closed-source models has restricted extensive research and development. To address this, we introduce the DeepSeek-Coder series, a range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2 trillion tokens. These models are pre-trained on a high-quality project-level code corpus and employ a fill-in-the-blank task with a 16K window to enhance code generation and infilling. Our extensive evaluations demonstrate that DeepSeek-Coder not only achieves state-of-the-art performance among open-source code models across multiple benchmarks but also surpasses existing closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models are under a permissive license that allows for both research and unrestricted commercial use.
&lt;/p&gt;</description></item><item><title>CMMU&#26159;&#19968;&#20010;&#29992;&#20110;&#20013;&#25991;&#22810;&#27169;&#24577;&#22810;&#31867;&#22411;&#38382;&#39064;&#29702;&#35299;&#21644;&#25512;&#29702;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;&#20174;&#23567;&#23398;&#21040;&#39640;&#20013;&#30340;&#30693;&#35782;&#65292;&#25552;&#20379;&#20102;&#22810;&#39033;&#36873;&#25321;&#39064;&#12289;&#22810;&#39033;&#22238;&#31572;&#39064;&#21644;&#22635;&#31354;&#39064;&#19977;&#31181;&#31867;&#22411;&#30340;&#38382;&#39064;&#65292;&#23545;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#27700;&#24179;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2401.14011</link><description>&lt;p&gt;
CMMU: &#19968;&#20010;&#29992;&#20110;&#20013;&#25991;&#22810;&#27169;&#24577;&#22810;&#31867;&#22411;&#38382;&#39064;&#29702;&#35299;&#19982;&#25512;&#29702;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
CMMU: A Benchmark for Chinese Multi-modal Multi-type Question Understanding and Reasoning. (arXiv:2401.14011v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14011
&lt;/p&gt;
&lt;p&gt;
CMMU&#26159;&#19968;&#20010;&#29992;&#20110;&#20013;&#25991;&#22810;&#27169;&#24577;&#22810;&#31867;&#22411;&#38382;&#39064;&#29702;&#35299;&#21644;&#25512;&#29702;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;&#20174;&#23567;&#23398;&#21040;&#39640;&#20013;&#30340;&#30693;&#35782;&#65292;&#25552;&#20379;&#20102;&#22810;&#39033;&#36873;&#25321;&#39064;&#12289;&#22810;&#39033;&#22238;&#31572;&#39064;&#21644;&#22635;&#31354;&#39064;&#19977;&#31181;&#31867;&#22411;&#30340;&#38382;&#39064;&#65292;&#23545;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#27700;&#24179;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#24182;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#30693;&#35782;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;MLLM&#30340;&#26234;&#33021;&#27700;&#24179;&#25152;&#38656;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#25484;&#25569;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#24403;&#21069;&#29992;&#20110;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#20027;&#35201;&#38598;&#20013;&#22312;&#33521;&#35821;&#22810;&#39033;&#36873;&#25321;&#39064;&#19978;&#65292;&#24182;&#19988;&#22312;&#35780;&#20272;&#30340;&#20840;&#38754;&#24615;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CMMU&#65292;&#19968;&#20010;&#29992;&#20110;&#20013;&#25991;&#22810;&#27169;&#24577;&#22810;&#31867;&#22411;&#38382;&#39064;&#29702;&#35299;&#21644;&#25512;&#29702;&#30340;&#26032;&#22411;&#22522;&#20934;&#27979;&#35797;&#12290;CMMU&#21253;&#21547;7&#20010;&#23398;&#31185;&#30340;3603&#20010;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#20174;&#23567;&#23398;&#21040;&#39640;&#20013;&#30340;&#30693;&#35782;&#12290;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#20998;&#20026;&#22810;&#39033;&#36873;&#25321;&#39064;&#12289;&#22810;&#39033;&#22238;&#31572;&#39064;&#21644;&#22635;&#31354;&#39064;&#19977;&#31867;&#65292;&#23545;MLLMs&#25552;&#20986;&#26356;&#22823;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20005;&#26684;&#30340;&#35780;&#20272;&#31574;&#30053;&#65292;&#31216;&#20026;ShiftCheck&#65292;&#29992;&#20110;&#35780;&#20272;&#22810;&#39033;&#36873;&#25321;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal large language models(MLLMs) have achieved remarkable progress and demonstrated powerful knowledge comprehension and reasoning abilities. However, the mastery of domain-specific knowledge, which is essential for evaluating the intelligence of MLLMs, continues to be a challenge. Current multi-modal benchmarks for domain-specific knowledge concentrate on multiple-choice questions and are predominantly available in English, which imposes limitations on the comprehensiveness of the evaluation. To this end, we introduce CMMU, a novel benchmark for multi-modal and multi-type question understanding and reasoning in Chinese. CMMU consists of 3,603 questions in 7 subjects, covering knowledge from primary to high school. The questions can be categorized into 3 types: multiple-choice, multiple-response, and fill-in-the-blank, bringing greater challenges to MLLMs. In addition, we propose a rigorous evaluation strategy called ShiftCheck for assessing multiple-choice questions. The strat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#23450;&#21046;&#30340;BERT&#19982;SVC&#30340;&#38598;&#25104;&#65292;&#38024;&#23545;ToS&#25991;&#26723;&#20013;&#30340;&#19981;&#20844;&#24179;&#26465;&#27454;&#36827;&#34892;&#20102;SOTA&#32423;&#21035;&#30340;&#26816;&#27979;&#65292;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.11207</link><description>&lt;p&gt;
&#19981;&#20844;&#24179;&#30340;&#26381;&#21153;&#26465;&#27454;&#65306;&#20351;&#29992;&#23450;&#21046;&#30340;BERT&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unfair TOS: An Automated Approach using Customized BERT. (arXiv:2401.11207v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#23450;&#21046;&#30340;BERT&#19982;SVC&#30340;&#38598;&#25104;&#65292;&#38024;&#23545;ToS&#25991;&#26723;&#20013;&#30340;&#19981;&#20844;&#24179;&#26465;&#27454;&#36827;&#34892;&#20102;SOTA&#32423;&#21035;&#30340;&#26816;&#27979;&#65292;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26381;&#21153;&#26465;&#27454;(Terms of Service&#65292;ToS)&#26159;&#20219;&#20309;&#21327;&#35758;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#23427;&#23450;&#20041;&#20102;&#26381;&#21153;&#25552;&#20379;&#21830;&#21644;&#26368;&#32456;&#29992;&#25143;&#20043;&#38388;&#30340;&#27861;&#24459;&#20851;&#31995;&#12290;&#23427;&#20204;&#19981;&#20165;&#30830;&#23450;&#21644;&#30028;&#23450;&#20102;&#30456;&#20114;&#30340;&#26435;&#21033;&#21644;&#36131;&#20219;&#65292;&#36824;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#19982;&#20351;&#29992;&#25968;&#23383;&#31354;&#38388;&#26377;&#20851;&#30340;&#21512;&#21516;&#37325;&#35201;&#26041;&#38754;&#30340;&#20449;&#24687;&#12290;&#36825;&#20123;&#26041;&#38754;&#21253;&#25324;&#36131;&#20219;&#38480;&#21046;&#12289;&#25968;&#25454;&#20445;&#25252;&#31561;&#21508;&#31181;&#20027;&#39064;&#12290;&#29992;&#25143;&#20542;&#21521;&#20110;&#22312;&#20351;&#29992;&#20219;&#20309;&#24212;&#29992;&#31243;&#24207;&#25110;&#26381;&#21153;&#20043;&#21069;&#25509;&#21463;ToS&#32780;&#19981;&#36827;&#34892;&#38405;&#35835;&#12290;&#36825;&#31181;&#26080;&#30693;&#21487;&#33021;&#20351;&#20182;&#20204;&#22312;&#38656;&#35201;&#37319;&#21462;&#20219;&#20309;&#34892;&#21160;&#26102;&#22788;&#20110;&#36739;&#24369;&#30340;&#29366;&#20917;&#12290;&#29616;&#26377;&#30340;&#26816;&#27979;&#25110;&#20998;&#31867;&#19981;&#20844;&#24179;&#26465;&#27454;&#30340;&#26041;&#27861;&#24050;&#32463;&#36807;&#26102;&#19988;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#36825;&#31687;&#30740;&#31350;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;Fine-tuning BERT&#19982;SVC&#65288;&#25903;&#25345;&#21521;&#37327;&#20998;&#31867;&#22120;&#65289;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#20851;&#20110;ToS&#25991;&#26723;&#20013;&#19981;&#20844;&#24179;&#26465;&#27454;&#26816;&#27979;&#30340;SOTA&#65288;&#26368;&#26032;&#25216;&#26415;&#65289;&#32467;&#26524;&#12290;&#30740;&#31350;&#34920;&#26126;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Terms of Service (ToS) form an integral part of any agreement as it defines the legal relationship between a service provider and an end-user. Not only do they establish and delineate reciprocal rights and responsibilities, but they also provide users with information on essential aspects of contracts that pertain to the use of digital spaces. These aspects include a wide range of topics, including limitation of liability, data protection, etc. Users tend to accept the ToS without going through it before using any application or service. Such ignorance puts them in a potentially weaker situation in case any action is required. Existing methodologies for the detection or classification of unfair clauses are however obsolete and show modest performance. In this research paper, we present SOTA(State of The Art) results on unfair clause detection from ToS documents based on unprecedented Fine-tuning BERT in integration with SVC(Support Vector Classifier). The study shows proficient perform
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GAAM&#30340;&#22810;&#22836;&#39640;&#26031;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#29992;&#20110;&#22686;&#24378;&#36328;&#22810;&#20010;&#27169;&#24577;&#30340;&#20449;&#24687;&#32858;&#21512;&#12290;&#36890;&#36807;&#23558;&#21487;&#23398;&#20064;&#30340;&#22343;&#20540;&#21644;&#26041;&#24046;&#32435;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#65292;GAAM&#33021;&#22815;&#21160;&#24577;&#22320;&#37325;&#26032;&#35843;&#25972;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#22312;&#22788;&#29702;&#38750;&#24179;&#31283;&#25968;&#25454;&#26102;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#36229;&#36807;&#20102;&#30446;&#21069;&#29616;&#26377;&#30340;&#27880;&#24847;&#21147;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#30340;&#36866;&#24212;&#24615;&#24378;&#19988;&#21442;&#25968;&#25968;&#37327;&#36739;&#23569;&#65292;&#20855;&#26377;&#25913;&#36827;&#29616;&#26377;&#27880;&#24847;&#21147;&#26694;&#26550;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.11143</link><description>&lt;p&gt;
&#39640;&#26031;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26159;&#21807;&#19968;&#25152;&#38656;&#30340;&#65306;&#36328;&#22810;&#20010;&#27169;&#24577;&#30340;&#20581;&#22766;&#19978;&#19979;&#25991;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Gaussian Adaptive Attention is All You Need: Robust Contextual Representations Across Multiple Modalities. (arXiv:2401.11143v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11143
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GAAM&#30340;&#22810;&#22836;&#39640;&#26031;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#29992;&#20110;&#22686;&#24378;&#36328;&#22810;&#20010;&#27169;&#24577;&#30340;&#20449;&#24687;&#32858;&#21512;&#12290;&#36890;&#36807;&#23558;&#21487;&#23398;&#20064;&#30340;&#22343;&#20540;&#21644;&#26041;&#24046;&#32435;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#65292;GAAM&#33021;&#22815;&#21160;&#24577;&#22320;&#37325;&#26032;&#35843;&#25972;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#22312;&#22788;&#29702;&#38750;&#24179;&#31283;&#25968;&#25454;&#26102;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#36229;&#36807;&#20102;&#30446;&#21069;&#29616;&#26377;&#30340;&#27880;&#24847;&#21147;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#30340;&#36866;&#24212;&#24615;&#24378;&#19988;&#21442;&#25968;&#25968;&#37327;&#36739;&#23569;&#65292;&#20855;&#26377;&#25913;&#36827;&#29616;&#26377;&#27880;&#24847;&#21147;&#26694;&#26550;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#22836;&#39640;&#26031;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;GAAM&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#27880;&#24847;&#21147;&#26694;&#26550;&#65292;&#24182;&#35774;&#35745;&#20102;&#39640;&#26031;&#33258;&#36866;&#24212;&#21464;&#21387;&#22120;&#65288;GAT&#65289;&#65292;&#26088;&#22312;&#22686;&#24378;&#36328;&#22810;&#20010;&#27169;&#24577;&#65288;&#21253;&#25324;&#35821;&#38899;&#12289;&#25991;&#26412;&#21644;&#35270;&#35273;&#65289;&#30340;&#20449;&#24687;&#32858;&#21512;&#12290;GAAM&#23558;&#21487;&#23398;&#20064;&#30340;&#22343;&#20540;&#21644;&#26041;&#24046;&#34701;&#20837;&#20854;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#65292;&#37319;&#29992;&#22810;&#22836;&#26694;&#26550;&#23454;&#29616;&#65292;&#20351;&#20854;&#33021;&#22815;&#38598;&#20307;&#24314;&#27169;&#20219;&#20309;&#27010;&#29575;&#20998;&#24067;&#65292;&#20197;&#21160;&#24577;&#37325;&#26032;&#35843;&#25972;&#29305;&#24449;&#37325;&#35201;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#39640;&#24230;&#38750;&#24179;&#31283;&#25968;&#25454;&#26102;&#34920;&#29616;&#20986;&#26174;&#33879;&#25913;&#36827;&#65292;&#36890;&#36807;&#35782;&#21035;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#20851;&#38190;&#20803;&#32032;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#27880;&#24847;&#21147;&#25216;&#26415;&#22312;&#27169;&#22411;&#24615;&#33021;&#19978;&#30340;&#29366;&#24577;&#65288;&#31934;&#24230;&#22686;&#21152;&#32422;20%&#65289;&#12290;GAAM&#19982;&#22522;&#20110;&#28857;&#31215;&#30340;&#27880;&#24847;&#21147;&#27169;&#22411;&#20860;&#23481;&#65292;&#24182;&#20855;&#26377;&#30456;&#23545;&#36739;&#20302;&#30340;&#21442;&#25968;&#25968;&#37327;&#65292;&#23637;&#31034;&#20102;&#20854;&#36866;&#24212;&#24615;&#21644;&#25552;&#21319;&#29616;&#26377;&#27880;&#24847;&#21147;&#26694;&#26550;&#30340;&#28508;&#21147;&#12290;&#22312;&#23454;&#35777;&#26041;&#38754;&#65292;GAAM&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#36866;&#24212;&#24615;&#21644;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the Multi-Head Gaussian Adaptive Attention Mechanism (GAAM), a novel probabilistic attention framework, and the Gaussian Adaptive Transformer (GAT), designed to enhance information aggregation across multiple modalities, including Speech, Text and Vision. GAAM integrates learnable mean and variance into its attention mechanism, implemented in a Multi-Headed framework enabling it to collectively model any Probability Distribution for dynamic recalibration of feature significance. This method demonstrates significant improvements, especially with highly non-stationary data, surpassing the state-of-the-art attention techniques in model performance (up to approximately +20% in accuracy) by identifying key elements within the feature space. GAAM's compatibility with dot-product-based attention models and relatively low number of parameters showcases its adaptability and potential to boost existing attention frameworks. Empirically, GAAM exhibits superior adaptability and efficacy
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Chem-FINESE&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#21270;&#23398;&#39046;&#22495;&#20013;&#32454;&#31890;&#24230;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#23454;&#20307;&#25552;&#21462;&#22120;&#21644;&#33258;&#25105;&#39564;&#35777;&#27169;&#22359;&#26469;&#20174;&#36755;&#20837;&#21477;&#23376;&#20013;&#25552;&#21462;&#21629;&#21517;&#23454;&#20307;&#24182;&#37325;&#26500;&#21407;&#22987;&#36755;&#20837;&#21477;&#23376;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10189</link><description>&lt;p&gt;
Chem-FINESE: &#36890;&#36807;&#25991;&#26412;&#37325;&#26500;&#39564;&#35777;&#32454;&#31890;&#24230;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Chem-FINESE: Validating Fine-Grained Few-shot Entity Extraction through Text Reconstruction. (arXiv:2401.10189v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10189
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Chem-FINESE&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#21270;&#23398;&#39046;&#22495;&#20013;&#32454;&#31890;&#24230;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#23454;&#20307;&#25552;&#21462;&#22120;&#21644;&#33258;&#25105;&#39564;&#35777;&#27169;&#22359;&#26469;&#20174;&#36755;&#20837;&#21477;&#23376;&#20013;&#25552;&#21462;&#21629;&#21517;&#23454;&#20307;&#24182;&#37325;&#26500;&#21407;&#22987;&#36755;&#20837;&#21477;&#23376;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21270;&#23398;&#39046;&#22495;&#20013;&#65292;&#32454;&#31890;&#24230;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;&#38754;&#20020;&#20004;&#20010;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#19982;&#19968;&#33324;&#39046;&#22495;&#30340;&#23454;&#20307;&#25552;&#21462;&#20219;&#21153;&#30456;&#27604;&#65292;&#21270;&#23398;&#35770;&#25991;&#20013;&#30340;&#21477;&#23376;&#36890;&#24120;&#21253;&#21547;&#26356;&#22810;&#30340;&#23454;&#20307;&#12290;&#27492;&#22806;&#65292;&#23454;&#20307;&#25552;&#21462;&#27169;&#22411;&#36890;&#24120;&#38590;&#20197;&#25552;&#21462;&#38271;&#23614;&#31867;&#22411;&#30340;&#23454;&#20307;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;&#26041;&#27861;Chem-FINESE&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;Chem-FINESE&#21253;&#21547;&#20004;&#20010;&#32452;&#20214;&#65306;&#19968;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#23454;&#20307;&#25552;&#21462;&#22120;&#29992;&#20110;&#20174;&#36755;&#20837;&#21477;&#23376;&#20013;&#25552;&#21462;&#21629;&#21517;&#23454;&#20307;&#65292;&#20197;&#21450;&#19968;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#33258;&#25105;&#39564;&#35777;&#27169;&#22359;&#29992;&#20110;&#20174;&#25552;&#21462;&#30340;&#23454;&#20307;&#20013;&#37325;&#26500;&#21407;&#22987;&#36755;&#20837;&#21477;&#23376;&#12290;&#21463;&#21040;&#19968;&#20010;&#22909;&#30340;&#23454;&#20307;&#25552;&#21462;&#31995;&#32479;&#38656;&#35201;&#24544;&#23454;&#25552;&#21462;&#23454;&#20307;&#30340;&#20107;&#23454;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26032;&#33258;&#25105;&#39564;&#35777;&#27169;&#22359;&#21033;&#29992;&#23454;&#20307;&#25552;&#21462;&#32467;&#26524;&#26469;&#37325;&#26500;&#21407;&#22987;&#36755;&#20837;&#21477;&#23376;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#25439;&#22833;&#26469;&#20943;&#23569;&#22312;&#25552;&#21462;&#36807;&#31243;&#20013;&#30340;&#36807;&#24230;&#22797;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-grained few-shot entity extraction in the chemical domain faces two unique challenges. First, compared with entity extraction tasks in the general domain, sentences from chemical papers usually contain more entities. Moreover, entity extraction models usually have difficulty extracting entities of long-tailed types. In this paper, we propose Chem-FINESE, a novel sequence-to-sequence (seq2seq) based few-shot entity extraction approach, to address these two challenges. Our Chem-FINESE has two components: a seq2seq entity extractor to extract named entities from the input sentence and a seq2seq self-validation module to reconstruct the original input sentence from extracted entities. Inspired by the fact that a good entity extraction system needs to extract entities faithfully, our new self-validation module leverages entity extraction results to reconstruct the original input sentence. Besides, we design a new contrastive loss to reduce excessive copying during the extraction proces
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#23618;&#38388;&#30456;&#20851;&#20256;&#25773;&#26041;&#27861;&#20043;&#19978;&#20351;&#29992;&#31934;&#32454;&#21270;&#30340;&#20449;&#24687;&#27969;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;Transformer&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#31361;&#20986;&#37325;&#35201;&#20449;&#24687;&#24182;&#28040;&#38500;&#26080;&#20851;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22788;&#29702;&#20998;&#31867;&#21644;&#38382;&#31572;&#20219;&#21153;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#30456;&#27604;&#20854;&#20182;&#20843;&#31181;&#22522;&#32447;&#27169;&#22411;&#26356;&#21152;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2401.09972</link><description>&lt;p&gt;
&#36890;&#36807;&#31361;&#20986;&#37325;&#35201;&#20449;&#24687;&#26469;&#26356;&#22909;&#22320;&#35299;&#37322;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Better Explain Transformers by Illuminating Important Information. (arXiv:2401.09972v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09972
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#23618;&#38388;&#30456;&#20851;&#20256;&#25773;&#26041;&#27861;&#20043;&#19978;&#20351;&#29992;&#31934;&#32454;&#21270;&#30340;&#20449;&#24687;&#27969;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;Transformer&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#31361;&#20986;&#37325;&#35201;&#20449;&#24687;&#24182;&#28040;&#38500;&#26080;&#20851;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22788;&#29702;&#20998;&#31867;&#21644;&#38382;&#31572;&#20219;&#21153;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#30456;&#27604;&#20854;&#20182;&#20843;&#31181;&#22522;&#32447;&#27169;&#22411;&#26356;&#21152;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#21560;&#24341;&#20102;&#26080;&#25968;&#21162;&#21147;&#26469;&#35299;&#37322;&#20854;&#20869;&#37096;&#24037;&#20316;&#21407;&#29702;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#20851;&#27880;&#21407;&#22987;&#26799;&#24230;&#21644;&#27880;&#24847;&#21147;&#26469;&#35299;&#37322;Transformer&#65292;&#23558;&#38750;&#30456;&#20851;&#20449;&#24687;&#36890;&#24120;&#35270;&#20026;&#35299;&#37322;&#35745;&#31639;&#30340;&#19968;&#37096;&#20998;&#65292;&#23548;&#33268;&#32467;&#26524;&#28151;&#20081;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23618;&#38388;&#30456;&#20851;&#20256;&#25773;&#65288;LRP&#65289;&#26041;&#27861;&#20043;&#19978;&#36890;&#36807;&#31934;&#32454;&#21270;&#20449;&#24687;&#27969;&#26469;&#31361;&#20986;&#37325;&#35201;&#20449;&#24687;&#24182;&#28040;&#38500;&#26080;&#20851;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#23558;&#21477;&#27861;&#21644;&#20301;&#32622;&#22836;&#35782;&#21035;&#20026;&#37325;&#35201;&#27880;&#24847;&#21147;&#22836;&#65292;&#24182;&#19987;&#27880;&#20110;&#20174;&#36825;&#20123;&#37325;&#35201;&#22836;&#37096;&#33719;&#24471;&#30340;&#30456;&#20851;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#26080;&#20851;&#20449;&#24687;&#30830;&#23454;&#20250;&#25197;&#26354;&#36755;&#20986;&#30340;&#24402;&#22240;&#20998;&#25968;&#65292;&#22240;&#27492;&#22312;&#35299;&#37322;&#35745;&#31639;&#36807;&#31243;&#20013;&#24212;&#35813;&#23545;&#20854;&#36827;&#34892;&#23631;&#34109;&#12290;&#19982;&#20843;&#31181;&#22522;&#32447;&#27169;&#22411;&#22312;&#20998;&#31867;&#21644;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#27604;&#36739;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#32467;&#26524;&#19978;&#19981;&#26029;&#22320;&#34920;&#29616;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based models excel in various natural language processing (NLP) tasks, attracting countless efforts to explain their inner workings. Prior methods explain Transformers by focusing on the raw gradient and attention as token attribution scores, where non-relevant information is often considered during explanation computation, resulting in confusing results. In this work, we propose highlighting the important information and eliminating irrelevant information by a refined information flow on top of the layer-wise relevance propagation (LRP) method. Specifically, we consider identifying syntactic and positional heads as important attention heads and focus on the relevance obtained from these important heads. Experimental results demonstrate that irrelevant information does distort output attribution scores and then should be masked during explanation computation. Compared to eight baselines on both classification and question-answering datasets, our method consistently outperfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DevEval&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#23454;&#38469;&#36719;&#20214;&#39033;&#30446;&#20013;&#30340;&#20195;&#30721;&#29983;&#25104;&#12290;&#19982;&#20043;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#30456;&#27604;&#65292;DevEval&#22312;&#30495;&#23454;&#30340;&#39033;&#30446;&#20998;&#24067;&#12289;&#20805;&#36275;&#30340;&#20381;&#36182;&#21644;&#36275;&#22815;&#35268;&#27169;&#30340;&#39033;&#30446;&#32972;&#26223;&#31561;&#26041;&#38754;&#26356;&#36148;&#21512;&#23454;&#38469;&#12290;&#36890;&#36807;&#23545;&#20116;&#20010;&#27969;&#34892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#23454;&#38469;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.06401</link><description>&lt;p&gt;
DevEval: &#35780;&#20272;&#23454;&#38469;&#36719;&#20214;&#39033;&#30446;&#20013;&#30340;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
DevEval: Evaluating Code Generation in Practical Software Projects. (arXiv:2401.06401v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DevEval&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#23454;&#38469;&#36719;&#20214;&#39033;&#30446;&#20013;&#30340;&#20195;&#30721;&#29983;&#25104;&#12290;&#19982;&#20043;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#30456;&#27604;&#65292;DevEval&#22312;&#30495;&#23454;&#30340;&#39033;&#30446;&#20998;&#24067;&#12289;&#20805;&#36275;&#30340;&#20381;&#36182;&#21644;&#36275;&#22815;&#35268;&#27169;&#30340;&#39033;&#30446;&#32972;&#26223;&#31561;&#26041;&#38754;&#26356;&#36148;&#21512;&#23454;&#38469;&#12290;&#36890;&#36807;&#23545;&#20116;&#20010;&#27969;&#34892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#23454;&#38469;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#34920;&#29616;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#35768;&#22810;&#22522;&#20934;&#27979;&#35797;&#24050;&#32463;&#25552;&#20986;&#65292;&#20294;&#26159;&#19982;&#23454;&#38469;&#36719;&#20214;&#39033;&#30446;&#19981;&#19968;&#33268;&#65292;&#20363;&#22914;&#34394;&#26500;&#30340;&#31243;&#24207;&#20998;&#24067;&#65292;&#20381;&#36182;&#19981;&#36275;&#21644;&#23567;&#35268;&#27169;&#39033;&#30446;&#32972;&#26223;&#12290;&#22240;&#27492;&#65292;LLMs&#22312;&#23454;&#38469;&#39033;&#30446;&#20013;&#30340;&#33021;&#21147;&#36824;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DevEval&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#19982;&#24320;&#21457;&#20154;&#21592;&#22312;&#23454;&#38469;&#39033;&#30446;&#20013;&#30340;&#32463;&#39564;&#30456;&#21563;&#21512;&#12290;DevEval&#36890;&#36807;&#19968;&#20010;&#20005;&#26684;&#30340;&#27969;&#31243;&#25910;&#38598;&#21040;&#20102;&#26469;&#33258;119&#20010;&#23454;&#38469;&#39033;&#30446;&#30340;2690&#20010;&#26679;&#26412;&#65292;&#28085;&#30422;10&#20010;&#39046;&#22495;&#12290;&#19982;&#20043;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#30456;&#27604;&#65292;DevEval&#22312;&#22810;&#20010;&#32500;&#24230;&#19978;&#19982;&#23454;&#38469;&#39033;&#30446;&#30456;&#21563;&#21512;&#65292;&#20363;&#22914;&#30495;&#23454;&#30340;&#31243;&#24207;&#20998;&#24067;&#65292;&#20805;&#36275;&#30340;&#20381;&#36182;&#21644;&#36275;&#22815;&#35268;&#27169;&#30340;&#39033;&#30446;&#32972;&#26223;&#12290;&#25105;&#20204;&#22312;DevEval&#19978;&#35780;&#20272;&#20102;&#20116;&#20010;&#27969;&#34892;&#30340;LLMs&#65288;&#20363;&#22914;gpt-4&#65292;gpt-3.5-turbo&#65292;CodeLLaMa&#21644;StarCoder&#65289;&#65292;&#24182;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#23454;&#38469;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;gpt-3.5-turbo&#30340;&#26368;&#39640;Pass@1&#21482;&#26377;42&#12290;
&lt;/p&gt;
&lt;p&gt;
How to evaluate Large Language Models (LLMs) in code generation is an open question. Many benchmarks have been proposed but are inconsistent with practical software projects, e.g., unreal program distributions, insufficient dependencies, and small-scale project contexts. Thus, the capabilities of LLMs in practical projects are still unclear. In this paper, we propose a new benchmark named DevEval, aligned with Developers' experiences in practical projects. DevEval is collected through a rigorous pipeline, containing 2,690 samples from 119 practical projects and covering 10 domains. Compared to previous benchmarks, DevEval aligns to practical projects in multiple dimensions, e.g., real program distributions, sufficient dependencies, and enough-scale project contexts. We assess five popular LLMs on DevEval (e.g., gpt-4, gpt-3.5-turbo, CodeLLaMa, and StarCoder) and reveal their actual abilities in code generation. For instance, the highest Pass@1 of gpt-3.5-turbo only is 42 in our experim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#38463;&#25289;&#20271;&#35821;&#22635;&#23383;&#28216;&#25103;&#29983;&#25104;&#22120;ArabIcros&#65292;&#21033;&#29992;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#29420;&#29305;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#32447;&#32034;&#65292;&#24182;&#36890;&#36807;&#25945;&#32946;&#24615;&#22635;&#23383;&#28216;&#25103;&#25552;&#21319;&#23398;&#20064;&#20307;&#39564;&#65292;&#25913;&#21464;&#20256;&#32479;&#23398;&#20064;&#26041;&#27861;&#30340;&#26684;&#23616;&#12290;</title><link>http://arxiv.org/abs/2312.01339</link><description>&lt;p&gt;
ArabIcros: &#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#38463;&#25289;&#20271;&#35821;&#22635;&#23383;&#28216;&#25103;&#29983;&#25104;&#22120;&#29992;&#20110;&#25945;&#32946;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
ArabIcros: AI-Powered Arabic Crossword Puzzle Generation for Educational Applications. (arXiv:2312.01339v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.01339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#38463;&#25289;&#20271;&#35821;&#22635;&#23383;&#28216;&#25103;&#29983;&#25104;&#22120;ArabIcros&#65292;&#21033;&#29992;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#29420;&#29305;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#32447;&#32034;&#65292;&#24182;&#36890;&#36807;&#25945;&#32946;&#24615;&#22635;&#23383;&#28216;&#25103;&#25552;&#21319;&#23398;&#20064;&#20307;&#39564;&#65292;&#25913;&#21464;&#20256;&#32479;&#23398;&#20064;&#26041;&#27861;&#30340;&#26684;&#23616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#27454;&#30001;&#20808;&#36827;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#39537;&#21160;&#30340;&#38463;&#25289;&#20271;&#35821;&#22635;&#23383;&#28216;&#25103;&#29983;&#25104;&#22120;&#12290;&#35813;&#31995;&#32479;&#21033;&#29992;GPT4&#12289;GPT3-Davinci&#12289;GPT3-Curie&#12289;GPT3-Babbage&#12289;GPT3-Ada&#21644;BERT&#31561;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29983;&#25104;&#29420;&#29305;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#32447;&#32034;&#12290;&#22522;&#20110;&#21253;&#21547;&#36229;&#36807;50,000&#20010;&#32447;&#32034;-&#31572;&#26696;&#23545;&#30340;&#25968;&#25454;&#38598;&#65292;&#29983;&#25104;&#22120;&#37319;&#29992;&#24494;&#35843;&#12289;&#23569;&#37327;/&#38646;&#26679;&#26412;&#23398;&#20064;&#31574;&#30053;&#21644;&#20005;&#26684;&#30340;&#36136;&#37327;&#26816;&#26597;&#21327;&#35758;&#65292;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#32447;&#32034;-&#31572;&#26696;&#23545;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25945;&#32946;&#24615;&#22635;&#23383;&#28216;&#25103;&#26377;&#21161;&#20110;&#22686;&#24378;&#35760;&#24518;&#21147;&#12289;&#25193;&#23637;&#35789;&#27719;&#37327;&#21644;&#20419;&#36827;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#65292;&#20174;&#32780;&#36890;&#36807;&#26377;&#36259;&#21644;&#21560;&#24341;&#20154;&#30340;&#26041;&#24335;&#22686;&#24378;&#23398;&#20064;&#20307;&#39564;&#65292;&#25913;&#21464;&#20256;&#32479;&#23398;&#20064;&#26041;&#27861;&#30340;&#26684;&#23616;&#12290;&#25972;&#20010;&#31995;&#32479;&#21487;&#20197;&#34987;&#29992;&#20316;&#24378;&#22823;&#30340;&#25945;&#32946;&#24037;&#20855;&#65292;&#34701;&#21512;&#20154;&#24037;&#26234;&#33021;&#21644;&#21019;&#26032;&#30340;&#23398;&#20064;&#25216;&#26415;&#65292;&#20026;&#38463;&#25289;&#20271;&#35821;&#22635;&#23383;&#28216;&#25103;&#21644;&#25216;&#26415;&#19982;&#25945;&#32946;&#20132;&#21449;&#39046;&#22495;&#24102;&#26469;&#21464;&#38761;&#26102;&#20195;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the first Arabic crossword puzzle generator driven by advanced AI technology. Leveraging cutting-edge large language models including GPT4, GPT3-Davinci, GPT3-Curie, GPT3-Babbage, GPT3-Ada, and BERT, the system generates distinctive and challenging clues. Based on a dataset comprising over 50,000 clue-answer pairs, the generator employs fine-tuning, few/zero-shot learning strategies, and rigorous quality-checking protocols to enforce the generation of high-quality clue-answer pairs. Importantly, educational crosswords contribute to enhancing memory, expanding vocabulary, and promoting problem-solving skills, thereby augmenting the learning experience through a fun and engaging approach, reshaping the landscape of traditional learning methods. The overall system can be exploited as a powerful educational tool that amalgamates AI and innovative learning techniques, heralding a transformative era for Arabic crossword puzzles and the intersection of technology and educa
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20010;&#24615;&#21270;&#33976;&#39311;&#65292;&#23558;&#38381;&#28304;LLMs&#30340;&#33021;&#21147;&#20256;&#36882;&#32473;&#24320;&#28304;LLMs&#65292;&#24182;&#22312;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#27604;&#26631;&#20934;&#33976;&#39311;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#21482;&#20351;&#29992;&#19977;&#20998;&#20043;&#19968;&#30340;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2310.18628</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#33976;&#39311;&#65306;&#20026;&#20195;&#30721;&#29983;&#25104;&#36171;&#33021;&#33258;&#36866;&#24212;&#23398;&#20064;&#30340;&#24320;&#28304;LLMs
&lt;/p&gt;
&lt;p&gt;
Personalised Distillation: Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation. (arXiv:2310.18628v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18628
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20010;&#24615;&#21270;&#33976;&#39311;&#65292;&#23558;&#38381;&#28304;LLMs&#30340;&#33021;&#21147;&#20256;&#36882;&#32473;&#24320;&#28304;LLMs&#65292;&#24182;&#22312;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#27604;&#26631;&#20934;&#33976;&#39311;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#21482;&#20351;&#29992;&#19977;&#20998;&#20043;&#19968;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24378;&#22823;&#30340;&#38381;&#28304;LLMs&#65288;ChatGPT&#65292;GPT-4&#65289;&#30340;&#23835;&#36215;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#23558;&#38381;&#28304;LLMs&#30340;&#21151;&#33021;&#33976;&#39311;&#21040;&#36739;&#23567;&#30340;&#24320;&#28304;LLMs&#20013;&#34920;&#31034;&#20852;&#36259;&#12290;&#20197;&#24448;&#30340;&#33976;&#39311;&#26041;&#27861;&#36890;&#24120;&#26159;&#24341;&#23548;ChatGPT&#29983;&#25104;&#19968;&#32452;&#25351;&#20196;&#21644;&#31572;&#26696;&#65292;&#20197;&#20379;&#23398;&#29983;&#27169;&#22411;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26631;&#20934;&#33976;&#39311;&#26041;&#27861;&#24573;&#35270;&#20102;&#23398;&#29983;&#27169;&#22411;&#30340;&#20248;&#28857;&#21644;&#26465;&#20214;&#12290;&#21463;&#29616;&#20195;&#25945;&#23398;&#21407;&#21017;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#33976;&#39311;&#36807;&#31243;&#65292;&#20854;&#20013;&#23398;&#29983;&#39318;&#20808;&#23581;&#35797;&#35299;&#20915;&#19968;&#20010;&#20219;&#21153;&#65292;&#28982;&#21518;&#32769;&#24072;&#25552;&#20379;&#33258;&#36866;&#24212;&#30340;&#25913;&#36827;&#26041;&#27861;&#26469;&#24110;&#21161;&#23398;&#29983;&#25552;&#39640;&#12290;&#20010;&#24615;&#21270;&#33976;&#39311;&#19981;&#21516;&#20110;&#25552;&#20379;&#32473;&#23398;&#29983;&#32769;&#24072;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#23427;&#20351;&#23398;&#29983;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#20010;&#24615;&#21270;&#23398;&#20064;&#65292;&#21482;&#22312;&#33258;&#24049;&#29359;&#38169;&#35823;&#30340;&#31034;&#20363;&#19978;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#25913;&#36827;&#33258;&#24049;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#65292;&#20010;&#24615;&#21270;&#33976;&#39311;&#22987;&#32456;&#20248;&#20110;&#21482;&#20351;&#29992;&#19977;&#20998;&#20043;&#19968;&#25968;&#25454;&#30340;&#26631;&#20934;&#33976;&#39311;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rise of powerful closed-sourced LLMs (ChatGPT, GPT-4), there are increasing interests in distilling the capabilies of close-sourced LLMs to smaller open-sourced LLMs. Previous distillation methods usually prompt ChatGPT to generate a set of instructions and answers, for the student model to learn. However, such standard distillation approach neglects the merits and conditions of the student model. Inspired by modern teaching principles, we design a personalised distillation process, in which the student attempts to solve a task first, then the teacher provides an adaptive refinement for the student to improve. Instead of feeding the student with teacher's prior, personalised distillation enables personalised learning for the student model, as it only learns on examples it makes mistakes upon and learns to improve its own solution. On code generation, personalised distillation consistently outperforms standard distillation with only one third of the data. With only 2.5-3K perso
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#39044;&#35757;&#32451;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#23646;&#24615;/&#20540;&#25552;&#21462;&#25216;&#26415;&#20013;&#38656;&#35201;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#21644;&#23545;&#26410;&#30693;&#23646;&#24615;&#20540;&#30340;&#25361;&#25112;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.12537</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20135;&#21697;&#23646;&#24615;&#20540;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Product Attribute Value Extraction using Large Language Models. (arXiv:2310.12537v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#39044;&#35757;&#32451;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#23646;&#24615;/&#20540;&#25552;&#21462;&#25216;&#26415;&#20013;&#38656;&#35201;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#21644;&#23545;&#26410;&#30693;&#23646;&#24615;&#20540;&#30340;&#25361;&#25112;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#21830;&#21153;&#24212;&#29992;&#65288;&#22914;&#38754;&#21521;&#23646;&#24615;&#30340;&#20135;&#21697;&#25628;&#32034;&#25110;&#20135;&#21697;&#27604;&#36739;&#65289;&#22522;&#20110;&#32467;&#26500;&#21270;&#30340;&#20135;&#21697;&#25551;&#36848;&#65292;&#22914;&#23646;&#24615;/&#20540;&#23545;&#12290;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#19978;&#30340;&#20379;&#24212;&#21830;&#19981;&#25552;&#20379;&#32467;&#26500;&#21270;&#30340;&#20135;&#21697;&#25551;&#36848;&#65292;&#32780;&#26159;&#20351;&#29992;&#26631;&#39064;&#25110;&#25551;&#36848;&#26469;&#25551;&#36848;&#20135;&#21697;&#12290;&#20026;&#20102;&#22788;&#29702;&#36825;&#26679;&#30340;&#20135;&#21697;&#65292;&#26377;&#24517;&#35201;&#20174;&#25991;&#26412;&#20135;&#21697;&#23646;&#24615;&#20013;&#25552;&#21462;&#23646;&#24615;/&#20540;&#23545;&#12290;&#29616;&#26377;&#25216;&#26415;&#20013;&#65292;&#23646;&#24615;/&#20540;&#25552;&#21462;&#26041;&#27861;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT&#65289;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#23646;&#24615;/&#20540;&#25552;&#21462;&#26041;&#38754;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#32570;&#28857;&#65306;&#65288;&#19968;&#65289;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#35757;&#32451;&#25968;&#25454;&#65307;&#65288;&#20108;&#65289;&#20248;&#21270;&#21518;&#30340;&#27169;&#22411;&#22312;&#25512;&#24191;&#21040;&#35757;&#32451;&#25968;&#25454;&#20013;&#26410;&#21253;&#21547;&#30340;&#23646;&#24615;&#20540;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#25928;&#29575;&#39640;&#19988;&#40065;&#26834;&#24615;&#24378;&#30340;&#26367;&#20195;&#26041;&#27861;&#22312;&#23646;&#24615;/&#20540;&#25552;&#21462;&#20013;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#25176;&#31649;&#30340;LLMs&#65292;&#22914;GPT-3.5&#21644;GPT-4&#12290;
&lt;/p&gt;
&lt;p&gt;
E-commerce applications such as faceted product search or product comparison are based on structured product descriptions like attribute/value pairs. The vendors on e-commerce platforms do not provide structured product descriptions but describe offers using titles or descriptions. To process such offers, it is necessary to extract attribute/value pairs from textual product attributes. State-of-the-art attribute/value extraction techniques rely on pre-trained language models (PLMs), such as BERT. Two major drawbacks of these models for attribute/value extraction are that (i) the models require significant amounts of task-specific training data and (ii) the fine-tuned models face challenges in generalizing to attribute values not included in the training data. This paper explores the potential of large language models (LLMs) as a training data-efficient and robust alternative to PLM-based attribute/value extraction methods. We consider hosted LLMs, such as GPT-3.5 and GPT-4, as well as 
&lt;/p&gt;</description></item><item><title>ECoFLaP&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31895;&#21040;&#32454;&#30340;&#36880;&#23618;&#21098;&#26525;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#21387;&#32553;&#21644;&#37096;&#32626;&#26102;&#30340;&#35745;&#31639;&#21644;&#33021;&#32791;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.02998</link><description>&lt;p&gt;
ECoFLaP: &#39640;&#25928;&#30340;&#31895;&#21040;&#32454;&#30340;&#36880;&#23618;&#21098;&#26525;&#26041;&#27861;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ECoFLaP: Efficient Coarse-to-Fine Layer-Wise Pruning for Vision-Language Models. (arXiv:2310.02998v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02998
&lt;/p&gt;
&lt;p&gt;
ECoFLaP&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31895;&#21040;&#32454;&#30340;&#36880;&#23618;&#21098;&#26525;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#21387;&#32553;&#21644;&#37096;&#32626;&#26102;&#30340;&#35745;&#31639;&#21644;&#33021;&#32791;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#27169;&#24577;&#30340;&#20016;&#23500;&#20449;&#24687;&#65292;&#20840;&#38754;&#29702;&#35299;&#19990;&#30028;&#65292;&#24182;&#22312;&#21508;&#31181;&#22810;&#27169;&#24577;&#19979;&#28216;&#20219;&#21153;&#19978;&#21462;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#24040;&#22823;&#30340;&#35745;&#31639;/&#33021;&#32791;&#21644;&#30899;&#25490;&#25918;&#65292;&#37096;&#32626;LVLMs&#24448;&#24448;&#23384;&#22312;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#20351;&#24471;&#37319;&#29992;&#20256;&#32479;&#30340;&#36845;&#20195;&#20840;&#23616;&#21098;&#26525;&#21464;&#24471;&#19981;&#21487;&#34892;&#65292;&#22240;&#20026;&#20854;&#38656;&#35201;&#35745;&#31639;&#25972;&#20010;&#22823;&#22411;&#27169;&#22411;&#30340;Hessian&#30697;&#38453;&#20197;&#36827;&#34892;&#31232;&#30095;&#21270;&#12290;&#30456;&#21453;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#36880;&#23618;&#21098;&#26525;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#20840;&#23616;&#21098;&#26525;&#30340;&#26114;&#36149;&#35745;&#31639;&#65292;&#24182;&#26681;&#25454;&#23618;&#20869;&#26435;&#37325;&#30340;&#37325;&#35201;&#24615;&#26377;&#25928;&#21387;&#32553;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24120;&#24120;&#30001;&#20110;&#32570;&#20047;&#20840;&#23616;&#35270;&#35282;&#32780;&#23548;&#33268;&#27169;&#22411;&#21387;&#32553;&#19981;&#22815;&#20248;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#22823;&#22411;&#27169;&#22411;&#26368;&#36817;&#39640;&#25928;&#21098;&#26525;&#26041;&#27861;&#30340;&#36825;&#19968;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#31895;&#21040;&#32454;&#30340;&#36880;&#23618;&#21098;&#26525;&#26041;&#27861;&#65288;ECoFLaP&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Vision-Language Models (LVLMs) can understand the world comprehensively by integrating rich information from different modalities, achieving remarkable performance improvements on various multimodal downstream tasks. However, deploying LVLMs is often problematic due to their massive computational/energy costs and carbon consumption. Such issues make it infeasible to adopt conventional iterative global pruning, which is costly due to computing the Hessian matrix of the entire large model for sparsification. Alternatively, several studies have recently proposed layer-wise pruning approaches to avoid the expensive computation of global pruning and efficiently compress model weights according to their importance within a layer. However, these methods often suffer from suboptimal model compression due to their lack of a global perspective. To address this limitation in recent efficient pruning methods for large models, we propose Efficient Coarse-to-Fine Layer-Wise Pruning (ECoFLaP), 
&lt;/p&gt;</description></item><item><title>ChaCha&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#40723;&#21169;&#20799;&#31461;&#20998;&#20139;&#20010;&#20154;&#20107;&#20214;&#21644;&#30456;&#20851;&#24773;&#32490;&#12290;&#36890;&#36807;&#19968;&#20010;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#21457;&#29616;&#20799;&#31461;&#23558;ChaCha&#35270;&#20026;&#20146;&#23494;&#30340;&#26379;&#21451;&#65292;&#24182;&#24895;&#24847;&#19982;&#20854;&#20998;&#20139;&#21508;&#31181;&#20027;&#39064;&#30340;&#25925;&#20107;&#12290;</title><link>http://arxiv.org/abs/2309.12244</link><description>&lt;p&gt;
ChaCha&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#20799;&#31461;&#20998;&#20139;&#19982;&#20010;&#20154;&#20107;&#20214;&#30456;&#20851;&#30340;&#24773;&#32490;
&lt;/p&gt;
&lt;p&gt;
ChaCha: Leveraging Large Language Models to Prompt Children to Share Their Emotions about Personal Events. (arXiv:2309.12244v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12244
&lt;/p&gt;
&lt;p&gt;
ChaCha&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#40723;&#21169;&#20799;&#31461;&#20998;&#20139;&#20010;&#20154;&#20107;&#20214;&#21644;&#30456;&#20851;&#24773;&#32490;&#12290;&#36890;&#36807;&#19968;&#20010;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#21457;&#29616;&#20799;&#31461;&#23558;ChaCha&#35270;&#20026;&#20146;&#23494;&#30340;&#26379;&#21451;&#65292;&#24182;&#24895;&#24847;&#19982;&#20854;&#20998;&#20139;&#21508;&#31181;&#20027;&#39064;&#30340;&#25925;&#20107;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20799;&#31461;&#36890;&#24120;&#36890;&#36807;&#19982;&#23478;&#20154;&#25110;&#20182;&#20154;&#20998;&#20139;&#25925;&#20107;&#21644;&#24863;&#21463;&#26469;&#23398;&#20064;&#36776;&#35782;&#21644;&#34920;&#36798;&#24773;&#32490;&#65292;&#28982;&#32780;&#65292;&#30001;&#20110;&#20799;&#31461;&#27491;&#22312;&#21457;&#23637;&#20182;&#20204;&#30340;&#20132;&#27969;&#25216;&#33021;&#65292;&#29238;&#27597;&#25110;&#20804;&#24351;&#22992;&#22969;&#24456;&#38590;&#19982;&#20182;&#20204;&#36827;&#34892;&#24773;&#24863;&#27807;&#36890;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ChaCha&#65292;&#19968;&#20010;&#40723;&#21169;&#21644;&#24341;&#23548;&#20799;&#31461;&#20998;&#20139;&#20010;&#20154;&#20107;&#20214;&#21644;&#30456;&#20851;&#24773;&#32490;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;ChaCha&#32467;&#21512;&#20102;&#29366;&#24577;&#26426;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22312;&#36827;&#34892;&#33258;&#30001;&#23545;&#35805;&#30340;&#21516;&#26102;&#20445;&#25345;&#23545;&#35805;&#30340;&#26041;&#21521;&#24615;&#12290;&#36890;&#36807;&#19982;20&#21517;&#24180;&#40836;&#22312;8-12&#23681;&#30340;&#20799;&#31461;&#36827;&#34892;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;ChaCha&#22914;&#20309;&#20419;&#20351;&#20799;&#31461;&#20998;&#20139;&#20010;&#20154;&#20107;&#20214;&#24182;&#24341;&#23548;&#20182;&#20204;&#25551;&#36848;&#30456;&#20851;&#24773;&#32490;&#12290;&#21442;&#19982;&#32773;&#35748;&#20026;ChaCha&#23601;&#20687;&#19968;&#20010;&#20146;&#23494;&#30340;&#26379;&#21451;&#65292;&#24182;&#20998;&#20139;&#20102;&#21508;&#31181;&#20027;&#39064;&#30340;&#25925;&#20107;&#65292;&#22914;&#23478;&#24237;&#26053;&#34892;&#21644;&#20010;&#20154;&#25104;&#23601;&#12290;&#22522;&#20110;&#23450;&#37327;&#21644;&#23450;&#24615;&#21457;&#29616;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#21033;&#29992;LLMs&#35774;&#35745;&#36866;&#21512;&#20799;&#31461;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
Children typically learn to identify and express emotions through sharing their stories and feelings with others, particularly their family. However, it is challenging for parents or siblings to have emotional communication with children since children are still developing their communication skills. We present ChaCha, a chatbot that encourages and guides children to share personal events and associated emotions. ChaCha combines a state machine and large language models (LLMs) to keep the dialogue on track while carrying on free-form conversations. Through an exploratory study with 20 children (aged 8-12), we examine how ChaCha prompts children to share personal events and guides them to describe associated emotions. Participants perceived ChaCha as a close friend and shared their stories on various topics, such as family trips and personal achievements. Based on the quantitative and qualitative findings, we discuss opportunities for leveraging LLMs to design child-friendly chatbots to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#27169;&#24577;&#27880;&#24847;&#22686;&#24378;&#30340;&#25991;&#26412;-&#35270;&#39057;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#65292;&#33021;&#22815;&#20934;&#30830;&#34913;&#37327;&#36328;&#27169;&#24577;&#30456;&#20284;&#24615;&#21644;&#25366;&#25496;&#38590;&#36127;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2309.11082</link><description>&lt;p&gt;
&#21452;&#27169;&#24577;&#27880;&#24847;&#22686;&#24378;&#30340;&#25991;&#26412;-&#35270;&#39057;&#26816;&#32034;&#19982;&#19977;&#20803;&#37096;&#20998;&#36793;&#38469;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Dual-Modal Attention-Enhanced Text-Video Retrieval with Triplet Partial Margin Contrastive Learning. (arXiv:2309.11082v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#27169;&#24577;&#27880;&#24847;&#22686;&#24378;&#30340;&#25991;&#26412;-&#35270;&#39057;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#65292;&#33021;&#22815;&#20934;&#30830;&#34913;&#37327;&#36328;&#27169;&#24577;&#30456;&#20284;&#24615;&#21644;&#25366;&#25496;&#38590;&#36127;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#32593;&#32476;&#35270;&#39057;&#30340;&#29190;&#28856;&#24615;&#22686;&#38271;&#20351;&#24471;&#25991;&#26412;-&#35270;&#39057;&#26816;&#32034;&#23545;&#20110;&#35270;&#39057;&#36807;&#28388;&#12289;&#25512;&#33616;&#21644;&#25628;&#32034;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#21644;&#27969;&#34892;&#12290;&#25991;&#26412;-&#35270;&#39057;&#26816;&#32034;&#26088;&#22312;&#23558;&#30456;&#20851;&#30340;&#25991;&#26412;/&#35270;&#39057;&#25490;&#22312;&#19981;&#30456;&#20851;&#30340;&#25991;&#26412;/&#35270;&#39057;&#20043;&#21069;&#12290;&#35813;&#20219;&#21153;&#30340;&#26680;&#24515;&#26159;&#20934;&#30830;&#34913;&#37327;&#25991;&#26412;&#21644;&#35270;&#39057;&#20043;&#38388;&#30340;&#36328;&#27169;&#24577;&#30456;&#20284;&#24615;&#12290;&#26368;&#36817;&#65292;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#22312;&#25991;&#26412;-&#35270;&#39057;&#26816;&#32034;&#26041;&#38754;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#26041;&#27861;&#20391;&#37325;&#20110;&#26500;&#24314;&#27491;&#36127;&#26679;&#26412;&#23545;&#20197;&#23398;&#20064;&#25991;&#26412;&#21644;&#35270;&#39057;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#22312;&#20851;&#27880;&#38590;&#36127;&#26679;&#26412;&#21644;&#27169;&#25311;&#19981;&#21516;&#23618;&#27425;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#26041;&#38754;&#19981;&#22815;&#65292;&#23384;&#22312;&#20004;&#20010;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#20351;&#29992;&#20004;&#20010;&#26032;&#26041;&#27861;&#25913;&#36827;&#20102;&#23545;&#27604;&#23398;&#20064;&#12290;&#39318;&#20808;&#65292;&#20026;&#20102;&#21033;&#29992;&#33392;&#38590;&#30340;&#20363;&#23376;&#26469;&#25552;&#39640;&#40065;&#26834;&#30340;&#21028;&#21035;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#27169;&#24577;&#27880;&#24847;&#22686;&#24378;&#27169;&#22359;(DMAE)&#65292;&#20174;&#25991;&#26412;&#21644;&#35270;&#35273;&#32447;&#32034;&#20013;&#25366;&#25496;&#38590;&#36127;&#26679;&#26412;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#24341;&#20837;&#19968;&#20010;&#36127;&#38754;&#26679;&#26412;&#31579;&#36873;&#26426;&#21046;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#24314;&#27169;&#19981;&#21516;&#32423;&#21035;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the explosion of web videos makes text-video retrieval increasingly essential and popular for video filtering, recommendation, and search. Text-video retrieval aims to rank relevant text/video higher than irrelevant ones. The core of this task is to precisely measure the cross-modal similarity between texts and videos. Recently, contrastive learning methods have shown promising results for text-video retrieval, most of which focus on the construction of positive and negative pairs to learn text and video representations. Nevertheless, they do not pay enough attention to hard negative pairs and lack the ability to model different levels of semantic similarity. To address these two issues, this paper improves contrastive learning using two novel techniques. First, to exploit hard examples for robust discriminative power, we propose a novel Dual-Modal Attention-Enhanced Module (DMAE) to mine hard negative pairs from textual and visual clues. By further introducing a Negat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#24076;&#33098;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#29616;&#20195;NLP&#27979;&#35797;&#22871;&#20214;&#65292;&#20854;&#20013;&#21253;&#21547;&#22235;&#20010;&#19987;&#23478;&#39564;&#35777;&#30340;&#35780;&#20272;&#20219;&#21153;&#65292;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#12289;&#35789;&#20041;&#28040;&#27495;&#21644;&#38544;&#21947;&#26816;&#27979;&#12290;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#25512;&#29702;&#25968;&#25454;&#38598;&#39318;&#27425;&#26631;&#35760;&#20102;&#25152;&#26377;&#21487;&#33021;&#30340;&#25512;&#29702;&#26631;&#31614;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#19968;&#31181;&#23545;&#20110;&#36164;&#28304;&#21294;&#20047;&#35821;&#35328;&#33719;&#21462;&#25968;&#25454;&#38598;&#30340;&#25104;&#26412;&#25928;&#30410;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.07009</link><description>&lt;p&gt;
OYXOY:&#36866;&#29992;&#20110;&#29616;&#20195;&#24076;&#33098;&#35821;&#30340;&#29616;&#20195;NLP&#27979;&#35797;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
OYXOY: A Modern NLP Test Suite for Modern Greek. (arXiv:2309.07009v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#24076;&#33098;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#29616;&#20195;NLP&#27979;&#35797;&#22871;&#20214;&#65292;&#20854;&#20013;&#21253;&#21547;&#22235;&#20010;&#19987;&#23478;&#39564;&#35777;&#30340;&#35780;&#20272;&#20219;&#21153;&#65292;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#12289;&#35789;&#20041;&#28040;&#27495;&#21644;&#38544;&#21947;&#26816;&#27979;&#12290;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#25512;&#29702;&#25968;&#25454;&#38598;&#39318;&#27425;&#26631;&#35760;&#20102;&#25152;&#26377;&#21487;&#33021;&#30340;&#25512;&#29702;&#26631;&#31614;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#19968;&#31181;&#23545;&#20110;&#36164;&#28304;&#21294;&#20047;&#35821;&#35328;&#33719;&#21462;&#25968;&#25454;&#38598;&#30340;&#25104;&#26412;&#25928;&#30410;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#20026;&#24076;&#33098;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24320;&#23637;&#30340;&#19968;&#39033;&#22522;&#30784;&#24615;&#24037;&#20316;&#65292;&#26088;&#22312;&#21019;&#24314;&#19968;&#20010;&#22522;&#20110;&#35821;&#35328;&#23398;&#21644;&#25216;&#26415;&#30456;&#20851;&#24615;&#30340;&#35780;&#20272;&#22871;&#20214;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#22235;&#20010;&#32463;&#36807;&#19987;&#23478;&#39564;&#35777;&#30340;&#35780;&#20272;&#20219;&#21153;&#26469;&#24320;&#23637;&#36825;&#39033;&#24037;&#20316;&#65292;&#36825;&#20123;&#20219;&#21153;&#19987;&#38376;&#38024;&#23545;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#12289;&#35789;&#20041;&#28040;&#27495;&#65288;&#36890;&#36807;&#31034;&#20363;&#27604;&#36739;&#25110;&#36873;&#25321;&#24847;&#20041;&#65289;&#21644;&#38544;&#21947;&#26816;&#27979;&#12290;&#19982;&#29616;&#26377;&#20219;&#21153;&#30340;&#35821;&#35328;&#36866;&#24212;&#21103;&#26412;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#36129;&#29486;&#20102;&#20004;&#20010;&#21019;&#26032;&#28857;&#65292;&#36825;&#20123;&#23558;&#19982;&#26356;&#24191;&#27867;&#30340;&#36164;&#28304;&#21644;&#35780;&#20272;&#31038;&#21306;&#20135;&#29983;&#20849;&#40483;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25512;&#29702;&#25968;&#25454;&#38598;&#26159;&#39318;&#20010;&#26631;&#35760;&#20102;&#19981;&#20165;&#20165;&#26159;\texttt{&#19968;}&#31181;&#65292;&#32780;&#26159;\texttt{&#25152;&#26377;}&#21487;&#33021;&#25512;&#29702;&#26631;&#31614;&#30340;&#25968;&#25454;&#38598;&#65292;&#32771;&#34385;&#21040;&#30001;&#20110;&#27495;&#20041;&#24615;&#25110;&#22810;&#20041;&#24615;&#21487;&#33021;&#23548;&#33268;&#30340;&#21487;&#33021;&#36716;&#21464;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#23545;&#20110;&#36164;&#28304;&#21294;&#20047;&#35821;&#35328;&#33719;&#21462;&#25968;&#25454;&#38598;&#30340;&#25104;&#26412;&#25928;&#30410;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;ChatGPT&#20316;&#20026;&#35821;&#35328;&#20013;&#31435;&#35299;&#26512;&#22120;&#65292;&#25105;&#20204;&#23558;&#24076;&#33098;&#29616;&#20195;&#26631;&#20934;&#35789;&#20856;&#36716;&#25442;&#20026;&#32467;&#26500;&#21270;&#26684;&#24335;&#65292;&#20174;&#20013;&#34893;&#29983;&#20986;&#20854;&#20182;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper serves as a foundational step towards the development of a linguistically motivated and technically relevant evaluation suite for Greek NLP. We initiate this endeavor by introducing four expert-verified evaluation tasks, specifically targeted at natural language inference, word sense disambiguation (through example comparison or sense selection) and metaphor detection. More than language-adapted replicas of existing tasks, we contribute two innovations which will resonate with the broader resource and evaluation community. Firstly, our inference dataset is the first of its kind, marking not just \textit{one}, but rather \textit{all} possible inference labels, accounting for possible shifts due to e.g. ambiguity or polysemy. Secondly, we demonstrate a cost-efficient method to obtain datasets for under-resourced languages. Using ChatGPT as a language-neutral parser, we transform the Dictionary of Standard Modern Greek into a structured format, from which we derive the other th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;HC3 Plus&#65292;&#19968;&#20010;&#35821;&#20041;&#19981;&#21464;&#30340;&#20154;&#31867;ChatGPT&#23545;&#27604;&#35821;&#26009;&#24211;&#12290;&#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#35813;&#35821;&#26009;&#24211;&#32771;&#34385;&#20102;&#26356;&#22810;&#31867;&#22411;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#35821;&#20041;&#19981;&#21464;&#20219;&#21153;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#35821;&#20041;&#19981;&#21464;&#20219;&#21153;&#20013;&#26816;&#27979;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#26356;&#21152;&#22256;&#38590;&#12290;&#36890;&#36807;&#22823;&#37327;&#20219;&#21153;&#25351;&#20196;&#24494;&#35843;&#21644;Tk-instruct&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.02731</link><description>&lt;p&gt;
HC3 Plus&#65306;&#19968;&#20010;&#35821;&#20041;&#19981;&#21464;&#30340;&#20154;&#31867;ChatGPT&#23545;&#27604;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
HC3 Plus: A Semantic-Invariant Human ChatGPT Comparison Corpus. (arXiv:2309.02731v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;HC3 Plus&#65292;&#19968;&#20010;&#35821;&#20041;&#19981;&#21464;&#30340;&#20154;&#31867;ChatGPT&#23545;&#27604;&#35821;&#26009;&#24211;&#12290;&#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#35813;&#35821;&#26009;&#24211;&#32771;&#34385;&#20102;&#26356;&#22810;&#31867;&#22411;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#35821;&#20041;&#19981;&#21464;&#20219;&#21153;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#35821;&#20041;&#19981;&#21464;&#20219;&#21153;&#20013;&#26816;&#27979;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#26356;&#21152;&#22256;&#38590;&#12290;&#36890;&#36807;&#22823;&#37327;&#20219;&#21153;&#25351;&#20196;&#24494;&#35843;&#21644;Tk-instruct&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#22240;&#20854;&#20986;&#33394;&#30340;&#24615;&#33021;&#32780;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#20154;&#20204;&#23545;&#20854;&#28508;&#22312;&#39118;&#38505;&#65292;&#23588;&#20854;&#26159;&#23545;AI&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#30340;&#26816;&#27979;&#36234;&#26469;&#36234;&#20851;&#27880;&#65292;&#36825;&#23545;&#26410;&#32463;&#35757;&#32451;&#30340;&#20154;&#31867;&#26469;&#35828;&#24448;&#24448;&#24456;&#38590;&#35782;&#21035;&#12290;&#30446;&#21069;&#29992;&#20110;&#26816;&#27979;ChatGPT&#29983;&#25104;&#25991;&#26412;&#30340;&#25968;&#25454;&#38598;&#20027;&#35201;&#38598;&#20013;&#22312;&#38382;&#31572;&#26041;&#38754;&#65292;&#20294;&#24448;&#24448;&#24573;&#35270;&#20102;&#20855;&#26377;&#35821;&#20041;&#19981;&#21464;&#24615;&#30340;&#20219;&#21153;&#65292;&#22914;&#25688;&#35201;&#12289;&#32763;&#35793;&#21644;&#25913;&#20889;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#35821;&#20041;&#19981;&#21464;&#20219;&#21153;&#19978;&#26816;&#27979;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#26356;&#21152;&#22256;&#38590;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26356;&#24191;&#27867;&#12289;&#26356;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#65292;&#32771;&#34385;&#20102;&#27604;&#20197;&#21069;&#30340;&#24037;&#20316;&#26356;&#22810;&#31867;&#22411;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#35821;&#20041;&#19981;&#21464;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#32463;&#36807;&#22823;&#37327;&#20219;&#21153;&#25351;&#20196;&#24494;&#35843;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#24615;&#33021;&#12290;&#22522;&#20110;&#20197;&#21069;&#30340;&#25104;&#21151;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25351;&#23548;&#24494;&#35843;&#20102;Tk-instruct&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT has gained significant interest due to its impressive performance, but people are increasingly concerned about its potential risks, particularly around the detection of AI-generated content (AIGC), which is often difficult for untrained humans to identify. Current datasets utilized for detecting ChatGPT-generated text primarily center around question-answering, yet they tend to disregard tasks that possess semantic-invariant properties, such as summarization, translation, and paraphrasing. Our primary studies demonstrate that detecting model-generated text on semantic-invariant tasks is more difficult. To fill this gap, we introduce a more extensive and comprehensive dataset that considers more types of tasks than previous work, including semantic-invariant tasks. In addition, the model after a large number of task instruction fine-tuning shows a strong powerful performance. Owing to its previous success, we further instruct fine-tuning Tk-instruct and built a more powerful det
&lt;/p&gt;</description></item><item><title>FlexKBQA&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#31243;&#24207;&#32763;&#35793;&#22120;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#31639;&#27861;&#20174;&#30693;&#35782;&#24211;&#20013;&#25277;&#26679;&#22810;&#26679;&#30340;&#31243;&#24207;&#65292;&#23558;&#20854;&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#65292;&#20174;&#32780;&#35299;&#20915;&#23569;&#26679;&#26412;&#30693;&#35782;&#24211;&#38382;&#31572;&#20219;&#21153;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.12060</link><description>&lt;p&gt;
FlexKBQA&#65306;&#19968;&#31181;&#29992;&#20110;&#23569;&#26679;&#26412;&#30693;&#35782;&#24211;&#38382;&#31572;&#30340;&#28789;&#27963;LLM&#39537;&#21160;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FlexKBQA: A Flexible LLM-Powered Framework for Few-Shot Knowledge Base Question Answering. (arXiv:2308.12060v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12060
&lt;/p&gt;
&lt;p&gt;
FlexKBQA&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#31243;&#24207;&#32763;&#35793;&#22120;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#31639;&#27861;&#20174;&#30693;&#35782;&#24211;&#20013;&#25277;&#26679;&#22810;&#26679;&#30340;&#31243;&#24207;&#65292;&#23558;&#20854;&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#65292;&#20174;&#32780;&#35299;&#20915;&#23569;&#26679;&#26412;&#30693;&#35782;&#24211;&#38382;&#31572;&#20219;&#21153;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#24211;&#38382;&#31572;&#65288;KBQA&#65289;&#26159;&#19968;&#20010;&#20851;&#38190;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#30693;&#35782;&#24211;&#20013;&#30340;&#23454;&#20307;&#25968;&#37327;&#24222;&#22823;&#65292;&#24182;&#19988;&#29992;&#25143;&#25552;&#20986;&#30340;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#22810;&#26679;&#21270;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22823;&#22810;&#25968;KBQA&#27169;&#22411;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#65292;&#22240;&#20026;&#39640;&#36136;&#37327;&#30340;&#27880;&#37322;&#25968;&#25454;&#19981;&#36275;&#12290;&#20026;&#20102;&#20943;&#36731;&#25163;&#21160;&#27880;&#37322;&#30340;&#36127;&#25285;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#31243;&#24207;&#32763;&#35793;&#22120;&#65292;&#20171;&#32461;&#20102;FlexKBQA&#26469;&#35299;&#20915;&#23569;&#26679;&#26412;KBQA&#20219;&#21153;&#20013;&#22266;&#26377;&#30340;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;FlexKBQA&#21033;&#29992;&#33258;&#21160;&#21270;&#31639;&#27861;&#20174;&#30693;&#35782;&#24211;&#20013;&#25277;&#26679;&#22810;&#26679;&#30340;&#31243;&#24207;&#65288;&#22914;SPARQL&#26597;&#35810;&#65289;&#65292;&#28982;&#21518;&#36890;&#36807;LLMs&#23558;&#20854;&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#12290;&#36825;&#20010;&#21512;&#25104;&#30340;&#25968;&#25454;&#38598;&#26377;&#21161;&#20110;&#35757;&#32451;&#19968;&#20010;&#19987;&#38376;&#30340;&#36731;&#37327;&#32423;&#27169;&#22411;&#26469;&#22788;&#29702;&#30693;&#35782;&#24211;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20943;&#23569;&#21512;&#25104;&#25968;&#25454;&#19982;&#30495;&#23454;&#29992;&#25143;&#38382;&#39064;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#30340;&#38556;&#30861;&#65292;FlexKBQA&#24341;&#20837;&#20102;&#19968;&#20010;&#25191;&#34892;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge base question answering (KBQA) is a critical yet challenging task due to the vast number of entities within knowledge bases and the diversity of natural language questions posed by users. Unfortunately, the performance of most KBQA models tends to decline significantly in real-world scenarios where high-quality annotated data is insufficient. To mitigate the burden associated with manual annotation, we introduce FlexKBQA by utilizing Large Language Models (LLMs) as program translators for addressing the challenges inherent in the few-shot KBQA task. Specifically, FlexKBQA leverages automated algorithms to sample diverse programs, such as SPARQL queries, from the knowledge base, which are subsequently converted into natural language questions via LLMs. This synthetic dataset facilitates training a specialized lightweight model for the KB. Additionally, to reduce the barriers of distribution shift between synthetic data and real user questions, FlexKBQA introduces an executiong
&lt;/p&gt;</description></item><item><title>LogPrompt&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#26412;&#21644;&#21487;&#35299;&#37322;&#30340;&#26085;&#24535;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#19987;&#20026;&#26085;&#24535;&#20219;&#21153;&#35774;&#35745;&#30340;&#39640;&#32423;&#25552;&#31034;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#23545;&#26085;&#24535;&#30340;&#20998;&#26512;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.07610</link><description>&lt;p&gt;
LogPrompt: &#38646;&#26679;&#26412;&#21644;&#21487;&#35299;&#37322;&#30340;&#26085;&#24535;&#20998;&#26512;&#30340;&#25552;&#31034;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
LogPrompt: Prompt Engineering Towards Zero-Shot and Interpretable Log Analysis. (arXiv:2308.07610v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07610
&lt;/p&gt;
&lt;p&gt;
LogPrompt&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#26412;&#21644;&#21487;&#35299;&#37322;&#30340;&#26085;&#24535;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#19987;&#20026;&#26085;&#24535;&#20219;&#21153;&#35774;&#35745;&#30340;&#39640;&#32423;&#25552;&#31034;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#23545;&#26085;&#24535;&#30340;&#20998;&#26512;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#36719;&#20214;&#23494;&#38598;&#22411;&#31995;&#32479;&#20013;&#65292;&#33258;&#21160;&#21270;&#26085;&#24535;&#20998;&#26512;&#23545;&#20110;&#30830;&#20445;&#36719;&#20214;&#32500;&#25252;&#21644;&#24037;&#31243;&#29983;&#21629;&#21608;&#26399;&#30340;&#21487;&#38752;&#24615;&#21644;&#24377;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#25552;&#20379;&#21333;&#20010;&#39044;&#27979;&#20540;&#32780;&#27809;&#26377;&#35299;&#37322;&#26469;&#25191;&#34892;&#35832;&#22914;&#26085;&#24535;&#35299;&#26512;&#21644;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#31561;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#31995;&#32479;&#20107;&#20214;&#30340;&#22686;&#21152;&#65292;&#20998;&#26512;&#32467;&#26524;&#30340;&#26377;&#38480;&#21487;&#35299;&#37322;&#24615;&#38459;&#30861;&#20102;&#20998;&#26512;&#20154;&#21592;&#23545;&#20854;&#30340;&#20449;&#20219;&#24230;&#21644;&#37319;&#21462;&#36866;&#24403;&#34892;&#21160;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#39046;&#22495;&#20869;&#22521;&#35757;&#25968;&#25454;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#24615;&#33021;&#22312;&#28041;&#21450;&#26032;&#22495;&#30340;&#26410;&#35265;&#36807;&#26085;&#24535;&#30340;&#22312;&#32447;&#22330;&#26223;&#20013;&#24613;&#21095;&#19979;&#38477;&#65288;&#26368;&#22810;&#19979;&#38477;62.5%&#65289;&#65292;&#36825;&#26159;&#30001;&#20110;&#36719;&#20214;&#26356;&#26032;&#30340;&#36805;&#36895;&#32780;&#24120;&#35265;&#30340;&#24773;&#20917;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#38646;&#26679;&#26412;&#21644;&#21487;&#35299;&#37322;&#30340;&#26085;&#24535;&#20998;&#26512;&#26041;&#27861;LogPrompt&#12290;LogPrompt&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#19968;&#22871;&#38024;&#23545;&#26085;&#24535;&#20219;&#21153;&#30340;&#39640;&#32423;&#25552;&#31034;&#31574;&#30053;&#25191;&#34892;&#38646;&#26679;&#26412;&#26085;&#24535;&#20998;&#26512;&#20219;&#21153;&#65292;&#20174;&#32780;&#22686;&#24378;LLM&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated log analysis is crucial in modern software-intensive systems for ensuring reliability and resilience throughout software maintenance and engineering life cycles. Existing methods perform tasks such as log parsing and log anomaly detection by providing a single prediction value without interpretation. However, given the increasing volume of system events, the limited interpretability of analysis results hinders analysts' trust and their ability to take appropriate actions. Moreover, these methods require substantial in-domain training data, and their performance declines sharply (by up to 62.5%) in online scenarios involving unseen logs from new domains, a common occurrence due to rapid software updates. In this paper, we propose LogPrompt, a novel zero-shot and interpretable log analysis approach. LogPrompt employs large language models (LLMs) to perform zero-shot log analysis tasks via a suite of advanced prompt strategies tailored for log tasks, which enhances LLMs' perform
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#25991;&#26412;&#25351;&#20196;&#20013;&#23398;&#20064;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#25214;&#20986;&#23450;&#20041;&#20013;&#30340;&#20851;&#38190;&#21477;&#23376;&#21644;&#20351;&#29992;&#25490;&#21517;&#30446;&#26631;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.03795</link><description>&lt;p&gt;
&#24536;&#35760;&#28436;&#31034;&#65292;&#19987;&#27880;&#20110;&#20174;&#25991;&#26412;&#25351;&#20196;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Forget Demonstrations, Focus on Learning from Textual Instructions. (arXiv:2308.03795v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#25991;&#26412;&#25351;&#20196;&#20013;&#23398;&#20064;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#25214;&#20986;&#23450;&#20041;&#20013;&#30340;&#20851;&#38190;&#21477;&#23376;&#21644;&#20351;&#29992;&#25490;&#21517;&#30446;&#26631;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#38646;&#31034;&#33539;&#36328;&#20219;&#21153;&#27867;&#21270;&#30340;&#19968;&#20010;&#26356;&#20855;&#25361;&#25112;&#24615;&#20294;&#26356;&#30495;&#23454;&#30340;&#24773;&#22659;&#36827;&#34892;&#30740;&#31350;&#65306;&#20174;&#25991;&#26412;&#25351;&#20196;&#20013;&#23398;&#20064;&#32780;&#26080;&#38656;&#31034;&#33539;&#65292;&#20551;&#35774;&#23384;&#22312;&#19968;&#31181;&#27573;&#33853;&#24335;&#20219;&#21153;&#23450;&#20041;&#20294;&#19981;&#23384;&#22312;&#31034;&#33539;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#20174;&#23450;&#20041;&#20013;&#23398;&#20064;&#20219;&#21153;&#30417;&#30563;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#65306;&#39318;&#20808;&#65292;&#33258;&#21160;&#25214;&#20986;&#23450;&#20041;&#20013;&#30340;&#20851;&#38190;&#21477;&#23376;&#65307;&#20854;&#27425;&#65292;&#20351;&#29992;&#25490;&#21517;&#30446;&#26631;&#26469;&#24378;&#21046;&#27169;&#22411;&#22312;&#23450;&#20041;&#20013;&#31361;&#20986;&#26174;&#31034;&#36825;&#20123;&#20851;&#38190;&#37096;&#20998;&#26102;&#29983;&#25104;&#37329;&#26631;&#36755;&#20986;&#30340;&#27010;&#29575;&#26356;&#39640;&#12290;&#36825;&#20004;&#31181;&#31574;&#30053;&#30340;&#20849;&#21516;&#21162;&#21147;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#20135;&#29983;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#23558;&#22312;&#35770;&#25991;&#30340;&#26368;&#32456;&#29256;&#26412;&#20013;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work studies a challenging yet more realistic setting for zero-shot cross-task generalization: demonstration-free learning from textual instructions, presuming the existence of a paragraph-style task definition while no demonstrations exist. To better learn the task supervision from the definition, we propose two strategies: first, to automatically find out the critical sentences in the definition; second, a ranking objective to force the model to generate the gold outputs with higher probabilities when those critical parts are highlighted in the definition. The joint efforts of the two strategies yield state-of-the-art performance on the challenging benchmark. Our code will be released in the final version of the paper.
&lt;/p&gt;</description></item><item><title>MeetEval&#26159;&#19968;&#20010;&#35745;&#31639;&#20250;&#35758;&#36716;&#24405;&#31995;&#32479;&#23383;&#38169;&#35823;&#29575;&#30340;&#24037;&#20855;&#21253;&#65292;&#23427;&#36890;&#36807;&#26102;&#38388;&#32422;&#26463;&#26469;&#25552;&#39640;&#21305;&#37197;&#36136;&#37327;&#24182;&#21152;&#36895;&#21305;&#37197;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.11394</link><description>&lt;p&gt;
MeetEval: &#19968;&#31181;&#29992;&#20110;&#20250;&#35758;&#36716;&#24405;&#31995;&#32479;&#23383;&#38169;&#35823;&#29575;&#35745;&#31639;&#30340;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
MeetEval: A Toolkit for Computation of Word Error Rates for Meeting Transcription Systems. (arXiv:2307.11394v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11394
&lt;/p&gt;
&lt;p&gt;
MeetEval&#26159;&#19968;&#20010;&#35745;&#31639;&#20250;&#35758;&#36716;&#24405;&#31995;&#32479;&#23383;&#38169;&#35823;&#29575;&#30340;&#24037;&#20855;&#21253;&#65292;&#23427;&#36890;&#36807;&#26102;&#38388;&#32422;&#26463;&#26469;&#25552;&#39640;&#21305;&#37197;&#36136;&#37327;&#24182;&#21152;&#36895;&#21305;&#37197;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
MeetEval&#26159;&#19968;&#20010;&#24320;&#28304;&#24037;&#20855;&#21253;&#65292;&#29992;&#20110;&#35780;&#20272;&#21508;&#31181;&#20250;&#35758;&#36716;&#24405;&#31995;&#32479;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#30028;&#38754;&#65292;&#29992;&#20110;&#35745;&#31639;&#24120;&#29992;&#30340;&#23383;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#65292;&#21253;&#25324;cpWER&#12289;ORC WER&#21644;MIMO WER&#31561;&#20854;&#20182;WER&#23450;&#20041;&#12290;&#25105;&#20204;&#36890;&#36807;&#26102;&#38388;&#32422;&#26463;&#25193;&#23637;&#20102;cpWER&#30340;&#35745;&#31639;&#65292;&#20197;&#30830;&#20445;&#21482;&#26377;&#22312;&#26102;&#38388;&#23545;&#40784;&#21512;&#29702;&#30340;&#24773;&#20917;&#19979;&#25165;&#23558;&#21333;&#35789;&#35782;&#21035;&#20026;&#27491;&#30830;&#12290;&#36825;&#26679;&#21487;&#20197;&#26356;&#22909;&#22320;&#21305;&#37197;&#20551;&#35774;&#23383;&#31526;&#20018;&#19982;&#21442;&#32771;&#23383;&#31526;&#20018;&#65292;&#26356;&#25509;&#36817;&#23454;&#38469;&#30340;&#36716;&#24405;&#36136;&#37327;&#65292;&#24182;&#19988;&#22914;&#26524;&#31995;&#32479;&#25552;&#20379;&#20102;&#19981;&#20934;&#30830;&#30340;&#26102;&#38388;&#26631;&#27880;&#65292;&#23558;&#23545;&#20854;&#36827;&#34892;&#24809;&#32602;&#12290;&#30001;&#20110;&#36890;&#24120;&#27809;&#26377;&#21333;&#35789;&#32423;&#21035;&#30340;&#26102;&#38388;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#20174;&#29255;&#27573;&#32423;&#21035;&#26102;&#38388;&#65288;&#20363;&#22914;&#19968;&#20010;&#21477;&#23376;&#65289;&#36817;&#20284;&#21040;&#30830;&#20999;&#30340;&#21333;&#35789;&#32423;&#26102;&#38388;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#36817;&#20284;&#26041;&#27861;&#19982;&#20855;&#26377;&#30830;&#20999;&#21333;&#35789;&#32423;&#21035;&#27880;&#37322;&#30340;&#21305;&#37197;&#23548;&#33268;&#31867;&#20284;&#30340;WER&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#26102;&#38388;&#32422;&#26463;&#36824;&#23548;&#33268;&#21305;&#37197;&#31639;&#27861;&#30340;&#21152;&#36895;&#65292;&#36825;&#36229;&#36807;&#20102;&#20542;&#21521;&#25340;&#20945;&#30340;&#26102;&#38388;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
MeetEval is an open-source toolkit to evaluate all kinds of meeting transcription systems. It provides a unified interface for the computation of commonly used Word Error Rates (WERs), specifically cpWER, ORC WER and MIMO WER along other WER definitions. We extend the cpWER computation by a temporal constraint to ensure that only words are identified as correct when the temporal alignment is plausible. This leads to a better quality of the matching of the hypothesis string to the reference string that more closely resembles the actual transcription quality, and a system is penalized if it provides poor time annotations. Since word-level timing information is often not available, we present a way to approximate exact word-level timings from segment-level timings (e.g., a sentence) and show that the approximation leads to a similar WER as a matching with exact word-level annotations. At the same time, the time constraint leads to a speedup of the matching algorithm, which outweighs the a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35843;&#26597;&#20998;&#26512;&#20102;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#38598;&#30340;&#21019;&#24314;&#36807;&#31243;&#20013;&#30340;&#36136;&#37327;&#31649;&#29702;&#23454;&#36341;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#24314;&#35758;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#27969;&#34892;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#36739;&#22810;&#30340;&#38169;&#35823;&#27880;&#37322;&#12289;&#20559;&#35265;&#25110;&#27880;&#37322;&#20266;&#20687;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#36129;&#29486;&#26159;&#22312;&#36825;&#19968;&#39046;&#22495;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#36341;&#25351;&#21335;&#12290;</title><link>http://arxiv.org/abs/2307.08153</link><description>&lt;p&gt;
&#22312;&#23454;&#36341;&#20013;&#20998;&#26512;&#25968;&#25454;&#38598;&#27880;&#37322;&#36136;&#37327;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Analyzing Dataset Annotation Quality Management in the Wild. (arXiv:2307.08153v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08153
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35843;&#26597;&#20998;&#26512;&#20102;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#38598;&#30340;&#21019;&#24314;&#36807;&#31243;&#20013;&#30340;&#36136;&#37327;&#31649;&#29702;&#23454;&#36341;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#24314;&#35758;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#27969;&#34892;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#36739;&#22810;&#30340;&#38169;&#35823;&#27880;&#37322;&#12289;&#20559;&#35265;&#25110;&#27880;&#37322;&#20266;&#20687;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#36129;&#29486;&#26159;&#22312;&#36825;&#19968;&#39046;&#22495;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#36341;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#36136;&#37327;&#23545;&#20110;&#35757;&#32451;&#20934;&#30830;&#12289;&#20844;&#27491;&#21644;&#21487;&#20449;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20197;&#21450;&#23427;&#20204;&#30340;&#27491;&#30830;&#35780;&#20272;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#27969;&#34892;&#25968;&#25454;&#38598;&#20013;&#20063;&#23384;&#22312;&#22823;&#37327;&#30340;&#38169;&#35823;&#27880;&#37322;&#12289;&#20559;&#35265;&#25110;&#27880;&#37322;&#20266;&#20687;&#12290;&#20851;&#20110;&#27880;&#37322;&#39033;&#30446;&#30340;&#26368;&#20339;&#23454;&#36341;&#21644;&#25351;&#21335;&#24050;&#32463;&#23384;&#22312;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36804;&#20170;&#20026;&#27490;&#36824;&#27809;&#26377;&#36827;&#34892;&#22823;&#35268;&#27169;&#20998;&#26512;&#65292;&#20197;&#30740;&#31350;&#21019;&#24314;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#38598;&#26102;&#23454;&#38469;&#36827;&#34892;&#30340;&#36136;&#37327;&#31649;&#29702;&#20197;&#21450;&#26159;&#21542;&#36981;&#24490;&#20102;&#36825;&#20123;&#24314;&#35758;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#35843;&#26597;&#24182;&#24635;&#32467;&#20102;&#25991;&#29486;&#20013;&#25551;&#36848;&#30340;&#25968;&#25454;&#38598;&#21019;&#24314;&#30340;&#25512;&#33616;&#36136;&#37327;&#31649;&#29702;&#23454;&#36341;&#65292;&#24182;&#25552;&#20379;&#20102;&#22914;&#20309;&#24212;&#29992;&#36825;&#20123;&#23454;&#36341;&#30340;&#24314;&#35758;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32534;&#21046;&#20102;&#19968;&#20010;&#30001;591&#31687;&#31185;&#23398;&#20986;&#29256;&#29289;&#32452;&#25104;&#30340;&#25991;&#26412;&#25968;&#25454;&#38598;&#35821;&#26009;&#24211;&#65292;&#24182;&#38024;&#23545;&#19982;&#36136;&#37327;&#30456;&#20851;&#30340;&#26041;&#38754;&#36827;&#34892;&#20102;&#27880;&#37322;&#65292;&#20363;&#22914;&#27880;&#37322;&#32773;&#31649;&#29702;&#12289;&#19968;&#33268;&#24615;&#12289;&#20210;&#35009;&#25110;&#25968;&#25454;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data quality is crucial for training accurate, unbiased, and trustworthy machine learning models and their correct evaluation. Recent works, however, have shown that even popular datasets used to train and evaluate state-of-the-art models contain a non-negligible amount of erroneous annotations, bias or annotation artifacts. There exist best practices and guidelines regarding annotation projects. But to the best of our knowledge, no large-scale analysis has been performed as of yet on how quality management is actually conducted when creating natural language datasets and whether these recommendations are followed. Therefore, we first survey and summarize recommended quality management practices for dataset creation as described in the literature and provide suggestions on how to apply them. Then, we compile a corpus of 591 scientific publications introducing text datasets and annotate it for quality-related aspects, such as annotator management, agreement, adjudication or data validat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#35757;&#32451;&#23494;&#38598;&#26816;&#32034;&#22120;&#26469;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35782;&#21035;&#39640;&#36136;&#37327;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#35757;&#32451;&#26399;&#38388;&#23545;&#26410;&#35265;&#36807;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.07164</link><description>&lt;p&gt;
&#23398;&#20064;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26816;&#32034;&#19978;&#19979;&#25991;&#31034;&#20363;
&lt;/p&gt;
&lt;p&gt;
Learning to Retrieve In-Context Examples for Large Language Models. (arXiv:2307.07164v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#35757;&#32451;&#23494;&#38598;&#26816;&#32034;&#22120;&#26469;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35782;&#21035;&#39640;&#36136;&#37327;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#35757;&#32451;&#26399;&#38388;&#23545;&#26410;&#35265;&#36807;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#26681;&#25454;&#23569;&#37327;&#30340;&#36755;&#20837;-&#36755;&#20986;&#31034;&#20363;&#25191;&#34892;&#21508;&#31181;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#25152;&#36873;&#31034;&#20363;&#30340;&#36136;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#35757;&#32451;&#23494;&#38598;&#26816;&#32034;&#22120;&#65292;&#21487;&#20197;&#20026;LLMs&#35782;&#21035;&#39640;&#36136;&#37327;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#39318;&#20808;&#35757;&#32451;&#22522;&#20110;LLM&#21453;&#39304;&#30340;&#22870;&#21169;&#27169;&#22411;&#26469;&#35780;&#20272;&#20505;&#36873;&#31034;&#20363;&#30340;&#36136;&#37327;&#65292;&#28982;&#21518;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#35757;&#32451;&#22522;&#20110;&#21452;&#32534;&#30721;&#22120;&#30340;&#23494;&#38598;&#26816;&#32034;&#22120;&#12290;&#25105;&#20204;&#22312;30&#20010;&#20219;&#21153;&#22871;&#20214;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#26174;&#33879;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#35757;&#32451;&#26399;&#38388;&#23545;&#26410;&#35265;&#36807;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28145;&#20837;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36890;&#36807;&#26816;&#32034;&#20855;&#26377;&#30456;&#20284;&#27169;&#24335;&#30340;&#31034;&#20363;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#32780;&#36825;&#31181;&#22686;&#30410;&#22312;&#19981;&#21516;&#35268;&#27169;&#30340;LLMs&#20013;&#26159;&#19968;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated their ability to learn in-context, allowing them to perform various tasks based on a few input-output examples. However, the effectiveness of in-context learning is heavily reliant on the quality of the selected examples. In this paper, we propose a novel framework to iteratively train dense retrievers that can identify high-quality in-context examples for LLMs. Our framework initially trains a reward model based on LLM feedback to evaluate the quality of candidate examples, followed by knowledge distillation to train a bi-encoder based dense retriever. Our experiments on a suite of 30 tasks demonstrate that our framework significantly enhances in-context learning performance. Furthermore, we show the generalization ability of our framework to unseen tasks during training. An in-depth analysis reveals that our model improves performance by retrieving examples with similar patterns, and the gains are consistent across LLMs of varying sizes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#21333;&#36755;&#20986;&#39640;&#26031;&#36807;&#31243;&#20013;&#20801;&#35768;&#21333;&#20010;&#36755;&#20837;&#20855;&#26377;&#22810;&#20010;&#36755;&#20986;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#20197;&#21033;&#29992;&#21487;&#29992;&#30340;&#36755;&#20986;&#19981;&#30830;&#23450;&#24615;&#20449;&#24687;&#12290;&#36890;&#36807;&#22312;speechocean762&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#33021;&#22815;&#35745;&#31639;&#20986;&#26356;&#25509;&#36817;&#22810;&#20010;&#20154;&#24037;&#35780;&#32423;&#22120;&#21442;&#32771;&#36755;&#20986;&#38598;&#30340;&#27979;&#35797;&#38598;&#36755;&#20986;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2306.02719</link><description>&lt;p&gt;
&#21333;&#36755;&#20986;&#39640;&#26031;&#36807;&#31243;&#20013;&#30340;&#21333;&#20010;&#36755;&#20837;&#22810;&#20010;&#36755;&#20986;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
Multiple output samples per input in a single-output Gaussian process. (arXiv:2306.02719v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#21333;&#36755;&#20986;&#39640;&#26031;&#36807;&#31243;&#20013;&#20801;&#35768;&#21333;&#20010;&#36755;&#20837;&#20855;&#26377;&#22810;&#20010;&#36755;&#20986;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#20197;&#21033;&#29992;&#21487;&#29992;&#30340;&#36755;&#20986;&#19981;&#30830;&#23450;&#24615;&#20449;&#24687;&#12290;&#36890;&#36807;&#22312;speechocean762&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#33021;&#22815;&#35745;&#31639;&#20986;&#26356;&#25509;&#36817;&#22810;&#20010;&#20154;&#24037;&#35780;&#32423;&#22120;&#21442;&#32771;&#36755;&#20986;&#38598;&#30340;&#27979;&#35797;&#38598;&#36755;&#20986;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#30340;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#21482;&#32771;&#34385;&#35757;&#32451;&#38598;&#20013;&#27599;&#20010;&#36755;&#20837;&#30340;&#21333;&#20010;&#36755;&#20986;&#26679;&#26412;&#12290;&#38024;&#23545;&#20027;&#35266;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#65292;&#20363;&#22914;&#21475;&#35821;&#35780;&#20272;&#65292;&#21487;&#20197;&#29992;&#22810;&#20010;&#20154;&#24037;&#35780;&#32423;&#22120;&#30340;&#36755;&#20986;&#26631;&#31614;&#23545;&#36755;&#20837;&#36827;&#34892;&#27880;&#37322;&#12290;&#26412;&#25991;&#25552;&#20986;&#23558;GP&#25512;&#24191;&#20026;&#20801;&#35768;&#22312;&#35757;&#32451;&#38598;&#20013;&#26377;&#36825;&#20123;&#22810;&#20010;&#36755;&#20986;&#26679;&#26412;&#65292;&#24182;&#19988;&#21033;&#29992;&#21487;&#29992;&#30340;&#36755;&#20986;&#19981;&#30830;&#23450;&#24615;&#20449;&#24687;&#12290;&#36825;&#19982;&#22810;&#36755;&#20986;GP&#19981;&#21516;&#65292;&#22240;&#20026;&#36825;&#37324;&#25152;&#26377;&#30340;&#36755;&#20986;&#26679;&#26412;&#37117;&#26469;&#33258;&#21516;&#19968;&#20219;&#21153;&#12290;&#36755;&#20986;&#23494;&#24230;&#20989;&#25968;&#34987;&#24418;&#24335;&#21270;&#20026;&#35266;&#23519;&#21040;&#25152;&#26377;&#36755;&#20986;&#26679;&#26412;&#30340;&#32852;&#21512;&#20284;&#28982;&#24230;&#37327;&#65292;&#20026;&#20102;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#65292;&#28508;&#22312;&#21464;&#37327;&#19981;&#20250;&#37325;&#22797;&#12290;&#27979;&#35797;&#38598;&#39044;&#27979;&#31867;&#20284;&#20110;&#26631;&#20934;GP&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#21807;&#19968;&#19981;&#21516;&#30340;&#26159;&#20248;&#21270;&#30340;&#36229;&#21442;&#25968;&#12290;&#36890;&#36807;&#22312;speechocean762&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20351;&#24471;GP&#33021;&#22815;&#35745;&#31639;&#20986;&#19982;&#22810;&#20010;&#20154;&#24037;&#35780;&#32423;&#22120;&#30340;&#21442;&#32771;&#36755;&#20986;&#38598;&#26356;&#30456;&#20284;&#30340;&#27979;&#35797;&#38598;&#36755;&#20986;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
The standard Gaussian Process (GP) only considers a single output sample per input in the training set. Datasets for subjective tasks, such as spoken language assessment, may be annotated with output labels from multiple human raters per input. This paper proposes to generalise the GP to allow for these multiple output samples in the training set, and thus make use of available output uncertainty information. This differs from a multi-output GP, as all output samples are from the same task here. The output density function is formulated to be the joint likelihood of observing all output samples, and latent variables are not repeated to reduce computation cost. The test set predictions are inferred similarly to a standard GP, with a difference being in the optimised hyper-parameters. This is evaluated on speechocean762, showing that it allows the GP to compute a test set output distribution that is more similar to the collection of reference outputs from the multiple human raters.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;ChatGPT&#22312;&#21477;&#23376;&#32423;&#21035;&#30340;&#26102;&#38388;&#12289;&#22240;&#26524;&#21644;&#35821;&#31687;&#20851;&#31995;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#20854;&#22312;&#26816;&#27979;&#21644;&#25512;&#29702;&#22240;&#26524;&#20851;&#31995;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#35782;&#21035;&#26102;&#38388;&#39034;&#24207;&#26041;&#38754;&#21487;&#33021;&#23384;&#22312;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.14827</link><description>&lt;p&gt;
ChatGPT&#22312;&#21477;&#23376;&#32423;&#20851;&#31995;&#19978;&#30340;&#35780;&#20272;&#65306;&#37325;&#28857;&#20851;&#27880;&#26102;&#38388;&#12289;&#22240;&#26524;&#21644;&#35821;&#31687;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
ChatGPT Evaluation on Sentence Level Relations: A Focus on Temporal, Causal, and Discourse Relations. (arXiv:2304.14827v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;ChatGPT&#22312;&#21477;&#23376;&#32423;&#21035;&#30340;&#26102;&#38388;&#12289;&#22240;&#26524;&#21644;&#35821;&#31687;&#20851;&#31995;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#20854;&#22312;&#26816;&#27979;&#21644;&#25512;&#29702;&#22240;&#26524;&#20851;&#31995;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#35782;&#21035;&#26102;&#38388;&#39034;&#24207;&#26041;&#38754;&#21487;&#33021;&#23384;&#22312;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#23450;&#37327;&#35780;&#20272;ChatGPT&#65292;&#22312;&#26102;&#38388;&#20851;&#31995;&#12289;&#22240;&#26524;&#20851;&#31995;&#21644;&#35821;&#31687;&#20851;&#31995;&#31561;&#21477;&#38388;&#20851;&#31995;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#32771;&#34385;&#21040;ChatGPT&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#25105;&#20204;&#22312;13&#20010;&#25968;&#25454;&#38598;&#30340;&#25972;&#20010;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#21253;&#25324;&#26102;&#38388;&#21644;&#22240;&#26524;&#20851;&#31995;&#12289;&#22522;&#20110;PDTB2.0&#21644;&#22522;&#20110;&#23545;&#35805;&#30340;&#35821;&#31687;&#20851;&#31995;&#65292;&#20197;&#21450;&#20851;&#20110;&#23545;&#35805;&#29702;&#35299;&#30340;&#19979;&#28216;&#24212;&#29992;&#12290;&#20026;&#20102;&#33719;&#24471;&#21487;&#38752;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19977;&#31181;&#38024;&#23545;&#27599;&#20010;&#20219;&#21153;&#30340;&#23450;&#21046;&#25552;&#31034;&#27169;&#26495;&#65292;&#21253;&#25324;&#38646;-shot&#25552;&#31034;&#27169;&#26495;&#12289;&#38646;-shot&#25552;&#31034;&#24037;&#31243;&#65288;PE&#65289;&#27169;&#26495;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#25552;&#31034;&#27169;&#26495;&#65292;&#20026;&#25152;&#26377;&#27969;&#34892;&#30340;&#21477;&#23545;&#20851;&#31995;&#20998;&#31867;&#20219;&#21153;&#24314;&#31435;&#20102;&#21021;&#22987;&#22522;&#20934;&#20998;&#25968;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;ChatGPT&#22312;&#26816;&#27979;&#21644;&#25512;&#29702;&#22240;&#26524;&#20851;&#31995;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#21487;&#33021;&#19981;&#25797;&#38271;&#35782;&#21035;&#21477;&#23376;&#38388;&#30340;&#26102;&#38388;&#39034;&#24207;&#12290;ICL&#26041;&#27861;&#29305;&#21035;&#36866;&#29992;&#20110;&#25552;&#39640;&#19968;&#20123;&#25968;&#25454;&#38598;&#19978;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20026;&#27169;&#22411;&#25913;&#36827;&#21644;&#26377;&#25928;&#25552;&#31034;&#27169;&#26495;&#30340;&#35774;&#35745;&#25552;&#20379;&#20102;&#19968;&#20123;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper aims to quantitatively evaluate the performance of ChatGPT, an interactive large language model, on inter-sentential relations such as temporal relations, causal relations, and discourse relations. Given ChatGPT's promising performance across various tasks, we conduct extensive evaluations on the whole test sets of 13 datasets, including temporal and causal relations, PDTB2.0-based and dialogue-based discourse relations, and downstream applications on discourse understanding. To achieve reliable results, we adopt three tailored prompt templates for each task, including the zero-shot prompt template, zero-shot prompt engineering (PE) template, and in-context learning (ICL) prompt template, to establish the initial baseline scores for all popular sentence-pair relation classification tasks for the first time. We find that ChatGPT exhibits strong performance in detecting and reasoning about causal relations, while it may not be proficient in identifying the temporal order betwe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#23383;&#31526;&#32423;&#21035;&#21644;&#23376;&#35789;&#32423;&#21035;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#26426;&#22120;&#32763;&#35793;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#32467;&#26524;&#34920;&#26126;&#23383;&#31526;&#32423;&#21035;&#24314;&#27169;&#22312;&#24418;&#20284;&#21333;&#35789;&#21644;&#31232;&#26377;&#21333;&#35789;&#30340;&#32763;&#35793;&#26041;&#38754;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#22312;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#23588;&#20026;&#26126;&#26174;&#12290;</title><link>http://arxiv.org/abs/2302.14220</link><description>&lt;p&gt;
&#23383;&#31526;&#32423;&#21035;&#30340;&#32763;&#35793;&#26159;&#21542;&#20540;&#24471;&#31561;&#24453;&#65311;&#23558;&#23383;&#31526;&#32423;&#21035;&#21644;&#23376;&#35789;&#32423;&#21035;&#27169;&#22411;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Are Character-level Translations Worth the Wait? Comparing Character- and Subword-level Models for Machine Translation. (arXiv:2302.14220v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#23383;&#31526;&#32423;&#21035;&#21644;&#23376;&#35789;&#32423;&#21035;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#26426;&#22120;&#32763;&#35793;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#32467;&#26524;&#34920;&#26126;&#23383;&#31526;&#32423;&#21035;&#24314;&#27169;&#22312;&#24418;&#20284;&#21333;&#35789;&#21644;&#31232;&#26377;&#21333;&#35789;&#30340;&#32763;&#35793;&#26041;&#38754;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#22312;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#23588;&#20026;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#22810;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;&#39044;&#20808;&#35757;&#32451;&#30340;&#23383;&#31526;&#32423;&#21035;&#35821;&#35328;&#27169;&#22411;&#19982;&#27969;&#34892;&#30340;&#23376;&#35789;&#27169;&#22411;&#20855;&#26377;&#30456;&#24403;&#30340;&#31454;&#20105;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#26041;&#38754;&#65292;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#40092;&#26377;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#22312;&#22810;&#31181;&#35821;&#35328;&#21644;&#23454;&#39564;&#26465;&#20214;&#19979;&#65292;&#27604;&#36739;&#20102;&#26368;&#20808;&#36827;&#30340;&#23383;&#31526;&#32423;&#21035;&#21644;&#23376;&#35789;&#32423;&#21035;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#20998;&#21035;&#20026;ByT5&#21644;mT5&#65289;&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#25928;&#26524;&#65292;&#32467;&#26524;&#34920;&#26126;&#23383;&#31526;&#32423;&#21035;&#24314;&#27169;&#22312;&#32763;&#35793;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#23383;&#31526;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#21319;&#22312;&#20110;&#26356;&#22909;&#22320;&#32763;&#35793;&#20102;&#24418;&#20284;&#21333;&#35789;&#21644;&#31232;&#26377;&#21333;&#35789;&#12290;&#22312;&#35780;&#20272;&#28304;&#25991;&#26412;&#22312;&#39537;&#21160;&#27169;&#22411;&#39044;&#27979;&#20013;&#30340;&#37325;&#35201;&#24615;&#26102;&#65292;&#25105;&#20204;&#31361;&#20986;ByT5&#21333;&#35789;&#32423;&#21035;&#27169;&#24335;&#34920;&#26126;&#23383;&#31526;&#32423;&#21035;&#24314;&#27169;&#30340;&#28508;&#22312;&#24369;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained character-level language models were recently shown to be competitive with popular subword models across a range of NLP tasks. However, there has been little research on their effectiveness for neural machine translation (NMT). This work performs an extensive comparison across multiple languages and experimental conditions of state-of-the-art character- and subword-level pre-trained models (ByT5 and mT5, respectively) on NMT, showing the effectiveness of character-level modeling in translation, particularly in cases where training data is limited. In our analysis, we show how character models' performance gains are reflected in better translations of orthographically similar words and rare words. While evaluating the importance of source texts in driving model predictions, we highlight ByT5 word-level patterns suggesting an ability to modulate word and character-level information during the translation, providing insights into a potential weakness of character-level modeling
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#38750;&#24130;&#31561;&#21322;&#29615;&#19978;&#30340;&#24102;&#26435;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#36827;&#34892;&#26368;&#30701;&#23383;&#31526;&#20018;&#35299;&#30721;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#31561;&#20215;&#30830;&#23450;&#24615;&#33258;&#21160;&#26426;&#30340;&#21453;&#21521;&#26368;&#30701;&#36317;&#31163;&#20316;&#20026;&#21551;&#21457;&#24335;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25214;&#21040;&#26368;&#30701;&#23383;&#31526;&#20018;&#12290;</title><link>http://arxiv.org/abs/2204.07236</link><description>&lt;p&gt;
&#38750;&#24130;&#31561;&#21322;&#29615;&#20013;&#30340;A*&#26368;&#30701;&#23383;&#31526;&#20018;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
A* shortest string decoding for non-idempotent semirings. (arXiv:2204.07236v2 [cs.FL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.07236
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#38750;&#24130;&#31561;&#21322;&#29615;&#19978;&#30340;&#24102;&#26435;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#36827;&#34892;&#26368;&#30701;&#23383;&#31526;&#20018;&#35299;&#30721;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#31561;&#20215;&#30830;&#23450;&#24615;&#33258;&#21160;&#26426;&#30340;&#21453;&#21521;&#26368;&#30701;&#36317;&#31163;&#20316;&#20026;&#21551;&#21457;&#24335;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25214;&#21040;&#26368;&#30701;&#23383;&#31526;&#20018;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#38750;&#24130;&#31561;&#21322;&#29615;&#19978;&#30340;&#24102;&#26435;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#65292;&#21333;&#19968;&#26368;&#30701;&#36335;&#24452;&#31639;&#27861;&#26159;&#26410;&#23450;&#20041;&#30340;&#65292;&#22240;&#20026;&#36825;&#26679;&#30340;&#21322;&#29615;&#19981;&#33021;&#20445;&#35777;&#26368;&#30701;&#36335;&#24452;&#30340;&#23384;&#22312;&#12290;&#28982;&#32780;&#65292;&#22312;&#28385;&#36275;&#21333;&#35843;&#24615;&#26465;&#20214;&#30340;&#38750;&#24130;&#31561;&#21322;&#29615;&#20013;&#65288;&#22914;&#21152;&#20056;&#25110;&#23545;&#25968;&#21322;&#29615;&#65289;&#65292;&#26368;&#30701;&#23383;&#31526;&#20018;&#30340;&#27010;&#24565;&#26159;&#26126;&#30830;&#23450;&#20041;&#30340;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#31561;&#20215;&#30830;&#23450;&#24615;&#33258;&#21160;&#26426;&#65288;DFA&#65289;&#30340;&#21453;&#21521;&#26368;&#30701;&#36317;&#31163;&#20316;&#20026;A*&#25628;&#32034;&#30340;&#21551;&#21457;&#24335;&#65292;&#36890;&#36807;&#22312;&#19968;&#20010;&#20276;&#38543;&#24130;&#31561;&#21322;&#29615;&#19978;&#25191;&#34892;&#25628;&#32034;&#65292;&#35777;&#26126;&#21487;&#20197;&#36820;&#22238;&#26368;&#30701;&#23383;&#31526;&#20018;&#65292;&#20174;&#32780;&#25214;&#21040;&#24102;&#26435;&#38750;&#30830;&#23450;&#24615;&#33258;&#21160;&#26426;&#19978;&#30340;&#26368;&#30701;&#23383;&#31526;&#20018;&#12290;&#34429;&#28982;DFA&#21487;&#33021;&#23384;&#22312;&#25351;&#25968;&#32423;&#26356;&#22810;&#30340;&#29366;&#24577;&#65292;&#20294;&#22914;&#26524;&#22312;&#36816;&#34892;&#20013;&#36827;&#34892;&#30830;&#23450;&#21270;&#65292;&#35813;&#31639;&#27861;&#21482;&#38656;&#35201;&#35775;&#38382;&#20854;&#20013;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
The single shortest path algorithm is undefined for weighted finite-state automata over non-idempotent semirings because such semirings do not guarantee the existence of a shortest path. However, in non-idempotent semirings admitting an order satisfying a monotonicity condition (such as the plus-times or log semirings), the notion of shortest string is well-defined. We describe an algorithm which finds the shortest string for a weighted non-deterministic automaton over such semirings using the backwards shortest distance of an equivalent deterministic automaton (DFA) as a heuristic for A* search performed over a companion idempotent semiring, which is proven to return the shortest string. While there may be exponentially more states in the DFA, this algorithm needs to visit only a small fraction of them if determinization is performed "on the fly".
&lt;/p&gt;</description></item></channel></rss>