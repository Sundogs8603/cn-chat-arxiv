<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#36136;&#37327;&#20272;&#35745;&#24230;&#37327;&#26469;&#22686;&#24378;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;(XAI)&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;NoRefER&#24230;&#37327;&#22312;&#35782;&#21035;&#21333;&#35789;&#38169;&#35823;&#21644;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#27169;&#22411;&#34892;&#20026;&#35265;&#35299;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;NoRefER&#22312;&#35821;&#26009;&#24211;&#26500;&#24314;&#21644;&#21518;&#26399;&#32534;&#36753;&#24037;&#20316;&#27969;&#31243;&#20013;&#30340;&#23454;&#29992;&#24615;&#65292;&#34920;&#26126;&#20854;&#26377;&#28508;&#21147;&#25104;&#20026;&#25552;&#39640;ASR&#31995;&#32479;&#25928;&#29575;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#20851;&#38190;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2401.11268</link><description>&lt;p&gt;
&#21333;&#35789;&#32423;&#21035;&#30340;ASR&#36136;&#37327;&#35780;&#20272;&#29992;&#20110;&#36890;&#36807;&#20998;&#26512;&#21442;&#32771;&#26080;&#20851;&#30340;&#25351;&#26631;&#30340;&#27880;&#24847;&#21147;&#36827;&#34892;&#39640;&#25928;&#35821;&#26009;&#24211;&#37319;&#26679;&#21644;&#21518;&#26399;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Word-Level ASR Quality Estimation for Efficient Corpus Sampling and Post-Editing through Analyzing Attentions of a Reference-Free Metric. (arXiv:2401.11268v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11268
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#36136;&#37327;&#20272;&#35745;&#24230;&#37327;&#26469;&#22686;&#24378;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;(XAI)&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;NoRefER&#24230;&#37327;&#22312;&#35782;&#21035;&#21333;&#35789;&#38169;&#35823;&#21644;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#27169;&#22411;&#34892;&#20026;&#35265;&#35299;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;NoRefER&#22312;&#35821;&#26009;&#24211;&#26500;&#24314;&#21644;&#21518;&#26399;&#32534;&#36753;&#24037;&#20316;&#27969;&#31243;&#20013;&#30340;&#23454;&#29992;&#24615;&#65292;&#34920;&#26126;&#20854;&#26377;&#28508;&#21147;&#25104;&#20026;&#25552;&#39640;ASR&#31995;&#32479;&#25928;&#29575;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#20851;&#38190;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#39046;&#22495;&#20013;&#65292;&#19981;&#20165;&#35201;&#26377;&#39640;&#20934;&#30830;&#24615;&#30340;&#27169;&#22411;&#65292;&#36824;&#35201;&#25552;&#20379;&#20915;&#31574;&#36807;&#31243;&#30340;&#21487;&#35299;&#37322;&#24615;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#24341;&#20837;&#21644;&#35780;&#20272;&#20102;&#36136;&#37327;&#20272;&#35745;&#65288;QE&#65289;&#24230;&#37327;&#20316;&#20026;&#22686;&#24378;ASR&#31995;&#32479;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#26032;&#24037;&#20855;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;NoRefER&#65288;&#26080;&#21442;&#32771;&#38169;&#35823;&#29575;&#65289;&#24230;&#37327;&#22312;&#35782;&#21035;&#21333;&#35789;&#32423;&#38169;&#35823;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#20197;&#24110;&#21161;&#21518;&#26399;&#32534;&#36753;&#32773;&#25913;&#36827;ASR&#20551;&#35774;&#12290;&#30740;&#31350;&#36824;&#25193;&#23637;&#21040;NoRefER&#22312;&#35821;&#26009;&#24211;&#26500;&#24314;&#36807;&#31243;&#20013;&#30340;&#23454;&#29992;&#24615;&#65292;&#23637;&#31034;&#20102;&#23427;&#22312;&#22686;&#24378;&#20855;&#26377;&#26377;&#35265;&#22320;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#23545;NoRefER&#30340;&#35786;&#26029;&#29305;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#20854;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#27169;&#22411;&#34892;&#20026;&#21644;&#20915;&#31574;&#27169;&#24335;&#35265;&#35299;&#30340;&#33021;&#21147;&#12290;&#36825;&#23545;&#20110;&#22312;&#21518;&#26399;&#32534;&#36753;&#24037;&#20316;&#27969;&#31243;&#21644;&#24494;&#35843;ASR&#27169;&#22411;&#20013;&#20248;&#20808;&#32771;&#34385;&#20551;&#35774;&#26159;&#26377;&#30410;&#30340;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;NoRefER&#20855;&#26377;&#28508;&#21147;&#25104;&#20026;&#25552;&#39640;ASR&#31995;&#32479;&#21487;&#35299;&#37322;&#24615;&#21644;&#25928;&#29575;&#30340;&#20851;&#38190;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of automatic speech recognition (ASR), the quest for models that not only perform with high accuracy but also offer transparency in their decision-making processes is crucial. The potential of quality estimation (QE) metrics is introduced and evaluated as a novel tool to enhance explainable artificial intelligence (XAI) in ASR systems. Through experiments and analyses, the capabilities of the NoRefER (No Reference Error Rate) metric are explored in identifying word-level errors to aid post-editors in refining ASR hypotheses. The investigation also extends to the utility of NoRefER in the corpus-building process, demonstrating its effectiveness in augmenting datasets with insightful annotations. The diagnostic aspects of NoRefER are examined, revealing its ability to provide valuable insights into model behaviors and decision patterns. This has proven beneficial for prioritizing hypotheses in post-editing workflows and fine-tuning ASR models. The findings suggest that NoRef
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#35789;&#34955;&#39044;&#27979;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#23494;&#38598;&#36890;&#34892;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#26367;&#25442;&#35299;&#30721;&#22120;&#23454;&#29616;&#20102;&#39640;&#25928;&#21387;&#32553;&#35789;&#27719;&#20449;&#21495;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#36755;&#20837;&#20196;&#29260;&#30340;&#26465;&#27454;&#35206;&#30422;&#12290;</title><link>http://arxiv.org/abs/2401.11248</link><description>&lt;p&gt;
&#25918;&#24323;&#35299;&#30721;&#22120;&#65306;&#20351;&#29992;&#35789;&#34955;&#39044;&#27979;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#23494;&#38598;&#36890;&#34892;&#26816;&#32034;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Drop your Decoder: Pre-training with Bag-of-Word Prediction for Dense Passage Retrieval. (arXiv:2401.11248v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11248
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#35789;&#34955;&#39044;&#27979;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#23494;&#38598;&#36890;&#34892;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#26367;&#25442;&#35299;&#30721;&#22120;&#23454;&#29616;&#20102;&#39640;&#25928;&#21387;&#32553;&#35789;&#27719;&#20449;&#21495;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#36755;&#20837;&#20196;&#29260;&#30340;&#26465;&#27454;&#35206;&#30422;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#39044;&#35757;&#32451;&#24050;&#25104;&#20026;&#21021;&#22987;&#21270;&#21644;&#22686;&#24378;&#23494;&#38598;&#26816;&#32034;&#31995;&#32479;&#30340;&#27969;&#34892;&#25216;&#26415;&#12290;&#23427;&#36890;&#24120;&#21033;&#29992;&#39069;&#22806;&#30340;Transformer&#35299;&#30721;&#22359;&#25552;&#20379;&#21487;&#25345;&#32493;&#30340;&#30417;&#30563;&#20449;&#21495;&#65292;&#24182;&#23558;&#19978;&#19979;&#25991;&#20449;&#24687;&#21387;&#32553;&#21040;&#23494;&#38598;&#34920;&#31034;&#20013;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#39044;&#35757;&#32451;&#25216;&#26415;&#26377;&#25928;&#24615;&#30340;&#21407;&#22240;&#23578;&#19981;&#28165;&#26970;&#12290;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#39069;&#22806;&#35299;&#30721;&#22120;&#20063;&#20250;&#20135;&#29983;&#26174;&#33879;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25581;&#31034;&#22686;&#24378;&#35299;&#30721;&#30340;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65288;MAE&#65289;&#39044;&#35757;&#32451;&#30456;&#23545;&#20110;&#26222;&#36890;BERT&#26816;&#26597;&#28857;&#22312;&#36755;&#20837;&#20196;&#29260;&#30340;&#26465;&#27454;&#35206;&#30422;&#19978;&#30340;&#26174;&#33879;&#25913;&#36827;&#65292;&#20197;&#35299;&#37322;&#36825;&#20010;&#38382;&#39064;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#20256;&#32479;MAE&#30340;&#20462;&#25913;&#65292;&#23558;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#30340;&#35299;&#30721;&#22120;&#26367;&#25442;&#20026;&#23436;&#20840;&#31616;&#21270;&#30340;&#35789;&#34955;&#39044;&#27979;&#20219;&#21153;&#12290;&#36825;&#31181;&#20462;&#25913;&#20351;&#24471;&#35789;&#27719;&#20449;&#21495;&#33021;&#22815;&#39640;&#25928;&#22320;&#21387;&#32553;&#21040;&#23494;&#38598;&#34920;&#31034;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked auto-encoder pre-training has emerged as a prevalent technique for initializing and enhancing dense retrieval systems. It generally utilizes additional Transformer decoder blocks to provide sustainable supervision signals and compress contextual information into dense representations. However, the underlying reasons for the effectiveness of such a pre-training technique remain unclear. The usage of additional Transformer-based decoders also incurs significant computational costs. In this study, we aim to shed light on this issue by revealing that masked auto-encoder (MAE) pre-training with enhanced decoding significantly improves the term coverage of input tokens in dense representations, compared to vanilla BERT checkpoints. Building upon this observation, we propose a modification to the traditional MAE by replacing the decoder of a masked auto-encoder with a completely simplified Bag-of-Word prediction task. This modification enables the efficient compression of lexical signa
&lt;/p&gt;</description></item><item><title>Prompt-RAG&#26159;&#19968;&#31181;&#22312;&#23567;&#20247;&#39046;&#22495;&#20013;&#22686;&#24378;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#26032;&#26041;&#27861;&#65292;&#19982;&#20256;&#32479;&#30340;RAG&#27169;&#22411;&#19981;&#21516;&#65292;&#23427;&#19981;&#38656;&#35201;&#20351;&#29992;&#23884;&#20837;&#21521;&#37327;&#12290;&#36890;&#36807;&#38382;&#31572;&#26426;&#22120;&#20154;&#24212;&#29992;&#31243;&#24207;&#30340;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;Prompt-RAG&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#65292;&#21253;&#25324;ChatGPT&#12290;</title><link>http://arxiv.org/abs/2401.11246</link><description>&lt;p&gt;
Prompt-RAG: &#22312;&#23567;&#20247;&#39046;&#22495;&#20013;&#30340;&#22522;&#20110;&#21521;&#37327;&#23884;&#20837;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#24320;&#21019;&#24615;&#30740;&#31350;&#65292;&#20197;&#38889;&#21307;&#23398;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Prompt-RAG: Pioneering Vector Embedding-Free Retrieval-Augmented Generation in Niche Domains, Exemplified by Korean Medicine. (arXiv:2401.11246v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11246
&lt;/p&gt;
&lt;p&gt;
Prompt-RAG&#26159;&#19968;&#31181;&#22312;&#23567;&#20247;&#39046;&#22495;&#20013;&#22686;&#24378;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#26032;&#26041;&#27861;&#65292;&#19982;&#20256;&#32479;&#30340;RAG&#27169;&#22411;&#19981;&#21516;&#65292;&#23427;&#19981;&#38656;&#35201;&#20351;&#29992;&#23884;&#20837;&#21521;&#37327;&#12290;&#36890;&#36807;&#38382;&#31572;&#26426;&#22120;&#20154;&#24212;&#29992;&#31243;&#24207;&#30340;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;Prompt-RAG&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#65292;&#21253;&#25324;ChatGPT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;Prompt-RAG&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#23567;&#20247;&#39046;&#22495;&#20013;&#30340;&#24615;&#33021;&#12290;&#20256;&#32479;&#30340;RAG&#26041;&#27861;&#22823;&#22810;&#38656;&#35201;&#21521;&#37327;&#23884;&#20837;&#65292;&#28982;&#32780;&#36890;&#29992;&#30340;LLM&#22522;&#20110;&#23884;&#20837;&#34920;&#31034;&#23545;&#20110;&#19987;&#19994;&#39046;&#22495;&#30340;&#36866;&#29992;&#24615;&#20173;&#28982;&#19981;&#30830;&#23450;&#12290;&#20026;&#20102;&#25506;&#32034;&#21644;&#20030;&#20363;&#35828;&#26126;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#38889;&#21307;&#23398;&#65288;KM&#65289;&#21644;&#20256;&#32479;&#21307;&#23398;&#65288;CM&#65289;&#25991;&#26723;&#30340;&#21521;&#37327;&#23884;&#20837;&#65292;&#21457;&#29616;KM&#25991;&#26723;&#30340;&#23884;&#20837;&#19982;&#26631;&#35760;&#37325;&#21472;&#30456;&#20851;&#24615;&#26356;&#24378;&#65292;&#19982;&#20154;&#24037;&#35780;&#20272;&#30340;&#25991;&#26723;&#30456;&#20851;&#24615;&#36739;&#23567;&#65292;&#32780;CM&#25991;&#26723;&#21017;&#30456;&#21453;&#12290;Prompt-RAG&#19982;&#20256;&#32479;&#30340;RAG&#27169;&#22411;&#19981;&#21516;&#65292;&#23427;&#19981;&#38656;&#35201;&#23884;&#20837;&#21521;&#37327;&#12290;&#36890;&#36807;&#38382;&#31572;&#26426;&#22120;&#20154;&#24212;&#29992;&#31243;&#24207;&#23545;&#20854;&#24615;&#33021;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20854;&#20013;&#22238;&#31572;&#30340;&#30456;&#20851;&#24615;&#12289;&#21487;&#35835;&#24615;&#21644;&#20449;&#24687;&#24615;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;Prompt-RAG&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#65292;&#21253;&#25324;ChatGPT&#21644;...
&lt;/p&gt;
&lt;p&gt;
We propose a natural language prompt-based retrieval augmented generation (Prompt-RAG), a novel approach to enhance the performance of generative large language models (LLMs) in niche domains. Conventional RAG methods mostly require vector embeddings, yet the suitability of generic LLM-based embedding representations for specialized domains remains uncertain. To explore and exemplify this point, we compared vector embeddings from Korean Medicine (KM) and Conventional Medicine (CM) documents, finding that KM document embeddings correlated more with token overlaps and less with human-assessed document relatedness, in contrast to CM embeddings. Prompt-RAG, distinct from conventional RAG models, operates without the need for embedding vectors. Its performance was assessed through a Question-Answering (QA) chatbot application, where responses were evaluated for relevance, readability, and informativeness. The results showed that Prompt-RAG outperformed existing models, including ChatGPT and
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#20381;&#23384;&#35299;&#26512;&#27169;&#22411;&#65292;&#21033;&#29992;&#20462;&#36766;&#20851;&#31995;&#21644;&#20462;&#36766;&#32467;&#26500;&#35757;&#32451;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36328;&#19981;&#21516;&#20462;&#36766;&#32467;&#26500;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#35770;&#35777;&#20998;&#26512;&#12290;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19981;&#21516;&#20462;&#36766;&#32467;&#26500;&#19979;&#36827;&#34892;&#30340;&#35770;&#35777;&#35299;&#26512;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2401.11218</link><description>&lt;p&gt;
&#36328;&#22810;&#31181;&#20462;&#36766;&#32467;&#26500;&#30340;&#31471;&#21040;&#31471;&#35770;&#35777;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
End-to-End Argument Mining over Varying Rhetorical Structures. (arXiv:2401.11218v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#20381;&#23384;&#35299;&#26512;&#27169;&#22411;&#65292;&#21033;&#29992;&#20462;&#36766;&#20851;&#31995;&#21644;&#20462;&#36766;&#32467;&#26500;&#35757;&#32451;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36328;&#19981;&#21516;&#20462;&#36766;&#32467;&#26500;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#35770;&#35777;&#20998;&#26512;&#12290;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19981;&#21516;&#20462;&#36766;&#32467;&#26500;&#19979;&#36827;&#34892;&#30340;&#35770;&#35777;&#35299;&#26512;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20462;&#36766;&#32467;&#26500;&#29702;&#35770;&#26263;&#31034;&#20102;&#25991;&#26412;&#30340;&#21333;&#19968;&#35805;&#35821;&#35299;&#37322;&#19981;&#23384;&#22312;&#65292;&#24182;&#19988;RST&#35299;&#26512;&#22120;&#30340;&#23616;&#38480;&#24615;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#31867;&#20284;&#32467;&#26500;&#30340;&#19981;&#19968;&#33268;&#35299;&#26512;&#12290;&#22240;&#27492;&#65292;&#37325;&#35201;&#30340;&#26159;&#35201;&#32771;&#34385;&#21040;&#22312;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#25991;&#26412;&#20013;&#21487;&#20197;&#25214;&#21040;&#30456;&#21516;&#30340;&#35770;&#35777;&#32467;&#26500;&#20294;&#20462;&#36766;&#32467;&#26500;&#21508;&#24322;&#12290;&#26412;&#30740;&#31350;&#20174;&#20462;&#36766;&#35282;&#24230;&#35780;&#20272;&#20102;&#21516;&#19968;&#35770;&#35777;&#26041;&#26696;&#20013;&#30340;&#37322;&#20041;&#24046;&#24322;&#12290;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#23618;&#20381;&#23384;&#35299;&#26512;&#27169;&#22411;&#26469;&#35780;&#20272;&#20462;&#36766;&#21644;&#35770;&#35777;&#32467;&#26500;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#20462;&#36766;&#20851;&#31995;&#65292;&#20462;&#36766;&#32467;&#26500;&#30340;&#37322;&#20041;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#22686;&#24378;&#12290;&#35813;&#26041;&#27861;&#20801;&#35768;&#20351;&#29992;&#20462;&#36766;&#26641;&#32780;&#19981;&#26159;&#35789;&#24207;&#21015;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#35770;&#35777;&#20998;&#26512;&#12290;&#23427;&#22312;&#21452;&#35821;&#24494;&#25991;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#25253;&#21578;&#20102;&#35813;&#35821;&#26009;&#24211;&#20420;&#25991;&#29256;&#26412;&#30340;&#23436;&#25972;&#35770;&#35777;&#35299;&#26512;&#30340;&#21021;&#27493;&#32467;&#26524;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35770;&#35777;&#32467;&#26500;&#21644;&#20462;&#36766;&#32467;&#26500;&#20043;&#38388;&#30340;&#36830;&#25509;&#21487;&#20197;&#26377;&#25928;&#22320;&#22312;&#19981;&#21516;&#20462;&#36766;&#32467;&#26500;&#19979;&#36827;&#34892;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rhetorical Structure Theory implies no single discourse interpretation of a text, and the limitations of RST parsers further exacerbate inconsistent parsing of similar structures. Therefore, it is important to take into account that the same argumentative structure can be found in semantically similar texts with varying rhetorical structures. In this work, the differences between paraphrases within the same argument scheme are evaluated from a rhetorical perspective. The study proposes a deep dependency parsing model to assess the connection between rhetorical and argument structures. The model utilizes rhetorical relations; RST structures of paraphrases serve as training data augmentations. The method allows for end-to-end argumentation analysis using a rhetorical tree instead of a word sequence. It is evaluated on the bilingual Microtexts corpus, and the first results on fully-fledged argument parsing for the Russian version of the corpus are reported. The results suggest that argume
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#23450;&#21046;&#30340;BERT&#19982;SVC&#30340;&#38598;&#25104;&#65292;&#38024;&#23545;ToS&#25991;&#26723;&#20013;&#30340;&#19981;&#20844;&#24179;&#26465;&#27454;&#36827;&#34892;&#20102;SOTA&#32423;&#21035;&#30340;&#26816;&#27979;&#65292;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.11207</link><description>&lt;p&gt;
&#19981;&#20844;&#24179;&#30340;&#26381;&#21153;&#26465;&#27454;&#65306;&#20351;&#29992;&#23450;&#21046;&#30340;BERT&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unfair TOS: An Automated Approach using Customized BERT. (arXiv:2401.11207v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#23450;&#21046;&#30340;BERT&#19982;SVC&#30340;&#38598;&#25104;&#65292;&#38024;&#23545;ToS&#25991;&#26723;&#20013;&#30340;&#19981;&#20844;&#24179;&#26465;&#27454;&#36827;&#34892;&#20102;SOTA&#32423;&#21035;&#30340;&#26816;&#27979;&#65292;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26381;&#21153;&#26465;&#27454;(Terms of Service&#65292;ToS)&#26159;&#20219;&#20309;&#21327;&#35758;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#23427;&#23450;&#20041;&#20102;&#26381;&#21153;&#25552;&#20379;&#21830;&#21644;&#26368;&#32456;&#29992;&#25143;&#20043;&#38388;&#30340;&#27861;&#24459;&#20851;&#31995;&#12290;&#23427;&#20204;&#19981;&#20165;&#30830;&#23450;&#21644;&#30028;&#23450;&#20102;&#30456;&#20114;&#30340;&#26435;&#21033;&#21644;&#36131;&#20219;&#65292;&#36824;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#19982;&#20351;&#29992;&#25968;&#23383;&#31354;&#38388;&#26377;&#20851;&#30340;&#21512;&#21516;&#37325;&#35201;&#26041;&#38754;&#30340;&#20449;&#24687;&#12290;&#36825;&#20123;&#26041;&#38754;&#21253;&#25324;&#36131;&#20219;&#38480;&#21046;&#12289;&#25968;&#25454;&#20445;&#25252;&#31561;&#21508;&#31181;&#20027;&#39064;&#12290;&#29992;&#25143;&#20542;&#21521;&#20110;&#22312;&#20351;&#29992;&#20219;&#20309;&#24212;&#29992;&#31243;&#24207;&#25110;&#26381;&#21153;&#20043;&#21069;&#25509;&#21463;ToS&#32780;&#19981;&#36827;&#34892;&#38405;&#35835;&#12290;&#36825;&#31181;&#26080;&#30693;&#21487;&#33021;&#20351;&#20182;&#20204;&#22312;&#38656;&#35201;&#37319;&#21462;&#20219;&#20309;&#34892;&#21160;&#26102;&#22788;&#20110;&#36739;&#24369;&#30340;&#29366;&#20917;&#12290;&#29616;&#26377;&#30340;&#26816;&#27979;&#25110;&#20998;&#31867;&#19981;&#20844;&#24179;&#26465;&#27454;&#30340;&#26041;&#27861;&#24050;&#32463;&#36807;&#26102;&#19988;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#36825;&#31687;&#30740;&#31350;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;Fine-tuning BERT&#19982;SVC&#65288;&#25903;&#25345;&#21521;&#37327;&#20998;&#31867;&#22120;&#65289;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#20851;&#20110;ToS&#25991;&#26723;&#20013;&#19981;&#20844;&#24179;&#26465;&#27454;&#26816;&#27979;&#30340;SOTA&#65288;&#26368;&#26032;&#25216;&#26415;&#65289;&#32467;&#26524;&#12290;&#30740;&#31350;&#34920;&#26126;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Terms of Service (ToS) form an integral part of any agreement as it defines the legal relationship between a service provider and an end-user. Not only do they establish and delineate reciprocal rights and responsibilities, but they also provide users with information on essential aspects of contracts that pertain to the use of digital spaces. These aspects include a wide range of topics, including limitation of liability, data protection, etc. Users tend to accept the ToS without going through it before using any application or service. Such ignorance puts them in a potentially weaker situation in case any action is required. Existing methodologies for the detection or classification of unfair clauses are however obsolete and show modest performance. In this research paper, we present SOTA(State of The Art) results on unfair clause detection from ToS documents based on unprecedented Fine-tuning BERT in integration with SVC(Support Vector Classifier). The study shows proficient perform
&lt;/p&gt;</description></item><item><title>InferAligner&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#25512;&#29702;&#26102;&#38388;&#23545;&#40784;&#26041;&#27861;&#65292;&#21033;&#29992;&#36328;&#27169;&#22411;&#24341;&#23548;&#23454;&#29616;&#26080;&#23475;&#21270;&#23545;&#40784;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#24212;&#29992;&#20110;&#39046;&#22495;&#29305;&#23450;&#27169;&#22411;&#20013;&#12290;</title><link>http://arxiv.org/abs/2401.11206</link><description>&lt;p&gt;
InferAligner: &#21033;&#29992;&#36328;&#27169;&#22411;&#24341;&#23548;&#36827;&#34892;&#26080;&#23475;&#21270;&#25512;&#29702;&#26102;&#38388;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
InferAligner: Inference-Time Alignment for Harmlessness through Cross-Model Guidance. (arXiv:2401.11206v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11206
&lt;/p&gt;
&lt;p&gt;
InferAligner&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#25512;&#29702;&#26102;&#38388;&#23545;&#40784;&#26041;&#27861;&#65292;&#21033;&#29992;&#36328;&#27169;&#22411;&#24341;&#23548;&#23454;&#29616;&#26080;&#23475;&#21270;&#23545;&#40784;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#24212;&#29992;&#20110;&#39046;&#22495;&#29305;&#23450;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#23427;&#20204;&#19981;&#20165;&#34987;&#29992;&#20316;&#36890;&#29992;AI&#21161;&#25163;&#65292;&#36824;&#36890;&#36807;&#36827;&#19968;&#27493;&#30340;&#24494;&#35843;&#23450;&#21046;&#20197;&#28385;&#36275;&#19981;&#21516;&#24212;&#29992;&#30340;&#35201;&#27714;&#12290;&#24403;&#21069;LLM&#25104;&#21151;&#30340;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#26159;&#23545;&#40784;&#36807;&#31243;&#12290;&#24403;&#21069;&#30340;&#23545;&#40784;&#26041;&#27861;&#65292;&#22914;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#65288;SFT&#65289;&#21644;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#20391;&#37325;&#20110;&#35757;&#32451;&#26102;&#38388;&#30340;&#23545;&#40784;&#65292;&#24448;&#24448;&#22797;&#26434;&#19988;&#38590;&#20197;&#23454;&#29616;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;InferAligner&#65292;&#19968;&#31181;&#21033;&#29992;&#36328;&#27169;&#22411;&#24341;&#23548;&#36827;&#34892;&#26080;&#23475;&#21270;&#23545;&#40784;&#30340;&#26032;&#26041;&#27861;&#12290;InferAligner&#21033;&#29992;&#20174;&#23433;&#20840;&#23545;&#40784;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#23433;&#20840;&#36716;&#21521;&#21521;&#37327;&#26469;&#20462;&#25913;&#30446;&#26631;&#27169;&#22411;&#22312;&#21709;&#24212;&#26377;&#23475;&#36755;&#20837;&#26102;&#30340;&#28608;&#27963;&#65292;&#20174;&#32780;&#24341;&#23548;&#30446;&#26631;&#27169;&#22411;&#25552;&#20379;&#26080;&#23475;&#21709;&#24212;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#38750;&#24120;&#26377;&#25928;&#22320;&#24212;&#29992;&#20110;&#37329;&#34701;&#12289;&#21307;&#23398;&#21644;&#24066;&#22330;&#31561;&#39046;&#22495;&#29305;&#23450;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid development of large language models (LLMs), they are not only used as general-purpose AI assistants but are also customized through further fine-tuning to meet the requirements of different applications. A pivotal factor in the success of current LLMs is the alignment process. Current alignment methods, such as supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF), focus on training-time alignment and are often complex and cumbersome to implement. Therefore, we develop \textbf{InferAligner}, a novel inference-time alignment method that utilizes cross-model guidance for harmlessness alignment. InferAligner utilizes safety steering vectors extracted from safety-aligned model to modify the activations of the target model when responding to harmful inputs, thereby guiding the target model to provide harmless responses. Experimental results show that our method can be very effectively applied to domain-specific models in finance, medicine, and ma
&lt;/p&gt;</description></item><item><title>&#19975;&#29289;&#26222;&#21450;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#26082;&#25512;&#21160;&#20102;&#21160;&#24577;&#23545;&#25239;&#24615;&#38382;&#39064;&#29983;&#25104;&#30340;&#21457;&#23637;&#65292;&#21448;&#38459;&#30861;&#20102;&#20854;&#36827;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20026;&#20316;&#32773;&#25552;&#20379;&#20102;LLMs&#21644;&#26816;&#32034;&#27169;&#22411;&#30340;&#20889;&#20316;&#25351;&#23548;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#21644;&#28608;&#21169;&#26426;&#21046;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#23545;&#25239;&#24615;&#38382;&#39064;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2401.11185</link><description>&lt;p&gt;
&#19975;&#29289;&#26222;&#21450;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#26082;&#38459;&#30861;&#21448;&#25512;&#21160;&#20102;&#21160;&#24577;&#23545;&#25239;&#24615;&#38382;&#39064;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
How the Advent of Ubiquitous Large Language Models both Stymie and Turbocharge Dynamic Adversarial Question Generation. (arXiv:2401.11185v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11185
&lt;/p&gt;
&lt;p&gt;
&#19975;&#29289;&#26222;&#21450;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#26082;&#25512;&#21160;&#20102;&#21160;&#24577;&#23545;&#25239;&#24615;&#38382;&#39064;&#29983;&#25104;&#30340;&#21457;&#23637;&#65292;&#21448;&#38459;&#30861;&#20102;&#20854;&#36827;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20026;&#20316;&#32773;&#25552;&#20379;&#20102;LLMs&#21644;&#26816;&#32034;&#27169;&#22411;&#30340;&#20889;&#20316;&#25351;&#23548;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#21644;&#28608;&#21169;&#26426;&#21046;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#23545;&#25239;&#24615;&#38382;&#39064;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#23545;&#25239;&#24615;&#38382;&#39064;&#29983;&#25104;&#26088;&#22312;&#29983;&#25104;&#26082;&#30495;&#23454;&#21448;&#26377;&#20449;&#24687;&#30340;&#31034;&#20363;&#26469;&#22256;&#25200;&#27169;&#22411;&#65292;&#28982;&#32780;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#23545;&#20154;&#31867;&#20316;&#32773;&#26469;&#35828;&#26159;&#19968;&#25226;&#21452;&#20995;&#21073;&#65306;&#26356;&#22810;&#20154;&#23545;&#36825;&#20123;&#27169;&#22411;&#24863;&#20852;&#36259;&#24182;&#25512;&#21160;&#20854;&#26497;&#38480;&#65292;&#20294;&#30001;&#20110;&#27169;&#22411;&#23545;&#25163;&#26356;&#24378;&#22823;&#65292;&#38590;&#20197;&#20987;&#36133;&#12290;&#20026;&#20102;&#20102;&#35299;&#36825;&#20123;&#27169;&#22411;&#23545;&#23545;&#25239;&#24615;&#38382;&#39064;&#32534;&#20889;&#36807;&#31243;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#20026;&#20316;&#32773;&#25552;&#20379;&#20102;LLMs&#21644;&#26816;&#32034;&#27169;&#22411;&#30340;&#20889;&#20316;&#25351;&#23548;&#65292;&#20197;&#29702;&#35299;&#20026;&#20160;&#20040;&#20182;&#20204;&#30340;&#38382;&#39064;&#19981;&#20855;&#22791;&#23545;&#25239;&#24615;&#12290;&#34429;&#28982;&#20316;&#32773;&#21487;&#20197;&#21019;&#24314;&#26377;&#36259;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23545;&#25239;&#24615;&#38382;&#39064;&#65292;&#20294;&#20182;&#20204;&#26377;&#26102;&#20250;&#37319;&#29992;&#35809;&#35745;&#23548;&#33268;&#38382;&#39064;&#36136;&#37327;&#21464;&#24046;&#65292;&#36825;&#20123;&#38382;&#39064;&#19981;&#20165;&#23545;&#35745;&#31639;&#26426;&#32780;&#19988;&#23545;&#20154;&#31867;&#20063;&#26159;&#27169;&#31946;&#12289;&#20027;&#35266;&#25110;&#28151;&#20081;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31934;&#24515;&#35774;&#35745;&#30340;&#24230;&#37327;&#26631;&#20934;&#21644;&#28608;&#21169;&#26426;&#21046;&#26469;&#24341;&#21457;&#22909;&#30340;&#12289;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23545;&#25239;&#24615;&#38382;&#39064;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic adversarial question generation, where humans write examples to stump a model, aims to create examples that are realistic and informative. However, the advent of large language models (LLMs) has been a double-edged sword for human authors: more people are interested in seeing and pushing the limits of these models, but because the models are so much stronger an opponent, they are harder to defeat. To understand how these models impact adversarial question writing process, we enrich the writing guidance with LLMs and retrieval models for the authors to reason why their questions are not adversarial. While authors could create interesting, challenging adversarial questions, they sometimes resort to tricks that result in poor questions that are ambiguous, subjective, or confusing not just to a computer but also to humans. To address these issues, we propose new metrics and incentives for eliciting good, challenging questions and present a new dataset of adversarially authored ques
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GAAM&#30340;&#22810;&#22836;&#39640;&#26031;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#29992;&#20110;&#22686;&#24378;&#36328;&#22810;&#20010;&#27169;&#24577;&#30340;&#20449;&#24687;&#32858;&#21512;&#12290;&#36890;&#36807;&#23558;&#21487;&#23398;&#20064;&#30340;&#22343;&#20540;&#21644;&#26041;&#24046;&#32435;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#65292;GAAM&#33021;&#22815;&#21160;&#24577;&#22320;&#37325;&#26032;&#35843;&#25972;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#22312;&#22788;&#29702;&#38750;&#24179;&#31283;&#25968;&#25454;&#26102;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#36229;&#36807;&#20102;&#30446;&#21069;&#29616;&#26377;&#30340;&#27880;&#24847;&#21147;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#30340;&#36866;&#24212;&#24615;&#24378;&#19988;&#21442;&#25968;&#25968;&#37327;&#36739;&#23569;&#65292;&#20855;&#26377;&#25913;&#36827;&#29616;&#26377;&#27880;&#24847;&#21147;&#26694;&#26550;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.11143</link><description>&lt;p&gt;
&#39640;&#26031;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26159;&#21807;&#19968;&#25152;&#38656;&#30340;&#65306;&#36328;&#22810;&#20010;&#27169;&#24577;&#30340;&#20581;&#22766;&#19978;&#19979;&#25991;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Gaussian Adaptive Attention is All You Need: Robust Contextual Representations Across Multiple Modalities. (arXiv:2401.11143v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11143
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GAAM&#30340;&#22810;&#22836;&#39640;&#26031;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#29992;&#20110;&#22686;&#24378;&#36328;&#22810;&#20010;&#27169;&#24577;&#30340;&#20449;&#24687;&#32858;&#21512;&#12290;&#36890;&#36807;&#23558;&#21487;&#23398;&#20064;&#30340;&#22343;&#20540;&#21644;&#26041;&#24046;&#32435;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#65292;GAAM&#33021;&#22815;&#21160;&#24577;&#22320;&#37325;&#26032;&#35843;&#25972;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#22312;&#22788;&#29702;&#38750;&#24179;&#31283;&#25968;&#25454;&#26102;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#36229;&#36807;&#20102;&#30446;&#21069;&#29616;&#26377;&#30340;&#27880;&#24847;&#21147;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#30340;&#36866;&#24212;&#24615;&#24378;&#19988;&#21442;&#25968;&#25968;&#37327;&#36739;&#23569;&#65292;&#20855;&#26377;&#25913;&#36827;&#29616;&#26377;&#27880;&#24847;&#21147;&#26694;&#26550;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#22836;&#39640;&#26031;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;GAAM&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#27880;&#24847;&#21147;&#26694;&#26550;&#65292;&#24182;&#35774;&#35745;&#20102;&#39640;&#26031;&#33258;&#36866;&#24212;&#21464;&#21387;&#22120;&#65288;GAT&#65289;&#65292;&#26088;&#22312;&#22686;&#24378;&#36328;&#22810;&#20010;&#27169;&#24577;&#65288;&#21253;&#25324;&#35821;&#38899;&#12289;&#25991;&#26412;&#21644;&#35270;&#35273;&#65289;&#30340;&#20449;&#24687;&#32858;&#21512;&#12290;GAAM&#23558;&#21487;&#23398;&#20064;&#30340;&#22343;&#20540;&#21644;&#26041;&#24046;&#34701;&#20837;&#20854;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#65292;&#37319;&#29992;&#22810;&#22836;&#26694;&#26550;&#23454;&#29616;&#65292;&#20351;&#20854;&#33021;&#22815;&#38598;&#20307;&#24314;&#27169;&#20219;&#20309;&#27010;&#29575;&#20998;&#24067;&#65292;&#20197;&#21160;&#24577;&#37325;&#26032;&#35843;&#25972;&#29305;&#24449;&#37325;&#35201;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#39640;&#24230;&#38750;&#24179;&#31283;&#25968;&#25454;&#26102;&#34920;&#29616;&#20986;&#26174;&#33879;&#25913;&#36827;&#65292;&#36890;&#36807;&#35782;&#21035;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#20851;&#38190;&#20803;&#32032;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#27880;&#24847;&#21147;&#25216;&#26415;&#22312;&#27169;&#22411;&#24615;&#33021;&#19978;&#30340;&#29366;&#24577;&#65288;&#31934;&#24230;&#22686;&#21152;&#32422;20%&#65289;&#12290;GAAM&#19982;&#22522;&#20110;&#28857;&#31215;&#30340;&#27880;&#24847;&#21147;&#27169;&#22411;&#20860;&#23481;&#65292;&#24182;&#20855;&#26377;&#30456;&#23545;&#36739;&#20302;&#30340;&#21442;&#25968;&#25968;&#37327;&#65292;&#23637;&#31034;&#20102;&#20854;&#36866;&#24212;&#24615;&#21644;&#25552;&#21319;&#29616;&#26377;&#27880;&#24847;&#21147;&#26694;&#26550;&#30340;&#28508;&#21147;&#12290;&#22312;&#23454;&#35777;&#26041;&#38754;&#65292;GAAM&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#36866;&#24212;&#24615;&#21644;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the Multi-Head Gaussian Adaptive Attention Mechanism (GAAM), a novel probabilistic attention framework, and the Gaussian Adaptive Transformer (GAT), designed to enhance information aggregation across multiple modalities, including Speech, Text and Vision. GAAM integrates learnable mean and variance into its attention mechanism, implemented in a Multi-Headed framework enabling it to collectively model any Probability Distribution for dynamic recalibration of feature significance. This method demonstrates significant improvements, especially with highly non-stationary data, surpassing the state-of-the-art attention techniques in model performance (up to approximately +20% in accuracy) by identifying key elements within the feature space. GAAM's compatibility with dot-product-based attention models and relatively low number of parameters showcases its adaptability and potential to boost existing attention frameworks. Empirically, GAAM exhibits superior adaptability and efficacy
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#20020;&#24202;&#23454;&#36341;&#25351;&#21335;&#32435;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#22686;&#24378;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#30340;&#26041;&#27861;&#12290;&#20182;&#20204;&#24320;&#21457;&#20102;&#19977;&#31181;&#26041;&#27861;&#65292;&#24182;&#23545;&#22235;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#22312;COVID-19&#38376;&#35786;&#27835;&#30103;&#26041;&#38754;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.11120</link><description>&lt;p&gt;
&#23558;&#20020;&#24202;&#23454;&#36341;&#25351;&#21335;&#32435;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#22686;&#24378;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Enhancing Large Language Models for Clinical Decision Support by Incorporating Clinical Practice Guidelines. (arXiv:2401.11120v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11120
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#20020;&#24202;&#23454;&#36341;&#25351;&#21335;&#32435;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#22686;&#24378;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#30340;&#26041;&#27861;&#12290;&#20182;&#20204;&#24320;&#21457;&#20102;&#19977;&#31181;&#26041;&#27861;&#65292;&#24182;&#23545;&#22235;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#22312;COVID-19&#38376;&#35786;&#27835;&#30103;&#26041;&#38754;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#25645;&#37197;&#20020;&#24202;&#23454;&#36341;&#25351;&#21335;&#65288;CPGs&#65289;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#65288;CDS&#65289;&#12290;&#28982;&#32780;&#65292;&#23558;CPGs&#32435;&#20837;LLMs&#30340;&#26041;&#27861;&#24182;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#23558;CPGs&#32435;&#20837;LLMs&#65306;&#20108;&#20803;&#20915;&#31574;&#26641;&#65288;BDT&#65289;&#65292;&#31243;&#24207;&#36741;&#21161;&#22270;&#26500;&#24314;&#65288;PAGC&#65289;&#65292;&#20197;&#21450;&#24605;&#32500;&#38142;&#23569;&#26679;&#26412;&#25552;&#31034;&#65288;CoT-FSP&#65289;&#12290;&#20026;&#35780;&#20272;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#32452;&#21512;&#25104;&#24739;&#32773;&#25551;&#36848;&#65292;&#24182;&#23545;&#30001;&#22235;&#20010;LLMs&#29983;&#25104;&#30340;&#21709;&#24212;&#36827;&#34892;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#65306;GPT-4&#65292;GPT-3.5 Turbo&#65292;LLaMA&#21644;PaLM 2&#12290;&#38646;&#26679;&#26412;&#25552;&#31034;&#65288;ZSP&#65289;&#34987;&#29992;&#20316;&#22522;&#32447;&#26041;&#27861;&#12290;&#25105;&#20204;&#20197;COVID-19&#38376;&#35786;&#27835;&#30103;&#30340;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#22235;&#20010;LLMs&#22312;&#22686;&#21152;&#20102;CPGs&#21518;&#30456;&#23545;&#20110;&#22522;&#32447;ZSP&#23637;&#29616;&#20102;&#25552;&#39640;&#30340;&#24615;&#33021;&#12290;BDT&#22312;&#33258;&#21160;&#35780;&#20272;&#20013;&#34920;&#29616;&#20248;&#20110;CoT-FSP&#21644;PAGC&#12290;&#25152;&#26377;&#25552;&#20986;&#30340;&#26041;&#27861;&#37117;&#34920;&#29616;&#20986;&#20102;&#36739;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background Large Language Models (LLMs), enhanced with Clinical Practice Guidelines (CPGs), can significantly improve Clinical Decision Support (CDS). However, methods for incorporating CPGs into LLMs are not well studied. Methods We develop three distinct methods for incorporating CPGs into LLMs: Binary Decision Tree (BDT), Program-Aided Graph Construction (PAGC), and Chain-of-Thought-Few-Shot Prompting (CoT-FSP). To evaluate the effectiveness of the proposed methods, we create a set of synthetic patient descriptions and conduct both automatic and human evaluation of the responses generated by four LLMs: GPT-4, GPT-3.5 Turbo, LLaMA, and PaLM 2. Zero-Shot Prompting (ZSP) was used as the baseline method. We focus on CDS for COVID-19 outpatient treatment as the case study. Results All four LLMs exhibit improved performance when enhanced with CPGs compared to the baseline ZSP. BDT outperformed both CoT-FSP and PAGC in automatic evaluation. All of the proposed methods demonstrated high per
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#24335;OpenIE&#27169;&#22411;DualOIE&#65292;&#36890;&#36807;&#23454;&#29616;&#19968;&#20010;&#21452;&#37325;&#20219;&#21153;&#65292;&#21363;&#23558;&#21477;&#23376;&#20013;&#30340;&#19977;&#20803;&#32452;&#36716;&#21270;&#20026;&#21477;&#23376;&#65292;&#26469;&#26356;&#26377;&#25928;&#22320;&#25552;&#21462;&#22797;&#26434;&#30340;&#19977;&#20803;&#32452;&#12290;&#27492;&#26041;&#27861;&#40723;&#21169;&#27169;&#22411;&#27491;&#30830;&#35782;&#21035;&#21477;&#23376;&#32467;&#26500;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#25552;&#21462;&#25152;&#26377;&#28508;&#22312;&#30340;&#19977;&#20803;&#32452;&#12290;</title><link>http://arxiv.org/abs/2401.11107</link><description>&lt;p&gt;
&#20351;&#29992;&#35859;&#35789;&#25552;&#31034;&#22312;&#24320;&#25918;&#20449;&#24687;&#25552;&#21462;&#20013;&#21033;&#29992;&#23545;&#20598;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploiting Duality in Open Information Extraction with Predicate Prompt. (arXiv:2401.11107v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#24335;OpenIE&#27169;&#22411;DualOIE&#65292;&#36890;&#36807;&#23454;&#29616;&#19968;&#20010;&#21452;&#37325;&#20219;&#21153;&#65292;&#21363;&#23558;&#21477;&#23376;&#20013;&#30340;&#19977;&#20803;&#32452;&#36716;&#21270;&#20026;&#21477;&#23376;&#65292;&#26469;&#26356;&#26377;&#25928;&#22320;&#25552;&#21462;&#22797;&#26434;&#30340;&#19977;&#20803;&#32452;&#12290;&#27492;&#26041;&#27861;&#40723;&#21169;&#27169;&#22411;&#27491;&#30830;&#35782;&#21035;&#21477;&#23376;&#32467;&#26500;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#25552;&#21462;&#25152;&#26377;&#28508;&#22312;&#30340;&#19977;&#20803;&#32452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#20449;&#24687;&#25552;&#21462;&#65288;OpenIE&#65289;&#26088;&#22312;&#20174;&#32473;&#23450;&#30340;&#21477;&#23376;&#20013;&#25552;&#21462;&#20197;&#65288;&#20027;&#20307;&#65292;&#35859;&#35789;&#65292;&#23486;&#35821;&#65289;&#24418;&#24335;&#21576;&#29616;&#30340;&#26080;&#27169;&#24335;&#19977;&#20803;&#32452;&#12290;&#19982;&#19968;&#33324;&#30340;&#20449;&#24687;&#25552;&#21462;&#65288;IE&#65289;&#30456;&#27604;&#65292;OpenIE&#23545;IE&#27169;&#22411;&#25552;&#20986;&#20102;&#26356;&#22810;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#22312;&#21477;&#23376;&#20013;&#23384;&#22312;&#22810;&#20010;&#22797;&#26434;&#30340;&#19977;&#20803;&#32452;&#26102;&#12290;&#20026;&#20102;&#26356;&#26377;&#25928;&#22320;&#25552;&#21462;&#36825;&#20123;&#22797;&#26434;&#30340;&#19977;&#20803;&#32452;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#24335;OpenIE&#27169;&#22411;&#65292;&#21517;&#20026;DualOIE&#65292;&#23427;&#22312;&#25552;&#21462;&#21477;&#23376;&#20013;&#30340;&#19968;&#20123;&#19977;&#20803;&#32452;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#21478;&#19968;&#20010;&#20219;&#21153;&#65292;&#21363;&#23558;&#19977;&#20803;&#32452;&#36716;&#21270;&#20026;&#21477;&#23376;&#12290;&#36825;&#31181;&#21452;&#37325;&#20219;&#21153;&#40723;&#21169;&#27169;&#22411;&#27491;&#30830;&#35782;&#21035;&#32473;&#23450;&#21477;&#23376;&#30340;&#32467;&#26500;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#20174;&#21477;&#23376;&#20013;&#25552;&#21462;&#25152;&#26377;&#28508;&#22312;&#30340;&#19977;&#20803;&#32452;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DualOIE&#20998;&#20026;&#20004;&#20010;&#27493;&#39588;&#25552;&#21462;&#19977;&#20803;&#32452;&#65306;&#39318;&#20808;&#25552;&#21462;&#25152;&#26377;&#28508;&#22312;&#35859;&#35789;&#30340;&#24207;&#21015;&#65292;&#28982;&#21518;&#20351;&#29992;&#35859;&#35789;&#24207;&#21015;&#20316;&#20026;&#25552;&#31034;&#35825;&#23548;&#19977;&#20803;&#32452;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open information extraction (OpenIE) aims to extract the schema-free triplets in the form of (\emph{subject}, \emph{predicate}, \emph{object}) from a given sentence. Compared with general information extraction (IE), OpenIE poses more challenges for the IE models, {especially when multiple complicated triplets exist in a sentence. To extract these complicated triplets more effectively, in this paper we propose a novel generative OpenIE model, namely \emph{DualOIE}, which achieves a dual task at the same time as extracting some triplets from the sentence, i.e., converting the triplets into the sentence.} Such dual task encourages the model to correctly recognize the structure of the given sentence and thus is helpful to extract all potential triplets from the sentence. Specifically, DualOIE extracts the triplets in two steps: 1) first extracting a sequence of all potential predicates, 2) then using the predicate sequence as a prompt to induce the generation of triplets. Our experiments 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#26448;&#26009;&#31185;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#26448;&#26009;&#31185;&#23398;&#20449;&#24687;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25552;&#21462;&#20219;&#21153;&#19978;&#65292;LLMs&#19982;&#20256;&#32479;&#27169;&#22411;&#30456;&#27604;&#34920;&#29616;&#26377;&#38480;&#65292;&#20294;&#22312;&#23569;&#25968;-shot&#25552;&#31034;&#19979;&#26377;&#19968;&#23450;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.11052</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#26448;&#26009;&#31185;&#23398;&#25991;&#29486;&#20013;&#25366;&#25496;&#23454;&#39564;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Mining experimental data from Materials Science literature with Large Language Models. (arXiv:2401.11052v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#26448;&#26009;&#31185;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#26448;&#26009;&#31185;&#23398;&#20449;&#24687;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25552;&#21462;&#20219;&#21153;&#19978;&#65292;LLMs&#19982;&#20256;&#32479;&#27169;&#22411;&#30456;&#27604;&#34920;&#29616;&#26377;&#38480;&#65292;&#20294;&#22312;&#23569;&#25968;-shot&#25552;&#31034;&#19979;&#26377;&#19968;&#23450;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#35780;&#20272;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT-3.5-Turbo&#12289;GPT-4&#21644;GPT-4-Turbo&#65292;&#22312;&#26448;&#26009;&#31185;&#23398;&#39046;&#22495;&#31185;&#23398;&#25991;&#26723;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#27604;&#36739;&#20998;&#26512;&#22797;&#26434;&#30340;&#26448;&#26009;&#34920;&#36798;&#24335;&#65292;&#24378;&#35843;&#21270;&#23398;&#24335;&#30340;&#26631;&#20934;&#21270;&#65292;&#20197;&#35299;&#20915;&#26448;&#26009;&#31185;&#23398;&#20449;&#24687;&#35780;&#20272;&#20013;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20027;&#35201;&#20851;&#27880;&#20449;&#24687;&#25552;&#21462;&#30340;&#20004;&#20010;&#20851;&#38190;&#20219;&#21153;&#65306;&#65288;i&#65289;&#30740;&#31350;&#26448;&#26009;&#21644;&#29289;&#29702;&#24615;&#36136;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#21644;&#65288;ii&#65289;&#36825;&#20123;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#25552;&#21462;&#65288;RE&#65289;&#12290;LLMs&#22312;&#25191;&#34892;&#36825;&#20123;&#20219;&#21153;&#26102;&#30340;&#34920;&#29616;&#19982;&#22522;&#20110;BERT&#26550;&#26500;&#21644;&#22522;&#20110;&#35268;&#21017;&#30340;&#20256;&#32479;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#23545;&#20110;NER&#65292;LLMs&#22312;&#38646;-shot&#25552;&#31034;&#19979;&#26080;&#27861;&#36229;&#36234;&#22522;&#20934;&#32447;&#65292;&#24182;&#19988;&#20165;&#22312;&#23569;&#25968;-shot&#25552;&#31034;&#19979;&#26377;&#23569;&#37327;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;...
&lt;/p&gt;
&lt;p&gt;
This study is dedicated to evaluating the capabilities of advanced large language models (LLMs) such as GPT-3.5-Turbo, GPT-4, and GPT-4-Turbo in the extraction of structured information from scientific documents within the field of materials science. We introduce a novel methodology for the comparative analysis of intricate material expressions, emphasising the standardisation of chemical formulas to tackle the complexities inherent in materials science information assessment. To this end, we primarily focus on two critical tasks of information extraction: (i) a named entity recognition (NER) of studied materials and physical properties and (ii) a relation extraction (RE) between these entities. The performance of LLMs in executing these tasks is benchmarked against traditional models based on the BERT architecture and rule-based approaches. For NER, LLMs fail to outperform the baseline with zero-shot prompting and exhibit only limited improvement with few-shot prompting. However, for 
&lt;/p&gt;</description></item><item><title>PubTator 3.0&#26159;&#19968;&#20010;&#20351;&#29992;AI&#25216;&#26415;&#30340;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#36164;&#28304;&#65292;&#25552;&#20379;&#35821;&#20041;&#21644;&#20851;&#31995;&#25628;&#32034;&#65292;&#20197;&#21450;&#39640;&#32423;&#25628;&#32034;&#21151;&#33021;&#21644;&#22823;&#35268;&#27169;&#20998;&#26512;&#12290;&#19982;PubMed&#21644;Google Scholar&#30456;&#27604;&#65292;&#22312;&#26816;&#32034;&#31934;&#24230;&#21644;&#25991;&#31456;&#25968;&#37327;&#19978;&#37117;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;&#21516;&#26102;&#65292;&#19982;ChatGPT&#65288;GPT-4&#65289;API&#38598;&#25104;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#25628;&#32034;&#32467;&#26524;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.11048</link><description>&lt;p&gt;
PubTator 3.0&#65306;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#35299;&#38145;&#25991;&#29486;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;
PubTator 3.0: an AI-powered Literature Resource for Unlocking Biomedical Knowledge. (arXiv:2401.11048v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11048
&lt;/p&gt;
&lt;p&gt;
PubTator 3.0&#26159;&#19968;&#20010;&#20351;&#29992;AI&#25216;&#26415;&#30340;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#36164;&#28304;&#65292;&#25552;&#20379;&#35821;&#20041;&#21644;&#20851;&#31995;&#25628;&#32034;&#65292;&#20197;&#21450;&#39640;&#32423;&#25628;&#32034;&#21151;&#33021;&#21644;&#22823;&#35268;&#27169;&#20998;&#26512;&#12290;&#19982;PubMed&#21644;Google Scholar&#30456;&#27604;&#65292;&#22312;&#26816;&#32034;&#31934;&#24230;&#21644;&#25991;&#31456;&#25968;&#37327;&#19978;&#37117;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;&#21516;&#26102;&#65292;&#19982;ChatGPT&#65288;GPT-4&#65289;API&#38598;&#25104;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#25628;&#32034;&#32467;&#26524;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
PubTator 3.0&#26159;&#19968;&#20010;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#25552;&#20379;&#35821;&#20041;&#21644;&#20851;&#31995;&#25628;&#32034;&#30340;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#36164;&#28304;&#65292;&#28085;&#30422;&#34507;&#30333;&#36136;&#12289;&#36951;&#20256;&#21464;&#24322;&#12289;&#30142;&#30149;&#21644;&#21270;&#21512;&#29289;&#31561;&#20851;&#38190;&#27010;&#24565;&#12290;&#23427;&#30446;&#21069;&#25552;&#20379;&#20102;&#36229;&#36807;10&#20159;&#20010;&#23454;&#20307;&#21644;&#20851;&#31995;&#27880;&#37322;&#65292;&#35206;&#30422;&#20102;&#22823;&#32422;3600&#19975;&#31687;PubMed&#25688;&#35201;&#21644;600&#19975;&#31687;PMC&#24320;&#25918;&#33719;&#21462;&#23376;&#38598;&#30340;&#20840;&#25991;&#25991;&#31456;&#65292;&#24182;&#27599;&#21608;&#26356;&#26032;&#12290;PubTator 3.0&#30340;&#22312;&#32447;&#30028;&#38754;&#21644;API&#21033;&#29992;&#36825;&#20123;&#39044;&#35745;&#31639;&#30340;&#23454;&#20307;&#20851;&#31995;&#21644;&#21516;&#20041;&#35789;&#25552;&#20379;&#39640;&#32423;&#25628;&#32034;&#21151;&#33021;&#65292;&#25903;&#25345;&#22823;&#35268;&#27169;&#20998;&#26512;&#65292;&#31616;&#21270;&#20102;&#35768;&#22810;&#22797;&#26434;&#20449;&#24687;&#38656;&#27714;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#20307;&#23545;&#26597;&#35810;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;PubTator 3.0&#30340;&#26816;&#32034;&#36136;&#37327;&#65292;&#35777;&#26126;&#23427;&#26816;&#32034;&#21040;&#30340;&#25991;&#31456;&#25968;&#37327;&#27604;PubMed&#21644;Google Scholar&#26356;&#22810;&#65292;&#24182;&#22312;&#21069;20&#20010;&#32467;&#26524;&#20013;&#20855;&#26377;&#26356;&#39640;&#30340;&#31934;&#24230;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#23558;ChatGPT&#65288;GPT-4&#65289;&#19982;PubTator API&#38598;&#25104;&#21487;&#20197;&#22823;&#22823;&#25913;&#21892;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
PubTator 3.0 (https://www.ncbi.nlm.nih.gov/research/pubtator3/) is a biomedical literature resource using state-of-the-art AI techniques to offer semantic and relation searches for key concepts like proteins, genetic variants, diseases, and chemicals. It currently provides over one billion entity and relation annotations across approximately 36 million PubMed abstracts and 6 million full-text articles from the PMC open access subset, updated weekly. PubTator 3.0's online interface and API utilize these precomputed entity relations and synonyms to provide advanced search capabilities and enable large-scale analyses, streamlining many complex information needs. We showcase the retrieval quality of PubTator 3.0 using a series of entity pair queries, demonstrating that PubTator 3.0 retrieves a greater number of articles than either PubMed or Google Scholar, with higher precision in the top 20 results. We further show that integrating ChatGPT (GPT-4) with PubTator APIs dramatically improves
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#20010;&#23558;FAIR&#25968;&#25454;&#21407;&#21017;&#23884;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#26694;&#26550;&#65292;&#24182;&#25552;&#20379;&#20102;&#25972;&#21512;&#25351;&#21335;&#21644;&#26816;&#26597;&#28165;&#21333;&#12290;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#22312;&#31526;&#21512;FAIR&#26631;&#20934;&#30340;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#21644;&#20943;&#36731;&#20559;&#35265;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.11033</link><description>&lt;p&gt;
FAIR&#21040;&#20301;&#65306;&#25105;&#20204;&#22914;&#20309;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#24320;&#21457;&#21644;&#35780;&#20272;&#31526;&#21512;FAIR&#26631;&#20934;&#30340;&#25968;&#25454;&#38598;&#65311;
&lt;/p&gt;
&lt;p&gt;
FAIR Enough: How Can We Develop and Assess a FAIR-Compliant Dataset for Large Language Models' Training?. (arXiv:2401.11033v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11033
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#20010;&#23558;FAIR&#25968;&#25454;&#21407;&#21017;&#23884;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#26694;&#26550;&#65292;&#24182;&#25552;&#20379;&#20102;&#25972;&#21512;&#25351;&#21335;&#21644;&#26816;&#26597;&#28165;&#21333;&#12290;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#22312;&#31526;&#21512;FAIR&#26631;&#20934;&#30340;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#21644;&#20943;&#36731;&#20559;&#35265;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#20984;&#26174;&#20102;&#36947;&#24503;&#23454;&#36341;&#21644;&#25968;&#25454;&#23436;&#25972;&#24615;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#23558;FAIR&#65288;&#21487;&#21457;&#29616;&#12289;&#21487;&#35775;&#38382;&#12289;&#21487;&#20114;&#25805;&#20316;&#12289;&#21487;&#37325;&#29992;&#65289;&#25968;&#25454;&#21407;&#21017;&#23884;&#20837;&#21040;LLM&#35757;&#32451;&#20013;&#30340;&#26694;&#26550;&#12290;&#36825;&#19968;&#26041;&#27861;&#26631;&#24535;&#30528;&#26397;&#30528;&#31526;&#21512;FAIR&#26631;&#20934;&#30340;&#23454;&#36341;&#30340;&#36716;&#21464;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#20379;&#20102;&#23558;FAIR&#25968;&#25454;&#21407;&#21017;&#25972;&#21512;&#21040;LLM&#35757;&#32451;&#20013;&#30340;&#25351;&#21335;&#12290;&#35813;&#20030;&#25514;&#21253;&#25324;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#20154;&#21592;&#30340;&#26816;&#26597;&#28165;&#21333;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#23427;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#37325;&#28857;&#26159;&#22312;&#25105;&#20204;&#31526;&#21512;FAIR&#26631;&#20934;&#30340;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#21644;&#20943;&#36731;&#20559;&#35265;&#12290;&#36825;&#39033;&#24037;&#20316;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#21644;&#25968;&#25454;&#31185;&#23398;&#26159;&#37325;&#35201;&#30340;&#36129;&#29486;&#65292;&#20513;&#23548;&#22312;LLMs&#20013;&#20351;&#29992;&#24179;&#34913;&#21644;&#36947;&#24503;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advancements in Large Language Models (LLMs) highlight the need for ethical practices and data integrity. We introduce a framework that embeds FAIR (Findable, Accessible, Interoperable, Reusable) data principles into LLM training. This approach marks a shift towards practices compliant with FAIR standards. Our framework presents guidelines for integrating FAIR data principles into LLM training. This initiative includes a checklist for researchers and developers. We also demonstrate its practical application through a case study focused on bias identification and mitigation in our FAIR-compliant dataset. This work is a significant contribution to AI ethics and data science, advocating for balanced and ethical training methods in LLMs.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#22810;&#35821;&#35328;&#20167;&#24680;&#35328;&#35770;&#12290;&#35813;&#27169;&#22411;&#22312;&#24847;&#22823;&#21033;&#35821;&#12289;&#33521;&#35821;&#12289;&#24503;&#35821;&#21644;&#23391;&#21152;&#25289;&#35821;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#27169;&#22411;&#26356;&#39640;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.11021</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#35821;&#35328;&#20167;&#24680;&#35328;&#35770;&#20998;&#26512;&#21644;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Analysis and Detection of Multilingual Hate Speech Using Transformer Based Deep Learning. (arXiv:2401.11021v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11021
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#22810;&#35821;&#35328;&#20167;&#24680;&#35328;&#35770;&#12290;&#35813;&#27169;&#22411;&#22312;&#24847;&#22823;&#21033;&#35821;&#12289;&#33521;&#35821;&#12289;&#24503;&#35821;&#21644;&#23391;&#21152;&#25289;&#35821;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#27169;&#22411;&#26356;&#39640;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20167;&#24680;&#35328;&#35770;&#26159;&#30452;&#25509;&#25915;&#20987;&#25110;&#23459;&#20256;&#38024;&#23545;&#29305;&#23450;&#32676;&#20307;&#25110;&#20010;&#20154;&#30340;&#24974;&#24680;&#30340;&#26377;&#23475;&#20869;&#23481;&#65292;&#20363;&#22914;&#31181;&#26063;&#20027;&#20041;&#12289;&#23447;&#25945;&#25110;&#24615;&#21462;&#21521;&#31561;&#12290;&#36825;&#20250;&#23545;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#30340;&#31038;&#20250;&#29983;&#27963;&#20135;&#29983;&#24433;&#21709;&#65292;&#22240;&#20026;&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#20998;&#20139;&#30340;&#20167;&#24680;&#20869;&#23481;&#21487;&#33021;&#20250;&#23545;&#20010;&#20154;&#21644;&#31038;&#21306;&#36896;&#25104;&#20260;&#23475;&#12290;&#38543;&#30528;&#32593;&#32476;&#19978;&#20167;&#24680;&#35328;&#35770;&#30340;&#22686;&#21152;&#65292;&#33258;&#21160;&#21270;&#26816;&#27979;&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#19968;&#20010;&#20219;&#21153;&#30340;&#38656;&#27714;&#20063;&#22312;&#22686;&#21152;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;Transformer&#27169;&#22411;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#26816;&#27979;&#20167;&#24680;&#35328;&#35770;&#30340;&#26041;&#27861;&#65292;&#22914;Twitter&#12289;Facebook&#12289;WhatsApp&#12289;Instagram&#31561;&#12290;&#35813;&#27169;&#22411;&#29420;&#31435;&#20110;&#35821;&#35328;&#65292;&#24182;&#24050;&#22312;&#24847;&#22823;&#21033;&#35821;&#12289;&#33521;&#35821;&#12289;&#24503;&#35821;&#12289;&#23391;&#21152;&#25289;&#35821;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#40644;&#37329;&#26631;&#20934;&#25968;&#25454;&#38598;&#30001;&#30693;&#21517;&#30740;&#31350;&#32773;Zeerak Talat&#12289;Sara Tonelli&#12289;Melanie Siegel&#21644;Rezaul Karim&#25910;&#38598;&#12290;&#25152;&#25552;&#20986;&#27169;&#22411;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26041;&#38754;&#30340;&#25104;&#21151;&#29575;&#39640;&#20110;&#29616;&#26377;&#22522;&#20934;&#21644;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#20934;&#30830;&#29575;&#36739;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hate speech is harmful content that directly attacks or promotes hatred against members of groups or individuals based on actual or perceived aspects of identity, such as racism, religion, or sexual orientation. This can affect social life on social media platforms as hateful content shared through social media can harm both individuals and communities. As the prevalence of hate speech increases online, the demand for automated detection as an NLP task is increasing. In this work, the proposed method is using transformer-based model to detect hate speech in social media, like twitter, Facebook, WhatsApp, Instagram, etc. The proposed model is independent of languages and has been tested on Italian, English, German, Bengali. The Gold standard datasets were collected from renowned researcher Zeerak Talat, Sara Tonelli, Melanie Siegel, and Rezaul Karim. The success rate of the proposed model for hate speech detection is higher than the existing baseline and state-of-the-art models with acc
&lt;/p&gt;</description></item><item><title>&#25918;&#23556;&#32959;&#30244;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25968;&#25454;&#24211;&#65288;ROND&#65289;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#25918;&#23556;&#32959;&#30244;&#23398;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#36923;&#36753;&#25512;&#29702;&#12289;&#25991;&#26412;&#20998;&#31867;&#12289;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31561;&#22810;&#20010;&#20219;&#21153;&#65292;&#24182;&#19988;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#25351;&#20196;&#23545;&#25968;&#25454;&#38598;&#21644;&#30456;&#24212;&#30340;&#35821;&#35328;&#27169;&#22411;CancerChat&#12290;</title><link>http://arxiv.org/abs/2401.10995</link><description>&lt;p&gt;
&#25918;&#23556;&#32959;&#30244;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25968;&#25454;&#24211;
&lt;/p&gt;
&lt;p&gt;
The Radiation Oncology NLP Database. (arXiv:2401.10995v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10995
&lt;/p&gt;
&lt;p&gt;
&#25918;&#23556;&#32959;&#30244;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25968;&#25454;&#24211;&#65288;ROND&#65289;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#25918;&#23556;&#32959;&#30244;&#23398;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#36923;&#36753;&#25512;&#29702;&#12289;&#25991;&#26412;&#20998;&#31867;&#12289;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31561;&#22810;&#20010;&#20219;&#21153;&#65292;&#24182;&#19988;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#25351;&#20196;&#23545;&#25968;&#25454;&#38598;&#21644;&#30456;&#24212;&#30340;&#35821;&#35328;&#27169;&#22411;CancerChat&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#25918;&#23556;&#32959;&#30244;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25968;&#25454;&#24211;&#65288;ROND&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#25918;&#23556;&#32959;&#30244;&#23398;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25968;&#25454;&#38598;&#12290;&#25918;&#23556;&#32959;&#30244;&#23398;&#26159;&#19968;&#38376;&#37325;&#35201;&#30340;&#21307;&#23398;&#19987;&#19994;&#65292;&#20294;&#22312;&#36807;&#21435;&#21463;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31038;&#21306;&#30340;&#20851;&#27880;&#26377;&#38480;&#12290;&#38543;&#30528;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#20986;&#29616;&#65292;&#38656;&#35201;&#19987;&#38376;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#26469;&#20419;&#36827;&#30740;&#31350;&#21644;&#21457;&#23637;&#12290;ROND&#19987;&#38376;&#38024;&#23545;&#25918;&#23556;&#32959;&#30244;&#23398;&#39046;&#22495;&#20013;&#30340;&#31354;&#30333;&#36827;&#34892;&#35774;&#35745;&#65292;&#36825;&#20010;&#39046;&#22495;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#25506;&#32034;&#25552;&#20379;&#20102;&#35768;&#22810;&#26426;&#20250;&#12290;&#23427;&#28085;&#30422;&#20102;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#21253;&#25324;&#36923;&#36753;&#25512;&#29702;&#12289;&#25991;&#26412;&#20998;&#31867;&#12289;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#12289;&#38382;&#39064;&#22238;&#31572;&#65288;QA&#65289;&#12289;&#25991;&#26412;&#25688;&#35201;&#21644;&#24739;&#32773;-&#20020;&#24202;&#21307;&#29983;&#23545;&#35805;&#65292;&#27599;&#20010;&#20219;&#21153;&#37117;&#30528;&#37325;&#25918;&#23556;&#32959;&#30244;&#23398;&#27010;&#24565;&#21644;&#24212;&#29992;&#26696;&#20363;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#37319;&#29992;ROND&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;20k&#22810;&#20010;&#25351;&#20196;&#23545;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#35757;&#32451;&#20102;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;CancerChat&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the Radiation Oncology NLP Database (ROND), the first dedicated Natural Language Processing (NLP) dataset for radiation oncology, an important medical specialty that has received limited attention from the NLP community in the past. With the advent of Artificial General Intelligence (AGI), there is an increasing need for specialized datasets and benchmarks to facilitate research and development. ROND is specifically designed to address this gap in the domain of radiation oncology, a field that offers many opportunities for NLP exploration. It encompasses various NLP tasks including Logic Reasoning, Text Classification, Named Entity Recognition (NER), Question Answering (QA), Text Summarization, and Patient-Clinician Conversations, each with a distinct focus on radiation oncology concepts and application cases. In addition, we have developed an instruction-tuning dataset consisting of over 20k instruction pairs (based on ROND) and trained a large language model, CancerChat. T
&lt;/p&gt;</description></item><item><title>RELIANCE&#26159;&#19968;&#20010;&#21487;&#38752;&#30340;&#38598;&#25104;&#23398;&#20064;&#31995;&#32479;&#65292;&#29992;&#20110;&#35780;&#20272;&#20449;&#24687;&#21644;&#26032;&#38395;&#30340;&#21487;&#20449;&#24230;&#12290;&#23427;&#36890;&#36807;&#25972;&#21512;&#22810;&#20010;&#22522;&#26412;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#25552;&#20379;&#20102;&#23545;&#21487;&#20449;&#21644;&#19981;&#21487;&#20449;&#20449;&#24687;&#28304;&#30340;&#20934;&#30830;&#21306;&#20998;&#65292;&#24182;&#22312;&#20449;&#24687;&#21644;&#26032;&#38395;&#21487;&#20449;&#24230;&#35780;&#20272;&#26041;&#38754;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.10940</link><description>&lt;p&gt;
RELIANCE: &#21487;&#38752;&#30340;&#38598;&#25104;&#23398;&#20064;&#29992;&#20110;&#20449;&#24687;&#21644;&#26032;&#38395;&#21487;&#20449;&#24230;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
RELIANCE: Reliable Ensemble Learning for Information and News Credibility Evaluation. (arXiv:2401.10940v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10940
&lt;/p&gt;
&lt;p&gt;
RELIANCE&#26159;&#19968;&#20010;&#21487;&#38752;&#30340;&#38598;&#25104;&#23398;&#20064;&#31995;&#32479;&#65292;&#29992;&#20110;&#35780;&#20272;&#20449;&#24687;&#21644;&#26032;&#38395;&#30340;&#21487;&#20449;&#24230;&#12290;&#23427;&#36890;&#36807;&#25972;&#21512;&#22810;&#20010;&#22522;&#26412;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#25552;&#20379;&#20102;&#23545;&#21487;&#20449;&#21644;&#19981;&#21487;&#20449;&#20449;&#24687;&#28304;&#30340;&#20934;&#30830;&#21306;&#20998;&#65292;&#24182;&#22312;&#20449;&#24687;&#21644;&#26032;&#38395;&#21487;&#20449;&#24230;&#35780;&#20272;&#26041;&#38754;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20449;&#24687;&#27867;&#28389;&#30340;&#26102;&#20195;&#65292;&#36776;&#21035;&#26032;&#38395;&#20869;&#23481;&#30340;&#21487;&#20449;&#24230;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;RELIANCE&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;&#40065;&#26834;&#20449;&#24687;&#21644;&#34394;&#20551;&#26032;&#38395;&#21487;&#20449;&#24230;&#35780;&#20272;&#32780;&#35774;&#35745;&#30340;&#20808;&#36827;&#30340;&#38598;&#25104;&#23398;&#20064;&#31995;&#32479;&#12290;RELIANCE&#30001;&#20116;&#20010;&#19981;&#21516;&#30340;&#22522;&#26412;&#27169;&#22411;&#32452;&#25104;&#65292;&#21253;&#25324;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#12289;&#26420;&#32032;&#36125;&#21494;&#26031;&#12289;&#36923;&#36753;&#22238;&#24402;&#12289;&#38543;&#26426;&#26862;&#26519;&#21644;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;BiLSTMs&#65289;&#12290;RELIANCE&#37319;&#29992;&#20102;&#21019;&#26032;&#30340;&#26041;&#27861;&#26469;&#25972;&#21512;&#23427;&#20204;&#30340;&#20248;&#21183;&#65292;&#21033;&#29992;&#38598;&#25104;&#30340;&#26234;&#33021;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;RELIANCE&#22312;&#21306;&#20998;&#21487;&#20449;&#21644;&#19981;&#21487;&#20449;&#20449;&#24687;&#28304;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#65292;&#34920;&#26126;&#20854;&#22312;&#20449;&#24687;&#21644;&#26032;&#38395;&#21487;&#20449;&#24230;&#35780;&#20272;&#26041;&#38754;&#36229;&#36807;&#20102;&#21333;&#20010;&#27169;&#22411;&#65292;&#24182;&#25104;&#20026;&#35780;&#20272;&#20449;&#24687;&#28304;&#21487;&#38752;&#24615;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of information proliferation, discerning the credibility of news content poses an ever-growing challenge. This paper introduces RELIANCE, a pioneering ensemble learning system designed for robust information and fake news credibility evaluation. Comprising five diverse base models, including Support Vector Machine (SVM), naive Bayes, logistic regression, random forest, and Bidirectional Long Short Term Memory Networks (BiLSTMs), RELIANCE employs an innovative approach to integrate their strengths, harnessing the collective intelligence of the ensemble for enhanced accuracy. Experiments demonstrate the superiority of RELIANCE over individual models, indicating its efficacy in distinguishing between credible and non-credible information sources. RELIANCE, also surpasses baseline models in information and news credibility assessment, establishing itself as an effective solution for evaluating the reliability of information sources.
&lt;/p&gt;</description></item><item><title>&#26500;&#24314;&#20102;&#19968;&#20010;&#38754;&#21521;&#25361;&#25112;&#23548;&#21521;&#30340;&#26234;&#33021;&#19987;&#19994;&#21270;&#30417;&#27979;&#24179;&#21488;RIS3-MCAT&#65292;&#21033;&#29992;&#24320;&#25918;&#25968;&#25454;&#12289;&#35821;&#20041;&#20998;&#26512;&#21644;&#25968;&#25454;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#20197;&#30417;&#27979;&#25361;&#25112;&#23548;&#21521;&#26234;&#33021;&#19987;&#19994;&#21270;&#30340;&#21152;&#27888;&#32599;&#23612;&#20122;&#20026;&#20363;&#65292;&#23454;&#29616;&#20102;&#23545;&#22823;&#37327;&#25991;&#26412;&#36827;&#34892;&#35814;&#32454;&#30740;&#31350;&#30340;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#12290;</title><link>http://arxiv.org/abs/2401.10900</link><description>&lt;p&gt;
&#26500;&#24314;&#19968;&#20010;&#38754;&#21521;&#25361;&#25112;&#23548;&#21521;&#30340;&#26234;&#33021;&#19987;&#19994;&#21270;&#30417;&#27979;&#24179;&#21488;RIS3-MCAT
&lt;/p&gt;
&lt;p&gt;
Towards building a monitoring platform for a challenge-oriented smart specialisation with RIS3-MCAT. (arXiv:2401.10900v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10900
&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#20102;&#19968;&#20010;&#38754;&#21521;&#25361;&#25112;&#23548;&#21521;&#30340;&#26234;&#33021;&#19987;&#19994;&#21270;&#30417;&#27979;&#24179;&#21488;RIS3-MCAT&#65292;&#21033;&#29992;&#24320;&#25918;&#25968;&#25454;&#12289;&#35821;&#20041;&#20998;&#26512;&#21644;&#25968;&#25454;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#20197;&#30417;&#27979;&#25361;&#25112;&#23548;&#21521;&#26234;&#33021;&#19987;&#19994;&#21270;&#30340;&#21152;&#27888;&#32599;&#23612;&#20122;&#20026;&#20363;&#65292;&#23454;&#29616;&#20102;&#23545;&#22823;&#37327;&#25991;&#26412;&#36827;&#34892;&#35814;&#32454;&#30740;&#31350;&#30340;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26032;&#30340;&#30740;&#31350;&#19982;&#21019;&#26032;&#65288;R&amp;I&#65289;&#33539;&#24335;&#20013;&#65292;&#20026;&#20102;&#35299;&#20915;&#31038;&#20250;&#21644;&#29615;&#22659;&#25361;&#25112;&#12289;&#29983;&#25104;&#26032;&#30340;&#19987;&#19994;&#21270;&#27169;&#24335;&#21644;&#31038;&#20250;&#32463;&#27982;&#21457;&#23637;&#36712;&#36857;&#65292;&#25552;&#20379;&#30417;&#27979;&#31995;&#32479;&#21644;&#24037;&#20855;&#20197;&#21450;&#23545; R&amp;I &#25919;&#31574;&#21644;&#39033;&#30446;&#30340;&#36129;&#29486;&#36827;&#34892;&#26144;&#23556;&#21644;&#29702;&#35299;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#36716;&#21464;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; RIS3-MCAT &#24179;&#21488;&#65292;&#23427;&#26159;&#25506;&#32034;&#24320;&#25918;&#25968;&#25454;&#12289;&#35821;&#20041;&#20998;&#26512;&#21644;&#25968;&#25454;&#21487;&#35270;&#21270;&#28508;&#21147;&#30340;&#19968;&#39033;&#24037;&#20316;&#30340;&#32467;&#26524;&#65292;&#29992;&#20110;&#30417;&#27979;&#21152;&#27888;&#32599;&#23612;&#20122;&#30340;&#25361;&#25112;&#23548;&#21521;&#26234;&#33021;&#19987;&#19994;&#21270;&#12290;RIS3-MCAT &#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#24179;&#21488;&#65292;&#21487;&#20197;&#20197;&#25903;&#25345;&#22797;&#26434;&#25991;&#26412;&#30340;&#35814;&#32454;&#30740;&#31350;&#20026;&#29305;&#33394;&#65292;&#20351;&#24471;&#35775;&#38382; R&amp;I &#39033;&#30446;&#25968;&#25454;&#21464;&#24471;&#26356;&#23481;&#26131;&#65292;&#21487;&#20197;&#36229;&#36234;&#32463;&#20856;&#30340;&#20998;&#31867;&#31995;&#32479;&#65292;&#23545;&#20027;&#39064;&#19987;&#19994;&#21270;&#21644;&#25361;&#25112;&#36827;&#34892;&#28145;&#20837;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the new research and innovation (R&amp;I) paradigm, aimed at a transformation towards more sustainable, inclusive and fair pathways to address societal and environmental challenges, and at generating new patterns of specialisation and new trajectories for socioeconomic development, it is essential to provide monitoring systems and tools to map and understand the contribution of R&amp;I policies and projects. To address this transformation, we present the RIS3-MCAT platform, the result of a line of work aimed at exploring the potential of open data, semantic analysis, and data visualisation, for monitoring challenge-oriented smart specialisation in Catalonia. RIS3-MCAT is an interactive platform that facilitates access to R&amp;I project data in formats that allow for sophisticated analyses of a large volume of texts, enabling the detailed study of thematic specialisations and challenges beyond classical classification systems. Its conceptualisation, development framework and use are presented i
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20301;&#32622;&#25935;&#24863;&#23884;&#20837;&#65288;LSE&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20851;&#31995;&#29305;&#23450;&#30340;&#26144;&#23556;&#26469;&#20462;&#25913;&#22836;&#23454;&#20307;&#65292;&#23558;&#20851;&#31995;&#27010;&#24565;&#21270;&#20026;&#32447;&#24615;&#21464;&#25442;&#12290;LSE&#22312;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#39046;&#22495;&#20855;&#26377;&#29702;&#35770;&#22522;&#30784;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26356;&#39640;&#25928;&#30340;&#21464;&#20307;LSEd&#12290;&#23454;&#39564;&#35777;&#26126;LSEd&#22312;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.10893</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#20301;&#32622;&#25935;&#24863;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Location Sensitive Embedding for Knowledge Graph Embedding. (arXiv:2401.10893v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10893
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20301;&#32622;&#25935;&#24863;&#23884;&#20837;&#65288;LSE&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20851;&#31995;&#29305;&#23450;&#30340;&#26144;&#23556;&#26469;&#20462;&#25913;&#22836;&#23454;&#20307;&#65292;&#23558;&#20851;&#31995;&#27010;&#24565;&#21270;&#20026;&#32447;&#24615;&#21464;&#25442;&#12290;LSE&#22312;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#39046;&#22495;&#20855;&#26377;&#29702;&#35770;&#22522;&#30784;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26356;&#39640;&#25928;&#30340;&#21464;&#20307;LSEd&#12290;&#23454;&#39564;&#35777;&#26126;LSEd&#22312;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#23558;&#30693;&#35782;&#22270;&#35889;&#36716;&#21270;&#20026;&#36830;&#32493;&#30340;&#12289;&#20302;&#32500;&#24230;&#30340;&#31354;&#38388;&#65292;&#26377;&#21161;&#20110;&#25512;&#29702;&#21644;&#34917;&#20840;&#20219;&#21153;&#12290;&#35813;&#39046;&#22495;&#20027;&#35201;&#20998;&#20026;&#20256;&#32479;&#30340;&#36317;&#31163;&#27169;&#22411;&#21644;&#35821;&#20041;&#21305;&#37197;&#27169;&#22411;&#12290;&#20256;&#32479;&#30340;&#36317;&#31163;&#27169;&#22411;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#26159;&#26080;&#27861;&#26377;&#25928;&#21306;&#20998;&#22270;&#35889;&#20013;&#30340;&#8220;&#22836;&#23454;&#20307;&#8221;&#21644;&#8220;&#23614;&#23454;&#20307;&#8221;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#20301;&#32622;&#25935;&#24863;&#23884;&#20837;&#65288;LSE&#65289;&#26041;&#27861;&#12290;LSE&#36890;&#36807;&#20851;&#31995;&#29305;&#23450;&#30340;&#26144;&#23556;&#20462;&#25913;&#22836;&#23454;&#20307;&#65292;&#23558;&#20851;&#31995;&#27010;&#24565;&#21270;&#20026;&#32447;&#24615;&#21464;&#25442;&#32780;&#19981;&#20165;&#20165;&#26159;&#24179;&#31227;&#12290;LSE&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#21253;&#25324;&#20854;&#34920;&#31034;&#33021;&#21147;&#21644;&#19982;&#29616;&#26377;&#27169;&#22411;&#30340;&#32852;&#31995;&#65292;&#37117;&#36827;&#34892;&#20102;&#35814;&#32454;&#30740;&#31350;&#12290;&#19968;&#31181;&#26356;&#31616;&#21270;&#30340;&#21464;&#20307;LSEd&#21033;&#29992;&#23545;&#35282;&#30697;&#38453;&#36827;&#34892;&#21464;&#25442;&#20197;&#25552;&#39640;&#23454;&#29992;&#24615;&#33021;&#12290;&#22312;&#23545;&#22235;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#30340;&#27979;&#35797;&#20013;&#65292;LSEd&#35201;&#20040;&#34920;&#29616;&#26356;&#22909;&#65292;&#35201;&#20040;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph embedding transforms knowledge graphs into a continuous, low-dimensional space, facilitating inference and completion tasks. This field is mainly divided into translational distance models and semantic matching models. A key challenge in translational distance models is their inability to effectively differentiate between 'head' and 'tail' entities in graphs. To address this, the novel location-sensitive embedding (LSE) method has been developed. LSE innovatively modifies the head entity using relation-specific mappings, conceptualizing relations as linear transformations rather than mere translations. The theoretical foundations of LSE, including its representational capabilities and its connections to existing models, have been thoroughly examined. A more streamlined variant, LSEd, employs a diagonal matrix for transformations to enhance practical efficiency. In tests conducted on four large-scale datasets for link prediction, LSEd either outperforms or is competitive
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#34701;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#29616;&#26377;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#21512;&#24182;&#20026;&#19968;&#20010;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#30446;&#26631;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#39564;&#35777;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10491</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Knowledge Fusion of Large Language Models. (arXiv:2401.10491v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#34701;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#29616;&#26377;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#21512;&#24182;&#20026;&#19968;&#20010;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#30446;&#26631;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#39564;&#35777;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#29420;&#29305;&#21151;&#33021;&#21644;&#20248;&#21183;&#30340;&#27169;&#22411;&#65292;&#20294;&#36825;&#23558;&#24102;&#26469;&#24040;&#22823;&#30340;&#25104;&#26412;&#65292;&#24182;&#21487;&#33021;&#23548;&#33268;&#20887;&#20313;&#30340;&#33021;&#21147;&#12290;&#30456;&#21453;&#65292;&#19968;&#31181;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#21644;&#24378;&#22823;&#21151;&#33021;&#30340;&#26041;&#27861;&#26159;&#23558;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;LLMs&#21512;&#24182;&#20026;&#19968;&#20010;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20123;LLMs&#30340;&#19981;&#21516;&#26550;&#26500;&#65292;&#30452;&#25509;&#28151;&#21512;&#23427;&#20204;&#30340;&#26435;&#37325;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LLMs&#30340;&#30693;&#35782;&#34701;&#21512;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#23558;&#29616;&#26377;LLMs&#30340;&#33021;&#21147;&#32467;&#21512;&#36215;&#26469;&#24182;&#36716;&#31227;&#21040;&#21333;&#20010;LLM&#20013;&#12290;&#36890;&#36807;&#21033;&#29992;&#28304;LLMs&#30340;&#29983;&#25104;&#20998;&#24067;&#65292;&#25105;&#20204;&#22806;&#37096;&#21270;&#23427;&#20204;&#30340;&#38598;&#20307;&#30693;&#35782;&#21644;&#29420;&#29305;&#20248;&#21183;&#65292;&#20174;&#32780;&#21487;&#33021;&#25552;&#39640;&#30446;&#26631;&#27169;&#22411;&#30340;&#33021;&#21147;&#36229;&#36807;&#20219;&#20309;&#21333;&#20010;&#28304;LLM&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#20855;&#26377;&#19981;&#21516;&#26550;&#26500;&#30340;&#19977;&#20010;&#27969;&#34892;LLMs &#8212;&#8212; Llama-2&#12289;MPT&#21644;OpenLLaMA&#22312;&#21508;&#31181;&#22522;&#20934;&#21644;&#20219;&#21153;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#35777;&#23454;&#20102;&#34701;&#21512;&#36825;&#19977;&#20010;LLMs&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
While training large language models (LLMs) from scratch can generate models with distinct functionalities and strengths, it comes at significant costs and may result in redundant capabilities. Alternatively, a cost-effective and compelling approach is to merge existing pre-trained LLMs into a more potent model. However, due to the varying architectures of these LLMs, directly blending their weights is impractical. In this paper, we introduce the notion of knowledge fusion for LLMs, aimed at combining the capabilities of existing LLMs and transferring them into a single LLM. By leveraging the generative distributions of source LLMs, we externalize their collective knowledge and unique strengths, thereby potentially elevating the capabilities of the target model beyond those of any individual source LLM. We validate our approach using three popular LLMs with different architectures--Llama-2, MPT, and OpenLLaMA--across various benchmarks and tasks. Our findings confirm that the fusion of
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#30340;&#20302;&#36164;&#28304;&#23433;&#20840;&#25915;&#20987;&#27169;&#24335;&#35782;&#21035;&#21305;&#37197;&#26694;&#26550;&#65292;&#36890;&#36807;&#30452;&#25509;&#35821;&#20041;&#30456;&#20284;&#24230;&#20915;&#23450;&#25991;&#26412;&#19982;&#25915;&#20987;&#27169;&#24335;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#20197;&#38477;&#20302;&#22823;&#37327;&#31867;&#21035;&#12289;&#26631;&#31614;&#20998;&#24067;&#19981;&#22343;&#21644;&#26631;&#31614;&#31354;&#38388;&#22797;&#26434;&#24615;&#24102;&#26469;&#30340;&#23398;&#20064;&#38590;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.10337</link><description>&lt;p&gt;
&#22522;&#20110;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#30340;&#20302;&#36164;&#28304;&#23433;&#20840;&#25915;&#20987;&#27169;&#24335;&#35782;&#21035;&#21305;&#37197;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Noise Contrastive Estimation-based Matching Framework for Low-resource Security Attack Pattern Recognition. (arXiv:2401.10337v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10337
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#30340;&#20302;&#36164;&#28304;&#23433;&#20840;&#25915;&#20987;&#27169;&#24335;&#35782;&#21035;&#21305;&#37197;&#26694;&#26550;&#65292;&#36890;&#36807;&#30452;&#25509;&#35821;&#20041;&#30456;&#20284;&#24230;&#20915;&#23450;&#25991;&#26412;&#19982;&#25915;&#20987;&#27169;&#24335;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#20197;&#38477;&#20302;&#22823;&#37327;&#31867;&#21035;&#12289;&#26631;&#31614;&#20998;&#24067;&#19981;&#22343;&#21644;&#26631;&#31614;&#31354;&#38388;&#22797;&#26434;&#24615;&#24102;&#26469;&#30340;&#23398;&#20064;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25112;&#26415;&#12289;&#25216;&#26415;&#21644;&#31243;&#24207;&#65288;TTPs&#65289;&#26159;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#22797;&#26434;&#30340;&#25915;&#20987;&#27169;&#24335;&#65292;&#22312;&#25991;&#26412;&#30693;&#35782;&#24211;&#20013;&#26377;&#35814;&#32454;&#30340;&#25551;&#36848;&#12290;&#22312;&#32593;&#32476;&#23433;&#20840;&#20889;&#20316;&#20013;&#35782;&#21035;TTPs&#65292;&#36890;&#24120;&#31216;&#20026;TTP&#26144;&#23556;&#65292;&#26159;&#19968;&#20010;&#37325;&#35201;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20256;&#32479;&#30340;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#20197;&#32463;&#20856;&#30340;&#22810;&#31867;&#25110;&#22810;&#26631;&#31614;&#20998;&#31867;&#35774;&#32622;&#20026;&#30446;&#26631;&#12290;&#30001;&#20110;&#23384;&#22312;&#22823;&#37327;&#30340;&#31867;&#21035;&#65288;&#21363;TTPs&#65289;&#65292;&#26631;&#31614;&#20998;&#24067;&#30340;&#19981;&#22343;&#34913;&#21644;&#26631;&#31614;&#31354;&#38388;&#30340;&#22797;&#26434;&#23618;&#27425;&#32467;&#26500;&#65292;&#36825;&#31181;&#35774;&#32622;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#23398;&#20064;&#33539;&#24335;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#23558;&#25991;&#26412;&#19982;TTP&#26631;&#31614;&#20043;&#38388;&#30340;&#30452;&#25509;&#35821;&#20041;&#30456;&#20284;&#24230;&#20915;&#23450;&#20026;&#25991;&#26412;&#20998;&#37197;&#32473;TTP&#26631;&#31614;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#20165;&#20165;&#22312;&#22823;&#22411;&#26631;&#31614;&#31354;&#38388;&#19978;&#31454;&#20105;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#26377;&#25928;&#30340;&#22522;&#20110;&#37319;&#26679;&#30340;&#23398;&#20064;&#27604;&#36739;&#26426;&#21046;&#30340;&#31070;&#32463;&#21305;&#37197;&#26550;&#26500;&#65292;&#20419;&#36827;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tactics, Techniques and Procedures (TTPs) represent sophisticated attack patterns in the cybersecurity domain, described encyclopedically in textual knowledge bases. Identifying TTPs in cybersecurity writing, often called TTP mapping, is an important and challenging task. Conventional learning approaches often target the problem in the classical multi-class or multilabel classification setting. This setting hinders the learning ability of the model due to a large number of classes (i.e., TTPs), the inevitable skewness of the label distribution and the complex hierarchical structure of the label space. We formulate the problem in a different learning paradigm, where the assignment of a text to a TTP label is decided by the direct semantic similarity between the two, thus reducing the complexity of competing solely over the large labeling space. To that end, we propose a neural matching architecture with an effective sampling-based learn-to-compare mechanism, facilitating the learning pr
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Chem-FINESE&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#21270;&#23398;&#39046;&#22495;&#20013;&#32454;&#31890;&#24230;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#23454;&#20307;&#25552;&#21462;&#22120;&#21644;&#33258;&#25105;&#39564;&#35777;&#27169;&#22359;&#26469;&#20174;&#36755;&#20837;&#21477;&#23376;&#20013;&#25552;&#21462;&#21629;&#21517;&#23454;&#20307;&#24182;&#37325;&#26500;&#21407;&#22987;&#36755;&#20837;&#21477;&#23376;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10189</link><description>&lt;p&gt;
Chem-FINESE: &#36890;&#36807;&#25991;&#26412;&#37325;&#26500;&#39564;&#35777;&#32454;&#31890;&#24230;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Chem-FINESE: Validating Fine-Grained Few-shot Entity Extraction through Text Reconstruction. (arXiv:2401.10189v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10189
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Chem-FINESE&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#21270;&#23398;&#39046;&#22495;&#20013;&#32454;&#31890;&#24230;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#23454;&#20307;&#25552;&#21462;&#22120;&#21644;&#33258;&#25105;&#39564;&#35777;&#27169;&#22359;&#26469;&#20174;&#36755;&#20837;&#21477;&#23376;&#20013;&#25552;&#21462;&#21629;&#21517;&#23454;&#20307;&#24182;&#37325;&#26500;&#21407;&#22987;&#36755;&#20837;&#21477;&#23376;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21270;&#23398;&#39046;&#22495;&#20013;&#65292;&#32454;&#31890;&#24230;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;&#38754;&#20020;&#20004;&#20010;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#19982;&#19968;&#33324;&#39046;&#22495;&#30340;&#23454;&#20307;&#25552;&#21462;&#20219;&#21153;&#30456;&#27604;&#65292;&#21270;&#23398;&#35770;&#25991;&#20013;&#30340;&#21477;&#23376;&#36890;&#24120;&#21253;&#21547;&#26356;&#22810;&#30340;&#23454;&#20307;&#12290;&#27492;&#22806;&#65292;&#23454;&#20307;&#25552;&#21462;&#27169;&#22411;&#36890;&#24120;&#38590;&#20197;&#25552;&#21462;&#38271;&#23614;&#31867;&#22411;&#30340;&#23454;&#20307;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;&#26041;&#27861;Chem-FINESE&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;Chem-FINESE&#21253;&#21547;&#20004;&#20010;&#32452;&#20214;&#65306;&#19968;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#23454;&#20307;&#25552;&#21462;&#22120;&#29992;&#20110;&#20174;&#36755;&#20837;&#21477;&#23376;&#20013;&#25552;&#21462;&#21629;&#21517;&#23454;&#20307;&#65292;&#20197;&#21450;&#19968;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#33258;&#25105;&#39564;&#35777;&#27169;&#22359;&#29992;&#20110;&#20174;&#25552;&#21462;&#30340;&#23454;&#20307;&#20013;&#37325;&#26500;&#21407;&#22987;&#36755;&#20837;&#21477;&#23376;&#12290;&#21463;&#21040;&#19968;&#20010;&#22909;&#30340;&#23454;&#20307;&#25552;&#21462;&#31995;&#32479;&#38656;&#35201;&#24544;&#23454;&#25552;&#21462;&#23454;&#20307;&#30340;&#20107;&#23454;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26032;&#33258;&#25105;&#39564;&#35777;&#27169;&#22359;&#21033;&#29992;&#23454;&#20307;&#25552;&#21462;&#32467;&#26524;&#26469;&#37325;&#26500;&#21407;&#22987;&#36755;&#20837;&#21477;&#23376;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#25439;&#22833;&#26469;&#20943;&#23569;&#22312;&#25552;&#21462;&#36807;&#31243;&#20013;&#30340;&#36807;&#24230;&#22797;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-grained few-shot entity extraction in the chemical domain faces two unique challenges. First, compared with entity extraction tasks in the general domain, sentences from chemical papers usually contain more entities. Moreover, entity extraction models usually have difficulty extracting entities of long-tailed types. In this paper, we propose Chem-FINESE, a novel sequence-to-sequence (seq2seq) based few-shot entity extraction approach, to address these two challenges. Our Chem-FINESE has two components: a seq2seq entity extractor to extract named entities from the input sentence and a seq2seq self-validation module to reconstruct the original input sentence from extracted entities. Inspired by the fact that a good entity extraction system needs to extract entities faithfully, our new self-validation module leverages entity extraction results to reconstruct the original input sentence. Besides, we design a new contrastive loss to reduce excessive copying during the extraction proces
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;H-UDM&#30340;&#23618;&#27425;&#21270;&#21475;&#35821;&#28132;&#22622;&#24314;&#27169;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#35299;&#20915;&#21475;&#35821;&#28132;&#22622;&#36716;&#24405;&#21644;&#26816;&#27979;&#38382;&#39064;&#65292;&#24182;&#19988;&#28040;&#38500;&#20102;&#23545;&#22823;&#37327;&#25163;&#21160;&#27880;&#37322;&#30340;&#38656;&#27714;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#36716;&#24405;&#21644;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10015</link><description>&lt;p&gt;
&#21521;&#23618;&#27425;&#21270;&#21475;&#35821;&#28132;&#22622;&#24314;&#27169;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Towards Hierarchical Spoken Language Dysfluency Modeling. (arXiv:2401.10015v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;H-UDM&#30340;&#23618;&#27425;&#21270;&#21475;&#35821;&#28132;&#22622;&#24314;&#27169;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#35299;&#20915;&#21475;&#35821;&#28132;&#22622;&#36716;&#24405;&#21644;&#26816;&#27979;&#38382;&#39064;&#65292;&#24182;&#19988;&#28040;&#38500;&#20102;&#23545;&#22823;&#37327;&#25163;&#21160;&#27880;&#37322;&#30340;&#38656;&#27714;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#36716;&#24405;&#21644;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21475;&#35821;&#28132;&#22622;&#24314;&#27169;&#26159;&#35821;&#35328;&#27835;&#30103;&#21644;&#35821;&#35328;&#23398;&#20064;&#30340;&#29942;&#39048;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#26469;&#31995;&#32479;&#22320;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#23450;&#20041;&#21475;&#35821;&#28132;&#22622;&#21644;&#21475;&#35821;&#28132;&#22622;&#24314;&#27169;&#30340;&#27010;&#24565;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Hierarchical Unconstrained Dysfluency Modeling (H-UDM)&#30340;&#26041;&#27861;&#65292;&#26082;&#35299;&#20915;&#20102;&#21475;&#35821;&#28132;&#22622;&#36716;&#24405;&#38382;&#39064;&#65292;&#21448;&#35299;&#20915;&#20102;&#26816;&#27979;&#38382;&#39064;&#65292;&#28040;&#38500;&#20102;&#23545;&#22823;&#37327;&#25163;&#21160;&#27880;&#37322;&#30340;&#38656;&#27714;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;VCTK++&#30340;&#27169;&#25311;&#28132;&#22622;&#25968;&#25454;&#38598;&#65292;&#20197;&#22686;&#24378;H-UDM&#22312;&#38899;&#26631;&#36716;&#24405;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#36716;&#24405;&#21644;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech dysfluency modeling is the bottleneck for both speech therapy and language learning. However, there is no AI solution to systematically tackle this problem. We first propose to define the concept of dysfluent speech and dysfluent speech modeling. We then present Hierarchical Unconstrained Dysfluency Modeling (H-UDM) approach that addresses both dysfluency transcription and detection to eliminate the need for extensive manual annotation. Furthermore, we introduce a simulated dysfluent dataset called VCTK++ to enhance the capabilities of H-UDM in phonetic transcription. Our experimental results demonstrate the effectiveness and robustness of our proposed methods in both transcription and detection tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#40657;&#30418;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#36234;&#29425;&#25915;&#20987;&#25552;&#31034;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#22797;&#26434;&#24615;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#38480;&#21046;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#33258;&#36523;&#65292;&#23558;&#26377;&#23475;&#25552;&#31034;&#37325;&#20889;&#20026;&#38750;&#26377;&#23475;&#34920;&#36798;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;80%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#24182;&#19988;&#21363;&#20351;&#27169;&#22411;&#26356;&#26032;&#65292;&#25928;&#26524;&#20173;&#28982;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2401.09798</link><description>&lt;p&gt;
&#19968;&#31181;&#31616;&#21333;&#30340;&#40657;&#30418;&#26041;&#27861;&#29992;&#20110;&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
All in How You Ask for It: Simple Black-Box Method for Jailbreak Attacks. (arXiv:2401.09798v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#40657;&#30418;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#36234;&#29425;&#25915;&#20987;&#25552;&#31034;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#22797;&#26434;&#24615;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#38480;&#21046;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#33258;&#36523;&#65292;&#23558;&#26377;&#23475;&#25552;&#31034;&#37325;&#20889;&#20026;&#38750;&#26377;&#23475;&#34920;&#36798;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;80%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#24182;&#19988;&#21363;&#20351;&#27169;&#22411;&#26356;&#26032;&#65292;&#25928;&#26524;&#20173;&#28982;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30528;&#8220;&#36234;&#29425;&#8221;&#25361;&#25112;&#65292;&#21363;&#35268;&#36991;&#20445;&#38556;&#25514;&#26045;&#20197;&#20135;&#29983;&#19981;&#31526;&#21512;&#20262;&#29702;&#30340;&#25552;&#31034;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#40657;&#30418;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#29983;&#25104;&#36234;&#29425;&#25552;&#31034;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#39640;&#22797;&#26434;&#24615;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#38480;&#21046;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#33258;&#36523;&#65292;&#36845;&#20195;&#22320;&#23558;&#26377;&#23475;&#25552;&#31034;&#37325;&#20889;&#20026;&#38750;&#26377;&#23475;&#34920;&#36798;&#65292;&#22522;&#20110;&#20551;&#35774;&#35748;&#20026;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#29983;&#25104;&#35268;&#36991;&#20445;&#38556;&#30340;&#34920;&#36798;&#12290;&#36890;&#36807;&#22312;ChatGPT&#65288;GPT-3.5&#21644;GPT-4&#65289;&#21644;Gemini-Pro&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#24179;&#22343;5&#27425;&#36845;&#20195;&#20869;&#23454;&#29616;&#20102;&#36229;&#36807;80%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#24182;&#19988;&#21363;&#20351;&#27169;&#22411;&#26356;&#26032;&#65292;&#25928;&#26524;&#20173;&#28982;&#26377;&#25928;&#12290;&#29983;&#25104;&#30340;&#36234;&#29425;&#25552;&#31034;&#33258;&#28982;&#32780;&#31616;&#32451;&#65292;&#34920;&#26126;&#23427;&#20204;&#36739;&#19981;&#26131;&#34987;&#26816;&#27979;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21019;&#24314;&#26377;&#25928;&#30340;&#36234;&#29425;&#25552;&#31034;&#27604;&#20808;&#21069;&#30740;&#31350;&#35748;&#20026;&#30340;&#35201;&#31616;&#21333;&#65292;&#24182;&#19988;&#40657;&#30418;&#36234;&#29425;&#25915;&#20987;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) like ChatGPT face `jailbreak' challenges, where safeguards are bypassed to produce ethically harmful prompts. This study introduces a simple black-box method to effectively generate jailbreak prompts, overcoming the limitations of high complexity and computational costs associated with existing methods. The proposed technique iteratively rewrites harmful prompts into non-harmful expressions using the target LLM itself, based on the hypothesis that LLMs can directly sample safeguard-bypassing expressions. Demonstrated through experiments with ChatGPT (GPT-3.5 and GPT-4) and Gemini-Pro, this method achieved an attack success rate of over 80% within an average of 5 iterations and remained effective despite model updates. The jailbreak prompts generated were naturally-worded and concise, suggesting they are less detectable. The results indicate that creating effective jailbreak prompts is simpler than previously considered, and black-box jailbreak attacks pose 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#36880;&#27493;&#21487;&#25512;&#24191;&#30340;&#20934;&#21017;&#65292;&#29992;&#20110;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#20013;&#35782;&#21035;&#21644;&#20998;&#31867;&#19981;&#21516;&#24418;&#24335;&#30340;&#31181;&#26063;&#20027;&#20041;&#35328;&#35770;&#12290;&#36890;&#36807;&#23545;&#31181;&#26063;&#20027;&#20041;&#30340;&#27010;&#24565;&#21270;&#21644;&#19978;&#19979;&#25991;&#21270;&#65292;&#20197;&#21450;&#20351;&#29992;XLM-R&#21644;XLM-R-Racismo&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#20013;&#36827;&#34892;&#31181;&#26063;&#20027;&#20041;&#20998;&#31867;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.09333</link><description>&lt;p&gt;
&#26426;&#22120;&#33021;&#22815;&#30475;&#21040;&#39068;&#33394;&#65306;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#20013;&#20998;&#31867;&#19981;&#21516;&#24418;&#24335;&#31181;&#26063;&#20027;&#20041;&#35328;&#35770;&#30340;&#20934;&#21017;
&lt;/p&gt;
&lt;p&gt;
Machines Do See Color: A Guideline to Classify Different Forms of Racist Discourse in Large Corpora. (arXiv:2401.09333v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#36880;&#27493;&#21487;&#25512;&#24191;&#30340;&#20934;&#21017;&#65292;&#29992;&#20110;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#20013;&#35782;&#21035;&#21644;&#20998;&#31867;&#19981;&#21516;&#24418;&#24335;&#30340;&#31181;&#26063;&#20027;&#20041;&#35328;&#35770;&#12290;&#36890;&#36807;&#23545;&#31181;&#26063;&#20027;&#20041;&#30340;&#27010;&#24565;&#21270;&#21644;&#19978;&#19979;&#25991;&#21270;&#65292;&#20197;&#21450;&#20351;&#29992;XLM-R&#21644;XLM-R-Racismo&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#20013;&#36827;&#34892;&#31181;&#26063;&#20027;&#20041;&#20998;&#31867;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#35782;&#21035;&#21644;&#20998;&#31867;&#25991;&#26412;&#20013;&#30340;&#31181;&#26063;&#20027;&#20041;&#35821;&#35328;&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#23567;&#35268;&#27169;&#30340;&#36136;&#24615;&#26041;&#27861;&#25110;&#22823;&#35268;&#27169;&#30340;&#26041;&#27861;&#65292;&#19987;&#27880;&#20110;&#26126;&#26174;&#30340;&#31181;&#26063;&#20027;&#20041;&#35328;&#35770;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#36880;&#27493;&#21487;&#25512;&#24191;&#30340;&#20934;&#21017;&#65292;&#29992;&#20110;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#20013;&#35782;&#21035;&#21644;&#20998;&#31867;&#19981;&#21516;&#24418;&#24335;&#30340;&#31181;&#26063;&#20027;&#20041;&#35328;&#35770;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#31181;&#26063;&#20027;&#20041;&#21450;&#20854;&#19981;&#21516;&#34920;&#29616;&#24418;&#24335;&#36827;&#34892;&#27010;&#24565;&#21270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#31181;&#26063;&#20027;&#20041;&#34920;&#29616;&#24418;&#24335;&#32622;&#20110;&#24863;&#20852;&#36259;&#30340;&#26102;&#38388;&#21644;&#22320;&#28857;&#32972;&#26223;&#19979;&#65292;&#20197;&#20415;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#35782;&#21035;&#23427;&#20204;&#30340;&#35805;&#35821;&#24418;&#24335;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;XLM-RoBERTa&#65288;XLM-R&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#20808;&#36827;&#19978;&#19979;&#25991;&#29702;&#35299;&#33021;&#21147;&#30340;&#36328;&#35821;&#35328;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;XLM-R&#21644;XLM-R-Racismo&#65288;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65289;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#20013;&#23545;&#31181;&#26063;&#20027;&#20041;&#36827;&#34892;&#20998;&#31867;&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#28041;&#21450;2018&#24180;&#33267;2021&#24180;&#21380;&#29916;&#22810;&#23572;&#26412;&#22303;&#32676;&#20307;&#30340;&#25512;&#25991;&#35821;&#26009;&#24211;&#26469;&#35828;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current methods to identify and classify racist language in text rely on small-n qualitative approaches or large-n approaches focusing exclusively on overt forms of racist discourse. This article provides a step-by-step generalizable guideline to identify and classify different forms of racist discourse in large corpora. In our approach, we start by conceptualizing racism and its different manifestations. We then contextualize these racist manifestations to the time and place of interest, which allows researchers to identify their discursive form. Finally, we apply XLM-RoBERTa (XLM-R), a cross-lingual model for supervised text classification with a cutting-edge contextual understanding of text. We show that XLM-R and XLM-R-Racismo, our pretrained model, outperform other state-of-the-art approaches in classifying racism in large corpora. We illustrate our approach using a corpus of tweets relating to the Ecuadorian ind\'igena community between 2018 and 2021.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27169;&#25311;&#35745;&#31639;&#26426;&#20195;&#30721;&#21644;&#31639;&#27861;&#25191;&#34892;&#26041;&#38754;&#36935;&#21040;&#25361;&#25112;&#65292;&#24615;&#33021;&#38543;&#30528;&#20195;&#30721;&#38271;&#24230;&#30340;&#22686;&#21152;&#32780;&#36805;&#36895;&#19979;&#38477;&#12290;&#22312;&#22788;&#29702;&#30701;&#31243;&#24207;&#25110;&#26631;&#20934;&#36807;&#31243;&#26102;&#65292;&#23427;&#20204;&#33021;&#20197;&#20302;&#38169;&#35823;&#29575;&#25353;&#39034;&#24207;&#25191;&#34892;&#25351;&#20196;&#65292;&#20294;&#23545;&#20110;&#22797;&#26434;&#30340;&#31243;&#24207;&#65292;&#29305;&#21035;&#26159;&#21253;&#21547;&#20851;&#38190;&#36335;&#24452;&#21644;&#20887;&#20313;&#25351;&#20196;&#30340;&#31243;&#24207;&#65292;&#27169;&#25311;&#25928;&#26524;&#36739;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36880;&#34892;&#27169;&#25311;&#20195;&#30721;&#25191;&#34892;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.09074</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20195;&#30721;&#27169;&#25311;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Code Simulation Challenges for Large Language Models. (arXiv:2401.09074v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09074
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27169;&#25311;&#35745;&#31639;&#26426;&#20195;&#30721;&#21644;&#31639;&#27861;&#25191;&#34892;&#26041;&#38754;&#36935;&#21040;&#25361;&#25112;&#65292;&#24615;&#33021;&#38543;&#30528;&#20195;&#30721;&#38271;&#24230;&#30340;&#22686;&#21152;&#32780;&#36805;&#36895;&#19979;&#38477;&#12290;&#22312;&#22788;&#29702;&#30701;&#31243;&#24207;&#25110;&#26631;&#20934;&#36807;&#31243;&#26102;&#65292;&#23427;&#20204;&#33021;&#20197;&#20302;&#38169;&#35823;&#29575;&#25353;&#39034;&#24207;&#25191;&#34892;&#25351;&#20196;&#65292;&#20294;&#23545;&#20110;&#22797;&#26434;&#30340;&#31243;&#24207;&#65292;&#29305;&#21035;&#26159;&#21253;&#21547;&#20851;&#38190;&#36335;&#24452;&#21644;&#20887;&#20313;&#25351;&#20196;&#30340;&#31243;&#24207;&#65292;&#27169;&#25311;&#25928;&#26524;&#36739;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36880;&#34892;&#27169;&#25311;&#20195;&#30721;&#25191;&#34892;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#27169;&#25311;&#35745;&#31639;&#26426;&#20195;&#30721;&#21644;&#31639;&#27861;&#25191;&#34892;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#30452;&#32447;&#31243;&#24207;&#65292;&#24182;&#23637;&#31034;&#20102;&#24403;&#21069;LLMs&#22312;&#22788;&#29702;&#36825;&#26679;&#31616;&#21333;&#30340;&#31243;&#24207;&#26102;&#34920;&#29616;&#20986;&#30340;&#24615;&#33021;&#36739;&#24046;&#8212;&#8212;&#24615;&#33021;&#38543;&#30528;&#20195;&#30721;&#38271;&#24230;&#30340;&#22686;&#21152;&#32780;&#36805;&#36895;&#19979;&#38477;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#22312;&#27169;&#25311;&#21253;&#21547;&#20851;&#38190;&#36335;&#24452;&#21644;&#20887;&#20313;&#25351;&#20196;&#30340;&#31243;&#24207;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#25490;&#24207;&#31639;&#27861;&#21644;&#23884;&#22871;&#24490;&#29615;&#36229;&#36234;&#20102;&#30452;&#32447;&#31243;&#24207;&#30340;&#27169;&#25311;&#65292;&#24182;&#23637;&#31034;&#20102;&#31243;&#24207;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#30452;&#25509;&#24433;&#21709;LLMs&#27169;&#25311;&#20854;&#25191;&#34892;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;LLMs&#21482;&#26377;&#22312;&#22788;&#29702;&#30701;&#31243;&#24207;&#25110;&#26631;&#20934;&#36807;&#31243;&#26102;&#25165;&#33021;&#20197;&#20302;&#38169;&#35823;&#29575;&#25353;&#39034;&#24207;&#25191;&#34892;&#25351;&#20196;&#12290;LLMs&#30340;&#20195;&#30721;&#27169;&#25311;&#19982;&#23427;&#20204;&#30340;&#27169;&#24335;&#35782;&#21035;&#21644;&#35760;&#24518;&#33021;&#21147;&#23384;&#22312;&#30683;&#30462;&#65306;&#22312;&#35760;&#24518;&#23545;&#20219;&#21153;&#26377;&#23475;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#36880;&#34892;&#27169;&#25311;&#20195;&#30721;&#30340;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the extent to which Large Language Models (LLMs) can simulate the execution of computer code and algorithms. We begin by looking straight line programs, and show that current LLMs demonstrate poor performance even with such simple programs -- performance rapidly degrades with the length of code. We then investigate the ability of LLMs to simulate programs that contain critical paths and redundant instructions. We also go beyond straight line program simulation with sorting algorithms and nested loops, and we show the computational complexity of a routine directly affects the ability of an LLM to simulate its execution. We observe that LLMs execute instructions sequentially and with a low error margin only for short programs or standard procedures. LLMs' code simulation is in tension with their pattern recognition and memorisation capabilities: on tasks where memorisation is detrimental, we propose a novel prompting method to simulate code execution line by line. Empirica
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;MMIQC&#25968;&#25454;&#38598;&#21644;&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;(IQC)&#30340;&#26032;&#39062;&#22686;&#24378;&#26041;&#27861;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#31454;&#36187;&#32423;&#25968;&#23398;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20808;&#21069;&#26368;&#20339;&#32467;&#26524;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.09003</link><description>&lt;p&gt;
&#36890;&#36807;&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;&#26469;&#22686;&#24378;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;
&lt;/p&gt;
&lt;p&gt;
Augmenting Math Word Problems via Iterative Question Composing. (arXiv:2401.09003v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;MMIQC&#25968;&#25454;&#38598;&#21644;&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;(IQC)&#30340;&#26032;&#39062;&#22686;&#24378;&#26041;&#27861;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#31454;&#36187;&#32423;&#25968;&#23398;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20808;&#21069;&#26368;&#20339;&#32467;&#26524;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#23450;&#36827;&#23637;&#65292;&#20294;&#22312;&#19981;&#20351;&#29992;&#22806;&#37096;&#24037;&#20855;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#31454;&#36187;&#32423;&#25968;&#23398;&#38382;&#39064;&#20173;&#28982;&#23545;&#24320;&#28304;LLMs&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MMIQC&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#28151;&#21512;&#22788;&#29702;&#30340;&#32593;&#32476;&#25968;&#25454;&#21644;&#21512;&#25104;&#38382;&#39064;-&#21709;&#24212;&#23545;&#30340;&#28151;&#21512;&#25968;&#25454;&#38598;&#65292;&#20197;&#25552;&#20379;&#22522;&#30784;&#27169;&#22411;&#26356;&#22909;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;MMIQC&#19978;&#23545;Mistral-7B(arXiv:2310.06825)&#36827;&#34892;&#24494;&#35843;&#33719;&#24471;&#30340;&#27169;&#22411;Mistral-7B-MMIQC&#65292;&#22312;MATH(arXiv:2103.03874)&#19978;&#36798;&#21040;&#20102;36.0%&#30340;&#20934;&#30830;&#29575;&#65292;&#27604;&#20043;&#21069;(model size $\sim$7B)&#30340;&#26368;&#20339;&#32467;&#26524;&#39640;&#20986;5.8%&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#36824;&#34920;&#26126;&#65292;&#25913;&#36827;&#30340;&#19968;&#20010;&#37325;&#35201;&#37096;&#20998;&#24402;&#21151;&#20110;&#25105;&#20204;&#30340;&#26032;&#39062;&#22686;&#24378;&#26041;&#27861;IQC(&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;)&#65292;&#20854;&#20013;&#25105;&#20204;&#36845;&#20195;&#22320;&#35201;&#27714;LLM&#20174;&#32473;&#23450;&#30340;&#31181;&#23376;&#38382;&#39064;&#20013;&#32452;&#21512;&#26032;&#38382;&#39064;&#65292;&#24182;&#20174;&#21478;&#19968;&#20010;LLM&#20013;&#36827;&#34892;&#25298;&#32477;&#25277;&#26679;&#12290;MMIQC&#29616;&#24050;&#22312;https://huggingface.co/datasets/Vivacem/MMIQC&#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent progress in improving the mathematical reasoning ability of large language models(LLMs), solving competition-level math problems without the use of external tools remains challenging for open-source LLMs. In this work, we introduce the MMIQC dataset, a mixture of processed web data and synthetic question-response pairs, to equip base models with better mathematical reasoning skills. Mistral-7B-MMIQC, the model obtained by fine-tuning Mistral-7B(arXiv:2310.06825) on MMIQC, achieves 36.0\% accuracy on MATH(arXiv:2103.03874), 5.8\% higher than the previous (model size $\sim$7B) SOTA. Our experiments also show that a large part of the improvement attributes to our novel augmentation method IQC(Iterative Question Composing), where we iteratively ask an LLM to compose new questions from the given seed problems and do rejection sampling from another LLM. MMIQC has now been released on https://huggingface.co/datasets/Vivacem/MMIQC.
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#29992;&#20110;&#29983;&#29289;&#23398;&#21644;&#21307;&#23398;&#30340;ChatGPT&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22810;&#27169;&#24577;&#33539;&#24335;&#65292;&#21152;&#36895;&#20102;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#30340;&#36827;&#23637;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#21307;&#23398;&#29615;&#22659;&#20013;&#30340;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#12289;&#26080;&#26631;&#31614;&#25968;&#25454;&#20998;&#26512;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2401.07510</link><description>&lt;p&gt;
&#24320;&#21457;&#29992;&#20110;&#29983;&#29289;&#23398;&#21644;&#21307;&#23398;&#30340;ChatGPT&#65306;&#29983;&#29289;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#30340;&#23436;&#25972;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Developing ChatGPT for Biology and Medicine: A Complete Review of Biomedical Question Answering. (arXiv:2401.07510v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07510
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#29992;&#20110;&#29983;&#29289;&#23398;&#21644;&#21307;&#23398;&#30340;ChatGPT&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22810;&#27169;&#24577;&#33539;&#24335;&#65292;&#21152;&#36895;&#20102;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#30340;&#36827;&#23637;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#21307;&#23398;&#29615;&#22659;&#20013;&#30340;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#12289;&#26080;&#26631;&#31614;&#25968;&#25454;&#20998;&#26512;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#22810;&#27169;&#24577;&#33539;&#24335;&#65292;&#36890;&#36807;&#22686;&#21152;&#21307;&#23398;&#39046;&#22495;&#25968;&#25454;&#30340;&#34701;&#20837;&#65292;&#25506;&#32034;&#20102;&#22312;&#25552;&#20379;&#21307;&#23398;&#35786;&#26029;&#12289;&#27835;&#30103;&#24314;&#35758;&#21644;&#20854;&#20182;&#21307;&#30103;&#25903;&#25345;&#26041;&#38754;&#30340;&#38382;&#31572;&#65288;QA&#65289;&#30340;&#25112;&#30053;&#34013;&#22270;&#12290;&#36890;&#36807;&#23558;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#20854;&#20182;&#27169;&#24577;&#20174;&#36890;&#29992;&#39046;&#22495;&#36716;&#21521;&#21307;&#23398;&#39046;&#22495;&#65292;&#36825;&#20123;&#25216;&#26415;&#21152;&#24555;&#20102;&#21307;&#23398;&#39046;&#22495;&#38382;&#39064;&#22238;&#31572;&#65288;MDQA&#65289;&#30340;&#36827;&#23637;&#12290;&#23427;&#20204;&#24357;&#21512;&#20102;&#20154;&#31867;&#33258;&#28982;&#35821;&#35328;&#21644;&#22797;&#26434;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#25110;&#19987;&#23478;&#25163;&#21160;&#27880;&#37322;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#22788;&#29702;&#20102;&#21307;&#23398;&#29615;&#22659;&#20013;&#30340;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#12289;&#19981;&#24179;&#34913;&#29978;&#33267;&#26080;&#26631;&#31614;&#25968;&#25454;&#20998;&#26512;&#22330;&#26223;&#12290;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#30340;&#26159;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#27169;&#24577;&#33539;&#24335;&#36827;&#34892;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#65292;&#26088;&#22312;&#25351;&#23548;&#30740;&#31350;&#30028;&#26681;&#25454;&#20854;&#29305;&#23450;&#30340;&#21307;&#23398;&#30740;&#31350;&#38656;&#27714;&#36873;&#25321;&#21512;&#36866;&#30340;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT explores a strategic blueprint of question answering (QA) in delivering medical diagnosis, treatment recommendations, and other healthcare support. This is achieved through the increasing incorporation of medical domain data via natural language processing (NLP) and multimodal paradigms. By transitioning the distribution of text, images, videos, and other modalities from the general domain to the medical domain, these techniques have expedited the progress of medical domain question answering (MDQA). They bridge the gap between human natural language and sophisticated medical domain knowledge or expert manual annotations, handling large-scale, diverse, unbalanced, or even unlabeled data analysis scenarios in medical contexts. Central to our focus is the utilizing of language models and multimodal paradigms for medical question answering, aiming to guide the research community in selecting appropriate mechanisms for their specific medical research requirements. Specialized tasks
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;&#33539;&#24335;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#28431;&#27934;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#27745;&#26579;&#31034;&#33539;&#19978;&#19979;&#25991;&#26469;&#25805;&#25511;&#27169;&#22411;&#34892;&#20026;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#24494;&#35843;&#12290;&#36825;&#39033;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;ICLAttack&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#27745;&#26579;&#31034;&#33539;&#26679;&#26412;&#21644;&#25552;&#31034;&#26469;&#20351;&#27169;&#22411;&#25353;&#29031;&#39044;&#23450;&#20041;&#30340;&#24847;&#22270;&#34892;&#20107;&#12290;</title><link>http://arxiv.org/abs/2401.05949</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36890;&#29992;&#28431;&#27934;&#65306;&#19978;&#19979;&#25991;&#23398;&#20064;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Universal Vulnerabilities in Large Language Models: In-context Learning Backdoor Attacks. (arXiv:2401.05949v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;&#33539;&#24335;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#28431;&#27934;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#27745;&#26579;&#31034;&#33539;&#19978;&#19979;&#25991;&#26469;&#25805;&#25511;&#27169;&#22411;&#34892;&#20026;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#24494;&#35843;&#12290;&#36825;&#39033;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;ICLAttack&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#27745;&#26579;&#31034;&#33539;&#26679;&#26412;&#21644;&#25552;&#31034;&#26469;&#20351;&#27169;&#22411;&#25353;&#29031;&#39044;&#23450;&#20041;&#30340;&#24847;&#22270;&#34892;&#20107;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#26159;&#19968;&#31181;&#22312;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20043;&#38388;&#24357;&#21512;&#24046;&#36317;&#30340;&#33539;&#24335;&#65292;&#22312;&#20960;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#39640;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#12290;&#19982;&#20256;&#32479;&#30340;&#24494;&#35843;&#26041;&#27861;&#19981;&#21516;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#22815;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#32780;&#26080;&#38656;&#26356;&#26032;&#20219;&#20309;&#21442;&#25968;&#12290;&#23613;&#31649;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#25915;&#20987;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23545;&#36825;&#19968;&#33539;&#24335;&#30340;&#23433;&#20840;&#24615;&#38382;&#39064;&#30340;&#20851;&#20999;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#27745;&#26579;&#31034;&#33539;&#19978;&#19979;&#25991;&#26469;&#25805;&#25511;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#32780;&#26080;&#38656;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;ICLAttack&#65292;&#38024;&#23545;&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#65306;&#27745;&#26579;&#31034;&#33539;&#26679;&#26412;&#21644;&#27745;&#26579;&#25552;&#31034;&#65292;&#21487;&#20197;&#20351;&#27169;&#22411;&#25353;&#29031;&#39044;&#23450;&#20041;&#30340;&#24847;&#22270;&#34892;&#20107;&#12290;ICLAttack&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning, a paradigm bridging the gap between pre-training and fine-tuning, has demonstrated high efficacy in several NLP tasks, especially in few-shot settings. Unlike traditional fine-tuning methods, in-context learning adapts pre-trained models to unseen tasks without updating any parameters. Despite being widely applied, in-context learning is vulnerable to malicious attacks. In this work, we raise security concerns regarding this paradigm. Our studies demonstrate that an attacker can manipulate the behavior of large language models by poisoning the demonstration context, without the need for fine-tuning the model. Specifically, we have designed a new backdoor attack method, named ICLAttack, to target large language models based on in-context learning. Our method encompasses two types of attacks: poisoning demonstration examples and poisoning prompts, which can make models behave in accordance with predefined intentions. ICLAttack does not require additional fine-tuning 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#25512;&#29702;&#27493;&#38271;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#22312;&#25552;&#31034;&#20013;&#22686;&#21152;&#25512;&#29702;&#27493;&#39588;&#33021;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#32780;&#20943;&#23569;&#25512;&#29702;&#27493;&#39588;&#21017;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.04925</link><description>&lt;p&gt;
&#25512;&#29702;&#27493;&#38271;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Impact of Reasoning Step Length on Large Language Models. (arXiv:2401.04925v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#25512;&#29702;&#27493;&#38271;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#22312;&#25552;&#31034;&#20013;&#22686;&#21152;&#25512;&#29702;&#27493;&#39588;&#33021;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#32780;&#20943;&#23569;&#25512;&#29702;&#27493;&#39588;&#21017;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24605;&#32500;&#38142;&#26465;&#65288;CoT&#65289;&#23545;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;CoT&#30340;&#26377;&#25928;&#24615;&#19982;&#25552;&#31034;&#20013;&#25512;&#29702;&#27493;&#39588;&#30340;&#38271;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#20173;&#28982;&#19981;&#20026;&#20154;&#25152;&#30693;&#12290;&#20026;&#20102;&#25581;&#31034;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20960;&#20010;&#23454;&#35777;&#23454;&#39564;&#26469;&#25506;&#32034;&#36825;&#20123;&#20851;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20123;&#23454;&#39564;&#65292;&#25193;&#23637;&#21644;&#21387;&#32553;CoT&#28436;&#31034;&#20013;&#30340;&#21512;&#29702;&#25512;&#29702;&#27493;&#39588;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#20182;&#22240;&#32032;&#19981;&#21464;&#12290;&#25105;&#20204;&#24471;&#20986;&#20102;&#20197;&#19979;&#20027;&#35201;&#21457;&#29616;&#12290;&#39318;&#20808;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25552;&#31034;&#20013;&#24310;&#38271;&#25512;&#29702;&#27493;&#39588;&#65292;&#21363;&#20351;&#27809;&#26377;&#21521;&#25552;&#31034;&#20013;&#28155;&#21152;&#26032;&#20449;&#24687;&#65292;&#20063;&#20250;&#26174;&#33879;&#25552;&#39640;LLM&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#30456;&#21453;&#65292;&#32553;&#30701;&#25512;&#29702;&#27493;&#39588;&#65292;&#21363;&#20351;&#20445;&#30041;&#20851;&#38190;&#20449;&#24687;&#65292;&#20063;&#20250;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#19968;&#21457;&#29616;&#31361;&#26174;&#20102;CoT&#25552;&#31034;&#20013;&#27493;&#39588;&#25968;&#37327;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#38469;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain of Thought (CoT) is significant in improving the reasoning abilities of large language models (LLMs). However, the correlation between the effectiveness of CoT and the length of reasoning steps in prompts remains largely unknown. To shed light on this, we have conducted several empirical experiments to explore the relations. Specifically, we design experiments that expand and compress the rationale reasoning steps within CoT demonstrations, while keeping all other factors constant. We have the following key findings. First, the results indicate that lengthening the reasoning steps in prompts, even without adding new information into the prompt, considerably enhances LLMs' reasoning abilities across multiple datasets. Alternatively, shortening the reasoning steps, even while preserving the key information, significantly diminishes the reasoning abilities of models. This finding highlights the importance of the number of steps in CoT prompts and provides practical guidance to make 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;EvolutionaryAgent&#30340;&#36827;&#21270;&#26694;&#26550;&#65292;&#23558;Agent&#23545;&#40784;&#36716;&#21270;&#20026;&#36866;&#32773;&#29983;&#23384;&#30340;&#28436;&#21270;&#21644;&#36873;&#25321;&#36807;&#31243;&#65292;&#22312;&#19981;&#26029;&#28436;&#21270;&#30340;&#31038;&#20250;&#35268;&#33539;&#20013;&#65292;&#19982;&#24403;&#21069;&#31038;&#20250;&#35268;&#33539;&#26356;&#22909;&#36866;&#24212;&#30340;Agent&#23558;&#20855;&#26377;&#26356;&#39640;&#30340;&#29983;&#23384;&#21644;&#20256;&#25773;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.04620</link><description>&lt;p&gt;
&#22312;&#19981;&#26029;&#28436;&#21270;&#30340;&#31038;&#20250;&#35268;&#33539;&#20013;&#30340;Agent&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Agent Alignment in Evolving Social Norms. (arXiv:2401.04620v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;EvolutionaryAgent&#30340;&#36827;&#21270;&#26694;&#26550;&#65292;&#23558;Agent&#23545;&#40784;&#36716;&#21270;&#20026;&#36866;&#32773;&#29983;&#23384;&#30340;&#28436;&#21270;&#21644;&#36873;&#25321;&#36807;&#31243;&#65292;&#22312;&#19981;&#26029;&#28436;&#21270;&#30340;&#31038;&#20250;&#35268;&#33539;&#20013;&#65292;&#19982;&#24403;&#21069;&#31038;&#20250;&#35268;&#33539;&#26356;&#22909;&#36866;&#24212;&#30340;Agent&#23558;&#20855;&#26377;&#26356;&#39640;&#30340;&#29983;&#23384;&#21644;&#20256;&#25773;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;Agent&#36234;&#26469;&#36234;&#22810;&#22320;&#28183;&#36879;&#21040;&#20154;&#31867;&#29983;&#20135;&#21644;&#29983;&#27963;&#30340;&#21508;&#20010;&#39046;&#22495;&#65292;&#20984;&#26174;&#20102;&#23558;&#20854;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#30340;&#37325;&#35201;&#24615;&#12290;&#30446;&#21069;AI&#31995;&#32479;&#30340;&#23545;&#40784;&#20027;&#35201;&#38598;&#20013;&#22312;&#36890;&#36807;&#20154;&#20026;&#24178;&#39044;&#23545;LLM&#36827;&#34892;&#34987;&#21160;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;Agent&#20855;&#26377;&#25509;&#21463;&#29615;&#22659;&#21453;&#39304;&#21644;&#33258;&#25105;&#36827;&#21270;&#31561;&#29305;&#24615;&#65292;&#20351;&#24471;LLM&#23545;&#40784;&#26041;&#27861;&#21464;&#24471;&#19981;&#36275;&#22815;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;EvolutionaryAgent&#30340;Agent&#36827;&#21270;&#21644;&#23545;&#40784;&#30340;&#36827;&#21270;&#26694;&#26550;&#65292;&#23558;Agent&#23545;&#40784;&#36716;&#21270;&#20026;&#36866;&#32773;&#29983;&#23384;&#30340;&#28436;&#21270;&#21644;&#36873;&#25321;&#36807;&#31243;&#12290;&#22312;&#31038;&#20250;&#35268;&#33539;&#19981;&#26029;&#28436;&#21270;&#30340;&#29615;&#22659;&#20013;&#65292;&#19982;&#24403;&#21069;&#31038;&#20250;&#35268;&#33539;&#26356;&#22909;&#36866;&#24212;&#30340;Agent&#23558;&#20855;&#26377;&#26356;&#39640;&#30340;&#29983;&#23384;&#21644;&#20256;&#25773;&#27010;&#29575;&#65292;&#32780;&#23545;&#40784;&#19981;&#36275;&#30340;Agent&#21017;&#36880;&#28176;&#20943;&#23569;&#12290;&#36890;&#36807;&#22810;&#20010;&#35282;&#24230;&#23545;&#19982;&#31038;&#20250;&#35268;&#33539;&#30456;&#23545;&#40784;&#30340;Agent&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Agents based on Large Language Models (LLMs) are increasingly permeating various domains of human production and life, highlighting the importance of aligning them with human values. The current alignment of AI systems primarily focuses on passively aligning LLMs through human intervention. However, agents possess characteristics like receiving environmental feedback and self-evolution, rendering the LLM alignment methods inadequate. In response, we propose an evolutionary framework for agent evolution and alignment, named EvolutionaryAgent, which transforms agent alignment into a process of evolution and selection under the principle of survival of the fittest. In an environment where social norms continuously evolve, agents better adapted to the current social norms will have a higher probability of survival and proliferation, while those inadequately aligned dwindle over time. Experimental results assessing the agents from multiple perspectives in aligning with social norms demonstr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#33539;&#24335;&#65292;&#36890;&#36807;&#25361;&#25112;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20803;&#25512;&#29702;&#65292;&#20174;&#32780;&#26377;&#25928;&#21306;&#20998;&#23427;&#20204;&#30340;&#35748;&#30693;&#33021;&#21147;&#12290;&#36825;&#19968;&#33539;&#24335;&#30340;&#37325;&#35201;&#24615;&#22312;&#20110;&#33021;&#22815;&#25581;&#31034;&#20986;&#20256;&#32479;&#22522;&#20934;&#27979;&#35797;&#26080;&#27861;&#21457;&#29616;&#30340;&#27169;&#22411;&#30340;&#28508;&#22312;&#35748;&#30693;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2312.17080</link><description>&lt;p&gt;
MR-GSM8K: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#30340;&#20803;&#25512;&#29702;&#38761;&#21629;
&lt;/p&gt;
&lt;p&gt;
MR-GSM8K: A Meta-Reasoning Revolution in Large Language Model Evaluation. (arXiv:2312.17080v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.17080
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#33539;&#24335;&#65292;&#36890;&#36807;&#25361;&#25112;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20803;&#25512;&#29702;&#65292;&#20174;&#32780;&#26377;&#25928;&#21306;&#20998;&#23427;&#20204;&#30340;&#35748;&#30693;&#33021;&#21147;&#12290;&#36825;&#19968;&#33539;&#24335;&#30340;&#37325;&#35201;&#24615;&#22312;&#20110;&#33021;&#22815;&#25581;&#31034;&#20986;&#20256;&#32479;&#22522;&#20934;&#27979;&#35797;&#26080;&#27861;&#21457;&#29616;&#30340;&#27169;&#22411;&#30340;&#28508;&#22312;&#35748;&#30693;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#33539;&#24335;&#65292;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36825;&#31181;&#33539;&#24335;&#25361;&#25112;&#23427;&#20204;&#20174;&#20107;&#20803;&#25512;&#29702;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#30340;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#22522;&#20934;&#20013;&#30340;&#20851;&#38190;&#32570;&#38519;&#65292;&#20256;&#32479;&#19978;&#29992;&#20110;&#35780;&#20272;&#26234;&#33021;&#20307;&#30340;&#35748;&#30693;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#33539;&#24335;&#23558;&#28966;&#28857;&#20174;&#20197;&#32467;&#26524;&#20026;&#23548;&#21521;&#30340;&#35780;&#20272;&#36716;&#31227;&#21040;&#20102;&#26356;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#26377;&#25928;&#22320;&#21306;&#20998;&#20102;&#27169;&#22411;&#20043;&#38388;&#30340;&#35748;&#30693;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;GPT-4 &#30340;&#24615;&#33021;&#36739; GPT3-5 &#25552;&#21319;&#20102;&#20116;&#20493;&#12290;&#36825;&#31181;&#26032;&#33539;&#24335;&#30340;&#37325;&#35201;&#24847;&#20041;&#22312;&#20110;&#23427;&#33021;&#22815;&#25581;&#31034;&#20986;&#24403;&#21069;&#22522;&#20934;&#27979;&#35797;&#65288;&#22914;GSM8K&#65289;&#26080;&#27861;&#21457;&#29616;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#35748;&#30693;&#32570;&#38519;&#65292;&#36825;&#26159;&#30001;&#20110;&#22522;&#20934;&#27979;&#35797;&#30340;&#39281;&#21644;&#24230;&#21644;&#23545;&#19981;&#21516;&#25512;&#29702;&#33021;&#21147;&#30340;&#26377;&#25928;&#21306;&#20998;&#19981;&#36275;&#12290;&#25105;&#20204;&#30340;&#32508;&#21512;&#20998;&#26512;&#21253;&#25324;&#20102;&#26469;&#33258;&#24320;&#28304;&#21644;&#38381;&#28304;&#31038;&#21306;&#30340;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#25968;&#23398;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#19968;&#20123;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#30693;&#33021;&#21147;&#30340;&#37325;&#35201;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce a novel evaluation paradigm for Large Language Models, one that challenges them to engage in meta-reasoning. This approach addresses critical shortcomings in existing math problem-solving benchmarks, traditionally used to evaluate the cognitive capabilities of agents. Our paradigm shifts the focus from result-oriented assessments, which often overlook the reasoning process, to a more holistic evaluation that effectively differentiates the cognitive capabilities among models. For example, in our benchmark, GPT-4 demonstrates a performance five times better than GPT3-5. The significance of this new paradigm lies in its ability to reveal potential cognitive deficiencies in LLMs that current benchmarks, such as GSM8K, fail to uncover due to their saturation and lack of effective differentiation among varying reasoning abilities. Our comprehensive analysis includes several state-of-the-art math models from both open-source and closed-source communities, uncovering
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#33258;&#21160;&#23545;&#35805;&#35780;&#20272;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#21457;&#29616;LLMs&#22312;&#23545;&#35805;&#21644;&#36716;&#21521;&#23618;&#38754;&#19978;&#20855;&#26377;&#22810;&#32500;&#35780;&#20272;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2312.15407</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#33258;&#21160;&#23545;&#35805;&#35780;&#20272;&#22120;&#30340;&#26377;&#25928;&#24615;&#32508;&#21512;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Analysis of the Effectiveness of Large Language Models as Automatic Dialogue Evaluators. (arXiv:2312.15407v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#33258;&#21160;&#23545;&#35805;&#35780;&#20272;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#21457;&#29616;LLMs&#22312;&#23545;&#35805;&#21644;&#36716;&#21521;&#23618;&#38754;&#19978;&#20855;&#26377;&#22810;&#32500;&#35780;&#20272;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35780;&#20272;&#26159;&#23545;&#35805;&#31995;&#32479;&#30740;&#31350;&#20013;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#21442;&#32771;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#25351;&#26631;&#36890;&#24120;&#19981;&#36866;&#29992;&#20110;&#23545;&#35805;&#35780;&#20272;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#21508;&#31181;&#29420;&#29305;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26080;&#21442;&#32771;&#25351;&#26631;&#65292;&#26356;&#31526;&#21512;&#20154;&#31867;&#35780;&#20272;&#30340;&#35201;&#27714;&#12290;&#20854;&#20013;&#29305;&#21035;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#23588;&#20854;&#26159;&#35843;&#25972;&#25351;&#20196;&#30340;&#21464;&#20307;&#65292;&#22914;ChatGPT&#65292;&#34987;&#35777;&#26126;&#26159;&#20154;&#31867;&#35780;&#21028;&#30340;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#21697;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20851;&#20110;&#21033;&#29992;LLMs&#36827;&#34892;&#33258;&#21160;&#23545;&#35805;&#35780;&#20272;&#30340;&#30740;&#31350;&#22312;&#20803;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#25968;&#37327;&#12289;&#35780;&#20272;&#26041;&#24335;&#12289;LLMs&#30340;&#35206;&#30422;&#33539;&#22260;&#31561;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#22240;&#27492;&#65292;LLMs&#30340;&#25928;&#26524;&#22914;&#20309;&#20173;&#28982;&#26410;&#23450;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23545;LLMs&#22312;&#33258;&#21160;&#23545;&#35805;&#35780;&#20272;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;30&#31181;&#26368;&#36817;&#20986;&#29616;&#30340;LLMs&#22312;&#23545;&#35805;&#21644;&#36716;&#21521;&#23618;&#38754;&#19978;&#30340;&#22810;&#32500;&#35780;&#20272;&#33021;&#21147;&#65292;&#20351;&#29992;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;12&#20010;&#20803;&#35780;&#20272;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic evaluation is an integral aspect of dialogue system research. The traditional reference-based NLG metrics are generally found to be unsuitable for dialogue assessment. Consequently, recent studies have suggested various unique, reference-free neural metrics that better align with human evaluations. Notably among them, large language models (LLMs), particularly the instruction-tuned variants like ChatGPT, are shown to be promising substitutes for human judges. Yet, existing works on utilizing LLMs for automatic dialogue evaluation are limited in their scope in terms of the number of meta-evaluation datasets, mode of evaluation, coverage of LLMs, etc. Hence, it remains inconclusive how effective these LLMs are. To this end, we conduct a comprehensive study on the application of LLMs for automatic dialogue evaluation. Specifically, we analyze the multi-dimensional evaluation capability of 30 recently emerged LLMs at both turn and dialogue levels, using a comprehensive set of 12 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#38544;&#21464;&#37327;&#30721;&#26412;&#23454;&#29616;&#28789;&#27963;&#30340;&#20027;&#39064;&#23548;&#21521;&#25991;&#26723;&#29983;&#25104;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21517;&#20026;TVQ-VAE&#30340;&#29983;&#25104;&#24335;&#20027;&#39064;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#25429;&#25417;&#20027;&#39064;&#19978;&#19979;&#25991;&#65292;&#24182;&#25903;&#25345;&#28789;&#27963;&#24418;&#24335;&#30340;&#25991;&#26723;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2312.11532</link><description>&lt;p&gt;
Topic-VQ-VAE: &#21033;&#29992;&#38544;&#21464;&#37327;&#30721;&#26412;&#23454;&#29616;&#28789;&#27963;&#30340;&#20027;&#39064;&#23548;&#21521;&#25991;&#26723;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Topic-VQ-VAE: Leveraging Latent Codebooks for Flexible Topic-Guided Document Generation. (arXiv:2312.11532v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11532
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#38544;&#21464;&#37327;&#30721;&#26412;&#23454;&#29616;&#28789;&#27963;&#30340;&#20027;&#39064;&#23548;&#21521;&#25991;&#26723;&#29983;&#25104;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21517;&#20026;TVQ-VAE&#30340;&#29983;&#25104;&#24335;&#20027;&#39064;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#25429;&#25417;&#20027;&#39064;&#19978;&#19979;&#25991;&#65292;&#24182;&#25903;&#25345;&#28789;&#27963;&#24418;&#24335;&#30340;&#25991;&#26723;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;Vector-Quantized Variational Auto-Encoder&#65288;VQ-VAE&#65289;&#20013;&#30340;&#38544;&#21464;&#37327;&#30721;&#26412;&#36827;&#34892;&#20027;&#39064;&#24314;&#27169;&#30340;&#26032;&#26041;&#27861;&#65292;&#31163;&#25955;&#22320;&#23553;&#35013;&#20102;&#39044;&#35757;&#32451;&#23884;&#20837;&#65288;&#20363;&#22914;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65289;&#30340;&#20016;&#23500;&#20449;&#24687;&#12290;&#26681;&#25454;&#23545;&#38544;&#21464;&#37327;&#30721;&#26412;&#21644;&#23884;&#20837;&#30340;&#26032;&#35299;&#37322;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#24335;&#20027;&#39064;&#27169;&#22411;&#65292;&#31216;&#20026;Topic-VQ-VAE&#65288;TVQ-VAE&#65289;&#65292;&#23427;&#21487;&#20197;&#21453;&#21521;&#29983;&#25104;&#19982;&#30456;&#24212;&#38544;&#21464;&#37327;&#30721;&#26412;&#30456;&#20851;&#30340;&#21407;&#22987;&#25991;&#26723;&#12290;TVQ-VAE&#21487;&#20197;&#36890;&#36807;&#21253;&#25324;&#20256;&#32479;&#30340;&#35789;&#34955;&#65288;BoW&#65289;&#20998;&#24067;&#21644;&#33258;&#22238;&#24402;&#22270;&#20687;&#29983;&#25104;&#22312;&#20869;&#30340;&#21508;&#31181;&#29983;&#25104;&#20998;&#24067;&#26469;&#21487;&#35270;&#21270;&#20027;&#39064;&#12290;&#25105;&#20204;&#22312;&#25991;&#26723;&#20998;&#26512;&#21644;&#22270;&#20687;&#29983;&#25104;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TVQ-VAE&#21487;&#20197;&#26377;&#25928;&#25429;&#25417;&#20027;&#39064;&#19978;&#19979;&#25991;&#65292;&#25581;&#31034;&#25968;&#25454;&#38598;&#30340;&#28508;&#22312;&#32467;&#26500;&#65292;&#24182;&#25903;&#25345;&#28789;&#27963;&#24418;&#24335;&#30340;&#25991;&#26723;&#29983;&#25104;&#12290;&#25152;&#25552;&#20986;&#30340;TVQ-VAE&#30340;&#23448;&#26041;&#23454;&#29616;&#21487;&#22312;https://github.com/clo&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel approach for topic modeling utilizing latent codebooks from Vector-Quantized Variational Auto-Encoder~(VQ-VAE), discretely encapsulating the rich information of the pre-trained embeddings such as the pre-trained language model. From the novel interpretation of the latent codebooks and embeddings as conceptual bag-of-words, we propose a new generative topic model called Topic-VQ-VAE~(TVQ-VAE) which inversely generates the original documents related to the respective latent codebook. The TVQ-VAE can visualize the topics with various generative distributions including the traditional BoW distribution and the autoregressive image generation. Our experimental results on document analysis and image generation demonstrate that TVQ-VAE effectively captures the topic context which reveals the underlying structures of the dataset and supports flexible forms of document generation. Official implementation of the proposed TVQ-VAE is available at https://github.com/clo
&lt;/p&gt;</description></item><item><title>&#36861;&#27714;&#26368;&#20248;&#32479;&#35745;&#27700;&#21360;&#25216;&#26415;&#12290;&#36890;&#36807;&#23558;&#32479;&#35745;&#27700;&#21360;&#25216;&#26415;&#35270;&#20026;&#20551;&#35774;&#26816;&#39564;&#38382;&#39064;&#24182;&#24341;&#20837;&#20266;&#38543;&#26426;&#29983;&#25104;&#22120;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#36755;&#20986;&#20196;&#29260;&#21644;&#25298;&#32477;&#21306;&#22495;&#30340;&#32806;&#21512;&#65292;&#23454;&#29616;&#20102;&#31532;&#19968;&#31867;&#38169;&#35823;&#21644;&#31532;&#20108;&#31867;&#38169;&#35823;&#20043;&#38388;&#30340;&#38750;&#24179;&#20961;&#26435;&#34913;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26368;&#32479;&#19968;&#26368;&#26377;&#21147;&#30340;&#27700;&#21360;&#21644;&#26368;&#23567;&#21270;&#31532;&#20108;&#31867;&#38169;&#35823;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#29420;&#31435;&#21516;&#20998;&#24067;&#20196;&#29260;&#25968;&#37327;&#30340;&#19978;&#19979;&#30028;&#65292;&#31361;&#26174;&#20102;&#25913;&#36827;&#30340;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#40065;&#26834;&#24615;&#27700;&#21360;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2312.07930</link><description>&lt;p&gt;
&#36861;&#27714;&#26368;&#20248;&#32479;&#35745;&#27700;&#21360;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Towards Optimal Statistical Watermarking. (arXiv:2312.07930v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.07930
&lt;/p&gt;
&lt;p&gt;
&#36861;&#27714;&#26368;&#20248;&#32479;&#35745;&#27700;&#21360;&#25216;&#26415;&#12290;&#36890;&#36807;&#23558;&#32479;&#35745;&#27700;&#21360;&#25216;&#26415;&#35270;&#20026;&#20551;&#35774;&#26816;&#39564;&#38382;&#39064;&#24182;&#24341;&#20837;&#20266;&#38543;&#26426;&#29983;&#25104;&#22120;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#36755;&#20986;&#20196;&#29260;&#21644;&#25298;&#32477;&#21306;&#22495;&#30340;&#32806;&#21512;&#65292;&#23454;&#29616;&#20102;&#31532;&#19968;&#31867;&#38169;&#35823;&#21644;&#31532;&#20108;&#31867;&#38169;&#35823;&#20043;&#38388;&#30340;&#38750;&#24179;&#20961;&#26435;&#34913;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26368;&#32479;&#19968;&#26368;&#26377;&#21147;&#30340;&#27700;&#21360;&#21644;&#26368;&#23567;&#21270;&#31532;&#20108;&#31867;&#38169;&#35823;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#29420;&#31435;&#21516;&#20998;&#24067;&#20196;&#29260;&#25968;&#37327;&#30340;&#19978;&#19979;&#30028;&#65292;&#31361;&#26174;&#20102;&#25913;&#36827;&#30340;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#40065;&#26834;&#24615;&#27700;&#21360;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#32479;&#35745;&#27700;&#21360;&#25216;&#26415;&#20316;&#20026;&#19968;&#20010;&#20551;&#35774;&#26816;&#39564;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#36825;&#26159;&#19968;&#20010;&#27867;&#21270;&#20102;&#25152;&#26377;&#20043;&#21069;&#32479;&#35745;&#27700;&#21360;&#26041;&#27861;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#26159;&#36890;&#36807;&#23454;&#36341;&#20013;&#30340;&#20266;&#38543;&#26426;&#29983;&#25104;&#22120;&#23454;&#29616;&#36755;&#20986;&#20196;&#29260;&#21644;&#25298;&#32477;&#21306;&#22495;&#30340;&#32806;&#21512;&#65292;&#20174;&#32780;&#20801;&#35768;&#22312;&#31532;&#19968;&#31867;&#38169;&#35823;&#21644;&#31532;&#20108;&#31867;&#38169;&#35823;&#20043;&#38388;&#36827;&#34892;&#38750;&#24179;&#20961;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#22312;&#19968;&#33324;&#30340;&#20551;&#35774;&#26816;&#39564;&#29615;&#22659;&#19979;&#34920;&#24449;&#20102;&#26368;&#32479;&#19968;&#26368;&#26377;&#21147;&#30340;&#27700;&#21360;&#20197;&#21450;&#22312;&#27169;&#22411;&#26080;&#20851;&#30340;&#29615;&#22659;&#20013;&#26368;&#23567;&#21270;&#31532;&#20108;&#31867;&#38169;&#35823;&#12290;&#22312;&#36755;&#20986;&#26159;$n$&#20010;&#20196;&#29260;&#30340;&#24120;&#35265;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23545;&#38656;&#35201;&#20445;&#35777;&#23567;&#30340;&#31532;&#19968;&#31867;&#21644;&#31532;&#20108;&#31867;&#38169;&#35823;&#30340;&#29420;&#31435;&#21516;&#20998;&#24067;&#20196;&#29260;&#25968;&#37327;&#24314;&#31435;&#20102;&#36817;&#20046;&#21305;&#37197;&#30340;&#19978;&#19979;&#30028;&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#20013;&#30340;$ h ^ {-2} $&#36895;&#29575;&#30456;&#27604;&#65292;&#25105;&#20204;&#30456;&#23545;&#20110;&#27599;&#20010;&#20196;&#29260;&#30340;&#24179;&#22343;&#29109;$h$&#30340;&#36895;&#29575;&#20026;$ \Theta(h ^ {-1} \log (1/h)) $&#65292;&#31361;&#26174;&#20102;&#25913;&#36827;&#30340;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#40065;&#26834;&#24615;&#27700;&#21360;&#38382;&#39064;&#65292;&#20854;&#20013;&#29992;&#25143;&#37117;&#26159;...
&lt;/p&gt;
&lt;p&gt;
We study statistical watermarking by formulating it as a hypothesis testing problem, a general framework which subsumes all previous statistical watermarking methods. Key to our formulation is a coupling of the output tokens and the rejection region, realized by pseudo-random generators in practice, that allows non-trivial trade-off between the Type I error and Type II error. We characterize the Uniformly Most Powerful (UMP) watermark in the general hypothesis testing setting and the minimax Type II error in the model-agnostic setting. In the common scenario where the output is a sequence of $n$ tokens, we establish nearly matching upper and lower bounds on the number of i.i.d. tokens required to guarantee small Type I and Type II errors. Our rate of $\Theta(h^{-1} \log (1/h))$ with respect to the average entropy per token $h$ highlights potentials for improvement from the rate of $h^{-2}$ in the previous works. Moreover, we formulate the robust watermarking problem where users are all
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21270;&#26029;&#35328;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#31934;&#30830;&#12289;&#35814;&#32454;&#30340;&#25945;&#32946;&#35299;&#37322;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#35299;&#37322;&#20934;&#30830;&#24615;&#19978;&#25552;&#21319;&#20102;15%&#65292;&#33719;&#24471;&#20102;&#25945;&#24072;&#35780;&#20272;&#20026;&#39640;&#36136;&#37327;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2312.03122</link><description>&lt;p&gt;
&#24378;&#21270;&#26029;&#35328;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25945;&#32946;&#35299;&#37322;&#30340;&#25945;&#23548;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Assertion Enhanced Few-Shot Learning: Instructive Technique for Large Language Models to Generate Educational Explanations. (arXiv:2312.03122v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.03122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21270;&#26029;&#35328;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#31934;&#30830;&#12289;&#35814;&#32454;&#30340;&#25945;&#32946;&#35299;&#37322;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#35299;&#37322;&#20934;&#30830;&#24615;&#19978;&#25552;&#21319;&#20102;15%&#65292;&#33719;&#24471;&#20102;&#25945;&#24072;&#35780;&#20272;&#20026;&#39640;&#36136;&#37327;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#25945;&#32946;&#32773;&#20855;&#22791;&#20174;&#23398;&#29983;&#20013;&#39044;&#27979;&#24182;&#23547;&#27714;&#25945;&#32946;&#35299;&#37322;&#30340;&#20869;&#22312;&#33021;&#21147;&#65292;&#24403;&#23398;&#29983;&#26080;&#27861;&#29420;&#31435;&#34920;&#36798;&#36825;&#20123;&#35299;&#37322;&#26102;&#65292;&#20182;&#20204;&#33021;&#22815;&#25552;&#20986;&#21457;&#20154;&#28145;&#30465;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#20026;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#36171;&#20104;&#36825;&#31181;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25945;&#23548;&#25216;&#26415;&#65292;&#21363;&#24378;&#21270;&#26029;&#35328;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#20197;&#20419;&#36827;&#20934;&#30830;&#12289;&#35814;&#32454;&#30340;&#25945;&#32946;&#35299;&#37322;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#20551;&#35774;&#26159;&#65292;&#22312;&#25945;&#32946;&#39046;&#22495;&#65292;&#23569;&#26679;&#26412;&#28436;&#31034;&#26159;&#24517;&#35201;&#20294;&#19981;&#36275;&#20197;&#20445;&#35777;&#39640;&#36136;&#37327;&#30340;&#35299;&#37322;&#29983;&#25104;&#30340;&#26465;&#20214;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#28041;&#21450;12&#21517;&#22312;&#32844;&#25945;&#24072;&#30340;&#30740;&#31350;&#65292;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#24378;&#21270;&#26029;&#35328;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#23558;&#35299;&#37322;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;15%&#65292;&#24182;&#24471;&#21040;&#20102;&#25945;&#24072;&#35780;&#20272;&#20026;&#39640;&#36136;&#37327;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#23450;&#24615;&#30340;&#21076;&#38500;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human educators possess an intrinsic ability to anticipate and seek educational explanations from students, which drives them to pose thought-provoking questions when students cannot articulate these explanations independently. We aim to imbue Intelligent Tutoring Systems with this ability using few-shot learning capability of Large Language Models. Our work proposes a novel prompting technique, Assertion Enhanced Few-Shot Learning, to facilitate the generation of accurate, detailed oriented educational explanations. Our central hypothesis is that, in educational domain, few-shot demonstrations are necessary but not a sufficient condition for quality explanation generation. We conducted a study involving 12 in-service teachers, comparing our approach to Traditional Few-Shot Learning. The results show that Assertion Enhanced Few-Shot Learning improves explanation accuracy by 15% and yields higher-quality explanations, as evaluated by teachers. We also conduct a qualitative ablation stud
&lt;/p&gt;</description></item><item><title>GNN2R&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20004;&#27493;&#25512;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#24369;&#30417;&#30563;&#35757;&#32451;&#65292;&#33021;&#22815;&#22312;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#20013;&#25552;&#20379;&#26368;&#32456;&#31572;&#26696;&#20197;&#21450;&#25512;&#29702;&#23376;&#22270;&#30340;&#29702;&#30001;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#32570;&#20047;&#35299;&#37322;&#20197;&#21450;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2312.02317</link><description>&lt;p&gt;
GNN2R: &#22522;&#20110;&#24369;&#30417;&#30563;&#30340;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#20013;&#25552;&#20379;&#29702;&#30001;&#30340;&#38382;&#39064;&#22238;&#31572;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GNN2R: Weakly-Supervised Rationale-Providing Question Answering over Knowledge Graphs. (arXiv:2312.02317v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.02317
&lt;/p&gt;
&lt;p&gt;
GNN2R&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20004;&#27493;&#25512;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#24369;&#30417;&#30563;&#35757;&#32451;&#65292;&#33021;&#22815;&#22312;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#20013;&#25552;&#20379;&#26368;&#32456;&#31572;&#26696;&#20197;&#21450;&#25512;&#29702;&#23376;&#22270;&#30340;&#29702;&#30001;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#32570;&#20047;&#35299;&#37322;&#20197;&#21450;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#22823;&#22810;&#25968;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#26041;&#27861;&#21482;&#25552;&#20379;&#26368;&#32456;&#30340;&#30830;&#23450;&#31572;&#26696;&#65292;&#32780;&#27809;&#26377;&#35299;&#37322;&#65292;&#23545;&#20110;&#26222;&#36890;&#29992;&#25143;&#38590;&#20197;&#29702;&#35299;&#21644;&#26597;&#30475;&#30340;KG&#23454;&#20307;&#38598;&#12290;&#36825;&#20005;&#37325;&#38480;&#21046;&#20102;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20004;&#27493;&#25512;&#29702;&#27169;&#22411;&#65288;GNN2R&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;GNN2R&#33021;&#22815;&#36890;&#36807;&#20165;&#26377;&#30340;&#38382;&#39064;-&#26368;&#32456;&#31572;&#26696;&#23545;&#25552;&#20379;&#26368;&#32456;&#31572;&#26696;&#20197;&#21450;&#20316;&#20026;&#26368;&#32456;&#31572;&#26696;&#32972;&#21518;&#30340;&#25512;&#29702;&#23376;&#22270;&#30340;&#29702;&#30001;&#65292;&#19988;&#20165;&#38656;&#35201;&#36890;&#36807;&#24369;&#30417;&#30563;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#23545;GNN2R&#36827;&#34892;&#20102;&#22823;&#37327;&#35780;&#20272;&#65292;&#24182;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most current methods for multi-hop question answering (QA) over knowledge graphs (KGs) only provide final conclusive answers without explanations, such as a set of KG entities that is difficult for normal users to review and comprehend. This issue severely limits the application of KG-based QA in real-world scenarios. However, it is non-trivial to solve due to two challenges: First, annotations of reasoning chains of multi-hop questions, which could serve as supervision for explanation generation, are usually lacking. Second, it is difficult to maintain high efficiency when explicit KG triples need to be retrieved to generate explanations. In this paper, we propose a novel Graph Neural Network-based Two-Step Reasoning model (GNN2R) to solve this issue. GNN2R can provide both final answers and reasoning subgraphs as a rationale behind final answers efficiently with only weak supervision that is available through question-final answer pairs. We extensively evaluated GNN2R with detailed a
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21457;&#29616;&#35757;&#32451;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#23545;&#27880;&#37322;&#26412;&#36523;&#21644;&#19979;&#28216;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#24433;&#21709;&#12290;&#22312;&#23545;&#20167;&#24680;&#35328;&#35770;&#21644;&#20882;&#29359;&#24615;&#35821;&#35328;&#36827;&#34892;&#27880;&#37322;&#25910;&#38598;&#30340;&#23454;&#39564;&#20013;&#65292;&#21457;&#29616;&#27880;&#37322;&#24037;&#20855;&#30340;&#35774;&#35745;&#36873;&#25321;&#20250;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#26126;&#26174;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2311.14212</link><description>&lt;p&gt;
&#27880;&#37322;&#25935;&#24863;&#24615;&#65306;&#35757;&#32451;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Annotation Sensitivity: Training Data Collection Methods Affect Model Performance. (arXiv:2311.14212v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.14212
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21457;&#29616;&#35757;&#32451;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#23545;&#27880;&#37322;&#26412;&#36523;&#21644;&#19979;&#28216;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#24433;&#21709;&#12290;&#22312;&#23545;&#20167;&#24680;&#35328;&#35770;&#21644;&#20882;&#29359;&#24615;&#35821;&#35328;&#36827;&#34892;&#27880;&#37322;&#25910;&#38598;&#30340;&#23454;&#39564;&#20013;&#65292;&#21457;&#29616;&#27880;&#37322;&#24037;&#20855;&#30340;&#35774;&#35745;&#36873;&#25321;&#20250;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#26126;&#26174;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#35757;&#32451;&#25968;&#25454;&#30001;&#20154;&#24037;&#27880;&#37322;&#32773;&#25910;&#38598;&#26102;&#65292;&#27880;&#37322;&#24037;&#20855;&#30340;&#35774;&#35745;&#12289;&#32473;&#20104;&#27880;&#37322;&#32773;&#30340;&#25351;&#31034;&#12289;&#27880;&#37322;&#32773;&#30340;&#29305;&#24449;&#20197;&#21450;&#20182;&#20204;&#20043;&#38388;&#30340;&#20114;&#21160;&#37117;&#21487;&#33021;&#23545;&#35757;&#32451;&#25968;&#25454;&#20135;&#29983;&#24433;&#21709;&#12290;&#36825;&#39033;&#30740;&#31350;&#35777;&#26126;&#20102;&#21019;&#24314;&#27880;&#37322;&#24037;&#20855;&#26102;&#30340;&#35774;&#35745;&#36873;&#25321;&#20063;&#20250;&#24433;&#21709;&#22522;&#20110;&#24471;&#21040;&#30340;&#27880;&#37322;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;"&#27880;&#37322;&#25935;&#24863;&#24615;"&#36825;&#20010;&#26415;&#35821;&#65292;&#29992;&#26469;&#25351;&#20195;&#27880;&#37322;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#23545;&#27880;&#37322;&#26412;&#36523;&#20197;&#21450;&#19979;&#28216;&#27169;&#22411;&#24615;&#33021;&#21644;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#20116;&#31181;&#23454;&#39564;&#26465;&#20214;&#19979;&#23545;&#20167;&#24680;&#35328;&#35770;&#21644;&#20882;&#29359;&#24615;&#35821;&#35328;&#36827;&#34892;&#27880;&#37322;&#25910;&#38598;&#65292;&#38543;&#26426;&#23558;&#27880;&#37322;&#32773;&#20998;&#37197;&#21040;&#19981;&#21516;&#26465;&#20214;&#19979;&#12290;&#28982;&#21518;&#65292;&#22312;&#27599;&#20010;&#24471;&#21040;&#30340;&#20116;&#20010;&#25968;&#25454;&#38598;&#19978;&#23545;BERT&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#22312;&#27599;&#20010;&#26465;&#20214;&#30340;&#20445;&#30041;&#37096;&#20998;&#19978;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#20197;&#19979;&#26041;&#38754;&#26465;&#20214;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#65306;1&#65289;&#20167;&#24680;&#35328;&#35770;/&#20882;&#29359;&#24615;&#35821;&#35328;&#27880;&#37322;&#30340;&#27604;&#20363;&#65292;2&#65289;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
When training data are collected from human annotators, the design of the annotation instrument, the instructions given to annotators, the characteristics of the annotators, and their interactions can impact training data. This study demonstrates that design choices made when creating an annotation instrument also impact the models trained on the resulting annotations. We introduce the term annotation sensitivity to refer to the impact of annotation data collection methods on the annotations themselves and on downstream model performance and predictions. We collect annotations of hate speech and offensive language in five experimental conditions of an annotation instrument, randomly assigning annotators to conditions. We then fine-tune BERT models on each of the five resulting datasets and evaluate model performance on a holdout portion of each condition. We find considerable differences between the conditions for 1) the share of hate speech/offensive language annotations, 2) model per
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#20195;&#30721;&#22788;&#29702;&#26041;&#38754;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#28085;&#30422;&#20102;50&#22810;&#20010;&#27169;&#22411;&#12289;30&#22810;&#20010;&#35780;&#20272;&#20219;&#21153;&#12289;170&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;700&#22810;&#20010;&#30456;&#20851;&#24037;&#20316;&#12290;&#23427;&#31361;&#20986;&#20102;&#20195;&#30721;&#24314;&#27169;&#20174;&#32479;&#35745;&#27169;&#22411;&#21644;RNN&#21040;&#39044;&#35757;&#32451;&#30340;Transformer&#21644;LLM&#20043;&#38388;&#30340;&#21382;&#21490;&#36716;&#21464;&#65292;&#24182;&#35752;&#35770;&#20102;&#20195;&#30721;&#29305;&#23450;&#30340;&#29305;&#24615;&#21644;&#20851;&#38190;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2311.07989</link><description>&lt;p&gt;
&#32479;&#19968;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#36719;&#20214;&#24037;&#31243;&#35270;&#35282;&#65306;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Unifying the Perspectives of NLP and Software Engineering: A Survey on Language Models for Code. (arXiv:2311.07989v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.07989
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#20195;&#30721;&#22788;&#29702;&#26041;&#38754;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#28085;&#30422;&#20102;50&#22810;&#20010;&#27169;&#22411;&#12289;30&#22810;&#20010;&#35780;&#20272;&#20219;&#21153;&#12289;170&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;700&#22810;&#20010;&#30456;&#20851;&#24037;&#20316;&#12290;&#23427;&#31361;&#20986;&#20102;&#20195;&#30721;&#24314;&#27169;&#20174;&#32479;&#35745;&#27169;&#22411;&#21644;RNN&#21040;&#39044;&#35757;&#32451;&#30340;Transformer&#21644;LLM&#20043;&#38388;&#30340;&#21382;&#21490;&#36716;&#21464;&#65292;&#24182;&#35752;&#35770;&#20102;&#20195;&#30721;&#29305;&#23450;&#30340;&#29305;&#24615;&#21644;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#26368;&#36817;&#22312;&#20195;&#30721;&#22788;&#29702;&#20013;&#30340;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#28085;&#30422;&#20102;50&#22810;&#20010;&#27169;&#22411;&#12289;30&#22810;&#20010;&#35780;&#20272;&#20219;&#21153;&#12289;170&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;700&#22810;&#20010;&#30456;&#20851;&#24037;&#20316;&#12290;&#25105;&#20204;&#23558;&#20195;&#30721;&#22788;&#29702;&#27169;&#22411;&#20998;&#20026;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT&#31995;&#21015;&#65289;&#21644;&#19987;&#38376;&#22312;&#20195;&#30721;&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#36890;&#24120;&#20855;&#26377;&#19987;&#38376;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#20123;&#27169;&#22411;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#24046;&#24322;&#65292;&#24182;&#31361;&#20986;&#20102;&#20195;&#30721;&#24314;&#27169;&#20174;&#32479;&#35745;&#27169;&#22411;&#21644;RNN&#21040;&#39044;&#35757;&#32451;&#30340;Transformer&#21644;LLM&#20043;&#38388;&#30340;&#21382;&#21490;&#36716;&#21464;&#65292;&#36825;&#27491;&#26159;NLP&#20063;&#32463;&#21382;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#20195;&#30721;&#29305;&#23450;&#30340;&#29305;&#24615;&#65292;&#22914;AST&#12289;CFG&#21644;&#21333;&#20803;&#27979;&#35797;&#65292;&#20197;&#21450;&#23427;&#20204;&#22312;&#35757;&#32451;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#30830;&#23450;&#20102;&#35813;&#39046;&#22495;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#21644;&#28508;&#22312;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;&#25105;&#20204;&#23558;&#36825;&#20221;&#32508;&#36848;&#20445;&#25345;&#24320;&#25918;&#65292;&#24182;&#22312;GitHub&#19978;&#26356;&#26032;&#65292;&#32593;&#22336;&#20026;https://github.com/codefuse-ai/Awesome-Code-LLM&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we systematically review the recent advancements in code processing with language models, covering 50+ models, 30+ evaluation tasks, 170+ datasets, and 700+ related works. We break down code processing models into general language models represented by the GPT family and specialized models that are specifically pretrained on code, often with tailored objectives. We discuss the relations and differences between these models, and highlight the historical transition of code modeling from statistical models and RNNs to pretrained Transformers and LLMs, which is exactly the same course that had been taken by NLP. We also discuss code-specific features such as AST, CFG, and unit tests, along with their application in training code language models, and identify key challenges and potential future directions in this domain. We keep the survey open and updated on GitHub at https://github.com/codefuse-ai/Awesome-Code-LLM.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;KSAA-RD&#20849;&#20139;&#20219;&#21153;&#20013;Rosetta Stone&#30340;&#24212;&#29992;&#65292;&#23558;&#35821;&#35328;&#24314;&#27169;&#24212;&#29992;&#21040;&#35789;--&#23450;&#20041;&#23545;&#40784;&#20013;&#12290;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#19968;&#32452;&#24494;&#35843;&#30340;&#38463;&#25289;&#20271;BERT&#27169;&#22411;&#26469;&#39044;&#27979;&#32473;&#23450;&#23450;&#20041;&#30340;&#35789;&#23884;&#20837;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#38463;&#25289;&#20271;&#35789;&#30340;&#21521;&#37327;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2310.15823</link><description>&lt;p&gt;
Rosetta Stone&#22312;KSAA-RD&#20849;&#20139;&#20219;&#21153;&#20013;&#65306;&#20174;&#35821;&#35328;&#24314;&#27169;&#21040;&#35789;--&#23450;&#20041;&#23545;&#40784;&#30340;&#36291;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rosetta Stone at KSAA-RD Shared Task: A Hop From Language Modeling To Word--Definition Alignment. (arXiv:2310.15823v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;KSAA-RD&#20849;&#20139;&#20219;&#21153;&#20013;Rosetta Stone&#30340;&#24212;&#29992;&#65292;&#23558;&#35821;&#35328;&#24314;&#27169;&#24212;&#29992;&#21040;&#35789;--&#23450;&#20041;&#23545;&#40784;&#20013;&#12290;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#19968;&#32452;&#24494;&#35843;&#30340;&#38463;&#25289;&#20271;BERT&#27169;&#22411;&#26469;&#39044;&#27979;&#32473;&#23450;&#23450;&#20041;&#30340;&#35789;&#23884;&#20837;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#38463;&#25289;&#20271;&#35789;&#30340;&#21521;&#37327;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21521;&#35789;&#20856;&#26159;&#19968;&#31181;&#24037;&#20855;&#65292;&#21487;&#26681;&#25454;&#25552;&#20379;&#30340;&#23450;&#20041;&#12289;&#21547;&#20041;&#25110;&#25551;&#36848;&#26469;&#21457;&#29616;&#19968;&#20010;&#35789;&#12290;&#36825;&#31181;&#25216;&#26415;&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#37117;&#38750;&#24120;&#26377;&#20215;&#20540;&#65292;&#21487;&#20197;&#24110;&#21161;&#25484;&#25569;&#19968;&#20010;&#35789;&#30340;&#25551;&#36848;&#32780;&#19981;&#30693;&#20854;&#36523;&#20221;&#30340;&#35821;&#35328;&#23398;&#20064;&#32773;&#65292;&#24182;&#20351;&#23547;&#27714;&#31934;&#30830;&#26415;&#35821;&#30340;&#20889;&#20316;&#32773;&#21463;&#30410;&#12290;&#36825;&#20123;&#22330;&#26223;&#36890;&#24120;&#28085;&#30422;&#34987;&#31216;&#20026;&#8220;&#33292;&#23574;&#19978;&#30340;&#35789;&#8221;&#29616;&#35937;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21576;&#29616;&#20102;&#25105;&#20204;&#22312;&#38463;&#25289;&#20271;&#35821;&#21453;&#21521;&#35789;&#20856;&#20849;&#20139;&#20219;&#21153;&#20013;&#33719;&#32988;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#20219;&#21153;&#30340;&#37325;&#28857;&#26159;&#20174;&#20276;&#38543;&#30340;&#25551;&#36848;&#20013;&#25512;&#23548;&#20986;&#38463;&#25289;&#20271;&#35789;&#30340;&#21521;&#37327;&#34920;&#31034;&#12290;&#20849;&#20139;&#20219;&#21153;&#21253;&#25324;&#20004;&#20010;&#19981;&#21516;&#30340;&#23376;&#20219;&#21153;&#65306;&#31532;&#19968;&#20010;&#23376;&#20219;&#21153;&#28041;&#21450;&#19968;&#20010;&#38463;&#25289;&#20271;&#23450;&#20041;&#20316;&#20026;&#36755;&#20837;&#65292;&#32780;&#31532;&#20108;&#20010;&#23376;&#20219;&#21153;&#21017;&#20351;&#29992;&#19968;&#20010;&#33521;&#25991;&#23450;&#20041;&#12290;&#23545;&#20110;&#31532;&#19968;&#20010;&#23376;&#20219;&#21153;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#32452;&#32463;&#36807;&#24494;&#35843;&#30340;&#38463;&#25289;&#20271;BERT&#27169;&#22411;&#65292;&#26469;&#39044;&#27979;&#32473;&#23450;&#23450;&#20041;&#30340;&#35789;&#23884;&#20837;&#12290;&#26368;&#32456;&#34920;&#31034;&#26159;&#36890;&#36807;&#23545;&#27599;&#20010;&#27169;&#22411;&#36755;&#20986;&#30340;&#23884;&#20837;&#36827;&#34892;&#24179;&#22343;&#24471;&#21040;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Reverse Dictionary is a tool enabling users to discover a word based on its provided definition, meaning, or description. Such a technique proves valuable in various scenarios, aiding language learners who possess a description of a word without its identity, and benefiting writers seeking precise terminology. These scenarios often encapsulate what is referred to as the "Tip-of-the-Tongue" (TOT) phenomena. In this work, we present our winning solution for the Arabic Reverse Dictionary shared task. This task focuses on deriving a vector representation of an Arabic word from its accompanying description. The shared task encompasses two distinct subtasks: the first involves an Arabic definition as input, while the second employs an English definition. For the first subtask, our approach relies on an ensemble of finetuned Arabic BERT-based models, predicting the word embedding for a given definition. The final representation is obtained through averaging the output embeddings from each m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24433;&#21709;&#39537;&#21160;&#30340;&#36873;&#25321;&#24615;&#27880;&#37322;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25913;&#21892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#36873;&#25321;&#20851;&#38190;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#23376;&#38598;&#36827;&#34892;&#27880;&#37322;&#65292;&#22312;&#38477;&#20302;&#27880;&#37322;&#25104;&#26412;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.10873</link><description>&lt;p&gt;
IDEAL: &#24378;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24433;&#21709;&#39537;&#21160;&#36873;&#25321;&#24615;&#27880;&#37322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
IDEAL: Influence-Driven Selective Annotations Empower In-Context Learners in Large Language Models. (arXiv:2310.10873v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24433;&#21709;&#39537;&#21160;&#30340;&#36873;&#25321;&#24615;&#27880;&#37322;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25913;&#21892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#36873;&#25321;&#20851;&#38190;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#23376;&#38598;&#36827;&#34892;&#27880;&#37322;&#65292;&#22312;&#38477;&#20302;&#27880;&#37322;&#25104;&#26412;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#33539;&#24335;&#65292;&#23427;&#21033;&#29992;&#19978;&#19979;&#25991;&#31034;&#20363;&#20316;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#30340;&#25552;&#31034;&#12290;&#36825;&#20123;&#25552;&#31034;&#23545;&#20110;&#33719;&#24471;&#24378;&#22823;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20123;&#25552;&#31034;&#38656;&#35201;&#20174;&#22823;&#37327;&#27880;&#37322;&#30340;&#31034;&#20363;&#20013;&#36827;&#34892;&#37319;&#26679;&#65292;&#25214;&#21040;&#27491;&#30830;&#30340;&#25552;&#31034;&#21487;&#33021;&#23548;&#33268;&#39640;&#26114;&#30340;&#27880;&#37322;&#25104;&#26412;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#24433;&#21709;&#39537;&#21160;&#30340;&#36873;&#25321;&#24615;&#27880;&#37322;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#25913;&#21892;&#19978;&#19979;&#25991;&#31034;&#20363;&#36136;&#37327;&#30340;&#21516;&#26102;&#26368;&#22823;&#31243;&#24230;&#22320;&#38477;&#20302;&#27880;&#37322;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#20174;&#22823;&#35268;&#27169;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#27744;&#20013;&#36873;&#25321;&#19968;&#20010;&#20851;&#38190;&#23376;&#38598;&#36827;&#34892;&#27880;&#37322;&#65292;&#20197;&#29992;&#20110;&#21518;&#32493;&#30340;&#25552;&#31034;&#37319;&#26679;&#12290;&#20855;&#20307;&#22320;&#65292;&#39318;&#20808;&#26500;&#24314;&#19968;&#20010;&#26377;&#21521;&#22270;&#26469;&#34920;&#31034;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#65292;&#28982;&#21518;&#21033;&#29992;&#25193;&#25955;&#36807;&#31243;&#37327;&#21270;&#20505;&#36873;&#26410;&#26631;&#35760;&#23376;&#38598;&#30340;&#24433;&#21709;&#21147;&#65292;&#26368;&#21518;&#24341;&#20837;&#19968;&#20010;&#31616;&#21333;&#21448;&#26377;&#25928;&#30340;&#36138;&#24515;&#31639;&#27861;&#26469;&#36873;&#25321;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#12290;&#22914;&#26524;&#25968;&#25454;&#25552;&#20379;&#20102;&#26368;&#22823;&#30340;&#24433;&#21709;&#21147;&#65292;&#31639;&#27861;&#23601;&#20250;&#36845;&#20195;&#22320;&#36873;&#25321;&#36825;&#20123;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning is a promising paradigm that utilizes in-context examples as prompts for the predictions of large language models. These prompts are crucial for achieving strong performance. However, since the prompts need to be sampled from a large volume of annotated examples, finding the right prompt may result in high annotation costs. To address this challenge, this paper introduces an influence-driven selective annotation method that aims to minimize annotation costs while improving the quality of in-context examples. The essence of our method is to select a pivotal subset from a large-scale unlabeled data pool to annotate for the subsequent sampling of prompts. Specifically, a directed graph is first constructed to represent unlabeled data. Afterward, the influence of candidate unlabeled subsets is quantified with a diffusion process. A simple yet effective greedy algorithm for unlabeled data selection is lastly introduced. It iteratively selects the data if it provides a ma
&lt;/p&gt;</description></item><item><title>EMO&#25552;&#20986;&#20102;&#22320;&#29699;&#31227;&#21160;&#36317;&#31163;&#20248;&#21270;&#65288;EMO&#65289;&#26469;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36864;&#21270;&#29616;&#35937;&#12290;EMO&#21033;&#29992;&#20102;&#22320;&#29699;&#31227;&#21160;&#36317;&#31163;&#30340;&#29305;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#34892;&#30340;&#19978;&#30028;&#26469;&#31616;&#21270;&#35757;&#32451;&#12290;&#32463;&#36807;&#35780;&#20272;&#65292;&#21457;&#29616;EMO&#22312;&#35821;&#35328;&#27169;&#22411;&#19978;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.04691</link><description>&lt;p&gt;
EMO: Earth Mover Distance Optimization for Auto-Regressive Language Modeling. (arXiv:2310.04691v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
EMO: Earth Mover Distance Optimization for Auto-Regressive Language Modeling. (arXiv:2310.04691v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04691
&lt;/p&gt;
&lt;p&gt;
EMO&#25552;&#20986;&#20102;&#22320;&#29699;&#31227;&#21160;&#36317;&#31163;&#20248;&#21270;&#65288;EMO&#65289;&#26469;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36864;&#21270;&#29616;&#35937;&#12290;EMO&#21033;&#29992;&#20102;&#22320;&#29699;&#31227;&#21160;&#36317;&#31163;&#30340;&#29305;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#34892;&#30340;&#19978;&#30028;&#26469;&#31616;&#21270;&#35757;&#32451;&#12290;&#32463;&#36807;&#35780;&#20272;&#65292;&#21457;&#29616;EMO&#22312;&#35821;&#35328;&#27169;&#22411;&#19978;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#26159;&#20154;&#25991;&#26412;&#30340;&#27010;&#29575;&#27169;&#22411;&#12290;&#23427;&#20204;&#20027;&#35201;&#36890;&#36807;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#26041;&#27861;&#31561;&#21516;&#20110;&#26368;&#23567;&#21270;&#32463;&#39564;&#25968;&#25454;&#20998;&#24067;&#21644;&#27169;&#22411;&#20998;&#24067;&#20043;&#38388;&#30340;&#21069;&#21521;&#20132;&#21449;&#29109;&#12290;&#28982;&#32780;&#65292;&#24403;&#20174;&#36825;&#20123;&#27169;&#22411;&#23398;&#20064;&#30340;&#20998;&#24067;&#35299;&#30721;&#26102;&#65292;&#20173;&#28982;&#32463;&#24120;&#35266;&#23519;&#21040;&#21508;&#31181;&#36864;&#21270;&#29616;&#35937;&#12290;&#25105;&#20204;&#30830;&#23450;&#21069;&#21521;&#20132;&#21449;&#29109;&#20316;&#20026;&#20154;&#19982;&#27169;&#22411;&#20998;&#24067;&#23545;&#40784;&#30340;&#36317;&#31163;&#24230;&#37327;&#26159;&#27425;&#20248;&#30340;&#65292;&#21407;&#22240;&#26377;&#65306;&#65288;1&#65289;&#21484;&#22238;&#20248;&#21270;&#65292;&#65288;2&#65289;&#36127;&#26679;&#26412;&#22810;&#26679;&#24615;&#24573;&#35270;&#21644;&#65288;3&#65289;&#35757;&#32451;&#27979;&#35797;&#19981;&#21305;&#37197;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#30340;&#22320;&#29699;&#31227;&#21160;&#36317;&#31163;&#20248;&#21270;&#65288;EMO&#65289;&#12290;EMO&#21033;&#29992;&#22320;&#29699;&#31227;&#21160;&#36317;&#31163;&#30340;&#20869;&#22312;&#29305;&#24615;&#26469;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#12290;&#30001;&#20110;&#30452;&#25509;&#35745;&#31639;&#30340;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;EMO&#19978;&#30028;&#26469;&#31616;&#21270;&#31471;&#21040;&#31471;&#35757;&#32451;&#12290;&#32463;&#36807;&#24191;&#27867;&#35780;&#20272;&#20043;&#21518;&#65292;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35821;&#35328;&#27169;&#22411;&#19978;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural language models are probabilistic models of human text. They are predominantly trained using maximum likelihood estimation (MLE), which is equivalent to minimizing the forward cross-entropy between the empirical data distribution and the model distribution. However, various degeneration phenomena are still widely observed when decoding from the distributions learned by such models. We establish that the forward cross-entropy is suboptimal as a distance metric for aligning human and model distribution due to its (1) recall-prioritization (2) negative diversity ignorance and (3) train-test mismatch. In this paper, we propose Earth Mover Distance Optimization (EMO) for auto-regressive language modeling. EMO capitalizes on the inherent properties of earth mover distance to address the aforementioned challenges. Due to the high complexity of direct computation, we further introduce a feasible upper bound for EMO to ease end-to-end training. Upon extensive evaluation of language model
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;MathVista&#65292;&#36825;&#26159;&#19968;&#20010;&#35780;&#20272;&#35270;&#35273;&#22330;&#26223;&#20013;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#36890;&#36807;&#23545;12&#20010;&#33879;&#21517;&#30340;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#30340;&#23450;&#37327;&#35780;&#20272;&#65292;&#21457;&#29616;&#26368;&#22909;&#30340;GPT-4V&#27169;&#22411;&#30456;&#23545;&#20110;&#31532;&#20108;&#21517;&#30340;Bard&#27169;&#22411;&#22312;&#20934;&#30830;&#29575;&#19978;&#25552;&#21319;&#20102;15.1%&#12290;</title><link>http://arxiv.org/abs/2310.02255</link><description>&lt;p&gt;
MathVista: &#29992;GPT-4V&#12289;Bard&#21644;&#20854;&#20182;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#35780;&#20272;&#35270;&#35273;&#22330;&#26223;&#20013;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
MathVista: Evaluating Math Reasoning in Visual Contexts with GPT-4V, Bard, and Other Large Multimodal Models. (arXiv:2310.02255v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;MathVista&#65292;&#36825;&#26159;&#19968;&#20010;&#35780;&#20272;&#35270;&#35273;&#22330;&#26223;&#20013;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#36890;&#36807;&#23545;12&#20010;&#33879;&#21517;&#30340;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#30340;&#23450;&#37327;&#35780;&#20272;&#65292;&#21457;&#29616;&#26368;&#22909;&#30340;GPT-4V&#27169;&#22411;&#30456;&#23545;&#20110;&#31532;&#20108;&#21517;&#30340;Bard&#27169;&#22411;&#22312;&#20934;&#30830;&#29575;&#19978;&#25552;&#21319;&#20102;15.1%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#22312;&#35768;&#22810;&#20219;&#21153;&#21644;&#39046;&#22495;&#20013;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#35270;&#35273;&#29615;&#22659;&#20013;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#23578;&#26410;&#24471;&#21040;&#31995;&#32479;&#30740;&#31350;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MathVista&#65292;&#36825;&#26159;&#19968;&#20010;&#32508;&#21512;&#20102;&#19981;&#21516;&#25968;&#23398;&#21644;&#35270;&#35273;&#20219;&#21153;&#30340;&#25361;&#25112;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#23427;&#21253;&#21547;&#20102;6141&#20010;&#20363;&#23376;&#65292;&#20854;&#20013;&#26377;28&#20010;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#21644;3&#20010;&#26032;&#21019;&#24314;&#30340;&#25968;&#25454;&#38598;&#65288;&#21363;IQTest&#12289;FunctionQA&#21644;PaperQA&#65289;&#12290;&#23436;&#25104;&#36825;&#20123;&#20219;&#21153;&#38656;&#35201;&#31934;&#32454;&#30340;&#12289;&#28145;&#20837;&#30340;&#35270;&#35273;&#29702;&#35299;&#21644;&#32452;&#21512;&#25512;&#29702;&#65292;&#36825;&#20123;&#37117;&#26159;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#30784;&#27169;&#22411;&#25152;&#38754;&#20020;&#30340;&#22256;&#38590;&#12290;&#36890;&#36807;MathVista&#65292;&#25105;&#20204;&#23545;12&#20010;&#33879;&#21517;&#30340;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23450;&#37327;&#35780;&#20272;&#12290;&#34920;&#29616;&#26368;&#22909;&#30340;GPT-4V&#27169;&#22411;&#30340;&#25972;&#20307;&#20934;&#30830;&#29575;&#20026;49.9%&#65292;&#26126;&#26174;&#20248;&#20110;&#31532;&#20108;&#21517;&#30340;Bard&#27169;&#22411;&#65292;&#30456;&#24046;15.1%&#12290;&#25105;&#20204;&#30340;&#28145;&#20837;&#20998;&#26512;&#25581;&#31034;&#20102;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) and Large Multimodal Models (LMMs) exhibit impressive problem-solving skills in many tasks and domains, but their ability in mathematical reasoning in visual contexts has not been systematically studied. To bridge this gap, we present MathVista, a benchmark designed to combine challenges from diverse mathematical and visual tasks. It consists of 6,141 examples, derived from 28 existing multimodal datasets involving mathematics and 3 newly created datasets (i.e., IQTest, FunctionQA, and PaperQA). Completing these tasks requires fine-grained, deep visual understanding and compositional reasoning, which all state-of-the-art foundation models find challenging. With MathVista, we have conducted a comprehensive, quantitative evaluation of 12 prominent foundation models. The best-performing GPT-4V model achieves an overall accuracy of 49.9%, substantially outperforming Bard, the second-best performer, by 15.1%. Our in-depth analysis reveals that the superiority of
&lt;/p&gt;</description></item><item><title>TWIZ-v2&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#23545;&#35805;&#21050;&#28608;&#30340;&#24043;&#24072;&#21161;&#25163;&#65292;&#26088;&#22312;&#36890;&#36807;&#20197;&#20154;&#24615;&#21270;&#30340;&#26041;&#24335;&#25552;&#20379;&#20449;&#24687;&#12289;&#21033;&#29992;&#22810;&#31181;&#27169;&#24577;&#36827;&#34892;&#21050;&#28608;&#20197;&#21450;&#25913;&#36827;&#23545;&#26410;&#35265;&#36807;&#22330;&#26223;&#30340;&#20132;&#20114;&#40065;&#26834;&#24615;&#26469;&#24341;&#23548;&#29992;&#25143;&#25104;&#21151;&#23436;&#25104;&#22797;&#26434;&#30340;&#25163;&#21160;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.02118</link><description>&lt;p&gt;
TWIZ-v2: &#22810;&#27169;&#24577;&#23545;&#35805;&#21050;&#28608;&#30340;&#24043;&#24072;
&lt;/p&gt;
&lt;p&gt;
TWIZ-v2: The Wizard of Multimodal Conversational-Stimulus. (arXiv:2310.02118v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02118
&lt;/p&gt;
&lt;p&gt;
TWIZ-v2&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#23545;&#35805;&#21050;&#28608;&#30340;&#24043;&#24072;&#21161;&#25163;&#65292;&#26088;&#22312;&#36890;&#36807;&#20197;&#20154;&#24615;&#21270;&#30340;&#26041;&#24335;&#25552;&#20379;&#20449;&#24687;&#12289;&#21033;&#29992;&#22810;&#31181;&#27169;&#24577;&#36827;&#34892;&#21050;&#28608;&#20197;&#21450;&#25913;&#36827;&#23545;&#26410;&#35265;&#36807;&#22330;&#26223;&#30340;&#20132;&#20114;&#40065;&#26834;&#24615;&#26469;&#24341;&#23548;&#29992;&#25143;&#25104;&#21151;&#23436;&#25104;&#22797;&#26434;&#30340;&#25163;&#21160;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;Task Wizard&#22242;&#38431;TWIZ&#22312;Alexa Prize TaskBot Challenge 2022&#20013;&#30340;&#24895;&#26223;&#12289;&#25361;&#25112;&#21644;&#31185;&#23398;&#36129;&#29486;&#12290;&#25105;&#20204;&#30340;&#24895;&#26223;&#26159;&#26500;&#24314;TWIZ&#26426;&#22120;&#20154;&#20316;&#20026;&#19968;&#20010;&#26377;&#29992;&#12289;&#22810;&#27169;&#24577;&#12289;&#30693;&#35782;&#20016;&#23500;&#21644;&#24341;&#20154;&#20837;&#32988;&#30340;&#21161;&#25163;&#65292;&#21487;&#20197;&#24341;&#23548;&#29992;&#25143;&#25104;&#21151;&#23436;&#25104;&#22797;&#26434;&#30340;&#25163;&#21160;&#20219;&#21153;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#21162;&#21147;&#38598;&#20013;&#22312;&#19977;&#20010;&#20027;&#35201;&#30340;&#30740;&#31350;&#38382;&#39064;&#19978;&#65306;&#65288;1&#65289;&#20197;&#20154;&#24615;&#21270;&#30340;&#23545;&#35805;&#26041;&#24335;&#25552;&#20379;&#20449;&#24687;&#65307;&#65288;2&#65289;&#21033;&#29992;&#22768;&#38899;&#12289;&#22270;&#20687;&#21644;&#35270;&#39057;&#31561;&#21508;&#31181;&#27169;&#24577;&#36827;&#34892;&#22810;&#27169;&#24577;&#21050;&#28608;&#65307;&#65288;3&#65289;&#38646;-shot&#23545;&#35805;&#27969;&#31243;&#65292;&#20197;&#25552;&#39640;&#23545;&#26410;&#35265;&#36807;&#22330;&#26223;&#30340;&#20132;&#20114;&#30340;&#40065;&#26834;&#24615;&#12290;TWIZ&#26159;&#19968;&#20010;&#33021;&#25903;&#25345;&#21508;&#31181;&#20219;&#21153;&#30340;&#21161;&#25163;&#65292;&#20855;&#26377;&#21019;&#26032;&#30340;&#21151;&#33021;&#65292;&#22914;&#21019;&#24847;&#28921;&#39274;&#12289;&#36890;&#36807;&#22768;&#38899;&#23548;&#33322;&#35270;&#39057;&#21644;&#24378;&#22823;&#30340;TWIZ-LLM&#65292;&#19968;&#20010;&#29992;&#20110;&#22797;&#26434;&#25163;&#21160;&#20219;&#21153;&#23545;&#35805;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#26681;&#25454;&#29992;&#25143;&#25552;&#20379;&#30340;&#35780;&#32423;&#21644;&#21453;&#39304;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;
&lt;/p&gt;
&lt;p&gt;
In this report, we describe the vision, challenges, and scientific contributions of the Task Wizard team, TWIZ, in the Alexa Prize TaskBot Challenge 2022. Our vision, is to build TWIZ bot as an helpful, multimodal, knowledgeable, and engaging assistant that can guide users towards the successful completion of complex manual tasks. To achieve this, we focus our efforts on three main research questions: (1) Humanly-Shaped Conversations, by providing information in a knowledgeable way; (2) Multimodal Stimulus, making use of various modalities including voice, images, and videos; and (3) Zero-shot Conversational Flows, to improve the robustness of the interaction to unseen scenarios. TWIZ is an assistant capable of supporting a wide range of tasks, with several innovative features such as creative cooking, video navigation through voice, and the robust TWIZ-LLM, a Large Language Model trained for dialoguing about complex manual tasks. Given ratings and feedback provided by users, we observ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;PsychoBench&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#30340;&#24515;&#29702;&#22240;&#32032;&#65292;&#24182;&#23545;text-davinci-003&#12289;gpt-3.5-turbo&#12289;gpt-4&#12289;LLaMA-2-7b&#21644;LLaMA-2-13b&#31561;&#20116;&#20010;&#27169;&#22411;&#36827;&#34892;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2310.01386</link><description>&lt;p&gt;
&#35841;&#26159;ChatGPT&#65311;&#20351;&#29992;PsychoBench&#23545;LLMs&#30340;&#24515;&#29702;&#25551;&#32472;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Who is ChatGPT? Benchmarking LLMs' Psychological Portrayal Using PsychoBench. (arXiv:2310.01386v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;PsychoBench&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#30340;&#24515;&#29702;&#22240;&#32032;&#65292;&#24182;&#23545;text-davinci-003&#12289;gpt-3.5-turbo&#12289;gpt-4&#12289;LLaMA-2-7b&#21644;LLaMA-2-13b&#31561;&#20116;&#20010;&#27169;&#22411;&#36827;&#34892;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26368;&#36817;&#23637;&#31034;&#20102;&#20854;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#19981;&#20165;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;&#32780;&#19988;&#36328;&#36234;&#20102;&#20020;&#24202;&#21307;&#23398;&#12289;&#27861;&#24459;&#21672;&#35810;&#21644;&#25945;&#32946;&#31561;&#21508;&#20010;&#39046;&#22495;&#12290;LLMs&#24050;&#32463;&#36229;&#20986;&#20102;&#31616;&#21333;&#30340;&#24212;&#29992;&#65292;&#28436;&#21464;&#25104;&#33021;&#22815;&#35299;&#20915;&#21508;&#31181;&#29992;&#25143;&#35831;&#27714;&#30340;&#21161;&#25163;&#12290;&#36825;&#32553;&#23567;&#20102;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24341;&#21457;&#20102;&#26377;&#20851;LLMs&#20869;&#37096;&#20010;&#24615;&#12289;&#24615;&#26684;&#21644;&#24773;&#32490;&#28508;&#22312;&#34920;&#29616;&#30340;&#26377;&#36259;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;PsychoBench&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#30340;&#21508;&#31181;&#24515;&#29702;&#22240;&#32032;&#12290;PsychoBench&#30001;&#20020;&#24202;&#24515;&#29702;&#23398;&#20013;&#24120;&#29992;&#30340;13&#20010;&#37327;&#34920;&#32452;&#25104;&#65292;&#36827;&#19968;&#27493;&#23558;&#36825;&#20123;&#37327;&#34920;&#20998;&#20026;&#22235;&#20010;&#19981;&#21516;&#30340;&#31867;&#21035;&#65306;&#20154;&#26684;&#29305;&#36136;&#12289;&#20154;&#38469;&#20851;&#31995;&#12289;&#21160;&#26426;&#27979;&#35797;&#21644;&#24773;&#24863;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#32771;&#23519;&#20102;&#20116;&#20010;&#27969;&#34892;&#30340;&#27169;&#22411;&#65292;&#20998;&#21035;&#26159;text-davinci-003&#12289;gpt-3.5-turbo&#12289;gpt-4&#12289;LLaMA-2-7b&#21644;LLaMA-2-13b&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have recently showcased their remarkable capacities, not only in natural language processing tasks but also across diverse domains such as clinical medicine, legal consultation, and education. LLMs become more than mere applications, evolving into assistants capable of addressing diverse user requests. This narrows the distinction between human beings and artificial intelligence agents, raising intriguing questions regarding the potential manifestation of personalities, temperaments, and emotions within LLMs. In this paper, we propose a framework, PsychoBench, for evaluating diverse psychological aspects of LLMs. Comprising thirteen scales commonly used in clinical psychology, PsychoBench further classifies these scales into four distinct categories: personality traits, interpersonal relationships, motivational tests, and emotional abilities. Our study examines five popular models, namely text-davinci-003, gpt-3.5-turbo, gpt-4, LLaMA-2-7b, and LLaMA-2-13b. 
&lt;/p&gt;</description></item><item><title>GenSim&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#20016;&#23500;&#30340;&#27169;&#25311;&#29615;&#22659;&#21644;&#19987;&#23478;&#31034;&#33539;&#65292;&#35299;&#20915;&#20102;&#30446;&#21069;&#27169;&#25311;&#25968;&#25454;&#20013;&#32570;&#20047;&#20219;&#21153;&#32423;&#21035;&#22810;&#26679;&#24615;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#26426;&#22120;&#20154;&#31574;&#30053;&#22312;&#20219;&#21153;&#32423;&#21035;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.01361</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26426;&#22120;&#20154;&#27169;&#25311;&#20219;&#21153;&#30340;GenSim
&lt;/p&gt;
&lt;p&gt;
GenSim: Generating Robotic Simulation Tasks via Large Language Models. (arXiv:2310.01361v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01361
&lt;/p&gt;
&lt;p&gt;
GenSim&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#20016;&#23500;&#30340;&#27169;&#25311;&#29615;&#22659;&#21644;&#19987;&#23478;&#31034;&#33539;&#65292;&#35299;&#20915;&#20102;&#30446;&#21069;&#27169;&#25311;&#25968;&#25454;&#20013;&#32570;&#20047;&#20219;&#21153;&#32423;&#21035;&#22810;&#26679;&#24615;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#26426;&#22120;&#20154;&#31574;&#30053;&#22312;&#20219;&#21153;&#32423;&#21035;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25910;&#38598;&#22823;&#37327;&#30495;&#23454;&#20132;&#20114;&#25968;&#25454;&#26469;&#35757;&#32451;&#36890;&#29992;&#26426;&#22120;&#20154;&#31574;&#30053;&#24448;&#24448;&#20195;&#20215;&#39640;&#26114;&#65292;&#22240;&#27492;&#22312;&#25968;&#25454;&#29983;&#25104;&#26041;&#38754;&#36890;&#24120;&#37319;&#29992;&#27169;&#25311;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#36890;&#24120;&#19987;&#27880;&#20110;&#22330;&#26223;&#32423;&#21035;&#30340;&#22810;&#26679;&#24615;&#65288;&#20363;&#22914;&#23545;&#35937;&#23454;&#20363;&#21644;&#23039;&#21183;&#65289;&#65292;&#32780;&#24573;&#35270;&#20102;&#20219;&#21153;&#32423;&#21035;&#30340;&#22810;&#26679;&#24615;&#65292;&#36825;&#26159;&#22240;&#20026;&#38656;&#35201;&#20154;&#24037;&#21162;&#21147;&#25552;&#20986;&#21644;&#39564;&#35777;&#26032;&#30340;&#20219;&#21153;&#12290;&#36825;&#23548;&#33268;&#22312;&#27169;&#25311;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#31574;&#30053;&#24456;&#38590;&#23637;&#31034;&#26174;&#33879;&#30340;&#20219;&#21153;&#32423;&#21035;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#22522;&#20110;&#22330;&#26223;&#21644;&#32534;&#30721;&#33021;&#21147;&#33258;&#21160;&#29983;&#25104;&#20016;&#23500;&#30340;&#27169;&#25311;&#29615;&#22659;&#21644;&#19987;&#23478;&#31034;&#33539;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;GenSim&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#20004;&#31181;&#27169;&#24335;&#65306;&#30446;&#26631;&#23548;&#21521;&#29983;&#25104;&#65292;&#20854;&#20013;&#23558;&#30446;&#26631;&#20219;&#21153;&#25552;&#20379;&#32473;LLM&#65292;LLM&#25552;&#20986;&#35299;&#20915;&#30446;&#26631;&#20219;&#21153;&#30340;&#20219;&#21153;&#35838;&#31243;&#65307;&#25506;&#32034;&#24615;&#29983;&#25104;&#65292;&#20854;&#20013;LLM&#20174;&#20808;&#21069;&#30340;&#20219;&#21153;&#20013;&#21551;&#21160;&#65292;&#24182;&#36845;&#20195;&#22320;&#25552;&#20986;&#26032;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collecting large amounts of real-world interaction data to train general robotic policies is often prohibitively expensive, thus motivating the use of simulation data. However, existing methods for data generation have generally focused on scene-level diversity (e.g., object instances and poses) rather than task-level diversity, due to the human effort required to come up with and verify novel tasks. This has made it challenging for policies trained on simulation data to demonstrate significant task-level generalization. In this paper, we propose to automatically generate rich simulation environments and expert demonstrations by exploiting a large language models' (LLM) grounding and coding ability. Our approach, dubbed GenSim, has two modes: goal-directed generation, wherein a target task is given to the LLM and the LLM proposes a task curriculum to solve the target task, and exploratory generation, wherein the LLM bootstraps from previous tasks and iteratively proposes novel tasks th
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#28508;&#21147;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;&#22797;&#26434;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#25581;&#31034;&#20551;&#26032;&#38395;&#24182;&#25552;&#20379;&#22810;&#35282;&#24230;&#35299;&#37322;&#65292;&#20294;&#20173;&#19981;&#22914;&#32463;&#36807;fine-tuned&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#33394;&#12290;&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#26080;&#27861;&#21462;&#20195;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20294;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#33391;&#22909;&#30340;&#36741;&#21161;&#39038;&#38382;&#12290;</title><link>http://arxiv.org/abs/2309.12247</link><description>&lt;p&gt;
&#22351;&#35282;&#33394;&#22909;&#39038;&#38382;&#65306;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20551;&#26032;&#38395;&#26816;&#27979;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Bad Actor, Good Advisor: Exploring the Role of Large Language Models in Fake News Detection. (arXiv:2309.12247v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12247
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#28508;&#21147;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;&#22797;&#26434;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#25581;&#31034;&#20551;&#26032;&#38395;&#24182;&#25552;&#20379;&#22810;&#35282;&#24230;&#35299;&#37322;&#65292;&#20294;&#20173;&#19981;&#22914;&#32463;&#36807;fine-tuned&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#33394;&#12290;&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#26080;&#27861;&#21462;&#20195;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20294;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#33391;&#22909;&#30340;&#36741;&#21161;&#39038;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#20551;&#26032;&#38395;&#38656;&#35201;&#23545;&#22810;&#26679;&#32447;&#32034;&#26377;&#25935;&#38160;&#30340;&#24863;&#30693;&#21644;&#23545;&#29616;&#23454;&#19990;&#30028;&#32972;&#26223;&#26377;&#28145;&#20837;&#30340;&#29702;&#35299;&#65292;&#23545;&#20110;&#22522;&#20110;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26816;&#27979;&#22120;&#26469;&#35828;&#65292;&#30001;&#20110;&#20854;&#30693;&#35782;&#21644;&#33021;&#21147;&#30340;&#38480;&#21046;&#65292;&#36825;&#20173;&#28982;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#36817;&#26399;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#27493;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#20197;&#21450;&#22914;&#20309;&#24110;&#21161;&#20551;&#26032;&#38395;&#26816;&#27979;&#20173;&#28982;&#26410;&#32463;&#36807;&#28145;&#20837;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20551;&#26032;&#38395;&#26816;&#27979;&#20013;&#30340;&#28508;&#21147;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#21457;&#29616;&#20687;GPT 3.5&#36825;&#26679;&#30340;&#22797;&#26434;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#33021;&#22815;&#25581;&#31034;&#20551;&#26032;&#38395;&#24182;&#25552;&#20379;&#29702;&#24819;&#30340;&#22810;&#35282;&#24230;&#35299;&#37322;&#65292;&#20294;&#20173;&#28982;&#19981;&#22914;&#22522;&#30784;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;fine-tuned BERT&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#38543;&#21518;&#30340;&#20998;&#26512;&#23558;&#36825;&#31181;&#24046;&#36317;&#24402;&#22240;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19981;&#33021;&#27491;&#30830;&#36873;&#25321;&#24182;&#25972;&#21512;&#35777;&#25454;&#20197;&#24471;&#20986;&#32467;&#35770;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#26080;&#27861;&#21462;&#20195;&#22312;&#20551;&#26032;&#38395;&#26816;&#27979;&#20013;&#32463;&#36807;fine-tuned&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20294;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#33391;&#22909;&#30340;&#36741;&#21161;&#39038;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting fake news requires both a delicate sense of diverse clues and a profound understanding of the real-world background, which remains challenging for detectors based on small language models (SLMs) due to their knowledge and capability limitations. Recent advances in large language models (LLMs) have shown remarkable performance in various tasks, but whether and how LLMs could help with fake news detection remains underexplored. In this paper, we investigate the potential of LLMs in fake news detection. First, we conduct an empirical study and find that a sophisticated LLM such as GPT 3.5 could generally expose fake news and provide desirable multi-perspective rationales but still underperforms the basic SLM, fine-tuned BERT. Our subsequent analysis attributes such a gap to the LLM's inability to select and integrate rationales properly to conclude. Based on these findings, we propose that current LLMs may not substitute fine-tuned SLMs in fake news detection but can be a good a
&lt;/p&gt;</description></item><item><title>ChaCha&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#40723;&#21169;&#20799;&#31461;&#20998;&#20139;&#20010;&#20154;&#20107;&#20214;&#21644;&#30456;&#20851;&#24773;&#32490;&#12290;&#36890;&#36807;&#19968;&#20010;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#21457;&#29616;&#20799;&#31461;&#23558;ChaCha&#35270;&#20026;&#20146;&#23494;&#30340;&#26379;&#21451;&#65292;&#24182;&#24895;&#24847;&#19982;&#20854;&#20998;&#20139;&#21508;&#31181;&#20027;&#39064;&#30340;&#25925;&#20107;&#12290;</title><link>http://arxiv.org/abs/2309.12244</link><description>&lt;p&gt;
ChaCha&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#20799;&#31461;&#20998;&#20139;&#19982;&#20010;&#20154;&#20107;&#20214;&#30456;&#20851;&#30340;&#24773;&#32490;
&lt;/p&gt;
&lt;p&gt;
ChaCha: Leveraging Large Language Models to Prompt Children to Share Their Emotions about Personal Events. (arXiv:2309.12244v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12244
&lt;/p&gt;
&lt;p&gt;
ChaCha&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#40723;&#21169;&#20799;&#31461;&#20998;&#20139;&#20010;&#20154;&#20107;&#20214;&#21644;&#30456;&#20851;&#24773;&#32490;&#12290;&#36890;&#36807;&#19968;&#20010;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#21457;&#29616;&#20799;&#31461;&#23558;ChaCha&#35270;&#20026;&#20146;&#23494;&#30340;&#26379;&#21451;&#65292;&#24182;&#24895;&#24847;&#19982;&#20854;&#20998;&#20139;&#21508;&#31181;&#20027;&#39064;&#30340;&#25925;&#20107;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20799;&#31461;&#36890;&#24120;&#36890;&#36807;&#19982;&#23478;&#20154;&#25110;&#20182;&#20154;&#20998;&#20139;&#25925;&#20107;&#21644;&#24863;&#21463;&#26469;&#23398;&#20064;&#36776;&#35782;&#21644;&#34920;&#36798;&#24773;&#32490;&#65292;&#28982;&#32780;&#65292;&#30001;&#20110;&#20799;&#31461;&#27491;&#22312;&#21457;&#23637;&#20182;&#20204;&#30340;&#20132;&#27969;&#25216;&#33021;&#65292;&#29238;&#27597;&#25110;&#20804;&#24351;&#22992;&#22969;&#24456;&#38590;&#19982;&#20182;&#20204;&#36827;&#34892;&#24773;&#24863;&#27807;&#36890;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ChaCha&#65292;&#19968;&#20010;&#40723;&#21169;&#21644;&#24341;&#23548;&#20799;&#31461;&#20998;&#20139;&#20010;&#20154;&#20107;&#20214;&#21644;&#30456;&#20851;&#24773;&#32490;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;ChaCha&#32467;&#21512;&#20102;&#29366;&#24577;&#26426;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22312;&#36827;&#34892;&#33258;&#30001;&#23545;&#35805;&#30340;&#21516;&#26102;&#20445;&#25345;&#23545;&#35805;&#30340;&#26041;&#21521;&#24615;&#12290;&#36890;&#36807;&#19982;20&#21517;&#24180;&#40836;&#22312;8-12&#23681;&#30340;&#20799;&#31461;&#36827;&#34892;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;ChaCha&#22914;&#20309;&#20419;&#20351;&#20799;&#31461;&#20998;&#20139;&#20010;&#20154;&#20107;&#20214;&#24182;&#24341;&#23548;&#20182;&#20204;&#25551;&#36848;&#30456;&#20851;&#24773;&#32490;&#12290;&#21442;&#19982;&#32773;&#35748;&#20026;ChaCha&#23601;&#20687;&#19968;&#20010;&#20146;&#23494;&#30340;&#26379;&#21451;&#65292;&#24182;&#20998;&#20139;&#20102;&#21508;&#31181;&#20027;&#39064;&#30340;&#25925;&#20107;&#65292;&#22914;&#23478;&#24237;&#26053;&#34892;&#21644;&#20010;&#20154;&#25104;&#23601;&#12290;&#22522;&#20110;&#23450;&#37327;&#21644;&#23450;&#24615;&#21457;&#29616;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#21033;&#29992;LLMs&#35774;&#35745;&#36866;&#21512;&#20799;&#31461;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
Children typically learn to identify and express emotions through sharing their stories and feelings with others, particularly their family. However, it is challenging for parents or siblings to have emotional communication with children since children are still developing their communication skills. We present ChaCha, a chatbot that encourages and guides children to share personal events and associated emotions. ChaCha combines a state machine and large language models (LLMs) to keep the dialogue on track while carrying on free-form conversations. Through an exploratory study with 20 children (aged 8-12), we examine how ChaCha prompts children to share personal events and guides them to describe associated emotions. Participants perceived ChaCha as a close friend and shared their stories on various topics, such as family trips and personal achievements. Based on the quantitative and qualitative findings, we discuss opportunities for leveraging LLMs to design child-friendly chatbots to
&lt;/p&gt;</description></item><item><title>DePT&#36890;&#36807;&#23558;&#36719;&#25552;&#31034;&#20998;&#35299;&#20026;&#36739;&#30701;&#30340;&#36719;&#25552;&#31034;&#21644;&#19968;&#23545;&#20302;&#31209;&#30697;&#38453;&#65292;&#24182;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#23398;&#20064;&#29575;&#26469;&#20248;&#21270;&#65292;&#20197;&#35299;&#20915;&#25552;&#31034;&#35843;&#25972;&#23545;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#20197;&#21450;&#20869;&#23384;&#20351;&#29992;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.05173</link><description>&lt;p&gt;
DePT:&#20998;&#35299;&#25552;&#31034;&#35843;&#25972;&#20197;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning. (arXiv:2309.05173v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05173
&lt;/p&gt;
&lt;p&gt;
DePT&#36890;&#36807;&#23558;&#36719;&#25552;&#31034;&#20998;&#35299;&#20026;&#36739;&#30701;&#30340;&#36719;&#25552;&#31034;&#21644;&#19968;&#23545;&#20302;&#31209;&#30697;&#38453;&#65292;&#24182;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#23398;&#20064;&#29575;&#26469;&#20248;&#21270;&#65292;&#20197;&#35299;&#20915;&#25552;&#31034;&#35843;&#25972;&#23545;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#20197;&#21450;&#20869;&#23384;&#20351;&#29992;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#35843;&#25972;&#65288;PT&#65289;&#26159;&#19968;&#31181;&#23558;&#21487;&#35757;&#32451;&#30340;&#23569;&#37327;&#36719;&#25552;&#31034;&#21521;&#37327;&#38468;&#21152;&#21040;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#36755;&#20837;&#20013;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#24050;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#27169;&#22411;&#20013;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290; &#19982;&#20854;&#20182;PEFT&#26041;&#27861;&#30456;&#27604;&#65292;PT&#30340;&#31454;&#20105;&#24615;&#33021;&#21487;&#20197;&#22312;&#21487;&#35757;&#32451;&#21442;&#25968;&#26356;&#23569;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#65292;&#24182;&#19988;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#30340;&#25193;&#22823;&#65292;&#20854;&#21442;&#25968;&#24182;&#19981;&#20250;&#26174;&#33879;&#22686;&#21152;&#12290; &#20294;&#26159;&#65292;PT&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#36719;&#25552;&#31034;&#26631;&#35760;&#65292;&#23548;&#33268;&#36755;&#20837;&#24207;&#21015;&#21464;&#38271;&#65292;&#36825;&#23545;&#20110;Transformer&#30340;&#20108;&#27425;&#22797;&#26434;&#24230;&#32780;&#35328;&#65292;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#20197;&#21450;&#20869;&#23384;&#20351;&#29992;&#26041;&#38754;&#20250;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290; &#36825;&#23545;&#20110;&#38754;&#20020;&#22823;&#37327;&#27599;&#26085;&#26597;&#35810;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23588;&#20854;&#20196;&#20154;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt tuning (PT), where a small amount of trainable soft (continuous) prompt vectors is affixed to the input of language models (LM), has shown promising results across various tasks and models for parameter-efficient fine-tuning (PEFT). PT stands out from other PEFT approaches because it maintains competitive performance with fewer trainable parameters and does not drastically scale up its parameters as the model size expands. However, PT introduces additional soft prompt tokens, leading to longer input sequences, which significantly impacts training and inference time and memory usage due to the Transformer's quadratic complexity. Particularly concerning for Large Language Models (LLMs) that face heavy daily querying. To address this issue, we propose Decomposed Prompt Tuning (DePT), which decomposes the soft prompt into a shorter soft prompt and a pair of low-rank matrices that are then optimised with two different learning rates. This allows DePT to achieve better performance whi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;ChatRule&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25366;&#25496;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#36923;&#36753;&#35268;&#21017;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#30340;&#35821;&#20041;&#21644;&#32467;&#26500;&#20449;&#24687;&#65292;&#33021;&#22815;&#25552;&#39640;&#25512;&#29702;&#24615;&#33021;&#24182;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.01538</link><description>&lt;p&gt;
ChatRule&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25366;&#25496;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#20013;&#30340;&#36923;&#36753;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
ChatRule: Mining Logical Rules with Large Language Models for Knowledge Graph Reasoning. (arXiv:2309.01538v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01538
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;ChatRule&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25366;&#25496;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#36923;&#36753;&#35268;&#21017;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#30340;&#35821;&#20041;&#21644;&#32467;&#26500;&#20449;&#24687;&#65292;&#33021;&#22815;&#25552;&#39640;&#25512;&#29702;&#24615;&#33021;&#24182;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36923;&#36753;&#35268;&#21017;&#23545;&#20110;&#21457;&#29616;&#20851;&#31995;&#20043;&#38388;&#30340;&#36923;&#36753;&#36830;&#25509;&#33267;&#20851;&#37325;&#35201;&#65292;&#21487;&#20197;&#25552;&#39640;&#25512;&#29702;&#24615;&#33021;&#24182;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#30693;&#35782;&#22270;&#35889;&#32467;&#26524;&#12290;&#23613;&#31649;&#24050;&#32463;&#26377;&#35768;&#22810;&#21162;&#21147;&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#25366;&#25496;&#26377;&#24847;&#20041;&#30340;&#36923;&#36753;&#35268;&#21017;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#22312;&#35268;&#21017;&#31354;&#38388;&#19978;&#25628;&#32034;&#35745;&#31639;&#23494;&#38598;&#19988;&#32570;&#20047;&#21487;&#20280;&#32553;&#24615;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#22823;&#35268;&#27169;&#30693;&#35782;&#22270;&#35889;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#24120;&#24120;&#24573;&#35270;&#20102;&#20851;&#31995;&#30340;&#35821;&#20041;&#65292;&#32780;&#36825;&#23545;&#20110;&#25581;&#31034;&#36923;&#36753;&#36830;&#25509;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21644;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#65292;&#24402;&#21151;&#20110;&#23427;&#20204;&#30340;&#26032;&#33021;&#21147;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;ChatRule&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25366;&#25496;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#36923;&#36753;&#35268;&#21017;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#26694;&#26550;&#20197;&#22522;&#20110;LLM&#30340;&#35268;&#21017;&#29983;&#25104;&#22120;&#20026;&#21021;&#22987;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#30693;&#35782;&#22270;&#35889;&#30340;&#35821;&#20041;&#21644;&#32467;&#26500;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Logical rules are essential for uncovering the logical connections between relations, which could improve the reasoning performance and provide interpretable results on knowledge graphs (KGs). Although there have been many efforts to mine meaningful logical rules over KGs, existing methods suffer from the computationally intensive searches over the rule space and a lack of scalability for large-scale KGs. Besides, they often ignore the semantics of relations which is crucial for uncovering logical connections. Recently, large language models (LLMs) have shown impressive performance in the field of natural language processing and various applications, owing to their emergent ability and generalizability. In this paper, we propose a novel framework, ChatRule, unleashing the power of large language models for mining logical rules over knowledge graphs. Specifically, the framework is initiated with an LLM-based rule generator, leveraging both the semantic and structural information of KGs 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#23545;&#27604;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32763;&#35793;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#21521;&#27169;&#22411;&#21576;&#29616;&#27491;&#30830;&#21644;&#38169;&#35823;&#32763;&#35793;&#30340;&#31034;&#20363;&#24182;&#20351;&#29992;&#20559;&#22909;&#25439;&#22833;&#26469;&#25351;&#23548;&#27169;&#22411;&#23398;&#20064;&#65292;&#25105;&#20204;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#22312;&#31934;&#35843;LLMs&#29992;&#20110;&#32763;&#35793;&#20219;&#21153;&#26041;&#38754;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2307.04408</link><description>&lt;p&gt;
TIM: &#20351;&#29992;&#23545;&#27604;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
TIM: Teaching Large Language Models to Translate with Comparison. (arXiv:2307.04408v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04408
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#23545;&#27604;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32763;&#35793;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#21521;&#27169;&#22411;&#21576;&#29616;&#27491;&#30830;&#21644;&#38169;&#35823;&#32763;&#35793;&#30340;&#31034;&#20363;&#24182;&#20351;&#29992;&#20559;&#22909;&#25439;&#22833;&#26469;&#25351;&#23548;&#27169;&#22411;&#23398;&#20064;&#65292;&#25105;&#20204;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#22312;&#31934;&#35843;LLMs&#29992;&#20110;&#32763;&#35793;&#20219;&#21153;&#26041;&#38754;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36890;&#36807;&#25351;&#20196;&#35843;&#25972;&#26041;&#38754;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#38656;&#35201;&#26356;&#19987;&#19994;&#30693;&#35782;&#30340;&#20219;&#21153;&#65288;&#22914;&#32763;&#35793;&#65289;&#20013;&#26377;&#26102;&#20250;&#36935;&#21040;&#22256;&#38590;&#12290;&#36825;&#31181;&#19981;&#36275;&#30340;&#21487;&#33021;&#21407;&#22240;&#20043;&#19968;&#26159;&#25351;&#20196;&#35843;&#25972;&#26088;&#22312;&#29983;&#25104;&#27969;&#30021;&#12289;&#36830;&#36143;&#30340;&#25991;&#26412;&#65292;&#32780;&#19981;&#21463;&#20219;&#20309;&#20219;&#21153;&#29305;&#23450;&#35201;&#27714;&#30340;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#35843;&#25972;&#36739;&#23567;&#30340;LLM&#24182;&#20351;&#29992;&#36739;&#20302;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#21487;&#33021;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#20363;&#23376;&#36827;&#34892;&#23545;&#27604;&#25945;&#25480;LLMs&#23398;&#20064;&#32763;&#35793;&#30340;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#21521;&#27169;&#22411;&#21576;&#29616;&#27491;&#30830;&#21644;&#38169;&#35823;&#32763;&#35793;&#30340;&#31034;&#20363;&#65292;&#24182;&#20351;&#29992;&#20559;&#22909;&#25439;&#22833;&#26469;&#25351;&#23548;&#27169;&#22411;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;WMT2022&#27979;&#35797;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20026;&#31934;&#35843;LLMs&#29992;&#20110;&#32763;&#35793;&#20219;&#21153;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open-sourced large language models (LLMs) have demonstrated remarkable efficacy in various tasks with instruction tuning. However, these models can sometimes struggle with tasks that require more specialized knowledge such as translation. One possible reason for such deficiency is that instruction tuning aims to generate fluent and coherent text that continues from a given instruction without being constrained by any task-specific requirements. Moreover, it can be more challenging for tuning smaller LLMs with lower-quality training data. To address this issue, we propose a novel framework using examples in comparison to teach LLMs to learn translation. Our approach involves presenting the model with examples of correct and incorrect translations and using a preference loss to guide the model's learning. We evaluate our method on WMT2022 test sets and show that it outperforms existing methods. Our findings offer a new perspective on fine-tuning LLMs for translation tasks and provide a p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#31616;&#21270;&#31038;&#20132;&#23186;&#20307;&#20449;&#24687;&#26816;&#32034;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35782;&#21035;&#21307;&#23398;&#23454;&#20307;&#12289;&#26631;&#20934;&#21270;&#23454;&#20307;&#21644;&#20998;&#37197;UMLS&#27010;&#24565;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;COVID-19&#30456;&#20851;&#25512;&#25991;&#30340;&#30151;&#29366;&#35789;&#20856;&#12290;</title><link>http://arxiv.org/abs/2306.16001</link><description>&lt;p&gt;
&#29992;&#28145;&#24230;&#23398;&#20064;&#31616;&#21270;&#31038;&#20132;&#23186;&#20307;&#20449;&#24687;&#26816;&#32034;&#20197;&#25903;&#25345;&#20844;&#20849;&#21355;&#29983;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Streamlining Social Media Information Retrieval for Public Health Research with Deep Learning. (arXiv:2306.16001v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#31616;&#21270;&#31038;&#20132;&#23186;&#20307;&#20449;&#24687;&#26816;&#32034;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35782;&#21035;&#21307;&#23398;&#23454;&#20307;&#12289;&#26631;&#20934;&#21270;&#23454;&#20307;&#21644;&#20998;&#37197;UMLS&#27010;&#24565;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;COVID-19&#30456;&#20851;&#25512;&#25991;&#30340;&#30151;&#29366;&#35789;&#20856;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#22312;&#27969;&#34892;&#30149;&#30417;&#27979;&#20013;&#30340;&#21033;&#29992;&#24050;&#32463;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#35777;&#23454;&#12290;&#28982;&#32780;&#65292;&#24403;&#20351;&#29992;&#39044;&#23450;&#20041;&#30340;&#35789;&#27719;&#34920;&#26469;&#26816;&#32034;&#30456;&#20851;&#35821;&#26009;&#24211;&#26102;&#65292;&#24120;&#24120;&#20250;&#24341;&#20837;&#20559;&#35265;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#26500;&#24314;&#21307;&#23398;&#20439;&#35821;&#21644;&#32479;&#19968;&#21307;&#23398;&#35821;&#35328;&#31995;&#32479;&#65288;UMLS&#65289;&#27010;&#24565;&#30340;&#24191;&#27867;&#23383;&#20856;&#12290;&#35813;&#26694;&#26550;&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65306;&#22522;&#20110;BERT&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#20013;&#35782;&#21035;&#20986;&#21307;&#23398;&#23454;&#20307;&#65307;&#28145;&#24230;&#23398;&#20064;&#39537;&#21160;&#30340;&#26631;&#20934;&#21270;&#27169;&#22359;&#65292;&#29992;&#20110;&#23545;&#25552;&#21462;&#20986;&#30340;&#23454;&#20307;&#36827;&#34892;&#35268;&#33539;&#21270;&#22788;&#29702;&#65307;&#21322;&#30417;&#30563;&#32858;&#31867;&#27169;&#22359;&#65292;&#23558;&#26368;&#21487;&#33021;&#30340;UMLS&#27010;&#24565;&#20998;&#37197;&#32473;&#27599;&#20010;&#35268;&#33539;&#21270;&#23454;&#20307;&#12290;&#25105;&#20204;&#23558;&#35813;&#26694;&#26550;&#24212;&#29992;&#20110;&#20174;2020&#24180;2&#26376;1&#26085;&#21040;2022&#24180;4&#26376;30&#26085;&#26399;&#38388;&#19982;COVID-19&#30456;&#20851;&#30340;&#25512;&#25991;&#65292;&#29983;&#25104;&#20102;&#19968;&#20010;&#30151;&#29366;&#35789;&#20856;&#65288;&#21487;&#22312;https://github.com/ningkko/UMLS_colloquialism/&#19978;&#33719;&#21462;&#65289;&#65292;&#20854;&#20013;&#21253;&#21547;9,249&#20010;&#26631;&#20934;&#21270;&#23454;&#20307;&#65292;&#26144;&#23556;&#21040;876&#20010;UMLS&#27010;&#24565;&#21644;38,175&#20010;&#20442;&#35821;&#34920;&#36798;&#12290;&#35813;&#26694;&#26550;&#30340;&#28436;&#31034;
&lt;/p&gt;
&lt;p&gt;
The utilization of social media in epidemic surveillance has been well established. Nonetheless, bias is often introduced when pre-defined lexicons are used to retrieve relevant corpus. This study introduces a framework aimed at curating extensive dictionaries of medical colloquialisms and Unified Medical Language System (UMLS) concepts. The framework comprises three modules: a BERT-based Named Entity Recognition (NER) model that identifies medical entities from social media content, a deep-learning powered normalization module that standardizes the extracted entities, and a semi-supervised clustering module that assigns the most probable UMLS concept to each standardized entity. We applied this framework to COVID-19-related tweets from February 1, 2020, to April 30, 2022, generating a symptom dictionary (available at https://github.com/ningkko/UMLS_colloquialism/) composed of 9,249 standardized entities mapped to 876 UMLS concepts and 38,175 colloquial expressions. This framework demo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;AmP&#30340;&#26694;&#26550;&#12289;&#25968;&#25454;&#38598;&#21644;&#25361;&#25112;&#65292;&#29992;&#20110;&#23558;&#27169;&#31946;&#33258;&#28982;&#35821;&#35328;&#32763;&#35793;&#25104;&#24418;&#24335;&#21270;&#34920;&#31034;&#65292;&#22914;&#36923;&#36753;&#21644;&#20195;&#30721;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#27809;&#26377;&#26126;&#30830;&#25351;&#31034;&#30340;&#24773;&#20917;&#19979;&#38590;&#20197;&#25429;&#25417;&#21040;&#21487;&#33021;&#30340;&#24847;&#20041;&#20998;&#24067;&#65292;&#20294;&#24403;&#36755;&#20837;&#20013;&#23384;&#22312;&#27169;&#31946;&#24615;&#26102;&#65292;&#27169;&#22411;&#33021;&#22815;&#24456;&#22909;&#22320;&#25429;&#25417;&#21040;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2306.00824</link><description>&lt;p&gt;
&#20855;&#26377;&#27169;&#31946;&#36755;&#20837;&#30340;&#38646;&#21644;&#23569;&#26679;&#26412;&#35821;&#20041;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
Zero and Few-shot Semantic Parsing with Ambiguous Inputs. (arXiv:2306.00824v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00824
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;AmP&#30340;&#26694;&#26550;&#12289;&#25968;&#25454;&#38598;&#21644;&#25361;&#25112;&#65292;&#29992;&#20110;&#23558;&#27169;&#31946;&#33258;&#28982;&#35821;&#35328;&#32763;&#35793;&#25104;&#24418;&#24335;&#21270;&#34920;&#31034;&#65292;&#22914;&#36923;&#36753;&#21644;&#20195;&#30721;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#27809;&#26377;&#26126;&#30830;&#25351;&#31034;&#30340;&#24773;&#20917;&#19979;&#38590;&#20197;&#25429;&#25417;&#21040;&#21487;&#33021;&#30340;&#24847;&#20041;&#20998;&#24067;&#65292;&#20294;&#24403;&#36755;&#20837;&#20013;&#23384;&#22312;&#27169;&#31946;&#24615;&#26102;&#65292;&#27169;&#22411;&#33021;&#22815;&#24456;&#22909;&#22320;&#25429;&#25417;&#21040;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29992;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#24847;&#24605;&#26102;&#24120;&#24120;&#20250;&#36935;&#21040;&#27169;&#31946;&#24615;&#30340;&#25361;&#25112;&#65292;&#20294;&#22312;&#23558;&#35821;&#35328;&#26144;&#23556;&#21040;&#24418;&#24335;&#21270;&#35774;&#35745;&#30340;&#34920;&#31034;&#26102;&#65292;&#36825;&#31181;&#27169;&#31946;&#24615;&#32463;&#24120;&#34987;&#24573;&#30053;&#25110;&#25925;&#24847;&#21024;&#38500;&#65292;&#36825;&#20123;&#20219;&#21153;&#36890;&#24120;&#20551;&#35774;&#35821;&#35328;&#21644;&#24418;&#24335;&#21270;&#34920;&#31034;&#20043;&#38388;&#26377;&#19968;&#23545;&#19968;&#30340;&#26144;&#23556;&#12290;&#25105;&#20204;&#23581;&#35797;&#36890;&#36807;&#24341;&#20837; AmP&#65292;&#19968;&#20010;&#23558;&#27169;&#31946;&#33258;&#28982;&#35821;&#35328;&#32763;&#35793;&#25104;&#36923;&#36753;&#21644;&#20195;&#30721;&#31561;&#24418;&#24335;&#21270;&#34920;&#31034;&#30340;&#26694;&#26550;&#12289;&#25968;&#25454;&#38598;&#21644;&#25361;&#25112;&#65292;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#27169;&#26495;&#24182;&#29983;&#25104;&#20102;&#20116;&#20010;&#26126;&#30830;&#35760;&#24405;&#30340;&#35821;&#35328;&#27169;&#31946;&#24615;&#30340;&#25968;&#25454;&#12290;&#20351;&#29992; AmP&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20960;&#20010;&#38646;&#26679;&#26412;&#25991;&#26412;&#21040;&#20195;&#30721;&#31995;&#32479;&#22914;&#20309;&#22788;&#29702;&#27169;&#31946;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19977;&#20010;&#26032;&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#27809;&#26377;&#26126;&#30830;&#25351;&#31034;&#30340;&#24773;&#20917;&#19979;&#24456;&#38590;&#25429;&#25417;&#21040;&#21487;&#33021;&#24847;&#20041;&#30340;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#24403;&#36755;&#20837;&#20013;&#23384;&#22312;&#27169;&#31946;&#24615;&#26102;&#65292;&#27169;&#22411;&#33021;&#22815;&#24456;&#22909;&#22320;&#25429;&#25417;&#21040;&#20998;&#24067;&#12290;&#36825;&#20123;&#32467;&#26524;&#21628;&#21505;&#22312;&#25968;&#25454;&#38598;&#20013;&#26126;&#30830;&#21253;&#21547;&#27169;&#31946;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the frequent challenges posed by ambiguity when representing meaning via natural language, it is often ignored or deliberately removed in tasks mapping language to formally-designed representations, which generally assume a one-to-one mapping between linguistic and formal representations. We attempt to address this shortcoming by introducing AmP, a framework, dataset, and challenge for translating ambiguous natural language to formal representations like logic and code. We define templates and generate data for five well-documented linguistic ambiguities. Using AmP, we investigate how several few-shot text-to-code systems handle ambiguity, introducing three new metrics. We find that large pre-trained models perform poorly at capturing the distribution of possible meanings without deliberate instruction. However, models are able to capture the distribution well when ambiguity is attested in their inputs. These results motivate a call for including ambiguity explicitly in dataset
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;GPT-3&#21644;GPT-4&#22312;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#34920;&#29616;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#21487;&#33021;&#20135;&#29983;&#30340;&#38169;&#35823;&#31867;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2305.16326</link><description>&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;: &#22522;&#20934;&#12289;&#22522;&#32447;&#21644;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations. (arXiv:2305.16326v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;GPT-3&#21644;GPT-4&#22312;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#34920;&#29616;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#21487;&#33021;&#20135;&#29983;&#30340;&#38169;&#35823;&#31867;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#25163;&#21160;&#31579;&#36873;&#21644;&#25552;&#21462;&#30693;&#35782;&#21464;&#24471;&#22256;&#38590;&#12290;&#33258;&#21160;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;BioNLP&#65289;&#25216;&#26415;&#26377;&#21161;&#20110;&#20943;&#36731;&#36825;&#31181;&#36127;&#25285;&#12290;&#36817;&#24180;&#26469;&#65292;&#22914;GPT-3&#21644;GPT-4&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22240;&#20854;&#21331;&#36234;&#30340;&#24615;&#33021;&#32780;&#21463;&#21040;&#37325;&#35270;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#22312;BioNLP&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#23545;&#26041;&#27861;&#24320;&#21457;&#21644;&#19979;&#28216;&#29992;&#25143;&#30340;&#24433;&#21709;&#20173;&#26410;&#24471;&#21040;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#65288;1&#65289;&#22312;&#22235;&#20010;&#24212;&#29992;&#31243;&#24207;&#20013;&#22312;&#20843;&#20010;BioNLP&#25968;&#25454;&#38598;&#20013;&#24314;&#31435;&#20102;GPT-3&#21644;GPT-4&#22312;&#38646;-shot&#21644;&#19968;-shot&#35774;&#32622;&#19979;&#30340;&#22522;&#20934;&#34920;&#29616;&#65292;&#21253;&#25324;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65292;&#20851;&#31995;&#25552;&#21462;&#65292;&#22810;&#26631;&#31614;&#25991;&#26723;&#20998;&#31867;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;&#25512;&#29702;&#65307;&#65288;2&#65289;&#23457;&#26597;&#20102;LLMs&#20135;&#29983;&#30340;&#38169;&#35823;&#65292;&#24182;&#23558;&#38169;&#35823;&#20998;&#20026;&#19977;&#31181;&#31867;&#22411;&#65306;&#32570;&#22833;&#65292;&#19981;&#19968;&#33268;&#21644;&#19981;&#38656;&#35201;&#30340;&#20154;&#24037;&#20869;&#23481;&#65307;&#65288;3&#65289;&#25552;&#20986;&#20102;&#20351;&#29992;LLMs&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biomedical literature is growing rapidly, making it challenging to curate and extract knowledge manually. Biomedical natural language processing (BioNLP) techniques that can automatically extract information from biomedical literature help alleviate this burden. Recently, large Language Models (LLMs), such as GPT-3 and GPT-4, have gained significant attention for their impressive performance. However, their effectiveness in BioNLP tasks and impact on method development and downstream users remain understudied. This pilot study (1) establishes the baseline performance of GPT-3 and GPT-4 at both zero-shot and one-shot settings in eight BioNLP datasets across four applications: named entity recognition, relation extraction, multi-label document classification, and semantic similarity and reasoning, (2) examines the errors produced by the LLMs and categorized the errors into three types: missingness, inconsistencies, and unwanted artificial content, and (3) provides suggestions for using L
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22270;&#30340;&#25991;&#26412;&#34920;&#31034;&#26041;&#27861;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#25991;&#26412;&#36755;&#20837;&#29305;&#24449;&#21644;&#39046;&#22495;&#35201;&#32032;&#23545;&#22270;&#30340;&#24615;&#33021;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;BERT&#22312;&#22788;&#29702;&#30701;&#25991;&#26412;&#26102;&#38590;&#20197;&#25910;&#25947;&#65292;&#22270;&#26041;&#27861;&#23545;&#20110;&#36739;&#38271;&#30340;&#25991;&#26723;&#29305;&#21035;&#26377;&#30410;&#12290;</title><link>http://arxiv.org/abs/2305.14578</link><description>&lt;p&gt;
&#36830;&#25509;&#28857;&#65306;&#22522;&#20110;&#22270;&#32593;&#32476;&#30340;&#25991;&#26412;&#34920;&#31034;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#26368;&#20339;&#34920;&#29616;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Connecting the Dots: What Graph-Based Text Representations Work Best for Text Classification using Graph Neural Networks?. (arXiv:2305.14578v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14578
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22270;&#30340;&#25991;&#26412;&#34920;&#31034;&#26041;&#27861;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#25991;&#26412;&#36755;&#20837;&#29305;&#24449;&#21644;&#39046;&#22495;&#35201;&#32032;&#23545;&#22270;&#30340;&#24615;&#33021;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;BERT&#22312;&#22788;&#29702;&#30701;&#25991;&#26412;&#26102;&#38590;&#20197;&#25910;&#25947;&#65292;&#22270;&#26041;&#27861;&#23545;&#20110;&#36739;&#38271;&#30340;&#25991;&#26723;&#29305;&#21035;&#26377;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#32467;&#26500;&#24863;&#30693;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25104;&#21151;&#65292;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#25506;&#32034;&#20102;&#23427;&#20204;&#20316;&#20026;&#20256;&#32479;&#29305;&#24449;&#34920;&#31034;&#27169;&#22411;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#20165;&#32771;&#34385;&#20102;&#29305;&#23450;&#39046;&#22495;&#65292;&#24182;&#39564;&#35777;&#20102;&#20855;&#26377;&#29305;&#23450;&#29305;&#24449;&#30340;&#25968;&#25454;&#12290;&#26412;&#25991;&#23545;&#25552;&#20986;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#30340;&#22522;&#20110;&#22270;&#30340;&#25991;&#26412;&#34920;&#31034;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#30830;&#23450;&#20102;&#23454;&#38469;&#23454;&#26045;&#30340;&#21547;&#20041;&#21644;&#39046;&#22495;&#20013;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20116;&#20010;&#25968;&#25454;&#38598;&#20013;&#30340;&#20960;&#31181;GNN&#26550;&#26500;&#20197;&#21450;BERT&#65292;&#28085;&#30422;&#20102;&#38271;&#30701;&#25991;&#26723;&#12290;&#32467;&#26524;&#34920;&#26126;&#65306;i&#65289;&#22270;&#30340;&#24615;&#33021;&#19982;&#25991;&#26412;&#36755;&#20837;&#29305;&#24449;&#21644;&#39046;&#22495;&#23494;&#20999;&#30456;&#20851;&#65292;ii&#65289;&#23613;&#31649;&#20854;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;BERT&#22312;&#22788;&#29702;&#30701;&#25991;&#26412;&#26102;&#38590;&#20197;&#25910;&#25947;&#65292; iii&#65289;&#22270;&#26041;&#27861;&#23545;&#20110;&#36739;&#38271;&#30340;&#25991;&#26723;&#29305;&#21035;&#26377;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the success of Graph Neural Networks (GNNs) for structure-aware machine learning, numerous studies have explored their application to text classification, as an alternative to traditional feature representation models. However, most studies considered just a specific domain and validated on data with particular characteristics. This work presents an extensive empirical investigation of graph-based text representation methods proposed for text classification, identifying practical implications and open challenges in the field. We compare several GNN architectures as well as BERT across five datasets, encompassing short and also long documents. The results show that: i) graph performance is highly related to the textual input features and domain, ii) despite its outstanding performance, BERT has difficulties converging when dealing with short texts, iii) graph methods are particularly beneficial for longer documents.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#20849;&#20139;&#35789;&#27719;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20041;&#35789;&#32423;&#20449;&#24687;&#20256;&#36755;&#36335;&#24452;&#21644;&#20351;&#29992;&#22270;&#32593;&#32476;&#26469;&#34701;&#21512;&#36328;&#35821;&#35328;&#30340;&#35789;&#23884;&#20837;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#25552;&#39640;&#30456;&#20284;&#21547;&#20041;&#35789;&#30340;&#23545;&#40784;&#24615;&#21644;BLEU&#20998;&#25968;&#30340;&#19968;&#33268;&#25552;&#21319;&#12290;&#27492;&#26041;&#27861;&#21482;&#38656;&#35201;&#23569;&#37327;&#39069;&#22806;&#21442;&#25968;&#19988;&#35745;&#31639;&#25104;&#26412;&#22686;&#21152;&#26377;&#38480;&#65292;&#24182;&#19988;&#25512;&#29702;&#26102;&#38388;&#19982;&#22522;&#32447;&#30456;&#21516;&#12290;</title><link>http://arxiv.org/abs/2305.14189</link><description>&lt;p&gt;
&#36229;&#36234;&#20849;&#20139;&#35789;&#27719;&#65306;&#22686;&#21152;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#34920;&#31034;&#35789;&#35821;&#30456;&#20284;&#24615;
&lt;/p&gt;
&lt;p&gt;
Beyond Shared Vocabulary: Increasing Representational Word Similarities across Languages for Multilingual Machine Translation. (arXiv:2305.14189v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#20849;&#20139;&#35789;&#27719;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20041;&#35789;&#32423;&#20449;&#24687;&#20256;&#36755;&#36335;&#24452;&#21644;&#20351;&#29992;&#22270;&#32593;&#32476;&#26469;&#34701;&#21512;&#36328;&#35821;&#35328;&#30340;&#35789;&#23884;&#20837;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#25552;&#39640;&#30456;&#20284;&#21547;&#20041;&#35789;&#30340;&#23545;&#40784;&#24615;&#21644;BLEU&#20998;&#25968;&#30340;&#19968;&#33268;&#25552;&#21319;&#12290;&#27492;&#26041;&#27861;&#21482;&#38656;&#35201;&#23569;&#37327;&#39069;&#22806;&#21442;&#25968;&#19988;&#35745;&#31639;&#25104;&#26412;&#22686;&#21152;&#26377;&#38480;&#65292;&#24182;&#19988;&#25512;&#29702;&#26102;&#38388;&#19982;&#22522;&#32447;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;(MNMT)&#20013;&#65292;&#20351;&#29992;&#20849;&#20139;&#30340;&#35789;&#27719;&#26159;&#24120;&#35265;&#30340;&#20570;&#27861;&#12290;&#38500;&#20102;&#31616;&#21333;&#30340;&#35774;&#35745;&#22806;&#65292;&#20849;&#20139;&#26631;&#35760;&#22312;&#31215;&#26497;&#30340;&#30693;&#35782;&#36716;&#31227;&#20013;&#36215;&#30528;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#20551;&#35774;&#20849;&#20139;&#26631;&#35760;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#25351;&#30340;&#26159;&#30456;&#20284;&#30340;&#21547;&#20041;&#12290;&#28982;&#32780;&#65292;&#24403;&#35789;&#27719;&#30340;&#37325;&#21472;&#36739;&#23567;&#26102;&#65292;&#23588;&#20854;&#26159;&#30001;&#20110;&#19981;&#21516;&#30340;&#20070;&#20889;&#31995;&#32479;&#65292;&#36716;&#31227;&#34987;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35789;&#31561;&#20215;&#31867;&#23450;&#20041;&#20102;&#35789;&#32423;&#20449;&#24687;&#20256;&#36755;&#36335;&#24452;&#65292;&#24182;&#20381;&#36182;&#22270;&#32593;&#32476;&#26469;&#34701;&#21512;&#36328;&#35821;&#35328;&#30340;&#35789;&#23884;&#20837;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#21183;&#65306;1) &#20855;&#26377;&#30456;&#20284;&#21547;&#20041;&#30340;&#35789;&#30340;&#23884;&#20837;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#26356;&#22909;&#22320;&#23545;&#40784;&#65292;2) &#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#39640;&#21644;&#20302;&#36164;&#28304;MNMT&#26041;&#38754;&#23454;&#29616;&#20102;&#19968;&#33268;&#30340;BLEU&#25552;&#21319;&#36798;2.3&#20010;&#28857;&#65292;3) &#38656;&#35201;&#23569;&#20110;1.0%&#30340;&#39069;&#22806;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#24182;&#19988;&#35745;&#31639;&#25104;&#26412;&#30340;&#22686;&#21152;&#26377;&#38480;&#65292;&#32780;&#25512;&#29702;&#26102;&#38388;&#19982;&#22522;&#32447;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Using a vocabulary that is shared across languages is common practice in Multilingual Neural Machine Translation (MNMT). In addition to its simple design, shared tokens play an important role in positive knowledge transfer, assuming that shared tokens refer to similar meanings across languages. However, when word overlap is small, especially due to different writing systems, transfer is inhibited. In this paper, we define word-level information transfer pathways via word equivalence classes and rely on graph networks to fuse word embeddings across languages. Our experiments demonstrate the advantages of our approach: 1) embeddings of words with similar meanings are better aligned across languages, 2) our method achieves consistent BLEU improvements of up to 2.3 points for high- and low-resource MNMT, and 3) less than 1.0\% additional trainable parameters are required with a limited increase in computational costs, while inference time remains identical to the baseline. We release the c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#20998;&#31867;&#20307;&#31995;&#65292;&#20998;&#31867;&#21644;&#27604;&#36739;&#20102;&#22522;&#30784;&#27169;&#22411;&#21644;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#30340;&#29305;&#28857;&#12290;&#23427;&#20026;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#26102;&#20570;&#20986;&#20027;&#35201;&#30340;&#35774;&#35745;&#20915;&#31574;&#25552;&#20379;&#20102;&#20855;&#20307;&#30340;&#25351;&#23548;&#65292;&#24182;&#31361;&#20986;&#20102;&#30456;&#20851;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.05352</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#35774;&#35745;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework for Designing Foundation Model based Systems. (arXiv:2305.05352v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05352
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#20998;&#31867;&#20307;&#31995;&#65292;&#20998;&#31867;&#21644;&#27604;&#36739;&#20102;&#22522;&#30784;&#27169;&#22411;&#21644;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#30340;&#29305;&#28857;&#12290;&#23427;&#20026;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#26102;&#20570;&#20986;&#20027;&#35201;&#30340;&#35774;&#35745;&#20915;&#31574;&#25552;&#20379;&#20102;&#20855;&#20307;&#30340;&#25351;&#23548;&#65292;&#24182;&#31361;&#20986;&#20102;&#30456;&#20851;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25512;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#22914;ChatGPT&#65292;&#36825;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#22522;&#30784;&#27169;&#22411;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#22522;&#30784;&#27169;&#22411;&#34987;&#24191;&#27867;&#35748;&#20026;&#23558;&#25104;&#20026;&#26410;&#26469;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#22522;&#30707;&#12290;&#30001;&#20110;&#22522;&#30784;&#27169;&#22411;&#22788;&#20110;&#26089;&#26399;&#38454;&#27573;&#65292;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#35774;&#35745;&#23578;&#26410;&#24471;&#21040;&#31995;&#32479;&#22320;&#25506;&#32034;&#12290;&#20154;&#20204;&#23545;&#22312;&#36719;&#20214;&#26550;&#26500;&#20013;&#24341;&#20837;&#22522;&#30784;&#27169;&#22411;&#30340;&#24433;&#21709;&#30693;&#20043;&#29978;&#23569;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#20998;&#31867;&#27861;&#65292;&#23545;&#22522;&#30784;&#27169;&#22411;&#21644;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#30340;&#29305;&#28857;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#20998;&#31867;&#27861;&#21253;&#25324;&#19977;&#20010;&#31867;&#21035;&#65306;&#22522;&#30784;&#27169;&#22411;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#12289;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#26550;&#26500;&#35774;&#35745;&#21644;&#36127;&#36131;&#20219;&#30340;AI&#35774;&#35745;&#12290;&#36825;&#20010;&#20998;&#31867;&#27861;&#20026;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#26102;&#20570;&#20986;&#20027;&#35201;&#30340;&#35774;&#35745;&#20915;&#31574;&#25552;&#20379;&#20102;&#20855;&#20307;&#30340;&#25351;&#23548;&#65292;&#24182;&#31361;&#20986;&#20102;&#30456;&#20851;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent release of large language model (LLM) based chatbots, such as ChatGPT, has attracted significant attention on foundations models. It is widely believed that foundation models will serve as the fundamental building blocks for future AI systems. As foundation models are in their early stages, the design of foundation model based systems has not yet been systematically explored. There is little understanding about the impact of introducing foundation models in software architecture. Therefore, in this paper, we propose a taxonomy of foundation model based systems, which classifies and compares the characteristics of foundation models and foundation model based systems. Our taxonomy comprises three categories: foundation model pretraining and fine-tuning, architecture design of foundation model based systems, and responsible-AI-by-design. This taxonomy provides concrete guidance for making major design decisions when designing foundation model based systems and highlights trade-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110; GPT-3.5 &#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#23616;&#38480;&#24615;&#65292;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.14317</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#20195;&#30721;&#29983;&#25104;&#30340;&#26368;&#20808;&#36827;&#35780;&#20272;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Are State-of-the-Art Evaluators of Code Generation. (arXiv:2304.14317v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110; GPT-3.5 &#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#23616;&#38480;&#24615;&#65292;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#25512;&#21160;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#29983;&#25104;&#25991;&#26412;&#30340;&#33021;&#21147;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#22312;&#26426;&#22120;&#32763;&#35793;&#21644;&#25688;&#35201;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#20854;&#22312;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#20173;&#28982;&#23384;&#22312;&#38480;&#21046;&#12290;&#36825;&#20123;&#20219;&#21153;&#25152;&#38656;&#30340;&#32534;&#31243;&#27010;&#24565;&#30340;&#22797;&#26434;&#24615;&#20351;&#24471;&#24320;&#21457;&#35780;&#20272;&#25351;&#26631;&#20197;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#19968;&#33268;&#21464;&#24471;&#22256;&#38590;&#12290;&#20197;&#35789;&#27719;&#21305;&#37197;&#20026;&#22522;&#30784;&#30340;&#24230;&#37327;&#26631;&#20934;&#65288;&#22914;BLEU&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#20013;&#19982;&#20154;&#24037;&#20174;&#19994;&#32773;&#30340;&#30456;&#20851;&#24615;&#36739;&#24369;&#12290;&#27492;&#22806;&#65292;&#22312;&#20302;&#36164;&#28304;&#39046;&#22495;&#20013;&#21033;&#29992;&#20154;&#20026;&#32534;&#20889;&#30340;&#27979;&#35797;&#22871;&#20214;&#36827;&#34892;&#21151;&#33021;&#27491;&#30830;&#24615;&#35780;&#20272;&#20063;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38556;&#30861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;GPT-3.5&#30340;&#20195;&#30721;&#29983;&#25104;&#35780;&#20272;&#26694;&#26550;&#65288;\texttt{GPT-3.5-turbo}&#65289;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#21462;&#24471;&#26356;&#22909;&#30340;&#30456;&#20851;&#24615;&#26469;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in the field of natural language generation have facilitated the use of large language models to assess the quality of generated text. Although these models have shown promising results in tasks such as machine translation and summarization, their applicability in code generation tasks remains limited without human involvement. The complexity of programming concepts required for such tasks makes it difficult to develop evaluation metrics that align with human judgment. Token-matching-based metrics, such as BLEU, have demonstrated weak correlations with human practitioners in code generation tasks. Moreover, the utilization of human-written test suites to evaluate functional correctness can be challenging in domains with low resources. To overcome these obstacles, we propose a new evaluation framework based on the GPT-3.5 (\texttt{GPT-3.5-turbo}), for code generation assessments. Our framework addresses the limitations of existing approaches by achieving superior cor
&lt;/p&gt;</description></item><item><title>ETPNav&#26159;&#19968;&#20010;&#33021;&#22815;&#22312;&#36830;&#32493;&#29615;&#22659;&#20013;&#36827;&#34892;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#30340;&#26032;&#23548;&#33322;&#26694;&#26550;&#65292;&#23427;&#20855;&#26377;&#20004;&#20010;&#20851;&#38190;&#25216;&#33021;&#65306;&#33021;&#22815;&#25277;&#35937;&#29615;&#22659;&#19982;&#29983;&#25104;&#38271;&#31243;&#23548;&#33322;&#35745;&#21010;&#20197;&#21450;&#22312;&#36830;&#32493;&#29615;&#22659;&#20013;&#36991;&#38556;&#25511;&#21046;&#30340;&#33021;&#21147;&#12290;ETPNav&#20351;&#29992;&#28436;&#21270;&#31639;&#27861;&#20248;&#21270;&#25299;&#25169;&#35268;&#21010;&#27169;&#22359;&#24182;&#22312;Matterport3D&#27169;&#25311;&#22120;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102;&#20154;&#31867;&#27700;&#24179;&#30340;VLN-CE&#20219;&#21153;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.03047</link><description>&lt;p&gt;
ETPNav: &#22312;&#36830;&#32493;&#29615;&#22659;&#20013;&#28436;&#21270;&#25299;&#25169;&#35268;&#21010;&#30340;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
ETPNav: Evolving Topological Planning for Vision-Language Navigation in Continuous Environments. (arXiv:2304.03047v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03047
&lt;/p&gt;
&lt;p&gt;
ETPNav&#26159;&#19968;&#20010;&#33021;&#22815;&#22312;&#36830;&#32493;&#29615;&#22659;&#20013;&#36827;&#34892;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#30340;&#26032;&#23548;&#33322;&#26694;&#26550;&#65292;&#23427;&#20855;&#26377;&#20004;&#20010;&#20851;&#38190;&#25216;&#33021;&#65306;&#33021;&#22815;&#25277;&#35937;&#29615;&#22659;&#19982;&#29983;&#25104;&#38271;&#31243;&#23548;&#33322;&#35745;&#21010;&#20197;&#21450;&#22312;&#36830;&#32493;&#29615;&#22659;&#20013;&#36991;&#38556;&#25511;&#21046;&#30340;&#33021;&#21147;&#12290;ETPNav&#20351;&#29992;&#28436;&#21270;&#31639;&#27861;&#20248;&#21270;&#25299;&#25169;&#35268;&#21010;&#27169;&#22359;&#24182;&#22312;Matterport3D&#27169;&#25311;&#22120;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102;&#20154;&#31867;&#27700;&#24179;&#30340;VLN-CE&#20219;&#21153;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#38656;&#35201;&#26234;&#33021;&#20307;&#36981;&#24490;&#25351;&#31034;&#22312;&#29615;&#22659;&#20013;&#23548;&#33322;&#65292;&#35813;&#20219;&#21153;&#22312;&#20307;&#39564;&#24335;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#65292;&#22914;&#33258;&#27835;&#23548;&#33322;&#12289;&#25628;&#32034;&#19982;&#25937;&#25588;&#21644;&#20154;&#26426;&#20132;&#20114;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#20026;&#23454;&#29992;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#26223; - &#22312;&#36830;&#32493;&#29615;&#22659;&#20013;&#36827;&#34892;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#65288;VLN-CE&#65289;&#12290;&#20026;&#20102;&#24320;&#21457;&#19968;&#20010;&#24378;&#22823;&#30340;VLN-CE&#20195;&#29702;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23548;&#33322;&#26694;&#26550;ETPNav&#65292;&#23427;&#19987;&#27880;&#20110;&#20004;&#20010;&#20851;&#38190;&#25216;&#33021;&#65306;1&#65289;&#25277;&#35937;&#29615;&#22659;&#21644;&#29983;&#25104;&#38271;&#31243;&#23548;&#33322;&#35745;&#21010;&#30340;&#33021;&#21147;&#65307;&#21644;2&#65289;&#22312;&#36830;&#32493;&#29615;&#22659;&#20013;&#36991;&#38556;&#25511;&#21046;&#30340;&#33021;&#21147;&#12290;ETPNav&#36890;&#36807;&#33258;&#32452;&#32455;&#27839;&#30528;&#32463;&#36807;&#30340;&#36335;&#24452;&#39044;&#27979;&#30340;&#36335;&#26631;&#36827;&#34892;&#22312;&#32447;&#29615;&#22659;&#25299;&#25169;&#26144;&#23556;&#65292;&#32780;&#19981;&#38656;&#35201;&#20808;&#21069;&#30340;&#29615;&#22659;&#32463;&#39564;&#12290;&#23427;&#23558;&#23548;&#33322;&#36807;&#31243;&#20998;&#35299;&#20026;&#39640;&#23618;&#35268;&#21010;&#21644;&#20302;&#23618;&#25511;&#21046;&#12290;&#21516;&#26102;&#65292;ETPNav&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#28436;&#21270;&#31639;&#27861;&#26469;&#20248;&#21270;&#25299;&#25169;&#35268;&#21010;&#27169;&#22359;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#38271;&#26399;&#23548;&#33322;&#35745;&#21010;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;Matterport3D&#27169;&#25311;&#22120;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#20219;&#24847;&#36215;&#28857;&#21644;&#32456;&#28857;&#30340;VLN-CE&#20219;&#21153;&#20013;&#36798;&#21040;&#20102;&#20154;&#31867;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-language navigation is a task that requires an agent to follow instructions to navigate in environments. It becomes increasingly crucial in the field of embodied AI, with potential applications in autonomous navigation, search and rescue, and human-robot interaction. In this paper, we propose to address a more practical yet challenging counterpart setting - vision-language navigation in continuous environments (VLN-CE). To develop a robust VLN-CE agent, we propose a new navigation framework, ETPNav, which focuses on two critical skills: 1) the capability to abstract environments and generate long-range navigation plans, and 2) the ability of obstacle-avoiding control in continuous environments. ETPNav performs online topological mapping of environments by self-organizing predicted waypoints along a traversed path, without prior environmental experience. It privileges the agent to break down the navigation procedure into high-level planning and low-level control. Concurrently, ET
&lt;/p&gt;</description></item><item><title>VivesDebate-Speech&#26159;&#19968;&#20010;&#29992;&#20110;&#21033;&#29992;&#38899;&#39057;&#29305;&#24449;&#36827;&#34892;&#35770;&#35777;&#25366;&#25496;&#30340;&#21475;&#35821;&#35770;&#35777;&#35821;&#26009;&#24211;&#65292;&#36890;&#36807;&#25972;&#21512;&#38899;&#39057;&#29305;&#24449;&#65292;&#25913;&#36827;&#20102;&#35770;&#35777;&#25366;&#25496;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.12584</link><description>&lt;p&gt;
VivesDebate-Speech: &#29992;&#20110;&#21033;&#29992;&#38899;&#39057;&#29305;&#24449;&#36827;&#34892;&#35770;&#35777;&#25366;&#25496;&#30340;&#21475;&#35821;&#35770;&#35777;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
VivesDebate-Speech: A Corpus of Spoken Argumentation to Leverage Audio Features for Argument Mining. (arXiv:2302.12584v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12584
&lt;/p&gt;
&lt;p&gt;
VivesDebate-Speech&#26159;&#19968;&#20010;&#29992;&#20110;&#21033;&#29992;&#38899;&#39057;&#29305;&#24449;&#36827;&#34892;&#35770;&#35777;&#25366;&#25496;&#30340;&#21475;&#35821;&#35770;&#35777;&#35821;&#26009;&#24211;&#65292;&#36890;&#36807;&#25972;&#21512;&#38899;&#39057;&#29305;&#24449;&#65292;&#25913;&#36827;&#20102;&#35770;&#35777;&#25366;&#25496;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;VivesDebate-Speech&#65292;&#19968;&#20010;&#29992;&#20110;&#21033;&#29992;&#38899;&#39057;&#29305;&#24449;&#36827;&#34892;&#35770;&#35777;&#25366;&#25496;&#20219;&#21153;&#30340;&#21475;&#35821;&#35770;&#35777;&#35821;&#26009;&#24211;&#12290;&#35813;&#35821;&#26009;&#24211;&#30340;&#21019;&#24314;&#23545;&#20110;&#35821;&#38899;&#22788;&#29702;&#21644;&#35770;&#35777;&#25366;&#25496;&#39046;&#22495;&#30340;&#20132;&#21449;&#30740;&#31350;&#20316;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#65292;&#26159;&#35813;&#39046;&#22495;&#20013;&#26368;&#23436;&#25972;&#30340;&#20844;&#24320;&#36164;&#28304;&#20043;&#19968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#39318;&#27425;&#23581;&#35797;&#30340;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#23558;&#38899;&#39057;&#29305;&#24449;&#25972;&#21512;&#21040;&#35770;&#35777;&#25366;&#25496;&#27969;&#31243;&#20013;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#25552;&#20379;&#30340;&#32467;&#26524;&#21487;&#20316;&#20026;&#26410;&#26469;&#30740;&#31350;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we describe VivesDebate-Speech, a corpus of spoken argumentation created to leverage audio features for argument mining tasks. The creation of this corpus represents an important contribution to the intersection of speech processing and argument mining communities, and one of the most complete publicly available resources in this topic. Moreover, we have performed a set of first-of-their-kind experiments which show an improvement when integrating audio features into the argument mining pipeline. The provided results can be used as a baseline for future research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#24341;&#20837;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;NLI&#27169;&#22411;&#35780;&#20272;&#29983;&#25104;&#30340;&#21477;&#23376;&#26159;&#21542;&#31526;&#21512;&#12289;&#19982;&#21407;&#22987;&#25991;&#26412;&#30456;&#30683;&#30462;&#25110;&#20013;&#31435;&#12290;&#26368;&#22823;&#21270;&#20013;&#31435;&#31867;&#21035;&#30340;NLI&#31574;&#30053;&#25552;&#20379;&#20102;&#26368;&#39640;&#36136;&#37327;&#30340;&#29983;&#25104;&#25991;&#26412;&#65292;&#26080;&#35770;&#21442;&#25968;&#21462;&#20540;&#22914;&#20309;&#12290;</title><link>http://arxiv.org/abs/2302.08577</link><description>&lt;p&gt;
&#20445;&#25345;&#20013;&#31435;&#65306;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#25913;&#36827;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Keep it Neutral: Using Natural Language Inference to Improve Generation. (arXiv:2302.08577v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08577
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#24341;&#20837;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;NLI&#27169;&#22411;&#35780;&#20272;&#29983;&#25104;&#30340;&#21477;&#23376;&#26159;&#21542;&#31526;&#21512;&#12289;&#19982;&#21407;&#22987;&#25991;&#26412;&#30456;&#30683;&#30462;&#25110;&#20013;&#31435;&#12290;&#26368;&#22823;&#21270;&#20013;&#31435;&#31867;&#21035;&#30340;NLI&#31574;&#30053;&#25552;&#20379;&#20102;&#26368;&#39640;&#36136;&#37327;&#30340;&#29983;&#25104;&#25991;&#26412;&#65292;&#26080;&#35770;&#21442;&#25968;&#21462;&#20540;&#22914;&#20309;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#23558;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#24341;&#20837;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;NLI&#27169;&#22411;&#26469;&#35780;&#20272;&#29983;&#25104;&#30340;&#21477;&#23376;&#26159;&#21542;&#31526;&#21512;&#12289;&#19982;&#21407;&#22987;&#25991;&#26412;&#30456;&#30683;&#30462;&#25110;&#20013;&#31435;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;NLI&#20219;&#21153;&#33021;&#22815;&#39044;&#27979;GPT-3&#29983;&#25104;&#38169;&#35823;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#32467;&#26524;&#20026;GPT-J&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;NLI&#30340;&#29983;&#25104;&#31574;&#30053;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20154;&#24037;&#26631;&#27880;&#38169;&#35823;&#31867;&#22411;&#21644;&#25972;&#20307;&#36136;&#37327;&#26469;&#35780;&#20272;&#29983;&#25104;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#26680;&#24515;&#37319;&#26679;&#30340;&#38543;&#26426;&#21442;&#25968;&#20540;&#36739;&#39640;&#26102;&#65292;&#26368;&#22823;&#21270;&#34164;&#28085;&#20851;&#31995;&#30340;NLI&#31574;&#30053;&#25913;&#21892;&#20102;&#25991;&#26412;&#29983;&#25104;&#65292;&#32780;&#22312;&#21442;&#25968;&#20540;&#36739;&#20302;&#26102;&#65292;&#26368;&#22823;&#21270;&#30683;&#30462;&#20851;&#31995;&#30340;&#31574;&#30053;&#23454;&#38469;&#19978;&#26159;&#26377;&#25928;&#30340;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#22823;&#21270;&#20013;&#31435;&#31867;&#21035;&#30340;NLI&#31574;&#30053;&#25552;&#20379;&#20102;&#26368;&#39640;&#36136;&#37327;&#30340;&#29983;&#25104;&#25991;&#26412;&#65288;&#26174;&#33879;&#20248;&#20110;&#26222;&#36890;&#29983;&#25104;&#22120;&#65289;&#65292;&#26080;&#35770;&#21442;&#25968;&#21462;&#20540;&#22914;&#20309;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore incorporating natural language inference (NLI) into the text generative pipeline by using a pre-trained NLI model to assess whether a generated sentence entails, contradicts, or is neutral to the prompt and preceding text. First, we show that the NLI task is predictive of generation errors made by GPT-3. We use these results to develop an NLI-informed generation procedure for GPT-J. Then, we evaluate these generations by obtaining human annotations on error types and overall quality. We find that an NLI strategy of maximizing entailment improves text generation when the nucleus sampling randomness parameter value is high, while one which maximizes contradiction is in fact productive when the parameter value is low. Overall, though, we demonstrate that an NLI strategy of maximizing the neutral class provides the highest quality of generated text (significantly better than the vanilla generations), regardless of parameter value.
&lt;/p&gt;</description></item><item><title>AV-data2vec&#26159;&#19968;&#31181;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#26469;&#26500;&#24314;&#38899;&#35270;&#39057;&#35821;&#38899;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#35757;&#32451;&#38899;&#39057;&#21644;&#35270;&#39057;&#30340;&#32852;&#21512;&#34920;&#31034;&#65292;&#24182;&#22312;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.06419</link><description>&lt;p&gt;
AV-data2vec: &#20351;&#29992;&#19978;&#19979;&#25991;&#21270;&#30446;&#26631;&#34920;&#31034;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#38899;&#35270;&#39057;&#35821;&#38899;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
AV-data2vec: Self-supervised Learning of Audio-Visual Speech Representations with Contextualized Target Representations. (arXiv:2302.06419v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06419
&lt;/p&gt;
&lt;p&gt;
AV-data2vec&#26159;&#19968;&#31181;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#26469;&#26500;&#24314;&#38899;&#35270;&#39057;&#35821;&#38899;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#35757;&#32451;&#38899;&#39057;&#21644;&#35270;&#39057;&#30340;&#32852;&#21512;&#34920;&#31034;&#65292;&#24182;&#22312;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#35821;&#38899;&#35782;&#21035;&#26041;&#38754;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#22823;&#22823;&#20943;&#23569;&#26500;&#24314;&#22909;&#30340;&#31995;&#32479;&#25152;&#38656;&#30340;&#26631;&#35760;&#25968;&#25454;&#37327;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#19981;&#23436;&#20840;&#31471;&#21040;&#31471;&#65292;&#35201;&#20040;&#19981;&#33021;&#21516;&#26102;&#35757;&#32451;&#20004;&#31181;&#27169;&#24577;&#30340;&#32852;&#21512;&#34920;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AV-data2vec&#65292;&#23427;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#22522;&#20110;&#39044;&#27979;&#19978;&#19979;&#25991;&#21270;&#34920;&#31034;&#26500;&#24314;&#38899;&#35270;&#39057;&#34920;&#31034;&#65292;&#36825;&#22312;&#21333;&#27169;&#24577;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#20849;&#20139;&#30340;Transformer&#32534;&#30721;&#22120;&#23545;&#38899;&#39057;&#21644;&#35270;&#39057;&#36827;&#34892;&#34920;&#31034;&#65292;&#24182;&#21487;&#20197;&#32467;&#21512;&#20004;&#31181;&#27169;&#24577;&#26469;&#25913;&#36827;&#35821;&#38899;&#35782;&#21035;&#12290;&#22312;LRS3&#19978;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;AV-data2vec&#22312;&#25152;&#26377;&#35774;&#32622;&#19979;&#37117;&#27604;&#29616;&#26377;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#32780;&#20351;&#29992;&#30340;&#25968;&#25454;&#37327;&#21644;&#27169;&#22411;&#22823;&#23567;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervision has shown great potential for audio-visual speech recognition by vastly reducing the amount of labeled data required to build good systems. However, existing methods are either not entirely end-to-end or do not train joint representations of both modalities. In this paper, we introduce AV-data2vec which addresses these challenges and builds audio-visual representations based on predicting contextualized representations which has been successful in the uni-modal case. The model uses a shared transformer encoder for both audio and video and can combine both modalities to improve speech recognition. Results on LRS3 show that AV-data2vec consistently outperforms existing methods under all settings with the same amount of data and model size.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Twitter&#25968;&#25454;&#20998;&#26512;&#20102;&#35299;&#20844;&#20247;&#23545;COVID-19&#30456;&#20851;&#33647;&#29289;&#30340;&#25209;&#20934;&#21644;&#31163;&#26631;&#20351;&#29992;&#30340;&#30475;&#27861;&#12290;Hydroxychloroquine&#21644;Ivermectin&#27604;Molnupiravir&#21644;Remdesivir&#30340;&#35752;&#35770;&#26356;&#22810;&#65292;&#26102;&#38388;&#36235;&#21183;&#20998;&#26512;&#21644;&#20869;&#23481;&#20998;&#26512;&#25581;&#31034;&#20102;&#20154;&#20204;&#23545;&#27599;&#31181;&#33647;&#29289;&#31435;&#22330;&#30340;&#21487;&#33021;&#29702;&#30001;&#12290;</title><link>http://arxiv.org/abs/2206.14358</link><description>&lt;p&gt;
&#20351;&#29992;Twitter&#25968;&#25454;&#20102;&#35299;&#20844;&#20247;&#23545;COVID-19&#30456;&#20851;&#33647;&#29289;&#25209;&#20934;&#21644;&#31163;&#26631;&#20351;&#29992;&#30340;&#30475;&#27861;
&lt;/p&gt;
&lt;p&gt;
Using Twitter Data to Understand Public Perceptions of Approved versus Off-label Use for COVID-19-related Medications. (arXiv:2206.14358v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.14358
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Twitter&#25968;&#25454;&#20998;&#26512;&#20102;&#35299;&#20844;&#20247;&#23545;COVID-19&#30456;&#20851;&#33647;&#29289;&#30340;&#25209;&#20934;&#21644;&#31163;&#26631;&#20351;&#29992;&#30340;&#30475;&#27861;&#12290;Hydroxychloroquine&#21644;Ivermectin&#27604;Molnupiravir&#21644;Remdesivir&#30340;&#35752;&#35770;&#26356;&#22810;&#65292;&#26102;&#38388;&#36235;&#21183;&#20998;&#26512;&#21644;&#20869;&#23481;&#20998;&#26512;&#25581;&#31034;&#20102;&#20154;&#20204;&#23545;&#27599;&#31181;&#33647;&#29289;&#31435;&#22330;&#30340;&#21487;&#33021;&#29702;&#30001;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#20844;&#20247;&#20851;&#20110;&#26410;&#32463;&#35777;&#23454;&#27835;&#30103;&#26041;&#27861;&#30340;&#32039;&#24613;&#20351;&#29992;&#30340;&#35752;&#35770;&#23545;&#20110;&#30417;&#27979;&#23433;&#20840;&#20351;&#29992;&#21644;&#25171;&#20987;&#38169;&#35823;&#20449;&#24687;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#27969;&#31243;&#65292;&#20197;&#29702;&#35299;Twitter&#19978;&#20851;&#20110;&#20896;&#29366;&#30149;&#27602;&#30149;2019&#65288;COVID-19&#65289;&#30456;&#20851;&#33647;&#29289;&#30340;&#20844;&#20247;&#30475;&#27861;&#21644;&#31435;&#22330;&#12290;&#36825;&#39033;&#22238;&#39038;&#24615;&#30740;&#31350;&#21253;&#25324;&#20102;&#22312;COVID-19&#22823;&#27969;&#34892;&#26399;&#38388;&#65292;&#20174;2020&#24180;1&#26376;29&#26085;&#21040;2021&#24180;11&#26376;30&#26085;&#65292;&#20851;&#20110;&#22235;&#31181;&#33647;&#29289;&#30340;609,189&#26465;&#32654;&#22269;&#25512;&#25991;&#65292;&#36825;&#22235;&#31181;&#33647;&#29289;&#22312;&#20844;&#20247;&#20013;&#24341;&#36215;&#20102;&#37325;&#22823;&#20851;&#27880;&#65306;&#65288;1&#65289;&#32671;&#27695;&#21945;&#21644;&#20234;&#32500;&#33740;&#32032;&#65292;&#20855;&#26377;&#26696;&#20363;&#35777;&#25454;&#30340;&#27835;&#30103;&#26041;&#27861;&#65307;&#65288;2&#65289;&#33707;&#31859;&#21305;&#38647;&#38886;&#21644;&#29790;&#24503;&#35199;&#38886;&#65292;FDA&#25209;&#20934;&#29992;&#20110;&#21512;&#26684;&#24739;&#32773;&#30340;&#27835;&#30103;&#26041;&#27861;&#12290;&#21033;&#29992;&#26102;&#38388;&#36235;&#21183;&#20998;&#26512;&#20102;&#35299;&#20854;&#21463;&#27426;&#36814;&#24230;&#36235;&#21183;&#21644;&#30456;&#20851;&#20107;&#20214;&#12290;&#36827;&#34892;&#20102;&#20869;&#23481;&#21644;&#20154;&#21475;&#32479;&#35745;&#23398;&#20998;&#26512;&#65292;&#20197;&#25506;&#32034;&#20154;&#20204;&#23545;&#27599;&#31181;&#33647;&#29289;&#31435;&#22330;&#30340;&#28508;&#22312;&#29702;&#30001;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding public discourse on emergency use of unproven therapeutics is crucial for monitoring safe use and combating misinformation. We developed a natural language processing-based pipeline to comprehend public perceptions of and stances on coronavirus disease 2019 (COVID-19)-related drugs on Twitter over time. This retrospective study included 609,189 US-based tweets from January 29, 2020, to November 30, 2021, about four drugs that garnered significant public attention during the COVID-19 pandemic: (1) Hydroxychloroquine and Ivermectin, therapies with anecdotal evidence; and (2) Molnupiravir and Remdesivir, FDA-approved treatments for eligible patients. Time-trend analysis was employed to understand popularity trends and related events. Content and demographic analyses were conducted to explore potential rationales behind people's stances on each drug. Time-trend analysis indicated that Hydroxychloroquine and Ivermectin were discussed more than Molnupiravir and Remdesivir, part
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35770;&#35777;&#35821;&#20041;&#21644;&#33258;&#28982;&#35821;&#35328;&#35770;&#35777;&#22270;&#32593;&#32476;&#30340;&#28151;&#21512;&#26041;&#27861;&#26469;&#33258;&#21160;&#35780;&#20272;&#36777;&#35770;&#65292;&#24182;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20026;&#33258;&#28982;&#35821;&#35328;&#35770;&#25454;&#30340;&#33258;&#21160;&#20998;&#26512;&#24320;&#36767;&#20102;&#26032;&#30340;&#26410;&#24320;&#21457;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2203.14647</link><description>&lt;p&gt;
&#20351;&#29992;&#35770;&#35777;&#35821;&#20041;&#21644;&#33258;&#28982;&#35821;&#35328;&#35770;&#35777;&#22270;&#32593;&#32476;&#23454;&#29616;&#33258;&#21160;&#36777;&#35770;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Automatic Debate Evaluation with Argumentation Semantics and Natural Language Argument Graph Networks. (arXiv:2203.14647v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.14647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35770;&#35777;&#35821;&#20041;&#21644;&#33258;&#28982;&#35821;&#35328;&#35770;&#35777;&#22270;&#32593;&#32476;&#30340;&#28151;&#21512;&#26041;&#27861;&#26469;&#33258;&#21160;&#35780;&#20272;&#36777;&#35770;&#65292;&#24182;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20026;&#33258;&#28982;&#35821;&#35328;&#35770;&#25454;&#30340;&#33258;&#21160;&#20998;&#26512;&#24320;&#36767;&#20102;&#26032;&#30340;&#26410;&#24320;&#21457;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32570;&#20047;&#19987;&#19994;&#35770;&#35777;&#21644;&#23436;&#25972;&#35770;&#36848;&#36777;&#35770;&#30340;&#27880;&#35299;&#25968;&#25454;&#23548;&#33268;&#20102;&#23545;&#26356;&#22797;&#26434;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#36807;&#20110;&#31616;&#21270;&#21644;&#26080;&#27861;&#22788;&#29702;&#12290;&#33258;&#21160;&#36777;&#35770;&#35780;&#20272;&#27491;&#26159;&#20854;&#26696;&#20363;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#22987;&#30340;&#28151;&#21512;&#26041;&#27861;&#26469;&#33258;&#21160;&#35780;&#20272;&#36777;&#35770;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#35770;&#35777;&#29702;&#35770;&#20013;&#30340;&#35770;&#35777;&#26694;&#26550;&#21644;&#35821;&#20041;&#27010;&#24565;&#65292;&#19982;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#21644;&#31070;&#32463;&#22270;&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20026;&#33258;&#28982;&#35821;&#35328;&#35770;&#25454;&#30340;&#33258;&#21160;&#20998;&#26512;&#24320;&#36767;&#20102;&#26032;&#30340;&#26410;&#24320;&#21457;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
The lack of annotated data on professional argumentation and complete argumentative debates has led to the oversimplification and the inability of approaching more complex natural language processing tasks. Such is the case of the automatic debate evaluation. In this paper, we propose an original hybrid method to automatically evaluate argumentative debates. For that purpose, we combine concepts from argumentation theory such as argumentation frameworks and semantics, with Transformer-based architectures and neural graph networks. Furthermore, we obtain promising results that lay the basis on an unexplored new instance of the automatic analysis of natural language arguments.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#20998;&#24067;&#24863;&#30693;&#35789;&#23884;&#20837;&#65292;&#24182;&#23454;&#26045;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#21033;&#29992;NER&#26694;&#26550;&#20013;&#30340;&#20998;&#24067;&#20449;&#24687;&#65292;&#23454;&#39564;&#34920;&#26126;&#23558;&#35789;&#30340;&#29305;&#24322;&#24615;&#34701;&#20837;NER&#26041;&#27861;&#21487;&#25552;&#39640;NER&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2109.01636</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#24067;&#24863;&#30693;&#35789;&#23884;&#20837;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#24615;&#33021;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Empirical Study of Named Entity Recognition Performance Using Distribution-aware Word Embedding. (arXiv:2109.01636v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.01636
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#20998;&#24067;&#24863;&#30693;&#35789;&#23884;&#20837;&#65292;&#24182;&#23454;&#26045;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#21033;&#29992;NER&#26694;&#26550;&#20013;&#30340;&#20998;&#24067;&#20449;&#24687;&#65292;&#23454;&#39564;&#34920;&#26126;&#23558;&#35789;&#30340;&#29305;&#24322;&#24615;&#34701;&#20837;NER&#26041;&#27861;&#21487;&#25552;&#39640;NER&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#22312;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;NER&#20219;&#21153;&#38754;&#20020;&#30340;&#26368;&#22823;&#22256;&#38590;&#26159;&#21363;&#20351;&#22312;NE&#31867;&#22411;&#21644;&#25991;&#26723;&#19981;&#29087;&#24713;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#38656;&#35201;&#20445;&#25345;&#21487;&#26816;&#27979;&#24615;&#12290;&#24847;&#35782;&#21040;&#29305;&#23450;&#24615;&#20449;&#24687;&#21487;&#33021;&#21253;&#21547;&#21333;&#35789;&#30340;&#28508;&#22312;&#21547;&#20041;&#24182;&#29983;&#25104;&#35789;&#23884;&#20837;&#30340;&#35821;&#20041;&#30456;&#20851;&#29305;&#24449;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20998;&#24067;&#24863;&#30693;&#35789;&#23884;&#20837;&#65292;&#24182;&#23454;&#26045;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#21033;&#29992;NER&#26694;&#26550;&#20013;&#30340;&#20998;&#24067;&#20449;&#24687;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22914;&#26524;&#23558;&#35789;&#30340;&#29305;&#24322;&#24615;&#34701;&#20837;&#29616;&#26377;&#30340;NER&#26041;&#27861;&#20013;&#65292;NER&#30340;&#24615;&#33021;&#23558;&#24471;&#21040;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the fast development of Deep Learning techniques, Named Entity Recognition (NER) is becoming more and more important in the information extraction task. The greatest difficulty that the NER task faces is to keep the detectability even when types of NE and documents are unfamiliar. Realizing that the specificity information may contain potential meanings of a word and generate semantic-related features for word embedding, we develop a distribution-aware word embedding and implement three different methods to make use of the distribution information in a NER framework. And the result shows that the performance of NER will be improved if the word specificity is incorporated into existing NER methods.
&lt;/p&gt;</description></item></channel></rss>