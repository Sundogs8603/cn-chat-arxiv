<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#25105;&#23545;&#24369;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#35843;&#65288;SPIN&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#33258;&#25105;&#23545;&#24328;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#20174;&#20013;&#20248;&#21270;&#27169;&#22411;&#31574;&#30053;&#65292;&#20174;&#32780;&#23558;&#24369;&#35821;&#35328;&#27169;&#22411;&#36716;&#21270;&#20026;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2401.01335</link><description>&lt;p&gt;
&#33258;&#25105;&#23545;&#24369;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#35843;&#21487;&#20197;&#23558;&#20854;&#36716;&#21270;&#20026;&#24378;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models. (arXiv:2401.01335v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#25105;&#23545;&#24369;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#35843;&#65288;SPIN&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#33258;&#25105;&#23545;&#24328;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#20174;&#20013;&#20248;&#21270;&#27169;&#22411;&#31574;&#30053;&#65292;&#20174;&#32780;&#23558;&#24369;&#35821;&#35328;&#27169;&#22411;&#36716;&#21270;&#20026;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30417;&#30563;&#32454;&#35843;&#65288;SFT&#65289;&#21033;&#29992;&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#30340;&#21147;&#37327;&#23545;&#20110;&#25512;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#19981;&#38656;&#35201;&#33719;&#21462;&#39069;&#22806;&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#24369;&#35821;&#35328;&#27169;&#22411;&#21457;&#23637;&#25104;&#20026;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#25105;&#23545;&#24369;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#35843;&#65288;SPIN&#65289;&#30340;&#26032;&#30340;&#32454;&#35843;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20174;&#19968;&#20010;&#32463;&#36807;&#30417;&#30563;&#32454;&#35843;&#30340;&#27169;&#22411;&#24320;&#22987;&#12290;SPIN&#30340;&#26680;&#24515;&#26159;&#33258;&#25105;&#23545;&#24369;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#21046;&#65292;&#20854;&#20013;&#24369;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#19982;&#33258;&#36523;&#30340;&#23454;&#20363;&#23545;&#24328;&#26469;&#25552;&#21319;&#33258;&#24049;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24369;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#29983;&#25104;&#33258;&#24049;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#20248;&#21270;&#33258;&#36523;&#31574;&#30053;&#65292;&#36890;&#36807;&#21306;&#20998;&#33258;&#25105;&#29983;&#25104;&#30340;&#22238;&#24212;&#19982;&#26469;&#33258;&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#30340;&#22238;&#24212;&#26469;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36880;&#27493;&#23558;&#24369;&#35821;&#35328;&#27169;&#22411;&#25552;&#21319;&#20026;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;&#20805;&#20998;&#21457;&#25496;&#20154;&#31867;&#26631;&#27880;&#31034;&#33539;&#25968;&#25454;&#22312;SFT&#20013;&#30340;&#28508;&#21147;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#35757;&#32451;&#30446;&#26631;&#20989;&#25968;&#30340;&#20840;&#23616;&#26368;&#20248;&#35299;&#26159;&#21487;&#20197;&#36798;&#21040;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Harnessing the power of human-annotated data through Supervised Fine-Tuning (SFT) is pivotal for advancing Large Language Models (LLMs). In this paper, we delve into the prospect of growing a strong LLM out of a weak one without the need for acquiring additional human-annotated data. We propose a new fine-tuning method called Self-Play fIne-tuNing (SPIN), which starts from a supervised fine-tuned model. At the heart of SPIN lies a self-play mechanism, where the LLM refines its capability by playing against instances of itself. More specifically, the LLM generates its own training data from its previous iterations, refining its policy by discerning these self-generated responses from those obtained from human-annotated data. Our method progressively elevates the LLM from a nascent model to a formidable one, unlocking the full potential of human-annotated demonstration data for SFT. Theoretically, we prove that the global optimum to the training objective function of our method is achiev
&lt;/p&gt;</description></item><item><title>TREC iKAT 2023&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#30340;&#30693;&#35782;&#36741;&#21161;&#20219;&#21153;&#65292;&#26088;&#22312;&#24320;&#21457;&#36866;&#24212;&#29992;&#25143;&#20132;&#20114;&#21644;&#19978;&#19979;&#25991;&#30340;&#20250;&#35805;&#25628;&#32034;&#20195;&#29702;&#12290;&#35813;&#20219;&#21153;&#36824;&#24378;&#35843;&#20915;&#31574;&#25628;&#32034;&#20219;&#21153;&#65292;&#29992;&#25143;&#36890;&#36807;&#31579;&#36873;&#25968;&#25454;&#21644;&#20449;&#24687;&#26469;&#36827;&#34892;&#20915;&#31574;&#21644;&#25191;&#34892;&#21160;&#20316;&#12290;</title><link>http://arxiv.org/abs/2401.01330</link><description>&lt;p&gt;
TREC iKAT 2023: &#20132;&#20114;&#24335;&#30693;&#35782;&#36741;&#21161;&#20219;&#21153;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
TREC iKAT 2023: The Interactive Knowledge Assistance Track Overview. (arXiv:2401.01330v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01330
&lt;/p&gt;
&lt;p&gt;
TREC iKAT 2023&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#30340;&#30693;&#35782;&#36741;&#21161;&#20219;&#21153;&#65292;&#26088;&#22312;&#24320;&#21457;&#36866;&#24212;&#29992;&#25143;&#20132;&#20114;&#21644;&#19978;&#19979;&#25991;&#30340;&#20250;&#35805;&#25628;&#32034;&#20195;&#29702;&#12290;&#35813;&#20219;&#21153;&#36824;&#24378;&#35843;&#20915;&#31574;&#25628;&#32034;&#20219;&#21153;&#65292;&#29992;&#25143;&#36890;&#36807;&#31579;&#36873;&#25968;&#25454;&#21644;&#20449;&#24687;&#26469;&#36827;&#34892;&#20915;&#31574;&#21644;&#25191;&#34892;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35805;&#24335;&#20449;&#24687;&#26597;&#35810;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#20043;&#21069;&#30340;&#24037;&#20316;&#20063;&#26377;&#24456;&#22823;&#30340;&#36129;&#29486;&#12290;TREC&#20132;&#20114;&#24335;&#30693;&#35782;&#36741;&#21161;&#20219;&#21153;&#65288;iKAT&#65289;&#24314;&#31435;&#22312;TREC&#20250;&#35805;&#36741;&#21161;&#20219;&#21153;&#65288;CAsT&#65289;&#30340;&#22522;&#30784;&#19978;&#12290;&#28982;&#32780;&#65292;iKAT&#30528;&#37325;&#20110;&#21019;&#24314;&#21644;&#30740;&#31350;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#20043;&#21069;&#30340;&#20132;&#20114;&#21644;&#24403;&#21069;&#24773;&#22659;&#33258;&#36866;&#24212;&#21709;&#24212;&#30340;&#20250;&#35805;&#25628;&#32034;&#20195;&#29702;&#12290;&#25361;&#25112;&#22312;&#20110;&#20351;&#20250;&#35805;&#25628;&#32034;&#20195;&#29702;&#33021;&#22815;&#23558;&#20010;&#24615;&#21270;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#34701;&#20837;&#21040;&#30456;&#24212;&#20013;&#65292;&#20197;&#39640;&#25928;&#22320;&#24341;&#23548;&#29992;&#25143;&#33719;&#21462;&#30456;&#20851;&#20449;&#24687;&#12290;iKAT&#36824;&#30528;&#37325;&#20110;&#20915;&#31574;&#25628;&#32034;&#20219;&#21153;&#65292;&#21363;&#29992;&#25143;&#36890;&#36807;&#25968;&#25454;&#21644;&#20449;&#24687;&#31579;&#36873;&#26469;&#34913;&#37327;&#21508;&#31181;&#36873;&#25321;&#65292;&#20197;&#36798;&#21040;&#32467;&#35770;&#25110;&#25191;&#34892;&#21160;&#20316;&#12290;&#36825;&#20123;&#20219;&#21153;&#22312;&#26085;&#24120;&#20449;&#24687;&#25628;&#32034;&#20915;&#31574;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#26080;&#35770;&#26159;&#26053;&#28216;&#12289;&#20581;&#24247;&#36824;&#26159;&#36141;&#29289;&#31561;&#65292;&#36890;&#24120;&#28041;&#21450;&#19968;&#32452;&#39640;&#32423;&#20449;&#24687;&#25805;&#20316;&#31526;&#65292;&#20854;&#20013;&#26597;&#35810;&#25110;&#38382;&#39064;&#21487;&#33021;&#20250;
&lt;/p&gt;
&lt;p&gt;
Conversational Information Seeking stands as a pivotal research area with significant contributions from previous works. The TREC Interactive Knowledge Assistance Track (iKAT) builds on the foundational work of the TREC Conversational Assistance Track (CAsT). However, iKAT distinctively emphasizes the creation and research of conversational search agents that adapt responses based on user's prior interactions and present context. The challenge lies in enabling Conversational Search Agents (CSA) to incorporate this personalized context to efficiency and effectively guide users through the relevant information to them. iKAT also emphasizes decisional search tasks, where users sift through data and information to weigh up options in order to reach a conclusion or perform an action. These tasks, prevalent in everyday information-seeking decisions -- be it related to travel, health, or shopping -- often revolve around a subset of high-level information operators where queries or questions a
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#38382;&#39064;&#20316;&#20026;&#26465;&#20214;&#24207;&#21015;&#29983;&#25104;&#38382;&#39064;&#26469;&#35299;&#20915;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#22522;&#20110;&#36328;&#24230;&#30340;&#22270;&#29983;&#25104;&#26041;&#24335;&#65292;&#24182;&#36890;&#36807;&#25351;&#21521;&#26426;&#21046;&#23558;&#29983;&#25104;&#30340;&#36755;&#20986;&#19982;&#21407;&#22987;&#25991;&#26412;&#23545;&#40784;&#12290;&#35780;&#20272;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#33719;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.01326</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#30340;&#33258;&#22238;&#24402;&#25991;&#26412;&#21040;&#22270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An Autoregressive Text-to-Graph Framework for Joint Entity and Relation Extraction. (arXiv:2401.01326v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01326
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#38382;&#39064;&#20316;&#20026;&#26465;&#20214;&#24207;&#21015;&#29983;&#25104;&#38382;&#39064;&#26469;&#35299;&#20915;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#22522;&#20110;&#36328;&#24230;&#30340;&#22270;&#29983;&#25104;&#26041;&#24335;&#65292;&#24182;&#36890;&#36807;&#25351;&#21521;&#26426;&#21046;&#23558;&#29983;&#25104;&#30340;&#36755;&#20986;&#19982;&#21407;&#22987;&#25991;&#26412;&#23545;&#40784;&#12290;&#35780;&#20272;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#33719;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#30340;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#38382;&#39064;&#20316;&#20026;&#26465;&#20214;&#24207;&#21015;&#29983;&#25104;&#38382;&#39064;&#26469;&#35299;&#20915;&#12290;&#19982;&#20256;&#32479;&#30340;&#29983;&#25104;&#24335;&#20449;&#24687;&#25277;&#21462;&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#36328;&#24230;&#30340;&#65292;&#23427;&#29983;&#25104;&#19968;&#20010;&#32447;&#24615;&#21270;&#30340;&#22270;&#65292;&#20854;&#20013;&#33410;&#28857;&#34920;&#31034;&#25991;&#26412;&#36328;&#24230;&#65292;&#36793;&#34920;&#31034;&#20851;&#31995;&#19977;&#20803;&#32452;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#19968;&#20010;&#20855;&#26377;&#25351;&#21521;&#26426;&#21046;&#30340;&#36716;&#25442;&#22120;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#20351;&#29992;&#19968;&#20010;&#21160;&#24577;&#35789;&#27719;&#34920;&#26469;&#34920;&#31034;&#36328;&#24230;&#21644;&#20851;&#31995;&#31867;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#36328;&#24230;&#34920;&#31034;&#25429;&#25417;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#32467;&#26500;&#29305;&#24449;&#21644;&#36793;&#30028;&#65292;&#21516;&#26102;&#36890;&#36807;&#25351;&#21521;&#26426;&#21046;&#23558;&#29983;&#25104;&#30340;&#36755;&#20986;&#19982;&#21407;&#22987;&#25991;&#26412;&#36827;&#34892;&#23545;&#40784;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/urchade/ATG&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel method for joint entity and relation extraction from unstructured text by framing it as a conditional sequence generation problem. In contrast to conventional generative information extraction models that are left-to-right token-level generators, our approach is \textit{span-based}. It generates a linearized graph where nodes represent text spans and edges represent relation triplets. Our method employs a transformer encoder-decoder architecture with pointing mechanism on a dynamic vocabulary of spans and relation types. Our model can capture the structural characteristics and boundaries of entities and relations through span representations while simultaneously grounding the generated output in the original text thanks to the pointing mechanism. Evaluation on benchmark datasets validates the effectiveness of our approach, demonstrating competitive results. Code is available at https://github.com/urchade/ATG.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Self-Extend&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36523;&#25193;&#23637;&#29616;&#26377;LLMs&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#26080;&#38656;&#35843;&#25972;&#65292;&#20805;&#20998;&#21033;&#29992;LLMs&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#30340;&#22266;&#26377;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.01325</link><description>&lt;p&gt;
&#33258;&#25193;&#23637;LLM:&#26080;&#38656;&#35843;&#25972;&#30340;LLM&#19978;&#19979;&#25991;&#31383;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning. (arXiv:2401.01325v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Self-Extend&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36523;&#25193;&#23637;&#29616;&#26377;LLMs&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#26080;&#38656;&#35843;&#25972;&#65292;&#20805;&#20998;&#21033;&#29992;LLMs&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#30340;&#22266;&#26377;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;LLM&#22312;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#26102;&#30340;&#22266;&#26377;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#31934;&#35843;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#35757;&#32451;&#24207;&#21015;&#30340;&#26377;&#38480;&#38271;&#24230;&#21487;&#33021;&#38480;&#21046;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#23545;&#38271;&#36755;&#20837;&#24207;&#21015;&#30340;&#24212;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#29616;&#26377;&#30340;LLMs&#26412;&#36523;&#20855;&#26377;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#30340;&#22266;&#26377;&#33021;&#21147;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#33258;&#36523;&#25193;&#23637;LLMs&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#20197;&#20805;&#20998;&#21033;&#29992;&#20854;&#22266;&#26377;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Self-Extend&#26041;&#27861;&#26469;&#28608;&#21457;LLMs&#30340;&#38271;&#19978;&#19979;&#25991;&#22788;&#29702;&#28508;&#21147;&#12290;&#22522;&#26412;&#24605;&#24819;&#26159;&#26500;&#24314;&#21452;&#23618;&#27880;&#24847;&#20449;&#24687;&#65306;&#32676;&#32452;&#32423;&#21644;&#37051;&#23621;&#32423;&#12290;&#36825;&#20004;&#20010;&#32423;&#21035;&#36890;&#36807;&#21407;&#22987;&#27169;&#22411;&#30340;&#33258;&#27880;&#24847;&#21147;&#35745;&#31639;&#65292;&#36825;&#24847;&#21619;&#30528;&#25152;&#25552;&#26041;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#35757;&#32451;&#12290;&#21482;&#38656;&#20462;&#25913;&#22235;&#34892;&#20195;&#30721;&#65292;&#25152;&#25552;&#26041;&#27861;&#23601;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#29616;&#26377;LLMs&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#31934;&#35843;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#26041;&#27861;&#21487;&#20197;+&#25688;&#35201;&#20943;&#25481;&#25991;&#31456;&#26368;&#21518;&#19968;&#21477;&#35441;
&lt;/p&gt;
&lt;p&gt;
This work elicits LLMs' inherent ability to handle long contexts without fine-tuning. The limited length of the training sequence during training may limit the application of Large Language Models (LLMs) on long input sequences for inference. In this work, we argue that existing LLMs themselves have inherent capabilities for handling long contexts. Based on this argument, we suggest extending LLMs' context window by themselves to fully utilize the inherent ability.We propose Self-Extend to stimulate LLMs' long context handling potential. The basic idea is to construct bi-level attention information: the group level and the neighbor level. The two levels are computed by the original model's self-attention, which means the proposed does not require any training. With only four lines of code modification, the proposed method can effortlessly extend existing LLMs' context window without any fine-tuning. We conduct comprehensive experiments and the results show that the proposed method can 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#25991;&#26412;&#26102;&#23481;&#26131;&#20986;&#29616;&#24187;&#35273;&#65292;&#36825;&#26159;&#23433;&#20840;&#37096;&#32626;&#36825;&#20123;&#27169;&#22411;&#30340;&#26368;&#22823;&#38556;&#30861;&#12290;&#35299;&#20915;&#24187;&#35273;&#38382;&#39064;&#23545;&#20110;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#24191;&#27867;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2401.01313</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#32531;&#35299;&#25216;&#26415;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models. (arXiv:2401.01313v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01313
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#25991;&#26412;&#26102;&#23481;&#26131;&#20986;&#29616;&#24187;&#35273;&#65292;&#36825;&#26159;&#23433;&#20840;&#37096;&#32626;&#36825;&#20123;&#27169;&#22411;&#30340;&#26368;&#22823;&#38556;&#30861;&#12290;&#35299;&#20915;&#24187;&#35273;&#38382;&#39064;&#23545;&#20110;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#24191;&#27867;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#25104;&#20154;&#31867;&#21270;&#25991;&#26412;&#26041;&#38754;&#30340;&#33021;&#21147;&#19981;&#26029;&#25552;&#39640;&#65292;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#23427;&#20204;&#20542;&#21521;&#20110;&#20135;&#29983;&#34394;&#26500;&#30340;&#20869;&#23481;&#65292;&#30475;&#20284;&#30495;&#23454;&#20294;&#27809;&#26377;&#20381;&#25454;&#12290;&#24187;&#35273;&#38382;&#39064;&#21487;&#20197;&#35828;&#26159;&#23433;&#20840;&#22320;&#23558;&#36825;&#20123;&#24378;&#22823;&#30340;LLMs&#37096;&#32626;&#21040;&#24433;&#21709;&#20154;&#20204;&#29983;&#27963;&#30340;&#29616;&#23454;&#29983;&#20135;&#31995;&#32479;&#20013;&#26368;&#22823;&#30340;&#38556;&#30861;&#12290;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#24191;&#27867;&#37319;&#29992;LLMs&#30340;&#36807;&#31243;&#20005;&#37325;&#20381;&#36182;&#20110;&#35299;&#20915;&#21644;&#20943;&#36731;&#24187;&#35273;&#12290;&#19982;&#20256;&#32479;&#30340;&#19987;&#27880;&#20110;&#26377;&#38480;&#20219;&#21153;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#19981;&#21516;&#65292;LLMs&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#20197;&#25509;&#35302;&#21040;&#22823;&#37327;&#30340;&#22312;&#32447;&#25991;&#26412;&#25968;&#25454;&#12290;&#36825;&#20351;&#23427;&#20204;&#33021;&#22815;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#35821;&#35328;&#27969;&#21033;&#24615;&#65292;&#20294;&#20063;&#24847;&#21619;&#30528;&#23427;&#20204;&#33021;&#22815;&#20174;&#35757;&#32451;&#25968;&#25454;&#30340;&#20559;&#35265;&#20013;&#25512;&#26029;&#20449;&#24687;&#65292;&#38169;&#35823;&#35299;&#37322;&#21547;&#31946;&#19981;&#28165;&#30340;&#25552;&#31034;&#65292;&#25110;&#32773;&#20462;&#25913;&#20449;&#24687;&#20197;&#34920;&#38754;&#19978;&#19982;&#36755;&#20837;&#19968;&#33268;&#12290;&#24403;&#25105;&#20204;&#20381;&#36182;&#35821;&#35328;&#29983;&#25104;&#33021;&#21147;&#26469;&#23436;&#25104;&#25935;&#24863;&#24212;&#29992;&#26102;&#65292;&#36825;&#21464;&#24471;&#38750;&#24120;&#20196;&#20154;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Large Language Models (LLMs) continue to advance in their ability to write human-like text, a key challenge remains around their tendency to hallucinate generating content that appears factual but is ungrounded. This issue of hallucination is arguably the biggest hindrance to safely deploying these powerful LLMs into real-world production systems that impact people's lives. The journey toward widespread adoption of LLMs in practical settings heavily relies on addressing and mitigating hallucinations. Unlike traditional AI systems focused on limited tasks, LLMs have been exposed to vast amounts of online text data during training. While this allows them to display impressive language fluency, it also means they are capable of extrapolating information from the biases in training data, misinterpreting ambiguous prompts, or modifying the information to align superficially with the input. This becomes hugely alarming when we rely on language generation capabilities for sensitive applica
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#27861;&#24459;&#24187;&#35273;&#65292;&#19981;&#19968;&#33268;&#27861;&#24459;&#20107;&#23454;&#65292;&#24187;&#35273;&#26222;&#36941;&#23384;&#22312;&#39640;&#36798;69%&#33267;88%&#30340;&#24773;&#20917;&#65292;&#26080;&#27861;&#32416;&#27491;&#29992;&#25143;&#38169;&#35823;&#27861;&#24459;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2401.01301</link><description>&lt;p&gt;
&#22823;&#22411;&#27861;&#24459;&#34394;&#26500;&#65306;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#27861;&#24459;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Large Legal Fictions: Profiling Legal Hallucinations in Large Language Models. (arXiv:2401.01301v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01301
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#27861;&#24459;&#24187;&#35273;&#65292;&#19981;&#19968;&#33268;&#27861;&#24459;&#20107;&#23454;&#65292;&#24187;&#35273;&#26222;&#36941;&#23384;&#22312;&#39640;&#36798;69%&#33267;88%&#30340;&#24773;&#20917;&#65292;&#26080;&#27861;&#32416;&#27491;&#29992;&#25143;&#38169;&#35823;&#27861;&#24459;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26377;&#21487;&#33021;&#25913;&#21464;&#27861;&#24459;&#23454;&#36341;&#65292;&#20294;&#20854;&#28508;&#21147;&#21463;&#21040;&#27861;&#24459;&#24187;&#35273;&#30340;&#23041;&#32961;&#65292;&#21363;&#36825;&#20123;&#27169;&#22411;&#20135;&#29983;&#19982;&#27861;&#24459;&#20107;&#23454;&#19981;&#19968;&#33268;&#30340;&#22238;&#31572;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#22871;&#21407;&#21019;&#30340;&#27861;&#24459;&#26597;&#35810;&#26469;&#35843;&#26597;&#36825;&#20123;&#24187;&#35273;&#30340;&#31243;&#24230;&#65292;&#23558;LLMs&#30340;&#22238;&#31572;&#19982;&#32467;&#26500;&#21270;&#30340;&#27861;&#24459;&#20803;&#25968;&#25454;&#36827;&#34892;&#23545;&#27604;&#65292;&#24182;&#26816;&#26597;&#20854;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26377;&#22235;&#20010;&#20851;&#38190;&#36129;&#29486;&#65306;&#65288;1&#65289;&#25105;&#20204;&#24314;&#31435;&#20102;&#27861;&#24459;&#24187;&#35273;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#20026;&#20170;&#21518;&#22312;&#36825;&#19968;&#39046;&#22495;&#36827;&#34892;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#27010;&#24565;&#26694;&#26550;&#12290;&#65288;2&#65289;&#25105;&#20204;&#21457;&#29616;&#65292;&#27861;&#24459;&#24187;&#35273;&#30340;&#26222;&#36941;&#24615;&#20196;&#20154;&#25285;&#24551;&#65292;&#22312;&#23545;&#38543;&#26426;&#32852;&#37030;&#27861;&#38498;&#26696;&#20363;&#36827;&#34892;&#20855;&#20307;&#12289;&#21487;&#39564;&#35777;&#30340;&#38382;&#39064;&#26102;&#65292;ChatGPT 3.5&#20135;&#29983;&#30340;&#24187;&#35273;&#21457;&#29983;&#29575;&#20026;69&#65285;&#65292;&#32780;Llama 2&#20026;88&#65285;&#12290;&#65288;3&#65289;&#25105;&#20204;&#23637;&#31034;&#20102;LLMs&#22312;&#36870;&#21521;&#38382;&#39064;&#35774;&#32622;&#20013;&#24448;&#24448;&#26080;&#27861;&#32416;&#27491;&#29992;&#25143;&#30340;&#38169;&#35823;&#27861;&#24459;&#20551;&#35774;&#12290;&#65288;4&#65289;&#25105;&#20204;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;LLMs&#24182;&#19981;&#24635;&#33021;&#39044;&#27979;&#25110;&#24182;&#19981;&#24635;&#30693;&#36947;...
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have the potential to transform the practice of law, but this potential is threatened by the presence of legal hallucinations -- responses from these models that are not consistent with legal facts. We investigate the extent of these hallucinations using an original suite of legal queries, comparing LLMs' responses to structured legal metadata and examining their consistency. Our work makes four key contributions: (1) We develop a typology of legal hallucinations, providing a conceptual framework for future research in this area. (2) We find that legal hallucinations are alarmingly prevalent, occurring between 69% of the time with ChatGPT 3.5 and 88% with Llama 2, when these models are asked specific, verifiable questions about random federal court cases. (3) We illustrate that LLMs often fail to correct a user's incorrect legal assumptions in a contra-factual question setup. (4) We provide evidence that LLMs cannot always predict, or do not always know, wh
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20840;&#38754;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#65292;&#26088;&#22312;&#26377;&#25928;&#20462;&#25913;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.01286</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#20840;&#38754;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Study of Knowledge Editing for Large Language Models. (arXiv:2401.01286v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20840;&#38754;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#65292;&#26088;&#22312;&#26377;&#25928;&#20462;&#25913;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#19982;&#20154;&#31867;&#20132;&#27969;&#32039;&#23494;&#30456;&#20284;&#30340;&#25991;&#26412;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20854;&#20027;&#35201;&#38480;&#21046;&#22312;&#20110;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#26174;&#33879;&#35745;&#31639;&#38656;&#27714;&#65292;&#36825;&#26159;&#30001;&#20110;&#20854;&#24191;&#27867;&#30340;&#21442;&#25968;&#21270;&#36896;&#25104;&#30340;&#12290;&#36825;&#19968;&#25361;&#25112;&#22312;&#20110;&#19990;&#30028;&#30340;&#21160;&#24577;&#24615;&#65292;&#38656;&#35201;&#39057;&#32321;&#26356;&#26032;LLM&#20197;&#20462;&#27491;&#36807;&#26102;&#30340;&#20449;&#24687;&#25110;&#38598;&#25104;&#26032;&#30693;&#35782;&#65292;&#20174;&#32780;&#30830;&#20445;&#20854;&#25345;&#32493;&#30340;&#30456;&#20851;&#24615;&#12290;&#35768;&#22810;&#24212;&#29992;&#38656;&#35201;&#22312;&#35757;&#32451;&#21518;&#36827;&#34892;&#25345;&#32493;&#30340;&#27169;&#22411;&#35843;&#25972;&#65292;&#20197;&#35299;&#20915;&#32570;&#38519;&#25110;&#19981;&#33391;&#34892;&#20026;&#12290;&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;LLM&#30340;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#30340;&#20852;&#36259;&#36234;&#26469;&#36234;&#39640;&#65292;&#22312;&#29305;&#23450;&#39046;&#22495;&#20869;&#26377;&#25928;&#22320;&#20462;&#25913;LLM&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#22312;&#21508;&#31181;&#36755;&#20837;&#20013;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#39318;&#20808;&#23450;&#20041;&#20102;&#30693;&#35782;&#32534;&#36753;&#30340;&#30446;&#26631;&#21644;&#25361;&#25112;&#65292;&#28982;&#21518;&#32508;&#36848;&#20102;&#29616;&#26377;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#21644;&#25216;&#26415;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#24212;&#29992;&#21644;&#26410;&#26469;&#21457;&#23637;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown extraordinary capabilities in understanding and generating text that closely mirrors human communication. However, a primary limitation lies in the significant computational demands during training, arising from their extensive parameterization. This challenge is further intensified by the dynamic nature of the world, necessitating frequent updates to LLMs to correct outdated information or integrate new knowledge, thereby ensuring their continued relevance. Note that many applications demand continual model adjustments post-training to address deficiencies or undesirable behaviors. There is an increasing interest in efficient, lightweight methods for on-the-fly model modifications. To this end, recent years have seen a burgeoning in the techniques of knowledge editing for LLMs, which aim to efficiently modify LLMs' behaviors within specific domains while preserving overall performance across various inputs. In this paper, we first define the kno
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#30340;&#36739;&#39640;&#36136;&#37327;&#21442;&#32771;&#25991;&#29486;&#23545;&#20110;&#35780;&#20272;&#25351;&#26631;&#19982;&#20154;&#31867;&#35780;&#20215;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26356;&#22909;&#12290;&#27599;&#20010;&#27573;&#33853;&#24179;&#22343;&#20351;&#29992;7&#20010;&#21442;&#32771;&#25991;&#29486;&#26377;&#21161;&#20110;&#25552;&#21319;&#25152;&#26377;&#35780;&#20272;&#25351;&#26631;&#12290;&#19981;&#21516;&#36136;&#37327;&#30340;&#20379;&#24212;&#21830;&#21442;&#32771;&#25991;&#29486;&#21487;&#20197;&#28151;&#21512;&#20351;&#29992;&#26469;&#25552;&#39640;&#35780;&#20272;&#25351;&#26631;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#21487;&#29992;&#20110;&#22312;&#29305;&#23450;&#39044;&#31639;&#19979;&#21019;&#24314;&#21442;&#32771;&#25991;&#29486;&#30340;&#20849;&#20139;&#20219;&#21153;&#30340;&#35780;&#20272;&#32773;&#12290;</title><link>http://arxiv.org/abs/2401.01283</link><description>&lt;p&gt;
&#26426;&#22120;&#32763;&#35793;&#33258;&#21160;&#35780;&#20272;&#30340;&#21442;&#32771;&#25991;&#29486;&#36136;&#37327;&#21644;&#25968;&#37327;
&lt;/p&gt;
&lt;p&gt;
Quality and Quantity of Machine Translation References for Automated Metrics. (arXiv:2401.01283v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#30340;&#36739;&#39640;&#36136;&#37327;&#21442;&#32771;&#25991;&#29486;&#23545;&#20110;&#35780;&#20272;&#25351;&#26631;&#19982;&#20154;&#31867;&#35780;&#20215;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26356;&#22909;&#12290;&#27599;&#20010;&#27573;&#33853;&#24179;&#22343;&#20351;&#29992;7&#20010;&#21442;&#32771;&#25991;&#29486;&#26377;&#21161;&#20110;&#25552;&#21319;&#25152;&#26377;&#35780;&#20272;&#25351;&#26631;&#12290;&#19981;&#21516;&#36136;&#37327;&#30340;&#20379;&#24212;&#21830;&#21442;&#32771;&#25991;&#29486;&#21487;&#20197;&#28151;&#21512;&#20351;&#29992;&#26469;&#25552;&#39640;&#35780;&#20272;&#25351;&#26631;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#21487;&#29992;&#20110;&#22312;&#29305;&#23450;&#39044;&#31639;&#19979;&#21019;&#24314;&#21442;&#32771;&#25991;&#29486;&#30340;&#20849;&#20139;&#20219;&#21153;&#30340;&#35780;&#20272;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#25351;&#26631;&#36890;&#24120;&#20351;&#29992;&#20154;&#24037;&#32763;&#35793;&#26469;&#30830;&#23450;&#31995;&#32479;&#32763;&#35793;&#30340;&#36136;&#37327;&#12290;&#39046;&#22495;&#20869;&#30340;&#20849;&#35782;&#35748;&#20026;&#20154;&#24037;&#21442;&#32771;&#25991;&#29486;&#24212;&#20855;&#26377;&#24456;&#39640;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#25104;&#26412;&#25928;&#30410;&#20998;&#26512;&#21487;&#20197;&#25351;&#23548;&#35745;&#21010;&#25910;&#38598;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#21442;&#32771;&#25991;&#29486;&#30340;&#20174;&#19994;&#32773;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36739;&#39640;&#36136;&#37327;&#30340;&#21442;&#32771;&#25991;&#29486;&#33021;&#22815;&#22312;&#27573;&#33853;&#32423;&#21035;&#19978;&#19982;&#20154;&#31867;&#35780;&#20215;&#30340;&#30456;&#20851;&#24615;&#26356;&#22909;&#12290;&#27599;&#20010;&#27573;&#33853;&#24179;&#22343;&#20351;&#29992;7&#20010;&#21442;&#32771;&#25991;&#29486;&#26377;&#21161;&#20110;&#25152;&#26377;&#35780;&#20272;&#25351;&#26631;&#30340;&#25552;&#21319;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#26469;&#33258;&#19981;&#21516;&#36136;&#37327;&#30340;&#20379;&#24212;&#21830;&#30340;&#21442;&#32771;&#25991;&#29486;&#21487;&#20197;&#28151;&#21512;&#20351;&#29992;&#65292;&#24182;&#25552;&#39640;&#35780;&#20272;&#25351;&#26631;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#36739;&#39640;&#36136;&#37327;&#30340;&#21442;&#32771;&#25991;&#29486;&#21046;&#20316;&#25104;&#26412;&#26356;&#39640;&#65292;&#25105;&#20204;&#23558;&#20854;&#35270;&#20026;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65306;&#22312;&#29305;&#23450;&#39044;&#31639;&#19979;&#65292;&#24212;&#35813;&#25910;&#38598;&#21738;&#20123;&#21442;&#32771;&#25991;&#29486;&#20197;&#26368;&#22823;&#21270;&#35780;&#20272;&#25351;&#26631;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#21487;&#29992;&#20110;&#22312;&#29305;&#23450;&#39044;&#31639;&#19979;&#21019;&#24314;&#21442;&#32771;&#25991;&#29486;&#30340;&#20849;&#20139;&#20219;&#21153;&#30340;&#35780;&#20272;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic machine translation metrics often use human translations to determine the quality system translations. Common wisdom in the field dictates that the human references should be of very high quality. However, there are no cost-benefit analyses that could be used to guide practitioners who plan to collect references for machine translation evaluation. We find that higher-quality references lead to better metric correlations with humans at the segment-level. Having up to 7 references per segment and taking their average helps all metrics. Interestingly, the references from vendors of different qualities can be mixed together and improve metric success. Higher quality references, however, cost more to create and we frame this as an optimization problem: given a specific budget, what references should be collected to maximize metric success. These findings can be used by evaluators of shared tasks when references need to be created under a certain budget.
&lt;/p&gt;</description></item><item><title>CharacterEval&#26159;&#19968;&#20010;&#29992;&#20110;&#35282;&#33394;&#25198;&#28436;&#23545;&#35805;&#20195;&#29702;&#35780;&#20272;&#30340;&#20013;&#25991;&#22522;&#20934;&#27979;&#35797;&#38598;&#65292;&#21253;&#21547;1,785&#20010;&#22810;&#36718;&#23545;&#35805;&#21644;23,020&#20010;&#31034;&#20363;&#65292;&#28085;&#30422;77&#20010;&#35282;&#33394;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;CharacterEval&#22312;&#35780;&#20272;RPCA&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2401.01275</link><description>&lt;p&gt;
CharacterEval: &#19968;&#31181;&#29992;&#20110;&#35282;&#33394;&#25198;&#28436;&#23545;&#35805;&#20195;&#29702;&#35780;&#20272;&#30340;&#20013;&#25991;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation. (arXiv:2401.01275v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01275
&lt;/p&gt;
&lt;p&gt;
CharacterEval&#26159;&#19968;&#20010;&#29992;&#20110;&#35282;&#33394;&#25198;&#28436;&#23545;&#35805;&#20195;&#29702;&#35780;&#20272;&#30340;&#20013;&#25991;&#22522;&#20934;&#27979;&#35797;&#38598;&#65292;&#21253;&#21547;1,785&#20010;&#22810;&#36718;&#23545;&#35805;&#21644;23,020&#20010;&#31034;&#20363;&#65292;&#28085;&#30422;77&#20010;&#35282;&#33394;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;CharacterEval&#22312;&#35780;&#20272;RPCA&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#24443;&#24213;&#25913;&#21464;&#20102;&#29983;&#25104;&#20195;&#29702;&#30340;&#26041;&#24335;&#12290;&#20854;&#20013;&#65292;&#35282;&#33394;&#25198;&#28436;&#23545;&#35805;&#20195;&#29702;&#65288;RPCA&#65289;&#30001;&#20110;&#20854;&#35302;&#21457;&#29992;&#25143;&#24773;&#24863;&#30340;&#33021;&#21147;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#38598;&#38459;&#30861;&#20102;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;CharacterEval&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#20840;&#38754;&#35780;&#20272;RPCA&#30340;&#20013;&#25991;&#22522;&#20934;&#27979;&#35797;&#38598;&#65292;&#24182;&#37197;&#26377;&#19968;&#20010;&#23450;&#21046;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;1,785&#20010;&#22810;&#36718;&#35282;&#33394;&#25198;&#28436;&#23545;&#35805;&#65292;&#28085;&#30422;&#20102;23,020&#20010;&#31034;&#20363;&#65292;&#28041;&#21450;&#20102;77&#20010;&#26469;&#28304;&#20110;&#20013;&#22269;&#23567;&#35828;&#21644;&#21095;&#26412;&#30340;&#35282;&#33394;&#12290;&#23427;&#32463;&#36807;&#31934;&#24515;&#26500;&#24314;&#65292;&#39318;&#20808;&#36890;&#36807;GPT-4&#36827;&#34892;&#21021;&#22987;&#23545;&#35805;&#25552;&#21462;&#65292;&#28982;&#21518;&#36827;&#34892;&#20005;&#26684;&#30340;&#20154;&#24037;&#36136;&#37327;&#25511;&#21046;&#65292;&#24182;&#36890;&#36807;&#30334;&#24230;&#30334;&#31185;&#33719;&#21462;&#20102;&#28145;&#20837;&#30340;&#35282;&#33394;&#36164;&#26009;&#12290;CharacterEval&#37319;&#29992;&#22810;&#26041;&#38754;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#21253;&#25324;&#22235;&#20010;&#32500;&#24230;&#19978;&#30340;&#21313;&#19977;&#20010;&#26377;&#38024;&#23545;&#24615;&#30340;&#25351;&#26631;&#12290;&#22312;CharacterEval&#19978;&#36827;&#34892;&#30340;&#20840;&#38754;&#23454;&#39564;&#35777;&#26126;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the advent of large language models (LLMs) has revolutionized generative agents. Among them, Role-Playing Conversational Agents (RPCAs) attract considerable attention due to their ability to emotionally engage users. However, the absence of a comprehensive benchmark impedes progress in this field. To bridge this gap, we introduce CharacterEval, a Chinese benchmark for comprehensive RPCA assessment, complemented by a tailored high-quality dataset. The dataset comprises 1,785 multi-turn role-playing dialogues, encompassing 23,020 examples and featuring 77 characters derived from Chinese novels and scripts. It was carefully constructed, beginning with initial dialogue extraction via GPT-4, followed by rigorous human-led quality control, and enhanced with in-depth character profiles sourced from Baidu Baike. CharacterEval employs a multifaceted evaluation approach, encompassing thirteen targeted metrics on four dimensions. Comprehensive experiments on CharacterEval demonstrate th
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24320;&#21457;&#20844;&#24179;&#24615;&#35748;&#35777;&#26041;&#27861;&#12290;&#36890;&#36807;&#32508;&#36848;&#22823;&#37327;&#25991;&#29486;&#21644;&#19987;&#23478;&#35775;&#35848;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20845;&#20010;&#20844;&#24179;&#24615;&#26631;&#20934;&#65292;&#20026;&#25805;&#20316;&#21270;&#21644;&#27979;&#35797;&#36807;&#31243;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2401.01262</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20844;&#24179;&#24615;&#35748;&#35777;
&lt;/p&gt;
&lt;p&gt;
Fairness Certification for Natural Language Processing and Large Language Models. (arXiv:2401.01262v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01262
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24320;&#21457;&#20844;&#24179;&#24615;&#35748;&#35777;&#26041;&#27861;&#12290;&#36890;&#36807;&#32508;&#36848;&#22823;&#37327;&#25991;&#29486;&#21644;&#19987;&#23478;&#35775;&#35848;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20845;&#20010;&#20844;&#24179;&#24615;&#26631;&#20934;&#65292;&#20026;&#25805;&#20316;&#21270;&#21644;&#27979;&#35797;&#36807;&#31243;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#22312;&#25105;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#65292;&#29305;&#21035;&#26159;&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24040;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;NLP&#22312;&#25307;&#32856;&#31561;&#20844;&#24179;&#20851;&#38190;&#24212;&#29992;&#22330;&#26223;&#20013;&#23384;&#22312;&#35768;&#22810;&#38382;&#39064;&#65292;&#20363;&#22914;&#20316;&#20026;&#19987;&#23478;&#31995;&#32479;&#25110;&#22522;&#20110;LLM&#30340;&#25945;&#32946;&#23548;&#24072;&#12290;&#30001;&#20110;NLP&#22522;&#20110;&#20154;&#31867;&#35821;&#35328;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#28508;&#22312;&#30340;&#26377;&#23475;&#20559;&#35265;&#28183;&#20837;NLP&#31995;&#32479;&#65292;&#20135;&#29983;&#19981;&#20844;&#24179;&#30340;&#32467;&#26524;&#65292;&#27495;&#35270;&#23569;&#25968;&#32676;&#20307;&#25110;&#24341;&#21457;&#27861;&#24459;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#24320;&#23637;NLP&#26041;&#27861;&#30340;&#20844;&#24179;&#24615;&#35748;&#35777;&#38750;&#24120;&#37325;&#35201;&#12290;&#25105;&#20204;&#37319;&#29992;&#23450;&#24615;&#30740;&#31350;&#26041;&#27861;&#65292;&#23545;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#22823;&#37327;&#25991;&#29486;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#24182;&#19982;&#35813;&#39046;&#22495;&#30340;&#22810;&#20301;&#19987;&#23478;&#36827;&#34892;&#20102;&#21322;&#32467;&#26500;&#21270;&#30340;&#19987;&#23478;&#35775;&#35848;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#25552;&#20986;&#20102;NLP&#30340;&#20845;&#20010;&#20844;&#24179;&#24615;&#26631;&#20934;&#65292;&#24182;&#36827;&#19968;&#27493;&#32454;&#21270;&#20026;18&#20010;&#23376;&#31867;&#21035;&#12290;&#25105;&#20204;&#30340;&#26631;&#20934;&#20026;&#23454;&#26045;&#21644;&#27979;&#35797;&#36807;&#31243;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural Language Processing (NLP) plays an important role in our daily lives, particularly due to the enormous progress of Large Language Models (LLM). However, NLP has many fairness-critical use cases, e.g., as an expert system in recruitment or as an LLM-based tutor in education. Since NLP is based on human language, potentially harmful biases can diffuse into NLP systems and produce unfair results, discriminate against minorities or generate legal issues. Hence, it is important to develop a fairness certification for NLP approaches. We follow a qualitative research approach towards a fairness certification for NLP. In particular, we have reviewed a large body of literature on algorithmic fairness, and we have conducted semi-structured expert interviews with a wide range of experts from that area. We have systematically devised six fairness criteria for NLP, which can be further refined into 18 sub-categories. Our criteria offer a foundation for operationalizing and testing processes
&lt;/p&gt;</description></item><item><title>VideoDrafter&#26159;&#19968;&#20010;&#21033;&#29992;LLM&#23454;&#29616;&#20869;&#23481;&#19968;&#33268;&#30340;&#22810;&#22330;&#26223;&#35270;&#39057;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#26681;&#25454;&#36755;&#20837;&#25552;&#31034;&#29983;&#25104;&#36923;&#36753;&#36830;&#36143;&#30340;&#22810;&#22330;&#26223;&#33050;&#26412;&#65292;&#24182;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35270;&#39057;&#12290;</title><link>http://arxiv.org/abs/2401.01256</link><description>&lt;p&gt;
VideoDrafter: &#21033;&#29992;LLM&#23454;&#29616;&#20869;&#23481;&#19968;&#33268;&#30340;&#22810;&#22330;&#26223;&#35270;&#39057;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
VideoDrafter: Content-Consistent Multi-Scene Video Generation with LLM. (arXiv:2401.01256v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01256
&lt;/p&gt;
&lt;p&gt;
VideoDrafter&#26159;&#19968;&#20010;&#21033;&#29992;LLM&#23454;&#29616;&#20869;&#23481;&#19968;&#33268;&#30340;&#22810;&#22330;&#26223;&#35270;&#39057;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#26681;&#25454;&#36755;&#20837;&#25552;&#31034;&#29983;&#25104;&#36923;&#36753;&#36830;&#36143;&#30340;&#22810;&#22330;&#26223;&#33050;&#26412;&#65292;&#24182;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35270;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25193;&#23637;&#27169;&#22411;&#30340;&#21019;&#26032;&#21644;&#31361;&#30772;&#26174;&#33879;&#25193;&#22823;&#20102;&#26681;&#25454;&#32473;&#23450;&#25552;&#31034;&#29983;&#25104;&#39640;&#36136;&#37327;&#35270;&#39057;&#30340;&#21487;&#33021;&#24615;&#12290;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#20316;&#21697;&#20165;&#22788;&#29702;&#22312;&#21333;&#20010;&#32972;&#26223;&#20013;&#21457;&#29983;&#21333;&#20010;&#35270;&#39057;&#20107;&#20214;&#30340;&#21333;&#22330;&#26223;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#25193;&#23637;&#21040;&#29983;&#25104;&#22810;&#22330;&#26223;&#35270;&#39057;&#24182;&#19988;&#22312;&#20445;&#25345;&#21508;&#20010;&#22330;&#26223;&#20043;&#38388;&#30340;&#36923;&#36753;&#19968;&#33268;&#21516;&#26102;&#20445;&#25345;&#35270;&#35273;&#22806;&#35266;&#19968;&#33268;&#24615;&#26041;&#38754;&#24182;&#19981;&#31616;&#21333;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21363;VideoDrafter&#65292;&#29992;&#20110;&#20869;&#23481;&#19968;&#33268;&#30340;&#22810;&#22330;&#26223;&#35270;&#39057;&#29983;&#25104;&#12290;&#25216;&#26415;&#19978;&#65292;VideoDrafter&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23558;&#36755;&#20837;&#25552;&#31034;&#36716;&#21270;&#20026;&#32508;&#21512;&#30340;&#22810;&#22330;&#26223;&#33050;&#26412;&#65292;&#35813;&#33050;&#26412;&#20174;LLM&#23398;&#21040;&#30340;&#36923;&#36753;&#30693;&#35782;&#20013;&#21463;&#30410;&#12290;&#27599;&#20010;&#22330;&#26223;&#30340;&#33050;&#26412;&#21253;&#25324;&#25551;&#36848;&#20107;&#20214;&#12289;&#21069;&#26223;/&#32972;&#26223;&#23454;&#20307;&#20197;&#21450;&#25668;&#20687;&#26426;&#36816;&#21160;&#30340;&#25552;&#31034;&#12290;VideoDrafter&#35782;&#21035;&#33050;&#26412;&#20013;&#30340;&#20849;&#21516;&#23454;&#20307;&#65292;&#24182;&#35810;&#38382;LLM&#26469;&#36873;&#25321;&#29983;&#25104;&#36923;&#36753;&#36830;&#36143;&#30340;&#35270;&#39057;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent innovations and breakthroughs in diffusion models have significantly expanded the possibilities of generating high-quality videos for the given prompts. Most existing works tackle the single-scene scenario with only one video event occurring in a single background. Extending to generate multi-scene videos nevertheless is not trivial and necessitates to nicely manage the logic in between while preserving the consistent visual appearance of key content across video scenes. In this paper, we propose a novel framework, namely VideoDrafter, for content-consistent multi-scene video generation. Technically, VideoDrafter leverages Large Language Models (LLM) to convert the input prompt into comprehensive multi-scene script that benefits from the logical knowledge learnt by LLM. The script for each scene includes a prompt describing the event, the foreground/background entities, as well as camera movement. VideoDrafter identifies the common entities throughout the script and asks LLM
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#20301;&#32622;&#21435;&#20559;&#26041;&#27861;&#65288;ZOE&#65289;&#26469;&#38477;&#20302;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20301;&#32622;&#20559;&#24046;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLMs&#30340;&#26080;&#30417;&#30563;&#21709;&#24212;&#36827;&#34892;&#21435;&#20559;&#12290;&#23454;&#39564;&#35777;&#23454;ZOE&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#20013;&#22343;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.01218</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#20301;&#32622;&#21435;&#20559;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Position Debiasing for Large Language Models. (arXiv:2401.01218v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#20301;&#32622;&#21435;&#20559;&#26041;&#27861;&#65288;ZOE&#65289;&#26469;&#38477;&#20302;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20301;&#32622;&#20559;&#24046;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLMs&#30340;&#26080;&#30417;&#30563;&#21709;&#24212;&#36827;&#34892;&#21435;&#20559;&#12290;&#23454;&#39564;&#35777;&#23454;ZOE&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#20013;&#22343;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#24050;&#34987;&#35777;&#26126;&#26159;&#25913;&#21892;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39046;&#22495;&#24615;&#33021;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;LLMs&#21487;&#33021;&#36866;&#24212;&#25968;&#25454;&#38598;&#20559;&#35265;&#21644;&#39044;&#27979;&#30340;&#25463;&#24452;&#65292;&#23548;&#33268;&#29983;&#25104;&#24615;&#33021;&#24046;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;LLMs&#23481;&#26131;&#34920;&#29616;&#20986;&#20301;&#32622;&#20559;&#24046;&#65292;&#21363;&#21033;&#29992;&#20301;&#20110;&#24320;&#22836;&#25110;&#26411;&#23614;&#25110;&#36755;&#20837;&#20013;&#29305;&#23450;&#20301;&#32622;&#32447;&#32034;&#30340;&#20449;&#24687;&#12290;&#29616;&#26377;&#30340;&#20943;&#36731;&#20301;&#32622;&#20559;&#24046;&#30340;&#24037;&#20316;&#38656;&#35201;&#22806;&#37096;&#20559;&#24046;&#30693;&#35782;&#25110;&#24102;&#27880;&#37322;&#30340;&#38750;&#20559;&#20506;&#26679;&#26412;&#65292;&#22312;&#23454;&#38469;&#20013;&#19981;&#22826;&#23454;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#20301;&#32622;&#21435;&#20559;&#65288;ZOE&#65289;&#26694;&#26550;&#23545;LLMs&#36827;&#34892;&#20301;&#32622;&#21435;&#20559;&#12290;ZOE&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLMs&#30340;&#26080;&#30417;&#30563;&#21709;&#24212;&#36827;&#34892;&#21435;&#20559;&#65292;&#22240;&#27492;&#19981;&#38656;&#35201;&#20219;&#20309;&#22806;&#37096;&#30693;&#35782;&#25110;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#25552;&#39640;&#26080;&#30417;&#30563;&#21709;&#24212;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#20174;&#23545;&#40784;&#65288;MSA&#65289;&#27169;&#22359;&#26469;&#20462;&#21098;&#36825;&#20123;&#21709;&#24212;&#12290;&#23545;&#20843;&#20010;&#25968;&#25454;&#38598;&#21644;&#20116;&#20010;&#20219;&#21153;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ZOE&#22987;&#32456;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning has been demonstrated to be an effective method to improve the domain performance of large language models (LLMs). However, LLMs might fit the dataset bias and shortcuts for prediction, leading to poor generation performance. Experimental result shows that LLMs are prone to exhibit position bias, i.e., leveraging information positioned at the beginning or end, or specific positional cues within the input. Existing works on mitigating position bias require external bias knowledge or annotated non-biased samples, which is unpractical in reality. In this work, we propose a zero-shot position debiasing (ZOE) framework to mitigate position bias for LLMs. ZOE leverages unsupervised responses from pre-trained LLMs for debiasing, thus without any external knowledge or datasets. To improve the quality of unsupervised responses, we propose a master-slave alignment (MSA) module to prune these responses. Experiments on eight datasets and five tasks show that ZOE consistently outperform
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#35299;&#20915;&#35823;&#35299;&#20449;&#24687;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#20998;&#31867;&#26694;&#26550;&#21644;&#29983;&#25104;&#26377;&#25928;&#30340;&#29992;&#25143;&#26597;&#35810;&#26469;&#35299;&#20915;&#32570;&#22833;&#19978;&#19979;&#25991;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#35823;&#35299;&#20449;&#24687;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.01197</link><description>&lt;p&gt;
&#35823;&#35299;&#20449;&#24687;&#26816;&#27979;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#35299;&#20915;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Resolution in Misinformation Detection. (arXiv:2401.01197v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01197
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#35299;&#20915;&#35823;&#35299;&#20449;&#24687;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#20998;&#31867;&#26694;&#26550;&#21644;&#29983;&#25104;&#26377;&#25928;&#30340;&#29992;&#25143;&#26597;&#35810;&#26469;&#35299;&#20915;&#32570;&#22833;&#19978;&#19979;&#25991;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#35823;&#35299;&#20449;&#24687;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35823;&#35299;&#20449;&#24687;&#23384;&#22312;&#21508;&#31181;&#39118;&#38505;&#65292;&#22914;&#30772;&#22351;&#20844;&#20247;&#20449;&#20219;&#21644;&#25197;&#26354;&#20107;&#23454;&#35328;&#35770;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-4&#24050;&#34987;&#35777;&#26126;&#22312;&#20943;&#36731;&#35823;&#35299;&#20449;&#24687;&#26041;&#38754;&#24456;&#26377;&#25928;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#25552;&#20379;&#36275;&#22815;&#19978;&#19979;&#25991;&#30340;&#38472;&#36848;&#26102;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24456;&#38590;&#20934;&#30830;&#35780;&#20272;&#27169;&#31946;&#25110;&#32570;&#20047;&#19978;&#19979;&#25991;&#30340;&#38472;&#36848;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35299;&#20915;&#27492;&#31867;&#38472;&#36848;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#23545;&#32570;&#22833;&#20449;&#24687;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#20026;LIAR-New&#25968;&#25454;&#38598;&#25552;&#20379;&#31867;&#21035;&#26631;&#31614;&#65292;&#35813;&#25968;&#25454;&#38598;&#36866;&#29992;&#20110;&#20855;&#26377;&#32570;&#22833;&#20449;&#24687;&#30340;&#36328;&#22495;&#20869;&#23481;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#26694;&#26550;&#26469;&#29983;&#25104;&#32570;&#22833;&#19978;&#19979;&#25991;&#30340;&#26377;&#25928;&#29992;&#25143;&#26597;&#35810;&#12290;&#19982;&#22522;&#32447;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#29992;&#25143;&#21487;&#22238;&#31572;&#38382;&#39064;&#30340;&#27604;&#20363;38&#20010;&#30334;&#20998;&#28857;&#65292;&#24182;&#19988;&#20998;&#31867;&#24615;&#33021;&#25552;&#39640;&#20102;&#36229;&#36807;10&#20010;&#30334;&#20998;&#28857;&#30340;&#23439;F1&#12290;&#22240;&#27492;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#20026;&#26410;&#26469;&#30340;&#35823;&#35299;&#20449;&#24687;&#32531;&#35299;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Misinformation poses a variety of risks, such as undermining public trust and distorting factual discourse. Large Language Models (LLMs) like GPT-4 have been shown effective in mitigating misinformation, particularly in handling statements where enough context is provided. However, they struggle to assess ambiguous or context-deficient statements accurately. This work introduces a new method to resolve uncertainty in such statements. We propose a framework to categorize missing information and publish category labels for the LIAR-New dataset, which is adaptable to cross-domain content with missing information. We then leverage this framework to generate effective user queries for missing context. Compared to baselines, our method improves the rate at which generated questions are answerable by the user by 38 percentage points and classification performance by over 10 percentage points macro F1. Thus, this approach may provide a valuable component for future misinformation mitigation pi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#19981;&#21516;&#31867;&#22411;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#32479;&#19968;&#20026;&#22270;&#24418;&#26684;&#24335;&#65292;&#24182;&#23558;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#36716;&#21270;&#20026;&#22270;&#24418;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#21033;&#29992;&#36755;&#20837;&#22270;&#24418;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#20316;&#32773;&#35774;&#35745;&#20102;&#19968;&#20010;&#32467;&#26500;&#22686;&#24378;&#30340;Transformer&#65292;&#24182;&#20351;&#29992;&#20301;&#32622;&#30697;&#38453;&#32534;&#30721;&#20102;&#30456;&#36830;&#33410;&#28857;&#30340;&#30456;&#23545;&#20301;&#32622;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2401.01183</link><description>&lt;p&gt;
&#23558;&#32467;&#26500;&#21270;&#25968;&#25454;&#32479;&#19968;&#20026;&#22270;&#24418;&#20197;&#36827;&#34892;&#25968;&#25454;&#21040;&#25991;&#26412;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Unifying Structured Data as Graph for Data-to-Text Pre-Training. (arXiv:2401.01183v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#19981;&#21516;&#31867;&#22411;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#32479;&#19968;&#20026;&#22270;&#24418;&#26684;&#24335;&#65292;&#24182;&#23558;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#36716;&#21270;&#20026;&#22270;&#24418;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#21033;&#29992;&#36755;&#20837;&#22270;&#24418;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#20316;&#32773;&#35774;&#35745;&#20102;&#19968;&#20010;&#32467;&#26500;&#22686;&#24378;&#30340;Transformer&#65292;&#24182;&#20351;&#29992;&#20301;&#32622;&#30697;&#38453;&#32534;&#30721;&#20102;&#30456;&#36830;&#33410;&#28857;&#30340;&#30456;&#23545;&#20301;&#32622;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#21040;&#25991;&#26412;&#65288;D2T&#65289;&#29983;&#25104;&#26088;&#22312;&#23558;&#32467;&#26500;&#21270;&#25968;&#25454;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#12290;&#25968;&#25454;&#21040;&#25991;&#26412;&#39044;&#35757;&#32451;&#24050;&#34987;&#35777;&#26126;&#22312;&#22686;&#24378;D2T&#29983;&#25104;&#24182;&#20135;&#29983;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#35201;&#20040;&#23558;&#32467;&#26500;&#21270;&#25968;&#25454;&#36807;&#24230;&#31616;&#21270;&#20026;&#19968;&#20010;&#24207;&#21015;&#65292;&#32780;&#19981;&#32771;&#34385;&#36755;&#20837;&#30340;&#32467;&#26500;&#65292;&#35201;&#20040;&#35774;&#35745;&#29992;&#20110;&#29305;&#23450;&#25968;&#25454;&#32467;&#26500;&#65288;&#20363;&#22914;&#34920;&#26684;&#25110;&#30693;&#35782;&#22270;&#65289;&#30340;&#35757;&#32451;&#30446;&#26631;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#19981;&#21516;&#31867;&#22411;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#21363;&#65292;&#34920;&#26684;&#12289;&#38190;&#20540;&#25968;&#25454;&#12289;&#30693;&#35782;&#22270;&#65289;&#32479;&#19968;&#20026;&#22270;&#24418;&#26684;&#24335;&#65292;&#24182;&#23558;&#19981;&#21516;&#30340;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#36716;&#21270;&#20026;&#22270;&#24418;&#21040;&#25991;&#26412;&#29983;&#25104;&#12290;&#20026;&#20102;&#26377;&#25928;&#21033;&#29992;&#36755;&#20837;&#22270;&#24418;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;D2T&#29983;&#25104;&#30340;&#32467;&#26500;&#22686;&#24378;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#32467;&#26500;&#22686;&#24378;&#30340;Transformer&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20026;Transformer&#35774;&#35745;&#20102;&#19968;&#20010;&#20301;&#32622;&#30697;&#38453;&#65292;&#29992;&#20110;&#32534;&#30721;&#36755;&#20837;&#22270;&#24418;&#20013;&#30456;&#36830;&#33410;&#28857;&#30340;&#30456;&#23545;&#20301;&#32622;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#32467;&#26500;&#30340;&#21477;&#27861;&#26631;&#31614;&#20998;&#31867;&#26041;&#27861;&#65292;&#29992;&#20110;&#26377;&#25928;&#32467;&#21512;&#21477;&#27861;&#20449;&#24687;&#21644;structure-enhanced&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-to-text (D2T) generation aims to transform structured data into natural language text. Data-to-text pre-training has proved to be powerful in enhancing D2T generation and yields impressive performances. However, previous pre-training methods either oversimplified structured data into a sequence without considering input structures or designed training objectives tailored for a specific data structure (e.g., table or knowledge graph). In this paper, we unify different types of structured data (i.e., table, key-value data, knowledge graph) into the graph format and cast different data-to-text generation tasks as graph-to-text generation. To effectively exploit the structural information of the input graph, we propose a structure-enhanced pre-training method for D2T generation by designing a structure-enhanced Transformer. Concretely, we devise a position matrix for the Transformer, encoding relative positional information of connected nodes in the input graph. In addition, we propos
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20018;&#32852;&#20998;&#31867;&#26694;&#26550;&#65292;&#29992;&#20110;&#25581;&#31034;&#36234;&#21335;&#20135;&#21697;&#35780;&#35770;&#20013;&#30340;&#27604;&#36739;&#24773;&#24863;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#35299;&#20915;&#19977;&#20010;&#39034;&#24207;&#23376;&#20219;&#21153;&#65292;&#21363;&#35782;&#21035;&#27604;&#36739;&#21477;&#12289;&#25552;&#21462;&#27604;&#36739;&#20803;&#32032;&#21644;&#20998;&#31867;&#27604;&#36739;&#31867;&#22411;&#12290;&#30740;&#31350;&#32467;&#26524;&#22312;&#36234;&#21335;&#35821;&#35328;&#21644;&#35821;&#38899;&#22788;&#29702;&#25361;&#25112;&#36187;&#20013;&#21462;&#24471;&#20102;&#31532;&#20116;&#21517;&#30340;&#25104;&#32489;&#12290;</title><link>http://arxiv.org/abs/2401.01108</link><description>&lt;p&gt;
&#25581;&#31034;&#36234;&#21335;&#20135;&#21697;&#35780;&#35770;&#20013;&#30340;&#27604;&#36739;&#24773;&#24863;&#65306;&#19968;&#31181;&#39034;&#24207;&#20998;&#31867;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Unveiling Comparative Sentiments in Vietnamese Product Reviews: A Sequential Classification Framework. (arXiv:2401.01108v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01108
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20018;&#32852;&#20998;&#31867;&#26694;&#26550;&#65292;&#29992;&#20110;&#25581;&#31034;&#36234;&#21335;&#20135;&#21697;&#35780;&#35770;&#20013;&#30340;&#27604;&#36739;&#24773;&#24863;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#35299;&#20915;&#19977;&#20010;&#39034;&#24207;&#23376;&#20219;&#21153;&#65292;&#21363;&#35782;&#21035;&#27604;&#36739;&#21477;&#12289;&#25552;&#21462;&#27604;&#36739;&#20803;&#32032;&#21644;&#20998;&#31867;&#27604;&#36739;&#31867;&#22411;&#12290;&#30740;&#31350;&#32467;&#26524;&#22312;&#36234;&#21335;&#35821;&#35328;&#21644;&#35821;&#38899;&#22788;&#29702;&#25361;&#25112;&#36187;&#20013;&#21462;&#24471;&#20102;&#31532;&#20116;&#21517;&#30340;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27604;&#36739;&#24847;&#35265;&#25366;&#25496;&#26159;&#24773;&#24863;&#20998;&#26512;&#30340;&#19968;&#20010;&#19987;&#38376;&#39046;&#22495;&#65292;&#26088;&#22312;&#35782;&#21035;&#21644;&#25552;&#21462;&#34920;&#36798;&#27604;&#36739;&#24773;&#24863;&#30340;&#20869;&#23481;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#35299;&#20915;&#19977;&#20010;&#39034;&#24207;&#23376;&#20219;&#21153;&#65306;&#65288;&#19968;&#65289;&#35782;&#21035;&#27604;&#36739;&#21477;&#65292;&#21363;&#21028;&#26029;&#21477;&#23376;&#26159;&#21542;&#20855;&#26377;&#27604;&#36739;&#24847;&#20041;&#65292;&#65288;&#20108;&#65289;&#25552;&#21462;&#27604;&#36739;&#20803;&#32032;&#65292;&#21363;&#27604;&#36739;&#20027;&#20307;&#12289;&#23545;&#35937;&#12289;&#26041;&#38754;&#12289;&#35859;&#35789;&#26159;&#20160;&#20040;&#65292;&#65288;&#19977;&#65289;&#20998;&#31867;&#27604;&#36739;&#31867;&#22411;&#65292;&#20174;&#32780;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#36234;&#21335;&#20135;&#21697;&#35780;&#35770;&#20013;&#29992;&#25143;&#30340;&#24773;&#24863;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36234;&#21335;&#35821;&#35328;&#21644;&#35821;&#38899;&#22788;&#29702;&#65288;VLSP&#65289;2023&#25361;&#25112;&#36187;&#30340;&#27604;&#36739;&#24847;&#35265;&#25366;&#25496;&#65288;ComOM&#65289;&#20219;&#21153;&#20013;&#25490;&#21517;&#31532;&#20116;&#12290;
&lt;/p&gt;
&lt;p&gt;
Comparative opinion mining is a specialized field of sentiment analysis that aims to identify and extract sentiments expressed comparatively. To address this task, we propose an approach that consists of solving three sequential sub-tasks: (i) identifying comparative sentence, i.e., if a sentence has a comparative meaning, (ii) extracting comparative elements, i.e., what are comparison subjects, objects, aspects, predicates, and (iii) classifying comparison types which contribute to a deeper comprehension of user sentiments in Vietnamese product reviews. Our method is ranked fifth at the Vietnamese Language and Speech Processing (VLSP) 2023 challenge on Comparative Opinion Mining (ComOM) from Vietnamese Product Reviews.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Quokka&#8212;&#8212;&#19968;&#20010;&#29992;&#20110;&#26448;&#26009;&#31185;&#23398;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#36890;&#36807;&#23545;&#36229;&#36807;&#19968;&#30334;&#19975;&#31687;&#39046;&#22495;&#29305;&#23450;&#30340;&#35770;&#25991;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#26448;&#26009;&#31185;&#23398;&#39046;&#22495;&#30340;&#26597;&#35810;&#20013;&#25552;&#20379;&#21363;&#26102;&#30340;&#19978;&#19979;&#25991;&#24847;&#35782;&#21709;&#24212;&#12290;</title><link>http://arxiv.org/abs/2401.01089</link><description>&lt;p&gt;
Quokka: &#19968;&#20010;&#29992;&#20110;&#26448;&#26009;&#31185;&#23398;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Quokka: An Open-source Large Language Model ChatBot for Material Science. (arXiv:2401.01089v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Quokka&#8212;&#8212;&#19968;&#20010;&#29992;&#20110;&#26448;&#26009;&#31185;&#23398;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#36890;&#36807;&#23545;&#36229;&#36807;&#19968;&#30334;&#19975;&#31687;&#39046;&#22495;&#29305;&#23450;&#30340;&#35770;&#25991;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#26448;&#26009;&#31185;&#23398;&#39046;&#22495;&#30340;&#26597;&#35810;&#20013;&#25552;&#20379;&#21363;&#26102;&#30340;&#19978;&#19979;&#25991;&#24847;&#35782;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#26448;&#26009;&#31185;&#23398;&#30340;&#19987;&#29992;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#24320;&#21457;&#65292;&#21033;&#29992;&#20102;Llama-2&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#22312;S2ORC&#25968;&#25454;&#38598;&#20013;&#30340;&#26448;&#26009;&#31185;&#23398;&#39046;&#22495;&#30340;&#24191;&#27867;&#30740;&#31350;&#25991;&#31456;&#19978;&#36827;&#34892;&#25345;&#32493;&#30340;&#39044;&#35757;&#32451;&#12290;&#26041;&#27861;&#21253;&#25324;&#39318;&#20808;&#22312;&#36229;&#36807;&#19968;&#30334;&#19975;&#31687;&#39046;&#22495;&#29305;&#23450;&#30340;&#35770;&#25991;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#36807;&#31243;&#26469;&#25913;&#21892;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#33021;&#21147;&#12290;&#35813;&#32842;&#22825;&#26426;&#22120;&#20154;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#21363;&#26102;&#30340;&#12289;&#20855;&#26377;&#19978;&#19979;&#25991;&#24847;&#35782;&#30340;&#26448;&#26009;&#31185;&#23398;&#39046;&#22495;&#30340;&#26597;&#35810;&#21709;&#24212;&#65292;&#26469;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#12289;&#25945;&#24072;&#21644;&#23398;&#29983;&#12290;&#25105;&#20204;&#23558;&#22235;&#20010;&#32463;&#36807;&#35757;&#32451;&#30340;&#26816;&#26597;&#28857;&#65288;7B&#12289;13B&#65292;&#24102;&#25110;&#19981;&#24102;&#32842;&#22825;&#21151;&#33021;&#65289;&#20813;&#36153;&#25552;&#20379;&#32473;&#30740;&#31350;&#30028;&#65292;&#32593;&#22336;&#20026;https://github.com/Xianjun-Yang/Quokka&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the development of a specialized chatbot for materials science, leveraging the Llama-2 language model, and continuing pre-training on the expansive research articles in the materials science domain from the S2ORC dataset. The methodology involves an initial pretraining phase on over one million domain-specific papers, followed by an instruction-tuning process to refine the chatbot's capabilities. The chatbot is designed to assist researchers, educators, and students by providing instant, context-aware responses to queries in the field of materials science. We make the four trained checkpoints (7B, 13B, with or without chat ability) freely available to the research community at https://github.com/Xianjun-Yang/Quokka.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25104;&#21151;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#36234;&#21335;&#35799;&#27468;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#32034;&#20102;&#23558;&#35799;&#27468;&#32763;&#35793;&#25104;&#19981;&#21516;&#35821;&#35328;&#30340;&#21487;&#33021;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#23545;&#29983;&#25104;&#20869;&#23481;&#30340;&#23436;&#20840;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.01078</link><description>&lt;p&gt;
&#36234;&#21335;&#35799;&#27468;&#29983;&#25104;&#19982;&#36328;&#35821;&#35328;&#35799;&#27468;&#32763;&#35793;&#30340;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
Vietnamese Poem Generation &amp; The Prospect Of Cross-Language Poem-To-Poem Translation. (arXiv:2401.01078v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01078
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25104;&#21151;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#36234;&#21335;&#35799;&#27468;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#32034;&#20102;&#23558;&#35799;&#27468;&#32763;&#35793;&#25104;&#19981;&#21516;&#35821;&#35328;&#30340;&#21487;&#33021;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#23545;&#29983;&#25104;&#20869;&#23481;&#30340;&#23436;&#20840;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35799;&#27468;&#29983;&#25104;&#19968;&#30452;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#19968;&#39033;&#25361;&#25112;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#35201;&#27714;&#27169;&#22411;&#29702;&#35299;&#35821;&#35328;&#12289;&#24773;&#24863;&#21644;&#39118;&#26684;&#30340;&#32454;&#24494;&#24046;&#21035;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#20013;&#29983;&#25104;&#36234;&#21335;&#35799;&#27468;&#65292;&#20174;&#32780;&#23454;&#29616;&#30452;&#35266;&#30340;&#36807;&#31243;&#21644;&#22686;&#24378;&#30340;&#20869;&#23481;&#25511;&#21046;&#12290;&#25105;&#20204;&#26368;&#26377;&#25928;&#30340;&#27169;&#22411;&#65292;GPT-3 Babbage&#21464;&#31181;&#65292;&#22312;&#36234;&#21335;&#35799;&#27468;&#30340;&#8220;&#20845;&#20843;&#35789;&#8221;&#31867;&#22411;&#20013;&#23454;&#29616;&#20102;0.8&#30340;&#33258;&#23450;&#20041;&#35780;&#20998;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#23558;&#35799;&#27468;&#25913;&#20889;&#25104;&#27491;&#24120;&#25991;&#26412;&#25552;&#31034;&#30340;&#24819;&#27861;&#65292;&#24182;&#22312;&#8220;&#20845;&#20843;&#35789;&#8221;&#31867;&#22411;&#20013;&#33719;&#24471;&#20102;&#30456;&#23545;&#36739;&#39640;&#30340;0.718&#20998;&#25968;&#12290;&#36825;&#20010;&#23454;&#39564;&#23637;&#31034;&#20102;&#20197;&#32763;&#35793;&#21518;&#30340;&#35799;&#27468;&#20316;&#20026;&#36755;&#20837;&#36827;&#34892;&#36328;&#35821;&#35328;&#35799;&#27468;&#32763;&#35793;&#30340;&#28508;&#21147;&#65292;&#24182;&#21516;&#26102;&#20445;&#25345;&#23545;&#29983;&#25104;&#20869;&#23481;&#30340;&#23436;&#20840;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Poetry generation has been a challenging task in the field of Natural Language Processing, as it requires the model to understand the nuances of language, sentiment, and style. In this paper, we propose using Large Language Models to generate Vietnamese poems from natural language prompts, thereby facilitating an intuitive process with enhanced content control. Our most efficacious model, the GPT-3 Babbage variant, achieves a custom evaluation score of 0.8, specifically tailored to the "luc bat" genre of Vietnamese poetry. Furthermore, we also explore the idea of paraphrasing poems into normal text prompts and yield a relatively high score of 0.718 in the "luc bat" genre. This experiment presents the potential for cross-Language poem-to-poem translation with translated poems as the inputs while concurrently maintaining complete control over the generated content.
&lt;/p&gt;</description></item><item><title>DialCLIP&#26159;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#23545;&#35805;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;CLIP&#20013;&#24341;&#20837;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#25552;&#31034;&#29983;&#25104;&#22120;&#21644;&#39046;&#22495;&#25552;&#31034;&#26469;&#25552;&#21319;&#23545;&#35805;&#26816;&#32034;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.01076</link><description>&lt;p&gt;
DialCLIP: &#23558;CLIP&#25193;&#23637;&#20026;&#22810;&#27169;&#24577;&#23545;&#35805;&#26816;&#32034;&#22120;
&lt;/p&gt;
&lt;p&gt;
DialCLIP: Empowering CLIP as Multi-Modal Dialog Retriever. (arXiv:2401.01076v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01076
&lt;/p&gt;
&lt;p&gt;
DialCLIP&#26159;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#23545;&#35805;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;CLIP&#20013;&#24341;&#20837;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#25552;&#31034;&#29983;&#25104;&#22120;&#21644;&#39046;&#22495;&#25552;&#31034;&#26469;&#25552;&#21319;&#23545;&#35805;&#26816;&#32034;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#26497;&#22823;&#22320;&#25552;&#21319;&#20102;&#22810;&#27169;&#24577;&#23545;&#35805;&#31995;&#32479;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20027;&#35201;&#38598;&#20013;&#22312;&#26377;&#25928;&#22320;&#25429;&#25417;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24577;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#24448;&#24448;&#24573;&#35270;&#20102;&#23545;&#35805;&#29615;&#22659;&#30340;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DialCLIP&#30340;&#21442;&#25968;&#39640;&#25928;&#30340;&#25552;&#31034;&#24494;&#35843;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#23545;&#35805;&#26816;&#32034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#25552;&#31034;&#29983;&#25104;&#22120;&#65292;&#29992;&#20110;&#23398;&#20064;&#19978;&#19979;&#25991;&#29305;&#24449;&#65292;&#24182;&#22312;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;CLIP&#20013;&#23558;&#20854;&#25552;&#28860;&#20026;&#25552;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#39046;&#22495;&#25552;&#31034;&#65292;&#20197;&#20943;&#36731;&#19979;&#28216;&#23545;&#35805;&#25968;&#25454;&#24341;&#36215;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#26041;&#20415;&#21508;&#31181;&#31867;&#22411;&#30340;&#26816;&#32034;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#22810;&#20010;&#19987;&#23478;&#65292;&#20174;CLIP&#30340;&#36755;&#20986;&#23398;&#20064;&#21040;&#22810;&#27169;&#24577;&#34920;&#31034;&#31354;&#38388;&#30340;&#26144;&#23556;&#65292;&#27599;&#20010;&#19987;&#23478;&#37117;&#26377;&#33258;&#24049;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, substantial advancements in pre-trained vision-language models have greatly enhanced the capabilities of multi-modal dialog systems. These models have demonstrated significant improvements by fine-tuning on downstream tasks. However, the existing pre-trained models primarily focus on effectively capturing the alignment between vision and language modalities, often ignoring the intricate nature of dialog context. In this paper, we propose a parameter-efficient prompt-tuning method named DialCLIP for multi-modal dialog retrieval. Specifically, our approach introduces a multi-modal context prompt generator to learn context features which are subsequently distilled into prompts within the pre-trained vision-language model CLIP. Besides, we introduce domain prompt to mitigate the disc repancy from the downstream dialog data. To facilitate various types of retrieval, we also design multiple experts to learn mappings from CLIP outputs to multi-modal representation space, with each e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#27969;&#31243;&#65292;&#36890;&#36807;&#20027;&#39064;&#27169;&#22411;&#21644;&#32479;&#35745;&#20998;&#26512;&#26041;&#27861;&#20174;&#27861;&#24459;&#35009;&#20915;&#25991;&#26412;&#20013;&#21457;&#29616;&#37325;&#35201;&#20027;&#39064;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#35782;&#21035;&#19982;&#32467;&#26524;&#30456;&#20851;&#30340;&#26696;&#20363;&#20027;&#39064;&#65292;&#24182;&#36890;&#36807;&#20027;&#39064;-&#35789;&#20998;&#24067;&#21644;&#26696;&#20363;-&#20027;&#39064;&#26435;&#37325;&#26469;&#25552;&#20379;&#26356;&#22810;&#20449;&#24687;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20855;&#26377;&#24456;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.01068</link><description>&lt;p&gt;
&#20174;&#27861;&#24459;&#35009;&#20915;&#20013;&#21457;&#29616;&#37325;&#35201;&#20027;&#39064;&#30340;&#36873;&#25321;&#25512;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Discovering Significant Topics from Legal Decisions with Selective Inference. (arXiv:2401.01068v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#27969;&#31243;&#65292;&#36890;&#36807;&#20027;&#39064;&#27169;&#22411;&#21644;&#32479;&#35745;&#20998;&#26512;&#26041;&#27861;&#20174;&#27861;&#24459;&#35009;&#20915;&#25991;&#26412;&#20013;&#21457;&#29616;&#37325;&#35201;&#20027;&#39064;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#35782;&#21035;&#19982;&#32467;&#26524;&#30456;&#20851;&#30340;&#26696;&#20363;&#20027;&#39064;&#65292;&#24182;&#36890;&#36807;&#20027;&#39064;-&#35789;&#20998;&#24067;&#21644;&#26696;&#20363;-&#20027;&#39064;&#26435;&#37325;&#26469;&#25552;&#20379;&#26356;&#22810;&#20449;&#24687;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20855;&#26377;&#24456;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#27969;&#31243;&#65292;&#36890;&#36807;&#23558;&#20027;&#39064;&#27169;&#22411;&#21512;&#25104;&#30340;&#29305;&#24449;&#36890;&#36807;&#21463;&#24809;&#32602;&#30340;&#22238;&#24402;&#21644;&#21518;&#36873;&#25321;&#26174;&#33879;&#24615;&#26816;&#39564;&#26469;&#21457;&#29616;&#27861;&#24459;&#35009;&#20915;&#25991;&#26412;&#20013;&#30340;&#37325;&#35201;&#20027;&#39064;&#12290;&#35813;&#26041;&#27861;&#35782;&#21035;&#20986;&#19982;&#32467;&#26524;&#26174;&#33879;&#30456;&#20851;&#30340;&#26696;&#20363;&#20027;&#39064;&#65292;&#21487;&#20197;&#25163;&#21160;&#35299;&#37322;&#20197;&#33719;&#21462;&#26377;&#20851;&#37325;&#35201;&#20027;&#39064;&#30340;&#35265;&#35299;&#30340;&#20027;&#39064;-&#35789;&#20998;&#24067;&#65292;&#20197;&#21450;&#21487;&#20197;&#29992;&#20110;&#26631;&#35782;&#27599;&#20010;&#20027;&#39064;&#30340;&#20195;&#34920;&#24615;&#26696;&#20363;&#30340;&#26696;&#20363;-&#20027;&#39064;&#26435;&#37325;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#26032;&#30340;&#22495;&#21517;&#20105;&#35758;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#27431;&#27954;&#20154;&#26435;&#27861;&#38498;&#36829;&#35268;&#26696;&#20363;&#30340;&#32463;&#20856;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#12290;&#22522;&#20110;&#28508;&#22312;&#35821;&#20041;&#20998;&#26512;&#21644;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#30340;&#20027;&#39064;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36890;&#36807;&#27969;&#31243;&#25512;&#23548;&#30340;&#20027;&#39064;&#22312;&#20004;&#20010;&#39046;&#22495;&#20013;&#19982;&#27861;&#24459;&#25945;&#26465;&#19968;&#33268;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#20854;&#20182;&#30456;&#20851;&#27861;&#24459;&#20998;&#26512;&#20219;&#21153;&#20013;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose and evaluate an automated pipeline for discovering significant topics from legal decision texts by passing features synthesized with topic models through penalised regressions and post-selection significance tests. The method identifies case topics significantly correlated with outcomes, topic-word distributions which can be manually-interpreted to gain insights about significant topics, and case-topic weights which can be used to identify representative cases for each topic. We demonstrate the method on a new dataset of domain name disputes and a canonical dataset of European Court of Human Rights violation cases. Topic models based on latent semantic analysis as well as language model embeddings are evaluated. We show that topics derived by the pipeline are consistent with legal doctrines in both areas and can be useful in other related legal analysis tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;LLaMA&#36229;&#36234;&#33521;&#35821;&#65306;&#35821;&#35328;&#33021;&#21147;&#36716;&#31227;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#36890;&#36807;&#23545;LLaMA&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#35777;&#35843;&#26597;&#65292;&#20998;&#26512;&#20102;&#35789;&#27719;&#25193;&#23637;&#12289;&#36827;&#19968;&#27493;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#31561;&#20851;&#38190;&#22240;&#32032;&#23545;&#38750;&#33521;&#35821;&#35821;&#35328;&#19978;&#30340;&#33021;&#21147;&#36716;&#31227;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#22235;&#20010;&#26631;&#20934;&#21270;&#27979;&#35797;&#22522;&#20934;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#30693;&#35782;&#27700;&#24179;&#21644;&#21709;&#24212;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.01055</link><description>&lt;p&gt;
LLaMA&#36229;&#36234;&#33521;&#35821;&#65306;&#35821;&#35328;&#33021;&#21147;&#36716;&#31227;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
LLaMA Beyond English: An Empirical Study on Language Capability Transfer. (arXiv:2401.01055v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;LLaMA&#36229;&#36234;&#33521;&#35821;&#65306;&#35821;&#35328;&#33021;&#21147;&#36716;&#31227;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#36890;&#36807;&#23545;LLaMA&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#35777;&#35843;&#26597;&#65292;&#20998;&#26512;&#20102;&#35789;&#27719;&#25193;&#23637;&#12289;&#36827;&#19968;&#27493;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#31561;&#20851;&#38190;&#22240;&#32032;&#23545;&#38750;&#33521;&#35821;&#35821;&#35328;&#19978;&#30340;&#33021;&#21147;&#36716;&#31227;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#22235;&#20010;&#26631;&#20934;&#21270;&#27979;&#35797;&#22522;&#20934;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#30693;&#35782;&#27700;&#24179;&#21644;&#21709;&#24212;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#22914;ChatGPT&#65292;&#22312;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#29087;&#32451;&#24230;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#20027;&#27969;&#30340;LLM&#65288;&#22914;LLaMA&#65289;&#26159;&#22522;&#20110;&#20197;&#33521;&#35821;&#20026;&#20027;&#23548;&#30340;&#35821;&#26009;&#24211;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#20854;&#20182;&#38750;&#33521;&#35821;&#35821;&#35328;&#20013;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#22914;&#20309;&#26377;&#25928;&#22320;&#23558;&#35821;&#35328;&#29983;&#25104;&#21644;&#36981;&#24490;&#25351;&#31034;&#30340;&#33021;&#21147;&#36716;&#31227;&#21040;&#38750;&#33521;&#35821;&#35821;&#35328;&#19978;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22522;&#20110;LLaMA&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#35843;&#26597;&#65292;&#24635;&#35745;&#32791;&#36153;&#20102;1440&#20010;GPU&#23567;&#26102;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#35832;&#22914;&#35789;&#27719;&#25193;&#23637;&#12289;&#36827;&#19968;&#27493;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#31561;&#20851;&#38190;&#22240;&#32032;&#23545;&#36716;&#31227;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#20934;&#30830;&#35780;&#20272;&#27169;&#22411;&#30340;&#30693;&#35782;&#27700;&#24179;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#22235;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#26631;&#20934;&#21270;&#27979;&#35797;&#22522;&#20934;&#65306;C-Eval&#12289;MMLU&#12289;AGI-Eval&#21644;GAOKAO-Bench&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;&#27169;&#22411;&#30340;&#21709;&#24212;&#36136;&#37327;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#32771;&#34385;&#20102;&#35832;&#22914;...
&lt;/p&gt;
&lt;p&gt;
In recent times, substantial advancements have been witnessed in large language models (LLMs), exemplified by ChatGPT, showcasing remarkable proficiency across a range of complex tasks. However, many mainstream LLMs (e.g. LLaMA) are pretrained on English-dominant corpus, which limits their performance in other non-English languages. In this paper, we focus on how to effectively transfer the capabilities of language generation and following instructions to a non-English language. To answer this question, we conduct an extensive empirical investigation based on LLaMA, accumulating over 1440 GPU hours. We analyze the impact of key factors such as vocabulary extension, further pretraining, and instruction tuning on transfer. To accurately assess the model's level of knowledge, we employ four widely used standardized testing benchmarks: C-Eval, MMLU, AGI-Eval, and GAOKAO-Bench. Furthermore, a comprehensive evaluation of the model's response quality is conducted, considering aspects such as 
&lt;/p&gt;</description></item><item><title>Cheetah&#26159;&#19968;&#20010;&#38754;&#21521;517&#31181;&#38750;&#27954;&#35821;&#35328;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#31181;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#32508;&#21512;&#35780;&#20272;&#21644;&#20154;&#24037;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#29983;&#25104;&#36830;&#36143;&#21644;&#19978;&#19979;&#25991;&#24688;&#24403;&#30340;&#25991;&#26412;&#26041;&#38754;&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#20419;&#36827;&#35821;&#35328;&#22810;&#26679;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.01053</link><description>&lt;p&gt;
Cheetah: 517&#31181;&#38750;&#27954;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Cheetah: Natural Language Generation for 517 African Languages. (arXiv:2401.01053v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01053
&lt;/p&gt;
&lt;p&gt;
Cheetah&#26159;&#19968;&#20010;&#38754;&#21521;517&#31181;&#38750;&#27954;&#35821;&#35328;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#31181;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#32508;&#21512;&#35780;&#20272;&#21644;&#20154;&#24037;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#29983;&#25104;&#36830;&#36143;&#21644;&#19978;&#19979;&#25991;&#24688;&#24403;&#30340;&#25991;&#26412;&#26041;&#38754;&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#20419;&#36827;&#35821;&#35328;&#22810;&#26679;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702; (NLP) &#20219;&#21153;&#26469;&#35828;&#65292;&#38750;&#27954;&#35821;&#35328;&#36164;&#28304;&#31232;&#32570;&#26159;&#19968;&#20010;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104; (NLG)&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;Cheetah&#65292;&#19968;&#20010;&#38754;&#21521;&#38750;&#27954;&#35821;&#35328;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#31181;NLG&#35821;&#35328;&#27169;&#22411;&#12290;Cheetah&#25903;&#25345;517&#31181;&#38750;&#27954;&#35821;&#35328;&#21644;&#35821;&#35328;&#21464;&#20307;&#65292;&#35299;&#20915;&#20102;NLG&#36164;&#28304;&#21294;&#20047;&#38382;&#39064;&#65292;&#24182;&#20026;&#20419;&#36827;&#35821;&#35328;&#22810;&#26679;&#24615;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#36890;&#36807;&#19971;&#20010;&#29983;&#25104;&#19979;&#28216;&#20219;&#21153;&#30340;&#32508;&#21512;&#35780;&#20272;&#35777;&#26126;&#20102;Cheetah&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#19971;&#20010;&#20219;&#21153;&#20013;&#30340;&#20116;&#20010;&#20219;&#21153;&#20013;&#65292;Cheetah&#30340;&#34920;&#29616;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#24191;&#27867;&#33539;&#22260;&#30340;&#38750;&#27954;&#35821;&#35328;&#20013;&#29983;&#25104;&#36830;&#36143;&#21644;&#19978;&#19979;&#25991;&#24688;&#24403;&#30340;&#25991;&#26412;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#20154;&#24037;&#35780;&#20272;&#65292;&#20197;&#28145;&#20837;&#20102;&#35299;Cheetah&#30340;&#35821;&#35328;&#33021;&#21147;&#12290;Cheetah&#30340;&#24341;&#20837;&#23545;&#35821;&#35328;&#22810;&#26679;&#24615;&#20855;&#26377;&#28145;&#36828;&#30340;&#30410;&#22788;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#24182;&#23558;&#20854;&#36866;&#24212;&#29305;&#23450;&#30340;&#38750;&#27954;&#35821;&#35328;&#65292;&#25105;&#20204;&#33021;&#22815;&#25552;&#20379;&#26356;&#22810;&#30340;&#35821;&#35328;&#29983;&#25104;&#36873;&#25321;&#21644;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-resource African languages pose unique challenges for natural language processing (NLP) tasks, including natural language generation (NLG). In this paper, we develop Cheetah, a massively multilingual NLG language model for African languages. Cheetah supports 517 African languages and language varieties, allowing us to address the scarcity of NLG resources and provide a solution to foster linguistic diversity. We demonstrate the effectiveness of Cheetah through comprehensive evaluations across seven generation downstream tasks. In five of the seven tasks, Cheetah significantly outperforms other models, showcasing its remarkable performance for generating coherent and contextually appropriate text in a wide range of African languages. We additionally conduct a detailed human evaluation to delve deeper into the linguistic capabilities of Cheetah. The introduction of Cheetah has far-reaching benefits for linguistic diversity. By leveraging pretrained models and adapting them to specifi
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Auffusion&#30340;&#25991;&#26412;&#21040;&#38899;&#39057;&#29983;&#25104;&#31995;&#32479;&#65292;&#23427;&#21033;&#29992;&#25193;&#25955;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#36328;&#27169;&#24577;&#23545;&#40784;&#30340;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#36136;&#37327;&#21644;&#25991;&#26412;-&#38899;&#39057;&#23545;&#40784;&#12290;&#35813;&#31995;&#32479;&#22312;&#26377;&#38480;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#19979;&#32988;&#36807;&#20102;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#20851;&#27880;&#20102;&#32534;&#30721;&#22120;&#36873;&#25321;&#23545;&#36328;&#27169;&#24577;&#23545;&#40784;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.01044</link><description>&lt;p&gt;
Auffusion: &#21033;&#29992;&#25193;&#25955;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21319;&#25991;&#26412;&#21040;&#38899;&#39057;&#29983;&#25104;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Auffusion: Leveraging the Power of Diffusion and Large Language Models for Text-to-Audio Generation. (arXiv:2401.01044v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01044
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Auffusion&#30340;&#25991;&#26412;&#21040;&#38899;&#39057;&#29983;&#25104;&#31995;&#32479;&#65292;&#23427;&#21033;&#29992;&#25193;&#25955;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#36328;&#27169;&#24577;&#23545;&#40784;&#30340;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#36136;&#37327;&#21644;&#25991;&#26412;-&#38899;&#39057;&#23545;&#40784;&#12290;&#35813;&#31995;&#32479;&#22312;&#26377;&#38480;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#19979;&#32988;&#36807;&#20102;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#20851;&#27880;&#20102;&#32534;&#30721;&#22120;&#36873;&#25321;&#23545;&#36328;&#27169;&#24577;&#23545;&#40784;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25193;&#25955;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#26174;&#33879;&#25512;&#21160;&#20102;AIGC&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#25991;&#26412;&#21040;&#38899;&#39057;&#65288;TTA&#65289;&#20316;&#20026;&#19968;&#31181;&#26032;&#20852;&#30340;AIGC&#24212;&#29992;&#65292;&#26088;&#22312;&#20174;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#29983;&#25104;&#38899;&#39057;&#65292;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;TTA&#30740;&#31350;&#24448;&#24448;&#22312;&#29983;&#25104;&#36136;&#37327;&#21644;&#25991;&#26412;-&#38899;&#39057;&#23545;&#40784;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#22797;&#26434;&#30340;&#25991;&#26412;&#36755;&#20837;&#12290;&#21463;&#21040;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#25193;&#25955;&#27169;&#22411;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Auffusion&#65292;&#19968;&#31181;&#23558;T2I&#27169;&#22411;&#26694;&#26550;&#36866;&#24212;TTA&#20219;&#21153;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#20854;&#22266;&#26377;&#30340;&#29983;&#25104;&#20248;&#21183;&#21644;&#31934;&#30830;&#30340;&#36328;&#27169;&#24577;&#23545;&#40784;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#34920;&#26126;&#65292;Auffusion&#22312;&#20351;&#29992;&#26377;&#38480;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#26102;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;TTA&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#20043;&#21069;&#30340;T2I&#30740;&#31350;&#35748;&#35782;&#21040;&#32534;&#30721;&#22120;&#36873;&#25321;&#23545;&#36328;&#27169;&#24577;&#23545;&#40784;&#30340;&#37325;&#35201;&#24433;&#21709;&#65292;&#20363;&#22914;&#32454;&#33410;&#21644;&#29289;&#20307;&#32465;&#23450;&#65292;&#32780;&#31867;&#20284;&#30340;&#35780;&#20272;&#21017;&#32570;&#20047;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in diffusion models and large language models (LLMs) have significantly propelled the field of AIGC. Text-to-Audio (TTA), a burgeoning AIGC application designed to generate audio from natural language prompts, is attracting increasing attention. However, existing TTA studies often struggle with generation quality and text-audio alignment, especially for complex textual inputs. Drawing inspiration from state-of-the-art Text-to-Image (T2I) diffusion models, we introduce Auffusion, a TTA system adapting T2I model frameworks to TTA task, by effectively leveraging their inherent generative strengths and precise cross-modal alignment. Our objective and subjective evaluations demonstrate that Auffusion surpasses previous TTA approaches using limited data and computational resource. Furthermore, previous studies in T2I recognizes the significant impact of encoder choice on cross-modal alignment, like fine-grained details and object bindings, while similar evaluation is lack
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DocLLM&#30340;&#38754;&#21521;&#24067;&#23616;&#24863;&#30693;&#30340;&#22810;&#27169;&#24577;&#25991;&#26723;&#29702;&#35299;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#25991;&#26412;&#35821;&#20041;&#21644;&#31354;&#38388;&#24067;&#23616;&#65292;&#36991;&#20813;&#20102;&#20351;&#29992;&#26114;&#36149;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#29702;&#35299;&#20225;&#19994;&#25991;&#26723;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.00908</link><description>&lt;p&gt;
DocLLM: &#19968;&#31181;&#38754;&#21521;&#24067;&#23616;&#24863;&#30693;&#30340;&#22810;&#27169;&#24577;&#25991;&#26723;&#29702;&#35299;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DocLLM: A layout-aware generative language model for multimodal document understanding. (arXiv:2401.00908v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DocLLM&#30340;&#38754;&#21521;&#24067;&#23616;&#24863;&#30693;&#30340;&#22810;&#27169;&#24577;&#25991;&#26723;&#29702;&#35299;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#25991;&#26412;&#35821;&#20041;&#21644;&#31354;&#38388;&#24067;&#23616;&#65292;&#36991;&#20813;&#20102;&#20351;&#29992;&#26114;&#36149;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#29702;&#35299;&#20225;&#19994;&#25991;&#26723;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20225;&#19994;&#25991;&#26723;&#65292;&#22914;&#34920;&#21333;&#12289;&#21457;&#31080;&#12289;&#25910;&#25454;&#12289;&#25253;&#21578;&#12289;&#21512;&#21516;&#31561;&#35760;&#24405;&#65292;&#36890;&#24120;&#22312;&#25991;&#26412;&#21644;&#31354;&#38388;&#27169;&#24577;&#30340;&#20132;&#27719;&#22788;&#20855;&#26377;&#20016;&#23500;&#30340;&#35821;&#20041;&#12290;&#23427;&#20204;&#22797;&#26434;&#30340;&#24067;&#23616;&#25152;&#25552;&#20379;&#30340;&#35270;&#35273;&#32447;&#32034;&#22312;&#26377;&#25928;&#29702;&#35299;&#36825;&#20123;&#25991;&#26723;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DocLLM&#65292;&#19968;&#20010;&#38024;&#23545;&#35270;&#35273;&#25991;&#26723;&#25512;&#29702;&#30340;&#36731;&#37327;&#32423;&#25193;&#23637;&#20256;&#32479;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#23427;&#32771;&#34385;&#20102;&#25991;&#26412;&#35821;&#20041;&#21644;&#31354;&#38388;&#24067;&#23616;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;LLMs&#19981;&#21516;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#65292;&#24182;&#19988;&#19987;&#27880;&#20110;&#21033;&#29992;&#36793;&#30028;&#26694;&#20449;&#24687;&#26469;&#34701;&#20837;&#31354;&#38388;&#24067;&#23616;&#32467;&#26500;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#32463;&#20856;Transformer&#20013;&#30340;&#27880;&#24847;&#26426;&#21046;&#20998;&#35299;&#20026;&#19968;&#32452;&#35299;&#32806;&#30697;&#38453;&#65292;&#26469;&#25429;&#25417;&#25991;&#26412;&#21644;&#31354;&#38388;&#27169;&#24577;&#20043;&#38388;&#30340;&#20132;&#21449;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#29992;&#20110;&#23398;&#20064;&#22635;&#20805;&#25991;&#26412;&#29255;&#27573;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#22788;&#29702;&#19981;&#35268;&#21017;&#30340;&#24067;&#23616;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Enterprise documents such as forms, invoices, receipts, reports, contracts, and other similar records, often carry rich semantics at the intersection of textual and spatial modalities. The visual cues offered by their complex layouts play a crucial role in comprehending these documents effectively. In this paper, we present DocLLM, a lightweight extension to traditional large language models (LLMs) for reasoning over visual documents, taking into account both textual semantics and spatial layout. Our model differs from existing multimodal LLMs by avoiding expensive image encoders and focuses exclusively on bounding box information to incorporate the spatial layout structure. Specifically, the cross-alignment between text and spatial modalities is captured by decomposing the attention mechanism in classical transformers to a set of disentangled matrices. Furthermore, we devise a pre-training objective that learns to infill text segments. This approach allows us to address irregular layo
&lt;/p&gt;</description></item><item><title>LaFFi&#26159;&#19968;&#31181;&#29992;&#20110;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#35201;&#27714;&#27169;&#22411;&#39044;&#27979;&#26631;&#27880;&#32773;&#23558;&#20250;&#32473;&#20986;&#30340;&#21453;&#39304;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#38382;&#31572;&#20219;&#21153;&#20013;&#30340;&#20934;&#30830;&#24615;&#65292;&#20026;&#24212;&#29992;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2401.00907</link><description>&lt;p&gt;
LaFFi: &#21033;&#29992;&#28151;&#21512;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#26469;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
LaFFi: Leveraging Hybrid Natural Language Feedback for Fine-tuning Language Models. (arXiv:2401.00907v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00907
&lt;/p&gt;
&lt;p&gt;
LaFFi&#26159;&#19968;&#31181;&#29992;&#20110;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#35201;&#27714;&#27169;&#22411;&#39044;&#27979;&#26631;&#27880;&#32773;&#23558;&#20250;&#32473;&#20986;&#30340;&#21453;&#39304;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#38382;&#31572;&#20219;&#21153;&#20013;&#30340;&#20934;&#30830;&#24615;&#65292;&#20026;&#24212;&#29992;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24494;&#35843;&#21487;&#20197;&#23558;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#36866;&#24212;&#29305;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20219;&#21153;&#29305;&#23450;&#24615;&#33021;&#12290;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;LLM&#34987;&#35757;&#32451;&#25104;&#20135;&#29983;&#26399;&#26395;&#30340;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;SFT&#35757;&#32451;&#30340;LLM&#22312;&#25512;&#29702;&#20219;&#21153;&#65288;&#22914;&#38382;&#31572;&#65289;&#20013;&#26377;&#26102;&#20250;&#20986;&#29616;&#31616;&#21333;&#38169;&#35823;&#21644;&#24187;&#35273;&#12290;&#22312;&#27809;&#26377;&#22806;&#37096;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#65292;SFT&#24456;&#38590;&#23398;&#20064;&#21040;&#38382;&#39064;&#21644;&#26399;&#26395;&#31572;&#26696;&#20043;&#38388;&#30340;&#33391;&#22909;&#26144;&#23556;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#25454;&#38598;&#36739;&#23567;&#30340;&#24773;&#20917;&#19979;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#24494;&#35843;LLM&#65288;LaFFi&#65289;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;LaFFi&#35201;&#27714;LLM&#30452;&#25509;&#39044;&#27979;&#26631;&#27880;&#32773;&#23558;&#20250;&#32473;&#20986;&#30340;&#21453;&#39304;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#26679;&#30340;&#21453;&#24605;&#35201;&#27714;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22312;&#39046;&#22495;&#20869;&#38382;&#31572;&#20219;&#21153;&#20013;&#30340;&#20934;&#30830;&#24615;&#65292;&#20026;&#22312;SFT LLM&#39046;&#22495;&#20013;&#24212;&#29992;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;&#39069;&#22806;&#30340;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#37096;&#20998;&#21487;&#20197;&#34987;&#26367;&#20195;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning Large Language Models (LLMs) adapts a trained model to specific downstream tasks, significantly improving task-specific performance. Supervised Fine-Tuning (SFT) is a common approach, where an LLM is trained to produce desired answers. However, LLMs trained with SFT sometimes make simple mistakes and result in hallucinations on reasoning tasks such as question-answering. Without external feedback, it is difficult for SFT to learn a good mapping between the question and the desired answer, especially with a small dataset. This paper introduces an alternative to SFT called Natural Language Feedback for Finetuning LLMs (LaFFi). LaFFi has LLMs directly predict the feedback they will receive from an annotator. We find that requiring such reflection can significantly improve the accuracy in in-domain question-answering tasks, providing a promising direction for the application of natural language feedback in the realm of SFT LLMs. Additional ablation studies show that the portion
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38271;&#19978;&#19979;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#19978;&#19979;&#25991;&#21033;&#29992;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23558;&#30456;&#20851;&#25991;&#26723;&#32435;&#20837;&#35757;&#32451;&#31034;&#20363;&#20013;&#26469;&#25913;&#36827;&#27169;&#22411;&#30340;&#22256;&#24785;&#24230;&#12290;&#36890;&#36807;&#24341;&#20837;Structured Packing for Long Context (SPLiCe)&#26041;&#27861;&#65292;&#20351;&#29992;&#26816;&#32034;&#26041;&#27861;&#23558;&#26368;&#20114;&#30456;&#20851;&#25991;&#26723;&#27719;&#38598;&#21040;&#21333;&#20010;&#35757;&#32451;&#19978;&#19979;&#25991;&#20013;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.17296</link><description>&lt;p&gt;
LLM&#35757;&#32451;&#20013;&#30340;&#32467;&#26500;&#21270;&#22635;&#20805;&#25913;&#36827;&#20102;&#38271;&#19978;&#19979;&#25991;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
Structured Packing in LLM Training Improves Long Context Utilization. (arXiv:2312.17296v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.17296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38271;&#19978;&#19979;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#19978;&#19979;&#25991;&#21033;&#29992;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23558;&#30456;&#20851;&#25991;&#26723;&#32435;&#20837;&#35757;&#32451;&#31034;&#20363;&#20013;&#26469;&#25913;&#36827;&#27169;&#22411;&#30340;&#22256;&#24785;&#24230;&#12290;&#36890;&#36807;&#24341;&#20837;Structured Packing for Long Context (SPLiCe)&#26041;&#27861;&#65292;&#20351;&#29992;&#26816;&#32034;&#26041;&#27861;&#23558;&#26368;&#20114;&#30456;&#20851;&#25991;&#26723;&#27719;&#38598;&#21040;&#21333;&#20010;&#35757;&#32451;&#19978;&#19979;&#25991;&#20013;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#19978;&#19979;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LCLM&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#29305;&#21035;&#26159;&#22312;&#26597;&#35810;&#31185;&#23398;&#30740;&#31350;&#35770;&#25991;&#31561;&#24212;&#29992;&#20013;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#28508;&#21147;&#24448;&#24448;&#21463;&#21040;&#19978;&#19979;&#25991;&#21033;&#29992;&#19981;&#36275;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#30830;&#23450;&#20856;&#22411;&#35757;&#32451;&#25968;&#25454;&#20013;&#32570;&#20047;&#38271;&#31243;&#35821;&#20041;&#20381;&#36182;&#26159;&#20027;&#35201;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#39057;&#32321;&#23558;&#30456;&#20851;&#25991;&#26723;&#32435;&#20837;&#35757;&#32451;&#36755;&#20837;&#30340;&#22909;&#22788;&#12290;&#21033;&#29992;&#20195;&#30721;&#25968;&#25454;&#30340;&#22266;&#26377;&#30446;&#24405;&#32467;&#26500;&#20316;&#20026;&#35757;&#32451;&#31034;&#20363;&#30340;&#26469;&#28304;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21363;&#20351;&#23545;&#20110;&#19982;&#32534;&#30721;&#26080;&#20851;&#30340;&#20219;&#21153;&#65292;&#22218;&#25324;&#30456;&#20851;&#25991;&#26723;&#33021;&#22815;&#25913;&#36827;&#27169;&#22411;&#30340;&#22256;&#24785;&#24230;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#24182;&#19988;&#26356;&#20855;&#24191;&#27867;&#30340;&#20851;&#27880;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;Structured Packing for Long Context (SPLiCe)&#30340;&#21019;&#26032;&#26041;&#27861;&#12290; SPLiCe&#26159;&#19968;&#31181;&#20351;&#29992;&#26816;&#32034;&#26041;&#27861;&#23558;&#26368;&#20114;&#30456;&#20851;&#25991;&#26723;&#27719;&#38598;&#21040;&#21333;&#20010;&#35757;&#32451;&#19978;&#19979;&#25991;&#20013;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;\method{}&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#21487;&#29992;&#20110;t
&lt;/p&gt;
&lt;p&gt;
Recent advances in long-context Large Language Models (LCLMs) have generated significant interest, especially in applications such as querying scientific research papers. However, their potential is often limited by inadequate context utilization. We identify the absence of long-range semantic dependencies in typical training data as a primary hindrance. To address this, we delve into the benefits of frequently incorporating related documents into training inputs. Using the inherent directory structure of code data as a source of training examples, we demonstrate improvements in perplexity, even for tasks unrelated to coding. Building on these findings, but with a broader focus, we introduce Structured Packing for Long Context (SPLiCe). SPLiCe is an innovative method for creating training examples by using a retrieval method to collate the most mutually relevant documents into a single training context. Our results indicate that \method{} enhances model performance and can be used to t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#38543;&#26426;&#20998;&#26512;&#33521;&#22269;&#22320;&#21517;&#19982;&#20854;&#20182;&#22269;&#23478;&#22320;&#21517;&#30340;&#30456;&#20284;&#24615;&#65292;&#30830;&#23450;&#35299;&#37322;&#22320;&#21517;&#25152;&#20351;&#29992;&#30340;&#21487;&#33021;&#35821;&#35328;&#12290;</title><link>http://arxiv.org/abs/2312.12850</link><description>&lt;p&gt;
&#33521;&#35821;&#22320;&#21517;&#30340;&#35821;&#35328;&#28170;&#28304;&#30340;&#38543;&#26426;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Stochastic Analysis of the Linguistic Provenance of English Place Names. (arXiv:2312.12850v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.12850
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#38543;&#26426;&#20998;&#26512;&#33521;&#22269;&#22320;&#21517;&#19982;&#20854;&#20182;&#22269;&#23478;&#22320;&#21517;&#30340;&#30456;&#20284;&#24615;&#65292;&#30830;&#23450;&#35299;&#37322;&#22320;&#21517;&#25152;&#20351;&#29992;&#30340;&#21487;&#33021;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33521;&#35821;&#22320;&#21517;&#20998;&#26512;&#20013;&#65292;&#36890;&#24120;&#26159;&#36890;&#36807;&#22320;&#21517;&#30340;&#35789;&#26681;&#19982;&#22320;&#35980;&#29305;&#24449;&#12289;&#19987;&#26377;&#21517;&#35789;&#21644;/&#25110;&#24433;&#21709;&#33521;&#35821;&#22320;&#21517;&#30340;&#35821;&#35328;&#20013;&#30340;&#23621;&#20303;&#22320;&#35789;&#27719;&#30456;&#20284;&#26469;&#30830;&#23450;&#20854;&#21547;&#20041;&#12290;&#38382;&#39064;&#22312;&#20110;&#26377;&#26102;&#38590;&#20197;&#30830;&#23450;&#29992;&#20110;&#35299;&#37322;&#35789;&#26681;&#30340;&#22522;&#30784;&#35821;&#35328;&#12290;&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#38543;&#26426;&#30830;&#23450;18799&#20010;&#33521;&#22269;&#22320;&#21517;&#19982;&#29233;&#23572;&#20848;&#12289;&#33487;&#26684;&#20848;&#12289;&#23041;&#23572;&#22763;&#12289;&#20025;&#40614;&#12289;&#25386;&#23041;&#12289;&#29790;&#20856;&#12289;&#27861;&#22269;&#12289;&#24503;&#22269;&#12289;&#33655;&#20848;&#21644;&#21476;&#32599;&#39532;&#30340;84687&#20010;&#22320;&#21517;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#26681;&#25454;&#33521;&#22269;&#22320;&#21517;&#19982;&#20854;&#20182;&#22269;&#23478;&#22320;&#21517;&#30340;&#30456;&#20284;&#24615;&#31243;&#24230;&#65292;&#23545;&#27599;&#20010;&#33521;&#22269;&#22320;&#21517;&#36827;&#34892;&#25490;&#21517;&#65292;&#20174;&#32780;&#30830;&#23450;&#35299;&#37322;&#22320;&#21517;&#25152;&#20351;&#29992;&#30340;&#21487;&#33021;&#35821;&#35328;&#12290;&#36890;&#36807;&#25552;&#20379;&#30340;&#25490;&#21517;&#21487;&#20197;&#24471;&#20986;&#19968;&#20123;&#35266;&#23519;&#32467;&#26524;&#12290;&#29305;&#21035;&#22320;&#65292;&#21457;&#29616;`Harlington'&#26159;&#33521;&#22269;&#26679;&#26412;&#20013;&#26368;&#20855;&#20856;&#22411;&#33521;&#22269;&#22320;&#21517;&#65292;&#32780;`Anna'&#26159;...
&lt;/p&gt;
&lt;p&gt;
In English place name analysis, meanings are often derived from the resemblance of roots in place names to topographical features, proper names and/or habitation terms in one of the languages that have had an influence on English place names. The problem here is that it is sometimes difficult to determine the base language to use to interpret the roots. The purpose of this paper is to stochastically determine the resemblance between 18799 English place names and 84687 place names from Ireland, Scotland, Wales, Denmark, Norway, Sweden, France, Germany, the Netherlands and Ancient Rome. Each English place name is ranked according to the extent to which it resembles place names from the other countries, and this provides a basis for determining the likely language to use to interpret the place name. A number of observations can be made using the ranking provided. In particular, it is found that `Harlington' is the most archetypically English place name in the English sample, and `Anna' is
&lt;/p&gt;</description></item><item><title>StyleSinger&#26159;&#38024;&#23545;&#39046;&#22495;&#22806;&#28436;&#21809;&#22768;&#38899;&#21512;&#25104;&#30340;&#39118;&#26684;&#36716;&#31227;&#27169;&#22411;&#65292;&#36890;&#36807;&#27531;&#24046;&#39118;&#26684;&#36866;&#37197;&#22120;&#65288;RSA&#65289;&#25429;&#25417;&#22810;&#26679;&#30340;&#39118;&#26684;&#29305;&#24449;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#28436;&#21809;&#22768;&#38899;&#12290;</title><link>http://arxiv.org/abs/2312.10741</link><description>&lt;p&gt;
StyleSinger: &#38024;&#23545;&#39046;&#22495;&#22806;&#28436;&#21809;&#22768;&#38899;&#21512;&#25104;&#30340;&#39118;&#26684;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
StyleSinger: Style Transfer for Out-of-Domain Singing Voice Synthesis. (arXiv:2312.10741v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10741
&lt;/p&gt;
&lt;p&gt;
StyleSinger&#26159;&#38024;&#23545;&#39046;&#22495;&#22806;&#28436;&#21809;&#22768;&#38899;&#21512;&#25104;&#30340;&#39118;&#26684;&#36716;&#31227;&#27169;&#22411;&#65292;&#36890;&#36807;&#27531;&#24046;&#39118;&#26684;&#36866;&#37197;&#22120;&#65288;RSA&#65289;&#25429;&#25417;&#22810;&#26679;&#30340;&#39118;&#26684;&#29305;&#24449;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#28436;&#21809;&#22768;&#38899;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#39046;&#22495;&#22806;&#28436;&#21809;&#22768;&#38899;&#21512;&#25104;&#65288;SVS&#65289;&#30340;&#39118;&#26684;&#36716;&#31227;&#19987;&#27880;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#28436;&#21809;&#22768;&#38899;&#65292;&#35813;&#22768;&#38899;&#20855;&#26377;&#20174;&#21442;&#32771;&#28436;&#21809;&#22768;&#38899;&#26679;&#26412;&#20013;&#34893;&#29983;&#30340;&#26410;&#35265;&#39118;&#26684;&#65288;&#22914;&#38899;&#33394;&#12289;&#24773;&#24863;&#12289;&#21457;&#38899;&#21644;&#21457;&#38899;&#25216;&#24039;&#65289;&#12290;&#28982;&#32780;&#65292;&#27169;&#25311;&#28436;&#21809;&#22768;&#38899;&#39118;&#26684;&#30340;&#31934;&#32454;&#24046;&#24322;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#28436;&#21809;&#22768;&#38899;&#20855;&#26377;&#38750;&#24120;&#39640;&#30340;&#34920;&#29616;&#21147;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;SVS&#26041;&#27861;&#22312;&#39046;&#22495;&#22806;&#22330;&#26223;&#20013;&#21512;&#25104;&#30340;&#28436;&#21809;&#22768;&#38899;&#36136;&#37327;&#19979;&#38477;&#65292;&#22240;&#20026;&#23427;&#20204;&#22522;&#20110;&#35757;&#32451;&#38454;&#27573;&#21487;&#36776;&#21035;&#20986;&#30446;&#26631;&#22768;&#38899;&#23646;&#24615;&#30340;&#20551;&#35774;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;StyleSinger&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#39046;&#22495;&#22806;&#21442;&#32771;&#28436;&#21809;&#22768;&#38899;&#26679;&#26412;&#30340;&#38646;&#26679;&#24335;&#36716;&#31227;&#30340;&#28436;&#21809;&#22768;&#38899;&#21512;&#25104;&#27169;&#22411;&#12290;StyleSinger&#37319;&#29992;&#20102;&#20004;&#31181;&#20851;&#38190;&#26041;&#27861;&#20197;&#25552;&#39640;&#25928;&#26524;&#65306;1&#65289;&#27531;&#24046;&#39118;&#26684;&#36866;&#37197;&#22120;&#65288;RSA&#65289;&#65292;&#23427;&#20351;&#29992;&#27531;&#24046;&#37327;&#21270;&#27169;&#22359;&#26469;&#25429;&#25417;&#22810;&#26679;&#30340;&#39118;&#26684;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Style transfer for out-of-domain (OOD) singing voice synthesis (SVS) focuses on generating high-quality singing voices with unseen styles (such as timbre, emotion, pronunciation, and articulation skills) derived from reference singing voice samples. However, the endeavor to model the intricate nuances of singing voice styles is an arduous task, as singing voices possess a remarkable degree of expressiveness. Moreover, existing SVS methods encounter a decline in the quality of synthesized singing voices in OOD scenarios, as they rest upon the assumption that the target vocal attributes are discernible during the training phase. To overcome these challenges, we propose StyleSinger, the first singing voice synthesis model for zero-shot style transfer of out-of-domain reference singing voice samples. StyleSinger incorporates two critical approaches for enhanced effectiveness: 1) the Residual Style Adaptor (RSA) which employs a residual quantization module to capture diverse style character
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#30340;&#21487;&#23398;&#20064;&#24615;&#65292;&#25552;&#20986;&#20102;&#27700;&#21360;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#23398;&#29983;&#27169;&#22411;&#20351;&#20854;&#27169;&#20223;&#20351;&#29992;&#35299;&#30721;&#27700;&#21360;&#30340;&#25945;&#24072;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#30452;&#25509;&#23398;&#20064;&#29983;&#25104;&#27700;&#21360;&#30340;&#33021;&#21147;&#65292;&#36825;&#23545;&#20110;&#27700;&#21360;&#30340;&#23454;&#38469;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2312.04469</link><description>&lt;p&gt;
&#35770;&#35821;&#35328;&#27169;&#22411;&#30340;&#27700;&#21360;&#21487;&#23398;&#20064;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Learnability of Watermarks for Language Models. (arXiv:2312.04469v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.04469
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#30340;&#21487;&#23398;&#20064;&#24615;&#65292;&#25552;&#20986;&#20102;&#27700;&#21360;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#23398;&#29983;&#27169;&#22411;&#20351;&#20854;&#27169;&#20223;&#20351;&#29992;&#35299;&#30721;&#27700;&#21360;&#30340;&#25945;&#24072;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#30452;&#25509;&#23398;&#20064;&#29983;&#25104;&#27700;&#21360;&#30340;&#33021;&#21147;&#65292;&#36825;&#23545;&#20110;&#27700;&#21360;&#30340;&#23454;&#38469;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#30340;&#27700;&#21360;&#21487;&#20197;&#23454;&#29616;&#23545;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#30340;&#32479;&#35745;&#26816;&#27979;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#36127;&#36131;&#20219;&#37096;&#32626;&#20013;&#12290;&#29616;&#26377;&#30340;&#27700;&#21360;&#31574;&#30053;&#36890;&#36807;&#25913;&#21464;&#29616;&#26377;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#30721;&#22120;&#26469;&#25805;&#20316;&#65292;&#32780;&#35821;&#35328;&#27169;&#22411;&#30452;&#25509;&#23398;&#20064;&#29983;&#25104;&#27700;&#21360;&#30340;&#33021;&#21147;&#23558;&#23545;&#27700;&#21360;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;&#39318;&#20808;&#65292;&#23398;&#20064;&#24471;&#21040;&#30340;&#27700;&#21360;&#21487;&#20197;&#29992;&#20110;&#26500;&#24314;&#33021;&#33258;&#28982;&#29983;&#25104;&#24102;&#27700;&#21360;&#25991;&#26412;&#30340;&#24320;&#25918;&#27169;&#22411;&#65292;&#20351;&#24320;&#25918;&#27169;&#22411;&#20063;&#33021;&#20174;&#27700;&#21360;&#20013;&#21463;&#30410;&#12290;&#20854;&#27425;&#65292;&#22914;&#26524;&#27700;&#21360;&#29992;&#20110;&#30830;&#23450;&#29983;&#25104;&#25991;&#26412;&#30340;&#26469;&#28304;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#20266;&#36896;&#27700;&#21360;&#24182;&#29983;&#25104;&#26377;&#23475;&#30340;&#24102;&#27700;&#21360;&#25991;&#26412;&#26469;&#25439;&#23475;&#21463;&#23475;&#27169;&#22411;&#30340;&#22768;&#35465;&#12290;&#20026;&#20102;&#30740;&#31350;&#27700;&#21360;&#30340;&#21487;&#23398;&#20064;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27700;&#21360;&#33976;&#39311;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#23398;&#29983;&#27169;&#22411;&#20351;&#20854;&#34892;&#20026;&#31867;&#20284;&#20110;&#20351;&#29992;&#22522;&#20110;&#35299;&#30721;&#30340;&#27700;&#21360;&#30340;&#25945;&#24072;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#23454;&#39564;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Watermarking of language model outputs enables statistical detection of model-generated text, which has many applications in the responsible deployment of language models. Existing watermarking strategies operate by altering the decoder of an existing language model, and the ability for a language model to directly learn to generate the watermark would have significant implications for the real-world deployment of watermarks. First, learned watermarks could be used to build open models that naturally generate watermarked text, allowing for open models to benefit from watermarking. Second, if watermarking is used to determine the provenance of generated text, an adversary can hurt the reputation of a victim model by spoofing its watermark and generating damaging watermarked text. To investigate the learnability of watermarks, we propose watermark distillation, which trains a student model to behave like a teacher model that uses decoding-based watermarking. We test our approach on three
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20851;&#27880;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#36890;&#36807;&#23450;&#21046;&#25552;&#31034;&#26469;&#35843;&#25972;&#38745;&#24577;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#65292;&#30740;&#31350;&#20102;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#20219;&#21153;&#20013;&#24615;&#33021;&#21644;&#26657;&#20934;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#30740;&#31350;&#21457;&#29616;&#38543;&#30528;ICL&#31034;&#20363;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#27169;&#22411;&#30340;&#26657;&#20934;&#20250;&#20808;&#22686;&#21152;&#32780;&#21518;&#24471;&#21040;&#25913;&#21892;&#65292;&#32780;&#26657;&#20934;&#35823;&#24046;&#20027;&#35201;&#20986;&#29616;&#22312;&#20302;&#26679;&#26412;&#22330;&#26223;&#19979;&#12290;&#27492;&#22806;&#65292;&#24494;&#35843;&#21644;CoT&#25552;&#31034;&#31561;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#26657;&#20934;&#35823;&#24046;&#21644;&#19981;&#21487;&#38752;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#65292;&#25552;&#31034;&#38656;&#35201;&#38024;&#23545;&#21487;&#38752;&#24615;&#22330;&#26223;&#24320;&#21457;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2312.04021</link><description>&lt;p&gt;
&#20851;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26657;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study on the Calibration of In-context Learning. (arXiv:2312.04021v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.04021
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#36890;&#36807;&#23450;&#21046;&#25552;&#31034;&#26469;&#35843;&#25972;&#38745;&#24577;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#65292;&#30740;&#31350;&#20102;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#20219;&#21153;&#20013;&#24615;&#33021;&#21644;&#26657;&#20934;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#30740;&#31350;&#21457;&#29616;&#38543;&#30528;ICL&#31034;&#20363;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#27169;&#22411;&#30340;&#26657;&#20934;&#20250;&#20808;&#22686;&#21152;&#32780;&#21518;&#24471;&#21040;&#25913;&#21892;&#65292;&#32780;&#26657;&#20934;&#35823;&#24046;&#20027;&#35201;&#20986;&#29616;&#22312;&#20302;&#26679;&#26412;&#22330;&#26223;&#19979;&#12290;&#27492;&#22806;&#65292;&#24494;&#35843;&#21644;CoT&#25552;&#31034;&#31561;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#26657;&#20934;&#35823;&#24046;&#21644;&#19981;&#21487;&#38752;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#65292;&#25552;&#31034;&#38656;&#35201;&#38024;&#23545;&#21487;&#38752;&#24615;&#22330;&#26223;&#24320;&#21457;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#23545;&#20110;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#23433;&#20840;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#29616;&#20195;LMs&#26657;&#20934;&#24615;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#19968;&#31181;&#36890;&#36807;&#23450;&#21046;&#25552;&#31034;&#26469;&#35843;&#25972;&#38745;&#24577;LMs&#30340;&#24120;&#35265;&#26041;&#27861;&#65292;&#24182;&#30740;&#31350;&#22312;&#24191;&#27867;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#20219;&#21153;&#20013;&#24615;&#33021;&#21644;&#26657;&#20934;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#38543;&#30528;ICL&#31034;&#20363;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#27169;&#22411;&#26368;&#21021;&#20250;&#20986;&#29616;&#22686;&#21152;&#30340;&#26657;&#20934;&#35823;&#24046;&#65292;&#28982;&#21518;&#25165;&#33021;&#23454;&#29616;&#26356;&#22909;&#30340;&#26657;&#20934;&#65292;&#32780;&#26657;&#20934;&#35823;&#24046;&#24448;&#24448;&#22312;&#20302;&#26679;&#26412;&#22330;&#26223;&#19979;&#20986;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20197;&#25552;&#39640;&#21487;&#29992;&#24615;&#20026;&#30446;&#26631;&#30340;&#26041;&#27861;&#65292;&#22914;&#24494;&#35843;&#21644;CoT&#25552;&#31034;&#65292;&#21487;&#33021;&#23548;&#33268;&#26657;&#20934;&#35823;&#24046;&#21644;&#19981;&#21487;&#38752;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#65292;&#36825;&#34920;&#26126;&#22312;&#26399;&#26395;&#27169;&#22411;&#21487;&#38752;&#24615;&#30340;&#22330;&#26223;&#20013;&#21487;&#33021;&#38656;&#35201;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate uncertainty quantification is crucial for the safe deployment of language models (LMs), and prior research has demonstrated improvements in the calibration of modern LMs. Our study focuses on in-context learning (ICL), a prevalent method for adapting static LMs through tailored prompts, and examines the balance between performance and calibration across a broad spectrum of natural language understanding and reasoning tasks. Through comprehensive experiments, we observe that, with an increasing number of ICL examples, models initially exhibit increased miscalibration before achieving better calibration and miscalibration tends to arise in low-shot settings. Moreover, we find that methods aimed at improving usability, such as fine-tuning and chain-of-thought (CoT) prompting, can lead to miscalibration and unreliable natural language explanations, suggesting that new methods may be required for scenarios where models are expected to be reliable.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#32508;&#21512;&#23450;&#37327;&#21644;&#23450;&#24615;&#20998;&#26512;&#30340;&#23454;&#26102;&#22312;&#32447;&#32929;&#31080;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#21253;&#21547;&#20102;&#26469;&#33258;&#21508;&#31181;&#26469;&#28304;&#30340;&#25968;&#25454;&#38598;&#65292;&#23558;&#25968;&#23383;&#32929;&#31080;&#25968;&#25454;&#21644;&#23450;&#24615;&#25991;&#26412;&#25968;&#25454;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#20998;&#26512;&#12290;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#22810;&#20010;&#20844;&#21496;&#21644;&#36947;&#29756;&#26031;&#24037;&#19994;&#24179;&#22343;&#25351;&#25968;&#30340;&#25968;&#25454;&#65292;&#20026;&#35757;&#32451;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#25968;&#25454;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2311.15218</link><description>&lt;p&gt;
&#23454;&#26102;&#22312;&#32447;&#32929;&#31080;&#39044;&#27979;&#21033;&#29992;&#32508;&#21512;&#23450;&#37327;&#21644;&#23450;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Real-Time Online Stock Forecasting Utilizing Integrated Quantitative and Qualitative Analysis. (arXiv:2311.15218v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.15218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#32508;&#21512;&#23450;&#37327;&#21644;&#23450;&#24615;&#20998;&#26512;&#30340;&#23454;&#26102;&#22312;&#32447;&#32929;&#31080;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#21253;&#21547;&#20102;&#26469;&#33258;&#21508;&#31181;&#26469;&#28304;&#30340;&#25968;&#25454;&#38598;&#65292;&#23558;&#25968;&#23383;&#32929;&#31080;&#25968;&#25454;&#21644;&#23450;&#24615;&#25991;&#26412;&#25968;&#25454;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#20998;&#26512;&#12290;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#22810;&#20010;&#20844;&#21496;&#21644;&#36947;&#29756;&#26031;&#24037;&#19994;&#24179;&#22343;&#25351;&#25968;&#30340;&#25968;&#25454;&#65292;&#20026;&#35757;&#32451;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#25968;&#25454;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#37329;&#34701;&#39046;&#22495;&#30340;&#24212;&#29992;&#24050;&#32463;&#21464;&#24471;&#38750;&#24120;&#24120;&#35265;&#65292;&#23588;&#20854;&#22312;&#32929;&#31080;&#24066;&#22330;&#39044;&#27979;&#20013;&#26356;&#26159;&#22914;&#27492;&#12290;&#32929;&#31080;&#24066;&#22330;&#39640;&#24230;&#27874;&#21160;&#65292;&#20840;&#29699;&#27599;&#20998;&#38047;&#37117;&#20250;&#20135;&#29983;&#22823;&#37327;&#25968;&#25454;&#12290;&#20174;&#36825;&#20123;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#25928;&#30340;&#26234;&#33021;&#20449;&#24687;&#21313;&#20998;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23558;&#25968;&#23383;&#32929;&#31080;&#25968;&#25454;&#19982;&#23450;&#24615;&#25991;&#26412;&#25968;&#25454;&#32467;&#21512;&#36215;&#26469;&#21487;&#33021;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#21490;&#26080;&#21069;&#20363;&#30340;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#65292;&#20174;&#26032;&#38395;&#26723;&#26696;&#12289;&#30005;&#35270;&#26032;&#38395;&#23383;&#24149;&#12289;&#24191;&#25773;&#25991;&#26412;&#12289;&#25512;&#25991;&#12289;&#27599;&#26085;&#36130;&#32463;&#25253;&#32440;&#31561;&#22788;&#25910;&#38598;&#21040;&#20102;&#21253;&#25324;&#25216;&#26415;&#21644;&#22522;&#26412;&#25968;&#25454;&#20197;&#21450;&#24773;&#24863;&#25968;&#25454;&#12290;&#29992;&#20110;&#24773;&#24863;&#25552;&#21462;&#30340;&#25991;&#26412;&#25968;&#25454;&#24635;&#20849;&#36229;&#36807;140&#19975;&#26465;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20174;2018&#24180;1&#26376;&#21040;2022&#24180;12&#26376;&#20026;&#26399;&#19968;&#24180;&#30340;&#20843;&#23478;&#20195;&#34920;&#19981;&#21516;&#20135;&#19994;&#37096;&#38376;&#30340;&#20844;&#21496;&#30340;&#27599;&#26085;&#25968;&#25454;&#65292;&#20197;&#21450;&#36947;&#29756;&#26031;&#24037;&#19994;&#24179;&#22343;&#25351;&#25968;&#65288;DJIA&#65289;&#25972;&#20307;&#30340;&#25968;&#25454;&#12290;&#32508;&#21512;&#30340;&#22522;&#26412;&#25968;&#25454;&#21644;&#25216;&#26415;&#25968;&#25454;&#21487;&#30452;&#25509;&#29992;&#20110;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
The application of Machine learning to finance has become a familiar approach, even more so in stock market forecasting. The stock market is highly volatile, and huge amounts of data are generated every minute globally. The extraction of effective intelligence from this data is of critical importance. However, a collaboration of numerical stock data with qualitative text data can be a challenging task. In this work, we accomplish this by providing an unprecedented, publicly available dataset with technical and fundamental data and sentiment that we gathered from news archives, TV news captions, radio transcripts, tweets, daily financial newspapers, etc. The text data entries used for sentiment extraction total more than 1.4 Million. The dataset consists of daily entries from January 2018 to December 2022 for eight companies representing diverse industrial sectors and the Dow Jones Industrial Average (DJIA) as a whole. Holistic Fundamental and Technical data is provided training ready f
&lt;/p&gt;</description></item><item><title>Jina Embeddings 2&#26159;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#38271;&#31687;&#25991;&#26723;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;512&#20010;&#26631;&#35760;&#38480;&#21046;&#65292;&#25552;&#20379;&#20102;&#39640;&#36798;8192&#20010;&#26631;&#35760;&#30340;&#23481;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.19923</link><description>&lt;p&gt;
Jina Embeddings 2: &#38754;&#21521;&#38271;&#31687;&#25991;&#26723;&#30340;8192-Token&#36890;&#29992;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents. (arXiv:2310.19923v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19923
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings 2&#26159;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#38271;&#31687;&#25991;&#26723;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;512&#20010;&#26631;&#35760;&#38480;&#21046;&#65292;&#25552;&#20379;&#20102;&#39640;&#36798;8192&#20010;&#26631;&#35760;&#30340;&#23481;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#23558;&#21477;&#23376;&#36716;&#21270;&#20026;&#22266;&#23450;&#22823;&#23567;&#29305;&#24449;&#21521;&#37327;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#36825;&#20123;&#21521;&#37327;&#21253;&#21547;&#20102;&#35821;&#20041;&#20449;&#24687;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#20449;&#24687;&#26816;&#32034;&#12289;&#35821;&#20041;&#32858;&#31867;&#21644;&#25991;&#26412;&#37325;&#25490;&#24207;&#31561;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24320;&#28304;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#22522;&#20110;BERT&#31561;&#26550;&#26500;&#26500;&#24314;&#30340;&#27169;&#22411;&#65292;&#38590;&#20197;&#34920;&#31034;&#38271;&#31687;&#25991;&#26723;&#65292;&#24182;&#19988;&#24120;&#24120;&#20250;&#36827;&#34892;&#25130;&#26029;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#25361;&#25112;&#65292;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#23558;&#25991;&#26723;&#20998;&#21106;&#25104;&#26356;&#23567;&#30340;&#27573;&#33853;&#36827;&#34892;&#23884;&#20837;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31574;&#30053;&#20250;&#23548;&#33268;&#26356;&#22823;&#30340;&#21521;&#37327;&#38598;&#21512;&#65292;&#36827;&#32780;&#22686;&#21152;&#20869;&#23384;&#28040;&#32791;&#65292;&#24182;&#19988;&#22312;&#21521;&#37327;&#25628;&#32034;&#26102;&#20250;&#20986;&#29616;&#35745;&#31639;&#23494;&#38598;&#21644;&#24310;&#36831;&#21319;&#39640;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Jina Embeddings 2&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#21487;&#20197;&#23481;&#32435;&#39640;&#36798;8192&#20010;&#26631;&#35760;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#31361;&#30772;&#20256;&#32479;&#30340;512&#20010;&#26631;&#35760;&#38480;&#21046;&#65292;&#33021;&#22815;&#28789;&#27963;&#22788;&#29702;&#38271;&#31687;&#25991;&#26723;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text embedding models have emerged as powerful tools for transforming sentences into fixed-sized feature vectors that encapsulate semantic information. While these models are essential for tasks like information retrieval, semantic clustering, and text re-ranking, most existing open-source models, especially those built on architectures like BERT, struggle to represent lengthy documents and often resort to truncation. One common approach to mitigate this challenge involves splitting documents into smaller paragraphs for embedding. However, this strategy results in a much larger set of vectors, consequently leading to increased memory consumption and computationally intensive vector searches with elevated latency.  To address these challenges, we introduce Jina Embeddings 2, an open-source text embedding model capable of accommodating up to 8192 tokens. This model is designed to transcend the conventional 512-token limit and adeptly process long documents. Jina Embeddings 2 not only ach
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31216;&#20026;&#22823;&#22411;&#25628;&#32034;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#25152;&#26377;&#25628;&#32034;&#20219;&#21153;&#32479;&#19968;&#20026;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#65292;&#37325;&#26032;&#23450;&#20041;&#20102;&#20256;&#32479;&#30340;&#25628;&#32034;&#22534;&#26632;&#12290;&#36825;&#20010;&#26694;&#26550;&#21033;&#29992;&#20102;LLM&#30340;&#24378;&#22823;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#26377;&#28508;&#21147;&#25552;&#39640;&#25628;&#32034;&#32467;&#26524;&#30340;&#36136;&#37327;&#65292;&#21516;&#26102;&#31616;&#21270;&#29616;&#26377;&#30340;&#32321;&#29712;&#30340;&#25628;&#32034;&#22534;&#26632;&#12290;</title><link>http://arxiv.org/abs/2310.14587</link><description>&lt;p&gt;
&#22823;&#22411;&#25628;&#32034;&#27169;&#22411;&#65306;&#37325;&#26032;&#23450;&#20041;LLM&#26102;&#20195;&#30340;&#25628;&#32034;&#22534;&#26632;
&lt;/p&gt;
&lt;p&gt;
Large Search Model: Redefining Search Stack in the Era of LLMs. (arXiv:2310.14587v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14587
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31216;&#20026;&#22823;&#22411;&#25628;&#32034;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#25152;&#26377;&#25628;&#32034;&#20219;&#21153;&#32479;&#19968;&#20026;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#65292;&#37325;&#26032;&#23450;&#20041;&#20102;&#20256;&#32479;&#30340;&#25628;&#32034;&#22534;&#26632;&#12290;&#36825;&#20010;&#26694;&#26550;&#21033;&#29992;&#20102;LLM&#30340;&#24378;&#22823;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#26377;&#28508;&#21147;&#25552;&#39640;&#25628;&#32034;&#32467;&#26524;&#30340;&#36136;&#37327;&#65292;&#21516;&#26102;&#31616;&#21270;&#29616;&#26377;&#30340;&#32321;&#29712;&#30340;&#25628;&#32034;&#22534;&#26632;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#25628;&#32034;&#24341;&#25806;&#26159;&#30001;&#19981;&#21516;&#32452;&#20214;&#26500;&#24314;&#30340;&#22534;&#26632;&#65292;&#21253;&#25324;&#26597;&#35810;&#29702;&#35299;&#12289;&#26816;&#32034;&#12289;&#22810;&#38454;&#27573;&#25490;&#21517;&#21644;&#38382;&#31572;&#31561;&#12290;&#36825;&#20123;&#32452;&#20214;&#36890;&#24120;&#26159;&#29420;&#31435;&#20248;&#21270;&#21644;&#37096;&#32626;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#27010;&#24565;&#24615;&#26694;&#26550;&#65292;&#31216;&#20026;&#22823;&#22411;&#25628;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#25152;&#26377;&#20219;&#21153;&#32479;&#19968;&#20026;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#26469;&#37325;&#26032;&#23450;&#20041;&#20256;&#32479;&#30340;&#25628;&#32034;&#22534;&#26632;&#12290;&#25152;&#26377;&#20219;&#21153;&#37117;&#34987;&#34920;&#36848;&#20026;&#33258;&#22238;&#24402;&#25991;&#26412;&#29983;&#25104;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#21487;&#20197;&#23450;&#21046;&#20219;&#21153;&#12290;&#36825;&#20010;&#25552;&#20986;&#30340;&#26694;&#26550;&#21033;&#29992;&#20102;LLM&#30340;&#24378;&#22823;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#26377;&#28508;&#21147;&#25552;&#39640;&#25628;&#32034;&#32467;&#26524;&#30340;&#36136;&#37327;&#65292;&#21516;&#26102;&#31616;&#21270;&#29616;&#26377;&#30340;&#32321;&#29712;&#30340;&#25628;&#32034;&#22534;&#26632;&#12290;&#20026;&#20102;&#39564;&#35777;&#36825;&#20010;&#26694;&#26550;&#30340;&#21487;&#34892;&#24615;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31995;&#21015;&#27010;&#24565;&#39564;&#35777;&#23454;&#39564;&#65292;&#24182;&#35752;&#35770;&#20102;&#23454;&#29616;&#36825;&#31181;&#26041;&#27861;&#25152;&#38754;&#20020;&#30340;&#28508;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern search engines are built on a stack of different components, including query understanding, retrieval, multi-stage ranking, and question answering, among others. These components are often optimized and deployed independently. In this paper, we introduce a novel conceptual framework called large search model, which redefines the conventional search stack by unifying search tasks with one large language model (LLM). All tasks are formulated as autoregressive text generation problems, allowing for the customization of tasks through the use of natural language prompts. This proposed framework capitalizes on the strong language understanding and reasoning capabilities of LLMs, offering the potential to enhance search result quality while simultaneously simplifying the existing cumbersome search stack. To substantiate the feasibility of this framework, we present a series of proof-of-concept experiments and discuss the potential challenges associated with implementing this approach w
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38169;&#35823;&#20998;&#26512;&#30340;&#23545;&#40784;&#31574;&#30053;&#65292;&#36890;&#36807;&#26292;&#38706;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38169;&#35823;&#36755;&#20986;&#24182;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#29702;&#35299;&#20869;&#37096;&#21407;&#22240;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#26377;&#27602;&#22238;&#24212;&#21487;&#20197;&#36716;&#21270;&#20026;&#27169;&#22411;&#23545;&#40784;&#30340;&#25351;&#23548;&#35843;&#35856;&#35821;&#26009;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#24182;&#35757;&#32451;&#20854;&#36827;&#34892;&#33258;&#25105;&#25209;&#35780;&#12290;</title><link>http://arxiv.org/abs/2310.10477</link><description>&lt;p&gt;
&#20174;&#25387;&#25240;&#20013;&#33719;&#24471;&#26234;&#24935;&#65306;&#36890;&#36807;&#38169;&#35823;&#20998;&#26512;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis. (arXiv:2310.10477v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10477
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38169;&#35823;&#20998;&#26512;&#30340;&#23545;&#40784;&#31574;&#30053;&#65292;&#36890;&#36807;&#26292;&#38706;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38169;&#35823;&#36755;&#20986;&#24182;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#29702;&#35299;&#20869;&#37096;&#21407;&#22240;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#26377;&#27602;&#22238;&#24212;&#21487;&#20197;&#36716;&#21270;&#20026;&#27169;&#22411;&#23545;&#40784;&#30340;&#25351;&#23548;&#35843;&#35856;&#35821;&#26009;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#24182;&#35757;&#32451;&#20854;&#36827;&#34892;&#33258;&#25105;&#25209;&#35780;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#26082;&#24102;&#26469;&#20102;&#26426;&#36935;&#65292;&#20063;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#24847;&#22806;&#29983;&#25104;&#26377;&#23475;&#21644;&#26377;&#27602;&#22238;&#24212;&#26041;&#38754;&#12290;&#20256;&#32479;&#30340;&#23545;&#40784;&#26041;&#27861;&#33268;&#21147;&#20110;&#24341;&#23548;LLMs&#26397;&#30528;&#26399;&#26395;&#30340;&#24615;&#33021;&#21457;&#23637;&#24182;&#20445;&#25252;&#23427;&#20204;&#20813;&#21463;&#24694;&#24847;&#20869;&#23481;&#30340;&#20405;&#23475;&#65292;&#32780;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38169;&#35823;&#20998;&#26512;&#30340;&#20840;&#26032;&#23545;&#40784;&#31574;&#30053;&#65292;&#36890;&#36807;&#26377;&#24847;&#26292;&#38706;LLMs&#30340;&#32570;&#38519;&#36755;&#20986;&#24182;&#36827;&#34892;&#28145;&#20837;&#35780;&#20272;&#65292;&#20197;&#23436;&#20840;&#29702;&#35299;&#20869;&#37096;&#21407;&#22240;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#20998;&#26512;&#12290;&#22240;&#27492;&#65292;&#26377;&#27602;&#22238;&#24212;&#21487;&#20197;&#36716;&#21270;&#20026;&#27169;&#22411;&#23545;&#40784;&#30340;&#25351;&#23548;&#35843;&#35856;&#35821;&#26009;&#65292;LLMs&#19981;&#20165;&#21487;&#20197;&#36991;&#20813;&#29983;&#25104;&#26377;&#32570;&#38519;&#30340;&#22238;&#24212;&#65292;&#36824;&#21487;&#20197;&#35757;&#32451;&#20854;&#36827;&#34892;&#33258;&#25105;&#25209;&#35780;&#65292;&#21457;&#25381;&#20854;&#36776;&#21035;&#26377;&#27602;&#20869;&#23481;&#30340;&#20869;&#22312;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#23433;&#20840;&#25351;&#20196;&#36981;&#24490;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;&#23545;&#40784;&#25216;&#26415;&#65292;&#21516;&#26102;&#36824;&#20445;&#25345;&#20102;&#21331;&#36234;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancement of large language models (LLMs) presents both opportunities and challenges, particularly concerning unintentional generation of harmful and toxic responses. While the traditional alignment methods strive to steer LLMs towards desired performance and shield them from malicious content, this study proposes a novel alignment strategy rooted in mistake analysis by exposing LLMs to flawed outputs purposefully and then conducting a thorough assessment to fully comprehend internal reasons via natural language analysis. Thus, toxic responses can be transformed into instruction tuning corpus for model alignment, and LLMs can not only be deterred from generating flawed responses but also trained to self-criticize, leveraging its innate ability to discriminate toxic content. Experimental results demonstrate that the proposed method outperforms conventional alignment techniques for safety instruction following, while maintaining superior efficiency.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Reward-Augmented Decoding (RAD)&#30340;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#29992;&#23567;&#22411;&#30340;&#21333;&#21521;&#22870;&#21169;&#27169;&#22411;&#26469;&#40723;&#21169;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#23646;&#24615;&#30340;&#25991;&#26412;&#12290;RAD&#22312;&#29983;&#25104;&#38750;&#26377;&#23475;&#21644;&#24773;&#24863;&#21463;&#25511;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#65292;&#24182;&#19988;&#22312;&#38750;&#24120;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#19978;&#20063;&#24456;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2310.09520</link><description>&lt;p&gt;
Reward-Augmented Decoding: &#20351;&#29992;&#21333;&#21521;&#22870;&#21169;&#27169;&#22411;&#23454;&#29616;&#39640;&#25928;&#30340;&#21463;&#25511;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Reward-Augmented Decoding: Efficient Controlled Text Generation With a Unidirectional Reward Model. (arXiv:2310.09520v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09520
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Reward-Augmented Decoding (RAD)&#30340;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#29992;&#23567;&#22411;&#30340;&#21333;&#21521;&#22870;&#21169;&#27169;&#22411;&#26469;&#40723;&#21169;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#23646;&#24615;&#30340;&#25991;&#26412;&#12290;RAD&#22312;&#29983;&#25104;&#38750;&#26377;&#23475;&#21644;&#24773;&#24863;&#21463;&#25511;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#65292;&#24182;&#19988;&#22312;&#38750;&#24120;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#19978;&#20063;&#24456;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#29983;&#25104;&#30340;&#25991;&#26412;&#23384;&#22312;&#38382;&#39064;&#25110;&#32773;&#32570;&#20047;&#25152;&#38656;&#30340;&#23646;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Reward-Augmented Decoding (RAD)&#30340;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#19968;&#20010;&#23567;&#22411;&#30340;&#21333;&#21521;&#22870;&#21169;&#27169;&#22411;&#26469;&#40723;&#21169;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#23646;&#24615;&#30340;&#25991;&#26412;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;RAD&#21033;&#29992;&#22870;&#21169;&#27169;&#22411;&#23545;&#29983;&#25104;&#30340;&#25991;&#26412;&#36827;&#34892;&#35780;&#20998;&#65292;&#24182;&#36890;&#36807;&#37325;&#26032;&#35843;&#25972;&#37319;&#26679;&#27010;&#29575;&#26469;&#26356;&#20542;&#21521;&#20110;&#39640;&#22870;&#21169;&#30340;&#26631;&#35760;&#12290;&#36890;&#36807;&#20351;&#29992;&#21333;&#21521;&#22870;&#21169;&#27169;&#22411;&#65292;RAD&#33021;&#22815;&#32531;&#23384;&#20808;&#21069;&#29983;&#25104;&#27493;&#39588;&#30340;&#28608;&#27963;&#20540;&#65292;&#38477;&#20302;&#35745;&#31639;&#24320;&#38144;&#12290;&#36890;&#36807;&#22312;&#29983;&#25104;&#38750;&#26377;&#23475;&#21644;&#24773;&#24863;&#21463;&#25511;&#25991;&#26412;&#26041;&#38754;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;RAD&#22312;&#20165;&#25913;&#21464;&#29983;&#25104;&#36807;&#31243;&#30340;&#26041;&#27861;&#20013;&#34920;&#29616;&#26368;&#20339;&#65292;&#24182;&#19988;&#19982;&#28041;&#21450;&#37325;&#26032;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;RAD&#22312;&#38750;&#24120;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models have proven effective in a huge range of downstream applications, they often generate text that is problematic or lacks a desired attribute. In this paper, we introduce Reward-Augmented Decoding (RAD), a text generation procedure that uses a small unidirectional reward model to encourage a language model to generate text that has certain properties. Specifically, RAD uses the reward model to score generations as they are produced and rescales sampling probabilities to favor high-reward tokens. By using a unidirectional reward model, RAD can cache activations from prior generation steps to decrease computational overhead. Through experiments on generating non-toxic and sentiment-controlled text, we demonstrate that RAD performs best among methods that change only the generation procedure and matches the performance of state-of-the-art methods that involve re-training the language model. We further validate that RAD is effective on very large language models w
&lt;/p&gt;</description></item><item><title>&#21073;&#26725;&#27861;&#24459;&#35821;&#26009;&#24211;&#26159;&#19968;&#20010;&#29992;&#20110;&#27861;&#24459;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;&#26469;&#33258;&#33521;&#22269;&#30340;&#36229;&#36807;250,000&#20010;&#27861;&#24237;&#26696;&#20363;&#12290;&#22312;&#35813;&#35821;&#26009;&#24211;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26696;&#20363;&#32467;&#26524;&#30340;&#19987;&#23478;&#27880;&#35299;&#65292;&#24182;&#20351;&#29992;&#22810;&#20010;&#27169;&#22411;&#36827;&#34892;&#20102;&#26696;&#20363;&#32467;&#26524;&#25552;&#21462;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#20026;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2309.12269</link><description>&lt;p&gt;
&#21073;&#26725;&#27861;&#24459;&#35821;&#26009;&#24211;&#65306;&#29992;&#20110;&#27861;&#24459;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
The Cambridge Law Corpus: A Corpus for Legal AI Research. (arXiv:2309.12269v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12269
&lt;/p&gt;
&lt;p&gt;
&#21073;&#26725;&#27861;&#24459;&#35821;&#26009;&#24211;&#26159;&#19968;&#20010;&#29992;&#20110;&#27861;&#24459;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;&#26469;&#33258;&#33521;&#22269;&#30340;&#36229;&#36807;250,000&#20010;&#27861;&#24237;&#26696;&#20363;&#12290;&#22312;&#35813;&#35821;&#26009;&#24211;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26696;&#20363;&#32467;&#26524;&#30340;&#19987;&#23478;&#27880;&#35299;&#65292;&#24182;&#20351;&#29992;&#22810;&#20010;&#27169;&#22411;&#36827;&#34892;&#20102;&#26696;&#20363;&#32467;&#26524;&#25552;&#21462;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#20026;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#21073;&#26725;&#27861;&#24459;&#35821;&#26009;&#24211;&#65288;CLC&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#27861;&#24459;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#35821;&#26009;&#24211;&#12290;&#23427;&#21253;&#21547;&#20102;&#26469;&#33258;&#33521;&#22269;&#30340;&#36229;&#36807;250,000&#20010;&#27861;&#24237;&#26696;&#20363;&#12290;&#22823;&#37096;&#20998;&#26696;&#20363;&#26469;&#33258;21&#19990;&#32426;&#65292;&#20294;&#35813;&#35821;&#26009;&#24211;&#21253;&#25324;&#20102;16&#19990;&#32426;&#20197;&#26469;&#30340;&#26696;&#20363;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#35813;&#35821;&#26009;&#24211;&#30340;&#39318;&#27425;&#21457;&#24067;&#65292;&#21253;&#25324;&#21407;&#22987;&#25991;&#26412;&#21644;&#20803;&#25968;&#25454;&#12290;&#22312;&#35821;&#26009;&#24211;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;638&#20010;&#26696;&#20363;&#30340;&#27861;&#24459;&#19987;&#23478;&#23545;&#26696;&#20363;&#32467;&#26524;&#30340;&#27880;&#35299;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#26631;&#27880;&#25968;&#25454;&#65292;&#35757;&#32451;&#21644;&#35780;&#20272;&#20102;GPT-3&#12289;GPT-4&#21644;RoBERTa&#27169;&#22411;&#36827;&#34892;&#26696;&#20363;&#32467;&#26524;&#25552;&#21462;&#65292;&#20197;&#25552;&#20379;&#22522;&#20934;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27861;&#24459;&#21644;&#20262;&#29702;&#35752;&#35770;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#26448;&#26009;&#21487;&#33021;&#20855;&#26377;&#25935;&#24863;&#24615;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#35813;&#35821;&#26009;&#24211;&#21482;&#20250;&#22312;&#19968;&#23450;&#38480;&#21046;&#19979;&#29992;&#20110;&#30740;&#31350;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the Cambridge Law Corpus (CLC), a corpus for legal AI research. It consists of over 250 000 court cases from the UK. Most cases are from the 21st century, but the corpus includes cases as old as the 16th century. This paper presents the first release of the corpus, containing the raw text and meta-data. Together with the corpus, we provide annotations on case outcomes for 638 cases, done by legal experts. Using our annotated data, we have trained and evaluated case outcome extraction with GPT-3, GPT-4 and RoBERTa models to provide benchmarks. We include an extensive legal and ethical discussion to address the potentially sensitive nature of this material. As a consequence, the corpus will only be released for research purposes under certain restrictions.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;mLLMs&#65289;&#22312;&#39044;&#27979;&#35821;&#35328;&#22788;&#29702;&#36807;&#31243;&#20013;&#19982;&#20154;&#31867;&#30340;&#35270;&#35273;-&#35821;&#35328;&#38598;&#25104;&#33021;&#21147;&#26159;&#21542;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;mLLMs&#30340;&#22810;&#27169;&#24577;&#36755;&#20837;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#35748;&#30693;&#36127;&#33655;&#65292;&#25552;&#39640;&#24863;&#30693;&#21644;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.06035</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#35821;&#35328;&#22788;&#29702;&#26399;&#38388;&#34920;&#29616;&#20986;&#20154;&#31867;&#35270;&#35273;-&#35821;&#35328;&#38598;&#25104;&#30340;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Evidence of Human-Like Visual-Linguistic Integration in Multimodal Large Language Models During Predictive Language Processing. (arXiv:2308.06035v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06035
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;mLLMs&#65289;&#22312;&#39044;&#27979;&#35821;&#35328;&#22788;&#29702;&#36807;&#31243;&#20013;&#19982;&#20154;&#31867;&#30340;&#35270;&#35273;-&#35821;&#35328;&#38598;&#25104;&#33021;&#21147;&#26159;&#21542;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;mLLMs&#30340;&#22810;&#27169;&#24577;&#36755;&#20837;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#35748;&#30693;&#36127;&#33655;&#65292;&#25552;&#39640;&#24863;&#30693;&#21644;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20808;&#36827;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#24341;&#21457;&#20102;&#20851;&#20110;&#23427;&#20204;&#26159;&#21542;&#33021;&#22815;&#22797;&#21046;&#20154;&#31867;&#35748;&#30693;&#36807;&#31243;&#30340;&#20105;&#35758;&#12290;LLMs&#21644;&#20154;&#31867;&#22312;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#19968;&#20010;&#21306;&#21035;&#22312;&#20110;&#65292;&#35821;&#35328;&#36755;&#20837;&#36890;&#24120;&#24314;&#31435;&#22312;&#22810;&#20010;&#30693;&#35273;&#27169;&#24577;&#19978;&#65292;&#32780;&#22823;&#22810;&#25968;LLMs&#20165;&#22788;&#29702;&#22522;&#20110;&#25991;&#26412;&#30340;&#20449;&#24687;&#12290;&#22810;&#27169;&#24577;&#22522;&#30784;&#20351;&#20154;&#31867;&#33021;&#22815;&#25972;&#21512;&#35270;&#35273;&#32972;&#26223;&#19982;&#35821;&#35328;&#20449;&#24687;&#65292;&#20174;&#32780;&#23545;&#21363;&#23558;&#20986;&#29616;&#30340;&#21333;&#35789;&#30340;&#31354;&#38388;&#26045;&#21152;&#38480;&#21046;&#65292;&#20943;&#23569;&#35748;&#30693;&#36127;&#33655;&#65292;&#25552;&#39640;&#24863;&#30693;&#21644;&#29702;&#35299;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#22810;&#27169;&#24577;LLMs&#65288;mLLMs&#65289;&#32467;&#21512;&#20102;&#35270;&#35273;&#21644;&#35821;&#35328;&#23884;&#20837;&#31354;&#38388;&#65292;&#24182;&#20351;&#29992;&#21464;&#21387;&#22120;&#31867;&#22411;&#30340;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#19979;&#19968;&#20010;&#21333;&#35789;&#30340;&#39044;&#27979;&#12290;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#65292;&#22522;&#20110;&#22810;&#27169;&#24577;&#36755;&#20837;&#30340;&#39044;&#27979;&#35821;&#35328;&#22788;&#29702;&#22312;mLLMs&#21644;&#20154;&#31867;&#20013;&#21563;&#21512;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;200&#21517;&#34987;&#35797;&#35266;&#30475;&#20102;&#30701;&#30340;&#35270;&#21548;&#21098;&#36753;&#65292;&#24182;&#20272;&#35745;&#20102;&#21363;&#23558;&#20986;&#29616;&#30340;&#21160;&#35789;&#25110;&#21517;&#35789;&#30340;&#21487;&#39044;&#27979;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advanced language processing abilities of large language models (LLMs) have stimulated debate over their capacity to replicate human-like cognitive processes. One differentiating factor between language processing in LLMs and humans is that language input is often grounded in more than one perceptual modality, whereas most LLMs process solely text-based information. Multimodal grounding allows humans to integrate - e.g. visual context with linguistic information and thereby place constraints on the space of upcoming words, reducing cognitive load and improving perception and comprehension. Recent multimodal LLMs (mLLMs) combine visual and linguistic embedding spaces with a transformer type attention mechanism for next-word prediction. To what extent does predictive language processing based on multimodal input align in mLLMs and humans? To answer this question, 200 human participants watched short audio-visual clips and estimated the predictability of an upcoming verb or noun. The 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;RS5M&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#39046;&#22495;&#22522;&#30784;&#27169;&#22411;&#65288;DFM&#65289;&#65292;&#29992;&#20110;&#23454;&#29616;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#65288;GFM&#65289;&#21644;&#39046;&#22495;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#36716;&#25442;&#12290;&#21478;&#22806;&#65292;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#36965;&#24863;&#39046;&#22495;&#30340;&#22823;&#35268;&#27169;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;RS5M&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#36807;&#28388;&#20844;&#24320;&#21487;&#29992;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#20026;&#26631;&#31614;&#25968;&#25454;&#38598;&#29983;&#25104;&#26631;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.11300</link><description>&lt;p&gt;
RS5M&#65306;&#29992;&#20110;&#36965;&#24863;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
RS5M: A Large Scale Vision-Language Dataset for Remote Sensing Vision-Language Foundation Model. (arXiv:2306.11300v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;RS5M&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#39046;&#22495;&#22522;&#30784;&#27169;&#22411;&#65288;DFM&#65289;&#65292;&#29992;&#20110;&#23454;&#29616;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#65288;GFM&#65289;&#21644;&#39046;&#22495;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#36716;&#25442;&#12290;&#21478;&#22806;&#65292;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#36965;&#24863;&#39046;&#22495;&#30340;&#22823;&#35268;&#27169;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;RS5M&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#36807;&#28388;&#20844;&#24320;&#21487;&#29992;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#20026;&#26631;&#31614;&#25968;&#25454;&#38598;&#29983;&#25104;&#26631;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#37327;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#23637;&#31034;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#22270;&#20687;-&#25991;&#26412;&#20851;&#32852;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;&#20851;&#38190;&#25361;&#25112;&#26159;&#22914;&#20309;&#21033;&#29992;&#24050;&#26377;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65292;&#22312;&#22495;&#30456;&#20851;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#36827;&#34892;&#39046;&#22495;&#29305;&#23450;&#30340;&#36801;&#31227;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#39046;&#22495;&#22522;&#30784;&#27169;&#22411;&#65288;DFM&#65289;&#65292;&#24357;&#21512;&#20102;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#65288;GFM&#65289;&#21644;&#39046;&#22495;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#36965;&#24863;&#39046;&#22495;&#65288;RS&#65289;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;RS5M&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;500&#19975;&#24352;&#24102;&#26377;&#33521;&#25991;&#25551;&#36848;&#30340;RS&#22270;&#20687;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#36807;&#28388;&#20844;&#24320;&#21487;&#29992;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#20026;&#20165;&#24102;&#26631;&#31614;&#30340;RS&#25968;&#25454;&#38598;&#29983;&#25104;&#26631;&#39064;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;RS&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Vision-Language Foundation Models utilizing extensive image-text paired data have demonstrated unprecedented image-text association capabilities, achieving remarkable results across various downstream tasks. A critical challenge is how to make use of existing large-scale pre-trained VLMs, which are trained on common objects, to perform the domain-specific transfer for accomplishing domain-related downstream tasks. In this paper, we propose a new framework that includes the Domain Foundation Model (DFM), bridging the gap between the General Foundation Model (GFM) and domain-specific downstream tasks. Moreover, we present an image-text paired dataset in the field of remote sensing (RS), RS5M, which has 5 million RS images with English descriptions. The dataset is obtained from filtering publicly available image-text paired datasets and captioning label-only RS datasets with pre-trained VLM. These constitute the first large-scale RS image-text paired dataset. Additionally, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#35748;&#30693;&#27169;&#22411;&#65292;&#31216;&#20026;&#26377;&#38480;&#23454;&#29992;&#35828;&#35805;&#32773;&#65292;&#29992;&#20110;&#34920;&#24449;&#19981;&#21516;&#21464;&#20307;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25805;&#20316;&#26041;&#24335;&#12290;&#32463;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#27010;&#24565;&#19978;&#31867;&#20284;&#20110; &#24555;&#19982;&#24930;&#24605;&#32771;&#27169;&#22411;&#30340;&#24605;&#32500;&#27169;&#22411;&#65292;&#32780;&#36825;&#31181;&#24605;&#32500;&#27169;&#22411;&#34987;&#24402;&#22240;&#20110;&#20154;&#31867;&#12290;&#27492;&#30740;&#31350;&#20984;&#26174;&#20102;&#37319;&#29992;&#35748;&#30693;&#27010;&#29575;&#24314;&#27169;&#26041;&#27861;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#12289;&#35780;&#20272;&#21644;&#25512;&#36827;&#30340;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.17760</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#26377;&#38480;&#23454;&#29992;&#35828;&#35805;&#32773;
&lt;/p&gt;
&lt;p&gt;
Language Models are Bounded Pragmatic Speakers. (arXiv:2305.17760v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#35748;&#30693;&#27169;&#22411;&#65292;&#31216;&#20026;&#26377;&#38480;&#23454;&#29992;&#35828;&#35805;&#32773;&#65292;&#29992;&#20110;&#34920;&#24449;&#19981;&#21516;&#21464;&#20307;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25805;&#20316;&#26041;&#24335;&#12290;&#32463;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#27010;&#24565;&#19978;&#31867;&#20284;&#20110; &#24555;&#19982;&#24930;&#24605;&#32771;&#27169;&#22411;&#30340;&#24605;&#32500;&#27169;&#22411;&#65292;&#32780;&#36825;&#31181;&#24605;&#32500;&#27169;&#22411;&#34987;&#24402;&#22240;&#20110;&#20154;&#31867;&#12290;&#27492;&#30740;&#31350;&#20984;&#26174;&#20102;&#37319;&#29992;&#35748;&#30693;&#27010;&#29575;&#24314;&#27169;&#26041;&#27861;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#12289;&#35780;&#20272;&#21644;&#25512;&#36827;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#35748;&#30693;&#27169;&#22411;&#65292;&#31216;&#20026;&#26377;&#38480;&#23454;&#29992;&#35828;&#35805;&#32773;&#65292;&#29992;&#20110;&#34920;&#24449;&#19981;&#21516;&#21464;&#20307;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25805;&#20316;&#26041;&#24335;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32463;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Ouyang&#31561;&#20154;&#65292;2022&#65289;&#20855;&#26377;&#27010;&#24565;&#19978;&#31867;&#20284;&#20110; &#24555;&#19982;&#24930;&#24605;&#32771;&#27169;&#22411;&#65288;Kahneman&#65292;2011&#65289;&#30340;&#24605;&#32500;&#27169;&#22411;&#65292;&#32780;&#36825;&#31181;&#24605;&#32500;&#27169;&#22411;&#34987;&#24515;&#29702;&#23398;&#23478;&#20204;&#24402;&#22240;&#20110;&#20154;&#31867;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#20316;&#20026;&#24555;&#19982;&#24930;&#24605;&#32771;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#25193;&#23637;&#36825;&#20010;&#26694;&#26550;&#30340;&#36884;&#24452;&#12290;&#26412;&#30740;&#31350;&#23454;&#36136;&#19978;&#20984;&#26174;&#20102;&#37319;&#29992;&#35748;&#30693;&#27010;&#29575;&#24314;&#27169;&#26041;&#27861;&#26469;&#33719;&#24471;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#12289;&#35780;&#20272;&#21644;&#25512;&#36827;&#26041;&#38754;&#30340;&#28145;&#21051;&#35265;&#35299;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
How do language models "think"? This paper formulates a probabilistic cognitive model called the bounded pragmatic speaker, which can characterize the operation of different variations of language models. Specifically, we demonstrate that large language models fine-tuned with reinforcement learning from human feedback (Ouyang et al., 2022) embody a model of thought that conceptually resembles a fast-and-slow model (Kahneman, 2011), which psychologists have attributed to humans. We discuss the limitations of reinforcement learning from human feedback as a fast-and-slow model of thought and propose avenues for expanding this framework. In essence, our research highlights the value of adopting a cognitive probabilistic modeling approach to gain insights into the comprehension, evaluation, and advancement of language models.
&lt;/p&gt;</description></item><item><title>ArtGPT-4&#26159;&#19968;&#31181;&#22522;&#20110;&#36866;&#37197;&#22120;&#22686;&#24378;&#30340;MiniGPT-4&#27169;&#22411;&#65292;&#19987;&#27880;&#20110;&#35299;&#20915;&#22270;&#20687;&#29702;&#35299;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#30701;&#26102;&#38388;&#20869;&#35757;&#32451;&#20986;&#20855;&#22791;&#33391;&#22909;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.07490</link><description>&lt;p&gt;
ArtGPT-4: &#22522;&#20110;&#36866;&#37197;&#22120;&#22686;&#24378;&#30340;MiniGPT-4&#27169;&#22411;&#30340;&#33402;&#26415;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
ArtGPT-4: Artistic Vision-Language Understanding with Adapter-enhanced MiniGPT-4. (arXiv:2305.07490v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07490
&lt;/p&gt;
&lt;p&gt;
ArtGPT-4&#26159;&#19968;&#31181;&#22522;&#20110;&#36866;&#37197;&#22120;&#22686;&#24378;&#30340;MiniGPT-4&#27169;&#22411;&#65292;&#19987;&#27880;&#20110;&#35299;&#20915;&#22270;&#20687;&#29702;&#35299;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#30701;&#26102;&#38388;&#20869;&#35757;&#32451;&#20986;&#20855;&#22791;&#33391;&#22909;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#27604;&#22914;ChatGPT&#21644;GPT-4&#31561;&#27169;&#22411;&#22312;&#22810;&#31181;&#35821;&#35328;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#33021;&#21147;&#12290;&#20294;&#26159;&#65292;&#23545;&#36825;&#26679;&#30340;&#22823;&#35268;&#27169;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#32780;&#25214;&#21040;&#19982;&#27169;&#22411;&#35268;&#27169;&#21305;&#37197;&#30340;&#25968;&#25454;&#38598;&#36890;&#24120;&#20063;&#24456;&#22256;&#38590;&#12290;&#24494;&#35843;&#21644;&#20351;&#29992;&#26032;&#26041;&#27861;&#35757;&#32451;&#21442;&#25968;&#36739;&#23569;&#30340;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;MiniGPT-4&#27169;&#22411;&#20415;&#26159;&#20854;&#20013;&#20043;&#19968;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#36816;&#29992;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#38761;&#26032;&#24615;&#30340;&#22521;&#35757;&#31574;&#30053;&#23454;&#29616;&#20102;&#19982;GPT-4&#30456;&#24403;&#30340;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;&#20294;&#26159;&#65292;&#35813;&#27169;&#22411;&#22312;&#22270;&#20687;&#29702;&#35299;&#26041;&#38754;&#20173;&#28982;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#33402;&#26415;&#22270;&#29255;&#26041;&#38754;&#12290;ArtGPT-4&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#26088;&#22312;&#24212;&#23545;&#36825;&#20123;&#23616;&#38480;&#12290;ArtGPT-4&#20351;&#29992;Tesla A100&#35774;&#22791;&#23545;&#22270;&#20687;-&#25991;&#26412;&#23545;&#36827;&#34892;&#35757;&#32451;&#65292;&#20165;&#29992;&#20102;&#32422;200GB&#30340;&#25968;&#25454;&#65292;&#22312;2&#23567;&#26102;&#20869;&#23601;&#33021;&#23637;&#31034;&#20986;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, large language models (LLMs) have made significant progress in natural language processing (NLP), with models like ChatGPT and GPT-4 achieving impressive capabilities in various linguistic tasks. However, training models on such a large scale is challenging, and finding datasets that match the model's scale is often difficult. Fine-tuning and training models with fewer parameters using novel methods have emerged as promising approaches to overcome these challenges. One such model is MiniGPT-4, which achieves comparable vision-language understanding to GPT-4 by leveraging novel pre-training models and innovative training strategies. However, the model still faces some challenges in image understanding, particularly in artistic pictures. A novel multimodal model called ArtGPT-4 has been proposed to address these limitations. ArtGPT-4 was trained on image-text pairs using a Tesla A100 device in just 2 hours, using only about 200 GB of data. The model can depict images wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;Essential Element Network (EEN)&#31639;&#27861;&#23558;&#38899;&#39057;&#32534;&#30721;&#25104;&#25991;&#26412;&#24182;&#36827;&#34892;&#30456;&#20851;&#24615;&#35745;&#31639;&#21644;&#20248;&#21270;&#24212;&#29992;&#20110;&#32858;&#31867;&#31995;&#25968;&#30340;&#39057;&#29575;&#21644;&#25490;&#21517;&#30340;&#26041;&#27861;&#65292;&#24471;&#21040;&#20102;&#38899;&#20048;&#30340;&#28145;&#23618;&#32467;&#26500;&#20449;&#24687;&#65292;&#20026;&#21400;&#28165;&#38899;&#20048;&#32467;&#26500;&#25552;&#20379;&#20102;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.13631</link><description>&lt;p&gt;
&#38899;&#20048;&#32467;&#26500;&#30340;&#33258;&#32452;&#32455;&#32593;&#32476;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
In-depth analysis of music structure as a self-organized network. (arXiv:2303.13631v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;Essential Element Network (EEN)&#31639;&#27861;&#23558;&#38899;&#39057;&#32534;&#30721;&#25104;&#25991;&#26412;&#24182;&#36827;&#34892;&#30456;&#20851;&#24615;&#35745;&#31639;&#21644;&#20248;&#21270;&#24212;&#29992;&#20110;&#32858;&#31867;&#31995;&#25968;&#30340;&#39057;&#29575;&#21644;&#25490;&#21517;&#30340;&#26041;&#27861;&#65292;&#24471;&#21040;&#20102;&#38899;&#20048;&#30340;&#28145;&#23618;&#32467;&#26500;&#20449;&#24687;&#65292;&#20026;&#21400;&#28165;&#38899;&#20048;&#32467;&#26500;&#25552;&#20379;&#20102;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#35789;&#27719;&#19981;&#20165;&#20256;&#36882;&#20449;&#24687;&#65292;&#36824;&#38543;&#30528;&#25991;&#26126;&#21644;&#20154;&#31867;&#36801;&#31227;&#32780;&#28436;&#21464;&#12290;&#38899;&#20048;&#20063;&#26159;&#22914;&#27492;&#12290;&#20026;&#20102;&#29702;&#35299;&#38899;&#20048;&#32972;&#21518;&#30340;&#22797;&#26434;&#32467;&#26500;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21483;&#20570;Essential Element Network (EEN)&#30340;&#31639;&#27861;&#23558;&#38899;&#39057;&#32534;&#30721;&#25104;&#25991;&#26412;&#12290;&#35813;&#32593;&#32476;&#36890;&#36807;&#35745;&#31639;&#38899;&#35843;&#12289;&#26102;&#38388;&#21644;&#38899;&#37327;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#24471;&#21040;&#65292;&#36890;&#36807;&#20248;&#21270;EEN&#31639;&#27861;&#20197;&#29983;&#25104;Zipf&#23450;&#24459;&#24212;&#29992;&#20110;&#32858;&#31867;&#31995;&#25968;&#30340;&#39057;&#29575;&#21644;&#25490;&#21517;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#35821;&#20041;&#20851;&#31995;&#35270;&#20026;&#35789;&#27719;&#24182;&#29983;&#25104;&#23427;&#20204;&#30340;&#26144;&#23556;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#32534;&#30721;&#21518;&#30340;&#35789;&#27719;&#26144;&#23556;&#21040;&#38899;&#35843;-&#26102;&#38388;&#31354;&#38388;&#20013;&#65292;&#26377;&#21161;&#20110;&#25105;&#20204;&#31995;&#32479;&#22320;&#32452;&#32455;&#38899;&#20048;&#28145;&#23618;&#32467;&#26500;&#20013;&#30340;&#21477;&#27861;&#12290;&#30456;&#27604;&#20110;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#40657;&#30418;&#23376;&#29305;&#24615;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#25552;&#20379;&#20102;&#23545;&#38899;&#20048;&#32972;&#21518;&#22797;&#26434;&#32593;&#32476;&#30340;&#31934;&#30830;&#25551;&#36848;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#36807;&#31243;&#31215;&#32047;&#30340;&#32463;&#39564;&#21644;&#23646;&#24615;&#19981;&#20165;&#20026;&#27492;&#31867;&#24212;&#29992;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#20063;&#20026;&#35768;&#22810;&#20854;&#20182;&#30456;&#20851;&#39046;&#22495;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#25506;&#32034;&#30340;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Words in a natural language not only transmit information but also evolve with the development of civilization and human migration. The same is true for music. To understand the complex structure behind the music, we introduced an algorithm called the Essential Element Network (EEN) to encode the audio into text. The network is obtained by calculating the correlations between scales, time, and volume. Optimizing EEN to generate Zipfs law for the frequency and rank of the clustering coefficient enables us to generate and regard the semantic relationships as words. We map these encoded words into the scale-temporal space, which helps us organize systematically the syntax in the deep structure of music. Our algorithm provides precise descriptions of the complex network behind the music, as opposed to the black-box nature of other deep learning approaches. As a result, the experience and properties accumulated through these processes can offer not only a new approach to the applications of
&lt;/p&gt;</description></item></channel></rss>