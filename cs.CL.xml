<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21307;&#23398;&#38382;&#39064;&#25968;&#25454;&#38598;ViMQ&#65292;&#35813;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#36234;&#21335;&#24739;&#32773;&#30340;&#21307;&#23398;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#24847;&#22270;&#20998;&#31867;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#30340;&#26631;&#31614;&#38598;&#65292;&#26377;&#21161;&#20110;&#20419;&#36827;&#21307;&#30103;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2304.14405</link><description>&lt;p&gt;
ViMQ&#65306;&#38754;&#21521;&#21307;&#30103;&#23545;&#35805;&#31995;&#32479;&#24320;&#21457;&#30340;&#36234;&#21335;&#21307;&#23398;&#38382;&#39064;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ViMQ: A Vietnamese Medical Question Dataset for Healthcare Dialogue System Development. (arXiv:2304.14405v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14405
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21307;&#23398;&#38382;&#39064;&#25968;&#25454;&#38598;ViMQ&#65292;&#35813;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#36234;&#21335;&#24739;&#32773;&#30340;&#21307;&#23398;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#24847;&#22270;&#20998;&#31867;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#30340;&#26631;&#31614;&#38598;&#65292;&#26377;&#21161;&#20110;&#20419;&#36827;&#21307;&#30103;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#21307;&#30103;&#25991;&#26412;&#25968;&#25454;&#38598;&#36890;&#24120;&#37319;&#29992;&#38382;&#39064;&#21644;&#31572;&#26696;&#23545;&#30340;&#24418;&#24335;&#65292;&#25903;&#25345;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#65292;&#20294;&#32570;&#20047;&#21307;&#23398;&#26415;&#35821;&#30340;&#32452;&#21512;&#27880;&#37322;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#36234;&#21335;&#21307;&#23398;&#38382;&#39064;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#38024;&#23545;&#24847;&#22270;&#20998;&#31867;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#30340;&#21477;&#23376;&#32423;&#21644;&#23454;&#20307;&#32423;&#27880;&#37322;&#12290;&#20004;&#20010;&#20219;&#21153;&#30340;&#26631;&#31614;&#38598;&#22343;&#23646;&#20110;&#21307;&#23398;&#39046;&#22495;&#65292;&#24182;&#33021;&#22815;&#20419;&#36827;&#38754;&#21521;&#20219;&#21153;&#30340;&#21307;&#30103;&#32842;&#22825;&#26426;&#22120;&#20154;&#20174;&#24739;&#32773;&#26597;&#35810;&#20013;&#26356;&#22909;&#22320;&#29702;&#35299;&#20854;&#21547;&#20041;&#12290;&#25105;&#20204;&#23545;&#20004;&#20010;&#20219;&#21153;&#36827;&#34892;&#20102;&#22522;&#32447;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#33258;&#30417;&#30563;&#35757;&#32451;&#31574;&#30053;&#65292;&#36890;&#36807;&#36328;&#24230;&#22122;&#22768;&#24314;&#27169;&#26174;&#30528;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#23558;&#21457;&#24067;&#22312; https://github.com/tadeephuy/ViMQ&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing medical text datasets usually take the form of ques- tion and answer pairs that support the task of natural language gener- ation, but lacking the composite annotations of the medical terms. In this study, we publish a Vietnamese dataset of medical questions from patients with sentence-level and entity-level annotations for the Intent Classification and Named Entity Recognition tasks. The tag sets for two tasks are in medical domain and can facilitate the development of task- oriented healthcare chatbots with better comprehension of queries from patients. We train baseline models for the two tasks and propose a simple self-supervised training strategy with span-noise modelling that substan- tially improves the performance. Dataset and code will be published at https://github.com/tadeephuy/ViMQ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;LaMini-LM&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#35268;&#27169;&#25351;&#20196;&#30340;&#22810;&#26679;&#24615;&#21387;&#32553;&#27169;&#22411;&#32676;&#38598;&#65292;&#20174;&#25351;&#20196;&#24494;&#35843;&#36807;&#30340;LLMs&#20013;&#25552;&#21462;&#30693;&#35782;&#21040;&#26356;&#23567;&#30340;&#27169;&#22411;&#20013;&#65292;&#20854;&#22312;15&#20010;&#19981;&#21516;&#30340;NLP&#22522;&#20934;&#27979;&#35797;&#20013;&#19982;&#20854;&#20182;&#31454;&#20105;&#22522;&#32447;&#30340;&#34920;&#29616;&#30456;&#24403;&#65292;&#20294;&#20307;&#31215;&#32422;&#23567;&#20102;10&#20493;&#12290;</title><link>http://arxiv.org/abs/2304.14402</link><description>&lt;p&gt;
LaMini-LM: &#22522;&#20110;&#22823;&#35268;&#27169;&#25351;&#20196;&#30340;&#22810;&#26679;&#24615;&#21387;&#32553;&#27169;&#22411;&#32676;&#38598;
&lt;/p&gt;
&lt;p&gt;
LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions. (arXiv:2304.14402v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;LaMini-LM&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#35268;&#27169;&#25351;&#20196;&#30340;&#22810;&#26679;&#24615;&#21387;&#32553;&#27169;&#22411;&#32676;&#38598;&#65292;&#20174;&#25351;&#20196;&#24494;&#35843;&#36807;&#30340;LLMs&#20013;&#25552;&#21462;&#30693;&#35782;&#21040;&#26356;&#23567;&#30340;&#27169;&#22411;&#20013;&#65292;&#20854;&#22312;15&#20010;&#19981;&#21516;&#30340;NLP&#22522;&#20934;&#27979;&#35797;&#20013;&#19982;&#20854;&#20182;&#31454;&#20105;&#22522;&#32447;&#30340;&#34920;&#29616;&#30456;&#24403;&#65292;&#20294;&#20307;&#31215;&#32422;&#23567;&#20102;10&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#20294;&#26159;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#36164;&#28304;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20174;&#24494;&#35843;&#36807;&#30340;LLMs&#20013;&#25552;&#21462;&#30693;&#35782;&#21040;&#26356;&#23567;&#30340;&#27169;&#22411;&#20013;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20180;&#32454;&#24320;&#21457;&#20102;&#19968;&#32452;258&#19975;&#20221;&#22522;&#20110;&#29616;&#26377;&#21644;&#26032;&#29983;&#25104;&#30340;&#25351;&#20196;&#12290;&#38500;&#20102;&#35268;&#27169;&#22823;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#24191;&#27867;&#30340;&#35805;&#39064;&#65292;&#20197;&#30830;&#20445;&#25351;&#20196;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#20351;&#29992;gpt-3.5-turbo&#20026;&#36825;&#20123;&#25351;&#20196;&#29983;&#25104;&#21709;&#24212;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#25351;&#20196;&#26469;&#24494;&#35843;&#22810;&#20010;&#27169;&#22411;&#65292;&#21363;LaMini-LM&#65292;&#21253;&#25324;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21644;&#20165;&#35299;&#30721;&#22120;&#31995;&#21015;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#65288;&#22312;15&#20010;&#19981;&#21516;&#30340;NLP&#22522;&#20934;&#27979;&#35797;&#20013;&#65289;&#21644;&#25163;&#21160;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;LaMini-LM&#19982;&#20854;&#20182;&#31454;&#20105;&#22522;&#32447;&#30340;&#34920;&#29616;&#30456;&#24403;&#65292;&#32780;&#19988;&#20307;&#31215;&#32422;&#23567;&#20102;10&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) with instruction finetuning demonstrate superior generative capabilities. However, these models are resource intensive. To alleviate this issue, we explore distilling knowledge from instruction-tuned LLMs to much smaller ones. To this end, we carefully develop a large set of 2.58M instructions based on both existing and newly-generated instructions. In addition to being sizeable, we design our instructions to cover a broad set of topics to ensure. A thorough investigation of our instruction data demonstrate their diversity, and we generate responses for these instructions using gpt-3.5-turbo. We then exploit the instructions to tune a host of models, dubbed LaMini-LM, of varying sizes, both from the encoder-decoder as well as the decoder-only families. We evaluate our models both automatically (on 15 different NLP benchmarks) and manually. Results show that our proposed LaMini-LM are on par with competitive baselines while being nearly 10 times smaller in s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#27495;&#20041;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;AmbiEnt&#22522;&#20934;&#25968;&#25454;&#38598;&#35774;&#35745;&#20102;&#19968;&#31995;&#21015;&#27979;&#35797;&#26469;&#35780;&#20272;GPT-4&#21644;&#20854;&#20182;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23545;&#27495;&#20041;&#30340;&#35782;&#21035;&#21644;&#20998;&#31163;&#33021;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#26159;&#19968;&#20010;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#30446;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#36824;&#26080;&#27861;&#20934;&#30830;&#22788;&#29702;&#27495;&#20041;&#12290;</title><link>http://arxiv.org/abs/2304.14399</link><description>&lt;p&gt;
&#25105;&#20204;&#25285;&#24515;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#24314;&#27169;&#27495;&#20041;
&lt;/p&gt;
&lt;p&gt;
We're Afraid Language Models Aren't Modeling Ambiguity. (arXiv:2304.14399v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#27495;&#20041;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;AmbiEnt&#22522;&#20934;&#25968;&#25454;&#38598;&#35774;&#35745;&#20102;&#19968;&#31995;&#21015;&#27979;&#35797;&#26469;&#35780;&#20272;GPT-4&#21644;&#20854;&#20182;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23545;&#27495;&#20041;&#30340;&#35782;&#21035;&#21644;&#20998;&#31163;&#33021;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#26159;&#19968;&#20010;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#30446;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#36824;&#26080;&#27861;&#20934;&#30830;&#22788;&#29702;&#27495;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27495;&#20041;&#26159;&#33258;&#28982;&#35821;&#35328;&#30340;&#20869;&#22312;&#29305;&#24449;&#12290;&#31649;&#29702;&#27495;&#20041;&#26159;&#20154;&#31867;&#35821;&#35328;&#29702;&#35299;&#30340;&#20851;&#38190;&#37096;&#20998;&#65292;&#20801;&#35768;&#25105;&#20204;&#20316;&#20026;&#27807;&#36890;&#32773;&#39044;&#26399;&#21040;&#35823;&#35299;&#65292;&#24182;&#20316;&#20026;&#21548;&#20247;&#20462;&#27491;&#25105;&#20204;&#30340;&#35299;&#37322;&#12290;&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20316;&#23545;&#35805;&#30028;&#38754;&#21644;&#20889;&#20316;&#21161;&#25163;&#65292;&#22788;&#29702;&#21547;&#31946;&#19981;&#28165;&#30340;&#35821;&#35328;&#23545;&#23427;&#20204;&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#36890;&#36807;&#19982;&#21478;&#19968;&#21477;&#23376;&#30340;&#34164;&#21547;&#20851;&#31995;&#26469;&#25551;&#36848;&#21477;&#23376;&#20013;&#30340;&#27495;&#20041;&#65292;&#24182;&#25910;&#38598;&#20102;&#19968;&#32452;&#21253;&#21547;1,645&#20010;&#19981;&#21516;&#31867;&#22411;&#27495;&#20041;&#30340;&#35821;&#35328;&#23398;&#27880;&#37322;&#31034;&#20363;&#30340;&#22522;&#20934;&#25968;&#25454;AmbiEnt&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31995;&#21015;&#22522;&#20110;AmbiEnt&#30340;&#27979;&#35797;&#65292;&#21576;&#29616;&#20102;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#35782;&#21035;&#27495;&#20041;&#21644;&#20998;&#31163;&#21487;&#33021;&#21547;&#20041;&#30340;&#31532;&#19968;&#27425;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20010;&#20219;&#21153;&#20173;&#28982;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21253;&#25324;&#26368;&#36817;&#21457;&#24067;&#30340;GPT-4&#12290;&#20154;&#31867;&#35780;&#20272;&#20013;&#65292;GPT-4&#20135;&#29983;&#30340;&#28040;&#27495;&#34987;&#35748;&#20026;&#20165;&#26377;32%&#26159;&#27491;&#30830;&#30340;&#65292;&#32780;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#20013;&#28040;&#27495;&#30340;&#27491;&#30830;&#29575;&#20026;90%&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#38416;&#26126;&#25105;&#20204;&#30740;&#31350;&#30340;&#20215;&#20540;&#65292;
&lt;/p&gt;
&lt;p&gt;
Ambiguity is an intrinsic feature of natural language. Managing ambiguity is a key part of human language understanding, allowing us to anticipate misunderstanding as communicators and revise our interpretations as listeners. As language models (LMs) are increasingly employed as dialogue interfaces and writing aids, handling ambiguous language is critical to their success. We characterize ambiguity in a sentence by its effect on entailment relations with another sentence, and collect AmbiEnt, a linguist-annotated benchmark of 1,645 examples with diverse kinds of ambiguity. We design a suite of tests based on AmbiEnt, presenting the first evaluation of pretrained LMs to recognize ambiguity and disentangle possible meanings. We find that the task remains extremely challenging, including for the recent GPT-4, whose generated disambiguations are considered correct only 32% of the time in human evaluation, compared to 90% for disambiguations in our dataset. Finally, to illustrate the value 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#22330;&#26223;&#37325;&#26032;&#25490;&#21015;&#35268;&#21010;&#22120;&#65292;&#36890;&#36807;&#35821;&#35328;&#25351;&#23548;&#30340;&#31354;&#38388;&#27010;&#24565;&#26469;&#23454;&#29616;&#38271;&#25351;&#20196;&#20197;&#21450;&#22312;&#35757;&#32451;&#26102;&#20174;&#26410;&#35265;&#36807;&#30340;&#31354;&#38388;&#27010;&#24565;&#32452;&#21512;&#12290;&#26412;&#25991;&#30340;&#27169;&#22411;&#22312;&#25351;&#20196;&#23548;&#21521;&#25805;&#20316;&#22522;&#20934;&#27979;&#35797;&#20197;&#21450;&#32452;&#21512;&#25351;&#20196;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20248;&#20110;&#22522;&#20110;&#35821;&#35328;&#34920;&#36798;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#25104;&#21151;&#22320;&#35299;&#20915;&#20043;&#21069;&#20174;&#26410;&#35265;&#36807;&#30340;&#22797;&#26434;&#25351;&#20196;&#21644;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2304.14391</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#22330;&#26223;&#37325;&#26032;&#25490;&#21015;&#35268;&#21010;&#22120;
&lt;/p&gt;
&lt;p&gt;
Energy-based Models as Zero-Shot Planners for Compositional Scene Rearrangement. (arXiv:2304.14391v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#22330;&#26223;&#37325;&#26032;&#25490;&#21015;&#35268;&#21010;&#22120;&#65292;&#36890;&#36807;&#35821;&#35328;&#25351;&#23548;&#30340;&#31354;&#38388;&#27010;&#24565;&#26469;&#23454;&#29616;&#38271;&#25351;&#20196;&#20197;&#21450;&#22312;&#35757;&#32451;&#26102;&#20174;&#26410;&#35265;&#36807;&#30340;&#31354;&#38388;&#27010;&#24565;&#32452;&#21512;&#12290;&#26412;&#25991;&#30340;&#27169;&#22411;&#22312;&#25351;&#20196;&#23548;&#21521;&#25805;&#20316;&#22522;&#20934;&#27979;&#35797;&#20197;&#21450;&#32452;&#21512;&#25351;&#20196;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20248;&#20110;&#22522;&#20110;&#35821;&#35328;&#34920;&#36798;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#25104;&#21151;&#22320;&#35299;&#20915;&#20043;&#21069;&#20174;&#26410;&#35265;&#36807;&#30340;&#22797;&#26434;&#25351;&#20196;&#21644;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#24320;&#21457;&#19968;&#20010;&#22330;&#26223;&#37325;&#25490;&#26694;&#26550;&#65292;&#21487;&#20197;&#35299;&#37322;&#38271;&#25351;&#20196;&#20197;&#21450;&#22312;&#35757;&#32451;&#26102;&#20174;&#26410;&#35265;&#36807;&#30340;&#31354;&#38388;&#27010;&#24565;&#32452;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#30456;&#23545;&#23545;&#35937;&#25490;&#21015;&#30340;&#33021;&#37327;&#20989;&#25968;&#26469;&#34920;&#31034;&#35821;&#35328;&#25351;&#23548;&#30340;&#31354;&#38388;&#27010;&#24565;&#12290;&#35821;&#35328;&#35299;&#26512;&#22120;&#23558;&#25351;&#20196;&#26144;&#23556;&#21040;&#30456;&#24212;&#30340;&#33021;&#37327;&#20989;&#25968;&#65292;&#32780;&#24320;&#25918;&#24335;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#23558;&#23427;&#20204;&#30340;&#21442;&#25968;&#22522;&#20110;&#22330;&#26223;&#20013;&#30340;&#30456;&#20851;&#23545;&#35937;&#36827;&#34892;&#20462;&#27491;&#12290;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#27714;&#35299;&#33021;&#37327;&#20989;&#25968;&#30340;&#24635;&#21644;&#65292;&#24182;&#21033;&#29992;&#22522;&#20110;&#26412;&#22320;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#31574;&#30053;&#23558;&#23545;&#35937;&#37325;&#26032;&#23450;&#20301;&#21040;&#25512;&#26029;&#30340;&#30446;&#26631;&#20301;&#32622;&#65292;&#21363;&#21487;&#29983;&#25104;&#30446;&#26631;&#22330;&#26223;&#37197;&#32622;&#12290;&#25105;&#20204;&#22312;&#24050;&#24314;&#31435;&#30340;&#25351;&#20196;&#23548;&#21521;&#25805;&#20316;&#22522;&#20934;&#27979;&#35797;&#20197;&#21450;&#25105;&#20204;&#25552;&#20986;&#30340;&#32452;&#21512;&#25351;&#20196;&#22522;&#20934;&#27979;&#35797;&#20013;&#27979;&#35797;&#20102;&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#32489;&#25928;&#20248;&#20110;&#22522;&#20110;&#35821;&#35328;&#34920;&#36798;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#25104;&#21151;&#22320;&#35299;&#20915;&#20043;&#21069;&#20174;&#26410;&#35265;&#36807;&#30340;&#22797;&#26434;&#25351;&#20196;&#21644;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language is compositional; an instruction can express multiple relation constraints to hold among objects in a scene that a robot is tasked to rearrange. Our focus in this work is an instructable scene rearranging framework that generalizes to longer instructions and to spatial concept compositions never seen at training time. We propose to represent language-instructed spatial concepts with energy functions over relative object arrangements. A language parser maps instructions to corresponding energy functions and an open-vocabulary visual-language model grounds their arguments to relevant objects in the scene. We generate goal scene configurations by gradient descent on the sum of energy functions, one per language predicate in the instruction. Local vision-based policies then relocate objects to the inferred goal locations. We test our model on established instruction-guided manipulation benchmarks, as well as benchmarks of compositional instructions we introduce. We show our model 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CONSCENDI&#30340;&#33976;&#39311;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#38450;&#25252;&#26639;&#27169;&#22411;&#65292;&#20197;&#30417;&#25511;&#20219;&#21153;&#22411;&#34394;&#25311;&#21161;&#25163;&#30340;&#36755;&#20986;&#12290;&#20851;&#38190;&#26041;&#27861;&#21253;&#25324;&#22330;&#26223;&#22686;&#24378;&#29983;&#25104;&#21644;&#23545;&#27604;&#35757;&#32451;&#26679;&#20363;&#12290;&#36825;&#31181;&#26041;&#27861;&#20135;&#29983;&#20102;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#36829;&#21453;&#35268;&#21017;&#30340;&#23545;&#35805;&#35757;&#32451;&#38598;&#65292;&#24182;&#19988;&#21487;&#20197;&#26356;&#22909;&#22320;&#26816;&#27979;&#20195;&#29702;&#30340;&#36755;&#20986;&#26159;&#21542;&#31526;&#21512;&#35774;&#35745;&#32773;&#25351;&#23450;&#30340;&#35268;&#21017;&#12290;</title><link>http://arxiv.org/abs/2304.14364</link><description>&lt;p&gt;
CONSCENDI: &#19968;&#31181;&#21453;&#23545;&#27604;&#19988;&#22330;&#26223;&#24341;&#23548;&#30340;&#33976;&#39311;&#26041;&#27861;&#26469;&#20026;&#34394;&#25311;&#21161;&#25163;&#26500;&#24314;&#38450;&#25252;&#26639;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CONSCENDI: A Contrastive and Scenario-Guided Distillation Approach to Guardrail Models for Virtual Assistants. (arXiv:2304.14364v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CONSCENDI&#30340;&#33976;&#39311;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#38450;&#25252;&#26639;&#27169;&#22411;&#65292;&#20197;&#30417;&#25511;&#20219;&#21153;&#22411;&#34394;&#25311;&#21161;&#25163;&#30340;&#36755;&#20986;&#12290;&#20851;&#38190;&#26041;&#27861;&#21253;&#25324;&#22330;&#26223;&#22686;&#24378;&#29983;&#25104;&#21644;&#23545;&#27604;&#35757;&#32451;&#26679;&#20363;&#12290;&#36825;&#31181;&#26041;&#27861;&#20135;&#29983;&#20102;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#36829;&#21453;&#35268;&#21017;&#30340;&#23545;&#35805;&#35757;&#32451;&#38598;&#65292;&#24182;&#19988;&#21487;&#20197;&#26356;&#22909;&#22320;&#26816;&#27979;&#20195;&#29702;&#30340;&#36755;&#20986;&#26159;&#21542;&#31526;&#21512;&#35774;&#35745;&#32773;&#25351;&#23450;&#30340;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;GPT-4&#31561;&#36234;&#26469;&#36234;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#26032;&#19968;&#20195;&#30340;&#22522;&#20110;&#20219;&#21153;&#30340;&#34394;&#25311;&#21161;&#25163;&#24212;&#36816;&#32780;&#29983;&#12290;&#36825;&#20123;&#23545;&#35805;&#31995;&#32479;&#21487;&#20197;&#26681;&#25454;&#23458;&#25143;&#30340;&#20855;&#20307;&#29992;&#20363;&#36827;&#34892;&#23450;&#21046;&#65292;&#20294;&#30830;&#20445;&#20195;&#29702;&#29983;&#25104;&#30340;&#25991;&#26412;&#20165;&#31526;&#21512;&#25552;&#31034;&#25351;&#20196;&#20013;&#35774;&#35745;&#32773;&#25351;&#23450;&#30340;&#35268;&#21017;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#35774;&#35745;&#24072;&#36890;&#24120;&#20351;&#29992;&#21478;&#19968;&#20010;&#31216;&#20026;&#38450;&#25252;&#26639;&#27169;&#22411;&#30340;&#27169;&#22411;&#26469;&#39564;&#35777;&#20195;&#29702;&#36755;&#20986;&#26159;&#21542;&#19982;&#20854;&#35268;&#21017;&#21644;&#32422;&#26463;&#23545;&#40784;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;&#33976;&#39311;&#26041;&#27861;&#26469;&#26500;&#24314;&#38450;&#25252;&#26639;&#27169;&#22411;&#65292;&#20197;&#30417;&#25511;&#20351;&#29992;GPT-4&#20013;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#31532;&#19968;&#20010;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25105;&#20204;&#30340;CONSCENDI&#36807;&#31243;&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#27493;&#39588;&#65306;&#22330;&#26223;&#22686;&#24378;&#29983;&#25104;&#21644;&#23545;&#27604;&#35757;&#32451;&#26679;&#20363;&#12290;&#22312;&#29983;&#25104;&#23545;&#35805;&#25968;&#25454;&#26102;&#65292;&#25105;&#20204;&#20250;&#29983;&#25104;&#19968;&#32452;&#36829;&#21453;&#35268;&#21017;&#30340;&#22330;&#26223;&#65292;&#36825;&#20123;&#22330;&#26223;&#21015;&#20030;&#20102;&#36829;&#21453;&#35268;&#21017;&#30340;&#22810;&#26679;&#21270;&#39640;&#32423;&#26041;&#24335;&#12290;&#36825;&#31181;&#22330;&#26223;&#24341;&#23548;&#26041;&#27861;&#20135;&#29983;&#20102;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#36829;&#21453;&#35268;&#21017;&#30340;&#23545;&#35805;&#35757;&#32451;&#38598;&#65292;&#24182;&#19988;&#23427;&#20351;&#24471;&#27169;&#22411;&#26356;&#23481;&#26131;&#26816;&#27979;&#21040;&#20195;&#29702;&#29983;&#25104;&#30340;&#25991;&#26412;&#26159;&#21542;&#31526;&#21512;&#35774;&#35745;&#32773;&#25351;&#23450;&#30340;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
A wave of new task-based virtual assistants has been fueled by increasingly powerful large language models, such as GPT-4. These conversational agents can be customized to serve customer-specific use cases, but ensuring that agent-generated text conforms to designer-specified rules included in prompt instructions alone is challenging. Therefore, chatbot designers often use another model, called a guardrail model, to verify that the agent output aligns with their rules and constraints. We explore using a distillation approach to guardrail models to monitor the output of the first model using training data from GPT-4. We find two crucial steps to our CONSCENDI process: scenario-augmented generation and contrastive training examples. When generating conversational data, we generate a set of rule-breaking scenarios, which enumerate a diverse set of high-level ways a rule can be violated. This scenario-guided approach produces a diverse training set of rule-violating conversations, and it p
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24037;&#19994;&#24037;&#31243;&#20013;&#30340;&#24212;&#29992;&#39046;&#22495;&#21021;&#29616;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#22312;&#27833;&#27668;&#38382;&#39064;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#30830;&#23450;LLM&#26368;&#26377;&#25928;&#30340;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2304.14354</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24037;&#19994;&#24037;&#31243;&#20013;&#30340;&#24212;&#29992;: ChatGPT&#22312;&#27833;&#27668;&#38382;&#39064;&#20013;&#30340;&#34920;&#29616;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Industrial Engineering with Large Language Models: A case study of ChatGPT's performance on Oil &amp; Gas problems. (arXiv:2304.14354v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14354
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24037;&#19994;&#24037;&#31243;&#20013;&#30340;&#24212;&#29992;&#39046;&#22495;&#21021;&#29616;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#22312;&#27833;&#27668;&#38382;&#39064;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#30830;&#23450;LLM&#26368;&#26377;&#25928;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#35299;&#20915;&#21508;&#31181;&#22797;&#26434;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#21253;&#25324;&#27833;&#27668;&#24037;&#31243;&#20197;&#21450;&#20854;&#20182;&#24037;&#19994;&#24037;&#31243;&#23398;&#31185;&#65292;&#20363;&#22914;&#24037;&#21378;&#33258;&#21160;&#21270;&#12289;PLC&#32534;&#31243;&#31561;&#12290;&#28982;&#32780;&#65292;&#23545;&#20960;&#20010;&#24037;&#19994;&#36807;&#31243;&#25511;&#21046;&#22522;&#30784;&#29289;&#29702;&#26041;&#31243;&#30340;&#24378;&#24369;&#35299;&#30340;&#33258;&#21160;&#35782;&#21035;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#30830;&#23450;&#20102;&#24403;&#21069;LLM&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#27833;&#27668;&#24037;&#31243;&#31561;&#36873;&#25321;&#24615;&#23454;&#38469;&#38382;&#39064;&#19978;&#30340;ChatGPT&#34920;&#29616;&#12290;&#35752;&#35770;&#20102;ChatGPT&#22312;&#35299;&#20915;&#27833;&#27668;&#24037;&#31243;&#20013;&#22797;&#26434;&#38382;&#39064;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20986;&#20102;LLM&#26368;&#26377;&#25928;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown great potential in solving complex problems in various fields, including oil and gas engineering and other industrial engineering disciplines like factory automation, PLC programming etc. However, automatic identification of strong and weak solutions to fundamental physics equations governing several industrial processes remain a challenging task. This paper identifies the limitation of current LLM approaches, particularly ChatGPT in selected practical problems native to oil and gas engineering but not exclusively. The performance of ChatGPT in solving complex problems in oil and gas engineering is discussed and the areas where LLMs are most effective are presented.
&lt;/p&gt;</description></item><item><title>ChatGPT&#24102;&#26469;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#34429;&#28982;&#26377;&#24456;&#22810;&#20248;&#21183;&#65292;&#20294;&#26159;&#38543;&#26426;&#40550;&#40521;&#21644;&#24187;&#35273;&#31561;&#26032;&#30340;&#27861;&#24459;&#21644;&#20262;&#29702;&#39118;&#38505;&#20063;&#38543;&#20043;&#32780;&#26469;&#12290;&#27431;&#27954;AI&#30417;&#31649;&#33539;&#24335;&#38656;&#35201;&#36827;&#19968;&#27493;&#21457;&#23637;&#20197;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2304.14347</link><description>&lt;p&gt;
ChatGPT&#30340;&#40657;&#26263;&#38754;&#65306;&#26469;&#33258;&#38543;&#26426;&#40550;&#40521;&#21644;&#24187;&#35273;&#30340;&#27861;&#24459;&#21644;&#20262;&#29702;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
The Dark Side of ChatGPT: Legal and Ethical Challenges from Stochastic Parrots and Hallucination. (arXiv:2304.14347v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14347
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#24102;&#26469;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#34429;&#28982;&#26377;&#24456;&#22810;&#20248;&#21183;&#65292;&#20294;&#26159;&#38543;&#26426;&#40550;&#40521;&#21644;&#24187;&#35273;&#31561;&#26032;&#30340;&#27861;&#24459;&#21644;&#20262;&#29702;&#39118;&#38505;&#20063;&#38543;&#20043;&#32780;&#26469;&#12290;&#27431;&#27954;AI&#30417;&#31649;&#33539;&#24335;&#38656;&#35201;&#36827;&#19968;&#27493;&#21457;&#23637;&#20197;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;ChatGPT&#30340;&#25512;&#20986;&#65292;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#27491;&#22312;&#21160;&#25671;&#25105;&#20204;&#25972;&#20010;&#31038;&#20250;&#65292;&#24555;&#36895;&#25913;&#21464;&#25105;&#20204;&#30340;&#24605;&#32500;&#12289;&#21019;&#36896;&#21644;&#29983;&#27963;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#38543;&#26426;&#40550;&#40521;&#21644;&#24187;&#35273;&#31561;&#26032;&#30340;&#27861;&#24459;&#21644;&#20262;&#29702;&#39118;&#38505;&#20986;&#29616;&#65292;&#26032;&#20852;LLMs&#20063;&#24102;&#26469;&#20102;&#35768;&#22810;&#25361;&#25112;&#12290;&#27431;&#30431;&#26159;&#31532;&#19968;&#20010;&#23558;&#37325;&#28857;&#25918;&#22312;AI&#27169;&#22411;&#30417;&#31649;&#19978;&#30340;&#21496;&#27861;&#31649;&#36758;&#21306;&#12290;&#28982;&#32780;&#65292;&#26032;LLMs&#24102;&#26469;&#30340;&#39118;&#38505;&#21487;&#33021;&#20250;&#34987;&#26032;&#20852;&#30340;&#27431;&#30431;&#30417;&#31649;&#33539;&#24335;&#25152;&#20302;&#20272;&#12290;&#22240;&#27492;&#65292;&#26412;&#20989;&#21578;&#35686;&#31034;&#27431;&#27954;AI&#30417;&#31649;&#33539;&#24335;&#24517;&#39035;&#36827;&#19968;&#27493;&#21457;&#23637;&#20197;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the launch of ChatGPT, Large Language Models (LLMs) are shaking up our whole society, rapidly altering the way we think, create and live. For instance, the GPT integration in Bing has altered our approach to online searching. While nascent LLMs have many advantages, new legal and ethical risks are also emerging, stemming in particular from stochastic parrots and hallucination. The EU is the first and foremost jurisdiction that has focused on the regulation of AI models. However, the risks posed by the new LLMs are likely to be underestimated by the emerging EU regulatory paradigm. Therefore, this correspondence warns that the European AI regulatory paradigm must evolve further to mitigate such risks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#22312;&#22810;&#35821;&#35328;&#35774;&#32622;&#19979;&#20351;&#29992;&#22810;&#26631;&#31614;&#23545;&#27604;&#25439;&#22833;&#26469;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#22312;&#20116;&#31181;&#35821;&#35328;&#19978;&#26368;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.14339</link><description>&lt;p&gt;
MarsEclipse&#22312;SemEval-2023&#20219;&#21153;3&#20013;&#30340;&#22810;&#35821;&#35328;&#21644;&#22810;&#26631;&#31614;&#26694;&#26550;&#26816;&#27979;&#21450;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MarsEclipse at SemEval-2023 Task 3: Multi-Lingual and Multi-Label Framing Detection with Contrastive Learning. (arXiv:2304.14339v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#22312;&#22810;&#35821;&#35328;&#35774;&#32622;&#19979;&#20351;&#29992;&#22810;&#26631;&#31614;&#23545;&#27604;&#25439;&#22833;&#26469;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#22312;&#20116;&#31181;&#35821;&#35328;&#19978;&#26368;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#22312;SemEval-2023&#20219;&#21153;3&#30340;&#23376;&#20219;&#21153;2&#19978;&#36827;&#34892;&#26694;&#26550;&#26816;&#27979;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#22810;&#26631;&#31614;&#23545;&#27604;&#25439;&#22833;&#26469;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#35774;&#32622;&#65292;&#21462;&#24471;&#20102;&#38750;&#24120;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#65306;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#23448;&#26041;&#27979;&#35797;&#38598;&#21644;&#23448;&#26041;&#25490;&#34892;&#27036;&#19978;&#25490;&#21517;&#31532;&#19968;&#65292;&#23545;&#20110;&#25105;&#20204;&#26377;&#35757;&#32451;&#25968;&#25454;&#24182;&#33021;&#36827;&#34892;&#24494;&#35843;&#30340;&#20845;&#31181;&#35821;&#35328;&#20013;&#30340;&#20116;&#31181;&#35821;&#35328;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#25105;&#20204;&#30340;&#23454;&#39564;&#35774;&#32622;&#20197;&#21450;&#21508;&#31181;&#28040;&#34701;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#20195;&#30721;&#21487;&#22312;https://github.com/QishengL/SemEval2023&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes our system for SemEval-2023 Task 3 Subtask 2 on Framing Detection. We used a multi-label contrastive loss for fine-tuning large pre-trained language models in a multi-lingual setting, achieving very competitive results: our system was ranked first on the official test set and on the official shared task leaderboard for five of the six languages for which we had training data and for which we could perform fine-tuning. Here, we describe our experimental setup, as well as various ablation studies. The code of our system is available at https://github.com/QishengL/SemEval2023
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#32467;&#26500;&#25506;&#27979;&#26041;&#27861;&#30740;&#31350;&#20102;&#25104;&#35821;&#22312;&#38745;&#24577;&#21644;&#19978;&#19979;&#25991;&#23884;&#20837;&#20013;&#30340;&#32534;&#30721;&#24773;&#20917;&#65292;&#24182;&#21457;&#29616;&#20173;&#26410;&#30830;&#23450;&#25104;&#35821;&#24615;&#26159;&#21542;&#22312;&#21521;&#37327;&#33539;&#25968;&#20013;&#32534;&#30721;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#25968;&#25454;&#38598;&#20197;&#36827;&#34892;&#25506;&#27979;&#20998;&#26512;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2304.14333</link><description>&lt;p&gt;
&#25104;&#35821;&#12289;&#25506;&#27979;&#21644;&#21361;&#38505;&#30340;&#20107;&#29289;&#65306;&#22522;&#20110;&#32467;&#26500;&#25506;&#27979;&#30340;&#35789;&#21521;&#37327;&#25104;&#35821;&#24615;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Idioms, Probing and Dangerous Things: Towards Structural Probing for Idiomaticity in Vector Space. (arXiv:2304.14333v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#32467;&#26500;&#25506;&#27979;&#26041;&#27861;&#30740;&#31350;&#20102;&#25104;&#35821;&#22312;&#38745;&#24577;&#21644;&#19978;&#19979;&#25991;&#23884;&#20837;&#20013;&#30340;&#32534;&#30721;&#24773;&#20917;&#65292;&#24182;&#21457;&#29616;&#20173;&#26410;&#30830;&#23450;&#25104;&#35821;&#24615;&#26159;&#21542;&#22312;&#21521;&#37327;&#33539;&#25968;&#20013;&#32534;&#30721;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#25968;&#25454;&#38598;&#20197;&#36827;&#34892;&#25506;&#27979;&#20998;&#26512;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#19968;&#31181;&#32467;&#26500;&#25506;&#27979;&#26041;&#27861;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#25104;&#35821;&#20449;&#24687;&#22914;&#20309;&#34987;&#23884;&#20837;&#21040;&#35789;&#23884;&#20837;&#20013;&#12290;&#25105;&#20204;&#37325;&#26032;&#21033;&#29992;&#20102;&#29616;&#26377;&#30340;&#33521;&#35821;&#21160;&#35789;&#22797;&#21512;&#35789;&#35821;&#65288;MWE&#65289;&#25968;&#25454;&#38598;&#26469;&#36866;&#24212;&#25506;&#27979;&#26694;&#26550;&#65292;&#24182;&#23545;&#38745;&#24577;&#65288;GloVe&#65289;&#21644;&#19978;&#19979;&#25991;&#65288;BERT&#65289;&#23884;&#20837;&#36827;&#34892;&#20102;&#27604;&#36739;&#25506;&#27979;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#20197;&#19981;&#21516;&#31243;&#24230;&#32534;&#30721;&#20102;&#19968;&#20123;&#25104;&#35821;&#20449;&#24687;&#65292;&#20294;&#23545;&#20110;&#25104;&#35821;&#24615;&#26159;&#21542;&#22312;&#21521;&#37327;&#33539;&#25968;&#20013;&#32534;&#30721;&#32473;&#20986;&#20102;&#20914;&#31361;&#30340;&#35777;&#25454;&#65292;&#36825;&#20173;&#28982;&#26159;&#20010;&#26410;&#35299;&#20043;&#35868;&#12290;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#25152;&#20351;&#29992;&#25968;&#25454;&#38598;&#30340;&#19968;&#20123;&#23616;&#38480;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#25913;&#36827;&#20854;&#36866;&#29992;&#24615;&#36827;&#34892;&#25506;&#27979;&#20998;&#26512;&#30340;&#37325;&#35201;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of this paper is to learn more about how idiomatic information is structurally encoded in embeddings, using a structural probing method. We repurpose an existing English verbal multi-word expression (MWE) dataset to suit the probing framework and perform a comparative probing study of static (GloVe) and contextual (BERT) embeddings. Our experiments indicate that both encode some idiomatic information to varying degrees, but yield conflicting evidence as to whether idiomaticity is encoded in the vector norm, leaving this an open question. We also identify some limitations of the used dataset and highlight important directions for future work in improving its suitability for a probing analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;q2d&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#21547;&#26377;&#26597;&#35810;&#30340;&#23545;&#35805;&#26469;&#25945;&#23548;&#27169;&#22411;&#22914;&#20309;&#21457;&#20986;&#25628;&#32034;&#26597;&#35810;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#30340;&#27169;&#22411;&#24615;&#33021;&#25509;&#36817;&#20351;&#29992;&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#32780;&#19988;&#20855;&#26377;&#26356;&#22909;&#30340;&#25511;&#21046;&#21644;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.14318</link><description>&lt;p&gt;
q2d&#65306;&#23558;&#38382;&#39064;&#36716;&#25442;&#20026;&#23545;&#35805;&#65292;&#25945;&#23548;&#27169;&#22411;&#22914;&#20309;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
q2d: Turning Questions into Dialogs to Teach Models How to Search. (arXiv:2304.14318v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;q2d&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#21547;&#26377;&#26597;&#35810;&#30340;&#23545;&#35805;&#26469;&#25945;&#23548;&#27169;&#22411;&#22914;&#20309;&#21457;&#20986;&#25628;&#32034;&#26597;&#35810;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#30340;&#27169;&#22411;&#24615;&#33021;&#25509;&#36817;&#20351;&#29992;&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#32780;&#19988;&#20855;&#26377;&#26356;&#22909;&#30340;&#25511;&#21046;&#21644;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#23545;&#35805;&#35821;&#35328;&#27169;&#22411;&#26377;&#19968;&#20010;&#28608;&#21160;&#20154;&#24515;&#30340;&#33021;&#21147;&#65292;&#21363;&#33021;&#22815;&#29420;&#31435;&#22320;&#25628;&#32034;&#30456;&#20851;&#20449;&#24687;&#65292;&#20197;&#30830;&#23450;&#32473;&#23450;&#23545;&#35805;&#30340;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#29992;&#20110;&#25945;&#23548;&#27169;&#22411;&#22914;&#20309;&#21457;&#20986;&#25628;&#32034;&#26597;&#35810;&#30340;&#35757;&#32451;&#25968;&#25454;&#26159;&#32791;&#36153;&#26102;&#38388;&#21644;&#36164;&#28304;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;q2d&#65306;&#19968;&#31181;&#33258;&#21160;&#29983;&#25104;&#20174;&#38382;&#39064;&#20013;&#33719;&#21462;&#20449;&#24687;&#30340;&#23545;&#35805;&#30340;&#25968;&#25454;&#29983;&#25104;&#27969;&#27700;&#32447;&#12290;&#25105;&#20204;&#25552;&#20379;&#32473;&#19968;&#20010;&#22823;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;PaLM&#65289;&#26469;&#21019;&#24314;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#23545;&#35805;&#29256;&#26412;&#65292;&#24182;&#20351;&#29992;&#23427;&#26469;&#25913;&#36827;&#19982;&#22806;&#37096;&#25628;&#32034;API&#36890;&#20449;&#20197;&#30830;&#23450;&#23545;&#35805;&#21709;&#24212;&#30340;&#26597;&#35810;&#29983;&#25104;&#27169;&#22411;&#12290;&#19982;&#20808;&#21069;&#20381;&#36182;&#20110;&#20154;&#31867;&#32534;&#20889;&#30340;&#24102;&#26377;&#25628;&#32034;&#26597;&#35810;&#30340;&#23545;&#35805;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#33258;&#21160;&#29983;&#25104;&#22522;&#20110;&#26597;&#35810;&#30340;&#23545;&#35805;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#25511;&#21046;&#21644;&#35268;&#27169;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65306;&#65288;1&#65289;&#38024;&#23545;QReCC&#25968;&#25454;&#38598;&#30340;&#26597;&#35810;&#29983;&#25104;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;&#20174;MNLI&#20013;&#36827;&#34892;&#20256;&#36755;&#23398;&#20064;&#30340;&#27169;&#22411;&#24615;&#33021;&#30340;90%--97%&#65292;&#32780;&#20351;&#29992;&#20154;&#24037;&#26631;&#27880;&#30340;&#26597;&#35810;&#29983;&#25104;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;91%--96%&#30340;&#24615;&#33021;&#12290;&#65288;2&#65289;&#38024;&#23545;QUAC&#21644;CoQA&#25968;&#25454;&#38598;&#30340;&#22522;&#30784;&#21709;&#24212;&#36873;&#25321;&#65292;&#20351;&#29992;&#25105;&#20204;&#33258;&#21160;&#29983;&#25104;&#30340;&#23545;&#35805;&#35757;&#32451;&#30340;&#27169;&#22411;&#24615;&#33021;&#20165;&#27604;&#20351;&#29992;&#24102;&#26377;&#25628;&#32034;&#26597;&#35810;&#30340;&#20154;&#31867;&#29983;&#25104;&#30340;&#23545;&#35805;&#35757;&#32451;&#30340;&#27169;&#22411;&#24615;&#33021;&#20302;&#20102;0.8-2.2&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the exciting capabilities of recent language models for dialog is their ability to independently search for relevant information to ground a given dialog response. However, obtaining training data to teach models how to issue search queries is time and resource consuming. In this work, we propose q2d: an automatic data generation pipeline that generates information-seeking dialogs from questions. We prompt a large language model (PaLM) to create conversational versions of question answering datasets, and use it to improve query generation models that communicate with external search APIs to ground dialog responses. Unlike previous approaches which relied on human written dialogs with search queries, our method allows to automatically generate query-based grounded dialogs with better control and scale. Our experiments demonstrate that: (1) For query generation on the QReCC dataset, models trained on our synthetically-generated data achieve 90%--97% of the performance of models tr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110; GPT-3.5 &#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#23616;&#38480;&#24615;&#65292;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.14317</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#20195;&#30721;&#29983;&#25104;&#30340;&#26368;&#20808;&#36827;&#35780;&#20272;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Are State-of-the-Art Evaluators of Code Generation. (arXiv:2304.14317v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110; GPT-3.5 &#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#23616;&#38480;&#24615;&#65292;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#25512;&#21160;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#29983;&#25104;&#25991;&#26412;&#30340;&#33021;&#21147;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#22312;&#26426;&#22120;&#32763;&#35793;&#21644;&#25688;&#35201;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#20854;&#22312;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#20173;&#28982;&#23384;&#22312;&#38480;&#21046;&#12290;&#36825;&#20123;&#20219;&#21153;&#25152;&#38656;&#30340;&#32534;&#31243;&#27010;&#24565;&#30340;&#22797;&#26434;&#24615;&#20351;&#24471;&#24320;&#21457;&#35780;&#20272;&#25351;&#26631;&#20197;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#19968;&#33268;&#21464;&#24471;&#22256;&#38590;&#12290;&#20197;&#35789;&#27719;&#21305;&#37197;&#20026;&#22522;&#30784;&#30340;&#24230;&#37327;&#26631;&#20934;&#65288;&#22914;BLEU&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#20013;&#19982;&#20154;&#24037;&#20174;&#19994;&#32773;&#30340;&#30456;&#20851;&#24615;&#36739;&#24369;&#12290;&#27492;&#22806;&#65292;&#22312;&#20302;&#36164;&#28304;&#39046;&#22495;&#20013;&#21033;&#29992;&#20154;&#20026;&#32534;&#20889;&#30340;&#27979;&#35797;&#22871;&#20214;&#36827;&#34892;&#21151;&#33021;&#27491;&#30830;&#24615;&#35780;&#20272;&#20063;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38556;&#30861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;GPT-3.5&#30340;&#20195;&#30721;&#29983;&#25104;&#35780;&#20272;&#26694;&#26550;&#65288;\texttt{GPT-3.5-turbo}&#65289;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#21462;&#24471;&#26356;&#22909;&#30340;&#30456;&#20851;&#24615;&#26469;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in the field of natural language generation have facilitated the use of large language models to assess the quality of generated text. Although these models have shown promising results in tasks such as machine translation and summarization, their applicability in code generation tasks remains limited without human involvement. The complexity of programming concepts required for such tasks makes it difficult to develop evaluation metrics that align with human judgment. Token-matching-based metrics, such as BLEU, have demonstrated weak correlations with human practitioners in code generation tasks. Moreover, the utilization of human-written test suites to evaluate functional correctness can be challenging in domains with low resources. To overcome these obstacles, we propose a new evaluation framework based on the GPT-3.5 (\texttt{GPT-3.5-turbo}), for code generation assessments. Our framework addresses the limitations of existing approaches by achieving superior cor
&lt;/p&gt;</description></item><item><title>InstructCTG&#26159;&#19968;&#20010;&#21487;&#20197;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#21644;&#28436;&#31034;&#26469;&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;&#24182;&#28385;&#36275;&#19981;&#21516;&#32422;&#26463;&#26465;&#20214;&#30340;&#26694;&#26550;&#65292;&#23427;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#29616;&#26377;&#25628;&#32034;&#25110;&#24471;&#20998;&#26041;&#27861;&#25152;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.14293</link><description>&lt;p&gt;
&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Controlled Text Generation with Natural Language Instructions. (arXiv:2304.14293v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14293
&lt;/p&gt;
&lt;p&gt;
InstructCTG&#26159;&#19968;&#20010;&#21487;&#20197;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#21644;&#28436;&#31034;&#26469;&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;&#24182;&#28385;&#36275;&#19981;&#21516;&#32422;&#26463;&#26465;&#20214;&#30340;&#26694;&#26550;&#65292;&#23427;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#29616;&#26377;&#25628;&#32034;&#25110;&#24471;&#20998;&#26041;&#27861;&#25152;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20135;&#29983;&#27969;&#30021;&#30340;&#25991;&#26412;&#65292;&#24182;&#33021;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#65292;&#26080;&#38656;&#29305;&#23450;&#30340;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#25511;&#21046;&#23427;&#20204;&#30340;&#29983;&#25104;&#20197;&#28385;&#36275;&#19981;&#21516;&#24212;&#29992;&#31243;&#24207;&#25152;&#38656;&#30340;&#21508;&#31181;&#32422;&#26463;&#26465;&#20214;&#26159;&#38750;&#24120;&#22256;&#38590;&#30340;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#24102;&#32422;&#26463;&#35843;&#33410;&#30340;&#25991;&#26412;&#29983;&#25104;&#26694;&#26550;&#8212;&#8212;InstructCTG&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#21644;&#32422;&#26463;&#28436;&#31034;&#26469;&#32435;&#20837;&#19981;&#21516;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#19968;&#31995;&#21015;&#29616;&#25104;&#30340;NLP&#24037;&#20855;&#21644;&#31616;&#21333;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#25552;&#21462;&#33258;&#28982;&#25991;&#26412;&#30340;&#28508;&#22312;&#32422;&#26463;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#32422;&#26463;&#26465;&#20214;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#65292;&#20197;&#24418;&#25104;&#24369;&#30417;&#30563;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#36890;&#36807;&#28155;&#21152;&#33258;&#28982;&#35821;&#35328;&#32422;&#26463;&#25551;&#36848;&#21644;&#23569;&#37327;&#28436;&#31034;&#65292;&#25105;&#20204;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#20197;&#32435;&#20837;&#21508;&#31181;&#31867;&#22411;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#19982;&#29616;&#26377;&#22522;&#20110;&#25628;&#32034;&#25110;&#24471;&#20998;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;InstructCTG &#26356;&#21152;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models generate fluent texts and can follow natural language instructions to solve a wide range of tasks without task-specific training. Nevertheless, it is notoriously difficult to control their generation to satisfy the various constraints required by different applications. In this work, we present InstructCTG, a controlled text generation framework that incorporates different constraints by conditioning on natural language descriptions and demonstrations of the constraints. In particular, we first extract the underlying constraints of natural texts through a combination of off-the-shelf NLP tools and simple heuristics. We then verbalize the constraints into natural language instructions to form weakly supervised training data. By prepending natural language descriptions of the constraints and a few demonstrations, we fine-tune a pre-trained language model to incorporate various types of constraints. Compared to existing search-based or score-based methods, InstructCT
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#19978;&#19979;&#25991;&#23884;&#20837;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#24212;&#29992;&#20110;&#35821;&#20041;&#26694;&#26550;&#24863;&#30693;&#65292;&#23454;&#29616;&#35821;&#20041;&#26694;&#26550;&#24863;&#30693;&#65292;&#24182;&#22312;FrameNet&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#25928;&#26524;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.14286</link><description>&lt;p&gt;
&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#19979;&#30340;&#35821;&#20041;&#26694;&#26550;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Semantic Frame Induction with Deep Metric Learning. (arXiv:2304.14286v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#19978;&#19979;&#25991;&#23884;&#20837;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#24212;&#29992;&#20110;&#35821;&#20041;&#26694;&#26550;&#24863;&#30693;&#65292;&#23454;&#29616;&#35821;&#20041;&#26694;&#26550;&#24863;&#30693;&#65292;&#24182;&#22312;FrameNet&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#25928;&#26524;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#19978;&#19979;&#25991;&#21270;&#35789;&#23884;&#20837;&#22312;&#26080;&#30417;&#30563;&#35821;&#20041;&#26694;&#26550;&#24863;&#30693;&#20013;&#26159;&#26377;&#29992;&#30340;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20063;&#25581;&#31034;&#20102;&#36890;&#29992;&#19978;&#19979;&#25991;&#21270;&#23884;&#20837;&#24182;&#19981;&#24635;&#26159;&#31526;&#21512;&#20154;&#31867;&#23545;&#35821;&#20041;&#26694;&#26550;&#30340;&#30452;&#35273;&#65292;&#36825;&#23548;&#33268;&#22522;&#20110;&#19978;&#19979;&#25991;&#21270;&#23884;&#20837;&#30340;&#35821;&#20041;&#26694;&#26550;&#24863;&#30693;&#24615;&#33021;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#35821;&#20041;&#24103;&#26631;&#27880;&#25968;&#25454;&#23545;&#19978;&#19979;&#25991;&#21270;&#23884;&#20837;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#23558;&#24494;&#35843;&#21518;&#30340;&#19978;&#19979;&#25991;&#21270;&#23884;&#20837;&#24212;&#29992;&#20110;&#35821;&#20041;&#26694;&#26550;&#24863;&#30693;&#12290;&#22312;FrameNet&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#30340;&#24494;&#35843;&#26174;&#33879;&#25552;&#39640;&#20102;&#32858;&#31867;&#35780;&#20272;&#20998;&#25968;&#65292;&#21363;B-cubed F&#20998;&#25968;&#21644;Purity F&#20998;&#25968;&#65292;&#25552;&#39640;&#20102;8&#20010;&#28857;&#20197;&#19978;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#21253;&#25324;&#20351;&#29992;&#19978;&#19979;&#25991;&#21270;&#23884;&#20837;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#22312;&#20869;&#30340;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#35821;&#20041;&#26694;&#26550;&#24863;&#30693;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have demonstrated the usefulness of contextualized word embeddings in unsupervised semantic frame induction. However, they have also revealed that generic contextualized embeddings are not always consistent with human intuitions about semantic frames, which causes unsatisfactory performance for frame induction based on contextualized embeddings. In this paper, we address supervised semantic frame induction, which assumes the existence of frame-annotated data for a subset of predicates in a corpus and aims to build a frame induction model that leverages the annotated data. We propose a model that uses deep metric learning to fine-tune a contextualized embedding model, and we apply the fine-tuned contextualized embeddings to perform semantic frame induction. Our experiments on FrameNet show that fine-tuning with deep metric learning considerably improves the clustering evaluation scores, namely, the B-cubed F-score and Purity F-score, by about 8 points or more. We also dem
&lt;/p&gt;</description></item><item><title>ChatGPT&#29983;&#25104;&#30340;&#25991;&#31456;&#36136;&#37327;&#27604;&#20154;&#31867;&#20889;&#20316;&#30340;&#25991;&#31456;&#26356;&#39640;&#65292;&#20889;&#20316;&#39118;&#26684;&#26356;&#27969;&#30021;&#65292;&#35821;&#27861;&#21644;&#25340;&#20889;&#38169;&#35823;&#26356;&#23569;&#12290;</title><link>http://arxiv.org/abs/2304.14276</link><description>&lt;p&gt;
AI&#65292;&#20026;&#25105;&#20889;&#19968;&#31687;&#25991;&#31456;&#65306;&#20154;&#31867;&#20889;&#20316;&#19982;ChatGPT&#29983;&#25104;&#25991;&#31456;&#30340;&#22823;&#35268;&#27169;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
AI, write an essay for me: A large-scale comparison of human-written versus ChatGPT-generated essays. (arXiv:2304.14276v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14276
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#29983;&#25104;&#30340;&#25991;&#31456;&#36136;&#37327;&#27604;&#20154;&#31867;&#20889;&#20316;&#30340;&#25991;&#31456;&#26356;&#39640;&#65292;&#20889;&#20316;&#39118;&#26684;&#26356;&#27969;&#30021;&#65292;&#35821;&#27861;&#21644;&#25340;&#20889;&#38169;&#35823;&#26356;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#26368;&#36817;&#65292;ChatGPT&#21450;&#31867;&#20284;&#30340;AI&#29983;&#25104;&#27169;&#22411;&#21560;&#24341;&#20102;&#25968;&#20159;&#29992;&#25143;&#65292;&#25104;&#20026;&#20844;&#20247;&#35805;&#39064;&#30340;&#19968;&#37096;&#20998;&#12290;&#35768;&#22810;&#20154;&#35748;&#20026;&#36825;&#26679;&#30340;&#27169;&#22411;&#23558;&#20250;&#25171;&#20081;&#31038;&#20250;&#65292;&#23548;&#33268;&#26410;&#26469;&#25945;&#32946;&#31995;&#32479;&#21644;&#20449;&#24687;&#29983;&#25104;&#30340;&#26174;&#33879;&#21464;&#21270;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#36825;&#31181;&#20449;&#24565;&#22522;&#20110;&#30340;&#26159;&#21475;&#22836;&#35777;&#25454;&#25110;&#27169;&#22411;&#25317;&#26377;&#32773;&#30340;&#22522;&#20934;&#8212;&#8212;&#36825;&#20004;&#32773;&#32570;&#20047;&#31185;&#23398;&#20005;&#35880;&#24615;&#12290;&#30446;&#26631;&#65306;&#36890;&#36807;&#22823;&#35268;&#27169;&#30740;&#31350;&#27604;&#36739;&#20154;&#31867;&#20889;&#20316;&#21644;ChatGPT&#29983;&#25104;&#35770;&#35777;&#24615;&#23398;&#29983;&#25991;&#31456;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#36136;&#37327;&#12290;&#26041;&#27861;&#65306;&#19968;&#22823;&#25209;&#25991;&#31456;&#35821;&#26009;&#24211;&#34987;&#35768;&#22810;&#20154;&#31867;&#19987;&#23478;&#65288;&#25945;&#24072;&#65289;&#20351;&#29992;&#26631;&#20934;&#26631;&#20934;&#35780;&#20998;&#12290;&#25105;&#20204;&#22312;&#20998;&#26512;&#20013;&#21152;&#20837;&#20102;&#23545;&#29983;&#25104;&#25991;&#31456;&#30340;&#35821;&#35328;&#29305;&#24449;&#30340;&#32771;&#34385;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#29983;&#25104;&#30340;&#25991;&#31456;&#27604;&#20154;&#31867;&#20889;&#20316;&#30340;&#25991;&#31456;&#36136;&#37327;&#26356;&#39640;&#12290;AI&#30340;&#20889;&#20316;&#39118;&#26684;&#26356;&#21152;&#27969;&#30021;&#65292;&#32780;&#19988;&#27604;&#20154;&#31867;&#20889;&#20316;&#20986;&#29616;&#26356;&#23569;&#30340;&#35821;&#27861;&#21644;&#25340;&#20889;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: Recently, ChatGPT and similar generative AI models have attracted hundreds of millions of users and become part of the public discourse. Many believe that such models will disrupt society and will result in a significant change in the education system and information generation in the future. So far, this belief is based on either colloquial evidence or benchmarks from the owners of the models -- both lack scientific rigour.  Objective: Through a large-scale study comparing human-written versus ChatGPT-generated argumentative student essays, we systematically assess the quality of the AI-generated content.  Methods: A large corpus of essays was rated using standard criteria by a large number of human experts (teachers). We augment the analysis with a consideration of the linguistic characteristics of the generated essays.  Results: Our results demonstrate that ChatGPT generates essays that are rated higher for quality than human-written essays. The writing style of the AI m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#65292;&#35745;&#31639;&#26426;&#36741;&#21161;&#35774;&#35745;&#65288;CAD&#65289;&#36719;&#20214;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#21517;&#31216;&#26159;&#37096;&#20214;&#20851;&#32852;&#30340;&#23453;&#36149;&#26469;&#28304;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#22788;&#29702;&#36825;&#31181;&#25968;&#25454;&#30340;&#26377;&#29992;&#30340;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#21517;&#31216;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#22312;&#19977;&#20010;&#33258;&#30417;&#30563;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24494;&#35843;&#36824;&#21487;&#20197;&#25552;&#39640;&#25152;&#26377;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#25552;&#39640;&#20102;&#25991;&#26412;&#25968;&#25454;&#30340;&#20215;&#20540;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#25163;&#21160;&#27880;&#37322;&#30340;&#26032;&#25968;&#25454;&#38598; CAD-120&#65292;&#20854;&#20013;&#21253;&#21547; 120 &#20010;&#35013;&#37197;&#65292;&#24182;&#25552;&#20379;&#20102;&#35821;&#20041;&#20851;&#31995;&#27880;&#37322;&#12290;</title><link>http://arxiv.org/abs/2304.14275</link><description>&lt;p&gt;
&#21517;&#23383;&#30340;&#24847;&#20041;&#65306;&#36890;&#36807; CAD &#25991;&#20214;&#20013;&#29992;&#25143;&#25552;&#20379;&#30340;&#21517;&#31216;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35013;&#37197; - &#38646;&#20214;&#35821;&#20041;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
What's in a Name? Evaluating Assembly-Part Semantic Knowledge in Language Models through User-Provided Names in CAD Files. (arXiv:2304.14275v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#65292;&#35745;&#31639;&#26426;&#36741;&#21161;&#35774;&#35745;&#65288;CAD&#65289;&#36719;&#20214;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#21517;&#31216;&#26159;&#37096;&#20214;&#20851;&#32852;&#30340;&#23453;&#36149;&#26469;&#28304;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#22788;&#29702;&#36825;&#31181;&#25968;&#25454;&#30340;&#26377;&#29992;&#30340;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#21517;&#31216;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#22312;&#19977;&#20010;&#33258;&#30417;&#30563;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24494;&#35843;&#36824;&#21487;&#20197;&#25552;&#39640;&#25152;&#26377;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#25552;&#39640;&#20102;&#25991;&#26412;&#25968;&#25454;&#30340;&#20215;&#20540;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#25163;&#21160;&#27880;&#37322;&#30340;&#26032;&#25968;&#25454;&#38598; CAD-120&#65292;&#20854;&#20013;&#21253;&#21547; 120 &#20010;&#35013;&#37197;&#65292;&#24182;&#25552;&#20379;&#20102;&#35821;&#20041;&#20851;&#31995;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35013;&#37197;&#20013;&#38646;&#20214;&#20043;&#38388;&#21644;&#38646;&#20214;&#19982;&#25972;&#20307;&#20043;&#38388;&#30340;&#35821;&#20041;&#30693;&#35782;&#23545;&#20110;&#20174;&#25628;&#32034;&#35774;&#35745;&#23384;&#20648;&#24211;&#21040;&#26500;&#24314;&#24037;&#31243;&#30693;&#35782;&#24211;&#31561;&#21508;&#31181;&#20219;&#21153;&#37117;&#38750;&#24120;&#26377;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#65292;&#35774;&#35745;&#24072;&#22312;&#35745;&#31639;&#26426;&#36741;&#21161;&#35774;&#35745; (CAD) &#36719;&#20214;&#20013;&#20351;&#29992;&#30340;&#33258;&#28982;&#35821;&#35328;&#21517;&#31216;&#26159;&#36825;&#31181;&#30693;&#35782;&#23453;&#36149;&#30340;&#26469;&#28304;&#65292;&#24182;&#19988;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#21253;&#21547;&#20102;&#29992;&#20110;&#22788;&#29702;&#27492;&#25968;&#25454;&#20197;&#21450;&#20854;&#20182; CAD &#21644;&#24037;&#31243;&#30456;&#20851;&#20219;&#21153;&#30340;&#26377;&#29992;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#21462;&#24182;&#28165;&#29702;&#20102;&#22823;&#37327;&#30340;&#33258;&#28982;&#35821;&#35328;&#38646;&#20214;&#12289;&#29305;&#24449;&#21644;&#25991;&#26723;&#21517;&#31216;&#35821;&#26009;&#24211;&#65292;&#24182;&#20351;&#29992;&#23427;&#26469;&#23450;&#37327;&#35777;&#26126;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#19977;&#20010;&#33258;&#30417;&#30563;&#20219;&#21153;&#19978;&#20248;&#20110;&#20247;&#22810;&#22522;&#20934;&#27979;&#35797;&#65292;&#32780;&#19988;&#20174;&#26410;&#35265;&#36807;&#36825;&#20123;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#25991;&#26412;&#25968;&#25454;&#35821;&#26009;&#24211;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#25152;&#26377;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#23637;&#31034;&#20102;&#36804;&#20170;&#20026;&#27490;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#34987;&#24573;&#30053;&#30340;&#25991;&#26412;&#25968;&#25454;&#30340;&#20215;&#20540;&#12290;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#36825;&#20010;&#39046;&#22495;&#38656;&#35201;&#26356;&#22810;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; CAD-120 &#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547; 120 &#20010; CAD &#35013;&#37197;&#20214;&#65292;&#20855;&#26377;&#25163;&#21160;&#27880;&#37322;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic knowledge of part-part and part-whole relationships in assemblies is useful for a variety of tasks from searching design repositories to the construction of engineering knowledge bases. In this work we propose that the natural language names designers use in Computer Aided Design (CAD) software are a valuable source of such knowledge, and that Large Language Models (LLMs) contain useful domain-specific information for working with this data as well as other CAD and engineering-related tasks.  In particular we extract and clean a large corpus of natural language part, feature and document names and use this to quantitatively demonstrate that a pre-trained language model can outperform numerous benchmarks on three self-supervised tasks, without ever having seen this data before. Moreover, we show that fine-tuning on the text data corpus further boosts the performance on all tasks, thus demonstrating the value of the text data which until now has been largely ignored. We also ide
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#23454;&#20307;&#32423;&#24773;&#24863;&#20998;&#26512;&#65288;ELSA&#65289;&#30340;&#20219;&#21153;&#65292;&#21363;&#35782;&#21035;&#25991;&#26723;&#20013;&#33258;&#24895;&#23454;&#20307;&#65288;&#20154;&#21644;&#32452;&#32455;&#65289;&#25152;&#34920;&#36798;&#24635;&#20307;&#24773;&#32490;&#30340;&#33021;&#21147;&#12290;&#34429;&#28982;&#23545;&#20110;&#20687;&#25512;&#25991;&#36825;&#26679;&#30340;&#36739;&#30701;&#25991;&#26412;&#35782;&#21035;&#23545;&#23454;&#20307;&#34920;&#36798;&#24773;&#24863;&#24050;&#32463;&#26377;&#20102;&#36739;&#22810;&#30740;&#31350;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#23545;&#20110;&#22312;&#25991;&#26412;&#20013;&#22810;&#27425;&#25552;&#21450;&#21516;&#19968;&#23454;&#20307;&#30340;&#36739;&#38271;&#25991;&#26412;&#65292;&#20960;&#20046;&#27809;&#26377;&#30456;&#20851;&#30740;&#31350;&#12290;&#20316;&#32773;&#27880;&#37322;&#20102;&#19968;&#32452;&#19987;&#19994;&#35780;&#35770;&#20197;&#35780;&#20272;&#29616;&#26377;&#20219;&#21153;&#21644;&#27169;&#22411;&#26159;&#21542;&#21487;&#20197;&#25512;&#23548;&#20986;ELSA&#65292;&#32467;&#26524;&#34920;&#26126;&#27809;&#26377;&#21333;&#19968;&#20195;&#29702;&#20219;&#21153;&#33021;&#22815;&#20197;&#20196;&#20154;&#28385;&#24847;&#30340;&#34920;&#29616;&#25552;&#20379;&#36825;&#31181;&#25105;&#20204;&#23547;&#27714;&#30340;&#23545;&#23454;&#20307;&#30340;&#24635;&#20307;&#24773;&#24863;&#12290;</title><link>http://arxiv.org/abs/2304.14241</link><description>&lt;p&gt;
&#23454;&#20307;&#32423;&#24773;&#24863;&#20998;&#26512;&#65288;ELSA&#65289;&#65306;&#19968;&#39033;&#25506;&#32034;&#24615;&#20219;&#21153;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
Entity-Level Sentiment Analysis (ELSA): An exploratory task survey. (arXiv:2304.14241v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#23454;&#20307;&#32423;&#24773;&#24863;&#20998;&#26512;&#65288;ELSA&#65289;&#30340;&#20219;&#21153;&#65292;&#21363;&#35782;&#21035;&#25991;&#26723;&#20013;&#33258;&#24895;&#23454;&#20307;&#65288;&#20154;&#21644;&#32452;&#32455;&#65289;&#25152;&#34920;&#36798;&#24635;&#20307;&#24773;&#32490;&#30340;&#33021;&#21147;&#12290;&#34429;&#28982;&#23545;&#20110;&#20687;&#25512;&#25991;&#36825;&#26679;&#30340;&#36739;&#30701;&#25991;&#26412;&#35782;&#21035;&#23545;&#23454;&#20307;&#34920;&#36798;&#24773;&#24863;&#24050;&#32463;&#26377;&#20102;&#36739;&#22810;&#30740;&#31350;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#23545;&#20110;&#22312;&#25991;&#26412;&#20013;&#22810;&#27425;&#25552;&#21450;&#21516;&#19968;&#23454;&#20307;&#30340;&#36739;&#38271;&#25991;&#26412;&#65292;&#20960;&#20046;&#27809;&#26377;&#30456;&#20851;&#30740;&#31350;&#12290;&#20316;&#32773;&#27880;&#37322;&#20102;&#19968;&#32452;&#19987;&#19994;&#35780;&#35770;&#20197;&#35780;&#20272;&#29616;&#26377;&#20219;&#21153;&#21644;&#27169;&#22411;&#26159;&#21542;&#21487;&#20197;&#25512;&#23548;&#20986;ELSA&#65292;&#32467;&#26524;&#34920;&#26126;&#27809;&#26377;&#21333;&#19968;&#20195;&#29702;&#20219;&#21153;&#33021;&#22815;&#20197;&#20196;&#20154;&#28385;&#24847;&#30340;&#34920;&#29616;&#25552;&#20379;&#36825;&#31181;&#25105;&#20204;&#23547;&#27714;&#30340;&#23545;&#23454;&#20307;&#30340;&#24635;&#20307;&#24773;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#35782;&#21035;&#25991;&#26723;&#20013;&#33258;&#24895;&#23454;&#20307;&#65288;&#20154;&#21644;&#32452;&#32455;&#65289;&#25152;&#34920;&#36798;&#24635;&#20307;&#24773;&#32490;&#30340;&#20219;&#21153;&#65292;&#21363;&#25105;&#20204;&#25152;&#31216;&#30340;&#23454;&#20307;&#32423;&#24773;&#24863;&#20998;&#26512;&#65288;ELSA&#65289;&#12290;&#34429;&#28982;&#23545;&#20110;&#20687;&#25512;&#25991;&#36825;&#26679;&#30340;&#36739;&#30701;&#25991;&#26412;&#35782;&#21035;&#23545;&#23454;&#20307;&#34920;&#36798;&#24773;&#24863;&#24050;&#32463;&#26377;&#20102;&#36739;&#22810;&#30740;&#31350;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#23545;&#20110;&#22312;&#25991;&#26412;&#20013;&#22810;&#27425;&#25552;&#21450;&#21516;&#19968;&#23454;&#20307;&#30340;&#36739;&#38271;&#25991;&#26412;&#65292;&#20960;&#20046;&#27809;&#26377;&#30456;&#20851;&#30740;&#31350;&#12290;&#22914;&#26524;&#29616;&#26377;&#20219;&#21153;&#21644;&#27169;&#22411;&#21487;&#20197;&#25512;&#23548;&#20986;ELSA&#65292;&#36825;&#31181;&#32570;&#20047;&#30740;&#31350;&#26159;&#21487;&#20197;&#29702;&#35299;&#30340;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#27880;&#37322;&#20102;&#19968;&#32452;&#19987;&#19994;&#35780;&#35770;&#20197;&#20102;&#35299;&#25991;&#26412;&#20013;&#27599;&#20010;&#33258;&#24895;&#23454;&#20307;&#30340;&#24635;&#20307;&#24773;&#24863;&#12290;&#25105;&#20204;&#20174;&#19968;&#20010;&#22810;&#39046;&#22495;&#35780;&#35770;&#35821;&#26009;&#24211;&#20013;&#24050;&#32463;&#27880;&#37322;&#20026;&#25991;&#26723;&#32423;&#12289;&#21477;&#23376;&#32423;&#21644;&#30446;&#26631;&#32423;&#24773;&#24863;&#30340;&#25968;&#25454;&#20013;&#36827;&#34892;&#37319;&#26679;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#27809;&#26377;&#19968;&#20010;&#21333;&#19968;&#30340;&#20195;&#29702;&#20219;&#21153;&#33021;&#22815;&#20197;&#20196;&#20154;&#28385;&#24847;&#30340;&#34920;&#29616;&#25552;&#20379;&#36825;&#31181;&#25105;&#20204;&#23547;&#27714;&#30340;&#23545;&#23454;&#20307;&#30340;&#24635;&#20307;&#24773;&#24863;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#22871;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the task of identifying the overall sentiment expressed towards volitional entities (persons and organizations) in a document -- what we refer to as Entity-Level Sentiment Analysis (ELSA). While identifying sentiment conveyed towards an entity is well researched for shorter texts like tweets, we find little to no research on this specific task for longer texts with multiple mentions and opinions towards the same entity. This lack of research would be understandable if ELSA can be derived from existing tasks and models. To assess this, we annotate a set of professional reviews for their overall sentiment towards each volitional entity in the text. We sample from data already annotated for document-level, sentence-level, and target-level sentiment in a multi-domain review corpus, and our results indicate that there is no single proxy task that provides this overall sentiment we seek for the entities at a satisfactory level of performance. We present a suite of experim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;100&#31687;&#39640;&#34987;&#24341;&#35770;&#25991;&#65292;&#21457;&#29616;&#33258;&#21160;&#21270;&#20107;&#23454;&#26680;&#26597;&#24037;&#20855;&#30340;&#20351;&#29992;&#24448;&#24448;&#32570;&#20047;&#20805;&#20998;&#35752;&#35770;&#12290;&#25991;&#31456;&#25552;&#20986;&#32570;&#20047;&#20805;&#20998;&#35752;&#35770;&#30340;&#29616;&#35937;&#40723;&#21169;&#20102;&#36807;&#24230;&#23459;&#20256;&#65292;&#38480;&#21046;&#20102;&#25209;&#35780;&#24182;&#38459;&#27490;&#20102;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#21453;&#39304;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#20851;&#20110;&#20107;&#23454;&#26680;&#26597;&#24037;&#20855;&#20351;&#29992;&#30340;&#20960;&#39033;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2304.14238</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#20107;&#23454;&#26680;&#26597;&#24037;&#20855;&#30340;&#39044;&#26399;&#29992;&#36884;&#65306;&#20026;&#20160;&#20040;&#12289;&#22914;&#20309;&#20197;&#21450;&#35841;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Intended Uses of Automated Fact-Checking Artefacts: Why, How and Who. (arXiv:2304.14238v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;100&#31687;&#39640;&#34987;&#24341;&#35770;&#25991;&#65292;&#21457;&#29616;&#33258;&#21160;&#21270;&#20107;&#23454;&#26680;&#26597;&#24037;&#20855;&#30340;&#20351;&#29992;&#24448;&#24448;&#32570;&#20047;&#20805;&#20998;&#35752;&#35770;&#12290;&#25991;&#31456;&#25552;&#20986;&#32570;&#20047;&#20805;&#20998;&#35752;&#35770;&#30340;&#29616;&#35937;&#40723;&#21169;&#20102;&#36807;&#24230;&#23459;&#20256;&#65292;&#38480;&#21046;&#20102;&#25209;&#35780;&#24182;&#38459;&#27490;&#20102;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#21453;&#39304;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#20851;&#20110;&#20107;&#23454;&#26680;&#26597;&#24037;&#20855;&#20351;&#29992;&#30340;&#20960;&#39033;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#32463;&#24120;&#34987;&#21576;&#29616;&#20026;&#19968;&#20010;&#30693;&#35782;&#24037;&#20855;&#65292;&#20107;&#23454;&#26680;&#26597;&#21592;&#12289;&#31038;&#20132;&#23186;&#20307;&#28040;&#36153;&#32773;&#21644;&#20854;&#20182;&#21033;&#30410;&#30456;&#20851;&#32773;&#21487;&#20197;&#20351;&#29992;&#23427;&#26469;&#25171;&#20987;&#38169;&#35823;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#35770;&#25991;&#28145;&#20837;&#35752;&#35770;&#22914;&#20309;&#20351;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;100&#31687;&#39640;&#34987;&#24341;&#29992;&#35770;&#25991;&#24182;&#27880;&#37322;&#19982;&#39044;&#26399;&#20351;&#29992;&#30456;&#20851;&#30340;&#35748;&#30693;&#20803;&#32032;&#65288;&#21363;&#25163;&#27573;&#12289;&#30446;&#30340;&#21644;&#21033;&#30410;&#30456;&#20851;&#32773;&#65289;&#26469;&#35760;&#24405;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24573;&#30053;&#20854;&#20013;&#19968;&#20123;&#26041;&#38754;&#30340;&#21465;&#20107;&#24456;&#26222;&#36941;&#65292;&#35768;&#22810;&#35770;&#25991;&#25552;&#20986;&#30340;&#25163;&#27573;&#21644;&#30446;&#30340;&#19981;&#19968;&#33268;&#65292;&#24182;&#19988;&#25512;&#33616;&#31574;&#30053;&#30340;&#21487;&#34892;&#24615;&#24456;&#23569;&#20855;&#26377;&#23454;&#35777;&#25903;&#25345;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#27169;&#31946;&#24615;&#20027;&#21160;&#38459;&#30861;&#20102;&#25216;&#26415;&#23454;&#29616;&#20854;&#30446;&#26631;&#65292;&#22240;&#20026;&#23427;&#40723;&#21169;&#20102;&#36807;&#24230;&#23459;&#20256;&#65292;&#38480;&#21046;&#20102;&#25209;&#35780;&#24182;&#38459;&#27490;&#20102;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#21453;&#39304;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#39033;&#20851;&#20110;&#24605;&#32771;&#21644;&#25776;&#20889;&#20107;&#23454;&#26680;&#26597;&#24037;&#20855;&#20351;&#29992;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated fact-checking is often presented as an epistemic tool that fact-checkers, social media consumers, and other stakeholders can use to fight misinformation. Nevertheless, few papers thoroughly discuss how. We document this by analysing 100 highly-cited papers, and annotating epistemic elements related to intended use, i.e., means, ends, and stakeholders. We find that narratives leaving out some of these aspects are common, that many papers propose inconsistent means and ends, and that the feasibility of suggested strategies rarely has empirical backing. We argue that this vagueness actively hinders the technology from reaching its goals, as it encourages overclaiming, limits criticism, and prevents stakeholder feedback. Accordingly, we provide several recommendations for thinking and writing about the use of fact-checking artefacts.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38646;-shot&#22330;&#26223;&#19979;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#22823;&#35268;&#27169;&#26816;&#32034;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#26597;&#35810;&#21644;&#26597;&#35810;&#30340;&#20505;&#36873;&#31572;&#26696;&#30340;&#32452;&#21512;&#20316;&#20026;&#25552;&#31034;&#65292;&#20351;LLM&#29983;&#25104;&#26356;&#31934;&#30830;&#30340;&#31572;&#26696;&#12290;&#30001;&#20110;&#33258;&#30417;&#30563;&#26816;&#32034;&#22120;&#22312;&#38646;-shot&#22330;&#26223;&#20013;&#24615;&#33021;&#36739;&#24046;&#65292;&#22240;&#27492;LameR&#20248;&#20110;&#33258;&#30417;&#30563;&#26816;&#32034;&#22120;&#12290;</title><link>http://arxiv.org/abs/2304.14233</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#26816;&#32034;&#20013;&#20855;&#26377;&#36739;&#24378;&#30340;&#34920;&#29616;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Strong Zero-Shot Retriever. (arXiv:2304.14233v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38646;-shot&#22330;&#26223;&#19979;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#22823;&#35268;&#27169;&#26816;&#32034;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#26597;&#35810;&#21644;&#26597;&#35810;&#30340;&#20505;&#36873;&#31572;&#26696;&#30340;&#32452;&#21512;&#20316;&#20026;&#25552;&#31034;&#65292;&#20351;LLM&#29983;&#25104;&#26356;&#31934;&#30830;&#30340;&#31572;&#26696;&#12290;&#30001;&#20110;&#33258;&#30417;&#30563;&#26816;&#32034;&#22120;&#22312;&#38646;-shot&#22330;&#26223;&#20013;&#24615;&#33021;&#36739;&#24046;&#65292;&#22240;&#27492;LameR&#20248;&#20110;&#33258;&#30417;&#30563;&#26816;&#32034;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#22312;&#38646;-shot&#22330;&#26223;&#19979;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#22823;&#35268;&#27169;&#26816;&#32034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;Language Model&#20316;&#20026;&#26816;&#32034;&#22120;&#65288;LameR&#65289;&#20165;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#32780;&#19981;&#26159;&#20854;&#20182;&#31070;&#32463;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;LLM&#19982;&#26816;&#32034;&#22120;&#30340;&#26292;&#21147;&#32452;&#21512;&#36827;&#34892;&#20998;&#35299;&#65292;&#23558;&#38646;-shot&#26816;&#32034;&#30340;&#24615;&#33021;&#25552;&#39640;&#21040;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#24456;&#24378;&#30340;&#31454;&#20105;&#21147;&#12290;&#26412;&#25991;&#20027;&#35201;&#25552;&#20986;&#36890;&#36807;&#20351;&#29992;&#26597;&#35810;&#21644;&#26597;&#35810;&#30340;&#20505;&#36873;&#31572;&#26696;&#30340;&#32452;&#21512;&#20316;&#20026;&#25552;&#31034;&#65292;&#20351;LLM&#29983;&#25104;&#26356;&#31934;&#30830;&#30340;&#31572;&#26696;&#12290;&#26080;&#35770;&#20505;&#36873;&#31572;&#26696;&#26159;&#21542;&#27491;&#30830;&#65292;&#37117;&#21487;&#20197;&#36890;&#36807;&#27169;&#24335;&#27169;&#20223;&#25110;&#20505;&#36873;&#25688;&#35201;&#26469;&#24110;&#21161;LLM&#20135;&#29983;&#26356;&#31934;&#30830;&#30340;&#31572;&#26696;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#33258;&#30417;&#30563;&#26816;&#32034;&#22120;&#22312;&#38646;-shot&#22330;&#26223;&#20013;&#24615;&#33021;&#36739;&#24046;&#65292;&#22240;&#27492;&#36890;&#36807;&#21033;&#29992;LLM&#23545;&#25991;&#26412;&#27169;&#24335;&#30340;&#24378;&#22823;&#34920;&#29616;&#33021;&#21147;&#65292;LameR&#21487;&#20197;&#20248;&#20110;&#33258;&#30417;&#30563;&#26816;&#32034;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a simple method that applies a large language model (LLM) to large-scale retrieval in zero-shot scenarios. Our method, Language language model as Retriever (LameR) is built upon no other neural models but an LLM, while breaking up brute-force combinations of retrievers with LLMs and lifting the performance of zero-shot retrieval to be very competitive on benchmark datasets. Essentially, we propose to augment a query with its potential answers by prompting LLMs with a composition of the query and the query's in-domain candidates. The candidates, regardless of correct or wrong, are obtained by a vanilla retrieval procedure on the target collection. Such candidates, as a part of prompts, are likely to help LLM generate more precise answers by pattern imitation or candidate summarization. Even if all the candidates are wrong, the prompts at least make LLM aware of in-collection patterns and genres. Moreover, due to the low performance of a self-supervised retriever
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#30340;&#22810;&#35821;&#35328;&#26102;&#38388;&#22788;&#29702;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#32467;&#21512;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26816;&#27979;&#26041;&#27861;&#21644;&#22522;&#20110;&#25991;&#27861;&#30340;&#35268;&#33539;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#36827;&#34892;&#26102;&#24577;&#34920;&#36798;&#24335;&#26816;&#27979;&#21644;&#35268;&#33539;&#21270;&#65292;&#24182;&#22312;&#37329;&#26631;&#20934; timex &#35268;&#33539;&#21270;&#12289;timex &#26816;&#27979;&#21644;&#31867;&#22411;&#35782;&#21035;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.14221</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#22522;&#20110;&#25991;&#27861;&#26041;&#27861;&#30340;&#22810;&#35821;&#35328;&#26102;&#24577;&#34920;&#36798;&#24335;&#26816;&#27979;&#21644;&#35268;&#33539;&#21270;&#30340;&#27169;&#22359;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Modular Approach for Multilingual Timex Detection and Normalization using Deep Learning and Grammar-based methods. (arXiv:2304.14221v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14221
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#30340;&#22810;&#35821;&#35328;&#26102;&#38388;&#22788;&#29702;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#32467;&#21512;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26816;&#27979;&#26041;&#27861;&#21644;&#22522;&#20110;&#25991;&#27861;&#30340;&#35268;&#33539;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#36827;&#34892;&#26102;&#24577;&#34920;&#36798;&#24335;&#26816;&#27979;&#21644;&#35268;&#33539;&#21270;&#65292;&#24182;&#22312;&#37329;&#26631;&#20934; timex &#35268;&#33539;&#21270;&#12289;timex &#26816;&#27979;&#21644;&#31867;&#22411;&#35782;&#21035;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#21644;&#35268;&#33539;&#21270;&#26102;&#38388;&#34920;&#36798;&#24335;&#26159;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#24517;&#19981;&#21487;&#23569;&#30340;&#27493;&#39588;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26816;&#27979;&#26041;&#27861;&#65292;&#20294;&#26368;&#20339;&#35268;&#33539;&#21270;&#26041;&#27861;&#20173;&#28982;&#20381;&#36182;&#20110;&#25163;&#24037;&#21046;&#23450;&#30340;&#35268;&#21017;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#22823;&#22810;&#25968;&#20165;&#38024;&#23545;&#33521;&#35821;&#35774;&#35745;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#30340;&#22810;&#35821;&#35328;&#26102;&#38388;&#22788;&#29702;&#31995;&#32479;&#65292;&#32467;&#21512;&#20102;&#22522;&#20110;&#25513;&#34109;&#35821;&#35328;&#27169;&#22411;&#30340;&#26816;&#27979;&#26041;&#27861;&#21644;&#22522;&#20110;&#25991;&#27861;&#30340;&#35268;&#33539;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#35199;&#29677;&#29273;&#35821;&#21644;&#33521;&#35821;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#19982; HeidelTime &#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21518;&#32773;&#26159;&#22810;&#35821;&#35328;&#26102;&#38388;&#22788;&#29702;&#30340;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#12290;&#25105;&#20204;&#22312;&#37329;&#26631;&#20934; timex &#35268;&#33539;&#21270;&#12289;timex &#26816;&#27979;&#21644;&#31867;&#22411;&#35782;&#21035;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#65292;&#24182;&#22312;&#32452;&#21512;&#30340; TempEval-3 &#23485;&#26494;&#24230;&#37327;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#30340;&#34920;&#29616;&#12290;&#35814;&#32454;&#30340;&#35823;&#24046;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#35813;&#25351;&#26631;&#20013;&#20165;&#26816;&#27979;&#21487;&#25552;&#20379;&#35268;&#33539;&#21270;&#30340; timexes &#23545;&#20854;&#26377;&#24456;&#22823;&#30340;&#22909;&#22788;&#12290;&#36825;&#24341;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363; timex &#22788;&#29702;&#30340;&#26368;&#20339;&#31574;&#30053;&#26159;&#20160;&#20040;&#65292;&#21363;&#21482;&#26816;&#27979;&#37027;&#20123;&#21487;&#34892;&#25552;&#20379;&#35268;&#33539;&#21270;&#30340; timexes &#25110;&#20165;&#26816;&#27979;&#37027;&#20123;&#26131;&#20110;&#35268;&#33539;&#21270;&#30340; timexes&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting and normalizing temporal expressions is an essential step for many NLP tasks. While a variety of methods have been proposed for detection, best normalization approaches rely on hand-crafted rules. Furthermore, most of them have been designed only for English. In this paper we present a modular multilingual temporal processing system combining a fine-tuned Masked Language Model for detection, and a grammar-based normalizer. We experiment in Spanish and English and compare with HeidelTime, the state-of-the-art in multilingual temporal processing. We obtain best results in gold timex normalization, timex detection and type recognition, and competitive performance in the combined TempEval-3 relaxed value metric. A detailed error analysis shows that detecting only those timexes for which it is feasible to provide a normalization is highly beneficial in this last metric. This raises the question of which is the best strategy for timex processing, namely, leaving undetected those ti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#36890;&#36807;&#20351;&#29992;&#22810;&#35821;&#35328;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38750;&#27954;&#20302;&#36164;&#28304;&#35821;&#35328;&#24773;&#24863;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#22312;&#25552;&#20379;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#20351;&#29992;&#21333;&#35821;&#24494;&#35843;&#30340;&#26041;&#27861;&#21487;&#20197;&#36798;&#21040;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.14189</link><description>&lt;p&gt;
SemEval-2023 &#20219;&#21153;12&#65306;&#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#24773;&#24863;&#20998;&#31867;&#30340;&#22810;&#35821;&#35328;&#24494;&#35843;&#30340;UIO&#26041;&#27861;(arXiv:2304.14189v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
UIO at SemEval-2023 Task 12: Multilingual fine-tuning for sentiment classification in low-resource languages. (arXiv:2304.14189v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#36890;&#36807;&#20351;&#29992;&#22810;&#35821;&#35328;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38750;&#27954;&#20302;&#36164;&#28304;&#35821;&#35328;&#24773;&#24863;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#22312;&#25552;&#20379;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#20351;&#29992;&#21333;&#35821;&#24494;&#35843;&#30340;&#26041;&#27861;&#21487;&#20197;&#36798;&#21040;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;2023&#24180;AfriSenti-SemEval&#20998;&#20139;&#20219;&#21153;12:&#38750;&#27954;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#36129;&#29486;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#22823;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#25104;&#20026;&#22312;&#38750;&#39044;&#35757;&#32451;&#35821;&#35328;&#19978;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#30340;&#36164;&#28304;&#30340;&#35265;&#35299;&#12290;&#35813;&#20219;&#21153;&#25552;&#20379;&#20102;&#19981;&#21516;&#35821;&#31995;&#30340;&#21508;&#31181;&#38750;&#27954;&#35821;&#35328;&#30340;&#25968;&#25454;&#38598;&#12290;&#36825;&#20123;&#35821;&#35328;&#22312;&#19981;&#21516;&#31243;&#24230;&#19978;&#19982;&#39044;&#35757;&#32451;&#25152;&#20351;&#29992;&#30340;&#35821;&#35328;&#26377;&#20851;&#65292;&#24182;&#19988;&#35821;&#35328;&#25968;&#25454;&#20855;&#26377;&#19981;&#21516;&#31243;&#24230;&#30340;&#20195;&#30721;&#20999;&#25442;&#12290;&#25105;&#20204;&#35797;&#39564;&#20102;&#21333;&#35821;&#21644;&#22810;&#35821;&#35821;&#26009;&#24211;&#36827;&#34892;&#26368;&#32456;&#24494;&#35843;&#65292;&#24182;&#21457;&#29616;&#23545;&#20110;&#21253;&#21547;&#25104;&#21315;&#19978;&#19975;&#20010;&#26679;&#26412;&#30340;&#25552;&#20379;&#25968;&#25454;&#38598;&#65292;&#21333;&#35821;&#24494;&#35843;&#20135;&#29983;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our contribution to the 2023 AfriSenti-SemEval shared task 12: Sentiment Analysis for African Languages, provides insight into how a multilingual large language model can be a resource for sentiment analysis in languages not seen during pretraining. The shared task provides datasets of a variety of African languages from different language families. The languages are to various degrees related to languages used during pretraining, and the language data contain various degrees of code-switching. We experiment with both monolingual and multilingual datasets for the final fine-tuning, and find that with the provided datasets that contain samples in the thousands, monolingual fine-tuning yields the best results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#29992;&#23569;&#37327;&#25968;&#25454;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#26816;&#27979;&#35828;&#26381;&#25216;&#24039;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#21033;&#29992;&#65288;&#22238;&#65289;&#32763;&#35793;&#20316;&#20026;&#25968;&#25454;&#22686;&#24378;&#30340;&#31574;&#30053;&#65292;&#24182;&#35777;&#23454;&#24179;&#34913;&#20154;&#24037;&#21644;&#26426;&#22120;&#29983;&#25104;&#25968;&#25454;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.14179</link><description>&lt;p&gt;
NAP&#22312;SemEval-2023&#20219;&#21153;3&#20013;&#65306;&#23569;&#26159;&#21542;&#30495;&#30340;&#26356;&#22810;&#65311;&#65288;&#22238;&#65289;&#32763;&#35793;&#20316;&#20026;&#26816;&#27979;&#35828;&#26381;&#25216;&#24039;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
NAP at SemEval-2023 Task 3: Is Less Really More? (Back-)Translation as Data Augmentation Strategies for Detecting Persuasion Techniques. (arXiv:2304.14179v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14179
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#29992;&#23569;&#37327;&#25968;&#25454;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#26816;&#27979;&#35828;&#26381;&#25216;&#24039;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#21033;&#29992;&#65288;&#22238;&#65289;&#32763;&#35793;&#20316;&#20026;&#25968;&#25454;&#22686;&#24378;&#30340;&#31574;&#30053;&#65292;&#24182;&#35777;&#23454;&#24179;&#34913;&#20154;&#24037;&#21644;&#26426;&#22120;&#29983;&#25104;&#25968;&#25454;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#26816;&#27979;&#26032;&#38395;&#20013;&#30340;&#35828;&#26381;&#25216;&#24039;&#26159;&#38750;&#24120;&#22256;&#38590;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#20854;&#20013;&#21253;&#25324;&#24456;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#25104;&#21151;&#22320;&#21033;&#29992;&#22810;&#35821;&#35328;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#65288;&#22238;&#65289;&#32763;&#35793;&#20316;&#20026;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#29992;&#20110;&#26816;&#27979;&#35828;&#26381;&#25216;&#24039;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#33258;&#21160;&#21270;&#21644;&#20154;&#24037;&#35780;&#20272;&#20801;&#35768;&#25105;&#20204;&#25506;&#32034;&#65288;&#22238;&#65289;&#32763;&#35793;&#26159;&#21542;&#26377;&#21161;&#20110;&#25110;&#38459;&#30861;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#28145;&#20837;&#20998;&#26512;&#34920;&#26126;&#65292;&#20004;&#31181;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#37117;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65307;&#28982;&#32780;&#65292;&#24179;&#34913;&#20154;&#24037;&#29983;&#20135;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#25968;&#25454;&#20284;&#20046;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Persuasion techniques detection in news in a multi-lingual setup is non-trivial and comes with challenges, including little training data. Our system successfully leverages (back-)translation as data augmentation strategies with multi-lingual transformer models for the task of detecting persuasion techniques. The automatic and human evaluation of our augmented data allows us to explore whether (back-)translation aid or hinder performance. Our in-depth analyses indicate that both data augmentation strategies boost performance; however, balancing human-produced and machine-generated data seems to be crucial.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;mPLUG-Owl&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#23427;&#36890;&#36807;&#27169;&#22359;&#21270;&#23398;&#20064;&#22522;&#30784;LLM&#12289;&#35270;&#35273;&#30693;&#35782;&#27169;&#22359;&#21644;&#35270;&#35273;&#25277;&#35937;&#22120;&#27169;&#22359;&#65292;&#36171;&#20104;LLMs&#22810;&#27169;&#24577;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;mPLUG-Owl&#22312;&#22270;&#20687;&#23383;&#24149;&#21644;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2304.14178</link><description>&lt;p&gt;
mPLUG-Owl: &#27169;&#22359;&#21270;&#22686;&#24378;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality. (arXiv:2304.14178v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14178
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;mPLUG-Owl&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#23427;&#36890;&#36807;&#27169;&#22359;&#21270;&#23398;&#20064;&#22522;&#30784;LLM&#12289;&#35270;&#35273;&#30693;&#35782;&#27169;&#22359;&#21644;&#35270;&#35273;&#25277;&#35937;&#22120;&#27169;&#22359;&#65292;&#36171;&#20104;LLMs&#22810;&#27169;&#24577;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;mPLUG-Owl&#22312;&#22270;&#20687;&#23383;&#24149;&#21644;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#22312;&#21508;&#31181;&#24320;&#25918;&#24335;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38646;-shot&#34920;&#29616;&#65292;&#32780;&#26368;&#36817;&#30340;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#23558;LLMs&#29992;&#20110;&#22810;&#27169;&#24577;&#29983;&#25104;&#30340;&#24212;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#33539;&#24335;mPLUG-Owl&#65292;&#36890;&#36807;&#22522;&#30784;LLM&#12289;&#35270;&#35273;&#30693;&#35782;&#27169;&#22359;&#21644;&#35270;&#35273;&#25277;&#35937;&#22120;&#27169;&#22359;&#30340;&#27169;&#22359;&#21270;&#23398;&#20064;&#65292;&#20351;LLMs&#20855;&#22791;&#20102;&#22810;&#27169;&#24577;&#30340;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#25903;&#25345;&#22810;&#31181;&#27169;&#24577;&#65292;&#24182;&#36890;&#36807;&#27169;&#24577;&#21327;&#20316;&#20419;&#36827;&#20102;&#22810;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#30340;&#33021;&#21147;&#12290;mPLUG-Owl&#30340;&#35757;&#32451;&#33539;&#24335;&#21253;&#25324;&#29992;&#20110;&#23545;&#40784;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;LLM&#30340;&#36741;&#21161;&#23398;&#20064;&#35270;&#35273;&#30693;&#35782;&#65292;&#21516;&#26102;&#20445;&#25345;&#29978;&#33267;&#25913;&#36827;&#20102;LLM&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#20013;&#65292;&#20351;&#29992;&#20923;&#32467;&#30340;LLM&#27169;&#22359;&#23545;&#35270;&#35273;&#30693;&#35782;&#27169;&#22359;&#21644;&#25277;&#35937;&#22120;&#27169;&#22359;&#36827;&#34892;&#35757;&#32451;&#20197;&#23545;&#40784;&#22270;&#20687;&#21644;&#25991;&#26412;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#20013;&#65292;&#20351;&#29992;&#20165;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#30417;&#30563;&#25968;&#25454;&#38598;&#20849;&#21516;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#23545;&#20110;&#22270;&#20687;&#23383;&#24149;&#21644;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;mPLUG-Owl&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated impressive zero-shot abilities on a variety of open-ended tasks, while recent research has also explored the use of LLMs for multi-modal generation. In this study, we introduce mPLUG-Owl, a novel training paradigm that equips LLMs with multi-modal abilities through modularized learning of foundation LLM, a visual knowledge module, and a visual abstractor module. This approach can support multiple modalities and facilitate diverse unimodal and multimodal abilities through modality collaboration. The training paradigm of mPLUG-Owl involves a two-stage method for aligning image and text, which learns visual knowledge with the assistance of LLM while maintaining and even improving the generation abilities of LLM. In the first stage, the visual knowledge module and abstractor module are trained with a frozen LLM module to align the image and text. In the second stage, language-only and multi-modal supervised datasets are used to jointly fine-tu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;ChatGPT&#21644;&#29616;&#26377;&#27169;&#22411;&#22312;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;ChatGPT&#22312;&#25152;&#26377;&#27979;&#35797;&#25968;&#25454;&#38598;&#21644;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#22343;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#21644;&#25991;&#26723;&#38271;&#24230;&#30340;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2304.14177</link><description>&lt;p&gt;
ChatGPT&#19982;&#29616;&#26377;&#27169;&#22411;&#20043;&#38388;&#30340;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#20219;&#21153;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
ChatGPT vs State-of-the-Art Models: A Benchmarking Study in Keyphrase Generation Task. (arXiv:2304.14177v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;ChatGPT&#21644;&#29616;&#26377;&#27169;&#22411;&#22312;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;ChatGPT&#22312;&#25152;&#26377;&#27979;&#35797;&#25968;&#25454;&#38598;&#21644;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#22343;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#21644;&#25991;&#26723;&#38271;&#24230;&#30340;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21253;&#25324;ChatGPT&#65292;&#24050;&#32463;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#22312;&#35780;&#20272;ChatGPT&#30340;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#33021;&#21147;&#26041;&#38754;&#65292;&#36824;&#27809;&#26377;&#22810;&#23569;&#30740;&#31350;&#65292;&#36825;&#28041;&#21450;&#21040;&#20934;&#30830;&#21453;&#26144;&#25991;&#26723;&#20869;&#23481;&#30340;&#20449;&#24687;&#24615;&#30701;&#35821;&#30340;&#35782;&#21035;&#12290;&#26412;&#25991;&#35797;&#22270;&#36890;&#36807;&#23558;ChatGPT&#30340;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#34920;&#29616;&#19982;&#29616;&#26377;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21516;&#26102;&#36824;&#27979;&#35797;&#20102;&#23427;&#20316;&#20026;&#35299;&#20915;&#39046;&#22495;&#36866;&#24212;&#21644;&#38271;&#25991;&#26723;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#20004;&#20010;&#37325;&#22823;&#25361;&#25112;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;&#31185;&#23398;&#25991;&#31456;&#21644;&#26032;&#38395;&#39046;&#22495;&#30340;&#20845;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20998;&#26512;&#20102;&#22312;&#30701;&#25991;&#26723;&#21644;&#38271;&#25991;&#26723;&#19978;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25152;&#26377;&#27979;&#35797;&#30340;&#25968;&#25454;&#38598;&#21644;&#29615;&#22659;&#20013;&#65292;ChatGPT&#30340;&#24615;&#33021;&#20248;&#20110;&#24403;&#21069;&#29616;&#26377;&#27169;&#22411;&#65292;&#20135;&#29983;&#36866;&#24212;&#19981;&#21516;&#39046;&#22495;&#21644;&#25991;&#26723;&#38271;&#24230;&#30340;&#39640;&#36136;&#37327;&#20851;&#38190;&#30701;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based language models, including ChatGPT, have demonstrated exceptional performance in various natural language generation tasks. However, there has been limited research evaluating ChatGPT's keyphrase generation ability, which involves identifying informative phrases that accurately reflect a document's content. This study seeks to address this gap by comparing ChatGPT's keyphrase generation performance with state-of-the-art models, while also testing its potential as a solution for two significant challenges in the field: domain adaptation and keyphrase generation from long documents. We conducted experiments on six publicly available datasets from scientific articles and news domains, analyzing performance on both short and long documents. Our results show that ChatGPT outperforms current state-of-the-art models in all tested datasets and environments, generating high-quality keyphrases that adapt well to diverse domains and document lengths.
&lt;/p&gt;</description></item><item><title>DataComp&#26159;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#35757;&#32451;&#38598;&#26469;&#35299;&#20915;&#25968;&#25454;&#38598;&#22312;&#26426;&#22120;&#23398;&#20064;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#32570;&#38519;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#35268;&#27169;&#35774;&#35745;&#30340;&#23454;&#39564;&#27979;&#35797;&#24179;&#21488;&#65292;&#20351;&#29992;12.8B&#20010;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#26032;&#20505;&#36873;&#27744;&#65292;&#35753;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#36807;&#28388;&#25216;&#26415;&#25110;&#31574;&#21010;&#26032;&#30340;&#25968;&#25454;&#28304;&#24182;&#35780;&#20272;&#23427;&#20204;&#30340;&#26032;&#25968;&#25454;&#38598;&#26469;&#36827;&#34892;&#21019;&#26032;&#12290;</title><link>http://arxiv.org/abs/2304.14108</link><description>&lt;p&gt;
DataComp&#65306;&#23547;&#25214;&#19979;&#19968;&#20195;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DataComp: In search of the next generation of multimodal datasets. (arXiv:2304.14108v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14108
&lt;/p&gt;
&lt;p&gt;
DataComp&#26159;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#35757;&#32451;&#38598;&#26469;&#35299;&#20915;&#25968;&#25454;&#38598;&#22312;&#26426;&#22120;&#23398;&#20064;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#32570;&#38519;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#35268;&#27169;&#35774;&#35745;&#30340;&#23454;&#39564;&#27979;&#35797;&#24179;&#21488;&#65292;&#20351;&#29992;12.8B&#20010;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#26032;&#20505;&#36873;&#27744;&#65292;&#35753;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#36807;&#28388;&#25216;&#26415;&#25110;&#31574;&#21010;&#26032;&#30340;&#25968;&#25454;&#28304;&#24182;&#35780;&#20272;&#23427;&#20204;&#30340;&#26032;&#25968;&#25454;&#38598;&#26469;&#36827;&#34892;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#22312;&#36817;&#26399;&#30340;&#31361;&#30772;&#20013;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#65292;&#27604;&#22914;CLIP&#12289;Stable Diffusion&#21644;GPT-4&#31561;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25968;&#25454;&#38598;&#24456;&#23569;&#24471;&#21040;&#19982;&#27169;&#22411;&#26550;&#26500;&#25110;&#35757;&#32451;&#31639;&#27861;&#21516;&#31561;&#30340;&#30740;&#31350;&#20851;&#27880;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#22312;&#26426;&#22120;&#23398;&#20064;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#32570;&#38519;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DataComp&#65292;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#35757;&#32451;&#20195;&#30721;&#26159;&#22266;&#23450;&#30340;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#35757;&#32451;&#38598;&#26469;&#36827;&#34892;&#21019;&#26032;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;Common Crawl&#30340;&#26032;&#20505;&#36873;&#27744;&#65292;&#20854;&#20013;&#21253;&#21547;12.8B&#20010;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#25968;&#25454;&#38598;&#23454;&#39564;&#27979;&#35797;&#24179;&#21488;&#12290;&#21442;&#21152;&#25105;&#20204;&#22522;&#20934;&#27979;&#35797;&#30340;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#35774;&#35745;&#26032;&#30340;&#36807;&#28388;&#25216;&#26415;&#25110;&#31574;&#21010;&#26032;&#30340;&#25968;&#25454;&#28304;&#65292;&#24182;&#36890;&#36807;&#36816;&#34892;&#25105;&#20204;&#26631;&#20934;&#21270;&#30340;CLIP&#35757;&#32451;&#20195;&#30721;&#24182;&#22312;38&#20010;&#19979;&#28216;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#27979;&#35797;&#26469;&#35780;&#20272;&#20182;&#20204;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;&#22810;&#20010;&#35268;&#27169;&#65292;&#22235;&#20010;&#20505;&#36873;&#27744;&#22823;&#23567;&#21644;&#30456;&#24212;&#30340;&#35745;&#31639;&#39044;&#31639;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#28085;&#30422;&#20102;&#20174;12.8M&#21040;12.8B&#20010;&#26679;&#26412;&#12290;&#36825;&#31181;&#22810;&#35268;&#27169;&#35774;&#35745;&#26377;&#21161;&#20110;&#30740;&#31350;&#35268;&#27169;&#36235;&#21183;&#65292;&#24182;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#26356;&#22810;&#30340;&#36873;&#25321;&#20313;&#22320;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large multimodal datasets have been instrumental in recent breakthroughs such as CLIP, Stable Diffusion, and GPT-4. At the same time, datasets rarely receive the same research attention as model architectures or training algorithms. To address this shortcoming in the machine learning ecosystem, we introduce DataComp, a benchmark where the training code is fixed and researchers innovate by proposing new training sets. We provide a testbed for dataset experiments centered around a new candidate pool of 12.8B image-text pairs from Common Crawl. Participants in our benchmark design new filtering techniques or curate new data sources and then evaluate their new dataset by running our standardized CLIP training code and testing on 38 downstream test sets. Our benchmark consists of multiple scales, with four candidate pool sizes and associated compute budgets ranging from 12.8M to 12.8B samples seen during training. This multi-scale design facilitates the study of scaling trends and makes the
&lt;/p&gt;</description></item><item><title>ChatLog&#26159;&#19968;&#20010;&#20998;&#26512;ChatGPT&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#25552;&#21462;ChatGPT&#30693;&#35782;&#21644;&#35821;&#35328;&#29305;&#24449;&#21457;&#29616;&#20102;&#19968;&#20123;&#31283;&#23450;&#30340;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;RoBERTa-based detector&#22312;&#26032;&#29256;&#26412;ChatGPT&#19978;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.14106</link><description>&lt;p&gt;
ChatLog: &#35760;&#24405;&#21644;&#20998;&#26512;ChatGPT&#38543;&#26102;&#38388;&#30340;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
ChatLog: Recording and Analyzing ChatGPT Across Time. (arXiv:2304.14106v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14106
&lt;/p&gt;
&lt;p&gt;
ChatLog&#26159;&#19968;&#20010;&#20998;&#26512;ChatGPT&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#25552;&#21462;ChatGPT&#30693;&#35782;&#21644;&#35821;&#35328;&#29305;&#24449;&#21457;&#29616;&#20102;&#19968;&#20123;&#31283;&#23450;&#30340;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;RoBERTa-based detector&#22312;&#26032;&#29256;&#26412;ChatGPT&#19978;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26377;&#22823;&#37327;&#20851;&#20110;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#35780;&#20272;ChatGPT&#30340;&#30740;&#31350;&#65292;&#20294;&#40092;&#26377;&#30740;&#31350;&#35843;&#26597;ChatGPT&#30340;&#34892;&#20026;&#22914;&#20309;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#31895;&#21040;&#32454;&#30340;&#26102;&#38388;&#25968;&#25454;&#38598;&#65292;&#31216;&#20026;ChatLog&#65292;&#30001;&#20004;&#20010;&#37096;&#20998;&#32452;&#25104;&#65292;&#27599;&#26376;&#21644;&#27599;&#22825;&#26356;&#26032;&#65306;ChatLog-Monthly&#26159;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#27599;&#20010;&#26376;&#25910;&#38598;&#30340;38,730&#20010;&#38382;&#39064;-&#22238;&#31572;&#23545;&#65292;&#20854;&#20013;&#21253;&#25324;&#25512;&#29702;&#21644;&#20998;&#31867;&#20219;&#21153;&#30340;&#38382;&#39064;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;ChatLog-Daily&#21253;&#25324;ChatGPT&#27599;&#22825;&#23545;1000&#20010;&#30456;&#21516;&#38382;&#39064;&#30340;&#38271;&#31687;&#22238;&#31572;&#12290;&#25105;&#20204;&#36827;&#34892;&#20840;&#38754;&#30340;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#65292;&#20197;&#25552;&#20379;ChatGPT&#36827;&#21270;&#27169;&#24335;&#23384;&#22312;&#30340;&#35777;&#25454;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#25552;&#21462;&#20854;&#30693;&#35782;&#21644;&#35821;&#35328;&#29305;&#24449;&#20998;&#26512;&#20102;ChatGPT&#38543;&#26102;&#38388;&#19981;&#21464;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#21457;&#29616;&#19968;&#20123;&#31283;&#23450;&#30340;&#29305;&#24449;&#65292;&#20197;&#25552;&#39640;&#22522;&#20110;RoBERTa&#30340;&#26816;&#27979;&#22120;&#22312;&#26032;&#29256;&#26412;&#30340;ChatGPT&#19978;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23558;&#32487;&#32493;&#32500;&#25252;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#20197;&#21450;&#38543;&#26102;&#38388;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
While there are abundant researches about evaluating ChatGPT on natural language understanding and generation tasks, few studies have investigated how ChatGPT's behavior changes over time. In this paper, we collect a coarse-to-fine temporal dataset called ChatLog, consisting of two parts that update monthly and daily: ChatLog-Monthly is a dataset of 38,730 question-answer pairs collected every month including questions from both the reasoning and classification tasks. ChatLog-Daily, on the other hand, consists of ChatGPT's responses to 1000 identical questions for long-form generation every day. We conduct comprehensive automatic and human evaluation to provide the evidence for the existence of ChatGPT evolving patterns. We further analyze the unchanged characteristics of ChatGPT over time by extracting its knowledge and linguistic features. We find some stable features to improve the robustness of a RoBERTa-based detector on new versions of ChatGPT. We will continuously maintain our p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#20174;&#21333;&#19968;&#30340;&#38745;&#24577;&#22270;&#20687;&#20013;&#23398;&#20064;&#33258;&#30001;&#25991;&#26412;&#30340;&#24418;&#24335;&#26469;&#28789;&#27963;&#24314;&#27169;&#20154;&#38469;&#20114;&#21160;&#12290;&#24182;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#29983;&#25104;&#20266;&#26631;&#31614;&#26469;&#35757;&#32451;&#19968;&#31181;&#23383;&#24149;&#27169;&#22411;&#65292;&#29992;&#20110;&#26377;&#25928;&#29702;&#35299;&#22270;&#20687;&#20013;&#30340;&#20154;&#38469;&#20114;&#21160;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#25991;&#26412;&#21644;&#35821;&#20041;&#36136;&#37327;&#65292;&#24182;&#22312;&#27492;&#20219;&#21153;&#19978;&#20248;&#20110;SOTA&#30340;&#22270;&#20687;&#23383;&#24149;&#21644;&#24773;&#22659;&#35782;&#21035;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.14104</link><description>&lt;p&gt;
&#20174;&#24369;&#25991;&#26412;&#30417;&#30563;&#20013;&#23398;&#20064;&#22270;&#20687;&#20013;&#30340;&#20154;&#38469;&#20114;&#21160;
&lt;/p&gt;
&lt;p&gt;
Learning Human-Human Interactions in Images from Weak Textual Supervision. (arXiv:2304.14104v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#20174;&#21333;&#19968;&#30340;&#38745;&#24577;&#22270;&#20687;&#20013;&#23398;&#20064;&#33258;&#30001;&#25991;&#26412;&#30340;&#24418;&#24335;&#26469;&#28789;&#27963;&#24314;&#27169;&#20154;&#38469;&#20114;&#21160;&#12290;&#24182;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#29983;&#25104;&#20266;&#26631;&#31614;&#26469;&#35757;&#32451;&#19968;&#31181;&#23383;&#24149;&#27169;&#22411;&#65292;&#29992;&#20110;&#26377;&#25928;&#29702;&#35299;&#22270;&#20687;&#20013;&#30340;&#20154;&#38469;&#20114;&#21160;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#25991;&#26412;&#21644;&#35821;&#20041;&#36136;&#37327;&#65292;&#24182;&#22312;&#27492;&#20219;&#21153;&#19978;&#20248;&#20110;SOTA&#30340;&#22270;&#20687;&#23383;&#24149;&#21644;&#24773;&#22659;&#35782;&#21035;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#38469;&#20114;&#21160;&#26159;&#22810;&#26679;&#19988;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#30340;&#65292;&#20294;&#20808;&#21069;&#30340;&#24037;&#20316;&#23558;&#23427;&#20204;&#35270;&#20026;&#20998;&#31867;&#65292;&#24573;&#30053;&#20102;&#21487;&#33021;&#30340;&#20114;&#21160;&#30340;&#37325;&#23614;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#20154;&#38469;&#20114;&#21160;&#30340;&#33539;&#24335;&#65292;&#23558;&#20854;&#20316;&#20026;&#33258;&#30001;&#25991;&#26412;&#20174;&#21333;&#19968;&#30340;&#38745;&#24577;&#22270;&#20687;&#20013;&#23398;&#20064;&#65292;&#20174;&#32780;&#20801;&#35768;&#23545;&#24773;&#20917;&#21644;&#20154;&#38469;&#20851;&#31995;&#30340;&#26080;&#38480;&#31354;&#38388;&#36827;&#34892;&#28789;&#27963;&#24314;&#27169;&#12290;&#20026;&#20102;&#20811;&#26381;&#32570;&#20047;&#29305;&#23450;&#20110;&#27492;&#20219;&#21153;&#30340;&#26631;&#35760;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#24212;&#29992;&#20110;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#30340;&#21512;&#25104;&#23383;&#24149;&#25968;&#25454;&#65292;&#20197;&#27492;&#29983;&#25104;&#20266;&#26631;&#31614;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#36825;&#20010;&#36807;&#31243;&#20135;&#29983;&#30340;&#20266;&#26631;&#31614;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#19968;&#31181;&#23383;&#24149;&#27169;&#22411;&#65292;&#33021;&#26377;&#25928;&#29702;&#35299;&#22270;&#20687;&#20013;&#30340;&#20154;&#38469;&#20114;&#21160;&#65292;&#36890;&#36807;&#34913;&#37327;&#25105;&#20204;&#39044;&#27979;&#30340;&#25991;&#26412;&#21644;&#35821;&#20041;&#36136;&#37327;&#19982;&#20107;&#23454;&#30340;&#22522;&#30784;&#24615;&#30340;&#21508;&#31181;&#25351;&#26631;&#26469;&#34913;&#37327;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;SOTA&#30340;&#22270;&#20687;&#23383;&#24149;&#21644;&#24773;&#22659;&#35782;&#21035;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#20844;&#24320;&#25105;&#20204;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interactions between humans are diverse and context-dependent, but previous works have treated them as categorical, disregarding the heavy tail of possible interactions. We propose a new paradigm of learning human-human interactions as free text from a single still image, allowing for flexibility in modeling the unlimited space of situations and relationships between people. To overcome the absence of data labelled specifically for this task, we use knowledge distillation applied to synthetic caption data produced by a large language model without explicit supervision. We show that the pseudo-labels produced by this procedure can be used to train a captioning model to effectively understand human-human interactions in images, as measured by a variety of metrics that measure textual and semantic faithfulness and factual groundedness of our predictions. We further show that our approach outperforms SOTA image captioning and situation recognition models on this task. We will release our c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#36319;&#36394;&#21644;&#26816;&#27979;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;AI&#25991;&#26412;&#65292;&#20351;&#20854;&#26469;&#28304;&#21487;&#20197;&#34987;&#23481;&#26131;&#36861;&#36394;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#21508;&#31181;LLMs&#19978;&#24191;&#27867;&#25512;&#24191;&#65292;&#19988;&#20165;&#38656;&#35201;&#26377;&#38480;&#30340;&#25968;&#25454;&#37327;&#12290;</title><link>http://arxiv.org/abs/2304.14072</link><description>&lt;p&gt;
LLM&#30340;&#26469;&#28304;&#36319;&#36394;&#21644;&#26816;&#27979;&#65288;arXiv:2304.14072v1 [cs.CL]&#65289;
&lt;/p&gt;
&lt;p&gt;
Origin Tracing and Detecting of LLMs. (arXiv:2304.14072v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#36319;&#36394;&#21644;&#26816;&#27979;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;AI&#25991;&#26412;&#65292;&#20351;&#20854;&#26469;&#28304;&#21487;&#20197;&#34987;&#23481;&#26131;&#36861;&#36394;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#21508;&#31181;LLMs&#19978;&#24191;&#27867;&#25512;&#24191;&#65292;&#19988;&#20165;&#38656;&#35201;&#26377;&#38480;&#30340;&#25968;&#25454;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#22240;&#27492;&#26816;&#27979;&#19978;&#19979;&#25991;&#26159;&#21542;&#30001;AI&#31995;&#32479;&#29983;&#25104;&#21464;&#24471;&#26356;&#20026;&#37325;&#35201;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#20844;&#21496;&#21644;&#26426;&#26500;&#21457;&#24067;&#33258;&#24049;&#30340;LLMs&#65292;&#20854;&#26469;&#28304;&#21487;&#33021;&#38590;&#20197;&#36861;&#36394;&#12290;&#30001;&#20110;LLMs&#27491;&#22312;&#26397;&#21521;AGI&#30340;&#26102;&#20195;&#21069;&#36827;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#23398;&#20013;&#30340;&#28304;&#36861;&#36394;&#65292;&#36861;&#36394;LLMs&#30340;&#26469;&#28304;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;LLMs&#30340;&#26469;&#28304;&#36861;&#36394;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#36861;&#36394;&#21644;&#26816;&#27979;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;LLMs&#20043;&#38388;&#30340;&#23545;&#27604;&#29305;&#24449;&#24182;&#25552;&#21462;&#27169;&#22411;&#29305;&#24449;&#20197;&#36861;&#36394;&#25991;&#26412;&#26469;&#28304;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#30333;&#30418;&#21644;&#40657;&#30418;&#35774;&#32622;&#19979;&#22343;&#21487;&#24037;&#20316;&#65292;&#22240;&#27492;&#21487;&#24191;&#27867;&#25512;&#24191;&#21040;&#26816;&#27979;&#21508;&#31181;LLMs&#65288;&#20363;&#22914;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#19981;&#24102;GPT-3&#27169;&#22411;&#30340;GPT-3&#27169;&#22411;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#30417;&#30563;&#23398;&#20064;&#30456;&#27604;&#20165;&#38656;&#35201;&#26377;&#38480;&#30340;&#25968;&#25454;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The extraordinary performance of large language models (LLMs) heightens the importance of detecting whether the context is generated by an AI system. More importantly, while more and more companies and institutions release their LLMs, the origin can be hard to trace. Since LLMs are heading towards the time of AGI, similar to the origin tracing in anthropology, it is of great importance to trace the origin of LLMs. In this paper, we first raise the concern of the origin tracing of LLMs and propose an effective method to trace and detect AI-generated contexts. We introduce a novel algorithm that leverages the contrastive features between LLMs and extracts model-wise features to trace the text origins. Our proposed method works under both white-box and black-box settings therefore can be widely generalized to detect various LLMs.(e.g. can be generalized to detect GPT-3 models without the GPT-3 models). Also, our proposed method requires only limited data compared with the supervised learn
&lt;/p&gt;</description></item><item><title>SweCTRL-Mini&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#29790;&#20856;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#25143;&#21487;&#20197;&#25511;&#21046;&#23427;&#29983;&#25104;&#30340;&#25991;&#26412;&#27969;&#27966;&#65292;&#23436;&#20840;&#24320;&#25918;&#19979;&#36733;&#12290;&#29983;&#25104;&#33021;&#21147;&#27604;&#36739;GPT-3&#12290;</title><link>http://arxiv.org/abs/2304.13994</link><description>&lt;p&gt;
SweCTRL-Mini&#65306;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#25968;&#25454;&#36879;&#26126;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#25511;&#21046;&#24615;&#25991;&#26412;&#29983;&#25104;&#30340;&#29790;&#20856;&#35821;&#35328;&#29256;
&lt;/p&gt;
&lt;p&gt;
SweCTRL-Mini: a data-transparent Transformer-based large language model for controllable text generation in Swedish. (arXiv:2304.13994v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13994
&lt;/p&gt;
&lt;p&gt;
SweCTRL-Mini&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#29790;&#20856;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#25143;&#21487;&#20197;&#25511;&#21046;&#23427;&#29983;&#25104;&#30340;&#25991;&#26412;&#27969;&#27966;&#65292;&#23436;&#20840;&#24320;&#25918;&#19979;&#36733;&#12290;&#29983;&#25104;&#33021;&#21147;&#27604;&#36739;GPT-3&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;SweCTRL-Mini&#65292;&#23427;&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#29790;&#20856;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#21333;&#20010;&#28040;&#36153;&#32423;GPU&#19978;&#30340;&#25512;&#29702;&#21644;fine-tuning&#12290;&#35813;&#27169;&#22411;&#22522;&#20110;&#30001;Keskar&#12289;McCann&#12289;Varshney&#12289;Xiong&#21644;Socher&#65288;2019&#65289;&#24320;&#21457;&#30340;CTRL&#20307;&#31995;&#32467;&#26500;&#65292;&#36825;&#24847;&#21619;&#30528;SweCTRL-Mini&#27169;&#22411;&#30340;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#22312;&#29983;&#25104;&#25552;&#31034;&#20013;&#25554;&#20837;&#29305;&#27530;&#26631;&#35760;&#26469;&#25511;&#21046;&#29983;&#25104;&#25991;&#26412;&#30340;&#27969;&#27966;&#12290;SweCTRL-Mini&#22312;&#29790;&#20856;&#37096;&#20998;mC4&#35821;&#26009;&#24211;&#21644;&#19968;&#32452;&#29790;&#20856;&#23567;&#35828;&#30340;&#23376;&#38598;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25552;&#20379;&#20102;(1)&#25152;&#20351;&#29992;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#25991;&#26412;&#39044;&#22788;&#29702;&#27493;&#39588;&#30340;&#35814;&#32454;&#35828;&#26126;&#65292;&#20197;&#20351;&#21487;&#20197;&#26816;&#26597;&#29305;&#23450;&#30701;&#35821;/&#26469;&#28304;&#26159;&#21542;&#26159;&#35757;&#32451;&#25968;&#25454;&#30340;&#19968;&#37096;&#20998;;(2)&#20351;&#29992;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#36827;&#34892;&#36776;&#21035;&#24615;&#20219;&#21153;&#30340;&#27169;&#22411;&#35780;&#20272;&#65292;&#20351;&#29992;&#20154;&#24037;&#35009;&#21028;&#36827;&#34892;&#29983;&#25104;&#24615;&#20219;&#21153;&#30340;&#35780;&#20272;;&#25105;&#20204;&#36824;&#23558;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#19982;GPT-3&#30340;&#29983;&#25104;&#33021;&#21147;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;SweCTRL-Mini &#26159;&#23436;&#20840;&#24320;&#25918;&#30340;&#65292;&#21487;&#20379;&#19979;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present SweCTRL-Mini, a large Swedish language model that can be used for inference and fine-tuning on a single consumer-grade GPU. The model is based on the CTRL architecture by Keskar, McCann, Varshney, Xiong, and Socher (2019), which means that users of the SweCTRL-Mini model can control the genre of the generated text by inserting special tokens in the generation prompts. SweCTRL-Mini is trained on a subset of the Swedish part of the mC4 corpus and a set of Swedish novels. In this article, we provide (1) a detailed account of the utilized training data and text pre-processing steps, to the extent that it is possible to check whether a specific phrase/source was a part of the training data, and (2) an evaluation of the model on both discriminative tasks, using automatic evaluation methods, and generative tasks, using human referees. We also compare the generative capabilities of the model with those of GPT-3. SweCTRL-Mini is fully open and available for download.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#21019;&#24314;&#36866;&#24230;&#30340;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;'Elder Scrolls Fandom'&#30340;&#25968;&#25454;&#35780;&#20272;&#20102;&#20004;&#20010;&#22312;&#21326;&#23572;&#34903;&#26085;&#25253;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#35789;&#24615;&#26631;&#27880;&#22120;&#22312;&#39046;&#22495;&#22806;&#30340;&#34920;&#29616;&#65292;&#24182;&#26174;&#31034;&#23427;&#20204;&#30340;&#34920;&#29616;&#20960;&#20046;&#19982;&#39046;&#22495;&#20869;&#30456;&#24403;&#65292;&#20294;&#22312;&#26410;&#30693;&#21306;&#22495;&#34920;&#29616;&#19981;&#20339;&#12290;&#20004;&#20010;&#26631;&#27880;&#22120;&#37117;&#38590;&#20197;&#22788;&#29702;&#19987;&#26377;&#21517;&#35789;&#21644;&#19981;&#19968;&#33268;&#30340;&#22823;&#20889;&#24418;&#24335;&#12290;</title><link>http://arxiv.org/abs/2304.13989</link><description>&lt;p&gt;
&#36328;&#22495;&#35780;&#20272;&#35789;&#24615;&#26631;&#27880;&#22120;&#65306;&#20174;&#21326;&#23572;&#34903;&#26085;&#25253;&#21040;Fandom Wiki
&lt;/p&gt;
&lt;p&gt;
Cross-Domain Evaluation of POS Taggers: From Wall Street Journal to Fandom Wiki. (arXiv:2304.13989v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13989
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#21019;&#24314;&#36866;&#24230;&#30340;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;'Elder Scrolls Fandom'&#30340;&#25968;&#25454;&#35780;&#20272;&#20102;&#20004;&#20010;&#22312;&#21326;&#23572;&#34903;&#26085;&#25253;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#35789;&#24615;&#26631;&#27880;&#22120;&#22312;&#39046;&#22495;&#22806;&#30340;&#34920;&#29616;&#65292;&#24182;&#26174;&#31034;&#23427;&#20204;&#30340;&#34920;&#29616;&#20960;&#20046;&#19982;&#39046;&#22495;&#20869;&#30456;&#24403;&#65292;&#20294;&#22312;&#26410;&#30693;&#21306;&#22495;&#34920;&#29616;&#19981;&#20339;&#12290;&#20004;&#20010;&#26631;&#27880;&#22120;&#37117;&#38590;&#20197;&#22788;&#29702;&#19987;&#26377;&#21517;&#35789;&#21644;&#19981;&#19968;&#33268;&#30340;&#22823;&#20889;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;Penn Treebank&#30340;&#21326;&#23572;&#34903;&#26085;&#25253;&#37096;&#20998;&#19968;&#30452;&#26159;&#35780;&#20272;&#35789;&#24615;&#26631;&#27880;&#22120;&#30340;&#20107;&#23454;&#26631;&#20934;&#65292;&#24182;&#19988;&#24050;&#32463;&#25253;&#21578;&#20102;97&#65285;&#20197;&#19978;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#39046;&#22495;&#22806;&#26631;&#27880;&#22120;&#24615;&#33021;&#30340;&#20102;&#35299;&#36739;&#23569;&#65292;&#29305;&#21035;&#26159;&#32454;&#31890;&#24230;&#26631;&#31614;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;Elder Scrolls Fandom&#20013;&#30340;&#25968;&#25454;&#21019;&#24314;&#20102;&#19968;&#20010;&#36866;&#24230;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#23450;&#24615;&#35780;&#20272;&#20004;&#20010;&#35789;&#24615;&#26631;&#27880;&#22120;&#30340;&#36328;&#22495;&#24615;&#33021;&#65306;Stanford&#26631;&#27880;&#22120;&#65288;Toutanova et al.&#65292;2003&#65289;&#21644;Bilty&#65288;Plank et al.&#65292;2016&#65289;&#65292;&#23427;&#20204;&#37117;&#26159;&#22312;WSJ&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#35757;&#32451;&#20013;&#30475;&#21040;&#30340;&#26631;&#35760;&#30340;&#24615;&#33021;&#20960;&#20046;&#19982;&#39046;&#22495;&#20869;&#24615;&#33021;&#19968;&#26679;&#22909;&#65292;&#20294;&#26159;&#26410;&#30693;&#26631;&#35760;&#30340;&#20934;&#30830;&#24615;&#20174;90.37&#65285;&#19979;&#38477;&#21040;78.37&#65285;&#65288;&#26031;&#22374;&#31119;&#22823;&#23398;&#65289;&#65292;&#20174;87.84&#65285;&#19979;&#38477;&#21040;80.41&#65285;&#65288;Bilty&#65289;&#36328;&#36234;&#39046;&#22495;&#12290;&#20004;&#20010;&#26631;&#27880;&#22120;&#37117;&#38590;&#20197;&#22788;&#29702;&#19987;&#26377;&#21517;&#35789;&#21644;&#19981;&#19968;&#33268;&#30340;&#22823;&#20889;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Wall Street Journal section of the Penn Treebank has been the de-facto standard for evaluating POS taggers for a long time, and accuracies over 97\% have been reported. However, less is known about out-of-domain tagger performance, especially with fine-grained label sets. Using data from Elder Scrolls Fandom, a wiki about the \textit{Elder Scrolls} video game universe, we create a modest dataset for qualitatively evaluating the cross-domain performance of two POS taggers: the Stanford tagger (Toutanova et al. 2003) and Bilty (Plank et al. 2016), both trained on WSJ. Our analyses show that performance on tokens seen during training is almost as good as in-domain performance, but accuracy on unknown tokens decreases from 90.37% to 78.37% (Stanford) and 87.84\% to 80.41\% (Bilty) across domains. Both taggers struggle with proper nouns and inconsistent capitalization.
&lt;/p&gt;</description></item><item><title>&#35813;&#39033;&#30446;&#26088;&#22312;&#32508;&#21512;&#22810;&#31181;&#25968;&#25454;&#28304;&#35780;&#20272;1990-2022&#24180;&#21360;&#24230;&#20892;&#26449;&#30340;&#36139;&#22256;&#24773;&#20917;&#65292;&#21253;&#25324;&#24120;&#35268;&#30340;&#23478;&#24237;&#35843;&#26597;&#12289;&#20154;&#21475;&#26222;&#26597;&#20197;&#21450;&#26469;&#33258;&#21355;&#26143;&#22270;&#20687;&#21644;&#36890;&#20449;&#32593;&#32476;&#30340;&#20195;&#29702;&#21464;&#37327;&#12290;&#36890;&#36807;&#23558;&#21439;&#21306;&#21010;&#20998;&#20026;&#19981;&#21516;&#30340;&#21306;&#22495;&#65292;&#30830;&#23450;&#33853;&#21518;&#22320;&#21306;&#24182;&#28145;&#20837;&#30740;&#31350;&#20854;&#23548;&#33268;&#36139;&#31351;&#30340;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2304.13958</link><description>&lt;p&gt;
&#23398;&#20064;&#21644;&#25512;&#29702;&#22810;&#26041;&#38754;&#21644;&#32437;&#21521;&#25968;&#25454;&#65292;&#20197;&#35780;&#20272;&#33853;&#21518;&#22320;&#21306;&#30340;&#21360;&#24230;&#20892;&#26449;&#36139;&#22256;&#29575;&#21644;&#29983;&#35745;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Learning and Reasoning Multifaceted and Longitudinal Data for Poverty Estimates and Livelihood Capabilities of Lagged Regions in Rural India. (arXiv:2304.13958v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13958
&lt;/p&gt;
&lt;p&gt;
&#35813;&#39033;&#30446;&#26088;&#22312;&#32508;&#21512;&#22810;&#31181;&#25968;&#25454;&#28304;&#35780;&#20272;1990-2022&#24180;&#21360;&#24230;&#20892;&#26449;&#30340;&#36139;&#22256;&#24773;&#20917;&#65292;&#21253;&#25324;&#24120;&#35268;&#30340;&#23478;&#24237;&#35843;&#26597;&#12289;&#20154;&#21475;&#26222;&#26597;&#20197;&#21450;&#26469;&#33258;&#21355;&#26143;&#22270;&#20687;&#21644;&#36890;&#20449;&#32593;&#32476;&#30340;&#20195;&#29702;&#21464;&#37327;&#12290;&#36890;&#36807;&#23558;&#21439;&#21306;&#21010;&#20998;&#20026;&#19981;&#21516;&#30340;&#21306;&#22495;&#65292;&#30830;&#23450;&#33853;&#21518;&#22320;&#21306;&#24182;&#28145;&#20837;&#30740;&#31350;&#20854;&#23548;&#33268;&#36139;&#31351;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36139;&#22256;&#26159;&#19968;&#20010;&#22810;&#26041;&#38754;&#30340;&#29616;&#35937;&#65292;&#19982;&#23478;&#24237;&#36186;&#21462;&#21487;&#25345;&#32493;&#29983;&#35745;&#30340;&#33021;&#21147;&#19981;&#36275;&#26377;&#20851;&#65292;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;&#22810;&#32500;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#12290;&#20854;&#31354;&#38388;&#27169;&#24335;&#21462;&#20915;&#20110;&#31038;&#20250;&#12289;&#32463;&#27982;&#12289;&#25919;&#27835;&#21644;&#21306;&#22495;&#21464;&#37327;&#12290;&#20154;&#24037;&#26234;&#33021;&#22312;&#20998;&#26512;&#36139;&#22256;&#30340;&#22797;&#26434;&#24615;&#21644;&#24494;&#22937;&#24615;&#26041;&#38754;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#35813;&#39033;&#30446;&#26088;&#22312;&#22522;&#20110;&#29983;&#27963;&#36136;&#37327;&#21644;&#29983;&#35745;&#25351;&#26631;&#65292;&#30740;&#31350;1990-2022&#24180;&#21360;&#24230;&#20892;&#26449;&#30340;&#36139;&#22256;&#24773;&#20917;&#12290;&#23558;&#21439;&#21306;&#20998;&#31867;&#20026;&#8220;&#20808;&#36827;&#8221;&#12289;&#8220;&#36861;&#36214;&#8221;&#12289;&#8220;&#33853;&#21518;&#8221;&#21644;&#8220;&#28382;&#21518;&#8221;&#21306;&#22495;&#12290;&#35813;&#39033;&#30446;&#25311;&#25972;&#21512;&#22810;&#20010;&#25968;&#25454;&#28304;&#65292;&#21253;&#25324;&#24120;&#35268;&#30340;&#20840;&#22269;&#22823;&#26679;&#26412;&#23478;&#24237;&#35843;&#26597;&#12289;&#20154;&#21475;&#26222;&#26597;&#20197;&#21450;&#26469;&#33258;&#21355;&#26143;&#22270;&#20687;&#30340;&#26085;&#38388;&#21644;&#22812;&#38388;&#25968;&#25454;&#65292;&#36890;&#20449;&#32593;&#32476;&#31561;&#20195;&#29702;&#21464;&#37327;&#65292;&#20197;&#25552;&#20379;&#21439;&#19968;&#32423;&#36139;&#22256;&#30340;&#20840;&#38754;&#35270;&#22270;&#12290;&#35813;&#39033;&#30446;&#36824;&#25171;&#31639;&#30740;&#31350;&#23548;&#33268;&#36139;&#31351;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Poverty is a multifaceted phenomenon linked to the lack of capabilities of households to earn a sustainable livelihood, increasingly being assessed using multidimensional indicators. Its spatial pattern depends on social, economic, political, and regional variables. Artificial intelligence has shown immense scope in analyzing the complexities and nuances of poverty. The proposed project aims to examine the poverty situation of rural India for the period of 1990-2022 based on the quality of life and livelihood indicators. The districts will be classified into `advanced', `catching up', `falling behind', and `lagged' regions. The project proposes to integrate multiple data sources, including conventional national-level large sample household surveys, census surveys, and proxy variables like daytime, and nighttime data from satellite images, and communication networks, to name a few, to provide a comprehensive view of poverty at the district level. The project also intends to examine caus
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#30340;&#30693;&#35782;&#22686;&#24378;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#23558;&#20174;&#30693;&#35782;&#22270;&#35889;&#20013;&#26816;&#32034;&#21040;&#30340;&#19990;&#30028;&#30693;&#35782;&#34701;&#20837;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#20013;&#65292;&#23558;&#26126;&#30830;&#30340;&#30693;&#35782;&#19982;&#35270;&#35273;&#35821;&#35328;&#23545;&#34701;&#21512;&#65292;&#36890;&#36807;&#22235;&#20010;&#30693;&#35782;&#24863;&#30693;&#30340;&#33258;&#30417;&#30563;&#20219;&#21153;&#25512;&#21160;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#30693;&#35782;&#30340;&#30456;&#20114;&#25972;&#21512;&#12290;</title><link>http://arxiv.org/abs/2304.13923</link><description>&lt;p&gt;
&#22522;&#20110;&#26816;&#32034;&#30340;&#30693;&#35782;&#22686;&#24378;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Retrieval-based Knowledge Augmented Vision Language Pre-training. (arXiv:2304.13923v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#30340;&#30693;&#35782;&#22686;&#24378;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#23558;&#20174;&#30693;&#35782;&#22270;&#35889;&#20013;&#26816;&#32034;&#21040;&#30340;&#19990;&#30028;&#30693;&#35782;&#34701;&#20837;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#20013;&#65292;&#23558;&#26126;&#30830;&#30340;&#30693;&#35782;&#19982;&#35270;&#35273;&#35821;&#35328;&#23545;&#34701;&#21512;&#65292;&#36890;&#36807;&#22235;&#20010;&#30693;&#35782;&#24863;&#30693;&#30340;&#33258;&#30417;&#30563;&#20219;&#21153;&#25512;&#21160;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#30693;&#35782;&#30340;&#30456;&#20114;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#35270;&#35273;&#21644;&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;&#30340;&#36827;&#23637;&#65292;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;(VLP)&#27169;&#22411;&#22312;&#21508;&#31181;&#22810;&#27169;&#24577;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#21487;&#21916;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#20173;&#26410;&#21033;&#29992;&#19990;&#30028;&#30693;&#35782;&#65292;&#19990;&#30028;&#30693;&#35782;&#38544;&#21547;&#22312;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#65292;&#20294;&#21253;&#21547;&#20016;&#23500;&#21644;&#20114;&#34917;&#30340;&#20449;&#24687;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;REtrieval-based knowledge Augmented Vision Language Pre-training (REAVL)&#27169;&#22411;&#65292;&#23427;&#20174;&#30693;&#35782;&#22270;&#35889;(KGs)&#20013;&#26816;&#32034;&#19990;&#30028;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#34701;&#20837;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
With recent progress in large-scale vision and language representation learning, Vision Language Pretraining (VLP) models have achieved promising improvements on various multi-modal downstream tasks. Albeit powerful, these pre-training models still do not take advantage of world knowledge, which is implicit in multi-modal data but comprises abundant and complementary information. In this work, we propose a REtrieval-based knowledge Augmented Vision Language Pre-training model (REAVL), which retrieves world knowledge from knowledge graphs (KGs) and incorporates them in vision-language pre-training. REAVL has two core components: a knowledge retriever that retrieves knowledge given multi-modal data, and a knowledge-augmented model that fuses multi-modal data and knowledge. By novelly unifying four knowledge-aware self-supervised tasks, REAVL promotes the mutual integration of multi-modal data and knowledge by fusing explicit knowledge with vision-language pairs for masked multi-modal dat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;ConDA&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#20132;&#20114;&#24335;&#38382;&#39064;&#21644;&#30456;&#24212;&#30340;SQL&#32467;&#26524;&#65292;&#24182;&#35774;&#35745;&#20102;SQL&#23545;&#35805;&#29366;&#24577;&#21644;&#36807;&#28388;&#26041;&#27861;&#26469;&#22686;&#24378;&#25968;&#25454;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#65292;&#33021;&#22815;&#25552;&#39640;&#22797;&#26434;&#38382;&#39064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.13902</link><description>&lt;p&gt;
&#21487;&#25511;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#29992;&#20110;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#25991;&#26412;&#21040;SQL&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Controllable Data Augmentation for Context-Dependent Text-to-SQL. (arXiv:2304.13902v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13902
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;ConDA&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#20132;&#20114;&#24335;&#38382;&#39064;&#21644;&#30456;&#24212;&#30340;SQL&#32467;&#26524;&#65292;&#24182;&#35774;&#35745;&#20102;SQL&#23545;&#35805;&#29366;&#24577;&#21644;&#36807;&#28388;&#26041;&#27861;&#26469;&#22686;&#24378;&#25968;&#25454;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#65292;&#33021;&#22815;&#25552;&#39640;&#22797;&#26434;&#38382;&#39064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#26631;&#27880;&#25968;&#25454;&#25968;&#37327;&#26377;&#38480;&#65292;&#26631;&#27880;&#22797;&#26434;&#24230;&#39640;&#65292;&#38480;&#21046;&#20102;&#29616;&#26377;&#30340;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#25991;&#26412;&#21040;SQL&#36716;&#25442;&#27169;&#22411;&#30340;&#35268;&#27169;&#12290;&#25968;&#25454;&#22686;&#24378;&#26159;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#24120;&#29992;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#22686;&#24378;&#26041;&#27861;&#20135;&#29983;&#30340;&#25968;&#25454;&#24448;&#24448;&#32570;&#20047;&#22810;&#26679;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ConDA&#65292;&#21487;&#20197;&#29983;&#25104;&#20132;&#20114;&#24335;&#30340;&#38382;&#39064;&#21644;&#30456;&#24212;&#30340;SQL&#32467;&#26524;&#65292;&#35774;&#35745;&#20102;SQL&#23545;&#35805;&#29366;&#24577;&#20197;&#36890;&#36807;&#29366;&#24577;&#36716;&#31227;&#22686;&#24378;&#25968;&#25454;&#22810;&#26679;&#24615;&#65292;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;grounding&#27169;&#22411;&#30340;&#36807;&#28388;&#22120;&#26041;&#27861;&#26469;&#30830;&#20445;&#25968;&#25454;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;grounding&#27169;&#22411;&#26469;&#35782;&#21035;&#21644;&#36807;&#28388;&#19982;&#29366;&#24577;&#20449;&#24687;&#19981;&#21305;&#37197;&#30340;&#20302;&#36136;&#37327;&#38382;&#39064;&#12290;&#22312;SParC&#21644;CoSQL&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ConDA&#23558;&#22522;&#32447;&#27169;&#22411;&#25552;&#39640;&#20102;&#24179;&#22343; $3.3\%$ &#30340;&#22797;&#26434;&#38382;&#39064;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#22686;&#24378;&#25968;&#25454;&#65292;&#21457;&#29616;ConDA&#29983;&#25104;&#30340;&#25968;&#25454;&#22312;SQL&#27169;&#26495;&#21305;&#37197;&#21644;&#35821;&#20041;&#27491;&#30830;&#24615;&#26041;&#38754;&#20855;&#26377;&#39640;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The limited scale of annotated data constraints existing context-dependent text-to-SQL models because of the complexity of labeling. The data augmentation method is a commonly used method to solve this problem. However, the data generated by current augmentation methods often lack diversity. In this paper, we introduce ConDA, which generates interactive questions and corresponding SQL results. We designed the SQL dialogue state to enhance the data diversity through the state transition. Meanwhile, we also present a filter method to ensure the data quality by a grounding model. Additionally, we utilize a grounding model to identify and filter low-quality questions that mismatch the state information. Experimental results on the SParC and CoSQL datasets show that ConDA boosts the baseline model to achieve an average improvement of $3.3\%$ on complex questions. Moreover, we analyze the augmented data, which reveals that the data generated by ConDA are of high quality in both SQL template 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#19977;&#31181;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65288;T5&#12289;CatSeq-Transformer&#12289;ExHiRD&#65289;&#22312;&#20851;&#38190;&#35789;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#21644;&#34892;&#20026;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;SoftKeyScore&#26469;&#34913;&#37327;&#20004;&#32452;&#20851;&#38190;&#35789;&#30340;&#30456;&#20284;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.13883</link><description>&lt;p&gt;
&#31070;&#32463;&#20851;&#38190;&#35789;&#29983;&#25104;&#65306;&#20998;&#26512;&#19982;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Neural Keyphrase Generation: Analysis and Evaluation. (arXiv:2304.13883v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#19977;&#31181;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65288;T5&#12289;CatSeq-Transformer&#12289;ExHiRD&#65289;&#22312;&#20851;&#38190;&#35789;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#21644;&#34892;&#20026;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;SoftKeyScore&#26469;&#34913;&#37327;&#20004;&#32452;&#20851;&#38190;&#35789;&#30340;&#30456;&#20284;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#38190;&#35789;&#29983;&#25104;&#26088;&#22312;&#36890;&#36807;&#20174;&#21407;&#22987;&#25991;&#26412;&#20013;&#22797;&#21046;&#65288;&#29616;&#26377;&#20851;&#38190;&#35789;&#65289;&#25110;&#29983;&#25104;&#25429;&#25417;&#25991;&#26412;&#35821;&#20041;&#24847;&#20041;&#30340;&#26032;&#20851;&#38190;&#35789;&#65288;&#32570;&#22833;&#20851;&#38190;&#35789;&#65289;&#26469;&#29983;&#25104;&#35805;&#39064;&#30701;&#35821;&#12290;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#22312;&#27492;&#20219;&#21153;&#20013;&#26368;&#24191;&#27867;&#20351;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#29983;&#25104;&#32570;&#22833;&#20851;&#38190;&#35789;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20960;&#20046;&#27809;&#26377;&#23545;&#27492;&#31867;&#27169;&#22411;&#22312;&#20851;&#38190;&#35789;&#29983;&#25104;&#20013;&#30340;&#24615;&#33021;&#21644;&#34892;&#20026;&#36827;&#34892;&#20998;&#26512;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19977;&#31181;&#24378;&#21170;&#27169;&#22411; T5&#65288;&#22522;&#20110;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65289;&#12289;CatSeq-Transformer&#65288;&#38750;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65289;&#21644; ExHiRD&#65288;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65289;&#25152;&#23637;&#31034;&#30340;&#21508;&#31181;&#36235;&#21183;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#39044;&#27979;&#32622;&#20449;&#24230;&#20998;&#25968;&#12289;&#27169;&#22411;&#26657;&#20934;&#20197;&#21450;&#35789;&#20803;&#20301;&#32622;&#23545;&#20851;&#38190;&#35789;&#29983;&#25104;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#25512;&#21160;&#19968;&#20010;&#26032;&#30340;&#24230;&#37327;&#26694;&#26550;SoftKeyScore&#65292;&#36890;&#36807;&#20351;&#29992; softscores &#26469;&#35745;&#31639;&#37096;&#20998;&#21305;&#37197;&#30340;&#30456;&#20284;&#24230;&#26469;&#35780;&#20272;&#20004;&#32452;&#20851;&#38190;&#35789;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Keyphrase generation aims at generating topical phrases from a given text either by copying from the original text (present keyphrases) or by producing new keyphrases (absent keyphrases) that capture the semantic meaning of the text. Encoder-decoder models are most widely used for this task because of their capabilities for absent keyphrase generation. However, there has been little to no analysis on the performance and behavior of such models for keyphrase generation. In this paper, we study various tendencies exhibited by three strong models: T5 (based on a pre-trained transformer), CatSeq-Transformer (a non-pretrained Transformer), and ExHiRD (based on a recurrent neural network). We analyze prediction confidence scores, model calibration, and the effect of token position on keyphrases generation. Moreover, we motivate and propose a novel metric framework, SoftKeyScore, to evaluate the similarity between two sets of keyphrases by using softscores to account for partial matching and 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22686;&#24378;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;MasonNLP+&#65292;&#33021;&#22815;&#20174;&#31038;&#20132;&#23186;&#20307;&#20013;&#25552;&#21462;&#21307;&#30103;&#38382;&#39064;&#12289;&#32463;&#39564;&#21644;&#22768;&#26126;&#65292;&#24182;&#22312;SemEval-2023&#20219;&#21153;8&#30340;&#20004;&#20010;&#23376;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.13875</link><description>&lt;p&gt;
SemEval-2023&#31532;8&#39033;&#20219;&#21153;&#20013;&#30340;MasonNLP+&#65306;&#20351;&#29992;&#30693;&#35782;&#22686;&#24378;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20174;&#31038;&#20132;&#23186;&#20307;&#20013;&#25552;&#21462;&#21307;&#30103;&#38382;&#39064;&#12289;&#32463;&#39564;&#21644;&#22768;&#26126;
&lt;/p&gt;
&lt;p&gt;
MasonNLP+ at SemEval-2023 Task 8: Extracting Medical Questions, Experiences and Claims from Social Media using Knowledge-Augmented Pre-trained Language Models. (arXiv:2304.13875v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22686;&#24378;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;MasonNLP+&#65292;&#33021;&#22815;&#20174;&#31038;&#20132;&#23186;&#20307;&#20013;&#25552;&#21462;&#21307;&#30103;&#38382;&#39064;&#12289;&#32463;&#39564;&#21644;&#22768;&#26126;&#65292;&#24182;&#22312;SemEval-2023&#20219;&#21153;8&#30340;&#20004;&#20010;&#23376;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#35770;&#22363;&#20013;&#65292;&#29992;&#25143;&#20998;&#20139;&#20182;&#20204;&#30340;&#21307;&#30103;&#29366;&#20917;&#21644;&#27835;&#30103;&#32463;&#39564;&#65292;&#21253;&#25324;&#25552;&#20986;&#22768;&#26126;&#12289;&#25552;&#38382;&#20197;&#21450;&#35752;&#35770;&#27835;&#30103;&#23545;&#20182;&#20204;&#30340;&#20581;&#24247;&#29366;&#20917;&#30340;&#24433;&#21709;&#12290;&#26500;&#24314;&#21487;&#29702;&#35299;&#36825;&#20123;&#20449;&#24687;&#30340;&#31995;&#32479;&#21487;&#20197;&#26377;&#25928;&#22320;&#30417;&#27979;&#38169;&#35823;&#20449;&#24687;&#30340;&#20256;&#25773;&#24182;&#39564;&#35777;&#29992;&#25143;&#30340;&#22768;&#26126;&#12290; SemEval-2023&#30340;&#31532;8&#39033;&#20219;&#21153;&#19987;&#27880;&#20110;&#21307;&#23398;&#24212;&#29992;&#65292;&#20855;&#20307;&#21253;&#25324;&#20174;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#24086;&#23376;&#20013;&#25552;&#21462;&#19982;&#24739;&#32773;&#32463;&#39564;&#21644;&#21307;&#30103;&#29366;&#20917;&#30456;&#20851;&#30340;&#23454;&#20307;&#12290; Reddit&#20581;&#24247;&#22312;&#32447;&#20132;&#27969;&#65288;RedHot&#65289;&#35821;&#26009;&#24211;&#21253;&#21547;&#26469;&#33258;&#19982;&#21307;&#30103;&#29366;&#20917;&#30456;&#20851;&#30340;subreddit&#30340;&#24086;&#23376;&#65292;&#24102;&#26377;&#34920;&#24449;&#24739;&#32773;&#32463;&#39564;&#21644;&#21307;&#30103;&#29366;&#20917;&#30340;&#27880;&#37322;&#12290;&#22312;&#23376;&#20219;&#21153;1&#20013;&#65292;&#24739;&#32773;&#32463;&#39564;&#36890;&#36807;&#20010;&#20154;&#32463;&#39564;&#12289;&#38382;&#39064;&#21644;&#22768;&#26126;&#26469;&#25551;&#36848;&#12290;&#22312;&#23376;&#20219;&#21153;2&#20013;&#65292;&#21307;&#30103;&#29366;&#20917;&#36890;&#36807;&#20154;&#32676;&#12289;&#24178;&#39044;&#21644;&#32467;&#26524;&#26469;&#25551;&#36848;&#12290;&#20026;&#33258;&#21160;&#25552;&#21462;&#24739;&#32773;&#32463;&#39564;&#21644;&#21307;&#30103;&#29366;&#20917;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22686;&#24378;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;MasonNLP+&#65292;&#23558;&#39046;&#22495;&#29305;&#23450;&#30340;&#30693;&#35782;&#34701;&#20837;&#39044;&#35757;&#32451;&#36807;&#31243;&#20197;&#25552;&#39640;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;SemEval-2023&#31532;8&#39033;&#20219;&#21153;&#30340;&#20004;&#20010;&#23376;&#20219;&#21153;&#20013;&#22343;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In online forums like Reddit, users share their experiences with medical conditions and treatments, including making claims, asking questions, and discussing the effects of treatments on their health. Building systems to understand this information can effectively monitor the spread of misinformation and verify user claims. The Task-8 of the 2023 International Workshop on Semantic Evaluation focused on medical applications, specifically extracting patient experience- and medical condition-related entities from user posts on social media. The Reddit Health Online Talk (RedHot) corpus contains posts from medical condition-related subreddits with annotations characterizing the patient experience and medical conditions. In Subtask-1, patient experience is characterized by personal experience, questions, and claims. In Subtask-2, medical conditions are characterized by population, intervention, and outcome. For the automatic extraction of patient experiences and medical condition informatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;GPT-4&#21644;ChatGPT&#23545;&#20302;&#36164;&#28304;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#25552;&#31034;&#23558;&#23567;&#22411;&#26631;&#35760;&#25968;&#25454;&#38598;&#25193;&#20805;&#20026;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#22312;&#20445;&#30041;&#21407;&#22987;&#26631;&#31614;&#20998;&#24067;&#25110;&#24179;&#34913;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#65292;&#20135;&#29983;&#20102;&#33391;&#22909;&#30340;&#19979;&#28216;&#24615;&#33021;&#12290;&#22312;&#27979;&#35797;&#38598;&#19978;&#65292;GPT-4&#21644;ChatGPT&#34920;&#29616;&#20986;&#20986;&#33394;&#30340;&#38646;-shot&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#20302;&#36164;&#28304;&#35774;&#32622;&#20013;&#33021;&#22815;&#36739;&#22909;&#22320;&#35782;&#21035;&#32597;&#35265;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2304.13861</link><description>&lt;p&gt;
&#19968;&#20010;&#25552;&#31034;&#21644;&#20960;&#20010;&#31034;&#20363;&#23601;&#36275;&#22815;&#20102;&#21527;&#65311;&#20351;&#29992;GPT-4&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#22312;&#20302;&#36164;&#28304;&#20998;&#31867;&#20219;&#21153;&#20013;
&lt;/p&gt;
&lt;p&gt;
Is a prompt and a few samples all you need? Using GPT-4 for data augmentation in low-resource classification tasks. (arXiv:2304.13861v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13861
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;GPT-4&#21644;ChatGPT&#23545;&#20302;&#36164;&#28304;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#25552;&#31034;&#23558;&#23567;&#22411;&#26631;&#35760;&#25968;&#25454;&#38598;&#25193;&#20805;&#20026;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#22312;&#20445;&#30041;&#21407;&#22987;&#26631;&#31614;&#20998;&#24067;&#25110;&#24179;&#34913;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#65292;&#20135;&#29983;&#20102;&#33391;&#22909;&#30340;&#19979;&#28216;&#24615;&#33021;&#12290;&#22312;&#27979;&#35797;&#38598;&#19978;&#65292;GPT-4&#21644;ChatGPT&#34920;&#29616;&#20986;&#20986;&#33394;&#30340;&#38646;-shot&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#20302;&#36164;&#28304;&#35774;&#32622;&#20013;&#33021;&#22815;&#36739;&#22909;&#22320;&#35782;&#21035;&#32597;&#35265;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22797;&#26434;&#30340;&#20302;&#36164;&#28304;&#39046;&#22495;&#20013;&#65292;&#33719;&#21462;&#21644;&#27880;&#37322;&#25968;&#25454;&#21487;&#33021;&#26159;&#26114;&#36149;&#21644;&#32791;&#26102;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;GPT-4&#21644;ChatGPT&#36890;&#36807;&#31616;&#21333;&#30340;&#25552;&#31034;&#23558;&#23567;&#22411;&#26631;&#35760;&#25968;&#25454;&#38598;&#25193;&#20805;&#20026;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#24212;&#29992;&#20110;&#19977;&#20010;&#19981;&#21516;&#30340;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#22797;&#26434;&#31243;&#24230;&#21508;&#24322;&#12290;&#23545;&#20110;&#27599;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#38543;&#26426;&#36873;&#25321;&#20102;500&#20010;&#25991;&#26412;&#20316;&#20026;&#22522;&#26412;&#26679;&#26412;&#65292;&#29983;&#25104;&#20102;5,000&#20010;&#26032;&#30340;&#21512;&#25104;&#26679;&#26412;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#20004;&#31181;&#22686;&#24378;&#31574;&#30053;&#65306;&#19968;&#31181;&#20445;&#30041;&#21407;&#22987;&#26631;&#31614;&#20998;&#24067;&#65292;&#21478;&#19968;&#31181;&#24179;&#34913;&#20998;&#24067;&#12290;&#20351;&#29992;&#36880;&#27493;&#21464;&#22823;&#30340;&#35757;&#32451;&#26679;&#26412;&#37327;&#65292;&#25105;&#20204;&#20998;&#21035;&#22312;&#30495;&#23454;&#25968;&#25454;&#21644;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#21644;&#35780;&#20272;&#20102;&#19968;&#20010;1.1&#20159;&#21442;&#25968;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#22312;&#27979;&#35797;&#38598;&#19978;&#27979;&#35797;&#20102;GPT-4&#21644;ChatGPT&#30340;&#38646;-shot&#35774;&#32622;&#12290;&#25105;&#20204;&#21457;&#29616;GPT-4&#21644;ChatGPT&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#37117;&#20855;&#26377;&#24456;&#24378;&#30340;&#38646;-shot&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20351;&#29992;&#21512;&#25104;&#26679;&#26412;&#22686;&#24378;&#30340;&#25968;&#25454;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#20135;&#29983;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#35782;&#21035;&#32597;&#35265;&#31867;&#21035;&#31561;&#20302;&#36164;&#28304;&#35774;&#32622;&#26041;&#38754;&#34920;&#29616;&#31361;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Obtaining and annotating data can be expensive and time-consuming, especially in complex, low-resource domains. We use GPT-4 and ChatGPT to augment small labeled datasets with synthetic data via simple prompts, in three different classification tasks with varying complexity. For each task, we randomly select a base sample of 500 texts to generate 5,000 new synthetic samples. We explore two augmentation strategies: one that preserves original label distribution and another that balances the distribution. Using a progressively larger training sample size, we train and evaluate a 110M parameter multilingual language model on the real and synthetic data separately. We also test GPT-4 and ChatGPT in a zero-shot setting on the test sets. We observe that GPT-4 and ChatGPT have strong zero-shot performance across all tasks. We find that data augmented with synthetic samples yields a good downstream performance, and particularly aids in low-resource settings, such as in identifying rare classes
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;GPT-3&#35821;&#35328;&#27169;&#22411;&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#33258;&#21160;&#21270;&#22320;&#25552;&#21462;&#37329;&#32435;&#31859;&#26834;&#21512;&#25104;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#39640;&#36890;&#37327;&#30340;&#25506;&#32034;&#37329;&#32435;&#31859;&#26834;&#30340;&#31181;&#23376;&#20171;&#23548;&#29983;&#38271;&#36807;&#31243;&#20197;&#21450;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.13846</link><description>&lt;p&gt;
&#20174;&#25991;&#29486;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#30340;&#31181;&#23376;&#20171;&#23548;&#37329;&#32435;&#31859;&#26834;&#29983;&#38271;&#26041;&#27861;&#65306;&#22522;&#20110;GPT-3&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Extracting Structured Seed-Mediated Gold Nanorod Growth Procedures from Literature with GPT-3. (arXiv:2304.13846v1 [physics.app-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13846
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;GPT-3&#35821;&#35328;&#27169;&#22411;&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#33258;&#21160;&#21270;&#22320;&#25552;&#21462;&#37329;&#32435;&#31859;&#26834;&#21512;&#25104;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#39640;&#36890;&#37327;&#30340;&#25506;&#32034;&#37329;&#32435;&#31859;&#26834;&#30340;&#31181;&#23376;&#20171;&#23548;&#29983;&#38271;&#36807;&#31243;&#20197;&#21450;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#37329;&#32435;&#31859;&#26834;&#24050;&#32463;&#25104;&#20026;&#30740;&#31350;&#30340;&#28909;&#28857;&#65292;&#20294;&#25511;&#21046;&#23427;&#20204;&#30340;&#24418;&#29366;&#65292;&#20174;&#32780;&#25511;&#21046;&#23427;&#20204;&#30340;&#20809;&#23398;&#29305;&#24615;&#30340;&#36884;&#24452;&#20173;&#28982;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#22522;&#20110;&#32463;&#39564;&#30340;&#12290;&#23613;&#31649;&#21512;&#25104;&#36807;&#31243;&#20013;&#19981;&#21516;&#35797;&#21058;&#29289;&#20043;&#38388;&#30340;&#20849;&#23384;&#21644;&#30456;&#20114;&#20316;&#29992;&#25511;&#21046;&#30528;&#36825;&#20123;&#29305;&#24615;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#65292;&#25506;&#32034;&#21512;&#25104;&#31354;&#38388;&#30340;&#35745;&#31639;&#21644;&#23454;&#39564;&#26041;&#27861;&#21487;&#33021;&#20250;&#26497;&#20854;&#32321;&#29712;&#25110;&#32791;&#36153;&#36807;&#22810;&#26102;&#38388;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31185;&#23398;&#25991;&#29486;&#20013;&#24050;&#32463;&#21253;&#21547;&#30340;&#22823;&#37327;&#21512;&#25104;&#20449;&#24687;&#26469;&#33258;&#21160;&#21270;&#25552;&#21462;&#30456;&#20851;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20197;&#39640;&#36890;&#37327;&#26041;&#24335;&#25506;&#23547;&#37329;&#32435;&#31859;&#26834;&#31181;&#23376;&#20171;&#23548;&#29983;&#38271;&#36807;&#31243;&#20197;&#21450;&#32467;&#26524;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#24378;&#22823;&#30340;GPT-3&#35821;&#35328;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#26469;&#20174;&#38750;&#32467;&#26500;&#21270;&#31185;&#23398;&#25991;&#26412;&#20013;&#25552;&#21462;&#37329;&#32435;&#31859;&#26834;&#30340;&#32467;&#26500;&#21270;&#22810;&#27493;&#31181;&#23376;&#20171;&#23548;&#29983;&#38271;&#36807;&#31243;&#21644;&#32467;&#26524;&#12290;&#23558;GPT-3&#30340;&#25552;&#31034;&#23436;&#25104;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#39044;&#27979;JSON&#25991;&#26723;&#24418;&#24335;&#30340;&#21512;&#25104;&#27169;&#26495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although gold nanorods have been the subject of much research, the pathways for controlling their shape and thereby their optical properties remain largely heuristically understood. Although it is apparent that the simultaneous presence of and interaction between various reagents during synthesis control these properties, computational and experimental approaches for exploring the synthesis space can be either intractable or too time-consuming in practice. This motivates an alternative approach leveraging the wealth of synthesis information already embedded in the body of scientific literature by developing tools to extract relevant structured data in an automated, high-throughput manner. To that end, we present an approach using the powerful GPT-3 language model to extract structured multi-step seed-mediated growth procedures and outcomes for gold nanorods from unstructured scientific text. GPT-3 prompt completions are fine-tuned to predict synthesis templates in the form of JSON docu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25910;&#38598;&#21644;&#35780;&#20272;&#22810;&#26041;&#23545;&#35805;&#24773;&#20917;&#65292;&#25506;&#35752;&#20102;&#27169;&#22411;&#22312;&#32676;&#20307;&#23545;&#35805;&#20013;&#38656;&#35201;&#20855;&#22791;&#30340;&#25216;&#33021;&#65292;&#21457;&#29616;&#26032;&#25968;&#25454;&#38598;MultiLIGHT&#21487;&#20197;&#22312;&#36825;&#20010;&#39046;&#22495;&#24102;&#26469;&#26174;&#30528;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2304.13835</link><description>&lt;p&gt;
&#22810;&#26041;&#32842;&#22825;&#65306;&#20154;&#31867;&#21644;&#27169;&#22411;&#20013;&#30340;&#32676;&#32842;&#23545;&#35805;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Multi-Party Chat: Conversational Agents in Group Settings with Humans and Models. (arXiv:2304.13835v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25910;&#38598;&#21644;&#35780;&#20272;&#22810;&#26041;&#23545;&#35805;&#24773;&#20917;&#65292;&#25506;&#35752;&#20102;&#27169;&#22411;&#22312;&#32676;&#20307;&#23545;&#35805;&#20013;&#38656;&#35201;&#20855;&#22791;&#30340;&#25216;&#33021;&#65292;&#21457;&#29616;&#26032;&#25968;&#25454;&#38598;MultiLIGHT&#21487;&#20197;&#22312;&#36825;&#20010;&#39046;&#22495;&#24102;&#26469;&#26174;&#30528;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#23545;&#35805;&#30740;&#31350;&#20027;&#35201;&#30740;&#31350;&#25104;&#23545;&#65288;&#21452;&#26041;&#65289;&#23545;&#35805;&#65292;&#24182;&#27809;&#26377;&#28041;&#21450;&#21040;&#22810;&#20110;&#20004;&#20010;&#20154;&#22312;&#19968;&#36215;&#23545;&#35805;&#30340;&#26085;&#24120;&#24773;&#26223;&#12290;&#26412;&#25991;&#20351;&#29992;LIGHT&#29615;&#22659;&#26500;&#24314;&#25509;&#22320;&#23545;&#35805;&#26469;&#25910;&#38598;&#21644;&#35780;&#20272;&#22810;&#26041;&#23545;&#35805;&#24773;&#20917;&#12290;&#25105;&#20204;&#23545;&#27604;&#22312;&#26032;&#25968;&#25454;&#38598;MultiLIGHT&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#21644;&#29616;&#26377;&#30340;&#25104;&#23545;&#35757;&#32451;&#30340;&#23545;&#35805;&#27169;&#22411;&#20197;&#21450;&#24102;&#26377;&#23569;&#37327;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25105;&#20204;&#23558;&#20844;&#24320;&#21457;&#24067;MultiLIGHT&#25968;&#25454;&#38598;&#65292;&#36825;&#23558;&#26377;&#21161;&#20110;&#22312;&#32676;&#20307;&#35774;&#32622;&#20013;&#24102;&#26469;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current dialogue research primarily studies pairwise (two-party) conversations, and does not address the everyday setting where more than two speakers converse together. In this work, we both collect and evaluate multi-party conversations to study this more general case. We use the LIGHT environment to construct grounded conversations, where each participant has an assigned character to role-play. We thus evaluate the ability of language models to act as one or more characters in such conversations. Models require two skills that pairwise-trained models appear to lack: (1) being able to decide when to talk; (2) producing coherent utterances grounded on multiple characters. We compare models trained on our new dataset to existing pairwise-trained dialogue models, as well as large language models with few-shot prompting. We find that our new dataset, MultiLIGHT, which we will publicly release, can help bring significant improvements in the group setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#23556;&#22810;&#35821;&#35328;&#35789;&#20041;&#28040;&#27495;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#25110;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;XL-WSD&#25968;&#25454;&#38598;&#30340;18&#31181;&#35821;&#35328;&#19978;&#23454;&#29616;&#20102;&#36229;&#36234;&#23436;&#20840;&#30417;&#30563;&#22522;&#32447;&#30340;&#21484;&#22238;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.13803</link><description>&lt;p&gt;
&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#23556;&#22810;&#35821;&#35328;&#35789;&#20041;&#28040;&#27495;
&lt;/p&gt;
&lt;p&gt;
Translate to Disambiguate: Zero-shot Multilingual Word Sense Disambiguation with Pretrained Language Models. (arXiv:2304.13803v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#23556;&#22810;&#35821;&#35328;&#35789;&#20041;&#28040;&#27495;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#25110;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;XL-WSD&#25968;&#25454;&#38598;&#30340;18&#31181;&#35821;&#35328;&#19978;&#23454;&#29616;&#20102;&#36229;&#36234;&#23436;&#20840;&#30417;&#30563;&#22522;&#32447;&#30340;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#23398;&#20064;&#20016;&#23500;&#30340;&#36328;&#35821;&#35328;&#30693;&#35782;&#65292;&#22312;&#32763;&#35793;&#21644;&#22810;&#35821;&#35328;&#35789;&#20041;&#28040;&#27495;&#65288;WSD&#65289;&#31561;&#22810;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#26032;&#30740;&#31350;&#65292;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#19978;&#19979;&#25991;&#35789;&#32423;&#32763;&#35793;&#65288;C-WLT&#65289;&#25429;&#25417;PLMs&#30340;&#36328;&#35821;&#35328;&#35789;&#20041;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#65292;PLMs&#32534;&#30721;&#26356;&#22810;&#30340;&#36328;&#35821;&#35328;&#35789;&#20041;&#30693;&#35782;&#65292;&#24182;&#26356;&#22909;&#22320;&#21033;&#29992;&#19978;&#19979;&#25991;&#26469;&#25552;&#39640;WLT&#24615;&#33021;&#12290;&#22312;C-WLT&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#38646;&#23556;WSD&#26041;&#27861;&#65292;&#24182;&#22312;XL-WSD&#25968;&#25454;&#38598;&#30340;18&#31181;&#35821;&#35328;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained Language Models (PLMs) learn rich cross-lingual knowledge and can be finetuned to perform well on diverse tasks such as translation and multilingual word sense disambiguation (WSD). However, they often struggle at disambiguating word sense in a zero-shot setting. To better understand this contrast, we present a new study investigating how well PLMs capture cross-lingual word sense with Contextual Word-Level Translation (C-WLT), an extension of word-level translation that prompts the model to translate a given word in context. We find that as the model size increases, PLMs encode more cross-lingual word sense knowledge and better use context to improve WLT performance. Building on C-WLT, we introduce a zero-shot approach for WSD, tested on 18 languages from the XL-WSD dataset. Our method outperforms fully supervised baselines on recall for many evaluation languages without additional training or finetuning. This study presents a first step towards understanding how to best le
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35782;&#21035;&#20986;&#25968;&#25454;&#38598;&#20013;&#30340;&#24322;&#24120;&#20363;&#23376;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#20462;&#21098;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#36825;&#21487;&#20197;&#20351;&#24471;&#20197;&#36825;&#20123;&#25968;&#25454;&#38598;&#23376;&#38598;&#20316;&#20026;&#35757;&#32451;&#38598;&#24494;&#35843;&#27169;&#22411;&#30340;&#24615;&#33021;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2304.13783</link><description>&lt;p&gt;
&#24102;&#26377;&#24322;&#24120;&#26696;&#20363;&#30340;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Fine Tuning with Abnormal Examples. (arXiv:2304.13783v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13783
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35782;&#21035;&#20986;&#25968;&#25454;&#38598;&#20013;&#30340;&#24322;&#24120;&#20363;&#23376;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#20462;&#21098;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#36825;&#21487;&#20197;&#20351;&#24471;&#20197;&#36825;&#20123;&#25968;&#25454;&#38598;&#23376;&#38598;&#20316;&#20026;&#35757;&#32451;&#38598;&#24494;&#35843;&#27169;&#22411;&#30340;&#24615;&#33021;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#20247;&#21253;&#21171;&#21160;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25968;&#25454;&#38598;&#20013;&#30340;&#26222;&#36941;&#24212;&#29992;&#65292;&#19978;&#36848;&#25968;&#25454;&#38598;&#21464;&#24471;&#36234;&#26469;&#36234;&#22823;&#12290;&#20363;&#22914;&#65292;SQUAD&#25968;&#25454;&#38598;&#24403;&#21069;&#24050;&#32463;&#36229;&#36807;80,000&#26465;&#35760;&#24405;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#33521;&#35821;&#30340;&#32467;&#26500;&#30456;&#23545;&#37325;&#22797;&#65292;&#22240;&#27492;SQUAD&#25968;&#25454;&#38598;&#19978;&#19979;&#25991;&#20013;&#21333;&#35789;&#39057;&#29575;&#30340;&#20998;&#24067;&#30456;&#23545;&#19981;&#21464;&#12290;&#36890;&#36807;&#27979;&#37327;&#27599;&#20010;&#21477;&#23376;&#19982;&#25968;&#25454;&#38598;&#20013;&#25152;&#26377;&#21477;&#23376;&#39057;&#29575;&#30340;&#20849;&#21464;&#36317;&#31163;&#65292;&#25105;&#20204;&#35782;&#21035;&#20986;&#20102;10,500&#20010;&#20363;&#23376;&#65292;&#20026;&#35757;&#32451;&#21019;&#36896;&#20102;&#26356;&#21152;&#22343;&#21248;&#30340;&#20998;&#24067;&#12290;&#22312;&#36825;&#20010;&#20363;&#23376;&#30340;&#23376;&#38598;&#19978;&#24494;&#35843;ELECTRA [4]&#36798;&#21040;&#20102;&#27604;&#22312;&#25152;&#26377;87,000&#20010;&#20363;&#23376;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#31995;&#32479;&#22320;&#20462;&#21098;&#29992;&#20110;&#24494;&#35843;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#25552;&#39640;&#22806;&#37096;&#26679;&#26412;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the prevalence of crowd sourced labor in creating Natural Language processing datasets, these aforementioned sets have become increasingly large. For instance, the SQUAD dataset currently sits at over 80,000 records. However, because the English language is rather repetitive in structure, the distribution of word frequencies in the SQUAD dataset's contexts are relatively unchanged. By measuring each sentences distance from the co-variate distance of frequencies of all sentences in the dataset, we identify 10,500 examples that create a more uniform distribution for training. While fine-tuning ELECTRA [4] on this subset of examples reaches better performance to a model trained on all 87,000 examples. Herein we introduce a methodology for systematically pruning datasets for fine tuning reaching better out of sample performance.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;LLM&#29983;&#25104;&#19981;&#20934;&#30830;&#25110;&#34394;&#20551;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;LLM&#30340;&#38544;&#34255;&#23618;&#28608;&#27963;&#26469;&#30830;&#23450;&#35821;&#21477;&#30340;&#30495;&#23454;&#24615;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#26816;&#27979;&#25928;&#26524;&#65292;&#24182;&#26377;&#21033;&#20110;&#25552;&#39640;LLM&#30340;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.13734</link><description>&lt;p&gt;
&#19968;&#20010;LLM&#30693;&#36947;&#33258;&#24049;&#22312;&#25746;&#35854;&#30340;&#20869;&#37096;&#29366;&#24577;
&lt;/p&gt;
&lt;p&gt;
The Internal State of an LLM Knows When its Lying. (arXiv:2304.13734v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13734
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;LLM&#29983;&#25104;&#19981;&#20934;&#30830;&#25110;&#34394;&#20551;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;LLM&#30340;&#38544;&#34255;&#23618;&#28608;&#27963;&#26469;&#30830;&#23450;&#35821;&#21477;&#30340;&#30495;&#23454;&#24615;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#26816;&#27979;&#25928;&#26524;&#65292;&#24182;&#26377;&#21033;&#20110;&#25552;&#39640;LLM&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#65288;&#21487;&#33021;&#65289;&#26368;&#20026;&#31361;&#20986;&#30340;&#32570;&#28857;&#26159;&#20197;&#33258;&#20449;&#30340;&#35821;&#27668;&#29983;&#25104;&#19981;&#20934;&#30830;&#25110;&#34394;&#20551;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#20551;&#35774;LLM&#30340;&#20869;&#37096;&#29366;&#24577;&#21487;&#20197;&#29992;&#20110;&#25581;&#31034;&#19968;&#20010;&#35821;&#21477;&#30340;&#30495;&#23454;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;LLM&#25152;&#29983;&#25104;&#35821;&#21477;&#30340;&#30495;&#23454;&#24615;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;LLM&#30340;&#38544;&#34255;&#23618;&#28608;&#27963;&#26469;&#30830;&#23450;&#35821;&#21477;&#30340;&#30495;&#23454;&#24615;&#12290;&#20026;&#20102;&#35757;&#32451;&#21644;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#20845;&#20010;&#19981;&#21516;&#20027;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#30495;&#23454;&#21644;&#34394;&#20551;&#30340;&#35821;&#21477;&#12290;&#19968;&#20010;&#20998;&#31867;&#22120;&#34987;&#35757;&#32451;&#20986;&#26469;&#65292;&#26681;&#25454;LLM&#30340;&#28608;&#27963;&#20540;&#26469;&#26816;&#27979;&#21738;&#20010;&#35821;&#21477;&#26159;&#30495;&#23454;&#30340;&#25110;&#34394;&#20551;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20998;&#31867;&#22120;&#25509;&#25910;LLM&#20026;&#25968;&#25454;&#38598;&#20013;&#27599;&#20010;&#35821;&#21477;&#29983;&#25104;&#30340;&#28608;&#27963;&#20540;&#20316;&#20026;&#36755;&#20837;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#26816;&#27979;&#35821;&#21477;&#30495;&#23454;&#24615;&#30340;&#26041;&#27861;&#29978;&#33267;&#27604;&#23569;&#37327;&#25552;&#31034;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#20984;&#26174;&#20102;&#21033;&#29992;LLM&#30340;&#20869;&#37096;&#29366;&#24577;&#26469;&#25552;&#39640;&#20854;&#21487;&#20449;&#24230;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Large Language Models (LLMs) have shown exceptional performance in various tasks, their (arguably) most prominent drawback is generating inaccurate or false information with a confident tone. In this paper, we hypothesize that the LLM's internal state can be used to reveal the truthfulness of a statement. Therefore, we introduce a simple yet effective method to detect the truthfulness of LLM-generated statements, which utilizes the LLM's hidden layer activations to determine the veracity of statements. To train and evaluate our method, we compose a dataset of true and false statements in six different topics. A classifier is trained to detect which statement is true or false based on an LLM's activation values. Specifically, the classifier receives as input the activation values from the LLM for each of the statements in the dataset. Our experiments demonstrate that our method for detecting statement veracity significantly outperforms even few-shot prompting methods, highlighting
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#30340;LLM Flan-T5&#20316;&#20026;&#25991;&#26412;&#32534;&#30721;&#22120;&#21644;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;(LDM)&#30340;&#26041;&#27861;TANGO&#29983;&#25104;&#25991;&#26412;&#21040;&#38899;&#39057;(TTA)&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;AudioCaps&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#20808;&#36827;&#30340;AudioLDM&#12290;</title><link>http://arxiv.org/abs/2304.13731</link><description>&lt;p&gt;
&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#30340;LLM&#21644;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#21040;&#38899;&#39057;
&lt;/p&gt;
&lt;p&gt;
Text-to-Audio Generation using Instruction-Tuned LLM and Latent Diffusion Model. (arXiv:2304.13731v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#30340;LLM Flan-T5&#20316;&#20026;&#25991;&#26412;&#32534;&#30721;&#22120;&#21644;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;(LDM)&#30340;&#26041;&#27861;TANGO&#29983;&#25104;&#25991;&#26412;&#21040;&#38899;&#39057;(TTA)&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;AudioCaps&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#20808;&#36827;&#30340;AudioLDM&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#24040;&#22823;&#35268;&#27169;&#20801;&#35768;&#35768;&#22810;&#26377;&#36259;&#30340;&#23646;&#24615;&#65292;&#27604;&#22914;&#65292;&#22522;&#20110;&#25351;&#20196;&#21644;&#24605;&#36335;&#38142;&#30340;&#24494;&#35843;&#65292;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20219;&#21153;&#20013;&#26174;&#30528;&#25552;&#39640;&#20102;&#38646;&#27425;&#21644;&#23569;&#37327;&#35757;&#32451;&#26679;&#26412;&#30340;&#24615;&#33021;&#12290;&#21463;&#21040;&#36825;&#20123;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#36825;&#26679;&#19968;&#31181;&#32463;&#36807;&#25351;&#20196;&#35843;&#25972;&#30340;LLM Flan-T5&#20316;&#20026;&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#25991;&#26412;&#21040;&#38899;&#39057;(TTA)&#29983;&#25104;&#20219;&#21153;&#8212;&#8212;&#30446;&#26631;&#26159;&#26681;&#25454;&#20854;&#25991;&#26412;&#25551;&#36848;&#29983;&#25104;&#38899;&#39057;&#12290;&#20043;&#21069;&#20851;&#20110;TTA&#30340;&#24037;&#20316;&#35201;&#20040;&#39044;&#20808;&#35757;&#32451;&#19968;&#20010;&#32852;&#21512;&#30340;&#25991;&#26412;-&#38899;&#39057;&#32534;&#30721;&#22120;&#65292;&#35201;&#20040;&#20351;&#29992;&#19968;&#20010;&#38750;&#25351;&#20196;&#35843;&#35856;&#30340;&#27169;&#22411;&#65292;&#22914;T5&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;(LDM)&#30340;&#26041;&#27861;TANGO&#22312;AudioCaps&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#20986;&#27604;&#26368;&#20808;&#36827;&#30340;AudioLDM&#26356;&#22909;&#30340;&#22823;&#22810;&#25968;&#25351;&#26631;&#65292;&#24182;&#22312;&#20854;&#20313;&#25351;&#26631;&#19978;&#25345;&#24179;&#65292;&#23613;&#31649;&#25105;&#20204;&#20351;&#29992;&#20102;63&#20493;&#23567;&#30340;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;LDM&#65292;&#24182;&#20445;&#25345;&#25991;&#26412;&#32534;&#30721;&#22120;&#19981;&#21464;&#12290;&#36825;&#31181;&#25913;&#36827;&#21487;&#33021;&#36824;&#24402;&#22240;&#20110;&#37319;&#29992;&#22522;&#20110;&#38899;&#39057;&#21387;&#21147;&#32423;&#30340;&#28151;&#38899;&#35757;&#32451;&#38598;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
The immense scale of the recent large language models (LLM) allows many interesting properties, such as, instruction- and chain-of-thought-based fine-tuning, that has significantly improved zero- and few-shot performance in many natural language processing (NLP) tasks. Inspired by such successes, we adopt such an instruction-tuned LLM Flan-T5 as the text encoder for text-to-audio (TTA) generation -- a task where the goal is to generate an audio from its textual description. The prior works on TTA either pre-trained a joint text-audio encoder or used a non-instruction-tuned model, such as, T5. Consequently, our latent diffusion model (LDM)-based approach TANGO outperforms the state-of-the-art AudioLDM on most metrics and stays comparable on the rest on AudioCaps test set, despite training the LDM on a 63 times smaller dataset and keeping the text encoder frozen. This improvement might also be attributed to the adoption of audio pressure level-based sound mixing for training set augmenta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;LLMs&#30340;&#20351;&#29992;&#32508;&#36848;&#65292;&#25506;&#35752;&#20102;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#20351;&#29992;&#21644;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2304.13712</link><description>&lt;p&gt;
&#21457;&#25381;LLMs&#22312;&#23454;&#36341;&#20013;&#30340;&#21147;&#37327;&#65306;ChatGPT&#21450;&#20854;&#24212;&#29992;&#30340;&#32508;&#36848;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond. (arXiv:2304.13712v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;LLMs&#30340;&#20351;&#29992;&#32508;&#36848;&#65292;&#25506;&#35752;&#20102;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#20351;&#29992;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20026;&#20174;&#20107;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#30340;&#20174;&#19994;&#20154;&#21592;&#21644;&#26368;&#32456;&#29992;&#25143;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#23454;&#29992;&#30340;&#25351;&#21335;&#65292;&#20171;&#32461;&#20102;&#22914;&#20309;&#21033;&#29992;Large Language Models&#65288;LLMs&#65289;&#12290;&#25105;&#20204;&#20174;&#27169;&#22411;&#12289;&#25968;&#25454;&#21644;&#19979;&#28216;&#20219;&#21153;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;LLMs&#30340;&#20351;&#29992;&#35752;&#35770;&#21644;&#35265;&#35299;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#24403;&#21069;&#30340;GPT&#21644;BERT&#26679;&#24335;&#30340;LLMs&#12290;&#28982;&#21518;&#65292;&#35752;&#35770;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#12289;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35814;&#32454;&#35752;&#35770;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#20351;&#29992;&#21644;&#38750;&#20351;&#29992;&#24773;&#20917;&#65292;&#20363;&#22914;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#12289;&#20256;&#32479;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#12289;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#12289;&#32039;&#24613;&#33021;&#21147;&#20197;&#21450;&#29305;&#23450;&#20219;&#21153;&#30340;&#32771;&#34385;&#12290;&#25105;&#20204;&#21576;&#29616;&#20102;&#21508;&#31181;&#20351;&#29992;&#21644;&#38750;&#20351;&#29992;&#24773;&#20917;&#65292;&#20197;&#35828;&#26126;LLMs&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#30340;&#23454;&#38469;&#24212;&#29992;&#21644;&#38480;&#21046;&#12290;&#25105;&#20204;&#36824;&#35797;&#22270;&#20102;&#35299;&#25968;&#25454;&#23545;&#20110;LLMs&#24212;&#29992;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a comprehensive and practical guide for practitioners and end-users working with Large Language Models (LLMs) in their downstream natural language processing (NLP) tasks. We provide discussions and insights into the usage of LLMs from the perspectives of models, data, and downstream tasks. Firstly, we offer an introduction and brief summary of current GPT- and BERT-style LLMs. Then, we discuss the influence of pre-training data, training data, and test data. Most importantly, we provide a detailed discussion about the use and non-use cases of large language models for various natural language processing tasks, such as knowledge-intensive tasks, traditional natural language understanding tasks, natural language generation tasks, emergent abilities, and considerations for specific tasks.We present various use cases and non-use cases to illustrate the practical applications and limitations of LLMs in real-world scenarios. We also try to understand the importance of dat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20851;&#32852;&#21040;&#29983;&#25104;&#30340;&#38646;-shot&#26041;&#27861;&#65306;&#36890;&#36807;&#23558;&#22270;&#20687;/&#35270;&#39057;&#25237;&#24433;&#21040;&#35821;&#35328;&#27169;&#24577;&#24182;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#29983;&#25104;&#25551;&#36848;&#24615;&#23383;&#24149;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#20026;&#26080;&#30417;&#30563;&#36328;&#27169;&#24577;&#26144;&#23556;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#24182;&#20855;&#26377;&#22312;&#35270;&#39057;&#23383;&#24149;&#65292;&#22270;&#20687;&#21512;&#25104;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#31561;&#39046;&#22495;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.13273</link><description>&lt;p&gt;
&#20174;&#20851;&#32852;&#21040;&#29983;&#25104;&#65306;&#26080;&#30417;&#30563;&#36328;&#27169;&#24577;&#26144;&#23556;&#30340;&#32431;&#25991;&#26412;&#23383;&#24149;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
From Association to Generation: Text-only Captioning by Unsupervised Cross-modal Mapping. (arXiv:2304.13273v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20851;&#32852;&#21040;&#29983;&#25104;&#30340;&#38646;-shot&#26041;&#27861;&#65306;&#36890;&#36807;&#23558;&#22270;&#20687;/&#35270;&#39057;&#25237;&#24433;&#21040;&#35821;&#35328;&#27169;&#24577;&#24182;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#29983;&#25104;&#25551;&#36848;&#24615;&#23383;&#24149;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#20026;&#26080;&#30417;&#30563;&#36328;&#27169;&#24577;&#26144;&#23556;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#24182;&#20855;&#26377;&#22312;&#35270;&#39057;&#23383;&#24149;&#65292;&#22270;&#20687;&#21512;&#25104;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#31561;&#39046;&#22495;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20197;CLIP&#21644;ALIGN&#20026;&#20195;&#34920;&#30340;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;CLIP&#30340;&#38646;-shot&#33021;&#21147;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#31561;&#22522;&#20110;&#20851;&#32852;&#30340;&#35270;&#35273;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#12290;&#20294;&#26159;&#65292;CLIP&#38590;&#20197;&#24212;&#29992;&#20110;&#22522;&#20110;&#29983;&#25104;&#30340;&#20219;&#21153;&#12290;&#36825;&#26159;&#30001;&#20110;&#32570;&#20047;&#35299;&#30721;&#22120;&#26550;&#26500;&#21644;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;K&#26368;&#36817;&#37051;&#36328;&#27169;&#24577;&#26144;&#23556;&#65288;Knight&#65289;&#65292;&#19968;&#31181;&#20174;&#20851;&#32852;&#21040;&#29983;&#25104;&#30340;&#38646;-shot&#26041;&#27861;&#12290;&#36890;&#36807;&#31364;&#23383;&#24149;&#20219;&#21153;&#30340;&#32431;&#25991;&#26412;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#26469;&#26377;&#25928;&#22320;&#23558;&#22270;&#20687;/&#35270;&#39057;&#25237;&#24433;&#21040;&#35821;&#35328;&#27169;&#24577;&#24182;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#29983;&#25104;&#25551;&#36848;&#24615;&#23383;&#24149;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Knight&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#26080;&#30417;&#30563;&#36328;&#27169;&#24577;&#26144;&#23556;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#24182;&#19988;&#23558;&#22312;&#35270;&#39057;&#23383;&#24149;&#65292;&#22270;&#20687;&#21512;&#25104;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#31561;&#39046;&#22495;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of Vision-Language Pre-training Models (VLPMs) represented by CLIP and ALIGN, significant breakthroughs have been achieved for association-based visual tasks such as image classification and image-text retrieval by the zero-shot capability of CLIP without fine-tuning. However, CLIP is hard to apply to generation-based tasks. This is due to the lack of decoder architecture and pre-training tasks for generation. Although previous works have created generation capacity for CLIP through additional language models, a modality gap between the CLIP representations of different modalities and the inability of CLIP to model the offset of this gap, which fails the concept to transfer across modalities. To solve the problem, we try to map images/videos to the language modality and generate captions from the language modality. In this paper, we propose the K-nearest-neighbor Cross-modality Mapping (Knight), a zero-shot method from association to generation. With text-only unsu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;Transformer&#27169;&#22411;&#65288;XBERT&#65289;&#36827;&#34892;&#26497;&#31471;&#22810;&#26631;&#31614;&#20998;&#31867;&#65292;&#36890;&#36807;&#23558;KG&#31867;&#22411;&#22522;&#20110;&#38382;&#39064;&#25991;&#26412;&#20351;&#29992;&#32467;&#26500;&#21644;&#35821;&#20041;&#29305;&#24449;&#36827;&#34892;&#32858;&#31867;&#65292;&#20197;&#25552;&#39640;&#38382;&#39064;&#22238;&#31572;&#65288;QA&#65289;&#31995;&#32479;&#20013;&#35821;&#20041;&#31572;&#26696;&#31867;&#22411;&#39044;&#27979;&#65288;SMART&#65289;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.12395</link><description>&lt;p&gt;
&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#31572;&#26696;&#31867;&#22411;&#39044;&#27979;&#30340;&#26497;&#38480;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Extreme Classification for Answer Type Prediction in Question Answering. (arXiv:2304.12395v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;Transformer&#27169;&#22411;&#65288;XBERT&#65289;&#36827;&#34892;&#26497;&#31471;&#22810;&#26631;&#31614;&#20998;&#31867;&#65292;&#36890;&#36807;&#23558;KG&#31867;&#22411;&#22522;&#20110;&#38382;&#39064;&#25991;&#26412;&#20351;&#29992;&#32467;&#26500;&#21644;&#35821;&#20041;&#29305;&#24449;&#36827;&#34892;&#32858;&#31867;&#65292;&#20197;&#25552;&#39640;&#38382;&#39064;&#22238;&#31572;&#65288;QA&#65289;&#31995;&#32479;&#20013;&#35821;&#20041;&#31572;&#26696;&#31867;&#22411;&#39044;&#27979;&#65288;SMART&#65289;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#31572;&#26696;&#31867;&#22411;&#39044;&#27979;&#65288;SMART&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#38382;&#39064;&#22238;&#31572;&#65288;QA&#65289;&#31995;&#32479;&#30340;&#26377;&#29992;&#27493;&#39588;&#12290; SMART&#20219;&#21153;&#28041;&#21450;&#39044;&#27979;&#32473;&#23450;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#30340;&#21069;k&#20010;&#30693;&#35782;&#22270;&#65288;KG&#65289;&#31867;&#22411;&#12290;&#30001;&#20110;KG&#20013;&#23384;&#22312;&#22823;&#37327;&#31867;&#22411;&#65292;&#36825;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;Transformer&#27169;&#22411;&#65288;XBERT&#65289;&#36827;&#34892;&#26497;&#31471;&#22810;&#26631;&#31614;&#20998;&#31867;&#65292;&#36890;&#36807;&#23558;KG&#31867;&#22411;&#22522;&#20110;&#38382;&#39064;&#25991;&#26412;&#20351;&#29992;&#32467;&#26500;&#21644;&#35821;&#20041;&#29305;&#24449;&#36827;&#34892;&#32858;&#31867;&#12290;&#25105;&#20204;&#20855;&#20307;&#22320;&#25913;&#21892;&#20102;XBERT&#27969;&#31243;&#30340;&#32858;&#31867;&#38454;&#27573;&#65292;&#21033;&#29992;&#20174;KG&#20013;&#27966;&#29983;&#30340;&#25991;&#26412;&#21644;&#32467;&#26500;&#29305;&#24449;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#20123;&#29305;&#24449;&#21487;&#20197;&#25552;&#39640;SMART&#20219;&#21153;&#30340;&#31471;&#21040;&#31471;&#24615;&#33021;&#65292;&#24182;&#20135;&#29983;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic answer type prediction (SMART) is known to be a useful step towards effective question answering (QA) systems. The SMART task involves predicting the top-$k$ knowledge graph (KG) types for a given natural language question. This is challenging due to the large number of types in KGs. In this paper, we propose use of extreme multi-label classification using Transformer models (XBERT) by clustering KG types using structural and semantic features based on question text. We specifically improve the clustering stage of the XBERT pipeline using textual and structural features derived from KGs. We show that these features can improve end-to-end performance for the SMART task, and yield state-of-the-art results.
&lt;/p&gt;</description></item><item><title>&#35813;&#32508;&#36848;&#20171;&#32461;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25991;&#26412;&#20998;&#31867;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#21487;&#20197;&#30452;&#25509;&#22788;&#29702;&#22797;&#26434;&#32467;&#26500;&#21270;&#25991;&#26412;&#25968;&#25454;&#24182;&#21033;&#29992;&#20840;&#23616;&#20449;&#24687;&#12290;&#35768;&#22810;&#30495;&#23454;&#30340;&#25991;&#26412;&#20998;&#31867;&#24212;&#29992;&#31243;&#24207;&#21487;&#20197;&#33258;&#28982;&#22320;&#34920;&#31034;&#20026;&#19968;&#20010;&#22270;&#12290;&#26412;&#32508;&#36848;&#35206;&#30422;&#21040;2023&#24180;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#35821;&#26009;&#24211;&#32423;&#21035;&#21644;&#25991;&#26723;&#32423;&#21035;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#35814;&#32454;&#35752;&#35770;&#20102;&#27599;&#31181;&#26041;&#27861;&#30340;&#22270;&#26500;&#24314;&#26426;&#21046;&#21644;&#22522;&#20110;&#22270;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#28085;&#30422;&#20102;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#23454;&#39564;&#35774;&#35745;&#65292;&#24182;&#24635;&#32467;&#20102;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21457;&#24067;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.11534</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25991;&#26412;&#20998;&#31867;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks for Text Classification: A Survey. (arXiv:2304.11534v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11534
&lt;/p&gt;
&lt;p&gt;
&#35813;&#32508;&#36848;&#20171;&#32461;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25991;&#26412;&#20998;&#31867;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#21487;&#20197;&#30452;&#25509;&#22788;&#29702;&#22797;&#26434;&#32467;&#26500;&#21270;&#25991;&#26412;&#25968;&#25454;&#24182;&#21033;&#29992;&#20840;&#23616;&#20449;&#24687;&#12290;&#35768;&#22810;&#30495;&#23454;&#30340;&#25991;&#26412;&#20998;&#31867;&#24212;&#29992;&#31243;&#24207;&#21487;&#20197;&#33258;&#28982;&#22320;&#34920;&#31034;&#20026;&#19968;&#20010;&#22270;&#12290;&#26412;&#32508;&#36848;&#35206;&#30422;&#21040;2023&#24180;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#35821;&#26009;&#24211;&#32423;&#21035;&#21644;&#25991;&#26723;&#32423;&#21035;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#35814;&#32454;&#35752;&#35770;&#20102;&#27599;&#31181;&#26041;&#27861;&#30340;&#22270;&#26500;&#24314;&#26426;&#21046;&#21644;&#22522;&#20110;&#22270;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#28085;&#30422;&#20102;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#23454;&#39564;&#35774;&#35745;&#65292;&#24182;&#24635;&#32467;&#20102;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21457;&#24067;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26368;&#22522;&#26412;&#21644;&#26368;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#35768;&#22810;&#26368;&#36817;&#30340;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#37319;&#29992;&#20102;&#24207;&#21015;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#20294;&#26159;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#22788;&#29702;&#22797;&#26434;&#32467;&#26500;&#21270;&#25991;&#26412;&#25968;&#25454;&#24182;&#21033;&#29992;&#20840;&#23616;&#20449;&#24687;&#12290;&#35768;&#22810;&#30495;&#23454;&#30340;&#25991;&#26412;&#20998;&#31867;&#24212;&#29992;&#31243;&#24207;&#21487;&#20197;&#33258;&#28982;&#22320;&#34920;&#31034;&#20026;&#19968;&#20010;&#22270;&#65292;&#20854;&#20013;&#25429;&#33719;&#20102;&#21333;&#35789;&#12289;&#25991;&#26723;&#21644;&#35821;&#26009;&#24211;&#30340;&#20840;&#23616;&#29305;&#24449;&#12290;&#26412;&#32508;&#36848;&#23558;&#35206;&#30422;&#21040;2023&#24180;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#35821;&#26009;&#24211;&#32423;&#21035;&#21644;&#25991;&#26723;&#32423;&#21035;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#35814;&#32454;&#35752;&#35770;&#20102;&#27599;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#22270;&#26500;&#24314;&#26426;&#21046;&#21644;&#22522;&#20110;&#22270;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#38500;&#20102;&#25216;&#26415;&#32508;&#36848;&#65292;&#25105;&#20204;&#36824;&#20851;&#27880;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#30340;&#38382;&#39064;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;&#25105;&#20204;&#36824;&#28085;&#30422;&#20102;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#23454;&#39564;&#35774;&#35745;&#65292;&#24182;&#21576;&#29616;&#20102;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21457;&#24067;&#30340;&#24615;&#33021;&#24635;&#32467;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25991;&#26412;&#20998;&#31867;&#39046;&#22495;&#30340;&#26368;&#26032;&#25216;&#26415;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text Classification is the most essential and fundamental problem in Natural Language Processing. While numerous recent text classification models applied the sequential deep learning technique, graph neural network-based models can directly deal with complex structured text data and exploit global information. Many real text classification applications can be naturally cast into a graph, which captures words, documents, and corpus global features. In this survey, we bring the coverage of methods up to 2023, including corpus-level and document-level graph neural networks. We discuss each of these methods in detail, dealing with the graph construction mechanisms and the graph-based learning process. As well as the technological survey, we look at issues behind and future directions addressed in text classification using graph neural networks. We also cover datasets, evaluation metrics, and experiment design and present a summary of published performance on the publicly available benchma
&lt;/p&gt;</description></item><item><title>DIN-SQL&#36890;&#36807;&#23558;&#22797;&#26434;&#30340;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#38382;&#39064;&#65292;&#24182;&#23558;&#36825;&#20123;&#23376;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#39304;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#23427;&#20204;&#30340;&#34920;&#29616;&#65292;&#20351;&#20934;&#30830;&#24615;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2304.11015</link><description>&lt;p&gt;
DIN-SQL: &#33258;&#32416;&#27491;&#30340;&#25991;&#26412;&#21040;SQL&#20998;&#35299;&#24335;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction. (arXiv:2304.11015v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11015
&lt;/p&gt;
&lt;p&gt;
DIN-SQL&#36890;&#36807;&#23558;&#22797;&#26434;&#30340;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#38382;&#39064;&#65292;&#24182;&#23558;&#36825;&#20123;&#23376;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#39304;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#23427;&#20204;&#30340;&#34920;&#29616;&#65292;&#20351;&#20934;&#30830;&#24615;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#22797;&#26434;&#30340;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#23376;&#20219;&#21153;&#65292;&#24182;&#19988;&#36825;&#31181;&#20998;&#35299;&#22914;&#20309;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23613;&#31649;SQL&#26597;&#35810;&#20855;&#26377;&#22768;&#26126;&#24335;&#32467;&#26500;&#65292;&#20294;&#21487;&#20197;&#23558;&#20854;&#20998;&#35299;&#20026;&#23376;&#38382;&#39064;&#65292;&#24182;&#23558;&#36825;&#20123;&#23376;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#39304;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#23427;&#20204;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#31283;&#23450;&#25552;&#39640;&#19977;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#22823;&#32422;&#25552;&#39640;&#20102;10&#65285;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#25512;&#21521;&#26368;&#26032;&#27700;&#24179;&#65292;&#24182;&#22312;Holdout Spider&#25968;&#25454;&#38598;&#19978;&#29978;&#33267;&#36229;&#36807;&#20102;&#32463;&#36807;&#31934;&#35843;&#30340;&#22823;&#22411;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of decomposing a complex text-to-sql task into smaller sub-tasks and how such a decomposition can significantly improve the performance of Large Language Models (LLMs) in the reasoning process. There is currently a significant gap between the performance of fine-tuned models and prompting approaches using LLMs on challenging text-to-sql datasets such as Spider. We show that SQL queries, despite their declarative structure, can be broken down into sub-problems and the solutions of those sub-problems can be fed into LLMs to significantly improve their performance. Our experiments with three LLMs show that this approach consistently improves their performance by roughly 10%, pushing the accuracy of LLMs towards state-of-the-art, and even beating large fine-tuned models on the holdout Spider dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#21382;&#31243;&#20197;&#21450;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#65292;&#24182;&#24378;&#35843;&#27169;&#22411;&#25193;&#23637;&#23558;&#24102;&#26469;&#24615;&#33021;&#25913;&#36827;&#21644;&#29305;&#27530;&#33021;&#21147;&#30340;&#21457;&#25496;&#12290;</title><link>http://arxiv.org/abs/2303.18223</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Large Language Models. (arXiv:2303.18223v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#21382;&#31243;&#20197;&#21450;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#65292;&#24182;&#24378;&#35843;&#27169;&#22411;&#25193;&#23637;&#23558;&#24102;&#26469;&#24615;&#33021;&#25913;&#36827;&#21644;&#29305;&#27530;&#33021;&#21147;&#30340;&#21457;&#25496;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#26412;&#36136;&#19978;&#26159;&#19968;&#20010;&#30001;&#35821;&#27861;&#35268;&#21017;&#25511;&#21046;&#30340;&#22797;&#26434;&#31934;&#32454;&#30340;&#20154;&#31867;&#34920;&#36798;&#31995;&#32479;&#65292;&#23545;&#20110;&#24320;&#21457;&#29702;&#35299;&#21644;&#25484;&#25569;&#35821;&#35328;&#30340;&#33021;&#21147;&#30340;AI&#31639;&#27861;&#26469;&#35828;&#26159;&#19968;&#39033;&#37325;&#22823;&#25361;&#25112;&#12290;&#20316;&#20026;&#20027;&#35201;&#26041;&#27861;&#20043;&#19968;&#65292;&#35821;&#35328;&#24314;&#27169;&#22312;&#36807;&#21435;&#20108;&#21313;&#24180;&#37324;&#24191;&#27867;&#30740;&#31350;&#29992;&#20110;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#65292;&#20174;&#32479;&#35745;&#35821;&#35328;&#27169;&#22411;&#28436;&#21270;&#20026;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#22312;&#35299;&#20915;&#21508;&#31181;NLP&#20219;&#21153;&#26041;&#38754;&#26174;&#31034;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#30001;&#20110;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#27169;&#22411;&#32553;&#25918;&#21487;&#20197;&#23548;&#33268;&#24615;&#33021;&#25913;&#36827;&#65292;&#20182;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#22686;&#21152;&#27169;&#22411;&#35268;&#27169;&#26469;&#30740;&#31350;&#32553;&#25918;&#25928;&#24212;&#65292;&#26377;&#36259;&#30340;&#26159;&#65292;&#24403;&#21442;&#25968;&#35268;&#27169;&#36229;&#36807;&#19968;&#23450;&#27700;&#24179;&#26102;&#65292;&#36825;&#20123;&#25193;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#23454;&#29616;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#32780;&#19988;&#36824;&#26174;&#31034;&#20986;&#19968;&#20123;&#23567;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25152;&#27809;&#26377;&#30340;&#29305;&#27530;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale langu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;PheME&#65292;&#19968;&#31181;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#36827;&#34892;&#34920;&#22411;&#39044;&#27979;&#30340;&#28145;&#24230;&#38598;&#25104;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#37319;&#29992;&#22810;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#38598;&#25104;&#23398;&#20064;&#65292;&#21487;&#20197;&#20174;EHR&#25968;&#25454;&#20013;&#20934;&#30830;&#19988;&#39640;&#25928;&#22320;&#25552;&#21462;&#34920;&#22411;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2303.10794</link><description>&lt;p&gt;
PheME&#65306;&#19968;&#31181;&#28145;&#24230;&#38598;&#25104;&#26694;&#26550;&#65292;&#21487;&#20174;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#25552;&#39640;&#34920;&#22411;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
PheME: A deep ensemble framework for improving phenotype prediction from multi-modal data. (arXiv:2303.10794v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;PheME&#65292;&#19968;&#31181;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#36827;&#34892;&#34920;&#22411;&#39044;&#27979;&#30340;&#28145;&#24230;&#38598;&#25104;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#37319;&#29992;&#22810;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#38598;&#25104;&#23398;&#20064;&#65292;&#21487;&#20197;&#20174;EHR&#25968;&#25454;&#20013;&#20934;&#30830;&#19988;&#39640;&#25928;&#22320;&#25552;&#21462;&#34920;&#22411;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35814;&#32454;&#30340;&#34920;&#22411;&#20449;&#24687;&#23545;&#20110;&#30142;&#30149;&#30340;&#20934;&#30830;&#35786;&#26029;&#21644;&#39118;&#38505;&#35780;&#20272;&#33267;&#20851;&#37325;&#35201;&#12290;&#20316;&#20026;&#34920;&#22411;&#20449;&#24687;&#30340;&#20016;&#23500;&#26469;&#28304;&#65292;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHRs&#65289;&#25215;&#35834;&#36171;&#20104;&#35786;&#26029;&#21464;&#24322;&#35299;&#37322;&#30340;&#26435;&#21147;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#20174;&#24322;&#26500;&#30340;EHR&#25968;&#25454;&#20013;&#20934;&#30830;&#39640;&#25928;&#22320;&#25552;&#21462;&#34920;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PheME&#65292;&#19968;&#31181;Ensemble&#26694;&#26550;&#65292;&#20351;&#29992;&#32467;&#26500;&#21270;EHR&#21644;&#38750;&#32467;&#26500;&#21270;&#30340;&#20020;&#24202;&#31508;&#35760;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#36827;&#34892;&#20934;&#30830;&#30340;&#34920;&#22411;&#39044;&#27979;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#22810;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20174;&#31232;&#30095;&#30340;&#32467;&#26500;&#21270;EHR&#25968;&#25454;&#21644;&#20887;&#20313;&#30340;&#20020;&#24202;&#31508;&#35760;&#20013;&#23398;&#20064;&#21487;&#38752;&#30340;&#34920;&#31034;&#12290;&#22810;&#27169;&#24577;&#27169;&#22411;&#23558;&#22810;&#27169;&#24577;&#29305;&#24449;&#23545;&#40784;&#21040;&#21516;&#19968;&#28508;&#22312;&#31354;&#38388;&#20197;&#39044;&#27979;&#34920;&#22411;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21033;&#29992;&#38598;&#25104;&#23398;&#20064;&#26469;&#23558;&#21333;&#27169;&#22411;&#21644;&#22810;&#27169;&#22411;&#30340;&#36755;&#20986;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#34920;&#22411;&#39044;&#27979;&#12290;&#25105;&#20204;&#36873;&#25321;&#20102;&#19971;&#31181;&#30142;&#30149;&#26469;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#34920;&#22411;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detailed phenotype information is fundamental to accurate diagnosis and risk estimation of diseases. As a rich source of phenotype information, electronic health records (EHRs) promise to empower diagnostic variant interpretation. However, how to accurately and efficiently extract phenotypes from the heterogeneous EHR data remains a challenge. In this work, we present PheME, an Ensemble framework using Multi-modality data of structured EHRs and unstructured clinical notes for accurate Phenotype prediction. Firstly, we employ multiple deep neural networks to learn reliable representations from the sparse structured EHR data and redundant clinical notes. A multi-modal model then aligns multi-modal features onto the same latent space to predict phenotypes. Secondly, we leverage ensemble learning to combine outputs from single-modal models and multi-modal models to improve phenotype predictions. We choose seven diseases to evaluate the phenotyping performance of the proposed framework. Exp
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#31639;&#27861;&#21457;&#29616;&#35270;&#20026;&#31243;&#24207;&#25628;&#32034;&#30340;&#26041;&#27861;&#65292;&#24182;&#29992;&#20110;&#21457;&#29616;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;&#20182;&#20204;&#30340;&#26041;&#27861;&#21457;&#29616;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20248;&#21270;&#31639;&#27861;Lion&#65292;&#23427;&#27604;Adam&#26356;&#33410;&#30465;&#20869;&#23384;&#24182;&#19988;&#22312;ImageNet&#19978;&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;2&#65285;&#65292;&#24182;&#19988;&#39044;&#35757;&#32451;&#30340;&#35745;&#31639;&#26102;&#38388;&#20063;&#20943;&#23569;&#20102;&#22810;&#36798;5&#20493;&#12290;</title><link>http://arxiv.org/abs/2302.06675</link><description>&lt;p&gt;
&#20248;&#21270;&#31639;&#27861;&#30340;&#31526;&#21495;&#24335;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Symbolic Discovery of Optimization Algorithms. (arXiv:2302.06675v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06675
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#31639;&#27861;&#21457;&#29616;&#35270;&#20026;&#31243;&#24207;&#25628;&#32034;&#30340;&#26041;&#27861;&#65292;&#24182;&#29992;&#20110;&#21457;&#29616;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;&#20182;&#20204;&#30340;&#26041;&#27861;&#21457;&#29616;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20248;&#21270;&#31639;&#27861;Lion&#65292;&#23427;&#27604;Adam&#26356;&#33410;&#30465;&#20869;&#23384;&#24182;&#19988;&#22312;ImageNet&#19978;&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;2&#65285;&#65292;&#24182;&#19988;&#39044;&#35757;&#32451;&#30340;&#35745;&#31639;&#26102;&#38388;&#20063;&#20943;&#23569;&#20102;&#22810;&#36798;5&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#31639;&#27861;&#21457;&#29616;&#35270;&#20026;&#31243;&#24207;&#25628;&#32034;&#30340;&#26041;&#27861;&#65292;&#24182;&#24212;&#29992;&#20110;&#21457;&#29616;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#39640;&#25928;&#25628;&#32034;&#25216;&#26415;&#26469;&#25506;&#32034;&#26080;&#38480;&#21644;&#31232;&#30095;&#30340;&#31243;&#24207;&#31354;&#38388;&#12290;&#20026;&#20102;&#22635;&#34917;&#20195;&#29702;&#20219;&#21153;&#21644;&#30446;&#26631;&#20219;&#21153;&#20043;&#38388;&#24040;&#22823;&#30340;&#27867;&#21270;&#24046;&#36317;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#31243;&#24207;&#36873;&#25321;&#21644;&#31616;&#21270;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21457;&#29616;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;$ \textbf {Lion} $&#65288;$ \textit {Evo $\textbf {L} $ved S $ \textbf {i} $ gn M $ \textbf {o} $ me $ \textbf {n} $ tum} $&#65289;&#12290;&#23427;&#30340;&#35760;&#24518;&#25928;&#29575;&#27604;Adam&#26356;&#39640;&#65292;&#22240;&#20026;&#23427;&#21482;&#36319;&#36394;&#21160;&#37327;&#12290;&#19982;&#33258;&#36866;&#24212;&#20248;&#21270;&#22120;&#19981;&#21516;&#65292;&#36890;&#36807;&#31526;&#21495;&#36816;&#31639;&#35745;&#31639;&#30340;&#27599;&#20010;&#21442;&#25968;&#30340;&#26356;&#26032;&#20855;&#26377;&#30456;&#21516;&#30340;&#22823;&#23567;&#12290;&#25105;&#20204;&#23558;Lion&#19982;&#24191;&#27867;&#20351;&#29992;&#30340;&#20248;&#21270;&#22120;&#65288;&#20363;&#22914;Adam&#21644;Adafactor&#65289;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#20197;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#35757;&#32451;&#21508;&#31181;&#27169;&#22411;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#65292;Lion&#23558;&#22312;ImageNet&#19978;ViT&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;&#26368;&#22810;2&#65285;&#65292;&#24182;&#33410;&#30465;&#20102;&#22810;&#36798;5&#20493;&#30340;&#39044;&#35757;&#32451;&#35745;&#31639;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a method to formulate algorithm discovery as program search, and apply it to discover optimization algorithms for deep neural network training. We leverage efficient search techniques to explore an infinite and sparse program space. To bridge the large generalization gap between proxy and target tasks, we also introduce program selection and simplification strategies. Our method discovers a simple and effective optimization algorithm, $\textbf{Lion}$ ($\textit{Evo$\textbf{L}$ved S$\textbf{i}$gn M$\textbf{o}$me$\textbf{n}$tum}$). It is more memory-efficient than Adam as it only keeps track of the momentum. Different from adaptive optimizers, its update has the same magnitude for each parameter calculated through the sign operation. We compare Lion with widely used optimizers, such as Adam and Adafactor, for training a variety of models on different tasks. On image classification, Lion boosts the accuracy of ViT by up to 2% on ImageNet and saves up to 5x the pre-training compu
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20998;&#35299;&#22120;&#65292;&#35299;&#20915;&#22522;&#20110;&#34920;&#26684;&#25512;&#29702;&#20013;&#30340;&#24615;&#33021;&#19979;&#38477;&#21644;&#22797;&#26434;&#38382;&#39064;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.13808</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#22810;&#25165;&#22810;&#33402;&#30340;&#20998;&#35299;&#22120;&#65306;&#23558;&#35777;&#25454;&#21644;&#38382;&#39064;&#20998;&#35299;&#20026;&#34920;&#26684;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning. (arXiv:2301.13808v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13808
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20998;&#35299;&#22120;&#65292;&#35299;&#20915;&#22522;&#20110;&#34920;&#26684;&#25512;&#29702;&#20013;&#30340;&#24615;&#33021;&#19979;&#38477;&#21644;&#22797;&#26434;&#38382;&#39064;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#34920;&#26684;&#30340;&#25512;&#29702;&#24050;&#32463;&#22312;&#32467;&#21512;&#28145;&#24230;&#27169;&#22411;&#21644;&#31163;&#25955;&#25512;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#23427;&#38656;&#35201;&#23545;&#33258;&#30001;&#24418;&#24335;&#30340;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#21644;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#22522;&#20110;&#34920;&#26684;&#30340;&#25512;&#29702;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#20250;&#22312;&#28023;&#37327;&#35777;&#25454;&#65288;&#34920;&#26684;&#65289;&#19978;&#36973;&#36935;&#26174;&#33879;&#30340;&#24615;&#33021;&#36864;&#21270;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#22312;&#22788;&#29702;&#22797;&#26434;&#38382;&#39064;&#26102;&#20063;&#38754;&#20020;&#22256;&#38590;&#65292;&#22240;&#20026;&#25152;&#38656;&#20449;&#24687;&#20998;&#25955;&#22312;&#19981;&#21516;&#30340;&#20301;&#32622;&#12290;&#20026;&#20102;&#32531;&#35299;&#19978;&#36848;&#25361;&#25112;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#26377;&#25928;&#30340;&#22522;&#20110;&#34920;&#26684;&#25512;&#29702;&#30340;&#20998;&#35299;&#22120;&#65292;&#23558;&#65288;i&#65289;&#24040;&#22823;&#30340;&#35777;&#25454;&#65288;&#19968;&#20010;&#24040;&#22823;&#30340;&#34920;&#26684;&#65289;&#20998;&#35299;&#25104;&#23376;&#35777;&#25454;&#65288;&#19968;&#20010;&#23567;&#34920;&#26684;&#65289;&#65292;&#20197;&#20943;&#36731;&#26080;&#29992;&#20449;&#24687;&#23545;&#34920;&#26684;&#25512;&#29702;&#30340;&#24178;&#25200;&#65307;&#21644;&#65288;ii&#65289;&#23558;&#22797;&#26434;&#38382;&#39064;&#20998;&#35299;&#25104;&#26356;&#31616;&#21333;&#30340;&#23376;&#38382;&#39064;&#36827;&#34892;&#25991;&#26412;&#25512;&#29702;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;LLMs&#20998;&#35299;&#24403;&#21069;&#38382;&#39064;&#28041;&#21450;&#30340;&#35777;&#25454;&#65288;&#34920;&#26684;&#65289;&#65292;&#20445;&#30041;&#30456;&#20851;&#35777;&#25454;&#24182;&#25490;&#38500;&#19981;&#30456;&#20851;&#37096;&#20998;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;LLMs&#23558;&#22797;&#26434;&#38382;&#39064;&#37325;&#26032;&#34920;&#36848;&#20026;&#26356;&#31616;&#21333;&#30340;&#23376;&#38382;&#39064;&#65292;&#20197;&#20415;&#26356;&#31934;&#30830;&#22320;&#26816;&#32034;&#27599;&#20010;&#23376;&#38382;&#39064;&#30340;&#30456;&#24212;&#35777;&#25454;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;&#34920;&#26684;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Table-based reasoning has shown remarkable progress in combining deep models with discrete reasoning, which requires reasoning over both free-form natural language (NL) questions and structured tabular data. However, previous table-based reasoning solutions usually suffer from significant performance degradation on huge evidence (tables). In addition, most existing methods struggle to reason over complex questions since the required information is scattered in different places. To alleviate the above challenges, we exploit large language models (LLMs) as decomposers for effective table-based reasoning, which (i) decompose huge evidence (a huge table) into sub-evidence (a small table) to mitigate the interference of useless information for table reasoning; and (ii) decompose complex questions into simpler sub-questions for text reasoning. Specifically, we first use the LLMs to break down the evidence (tables) involved in the current question, retaining the relevant evidence and excludin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20351;&#29992;&#23569;&#37327;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#36890;&#36807;&#20551;&#35774;&#25277;&#35937;&#19990;&#30028;&#27169;&#22411;&#24182;&#36890;&#36807;&#20195;&#29702;&#30340;&#19990;&#30028;&#32463;&#39564;&#36827;&#34892;&#39564;&#35777;&#65292;&#21487;&#20197;&#36827;&#34892;&#35268;&#21010;&#21644;&#25506;&#32034;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#21487;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#19968;&#20010;&#25968;&#37327;&#32423;&#65292;&#32780;&#19988;&#36824;&#20855;&#26377;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.12050</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#24341;&#23548;&#30340;&#19990;&#30028;&#27169;&#22411;&#30340;&#20855;&#36523;&#20915;&#31574;&#21046;&#23450;
&lt;/p&gt;
&lt;p&gt;
Do Embodied Agents Dream of Pixelated Sheep: Embodied Decision Making using Language Guided World Modelling. (arXiv:2301.12050v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20351;&#29992;&#23569;&#37327;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#36890;&#36807;&#20551;&#35774;&#25277;&#35937;&#19990;&#30028;&#27169;&#22411;&#24182;&#36890;&#36807;&#20195;&#29702;&#30340;&#19990;&#30028;&#32463;&#39564;&#36827;&#34892;&#39564;&#35777;&#65292;&#21487;&#20197;&#36827;&#34892;&#35268;&#21010;&#21644;&#25506;&#32034;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#21487;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#19968;&#20010;&#25968;&#37327;&#32423;&#65292;&#32780;&#19988;&#36824;&#20855;&#26377;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#36890;&#24120;&#22312;&#27809;&#26377;&#20808;&#21069;&#30340;&#19990;&#30028;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#21021;&#22987;&#21270;&#39640;&#23618;&#23376;&#30446;&#26631;&#21644;&#23376;&#30446;&#26631;&#20043;&#38388;&#30340;&#36716;&#25442;&#30693;&#35782;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#21487;&#20197;&#21033;&#29992;&#27492;&#25277;&#35937;&#19990;&#30028;&#27169;&#22411;&#65288;AWM&#65289;&#36827;&#34892;&#35268;&#21010;&#21644;&#25506;&#32034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#23569;&#37327;&#26679;&#26412;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#20551;&#35774;AWM&#65292;&#36890;&#36807;&#19990;&#30028;&#32463;&#39564;&#36827;&#34892;&#39564;&#35777;&#65292;&#20197;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;DECKARD&#20195;&#29702;&#23558;LLM&#24341;&#23548;&#30340;&#25506;&#32034;&#24212;&#29992;&#20110;Minecraft&#20013;&#30340;&#29289;&#21697;&#21046;&#20316;&#65292;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;&#65288;1&#65289;&#26790;&#24819;&#38454;&#27573;&#65292;&#20195;&#29702;&#20351;&#29992;LLM&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#23376;&#30446;&#26631;&#65292;&#21363;&#20551;&#35774;&#30340;AWM&#65307;&#65288;2&#65289;&#21796;&#37266;&#38454;&#27573;&#65292;&#20195;&#29702;&#20026;&#27599;&#20010;&#23376;&#30446;&#26631;&#23398;&#20064;&#27169;&#22359;&#21270;&#31574;&#30053;&#24182;&#39564;&#35777;&#25110;&#32416;&#27491;&#20551;&#35774;&#30340;AWM&#12290;&#25105;&#20204;&#36890;&#36807;LLMs&#20551;&#35774;AWM&#65292;&#28982;&#21518;&#26681;&#25454;&#20195;&#29702;&#32463;&#39564;&#39564;&#35777;AWM&#30340;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#23558;&#26679;&#26412;&#25928;&#29575;&#25552;&#39640;&#19968;&#20010;&#25968;&#37327;&#32423;&#65292;&#32780;&#19988;&#36824;&#20855;&#26377;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) agents typically learn tabula rasa, without prior knowledge of the world. However, if initialized with knowledge of high-level subgoals and transitions between subgoals, RL agents could utilize this Abstract World Model (AWM) for planning and exploration. We propose using few-shot large language models (LLMs) to hypothesize an AWM, that will be verified through world experience, to improve sample efficiency of RL agents. Our DECKARD agent applies LLM-guided exploration to item crafting in Minecraft in two phases: (1) the Dream phase where the agent uses an LLM to decompose a task into a sequence of subgoals, the hypothesized AWM; and (2) the Wake phase where the agent learns a modular policy for each subgoal and verifies or corrects the hypothesized AWM. Our method of hypothesizing an AWM with LLMs and then verifying the AWM based on agent experience not only increases sample efficiency over contemporary methods by an order of magnitude but is also robust to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;OPUS-MT&#29983;&#24577;&#31995;&#32479;&#30340;&#21457;&#23637;&#65292;&#21253;&#25324;&#24320;&#25918;&#24335;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#21644;&#24037;&#20855;&#30340;&#24320;&#21457;&#65292;&#20197;&#21450;&#23427;&#20204;&#19982;&#26368;&#32456;&#29992;&#25143;&#24212;&#29992;&#31243;&#24207;&#12289;&#24320;&#21457;&#24179;&#21488;&#21644;&#19987;&#19994;&#24037;&#20316;&#27969;&#31243;&#30340;&#25972;&#21512;&#12290;&#36890;&#36807;&#22686;&#21152;&#35821;&#35328;&#35206;&#30422;&#33539;&#22260;&#21644;&#32763;&#35793;&#36136;&#37327;&#65292;&#23454;&#29616;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#27665;&#20027;&#21270;&#12290;</title><link>http://arxiv.org/abs/2212.01936</link><description>&lt;p&gt;
&#29992;OPUS-MT&#23454;&#29616;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27665;&#20027;&#21270;
&lt;/p&gt;
&lt;p&gt;
Democratizing Neural Machine Translation with OPUS-MT. (arXiv:2212.01936v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01936
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;OPUS-MT&#29983;&#24577;&#31995;&#32479;&#30340;&#21457;&#23637;&#65292;&#21253;&#25324;&#24320;&#25918;&#24335;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#21644;&#24037;&#20855;&#30340;&#24320;&#21457;&#65292;&#20197;&#21450;&#23427;&#20204;&#19982;&#26368;&#32456;&#29992;&#25143;&#24212;&#29992;&#31243;&#24207;&#12289;&#24320;&#21457;&#24179;&#21488;&#21644;&#19987;&#19994;&#24037;&#20316;&#27969;&#31243;&#30340;&#25972;&#21512;&#12290;&#36890;&#36807;&#22686;&#21152;&#35821;&#35328;&#35206;&#30422;&#33539;&#22260;&#21644;&#32763;&#35793;&#36136;&#37327;&#65292;&#23454;&#29616;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#27665;&#20027;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;OPUS&#29983;&#24577;&#31995;&#32479;&#65292;&#37325;&#28857;&#20171;&#32461;&#24320;&#25918;&#24335;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#21644;&#24037;&#20855;&#30340;&#24320;&#21457;&#65292;&#20197;&#21450;&#23427;&#20204;&#19982;&#26368;&#32456;&#29992;&#25143;&#24212;&#29992;&#31243;&#24207;&#12289;&#24320;&#21457;&#24179;&#21488;&#21644;&#19987;&#19994;&#24037;&#20316;&#27969;&#31243;&#30340;&#25972;&#21512;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#27491;&#22312;&#36827;&#34892;&#30340;&#22686;&#21152;&#35821;&#35328;&#35206;&#30422;&#33539;&#22260;&#21644;&#32763;&#35793;&#36136;&#37327;&#30340;&#20219;&#21153;&#65292;&#36824;&#25551;&#36848;&#20102;&#27491;&#22312;&#36827;&#34892;&#30340;&#24037;&#20316;&#65292;&#21253;&#25324;&#27169;&#22359;&#21270;&#32763;&#35793;&#27169;&#22411;&#30340;&#24320;&#21457;&#21644;&#38754;&#21521;&#24120;&#35268;&#26700;&#38754;&#21644;&#23567;&#22411;&#35774;&#22791;&#23454;&#29616;&#23454;&#26102;&#32763;&#35793;&#30340;&#36895;&#24230;&#20248;&#21270;&#32039;&#20945;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the OPUS ecosystem with a focus on the development of open machine translation models and tools, and their integration into end-user applications, development platforms and professional workflows. We discuss our on-going mission of increasing language coverage and translation quality, and also describe on-going work on the development of modular translation models and speed-optimized compact solutions for real-time translation on regular desktops and small devices.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#34920;&#26126;&#65292;&#32473;&#23450;&#30456;&#21516;&#30340;&#23383;&#24149;&#65292;&#31070;&#32463;&#22270;&#20687;&#26816;&#32034;&#22120;&#30340;&#24615;&#33021;&#27604;&#20154;&#31867;&#35201;&#39640;&#24471;&#22810;&#65292;&#32780;&#20154;&#31867;&#22312;&#32473;&#23450;&#30456;&#21516;&#30340;&#31070;&#32463;&#23383;&#24149;&#26102;&#30340;&#34920;&#29616;&#20960;&#20046;&#19982;&#38543;&#26426;&#26080;&#24322;&#12290;</title><link>http://arxiv.org/abs/2210.11512</link><description>&lt;p&gt;
&#27807;&#36890;&#30772;&#35010;&#65306;&#20154;&#31867;&#21644;&#31070;&#32463;&#23383;&#24149;&#20043;&#38388;&#30340;&#20302;&#20114;&#36890;&#24615;
&lt;/p&gt;
&lt;p&gt;
Communication breakdown: On the low mutual intelligibility between human and neural captioning. (arXiv:2210.11512v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11512
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#32473;&#23450;&#30456;&#21516;&#30340;&#23383;&#24149;&#65292;&#31070;&#32463;&#22270;&#20687;&#26816;&#32034;&#22120;&#30340;&#24615;&#33021;&#27604;&#20154;&#31867;&#35201;&#39640;&#24471;&#22810;&#65292;&#32780;&#20154;&#31867;&#22312;&#32473;&#23450;&#30456;&#21516;&#30340;&#31070;&#32463;&#23383;&#24149;&#26102;&#30340;&#34920;&#29616;&#20960;&#20046;&#19982;&#38543;&#26426;&#26080;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#27604;&#36739;&#20102;&#22522;&#20110;&#31070;&#32463;&#23383;&#24149;&#30340;&#22270;&#20687;&#26816;&#32034;&#22120;&#30340;0-shot&#34920;&#29616;&#65292;&#24403;&#36755;&#20837;&#20154;&#31867;&#21046;&#20316;&#30340;&#23383;&#24149;&#25110;&#31070;&#32463;&#23383;&#24149;&#29983;&#25104;&#30340;&#23383;&#24149;&#26102;&#12290;&#25105;&#20204;&#22312;&#26368;&#36817;&#25512;&#20986;&#30340;ImageCoDe&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#36825;&#20010;&#27604;&#36739;(Krojer&#31561;&#20154;&#65292;2022)&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#19982;&#24453;&#26816;&#32034;&#22270;&#20687;&#20960;&#20046;&#30456;&#21516;&#30340;&#38590;&#20197;&#21306;&#20998;&#30340;&#24178;&#25200;&#39033;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#36755;&#20837;&#31070;&#32463;&#23383;&#24149;&#32780;&#19981;&#26159;&#20154;&#31867;&#23383;&#24149;&#26102;&#65292;&#31070;&#32463;&#26816;&#32034;&#22120;&#30340;&#24615;&#33021;&#35201;&#39640;&#24471;&#22810;&#65292;&#23613;&#31649;&#21069;&#32773;&#19981;&#20687;&#21518;&#32773;&#37027;&#26679;&#22312;&#29983;&#25104;&#26102;&#24847;&#35782;&#21040;&#20219;&#21153;&#21464;&#24471;&#22256;&#38590;&#30340;&#24178;&#25200;&#39033;&#12290;&#26356;&#20026;&#26126;&#26174;&#30340;&#26159;&#65292;&#24403;&#23558;&#30456;&#21516;&#30340;&#31070;&#32463;&#23383;&#24149;&#25552;&#20379;&#32473;&#20154;&#31867;&#21463;&#35797;&#32773;&#26102;&#65292;&#23427;&#20204;&#30340;&#26816;&#32034;&#34920;&#29616;&#20960;&#20046;&#22788;&#20110;&#38543;&#26426;&#27700;&#24179;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26159;&#23545;&#36234;&#26469;&#36234;&#22810;&#30340;&#35777;&#25454;&#30340;&#34917;&#20805;&#65292;&#21363;&#21363;&#20351;&#31070;&#32463;&#27169;&#22411;&#30340;&#8220;&#35821;&#35328;&#8221;&#31867;&#20284;&#20110;&#33521;&#35821;&#65292;&#36825;&#31181;&#34920;&#38754;&#19978;&#30340;&#31867;&#20284;&#21487;&#33021;&#26159;&#38750;&#24120;&#35823;&#23548;&#24615;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We compare the 0-shot performance of a neural caption-based image retriever when given as input either human-produced captions or captions generated by a neural captioner. We conduct this comparison on the recently introduced ImageCoDe data-set (Krojer et al., 2022) which contains hard distractors nearly identical to the images to be retrieved. We find that the neural retriever has much higher performance when fed neural rather than human captions, despite the fact that the former, unlike the latter, were generated without awareness of the distractors that make the task hard. Even more remarkably, when the same neural captions are given to human subjects, their retrieval performance is almost at chance level. Our results thus add to the growing body of evidence that, even when the ``language'' of neural models resembles English, this superficial resemblance might be deeply misleading.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#38381;&#24335;&#38382;&#31572;&#26694;&#26550;&#65292;&#39318;&#20808;&#36890;&#36807;&#29983;&#25104;&#19978;&#19979;&#25991;&#26469;&#25552;&#21462;&#30456;&#20851;&#30693;&#35782;&#65292;&#28982;&#21518;&#20351;&#29992;&#36825;&#20010;&#30693;&#35782;&#22238;&#31572;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19977;&#20010;&#38382;&#31572;&#22522;&#20934;&#27979;&#35797;&#19978;&#26126;&#26174;&#20248;&#20110;&#20197;&#24448;&#30340;&#23553;&#38381;&#24335;&#38382;&#31572;&#26041;&#27861;&#65292;&#24182;&#19988;&#19982;&#24320;&#25918;&#24335;&#26041;&#27861;&#25345;&#24179;&#12290;</title><link>http://arxiv.org/abs/2210.06349</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#29983;&#25104;&#25552;&#39640;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Context Generation Improves Open Domain Question Answering. (arXiv:2210.06349v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06349
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#38381;&#24335;&#38382;&#31572;&#26694;&#26550;&#65292;&#39318;&#20808;&#36890;&#36807;&#29983;&#25104;&#19978;&#19979;&#25991;&#26469;&#25552;&#21462;&#30456;&#20851;&#30693;&#35782;&#65292;&#28982;&#21518;&#20351;&#29992;&#36825;&#20010;&#30693;&#35782;&#22238;&#31572;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19977;&#20010;&#38382;&#31572;&#22522;&#20934;&#27979;&#35797;&#19978;&#26126;&#26174;&#20248;&#20110;&#20197;&#24448;&#30340;&#23553;&#38381;&#24335;&#38382;&#31572;&#26041;&#27861;&#65292;&#24182;&#19988;&#19982;&#24320;&#25918;&#24335;&#26041;&#27861;&#25345;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23553;&#38381;&#24335;&#38382;&#31572;&#38656;&#35201;&#27169;&#22411;&#22312;&#27809;&#26377;&#20219;&#20309;&#22806;&#37096;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#30452;&#25509;&#22238;&#31572;&#24320;&#25918;&#24335;&#39046;&#22495;&#30340;&#38382;&#39064;&#12290;&#20197;&#24448;&#30340;&#23553;&#38381;&#24335;&#38382;&#31572;&#24037;&#20316;&#35201;&#20040;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#30452;&#25509;&#24494;&#35843;&#65292;&#35201;&#20040;&#36890;&#36807;&#25552;&#31034;&#20449;&#24687;&#26469;&#21033;&#29992;&#23384;&#20648;&#30340;&#30693;&#35782;&#12290;&#20294;&#23427;&#20204;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#21442;&#25968;&#21270;&#30693;&#35782;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#38381;&#24335;&#38382;&#31572;&#26694;&#26550;&#65292;&#23427;&#37319;&#29992;&#31895;&#30053;&#21040;&#31934;&#32454;&#30340;&#26041;&#27861;&#26469;&#25552;&#21462;&#30456;&#20851;&#30693;&#35782;&#21644;&#22238;&#31572;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#36890;&#36807;&#25552;&#31034;&#39044;&#20808;&#35757;&#32451;&#30340;LM&#29983;&#25104;&#38024;&#23545;&#32473;&#23450;&#38382;&#39064;&#30340;&#30456;&#20851;&#19978;&#19979;&#25991;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20877;&#20351;&#29992;&#36825;&#20010;LM&#36890;&#36807;&#29983;&#25104;&#30340;&#19978;&#19979;&#25991;&#21644;&#38382;&#39064;&#25552;&#31034;&#31572;&#26696;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#28040;&#38500;&#19978;&#19979;&#25991;&#19981;&#30830;&#23450;&#24615;&#24102;&#26469;&#30340;&#38169;&#35823;&#65292;&#25105;&#20204;&#36824;&#23545;&#29983;&#25104;&#30340;&#19978;&#19979;&#25991;&#36827;&#34892;&#20102;&#36793;&#38469;&#21270;&#22788;&#29702;&#12290;&#22312;&#19977;&#20010;&#38382;&#31572;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#20197;&#21069;&#30340;&#23553;&#38381;&#24335;&#38382;&#31572;&#26041;&#27861;&#65288;&#22914;&#31934;&#30830;&#21305;&#37197; 68.6% &#23545; 55.3%&#65289;&#65292;&#19988;&#19982;&#24320;&#25918;&#24335;&#26041;&#27861;&#25345;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Closed-book question answering (QA) requires a model to directly answer an open-domain question without access to any external knowledge. Prior work on closed-book QA either directly finetunes or prompts a pretrained language model (LM) to leverage the stored knowledge. However, they do not fully exploit the parameterized knowledge. To address this issue, we propose a two-stage, closed-book QA framework which employs a coarse-to-fine approach to extract relevant knowledge and answer a question. Our approach first generates a related context for a given question by prompting a pretrained LM. We then prompt the same LM for answer prediction using the generated context and the question. Additionally, to eliminate failure caused by context uncertainty, we marginalize over generated contexts. Experimental results on three QA benchmarks show that our method significantly outperforms previous closed-book QA methods (e.g. exact matching 68.6% vs. 55.3%), and is on par with open-book methods th
&lt;/p&gt;</description></item><item><title>&#35777;&#26126;&#20102;&#22312;&#23545;&#25968;&#31934;&#24230;&#19979;&#65292;Transformer&#30340;&#35745;&#31639;&#33021;&#21147;&#23384;&#22312;&#38480;&#21046;&#65292;&#36825;&#26159;&#22240;&#20026;Transformer&#26550;&#26500;&#30340;&#39640;&#24182;&#34892;&#24615;&#20351;&#20854;&#36981;&#23432;&#19968;&#20010;&#22522;&#26412;&#30340;&#24182;&#34892;&#24615;&#25481;&#20215;&#65292;&#21363;&#27169;&#22411;&#26550;&#26500;&#36234;&#21487;&#24182;&#34892;&#21270;&#65292;&#23601;&#36234;&#20250;&#21463;&#21040;&#31934;&#24230;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2207.00729</link><description>&lt;p&gt;
&#24182;&#34892;&#24615;&#25481;&#20215;&#65306;&#23545;&#23545;&#25968;&#31934;&#24230;Transformer&#30340;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
The Parallelism Tradeoff: Limitations of Log-Precision Transformers. (arXiv:2207.00729v4 [cs.CC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.00729
&lt;/p&gt;
&lt;p&gt;
&#35777;&#26126;&#20102;&#22312;&#23545;&#25968;&#31934;&#24230;&#19979;&#65292;Transformer&#30340;&#35745;&#31639;&#33021;&#21147;&#23384;&#22312;&#38480;&#21046;&#65292;&#36825;&#26159;&#22240;&#20026;Transformer&#26550;&#26500;&#30340;&#39640;&#24182;&#34892;&#24615;&#20351;&#20854;&#36981;&#23432;&#19968;&#20010;&#22522;&#26412;&#30340;&#24182;&#34892;&#24615;&#25481;&#20215;&#65292;&#21363;&#27169;&#22411;&#26550;&#26500;&#36234;&#21487;&#24182;&#34892;&#21270;&#65292;&#23601;&#36234;&#20250;&#21463;&#21040;&#31934;&#24230;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;Transformer&#31070;&#32463;&#32593;&#32476;&#22312;&#29616;&#20195;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26080;&#22788;&#19981;&#22312;&#65292;&#20294;&#25551;&#36848;&#20854;&#35745;&#31639;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#26377;&#36259;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#22312;&#36755;&#20837;&#20196;&#29260;&#25968;&#30340;&#23545;&#25968;&#31934;&#24230;&#19979;&#65288;&#19988;&#20854;&#21069;&#39304;&#32593;&#32476;&#21487;&#20351;&#29992;&#20854;&#36755;&#20837;&#30340;&#32447;&#24615;&#31354;&#38388;&#35745;&#31639;&#65289;&#65292;&#21487;&#20197;&#36890;&#36807;&#24120;&#25968;&#28145;&#24230;&#30340;&#23545;&#25968;&#31354;&#38388;&#22343;&#21248;&#38408;&#20540;&#30005;&#36335;&#26469;&#27169;&#25311;Transformer&#12290;&#36825;&#25552;&#20379;&#20102;&#20351;&#29992;&#22797;&#26434;&#24615;&#29702;&#35770;&#20013;&#24050;&#30693;&#32467;&#26524;&#26469;&#20102;&#35299;Transformer&#21151;&#29575;&#30340;&#35265;&#35299;&#12290;&#20363;&#22914;&#65292;&#22914;&#26524;$\mathsf L \neq \mathsf P$&#65288;&#21363;&#65292;&#19981;&#26159;&#25152;&#26377;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#38382;&#39064;&#37117;&#21487;&#20197;&#20351;&#29992;&#23545;&#25968;&#31354;&#38388;&#35299;&#20915;&#65289;&#65292;&#37027;&#20040;Transformer&#29978;&#33267;&#19981;&#33021;&#20934;&#30830;&#22320;&#35299;&#20915;&#32447;&#24615;&#31561;&#24335;&#25110;&#26816;&#26597;&#20219;&#24847;&#26080;&#19978;&#19979;&#25991;&#25991;&#27861;&#30340;&#25104;&#21592;&#36164;&#26684;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20174;Transformer&#26550;&#26500;&#30340;&#39640;&#24182;&#34892;&#24615;&#20013;&#30452;&#35266;&#22320;&#20986;&#29616;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#29468;&#27979;&#22320;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#26412;&#30340;&#24182;&#34892;&#24615;&#25481;&#20215;&#30340;&#24819;&#27861;&#65306;&#20219;&#20309;&#20687;Transformer&#19968;&#26679;&#21487;&#24182;&#34892;&#21270;&#30340;&#27169;&#22411;&#26550;&#26500;&#37117;&#23558;&#36981;&#23432;&#31934;&#24230;&#30340;&#38480;&#21046;&#65292;&#32780;&#20855;&#26377;&#26356;&#39640;&#31934;&#24230;&#30340;&#27169;&#22411;&#23558;&#22266;&#26377;&#22320;&#19981;&#22826;&#21487;&#24182;&#34892;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their omnipresence in modern NLP, characterizing the computational power of transformer neural nets remains an interesting open question. We prove that transformers whose arithmetic precision is logarithmic in the number of input tokens (and whose feedforward nets are computable using space linear in their input) can be simulated by constant-depth logspace-uniform threshold circuits. This provides insight on the power of transformers using known results in complexity theory. For example, if $\mathsf L \neq \mathsf P$ (i.e., not all poly-time problems can be solved using logarithmic space), then transformers cannot even accurately solve linear equalities or check membership in an arbitrary context-free grammar with empty productions. Our result intuitively emerges from the transformer architecture's high parallelizability. We thus speculatively introduce the idea of a fundamental parallelism tradeoff: any model architecture as parallelizable as the transformer will obey limitati
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#20132;&#20114;&#24335;&#38382;&#21367;&#65292;&#23454;&#29616;&#22522;&#20110;&#20559;&#22909;&#30340;&#20250;&#35758;&#25506;&#32034;&#65292;&#24110;&#21161;&#29992;&#25143;&#24555;&#36895;&#25506;&#32034;&#20250;&#35758;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2205.02370</link><description>&lt;p&gt;
&#36890;&#36807;&#20132;&#20114;&#24335;&#38382;&#21367;&#36827;&#34892;&#22522;&#20110;&#20559;&#22909;&#30340;&#20250;&#35758;&#25506;&#32034;&#30340;PREME
&lt;/p&gt;
&lt;p&gt;
PREME: Preference-based Meeting Exploration through an Interactive Questionnaire. (arXiv:2205.02370v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.02370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#20132;&#20114;&#24335;&#38382;&#21367;&#65292;&#23454;&#29616;&#22522;&#20110;&#20559;&#22909;&#30340;&#20250;&#35758;&#25506;&#32034;&#65292;&#24110;&#21161;&#29992;&#25143;&#24555;&#36895;&#25506;&#32034;&#20250;&#35758;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#20250;&#35758;&#30340;&#25968;&#37327;&#19981;&#26029;&#22686;&#21152;&#65292;&#38656;&#35201;&#33258;&#21160;&#21270;&#24037;&#20855;&#26469;&#31649;&#29702;&#21644;&#32452;&#32455;&#26448;&#26009;&#65292;&#29305;&#21035;&#26159;&#24403;&#21442;&#19982;&#32773;&#38169;&#36807;&#35752;&#35770;&#24182;&#38656;&#35201;&#24110;&#21161;&#24555;&#36895;&#25506;&#32034;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#22522;&#20110;&#20559;&#22909;&#30340;&#20250;&#35758;&#25506;&#32034;&#30340;&#20132;&#20114;&#24335;&#38382;&#21367;&#12290;&#32467;&#26524;&#65292;&#29992;&#25143;&#20250;&#33719;&#24471;&#19968;&#20010;&#25512;&#33616;&#38382;&#39064;&#21015;&#34920;&#65292;&#20197;&#21453;&#26144;&#20182;&#20204;&#30340;&#20559;&#22909;&#12290;&#30001;&#20110;&#36825;&#39033;&#20219;&#21153;&#26159;&#26032;&#30340;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#21160;&#35780;&#20272;&#31574;&#30053;&#12290;&#21363;&#65292;&#23427;&#36890;&#36807;&#24230;&#37327;&#36890;&#36807;&#38382;&#21367;&#29983;&#25104;&#30340;&#38382;&#39064;&#26377;&#22810;&#23569;&#21487;&#22238;&#31572;&#26469;&#30830;&#20445;&#20107;&#23454;&#30340;&#27491;&#30830;&#24615;&#65292;&#24182;&#20026;&#25506;&#32034;&#28304;&#20250;&#35758;&#30340;&#28145;&#24230;&#25552;&#20379;&#20102;&#35206;&#30422;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent increase in the volume of online meetings necessitates automated tools for managing and organizing the material, especially when an attendee has missed the discussion and needs assistance in quickly exploring it. In this work, we propose a novel end-to-end framework for generating interactive questionnaires for preference-based meeting exploration. As a result, users are supplied with a list of suggested questions reflecting their preferences. Since the task is new, we introduce an automatic evaluation strategy. Namely, it measures how much the generated questions via questionnaire are answerable to ensure factual correctness and covers the source meeting for the depth of possible exploration.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#35328;&#34920;&#31034;&#27169;&#22411;BiTimeBERT&#65292;&#20351;&#29992;&#38271;&#36328;&#24230;&#26102;&#38388;&#26032;&#38395;&#25991;&#31456;&#38598;&#21512;&#26500;&#24314;&#21333;&#35789;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#20004;&#20010;&#26032;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#21033;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#26102;&#38388;&#20449;&#21495;&#26500;&#24314;&#26102;&#38388;&#24863;&#30693;&#30340;&#35821;&#35328;&#34920;&#31034;&#12290;BiTimeBERT&#22312;&#26102;&#38388;&#30456;&#20851;&#20219;&#21153;&#20013;&#20855;&#26377;&#26174;&#33879;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2204.13032</link><description>&lt;p&gt;
BiTimeBERT: &#21033;&#29992;&#21452;&#37325;&#26102;&#38388;&#20449;&#24687;&#25193;&#23637;&#39044;&#35757;&#32451;&#35821;&#35328;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
BiTimeBERT: Extending Pre-Trained Language Representations with Bi-Temporal Information. (arXiv:2204.13032v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.13032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#35328;&#34920;&#31034;&#27169;&#22411;BiTimeBERT&#65292;&#20351;&#29992;&#38271;&#36328;&#24230;&#26102;&#38388;&#26032;&#38395;&#25991;&#31456;&#38598;&#21512;&#26500;&#24314;&#21333;&#35789;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#20004;&#20010;&#26032;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#21033;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#26102;&#38388;&#20449;&#21495;&#26500;&#24314;&#26102;&#38388;&#24863;&#30693;&#30340;&#35821;&#35328;&#34920;&#31034;&#12290;BiTimeBERT&#22312;&#26102;&#38388;&#30456;&#20851;&#20219;&#21153;&#20013;&#20855;&#26377;&#26174;&#33879;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#26159;&#25991;&#26723;&#30340;&#37325;&#35201;&#26041;&#38754;&#65292;&#24182;&#22312;&#21508;&#31181;NLP&#21644;IR&#20219;&#21153;&#20013;&#20351;&#29992;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#24341;&#20837;&#26102;&#38388;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#22312;&#26102;&#38388;&#30456;&#20851;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#19982;&#21033;&#29992;&#21516;&#27493;&#25991;&#26723;&#38598;&#21512;&#65288;&#20363;&#22914;BookCorpus&#21644;Wikipedia&#65289;&#20316;&#20026;&#35757;&#32451;&#35821;&#26009;&#24211;&#30340;&#24120;&#35265;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;BERT&#30456;&#27604;&#65292;&#25105;&#20204;&#20351;&#29992;&#38271;&#36328;&#24230;&#26102;&#38388;&#26032;&#38395;&#25991;&#31456;&#38598;&#21512;&#26469;&#26500;&#24314;&#21333;&#35789;&#34920;&#31034;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#35328;&#34920;&#31034;&#27169;&#22411;BiTimeBERT&#65292;&#22312;&#26102;&#38388;&#26032;&#38395;&#25991;&#31456;&#38598;&#21512;&#19978;&#36890;&#36807;&#20004;&#20010;&#26032;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#65292;&#21033;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#26102;&#38388;&#20449;&#21495;&#26500;&#24314;&#26102;&#38388;&#24863;&#30693;&#30340;&#35821;&#35328;&#34920;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;BiTimeBERT&#22987;&#32456;&#20248;&#20110;BERT&#21644;&#20854;&#20182;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#26102;&#38388;&#37325;&#35201;&#30340;&#19981;&#21516;&#19979;&#28216;NLP&#20219;&#21153;&#21644;&#24212;&#29992;&#20013;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65288;&#20363;&#22914;&#19982;BERT&#30456;&#27604;&#65292;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;155&#65285;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time is an important aspect of documents and is used in a range of NLP and IR tasks. In this work, we investigate methods for incorporating temporal information during pre-training to further improve the performance on time-related tasks. Compared with common pre-trained language models like BERT which utilize synchronic document collections (e.g., BookCorpus and Wikipedia) as the training corpora, we use long-span temporal news article collection for building word representations. We introduce BiTimeBERT, a novel language representation model trained on a temporal collection of news articles via two new pre-training tasks, which harnesses two distinct temporal signals to construct time-aware language representations. The experimental results show that BiTimeBERT consistently outperforms BERT and other existing pre-trained models with substantial gains on different downstream NLP tasks and applications for which time is of importance (e.g., the accuracy improvement over BERT is 155\% o
&lt;/p&gt;</description></item><item><title>GrIPS&#26159;&#19968;&#31181;&#22522;&#20110;&#32534;&#36753;&#30340;&#26080;&#26799;&#24230;&#25628;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20219;&#21153;&#25351;&#20196;&#65292;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2203.07281</link><description>&lt;p&gt;
GrIPS&#65306;&#22522;&#20110;&#32534;&#36753;&#30340;&#26080;&#26799;&#24230;&#25351;&#20196;&#25628;&#32034;&#65292;&#29992;&#20110;&#36741;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models. (arXiv:2203.07281v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.07281
&lt;/p&gt;
&lt;p&gt;
GrIPS&#26159;&#19968;&#31181;&#22522;&#20110;&#32534;&#36753;&#30340;&#26080;&#26799;&#24230;&#25628;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20219;&#21153;&#25351;&#20196;&#65292;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#30340;&#25552;&#31034;&#26159;&#19968;&#31181;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#20219;&#21153;&#24615;&#33021;&#30340;&#26377;&#29992;&#26032;&#33539;&#20363;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#33268;&#21147;&#20110;&#36890;&#36807;&#25163;&#21160;&#37325;&#20889;&#25110;&#26799;&#24230;&#35843;&#25972;&#26469;&#25552;&#39640;&#36825;&#20123;&#25552;&#31034;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#37325;&#20889;&#32791;&#26102;&#19988;&#38656;&#35201;&#20027;&#35266;&#35299;&#37322;&#65292;&#32780;&#22522;&#20110;&#26799;&#24230;&#30340;&#35843;&#25972;&#23545;&#20110;&#22823;&#22411;&#27169;&#22411;&#32780;&#35328;&#35745;&#31639;&#25104;&#26412;&#26497;&#39640;&#65292;&#23545;&#20110;&#22522;&#20110;API&#30340;&#27169;&#22411;&#26469;&#35828;&#21487;&#33021;&#19981;&#21487;&#34892;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Gradient-free Instructional Prompt Search (GrIPS)&#65292;&#19968;&#31181;&#22522;&#20110;&#32534;&#36753;&#30340;&#26080;&#26799;&#24230;&#25628;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20219;&#21153;&#25351;&#20196;&#12290;GrIPS&#25509;&#21463;&#38754;&#21521;&#20154;&#31867;&#35774;&#35745;&#30340;&#25351;&#20196;&#65292;&#24182;&#33258;&#21160;&#36820;&#22238;&#23436;&#21892;&#30340;&#32534;&#36753;&#25552;&#31034;&#65292;&#21516;&#26102;&#20801;&#35768;&#22522;&#20110;API&#30340;&#35843;&#25972;&#12290;&#20351;&#29992;InstructGPT&#27169;&#22411;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#25968;&#25454;&#38598;&#30340;&#20843;&#20010;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;GrIPS&#23558;&#24179;&#22343;&#20219;&#21153;&#24615;&#33021;&#25552;&#39640;&#20102;&#39640;&#36798;4.30&#20010;&#30334;&#20998;&#28857;&#65288;OPT&#65292;BLOOM&#31561;&#20219;&#21153;&#20063;&#26377;&#31867;&#20284;&#30340;&#25913;&#36827;&#65289;
&lt;/p&gt;
&lt;p&gt;
Providing natural language instructions in prompts is a useful new paradigm for improving task performance of large language models in a zero-shot setting. Recent work has aimed to improve such prompts via manual rewriting or gradient-based tuning. However, manual rewriting is time-consuming and requires subjective interpretation, while gradient-based tuning can be extremely computationally demanding for large models and may not be feasible for API-based models. In this work, we introduce Gradient-free Instructional Prompt Search (GrIPS), a gradient-free, edit-based search approach for improving task instructions for large language models. GrIPS takes in instructions designed for humans and automatically returns an improved, edited prompt, while allowing for API-based tuning. With InstructGPT models, GrIPS improves the average task performance by up to 4.30 percentage points on eight classification tasks from the Natural Instructions dataset (with similar improvements for OPT, BLOOM, a
&lt;/p&gt;</description></item></channel></rss>