<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;AutoCalibrate&#65292;&#19968;&#31181;&#22810;&#38454;&#27573;&#12289;&#26080;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#26657;&#20934;&#21644;&#35843;&#25972;&#22522;&#20110;LLM&#30340;&#35780;&#20272;&#22120;&#20197;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#12290;&#36890;&#36807;&#22312;&#23569;&#26679;&#26412;&#31034;&#20363;&#19978;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#38544;&#21547;&#22320;&#21253;&#25324;&#20154;&#31867;&#20559;&#22909;&#65292;&#24182;&#36890;&#36807;&#36873;&#25321;&#26368;&#20339;&#34920;&#29616;&#32773;&#21644;&#33258;&#25105;&#23436;&#21892;&#26469;&#36827;&#19968;&#27493;&#26657;&#20934;&#35780;&#20998;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2309.13308</link><description>&lt;p&gt;
&#26657;&#20934;&#22522;&#20110;LLM&#30340;&#35780;&#20272;&#22120;
&lt;/p&gt;
&lt;p&gt;
Calibrating LLM-Based Evaluator. (arXiv:2309.13308v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13308
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;AutoCalibrate&#65292;&#19968;&#31181;&#22810;&#38454;&#27573;&#12289;&#26080;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#26657;&#20934;&#21644;&#35843;&#25972;&#22522;&#20110;LLM&#30340;&#35780;&#20272;&#22120;&#20197;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#12290;&#36890;&#36807;&#22312;&#23569;&#26679;&#26412;&#31034;&#20363;&#19978;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#38544;&#21547;&#22320;&#21253;&#25324;&#20154;&#31867;&#20559;&#22909;&#65292;&#24182;&#36890;&#36807;&#36873;&#25321;&#26368;&#20339;&#34920;&#29616;&#32773;&#21644;&#33258;&#25105;&#23436;&#21892;&#26469;&#36827;&#19968;&#27493;&#26657;&#20934;&#35780;&#20998;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#22312;&#35821;&#35328;&#24314;&#27169;&#26041;&#38754;&#21644;&#20986;&#33394;&#30340;&#33021;&#21147;&#20351;&#23427;&#20204;&#25104;&#20026;&#26377;&#21069;&#26223;&#30340;&#26080;&#21442;&#32771;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#36136;&#37327;&#35780;&#20272;&#22120;&#65292;&#24182;&#19988;&#26159;&#20154;&#24037;&#35780;&#20272;&#30340;&#26377;&#31454;&#20105;&#21147;&#26367;&#20195;&#21697;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38381;&#28304;&#25110;&#39640;&#35745;&#31639;&#28040;&#32791;&#26469;&#25176;&#31649;&#21644;&#35843;&#33410;&#65292;&#32570;&#20047;&#36827;&#19968;&#27493;&#26657;&#20934;&#29616;&#25104;LLM&#35780;&#20272;&#22120;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#19982;&#20154;&#31867;&#19968;&#33268;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AutoCalibrate&#65292;&#19968;&#20010;&#22810;&#38454;&#27573;&#65292;&#26080;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#26657;&#20934;&#21644;&#35843;&#25972;&#22522;&#20110;LLM&#30340;&#35780;&#20272;&#22120;&#20197;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#12290;&#25105;&#20204;&#19981;&#26159;&#30452;&#25509;&#24314;&#27169;&#20154;&#31867;&#20559;&#22909;&#65292;&#32780;&#26159;&#22312;&#19968;&#32452;&#20154;&#21592;&#26631;&#31614;&#20013;&#38544;&#21547;&#22320;&#21253;&#25324;&#23427;&#20204;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#30340;&#23569;&#26679;&#26412;&#31034;&#20363;&#19978;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#35821;&#35328;&#27169;&#22411;&#26412;&#36523;&#36215;&#33609;&#20102;&#19968;&#32452;&#21021;&#27493;&#30340;&#35780;&#20998;&#26631;&#20934;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#26657;&#20934;&#27492;&#19968;&#32452;&#26631;&#20934;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#34920;&#29616;&#26368;&#22909;&#30340;&#28436;&#21592;&#65292;&#24182;&#36827;&#34892;&#33258;&#25105;&#23436;&#21892;&#30340;&#20877;&#36215;&#33609;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#22810;&#20219;&#21153;&#23398;&#20064;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#35780;&#20215;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large language models (LLMs) on language modeling and emergent capabilities make them a promising reference-free evaluator of natural language generation quality, and a competent alternative to human evaluation. However, hindered by the closed-source or high computational demand to host and tune, there is a lack of practice to further calibrate an off-the-shelf LLM-based evaluator towards better human alignment. In this work, we propose AutoCalibrate, a multi-stage, gradient-free approach to automatically calibrate and align an LLM-based evaluator toward human preference. Instead of explicitly modeling human preferences, we first implicitly encompass them within a set of human labels. Then, an initial set of scoring criteria is drafted by the language model itself, leveraging in-context learning on different few-shot examples. To further calibrate this set of criteria, we select the best performers and re-draft them with self-refinement. Our experiments on multip
&lt;/p&gt;</description></item><item><title>OATS&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#20840;&#26032;&#30340;&#35266;&#28857;&#26041;&#38754;&#30446;&#26631;&#24773;&#24863;&#22235;&#20803;&#32452;&#25277;&#21462;&#25968;&#25454;&#38598;&#65292;&#23427;&#35299;&#20915;&#20102;&#38754;&#21521;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#39046;&#22495;&#38480;&#21046;&#21644;&#25968;&#25454;&#31890;&#24230;&#25361;&#25112;&#65292;&#24182;&#22635;&#34917;&#20102;&#39184;&#39302;&#21644;&#31508;&#35760;&#26412;&#30005;&#33041;&#31561;&#24120;&#35265;&#39046;&#22495;&#30340;&#25968;&#25454;&#19981;&#36275;&#21644;&#21477;&#23376;&#19982;&#35780;&#35770;&#32423;&#24773;&#24863;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.13297</link><description>&lt;p&gt;
OATS: &#35266;&#28857;&#26041;&#38754;&#30446;&#26631;&#24773;&#24863;&#22235;&#20803;&#32452;&#25277;&#21462;&#25968;&#25454;&#38598;&#29992;&#20110;&#38754;&#21521;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
OATS: Opinion Aspect Target Sentiment Quadruple Extraction Dataset for Aspect-Based Sentiment Analysis. (arXiv:2309.13297v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13297
&lt;/p&gt;
&lt;p&gt;
OATS&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#20840;&#26032;&#30340;&#35266;&#28857;&#26041;&#38754;&#30446;&#26631;&#24773;&#24863;&#22235;&#20803;&#32452;&#25277;&#21462;&#25968;&#25454;&#38598;&#65292;&#23427;&#35299;&#20915;&#20102;&#38754;&#21521;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#39046;&#22495;&#38480;&#21046;&#21644;&#25968;&#25454;&#31890;&#24230;&#25361;&#25112;&#65292;&#24182;&#22635;&#34917;&#20102;&#39184;&#39302;&#21644;&#31508;&#35760;&#26412;&#30005;&#33041;&#31561;&#24120;&#35265;&#39046;&#22495;&#30340;&#25968;&#25454;&#19981;&#36275;&#21644;&#21477;&#23376;&#19982;&#35780;&#35770;&#32423;&#24773;&#24863;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#65288;ABSA&#65289;&#26088;&#22312;&#29702;&#35299;&#25991;&#26412;&#20869;&#23481;&#20013;&#29305;&#23450;&#35201;&#32032;&#30340;&#24773;&#24863;&#12290;&#23427;&#26088;&#22312;&#20998;&#26512;&#29992;&#25143;&#29983;&#25104;&#30340;&#35780;&#35770;&#65292;&#30830;&#23450;a) &#34987;&#35780;&#35770;&#30340;&#30446;&#26631;&#23454;&#20307;&#65292;b) &#23427;&#25152;&#23646;&#30340;&#39640;&#32423;&#26041;&#38754;&#65292;c) &#29992;&#20110;&#34920;&#36798;&#35266;&#28857;&#30340;&#24773;&#24863;&#35789;&#65292;d) &#23545;&#30446;&#26631;&#21644;&#26041;&#38754;&#34920;&#36798;&#30340;&#24773;&#24863;&#12290;&#23613;&#31649;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#25512;&#21160;&#20102;ABSA&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#24102;&#26469;&#39046;&#22495;&#38480;&#21046;&#21644;&#25968;&#25454;&#31890;&#24230;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;OATS&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#19977;&#20010;&#20840;&#26032;&#30340;&#39046;&#22495;&#65292;&#24182;&#21253;&#21547;20,000&#20010;&#21477;&#23376;&#32423;&#22235;&#20803;&#32452;&#21644;13,000&#20010;&#35780;&#35770;&#32423;&#20803;&#32452;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22635;&#34917;&#19968;&#20123;&#29305;&#23450;&#30340;&#35266;&#23519;&#21040;&#30340;&#24046;&#36317;&#65306;&#23545;&#29087;&#24713;&#39046;&#22495;&#65288;&#22914;&#39184;&#39302;&#21644;&#31508;&#35760;&#26412;&#30005;&#33041;&#65289;&#30340;&#21453;&#22797;&#20851;&#27880;&#65292;&#29992;&#20110;&#22797;&#26434;&#22235;&#20803;&#32452;&#25277;&#21462;&#20219;&#21153;&#30340;&#26377;&#38480;&#25968;&#25454;&#65292;&#20197;&#21450;&#20598;&#23572;&#24573;&#35270;&#21477;&#23376;&#21644;&#35780;&#35770;&#32423;&#24773;&#24863;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#38416;&#26126;OATS&#30340;&#28508;&#22312;&#33021;&#21147;&#65292;
&lt;/p&gt;
&lt;p&gt;
Aspect-based sentiment Analysis (ABSA) delves into understanding sentiments specific to distinct elements within textual content. It aims to analyze user-generated reviews to determine a) the target entity being reviewed, b) the high-level aspect to which it belongs, c) the sentiment words used to express the opinion, and d) the sentiment expressed toward the targets and the aspects. While various benchmark datasets have fostered advancements in ABSA, they often come with domain limitations and data granularity challenges. Addressing these, we introduce the OATS dataset, which encompasses three fresh domains and consists of 20,000 sentence-level quadruples and 13,000 review-level tuples. Our initiative seeks to bridge specific observed gaps: the recurrent focus on familiar domains like restaurants and laptops, limited data for intricate quadruple extraction tasks, and an occasional oversight of the synergy between sentence and review-level sentiments. Moreover, to elucidate OATS's pote
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#38656;&#27714;&#24418;&#24335;&#21270;&#20013;&#30340;&#24212;&#29992;&#21644;&#26368;&#26032;&#26041;&#27861;&#65292;&#20197;&#21450;&#20854;&#22312;&#29983;&#25104;&#35268;&#33539;&#27169;&#22411;&#20013;&#36215;&#21040;&#30340;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.13272</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#29992;&#20110;&#38656;&#27714;&#24418;&#24335;&#21270;&#65306;&#22914;&#20309;&#24471;&#20986;&#26032;&#26041;&#27861;&#65311;
&lt;/p&gt;
&lt;p&gt;
Natural Language Processing for Requirements Formalization: How to Derive New Approaches?. (arXiv:2309.13272v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#38656;&#27714;&#24418;&#24335;&#21270;&#20013;&#30340;&#24212;&#29992;&#21644;&#26368;&#26032;&#26041;&#27861;&#65292;&#20197;&#21450;&#20854;&#22312;&#29983;&#25104;&#35268;&#33539;&#27169;&#22411;&#20013;&#36215;&#21040;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20225;&#19994;&#21644;&#30740;&#31350;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#24076;&#26395;&#33021;&#33258;&#21160;&#21270;&#23613;&#21487;&#33021;&#22810;&#30340;&#36719;&#20214;&#24320;&#21457;&#21644;&#27979;&#35797;&#36807;&#31243;&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;&#38656;&#27714;&#24037;&#31243;&#65288;RE&#65289;&#23545;&#20110;&#26500;&#24314;&#22312;&#20854;&#22522;&#30784;&#19978;&#30340;&#25152;&#26377;&#20854;&#20182;&#27493;&#39588;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#35774;&#35745;&#21644;&#27979;&#35797;&#26041;&#27861;&#26469;&#22788;&#29702;&#36719;&#20214;&#31995;&#32479;&#26085;&#30410;&#22686;&#38271;&#30340;&#22797;&#26434;&#24615;&#21644;&#21464;&#24322;&#24615;&#12290;&#28982;&#32780;&#65292;&#20173;&#38656;&#35201;&#22823;&#37327;&#24037;&#20316;&#26469;&#20174;&#20197;&#33258;&#28982;&#35821;&#35328;&#25552;&#20379;&#30340;&#22823;&#37327;&#21151;&#33021;&#38656;&#27714;&#20013;&#21019;&#24314;&#35268;&#33539;&#27169;&#22411;&#12290;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#35768;&#22810;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#26041;&#27861;&#26469;&#20351;&#29992;&#20027;&#35201;&#30340;&#21477;&#27861;&#23646;&#24615;&#29983;&#25104;&#38656;&#27714;&#27169;&#22411;&#12290;NLP&#30340;&#26368;&#26032;&#36827;&#23637;&#34920;&#26126;&#65292;&#20063;&#21487;&#20197;&#35782;&#21035;&#21644;&#20351;&#29992;&#35821;&#20041;&#37327;&#26469;&#22312;&#38656;&#27714;&#24418;&#24335;&#21270;&#36807;&#31243;&#20013;&#25552;&#20379;&#26356;&#22909;&#30340;&#24110;&#21161;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#21644;&#35752;&#35770;&#20102;NLP&#39046;&#22495;&#30340;&#20027;&#35201;&#24605;&#24819;&#21644;&#26368;&#26032;&#26041;&#27861;&#35770;&#65292;&#20197;&#25351;&#23548;&#35835;&#32773;&#22914;&#20309;&#21019;&#24314;&#19968;&#20010;&#35268;&#33539;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is a long-standing desire of industry and research to automate the software development and testing process as much as possible. In this process, requirements engineering (RE) plays a fundamental role for all other steps that build on it. Model-based design and testing methods have been developed to handle the growing complexity and variability of software systems. However, major effort is still required to create specification models from a large set of functional requirements provided in natural language. Numerous approaches based on natural language processing (NLP) have been proposed in the literature to generate requirements models using mainly syntactic properties. Recent advances in NLP show that semantic quantities can also be identified and used to provide better assistance in the requirements formalization process. In this work, we present and discuss principal ideas and state-of-the-art methodologies from the field of NLP in order to guide the readers on how to create a s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#36817;&#26399;&#25991;&#26723;&#32423;&#20449;&#24687;&#25277;&#21462;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#21457;&#29616;&#26631;&#27880;&#35823;&#24046;&#12289;&#23454;&#20307;&#20849;&#25351;&#35299;&#26512;&#21644;&#25512;&#29702;&#30340;&#32570;&#20047;&#26159;&#38480;&#21046;&#25991;&#26723;&#32423;IE&#24615;&#33021;&#30340;&#20027;&#35201;&#22240;&#32032;&#65292;&#20026;&#36827;&#19968;&#27493;&#22686;&#24378;&#25991;&#26723;&#32423;IE&#24615;&#33021;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.13249</link><description>&lt;p&gt;
&#25991;&#26723;&#32423;&#20449;&#24687;&#25277;&#21462;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Document-Level Information Extraction. (arXiv:2309.13249v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#36817;&#26399;&#25991;&#26723;&#32423;&#20449;&#24687;&#25277;&#21462;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#21457;&#29616;&#26631;&#27880;&#35823;&#24046;&#12289;&#23454;&#20307;&#20849;&#25351;&#35299;&#26512;&#21644;&#25512;&#29702;&#30340;&#32570;&#20047;&#26159;&#38480;&#21046;&#25991;&#26723;&#32423;IE&#24615;&#33021;&#30340;&#20027;&#35201;&#22240;&#32032;&#65292;&#20026;&#36827;&#19968;&#27493;&#22686;&#24378;&#25991;&#26723;&#32423;IE&#24615;&#33021;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#32423;&#20449;&#24687;&#25277;&#21462;&#65288;IE&#65289;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#12290;&#26412;&#25991;&#23545;&#36817;&#26399;&#25991;&#26723;&#32423;IE&#25991;&#29486;&#36827;&#34892;&#20102;&#31995;&#32479;&#22238;&#39038;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#38169;&#35823;&#20998;&#26512;&#65292;&#24182;&#30830;&#23450;&#20102;&#23427;&#20204;&#22312;&#25991;&#26723;&#32423;IE&#20219;&#21153;&#20013;&#30340;&#38480;&#21046;&#20197;&#21450;&#21097;&#20313;&#25361;&#25112;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#26631;&#27880;&#35823;&#24046;&#12289;&#23454;&#20307;&#20849;&#25351;&#35299;&#26512;&#21644;&#25512;&#29702;&#30340;&#32570;&#20047;&#20005;&#37325;&#24433;&#21709;&#20102;&#25991;&#26723;&#32423;IE&#30340;&#24615;&#33021;&#12290;&#26412;&#32508;&#36848;&#30340;&#30446;&#26631;&#26159;&#25552;&#20379;&#26356;&#22810;&#30340;&#35265;&#35299;&#65292;&#24110;&#21161;NLP&#30740;&#31350;&#20154;&#21592;&#36827;&#19968;&#27493;&#25552;&#39640;&#25991;&#26723;&#32423;IE&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document-level information extraction (IE) is a crucial task in natural language processing (NLP). This paper conducts a systematic review of recent document-level IE literature. In addition, we conduct a thorough error analysis with current state-of-the-art algorithms and identify their limitations as well as the remaining challenges for the task of document-level IE. According to our findings, labeling noises, entity coreference resolution, and lack of reasoning, severely affect the performance of document-level IE. The objective of this survey paper is to provide more insights and help NLP researchers to further enhance document-level IE performance.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;ChEDDAR&#65292;&#19968;&#20010;&#22312;EFL&#20889;&#20316;&#25945;&#32946;&#20013;&#24212;&#29992;&#30340;&#23398;&#29983;-ChatGPT&#23545;&#35805;&#25968;&#25454;&#38598;&#12290;&#35813;&#30740;&#31350;&#20998;&#26512;&#20102;&#23398;&#29983;&#23545;&#29983;&#25104;&#22411;AI&#30340;&#20351;&#29992;&#27169;&#24335;&#21644;&#24863;&#30693;&#65292;&#24182;&#20026;&#25945;&#32946;&#32972;&#26223;&#19979;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#24314;&#31435;&#20102;&#22522;&#20934;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.13243</link><description>&lt;p&gt;
ChEDDAR: &#22312;EFL&#20889;&#20316;&#25945;&#32946;&#20013;&#30340;&#23398;&#29983;-ChatGPT&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
ChEDDAR: Student-ChatGPT Dialogue in EFL Writing Education. (arXiv:2309.13243v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13243
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;ChEDDAR&#65292;&#19968;&#20010;&#22312;EFL&#20889;&#20316;&#25945;&#32946;&#20013;&#24212;&#29992;&#30340;&#23398;&#29983;-ChatGPT&#23545;&#35805;&#25968;&#25454;&#38598;&#12290;&#35813;&#30740;&#31350;&#20998;&#26512;&#20102;&#23398;&#29983;&#23545;&#29983;&#25104;&#22411;AI&#30340;&#20351;&#29992;&#27169;&#24335;&#21644;&#24863;&#30693;&#65292;&#24182;&#20026;&#25945;&#32946;&#32972;&#26223;&#19979;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#24314;&#31435;&#20102;&#22522;&#20934;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#23558;&#29983;&#25104;&#22411;AI&#24212;&#29992;&#20110;&#25945;&#32946;&#39046;&#22495;&#24050;&#26377;&#19981;&#23569;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#23398;&#29983;&#21644;AI&#31995;&#32479;&#20043;&#38388;&#22823;&#35268;&#27169;&#19988;&#30495;&#23454;&#30340;&#20114;&#21160;&#30340;&#23454;&#35777;&#20998;&#26512;&#20173;&#28982;&#24456;&#26377;&#38480;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ChEDDAR&#65292;&#21363;ChatGPT&#21644;EFL&#23398;&#20064;&#32773;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#22312;&#19968;&#20010;&#23398;&#26399;&#38271;&#30340;&#32437;&#21521;&#23454;&#39564;&#20013;&#25910;&#38598;&#30340;&#65292;&#30740;&#31350;&#23545;&#35937;&#21253;&#25324;212&#21517;&#21442;&#21152;&#33521;&#35821;&#20316;&#20026;&#22806;&#35821;&#65288;EFL&#65289;&#20889;&#20316;&#35838;&#31243;&#30340;&#22823;&#23398;&#29983;&#12290;&#23398;&#29983;&#34987;&#35201;&#27714;&#36890;&#36807;&#19982;ChatGPT&#30340;&#23545;&#35805;&#26469;&#20462;&#25913;&#20182;&#20204;&#30340;&#25991;&#31456;&#12290;ChEDDAR&#21253;&#25324;&#23545;&#35805;&#26085;&#24535;&#65292;&#35805;&#35821;&#32423;&#21035;&#30340;&#25991;&#31456;&#32534;&#36753;&#21382;&#21490;&#65292;&#33258;&#25105;&#35780;&#20215;&#28385;&#24847;&#24230;&#21644;&#23398;&#29983;&#24847;&#22270;&#65292;&#20197;&#21450;&#35760;&#24405;&#20182;&#20204;&#30446;&#26631;&#21644;&#25972;&#20307;&#20307;&#39564;&#30340;&#20250;&#35805;&#32423;&#21035;&#30340;&#21069;&#21518;&#35843;&#26597;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#23398;&#29983;&#23545;&#29983;&#25104;&#22411;AI&#30340;&#20351;&#29992;&#27169;&#24335;&#21644;&#24863;&#30693;&#65292;&#20197;&#21450;&#20182;&#20204;&#30340;&#24847;&#22270;&#21644;&#28385;&#24847;&#24230;&#12290;&#20316;&#20026;&#22522;&#30784;&#24615;&#27493;&#39588;&#65292;&#25105;&#20204;&#20026;&#25945;&#32946;&#32972;&#26223;&#19979;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#20004;&#20010;&#20851;&#38190;&#20219;&#21153;&#24314;&#31435;&#20102;&#22522;&#20934;&#32467;&#26524;&#65306;&#22312;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
The integration of generative AI in education is expanding, yet empirical analyses of large-scale, real-world interactions between students and AI systems still remain limited. In this study, we present ChEDDAR, ChatGPT &amp; EFL Learner's Dialogue Dataset As Revising an essay, which is collected from a semester-long longitudinal experiment involving 212 college students enrolled in English as Foreign Langauge (EFL) writing courses. The students were asked to revise their essays through dialogues with ChatGPT. ChEDDAR includes a conversation log, utterance-level essay edit history, self-rated satisfaction, and students' intent, in addition to session-level pre-and-post surveys documenting their objectives and overall experiences. We analyze students' usage patterns and perceptions regarding generative AI with respect to their intent and satisfaction. As a foundational step, we establish baseline results for two pivotal tasks in task-oriented dialogue systems within educational contexts: in
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#29992;&#25143;&#27169;&#25311;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#12290;&#36890;&#36807;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#65292;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#35821;&#35328;&#22810;&#26679;&#24615;&#30340;&#36755;&#20986;&#65292;&#27169;&#25311;&#20154;&#31867;&#23545;&#35805;&#21442;&#19982;&#32773;&#30340;&#34892;&#20026;&#12290;&#30446;&#26631;&#26159;&#20351;&#31995;&#32479;&#30340;&#25104;&#21151;&#29575;&#19982;&#20154;&#31867;&#21644;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#20132;&#20114;&#30456;&#20284;&#12290;</title><link>http://arxiv.org/abs/2309.13233</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#29992;&#25143;&#27169;&#25311;&#65292;&#29992;&#20110;&#35780;&#20272;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
User Simulation with Large Language Models for Evaluating Task-Oriented Dialogue. (arXiv:2309.13233v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13233
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#29992;&#25143;&#27169;&#25311;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#12290;&#36890;&#36807;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#65292;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#35821;&#35328;&#22810;&#26679;&#24615;&#30340;&#36755;&#20986;&#65292;&#27169;&#25311;&#20154;&#31867;&#23545;&#35805;&#21442;&#19982;&#32773;&#30340;&#34892;&#20026;&#12290;&#30446;&#26631;&#26159;&#20351;&#31995;&#32479;&#30340;&#25104;&#21151;&#29575;&#19982;&#20154;&#31867;&#21644;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#20132;&#20114;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#24320;&#21457;&#36807;&#31243;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#26159;&#38656;&#35201;&#22312;&#22810;&#20010;&#38454;&#27573;&#21644;&#36845;&#20195;&#20013;&#36827;&#34892;&#20154;&#24037;&#35780;&#20272;&#12290;&#20026;&#20102;&#23454;&#29616;&#23545;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#30340;&#33258;&#21160;&#21270;&#35780;&#20272;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#25143;&#27169;&#25311;&#22120;&#65292;&#21033;&#29992;&#26368;&#36817;&#24320;&#21457;&#30340;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26500;&#24314;&#12290;&#20026;&#20102;&#22686;&#21152;&#25105;&#20204;&#31995;&#32479;&#30340;&#35821;&#35328;&#22810;&#26679;&#24615;&#65292;&#30456;&#23545;&#20110;&#30456;&#20851;&#20808;&#21069;&#30340;&#24037;&#20316;&#65292;&#25105;&#20204;&#19981;&#22312;&#29616;&#26377;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#23545;&#25105;&#20204;&#31995;&#32479;&#20351;&#29992;&#30340;LLMs&#36827;&#34892;&#24494;&#35843;&#65307;&#30456;&#21453;&#65292;&#25105;&#20204;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#25552;&#31034;LLMs&#29983;&#25104;&#40065;&#26834;&#19988;&#35821;&#35328;&#22810;&#26679;&#30340;&#36755;&#20986;&#65292;&#20197;&#27169;&#25311;&#20154;&#31867;&#23545;&#35805;&#21442;&#19982;&#32773;&#30340;&#34892;&#20026;&#12290;&#19981;&#21516;&#20110;&#20197;&#30446;&#26631;&#25104;&#21151;&#29575;&#65288;GSR&#65289;&#20316;&#20026;&#27169;&#25311;&#22120;&#24615;&#33021;&#30340;&#20027;&#35201;&#25351;&#26631;&#30340;&#20808;&#21069;&#24037;&#20316;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20351;&#31995;&#32479;&#23454;&#29616;&#19982;&#20154;&#31867;&#19982;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#20132;&#20114;&#20013;&#35266;&#23519;&#21040;&#30340;GSR&#30456;&#20284;&#12290;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#30446;&#21069;&#30340;&#27169;&#25311;&#22120;&#33021;&#22815;&#26377;&#25928;&#22320;&#19982;&#20154;&#31867;&#36827;&#34892;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the major impediments to the development of new task-oriented dialogue (TOD) systems is the need for human evaluation at multiple stages and iterations of the development process. In an effort to move toward automated evaluation of TOD, we propose a novel user simulator built using recently developed large pretrained language models (LLMs). In order to increase the linguistic diversity of our system relative to the related previous work, we do not fine-tune the LLMs used by our system on existing TOD datasets; rather we use in-context learning to prompt the LLMs to generate robust and linguistically diverse output with the goal of simulating the behavior of human interlocutors. Unlike previous work, which sought to maximize goal success rate (GSR) as the primary metric of simulator performance, our goal is a system which achieves a GSR similar to that observed in human interactions with TOD systems. Using this approach, our current simulator is effectively able to interact with 
&lt;/p&gt;</description></item><item><title>NJUNLP&#22242;&#38431;&#23545;WMT2023&#36136;&#37327;&#35780;&#20272;&#20849;&#20139;&#20219;&#21153;&#36827;&#34892;&#20102;&#25237;&#31295;&#65292;&#36890;&#36807;&#20351;&#29992;&#20266;&#25968;&#25454;&#26041;&#27861;&#21644;&#26680;&#24515;&#36229;&#21442;&#25968;&#30340;&#23454;&#39564;&#30740;&#31350;&#65292;&#20182;&#20204;&#30340;&#27169;&#22411;&#22312;&#33521;&#24503;&#35821;&#35328;&#23545;&#30340;&#36136;&#37327;&#39044;&#27979;&#21644;&#38169;&#35823;&#36328;&#24230;&#26816;&#27979;&#19978;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.13230</link><description>&lt;p&gt;
NJUNLP&#23545;WMT2023&#36136;&#37327;&#35780;&#20272;&#20849;&#20139;&#20219;&#21153;&#30340;&#21442;&#19982;
&lt;/p&gt;
&lt;p&gt;
NJUNLP's Participation for the WMT2023 Quality Estimation Shared Task. (arXiv:2309.13230v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13230
&lt;/p&gt;
&lt;p&gt;
NJUNLP&#22242;&#38431;&#23545;WMT2023&#36136;&#37327;&#35780;&#20272;&#20849;&#20139;&#20219;&#21153;&#36827;&#34892;&#20102;&#25237;&#31295;&#65292;&#36890;&#36807;&#20351;&#29992;&#20266;&#25968;&#25454;&#26041;&#27861;&#21644;&#26680;&#24515;&#36229;&#21442;&#25968;&#30340;&#23454;&#39564;&#30740;&#31350;&#65292;&#20182;&#20204;&#30340;&#27169;&#22411;&#22312;&#33521;&#24503;&#35821;&#35328;&#23545;&#30340;&#36136;&#37327;&#39044;&#27979;&#21644;&#38169;&#35823;&#36328;&#24230;&#26816;&#27979;&#19978;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;NJUNLP&#22242;&#38431;&#22312;WMT 2023&#36136;&#37327;&#20272;&#35745;&#65288;QE&#65289;&#20849;&#20139;&#20219;&#21153;&#20013;&#30340;&#25237;&#31295;&#12290;&#25105;&#20204;&#30340;&#22242;&#38431;&#25552;&#20132;&#20102;&#23545;&#33521;&#24503;&#35821;&#35328;&#23545;&#30340;&#25152;&#26377;&#20004;&#20010;&#23376;&#20219;&#21153;&#30340;&#39044;&#27979;&#65306;&#65288;i&#65289;&#21477;&#23376;&#21644;&#21333;&#35789;&#32423;&#21035;&#30340;&#36136;&#37327;&#39044;&#27979;&#65307;&#65288;ii&#65289;&#32454;&#31890;&#24230;&#38169;&#35823;&#36328;&#24230;&#26816;&#27979;&#12290;&#20170;&#24180;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;&#22522;&#20110;NJUQE&#26694;&#26550;&#65288;https://github.com/NJUNLP/njuqe&#65289;&#30340;&#20266;&#25968;&#25454;&#26041;&#27861;&#36827;&#34892;QE&#12290;&#25105;&#20204;&#20351;&#29992;WMT&#32763;&#35793;&#20219;&#21153;&#30340;&#24182;&#34892;&#25968;&#25454;&#29983;&#25104;&#20266;MQM&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#20266;QE&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;XLMR&#22823;&#27169;&#22411;&#65292;&#28982;&#21518;&#22312;&#30495;&#23454;QE&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#20004;&#20010;&#38454;&#27573;&#65292;&#25105;&#20204;&#20849;&#21516;&#23398;&#20064;&#21477;&#23376;&#32423;&#20998;&#25968;&#21644;&#21333;&#35789;&#32423;&#26631;&#31614;&#12290;&#22312;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#36827;&#34892;&#23454;&#39564;&#26469;&#23547;&#25214;&#25913;&#21892;&#24615;&#33021;&#30340;&#20851;&#38190;&#36229;&#21442;&#25968;&#12290;&#22312;&#25216;&#26415;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#23558;&#21333;&#35789;&#32423;&#36755;&#20986;&#36716;&#25442;&#20026;&#32454;&#31890;&#24230;&#38169;&#35823;&#36328;&#24230;&#32467;&#26524;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#33521;&#24503;&#35821;&#35328;&#23545;&#30340;&#21333;&#35789;&#32423;&#21035;&#21644;&#32454;&#31890;&#24230;&#38169;&#35823;&#36328;&#24230;&#26816;&#27979;&#23376;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the submissions of the NJUNLP team to the WMT 2023 Quality Estimation (QE) shared task. Our team submitted predictions for the English-German language pair on all two sub-tasks: (i) sentence- and word-level quality prediction; and (ii) fine-grained error span detection. This year, we further explore pseudo data methods for QE based on NJUQE framework (https://github.com/NJUNLP/njuqe). We generate pseudo MQM data using parallel data from the WMT translation task. We pre-train the XLMR large model on pseudo QE data, then fine-tune it on real QE data. At both stages, we jointly learn sentence-level scores and word-level tags. Empirically, we conduct experiments to find the key hyper-parameters that improve the performance. Technically, we propose a simple method that covert the word-level outputs to fine-grained error span results. Overall, our models achieved the best results in English-German for both word-level and fine-grained error span detection sub-tasks by a considera
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;Transformer&#27169;&#22411;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#21360;&#24230;&#35821;Hindi&#21040;&#33521;&#25991;&#30340;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#65292;&#24182;&#36890;&#36807;&#22238;&#35793;&#21644;&#19981;&#21516;&#30340;&#20998;&#35789;&#26041;&#27861;&#25552;&#21319;&#20102;&#32763;&#35793;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.13222</link><description>&lt;p&gt;
Hindi to English: &#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Hindi to English: Transformer-Based Neural Machine Translation. (arXiv:2309.13222v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;Transformer&#27169;&#22411;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#21360;&#24230;&#35821;Hindi&#21040;&#33521;&#25991;&#30340;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#65292;&#24182;&#36890;&#36807;&#22238;&#35793;&#21644;&#19981;&#21516;&#30340;&#20998;&#35789;&#26041;&#27861;&#25552;&#21319;&#20102;&#32763;&#35793;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#26368;&#37325;&#35201;&#30340;&#20219;&#21153;&#20043;&#19968;&#65292;&#23427;&#28041;&#21450;&#23558;&#25991;&#26412;&#20174;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#33258;&#21160;&#36716;&#25442;&#20026;&#21478;&#19968;&#31181;&#35821;&#35328;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#21547;&#20041;&#21644;&#27969;&#30021;&#24615;&#12290;&#23613;&#31649;&#26426;&#22120;&#32763;&#35793;&#30340;&#30740;&#31350;&#24050;&#32463;&#25345;&#32493;&#20102;&#25968;&#21313;&#24180;&#65292;&#20294;&#23558;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#19982;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#20174;&#26681;&#26412;&#19978;&#25913;&#21892;&#20102;&#32763;&#35793;&#36136;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;Transformer&#27169;&#22411;&#24320;&#21457;&#20102;&#19968;&#31181;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#31995;&#32479;&#65292;&#29992;&#20110;&#23558;&#21360;&#24230;&#35821;Hindi&#25991;&#26412;&#32763;&#35793;&#25104;&#33521;&#25991;&#12290;Hindi&#20316;&#20026;&#19968;&#31181;&#36164;&#28304;&#31232;&#32570;&#30340;&#35821;&#35328;&#65292;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#38590;&#20197;&#29702;&#35299;&#35813;&#35821;&#35328;&#65292;&#20174;&#32780;&#23548;&#33268;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#22120;&#30340;&#21457;&#23637;&#32531;&#24930;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#23454;&#26045;&#20102;&#22238;&#35793;&#26469;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#20351;&#29992;&#20102;&#35789;&#32423;&#21644;&#23376;&#35789;&#32423;&#30340;&#20998;&#35789;&#26041;&#27861;&#21019;&#24314;&#20102;&#35789;&#27719;&#34920;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Translation (MT) is one of the most prominent tasks in Natural Language Processing (NLP) which involves the automatic conversion of texts from one natural language to another while preserving its meaning and fluency. Although the research in machine translation has been going on since multiple decades, the newer approach of integrating deep learning techniques in natural language processing has led to significant improvements in the translation quality. In this paper, we have developed a Neural Machine Translation (NMT) system by training the Transformer model to translate texts from Indian Language Hindi to English. Hindi being a low resource language has made it difficult for neural networks to understand the language thereby leading to a slow growth in the development of neural machine translators. Thus, to address this gap, we implemented back-translation to augment the training data and for creating the vocabulary, we experimented with both word and subword level tokenizat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#38024;&#23545;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#38646;&#26679;&#26412;&#25552;&#31034;&#35774;&#35745;&#25216;&#26415;&#65292;&#24182;&#25506;&#35752;&#20102;&#19981;&#21516;&#31867;&#22411;&#25552;&#31034;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#37325;&#28857;&#35752;&#35770;&#20102;&#20154;&#24037;&#35774;&#35745;&#12289;&#20248;&#21270;&#31639;&#27861;&#21644;&#35780;&#20215;&#26041;&#27861;&#31561;&#22810;&#31181;&#25552;&#31034;&#35774;&#35745;&#26041;&#27861;&#65292;&#20197;&#20248;&#21270;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#24378;&#35843;&#20102;&#32771;&#34385;&#22810;&#31181;&#25351;&#26631;&#21644;&#32570;&#20047;&#21333;&#19968;&#26368;&#20339;&#25552;&#31034;&#31561;&#35780;&#20272;&#25361;&#25112;&#12290;&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;&#25552;&#31034;&#35774;&#35745;&#22312;&#20805;&#20998;&#21457;&#25381;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#28508;&#21147;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.13205</link><description>&lt;p&gt;
&#38024;&#23545;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#38646;&#26679;&#26412;&#25552;&#31034;&#35774;&#35745;&#30340;&#23454;&#38469;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Practical Survey on Zero-shot Prompt Design for In-context Learning. (arXiv:2309.13205v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13205
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#38024;&#23545;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#38646;&#26679;&#26412;&#25552;&#31034;&#35774;&#35745;&#25216;&#26415;&#65292;&#24182;&#25506;&#35752;&#20102;&#19981;&#21516;&#31867;&#22411;&#25552;&#31034;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#37325;&#28857;&#35752;&#35770;&#20102;&#20154;&#24037;&#35774;&#35745;&#12289;&#20248;&#21270;&#31639;&#27861;&#21644;&#35780;&#20215;&#26041;&#27861;&#31561;&#22810;&#31181;&#25552;&#31034;&#35774;&#35745;&#26041;&#27861;&#65292;&#20197;&#20248;&#21270;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#24378;&#35843;&#20102;&#32771;&#34385;&#22810;&#31181;&#25351;&#26631;&#21644;&#32570;&#20047;&#21333;&#19968;&#26368;&#20339;&#25552;&#31034;&#31561;&#35780;&#20272;&#25361;&#25112;&#12290;&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;&#25552;&#31034;&#35774;&#35745;&#22312;&#20805;&#20998;&#21457;&#25381;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#28508;&#21147;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26174;&#33879;&#36827;&#23637;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#26412;&#25991;&#23545;&#19978;&#19979;&#25991;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#20102;&#32508;&#21512;&#22238;&#39038;&#65292;&#37325;&#28857;&#20851;&#27880;&#19981;&#21516;&#31867;&#22411;&#30340;&#25552;&#31034;&#65292;&#21253;&#25324;&#31163;&#25955;&#12289;&#36830;&#32493;&#12289;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#65292;&#24182;&#25506;&#35752;&#23427;&#20204;&#23545;LLM&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#21508;&#31181;&#25552;&#31034;&#35774;&#35745;&#26041;&#27861;&#65292;&#22914;&#20154;&#24037;&#35774;&#35745;&#12289;&#20248;&#21270;&#31639;&#27861;&#21644;&#35780;&#20215;&#26041;&#27861;&#65292;&#20197;&#20248;&#21270;LLM&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#22238;&#39038;&#28085;&#30422;&#20102;&#25552;&#31034;&#24037;&#31243;&#39046;&#22495;&#30340;&#20851;&#38190;&#30740;&#31350;&#65292;&#35752;&#35770;&#20102;&#20854;&#26041;&#27861;&#35770;&#21644;&#23545;&#35813;&#39046;&#22495;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#36824;&#28145;&#20837;&#25506;&#35752;&#20102;&#22312;&#35780;&#20272;&#25552;&#31034;&#24615;&#33021;&#26041;&#38754;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#32570;&#20047;&#21333;&#19968;&#30340;"&#26368;&#20339;"&#25552;&#31034;&#21644;&#32771;&#34385;&#22810;&#20010;&#25351;&#26631;&#30340;&#37325;&#35201;&#24615;&#12290;&#24635;&#20043;&#65292;&#26412;&#25991;&#24378;&#35843;&#20102;&#25552;&#31034;&#35774;&#35745;&#22312;&#21457;&#25381;LLM&#30340;&#20840;&#37096;&#28508;&#21147;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#20154;&#24037;&#35774;&#35745;&#12289;&#20248;&#21270;&#31639;&#27861;&#21644;&#35780;&#20215;&#26041;&#27861;&#32467;&#21512;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The remarkable advancements in large language models (LLMs) have brought about significant improvements in Natural Language Processing(NLP) tasks. This paper presents a comprehensive review of in-context learning techniques, focusing on different types of prompts, including discrete, continuous, few-shot, and zero-shot, and their impact on LLM performance. We explore various approaches to prompt design, such as manual design, optimization algorithms, and evaluation methods, to optimize LLM performance across diverse tasks. Our review covers key research studies in prompt engineering, discussing their methodologies and contributions to the field. We also delve into the challenges faced in evaluating prompt performance, given the absence of a single "best" prompt and the importance of considering multiple metrics. In conclusion, the paper highlights the critical role of prompt design in harnessing the full potential of LLMs and provides insights into the combination of manual design, opt
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25511;&#21046;&#26426;&#21046;&#25913;&#21892;&#20102;&#29983;&#29289;&#21307;&#23398;&#25688;&#35201;&#30340;&#25991;&#26412;&#21487;&#35835;&#24615;&#65292;&#20855;&#20307;&#21253;&#25324;&#39046;&#22495;&#24494;&#35843;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21450;&#24212;&#29992;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#21644;GPT&#27169;&#22411;&#30340;&#25511;&#21046;&#20196;&#29260;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.13202</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25511;&#21046;&#26426;&#21046;&#25552;&#39640;&#20102;&#29983;&#29289;&#21307;&#23398;&#25688;&#35201;&#30340;&#25991;&#26412;&#21487;&#35835;&#24615;
&lt;/p&gt;
&lt;p&gt;
Large Language Models and Control Mechanisms Improve Text Readability of Biomedical Abstracts. (arXiv:2309.13202v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25511;&#21046;&#26426;&#21046;&#25913;&#21892;&#20102;&#29983;&#29289;&#21307;&#23398;&#25688;&#35201;&#30340;&#25991;&#26412;&#21487;&#35835;&#24615;&#65292;&#20855;&#20307;&#21253;&#25324;&#39046;&#22495;&#24494;&#35843;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21450;&#24212;&#29992;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#21644;GPT&#27169;&#22411;&#30340;&#25511;&#21046;&#20196;&#29260;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#36890;&#24120;&#20351;&#29992;&#22797;&#26434;&#30340;&#35821;&#35328;&#21644;&#38590;&#20197;&#29702;&#35299;&#30340;&#19987;&#19994;&#26415;&#35821;&#12290;&#22240;&#27492;&#65292;&#31616;&#21270;&#22312;&#25552;&#39640;&#20844;&#20849;&#20581;&#24247;&#32032;&#20859;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#23558;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#24212;&#29992;&#20110;&#33258;&#21160;&#21270;&#27492;&#31867;&#20219;&#21153;&#21487;&#20197;&#20351;&#38750;&#19987;&#19994;&#35835;&#32773;&#24555;&#36895;&#30452;&#25509;&#22320;&#33719;&#21462;&#20449;&#24687;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20844;&#24320;&#21487;&#29992;&#30340;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#25688;&#35201;&#31616;&#21270;&#30340;&#25968;&#25454;&#38598;&#65288;PLABA&#65289;&#26469;&#35843;&#26597;&#26368;&#20808;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#29289;&#21307;&#23398;&#25688;&#35201;&#31616;&#21270;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#12290;&#24212;&#29992;&#30340;&#26041;&#27861;&#21253;&#25324;&#39046;&#22495;&#24494;&#35843;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#65288;PBL&#65289;&#22312;&#65306;1&#65289;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#65288;T5&#12289;SciFive&#21644;BART&#65289;&#19978;&#65292;2&#65289;&#20165;&#35299;&#30721;&#22120;&#30340;GPT&#27169;&#22411;&#65288;GPT-3.5&#21644;GPT-4&#65289;&#26469;&#33258;OpenAI&#21644;BioGPT&#65292;&#20197;&#21450;3&#65289;&#22522;&#20110;&#25511;&#21046;&#20196;&#29260;&#26426;&#21046;&#30340;&#22522;&#20110;BART&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31995;&#21015;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#65292;&#21253;&#25324;BLEU&#12289;ROUGE&#12289;SARI&#21644;BERTscore&#65292;&#24182;&#36827;&#34892;&#20102;&#20154;&#24037;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biomedical literature often uses complex language and inaccessible professional terminologies. That is why simplification plays an important role in improving public health literacy. Applying Natural Language Processing (NLP) models to automate such tasks allows for quick and direct accessibility for lay readers. In this work, we investigate the ability of state-of-the-art large language models (LLMs) on the task of biomedical abstract simplification, using the publicly available dataset for plain language adaptation of biomedical abstracts (\textbf{PLABA}). The methods applied include domain fine-tuning and prompt-based learning (PBL) on: 1) Encoder-decoder models (T5, SciFive, and BART), 2) Decoder-only GPT models (GPT-3.5 and GPT-4) from OpenAI and BioGPT, and 3) Control-token mechanisms on BART-based models. We used a range of automatic evaluation metrics, including BLEU, ROUGE, SARI, and BERTscore, and also conducted human evaluations. BART-Large with Control Token (BART-L-w-CT) m
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#27169;&#22411;&#65292;&#32467;&#21512;LayoutLMv3&#21644;&#39046;&#22495;&#29305;&#23450;&#35268;&#21017;&#65292;&#29992;&#20110;&#35782;&#21035;&#20256;&#30495;&#36716;&#35786;&#25991;&#26723;&#20013;&#30340;&#20851;&#38190;&#23454;&#20307;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21464;&#25442;&#22120;&#27169;&#22411;&#20013;&#28155;&#21152;&#39046;&#22495;&#29305;&#23450;&#35268;&#21017;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#31934;&#30830;&#24230;&#21644;F1&#20998;&#25968;&#65292;&#20174;&#32780;&#25552;&#39640;&#36716;&#35786;&#31649;&#29702;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.13184</link><description>&lt;p&gt;
&#21307;&#30103;&#36716;&#35786;&#30340;&#25991;&#20214;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Document Understanding for Healthcare Referrals. (arXiv:2309.13184v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13184
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#27169;&#22411;&#65292;&#32467;&#21512;LayoutLMv3&#21644;&#39046;&#22495;&#29305;&#23450;&#35268;&#21017;&#65292;&#29992;&#20110;&#35782;&#21035;&#20256;&#30495;&#36716;&#35786;&#25991;&#26723;&#20013;&#30340;&#20851;&#38190;&#23454;&#20307;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21464;&#25442;&#22120;&#27169;&#22411;&#20013;&#28155;&#21152;&#39046;&#22495;&#29305;&#23450;&#35268;&#21017;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#31934;&#30830;&#24230;&#21644;F1&#20998;&#25968;&#65292;&#20174;&#32780;&#25552;&#39640;&#36716;&#35786;&#31649;&#29702;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20381;&#36182;&#20110;&#25195;&#25551;&#25991;&#26723;&#21644;&#20256;&#30495;&#36890;&#20449;&#30340;&#21307;&#30103;&#36716;&#35786;&#23548;&#33268;&#20102;&#39640;&#26114;&#30340;&#34892;&#25919;&#25104;&#26412;&#21644;&#21487;&#33021;&#24433;&#21709;&#30149;&#20154;&#25252;&#29702;&#30340;&#38169;&#35823;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#27169;&#22411;&#65292;&#21033;&#29992;LayoutLMv3&#21644;&#39046;&#22495;&#29305;&#23450;&#35268;&#21017;&#26469;&#35782;&#21035;&#20256;&#30495;&#36716;&#35786;&#25991;&#26723;&#20013;&#30340;&#20851;&#38190;&#30149;&#20154;&#12289;&#21307;&#29983;&#21644;&#26816;&#26597;&#30456;&#20851;&#23454;&#20307;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#23558;&#25991;&#26723;&#29702;&#35299;&#27169;&#22411;&#24212;&#29992;&#20110;&#36716;&#35786;&#20013;&#25152;&#38754;&#20020;&#30340;&#19968;&#20123;&#25361;&#25112;&#65292;&#36825;&#20123;&#36716;&#35786;&#30340;&#26684;&#24335;&#22240;&#21307;&#30103;&#23454;&#36341;&#32780;&#24322;&#65292;&#24182;&#20351;&#29992;MUC-5&#25351;&#26631;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#65292;&#20197;&#33719;&#24471;&#36866;&#29992;&#20110;&#23454;&#38469;&#29992;&#20363;&#30340;&#36866;&#24403;&#25351;&#26631;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#32467;&#26524;&#26174;&#31034;&#65292;&#23558;&#39046;&#22495;&#29305;&#23450;&#35268;&#21017;&#28155;&#21152;&#21040;&#21464;&#25442;&#22120;&#27169;&#22411;&#20013;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#31934;&#30830;&#24230;&#21644;F1&#20998;&#25968;&#65292;&#36825;&#34920;&#26126;&#22312;&#32463;&#36807;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#28151;&#21512;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#36716;&#35786;&#31649;&#29702;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliance on scanned documents and fax communication for healthcare referrals leads to high administrative costs and errors that may affect patient care. In this work we propose a hybrid model leveraging LayoutLMv3 along with domain-specific rules to identify key patient, physician, and exam-related entities in faxed referral documents. We explore some of the challenges in applying a document understanding model to referrals, which have formats varying by medical practice, and evaluate model performance using MUC-5 metrics to obtain appropriate metrics for the practical use case. Our analysis shows the addition of domain-specific rules to the transformer model yields greatly increased precision and F1 scores, suggesting a hybrid model trained on a curated dataset can increase efficiency in referral management.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;LLMs&#20013;&#25552;&#21462;&#22522;&#20110;&#34920;&#26684;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33976;&#39311;&#23558;&#22823;&#22411;&#27169;&#22411;&#36716;&#21270;&#20026;&#19987;&#38376;&#29992;&#20110;&#22522;&#20110;&#34920;&#26684;&#25512;&#29702;&#20219;&#21153;&#30340;&#23567;&#22411;&#27169;&#22411;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.13182</link><description>&lt;p&gt;
&#20174;LLMs&#20013;&#26377;&#25928;&#25552;&#21462;&#22522;&#20110;&#34920;&#26684;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Effective Distillation of Table-based Reasoning Ability from LLMs. (arXiv:2309.13182v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13182
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;LLMs&#20013;&#25552;&#21462;&#22522;&#20110;&#34920;&#26684;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33976;&#39311;&#23558;&#22823;&#22411;&#27169;&#22411;&#36716;&#21270;&#20026;&#19987;&#38376;&#29992;&#20110;&#22522;&#20110;&#34920;&#26684;&#25512;&#29702;&#20219;&#21153;&#30340;&#23567;&#22411;&#27169;&#22411;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24222;&#22823;&#30340;&#21442;&#25968;&#21644;&#23545;&#35745;&#31639;&#36164;&#28304;&#30340;&#39640;&#38656;&#27714;&#32473;&#23454;&#38469;&#24212;&#29992;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;LLMs&#30340;&#29305;&#23450;&#33021;&#21147;&#65292;&#22914;&#25968;&#20540;&#25512;&#29702;&#65292;&#21487;&#20197;&#36890;&#36807;&#33976;&#39311;&#20256;&#36882;&#32473;&#36739;&#23567;&#30340;&#27169;&#22411;&#12290;&#19968;&#20123;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;LLMs&#36827;&#34892;&#22522;&#20110;&#34920;&#26684;&#25512;&#29702;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20043;&#21069;&#65292;&#23578;&#26410;&#23545;&#19987;&#38376;&#20026;&#34920;&#26684;&#29983;&#25104;&#20219;&#21153;&#23450;&#21046;&#30340;&#36739;&#23567;&#27169;&#22411;&#30340;&#34920;&#26684;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#34920;&#26684;&#25512;&#29702;&#30340;&#33976;&#39311;&#26041;&#27861;&#65292;&#26088;&#22312;&#23558;LLMs&#33976;&#39311;&#25104;&#19987;&#38376;&#20026;&#22522;&#20110;&#34920;&#26684;&#25512;&#29702;&#20219;&#21153;&#35774;&#35745;&#30340;&#36739;&#23567;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#20010;&#20855;&#26377;0.22&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#65288;Flan-T5-base&#65289;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#33976;&#39311;&#65292;&#24182;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of natural language processing tasks. However, their remarkable parameter size and their impressive high requirement of computing resources pose challenges for their practical deployment. Recent research has revealed that specific capabilities of LLMs, such as numerical reasoning, can be transferred to smaller models through distillation. Some studies explore the potential of leveraging LLMs to perform table-based reasoning. Nevertheless, prior to our work, there has been no investigation into the prospect of specialising table reasoning skills in smaller models specifically tailored for table-to-text generation tasks. In this paper, we propose a novel table-based reasoning distillation, with the aim of distilling distilling LLMs into tailored, smaller models specifically designed for table-based reasoning task. Experimental results have shown that a 0.22 billion parameter model (Flan-T5-base) fin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23391;&#21152;&#25289;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#28508;&#21147;&#21644;&#32570;&#38519;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;LLMs&#22312;&#23391;&#21152;&#25289;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#36739;&#24046;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#21162;&#21147;&#24320;&#21457;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;LLMs&#30340;&#26356;&#22909;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.13173</link><description>&lt;p&gt;
BenLLMEval: &#19968;&#39033;&#23545;&#23391;&#21152;&#25289;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#28508;&#21147;&#21644;&#32570;&#38519;&#30340;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
BenLLMEval: A Comprehensive Evaluation into the Potentials and Pitfalls of Large Language Models on Bengali NLP. (arXiv:2309.13173v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23391;&#21152;&#25289;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#28508;&#21147;&#21644;&#32570;&#38519;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;LLMs&#22312;&#23391;&#21152;&#25289;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#36739;&#24046;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#21162;&#21147;&#24320;&#21457;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;LLMs&#30340;&#26356;&#22909;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22240;&#20854;&#22312;&#35821;&#35328;&#29983;&#25104;&#21644;&#20854;&#20182;&#20855;&#20307;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#20986;&#33394;&#33021;&#21147;&#32780;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26368;&#37325;&#35201;&#30340;&#31361;&#30772;&#20043;&#19968;&#12290;&#23613;&#31649;LLMs&#24050;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#24471;&#21040;&#35780;&#20272;&#65292;&#20294;&#22823;&#37096;&#20998;&#35780;&#20272;&#38598;&#20013;&#22312;&#33521;&#35821;&#19978;&#65292;&#23578;&#26410;&#23545;&#23391;&#21152;&#25289;&#35821;&#31561;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;LLMs&#22312;&#36164;&#28304;&#21294;&#20047;&#30340;&#23391;&#21152;&#25289;&#35821;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36873;&#25321;&#20102;&#21508;&#31181;&#37325;&#35201;&#19988;&#22810;&#26679;&#30340;&#23391;&#21152;&#25289;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#22914;&#25277;&#35937;&#25688;&#35201;&#12289;&#38382;&#31572;&#12289;&#25913;&#20889;&#12289;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#12289;&#25991;&#26412;&#20998;&#31867;&#21644;&#24773;&#24863;&#20998;&#26512;&#65292;&#20197;&#38646;-shot&#35780;&#20272;ChatGPT&#12289;LLaMA-2&#21644;Claude-2&#65292;&#24182;&#23558;&#24615;&#33021;&#19982;&#26368;&#20808;&#36827;&#30340;&#24494;&#35843;&#27169;&#22411;&#36827;&#34892;&#23545;&#27604;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;LLMs&#22312;&#19981;&#21516;&#30340;&#23391;&#21152;&#25289;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#36739;&#24046;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#21162;&#21147;&#24320;&#21457;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#65288;&#22914;&#23391;&#21152;&#25289;&#35821;&#65289;&#20013;LLMs&#30340;&#26356;&#22909;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have emerged as one of the most important breakthroughs in natural language processing (NLP) for their impressive skills in language generation and other language-specific tasks. Though LLMs have been evaluated in various tasks, mostly in English, they have not yet undergone thorough evaluation in under-resourced languages such as Bengali (Bangla). In this paper, we evaluate the performance of LLMs for the low-resourced Bangla language. We select various important and diverse Bangla NLP tasks, such as abstractive summarization, question answering, paraphrasing, natural language inference, text classification, and sentiment analysis for zero-shot evaluation with ChatGPT, LLaMA-2, and Claude-2 and compare the performance with state-of-the-art fine-tuned models. Our experimental results demonstrate an inferior performance of LLMs for different Bangla NLP tasks, calling for further effort to develop better understanding of LLMs in low-resource languages like Ba
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24120;&#35782;&#25512;&#29702;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#26356;&#22909;&#30340;&#25552;&#31034;&#65292;&#23454;&#29616;&#20102;&#22312;ProtoQA&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#26032;&#26368;&#20339;&#32467;&#26524;&#65292;&#26368;&#22823;&#31572;&#26696;&#27491;&#30830;&#29575;&#25552;&#39640;&#20102;8&#65285;&#65292;&#26368;&#22823;&#38169;&#35823;&#29575;&#38477;&#20302;&#20102;4&#65285;&#12290;</title><link>http://arxiv.org/abs/2309.13165</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20063;&#26159;&#33391;&#22909;&#30340;&#20856;&#22411;&#24120;&#35782;&#25512;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Are Also Good Prototypical Commonsense Reasoners. (arXiv:2309.13165v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24120;&#35782;&#25512;&#29702;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#26356;&#22909;&#30340;&#25552;&#31034;&#65292;&#23454;&#29616;&#20102;&#22312;ProtoQA&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#26032;&#26368;&#20339;&#32467;&#26524;&#65292;&#26368;&#22823;&#31572;&#26696;&#27491;&#30830;&#29575;&#25552;&#39640;&#20102;8&#65285;&#65292;&#26368;&#22823;&#38169;&#35823;&#29575;&#38477;&#20302;&#20102;4&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24120;&#35782;&#25512;&#29702;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#25216;&#33021;&#65292;&#20294;&#22312;&#28041;&#21450;&#27492;&#33021;&#21147;&#30340;&#29305;&#23450;&#20219;&#21153;&#20013;&#20173;&#23384;&#22312;&#25345;&#32493;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;&#24494;&#35843;&#26041;&#27861;&#21487;&#33021;&#32791;&#36153;&#22823;&#37327;&#36164;&#28304;&#65292;&#24182;&#21487;&#33021;&#25439;&#23475;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#20687;GPT-3.5&#21644;Claude&#36825;&#26679;&#30340;&#26368;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;&#20027;&#35201;&#36890;&#36807;API&#35843;&#29992;&#36827;&#34892;&#35775;&#38382;&#65292;&#36825;&#20351;&#24471;&#24494;&#35843;&#27169;&#22411;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#22823;&#22411;&#27169;&#22411;&#30340;&#36755;&#20986;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#21322;&#33258;&#21160;&#22320;&#24320;&#21457;&#20102;&#19968;&#32452;&#26032;&#39062;&#30340;&#25552;&#31034;&#65292;&#21253;&#25324;&#20219;&#21153;&#30456;&#20851;&#24615;&#12289;&#25903;&#25345;&#24615;&#35777;&#25454;&#29983;&#25104;&#65288;&#20363;&#22914;&#24605;&#36335;&#38142;&#21644;&#30693;&#35782;&#65289;&#12289;&#22810;&#26679;&#36335;&#24452;&#35299;&#30721;&#31561;&#65292;&#20197;&#24110;&#21161;&#27169;&#22411;&#12290;&#22312;ProtoQA&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#26356;&#22909;&#35774;&#35745;&#30340;&#25552;&#31034;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;ProtoQA&#25490;&#34892;&#27036;&#19978;&#21462;&#24471;&#26032;&#30340;&#26368;&#20339;&#25104;&#32489;&#65292;&#23558;&#26368;&#22823;&#31572;&#26696;&#27491;&#30830;&#29575;&#25552;&#39640;&#20102;8&#65285;&#65292;&#26368;&#22823;&#38169;&#35823;&#29575;&#38477;&#20302;&#20102;4&#65285;&#65288;&#31361;&#30772;50&#65285;&#65289;
&lt;/p&gt;
&lt;p&gt;
Commonsense reasoning is a pivotal skill for large language models, yet it presents persistent challenges in specific tasks requiring this competence. Traditional fine-tuning approaches can be resource-intensive and potentially compromise a model's generalization capacity. Furthermore, state-of-the-art language models like GPT-3.5 and Claude are primarily accessible through API calls, which makes fine-tuning models challenging. To address these challenges, we draw inspiration from the outputs of large models for tailored tasks and semi-automatically developed a set of novel prompts from several perspectives, including task-relevance, supportive evidence generation (e.g. chain-of-thought and knowledge), diverse path decoding to aid the model. Experimental results on ProtoQA dataset demonstrate that with better designed prompts we can achieve the new state-of-art(SOTA) on the ProtoQA leaderboard, improving the Max Answer@1 score by 8%, Max Incorrect@1 score by 4% (breakthrough 50% for th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;Twitter&#19978;&#30340;&#24773;&#24863;&#20869;&#23481;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#19982;&#24515;&#34880;&#31649;&#30142;&#30149;&#30456;&#20851;&#30340;&#20851;&#38190;&#35789;&#35789;&#20856;&#65292;&#24182;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;&#20010;&#20307;&#30340;&#24515;&#34880;&#31649;&#30142;&#30149;&#39118;&#38505;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20998;&#26512;&#24773;&#24863;&#20869;&#23481;&#33021;&#22815;&#36229;&#36234;&#20165;&#20351;&#29992;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#65292;&#35782;&#21035;&#20986;&#28508;&#22312;&#39118;&#38505;&#24739;&#32773;&#12290;&#36825;&#39033;&#30740;&#31350;&#26174;&#31034;&#20102;&#31038;&#20132;&#23186;&#20307;&#22312;&#24515;&#34880;&#31649;&#30142;&#30149;&#39118;&#38505;&#39044;&#27979;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.13147</link><description>&lt;p&gt;
&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#39044;&#27979;&#24515;&#34880;&#31649;&#30142;&#30149;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Cardiovascular Disease Risk Prediction via Social Media. (arXiv:2309.13147v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13147
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;Twitter&#19978;&#30340;&#24773;&#24863;&#20869;&#23481;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#19982;&#24515;&#34880;&#31649;&#30142;&#30149;&#30456;&#20851;&#30340;&#20851;&#38190;&#35789;&#35789;&#20856;&#65292;&#24182;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;&#20010;&#20307;&#30340;&#24515;&#34880;&#31649;&#30142;&#30149;&#39118;&#38505;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20998;&#26512;&#24773;&#24863;&#20869;&#23481;&#33021;&#22815;&#36229;&#36234;&#20165;&#20351;&#29992;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#65292;&#35782;&#21035;&#20986;&#28508;&#22312;&#39118;&#38505;&#24739;&#32773;&#12290;&#36825;&#39033;&#30740;&#31350;&#26174;&#31034;&#20102;&#31038;&#20132;&#23186;&#20307;&#22312;&#24515;&#34880;&#31649;&#30142;&#30149;&#39118;&#38505;&#39044;&#27979;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#21033;&#29992;Twitter&#21644;&#24773;&#24863;&#20998;&#26512;&#39044;&#27979;&#24515;&#34880;&#31649;&#30142;&#30149;&#65288;CVD&#65289;&#30340;&#39118;&#38505;&#12290;&#25105;&#20204;&#36890;&#36807;&#23457;&#35270;&#25512;&#25991;&#20013;&#20256;&#36798;&#30340;&#24773;&#24863;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#19982;CVD&#30456;&#20851;&#30340;&#20851;&#38190;&#35789;&#35789;&#20856;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#26469;&#33258;&#32654;&#22269;18&#20010;&#24030;&#30340;&#25512;&#25991;&#65292;&#28085;&#30422;&#20102;&#38463;&#24052;&#25289;&#22865;&#20122;&#22320;&#21306;&#12290;&#37319;&#29992;VADER&#27169;&#22411;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#65292;&#25105;&#20204;&#23558;&#29992;&#25143;&#24402;&#31867;&#20026;&#28508;&#22312;&#30340;CVD&#39118;&#38505;&#12290;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#35780;&#20272;&#20010;&#20307;&#30340;CVD&#39118;&#38505;&#65292;&#24182;&#38543;&#21518;&#23558;&#20854;&#24212;&#29992;&#20110;CDC&#25968;&#25454;&#38598;&#20013;&#30340;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#21508;&#31181;&#24615;&#33021;&#35780;&#20272;&#25351;&#26631;&#65292;&#21253;&#25324;&#27979;&#35797;&#20934;&#30830;&#29575;&#65292;&#31934;&#30830;&#29575;&#65292;&#21484;&#22238;&#29575;&#65292;F1&#20998;&#25968;&#65292;&#39532;&#20462;&#26031;&#30456;&#20851;&#31995;&#25968;&#65288;MCC&#65289;&#21644;&#31185;&#24681;&#30340;Kappa&#20998;&#25968;&#65288;CK&#65289;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20998;&#26512;&#25512;&#25991;&#20013;&#30340;&#24773;&#24863;&#20869;&#23481;&#20248;&#20110;&#20165;&#20351;&#29992;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#65292;&#33021;&#22815;&#35782;&#21035;&#20986;&#28508;&#22312;&#39118;&#38505;&#24739;&#32773;&#30340;&#20010;&#20307;&#12290;&#36825;&#39033;&#30740;&#31350;&#24378;&#35843;&#20102;&#31038;&#20132;&#23186;&#20307;&#22312;&#39044;&#27979;CVD&#39118;&#38505;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Researchers utilize Twitter and sentiment analysis to forecast the risk of Cardiovascular Disease (CVD). We have introduced a novel CVD-related keyword dictionary by scrutinizing the emotions conveyed in tweets. We gathered tweets from eighteen U.S. states, encompassing the Appalachian region. Employing the VADER model for sentiment analysis, we categorized users as potentially at risk for CVD. Machine Learning (ML) models were employed to assess individuals' CVD risk and were subsequently applied to a CDC dataset containing demographic information for comparison. We considered various performance evaluation metrics, including Test Accuracy, Precision, Recall, F1 score, Mathew's Correlation Coefficient (MCC), and Cohen's Kappa (CK) score. Our findings demonstrate that analyzing the emotional content of tweets outperforms the predictive capabilities of demographic data alone, enabling the identification of individuals at potential risk of developing CVD. This research underscores the po
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#32447;&#35270;&#39057;&#30340;&#25968;&#25454;&#39537;&#21160;&#30740;&#31350;&#65292;&#25506;&#32034;&#20102;&#29399;&#21483;&#22768;&#30340;&#35821;&#20041;&#65292;&#21457;&#29616;&#20102;&#25903;&#25345;&#20197;&#21069;&#21551;&#21457;&#24335;&#30740;&#31350;&#30340;&#35777;&#25454;&#65292;&#24182;&#25552;&#20986;&#20102;&#20851;&#20110;&#29399;&#21483;&#22768;&#30340;&#26032;&#30340;&#35266;&#28857;&#21644;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.13086</link><description>&lt;p&gt;
&#36890;&#36807;&#22312;&#32447;&#35270;&#39057;&#23545;&#29399;&#21483;&#22768;&#36827;&#34892;&#35789;&#27719;&#20998;&#26512;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Lexical Analysis of Dog Vocalizations via Online Videos. (arXiv:2309.13086v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13086
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#32447;&#35270;&#39057;&#30340;&#25968;&#25454;&#39537;&#21160;&#30740;&#31350;&#65292;&#25506;&#32034;&#20102;&#29399;&#21483;&#22768;&#30340;&#35821;&#20041;&#65292;&#21457;&#29616;&#20102;&#25903;&#25345;&#20197;&#21069;&#21551;&#21457;&#24335;&#30740;&#31350;&#30340;&#35777;&#25454;&#65292;&#24182;&#25552;&#20986;&#20102;&#20851;&#20110;&#29399;&#21483;&#22768;&#30340;&#26032;&#30340;&#35266;&#28857;&#21644;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#26512;&#21160;&#29289;&#35821;&#35328;&#30340;&#35821;&#20041;&#19968;&#30452;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#19981;&#21516;&#22768;&#38899;&#31867;&#22411;&#19982;&#19968;&#33268;&#30340;&#35821;&#20041;&#30456;&#20851;&#32852;&#65292;&#23545;&#29399;&#21483;&#22768;&#30340;&#35821;&#20041;&#36827;&#34892;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;Shiba Inu&#22768;&#38899;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#36824;&#25910;&#38598;&#20102;&#26469;&#33258;YouTube&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#22914;&#20301;&#32622;&#21644;&#27963;&#21160;&#65292;&#36890;&#36807;&#19968;&#22871;&#23436;&#21892;&#30340;&#27969;&#31243;&#12290;&#35813;&#26694;&#26550;&#20063;&#36866;&#29992;&#20110;&#20854;&#20182;&#21160;&#29289;&#29289;&#31181;&#12290;&#36890;&#36807;&#30740;&#31350;&#29399;&#21483;&#22768;&#19982;&#30456;&#24212;&#30340;&#20301;&#32622;&#21644;&#27963;&#21160;&#20043;&#38388;&#30340;&#26465;&#20214;&#27010;&#29575;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#25903;&#25345;&#20197;&#21069;&#21551;&#21457;&#24335;&#30740;&#31350;&#20851;&#20110;&#19981;&#21516;&#29399;&#21483;&#22768;&#30340;&#35821;&#20041;&#24847;&#20041;&#30340;&#35777;&#25454;&#12290;&#20363;&#22914;&#65292;&#21638;&#21742;&#21487;&#20197;&#34920;&#31034;&#20114;&#21160;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#24471;&#20986;&#20102;&#26032;&#30340;&#35266;&#28857;&#65292;&#21363;&#29616;&#26377;&#30340;&#35789;&#27719;&#31867;&#22411;&#21487;&#20197;&#32454;&#20998;&#20026;&#26356;&#31934;&#32454;&#30340;&#23376;&#31867;&#22411;&#65292;&#23545;&#20110;Shiba Inu&#26469;&#35828;&#65292;&#26368;&#23567;&#30340;&#35821;&#20041;&#21333;&#20803;&#26159;&#19982;&#35789;&#27719;&#30456;&#20851;&#30340;&#12290;&#20363;&#22914;&#65292;&#21596;&#21693;&#22768;&#21487;&#20197;&#32454;&#20998;&#20026;&#20004;&#31181;&#31867;&#22411;&#65292;&#27714;&#20851;&#27880;&#21644;&#19981;&#36866;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deciphering the semantics of animal language has been a grand challenge. This study presents a data-driven investigation into the semantics of dog vocalizations via correlating different sound types with consistent semantics. We first present a new dataset of Shiba Inu sounds, along with contextual information such as location and activity, collected from YouTube with a well-constructed pipeline. The framework is also applicable to other animal species. Based on the analysis of conditioned probability between dog vocalizations and corresponding location and activity, we discover supporting evidence for previous heuristic research on the semantic meaning of various dog sounds. For instance, growls can signify interactions. Furthermore, our study yields new insights that existing word types can be subdivided into finer-grained subtypes and minimal semantic unit for Shiba Inu is word-related. For example, whimper can be subdivided into two types, attention-seeking and discomfort.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SPICED&#30340;&#26032;&#38395;&#30456;&#20284;&#24615;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#19971;&#20010;&#20027;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#26032;&#38395;&#12290;</title><link>http://arxiv.org/abs/2309.13080</link><description>&lt;p&gt;
SPICED: &#20855;&#26377;&#22810;&#20010;&#20027;&#39064;&#21644;&#22797;&#26434;&#31243;&#24230;&#30340;&#26032;&#38395;&#30456;&#20284;&#24615;&#26816;&#27979;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SPICED: News Similarity Detection Dataset with Multiple Topics and Complexity Levels. (arXiv:2309.13080v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13080
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SPICED&#30340;&#26032;&#38395;&#30456;&#20284;&#24615;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#19971;&#20010;&#20027;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#26032;&#38395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#20351;&#29992;&#26234;&#33021;&#31995;&#32479;&#26469;&#26816;&#27979;&#26032;&#38395;&#25991;&#31456;&#20013;&#30340;&#20887;&#20313;&#20449;&#24687;&#24050;&#32463;&#21464;&#24471;&#38750;&#24120;&#26222;&#36941;&#65292;&#20197;&#22686;&#24378;&#29992;&#25143;&#20307;&#39564;&#65292;&#23588;&#20854;&#26159;&#38543;&#30528;&#26032;&#38395;&#23186;&#20307;&#30340;&#34028;&#21187;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#26032;&#38395;&#30340;&#24322;&#36136;&#24615;&#21487;&#33021;&#23548;&#33268;&#36825;&#20123;&#31995;&#32479;&#20013;&#30340;&#34394;&#20551;&#21457;&#29616;&#65306;&#31616;&#21333;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#27604;&#22914;&#19968;&#23545;&#26032;&#38395;&#26159;&#21542;&#37117;&#28041;&#21450;&#25919;&#27835;&#38382;&#39064;&#65292;&#21487;&#20197;&#25552;&#20379;&#24378;&#22823;&#20294;&#20855;&#26377;&#35823;&#23548;&#24615;&#30340;&#19979;&#28216;&#24615;&#33021;&#12290;&#23558;&#26032;&#38395;&#30456;&#20284;&#24615;&#25968;&#25454;&#38598;&#20998;&#21106;&#25104;&#20027;&#39064;&#21487;&#20197;&#36890;&#36807;&#24378;&#21046;&#27169;&#22411;&#23398;&#20064;&#22914;&#20309;&#22312;&#26356;&#29421;&#31364;&#30340;&#39046;&#22495;&#20013;&#21306;&#20998;&#26174;&#33879;&#29305;&#24449;&#26469;&#25913;&#36827;&#36825;&#20123;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#23384;&#22312;&#30446;&#21069;&#32570;&#20047;&#30340;&#19987;&#39064;&#29305;&#23450;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#30456;&#20284;&#26032;&#38395;&#25968;&#25454;&#38598;SPICED&#65292;&#20854;&#20013;&#21253;&#25324;&#19971;&#20010;&#20027;&#39064;&#65306;&#29359;&#32618;&#19982;&#27861;&#24459;&#12289;&#25991;&#21270;&#19982;&#23089;&#20048;&#12289;&#28798;&#38590;&#19982;&#20107;&#25925;&#12289;&#32463;&#27982;&#19982;&#21830;&#19994;&#12289;&#25919;&#27835;&#19982;&#20914;&#31361;&#12289;&#31185;&#23398;&#19982;&#25216;&#26415;&#20197;&#21450;&#20307;&#32946;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#26032;&#38395;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, the use of intelligent systems to detect redundant information in news articles has become especially prevalent with the proliferation of news media outlets in order to enhance user experience. However, the heterogeneous nature of news can lead to spurious findings in these systems: Simple heuristics such as whether a pair of news are both about politics can provide strong but deceptive downstream performance. Segmenting news similarity datasets into topics improves the training of these models by forcing them to learn how to distinguish salient characteristics under more narrow domains. However, this requires the existence of topic-specific datasets, which are currently lacking. In this article, we propose a new dataset of similar news, SPICED, which includes seven topics: Crime &amp; Law, Culture &amp; Entertainment, Disasters &amp; Accidents, Economy &amp; Business, Politics &amp; Conflicts, Science &amp; Technology, and Sports. Futhermore, we present four distinct approaches for generating news 
&lt;/p&gt;</description></item><item><title>MiChao-HuaFen 1.0&#26159;&#19968;&#20010;&#19987;&#20026;&#26032;&#38395;&#21644;&#25919;&#24220;&#37096;&#38376;&#23450;&#21046;&#30340;&#38754;&#21521;&#39046;&#22495;&#29305;&#23450;&#22823;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#35821;&#26009;&#25968;&#25454;&#38598;&#65292;&#23427;&#19981;&#20165;&#33021;&#22815;&#28385;&#36275;&#29305;&#23450;&#39046;&#22495;&#30340;&#39640;&#36136;&#37327;&#38656;&#27714;&#65292;&#36824;&#26377;&#21161;&#20110;&#25512;&#21160;&#30456;&#20851;&#39046;&#22495;&#30340;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.13079</link><description>&lt;p&gt;
MiChao-HuaFen 1.0&#65306;&#38754;&#21521;&#39046;&#22495;&#29305;&#23450;&#22823;&#27169;&#22411;&#30340;&#19987;&#29992;&#39044;&#35757;&#32451;&#35821;&#26009;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MiChao-HuaFen 1.0: A Specialized Pre-trained Corpus Dataset for Domain-specific Large Models. (arXiv:2309.13079v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13079
&lt;/p&gt;
&lt;p&gt;
MiChao-HuaFen 1.0&#26159;&#19968;&#20010;&#19987;&#20026;&#26032;&#38395;&#21644;&#25919;&#24220;&#37096;&#38376;&#23450;&#21046;&#30340;&#38754;&#21521;&#39046;&#22495;&#29305;&#23450;&#22823;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#35821;&#26009;&#25968;&#25454;&#38598;&#65292;&#23427;&#19981;&#20165;&#33021;&#22815;&#28385;&#36275;&#29305;&#23450;&#39046;&#22495;&#30340;&#39640;&#36136;&#37327;&#38656;&#27714;&#65292;&#36824;&#26377;&#21161;&#20110;&#25512;&#21160;&#30456;&#20851;&#39046;&#22495;&#30340;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#22914;GPT-4&#31561;&#36890;&#29992;&#22823;&#27169;&#22411;&#24050;&#32463;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#35832;&#22914;&#21307;&#30103;&#12289;&#27861;&#24459;&#21644;&#37329;&#34701;&#31561;&#39046;&#22495;&#20173;&#28982;&#23384;&#22312;&#23545;&#39640;&#36136;&#37327;&#30340;&#39046;&#22495;&#29305;&#23450;&#36755;&#20986;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#39318;&#20808;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;&#38754;&#21521;&#29305;&#23450;&#39046;&#22495;&#30340;&#22823;&#27169;&#22411;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#28385;&#36275;&#29305;&#23450;&#39046;&#22495;&#30340;&#29305;&#27530;&#38656;&#27714;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;MiChao-HuaFen 1.0&#8221;&#39044;&#35757;&#32451;&#35821;&#26009;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#29305;&#21035;&#38024;&#23545;&#26032;&#38395;&#21644;&#25919;&#24220;&#37096;&#38376;&#12290;&#35813;&#25968;&#25454;&#38598;&#26469;&#28304;&#20110;2022&#24180;&#20844;&#24320;&#21487;&#29992;&#30340;&#20114;&#32852;&#32593;&#25968;&#25454;&#65292;&#32463;&#36807;&#22810;&#36718;&#28165;&#27905;&#21644;&#22788;&#29702;&#20197;&#30830;&#20445;&#39640;&#36136;&#37327;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#20855;&#22791;&#25345;&#32493;&#21644;&#31283;&#23450;&#30340;&#26356;&#26032;&#26426;&#21046;&#12290;&#35813;&#25968;&#25454;&#38598;&#19981;&#20165;&#25903;&#25345;&#38024;&#23545;&#20013;&#25991;&#22402;&#30452;&#39046;&#22495;&#30340;&#22823;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#65292;&#36824;&#21161;&#21147;&#20110;&#25512;&#21160;&#30456;&#20851;&#39046;&#22495;&#30340;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advancement of deep learning technologies, general-purpose large models such as GPT-4 have demonstrated exceptional capabilities across various domains. Nevertheless, there remains a demand for high-quality, domain-specific outputs in areas like healthcare, law, and finance. This paper first evaluates the existing large models for specialized domains and discusses their limitations. To cater to the specific needs of certain domains, we introduce the ``MiChao-HuaFen 1.0'' pre-trained corpus dataset, tailored for the news and governmental sectors. The dataset, sourced from publicly available internet data from 2022, underwent multiple rounds of cleansing and processing to ensure high quality and reliable origins, with provisions for consistent and stable updates. This dataset not only supports the pre-training of large models for Chinese vertical domains but also aids in propelling deep learning research and applications in related fields.
&lt;/p&gt;</description></item><item><title>SCREWS&#26159;&#19968;&#20010;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#25512;&#29702;&#20462;&#35746;&#12290;&#23427;&#33021;&#22815;&#32479;&#19968;&#20808;&#21069;&#30340;&#26041;&#27861;&#24182;&#25552;&#20379;&#26032;&#30340;&#31574;&#30053;&#26469;&#35782;&#21035;&#25913;&#36827;&#30340;&#25512;&#29702;&#38142;&#12290;&#22312;&#22810;&#26679;&#30340;&#25512;&#29702;&#20219;&#21153;&#19978;&#65292;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;LLMs&#65288;ChatGPT&#21644;GPT-4&#65289;&#35780;&#20272;SCREWS&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#20102;&#26377;&#29992;&#30340;&#26032;&#30340;&#25512;&#29702;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2309.13075</link><description>&lt;p&gt;
SCREWS: &#19968;&#31181;&#29992;&#20110;&#25512;&#29702;&#20462;&#35746;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SCREWS: A Modular Framework for Reasoning with Revisions. (arXiv:2309.13075v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13075
&lt;/p&gt;
&lt;p&gt;
SCREWS&#26159;&#19968;&#20010;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#25512;&#29702;&#20462;&#35746;&#12290;&#23427;&#33021;&#22815;&#32479;&#19968;&#20808;&#21069;&#30340;&#26041;&#27861;&#24182;&#25552;&#20379;&#26032;&#30340;&#31574;&#30053;&#26469;&#35782;&#21035;&#25913;&#36827;&#30340;&#25512;&#29702;&#38142;&#12290;&#22312;&#22810;&#26679;&#30340;&#25512;&#29702;&#20219;&#21153;&#19978;&#65292;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;LLMs&#65288;ChatGPT&#21644;GPT-4&#65289;&#35780;&#20272;SCREWS&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#20102;&#26377;&#29992;&#30340;&#26032;&#30340;&#25512;&#29702;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#21487;&#20197;&#36890;&#36807;&#26681;&#25454;&#21453;&#39304;&#19981;&#26029;&#25913;&#36827;&#21644;&#20462;&#35746;&#20854;&#36755;&#20986;&#26469;&#25552;&#39640;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20123;&#20462;&#35746;&#21487;&#33021;&#20250;&#24341;&#20837;&#38169;&#35823;&#65292;&#22914;&#26524;&#26159;&#36825;&#26679;&#30340;&#35805;&#65292;&#26368;&#22909;&#22238;&#28378;&#21040;&#20808;&#21069;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#20462;&#35746;&#36890;&#24120;&#26159;&#21516;&#36136;&#30340;&#65306;&#23427;&#20204;&#20351;&#29992;&#19982;&#20135;&#29983;&#21021;&#22987;&#31572;&#26696;&#30340;&#30456;&#21516;&#25512;&#29702;&#26041;&#27861;&#65292;&#36825;&#21487;&#33021;&#26080;&#27861;&#32416;&#27491;&#38169;&#35823;&#12290;&#20026;&#20102;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#36827;&#34892;&#25506;&#32034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; SCREWS&#65292;&#19968;&#31181;&#29992;&#20110;&#25512;&#29702;&#20462;&#35746;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;&#12290;&#23427;&#30001;&#19977;&#20010;&#20027;&#35201;&#27169;&#22359;&#32452;&#25104;: &#37319;&#26679;&#12289;&#26465;&#20214;&#37325;&#26032;&#37319;&#26679;&#21644;&#36873;&#25321;&#65292;&#27599;&#20010;&#27169;&#22359;&#37117;&#21253;&#21547;&#21487;&#20197;&#26681;&#25454;&#20219;&#21153;&#25163;&#21160;&#36873;&#25321;&#30340;&#23376;&#27169;&#22359;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; SCREWS &#19981;&#20165;&#23558;&#20960;&#20010;&#20808;&#21069;&#30340;&#26041;&#27861;&#32479;&#19968;&#21040;&#19968;&#20010;&#20849;&#21516;&#30340;&#26694;&#26550;&#20013;&#65292;&#36824;&#25581;&#31034;&#20102;&#20960;&#31181;&#29992;&#20110;&#35782;&#21035;&#25913;&#36827;&#30340;&#25512;&#29702;&#38142;&#30340;&#26032;&#31574;&#30053;&#12290;&#25105;&#20204;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;LLMs &#65288;ChatGPT &#21644; GPT-4&#65289;&#22312;&#22810;&#26679;&#30340;&#25512;&#29702;&#20219;&#21153;&#19978;&#35780;&#20272;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#24182;&#25581;&#31034;&#20102;&#26377;&#29992;&#30340;&#26032;&#30340;&#25512;&#29702;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) can improve their accuracy on various tasks through iteratively refining and revising their output based on feedback. We observe that these revisions can introduce errors, in which case it is better to roll back to a previous result. Further, revisions are typically homogeneous: they use the same reasoning method that produced the initial answer, which may not correct errors. To enable exploration in this space, we present SCREWS, a modular framework for reasoning with revisions. It is comprised of three main modules: Sampling, Conditional Resampling, and Selection, each consisting of sub-modules that can be hand-selected per task. We show that SCREWS not only unifies several previous approaches under a common framework, but also reveals several novel strategies for identifying improved reasoning chains. We evaluate our framework with state-of-the-art LLMs (ChatGPT and GPT-4) on a diverse set of reasoning tasks and uncover useful new reasoning strategies fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#30340;&#24369;&#30417;&#30563;&#25512;&#29702;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#31526;&#21495;&#20027;&#20041;&#21644;&#36830;&#25509;&#20027;&#20041;&#32467;&#21512;&#36215;&#26469;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#20855;&#26377;&#31526;&#21495;&#28508;&#22312;&#32467;&#26500;&#30340;&#31070;&#32463;&#31995;&#32479;&#65292;&#24182;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#25110;&#26494;&#24347;&#26041;&#27861;&#26469;&#36827;&#34892;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2309.13072</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#30340;&#24369;&#30417;&#30563;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Weakly Supervised Reasoning by Neuro-Symbolic Approaches. (arXiv:2309.13072v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#30340;&#24369;&#30417;&#30563;&#25512;&#29702;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#31526;&#21495;&#20027;&#20041;&#21644;&#36830;&#25509;&#20027;&#20041;&#32467;&#21512;&#36215;&#26469;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#20855;&#26377;&#31526;&#21495;&#28508;&#22312;&#32467;&#26500;&#30340;&#31070;&#32463;&#31995;&#32479;&#65292;&#24182;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#25110;&#26494;&#24347;&#26041;&#27861;&#26469;&#36827;&#34892;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26159;&#40657;&#30418;&#26426;&#22120;&#65292;&#32570;&#20047;&#26126;&#30830;&#30340;&#35299;&#37322;&#12290;&#22312;&#26412;&#31456;&#20013;&#65292;&#25105;&#20204;&#23558;&#20171;&#32461;&#25105;&#20204;&#22312;NLP&#26041;&#38754;&#30340;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#19981;&#21516;&#30340;&#20154;&#24037;&#26234;&#33021;&#23398;&#27966;&#65292;&#21363;&#31526;&#21495;&#20027;&#20041;&#21644;&#36830;&#25509;&#20027;&#20041;&#12290;&#19968;&#33324;&#32780;&#35328;&#65292;&#25105;&#20204;&#20250;&#35774;&#35745;&#19968;&#20010;&#24102;&#26377;&#31526;&#21495;&#28508;&#22312;&#32467;&#26500;&#30340;&#31070;&#32463;&#31995;&#32479;&#65292;&#29992;&#20110;NLP&#20219;&#21153;&#65292;&#24182;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#25110;&#20854;&#26494;&#24347;&#26041;&#27861;&#26469;&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24369;&#30417;&#30563;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#34920;&#26684;&#26597;&#35810;&#25512;&#29702;&#12289;&#21477;&#27861;&#32467;&#26500;&#25512;&#29702;&#12289;&#20449;&#24687;&#25277;&#21462;&#25512;&#29702;&#21644;&#35268;&#21017;&#25512;&#29702;&#12290;&#23545;&#20110;&#27599;&#20010;&#24212;&#29992;&#65292;&#25105;&#20204;&#23558;&#20171;&#32461;&#32972;&#26223;&#12289;&#25105;&#20204;&#30340;&#26041;&#27861;&#21644;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has largely improved the performance of various natural language processing (NLP) tasks. However, most deep learning models are black-box machinery, and lack explicit interpretation. In this chapter, we will introduce our recent progress on neuro-symbolic approaches to NLP, which combines different schools of AI, namely, symbolism and connectionism. Generally, we will design a neural system with symbolic latent structures for an NLP task, and apply reinforcement learning or its relaxation to perform weakly supervised reasoning in the downstream task. Our framework has been successfully applied to various tasks, including table query reasoning, syntactic structure reasoning, information extraction reasoning, and rule reasoning. For each application, we will introduce the background, our approach, and experimental results.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#35757;&#32451;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26816;&#27979;&#20551;&#26032;&#38395;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#26420;&#32032;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#30340;&#20934;&#30830;&#29575;&#20026;56%&#65292;&#26159;&#26368;&#20339;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.13069</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Technique Based Fake News Detection. (arXiv:2309.13069v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13069
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#35757;&#32451;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26816;&#27979;&#20551;&#26032;&#38395;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#26420;&#32032;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#30340;&#20934;&#30830;&#29575;&#20026;56%&#65292;&#26159;&#26368;&#20339;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#26032;&#38395;&#24341;&#36215;&#20102;&#20844;&#20247;&#21644;&#23398;&#26415;&#30028;&#30340;&#20851;&#27880;&#12290;&#36825;&#31181;&#34394;&#20551;&#20449;&#24687;&#26377;&#33021;&#21147;&#24433;&#21709;&#20844;&#20247;&#30340;&#30475;&#27861;&#65292;&#32473;&#24694;&#24847;&#22242;&#20307;&#24433;&#21709;&#20844;&#20849;&#20107;&#20214;&#65288;&#22914;&#36873;&#20030;&#65289;&#30340;&#26426;&#20250;&#12290;&#20219;&#20309;&#20154;&#37117;&#21487;&#20197;&#20998;&#20139;&#20851;&#20110;&#20219;&#20309;&#20154;&#25110;&#20219;&#20309;&#20107;&#24773;&#30340;&#34394;&#20551;&#26032;&#38395;&#25110;&#20107;&#23454;&#65292;&#20197;&#35851;&#21462;&#20010;&#20154;&#21033;&#30410;&#25110;&#32473;&#26576;&#20154;&#24102;&#26469;&#40635;&#28902;&#12290;&#27492;&#22806;&#65292;&#20449;&#24687;&#22240;&#20998;&#20139;&#30340;&#22320;&#21306;&#32780;&#24322;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#25910;&#38598;&#30340;1876&#26465;&#26032;&#38395;&#25968;&#25454;&#35757;&#32451;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#23545;&#25968;&#25454;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#20174;&#32780;&#33719;&#24471;&#24178;&#20928;&#21644;&#36807;&#28388;&#30340;&#25991;&#26412;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20351;&#29992;&#20102;3&#20010;&#27969;&#34892;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65288;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#12289;&#26420;&#32032;&#36125;&#21494;&#26031;&#12289;&#36923;&#36753;&#22238;&#24402;&#65289;&#21644;2&#20010;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65288;&#38271;&#30701;&#26399;&#35760;&#24518;&#12289;&#26435;&#37325;&#20002;&#24323;LSTM&#25110;AWD-LSTM&#65289;&#12290;&#32463;&#36807;&#25105;&#20204;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#20934;&#30830;&#29575;&#20026;56%&#65292;F1-macro&#20998;&#25968;&#20026;&#30340;&#26368;&#20339;&#26420;&#32032;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
False news has received attention from both the general public and the scholarly world. Such false information has the ability to affect public perception, giving nefarious groups the chance to influence the results of public events like elections. Anyone can share fake news or facts about anyone or anything for their personal gain or to cause someone trouble. Also, information varies depending on the part of the world it is shared on. Thus, in this paper, we have trained a model to classify fake and true news by utilizing the 1876 news data from our collected dataset. We have preprocessed the data to get clean and filtered texts by following the Natural Language Processing approaches. Our research conducts 3 popular Machine Learning (Stochastic gradient descent, Na\"ive Bayes, Logistic Regression,) and 2 Deep Learning (Long-Short Term Memory, ASGD Weight-Dropped LSTM, or AWD-LSTM) algorithms. After we have found our best Naive Bayes classifier with 56% accuracy and an F1-macro score o
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#36164;&#26009;&#39044;&#27979;&#20010;&#20154;&#20449;&#24687;&#30340;&#20010;&#24615;&#21270;&#20998;&#26512;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#22810;&#29992;&#36884;&#24615;&#65292;&#24182;&#21457;&#29616;&#25903;&#25345;&#21521;&#37327;&#26426;&#27169;&#22411;&#22312;&#39044;&#27979;&#20010;&#24615;&#31867;&#22411;&#26041;&#38754;&#20855;&#26377;&#26368;&#20339;&#20934;&#30830;&#29575;&#65292;&#32780;&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#22312;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#19978;&#34920;&#29616;&#36739;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.13065</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#20998;&#26512;&#65306;&#31038;&#20132;&#23186;&#20307;&#36164;&#26009;&#22312;&#39044;&#27979;&#20010;&#20154;&#20449;&#24687;&#26041;&#38754;&#26377;&#22810;&#26377;&#29992;&#65311;
&lt;/p&gt;
&lt;p&gt;
Personality Profiling: How informative are social media profiles in predicting personal information?. (arXiv:2309.13065v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13065
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#36164;&#26009;&#39044;&#27979;&#20010;&#20154;&#20449;&#24687;&#30340;&#20010;&#24615;&#21270;&#20998;&#26512;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#22810;&#29992;&#36884;&#24615;&#65292;&#24182;&#21457;&#29616;&#25903;&#25345;&#21521;&#37327;&#26426;&#27169;&#22411;&#22312;&#39044;&#27979;&#20010;&#24615;&#31867;&#22411;&#26041;&#38754;&#20855;&#26377;&#26368;&#20339;&#20934;&#30830;&#29575;&#65292;&#32780;&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#22312;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#19978;&#34920;&#29616;&#36739;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#21496;&#21033;&#29992;&#20010;&#24615;&#21270;&#20998;&#26512;&#36827;&#34892;&#23450;&#21521;&#24191;&#21578;&#12289;&#25919;&#27835;&#23459;&#20256;&#21644;&#30123;&#33495;&#23459;&#20256;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#22810;&#29992;&#36884;&#24615;&#20173;&#28982;&#30456;&#23545;&#26410;&#30693;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#26088;&#22312;&#25506;&#32034;&#20154;&#20204;&#30340;&#22312;&#32447;&#25968;&#23383;&#36275;&#36857;&#33021;&#22815;&#34987;&#29992;&#26469;&#20998;&#26512;&#20854;&#36808;&#23572;&#26031;-&#24067;&#37324;&#26684;&#26031;&#20154;&#26684;&#31867;&#22411;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#20998;&#26512;&#21644;&#27604;&#36739;&#20102;&#22235;&#20010;&#27169;&#22411;&#30340;&#32467;&#26524;&#65306;&#36923;&#36753;&#22238;&#24402;&#12289;&#26420;&#32032;&#36125;&#21494;&#26031;&#12289;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#21644;&#38543;&#26426;&#26862;&#26519;&#12290;&#25105;&#20204;&#21457;&#29616;SVM&#27169;&#22411;&#22312;&#39044;&#27979;&#26576;&#20154;&#30340;&#23436;&#25972;&#20010;&#24615;&#31867;&#22411;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20339;&#20934;&#30830;&#29575;20.95%&#12290;&#28982;&#32780;&#65292;&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#30340;&#34920;&#29616;&#21482;&#31245;&#24494;&#24046;&#19968;&#20123;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#21644;&#36827;&#34892;&#39044;&#27979;&#26102;&#36895;&#24230;&#26356;&#24555;&#12290;&#25105;&#20204;&#21457;&#29616;&#35768;&#22810;&#26631;&#35760;&#25968;&#25454;&#38598;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#21576;&#29616;&#20986;&#20010;&#20154;&#29305;&#24449;&#30340;&#20005;&#37325;&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#21253;&#25324;&#25105;&#20204;&#33258;&#24049;&#30340;&#25968;&#25454;&#38598;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24378;&#35843;&#38656;&#35201;&#22312;&#25253;&#21578;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#27169;&#22411;&#24615;&#33021;&#26102;&#36827;&#34892;&#20180;&#32454;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personality profiling has been utilised by companies for targeted advertising, political campaigns and vaccine campaigns. However, the accuracy and versatility of such models still remains relatively unknown. Consequently, we aim to explore the extent to which peoples' online digital footprints can be used to profile their Myers-Briggs personality type. We analyse and compare the results of four models: logistic regression, naive Bayes, support vector machines (SVMs) and random forests. We discover that a SVM model achieves the best accuracy of 20.95% for predicting someones complete personality type. However, logistic regression models perform only marginally worse and are significantly faster to train and perform predictions. We discover that many labelled datasets present substantial class imbalances of personal characteristics on social media, including our own. As a result, we highlight the need for attentive consideration when reporting model performance on these datasets and com
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#29992;&#25143;&#24847;&#22270;&#20998;&#31867;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#20998;&#26512;&#21644;&#39564;&#35777;&#26085;&#24535;&#25968;&#25454;&#20013;&#30340;&#29992;&#25143;&#24847;&#22270;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#25163;&#21160;&#25110;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26631;&#27880;&#26041;&#27861;&#22312;&#22823;&#22411;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.13063</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#12289;&#39564;&#35777;&#21644;&#24212;&#29992;&#29992;&#25143;&#24847;&#22270;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Using Large Language Models to Generate, Validate, and Apply User Intent Taxonomies. (arXiv:2309.13063v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13063
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#29992;&#25143;&#24847;&#22270;&#20998;&#31867;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#20998;&#26512;&#21644;&#39564;&#35777;&#26085;&#24535;&#25968;&#25454;&#20013;&#30340;&#29992;&#25143;&#24847;&#22270;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#25163;&#21160;&#25110;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26631;&#27880;&#26041;&#27861;&#22312;&#22823;&#22411;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26085;&#24535;&#25968;&#25454;&#21487;&#20197;&#25581;&#31034;&#29992;&#25143;&#19982;&#32593;&#32476;&#25628;&#32034;&#26381;&#21153;&#30340;&#20132;&#20114;&#26041;&#24335;&#12289;&#29992;&#25143;&#30340;&#38656;&#27714;&#20197;&#21450;&#28385;&#24847;&#31243;&#24230;&#31561;&#23453;&#36149;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#20998;&#26512;&#26085;&#24535;&#25968;&#25454;&#20013;&#30340;&#29992;&#25143;&#24847;&#22270;&#24182;&#19981;&#23481;&#26131;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#26032;&#30340;&#32593;&#32476;&#25628;&#32034;&#24418;&#24335;&#65292;&#22914;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#32842;&#22825;&#12290;&#20026;&#20102;&#29702;&#35299;&#26085;&#24535;&#25968;&#25454;&#20013;&#30340;&#29992;&#25143;&#24847;&#22270;&#65292;&#25105;&#20204;&#38656;&#35201;&#19968;&#31181;&#33021;&#22815;&#29992;&#26377;&#24847;&#20041;&#30340;&#20998;&#31867;&#26041;&#24335;&#26631;&#35760;&#23427;&#20204;&#30340;&#26041;&#27861;&#65292;&#20197;&#25429;&#25417;&#20854;&#22810;&#26679;&#24615;&#21644;&#21160;&#24577;&#24615;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#21160;&#25110;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26631;&#27880;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#20110;&#22823;&#22411;&#19988;&#19981;&#26029;&#21464;&#21270;&#30340;&#25968;&#25454;&#38598;&#32780;&#35328;&#65292;&#35201;&#20040;&#20195;&#20215;&#39640;&#26114;&#35201;&#20040;&#19981;&#22815;&#28789;&#27963;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#26032;&#26041;&#27861;&#65292;&#36825;&#31181;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#20016;&#23500;&#19988;&#30456;&#20851;&#30340;&#27010;&#24565;&#12289;&#25551;&#36848;&#21644;&#31034;&#20363;&#26469;&#34920;&#31034;&#29992;&#25143;&#24847;&#22270;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;LLM&#29983;&#25104;&#29992;&#25143;&#24847;&#22270;&#20998;&#31867;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#26085;&#24535;&#20998;&#26512;&#21487;&#33021;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65306;&#36825;&#26679;&#30340;&#20998;&#31867;&#24471;&#19981;&#21040;&#22806;&#37096;&#39564;&#35777;&#65292;&#24182;&#19988;&#21487;&#33021;&#23384;&#22312;&#19981;&#33391;&#30340;&#21453;&#39304;&#22238;&#36335;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20154;&#24037;&#19987;&#23478;&#21644;&#35780;&#20272;&#32773;&#26469;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Log data can reveal valuable information about how users interact with web search services, what they want, and how satisfied they are. However, analyzing user intents in log data is not easy, especially for new forms of web search such as AI-driven chat. To understand user intents from log data, we need a way to label them with meaningful categories that capture their diversity and dynamics. Existing methods rely on manual or ML-based labeling, which are either expensive or inflexible for large and changing datasets. We propose a novel solution using large language models (LLMs), which can generate rich and relevant concepts, descriptions, and examples for user intents. However, using LLMs to generate a user intent taxonomy and apply it to do log analysis can be problematic for two main reasons: such a taxonomy is not externally validated, and there may be an undesirable feedback loop. To overcome these issues, we propose a new methodology with human experts and assessors to verify th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#26041;&#27861;&#65292;&#21033;&#29992;BioBERT&#27169;&#22411;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#29983;&#27542;&#32454;&#32990;&#31995;&#22522;&#22240;&#19982;&#30142;&#30149;&#30340;&#20851;&#32852;&#65292;&#23637;&#31034;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#37325;&#35201;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2309.13061</link><description>&lt;p&gt;
&#23558;BioBERT&#24212;&#29992;&#20110;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#29983;&#27542;&#32454;&#32990;&#31995;&#22522;&#22240;&#19982;&#30142;&#30149;&#20851;&#32852;&#20197;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
Applying BioBERT to Extract Germline Gene-Disease Associations for Building a Knowledge Graph from the Biomedical Literature. (arXiv:2309.13061v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13061
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#26041;&#27861;&#65292;&#21033;&#29992;BioBERT&#27169;&#22411;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#29983;&#27542;&#32454;&#32990;&#31995;&#22522;&#22240;&#19982;&#30142;&#30149;&#30340;&#20851;&#32852;&#65292;&#23637;&#31034;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#37325;&#35201;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#34920;&#30340;&#29983;&#29289;&#21307;&#23398;&#20449;&#24687;&#25968;&#37327;&#19981;&#26029;&#22686;&#21152;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#30340;&#26368;&#26032;&#36827;&#23637;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#33258;&#21160;&#25552;&#21462;&#12289;&#35268;&#33539;&#21270;&#21644;&#34920;&#31034;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;(&#22914;&#22522;&#22240;&#21644;&#30142;&#30149;)&#30693;&#35782;&#30340;&#27987;&#21402;&#20852;&#36259;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22522;&#22240;&#21644;&#30142;&#30149;&#39046;&#22495;&#30340;&#29983;&#27542;&#32454;&#32990;&#31995;&#25688;&#35201;&#65292;&#29992;&#20110;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#20197;&#23637;&#31034;&#36825;&#19968;&#39046;&#22495;&#30340;&#22823;&#37327;&#24037;&#20316;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SimpleGermKG&#30340;&#33258;&#21160;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#26041;&#27861;&#65292;&#23558;&#29983;&#27542;&#32454;&#32990;&#31995;&#22522;&#22240;&#21644;&#30142;&#30149;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#22312;&#29983;&#29289;&#21307;&#23398;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#30340;BioBERT&#27169;&#22411;&#26469;&#25552;&#21462;&#22522;&#22240;&#21644;&#30142;&#30149;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26412;&#20307;&#21644;&#35268;&#21017;&#30340;&#31639;&#27861;&#26469;&#35268;&#33539;&#21270;&#21644;&#28040;&#27495;&#20041;&#21307;&#23398;&#26415;&#35821;&#12290;&#23545;&#20110;&#25991;&#31456;&#12289;&#22522;&#22240;&#21644;&#30142;&#30149;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#31181;&#37096;&#20998;-&#25972;&#20307;&#20851;&#31995;&#26041;&#27861;&#26469;&#23558;&#27599;&#20010;&#23454;&#20307;&#19982;&#20854;&#25968;&#25454;&#28304;&#36830;&#25509;&#24182;&#20197;&#22270;&#24418;&#21270;&#30693;&#35782;&#23637;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Published biomedical information has and continues to rapidly increase. The recent advancements in Natural Language Processing (NLP), have generated considerable interest in automating the extraction, normalization, and representation of biomedical knowledge about entities such as genes and diseases. Our study analyzes germline abstracts in the construction of knowledge graphs of the of the immense work that has been done in this area for genes and diseases. This paper presents SimpleGermKG, an automatic knowledge graph construction approach that connects germline genes and diseases. For the extraction of genes and diseases, we employ BioBERT, a pre-trained BERT model on biomedical corpora. We propose an ontology-based and rule-based algorithm to standardize and disambiguate medical terms. For semantic relationships between articles, genes, and diseases, we implemented a part-whole relation approach to connect each entity with its data source and visualize them in a graph-based knowled
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ChatGPT&#30340;&#20010;&#24615;&#21270;&#33521;&#35821;&#38405;&#35835;&#29702;&#35299;&#36741;&#21161;&#31995;&#32479;ChatPRCS&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20808;&#36827;&#33021;&#21147;&#65292;&#35813;&#31995;&#32479;&#37319;&#29992;&#38405;&#35835;&#29702;&#35299;&#33021;&#21147;&#39044;&#27979;&#12289;&#38382;&#39064;&#29983;&#25104;&#12289;&#33258;&#21160;&#35780;&#20272;&#31561;&#26041;&#27861;&#26469;&#22686;&#24378;&#38405;&#35835;&#29702;&#35299;&#25945;&#23398;&#12290;&#20351;&#29992;&#23398;&#29983;&#30340;&#21382;&#21490;&#25968;&#25454;&#26469;&#39044;&#27979;&#38405;&#35835;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#29983;&#25104;&#36866;&#24403;&#38590;&#24230;&#30340;&#38382;&#39064;&#12290;&#36741;&#21161;&#31995;&#32479;&#25552;&#20379;&#20102;&#20010;&#20307;&#21270;&#30340;&#38405;&#35835;&#29702;&#35299;&#35757;&#32451;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2309.12808</link><description>&lt;p&gt;
ChatPRCS: &#22522;&#20110;ChatGPT&#30340;&#20010;&#24615;&#21270;&#33521;&#35821;&#38405;&#35835;&#29702;&#35299;&#36741;&#21161;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
ChatPRCS: A Personalized Support System for English Reading Comprehension based on ChatGPT. (arXiv:2309.12808v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ChatGPT&#30340;&#20010;&#24615;&#21270;&#33521;&#35821;&#38405;&#35835;&#29702;&#35299;&#36741;&#21161;&#31995;&#32479;ChatPRCS&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20808;&#36827;&#33021;&#21147;&#65292;&#35813;&#31995;&#32479;&#37319;&#29992;&#38405;&#35835;&#29702;&#35299;&#33021;&#21147;&#39044;&#27979;&#12289;&#38382;&#39064;&#29983;&#25104;&#12289;&#33258;&#21160;&#35780;&#20272;&#31561;&#26041;&#27861;&#26469;&#22686;&#24378;&#38405;&#35835;&#29702;&#35299;&#25945;&#23398;&#12290;&#20351;&#29992;&#23398;&#29983;&#30340;&#21382;&#21490;&#25968;&#25454;&#26469;&#39044;&#27979;&#38405;&#35835;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#29983;&#25104;&#36866;&#24403;&#38590;&#24230;&#30340;&#38382;&#39064;&#12290;&#36741;&#21161;&#31995;&#32479;&#25552;&#20379;&#20102;&#20010;&#20307;&#21270;&#30340;&#38405;&#35835;&#29702;&#35299;&#35757;&#32451;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#23398;&#20064;&#33521;&#35821;&#30340;&#24120;&#35265;&#26041;&#27861;&#65292;&#38405;&#35835;&#29702;&#35299;&#20027;&#35201;&#21253;&#25324;&#38405;&#35835;&#25991;&#31456;&#21644;&#22238;&#31572;&#30456;&#20851;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#26377;&#25928;&#32451;&#20064;&#30340;&#22797;&#26434;&#24615;&#23548;&#33268;&#23398;&#29983;&#36935;&#21040;&#26631;&#20934;&#21270;&#38382;&#39064;&#65292;&#20351;&#20854;&#38590;&#20197;&#19982;&#20010;&#20307;&#21270;&#23398;&#20064;&#32773;&#30340;&#38405;&#35835;&#29702;&#35299;&#33021;&#21147;&#30456;&#21305;&#37197;&#12290;&#26412;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#25552;&#20379;&#30340;&#20808;&#36827;&#33021;&#21147;&#65292;&#22522;&#20110;&#36817;&#21457;&#23637;&#21306;&#29702;&#35770;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#20010;&#24615;&#21270;&#38405;&#35835;&#29702;&#35299;&#36741;&#21161;&#31995;&#32479;ChatPRCS&#12290;ChatPRCS&#37319;&#29992;&#38405;&#35835;&#29702;&#35299;&#33021;&#21147;&#39044;&#27979;&#12289;&#38382;&#39064;&#29983;&#25104;&#12289;&#33258;&#21160;&#35780;&#20272;&#31561;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#38405;&#35835;&#29702;&#35299;&#25945;&#23398;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#26681;&#25454;&#23398;&#20064;&#32773;&#30340;&#21382;&#21490;&#25968;&#25454;&#39044;&#27979;&#20182;&#20204;&#30340;&#38405;&#35835;&#29702;&#35299;&#33021;&#21147;&#65292;&#20026;&#29983;&#25104;&#20855;&#26377;&#30456;&#24212;&#38590;&#24230;&#27700;&#24179;&#30340;&#38382;&#39064;&#22880;&#23450;&#22522;&#30784;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31995;&#21015;&#30340;&#38405;&#35835;&#29702;&#35299;&#25903;&#25345;&#21151;&#33021;&#65292;&#22914;&#38382;&#39064;&#29983;&#25104;&#12289;&#33258;&#21160;&#35780;&#20272;&#31561;&#65292;&#26469;&#24110;&#21161;&#23398;&#29983;&#36827;&#34892;&#20010;&#24615;&#21270;&#30340;&#38405;&#35835;&#29702;&#35299;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a common approach to learning English, reading comprehension primarily entails reading articles and answering related questions. However, the complexity of designing effective exercises results in students encountering standardized questions, making it challenging to align with individualized learners' reading comprehension ability. By leveraging the advanced capabilities offered by large language models, exemplified by ChatGPT, this paper presents a novel personalized support system for reading comprehension, referred to as ChatPRCS, based on the Zone of Proximal Development theory. ChatPRCS employs methods including reading comprehension proficiency prediction, question generation, and automatic evaluation, among others, to enhance reading comprehension instruction. First, we develop a new algorithm that can predict learners' reading comprehension abilities using their historical data as the foundation for generating questions at an appropriate level of difficulty. Second, a serie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21327;&#21161;&#19987;&#19994;&#20316;&#23478;&#26041;&#38754;&#30340;&#25928;&#29992;&#65292;&#24182;&#21457;&#29616;&#20316;&#23478;&#20204;&#26356;&#20542;&#21521;&#20110;&#22312;&#32763;&#35793;&#21644;&#23457;&#26597;&#38454;&#27573;&#20013;&#23547;&#27714;LLM&#30340;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2309.12570</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19979;&#30340;&#21019;&#36896;&#21147;&#25903;&#25345;: &#19968;&#39033;&#28041;&#21450;&#26032;&#20852;&#20316;&#23478;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Creativity Support in the Age of Large Language Models: An Empirical Study Involving Emerging Writers. (arXiv:2309.12570v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21327;&#21161;&#19987;&#19994;&#20316;&#23478;&#26041;&#38754;&#30340;&#25928;&#29992;&#65292;&#24182;&#21457;&#29616;&#20316;&#23478;&#20204;&#26356;&#20542;&#21521;&#20110;&#22312;&#32763;&#35793;&#21644;&#23457;&#26597;&#38454;&#27573;&#20013;&#23547;&#27714;LLM&#30340;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#20351;&#24471;&#20854;&#33021;&#22815;&#36981;&#24490;&#25351;&#20196;&#24182;&#21442;&#19982;&#23545;&#35805;&#20114;&#21160;&#65292;&#24341;&#21457;&#20102;&#22312;&#21508;&#31181;&#25903;&#25345;&#24037;&#20855;&#20013;&#21033;&#29992;&#23427;&#20204;&#30340;&#20852;&#36259;&#22686;&#21152;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#39033;&#23454;&#35777;&#29992;&#25143;&#30740;&#31350;&#65288;n=30&#65289;&#25506;&#35752;&#20102;&#29616;&#20195;LLM&#22312;&#21327;&#21161;&#19987;&#19994;&#20316;&#23478;&#26041;&#38754;&#30340;&#25928;&#29992;&#12290;&#25105;&#20204;&#30340;&#21512;&#20316;&#20889;&#20316;&#30028;&#38754;&#35774;&#35745;&#22522;&#20110;&#23558;&#20889;&#20316;&#35270;&#20026;&#19968;&#20010;&#30446;&#26631;&#23548;&#21521;&#30340;&#24605;&#32500;&#36807;&#31243;&#30340;&#35748;&#30693;&#36807;&#31243;&#27169;&#22411;&#65292;&#28085;&#30422;&#20102;&#38750;&#32447;&#24615;&#30340;&#35748;&#30693;&#27963;&#21160;&#65306;&#35268;&#21010;&#12289;&#32763;&#35793;&#21644;&#23457;&#26597;&#12290;&#21442;&#19982;&#32773;&#34987;&#35201;&#27714;&#25552;&#20132;&#19968;&#20221;&#21518;&#23436;&#25104;&#35843;&#26597;&#65292;&#20197;&#25552;&#20379;&#20851;&#20110;LLM&#20316;&#20026;&#20889;&#20316;&#21512;&#20316;&#32773;&#28508;&#21147;&#21644;&#38382;&#39064;&#30340;&#21453;&#39304;&#12290;&#36890;&#36807;&#20998;&#26512;&#20316;&#23478;-LLM&#20114;&#21160;,&#25105;&#20204;&#21457;&#29616;&#20316;&#23478;&#22312;&#19977;&#31181;&#31867;&#22411;&#30340;&#35748;&#30693;&#27963;&#21160;&#20013;&#37117;&#23547;&#27714;LLM&#30340;&#24110;&#21161;&#65292;&#20294;&#20182;&#20204;&#21457;&#29616;LLM&#22312;&#32763;&#35793;&#21644;&#23457;&#26597;&#26041;&#38754;&#26356;&#26377;&#24110;&#21161;&#12290;&#36890;&#36807;&#20998;&#26512;&#20114;&#21160;&#21644;&#35843;&#26597;&#32467;&#26524;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#24378;&#35843;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of large language models (LLMs) capable of following instructions and engaging in conversational interactions sparked increased interest in their utilization across various support tools. We investigate the utility of modern LLMs in assisting professional writers via an empirical user study (n=30). The design of our collaborative writing interface is grounded in the cognitive process model of writing that views writing as a goal-oriented thinking process encompassing non-linear cognitive activities: planning, translating, and reviewing. Participants are asked to submit a post-completion survey to provide feedback on the potential and pitfalls of LLMs as writing collaborators. Upon analyzing the writer-LLM interactions, we find that while writers seek LLM's help across all three types of cognitive activities, they find LLMs more helpful in translation and reviewing. Our findings from analyzing both the interactions and the survey responses highlight future research direc
&lt;/p&gt;</description></item><item><title>LLMs&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#21482;&#33021;&#23398;&#20064;&#21040;"A&#26159;B"&#30340;&#32467;&#26500;&#65292;&#26080;&#27861;&#33258;&#21160;&#25512;&#24191;&#21040;"B&#26159;A"&#12290;&#36825;&#34920;&#26126;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#26029;&#19978;&#23384;&#22312;&#22522;&#26412;&#22833;&#36133;&#21644;&#35757;&#32451;&#38598;&#20013;&#27169;&#24335;&#30340;&#25512;&#24191;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.12288</link><description>&lt;p&gt;
&#32763;&#36716;&#35781;&#21650;: &#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35757;&#32451;&#30340;"A&#26159;B"&#26080;&#27861;&#23398;&#20064;"B&#26159;A"
&lt;/p&gt;
&lt;p&gt;
The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A". (arXiv:2309.12288v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12288
&lt;/p&gt;
&lt;p&gt;
LLMs&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#21482;&#33021;&#23398;&#20064;&#21040;"A&#26159;B"&#30340;&#32467;&#26500;&#65292;&#26080;&#27861;&#33258;&#21160;&#25512;&#24191;&#21040;"B&#26159;A"&#12290;&#36825;&#34920;&#26126;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#26029;&#19978;&#23384;&#22312;&#22522;&#26412;&#22833;&#36133;&#21644;&#35757;&#32451;&#38598;&#20013;&#27169;&#24335;&#30340;&#25512;&#24191;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25581;&#31034;&#20102;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#27867;&#21270;&#19978;&#30340;&#20196;&#20154;&#24778;&#35766;&#30340;&#22833;&#36133;&#12290;&#22914;&#26524;&#19968;&#20010;&#27169;&#22411;&#26159;&#22522;&#20110;"A&#26159;B"&#24418;&#24335;&#30340;&#21477;&#23376;&#36827;&#34892;&#35757;&#32451;&#65292;&#23427;&#19981;&#20250;&#33258;&#21160;&#25512;&#24191;&#21040;&#30456;&#21453;&#30340;&#26041;&#21521;"B&#26159;A"&#12290;&#36825;&#23601;&#26159;&#32763;&#36716;&#35781;&#21650;&#12290;&#20363;&#22914;&#65292;&#22914;&#26524;&#19968;&#20010;&#27169;&#22411;&#26159;&#22522;&#20110;"Olaf Scholz&#26159;&#24503;&#22269;&#31532;&#20061;&#20219;&#24635;&#29702;"&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#23427;&#19981;&#20250;&#33258;&#21160;&#33021;&#22815;&#22238;&#31572;&#38382;&#39064;"&#35841;&#26159;&#24503;&#22269;&#31532;&#20061;&#20219;&#24635;&#29702;&#65311;"&#12290;&#27492;&#22806;&#65292;&#27491;&#30830;&#31572;&#26696;&#65288;"Olaf Scholz"&#65289;&#30340;&#21487;&#33021;&#24615;&#19981;&#20250;&#27604;&#38543;&#26426;&#21517;&#23383;&#26356;&#39640;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#26029;&#19978;&#23384;&#22312;&#22522;&#26412;&#22833;&#36133;&#65292;&#24182;&#19988;&#19981;&#20250;&#25512;&#24191;&#21040;&#23427;&#20204;&#35757;&#32451;&#38598;&#20013;&#30340;&#26222;&#36941;&#27169;&#24335;&#65288;&#21363;&#22914;&#26524;&#20986;&#29616;"A&#26159;B"&#65292;&#21017;"B&#26159;A"&#26356;&#21487;&#33021;&#20986;&#29616;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#34394;&#26500;&#30340;&#38472;&#36848;&#65288;&#22914;"Uriah Hawthorne&#26159;'Abyssal Melodies'&#30340;&#20316;&#26354;&#23478;"&#65289;&#19978;&#23545;GPT-3&#21644;Llama-1&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#23637;&#31034;&#23427;&#20204;&#26080;&#27861;&#27491;&#30830;&#22238;&#31572;"&#35841;&#21019;&#20316;&#20102;'Abyssal Melodies'?"&#26469;&#25552;&#20379;&#32763;&#36716;&#35781;&#21650;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We expose a surprising failure of generalization in auto-regressive large language models (LLMs). If a model is trained on a sentence of the form "A is B", it will not automatically generalize to the reverse direction "B is A". This is the Reversal Curse. For instance, if a model is trained on "Olaf Scholz was the ninth Chancellor of Germany", it will not automatically be able to answer the question, "Who was the ninth Chancellor of Germany?". Moreover, the likelihood of the correct answer ("Olaf Scholz") will not be higher than for a random name. Thus, models exhibit a basic failure of logical deduction and do not generalize a prevalent pattern in their training set (i.e. if "A is B'' occurs, "B is A" is more likely to occur). We provide evidence for the Reversal Curse by finetuning GPT-3 and Llama-1 on fictitious statements such as "Uriah Hawthorne is the composer of 'Abyssal Melodies'" and showing that they fail to correctly answer "Who composed 'Abyssal Melodies?'". The Reversal Cu
&lt;/p&gt;</description></item><item><title>&#21073;&#26725;&#27861;&#24459;&#35821;&#26009;&#24211;&#26159;&#19968;&#20010;&#29992;&#20110;&#27861;&#24459;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;&#26469;&#33258;&#33521;&#22269;&#30340;&#36229;&#36807;250,000&#20010;&#27861;&#24237;&#26696;&#20363;&#12290;&#22312;&#35813;&#35821;&#26009;&#24211;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26696;&#20363;&#32467;&#26524;&#30340;&#19987;&#23478;&#27880;&#35299;&#65292;&#24182;&#20351;&#29992;&#22810;&#20010;&#27169;&#22411;&#36827;&#34892;&#20102;&#26696;&#20363;&#32467;&#26524;&#25552;&#21462;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#20026;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2309.12269</link><description>&lt;p&gt;
&#21073;&#26725;&#27861;&#24459;&#35821;&#26009;&#24211;&#65306;&#29992;&#20110;&#27861;&#24459;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
The Cambridge Law Corpus: A Corpus for Legal AI Research. (arXiv:2309.12269v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12269
&lt;/p&gt;
&lt;p&gt;
&#21073;&#26725;&#27861;&#24459;&#35821;&#26009;&#24211;&#26159;&#19968;&#20010;&#29992;&#20110;&#27861;&#24459;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;&#26469;&#33258;&#33521;&#22269;&#30340;&#36229;&#36807;250,000&#20010;&#27861;&#24237;&#26696;&#20363;&#12290;&#22312;&#35813;&#35821;&#26009;&#24211;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26696;&#20363;&#32467;&#26524;&#30340;&#19987;&#23478;&#27880;&#35299;&#65292;&#24182;&#20351;&#29992;&#22810;&#20010;&#27169;&#22411;&#36827;&#34892;&#20102;&#26696;&#20363;&#32467;&#26524;&#25552;&#21462;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#20026;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#21073;&#26725;&#27861;&#24459;&#35821;&#26009;&#24211;&#65288;CLC&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#27861;&#24459;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#35821;&#26009;&#24211;&#12290;&#23427;&#21253;&#21547;&#20102;&#26469;&#33258;&#33521;&#22269;&#30340;&#36229;&#36807;250,000&#20010;&#27861;&#24237;&#26696;&#20363;&#12290;&#22823;&#37096;&#20998;&#26696;&#20363;&#26469;&#33258;21&#19990;&#32426;&#65292;&#20294;&#35813;&#35821;&#26009;&#24211;&#21253;&#25324;&#20102;16&#19990;&#32426;&#20197;&#26469;&#30340;&#26696;&#20363;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#35813;&#35821;&#26009;&#24211;&#30340;&#39318;&#27425;&#21457;&#24067;&#65292;&#21253;&#25324;&#21407;&#22987;&#25991;&#26412;&#21644;&#20803;&#25968;&#25454;&#12290;&#22312;&#35821;&#26009;&#24211;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;638&#20010;&#26696;&#20363;&#30340;&#27861;&#24459;&#19987;&#23478;&#23545;&#26696;&#20363;&#32467;&#26524;&#30340;&#27880;&#35299;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#26631;&#27880;&#25968;&#25454;&#65292;&#35757;&#32451;&#21644;&#35780;&#20272;&#20102;GPT-3&#12289;GPT-4&#21644;RoBERTa&#27169;&#22411;&#36827;&#34892;&#26696;&#20363;&#32467;&#26524;&#25552;&#21462;&#65292;&#20197;&#25552;&#20379;&#22522;&#20934;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27861;&#24459;&#21644;&#20262;&#29702;&#35752;&#35770;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#26448;&#26009;&#21487;&#33021;&#20855;&#26377;&#25935;&#24863;&#24615;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#35813;&#35821;&#26009;&#24211;&#21482;&#20250;&#22312;&#19968;&#23450;&#38480;&#21046;&#19979;&#29992;&#20110;&#30740;&#31350;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the Cambridge Law Corpus (CLC), a corpus for legal AI research. It consists of over 250 000 court cases from the UK. Most cases are from the 21st century, but the corpus includes cases as old as the 16th century. This paper presents the first release of the corpus, containing the raw text and meta-data. Together with the corpus, we provide annotations on case outcomes for 638 cases, done by legal experts. Using our annotated data, we have trained and evaluated case outcome extraction with GPT-3, GPT-4 and RoBERTa models to provide benchmarks. We include an extensive legal and ethical discussion to address the potentially sensitive nature of this material. As a consequence, the corpus will only be released for research purposes under certain restrictions.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38142;&#24335;&#39564;&#35777;&#26041;&#27861;&#65288;CoVe&#65289;&#65292;&#36890;&#36807;&#22312;&#22238;&#31572;&#20043;&#21069;&#36827;&#34892;&#22791;&#26597;&#38382;&#39064;&#26469;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#12290;&#23454;&#39564;&#35777;&#26126;CoVe&#26041;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#37117;&#33021;&#26377;&#25928;&#38477;&#20302;&#24187;&#35273;&#30340;&#21457;&#29983;&#12290;</title><link>http://arxiv.org/abs/2309.11495</link><description>&lt;p&gt;
&#38142;&#24335;&#39564;&#35777;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Verification Reduces Hallucination in Large Language Models. (arXiv:2309.11495v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11495
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38142;&#24335;&#39564;&#35777;&#26041;&#27861;&#65288;CoVe&#65289;&#65292;&#36890;&#36807;&#22312;&#22238;&#31572;&#20043;&#21069;&#36827;&#34892;&#22791;&#26597;&#38382;&#39064;&#26469;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#12290;&#23454;&#39564;&#35777;&#26126;CoVe&#26041;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#37117;&#33021;&#26377;&#25928;&#38477;&#20302;&#24187;&#35273;&#30340;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#29983;&#25104;&#21512;&#29702;&#20294;&#19981;&#27491;&#30830;&#30340;&#20107;&#23454;&#20449;&#24687;&#65288;&#21363;&#24187;&#35273;&#65289;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#32473;&#20986;&#22238;&#22797;&#26102;&#36827;&#34892;&#24605;&#32771;&#20197;&#32416;&#27491;&#38169;&#35823;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#38142;&#24335;&#39564;&#35777;&#65288;CoVe&#65289;&#26041;&#27861;&#65292;&#27169;&#22411;&#39318;&#20808;&#65288;i&#65289;&#36215;&#33609;&#21021;&#22987;&#22238;&#22797;&#65307;&#28982;&#21518;&#65288;ii&#65289;&#35745;&#21010;&#39564;&#35777;&#38382;&#39064;&#26469;&#20107;&#23454;&#26816;&#26597;&#33609;&#31295;&#65307;&#65288;iii&#65289;&#29420;&#31435;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#20197;&#36991;&#20813;&#31572;&#26696;&#21463;&#20854;&#20182;&#22238;&#22797;&#30340;&#24433;&#21709;&#65307;&#26368;&#21518;&#65288;iv&#65289;&#29983;&#25104;&#26368;&#32456;&#30340;&#32463;&#36807;&#39564;&#35777;&#30340;&#22238;&#31572;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;CoVe&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#38477;&#20302;&#20102;&#24187;&#35273;&#30340;&#24773;&#20917;&#65292;&#21253;&#25324;&#26469;&#33258;&#32500;&#22522;&#25968;&#25454;&#30340;&#21015;&#34920;&#38382;&#39064;&#12289;&#23553;&#38381;&#20070;&#31821;MultiSpanQA&#21644;&#38271;&#25991;&#26412;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generation of plausible yet incorrect factual information, termed hallucination, is an unsolved issue in large language models. We study the ability of language models to deliberate on the responses they give in order to correct their mistakes. We develop the Chain-of-Verification (CoVe) method whereby the model first (i) drafts an initial response; then (ii) plans verification questions to fact-check its draft; (iii) answers those questions independently so the answers are not biased by other responses; and (iv) generates its final verified response. In experiments, we show CoVe decreases hallucinations across a variety of tasks, from list-based questions from Wikidata, closed book MultiSpanQA and longform text generation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25910;&#38598;&#21644;&#26631;&#27880;&#25968;&#25454;&#20197;&#21450;&#25506;&#32034;&#20999;&#25442;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#31361;&#23612;&#26031;&#26041;&#35328;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#26469;&#28040;&#38500;&#25340;&#20889;&#19981;&#21512;&#36866;&#30340;&#24178;&#25200;&#12290;</title><link>http://arxiv.org/abs/2309.11327</link><description>&lt;p&gt;
&#21033;&#29992;&#25968;&#25454;&#25910;&#38598;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#20999;&#25442;&#31361;&#23612;&#26031;&#38463;&#25289;&#20271;&#35821;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Leveraging Data Collection and Unsupervised Learning for Code-switched Tunisian Arabic Automatic Speech Recognition. (arXiv:2309.11327v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25910;&#38598;&#21644;&#26631;&#27880;&#25968;&#25454;&#20197;&#21450;&#25506;&#32034;&#20999;&#25442;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#31361;&#23612;&#26031;&#26041;&#35328;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#26469;&#28040;&#38500;&#25340;&#20889;&#19981;&#21512;&#36866;&#30340;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#33021;&#22815;&#26377;&#25928;&#35782;&#21035;&#26041;&#35328;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#35299;&#20915;&#26041;&#26696;&#38656;&#35201;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#19981;&#20165;&#35201;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#36824;&#35201;&#22788;&#29702;&#35821;&#35328;&#22810;&#26679;&#24615;&#30340;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#38024;&#23545;&#31361;&#23612;&#26031;&#26041;&#35328;&#65292;&#35299;&#20915;&#20102;&#19978;&#36848;ASR&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#25910;&#38598;&#20102;&#25991;&#26412;&#21644;&#38899;&#39057;&#25968;&#25454;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#26631;&#27880;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25506;&#32034;&#33258;&#25105;&#30417;&#30563;&#12289;&#21322;&#30417;&#30563;&#21644;&#23569;&#26679;&#26412;&#20999;&#25442;&#26041;&#27861;&#65292;&#20197;&#22312;&#19981;&#21516;&#31361;&#23612;&#26031;&#27979;&#35797;&#38598;&#19978;&#25512;&#21160;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#65307;&#28085;&#30422;&#19981;&#21516;&#30340;&#22768;&#23398;&#12289;&#35821;&#35328;&#21644;&#38901;&#24459;&#26465;&#20214;&#12290;&#26368;&#21518;&#65292;&#37492;&#20110;&#24120;&#35268;&#25340;&#20889;&#30340;&#32570;&#22833;&#65292;&#25105;&#20204;&#23545;&#36716;&#24405;&#36827;&#34892;&#20154;&#24037;&#35780;&#20272;&#65292;&#20197;&#36991;&#20813;&#27979;&#35797;&#21442;&#32771;&#20013;&#30340;&#25340;&#20889;&#19981;&#21512;&#36866;&#25152;&#24102;&#26469;&#30340;&#22122;&#22768;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#36716;&#24405;&#31361;&#23612;&#26031;&#38463;&#25289;&#20271;&#35821;&#12289;&#33521;&#35821;&#21644;&#27861;&#35821;&#28151;&#21512;&#35821;&#35328;&#30340;&#38899;&#39057;&#26679;&#26412;&#65292;&#24182;&#20844;&#24320;&#21457;&#24067;&#20102;&#25152;&#26377;&#35757;&#32451;&#21644;&#27979;&#35797;&#25152;&#20351;&#29992;&#30340;&#25968;&#25454;&#65292;&#20379;&#20844;&#20247;&#20351;&#29992;&#21644;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Crafting an effective Automatic Speech Recognition (ASR) solution for dialects demands innovative approaches that not only address the data scarcity issue but also navigate the intricacies of linguistic diversity. In this paper, we address the aforementioned ASR challenge, focusing on the Tunisian dialect. First, textual and audio data is collected and in some cases annotated. Second, we explore self-supervision, semi-supervision and few-shot code-switching approaches to push the state-of-the-art on different Tunisian test sets; covering different acoustic, linguistic and prosodic conditions. Finally, and given the absence of conventional spelling, we produce a human evaluation of our transcripts to avoid the noise coming from spelling inadequacies in our testing references. Our models, allowing to transcribe audio samples in a linguistic mix involving Tunisian Arabic, English and French, and all the data used during training and testing are released for public use and further improvem
&lt;/p&gt;</description></item><item><title>DISC-LawLLM&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20026;&#26234;&#33021;&#27861;&#24459;&#26381;&#21153;&#32454;&#35843;&#30340;&#26234;&#33021;&#27861;&#24459;&#31995;&#32479;&#65292;&#36890;&#36807;&#37319;&#29992;&#27861;&#24459;&#25512;&#29702;&#25552;&#31034;&#31574;&#30053;&#21644;&#22686;&#24378;&#30340;&#26816;&#32034;&#27169;&#22359;&#65292;&#25552;&#20379;&#20102;&#22312;&#20013;&#22269;&#21496;&#27861;&#39046;&#22495;&#22810;&#26679;&#21270;&#27861;&#24459;&#22330;&#26223;&#19979;&#30340;&#26377;&#25928;&#27861;&#24459;&#26381;&#21153;&#12290;</title><link>http://arxiv.org/abs/2309.11325</link><description>&lt;p&gt;
DISC-LawLLM:&#20026;&#26234;&#33021;&#27861;&#24459;&#26381;&#21153;&#32454;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DISC-LawLLM: Fine-tuning Large Language Models for Intelligent Legal Services. (arXiv:2309.11325v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11325
&lt;/p&gt;
&lt;p&gt;
DISC-LawLLM&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20026;&#26234;&#33021;&#27861;&#24459;&#26381;&#21153;&#32454;&#35843;&#30340;&#26234;&#33021;&#27861;&#24459;&#31995;&#32479;&#65292;&#36890;&#36807;&#37319;&#29992;&#27861;&#24459;&#25512;&#29702;&#25552;&#31034;&#31574;&#30053;&#21644;&#22686;&#24378;&#30340;&#26816;&#32034;&#27169;&#22359;&#65292;&#25552;&#20379;&#20102;&#22312;&#20013;&#22269;&#21496;&#27861;&#39046;&#22495;&#22810;&#26679;&#21270;&#27861;&#24459;&#22330;&#26223;&#19979;&#30340;&#26377;&#25928;&#27861;&#24459;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;DISC-LawLLM&#65292;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25552;&#20379;&#21508;&#31181;&#27861;&#24459;&#26381;&#21153;&#30340;&#26234;&#33021;&#27861;&#24459;&#31995;&#32479;&#12290;&#25105;&#20204;&#37319;&#29992;&#27861;&#24459;&#25512;&#29702;&#25552;&#31034;&#31574;&#30053;&#65292;&#22312;&#20013;&#22269;&#21496;&#27861;&#39046;&#22495;&#26500;&#24314;&#20102;&#30417;&#30563;&#24494;&#35843;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#20855;&#22791;&#27861;&#24459;&#25512;&#29702;&#33021;&#21147;&#30340;LLMs&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#21033;&#29992;&#26816;&#32034;&#27169;&#22359;&#22686;&#24378;&#20102;LLMs&#30340;&#33021;&#21147;&#65292;&#20197;&#35775;&#38382;&#21644;&#21033;&#29992;&#22806;&#37096;&#27861;&#24459;&#30693;&#35782;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#27861;&#24459;&#22522;&#20934;&#35780;&#20272;&#31995;&#32479;&#65292;DISC-Law-Eval&#65292;&#20174;&#23458;&#35266;&#21644;&#20027;&#35266;&#20004;&#20010;&#32500;&#24230;&#35780;&#20272;&#26234;&#33021;&#27861;&#24459;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#22312;DISC-Law-Eval&#19978;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#20026;&#19981;&#21516;&#27861;&#24459;&#22330;&#26223;&#19979;&#30340;&#21508;&#31181;&#29992;&#25143;&#25552;&#20379;&#26381;&#21153;&#26041;&#38754;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;&#35814;&#32454;&#30340;&#36164;&#28304;&#21487;&#20197;&#22312;https://github.com/FudanDISC/DISC-LawLLM&#19978;&#26597;&#30475;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose DISC-LawLLM, an intelligent legal system utilizing large language models (LLMs) to provide a wide range of legal services. We adopt legal syllogism prompting strategies to construct supervised fine-tuning datasets in the Chinese Judicial domain and fine-tune LLMs with legal reasoning capability. We augment LLMs with a retrieval module to enhance models' ability to access and utilize external legal knowledge. A comprehensive legal benchmark, DISC-Law-Eval, is presented to evaluate intelligent legal systems from both objective and subjective dimensions. Quantitative and qualitative results on DISC-Law-Eval demonstrate the effectiveness of our system in serving various users across diverse legal scenarios. The detailed resources are available at https://github.com/FudanDISC/DISC-LawLLM.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25506;&#32034;&#20102;&#26367;&#25442;&#26631;&#35782;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#19979;&#28216;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#28151;&#28102;&#35821;&#26009;&#24211;&#35757;&#32451;&#30340;&#27169;&#22411;&#33021;&#22815;&#36798;&#21040;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.08628</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38544;&#31169;&#20445;&#25252;&#25513;&#30721;&#30340;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Recovering from Privacy-Preserving Masking with Large Language Models. (arXiv:2309.08628v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08628
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25506;&#32034;&#20102;&#26367;&#25442;&#26631;&#35782;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#19979;&#28216;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#28151;&#28102;&#35821;&#26009;&#24211;&#35757;&#32451;&#30340;&#27169;&#22411;&#33021;&#22815;&#36798;&#21040;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#36866;&#24212;&#23545;&#20110;&#22788;&#29702;&#20195;&#29702;&#35757;&#32451;&#25968;&#25454;&#21644;&#23454;&#38469;&#29992;&#25143;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#38750;&#24120;&#37325;&#35201;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#36827;&#34892;&#36866;&#24212;&#65292;&#29992;&#25143;&#30340;&#25991;&#26412;&#25968;&#25454;&#36890;&#24120;&#23384;&#20648;&#22312;&#26381;&#21153;&#22120;&#25110;&#26412;&#22320;&#35774;&#22791;&#19978;&#65292;&#19979;&#28216;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#36825;&#20123;&#39046;&#22495;&#20869;&#30340;&#25968;&#25454;&#36827;&#34892;&#30452;&#25509;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#21487;&#33021;&#20250;&#24341;&#36215;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#65292;&#22240;&#20026;&#23384;&#22312;&#21521;&#23545;&#25163;&#27844;&#38706;&#29992;&#25143;&#20449;&#24687;&#30340;&#39069;&#22806;&#39118;&#38505;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#24320;&#22987;&#25506;&#32034;&#20351;&#29992;&#36890;&#29992;&#26631;&#35760;&#26367;&#25442;&#25991;&#26412;&#20013;&#30340;&#26631;&#35782;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#24314;&#35758;&#26367;&#25442;&#25513;&#30721;&#26631;&#35760;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#19979;&#28216;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#19978;&#35780;&#20272;&#20854;&#25928;&#26524;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#30340;LLM&#26041;&#27861;&#65292;&#24182;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#20197;&#27604;&#36739;&#36825;&#20123;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#28151;&#28102;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#33021;&#22815;&#36798;&#21040;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model adaptation is crucial to handle the discrepancy between proxy training data and actual users data received. To effectively perform adaptation, textual data of users is typically stored on servers or their local devices, where downstream natural language processing (NLP) models can be directly trained using such in-domain data. However, this might raise privacy and security concerns due to the extra risks of exposing user information to adversaries. Replacing identifying information in textual data with a generic marker has been recently explored. In this work, we leverage large language models (LLMs) to suggest substitutes of masked tokens and have their effectiveness evaluated on downstream language modeling tasks. Specifically, we propose multiple pre-trained and fine-tuned LLM-based approaches and perform empirical studies on various datasets for the comparison of these methods. Experimental results show that models trained on the obfuscation corpora are able to achieve compar
&lt;/p&gt;</description></item><item><title>&#22312;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36981;&#24490;&#25351;&#20196;&#26102;&#65292;&#20165;&#24378;&#35843;&#24110;&#21161;&#24615;&#32780;&#19981;&#32771;&#34385;&#23433;&#20840;&#24615;&#20250;&#23548;&#33268;&#27169;&#22411;&#20135;&#29983;&#26377;&#23475;&#20869;&#23481;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#35757;&#32451;LLaMA&#27169;&#22411;&#26102;&#28155;&#21152;&#23569;&#37327;&#23433;&#20840;&#31034;&#20363;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20854;&#23433;&#20840;&#24615;&#65292;&#32780;&#19981;&#24433;&#21709;&#20854;&#33021;&#21147;&#21644;&#24110;&#21161;&#24615;&#12290;&#28982;&#32780;&#65292;&#36807;&#24230;&#23433;&#20840;&#35843;&#20248;&#20250;&#20351;&#27169;&#22411;&#25298;&#32477;&#22238;&#24212;&#34920;&#38754;&#19978;&#31867;&#20284;&#20110;&#19981;&#23433;&#20840;&#25552;&#31034;&#30340;&#21512;&#29702;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.07875</link><description>&lt;p&gt;
&#23433;&#20840;&#35843;&#20248;&#30340;LLaMAs&#65306;&#20174;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36981;&#24490;&#25351;&#20196;&#30340;&#23433;&#20840;&#24615;&#20013;&#23398;&#21040;&#30340;&#32463;&#39564;
&lt;/p&gt;
&lt;p&gt;
Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions. (arXiv:2309.07875v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07875
&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36981;&#24490;&#25351;&#20196;&#26102;&#65292;&#20165;&#24378;&#35843;&#24110;&#21161;&#24615;&#32780;&#19981;&#32771;&#34385;&#23433;&#20840;&#24615;&#20250;&#23548;&#33268;&#27169;&#22411;&#20135;&#29983;&#26377;&#23475;&#20869;&#23481;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#35757;&#32451;LLaMA&#27169;&#22411;&#26102;&#28155;&#21152;&#23569;&#37327;&#23433;&#20840;&#31034;&#20363;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20854;&#23433;&#20840;&#24615;&#65292;&#32780;&#19981;&#24433;&#21709;&#20854;&#33021;&#21147;&#21644;&#24110;&#21161;&#24615;&#12290;&#28982;&#32780;&#65292;&#36807;&#24230;&#23433;&#20840;&#35843;&#20248;&#20250;&#20351;&#27169;&#22411;&#25298;&#32477;&#22238;&#24212;&#34920;&#38754;&#19978;&#31867;&#20284;&#20110;&#19981;&#23433;&#20840;&#25552;&#31034;&#30340;&#21512;&#29702;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36981;&#24490;&#25351;&#20196;&#21487;&#20197;&#20351;&#23427;&#20204;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#24471;&#26356;&#22909;&#65292;&#36890;&#24120;&#26356;&#20855;&#26377;&#24110;&#21161;&#24615;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#23436;&#20840;&#26377;&#29992;&#30340;&#27169;&#22411;&#20250;&#36981;&#24490;&#29978;&#33267;&#26368;&#24694;&#24847;&#30340;&#25351;&#20196;&#65292;&#24182;&#36731;&#26131;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#12290;&#26412;&#25991;&#20851;&#27880;&#30340;&#26159;&#37027;&#20123;&#21482;&#24378;&#35843;&#24110;&#21161;&#24615;&#32780;&#19981;&#32771;&#34385;&#23433;&#20840;&#24615;&#30340;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20123;&#27969;&#34892;&#30340;&#25351;&#20196;&#35843;&#20248;&#27169;&#22411;&#38750;&#24120;&#19981;&#23433;&#20840;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;fine-tuning&#31867;&#20284;LLaMA&#30340;&#27169;&#22411;&#26102;&#65292;&#21482;&#38656;&#23558;3%&#30340;&#23433;&#20840;&#31034;&#20363;&#65288;&#20960;&#30334;&#20010;&#28436;&#31034;&#65289;&#28155;&#21152;&#21040;&#35757;&#32451;&#38598;&#20013;&#65292;&#23601;&#33021;&#26174;&#33879;&#25552;&#39640;&#20854;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#30340;&#23433;&#20840;&#35843;&#20248;&#24182;&#19981;&#20250;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#30340;&#33021;&#21147;&#25110;&#24110;&#21161;&#24615;&#65292;&#36825;&#26159;&#36890;&#36807;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#26469;&#34913;&#37327;&#30340;&#12290;&#20294;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#19968;&#31181;&#36807;&#24230;&#23433;&#20840;&#30340;&#34892;&#20026;&#65292;&#21363;&#36807;&#24230;&#30340;&#23433;&#20840;&#35843;&#20248;&#20250;&#20351;&#24471;&#27169;&#22411;&#25298;&#32477;&#23545;&#34920;&#38754;&#19978;&#31867;&#20284;&#20110;&#19981;&#23433;&#20840;&#25552;&#31034;&#30340;&#21512;&#29702;&#25552;&#31034;&#20570;&#20986;&#22238;&#24212;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#35757;&#32451;LLM&#27169;&#22411;&#36981;&#24490;&#25351;&#20196;&#26102;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training large language models to follow instructions makes them perform better on a wide range of tasks, generally becoming more helpful. However, a perfectly helpful model will follow even the most malicious instructions and readily generate harmful content. In this paper, we raise concerns over the safety of models that only emphasize helpfulness, not safety, in their instruction-tuning. We show that several popular instruction-tuned models are highly unsafe. Moreover, we show that adding just 3% safety examples (a few hundred demonstrations) in the training set when fine-tuning a model like LLaMA can substantially improve their safety. Our safety-tuning does not make models significantly less capable or helpful as measured by standard benchmarks. However, we do find a behavior of exaggerated safety, where too much safety-tuning makes models refuse to respond to reasonable prompts that superficially resemble unsafe ones. Our study sheds light on trade-offs in training LLMs to follow
&lt;/p&gt;</description></item><item><title>DBLPLink&#26159;&#19968;&#20010;&#29992;&#20110;DBLP&#23398;&#26415;&#30693;&#35782;&#22270;&#30340;&#23454;&#20307;&#38142;&#25509;&#22120;&#65292;&#23427;&#20351;&#29992;&#25991;&#26412;&#21040;&#25991;&#26412;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#23454;&#20307;&#23884;&#20837;&#26469;&#36827;&#34892;&#23454;&#20307;&#26631;&#31614;&#29983;&#25104;&#21644;&#25490;&#24207;&#12290;</title><link>http://arxiv.org/abs/2309.07545</link><description>&lt;p&gt;
DBLPLink: &#19968;&#20010;&#29992;&#20110;DBLP&#23398;&#26415;&#30693;&#35782;&#22270;&#30340;&#23454;&#20307;&#38142;&#25509;&#22120;
&lt;/p&gt;
&lt;p&gt;
DBLPLink: An Entity Linker for the DBLP Scholarly Knowledge Graph. (arXiv:2309.07545v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07545
&lt;/p&gt;
&lt;p&gt;
DBLPLink&#26159;&#19968;&#20010;&#29992;&#20110;DBLP&#23398;&#26415;&#30693;&#35782;&#22270;&#30340;&#23454;&#20307;&#38142;&#25509;&#22120;&#65292;&#23427;&#20351;&#29992;&#25991;&#26412;&#21040;&#25991;&#26412;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#23454;&#20307;&#23884;&#20837;&#26469;&#36827;&#34892;&#23454;&#20307;&#26631;&#31614;&#29983;&#25104;&#21644;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;DBLPLink&#30340;&#32593;&#32476;&#24212;&#29992;&#31243;&#24207;&#65292;&#23427;&#22312;DBLP&#23398;&#26415;&#30693;&#35782;&#22270;&#19978;&#25191;&#34892;&#23454;&#20307;&#38142;&#25509;&#12290;DBLPLink&#20351;&#29992;&#25991;&#26412;&#21040;&#25991;&#26412;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;T5&#65289;&#20174;&#36755;&#20837;&#30340;&#25991;&#26412;&#38382;&#39064;&#20013;&#29983;&#25104;&#23454;&#20307;&#26631;&#31614;&#36328;&#24230;&#12290;&#22522;&#20110;&#36825;&#20123;&#26631;&#31614;&#65292;&#20174;&#25968;&#25454;&#24211;&#20013;&#33719;&#21462;&#23454;&#20307;&#20505;&#36873;&#39033;&#65292;&#24182;&#20351;&#29992;&#23454;&#20307;&#23884;&#20837;&#65288;&#22914;TransE&#12289;DistMult&#21644;ComplEx&#65289;&#23545;&#23427;&#20204;&#36827;&#34892;&#25490;&#24207;&#12290;&#32467;&#26524;&#20197;&#29992;&#25143;&#21487;&#20197;&#27604;&#36739;&#21644;&#23545;&#27604;T5-small&#12289;T5-base&#21644;&#19981;&#21516;&#30340;&#30693;&#35782;&#22270;&#23884;&#20837;&#20043;&#38388;&#30340;&#32467;&#26524;&#12290;&#28436;&#31034;&#21487;&#20197;&#22312;https://ltdemos.informatik.uni-hamburg.de/dblplink/&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present a web application named DBLPLink, which performs entity linking over the DBLP scholarly knowledge graph. DBLPLink uses text-to-text pre-trained language models, such as T5, to produce entity label spans from an input text question. Entity candidates are fetched from a database based on the labels, and an entity re-ranker sorts them based on entity embeddings, such as TransE, DistMult and ComplEx. The results are displayed so that users may compare and contrast the results between T5-small, T5-base and the different KG embeddings used. The demo can be accessed at https://ltdemos.informatik.uni-hamburg.de/dblplink/.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#21160;&#35782;&#21035;&#20154;&#31867;&#21160;&#20316;&#20849;&#29616;&#30340;&#20219;&#21153;&#65292;&#24182;&#21019;&#24314;&#20102;ACE&#25968;&#25454;&#38598;&#20197;&#21450;&#30456;&#24212;&#30340;&#20195;&#30721;&#12290;&#36890;&#36807;&#21033;&#29992;&#35270;&#35273;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#22270;&#38142;&#25509;&#39044;&#27979;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#25429;&#25417;&#19981;&#21516;&#25968;&#25454;&#22495;&#20013;&#30340;&#20154;&#31867;&#21160;&#20316;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2309.06219</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#38142;&#25509;&#39044;&#27979;&#22312;&#29983;&#27963;&#26041;&#24335;vlog&#20013;&#30340;&#20154;&#31867;&#21160;&#20316;&#20849;&#29616;
&lt;/p&gt;
&lt;p&gt;
Human Action Co-occurrence in Lifestyle Vlogs using Graph Link Prediction. (arXiv:2309.06219v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06219
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#21160;&#35782;&#21035;&#20154;&#31867;&#21160;&#20316;&#20849;&#29616;&#30340;&#20219;&#21153;&#65292;&#24182;&#21019;&#24314;&#20102;ACE&#25968;&#25454;&#38598;&#20197;&#21450;&#30456;&#24212;&#30340;&#20195;&#30721;&#12290;&#36890;&#36807;&#21033;&#29992;&#35270;&#35273;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#22270;&#38142;&#25509;&#39044;&#27979;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#25429;&#25417;&#19981;&#21516;&#25968;&#25454;&#22495;&#20013;&#30340;&#20154;&#31867;&#21160;&#20316;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#33258;&#21160;&#35782;&#21035;&#20154;&#31867;&#21160;&#20316;&#20849;&#29616;&#30340;&#20219;&#21153;&#65292;&#21363;&#30830;&#23450;&#20004;&#20010;&#20154;&#31867;&#21160;&#20316;&#26159;&#21542;&#21487;&#20197;&#22312;&#21516;&#19968;&#26102;&#38388;&#38388;&#38548;&#20869;&#20849;&#29616;&#12290;&#25105;&#20204;&#21019;&#24314;&#24182;&#20844;&#24320;&#20102;ACE&#65288;Action Co-occurrencE&#65289;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#30001;&#32422;12k&#20010;&#20849;&#29616;&#30340;&#35270;&#35273;&#21160;&#20316;&#23545;&#21644;&#23427;&#20204;&#23545;&#24212;&#30340;&#35270;&#39057;&#29255;&#27573;&#32452;&#25104;&#30340;&#22823;&#22411;&#22270;&#24418;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#21033;&#29992;&#35270;&#35273;&#21644;&#25991;&#26412;&#20449;&#24687;&#26469;&#33258;&#21160;&#25512;&#26029;&#20004;&#20010;&#21160;&#20316;&#26159;&#21542;&#20849;&#29616;&#30340;&#22270;&#38142;&#25509;&#39044;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22270;&#24418;&#29305;&#21035;&#36866;&#21512;&#25429;&#25417;&#20154;&#31867;&#21160;&#20316;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#19988;&#25152;&#23398;&#20064;&#30340;&#22270;&#24418;&#34920;&#31034;&#23545;&#20110;&#25105;&#20204;&#30340;&#20219;&#21153;&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#22495;&#20013;&#25429;&#25417;&#21040;&#26032;&#39062;&#32780;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#20171;&#32461;&#30340;ACE&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#21487;&#22312;https://github.com/MichiganNLP/vlog_action_co-occurrence&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the task of automatic human action co-occurrence identification, i.e., determine whether two human actions can co-occur in the same interval of time. We create and make publicly available the ACE (Action Co-occurrencE) dataset, consisting of a large graph of ~12k co-occurring pairs of visual actions and their corresponding video clips. We describe graph link prediction models that leverage visual and textual information to automatically infer if two actions are co-occurring. We show that graphs are particularly well suited to capture relations between human actions, and the learned graph representations are effective for our task and capture novel and relevant information across different data domains. The ACE dataset and the code introduced in this paper are publicly available at https://github.com/MichiganNLP/vlog_action_co-occurrence.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411; (VLMs) &#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#26469;&#36991;&#20813;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#65292;&#37319;&#29992;&#32842;&#22825;&#24335;&#30340;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#40657;&#30418;&#20248;&#21270;&#22120;&#65292;&#22312;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#36798;&#21040;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.05950</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#40657;&#30418;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Language Models as Black-Box Optimizers for Vision-Language Models. (arXiv:2309.05950v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411; (VLMs) &#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#26469;&#36991;&#20813;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#65292;&#37319;&#29992;&#32842;&#22825;&#24335;&#30340;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#40657;&#30418;&#20248;&#21270;&#22120;&#65292;&#22312;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#36798;&#21040;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#25968;&#25454;&#38598;&#19978;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411; (VLMs) &#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#35270;&#35273;&#21644;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#30446;&#21069;&#65292;VLMs &#30340;&#24494;&#35843;&#26041;&#27861;&#20027;&#35201;&#22312;&#30333;&#30418;&#29615;&#22659;&#20013;&#25805;&#20316;&#65292;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810; VLMs &#20381;&#36182;&#20110;&#19987;&#26377;&#25968;&#25454;&#19988;&#19981;&#24320;&#28304;&#65292;&#38480;&#21046;&#20102;&#20351;&#29992;&#30333;&#30418;&#26041;&#27861;&#36827;&#34892;&#24494;&#35843;&#12290;&#37492;&#20110;&#20687; ChatGPT &#36825;&#26679;&#30340;&#21463;&#27426;&#36814;&#31169;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#20173;&#28982;&#25552;&#20379;&#22522;&#20110;&#35821;&#35328;&#30340;&#29992;&#25143;&#30028;&#38754;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#24320;&#21457;&#19968;&#31181;&#26032;&#30340; VLMs &#24494;&#35843;&#26041;&#27861;&#65292;&#20174;&#32780;&#36991;&#20813;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#12289;&#29305;&#24449;&#23884;&#20837;&#25110;&#36755;&#20986; logits &#30340;&#38656;&#35201;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22522;&#20110;&#32842;&#22825;&#30340; LLMs &#20316;&#20026;&#40657;&#30418;&#20248;&#21270;&#22120;&#65292;&#20197;&#22312;&#20351;&#29992; CLIP &#36827;&#34892;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#30340;&#31034;&#20363;&#20219;&#21153;&#20013;&#23547;&#25214;&#26368;&#20339;&#25991;&#26412;&#25552;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#33258;&#21160;"&#29228;&#23665;"&#31243;&#24207;&#65292;&#23427;&#33021;&#25910;&#25947;&#21040;&#26377;&#25928;&#30340;&#25552;&#31034;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-language models (VLMs) pre-trained on web-scale datasets have demonstrated remarkable capabilities across a variety of vision and multimodal tasks. Currently, fine-tuning methods for VLMs mainly operate in a white-box setting, requiring access to model parameters for backpropagation. However, many VLMs rely on proprietary data and are not open-source, which restricts the use of white-box approaches for fine-tuning. Given that popular private large language models (LLMs) like ChatGPT still offer a language-based user interface, we aim to develop a novel fine-tuning approach for VLMs through natural language prompts, thereby avoiding the need to access model parameters, feature embeddings, or output logits. In this setup, we propose employing chat-based LLMs as black-box optimizers to search for the best text prompt on the illustrative task of few-shot image classification using CLIP. Specifically, we adopt an automatic "hill-climbing" procedure that converges on an effective prom
&lt;/p&gt;</description></item><item><title>HAE-RAE Bench&#35780;&#20272;&#20102;&#35821;&#35328;&#27169;&#22411;&#23545;&#38889;&#22269;&#30693;&#35782;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#20351;&#29992;&#27604;GPT-3.5&#23567;&#30340;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#31867;&#20284;&#30340;&#24615;&#33021;&#27700;&#24179;&#65292;&#24378;&#35843;&#20102;&#21516;&#36136;&#35821;&#26009;&#24211;&#22312;&#35757;&#32451;&#19987;&#19994;&#32423;&#35821;&#35328;&#29305;&#23450;&#27169;&#22411;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.02706</link><description>&lt;p&gt;
HAE-RAE Bench: &#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#23545;&#38889;&#22269;&#30693;&#35782;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
HAE-RAE Bench: Evaluation of Korean Knowledge in Language Models. (arXiv:2309.02706v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02706
&lt;/p&gt;
&lt;p&gt;
HAE-RAE Bench&#35780;&#20272;&#20102;&#35821;&#35328;&#27169;&#22411;&#23545;&#38889;&#22269;&#30693;&#35782;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#20351;&#29992;&#27604;GPT-3.5&#23567;&#30340;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#31867;&#20284;&#30340;&#24615;&#33021;&#27700;&#24179;&#65292;&#24378;&#35843;&#20102;&#21516;&#36136;&#35821;&#26009;&#24211;&#22312;&#35757;&#32451;&#19987;&#19994;&#32423;&#35821;&#35328;&#29305;&#23450;&#27169;&#22411;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#65292;&#20294;&#26159;&#23545;&#38750;&#33521;&#35821;&#35821;&#35328;&#30340;&#20851;&#27880;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#20013;&#26377;&#38480;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#24182;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#38889;&#35821;&#35821;&#35328;&#21644;&#25991;&#21270;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HAE-RAE Bench&#65292;&#22312;&#35789;&#27719;&#12289;&#21382;&#21490;&#21644;&#19968;&#33324;&#30693;&#35782;&#31561;6&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#23545;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20010;&#22522;&#20934;&#19978;&#30340;&#35780;&#20272;&#31361;&#20986;&#20102;&#20351;&#29992;&#22823;&#22411;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;(LLSMs)&#19982;&#20687;GPT-3.5&#36825;&#26679;&#30340;&#20840;&#38754;&#36890;&#29992;&#27169;&#22411;&#30456;&#27604;&#30340;&#28508;&#22312;&#20248;&#21183;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#27604;GPT-3.5&#32422;&#23567;13&#20493;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#35821;&#35328;&#29305;&#23450;&#30693;&#35782;&#26816;&#32034;&#26041;&#38754;&#23637;&#29616;&#20986;&#31867;&#20284;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#36825;&#19968;&#35266;&#23519;&#24378;&#35843;&#20102;&#22312;&#35757;&#32451;&#19987;&#19994;&#32423;&#35821;&#35328;&#29305;&#23450;&#27169;&#22411;&#26102;&#21516;&#36136;&#35821;&#26009;&#24211;&#30340;&#37325;&#35201;&#24615;&#12290;&#30456;&#21453;&#65292;&#24403;&#36825;&#20123;&#36739;&#23567;&#30340;&#27169;&#22411;&#22312;......
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) pretrained on massive corpora exhibit remarkable capabilities across a wide range of tasks, however, the attention given to non-English languages has been limited in this field of research. To address this gap and assess the proficiency of language models in the Korean language and culture, we present HAE-RAE Bench, covering 6 tasks including vocabulary, history, and general knowledge. Our evaluation of language models on this benchmark highlights the potential advantages of employing Large Language-Specific Models(LLSMs) over a comprehensive, universal model like GPT-3.5. Remarkably, our study reveals that models approximately 13 times smaller than GPT-3.5 can exhibit similar performance levels in terms of language-specific knowledge retrieval. This observation underscores the importance of homogeneous corpora for training professional-level language-specific models. On the contrary, we also observe a perplexing performance dip in these smaller LMs when th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#30340;&#26816;&#27979;&#12289;&#35299;&#37322;&#21644;&#32531;&#35299;&#30340;&#26368;&#26032;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#24187;&#35273;&#29616;&#35937;&#21644;&#35780;&#20272;&#22522;&#20934;&#30340;&#20998;&#31867;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2309.01219</link><description>&lt;p&gt;
AI&#28023;&#27915;&#20013;&#30340;&#22934;&#24618;&#20043;&#27468;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models. (arXiv:2309.01219v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#30340;&#26816;&#27979;&#12289;&#35299;&#37322;&#21644;&#32531;&#35299;&#30340;&#26368;&#26032;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#24187;&#35273;&#29616;&#35937;&#21644;&#35780;&#20272;&#22522;&#20934;&#30340;&#20998;&#31867;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#20294;&#20154;&#20204;&#23545;&#20854;&#20135;&#29983;&#24187;&#35273;&#30340;&#20542;&#21521;&#34920;&#31034;&#25285;&#24551;&#65306;LLMs&#26377;&#26102;&#20250;&#29983;&#25104;&#19982;&#29992;&#25143;&#36755;&#20837;&#19981;&#31526;&#12289;&#19982;&#20808;&#21069;&#29983;&#25104;&#30340;&#20869;&#23481;&#30456;&#30683;&#30462;&#25110;&#19982;&#24050;&#24314;&#31435;&#30340;&#19990;&#30028;&#30693;&#35782;&#19981;&#31526;&#30340;&#20869;&#23481;&#12290;&#36825;&#31181;&#29616;&#35937;&#23545;LLMs&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#21487;&#38752;&#24615;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#23545;&#20851;&#20110;&#24187;&#35273;&#26816;&#27979;&#12289;&#35299;&#37322;&#21644;&#32531;&#35299;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#37325;&#28857;&#25506;&#35752;&#20102;LLMs&#25152;&#38754;&#20020;&#30340;&#29420;&#29305;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LLM&#24187;&#35273;&#29616;&#35937;&#21644;&#35780;&#20272;&#22522;&#20934;&#30340;&#20998;&#31867;&#65292;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#26088;&#22312;&#32531;&#35299;LLM&#24187;&#35273;&#30340;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs) have demonstrated remarkable capabilities across a range of downstream tasks, a significant concern revolves around their propensity to exhibit hallucinations: LLMs occasionally generate content that diverges from the user input, contradicts previously generated context, or misaligns with established world knowledge. This phenomenon poses a substantial challenge to the reliability of LLMs in real-world scenarios. In this paper, we survey recent efforts on the detection, explanation, and mitigation of hallucination, with an emphasis on the unique challenges posed by LLMs. We present taxonomies of the LLM hallucination phenomena and evaluation benchmarks, analyze existing approaches aiming at mitigating LLM hallucination, and discuss potential directions for future research.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CPSP&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#32454;&#31890;&#24230;&#30340;&#20013;&#38388;&#34920;&#31034;&#65292;&#20351;&#24471;&#25552;&#21462;&#30340;&#20449;&#24687;&#26082;&#21253;&#21547;&#35821;&#35328;&#20869;&#23481;&#21448;&#21435;&#38500;&#20102;&#21457;&#35328;&#20154;&#36523;&#20221;&#21644;&#22768;&#23398;&#32454;&#33410;&#65292;&#36866;&#29992;&#20110;TTS&#12289;VC&#21644;ASR&#31561;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2309.00424</link><description>&lt;p&gt;
CPSP: &#20174;&#38899;&#32032;&#30417;&#30563;&#20013;&#23398;&#20064;&#35821;&#38899;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
CPSP: Learning Speech Concepts From Phoneme Supervision. (arXiv:2309.00424v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00424
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CPSP&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#32454;&#31890;&#24230;&#30340;&#20013;&#38388;&#34920;&#31034;&#65292;&#20351;&#24471;&#25552;&#21462;&#30340;&#20449;&#24687;&#26082;&#21253;&#21547;&#35821;&#35328;&#20869;&#23481;&#21448;&#21435;&#38500;&#20102;&#21457;&#35328;&#20154;&#36523;&#20221;&#21644;&#22768;&#23398;&#32454;&#33410;&#65292;&#36866;&#29992;&#20110;TTS&#12289;VC&#21644;ASR&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35832;&#22914;&#26368;&#23567;&#30417;&#30563;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#12289;&#35821;&#38899;&#36716;&#25442;&#65288;VC&#65289;&#21644;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31561;&#32454;&#31890;&#24230;&#29983;&#25104;&#21644;&#35782;&#21035;&#20219;&#21153;&#65292;&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#30340;&#20013;&#38388;&#34920;&#31034;&#24212;&#21253;&#21547;&#20171;&#20110;&#25991;&#26412;&#32534;&#30721;&#21644;&#22768;&#23398;&#32534;&#30721;&#20043;&#38388;&#30340;&#20449;&#24687;&#12290;&#35821;&#35328;&#20869;&#23481;&#31361;&#20986;&#65292;&#32780;&#21457;&#35328;&#20154;&#36523;&#20221;&#21644;&#22768;&#23398;&#32454;&#33410;&#31561;&#35821;&#38899;&#20449;&#24687;&#24212;&#35813;&#34987;&#21435;&#38500;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#32454;&#31890;&#24230;&#20013;&#38388;&#34920;&#31034;&#30340;&#26041;&#27861;&#23384;&#22312;&#20887;&#20313;&#24615;&#36807;&#39640;&#21644;&#32500;&#24230;&#29190;&#28856;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#38899;&#39057;&#39046;&#22495;&#20013;&#29616;&#26377;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#25552;&#21462;&#29992;&#20110;&#19979;&#28216;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#30340;&#20840;&#23616;&#25551;&#36848;&#20449;&#24687;&#65292;&#19981;&#36866;&#21512;TTS&#12289;VC&#21644;ASR&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#38899;&#32032;-&#35821;&#38899;&#39044;&#35757;&#32451;&#65288;CPSP&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#19977;&#20010;&#32534;&#30721;&#22120;&#12289;&#19968;&#20010;&#35299;&#30721;&#22120;&#21644;&#23545;&#27604;&#23398;&#20064;&#26469;&#23558;&#38899;&#32032;&#21644;&#35821;&#38899;&#20449;&#24687;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
For fine-grained generation and recognition tasks such as minimally-supervised text-to-speech (TTS), voice conversion (VC), and automatic speech recognition (ASR), the intermediate representation extracted from speech should contain information that is between text coding and acoustic coding. The linguistic content is salient, while the paralinguistic information such as speaker identity and acoustic details should be removed. However, existing methods for extracting fine-grained intermediate representations from speech suffer from issues of excessive redundancy and dimension explosion. Additionally, existing contrastive learning methods in the audio field focus on extracting global descriptive information for downstream audio classification tasks, making them unsuitable for TTS, VC, and ASR tasks. To address these issues, we propose a method named Contrastive Phoneme-Speech Pretraining (CPSP), which uses three encoders, one decoder, and contrastive learning to bring phoneme and speech
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26377;&#28508;&#21147;&#25104;&#20026;&#20154;&#24037;&#35780;&#20272;&#21644;&#20854;&#20182;&#33258;&#21160;&#21270;&#35780;&#20215;&#25351;&#26631;&#30340;&#21487;&#34892;&#26367;&#20195;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2308.13577</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Text Style Transfer Evaluation Using Large Language Models. (arXiv:2308.13577v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13577
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26377;&#28508;&#21147;&#25104;&#20026;&#20154;&#24037;&#35780;&#20272;&#21644;&#20854;&#20182;&#33258;&#21160;&#21270;&#35780;&#20215;&#25351;&#26631;&#30340;&#21487;&#34892;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;&#65288;TST&#65289;&#30340;&#35780;&#20272;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#34920;&#29616;&#22312;&#22810;&#20010;&#26041;&#38754;&#65292;&#27599;&#20010;&#26041;&#38754;&#37117;&#24456;&#38590;&#21333;&#29420;&#34913;&#37327;&#65306;&#39118;&#26684;&#36716;&#25442;&#20934;&#30830;&#24615;&#12289;&#20869;&#23481;&#20445;&#30041;&#21644;&#25972;&#20307;&#27969;&#30021;&#24615;&#12290;&#20154;&#24037;&#35780;&#20272;&#26159;TST&#35780;&#20272;&#30340;&#40644;&#37329;&#26631;&#20934;&#65292;&#28982;&#32780;&#65292;&#23427;&#36153;&#26102;&#36153;&#21147;&#65292;&#24182;&#19988;&#32467;&#26524;&#38590;&#20197;&#37325;&#22797;&#12290;&#35768;&#22810;&#33258;&#21160;&#21270;&#25351;&#26631;&#34987;&#29992;&#20110;&#35780;&#20272;&#36825;&#20123;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#20316;&#20026;&#20154;&#24037;&#35780;&#20272;&#30340;&#26367;&#20195;&#21697;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#33258;&#21160;&#21270;&#25351;&#26631;&#19982;&#20154;&#24037;&#35780;&#20272;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20173;&#28982;&#19981;&#28165;&#26970;&#65292;&#23545;&#23427;&#20204;&#20316;&#20026;&#21487;&#38752;&#22522;&#20934;&#30340;&#25928;&#26524;&#20135;&#29983;&#20102;&#24576;&#30097;&#12290;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#24050;&#32463;&#35777;&#26126;&#20102;&#23427;&#20204;&#19981;&#20165;&#33021;&#22815;&#21305;&#37197;&#65292;&#32780;&#19988;&#22312;&#21508;&#31181;&#26410;&#35265;&#20219;&#21153;&#20013;&#36824;&#33021;&#36229;&#36807;&#24179;&#22343;&#20154;&#31867;&#34920;&#29616;&#12290;&#36825;&#34920;&#26126;LLMs&#26377;&#28508;&#21147;&#25104;&#20026;&#20154;&#24037;&#35780;&#20272;&#21644;&#20854;&#20182;&#33258;&#21160;&#21270;&#25351;&#26631;&#30340;&#21487;&#34892;&#26367;&#20195;&#26041;&#26696;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Text Style Transfer (TST) is challenging to evaluate because the quality of the generated text manifests itself in multiple aspects, each of which is hard to measure individually: style transfer accuracy, content preservation, and overall fluency of the text. Human evaluation is the gold standard in TST evaluation; however, it is expensive, and the results are difficult to reproduce. Numerous automated metrics are employed to assess performance in these aspects, serving as substitutes for human evaluation. However, the correlation between many of these automated metrics and human evaluations remains unclear, raising doubts about their effectiveness as reliable benchmarks. Recent advancements in Large Language Models (LLMs) have demonstrated their ability to not only match but also surpass the average human performance across a wide range of unseen tasks. This suggests that LLMs have the potential to serve as a viable alternative to human evaluation and other automated metrics. We asses
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23458;&#35266;&#35780;&#20215;&#31038;&#20132;&#20301;&#32622;&#23545;&#35805;&#26426;&#22120;&#20154;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#27169;&#24577;&#29992;&#25143;&#34892;&#20026;&#26469;&#35780;&#20272;&#26426;&#22120;&#20154;&#30340;&#20154;&#31867;&#30456;&#20284;&#24230;&#65292;&#22686;&#24378;&#20102;&#23458;&#35266;&#24615;&#21644;&#21487;&#22797;&#29616;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.11020</link><description>&lt;p&gt;
&#35780;&#20272;&#31038;&#20132;&#20301;&#32622;&#23545;&#35805;&#26426;&#22120;&#20154;&#30340;&#23458;&#35266;&#35780;&#20215;&#65306;&#36890;&#36807;&#22810;&#27169;&#24577;&#29992;&#25143;&#34892;&#20026;&#35780;&#20272;&#20154;&#31867;&#30456;&#20284;&#24230;
&lt;/p&gt;
&lt;p&gt;
Towards Objective Evaluation of Socially-Situated Conversational Robots: Assessing Human-Likeness through Multimodal User Behaviors. (arXiv:2308.11020v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11020
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23458;&#35266;&#35780;&#20215;&#31038;&#20132;&#20301;&#32622;&#23545;&#35805;&#26426;&#22120;&#20154;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#27169;&#24577;&#29992;&#25143;&#34892;&#20026;&#26469;&#35780;&#20272;&#26426;&#22120;&#20154;&#30340;&#20154;&#31867;&#30456;&#20284;&#24230;&#65292;&#22686;&#24378;&#20102;&#23458;&#35266;&#24615;&#21644;&#21487;&#22797;&#29616;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#35780;&#20272;&#31038;&#20132;&#20301;&#32622;&#23545;&#35805;&#26426;&#22120;&#20154;&#30340;&#25361;&#25112;&#24615;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23458;&#35266;&#35780;&#20215;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;&#22810;&#27169;&#24577;&#29992;&#25143;&#34892;&#20026;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#20851;&#27880;&#28857;&#26159;&#35780;&#20272;&#26426;&#22120;&#20154;&#30340;&#20154;&#31867;&#30456;&#20284;&#24230;&#20316;&#20026;&#20027;&#35201;&#35780;&#20215;&#25351;&#26631;&#12290;&#32780;&#20197;&#24448;&#30340;&#30740;&#31350;&#24120;&#24120;&#20381;&#36182;&#20110;&#29992;&#25143;&#30340;&#20027;&#35266;&#35780;&#20215;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#38388;&#25509;&#35266;&#23519;&#29992;&#25143;&#34892;&#20026;&#26469;&#35780;&#20272;&#26426;&#22120;&#20154;&#30340;&#20154;&#31867;&#30456;&#20284;&#24230;&#65292;&#20174;&#32780;&#22686;&#24378;&#23458;&#35266;&#24615;&#21644;&#21487;&#22797;&#29616;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#20351;&#29992;&#20851;&#27880;&#24615;&#23545;&#35805;&#35821;&#26009;&#24211;&#20013;&#30340;&#29992;&#25143;&#34892;&#20026;&#26469;&#26631;&#27880;&#20154;&#31867;&#30456;&#20284;&#24230;&#20998;&#25968;&#30340;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#25105;&#20204;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#30830;&#23450;&#20102;&#22810;&#27169;&#24577;&#29992;&#25143;&#34892;&#20026;&#19982;&#20154;&#31867;&#30456;&#20284;&#24230;&#20998;&#25968;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20110;&#34892;&#20026;&#30340;&#35780;&#20272;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper tackles the challenging task of evaluating socially situated conversational robots and presents a novel objective evaluation approach that relies on multimodal user behaviors. In this study, our main focus is on assessing the human-likeness of the robot as the primary evaluation metric. While previous research often relied on subjective evaluations from users, our approach aims to evaluate the robot's human-likeness based on observable user behaviors indirectly, thus enhancing objectivity and reproducibility. To begin, we created an annotated dataset of human-likeness scores, utilizing user behaviors found in an attentive listening dialogue corpus. We then conducted an analysis to determine the correlation between multimodal user behaviors and human-likeness scores, demonstrating the feasibility of our proposed behavior-based evaluation method.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#26412;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#21033;&#29992;&#30142;&#30149;&#26412;&#20307;&#21644;&#30151;&#29366;&#26412;&#20307;&#26500;&#24314;&#25968;&#23398;&#27169;&#22411;&#65292;&#21033;&#29992;&#20107;&#23454;&#26680;&#26597;&#31639;&#27861;&#21644;&#32593;&#32476;&#20013;&#24515;&#24230;&#25351;&#26631;&#20998;&#26512;ChatGPT&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#30495;&#23454;&#21307;&#23398;&#25991;&#29486;&#20043;&#38388;&#30340;&#20934;&#30830;&#24615;&#65292;&#20197;&#39564;&#35777;&#30142;&#30149;-&#30151;&#29366;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2308.03929</link><description>&lt;p&gt;
ChatGPT&#29983;&#29289;&#21307;&#23398;&#29983;&#25104;&#25991;&#26412;&#20013;&#24314;&#31435;&#20449;&#20219;&#30340;&#26041;&#27861;&#65306;&#22522;&#20110;&#26412;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#29992;&#20110;&#39564;&#35777;&#30142;&#30149;-&#30151;&#29366;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Establishing Trust in ChatGPT BioMedical Generated Text: An Ontology-Based Knowledge Graph to Validate Disease-Symptom Links. (arXiv:2308.03929v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#26412;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#21033;&#29992;&#30142;&#30149;&#26412;&#20307;&#21644;&#30151;&#29366;&#26412;&#20307;&#26500;&#24314;&#25968;&#23398;&#27169;&#22411;&#65292;&#21033;&#29992;&#20107;&#23454;&#26680;&#26597;&#31639;&#27861;&#21644;&#32593;&#32476;&#20013;&#24515;&#24230;&#25351;&#26631;&#20998;&#26512;ChatGPT&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#30495;&#23454;&#21307;&#23398;&#25991;&#29486;&#20043;&#38388;&#30340;&#20934;&#30830;&#24615;&#65292;&#20197;&#39564;&#35777;&#30142;&#30149;-&#30151;&#29366;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26041;&#27861;&#65306;&#36890;&#36807;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#20174;&#30495;&#23454;&#30340;&#21307;&#23398;&#25991;&#29486;&#21644;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#20869;&#23481;&#26500;&#24314;&#20102;&#22522;&#20110;&#26412;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21306;&#20998;&#20107;&#23454;&#20449;&#24687;&#21644;&#26410;&#32463;&#39564;&#35777;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65306;&#19968;&#20010;&#26159;&#20351;&#29992;&#8220;&#20154;&#31867;&#30142;&#30149;&#21644;&#30151;&#29366;&#8221;&#26597;&#35810;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#32534;&#35793;&#30340;&#65292;&#21478;&#19968;&#20010;&#26159;&#30001;ChatGPT&#29983;&#25104;&#30340;&#27169;&#25311;&#25991;&#31456;&#12290;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#38598;&#65288;PubMed&#21644;ChatGPT&#65289;&#65292;&#25105;&#20204;&#38543;&#26426;&#36873;&#25321;&#20102;10&#32452;&#27599;&#32452;250&#20010;&#25688;&#35201;&#65292;&#24182;&#20351;&#29992;&#29305;&#23450;&#30340;&#31181;&#23376;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20027;&#35201;&#26159;&#21033;&#29992;&#30142;&#30149;&#26412;&#20307;&#65288;DOID&#65289;&#21644;&#30151;&#29366;&#26412;&#20307;&#65288;SYMP&#65289;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#65292;&#36825;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#25968;&#23398;&#27169;&#22411;&#65292;&#21487;&#20197;&#36827;&#34892;&#26080;&#20559;&#24046;&#30340;&#27604;&#36739;&#12290;&#36890;&#36807;&#20351;&#29992;&#25105;&#20204;&#30340;&#20107;&#23454;&#26680;&#26597;&#31639;&#27861;&#21644;&#32593;&#32476;&#20013;&#24515;&#24230;&#25351;&#26631;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;GPT&#30142;&#30149;-&#30151;&#29366;&#38142;&#25509;&#20998;&#26512;&#65292;&#20197;&#37327;&#21270;&#22312;&#22122;&#22768;&#12289;&#20551;&#35774;&#21644;&#37325;&#35201;&#21457;&#29616;&#20013;&#30340;&#20107;&#23454;&#30693;&#35782;&#30340;&#20934;&#30830;&#24615;&#12290;&#32467;&#26524;&#65306;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;ChatGPT&#30693;&#35782;&#22270;&#35889;&#21450;&#20854;PubMed&#35745;&#25968;&#33719;&#24471;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#21457;&#29616;...
&lt;/p&gt;
&lt;p&gt;
Methods: Through an innovative approach, we construct ontology-based knowledge graphs from authentic medical literature and AI-generated content. Our goal is to distinguish factual information from unverified data. We compiled two datasets: one from biomedical literature using a "human disease and symptoms" query, and another generated by ChatGPT, simulating articles. With these datasets (PubMed and ChatGPT), we curated 10 sets of 250 abstracts each, selected randomly with a specific seed. Our method focuses on utilizing disease ontology (DOID) and symptom ontology (SYMP) to build knowledge graphs, robust mathematical models that facilitate unbiased comparisons. By employing our fact-checking algorithms and network centrality metrics, we conducted GPT disease-symptoms link analysis to quantify the accuracy of factual knowledge amid noise, hypotheses, and significant findings.  Results: The findings obtained from the comparison of diverse ChatGPT knowledge graphs with their PubMed count
&lt;/p&gt;</description></item><item><title>MASR&#26159;&#19968;&#31181;&#22810;&#26631;&#31614;&#24863;&#30693;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#21033;&#29992;&#22810;&#20010;&#22806;&#37096;&#30693;&#35782;&#28304;&#22686;&#24378;&#20803;&#25968;&#25454;&#20449;&#24687;&#30340;&#21033;&#29992;&#65292;&#24182;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2307.10982</link><description>&lt;p&gt;
MASR: &#22810;&#26631;&#31614;&#24863;&#30693;&#35821;&#38899;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
MASR: Multi-label Aware Speech Representation. (arXiv:2307.10982v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10982
&lt;/p&gt;
&lt;p&gt;
MASR&#26159;&#19968;&#31181;&#22810;&#26631;&#31614;&#24863;&#30693;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#21033;&#29992;&#22810;&#20010;&#22806;&#37096;&#30693;&#35782;&#28304;&#22686;&#24378;&#20803;&#25968;&#25454;&#20449;&#24687;&#30340;&#21033;&#29992;&#65292;&#24182;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#20027;&#35201;&#20197;&#33258;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#20026;&#22522;&#30784;&#65292;&#20165;&#20351;&#29992;&#21407;&#22987;&#38899;&#39057;&#20449;&#21495;&#65292;&#24573;&#30053;&#20102;&#36890;&#24120;&#21487;&#29992;&#20110;&#32473;&#23450;&#35821;&#38899;&#35760;&#24405;&#30340;&#38468;&#21152;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MASR&#65292;&#19968;&#31181;&#22810;&#26631;&#31614;&#24863;&#30693;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#19978;&#36848;&#38480;&#21046;&#12290;MASR&#33021;&#22815;&#24341;&#20837;&#22810;&#20010;&#22806;&#37096;&#30693;&#35782;&#28304;&#65292;&#22686;&#24378;&#20803;&#25968;&#25454;&#20449;&#24687;&#30340;&#21033;&#29992;&#12290;&#22806;&#37096;&#30693;&#35782;&#28304;&#20197;&#26679;&#26412;&#32423;&#25104;&#23545;&#30456;&#20284;&#24615;&#30697;&#38453;&#30340;&#24418;&#24335;&#34987;&#32435;&#20837;&#21040;&#19968;&#20010;&#30828;&#25366;&#25496;&#25439;&#22833;&#20989;&#25968;&#20013;&#12290;MASR&#26694;&#26550;&#30340;&#19968;&#20010;&#20851;&#38190;&#20248;&#21183;&#26159;&#23427;&#21487;&#20197;&#19982;&#20219;&#20309;&#36873;&#25321;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#21033;&#29992;MASR&#34920;&#31034;&#65292;&#25105;&#20204;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#22914;&#35821;&#35328;&#35782;&#21035;&#12289;&#35821;&#38899;&#35782;&#21035;&#20197;&#21450;&#35828;&#35805;&#20154;&#21644;&#24773;&#24863;&#35782;&#21035;&#31561;&#38750;&#35821;&#20041;&#20219;&#21153;&#12290;&#22312;&#36825;&#20123;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the recent years, speech representation learning is constructed primarily as a self-supervised learning (SSL) task, using the raw audio signal alone, while ignoring the side-information that is often available for a given speech recording. In this paper, we propose MASR, a Multi-label Aware Speech Representation learning framework, which addresses the aforementioned limitations. MASR enables the inclusion of multiple external knowledge sources to enhance the utilization of meta-data information. The external knowledge sources are incorporated in the form of sample-level pair-wise similarity matrices that are useful in a hard-mining loss. A key advantage of the MASR framework is that it can be combined with any choice of SSL method. Using MASR representations, we perform evaluations on several downstream tasks such as language identification, speech recognition and other non-semantic tasks such as speaker and emotion recognition. In these experiments, we illustrate significant perfor
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#31995;&#32479;&#20998;&#31867;&#21644;&#20998;&#26512;&#20102;ACL Anthology&#20013;&#30340;&#30740;&#31350;&#35770;&#25991;&#65292;&#25552;&#20379;&#20102;&#23545;&#30740;&#31350;&#39046;&#22495;&#30340;&#32467;&#26500;&#21270;&#27010;&#36848;&#21644;NLP&#39046;&#22495;&#30340;&#20998;&#31867;&#23398;&#12290;&#26412;&#30740;&#31350;&#24635;&#32467;&#20102;&#26368;&#26032;&#30340;NLP&#21457;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.10652</link><description>&lt;p&gt;
&#25506;&#35752;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#39046;&#22495;&#30340;&#21457;&#23637;&#36235;&#21183;
&lt;/p&gt;
&lt;p&gt;
Exploring the Landscape of Natural Language Processing Research. (arXiv:2307.10652v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10652
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#31995;&#32479;&#20998;&#31867;&#21644;&#20998;&#26512;&#20102;ACL Anthology&#20013;&#30340;&#30740;&#31350;&#35770;&#25991;&#65292;&#25552;&#20379;&#20102;&#23545;&#30740;&#31350;&#39046;&#22495;&#30340;&#32467;&#26500;&#21270;&#27010;&#36848;&#21644;NLP&#39046;&#22495;&#30340;&#20998;&#31867;&#23398;&#12290;&#26412;&#30740;&#31350;&#24635;&#32467;&#20102;&#26368;&#26032;&#30340;NLP&#21457;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20316;&#20026;&#29702;&#35299;&#12289;&#29983;&#25104;&#21644;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#30340;&#19968;&#31181;&#39640;&#25928;&#26041;&#27861;&#65292;&#22312;&#36817;&#24180;&#26469;&#24471;&#21040;&#20102;&#24555;&#36895;&#20256;&#25773;&#21644;&#24191;&#27867;&#24212;&#29992;&#12290;&#37492;&#20110;&#35813;&#39046;&#22495;&#30740;&#31350;&#24037;&#20316;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#30740;&#31350;&#30028;&#24050;&#23545;&#25968;&#20010;&#19982;NLP&#30456;&#20851;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#20173;&#32570;&#23569;&#19968;&#39033;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#23545;&#24050;&#24314;&#31435;&#30340;&#20027;&#39064;&#36827;&#34892;&#20998;&#31867;&#12289;&#35782;&#21035;&#36235;&#21183;&#24182;&#27010;&#25324;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#23545;ACL Anthology&#20013;&#21253;&#21547;&#30340;&#30740;&#31350;&#35770;&#25991;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#31867;&#21644;&#20998;&#26512;&#12290;&#32467;&#26524;&#21576;&#29616;&#20102;&#30740;&#31350;&#39046;&#22495;&#30340;&#32467;&#26500;&#21270;&#27010;&#36848;&#65292;&#20026;NLP&#39046;&#22495;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#20998;&#31867;&#23398;&#65292;&#20998;&#26512;&#20102;NLP&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#24635;&#32467;&#20102;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#24182;&#31361;&#20986;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
As an efficient approach to understand, generate, and process natural language texts, research in natural language processing (NLP) has exhibited a rapid spread and wide adoption in recent years. Given the increasing amount of research work in this area, several NLP-related approaches have been surveyed in the research community. However, a comprehensive study that categorizes established topics, identifies trends, and outlines areas for future research remains absent to this day. Contributing to closing this gap, we have systematically classified and analyzed research papers included in the ACL Anthology. As a result, we present a structured overview of the research landscape, provide a taxonomy of fields-of-study in NLP, analyze recent developments in NLP, summarize our findings, and highlight directions for future work.
&lt;/p&gt;</description></item><item><title>AspectCSE&#26159;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#32467;&#26500;&#21270;&#30693;&#35782;&#36827;&#34892;&#22522;&#20110;&#26041;&#38754;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#30340;&#21477;&#23376;&#23884;&#20837;&#26041;&#27861;&#65292;&#23427;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#30456;&#27604;&#20043;&#21069;&#30340;&#26368;&#22909;&#32467;&#26524;&#24179;&#22343;&#25552;&#39640;&#20102;3.97%&#65292;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#22810;&#20010;&#29305;&#23450;&#26041;&#38754;&#30340;&#23884;&#20837;&#27169;&#22411;&#20248;&#20110;&#21333;&#26041;&#38754;&#23884;&#20837;&#12290;</title><link>http://arxiv.org/abs/2307.07851</link><description>&lt;p&gt;
AspectCSE: &#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#32467;&#26500;&#21270;&#30693;&#35782;&#36827;&#34892;&#22522;&#20110;&#26041;&#38754;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#30340;&#21477;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
AspectCSE: Sentence Embeddings for Aspect-based Semantic Textual Similarity using Contrastive Learning and Structured Knowledge. (arXiv:2307.07851v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07851
&lt;/p&gt;
&lt;p&gt;
AspectCSE&#26159;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#32467;&#26500;&#21270;&#30693;&#35782;&#36827;&#34892;&#22522;&#20110;&#26041;&#38754;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#30340;&#21477;&#23376;&#23884;&#20837;&#26041;&#27861;&#65292;&#23427;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#30456;&#27604;&#20043;&#21069;&#30340;&#26368;&#22909;&#32467;&#26524;&#24179;&#22343;&#25552;&#39640;&#20102;3.97%&#65292;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#22810;&#20010;&#29305;&#23450;&#26041;&#38754;&#30340;&#23884;&#20837;&#27169;&#22411;&#20248;&#20110;&#21333;&#26041;&#38754;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#30340;&#21477;&#23376;&#23884;&#20837;&#25552;&#20379;&#20102;&#23545;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#30340;&#31895;&#30053;&#36817;&#20284;&#65292;&#20294;&#24573;&#30053;&#20102;&#20351;&#25991;&#26412;&#30456;&#20284;&#30340;&#29305;&#23450;&#26041;&#38754;&#12290;&#30456;&#21453;&#65292;&#22522;&#20110;&#26041;&#38754;&#30340;&#21477;&#23376;&#23884;&#20837;&#25552;&#20379;&#20102;&#22522;&#20110;&#39044;&#23450;&#20041;&#26041;&#38754;&#30340;&#25991;&#26412;&#30456;&#20284;&#24615;&#12290;&#22240;&#27492;&#65292;&#25991;&#26412;&#30340;&#30456;&#20284;&#24615;&#39044;&#27979;&#26356;&#21152;&#38024;&#23545;&#29305;&#23450;&#35201;&#27714;&#65292;&#24182;&#19988;&#26356;&#23481;&#26131;&#35299;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AspectCSE&#65292;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#26041;&#38754;&#30340;&#23545;&#27604;&#23398;&#20064;&#21477;&#23376;&#23884;&#20837;&#30340;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20043;&#21069;&#26368;&#22909;&#30340;&#32467;&#26524;&#30456;&#27604;&#65292;AspectCSE&#22312;&#22810;&#20010;&#26041;&#38754;&#30340;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#24179;&#22343;&#25913;&#21892;3.97%&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20351;&#29992;Wikidata&#30693;&#35782;&#22270;&#23646;&#24615;&#26469;&#35757;&#32451;&#22810;&#26041;&#38754;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#65292;&#20854;&#20013;&#22312;&#30456;&#20284;&#24615;&#39044;&#27979;&#36807;&#31243;&#20013;&#21516;&#26102;&#32771;&#34385;&#22810;&#20010;&#29305;&#23450;&#26041;&#38754;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22810;&#26041;&#38754;&#23884;&#20837;&#22312;&#29305;&#23450;&#26041;&#38754;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#19978;&#20248;&#20110;&#21333;&#26041;&#38754;&#23884;&#20837;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23884;&#20837;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#25552;&#20986;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#25913;&#36827;&#23884;&#20837;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generic sentence embeddings provide a coarse-grained approximation of semantic textual similarity but ignore specific aspects that make texts similar. Conversely, aspect-based sentence embeddings provide similarities between texts based on certain predefined aspects. Thus, similarity predictions of texts are more targeted to specific requirements and more easily explainable. In this paper, we present AspectCSE, an approach for aspect-based contrastive learning of sentence embeddings. Results indicate that AspectCSE achieves an average improvement of 3.97% on information retrieval tasks across multiple aspects compared to the previous best results. We also propose using Wikidata knowledge graph properties to train models of multi-aspect sentence embeddings in which multiple specific aspects are simultaneously considered during similarity predictions. We demonstrate that multi-aspect embeddings outperform single-aspect embeddings on aspect-specific information retrieval tasks. Finally, w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;GPT&#20197;&#35782;&#21035;&#21487;&#27604;&#20844;&#21496;&#12290;&#20256;&#32479;&#30340;&#21487;&#27604;&#20844;&#21496;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#23450;&#24615;&#26041;&#27861;&#26469;&#35782;&#21035;&#30456;&#20284;&#30340;&#21516;&#34892;&#20844;&#21496;&#65292;&#32780;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#25552;&#21462;&#20844;&#21496;&#25551;&#36848;/&#25688;&#35201;&#20174;&#32780;&#36827;&#34892;&#30456;&#20284;&#24615;&#20998;&#26512;&#65292;&#23454;&#29616;&#26356;&#37327;&#21270;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.07420</link><description>&lt;p&gt;
&#20351;&#29992;GPT&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20197;&#35782;&#21035;&#21487;&#27604;&#20844;&#21496;
&lt;/p&gt;
&lt;p&gt;
Named entity recognition using GPT for identifying comparable companies. (arXiv:2307.07420v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;GPT&#20197;&#35782;&#21035;&#21487;&#27604;&#20844;&#21496;&#12290;&#20256;&#32479;&#30340;&#21487;&#27604;&#20844;&#21496;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#23450;&#24615;&#26041;&#27861;&#26469;&#35782;&#21035;&#30456;&#20284;&#30340;&#21516;&#34892;&#20844;&#21496;&#65292;&#32780;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#25552;&#21462;&#20844;&#21496;&#25551;&#36848;/&#25688;&#35201;&#20174;&#32780;&#36827;&#34892;&#30456;&#20284;&#24615;&#20998;&#26512;&#65292;&#23454;&#29616;&#26356;&#37327;&#21270;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20844;&#20849;&#21644;&#31169;&#20154;&#20844;&#21496;&#65292;&#21487;&#27604;&#20844;&#21496;&#20998;&#26512;&#34987;&#24191;&#27867;&#29992;&#20316;&#20844;&#21496;&#20272;&#20540;&#30340;&#26041;&#27861;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#31169;&#21215;&#32929;&#26435;&#20844;&#21496;&#30340;&#20272;&#20540;&#65292;&#35813;&#26041;&#27861;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#21487;&#27604;&#20844;&#21496;&#26041;&#27861;&#30340;&#20960;&#31181;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#23450;&#24615;&#26041;&#27861;&#26469;&#35782;&#21035;&#30456;&#20284;&#30340;&#21516;&#34892;&#20844;&#21496;&#65292;&#36825;&#24448;&#24448;&#20351;&#29992;&#24050;&#24314;&#31435;&#30340;&#34892;&#19994;&#20998;&#31867;&#26041;&#26696;&#21644;/&#25110;&#20998;&#26512;&#24072;&#30340;&#30452;&#35273;&#21644;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#21644;&#31169;&#21215;&#32929;&#26435;&#34892;&#19994;&#24320;&#22987;&#20351;&#29992;&#26356;&#22810;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#26426;&#22120;&#23398;&#20064;&#32858;&#31867;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#12290;&#23545;&#20110;NLP&#26041;&#27861;&#65292;&#35813;&#36807;&#31243;&#21253;&#25324;&#20174;&#20844;&#21496;&#30340;&#32593;&#31449;&#25110;&#26469;&#33258;&#26576;&#20123;&#37329;&#34701;&#25968;&#25454;&#24211;&#31995;&#32479;&#30340;&#20844;&#21496;&#25551;&#36848;&#20013;&#25552;&#21462;&#20135;&#21697;&#23454;&#20307;&#65292;&#28982;&#21518;&#36827;&#34892;&#30456;&#20284;&#24615;&#20998;&#26512;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20351;&#29992;&#20844;&#24320;&#21487;&#29992;&#30340;&#20844;&#21496;&#32500;&#22522;&#30334;&#31185;&#32593;&#31449;&#30340;&#20844;&#21496;&#25551;&#36848;/&#25688;&#35201;&#65292;&#23637;&#31034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#20363;&#22914;GPT
&lt;/p&gt;
&lt;p&gt;
For both public and private firms, comparable companies analysis is widely used as a method for company valuation. In particular, the method is of great value for valuation of private equity companies. The several approaches to the comparable companies method usually rely on a qualitative approach to identifying similar peer companies, which tends to use established industry classification schemes and/or analyst intuition and knowledge. However, more quantitative methods have started being used in the literature and in the private equity industry, in particular, machine learning clustering, and natural language processing (NLP). For NLP methods, the process consists of extracting product entities from e.g., the company's website or company descriptions from some financial database system and then to perform similarity analysis. Here, using companies descriptions/summaries from publicly available companies' Wikipedia websites, we show that using large language models (LLMs), such as GPT
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35757;&#32451;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#26469;&#39640;&#25928;&#22495;&#33258;&#36866;&#24212;&#21477;&#23376;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#24494;&#35843;&#25972;&#20010;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#30340;&#36164;&#28304;&#28040;&#32791;&#12290;&#36890;&#36807;&#35757;&#32451;&#29305;&#23450;&#39046;&#22495;&#30340;&#36866;&#37197;&#22120;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#20351;&#29992;&#21516;&#19968;&#27169;&#22411;&#33719;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.03104</link><description>&lt;p&gt;
&#20351;&#29992;&#36866;&#37197;&#22120;&#39640;&#25928;&#22495;&#33258;&#36866;&#24212;&#21477;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Efficient Domain Adaptation of Sentence Embeddings using Adapters. (arXiv:2307.03104v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35757;&#32451;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#26469;&#39640;&#25928;&#22495;&#33258;&#36866;&#24212;&#21477;&#23376;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#24494;&#35843;&#25972;&#20010;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#30340;&#36164;&#28304;&#28040;&#32791;&#12290;&#36890;&#36807;&#35757;&#32451;&#29305;&#23450;&#39046;&#22495;&#30340;&#36866;&#37197;&#22120;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#20351;&#29992;&#21516;&#19968;&#27169;&#22411;&#33719;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21477;&#23376;&#23884;&#20837;&#20351;&#25105;&#20204;&#33021;&#22815;&#25429;&#25417;&#30701;&#25991;&#26412;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;&#22823;&#22810;&#25968;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#26159;&#38024;&#23545;&#19968;&#33324;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#65288;STS&#65289;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#22240;&#27492;&#65292;&#35201;&#22312;&#29305;&#23450;&#39046;&#22495;&#20013;&#20351;&#29992;&#21477;&#23376;&#23884;&#20837;&#65292;&#24517;&#39035;&#23558;&#27169;&#22411;&#36866;&#24212;&#20110;&#35813;&#39046;&#22495;&#20197;&#33719;&#24471;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#36890;&#24120;&#65292;&#36825;&#26159;&#36890;&#36807;&#23545;&#24863;&#20852;&#36259;&#30340;&#22495;&#23545;&#25972;&#20010;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#26469;&#23454;&#29616;&#30340;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20135;&#29983;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20294;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#26356;&#26032;&#20102;&#25152;&#26377;&#27169;&#22411;&#30340;&#26435;&#37325;&#65292;&#20351;&#35813;&#26041;&#27861;&#22312;&#36164;&#28304;&#19978;&#35201;&#27714;&#36739;&#39640;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35757;&#32451;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#26159;&#21333;&#29420;&#20026;&#27599;&#20010;&#30446;&#26631;&#39046;&#22495;&#24494;&#35843;&#25972;&#20010;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#12290;&#36825;&#20123;&#29305;&#23450;&#39046;&#22495;&#30340;&#36866;&#37197;&#22120;&#19981;&#38656;&#35201;&#24494;&#35843;&#25152;&#26377;&#24213;&#23618;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#21482;&#35757;&#32451;&#23569;&#37327;&#30340;&#39069;&#22806;&#21442;&#25968;&#65292;&#21516;&#26102;&#20445;&#25345;&#24213;&#23618;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#30340;&#26435;&#37325;&#19981;&#21464;&#12290;&#35757;&#32451;&#29305;&#23450;&#39046;&#22495;&#30340;&#36866;&#37197;&#22120;&#21487;&#20197;&#22987;&#32456;&#20351;&#29992;&#21516;&#19968;&#27169;&#22411;&#24182;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#33719;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentence embeddings enable us to capture the semantic similarity of short texts. Most sentence embedding models are trained for general semantic textual similarity (STS) tasks. Therefore, to use sentence embeddings in a particular domain, the model must be adapted to it in order to achieve good results. Usually, this is done by fine-tuning the entire sentence embedding model for the domain of interest. While this approach yields state-of-the-art results, all of the model's weights are updated during fine-tuning, making this method resource-intensive. Therefore, instead of fine-tuning entire sentence embedding models for each target domain individually, we propose to train lightweight adapters. These domain-specific adapters do not require fine-tuning all underlying sentence embedding model parameters. Instead, we only train a small number of additional parameters while keeping the weights of the underlying sentence embedding model fixed. Training domain-specific adapters allows always 
&lt;/p&gt;</description></item><item><title>CARE-MI&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#20013;&#22269;&#23381;&#23156;&#25252;&#29702;&#39046;&#22495;LLM&#34394;&#20551;&#20449;&#24687;&#30340;&#22522;&#20934;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#24182;&#25552;&#20379;&#20102;&#26500;&#24314;&#38271;&#31687;&#29983;&#25104;&#35780;&#20272;&#22522;&#20934;&#30340;&#21019;&#26032;&#33539;&#24335;&#12290;</title><link>http://arxiv.org/abs/2307.01458</link><description>&lt;p&gt;
CARE-MI: &#20013;&#22269;&#23381;&#23156;&#25252;&#29702;&#39046;&#22495;&#30340;&#34394;&#20551;&#20449;&#24687;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
CARE-MI: Chinese Benchmark for Misinformation Evaluation in Maternity and Infant Care. (arXiv:2307.01458v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01458
&lt;/p&gt;
&lt;p&gt;
CARE-MI&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#20013;&#22269;&#23381;&#23156;&#25252;&#29702;&#39046;&#22495;LLM&#34394;&#20551;&#20449;&#24687;&#30340;&#22522;&#20934;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#24182;&#25552;&#20379;&#20102;&#26500;&#24314;&#38271;&#31687;&#29983;&#25104;&#35780;&#20272;&#22522;&#20934;&#30340;&#21019;&#26032;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#36827;&#23637;&#23548;&#33268;&#20102;&#23558;LLM&#24212;&#29992;&#20110;&#29616;&#23454;&#22330;&#26223;&#30340;&#26032;&#36235;&#21183;&#12290;&#23613;&#31649;&#26368;&#26032;&#30340;LLM&#22312;&#19982;&#20154;&#31867;&#20114;&#21160;&#26102;&#20196;&#20154;&#24778;&#21497;&#22320;&#27969;&#21033;&#65292;&#20294;&#23427;&#20204;&#22312;&#29983;&#25104;&#38169;&#35823;&#20107;&#23454;&#38472;&#36848;&#26102;&#20250;&#24847;&#22806;&#20135;&#29983;&#34394;&#20551;&#20449;&#24687;&#38382;&#39064;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#26377;&#23475;&#21518;&#26524;&#65292;&#23588;&#20854;&#26159;&#22312;&#25935;&#24863;&#29615;&#22659;&#19979;&#65292;&#27604;&#22914;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#24456;&#23569;&#26377;&#30740;&#31350;&#20851;&#27880;&#35780;&#20272;LLM&#38271;&#31687;&#29983;&#25104;&#20013;&#30340;&#34394;&#20551;&#20449;&#24687;&#65292;&#23588;&#20854;&#26159;&#38024;&#23545;&#30693;&#35782;&#23494;&#38598;&#22411;&#20027;&#39064;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;LLM&#22312;&#19981;&#21516;&#35821;&#35328;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#34394;&#20551;&#20449;&#24687;&#35780;&#20272;&#20027;&#35201;&#22312;&#33521;&#35821;&#20013;&#36827;&#34892;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;CARE-MI&#65292;&#29992;&#20110;&#35780;&#20272;LLM&#34394;&#20551;&#20449;&#24687;&#22312;&#65306;1&#65289;&#19968;&#20010;&#25935;&#24863;&#20027;&#39064;&#65292;&#20855;&#20307;&#26159;&#23381;&#23156;&#25252;&#29702;&#39046;&#22495;&#65307;&#21644;2&#65289;&#19968;&#31181;&#38750;&#33521;&#35821;&#35821;&#35328;&#65292;&#21363;&#20013;&#25991;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#33539;&#24335;&#65292;&#29992;&#20110;&#26500;&#24314;&#38271;&#31687;&#29983;&#25104;&#35780;&#20272;&#22522;&#20934;&#65292;&#21487;&#20197;
&lt;/p&gt;
&lt;p&gt;
The recent advances in NLP, have led to a new trend of applying LLMs to real-world scenarios. While the latest LLMs are astonishingly fluent when interacting with humans, they suffer from the misinformation problem by unintentionally generating factually false statements. This can lead to harmful consequences, especially when produced within sensitive contexts, such as healthcare. Yet few previous works have focused on evaluating misinformation in the long-form generation of LLMs, especially for knowledge-intensive topics. Moreover, although LLMs have been shown to perform well in different languages, misinformation evaluation has been mostly conducted in English. To this end, we present a benchmark, CARE-MI, for evaluating LLM misinformation in: 1) a sensitive topic, specifically the maternity and infant care domain; and 2) a language other than English, namely Chinese. Most importantly, we provide an innovative paradigm for building long-form generation evaluation benchmarks that can
&lt;/p&gt;</description></item><item><title>3D-Speaker&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#22810;&#35774;&#22791;&#12289;&#22810;&#36317;&#31163;&#21644;&#22810;&#26041;&#35328;&#35821;&#38899;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#30740;&#31350;&#35821;&#38899;&#34920;&#31034;&#35299;&#32544;&#12290;&#23427;&#21253;&#21547;&#20102;10,000&#22810;&#20010;&#35828;&#35805;&#20154;&#30340;&#25968;&#25454;&#65292;&#21487;&#20197;&#29992;&#26469;&#35780;&#20272;&#22823;&#22411;&#36890;&#29992;&#35821;&#38899;&#27169;&#22411;&#21644;&#25506;&#32034;&#22495;&#22806;&#23398;&#20064;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.15354</link><description>&lt;p&gt;
3D-Speaker&#65306;&#29992;&#20110;&#35821;&#38899;&#34920;&#31034;&#35299;&#32544;&#30340;&#22823;&#35268;&#27169;&#22810;&#35774;&#22791;&#12289;&#22810;&#36317;&#31163;&#21644;&#22810;&#26041;&#35328;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
3D-Speaker: A Large-Scale Multi-Device, Multi-Distance, and Multi-Dialect Corpus for Speech Representation Disentanglement. (arXiv:2306.15354v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15354
&lt;/p&gt;
&lt;p&gt;
3D-Speaker&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#22810;&#35774;&#22791;&#12289;&#22810;&#36317;&#31163;&#21644;&#22810;&#26041;&#35328;&#35821;&#38899;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#30740;&#31350;&#35821;&#38899;&#34920;&#31034;&#35299;&#32544;&#12290;&#23427;&#21253;&#21547;&#20102;10,000&#22810;&#20010;&#35828;&#35805;&#20154;&#30340;&#25968;&#25454;&#65292;&#21487;&#20197;&#29992;&#26469;&#35780;&#20272;&#22823;&#22411;&#36890;&#29992;&#35821;&#38899;&#27169;&#22411;&#21644;&#25506;&#32034;&#22495;&#22806;&#23398;&#20064;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35821;&#38899;&#31038;&#21306;&#20013;&#65292;&#20998;&#31163;&#35821;&#38899;&#35805;&#35821;&#20013;&#30340;&#19981;&#30456;&#20851;&#20449;&#24687;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#19981;&#21516;&#30340;&#35821;&#38899;&#30456;&#20851;&#20219;&#21153;&#19987;&#27880;&#20110;&#25552;&#21462;&#19981;&#21516;&#30340;&#35821;&#38899;&#34920;&#31034;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20854;&#20182;&#19981;&#30456;&#20851;&#20449;&#24687;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#35821;&#38899;&#35821;&#26009;&#24211;&#65292;&#20197;&#20419;&#36827;&#35821;&#38899;&#34920;&#31034;&#35299;&#32544;&#30340;&#30740;&#31350;&#12290;3D-Speaker&#21253;&#21547;&#36229;&#36807;10,000&#20010;&#35828;&#35805;&#20154;&#65292;&#27599;&#20010;&#35828;&#35805;&#20154;&#21516;&#26102;&#30001;&#22810;&#20010;&#35774;&#22791;&#24405;&#21046;&#65292;&#22312;&#19981;&#21516;&#30340;&#36317;&#31163;&#19978;&#65292;&#24182;&#19988;&#19968;&#20123;&#35828;&#35805;&#20154;&#20250;&#35762;&#22810;&#31181;&#26041;&#35328;&#12290;&#22810;&#32500;&#38899;&#39057;&#25968;&#25454;&#30340;&#21463;&#25511;&#32452;&#21512;&#20135;&#29983;&#20102;&#19968;&#20010;&#22810;&#26679;&#30340;&#28151;&#21512;&#35821;&#38899;&#34920;&#31034;&#32416;&#32544;&#30697;&#38453;&#65292;&#20174;&#32780;&#28608;&#21457;&#20986;&#35299;&#24320;&#23427;&#20204;&#30340;&#26377;&#36259;&#26041;&#27861;&#12290;3D-Speaker&#30340;&#22810;&#39046;&#22495;&#24615;&#36136;&#36824;&#20351;&#20854;&#25104;&#20026;&#35780;&#20272;&#22823;&#22411;&#36890;&#29992;&#35821;&#38899;&#27169;&#22411;&#21644;&#23454;&#39564;&#22495;&#22806;&#23398;&#20064;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;&#21512;&#36866;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Disentangling uncorrelated information in speech utterances is a crucial research topic within speech community. Different speech-related tasks focus on extracting distinct speech representations while minimizing the affects of other uncorrelated information. We present a large-scale speech corpus to facilitate the research of speech representation disentanglement. 3D-Speaker contains over 10,000 speakers, each of whom are simultaneously recorded by multiple Devices, locating at different Distances, and some speakers are speaking multiple Dialects. The controlled combinations of multi-dimensional audio data yield a matrix of a diverse blend of speech representation entanglement, thereby motivating intriguing methods to untangle them. The multi-domain nature of 3D-Speaker also makes it a suitable resource to evaluate large universal speech models and experiment methods of out-of-domain learning and self-supervised learning. https://3dspeaker.github.io/
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#20013;&#38388;&#20219;&#21153;&#24494;&#35843;(ITFT)&#23545;&#20110;&#20302;&#36164;&#28304;&#12289;&#22810;&#35821;&#35328;&#12289;&#22810;&#39046;&#22495;&#30340;NMT&#38750;&#24120;&#26377;&#25928;&#65292;&#33021;&#22815;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#32531;&#35299;&#39046;&#22495;&#20998;&#27495;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.01382</link><description>&lt;p&gt;
&#21033;&#29992;&#36741;&#21161;&#39046;&#22495;&#24179;&#34892;&#25968;&#25454;&#22312;&#20013;&#38388;&#20219;&#21153;&#24494;&#35843;&#20013;&#23454;&#29616;&#20302;&#36164;&#28304;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Leveraging Auxiliary Domain Parallel Data in Intermediate Task Fine-tuning for Low-resource Translation. (arXiv:2306.01382v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#20013;&#38388;&#20219;&#21153;&#24494;&#35843;(ITFT)&#23545;&#20110;&#20302;&#36164;&#28304;&#12289;&#22810;&#35821;&#35328;&#12289;&#22810;&#39046;&#22495;&#30340;NMT&#38750;&#24120;&#26377;&#25928;&#65292;&#33021;&#22815;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#32531;&#35299;&#39046;&#22495;&#20998;&#27495;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#27809;&#26377;&#36275;&#22815;&#30340;&#24179;&#34892;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#26102;&#65292;&#22522;&#20110;&#39044;&#20808;&#35757;&#32451;&#30340;&#22810;&#35821;&#31181;Seq-to-Seq&#27169;&#22411;&#30340;NMT&#31995;&#32479;&#24212;&#29992;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#36825;&#20123;&#27169;&#22411;&#20013;&#32570;&#22833;/&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#35821;&#35328;&#12290;&#24403;&#25968;&#25454;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#26102;&#65292;&#38382;&#39064;&#36827;&#19968;&#27493;&#24694;&#21270;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#20013;&#38388;&#20219;&#21153;&#24494;&#35843;(ITFT)&#23545;&#20110;&#29305;&#23450;&#39046;&#22495;&#30340;NMT&#38750;&#24120;&#26377;&#30410;&#65292;&#23588;&#20854;&#26159;&#24403;&#30446;&#26631;&#39046;&#22495;&#30340;&#25968;&#25454;&#26377;&#38480;/&#19981;&#21487;&#29992;&#65292;&#32780;&#32771;&#34385;&#30340;&#35821;&#35328;&#22312;PMSS&#27169;&#22411;&#20013;&#32570;&#22833;&#25110;&#20195;&#34920;&#24615;&#19981;&#36275;&#26102;&#12290;&#25105;&#20204;&#20351;&#29992;&#39046;&#22495;&#20998;&#27495;&#27979;&#35797;&#37327;&#21270;&#20102;&#39046;&#22495;&#29305;&#23450;&#32467;&#26524;&#30340;&#21464;&#21270;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;ITFT&#33021;&#22815;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#32531;&#35299;&#39046;&#22495;&#20998;&#27495;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
NMT systems trained on Pre-trained Multilingual Sequence-Sequence (PMSS) models flounder when sufficient amounts of parallel data is not available for fine-tuning. This specifically holds for languages missing/under-represented in these models. The problem gets aggravated when the data comes from different domains. In this paper, we show that intermediate-task fine-tuning (ITFT) of PMSS models is extremely beneficial for domain-specific NMT, especially when target domain data is limited/unavailable and the considered languages are missing or under-represented in the PMSS model. We quantify the domain-specific results variations using a domain-divergence test, and show that ITFT can mitigate the impact of domain divergence to some extent.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#20010;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#65288;GenQ&#65289;&#65292;&#21487;&#20197;&#26681;&#25454;&#29031;&#39038;&#32773;&#21644;&#23401;&#23376;&#20043;&#38388;&#30340;&#23545;&#35805;&#20419;&#36827;&#23401;&#23376;&#30340;&#38405;&#35835;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#25991;&#21270;&#32972;&#26223;&#21644;&#35821;&#22659;&#21464;&#21270;&#20197;&#25552;&#39640;&#31995;&#32479;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.16809</link><description>&lt;p&gt;
GenQ&#65306;&#33258;&#21160;&#21270;&#38382;&#31572;&#29983;&#25104;&#22120;&#20197;&#24110;&#21161;&#29031;&#39038;&#32773;&#19982;&#23401;&#23376;&#20849;&#35835;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
GenQ: Automated Question Generation to Support Caregivers While Reading Stories with Children. (arXiv:2305.16809v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#20010;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#65288;GenQ&#65289;&#65292;&#21487;&#20197;&#26681;&#25454;&#29031;&#39038;&#32773;&#21644;&#23401;&#23376;&#20043;&#38388;&#30340;&#23545;&#35805;&#20419;&#36827;&#23401;&#23376;&#30340;&#38405;&#35835;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#25991;&#21270;&#32972;&#26223;&#21644;&#35821;&#22659;&#21464;&#21270;&#20197;&#25552;&#39640;&#31995;&#32479;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#29031;&#39038;&#32773;&#35810;&#38382;&#24320;&#25918;&#24335;&#38382;&#39064;&#20197;&#28608;&#21457;&#19982;&#23401;&#23376;&#30340;&#23545;&#35805;&#26102;&#65292;&#21487;&#20197;&#20419;&#36827;&#23401;&#23376;&#30340;&#38405;&#35835;&#29702;&#35299;&#33021;&#21147;&#12290;&#34429;&#28982;&#26377;&#21033;&#29992;&#25216;&#26415;&#24037;&#20855;&#26469;&#25903;&#25345;&#36825;&#20010;&#36807;&#31243;&#65292;&#21363;&#25152;&#35859;&#30340;&#8220;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#8221;&#30340;&#31354;&#38388;&#65292;&#20294;&#30446;&#21069;&#20173;&#19981;&#28165;&#26970;&#29616;&#26377;&#30340;&#29983;&#25104;&#31867;&#20154;&#35821;&#35328;&#38382;&#39064;&#30340;&#26234;&#33021;&#31995;&#32479;&#26159;&#21542;&#26377;&#30410;&#12290;&#27492;&#22806;&#65292;&#29992;&#20110;&#24320;&#21457;&#36825;&#20123;&#33258;&#21160;&#29983;&#25104;&#38382;&#39064;&#31995;&#32479;&#30340;&#22521;&#35757;&#25968;&#25454;&#36890;&#24120;&#27809;&#26377;&#32771;&#34385;&#21040;&#20154;&#21475;&#32479;&#35745;&#23398;&#65292;&#20294;&#20855;&#26377;&#19981;&#21516;&#25991;&#21270;&#32972;&#26223;&#30340;&#20154;&#21487;&#33021;&#20250;&#25552;&#20986;&#19981;&#21516;&#30340;&#38382;&#39064;&#12290;&#20316;&#20026;&#20026;&#25289;&#19969;&#35028;&#20799;&#31461;&#35774;&#35745;&#26234;&#33021;&#38405;&#35835;&#25903;&#25345;&#24212;&#29992;&#31243;&#24207;&#30340;&#24191;&#27867;&#39033;&#30446;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#20174;&#26469;&#33258;&#19981;&#21516;&#20154;&#21475;&#32479;&#35745;&#23398;&#30340;&#25289;&#19969;&#35028;&#25252;&#29702;&#20154;&#21592;&#21644;&#38750;&#25252;&#29702;&#20154;&#21592;&#20197;&#21450;&#20854;&#20182;&#20154;&#21475;&#32479;&#35745;&#23398;&#32972;&#26223;&#30340;&#25252;&#29702;&#20154;&#21592;&#21644;&#38750;&#25252;&#29702;&#20154;&#21592;&#20013;&#32676;&#38598;&#22823;&#37327;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20010;&#25968;&#25454;&#38598;&#20013;&#20010;&#20307;&#12289;&#25991;&#21270;&#21644;&#29615;&#22659;&#22240;&#32032;&#20013;&#20171;&#30340;&#38382;&#39064;&#25552;&#38382;&#30340;&#21464;&#21270;&#12290;&#28982;&#21518;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31995;&#32479;&#26469;&#33258;&#21160;&#20135;&#29983;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
When caregivers ask open--ended questions to motivate dialogue with children, it facilitates the child's reading comprehension skills.Although there is scope for use of technological tools, referred here as "intelligent tutoring systems", to scaffold this process, it is currently unclear whether existing intelligent systems that generate human--language like questions is beneficial. Additionally, training data used in the development of these automated question generation systems is typically sourced without attention to demographics, but people with different cultural backgrounds may ask different questions. As a part of a broader project to design an intelligent reading support app for Latinx children, we crowdsourced questions from Latinx caregivers and noncaregivers as well as caregivers and noncaregivers from other demographics. We examine variations in question--asking within this dataset mediated by individual, cultural, and contextual factors. We then design a system that autom
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;ArguGPT&#65292;&#23427;&#26159;&#30001;7&#20010;GPT&#27169;&#22411;&#29983;&#25104;&#30340;&#35770;&#35777;&#25991;&#31456;&#35821;&#26009;&#24211;&#65292;&#26088;&#22312;&#35299;&#20915;AI&#29983;&#25104;&#20869;&#23481;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#25945;&#24072;&#39318;&#27425;&#25509;&#35302;&#26426;&#22120;&#29983;&#25104;&#30340;&#35770;&#25991;&#26102;&#21482;&#26377;61%&#30340;&#20934;&#30830;&#24230;&#65292;&#20294;&#32463;&#36807;&#19968;&#36718;&#35757;&#32451;&#21518;&#25552;&#39640;&#21040;&#20102;67%&#12290;</title><link>http://arxiv.org/abs/2304.07666</link><description>&lt;p&gt;
ArguGPT&#65306;&#35780;&#20272;&#12289;&#29702;&#35299;&#21644;&#35782;&#21035;&#30001;GPT&#27169;&#22411;&#29983;&#25104;&#30340;&#35770;&#35777;&#25991;&#31456;
&lt;/p&gt;
&lt;p&gt;
ArguGPT: evaluating, understanding and identifying argumentative essays generated by GPT models. (arXiv:2304.07666v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07666
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;ArguGPT&#65292;&#23427;&#26159;&#30001;7&#20010;GPT&#27169;&#22411;&#29983;&#25104;&#30340;&#35770;&#35777;&#25991;&#31456;&#35821;&#26009;&#24211;&#65292;&#26088;&#22312;&#35299;&#20915;AI&#29983;&#25104;&#20869;&#23481;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#25945;&#24072;&#39318;&#27425;&#25509;&#35302;&#26426;&#22120;&#29983;&#25104;&#30340;&#35770;&#25991;&#26102;&#21482;&#26377;61%&#30340;&#20934;&#30830;&#24230;&#65292;&#20294;&#32463;&#36807;&#19968;&#36718;&#35757;&#32451;&#21518;&#25552;&#39640;&#21040;&#20102;67%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#20869;&#23481;&#65288;AIGC&#65289;&#23545;&#20840;&#29699;&#25945;&#32946;&#24037;&#20316;&#32773;&#25552;&#20986;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#25945;&#24072;&#20204;&#38656;&#35201;&#33021;&#22815;&#29992;&#32905;&#30524;&#25110;&#24037;&#20855;&#26816;&#27979;&#20986;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#38656;&#35201;&#26356;&#22810;&#22320;&#20102;&#35299;AIGC&#30340;&#35789;&#27719;&#12289;&#21477;&#27861;&#21644;&#39118;&#26684;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#33521;&#35821;&#25945;&#23398;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;ArguGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;7&#20010;GPT&#27169;&#22411;&#29983;&#25104;&#30340;4038&#31687;&#26377;&#24179;&#34913;&#30340;&#35770;&#35777;&#25991;&#31456;&#35821;&#26009;&#24211;&#65292;&#36825;&#20123;&#35770;&#35777;&#25991;&#31456;&#26159;&#22312;&#20197;&#19979;&#19977;&#20010;&#26469;&#28304;&#30340;&#35770;&#25991;&#25552;&#31034;&#19979;&#29983;&#25104;&#30340;&#65306;&#65288;1&#65289;&#35838;&#22530;&#25110;&#23478;&#24237;&#20316;&#19994;&#32451;&#20064;&#65292;&#65288;2&#65289;&#25176;&#31119;&#21644;&#65288;3&#65289;GRE&#20889;&#20316;&#20219;&#21153;&#12290;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#22823;&#33268;&#30456;&#31561;&#25968;&#37327;&#30340;&#20154;&#24037;&#32534;&#20889;&#30340;&#25991;&#31456;&#37197;&#23545;&#65292;&#36825;&#20123;&#25991;&#31456;&#30340;&#19977;&#20010;&#24471;&#20998;&#32423;&#21035;&#21305;&#37197;&#35770;&#25991;&#25552;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#38599;&#29992;&#33521;&#35821;&#25945;&#24072;&#26469;&#21306;&#20998;&#26426;&#22120;&#35770;&#25991;&#21644;&#20154;&#24037;&#35770;&#25991;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#25945;&#24072;&#20204;&#39318;&#27425;&#25509;&#35302;&#26426;&#22120;&#29983;&#25104;&#30340;&#35770;&#25991;&#26102;&#65292;&#20182;&#20204;&#20165;&#33021;&#20197;61%&#30340;&#20934;&#30830;&#24230;&#26816;&#27979;&#20986;&#23427;&#20204;&#12290;&#20294;&#32463;&#36807;&#19968;&#36718;&#35757;&#32451;&#21518;&#65292;&#36825;&#20010;&#25968;&#23383;&#25552;&#39640;&#21040;&#20102;67%&#12290;
&lt;/p&gt;
&lt;p&gt;
AI generated content (AIGC) presents considerable challenge to educators around the world. Instructors need to be able to detect such text generated by large language models, either with the naked eye or with the help of some tools. There is also growing need to understand the lexical, syntactic and stylistic features of AIGC. To address these challenges in English language teaching, we first present ArguGPT, a balanced corpus of 4,038 argumentative essays generated by 7 GPT models in response to essay prompts from three sources: (1) in-class or homework exercises, (2) TOEFL and (3) GRE writing tasks. Machine-generated texts are paired with roughly equal number of human-written essays with three score levels matched in essay prompts. We then hire English instructors to distinguish machine essays from human ones. Results show that when first exposed to machine-generated essays, the instructors only have an accuracy of 61% in detecting them. But the number rises to 67% after one round of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COTI&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#24341;&#20837;&#29702;&#35770;&#25351;&#23548;&#30340;&#25439;&#22833;&#30446;&#26631;&#21644;&#20840;&#38754;&#30340;&#21152;&#26435;&#35780;&#20998;&#26426;&#21046;&#65292;&#24182;&#32467;&#21512;&#20027;&#21160;&#23398;&#20064;&#33539;&#24335;&#26469;&#35299;&#20915;&#25991;&#26412;&#21453;&#36716;&#26102;&#30340;&#22256;&#38590;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#65292;&#25968;&#25454;&#25928;&#29575;&#39640;&#65292;&#26131;&#20110;&#20351;&#29992;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2304.05265</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#21487;&#25511;&#25991;&#26412;&#21453;&#36716;
&lt;/p&gt;
&lt;p&gt;
Controllable Textual Inversion for Personalized Text-to-Image Generation. (arXiv:2304.05265v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COTI&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#24341;&#20837;&#29702;&#35770;&#25351;&#23548;&#30340;&#25439;&#22833;&#30446;&#26631;&#21644;&#20840;&#38754;&#30340;&#21152;&#26435;&#35780;&#20998;&#26426;&#21046;&#65292;&#24182;&#32467;&#21512;&#20027;&#21160;&#23398;&#20064;&#33539;&#24335;&#26469;&#35299;&#20915;&#25991;&#26412;&#21453;&#36716;&#26102;&#30340;&#22256;&#38590;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#65292;&#25968;&#25454;&#25928;&#29575;&#39640;&#65292;&#26131;&#20110;&#20351;&#29992;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#22312;&#20197;&#25991;&#26412;&#20026;&#24341;&#23548;&#30340;&#39640;&#20445;&#30495;&#22270;&#20687;&#30340;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#12290;&#24403;&#24341;&#23548;&#20449;&#24687;&#21253;&#21547;&#29992;&#25143;&#23450;&#20041;&#30340;&#12289;&#26410;&#35265;&#36807;&#30340;&#25110;&#38271;&#23614;&#27010;&#24565;&#26631;&#35760;&#26102;&#65292;&#25991;&#26412;&#21453;&#36716;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#20010;&#24615;&#21270;&#29983;&#25104;&#25216;&#26415;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#25105;&#20204;&#21457;&#29616;&#24182;&#23637;&#31034;&#20102;&#25991;&#26412;&#21453;&#36716;&#30340;&#37096;&#32626;&#20173;&#20805;&#28385;&#20102;&#8220;&#40657;&#39764;&#27861;&#8221;&#65292;&#20363;&#22914;&#39069;&#22806;&#25968;&#25454;&#38598;&#30340;&#20005;&#33499;&#35201;&#27714;&#65292;&#22312;&#24490;&#29615;&#20013;&#38656;&#35201;&#33392;&#33510;&#30340;&#20154;&#21147;&#25104;&#26412;&#21644;&#32570;&#20047;&#40065;&#26834;&#24615;&#31561;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21487;&#25511;&#25991;&#26412;&#21453;&#36716;&#30340;&#22823;&#22823;&#22686;&#24378;&#29256;&#21453;&#36716;&#65292;&#35299;&#20915;&#20102;&#25152;&#26377;&#19978;&#36848;&#38382;&#39064;&#65292;&#24182;&#21453;&#36807;&#26469;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#65292;&#25968;&#25454;&#25928;&#29575;&#39640;&#65292;&#26131;&#20110;&#20351;&#29992;&#30340;&#26694;&#26550;&#12290;COTI&#30340;&#26680;&#24515;&#26159;&#22522;&#20110;&#29702;&#35770;&#30340;&#25439;&#22833;&#30446;&#26631;&#65292;&#20855;&#26377;&#20840;&#38754;&#21644;&#26032;&#39062;&#30340;&#21152;&#26435;&#35780;&#20998;&#26426;&#21046;&#65292;&#24182;&#30001;&#20027;&#21160;&#23398;&#20064;&#33539;&#24335;&#25152;&#25552;&#21462;&#12290;&#24191;&#27867;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;COTI&#30340;&#24615;&#33021;&#27604;&#20043;&#21069;&#25216;&#26415;&#26377;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#65292;&#23588;&#20854;&#26159;&#22312;&#25968;&#25454;&#23569;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent large-scale generative modeling has attained unprecedented performance especially in producing high-fidelity images driven by text prompts. Text inversion (TI), alongside the text-to-image model backbones, is proposed as an effective technique in personalizing the generation when the prompts contain user-defined, unseen or long-tail concept tokens. Despite that, we find and show that the deployment of TI remains full of "dark-magics" -- to name a few, the harsh requirement of additional datasets, arduous human efforts in the loop and lack of robustness. In this work, we propose a much-enhanced version of TI, dubbed Controllable Textual Inversion (COTI), in resolving all the aforementioned problems and in turn delivering a robust, data-efficient and easy-to-use framework. The core to COTI is a theoretically-guided loss objective instantiated with a comprehensive and novel weighted scoring mechanism, encapsulated by an active-learning paradigm. The extensive results show that 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#26469;&#36827;&#34892;&#38899;&#39057;&#28304;&#20998;&#31163;&#65292;&#23454;&#29616;&#20102;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#38899;&#39057;&#30340;&#19968;&#33268;&#24615;&#23545;&#40784;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.16342</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#35328;&#25351;&#23548;&#30340;&#19977;&#27169;&#24577;&#19968;&#33268;&#24615;&#36827;&#34892;&#38899;&#39057;-&#35270;&#39057;&#28304;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Language-Guided Audio-Visual Source Separation via Trimodal Consistency. (arXiv:2303.16342v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16342
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#26469;&#36827;&#34892;&#38899;&#39057;&#28304;&#20998;&#31163;&#65292;&#23454;&#29616;&#20102;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#38899;&#39057;&#30340;&#19968;&#33268;&#24615;&#23545;&#40784;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#22312;&#35270;&#39057;&#20013;&#23398;&#20064;&#25191;&#34892;&#38899;&#39057;&#28304;&#20998;&#31163;&#65292;&#20165;&#20351;&#29992;&#26410;&#26631;&#35760;&#30340;&#35270;&#39057;&#21644;&#38899;&#39057;&#23545;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#12290;&#36825;&#39033;&#20219;&#21153;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#23398;&#20064;&#23558;&#21457;&#22768;&#29289;&#20307;&#30340;&#35821;&#35328;&#25551;&#36848;&#19982;&#20854;&#35270;&#35273;&#29305;&#24449;&#21644;&#30456;&#24212;&#30340;&#38899;&#39057;&#27874;&#24418;&#32452;&#20214;&#32852;&#31995;&#36215;&#26469;&#65292;&#32780;&#22312;&#35757;&#32451;&#26399;&#38388;&#27809;&#26377;&#35775;&#38382;&#27880;&#37322;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#29616;&#25104;&#30340;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#20004;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#25552;&#20379;&#20266;&#30446;&#26631;&#30417;&#30563;&#65292;&#24182;&#20419;&#36827;&#38899;&#39057;&#12289;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#27169;&#24577;&#20043;&#38388;&#26356;&#24378;&#30340;&#23545;&#40784;&#12290;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#32473;&#23450;&#25991;&#26412;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#36755;&#20837;&#25110;&#20165;&#32473;&#23450;&#25991;&#26412;&#21644;&#38899;&#39057;&#36755;&#20837;&#26102;&#20998;&#31163;&#22768;&#38899;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#38899;&#39057;-&#35270;&#39057;&#20998;&#31163;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;MUSIC&#12289;SOLOS&#21644;AudioSet&#65289;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#33258;&#30417;&#30563;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20854;&#20013;&#25105;&#20204;&#30340;&#27169;&#22411;&#32988;&#36807;&#20102;&#29616;&#26377;&#24378;&#30417;&#30563;&#26041;&#27861;&#30340;&#26368;&#26032;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a self-supervised approach for learning to perform audio source separation in videos based on natural language queries, using only unlabeled video and audio pairs as training data. A key challenge in this task is learning to associate the linguistic description of a sound-emitting object to its visual features and the corresponding components of the audio waveform, all without access to annotations during training. To overcome this challenge, we adapt off-the-shelf vision-language foundation models to provide pseudo-target supervision via two novel loss functions and encourage a stronger alignment between the audio, visual and natural language modalities. During inference, our approach can separate sounds given text, video and audio input, or given text and audio input alone. We demonstrate the effectiveness of our self-supervised approach on three audio-visual separation datasets, including MUSIC, SOLOS and AudioSet, where we outperform state-of-the-art strongly supervised 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#19968;&#33268;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#37492;&#21035;&#22120;&#21644;&#29983;&#25104;&#22120;&#30340;&#21512;&#20316;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#26631;&#20934;GAN&#35757;&#32451;&#19981;&#31283;&#23450;&#12289;&#26679;&#26412;&#23481;&#26131;&#20559;&#31163;&#23454;&#38469;&#25968;&#25454;&#20998;&#24067;&#12289;&#37492;&#21035;&#27169;&#22411;&#25913;&#36827;&#39281;&#21644;&#31561;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#19981;&#20165;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;GAN&#65292;&#22312;&#25991;&#26412;&#21644;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#20063;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#12290;</title><link>http://arxiv.org/abs/2303.09075</link><description>&lt;p&gt;
&#33258;&#19968;&#33268;&#23398;&#20064;&#65306;&#29983;&#25104;&#22120;&#21644;&#37492;&#21035;&#22120;&#30340;&#21512;&#20316;
&lt;/p&gt;
&lt;p&gt;
Self-Consistent Learning: Cooperation between Generators and Discriminators. (arXiv:2303.09075v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#19968;&#33268;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#37492;&#21035;&#22120;&#21644;&#29983;&#25104;&#22120;&#30340;&#21512;&#20316;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#26631;&#20934;GAN&#35757;&#32451;&#19981;&#31283;&#23450;&#12289;&#26679;&#26412;&#23481;&#26131;&#20559;&#31163;&#23454;&#38469;&#25968;&#25454;&#20998;&#24067;&#12289;&#37492;&#21035;&#27169;&#22411;&#25913;&#36827;&#39281;&#21644;&#31561;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#19981;&#20165;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;GAN&#65292;&#22312;&#25991;&#26412;&#21644;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#20063;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20351;&#29992;&#29983;&#25104;&#30340;&#25968;&#25454;&#26469;&#25552;&#39640;&#19979;&#28216;&#37492;&#21035;&#27169;&#22411;&#30340;&#24615;&#33021;&#24050;&#32463;&#22240;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24040;&#22823;&#21457;&#23637;&#32780;&#24191;&#21463;&#27426;&#36814;&#12290;&#22312;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#65292;&#29983;&#25104;&#27169;&#22411;&#21644;&#37492;&#21035;&#27169;&#22411;&#26159;&#20998;&#21035;&#35757;&#32451;&#30340;&#65292;&#22240;&#27492;&#23427;&#20204;&#19981;&#33021;&#36866;&#24212;&#24444;&#27492;&#30340;&#20219;&#20309;&#21464;&#21270;&#12290;&#22240;&#27492;&#65292;&#29983;&#25104;&#30340;&#26679;&#26412;&#24456;&#23481;&#26131;&#20559;&#31163;&#23454;&#38469;&#25968;&#25454;&#20998;&#24067;&#65292;&#32780;&#37492;&#21035;&#27169;&#22411;&#30340;&#25913;&#36827;&#24456;&#24555;&#23601;&#20250;&#36798;&#21040;&#39281;&#21644;&#12290;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#36890;&#36807;&#19968;&#31181;&#23545;&#25239;&#24615;&#36807;&#31243;&#19982;&#37492;&#21035;&#27169;&#22411;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#20197;&#23454;&#29616;&#32852;&#21512;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;GAN&#30340;&#35757;&#32451;&#26497;&#19981;&#31283;&#23450;&#65292;&#24448;&#24448;&#38590;&#20197;&#25910;&#25947;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#19968;&#33268;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#19968;&#20010;&#37492;&#21035;&#22120;&#21644;&#19968;&#20010;&#29983;&#25104;&#22120;&#20197;&#38381;&#29615;&#24418;&#24335;&#21512;&#20316;&#35757;&#32451;&#12290;&#37492;&#21035;&#22120;&#21644;&#29983;&#25104;&#22120;&#22312;&#22810;&#36718;&#26356;&#26032;&#20013;&#30456;&#20114;&#22686;&#24378;&#65292;&#29983;&#25104;&#30340;&#26679;&#26412;&#36880;&#28176;&#25509;&#36817;&#23454;&#38469;&#25968;&#25454;&#20998;&#24067;&#65292;&#32780;&#37492;&#21035;&#27169;&#22411;&#19981;&#26029;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#19981;&#20165;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;GAN&#65292;&#32780;&#19988;&#22312;&#25991;&#26412;&#21644;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Using generated data to improve the performance of downstream discriminative models has recently gained popularity due to the great development of pre-trained language models. In most previous studies, generative models and discriminative models are trained separately and thus could not adapt to any changes in each other. As a result, the generated samples can easily deviate from the real data distribution, while the improvement of the discriminative model quickly reaches saturation. Generative adversarial networks (GANs) train generative models via an adversarial process with discriminative models to achieve joint training. However, the training of standard GANs is notoriously unstable and often falls short of convergence. In this paper, to address these issues, we propose a $\textit{self-consistent learning}$ framework, in which a discriminator and a generator are cooperatively trained in a closed-loop form. The discriminator and the generator enhance each other during multiple round
&lt;/p&gt;</description></item><item><title>Google USM&#26159;&#19968;&#20010;&#36890;&#29992;&#35821;&#38899;&#27169;&#22411;&#65292;&#36890;&#36807;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#22312;100&#22810;&#31181;&#35821;&#35328;&#19978;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#21644;&#35821;&#38899;-&#25991;&#26412;&#32763;&#35793;&#20219;&#21153;&#30340;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.01037</link><description>&lt;p&gt;
Google USM: &#23558;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#25193;&#23637;&#21040;100&#22810;&#31181;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Google USM: Scaling Automatic Speech Recognition Beyond 100 Languages. (arXiv:2303.01037v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01037
&lt;/p&gt;
&lt;p&gt;
Google USM&#26159;&#19968;&#20010;&#36890;&#29992;&#35821;&#38899;&#27169;&#22411;&#65292;&#36890;&#36807;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#22312;100&#22810;&#31181;&#35821;&#35328;&#19978;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#21644;&#35821;&#38899;-&#25991;&#26412;&#32763;&#35793;&#20219;&#21153;&#30340;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#36890;&#29992;&#35821;&#38899;&#27169;&#22411;&#65288;USM&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;100&#22810;&#31181;&#35821;&#35328;&#19978;&#36827;&#34892;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#30340;&#21333;&#20010;&#22823;&#27169;&#22411;&#12290;&#36890;&#36807;&#22312;&#22810;&#26679;&#21270;&#30340;&#26080;&#26631;&#31614;&#35821;&#26009;&#24211;&#19978;&#23545;&#27169;&#22411;&#30340;&#32534;&#30721;&#22120;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#35813;&#35821;&#26009;&#24211;&#36328;&#36234;300&#22810;&#31181;&#35821;&#35328;&#65292;&#24635;&#26102;&#38271;&#36798;&#21040;1200&#19975;&#23567;&#26102;&#65292;&#24182;&#22312;&#36739;&#23567;&#30340;&#24050;&#26631;&#35760;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#12289;&#38543;&#26426;&#25237;&#24433;&#37327;&#21270;&#21644;&#35821;&#38899;-&#25991;&#26412;&#27169;&#24577;&#21305;&#37197;&#65292;&#23454;&#29616;&#20102;&#19979;&#28216;&#22810;&#35821;&#35328;ASR&#21644;&#35821;&#38899;-&#25991;&#26412;&#32763;&#35793;&#20219;&#21153;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#65292;&#23613;&#31649;&#20351;&#29992;&#30340;&#35757;&#32451;&#38598;&#22823;&#23567;&#20165;&#20026;Whisper&#27169;&#22411;&#20351;&#29992;&#30340;1/7&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#35768;&#22810;&#35821;&#35328;&#30340;&#39046;&#22495;&#20869;&#21644;&#39046;&#22495;&#22806;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21487;&#27604;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the Universal Speech Model (USM), a single large model that performs automatic speech recognition (ASR) across 100+ languages. This is achieved by pre-training the encoder of the model on a large unlabeled multilingual dataset of 12 million (M) hours spanning over 300 languages, and fine-tuning on a smaller labeled dataset. We use multilingual pre-training with random-projection quantization and speech-text modality matching to achieve state-of-the-art performance on downstream multilingual ASR and speech-to-text translation tasks. We also demonstrate that despite using a labeled training set 1/7-th the size of that used for the Whisper model, our model exhibits comparable or better performance on both in-domain and out-of-domain speech recognition tasks across many languages.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;HL&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#25193;&#23637;&#20102;COCO&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;14997&#20010;&#22270;&#20687;&#21644;134,973&#20010;&#20154;&#24037;&#27880;&#37322;&#30340;&#39640;&#32423;&#21035;&#25551;&#36848;&#65292;&#28041;&#21450;&#22330;&#26223;&#12289;&#21160;&#20316;&#21644;&#29702;&#30001;&#65292;&#21487;&#20197;&#29992;&#20110;&#23545;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26356;&#39640;&#32423;&#21035;&#30340;&#27979;&#35797;&#21644;&#24494;&#35843;&#12290;</title><link>http://arxiv.org/abs/2302.12189</link><description>&lt;p&gt;
HL&#25968;&#25454;&#38598;: &#22522;&#20110;&#35270;&#35273;&#30340;&#22330;&#26223;&#12289;&#21160;&#20316;&#21644;&#29702;&#30001;&#30340;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;
HL Dataset: Visually-grounded Description of Scenes, Actions and Rationales. (arXiv:2302.12189v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12189
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;HL&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#25193;&#23637;&#20102;COCO&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;14997&#20010;&#22270;&#20687;&#21644;134,973&#20010;&#20154;&#24037;&#27880;&#37322;&#30340;&#39640;&#32423;&#21035;&#25551;&#36848;&#65292;&#28041;&#21450;&#22330;&#26223;&#12289;&#21160;&#20316;&#21644;&#29702;&#30001;&#65292;&#21487;&#20197;&#29992;&#20110;&#23545;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26356;&#39640;&#32423;&#21035;&#30340;&#27979;&#35797;&#21644;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#25551;&#36848;&#25968;&#25454;&#38598;&#20391;&#37325;&#20110;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#25551;&#36848;&#65292;&#25551;&#36848;&#22270;&#20687;&#20013;&#21487;&#35265;&#30340;&#29289;&#20307;&#65292;&#20363;&#22914;&#8220;&#20154;&#20204;&#22312;&#20844;&#22253;&#37324;&#21507;&#19996;&#35199;&#8221;&#12290;&#34429;&#28982;&#36825;&#20123;&#25968;&#25454;&#38598;&#23545;&#20110;&#35780;&#20272;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#35782;&#21035;&#21644;&#25551;&#36848;&#35270;&#35273;&#20869;&#23481;&#30340;&#33021;&#21147;&#24456;&#26377;&#29992;&#65292;&#20294;&#23427;&#20204;&#19981;&#25903;&#25345;&#28041;&#21450;&#27169;&#22411;&#27979;&#35797;&#25110;&#24494;&#35843;&#30340;&#21463;&#25511;&#23454;&#39564;&#65292;&#20351;&#29992;&#26356;&#39640;&#32423;&#30340;&#25551;&#36848;&#65292;&#20154;&#20204;&#21457;&#29616;&#24456;&#23481;&#26131;&#21644;&#33258;&#28982;&#22320;&#20135;&#29983;&#12290;&#20363;&#22914;&#65292;&#20154;&#20204;&#36890;&#24120;&#26681;&#25454;&#22270;&#20687;&#25152;&#25551;&#32472;&#30340;&#22330;&#26223;&#31867;&#22411;&#65288;&#8220;&#20154;&#20204;&#22312;&#24230;&#20551;&#32988;&#22320;&#8221;&#65289;&#21644;&#20182;&#20204;&#36827;&#34892;&#30340;&#21160;&#20316;&#65288;&#8220;&#20154;&#20204;&#27491;&#22312;&#37326;&#39184;&#8221;&#65289;&#26469;&#25551;&#36848;&#22270;&#20687;&#12290;&#36825;&#20123;&#25551;&#36848;&#22522;&#20110;&#20010;&#20154;&#32463;&#39564;&#21644;&#24120;&#35782;&#24615;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#39640;&#32423;&#21035;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#25193;&#23637;&#20102;&#26469;&#33258;COCO&#25968;&#25454;&#38598;&#30340;14997&#20010;&#22270;&#20687;&#65292;&#24182;&#19982;&#19968;&#32452;&#26032;&#30340;134,973&#20010;&#20154;&#24037;&#27880;&#37322;&#65288;&#39640;&#32423;&#21035;&#65289;&#25551;&#36848;&#23545;&#40784;&#65292;&#36825;&#20123;&#25551;&#36848;&#20174;&#22330;&#26223;&#12289;&#21160;&#20316;&#21644;&#29702;&#30001;&#19977;&#20010;&#26041;&#38754;&#36827;&#34892;&#20102;&#25910;&#38598;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20351;&#29992;&#20174;&#29420;&#31435;&#38405;&#35835;&#32773;&#32452;&#25910;&#38598;&#30340;&#20449;&#24515;&#24471;&#20998;&#25193;&#23637;&#20102;&#35813;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current captioning datasets focus on object-centric captions, describing the visible objects in the image, e.g. "people eating food in a park". Although these datasets are useful to evaluate the ability of Vision &amp; Language models to recognize and describe visual content, they do not support controlled experiments involving model testing or fine-tuning, with more high-level captions, which humans find easy and natural to produce. For example, people often describe images based on the type of scene they depict ('people at a holiday resort') and the actions they perform ('people having a picnic'). Such descriptions draw on personal experience and commonsense assumptions. We present the High-Level Dataset a dataset extending 14997 images from the COCO dataset, aligned with a new set of 134,973 human-annotated (high-level) captions collected along three axes: scenes, actions, and rationales. We further extend this dataset with confidence scores collected from an independent set of readers,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#35774;&#35745;&#25552;&#31034;&#30340;&#22240;&#32032;&#65292;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#32842;&#22825;&#26426;&#22120;&#20154;&#26469;&#36827;&#34892;&#33258;&#28982;&#23545;&#35805;&#21644;&#21487;&#38752;&#22320;&#25910;&#38598;&#29992;&#25143;&#33258;&#25105;&#25253;&#21578;&#25968;&#25454;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25552;&#31034;&#30340;&#35774;&#35745;&#21644;&#23545;&#35805;&#20027;&#39064;&#26126;&#26174;&#24433;&#21709;&#20102;&#23545;&#35805;&#27969;&#31243;&#21644;&#25968;&#25454;&#25910;&#38598;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.05843</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21319;&#29992;&#20110;&#25910;&#38598;&#29992;&#25143;&#33258;&#25105;&#25253;&#21578;&#25968;&#25454;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models to Power Chatbots for Collecting User Self-Reported Data. (arXiv:2301.05843v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#35774;&#35745;&#25552;&#31034;&#30340;&#22240;&#32032;&#65292;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#32842;&#22825;&#26426;&#22120;&#20154;&#26469;&#36827;&#34892;&#33258;&#28982;&#23545;&#35805;&#21644;&#21487;&#38752;&#22320;&#25910;&#38598;&#29992;&#25143;&#33258;&#25105;&#25253;&#21578;&#25968;&#25454;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25552;&#31034;&#30340;&#35774;&#35745;&#21644;&#23545;&#35805;&#20027;&#39064;&#26126;&#26174;&#24433;&#21709;&#20102;&#23545;&#35805;&#27969;&#31243;&#21644;&#25968;&#25454;&#25910;&#38598;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#36807;&#25509;&#21463;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#26469;&#26500;&#24314;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#26032;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#35774;&#35745;&#25552;&#31034;&#26469;&#20351;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#36861;&#27714;&#32473;&#23450;&#30446;&#26631;&#65288;&#22914;&#20174;&#29992;&#25143;&#25910;&#38598;&#33258;&#25105;&#25253;&#21578;&#25968;&#25454;&#65289;&#30340;&#21516;&#26102;&#36827;&#34892;&#33258;&#28982;&#23545;&#35805;&#23578;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#21738;&#20123;&#25552;&#31034;&#30340;&#35774;&#35745;&#22240;&#32032;&#21487;&#20197;&#24110;&#21161;&#24341;&#23548;&#32842;&#22825;&#26426;&#22120;&#20154;&#36827;&#34892;&#33258;&#28982;&#23545;&#35805;&#24182;&#21487;&#38752;&#22320;&#25910;&#38598;&#25968;&#25454;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22235;&#31181;&#20855;&#26377;&#19981;&#21516;&#32467;&#26500;&#21644;&#20154;&#35774;&#30340;&#25552;&#31034;&#24418;&#24335;&#12290;&#36890;&#36807;&#19968;&#39033;&#22312;&#32447;&#30740;&#31350;&#65288;N = 48&#65289;&#65292;&#21442;&#19982;&#32773;&#19982;&#30001;&#19981;&#21516;&#35774;&#35745;&#25552;&#31034;&#39537;&#21160;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#36827;&#34892;&#23545;&#35805;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#35774;&#35745;&#25552;&#31034;&#21644;&#23545;&#35805;&#20027;&#39064;&#22914;&#20309;&#24433;&#21709;&#23545;&#35805;&#27969;&#31243;&#21644;&#29992;&#25143;&#23545;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#24863;&#30693;&#12290;&#22312;&#23545;&#35805;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#35206;&#30422;&#20102;79%&#30340;&#25152;&#38656;&#20449;&#24687;&#27133;&#65292;&#24182;&#19988;&#25552;&#31034;&#21644;&#20027;&#39064;&#30340;&#35774;&#35745;&#26174;&#33879;&#24433;&#21709;&#20102;&#23545;&#35805;&#27969;&#31243;&#21644;&#25968;&#25454;&#25910;&#38598;&#24615;&#33021;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#21033;&#29992;LLMs&#26500;&#24314;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) provide a new way to build chatbots by accepting natural language prompts. Yet, it is unclear how to design prompts to power chatbots to carry on naturalistic conversations while pursuing a given goal, such as collecting self-report data from users. We explore what design factors of prompts can help steer chatbots to talk naturally and collect data reliably. To this aim, we formulated four prompt designs with different structures and personas. Through an online study (N = 48) where participants conversed with chatbots driven by different designs of prompts, we assessed how prompt designs and conversation topics affected the conversation flows and users' perceptions of chatbots. Our chatbots covered 79% of the desired information slots during conversations, and the designs of prompts and topics significantly influenced the conversation flows and the data collection performance. We discuss the opportunities and challenges of building chatbots with LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#33258;&#21160;&#25991;&#26412;&#25688;&#35201;&#31995;&#32479;&#30340;&#32508;&#36848;&#65292;&#21253;&#25324;&#26041;&#27861;&#12289;&#25968;&#25454;&#12289;&#35780;&#20272;&#21644;&#32534;&#30721;&#12290;&#20316;&#32773;&#36890;&#36807;&#24341;&#29992;&#30340;&#26041;&#24335;&#22238;&#39038;&#20102;&#30456;&#20851;&#25991;&#29486;&#65292;&#24182;&#20171;&#32461;&#20102;&#19981;&#21516;&#30340;&#25688;&#35201;&#29983;&#25104;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36824;&#23545;&#21487;&#29992;&#20110;&#35780;&#20272;&#21644;&#25968;&#25454;&#35757;&#32451;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#24182;&#20351;&#29992;CNN&#35821;&#26009;&#24211;&#25968;&#25454;&#38598;&#23545;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#35777;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2301.03403</link><description>&lt;p&gt;
&#33258;&#21160;&#25991;&#26412;&#25688;&#35201;&#25216;&#26415;&#30340;&#32508;&#21512;&#22238;&#39038;&#65306;&#26041;&#27861;&#12289;&#25968;&#25454;&#12289;&#35780;&#20272;&#21644;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
A comprehensive review of automatic text summarization techniques: method, data, evaluation and coding. (arXiv:2301.03403v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.03403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#33258;&#21160;&#25991;&#26412;&#25688;&#35201;&#31995;&#32479;&#30340;&#32508;&#36848;&#65292;&#21253;&#25324;&#26041;&#27861;&#12289;&#25968;&#25454;&#12289;&#35780;&#20272;&#21644;&#32534;&#30721;&#12290;&#20316;&#32773;&#36890;&#36807;&#24341;&#29992;&#30340;&#26041;&#24335;&#22238;&#39038;&#20102;&#30456;&#20851;&#25991;&#29486;&#65292;&#24182;&#20171;&#32461;&#20102;&#19981;&#21516;&#30340;&#25688;&#35201;&#29983;&#25104;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36824;&#23545;&#21487;&#29992;&#20110;&#35780;&#20272;&#21644;&#25968;&#25454;&#35757;&#32451;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#24182;&#20351;&#29992;CNN&#35821;&#26009;&#24211;&#25968;&#25454;&#38598;&#23545;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#35777;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#33258;&#21160;&#25991;&#26412;&#25688;&#35201;&#31995;&#32479;&#30340;&#25991;&#29486;&#32508;&#36848;&#12290;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#24341;&#29992;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20174;&#25105;&#20204;&#24050;&#32463;&#25484;&#25569;&#30340;&#20851;&#20110;&#27599;&#20010;&#25105;&#20204;&#24819;&#28085;&#30422;&#30340;&#20027;&#39064;&#30340;&#19968;&#20123;&#27969;&#34892;&#21644;&#33879;&#21517;&#30340;&#35770;&#25991;&#24320;&#22987;&#65292;&#28982;&#21518;&#25105;&#20204;&#36861;&#36394;&#20102;&#8220;&#21521;&#21518;&#24341;&#29992;&#8221;&#65288;&#34987;&#25105;&#20204;&#20043;&#21069;&#30693;&#36947;&#30340;&#19968;&#31995;&#21015;&#35770;&#25991;&#24341;&#29992;&#30340;&#35770;&#25991;&#65289;&#21644;&#8220;&#21521;&#21069;&#24341;&#29992;&#8221;&#65288;&#24341;&#29992;&#25105;&#20204;&#20043;&#21069;&#30693;&#36947;&#30340;&#19968;&#31995;&#21015;&#35770;&#25991;&#30340;&#36739;&#26032;&#35770;&#25991;&#65289;&#12290;&#20026;&#20102;&#32452;&#32455;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21508;&#31181;&#22522;&#20110;&#19981;&#21516;&#26426;&#21046;&#29983;&#25104;&#25688;&#35201;&#30340;&#33258;&#21160;&#25991;&#26412;&#25688;&#35201;&#26041;&#27861;&#12290;&#38500;&#20102;&#20171;&#32461;&#26041;&#27861;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;&#21487;&#29992;&#20110;&#25688;&#35201;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#21644;&#29992;&#20110;&#35780;&#20272;&#25688;&#35201;&#36136;&#37327;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#22238;&#39038;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;CNN&#35821;&#26009;&#24211;&#25968;&#25454;&#38598;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#35777;&#25506;&#32034;&#65292;&#35813;&#25968;&#25454;&#38598;&#20026;&#25277;&#21462;&#24335;&#21644;&#29983;&#25104;&#24335;&#26041;&#27861;&#25552;&#20379;&#20102;&#37329;&#26631;&#20934;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide a literature review about Automatic Text Summarization (ATS) systems. We consider a citation-based approach. We start with some popular and well-known papers that we have in hand about each topic we want to cover and we have tracked the "backward citations" (papers that are cited by the set of papers we knew beforehand) and the "forward citations" (newer papers that cite the set of papers we knew beforehand). In order to organize the different methods, we present the diverse approaches to ATS guided by the mechanisms they use to generate a summary. Besides presenting the methods, we also present an extensive review of the datasets available for summarization tasks and the methods used to evaluate the quality of the summaries. Finally, we present an empirical exploration of these methods using the CNN Corpus dataset that provides golden summaries for extractive and abstractive methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#40065;&#26834;&#19988;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#33258;&#28982;&#35821;&#35328;&#35770;&#35777;&#20013;&#30340;&#36923;&#36753;&#35884;&#35823;&#12290;&#36890;&#36807;&#19977;&#38454;&#27573;&#30340;&#35780;&#20272;&#26694;&#26550;&#21644;&#19981;&#21516;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#32467;&#21512;&#35821;&#35328;&#27169;&#22411;&#21644;&#32972;&#26223;&#30693;&#35782;&#65292;&#26377;&#25928;&#22788;&#29702;&#20102;&#22823;&#37327;&#25968;&#25454;&#21644;&#25968;&#25454;&#31232;&#30095;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.07425</link><description>&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#35770;&#35777;&#20013;&#40065;&#26834;&#19988;&#21487;&#35299;&#37322;&#30340;&#35782;&#21035;&#36923;&#36753;&#35884;&#35823;
&lt;/p&gt;
&lt;p&gt;
Robust and Explainable Identification of Logical Fallacies in Natural Language Arguments. (arXiv:2212.07425v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#40065;&#26834;&#19988;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#33258;&#28982;&#35821;&#35328;&#35770;&#35777;&#20013;&#30340;&#36923;&#36753;&#35884;&#35823;&#12290;&#36890;&#36807;&#19977;&#38454;&#27573;&#30340;&#35780;&#20272;&#26694;&#26550;&#21644;&#19981;&#21516;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#32467;&#21512;&#35821;&#35328;&#27169;&#22411;&#21644;&#32972;&#26223;&#30693;&#35782;&#65292;&#26377;&#25928;&#22788;&#29702;&#20102;&#22823;&#37327;&#25968;&#25454;&#21644;&#25968;&#25454;&#31232;&#30095;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20114;&#32852;&#32593;&#26102;&#20195;&#65292;&#34394;&#20551;&#20449;&#24687;&#12289;&#23459;&#20256;&#21644;&#38169;&#35823;&#35770;&#35777;&#30340;&#20256;&#25773;&#29616;&#35937;&#24471;&#21040;&#20102;&#25918;&#22823;&#12290;&#32473;&#23450;&#25968;&#25454;&#37327;&#30340;&#24222;&#22823;&#21644;&#35782;&#21035;&#35770;&#35777;&#35268;&#33539;&#36829;&#35268;&#30340;&#24494;&#22937;&#24615;&#65292;&#20351;&#29992;&#21487;&#38752;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#36923;&#36753;&#35884;&#35823;&#26469;&#25903;&#25345;&#20449;&#24687;&#20998;&#26512;&#20219;&#21153;&#65288;&#22914;&#20869;&#23481;&#23457;&#26680;&#65289;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26412;&#25991;&#23558;&#20197;&#21069;&#20851;&#20110;&#36923;&#36753;&#35884;&#35823;&#30340;&#29702;&#35770;&#24037;&#20316;&#21046;&#23450;&#20026;&#26816;&#27979;&#12289;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#20998;&#31867;&#30340;&#32508;&#21512;&#19977;&#38454;&#27573;&#35780;&#20272;&#26694;&#26550;&#12290;&#25105;&#20204;&#38024;&#23545;&#35780;&#20272;&#30340;&#27599;&#20010;&#38454;&#27573;&#23545;&#29616;&#26377;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#36866;&#24212;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#22522;&#20110;&#21407;&#22411;&#25512;&#29702;&#12289;&#22522;&#20110;&#23454;&#20363;&#25512;&#29702;&#21644;&#30693;&#35782;&#27880;&#20837;&#30340;&#19977;&#20010;&#40065;&#26834;&#19988;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#26063;&#12290;&#36825;&#20123;&#26041;&#27861;&#32467;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#12289;&#32972;&#26223;&#30693;&#35782;&#21644;&#21487;&#35299;&#37322;&#30340;&#26426;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#35838;&#31243;&#23398;&#20064;&#30340;&#31574;&#30053;&#35299;&#20915;&#20102;&#25968;&#25454;&#31232;&#30095;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#19977;&#38454;&#27573;&#26694;&#26550;&#33258;&#28982;&#22320;&#24041;&#22266;&#20102;&#20197;&#21069;&#30340;&#25968;&#25454;&#38598;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The spread of misinformation, propaganda, and flawed argumentation has been amplified in the Internet era. Given the volume of data and the subtlety of identifying violations of argumentation norms, supporting information analytics tasks, like content moderation, with trustworthy methods that can identify logical fallacies is essential. In this paper, we formalize prior theoretical work on logical fallacies into a comprehensive three-stage evaluation framework of detection, coarse-grained, and fine-grained classification. We adapt existing evaluation datasets for each stage of the evaluation. We employ three families of robust and explainable methods based on prototype reasoning, instance-based reasoning, and knowledge injection. The methods combine language models with background knowledge and explainable mechanisms. Moreover, we address data sparsity with strategies for data augmentation and curriculum learning. Our three-stage framework natively consolidates prior datasets and metho
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#23569;&#26679;&#26412;&#20154;&#26426;&#20132;&#20114;&#35757;&#32451;&#31639;&#27861;Nano&#65292;&#29992;&#20110;&#25353;&#20219;&#24847;&#20998;&#24067;&#65288;&#23450;&#37327;&#21644;&#26410;&#23450;&#37327;&#65289;&#29983;&#25104;&#25991;&#26412;&#12290;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;Nano&#22312;&#21333;&#19968;&#20027;&#39064;/&#23646;&#24615;&#20197;&#21450;&#23450;&#37327;&#20998;&#24067;&#25511;&#21046;&#26041;&#38754;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.05750</link><description>&lt;p&gt;
Nano: &#23884;&#22871;&#30340;&#20154;&#26426;&#20132;&#20114;&#22870;&#21169;&#23398;&#20064;&#29992;&#20110;&#23569;&#26679;&#26412;&#35821;&#35328;&#27169;&#22411;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Nano: Nested Human-in-the-Loop Reward Learning for Few-shot Language Model Control. (arXiv:2211.05750v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#23569;&#26679;&#26412;&#20154;&#26426;&#20132;&#20114;&#35757;&#32451;&#31639;&#27861;Nano&#65292;&#29992;&#20110;&#25353;&#20219;&#24847;&#20998;&#24067;&#65288;&#23450;&#37327;&#21644;&#26410;&#23450;&#37327;&#65289;&#29983;&#25104;&#25991;&#26412;&#12290;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;Nano&#22312;&#21333;&#19968;&#20027;&#39064;/&#23646;&#24615;&#20197;&#21450;&#23450;&#37327;&#20998;&#24067;&#25511;&#21046;&#26041;&#38754;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#23637;&#31034;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30495;&#23454;&#19990;&#30028;&#30340;&#20219;&#21153;&#32463;&#24120;&#38656;&#35201;&#25511;&#21046;&#29983;&#25104;&#25991;&#26412;&#30340;&#20998;&#24067;&#65292;&#20197;&#20943;&#36731;&#20559;&#35265;&#12289;&#20419;&#36827;&#20844;&#24179;&#24615;&#21644;&#23454;&#29616;&#20010;&#24615;&#21270;&#12290;&#29616;&#26377;&#30340;&#25991;&#26412;&#20998;&#24067;&#25511;&#21046;&#25216;&#26415;&#21482;&#36866;&#29992;&#20110;&#23450;&#37327;&#20998;&#24067;&#65292;&#36825;&#35201;&#27714;&#39044;&#20808;&#23450;&#20041;&#30340;&#31867;&#21035;&#12289;&#20998;&#24067;&#27604;&#20363;&#25110;&#31526;&#21512;&#25152;&#38656;&#20998;&#24067;&#30340;&#29616;&#26377;&#35821;&#26009;&#24211;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#37325;&#35201;&#30340;&#20998;&#24067;&#65292;&#22914;&#20010;&#20154;&#20559;&#22909;&#65292;&#26159;&#26410;&#23450;&#37327;&#21270;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;Nano&#65292;&#19968;&#20010;&#23569;&#26679;&#26412;&#20154;&#26426;&#20132;&#20114;&#35757;&#32451;&#31639;&#27861;&#65292;&#19981;&#26029;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#65292;&#26469;&#35299;&#20915;&#25353;&#20219;&#24847;&#20998;&#24067;&#65288;&#23450;&#37327;&#21644;&#26410;&#23450;&#37327;&#65289;&#29983;&#25104;&#25991;&#26412;&#30340;&#38382;&#39064;&#12290;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;Nano&#22312;&#21333;&#19968;&#20027;&#39064;/&#23646;&#24615;&#20197;&#21450;&#23450;&#37327;&#20998;&#24067;&#25511;&#21046;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;Nano&#33021;&#22815;&#23398;&#20064;&#26410;&#23450;&#37327;&#21270;&#30340;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models have demonstrated extraordinary capabilities in language generation. However, real-world tasks often require controlling the distribution of generated text in order to mitigate bias, promote fairness, and achieve personalization. Existing techniques for controlling the distribution of generated text only work with quantified distributions, which require pre-defined categories, proportions of the distribution, or an existing corpus following the desired distributions. However, many important distributions, such as personal preferences, are unquantified. In this work, we tackle the problem of generating text following arbitrary distributions (quantified and unquantified) by proposing Nano, a few-shot human-in-the-loop training algorithm that continuously learns from human feedback. Nano achieves state-of-the-art results on single topic/attribute as well as quantified distribution control compared to previous works. We also show that Nano is able to learn unquan
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;CC-Riddle&#30340;&#20013;&#25991;&#23383;&#35868;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#35206;&#30422;&#20102;&#22823;&#22810;&#25968;&#24120;&#35265;&#30340;&#31616;&#20307;&#20013;&#25991;&#23383;&#31526;&#12290;&#35813;&#25968;&#25454;&#38598;&#30340;&#26500;&#24314;&#36807;&#31243;&#32467;&#21512;&#20102;&#32593;&#32476;&#29228;&#34411;&#12289;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21644;&#25163;&#21160;&#36807;&#28388;&#65292;&#20026;&#35299;&#20915;&#20013;&#25991;&#23383;&#35868;&#38382;&#39064;&#25552;&#20379;&#20102;&#37325;&#35201;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2206.13778</link><description>&lt;p&gt;
CC-Riddle&#65306;&#19968;&#20010;&#20013;&#25991;&#23383;&#35868;&#38382;&#31572;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CC-Riddle: A Question Answering Dataset of Chinese Character Riddles. (arXiv:2206.13778v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.13778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;CC-Riddle&#30340;&#20013;&#25991;&#23383;&#35868;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#35206;&#30422;&#20102;&#22823;&#22810;&#25968;&#24120;&#35265;&#30340;&#31616;&#20307;&#20013;&#25991;&#23383;&#31526;&#12290;&#35813;&#25968;&#25454;&#38598;&#30340;&#26500;&#24314;&#36807;&#31243;&#32467;&#21512;&#20102;&#32593;&#32476;&#29228;&#34411;&#12289;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21644;&#25163;&#21160;&#36807;&#28388;&#65292;&#20026;&#35299;&#20915;&#20013;&#25991;&#23383;&#35868;&#38382;&#39064;&#25552;&#20379;&#20102;&#37325;&#35201;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20013;&#25991;&#23383;&#35868;&#26159;&#20013;&#25991;&#29305;&#26377;&#30340;&#19968;&#31181;&#25991;&#21270;&#23089;&#20048;&#24418;&#24335;&#65292;&#36890;&#24120;&#21253;&#25324;&#35868;&#35821;&#25551;&#36848;&#21644;&#35868;&#24213;&#20004;&#20010;&#37096;&#20998;&#12290;&#35868;&#24213;&#26159;&#19968;&#20010;&#21333;&#23383;&#65292;&#32780;&#35868;&#35821;&#25551;&#36848;&#20027;&#35201;&#25551;&#36848;&#20102;&#35868;&#24213;&#30340;&#23383;&#24418;&#65292;&#26377;&#26102;&#36824;&#20250;&#38468;&#24102;&#35299;&#37322;&#21644;&#21457;&#38899;&#12290;&#35299;&#20915;&#20013;&#25991;&#23383;&#35868;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#35201;&#27714;&#29702;&#35299;&#23383;&#24418;&#12289;&#26222;&#36890;&#30693;&#35782;&#21644;&#25484;&#25569;&#27604;&#21947;&#35821;&#35328;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;CC-Riddle&#30340;&#20013;&#25991;&#23383;&#35868;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#22823;&#22810;&#25968;&#24120;&#35265;&#30340;&#31616;&#20307;&#20013;&#25991;&#23383;&#31526;&#12290;&#26500;&#24314;&#36807;&#31243;&#32467;&#21512;&#20102;&#32593;&#32476;&#29228;&#34411;&#12289;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21644;&#25163;&#21160;&#36807;&#28388;&#12290;&#22312;&#29983;&#25104;&#38454;&#27573;&#65292;&#25105;&#20204;&#23558;&#35868;&#24213;&#23383;&#30340;&#27721;&#35821;&#25340;&#38899;&#12289;&#23383;&#24418;&#21644;&#21547;&#20041;&#36755;&#20837;&#21040;&#29983;&#25104;&#27169;&#22411;&#20013;&#65292;&#27169;&#22411;&#20250;&#29983;&#25104;&#22810;&#20010;&#35868;&#35821;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Chinese character riddle is a unique form of cultural entertainment specific to the Chinese language. It typically comprises two parts: the riddle description and the solution. The solution to the riddle is a single character, while the riddle description primarily describes the glyph of the solution, occasionally supplemented with its explanation and pronunciation. Solving Chinese character riddles is a challenging task that demands understanding of character glyph, general knowledge, and a grasp of figurative language. In this paper, we construct a \textbf{C}hinese \textbf{C}haracter riddle dataset named CC-Riddle, which covers the majority of common simplified Chinese characters. The construction process is a combination of web crawling, language model generation and manual filtering. In generation stage, we input the Chinese phonetic alphabet, glyph and meaning of the solution character into the generation model, which then produces multiple riddle descriptions. The generated r
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#19988;&#39640;&#24615;&#33021;&#30340;&#27169;&#22411;&#65292;&#22522;&#20110;XGBoost&#31639;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#30340;&#20167;&#24680;&#21644;&#20882;&#29359;&#24615;&#35328;&#35770;&#12290;&#35813;&#27169;&#22411;&#22312;&#19981;&#24179;&#34913;&#30340;Twitter&#25968;&#25454;&#19978;&#26174;&#31034;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#38477;&#37319;&#26679;&#21518;&#30340;&#25968;&#25454;&#20013;&#20063;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.12983</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#19988;&#39640;&#24615;&#33021;&#30340;&#20167;&#24680;&#21644;&#20882;&#29359;&#24615;&#35328;&#35770;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Explainable and High-Performance Hate and Offensive Speech Detection. (arXiv:2206.12983v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.12983
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#19988;&#39640;&#24615;&#33021;&#30340;&#27169;&#22411;&#65292;&#22522;&#20110;XGBoost&#31639;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#30340;&#20167;&#24680;&#21644;&#20882;&#29359;&#24615;&#35328;&#35770;&#12290;&#35813;&#27169;&#22411;&#22312;&#19981;&#24179;&#34913;&#30340;Twitter&#25968;&#25454;&#19978;&#26174;&#31034;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#38477;&#37319;&#26679;&#21518;&#30340;&#25968;&#25454;&#20013;&#20063;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#20449;&#24687;&#30340;&#20256;&#25773;&#21487;&#33021;&#20250;&#22312;&#33030;&#24369;&#31038;&#32676;&#20013;&#21019;&#36896;&#25932;&#23545;&#30340;&#29615;&#22659;&#65292;&#24182;&#20351;&#26576;&#20123;&#32676;&#20307;&#27785;&#40664;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#24773;&#20917;&#65292;&#24050;&#24320;&#21457;&#20102;&#22810;&#20010;&#27169;&#22411;&#26469;&#26816;&#27979;&#20167;&#24680;&#21644;&#20882;&#29359;&#24615;&#35328;&#35770;&#12290;&#30001;&#20110;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#26816;&#27979;&#20167;&#24680;&#21644;&#20882;&#29359;&#24615;&#35328;&#35770;&#21487;&#33021;&#38169;&#35823;&#22320;&#23558;&#20010;&#20307;&#25490;&#38500;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#20043;&#22806;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#20449;&#20219;&#24230;&#65292;&#22240;&#27492;&#26377;&#24517;&#35201;&#21019;&#24314;&#21487;&#35299;&#37322;&#19988;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22522;&#20110;XGBoost&#31639;&#27861;&#26500;&#24314;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#19988;&#21487;&#35299;&#37322;&#30340;&#39640;&#24615;&#33021;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;Twitter&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#23545;&#20110;&#19981;&#24179;&#34913;&#30340;Twitter&#25968;&#25454;&#65292;&#30456;&#27604;&#20110;LSTM&#12289;AutoGluon&#21644;ULMFiT&#27169;&#22411;&#30340;F1&#24471;&#20998;&#20998;&#21035;&#20026;0.38&#21644;0.37&#65292;&#20197;&#21450;0.38&#65292;XGBoost&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26041;&#38754;&#30340;F1&#24471;&#20998;&#20026;0.75&#65292;&#34920;&#29616;&#26356;&#22909;&#12290;&#24403;&#25105;&#20204;&#23558;&#25968;&#25454;&#38477;&#37319;&#26679;&#20026;&#32422;5000&#26465;&#25512;&#25991;&#30340;&#19977;&#20010;&#29420;&#31435;&#31867;&#21035;&#26102;&#65292;XGBoost&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26041;&#38754;&#30340;F1&#24471;&#20998;&#20063;&#20248;&#20110;LSTM&#12289;AutoGluon&#21644;ULMFiT&#65292;&#20026;0.79&#12290;
&lt;/p&gt;
&lt;p&gt;
The spread of information through social media platforms can create environments possibly hostile to vulnerable communities and silence certain groups in society. To mitigate such instances, several models have been developed to detect hate and offensive speech. Since detecting hate and offensive speech in social media platforms could incorrectly exclude individuals from social media platforms, which can reduce trust, there is a need to create explainable and interpretable models. Thus, we build an explainable and interpretable high performance model based on the XGBoost algorithm, trained on Twitter data. For unbalanced Twitter data, XGboost outperformed the LSTM, AutoGluon, and ULMFiT models on hate speech detection with an F1 score of 0.75 compared to 0.38 and 0.37, and 0.38 respectively. When we down-sampled the data to three separate classes of approximately 5000 tweets, XGBoost performed better than LSTM, AutoGluon, and ULMFiT; with F1 scores for hate speech detection of 0.79 vs 
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#25945;&#31243;&#20171;&#32461;&#20102;&#20351;&#29992;Transformer&#27169;&#22411;&#30340;&#26041;&#27861;&#23558;&#25991;&#26412;&#25968;&#25454;&#24212;&#29992;&#20110;&#31934;&#31639;&#23398;&#20013;&#30340;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#22810;&#35821;&#35328;&#21644;&#38271;&#36755;&#20837;&#24207;&#21015;&#30340;&#24773;&#20917;&#19979;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;&#35813;&#25945;&#31243;&#36824;&#25552;&#20379;&#20102;&#22788;&#29702;&#26080;&#26631;&#35760;&#25968;&#25454;&#24773;&#20917;&#19979;&#30340;&#20998;&#31867;&#20219;&#21153;&#30340;&#23454;&#29992;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2206.02014</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#31934;&#31639;&#23398;&#20013;&#30340;&#24212;&#29992;&#65306;&#20351;&#29992;Transformer&#30340;&#26696;&#20363;&#30740;&#31350;&#26469;&#22788;&#29702;&#25991;&#26412;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Actuarial Applications of Natural Language Processing Using Transformers: Case Studies for Using Text Features in an Actuarial Context. (arXiv:2206.02014v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02014
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#25945;&#31243;&#20171;&#32461;&#20102;&#20351;&#29992;Transformer&#27169;&#22411;&#30340;&#26041;&#27861;&#23558;&#25991;&#26412;&#25968;&#25454;&#24212;&#29992;&#20110;&#31934;&#31639;&#23398;&#20013;&#30340;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#22810;&#35821;&#35328;&#21644;&#38271;&#36755;&#20837;&#24207;&#21015;&#30340;&#24773;&#20917;&#19979;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;&#35813;&#25945;&#31243;&#36824;&#25552;&#20379;&#20102;&#22788;&#29702;&#26080;&#26631;&#35760;&#25968;&#25454;&#24773;&#20917;&#19979;&#30340;&#20998;&#31867;&#20219;&#21153;&#30340;&#23454;&#29992;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25945;&#31243;&#28436;&#31034;&#20102;&#22914;&#20309;&#23558;&#25991;&#26412;&#25968;&#25454;&#24212;&#29992;&#20110;&#31934;&#31639;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#20013;&#65292;&#20027;&#35201;&#20851;&#27880;&#20351;&#29992;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#20351;&#29992;&#38271;&#24230;&#24179;&#22343;&#20026;400&#20010;&#21333;&#35789;&#30340;&#36710;&#31096;&#25551;&#36848;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;&#33521;&#35821;&#21644;&#24503;&#35821;&#65289;&#20197;&#21450;&#30701;&#26399;&#36130;&#20135;&#20445;&#38505;&#32034;&#36180;&#25551;&#36848;&#25968;&#25454;&#38598;&#26469;&#23637;&#31034;&#36825;&#20123;&#25216;&#26415;&#12290;&#26696;&#20363;&#30740;&#31350;&#35299;&#20915;&#20102;&#22810;&#35821;&#35328;&#29615;&#22659;&#21644;&#38271;&#36755;&#20837;&#24207;&#21015;&#30340;&#25361;&#25112;&#65292;&#24182;&#23637;&#31034;&#20102;&#35299;&#37322;&#27169;&#22411;&#36755;&#20986;&#12289;&#35780;&#20272;&#21644;&#25913;&#36827;&#27169;&#22411;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#23558;&#27169;&#22411;&#24494;&#35843;&#21040;&#29305;&#23450;&#24212;&#29992;&#39046;&#22495;&#25110;&#29305;&#23450;&#39044;&#27979;&#20219;&#21153;&#12290;&#26368;&#21518;&#65292;&#26412;&#25945;&#31243;&#25552;&#20379;&#20102;&#22788;&#29702;&#22312;&#27809;&#26377;&#25110;&#21482;&#26377;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20998;&#31867;&#20219;&#21153;&#30340;&#23454;&#29992;&#26041;&#27861;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;ChatGPT&#12290;&#36890;&#36807;&#20165;&#36827;&#34892;&#26368;&#23567;&#30340;&#39044;&#22788;&#29702;&#21644;&#24494;&#35843;&#65292;&#21033;&#29992;&#29616;&#25104;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#21487;&#20197;&#21462;&#24471;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This tutorial demonstrates workflows to incorporate text data into actuarial classification and regression tasks. The main focus is on methods employing transformer-based models. A dataset of car accident descriptions with an average length of 400 words, available in English and German, and a dataset with short property insurance claims descriptions are used to demonstrate these techniques. The case studies tackle challenges related to a multi-lingual setting and long input sequences. They also show ways to interpret model output, to assess and improve model performance, by fine-tuning the models to the domain of application or to a specific prediction task. Finally, the tutorial provides practical approaches to handle classification tasks in situations with no or only few labeled data, including but not limited to ChatGPT. The results achieved by using the language-understanding skills of off-the-shelf natural language processing (NLP) models with only minimal pre-processing and fine-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20027;&#39064;&#24314;&#27169;&#21644;&#30456;&#23545;&#23494;&#24230;&#20272;&#35745;&#26469;&#36827;&#34892;&#29359;&#32618;&#28909;&#28857;&#24314;&#27169;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#25429;&#25417;&#21040;&#34987;&#35843;&#24230;&#21592;&#24573;&#35270;&#30340;&#22320;&#29702;&#28909;&#28857;&#36235;&#21183;&#65292;&#36825;&#20123;&#28909;&#28857;&#36235;&#21183;&#24448;&#24448;&#19982;&#25972;&#20307;&#20107;&#20214;&#23494;&#24230;&#30340;&#22686;&#21152;&#30456;&#28151;&#28102;&#12290;</title><link>http://arxiv.org/abs/2202.04176</link><description>&lt;p&gt;
&#36890;&#36807;&#20027;&#39064;&#24314;&#27169;&#21644;&#30456;&#23545;&#23494;&#24230;&#20272;&#35745;&#30340;&#29359;&#32618;&#28909;&#28857;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Crime Hot-Spot Modeling via Topic Modeling and Relative Density Estimation. (arXiv:2202.04176v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.04176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20027;&#39064;&#24314;&#27169;&#21644;&#30456;&#23545;&#23494;&#24230;&#20272;&#35745;&#26469;&#36827;&#34892;&#29359;&#32618;&#28909;&#28857;&#24314;&#27169;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#25429;&#25417;&#21040;&#34987;&#35843;&#24230;&#21592;&#24573;&#35270;&#30340;&#22320;&#29702;&#28909;&#28857;&#36235;&#21183;&#65292;&#36825;&#20123;&#28909;&#28857;&#36235;&#21183;&#24448;&#24448;&#19982;&#25972;&#20307;&#20107;&#20214;&#23494;&#24230;&#30340;&#22686;&#21152;&#30456;&#28151;&#28102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#29359;&#32618;&#35760;&#24405;&#21465;&#36848;&#30340;&#38598;&#21512;&#36827;&#34892;&#20027;&#39064;&#20998;&#24067;&#30340;&#35745;&#31639;&#65292;&#30830;&#23450;&#30456;&#20284;&#21628;&#21483;&#30340;&#20998;&#32452;&#20197;&#21450;&#23427;&#20204;&#30340;&#30456;&#23545;&#31354;&#38388;&#20998;&#24067;&#12290;&#25105;&#20204;&#39318;&#20808;&#20026;&#27599;&#20010;&#21465;&#36848;&#33719;&#21462;&#19968;&#20010;&#20027;&#39064;&#20998;&#24067;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#36817;&#37051;&#30456;&#23545;&#23494;&#24230;&#20272;&#35745;&#65288;kNN-RDE&#65289;&#26041;&#27861;&#26469;&#33719;&#24471;&#27599;&#20010;&#20027;&#39064;&#30340;&#31354;&#38388;&#30456;&#23545;&#23494;&#24230;&#12290;&#22312;&#20122;&#29305;&#20848;&#22823;&#35686;&#23519;&#23616;&#30340;&#22823;&#37327;&#21465;&#36848;&#25991;&#26723;&#65288;$n=475,019$&#65289;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#21040;&#36890;&#24120;&#34987;&#21628;&#21483;&#35843;&#24230;&#21592;&#19968;&#24320;&#22987;&#27809;&#26377;&#23519;&#35273;&#21040;&#30340;&#22320;&#29702;&#28909;&#28857;&#36235;&#21183;&#65292;&#36825;&#20123;&#36235;&#21183;&#30001;&#20110;&#19982;&#19968;&#33324;&#20107;&#20214;&#23494;&#24230;&#30340;&#28151;&#28102;&#32780;&#34987;&#24573;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a method to capture groupings of similar calls and determine their relative spatial distribution from a collection of crime record narratives. We first obtain a topic distribution for each narrative, and then propose a nearest neighbors relative density estimation (kNN-RDE) approach to obtain spatial relative densities per topic. Experiments over a large corpus ($n=475,019$) of narrative documents from the Atlanta Police Department demonstrate the viability of our method in capturing geographic hot-spot trends which call dispatchers do not initially pick up on and which go unnoticed due to conflation with elevated event density in general.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#25512;&#29305;&#19978;&#26816;&#27979;&#40657;&#20154;&#29983;&#21629;&#33267;&#20851;&#37325;&#35201;&#35805;&#39064;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;BLM-17m&#65292;&#28085;&#30422;&#20102;&#20052;&#27835;&#183;&#24343;&#27931;&#20234;&#24503;&#20107;&#20214;&#26399;&#38388;&#30340;17&#30334;&#19975;&#25512;&#25991;&#12290;&#20316;&#32773;&#25552;&#20379;&#20102;&#20004;&#20010;&#22522;&#32447;&#27169;&#22411;TF-IDF&#21644;LDA&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2105.01331</link><description>&lt;p&gt;
BLM-17m: &#19968;&#20010;&#29992;&#20110;&#25512;&#29305;&#19978;&#40657;&#20154;&#29983;&#21629;&#33267;&#20851;&#37325;&#35201;&#35805;&#39064;&#26816;&#27979;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
BLM-17m: A Large-Scale Dataset for Black Lives Matter Topic Detection on Twitter. (arXiv:2105.01331v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.01331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#25512;&#29305;&#19978;&#26816;&#27979;&#40657;&#20154;&#29983;&#21629;&#33267;&#20851;&#37325;&#35201;&#35805;&#39064;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;BLM-17m&#65292;&#28085;&#30422;&#20102;&#20052;&#27835;&#183;&#24343;&#27931;&#20234;&#24503;&#20107;&#20214;&#26399;&#38388;&#30340;17&#30334;&#19975;&#25512;&#25991;&#12290;&#20316;&#32773;&#25552;&#20379;&#20102;&#20004;&#20010;&#22522;&#32447;&#27169;&#22411;TF-IDF&#21644;LDA&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#26435;&#20445;&#25252;&#26159;&#19990;&#30028;&#19978;&#26368;&#37325;&#35201;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#28085;&#30422;&#26368;&#36817;&#20960;&#20010;&#26376;&#20840;&#29699;&#24433;&#21709;&#28145;&#36828;&#30340;&#20154;&#26435;&#30683;&#30462;&#20043;&#19968;&#8212;&#8212;&#20052;&#27835;&#183;&#24343;&#27931;&#20234;&#24503;&#20107;&#20214;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;17&#30334;&#19975;&#25512;&#25991;&#30340;&#20027;&#39064;&#26816;&#27979;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;&#36825;&#20123;&#25512;&#25991;&#26159;&#20174;2020&#24180;5&#26376;25&#26085;&#33267;2020&#24180;8&#26376;21&#26085;&#25910;&#38598;&#30340;&#65292;&#28085;&#30422;&#20102;&#36825;&#19968;&#20107;&#20214;&#24320;&#22987;&#21518;&#30340;89&#22825;&#12290;&#25105;&#20204;&#36890;&#36807;&#30417;&#27979;&#20840;&#29699;&#21644;&#26412;&#22320;&#25253;&#32440;&#30340;&#26368;&#28909;&#38376;&#26032;&#38395;&#20027;&#39064;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#26631;&#35760;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20004;&#20010;&#22522;&#32447;&#27169;&#22411;&#65292;TF-IDF&#21644;LDA&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#20010;&#19981;&#21516;&#30340;k&#20540;&#23545;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#20998;&#25968;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#22312;https://github.com/MeysamAsgariC/BLMT &#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Protection of human rights is one of the most important problems of our world. In this paper, our aim is to provide a dataset which covers one of the most significant human rights contradiction in recent months affected the whole world, George Floyd incident. We propose a labeled dataset for topic detection that contains 17 million tweets. These Tweets are collected from 25 May 2020 to 21 August 2020 that covers 89 days from start of this incident. We labeled the dataset by monitoring most trending news topics from global and local newspapers. Apart from that, we present two baselines, TF-IDF and LDA. We evaluated the results of these two methods with three different k values for metrics of precision, recall and f1-score. The collected dataset is available at https://github.com/MeysamAsgariC/BLMT.
&lt;/p&gt;</description></item></channel></rss>