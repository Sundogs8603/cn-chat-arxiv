<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>CodeBPE&#30740;&#31350;&#20102;&#29992;&#20110;&#28304;&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#20013;&#19981;&#21516;&#23376;&#26631;&#35760;&#21270;&#36873;&#39033;&#30340;&#24433;&#21709;&#65292;&#25214;&#20986;&#20102;&#26368;&#26377;&#25928;&#21644;&#38271;&#24230;&#39640;&#25928;&#30340;&#23376;&#26631;&#35760;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20943;&#23569;&#24179;&#22343;&#38271;&#24230;17%&#19988;&#19981;&#24433;&#21709;&#19979;&#28216;&#24615;&#33021;&#65292;&#21487;&#33021;&#25552;&#39640;&#36136;&#37327;0.5-2%&#12290;</title><link>http://arxiv.org/abs/2308.00683</link><description>&lt;p&gt;
CodeBPE: &#25506;&#32034;&#29992;&#20110;&#28304;&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#23376;&#26631;&#35760;&#21270;&#36873;&#39033;
&lt;/p&gt;
&lt;p&gt;
CodeBPE: Investigating Subtokenization Options for Large Language Model Pretraining on Source Code. (arXiv:2308.00683v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00683
&lt;/p&gt;
&lt;p&gt;
CodeBPE&#30740;&#31350;&#20102;&#29992;&#20110;&#28304;&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#20013;&#19981;&#21516;&#23376;&#26631;&#35760;&#21270;&#36873;&#39033;&#30340;&#24433;&#21709;&#65292;&#25214;&#20986;&#20102;&#26368;&#26377;&#25928;&#21644;&#38271;&#24230;&#39640;&#25928;&#30340;&#23376;&#26631;&#35760;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20943;&#23569;&#24179;&#22343;&#38271;&#24230;17%&#19988;&#19981;&#24433;&#21709;&#19979;&#28216;&#24615;&#33021;&#65292;&#21487;&#33021;&#25552;&#39640;&#36136;&#37327;0.5-2%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#24191;&#27867;&#37319;&#29992;&#20102;&#38024;&#23545;&#28304;&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#65292;&#25552;&#20986;&#20102;&#28304;&#20195;&#30721;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#24182;&#30740;&#31350;&#20102;&#19981;&#21516;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#22312;&#28304;&#20195;&#30721;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#21478;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#65292;&#21363;&#19981;&#21516;&#23376;&#26631;&#35760;&#21270;&#36873;&#39033;&#30340;&#24433;&#21709;&#65292;&#24182;&#26088;&#22312;&#30830;&#23450;&#26368;&#26377;&#25928;&#21644;&#38271;&#24230;&#39640;&#25928;&#30340;&#23376;&#26631;&#35760;&#21270;&#65292;&#32771;&#34385;&#21040;&#20195;&#30721;&#30340;&#29305;&#27530;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23376;&#26631;&#35760;&#21270;&#26041;&#27861;&#65292;&#24179;&#22343;&#38271;&#24230;&#20943;&#23569;&#20102;17%&#65292;&#19988;&#27809;&#26377;&#19979;&#28216;&#24615;&#33021;&#19979;&#38477;&#65292;&#24182;&#19988;&#34920;&#26126;&#31934;&#24515;&#36873;&#25321;&#30340;&#23376;&#26631;&#35760;&#21270;&#21487;&#33021;&#20250;&#25552;&#39640;&#36136;&#37327;0.5-2%&#65292;&#21487;&#33021;&#20250;&#30053;&#24494;&#22686;&#21152;&#38271;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have widely adopted large language model pretraining for source code, suggested source code-specific pretraining objectives and investigated the applicability of various Transformer-based language model architectures for source code. This work investigates another important aspect of such models, namely the effect of different subtokenization options, and aims at identifying most effective and length-efficient subtokenizations, taking into account code specifics. We propose subtokenziation that reduces average length by 17% without downstream performance drop, and show that a carefully chosen subtokenization may improve quality by 0.5-2%, possibly with some length increase.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#20351;&#29992;&#24037;&#20855;&#25991;&#26723;&#20316;&#20026;&#25945;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#26032;&#24037;&#20855;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#35777;&#26126;&#65292;&#20165;&#20351;&#29992;&#24037;&#20855;&#25991;&#26723;&#30340;&#38646;-shot&#25552;&#31034;&#36275;&#20197;&#23454;&#29616;&#27491;&#30830;&#30340;&#24037;&#20855;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.00675</link><description>&lt;p&gt;
&#24037;&#20855;&#25991;&#26723;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#38646;-shot&#24037;&#20855;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models. (arXiv:2308.00675v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00675
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#20351;&#29992;&#24037;&#20855;&#25991;&#26723;&#20316;&#20026;&#25945;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#26032;&#24037;&#20855;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#35777;&#26126;&#65292;&#20165;&#20351;&#29992;&#24037;&#20855;&#25991;&#26723;&#30340;&#38646;-shot&#25552;&#31034;&#36275;&#20197;&#23454;&#29616;&#27491;&#30830;&#30340;&#24037;&#20855;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#36890;&#36807;&#25552;&#20379;&#19968;&#20123;&#24037;&#20855;&#20351;&#29992;&#30340;&#28436;&#31034;&#26469;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20351;&#29992;&#26032;&#24037;&#20855;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#28436;&#31034;&#24456;&#38590;&#33719;&#24471;&#65292;&#24182;&#19988;&#22914;&#26524;&#36873;&#25321;&#20102;&#38169;&#35823;&#30340;&#28436;&#31034;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#33391;&#30340;&#20559;&#35265;&#20351;&#29992;&#12290;&#21363;&#20351;&#22312;&#32597;&#35265;&#30340;&#24773;&#20917;&#19979;&#65292;&#28436;&#31034;&#26159;readily available&#30340;&#65292;&#20063;&#27809;&#26377;&#21407;&#21017;&#24615;&#30340;&#36873;&#25321;&#21327;&#35758;&#26469;&#30830;&#23450;&#25552;&#20379;&#22810;&#23569;&#20010;&#21644;&#21738;&#20123;&#28436;&#31034;&#12290;&#38543;&#30528;&#20219;&#21153;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#65292;&#36873;&#25321;&#25628;&#32034;&#32452;&#21512;&#25968;&#30340;&#22686;&#38271;&#25104;&#20026;&#19981;&#21487;&#22788;&#29702;&#30340;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#19968;&#31181;&#26367;&#20195;&#28436;&#31034;&#30340;&#26041;&#27861;&#65306;&#24037;&#20855;&#25991;&#26723;&#12290;&#25105;&#20204;&#20027;&#24352;&#20351;&#29992;&#24037;&#20855;&#25991;&#26723;&#26469;&#25551;&#36848;&#21508;&#20010;&#24037;&#20855;&#30340;&#20351;&#29992;&#65292;&#32780;&#19981;&#26159;&#28436;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#36328;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24577;&#30340;6&#20010;&#20219;&#21153;&#19978;&#30340;&#19977;&#20010;&#20027;&#35201;&#23454;&#35777;&#21457;&#29616;&#25903;&#25345;&#25105;&#20204;&#30340;&#20027;&#24352;&#12290;&#39318;&#20808;&#65292;&#22312;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#65292;&#20165;&#20351;&#29992;&#24037;&#20855;&#25991;&#26723;&#30340;&#38646;-shot&#25552;&#31034;&#36275;&#20197;&#24341;&#20986;&#27491;&#30830;&#30340;&#24037;&#20855;&#20351;&#29992;&#65292;&#36798;&#21040;&#20102;few-shot&#25552;&#31034;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Today, large language models (LLMs) are taught to use new tools by providing a few demonstrations of the tool's usage. Unfortunately, demonstrations are hard to acquire, and can result in undesirable biased usage if the wrong demonstration is chosen. Even in the rare scenario that demonstrations are readily available, there is no principled selection protocol to determine how many and which ones to provide. As tasks grow more complex, the selection search grows combinatorially and invariably becomes intractable. Our work provides an alternative to demonstrations: tool documentation. We advocate the use of tool documentation, descriptions for the individual tool usage, over demonstrations. We substantiate our claim through three main empirical findings on 6 tasks across both vision and language modalities. First, on existing benchmarks, zero-shot prompts with only tool documentation are sufficient for eliciting proper tool usage, achieving performance on par with few-shot prompts. Secon
&lt;/p&gt;</description></item><item><title>JIANG&#26159;&#19968;&#20010;&#19987;&#20026;&#20013;&#25991;&#35774;&#35745;&#30340;&#24320;&#25918;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#37327;&#30340;&#20013;&#25991;&#35821;&#26009;&#24211;&#36827;&#34892;&#35757;&#32451;&#21644;&#20248;&#21270;&#32467;&#26500;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#22312;&#20013;&#25991;&#20013;&#21457;&#25381;&#20854;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.00624</link><description>&lt;p&gt;
JIANG: &#20013;&#22269;&#24320;&#25918;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
JIANG: Chinese Open Foundation Language Model. (arXiv:2308.00624v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00624
&lt;/p&gt;
&lt;p&gt;
JIANG&#26159;&#19968;&#20010;&#19987;&#20026;&#20013;&#25991;&#35774;&#35745;&#30340;&#24320;&#25918;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#37327;&#30340;&#20013;&#25991;&#35821;&#26009;&#24211;&#36827;&#34892;&#35757;&#32451;&#21644;&#20248;&#21270;&#32467;&#26500;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#22312;&#20013;&#25991;&#20013;&#21457;&#25381;&#20854;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#23427;&#23637;&#31034;&#20102;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#19968;&#25104;&#23601;&#24341;&#36215;&#20102;&#20844;&#21496;&#21644;&#31185;&#30740;&#26426;&#26500;&#30340;&#26497;&#22823;&#20852;&#36259;&#65292;&#23548;&#33268;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#36827;&#34892;&#20102;&#22823;&#37327;&#25237;&#36164;&#12290;&#34429;&#28982;&#22312;&#36825;&#20010;&#26102;&#26399;&#20986;&#29616;&#20102;&#35768;&#22810;&#22823;&#22411;&#27169;&#22411;&#65292;&#20294;&#20854;&#20013;&#22823;&#22810;&#25968;&#20027;&#35201;&#26159;&#22522;&#20110;&#33521;&#25991;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#34429;&#28982;&#23427;&#20204;&#22312;&#20854;&#20182;&#35821;&#35328;&#65288;&#22914;&#20013;&#25991;&#65289;&#20013;&#34920;&#29616;&#20986;&#20102;&#19981;&#38169;&#30340;&#24615;&#33021;&#65292;&#20294;&#30001;&#20110;&#35789;&#27719;&#35774;&#35745;&#21644;&#35757;&#32451;&#35821;&#26009;&#24211;&#31561;&#22240;&#32032;&#65292;&#20854;&#28508;&#21147;&#20173;&#28982;&#21463;&#38480;&#65292;&#26080;&#27861;&#23436;&#20840;&#21457;&#25381;&#22312;&#20013;&#25991;&#20013;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21517;&#20026;JIANG&#65288;&#23004;&#30340;&#25340;&#38899;&#65289;&#30340;&#19987;&#38376;&#38024;&#23545;&#20013;&#25991;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#22823;&#37327;&#30340;&#20013;&#25991;&#35821;&#26009;&#24211;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#23545;&#20854;&#32467;&#26500;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
With the advancements in large language model technology, it has showcased capabilities that come close to those of human beings across various tasks. This achievement has garnered significant interest from companies and scientific research institutions, leading to substantial investments in the research and development of these models. While numerous large models have emerged during this period, the majority of them have been trained primarily on English data. Although they exhibit decent performance in other languages, such as Chinese, their potential remains limited due to factors like vocabulary design and training corpus. Consequently, their ability to fully express their capabilities in Chinese falls short. To address this issue, we introduce the model named JIANG (Chinese pinyin of ginger) specifically designed for the Chinese language. We have gathered a substantial amount of Chinese corpus to train the model and have also optimized its structure. The extensive experimental res
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#34917;&#20805;&#22810;&#27169;&#24577;&#34920;&#24773;&#21253;&#20998;&#31867;&#22120;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#20013;&#38388;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#37327;&#24773;&#24863;&#26631;&#35760;&#30340;&#21333;&#27169;&#24577;&#25968;&#25454;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#19981;&#20943;&#23569;&#19979;&#28216;&#27169;&#22411;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#20943;&#23569;&#26631;&#35760;&#34920;&#24773;&#21253;&#30340;&#35757;&#32451;&#38598;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.00528</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#34920;&#24773;&#21253;&#24773;&#24863;&#20998;&#31867;&#30340;&#21333;&#27169;&#24577;&#20013;&#38388;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Unimodal Intermediate Training for Multimodal Meme Sentiment Classification. (arXiv:2308.00528v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00528
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#34917;&#20805;&#22810;&#27169;&#24577;&#34920;&#24773;&#21253;&#20998;&#31867;&#22120;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#20013;&#38388;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#37327;&#24773;&#24863;&#26631;&#35760;&#30340;&#21333;&#27169;&#24577;&#25968;&#25454;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#19981;&#20943;&#23569;&#19979;&#28216;&#27169;&#22411;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#20943;&#23569;&#26631;&#35760;&#34920;&#24773;&#21253;&#30340;&#35757;&#32451;&#38598;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#34920;&#24773;&#21253;&#20316;&#20026;&#29992;&#25143;&#29983;&#25104;&#30340;&#20869;&#23481;&#20043;&#19968;&#65292;&#23545;&#20110;&#33258;&#21160;&#24773;&#24863;&#20998;&#31867;&#26469;&#35828;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#12290;&#24050;&#26631;&#35760;&#30340;&#34920;&#24773;&#21253;&#30340;&#21487;&#29992;&#24615;&#26159;&#24320;&#21457;&#22810;&#27169;&#24577;&#34920;&#24773;&#21253;&#20998;&#31867;&#22120;&#30340;&#19968;&#31181;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#26631;&#35760;&#34920;&#24773;&#21253;&#25968;&#37327;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#21333;&#27169;&#24577;(&#20165;&#22270;&#20687;&#21644;&#20165;&#25991;&#26412;)&#25968;&#25454;&#26469;&#34917;&#20805;&#22810;&#27169;&#24577;&#34920;&#24773;&#21253;&#20998;&#31867;&#22120;&#30340;&#35757;&#32451;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30417;&#30563;&#24335;&#20013;&#38388;&#35757;&#32451;&#30340;&#21464;&#20307;&#65292;&#20351;&#29992;&#30456;&#23545;&#20016;&#23500;&#30340;&#24773;&#24863;&#26631;&#35760;&#21333;&#27169;&#24577;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#21333;&#27169;&#24577;&#25991;&#26412;&#25968;&#25454;&#30340;&#34701;&#21512;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#21487;&#20197;&#23558;&#26631;&#35760;&#34920;&#24773;&#21253;&#30340;&#35757;&#32451;&#38598;&#20943;&#23569;40%&#65292;&#32780;&#19981;&#38477;&#20302;&#19979;&#28216;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Internet Memes remain a challenging form of user-generated content for automated sentiment classification. The availability of labelled memes is a barrier to developing sentiment classifiers of multimodal memes. To address the shortage of labelled memes, we propose to supplement the training of a multimodal meme classifier with unimodal (image-only and text-only) data. In this work, we present a novel variant of supervised intermediate training that uses relatively abundant sentiment-labelled unimodal data. Our results show a statistically significant performance improvement from the incorporation of unimodal text data. Furthermore, we show that the training set of labelled memes can be reduced by 40% without reducing the performance of the downstream model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#21307;&#23398;&#25945;&#32946;&#39046;&#22495;&#20013;&#65292;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#23558;&#38750;&#21442;&#25968;&#21270;&#30693;&#35782;&#24211;&#19982;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#24187;&#35273;&#21644;&#26377;&#23475;&#31572;&#26696;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20195;&#34920;&#24615;&#21521;&#37327;&#30340;&#25277;&#21462;&#24615;&#21644;&#25277;&#35937;&#24615;&#25688;&#35201;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.00479</link><description>&lt;p&gt;
&#22312;&#21307;&#23398;&#25945;&#32946;&#20013;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#25968;&#25454;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#21644;&#20195;&#34920;&#24615;&#21521;&#37327;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Retrieval Augmented Generation and Representative Vector Summarization for large unstructured textual data in Medical Education. (arXiv:2308.00479v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00479
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#21307;&#23398;&#25945;&#32946;&#39046;&#22495;&#20013;&#65292;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#23558;&#38750;&#21442;&#25968;&#21270;&#30693;&#35782;&#24211;&#19982;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#24187;&#35273;&#21644;&#26377;&#23475;&#31572;&#26696;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20195;&#34920;&#24615;&#21521;&#37327;&#30340;&#25277;&#21462;&#24615;&#21644;&#25277;&#35937;&#24615;&#25688;&#35201;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#20869;&#23481;&#29983;&#25104;&#21644;&#20316;&#20026;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#23613;&#31649;&#23427;&#20204;&#22312;&#19968;&#33324;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#24212;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#20219;&#21153;&#26102;&#65292;&#38656;&#35201;&#23545;LLMs&#36827;&#34892;&#35843;&#25972;&#20197;&#20943;&#36731;&#20135;&#29983;&#24187;&#35273;&#21644;&#26377;&#23475;&#31572;&#26696;&#30340;&#38382;&#39064;&#12290;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#20801;&#35768;&#36731;&#26494;&#22320;&#36830;&#25509;&#21644;&#25805;&#20316;&#38750;&#21442;&#25968;&#21270;&#30693;&#35782;&#24211;&#21040;LLMs&#19978;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;RAG&#22312;&#21307;&#23398;&#25945;&#32946;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20195;&#34920;&#24615;&#21521;&#37327;&#30340;&#22823;&#35268;&#27169;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#25968;&#25454;&#30340;&#25277;&#21462;&#24615;&#21644;&#25277;&#35937;&#24615;&#25688;&#35201;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are increasingly being used for various tasks including content generation and as chatbots. Despite their impressive performances in general tasks, LLMs need to be aligned when applying for domain specific tasks to mitigate the problems of hallucination and producing harmful answers. Retrieval Augmented Generation (RAG) allows to easily attach and manipulate a non-parametric knowledgebases to LLMs. Applications of RAG in the field of medical education are discussed in this paper. A combined extractive and abstractive summarization method for large unstructured textual data using representative vectors is proposed.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#31361;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#22806;&#37096;&#24037;&#20855;&#20043;&#38388;&#22522;&#20110;&#22270;&#30340;&#20132;&#20114;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#25351;&#23548;&#19982;LLM&#38598;&#25104;&#22823;&#37327;&#22806;&#37096;&#24037;&#20855;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2308.00447</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#32467;&#26500;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Structural Embeddings of Tools for Large Language Models. (arXiv:2308.00447v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00447
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#31361;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#22806;&#37096;&#24037;&#20855;&#20043;&#38388;&#22522;&#20110;&#22270;&#30340;&#20132;&#20114;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#25351;&#23548;&#19982;LLM&#38598;&#25104;&#22823;&#37327;&#22806;&#37096;&#24037;&#20855;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26174;&#32780;&#26131;&#35265;&#65292;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#29366;&#24577;&#38656;&#35201;&#24341;&#20837;&#22806;&#37096;&#24037;&#20855;&#12290;&#24050;&#32463;&#26377;&#22823;&#37327;&#25991;&#29486;&#35760;&#24405;&#20102;&#20854;&#32570;&#20047;&#30452;&#25509;&#30340;&#20195;&#25968;&#21644;&#36923;&#36753;&#25512;&#29702;&#65292;&#24182;&#20419;&#20351;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#20801;&#35768;LLMs&#36890;&#36807;&#22806;&#37096;&#24037;&#20855;&#36816;&#34892;&#30340;&#26694;&#26550;&#12290;&#29305;&#23450;&#20219;&#21153;&#30340;&#24037;&#20855;&#21033;&#29992;&#30340;&#26412;&#20307;&#24615;&#36136;&#21487;&#20197;&#29992;&#26377;&#21521;&#26080;&#29615;&#22270;(DAG)&#24456;&#22909;&#22320;&#25551;&#36848;&#12290;&#26412;&#25991;&#30340;&#26680;&#24515;&#30446;&#26631;&#26159;&#31361;&#20986;&#24378;&#35843;&#22312;&#19981;&#20037;&#30340;&#23558;&#26469;&#65292;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#23545;LLM-&#24037;&#20855;&#20132;&#20114;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31034;&#33539;&#24615;&#26694;&#26550;&#65292;&#29992;&#20110;&#25351;&#23548;&#25351;&#25968;&#32423;&#22686;&#21152;&#30340;&#22806;&#37096;&#24037;&#20855;&#19982;LLMs&#30340;&#32534;&#25490;&#65292;&#20854;&#20013;&#24037;&#20855;&#30340;&#30446;&#26631;&#21644;&#21151;&#33021;&#20197;&#22270;&#24418;&#26041;&#24335;&#36827;&#34892;&#23618;&#27425;&#32467;&#26500;&#32534;&#30721;&#12290;&#20551;&#35774;&#20316;&#20026;&#23450;&#20041;&#22312;&#36825;&#37324;&#30340;&#24037;&#20855;&#65292;&#24605;&#32500;&#38142;(CoT)&#30340;&#25991;&#26412;&#29255;&#27573;&#21487;&#20197;&#34987;&#24819;&#35937;&#20026;&#19968;&#31181;&#24037;&#20855;&#65292;&#37027;&#20040;&#22522;&#20110;&#22270;&#30340;&#26694;&#26550;&#20063;&#21487;&#20197;&#22312;&#36825;&#20010;&#29305;&#23450;&#26041;&#21521;&#19978;&#24320;&#36767;&#26032;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is evident that the current state of Large Language Models (LLMs) necessitates the incorporation of external tools. The lack of straightforward algebraic and logical reasoning is well documented and prompted researchers to develop frameworks which allow LLMs to operate via external tools. The ontological nature of tool utilization for a specific task can be well formulated with a Directed Acyclic Graph (DAG). The central aim of the paper is to highlight the importance of graph based approaches to LLM-tool interaction in near future. We propose an exemplary framework to guide the orchestration of exponentially increasing numbers of external tools with LLMs,where objectives and functionalities of tools are graph encoded hierarchically. Assuming that textual segments of a Chain-of-Thought (CoT) can be imagined as a tool as defined here, the graph based framework can pave new avenues in that particular direction as well.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;LLMs&#33258;&#26816;&#36880;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;-shot&#39564;&#35777;&#26041;&#26696;&#65292;&#25104;&#21151;&#35782;&#21035;&#38169;&#35823;&#24182;&#25552;&#39640;&#20102;&#38382;&#31572;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.00436</link><description>&lt;p&gt;
SelfCheck: &#20351;&#29992;LLMs&#33258;&#26816;&#20854;&#36880;&#27493;&#25512;&#29702;&#30340;&#21019;&#26032;
&lt;/p&gt;
&lt;p&gt;
SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning. (arXiv:2308.00436v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;LLMs&#33258;&#26816;&#36880;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;-shot&#39564;&#35777;&#26041;&#26696;&#65292;&#25104;&#21151;&#35782;&#21035;&#38169;&#35823;&#24182;&#25552;&#39640;&#20102;&#38382;&#31572;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#65292;&#23588;&#20854;&#26159;&#38142;&#24335;&#24605;&#32500;&#65288;CoT&#65289;&#30340;&#21457;&#26126;&#65292;&#20351;&#24471;&#35299;&#20915;&#25512;&#29702;&#38382;&#39064;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26368;&#24378;&#22823;&#30340;LLMs&#20173;&#28982;&#38590;&#20197;&#22788;&#29702;&#38656;&#35201;&#38750;&#32447;&#24615;&#24605;&#32500;&#21644;&#22810;&#27493;&#25512;&#29702;&#30340;&#22797;&#26434;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;LLMs&#26159;&#21542;&#20855;&#26377;&#35782;&#21035;&#33258;&#24049;&#38169;&#35823;&#30340;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#22806;&#37096;&#36164;&#28304;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23427;&#20204;&#26159;&#21542;&#21487;&#20197;&#29992;&#20110;&#35782;&#21035;&#36880;&#27493;&#25512;&#29702;&#20013;&#30340;&#20010;&#21035;&#38169;&#35823;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;-shot&#39564;&#35777;&#26041;&#26696;&#20197;&#35782;&#21035;&#27492;&#31867;&#38169;&#35823;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#27492;&#39564;&#35777;&#26041;&#26696;&#26469;&#25913;&#36827;&#38382;&#31572;&#24615;&#33021;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#29983;&#25104;&#30340;&#31572;&#26696;&#36827;&#34892;&#21152;&#26435;&#25237;&#31080;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25968;&#23398;&#25968;&#25454;&#38598;-GSM8K&#65292;MathQA&#21644;MATH&#19978;&#27979;&#35797;&#20102;&#35813;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#23427;&#25104;&#21151;&#35782;&#21035;&#38169;&#35823;&#65292;&#24182;&#36827;&#32780;&#25552;&#39640;&#20102;&#26368;&#32456;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent progress in large language models (LLMs), especially the invention of chain-of-thoughts (CoT) prompting, makes it possible to solve reasoning problems. However, even the strongest LLMs are still struggling with more complicated problems that require non-linear thinking and multi-step reasoning. In this work, we explore whether LLMs have the ability to recognize their own errors, without resorting to external resources. In particular, we investigate whether they can be used to identify individual errors within a step-by-step reasoning. To this end, we propose a zero-shot verification scheme to recognize such errors. We then use this verification scheme to improve question-answering performance, by using it to perform weighted voting on different generated answers. We test the method on three math datasets-GSM8K, MathQA, and MATH-and find that it successfully recognizes errors and, in turn, increases final predictive performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35805;&#35821;&#24863;&#30693;&#30340;&#25991;&#26412;&#31616;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21477;&#23376;&#30340;&#35821;&#20041;&#19978;&#19979;&#25991;&#20013;&#25286;&#20998;&#21644;&#37325;&#26032;&#34920;&#36798;&#22797;&#26434;&#30340;&#33521;&#35821;&#21477;&#23376;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#21477;&#27861;&#25991;&#26412;&#31616;&#21270;&#26041;&#27861;&#30340;&#20445;&#23432;&#24615;&#21644;&#24573;&#30053;&#19978;&#19979;&#25991;&#36830;&#36143;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.00425</link><description>&lt;p&gt;
&#20174;&#22797;&#26434;&#21477;&#23376;&#21040;&#38142;&#25509;&#21629;&#39064;&#30340;&#35805;&#35821;&#24863;&#30693;&#25991;&#26412;&#31616;&#21270;
&lt;/p&gt;
&lt;p&gt;
Discourse-Aware Text Simplification: From Complex Sentences to Linked Propositions. (arXiv:2308.00425v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35805;&#35821;&#24863;&#30693;&#30340;&#25991;&#26412;&#31616;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21477;&#23376;&#30340;&#35821;&#20041;&#19978;&#19979;&#25991;&#20013;&#25286;&#20998;&#21644;&#37325;&#26032;&#34920;&#36798;&#22797;&#26434;&#30340;&#33521;&#35821;&#21477;&#23376;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#21477;&#27861;&#25991;&#26412;&#31616;&#21270;&#26041;&#27861;&#30340;&#20445;&#23432;&#24615;&#21644;&#24573;&#30053;&#19978;&#19979;&#25991;&#36830;&#36143;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#26469;&#35828;&#65292;&#22797;&#26434;&#21477;&#23376;&#26159;&#20854;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#38556;&#30861;&#65292;&#21477;&#23376;&#30340;&#38271;&#24230;&#21644;&#22797;&#26434;&#24615;&#20250;&#23548;&#33268;&#39044;&#27979;&#36136;&#37327;&#19979;&#38477;&#12290;&#25991;&#26412;&#31616;&#21270;&#30340;&#20219;&#21153;&#23601;&#26159;&#20026;&#20102;&#20351;&#21477;&#23376;&#26356;&#23481;&#26131;&#22788;&#29702;&#32780;&#23545;&#20854;&#36827;&#34892;&#20462;&#25913;&#65292;&#20351;&#29992;&#19968;&#31995;&#21015;&#30340;&#37325;&#20889;&#25805;&#20316;&#65292;&#27604;&#22914;&#37325;&#26032;&#25490;&#24207;&#12289;&#21024;&#38500;&#25110;&#25286;&#20998;&#12290;&#29616;&#26377;&#30340;&#21477;&#27861;&#25991;&#26412;&#31616;&#21270;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#32570;&#28857;&#65306;&#19968;&#26041;&#38754;&#65292;&#23427;&#20204;&#37319;&#21462;&#20102;&#38750;&#24120;&#20445;&#23432;&#30340;&#26041;&#27861;&#65292;&#20542;&#21521;&#20110;&#20445;&#30041;&#36755;&#20837;&#32780;&#19981;&#36827;&#34892;&#36716;&#25442;&#65307;&#21478;&#19968;&#26041;&#38754;&#65292;&#23427;&#20204;&#24573;&#30053;&#20102;&#25991;&#26412;&#30340;&#36830;&#36143;&#24615;&#65292;&#38656;&#35201;&#36328;&#36234;&#20174;&#21477;&#25110;&#21477;&#23376;&#30340;&#19978;&#19979;&#25991;&#26469;&#25512;&#26029;&#35821;&#21477;&#30340;&#30495;&#27491;&#21547;&#20041;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35805;&#35821;&#24863;&#30693;&#30340;&#25991;&#26412;&#31616;&#21270;&#26041;&#27861;&#65292;&#22312;&#21477;&#23376;&#25152;&#22788;&#30340;&#35821;&#20041;&#19978;&#19979;&#25991;&#20013;&#25286;&#20998;&#21644;&#37325;&#26032;&#34920;&#36798;&#22797;&#26434;&#30340;&#33521;&#35821;&#21477;&#23376;&#12290;&#22522;&#20110;&#19968;&#20010;&#22522;&#20110;&#35821;&#35328;&#23398;&#30340;&#36716;&#25442;&#38454;&#27573;&#65292;&#35813;&#38454;&#27573;&#21033;&#29992;&#20174;&#21477;&#21644;&#21629;&#39064;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#23436;&#25104;&#31616;&#21270;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentences that present a complex syntax act as a major stumbling block for downstream Natural Language Processing applications whose predictive quality deteriorates with sentence length and complexity. The task of Text Simplification (TS) may remedy this situation. It aims to modify sentences in order to make them easier to process, using a set of rewriting operations, such as reordering, deletion, or splitting. State-of-the-art syntactic TS approaches suffer from two major drawbacks: first, they follow a very conservative approach in that they tend to retain the input rather than transforming it, and second, they ignore the cohesive nature of texts, where context spread across clauses or sentences is needed to infer the true meaning of a statement. To address these problems, we present a discourse-aware TS approach that splits and rephrases complex English sentences within the semantic context in which they occur. Based on a linguistically grounded transformation stage that uses claus
&lt;/p&gt;</description></item><item><title>ZRIGF&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;&#65292;&#29992;&#20110;&#26080;&#36164;&#28304;&#24773;&#22659;&#19979;&#30340;&#22270;&#20687;&#39537;&#21160;&#23545;&#35805;&#29983;&#25104;&#12290;&#23427;&#36890;&#36807;&#23545;&#27604;&#39044;&#35757;&#32451;&#21644;&#29983;&#25104;&#39044;&#35757;&#32451;&#23454;&#29616;&#20102;&#35270;&#35273;&#29305;&#24449;&#30340;&#23545;&#40784;&#65292;&#29983;&#25104;&#26377;&#27934;&#23519;&#21147;&#30340;&#22238;&#24212;&#12290;</title><link>http://arxiv.org/abs/2308.00400</link><description>&lt;p&gt;
ZRIGF&#65306;&#19968;&#31181;&#29992;&#20110;&#26080;&#36164;&#28304;&#22270;&#20687;&#39537;&#21160;&#23545;&#35805;&#29983;&#25104;&#30340;&#21019;&#26032;&#22810;&#27169;&#24577;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ZRIGF: An Innovative Multimodal Framework for Zero-Resource Image-Grounded Dialogue Generation. (arXiv:2308.00400v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00400
&lt;/p&gt;
&lt;p&gt;
ZRIGF&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;&#65292;&#29992;&#20110;&#26080;&#36164;&#28304;&#24773;&#22659;&#19979;&#30340;&#22270;&#20687;&#39537;&#21160;&#23545;&#35805;&#29983;&#25104;&#12290;&#23427;&#36890;&#36807;&#23545;&#27604;&#39044;&#35757;&#32451;&#21644;&#29983;&#25104;&#39044;&#35757;&#32451;&#23454;&#29616;&#20102;&#35270;&#35273;&#29305;&#24449;&#30340;&#23545;&#40784;&#65292;&#29983;&#25104;&#26377;&#27934;&#23519;&#21147;&#30340;&#22238;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#39537;&#21160;&#30340;&#23545;&#35805;&#31995;&#32479;&#36890;&#36807;&#25972;&#21512;&#35270;&#35273;&#20449;&#24687;&#65292;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22238;&#24212;&#26041;&#38754;&#20855;&#26377;&#24456;&#22823;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#27169;&#22411;&#22312;&#26080;&#36164;&#28304;&#24773;&#22659;&#20013;&#38590;&#20197;&#26377;&#25928;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;&#22270;&#20687;&#21644;&#25991;&#26412;&#27169;&#24577;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;&#65292;&#31216;&#20026;ZRIGF&#65292;&#23427;&#22312;&#26080;&#36164;&#28304;&#24773;&#22659;&#20013;&#34701;&#21512;&#20102;&#22270;&#20687;&#39537;&#21160;&#20449;&#24687;&#26469;&#29983;&#25104;&#23545;&#35805;&#12290;ZRIGF&#37319;&#29992;&#20004;&#38454;&#27573;&#23398;&#20064;&#31574;&#30053;&#65292;&#21253;&#25324;&#23545;&#27604;&#39044;&#35757;&#32451;&#21644;&#29983;&#25104;&#39044;&#35757;&#32451;&#12290;&#23545;&#27604;&#39044;&#35757;&#32451;&#21253;&#25324;&#19968;&#20010;&#25991;&#26412;-&#22270;&#20687;&#21305;&#37197;&#27169;&#22359;&#65292;&#23558;&#22270;&#20687;&#21644;&#25991;&#26412;&#26144;&#23556;&#21040;&#32479;&#19968;&#30340;&#32534;&#30721;&#21521;&#37327;&#31354;&#38388;&#20013;&#65292;&#20197;&#21450;&#19968;&#20010;&#25991;&#26412;&#36741;&#21161;&#30340;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;&#27169;&#22359;&#65292;&#29992;&#20110;&#20445;&#23384;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#29305;&#24449;&#24182;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#22810;&#27169;&#24577;&#29305;&#24449;&#23545;&#40784;&#12290;&#29983;&#25104;&#39044;&#35757;&#32451;&#20351;&#29992;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22359;&#21644;&#20449;&#24687;&#20256;&#36882;&#27169;&#22359;&#26469;&#29983;&#25104;&#26377;&#27934;&#23519;&#21147;&#30340;&#22238;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image-grounded dialogue systems benefit greatly from integrating visual information, resulting in high-quality response generation. However, current models struggle to effectively utilize such information in zero-resource scenarios, mainly due to the disparity between image and text modalities. To overcome this challenge, we propose an innovative multimodal framework, called ZRIGF, which assimilates image-grounded information for dialogue generation in zero-resource situations. ZRIGF implements a two-stage learning strategy, comprising contrastive pre-training and generative pre-training. Contrastive pre-training includes a text-image matching module that maps images and texts into a unified encoded vector space, along with a text-assisted masked image modeling module that preserves pre-training visual features and fosters further multimodal feature alignment. Generative pre-training employs a multimodal fusion module and an information transfer module to produce insightful responses b
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#31070;&#32463;&#22270;&#34920;&#25688;&#35201;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#39044;&#22788;&#29702;&#35757;&#32451;&#25968;&#25454;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#24187;&#35273;&#30340;&#20135;&#29983;&#65292;&#24182;&#36890;&#36807;&#32553;&#30701;&#36828;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#21644;&#28155;&#21152;&#19982;&#22270;&#34920;&#30456;&#20851;&#30340;&#20449;&#24687;&#26469;&#25552;&#39640;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.00399</link><description>&lt;p&gt;
&#22788;&#29702;&#31070;&#32463;&#22270;&#34920;&#25688;&#35201;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Tackling Hallucinations in Neural Chart Summarization. (arXiv:2308.00399v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#31070;&#32463;&#22270;&#34920;&#25688;&#35201;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#39044;&#22788;&#29702;&#35757;&#32451;&#25968;&#25454;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#24187;&#35273;&#30340;&#20135;&#29983;&#65292;&#24182;&#36890;&#36807;&#32553;&#30701;&#36828;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#21644;&#28155;&#21152;&#19982;&#22270;&#34920;&#30456;&#20851;&#30340;&#20449;&#24687;&#26469;&#25552;&#39640;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#24187;&#35273;&#26159;&#25351;&#31995;&#32479;&#20135;&#29983;&#30340;&#25991;&#26412;&#26410;&#19982;&#36755;&#20837;&#20851;&#32852;&#12290;&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#31070;&#32463;&#22270;&#34920;&#25688;&#35201;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22270;&#34920;&#25688;&#35201;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#30446;&#26631;&#31471;&#32463;&#24120;&#21253;&#21547;&#39069;&#22806;&#30340;&#20449;&#24687;&#65292;&#23548;&#33268;&#24187;&#35273;&#30340;&#20135;&#29983;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;(NLI)&#30340;&#26041;&#27861;&#26469;&#39044;&#22788;&#29702;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#24187;&#35273;&#30340;&#20135;&#29983;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#32553;&#30701;&#36755;&#20837;&#24207;&#21015;&#20013;&#30340;&#36828;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#28155;&#21152;&#19982;&#22270;&#34920;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#22914;&#26631;&#39064;&#21644;&#22270;&#20363;&#65292;&#21487;&#20197;&#25552;&#39640;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hallucinations in text generation occur when the system produces text that is not grounded in the input. In this work, we tackle the problem of hallucinations in neural chart summarization. Our analysis shows that the target side of chart summarization training datasets often contains additional information, leading to hallucinations. We propose a natural language inference (NLI) based method to preprocess the training data and show through human evaluation that our method significantly reduces hallucinations. We also found that shortening long-distance dependencies in the input sequence and adding chart-related information like title and legends improves the overall performance.
&lt;/p&gt;</description></item><item><title>Fountain&#26159;&#19968;&#20010;&#26234;&#33021;&#19978;&#19979;&#25991;&#21161;&#25163;&#65292;&#23558;&#30693;&#35782;&#34920;&#31034;&#21644;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#65292;&#29992;&#20110;&#21046;&#36896;&#39118;&#38505;&#35782;&#21035;&#12290;&#23427;&#36890;&#36807;&#25551;&#36848;&#29616;&#26377;&#35774;&#35745;&#21644;&#27969;&#31243;&#20934;&#21017;&#20197;&#21450;&#25552;&#20986;&#30340;&#20559;&#24046;&#26469;&#24110;&#21161;&#35782;&#21035;&#39118;&#38505;&#65292;&#24182;&#25552;&#20379;&#21487;&#35299;&#37322;&#21644;&#19968;&#33268;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2308.00364</link><description>&lt;p&gt;
Fountain --&#19968;&#20010;&#26234;&#33021;&#19978;&#19979;&#25991;&#21161;&#25163;&#65292;&#32467;&#21512;&#30693;&#35782;&#34920;&#31034;&#21644;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#21046;&#36896;&#39118;&#38505;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Fountain -- an intelligent contextual assistant combining knowledge representation and language models for manufacturing risk identification. (arXiv:2308.00364v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00364
&lt;/p&gt;
&lt;p&gt;
Fountain&#26159;&#19968;&#20010;&#26234;&#33021;&#19978;&#19979;&#25991;&#21161;&#25163;&#65292;&#23558;&#30693;&#35782;&#34920;&#31034;&#21644;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#65292;&#29992;&#20110;&#21046;&#36896;&#39118;&#38505;&#35782;&#21035;&#12290;&#23427;&#36890;&#36807;&#25551;&#36848;&#29616;&#26377;&#35774;&#35745;&#21644;&#27969;&#31243;&#20934;&#21017;&#20197;&#21450;&#25552;&#20986;&#30340;&#20559;&#24046;&#26469;&#24110;&#21161;&#35782;&#21035;&#39118;&#38505;&#65292;&#24182;&#25552;&#20379;&#21487;&#35299;&#37322;&#21644;&#19968;&#33268;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#29983;&#20135;&#36807;&#31243;&#20013;&#65292;&#19982;&#25209;&#20934;&#30340;&#35774;&#35745;&#25110;&#27969;&#31243;&#20559;&#31163;&#20250;&#23548;&#33268;&#24847;&#24819;&#19981;&#21040;&#30340;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21464;&#21270;&#26377;&#26102;&#26159;&#24517;&#35201;&#30340;&#65292;&#22240;&#20026;&#20135;&#21697;&#35774;&#35745;&#29305;&#24449;&#25110;&#21046;&#36896;&#36807;&#31243;&#30340;&#36866;&#24212;&#24615;&#21457;&#29983;&#20102;&#21464;&#21270;&#12290;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#22312;&#24037;&#20316;&#27969;&#31243;&#30340;&#26089;&#26399;&#38454;&#27573;&#35782;&#21035;&#36825;&#20123;&#39118;&#38505;&#65292;&#20197;&#36991;&#20813;&#23548;&#33268;&#20445;&#20462;&#32034;&#36180;&#30340;&#25925;&#38556;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;Fountain&#20316;&#20026;&#19968;&#20010;&#19978;&#19979;&#25991;&#21161;&#25163;&#65292;&#38598;&#25104;&#22312;&#20559;&#24046;&#31649;&#29702;&#24037;&#20316;&#27969;&#31243;&#20013;&#65292;&#36890;&#36807;&#23545;&#29616;&#26377;&#35774;&#35745;&#21644;&#27969;&#31243;&#20934;&#21017;&#20197;&#21450;&#25552;&#20986;&#30340;&#20559;&#24046;&#30340;&#25551;&#36848;&#26469;&#24110;&#21161;&#35782;&#21035;&#39118;&#38505;&#12290;&#22312;&#21046;&#36896;&#29615;&#22659;&#20013;&#65292;&#35813;&#21161;&#25163;&#25552;&#20379;&#30340;&#24314;&#35758;&#24517;&#39035;&#26159;&#21487;&#35299;&#37322;&#21644;&#19968;&#33268;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#20197;&#19979;&#20004;&#20010;&#32452;&#20214;&#30340;&#32467;&#21512;&#23454;&#29616;&#20102;&#36825;&#19968;&#28857;&#65306;1&#65289;&#20026;&#29305;&#23450;&#39046;&#22495;&#35821;&#20041;&#30456;&#20284;&#24615;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21644;2&#65289;&#20197;&#29289;&#26009;&#28165;&#21333;&#12289;&#22833;&#25928;&#27169;&#24335;&#21644;&#25928;&#24212;&#20998;&#26512;&#20026;&#22522;&#30784;&#30340;&#23646;&#24615;&#22270;&#30340;&#30693;&#35782;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deviations from the approved design or processes during mass production can lead to unforeseen risks. However, these changes are sometimes necessary due to changes in the product design characteristics or an adaptation in the manufacturing process. A major challenge is to identify these risks early in the workflow so that failures leading to warranty claims can be avoided. We developed Fountain as a contextual assistant integrated in the deviation management workflow that helps in identifying the risks based on the description of the existing design and process criteria and the proposed deviation. In the manufacturing context, it is important that the assistant provides recommendations that are explainable and consistent. We achieve this through a combination of the following two components 1) language models finetuned for domain specific semantic similarity and, 2) knowledge representation in the form of a property graph derived from the bill of materials, Failure Modes and Effect Ana
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LimeAttack&#30340;&#30828;&#26631;&#31614;&#25915;&#20987;&#31639;&#27861;&#65292;&#36890;&#36807;&#26412;&#22320;&#21487;&#35299;&#37322;&#26041;&#27861;&#26469;&#36817;&#20284;&#21333;&#35789;&#37325;&#35201;&#24615;&#25490;&#24207;&#65292;&#28982;&#21518;&#21033;&#29992;&#27874;&#26463;&#25628;&#32034;&#25214;&#21040;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2308.00319</link><description>&lt;p&gt;
LimeAttack: &#26412;&#22320;&#21487;&#35299;&#37322;&#26041;&#27861;&#29992;&#20110;&#25991;&#26412;&#30828;&#26631;&#31614;&#23545;&#25239;&#24615;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
LimeAttack: Local Explainable Method for Textual Hard-Label Adversarial Attack. (arXiv:2308.00319v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LimeAttack&#30340;&#30828;&#26631;&#31614;&#25915;&#20987;&#31639;&#27861;&#65292;&#36890;&#36807;&#26412;&#22320;&#21487;&#35299;&#37322;&#26041;&#27861;&#26469;&#36817;&#20284;&#21333;&#35789;&#37325;&#35201;&#24615;&#25490;&#24207;&#65292;&#28982;&#21518;&#21033;&#29992;&#27874;&#26463;&#25628;&#32034;&#25214;&#21040;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#25915;&#20987;&#12290;&#20808;&#21069;&#30340;&#25991;&#26412;&#23545;&#25239;&#24615;&#25915;&#20987;&#37319;&#29992;&#26799;&#24230;&#25110;&#32622;&#20449;&#24230;&#20998;&#25968;&#26469;&#35745;&#31639;&#21333;&#35789;&#37325;&#35201;&#24615;&#25490;&#24207;&#65292;&#24182;&#29983;&#25104;&#23545;&#25239;&#24615;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#36825;&#20123;&#20449;&#24687;&#26159;&#19981;&#21487;&#29992;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#37325;&#28857;&#25918;&#22312;&#19968;&#20010;&#26356;&#29616;&#23454;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#19978;&#65292;&#21517;&#20026;&#30828;&#26631;&#31614;&#25915;&#20987;&#65292;&#20854;&#20013;&#25915;&#20987;&#32773;&#21482;&#33021;&#26597;&#35810;&#27169;&#22411;&#24182;&#33719;&#21462;&#31163;&#25955;&#30340;&#39044;&#27979;&#26631;&#31614;&#12290;&#29616;&#26377;&#30340;&#30828;&#26631;&#31614;&#25915;&#20987;&#31639;&#27861;&#24448;&#24448;&#36890;&#36807;&#38543;&#26426;&#26367;&#25442;&#26469;&#21021;&#22987;&#21270;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#28982;&#21518;&#21033;&#29992;&#22797;&#26434;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#26469;&#20248;&#21270;&#23545;&#25239;&#25200;&#21160;&#12290;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#27169;&#22411;&#26597;&#35810;&#65292;&#24182;&#19988;&#25915;&#20987;&#25104;&#21151;&#29575;&#21463;&#21040;&#23545;&#25163;&#21021;&#22987;&#21270;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LimeAttack&#30340;&#26032;&#22411;&#30828;&#26631;&#31614;&#25915;&#20987;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;&#26412;&#22320;&#21487;&#35299;&#37322;&#26041;&#27861;&#26469;&#36817;&#20284;&#21333;&#35789;&#37325;&#35201;&#24615;&#25490;&#24207;&#65292;&#28982;&#21518;&#37319;&#29992;&#27874;&#26463;&#25628;&#32034;&#26469;&#25214;&#21040;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language processing models are vulnerable to adversarial examples. Previous textual adversarial attacks adopt gradients or confidence scores to calculate word importance ranking and generate adversarial examples. However, this information is unavailable in the real world. Therefore, we focus on a more realistic and challenging setting, named hard-label attack, in which the attacker can only query the model and obtain a discrete prediction label. Existing hard-label attack algorithms tend to initialize adversarial examples by random substitution and then utilize complex heuristic algorithms to optimize the adversarial perturbation. These methods require a lot of model queries and the attack success rate is restricted by adversary initialization. In this paper, we propose a novel hard-label attack algorithm named LimeAttack, which leverages a local explainable method to approximate word importance ranking, and then adopts beam search to find the optimal solution. Extensive experi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#25216;&#33021;&#25351;&#23548; (SKiC) &#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#19978;&#19979;&#25991;&#20013;&#28436;&#31034;&#22522;&#26412;&#25216;&#33021;&#21644;&#32452;&#21512;&#24615;&#31034;&#20363;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#35299;&#20915;&#26356;&#22797;&#26434;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#27867;&#21270;&#33021;&#21147;&#19978;&#21462;&#24471;&#20960;&#20046;&#23436;&#32654;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2308.00304</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35299;&#38145;&#32452;&#21512;&#24615;&#30340;&#19978;&#19979;&#25991;&#25552;&#31034;: &#25216;&#33021;&#25351;&#23548;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Skills-in-Context Prompting: Unlocking Compositionality in Large Language Models. (arXiv:2308.00304v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#25216;&#33021;&#25351;&#23548; (SKiC) &#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#19978;&#19979;&#25991;&#20013;&#28436;&#31034;&#22522;&#26412;&#25216;&#33021;&#21644;&#32452;&#21512;&#24615;&#31034;&#20363;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#35299;&#20915;&#26356;&#22797;&#26434;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#27867;&#21270;&#33021;&#21147;&#19978;&#21462;&#24471;&#20960;&#20046;&#23436;&#32654;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#22914;&#20309;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#28608;&#21457;&#32452;&#21512;&#24615;&#27867;&#21270;&#33021;&#21147;&#30340;&#38382;&#39064;&#12290;&#32452;&#21512;&#24615;&#27867;&#21270;&#20351;&#24471;LLMs&#33021;&#22815;&#35299;&#20915;&#27604;&#23427;&#20204;&#25152;&#35265;&#36807;&#30340;&#38382;&#39064;&#26356;&#22256;&#38590;&#30340;&#38382;&#39064;&#65288;&#21363;&#26131;&#20110;&#38590;&#30340;&#27867;&#21270;&#65289;&#65292;&#36825;&#26159;&#20154;&#31867;&#26234;&#33021;&#30340;&#20851;&#38190;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;LLMs&#22312;&#36825;&#31181;&#24418;&#24335;&#30340;&#25512;&#29702;&#19978;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#19978;&#19979;&#25991;&#20013;&#30340;&#25216;&#33021;&#25351;&#23548;&#65288;SKiC&#65289;&#25552;&#31034;&#65292;&#23427;&#25351;&#23548;LLMs&#22914;&#20309;&#32452;&#21512;&#22522;&#26412;&#25216;&#33021;&#26469;&#35299;&#20915;&#26356;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#30456;&#21516;&#30340;&#25552;&#31034;&#19978;&#23637;&#31034;&#25216;&#33021;&#21644;&#32452;&#21512;&#24615;&#31034;&#20363;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20165;&#20165;&#36890;&#36807;&#20004;&#20010;&#31034;&#20363;&#65292;&#25105;&#20204;&#30340;SKiC&#25552;&#31034;&#22312;&#25216;&#33021;&#21644;&#23427;&#20204;&#30340;&#32452;&#21512;&#33021;&#21147;&#20043;&#38388;&#24418;&#25104;&#20102;&#24378;&#22823;&#30340;&#21327;&#21516;&#25928;&#24212;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23427;&#36171;&#20104;&#20102;LLMs&#35299;&#20915;&#38656;&#35201;&#21019;&#26032;&#25216;&#33021;&#32452;&#21512;&#30340;&#26410;&#35265;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#20960;&#20046;&#23436;&#32654;&#30340;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of eliciting compositional generalization capabilities in large language models (LLMs) with a novel type of prompting strategy. Compositional generalization empowers the LLMs to solve problems that are harder than the ones they have seen (i.e., easy-to-hard generalization), which is a critical reasoning capability of human-like intelligence. However, even the current state-of-the-art LLMs still struggle with this form of reasoning. To bridge this gap, we propose skills-in-context (SKiC) prompting, which instructs LLMs how to compose basic skills to resolve more complex problems. We find that it is crucial to demonstrate both the skills and the compositional examples within the same prompting context. With as few as two examplars, our SKiC prompting initiates strong synergies between skills and their composition capabilities. Notably, it empowers LLMs to solve unseen problems that require innovative skill compositions, achieving near-perfect generalization on a b
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#25991;&#26412;VQA&#20013;&#23545;&#35270;&#35273;&#29305;&#24449;&#29702;&#35299;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#36890;&#36807;&#23398;&#20064;&#35270;&#35273;&#29305;&#24449;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;TextVQA&#21644;VQA&#25968;&#25454;&#38598;&#30456;&#32467;&#21512;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.00295</link><description>&lt;p&gt;
&#35753;Text-VQA&#20013;&#30340;V&#21464;&#24471;&#37325;&#35201;
&lt;/p&gt;
&lt;p&gt;
Making the V in Text-VQA Matter. (arXiv:2308.00295v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#25991;&#26412;VQA&#20013;&#23545;&#35270;&#35273;&#29305;&#24449;&#29702;&#35299;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#36890;&#36807;&#23398;&#20064;&#35270;&#35273;&#29305;&#24449;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;TextVQA&#21644;VQA&#25968;&#25454;&#38598;&#30456;&#32467;&#21512;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;VQA&#26088;&#22312;&#36890;&#36807;&#38405;&#35835;&#22270;&#20687;&#20013;&#30340;&#25991;&#26412;&#26469;&#22238;&#31572;&#38382;&#39064;&#12290;&#19982;VQA&#20219;&#21153;&#30456;&#27604;&#65292;&#23427;&#38656;&#35201;&#22823;&#37327;&#30340;&#22330;&#26223;-&#25991;&#26412;&#20851;&#31995;&#29702;&#35299;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25968;&#25454;&#38598;&#20013;&#30340;&#38382;&#39064;-&#31572;&#26696;&#23545;&#26356;&#20851;&#27880;&#22270;&#20687;&#20013;&#30340;&#25991;&#26412;&#65292;&#32780;&#23545;&#20110;&#35270;&#35273;&#29305;&#24449;&#21017;&#32473;&#20104;&#36739;&#23569;&#37325;&#35270;&#65292;&#32780;&#19988;&#26377;&#20123;&#38382;&#39064;&#19981;&#38656;&#35201;&#29702;&#35299;&#22270;&#20687;&#12290;&#30001;&#20110;&#32570;&#20047;&#23545;&#35270;&#35273;&#19978;&#19979;&#25991;&#30340;&#29702;&#35299;&#65292;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#20250;&#39044;&#27979;&#20986;&#26377;&#20559;&#24046;&#30340;&#31572;&#26696;&#12290;&#20363;&#22914;&#65292;&#22312;&#31867;&#20284;&#8220;&#26631;&#29260;&#19978;&#20889;&#30528;&#20160;&#20040;&#65311;&#8221;&#30340;&#38382;&#39064;&#20013;&#65292;&#27169;&#22411;&#39044;&#27979;&#30340;&#31572;&#26696;&#24635;&#26159;&#8220;STOP&#8221;&#65292;&#36825;&#20351;&#24471;&#27169;&#22411;&#24573;&#30053;&#20102;&#22270;&#20687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;VQA&#25968;&#25454;&#38598;&#20316;&#20026;Text-VQA&#30340;&#22806;&#37096;&#30693;&#35782;&#65292;&#23398;&#20064;&#35270;&#35273;&#29305;&#24449;&#65288;&#35753;V&#22312;Text-VQA&#20013;&#21464;&#24471;&#37325;&#35201;&#65289;&#65292;&#20197;&#21450;OCR&#29305;&#24449;&#21644;&#38382;&#39064;&#29305;&#24449;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;TextVQA&#25968;&#25454;&#38598;&#21644;VQA&#25968;&#25454;&#38598;&#36827;&#34892;&#21512;&#24182;&#65292;&#24182;&#22312;&#36825;&#20010;&#21512;&#24182;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-based VQA aims at answering questions by reading the text present in the images. It requires a large amount of scene-text relationship understanding compared to the VQA task. Recent studies have shown that the question-answer pairs in the dataset are more focused on the text present in the image but less importance is given to visual features and some questions do not require understanding the image. The models trained on this dataset predict biased answers due to the lack of understanding of visual context. For example, in questions like "What is written on the signboard?", the answer predicted by the model is always "STOP" which makes the model to ignore the image. To address these issues, we propose a method to learn visual features (making V matter in TextVQA) along with the OCR features and question features using VQA dataset as external knowledge for Text-based VQA. Specifically, we combine the TextVQA dataset and VQA dataset and train the model on this combined dataset. Suc
&lt;/p&gt;</description></item><item><title>&#22810;&#27169;&#24577;&#22810;&#25439;&#22833;&#34701;&#21512;&#32593;&#32476;&#36890;&#36807;&#26368;&#20339;&#36873;&#25321;&#21644;&#34701;&#21512;&#22810;&#20010;&#27169;&#24577;&#30340;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#24773;&#24863;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#20102;&#29992;&#20110;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#20013;&#24773;&#24863;&#26816;&#27979;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#34701;&#21512;&#26041;&#27861;&#30340;&#20248;&#21270;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.00264</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22810;&#25439;&#22833;&#34701;&#21512;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Multi-Modality Multi-Loss Fusion Network. (arXiv:2308.00264v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00264
&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22810;&#25439;&#22833;&#34701;&#21512;&#32593;&#32476;&#36890;&#36807;&#26368;&#20339;&#36873;&#25321;&#21644;&#34701;&#21512;&#22810;&#20010;&#27169;&#24577;&#30340;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#24773;&#24863;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#20102;&#29992;&#20110;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#20013;&#24773;&#24863;&#26816;&#27979;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#34701;&#21512;&#26041;&#27861;&#30340;&#20248;&#21270;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36328;&#22810;&#20010;&#27169;&#24577;&#36873;&#25321;&#21644;&#34701;&#21512;&#29305;&#24449;&#30340;&#26368;&#20339;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#32452;&#21512;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#20197;&#25913;&#21892;&#24773;&#24863;&#26816;&#27979;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#34701;&#21512;&#26041;&#27861;&#24182;&#19988;&#30740;&#31350;&#20102;&#22810;&#25439;&#22833;&#35757;&#32451;&#22312;&#22810;&#27169;&#24577;&#34701;&#21512;&#32593;&#32476;&#20013;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#30830;&#23450;&#20102;&#19982;&#23376;&#32593;&#24615;&#33021;&#30456;&#20851;&#30340;&#26377;&#29992;&#21457;&#29616;&#12290;&#25105;&#20204;&#26368;&#22909;&#30340;&#27169;&#22411;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#65288;CMU-MOSI&#12289;CMU-MOSEI&#21644;CH-SIMS&#65289;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#22823;&#22810;&#25968;&#25351;&#26631;&#19978;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35757;&#32451;&#22810;&#27169;&#24577;&#29305;&#24449;&#21487;&#20197;&#25913;&#21892;&#21333;&#27169;&#24577;&#27979;&#35797;&#65292;&#24182;&#19988;&#22522;&#20110;&#25968;&#25454;&#38598;&#27880;&#37322;&#27169;&#24335;&#35774;&#35745;&#34701;&#21512;&#26041;&#27861;&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#20102;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#22686;&#24378;&#24773;&#24863;&#26816;&#27979;&#30340;&#20248;&#21270;&#29305;&#24449;&#36873;&#25321;&#21644;&#34701;&#21512;&#26041;&#27861;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we investigate the optimal selection and fusion of features across multiple modalities and combine these in a neural network to improve emotion detection. We compare different fusion methods and examine the impact of multi-loss training within the multi-modality fusion network, identifying useful findings relating to subnet performance. Our best model achieves state-of-the-art performance for three datasets (CMU-MOSI, CMU-MOSEI and CH-SIMS), and outperforms the other methods in most metrics. We have found that training on multimodal features improves single modality testing and designing fusion methods based on dataset annotation schema enhances model performance. These results suggest a roadmap towards an optimized feature selection and fusion approach for enhancing emotion detection in neural networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Erya&#27169;&#22411;&#65292;&#36890;&#36807;&#25910;&#38598;&#12289;&#28165;&#29702;&#21644;&#20998;&#31867;&#22823;&#37327;&#21476;&#20195;&#27721;&#35821;&#26448;&#26009;&#65292;&#24418;&#25104;&#20102;&#26368;&#20840;&#38754;&#30340;&#21476;&#20195;&#27721;&#35821;&#36164;&#28304;&#12290;&#35813;&#27169;&#22411;&#22312;&#21452;&#38899;&#33410;&#23545;&#40784;&#26367;&#25442;&#21644;&#21452;&#23618;&#36974;&#32617;&#35821;&#35328;&#27169;&#22411;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#22312;&#22810;&#20010;&#39046;&#22495;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#38646;-shot&#24615;&#33021;&#21644;&#36801;&#31227;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.00240</link><description>&lt;p&gt;
&#26377;&#25928;&#30340;&#21476;&#20195;&#27721;&#35821;&#32763;&#35793;&#65306;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Towards Effective Ancient Chinese Translation: Dataset, Model, and Evaluation. (arXiv:2308.00240v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Erya&#27169;&#22411;&#65292;&#36890;&#36807;&#25910;&#38598;&#12289;&#28165;&#29702;&#21644;&#20998;&#31867;&#22823;&#37327;&#21476;&#20195;&#27721;&#35821;&#26448;&#26009;&#65292;&#24418;&#25104;&#20102;&#26368;&#20840;&#38754;&#30340;&#21476;&#20195;&#27721;&#35821;&#36164;&#28304;&#12290;&#35813;&#27169;&#22411;&#22312;&#21452;&#38899;&#33410;&#23545;&#40784;&#26367;&#25442;&#21644;&#21452;&#23618;&#36974;&#32617;&#35821;&#35328;&#27169;&#22411;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#22312;&#22810;&#20010;&#39046;&#22495;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#38646;-shot&#24615;&#33021;&#21644;&#36801;&#31227;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#35835;&#21476;&#20195;&#27721;&#35821;&#19968;&#30452;&#26159;&#29702;&#35299;&#20013;&#22269;&#24191;&#38420;&#25991;&#23398;&#12289;&#20256;&#32479;&#21644;&#25991;&#26126;&#30340;&#20851;&#38190;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#21476;&#20195;&#27721;&#35821;&#30340;Erya&#32763;&#35793;&#27169;&#22411;&#12290;&#22312;&#25968;&#25454;&#38598;&#26041;&#38754;&#65292;&#25105;&#20204;&#20174;&#22810;&#20010;&#26469;&#28304;&#25910;&#38598;&#12289;&#28165;&#29702;&#21644;&#20998;&#31867;&#21476;&#20195;&#27721;&#35821;&#26448;&#26009;&#65292;&#24418;&#25104;&#36804;&#20170;&#20026;&#27490;&#26368;&#20840;&#38754;&#30340;&#21476;&#20195;&#27721;&#35821;&#36164;&#28304;&#12290;&#20174;&#27169;&#22411;&#35282;&#24230;&#26469;&#30475;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#36866;&#29992;&#20110;&#21476;&#20195;&#27721;&#35821;&#30340;Erya&#35757;&#32451;&#26041;&#27861;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#20849;&#21516;&#24037;&#20316;&#30340;&#20219;&#21153;&#65306;&#21452;&#38899;&#33410;&#23545;&#40784;&#26367;&#25442;&#65288;DAS&#65289;&#21644;&#21452;&#23618;&#36974;&#32617;&#35821;&#35328;&#27169;&#22411;&#65288;DMLM&#65289;&#12290;&#20174;&#35780;&#20272;&#35282;&#24230;&#26469;&#30475;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#35780;&#21028;&#21476;&#20195;&#27721;&#35821;&#32763;&#35793;&#36136;&#37327;&#65292;&#24182;&#35780;&#20272;&#21508;&#31181;&#29616;&#26377;&#27169;&#22411;&#30340;&#21476;&#20195;&#27721;&#35821;&#32763;&#35793;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20116;&#20010;&#39046;&#22495;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#38646;-shot&#24615;&#33021;&#65292;&#23545;GPT-3.5&#27169;&#22411;&#30340;BLEU&#20998;&#25968;&#25552;&#39640;&#20102;+12.0&#65292;&#24182;&#19988;&#22312;&#20154;&#24037;&#35780;&#20272;&#32467;&#26524;&#19978;&#20248;&#20110;ERNIE Bot&#12290;&#38543;&#21518;&#30340;&#24494;&#35843;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#26356;&#24378;&#30340;&#36801;&#31227;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpreting ancient Chinese has been the key to comprehending vast Chinese literature, tradition, and civilization. In this paper, we propose Erya for ancient Chinese translation. From a dataset perspective, we collect, clean, and classify ancient Chinese materials from various sources, forming the most extensive ancient Chinese resource to date. From a model perspective, we devise Erya training method oriented towards ancient Chinese. We design two jointly-working tasks: disyllabic aligned substitution (DAS) and dual masked language model (DMLM). From an evaluation perspective, we build a benchmark to judge ancient Chinese translation quality in different scenarios and evaluate the ancient Chinese translation capacities of various existing models. Our model exhibits remarkable zero-shot performance across five domains, with over +12.0 BLEU against GPT-3.5 models and better human evaluation results than ERNIE Bot. Subsequent fine-tuning further shows the superior transfer capability o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#20301;&#27700;&#21360;&#25216;&#26415;&#8212;&#8212;COLOR&#65292;&#21487;&#22312;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36807;&#31243;&#20013;&#23884;&#20837;&#21487;&#36861;&#36394;&#30340;&#22810;&#20301;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#25552;&#21462;&#27700;&#21360;&#12289;&#21363;&#26102;&#23884;&#20837;&#21644;&#32500;&#25345;&#25991;&#26412;&#36136;&#37327;&#31561;&#21151;&#33021;&#65292;&#21516;&#26102;&#20801;&#35768;&#38646;&#20301;&#26816;&#27979;&#12290;&#21021;&#27493;&#23454;&#39564;&#26174;&#31034;&#25104;&#21151;&#22312;&#20013;&#31561;&#38271;&#24230;&#30340;&#25991;&#26412;&#20013;&#23884;&#20837;&#20102;32&#20301;&#28040;&#24687;&#65292;&#20934;&#30830;&#29575;&#20026;91.9&#65285;&#12290;&#36825;&#39033;&#30740;&#31350;&#26377;&#25928;&#25512;&#36827;&#20102;&#23545;&#35821;&#35328;&#27169;&#22411;&#28389;&#29992;&#30340;&#21453;&#21046;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2308.00221</link><description>&lt;p&gt;
&#36229;&#36234;&#35782;&#21035;&#65306;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#20301;&#27700;&#21360;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Advancing Beyond Identification: Multi-bit Watermark for Language Models. (arXiv:2308.00221v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00221
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#20301;&#27700;&#21360;&#25216;&#26415;&#8212;&#8212;COLOR&#65292;&#21487;&#22312;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36807;&#31243;&#20013;&#23884;&#20837;&#21487;&#36861;&#36394;&#30340;&#22810;&#20301;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#25552;&#21462;&#27700;&#21360;&#12289;&#21363;&#26102;&#23884;&#20837;&#21644;&#32500;&#25345;&#25991;&#26412;&#36136;&#37327;&#31561;&#21151;&#33021;&#65292;&#21516;&#26102;&#20801;&#35768;&#38646;&#20301;&#26816;&#27979;&#12290;&#21021;&#27493;&#23454;&#39564;&#26174;&#31034;&#25104;&#21151;&#22312;&#20013;&#31561;&#38271;&#24230;&#30340;&#25991;&#26412;&#20013;&#23884;&#20837;&#20102;32&#20301;&#28040;&#24687;&#65292;&#20934;&#30830;&#29575;&#20026;91.9&#65285;&#12290;&#36825;&#39033;&#30740;&#31350;&#26377;&#25928;&#25512;&#36827;&#20102;&#23545;&#35821;&#35328;&#27169;&#22411;&#28389;&#29992;&#30340;&#21453;&#21046;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#31215;&#26497;&#24212;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26041;&#38754;&#30340;&#28389;&#29992;&#12290;&#23613;&#31649;&#29616;&#26377;&#26041;&#27861;&#20391;&#37325;&#20110;&#26816;&#27979;&#65292;&#20294;&#26576;&#20123;&#24694;&#24847;&#28389;&#29992;&#38656;&#35201;&#36319;&#36394;&#23545;&#25163;&#29992;&#25143;&#20197;&#36827;&#34892;&#21453;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#22810;&#20301;&#27700;&#21360;&#36890;&#36807;&#39068;&#33394;&#32534;&#30721;&#8221;&#65288;COLOR&#65289;&#30340;&#26041;&#27861;&#65292;&#22312;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36807;&#31243;&#20013;&#23884;&#20837;&#21487;&#36861;&#36394;&#30340;&#22810;&#20301;&#20449;&#24687;&#12290;&#21033;&#29992;&#38646;&#20301;&#27700;&#21360;&#25216;&#26415;&#30340;&#20248;&#21183;&#65288;Kirchenbauer&#31561;&#65292;2023a&#65289;&#65292;COLOR&#23454;&#29616;&#20102;&#22312;&#27809;&#26377;&#27169;&#22411;&#35775;&#38382;&#26435;&#38480;&#30340;&#24773;&#20917;&#19979;&#25552;&#21462;&#27700;&#21360;&#12289;&#21363;&#26102;&#23884;&#20837;&#21644;&#32500;&#25345;&#25991;&#26412;&#36136;&#37327;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20801;&#35768;&#38646;&#20301;&#26816;&#27979;&#12290;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#20013;&#31561;&#38271;&#24230;&#30340;&#25991;&#26412;&#65288;&#32422;500&#20010;&#26631;&#35760;&#65289;&#20013;&#25104;&#21151;&#23884;&#20837;&#20102;32&#20301;&#28040;&#24687;&#65292;&#20934;&#30830;&#29575;&#20026;91.9&#65285;&#12290;&#36825;&#39033;&#24037;&#20316;&#26377;&#25928;&#22320;&#25512;&#36827;&#20102;&#23545;&#35821;&#35328;&#27169;&#22411;&#28389;&#29992;&#36827;&#34892;&#21453;&#21046;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study aims to proactively tackle misuse of large language models beyond identification of machine-generated text. While existing methods focus on detection, some malicious misuses demand tracing the adversary user for counteracting them. To address this, we propose "Multi-bit Watermark through Color-listing" (COLOR), embedding traceable multi-bit information during language model generation. Leveraging the benefits of zero-bit watermarking (Kirchenbauer et al., 2023a), COLOR enables extraction without model access, on-the-fly embedding, and maintains text quality, while allowing zero-bit detection all at the same time. Preliminary experiments demonstrates successful embedding of 32-bit messages with 91.9% accuracy in moderate-length texts ($\sim$500 tokens). This work advances strategies to counter language model misuse effectively.
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#27169;&#22411;&#20316;&#20026;&#22797;&#26434;&#31995;&#32479;&#31185;&#23398;&#65292;&#23427;&#20204;&#33021;&#22815;&#23436;&#25104;&#20219;&#21153;&#30340;&#34892;&#20026;&#34920;&#29616;&#38656;&#35201;&#34987;&#35299;&#37322;&#21644;&#29702;&#35299;&#65292;&#20197;&#23454;&#29616;&#23545;&#20854;&#34892;&#20026;&#30340;&#25511;&#21046;&#21644;&#26410;&#26469;&#30740;&#31350;&#30340;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2308.00189</link><description>&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#20316;&#20026;&#22797;&#26434;&#31995;&#32479;&#31185;&#23398;&#65306;&#22914;&#20309;&#29702;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#65311;
&lt;/p&gt;
&lt;p&gt;
Generative Models as a Complex Systems Science: How can we make sense of large language model behavior?. (arXiv:2308.00189v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00189
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#20316;&#20026;&#22797;&#26434;&#31995;&#32479;&#31185;&#23398;&#65292;&#23427;&#20204;&#33021;&#22815;&#23436;&#25104;&#20219;&#21153;&#30340;&#34892;&#20026;&#34920;&#29616;&#38656;&#35201;&#34987;&#35299;&#37322;&#21644;&#29702;&#35299;&#65292;&#20197;&#23454;&#29616;&#23545;&#20854;&#34892;&#20026;&#30340;&#25511;&#21046;&#21644;&#26410;&#26469;&#30740;&#31350;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#24341;&#23548;&#20986;&#26399;&#26395;&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#36991;&#20813;&#19981;&#33391;&#34892;&#20026;&#65292;&#37325;&#26032;&#23450;&#20041;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24182;&#27491;&#22312;&#37325;&#26032;&#22609;&#36896;&#25105;&#20204;&#19982;&#35745;&#31639;&#26426;&#30340;&#20132;&#20114;&#26041;&#24335;&#12290;&#26366;&#32463;&#26159;&#19968;&#20010;&#31185;&#23398;&#24037;&#31243;&#23398;&#31185;&#65292;&#23558;&#26500;&#24314;&#27169;&#22359;&#22534;&#21472;&#22312;&#19968;&#36215;&#65292;&#29616;&#22312;&#21487;&#20197;&#35828;&#24050;&#32463;&#26159;&#19968;&#20010;&#22797;&#26434;&#31995;&#32479;&#31185;&#23398;&#65292;&#20854;&#20013;&#23547;&#27714;&#20986;&#29616;&#30340;&#34892;&#20026;&#20197;&#25903;&#25345;&#20197;&#21069;&#26080;&#27861;&#24819;&#35937;&#30340;&#29992;&#20363;&#12290;&#23613;&#31649;&#26377;&#36234;&#26469;&#36234;&#22810;&#30340;&#22522;&#20934;&#27979;&#35797;&#26469;&#34913;&#37327;&#20219;&#21153;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#32570;&#20047;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#36825;&#20123;&#20219;&#21153;&#23436;&#25104;&#30340;&#34892;&#20026;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#24615;&#30340;&#21162;&#21147;&#65292;&#23558;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#20998;&#35299;&#20026;&#35299;&#37322;&#36328;&#20219;&#21153;&#24615;&#33021;&#30340;&#31867;&#21035;&#65292;&#20197;&#25351;&#23548;&#26426;&#26800;&#35299;&#37322;&#24182;&#24110;&#21161;&#26410;&#26469;&#20998;&#26512;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Coaxing out desired behavior from pretrained models, while avoiding undesirable ones, has redefined NLP and is reshaping how we interact with computers. What was once a scientific engineering discipline-in which building blocks are stacked one on top of the other-is arguably already a complex systems science, in which emergent behaviors are sought out to support previously unimagined use cases.  Despite the ever increasing number of benchmarks that measure task performance, we lack explanations of what behaviors language models exhibit that allow them to complete these tasks in the first place. We argue for a systematic effort to decompose language model behavior into categories that explain cross-task performance, to guide mechanistic explanations and help future-proof analytic research.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#40065;&#26834;&#30340;&#31070;&#32463;&#27861;&#24459;&#21028;&#26029;&#31995;&#32479;&#65292;&#36890;&#36807;&#23545;&#26089;&#26399;&#23384;&#22312;&#30340;&#31995;&#32479;&#36827;&#34892;&#23454;&#39564;&#21457;&#29616;&#23427;&#20204;&#26080;&#27861;&#22788;&#29702;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#32463;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#27861;&#24459;&#21028;&#26029;&#39044;&#27979;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2308.00165</link><description>&lt;p&gt;
&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#31070;&#32463;&#27861;&#24459;&#21028;&#26029;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Adversarially Robust Neural Legal Judgement Systems. (arXiv:2308.00165v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#40065;&#26834;&#30340;&#31070;&#32463;&#27861;&#24459;&#21028;&#26029;&#31995;&#32479;&#65292;&#36890;&#36807;&#23545;&#26089;&#26399;&#23384;&#22312;&#30340;&#31995;&#32479;&#36827;&#34892;&#23454;&#39564;&#21457;&#29616;&#23427;&#20204;&#26080;&#27861;&#22788;&#29702;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#32463;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#27861;&#24459;&#21028;&#26029;&#39044;&#27979;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27861;&#24459;&#21028;&#26029;&#39044;&#27979;&#26159;&#26681;&#25454;&#26696;&#24773;&#25551;&#36848;&#26469;&#39044;&#27979;&#27861;&#38498;&#26696;&#20214;&#32467;&#26524;&#30340;&#20219;&#21153;&#12290;&#36825;&#20123;&#20219;&#21153;&#24212;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#26469;&#22522;&#20110;&#20107;&#23454;&#39044;&#27979;&#27861;&#24459;&#21028;&#26029;&#32467;&#26524;&#12290;&#26368;&#36817;&#65292;&#22823;&#35268;&#27169;&#20844;&#24320;&#25968;&#25454;&#38598;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#24050;&#32463;&#22686;&#21152;&#20102;&#19982;&#27861;&#24459;&#21028;&#26029;&#39044;&#27979;&#31995;&#32479;&#30456;&#20851;&#30340;&#30740;&#31350;&#12290;&#20026;&#20102;&#20351;&#36825;&#31181;&#31995;&#32479;&#22312;&#23454;&#36341;&#20013;&#26377;&#25152;&#24110;&#21161;&#65292;&#23427;&#20204;&#24212;&#35813;&#33021;&#22815;&#25269;&#24481;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#26500;&#24314;&#31070;&#32463;&#27861;&#24459;&#21028;&#26029;&#31995;&#32479;&#19978;&#65292;&#20294;&#23545;&#20110;&#21019;&#24314;&#40065;&#26834;&#30340;&#27861;&#24459;&#21028;&#26029;&#39044;&#27979;&#31995;&#32479;&#65288;LJP&#65289;&#20960;&#20046;&#27809;&#26377;&#32473;&#20104;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#25105;&#20204;&#23545;&#26089;&#26399;&#23384;&#22312;&#30340;LJP&#31995;&#32479;&#36827;&#34892;&#20102;&#23545;&#25239;&#24615;&#25915;&#20987;&#23454;&#39564;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#37117;&#26080;&#27861;&#22788;&#29702;&#25915;&#20987;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26500;&#24314;&#40065;&#26834;LJP&#31995;&#32479;&#30340;&#26041;&#27861;&#12290;&#22312;&#19977;&#20010;&#27861;&#24459;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;LJP&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Legal judgment prediction is the task of predicting the outcome of court cases on a given text description of facts of cases. These tasks apply Natural Language Processing (NLP) techniques to predict legal judgment results based on facts. Recently, large-scale public datasets and NLP models have increased research in areas related to legal judgment prediction systems. For such systems to be practically helpful, they should be robust from adversarial attacks. Previous works mainly focus on making a neural legal judgement system; however, significantly less or no attention has been given to creating a robust Legal Judgement Prediction(LJP) system. We implemented adversarial attacks on early existing LJP systems and found that none of them could handle attacks. In this work, we proposed an approach for making robust LJP systems. Extensive experiments on three legal datasets show significant improvements in our approach over the state-of-the-art LJP system in handling adversarial attacks. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;Fine-Tuned&#30340;OpenAI LLM&#36827;&#34892;&#32763;&#35793;&#36136;&#37327;&#20272;&#35745;&#30340;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;&#21487;&#20197;&#36890;&#36807;Fine-Tuned&#30340;ChatGPT&#26469;&#39044;&#27979;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2308.00158</link><description>&lt;p&gt;
&#20351;&#29992;Fine-Tuned&#30340;OpenAI LLM&#39044;&#27979;&#26426;&#22120;&#32763;&#35793;&#36755;&#20986;&#20013;&#30340;&#23436;&#32654;&#36136;&#37327;&#27573;&#33853;&#65306;&#26159;&#21542;&#21487;&#20197;&#20174;&#21382;&#21490;&#25968;&#25454;&#20013;&#25429;&#25417;&#32534;&#36753;&#36317;&#31163;&#27169;&#24335;&#65311;
&lt;/p&gt;
&lt;p&gt;
Predicting Perfect Quality Segments in MT Output with Fine-Tuned OpenAI LLM: Is it possible to capture editing distance patterns from historical data?. (arXiv:2308.00158v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;Fine-Tuned&#30340;OpenAI LLM&#36827;&#34892;&#32763;&#35793;&#36136;&#37327;&#20272;&#35745;&#30340;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;&#21487;&#20197;&#36890;&#36807;Fine-Tuned&#30340;ChatGPT&#26469;&#39044;&#27979;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32763;&#35793;&#36136;&#37327;&#20272;&#35745;&#65288;TQE&#65289;&#26159;&#23558;&#36755;&#20986;&#32763;&#35793;&#37096;&#32626;&#21040;&#20351;&#29992;&#20013;&#20043;&#21069;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290; TQE&#23545;&#20110;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#21644;&#20154;&#24037;&#32763;&#35793;&#65288;HT&#65289;&#30340;&#36136;&#37327;&#20063;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#32780;&#19981;&#38656;&#35201;&#26597;&#30475;&#21442;&#32771;&#32763;&#35793;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26816;&#26597;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#21487;&#20197;&#20026;TQE&#20219;&#21153;&#21644;&#23427;&#20204;&#30340;&#33021;&#21147;&#36827;&#34892;Fine-Tune&#12290;&#25105;&#20204;&#20197;ChatGPT&#20026;&#20363;&#65292;&#23558;TQE&#35270;&#20026;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#12290;&#20351;&#29992;&#33521;&#24847;&#21644;&#33521;&#24503;&#35757;&#32451;&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36890;&#36807;ChatGPT&#30340;API Fine-Tuned&#21487;&#20197;&#22312;&#39044;&#27979;&#32763;&#35793;&#36136;&#37327;&#26041;&#38754;&#33719;&#24471;&#30456;&#23545;&#36739;&#39640;&#30340;&#24471;&#20998;&#65292;&#21363;&#26159;&#21542;&#38656;&#35201;&#32534;&#36753;&#32763;&#35793;&#65292;&#20294;&#32943;&#23450;&#26377;&#25913;&#36827;&#20934;&#30830;&#24615;&#30340;&#31354;&#38388;&#12290;&#33521;&#24847;&#21452;&#35821;&#25688;&#35201;&#21487;&#22312;&#35770;&#25991;&#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Translation Quality Estimation (TQE) is an important step before deploying the output translation into usage. TQE is also critical in assessing machine translation (MT) and human translation (HT) quality without seeing the reference translations. In this work, we examine if the state-of-the-art large language models (LLMs) can be fine-tuned for the TQE task and their capability. We take ChatGPT as one example and approach TQE as a binary classification task. Using English-Italian and English-German training corpus, our experimental results show that fine-tuned ChatGPT via its API can achieve a relatively high score on predicting translation quality, i.e. if the translation needs to be edited, but there is definitely space to improve the accuracy. English-Italiano bilingual Abstract is available in the paper.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36890;&#29992;&#27169;&#22411;&#21021;&#22987;&#21270;&#21644;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#24494;&#35843;&#65292;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#25552;&#21319;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#24402;&#19968;&#21270;&#30340;&#25928;&#26524;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.00157</link><description>&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#25552;&#21319;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#24402;&#19968;&#21270;&#65306;&#36890;&#29992;&#27169;&#22411;&#21021;&#22987;&#21270;&#21644;&#29983;&#29289;&#21307;&#23398;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#22312;&#38750;&#27491;&#24335;&#29615;&#22659;&#20013;&#21463;&#30410;&#30340;&#38646;&#26679;&#20363;&#38142;&#25509;
&lt;/p&gt;
&lt;p&gt;
Boosting Adverse Drug Event Normalization on Social Media: General-Purpose Model Initialization and Biomedical Semantic Text Similarity Benefit Zero-Shot Linking in Informal Contexts. (arXiv:2308.00157v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36890;&#29992;&#27169;&#22411;&#21021;&#22987;&#21270;&#21644;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#24494;&#35843;&#65292;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#25552;&#21319;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#24402;&#19968;&#21270;&#30340;&#25928;&#26524;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#65292;&#20063;&#34987;&#31216;&#20026;&#29983;&#29289;&#21307;&#23398;&#27010;&#24565;&#24402;&#19968;&#21270;&#65292;&#26368;&#36817;&#20986;&#29616;&#20102;&#38646;&#26679;&#20363;&#23545;&#27604;&#27169;&#22411;&#30340;&#20852;&#36215;&#12290;&#28982;&#32780;&#65292;&#30452;&#21040;&#29616;&#22312;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#26448;&#26009;&#20027;&#35201;&#21253;&#25324;&#19987;&#38376;&#30340;&#29983;&#29289;&#21307;&#23398;&#20869;&#23481;&#65292;&#27604;&#22914;MIMIC-III&#20020;&#24202;&#31508;&#35760;&#21644;PubMed&#35770;&#25991;&#12290;&#34429;&#28982;&#20135;&#29983;&#30340;&#39046;&#22495;&#20869;&#27169;&#22411;&#22312;&#35768;&#22810;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#20013;&#24050;&#32463;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#19978;&#30340;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#24402;&#19968;&#21270;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;BioLORD&#30340;&#36890;&#29992;&#27169;&#22411;&#21021;&#22987;&#21270;&#21644;&#21517;&#20026;STS&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#24494;&#35843;&#65292;&#23545;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#24402;&#19968;&#21270;&#36827;&#34892;&#30740;&#31350;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biomedical entity linking, also known as biomedical concept normalization, has recently witnessed the rise to prominence of zero-shot contrastive models. However, the pre-training material used for these models has, until now, largely consisted of specialist biomedical content such as MIMIC-III clinical notes (Johnson et al., 2016) and PubMed papers (Sayers et al., 2021; Gao et al., 2020). While the resulting in-domain models have shown promising results for many biomedical tasks, adverse drug event normalization on social media texts has so far remained challenging for them (Portelli et al., 2022). In this paper, we propose a new approach for adverse drug event normalization on social media relying on general-purpose model initialization via BioLORD (Remy et al., 2022) and a semantic-text-similarity fine-tuning named STS. Our experimental results on several social media datasets demonstrate the effectiveness of our proposed approach, by achieving state-of-the-art performance. Based on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#38899;&#25968;&#25454;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#20027;&#35201;&#20851;&#27880;&#22810;&#31181;&#35774;&#32622;&#21644;&#22810;&#31181;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#22823;&#37327;&#26080;&#26631;&#31614;&#21644;&#24369;&#26631;&#31614;&#25968;&#25454;&#20197;&#21450;&#38468;&#21152;&#25968;&#25454;&#27169;&#24577;&#65292;&#25913;&#36827;&#19979;&#28216;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.00129</link><description>&lt;p&gt;
&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#65306;&#20351;&#29992;&#21333;&#35270;&#22270;&#12289;&#22810;&#35270;&#22270;&#21644;&#22810;&#20219;&#21153;&#26041;&#27861;&#23398;&#20064;&#21452;&#21521;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Speech representation learning: Learning bidirectional encoders with single-view, multi-view, and multi-task methods. (arXiv:2308.00129v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#38899;&#25968;&#25454;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#20027;&#35201;&#20851;&#27880;&#22810;&#31181;&#35774;&#32622;&#21644;&#22810;&#31181;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#22823;&#37327;&#26080;&#26631;&#31614;&#21644;&#24369;&#26631;&#31614;&#25968;&#25454;&#20197;&#21450;&#38468;&#21152;&#25968;&#25454;&#27169;&#24577;&#65292;&#25913;&#36827;&#19979;&#28216;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38598;&#20013;&#30740;&#31350;&#20102;&#38024;&#23545;&#26102;&#38388;&#25110;&#31354;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#25913;&#36827;&#19979;&#28216;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#12290;&#26377;&#30417;&#30563;&#23398;&#20064;&#19968;&#30452;&#26159;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#33391;&#22909;&#39034;&#24207;&#34920;&#31034;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#25193;&#23637;&#26377;&#30417;&#30563;&#23398;&#20064;&#30340;&#19968;&#20010;&#38480;&#21046;&#22240;&#32032;&#26159;&#32570;&#20047;&#36275;&#22815;&#30340;&#27880;&#37322;&#25968;&#25454;&#12290;&#21463;&#21040;&#36825;&#19968;&#25361;&#25112;&#30340;&#21551;&#21457;&#65292;&#33258;&#28982;&#32780;&#28982;&#22320;&#25506;&#32034;&#33021;&#22815;&#21033;&#29992;&#22823;&#37327;&#26080;&#26631;&#31614;&#21644;&#24369;&#26631;&#31614;&#25968;&#25454;&#20197;&#21450;&#38468;&#21152;&#25968;&#25454;&#27169;&#24577;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#23545;&#35821;&#38899;&#25968;&#25454;&#30340;&#24191;&#27867;&#30740;&#31350;&#12290;&#19982;&#22823;&#22810;&#25968;&#20851;&#27880;&#21333;&#19968;&#23398;&#20064;&#35774;&#32622;&#30340;&#20854;&#20182;&#20316;&#21697;&#19981;&#21516;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#31181;&#35774;&#32622;&#65306;&#24102;&#36741;&#21161;&#25439;&#22833;&#30340;&#26377;&#30417;&#30563;&#23398;&#20064;&#12289;&#26080;&#30417;&#30563;&#23398;&#20064;&#12289;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#22810;&#35270;&#22270;&#23398;&#20064;&#12290;&#38500;&#20102;&#19981;&#21516;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#26412;&#25991;&#36824;&#25506;&#35752;&#20102;&#22810;&#31181;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This thesis focuses on representation learning for sequence data over time or space, aiming to improve downstream sequence prediction tasks by using the learned representations. Supervised learning has been the most dominant approach for training deep neural networks for learning good sequential representations. However, one limiting factor to scale supervised learning is the lack of enough annotated data. Motivated by this challenge, it is natural to explore representation learning methods that can utilize large amounts of unlabeled and weakly labeled data, as well as an additional data modality. I describe my broad study of representation learning for speech data. Unlike most other works that focus on a single learning setting, this thesis studies multiple settings: supervised learning with auxiliary losses, unsupervised learning, semi-supervised learning, and multi-view learning. Besides different learning problems, I also explore multiple approaches for representation learning. Tho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT3.5&#65289;&#20316;&#20026;AI&#21161;&#25163;&#26469;&#22686;&#24378;&#28183;&#36879;&#27979;&#35797;&#20154;&#21592;&#30340;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#39640;&#32423;&#20219;&#21153;&#35268;&#21010;&#21644;&#20302;&#32423;&#28431;&#27934;&#23547;&#25214;&#20004;&#31181;&#29992;&#20363;&#65292;&#21462;&#24471;&#20102;&#26377;&#21069;&#26223;&#30340;&#21021;&#27493;&#32467;&#26524;&#65292;&#24182;&#23601;&#25552;&#20379;&#35813;&#25216;&#26415;&#30340;&#20262;&#29702;&#38382;&#39064;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2308.00121</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28183;&#36879;&#27979;&#35797;&#65306;AI&#20316;&#20026;&#36741;&#21161;
&lt;/p&gt;
&lt;p&gt;
Getting pwn'd by AI: Penetration Testing with Large Language Models. (arXiv:2308.00121v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT3.5&#65289;&#20316;&#20026;AI&#21161;&#25163;&#26469;&#22686;&#24378;&#28183;&#36879;&#27979;&#35797;&#20154;&#21592;&#30340;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#39640;&#32423;&#20219;&#21153;&#35268;&#21010;&#21644;&#20302;&#32423;&#28431;&#27934;&#23547;&#25214;&#20004;&#31181;&#29992;&#20363;&#65292;&#21462;&#24471;&#20102;&#26377;&#21069;&#26223;&#30340;&#21021;&#27493;&#32467;&#26524;&#65292;&#24182;&#23601;&#25552;&#20379;&#35813;&#25216;&#26415;&#30340;&#20262;&#29702;&#38382;&#39064;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#23433;&#20840;&#27979;&#35797;&#39046;&#22495;&#65292;&#23588;&#20854;&#26159;&#28183;&#36879;&#27979;&#35797;&#26159;&#19968;&#39033;&#38656;&#35201;&#39640;&#27700;&#24179;&#19987;&#19994;&#30693;&#35782;&#30340;&#27963;&#21160;&#65292;&#24182;&#28041;&#21450;&#35768;&#22810;&#25163;&#21160;&#27979;&#35797;&#21644;&#20998;&#26512;&#27493;&#39588;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT3.5&#65289;&#26469;&#22686;&#24378;&#28183;&#36879;&#27979;&#35797;&#20154;&#21592;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#29992;&#20363;&#65306;&#29992;&#20110;&#23433;&#20840;&#27979;&#35797;&#20219;&#21153;&#30340;&#39640;&#32423;&#20219;&#21153;&#35268;&#21010;&#21644;&#22312;&#26131;&#21463;&#25915;&#20987;&#30340;&#34394;&#25311;&#26426;&#20013;&#36827;&#34892;&#20302;&#32423;&#28431;&#27934;&#23547;&#25214;&#12290;&#23545;&#20110;&#21518;&#32773;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;&#38381;&#29615;&#21453;&#39304;&#65292;&#23558;&#30001;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20302;&#32423;&#25805;&#20316;&#19982;&#26131;&#21463;&#25915;&#20987;&#30340;&#34394;&#25311;&#26426;&#65288;&#36890;&#36807;SSH&#36830;&#25509;&#65289;&#30456;&#36830;&#65292;&#24182;&#20801;&#35768;&#35821;&#35328;&#27169;&#22411;&#20998;&#26512;&#34394;&#25311;&#26426;&#29366;&#24577;&#20197;&#23547;&#25214;&#28431;&#27934;&#65292;&#24182;&#25552;&#20379;&#20855;&#20307;&#30340;&#25915;&#20987;&#21521;&#37327;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#26377;&#21069;&#26223;&#30340;&#21021;&#27493;&#32467;&#26524;&#65292;&#35814;&#32454;&#20171;&#32461;&#20102;&#25913;&#36827;&#30340;&#36884;&#24452;&#65292;&#24182;&#23601;&#25552;&#20379;&#35813;&#25216;&#26415;&#30340;&#20262;&#29702;&#38382;&#39064;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of software security testing, more specifically penetration testing, is an activity that requires high levels of expertise and involves many manual testing and analysis steps. This paper explores the potential usage of large-language models, such as GPT3.5, to augment penetration testers with AI sparring partners. We explore the feasibility of supplementing penetration testers with AI models for two distinct use cases: high-level task planning for security testing assignments and low-level vulnerability hunting within a vulnerable virtual machine. For the latter, we implemented a closed-feedback loop between LLM-generated low-level actions with a vulnerable virtual machine (connected through SSH) and allowed the LLM to analyze the machine state for vulnerabilities and suggest concrete attack vectors which were automatically executed within the virtual machine. We discuss promising initial results, detail avenues for improvement, and close deliberating on the ethics of providi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#27169;&#22359;&#21270;MODS&#26412;&#20307;&#35770;&#65288;MMODS-O&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#20803;&#25968;&#25454;&#23545;&#35937;&#25551;&#36848;&#27169;&#24335;&#65288;MODS&#65289;&#22312;&#30693;&#35782;&#22270;&#35889;&#29615;&#22659;&#19979;&#30340;&#38480;&#21046;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#27169;&#22359;&#21270;&#26412;&#20307;&#35770;&#35774;&#35745;&#26041;&#27861;&#23398;&#65288;MOMo&#65289;&#23454;&#29616;&#20102;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2308.00116</link><description>&lt;p&gt;
MODS&#30340;&#27169;&#22359;&#21270;&#26412;&#20307;&#35770; - &#20803;&#25968;&#25454;&#23545;&#35937;&#25551;&#36848;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
A Modular Ontology for MODS -- Metadata Object Description Schema. (arXiv:2308.00116v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#27169;&#22359;&#21270;MODS&#26412;&#20307;&#35770;&#65288;MMODS-O&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#20803;&#25968;&#25454;&#23545;&#35937;&#25551;&#36848;&#27169;&#24335;&#65288;MODS&#65289;&#22312;&#30693;&#35782;&#22270;&#35889;&#29615;&#22659;&#19979;&#30340;&#38480;&#21046;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#27169;&#22359;&#21270;&#26412;&#20307;&#35770;&#35774;&#35745;&#26041;&#27861;&#23398;&#65288;MOMo&#65289;&#23454;&#29616;&#20102;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#25968;&#25454;&#23545;&#35937;&#25551;&#36848;&#27169;&#24335;&#65288;MODS&#65289;&#26159;&#29992;&#20110;&#25551;&#36848;&#22270;&#20070;&#27010;&#24565;&#21644;&#20803;&#25968;&#25454;&#30340;&#65292;&#24182;&#30001;&#32654;&#22269;&#22269;&#20250;&#22270;&#20070;&#39302;&#32500;&#25252;&#12290;MODS&#30340;&#26631;&#20934;&#29256;&#26412;&#26159;&#22522;&#20110;XML&#24605;&#32500;&#30340;XML&#27169;&#24335;&#65292;&#36825;&#24847;&#21619;&#30528;&#23427;&#22312;&#30693;&#35782;&#22270;&#35889;&#29615;&#22659;&#19979;&#23384;&#22312;&#19968;&#20123;&#37325;&#35201;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#27169;&#22359;&#21270;MODS&#26412;&#20307;&#35770;&#65288;MMODS-O&#65289;&#65292;&#23427;&#38598;&#25104;&#20102;MODS&#30340;&#25152;&#26377;&#20803;&#32032;&#21644;&#23646;&#24615;&#12290;&#22312;&#35774;&#35745;&#26412;&#20307;&#35770;&#26102;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#26368;&#36817;&#30340;&#27169;&#22359;&#21270;&#26412;&#20307;&#35770;&#35774;&#35745;&#26041;&#27861;&#23398;&#65288;MOMo&#65289;&#65292;&#26088;&#22312;&#22312;&#20445;&#23432;&#22320;&#19982;MODS&#21521;&#21518;&#20860;&#23481;&#30340;&#21516;&#26102;&#65292;&#22312;&#27169;&#22359;&#21270;&#21644;&#39640;&#36136;&#37327;&#26412;&#20307;&#35770;&#35774;&#35745;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Metadata Object Description Schema (MODS) was developed to describe bibliographic concepts and metadata and is maintained by the Library of Congress. Its authoritative version is given as an XML schema based on an XML mindset which means that it has significant limitations for use in a knowledge graphs context. We have therefore developed the Modular MODS Ontology (MMODS-O) which incorporates all elements and attributes of the MODS XML schema. In designing the ontology, we adopt the recent Modular Ontology Design Methodology (MOMo) with the intention to strike a balance between modularity and quality ontology design on the one hand, and conservative backward compatibility with MODS on the other.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19977;&#31181;&#22522;&#20110;&#29702;&#35770;&#21644;&#23454;&#35777;&#32771;&#34385;&#30340;&#26041;&#27861;&#65292;&#24041;&#22266;&#20102;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27700;&#21360;&#25216;&#26415;&#12290;&#26032;&#30340;&#32479;&#35745;&#26816;&#39564;&#26041;&#27861;&#33021;&#22815;&#22312;&#20302;&#38169;&#35823;&#38451;&#24615;&#29575;&#19979;&#25552;&#20379;&#31283;&#23450;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#19982;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#32463;&#20856;&#22522;&#20934;&#27979;&#35797;&#30456;&#27604;&#65292;&#27700;&#21360;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#24182;&#19988;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#20808;&#36827;&#30340;&#26816;&#27979;&#26041;&#26696;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35775;&#38382;&#26435;&#38480;&#21644;&#22810;&#20301;&#27700;&#21360;&#25216;&#26415;&#30340;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2308.00113</link><description>&lt;p&gt;
&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19977;&#20010;&#26041;&#27861;&#24041;&#22266;&#27700;&#21360;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Three Bricks to Consolidate Watermarks for Large Language Models. (arXiv:2308.00113v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00113
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19977;&#31181;&#22522;&#20110;&#29702;&#35770;&#21644;&#23454;&#35777;&#32771;&#34385;&#30340;&#26041;&#27861;&#65292;&#24041;&#22266;&#20102;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27700;&#21360;&#25216;&#26415;&#12290;&#26032;&#30340;&#32479;&#35745;&#26816;&#39564;&#26041;&#27861;&#33021;&#22815;&#22312;&#20302;&#38169;&#35823;&#38451;&#24615;&#29575;&#19979;&#25552;&#20379;&#31283;&#23450;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#19982;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#32463;&#20856;&#22522;&#20934;&#27979;&#35797;&#30456;&#27604;&#65292;&#27700;&#21360;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#24182;&#19988;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#20808;&#36827;&#30340;&#26816;&#27979;&#26041;&#26696;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35775;&#38382;&#26435;&#38480;&#21644;&#22810;&#20301;&#27700;&#21360;&#25216;&#26415;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21028;&#26029;&#29983;&#25104;&#25991;&#26412;&#21644;&#33258;&#28982;&#25991;&#26412;&#20043;&#38388;&#30340;&#24046;&#24322;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#32972;&#26223;&#19979;&#65292;&#27700;&#21360;&#25216;&#26415;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#23558;&#29983;&#25104;&#25991;&#26412;&#24402;&#23646;&#20110;&#29305;&#23450;&#27169;&#22411;&#30340;&#26377;&#21069;&#26223;&#30340;&#25216;&#26415;&#12290;&#23427;&#25913;&#21464;&#20102;&#37319;&#26679;&#29983;&#25104;&#36807;&#31243;&#65292;&#30041;&#19979;&#20102;&#26080;&#24418;&#30340;&#30165;&#36857;&#22312;&#29983;&#25104;&#30340;&#36755;&#20986;&#20013;&#65292;&#20197;&#20415;&#20110;&#21518;&#32493;&#30340;&#26816;&#27979;&#12290;&#26412;&#30740;&#31350;&#22522;&#20110;&#19977;&#20010;&#29702;&#35770;&#21644;&#23454;&#35777;&#32771;&#34385;&#65292;&#24041;&#22266;&#20102;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27700;&#21360;&#25216;&#26415;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#32479;&#35745;&#26816;&#39564;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#29282;&#22266;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#21363;&#20351;&#22312;&#20302;&#38169;&#35823;&#38451;&#24615;&#29575;&#19979;&#65288;&#23567;&#20110;10^(-6)&#65289;&#65292;&#36825;&#20123;&#20445;&#35777;&#20381;&#28982;&#26377;&#25928;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#20351;&#29992;&#32463;&#20856;&#22522;&#20934;&#27979;&#35797;&#23545;&#27604;&#20102;&#27700;&#21360;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#65292;&#20174;&#32780;&#33719;&#24471;&#20102;&#20851;&#20110;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#34892;&#24615;&#30340;&#35265;&#35299;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#20026;&#21487;&#20197;&#35775;&#38382;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#26223;&#20197;&#21450;&#22810;&#20301;&#27700;&#21360;&#25216;&#26415;&#24320;&#21457;&#20102;&#20808;&#36827;&#30340;&#26816;&#27979;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of discerning between generated and natural texts is increasingly challenging. In this context, watermarking emerges as a promising technique for ascribing generated text to a specific model. It alters the sampling generation process so as to leave an invisible trace in the generated output, facilitating later detection. This research consolidates watermarks for large language models based on three theoretical and empirical considerations. First, we introduce new statistical tests that offer robust theoretical guarantees which remain valid even at low false-positive rates (less than 10$^{\text{-6}}$). Second, we compare the effectiveness of watermarks using classical benchmarks in the field of natural language processing, gaining insights into their real-world applicability. Third, we develop advanced detection schemes for scenarios where access to the LLM is available, as well as multi-bit watermarking.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#29702;&#35770;&#20449;&#24687;&#20016;&#23500;&#34920;&#31034;&#21644;&#38750;&#29702;&#35770;&#24378;&#22823;&#26426;&#26800;&#24037;&#20855;&#30340;&#36129;&#29486;&#65292;&#24182;&#25351;&#20986;&#24403;&#21069;&#30340;&#27169;&#22411;&#21457;&#23637;&#21644;&#21033;&#29992;&#20013;&#20173;&#28982;&#32570;&#20047;&#20851;&#38190;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.00109</link><description>&lt;p&gt;
&#19968;&#21477;&#35805;&#32988;&#21315;&#24352;&#22270;&#29255;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#29702;&#35299;&#20154;&#31867;&#35821;&#35328;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
A Sentence is Worth a Thousand Pictures: Can Large Language Models Understand Human Language?. (arXiv:2308.00109v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#29702;&#35770;&#20449;&#24687;&#20016;&#23500;&#34920;&#31034;&#21644;&#38750;&#29702;&#35770;&#24378;&#22823;&#26426;&#26800;&#24037;&#20855;&#30340;&#36129;&#29486;&#65292;&#24182;&#25351;&#20986;&#24403;&#21069;&#30340;&#27169;&#22411;&#21457;&#23637;&#21644;&#21033;&#29992;&#20013;&#20173;&#28982;&#32570;&#20047;&#20851;&#38190;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#22312;&#20381;&#36182;&#20110;&#19979;&#19968;&#20010;&#21333;&#35789;&#39044;&#27979;&#30340;&#35821;&#35328;&#30456;&#20851;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#24403;&#21069;&#19968;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#35748;&#20026;&#33021;&#22815;&#36798;&#21040;&#31867;&#20154;&#35821;&#35328;&#34920;&#29616;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#24212;&#29992;&#34987;&#35465;&#20026;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#21516;&#26102;&#20063;&#26159;&#23545;&#20154;&#31867;&#35821;&#35328;&#35748;&#30693;&#21644;&#31070;&#32463;&#22522;&#30784;&#30340;&#37325;&#22823;&#36827;&#23637;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#30446;&#26631;&#31995;&#32479;&#30340;&#29702;&#35770;&#20449;&#24687;&#20016;&#23500;&#34920;&#31034;&#19982;&#38750;&#29702;&#35770;&#24378;&#22823;&#26426;&#26800;&#24037;&#20855;&#30340;&#36129;&#29486;&#65292;&#24182;&#30830;&#23450;&#20102;&#24403;&#21069;&#21457;&#23637;&#21644;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#25152;&#32570;&#22833;&#30340;&#20851;&#38190;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence applications show great potential for language-related tasks that rely on next-word prediction. The current generation of large language models have been linked to claims about human-like linguistic performance and their applications are hailed both as a key step towards Artificial General Intelligence and as major advance in understanding the cognitive, and even neural basis of human language. We analyze the contribution of large language models as theoretically informative representations of a target system vs. atheoretical powerful mechanistic tools, and we identify the key abilities that are still missing from the current state of development and exploitation of these models.
&lt;/p&gt;</description></item><item><title>DPBERT&#26159;&#19968;&#20010;&#22522;&#20110;&#21160;&#24577;&#35268;&#21010;&#30340;&#39640;&#25928;BERT&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#19968;&#37096;&#20998;transformer&#23618;&#26469;&#21152;&#36895;&#25512;&#29702;&#36807;&#31243;&#65292;&#22312;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#38477;&#20302;&#24310;&#36831;&#12290;</title><link>http://arxiv.org/abs/2308.00108</link><description>&lt;p&gt;
DPBERT: &#22522;&#20110;&#21160;&#24577;&#35268;&#21010;&#30340;&#39640;&#25928;BERT&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
DPBERT: Efficient Inference for BERT based on Dynamic Planning. (arXiv:2308.00108v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00108
&lt;/p&gt;
&lt;p&gt;
DPBERT&#26159;&#19968;&#20010;&#22522;&#20110;&#21160;&#24577;&#35268;&#21010;&#30340;&#39640;&#25928;BERT&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#19968;&#37096;&#20998;transformer&#23618;&#26469;&#21152;&#36895;&#25512;&#29702;&#36807;&#31243;&#65292;&#22312;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#38477;&#20302;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT&#65289;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#21457;&#23637;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#38590;&#20197;&#24212;&#29992;&#20110;&#35745;&#31639;&#33021;&#21147;&#26377;&#38480;&#30340;&#31227;&#21160;&#35774;&#22791;&#19978;&#12290;&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#33258;&#36866;&#24212;&#36755;&#20837;&#25512;&#29702;&#26041;&#27861;&#30340;&#32570;&#28857;&#65292;&#36825;&#20123;&#26041;&#27861;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;BERT&#30340;&#32467;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DPBERT&#65292;&#19968;&#31181;&#26032;&#30340;&#24494;&#35843;&#31574;&#30053;&#65292;&#36890;&#36807;&#36873;&#25321;BERT&#30340;&#19968;&#37096;&#20998;transformer&#23618;&#20316;&#20026;&#35745;&#31639;&#36335;&#24452;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21152;&#36895;BERT&#30340;&#25512;&#29702;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21407;&#22987;BERT&#27169;&#22411;&#20013;&#28155;&#21152;&#20102;&#19968;&#20010;&#35268;&#21010;&#27169;&#22359;&#65292;&#29992;&#20110;&#30830;&#23450;&#25512;&#29702;&#36807;&#31243;&#20013;&#26159;&#21542;&#21253;&#21547;&#25110;&#32469;&#36807;&#26576;&#20010;&#23618;&#12290;&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20445;&#25345;98%&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#24310;&#36831;&#38477;&#20302;&#21040;75%&#65292;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#33258;&#36866;&#24212;&#36755;&#20837;&#26041;&#27861;&#65292;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#20934;&#30830;&#24230;&#21644;&#36895;&#24230;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale pre-trained language models such as BERT have contributed significantly to the development of NLP. However, those models require large computational resources, making it difficult to be applied to mobile devices where computing power is limited. In this paper we aim to address the weakness of existing input-adaptive inference methods which fail to take full advantage of the structure of BERT. We propose Dynamic Planning in BERT, a novel fine-tuning strategy that can accelerate the inference process of BERT through selecting a subsequence of transformer layers list of backbone as a computational path for an input sample. To do this, our approach adds a planning module to the original BERT model to determine whether a layer is included or bypassed during inference. Experimental results on the GLUE benchmark exhibit that our method reduces latency to 75\% while maintaining 98\% accuracy, yielding a better accuracy-speed trade-off compared to state-of-the-art input-adaptive met
&lt;/p&gt;</description></item><item><title>&#39564;&#35777;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#23398;&#20064;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#30340;&#25968;&#25454;&#25277;&#35937;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#35813;&#24037;&#20855;&#22312;&#25552;&#21462;&#20449;&#24687;&#30340;&#36895;&#24230;&#19978;&#20248;&#20110;&#21307;&#24072;&#20154;&#24037;&#25277;&#35937;&#32773;&#65292;&#24182;&#22312;&#20934;&#30830;&#24615;&#19978;&#38750;&#21155;&#12290;</title><link>http://arxiv.org/abs/2308.00107</link><description>&lt;p&gt;
&#39564;&#35777;&#19968;&#31181;&#38024;&#23545;&#38750;&#32467;&#26500;&#21270;&#21307;&#30103;&#25968;&#25454;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#30340;&#25968;&#25454;&#25277;&#35937;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Validation of a Zero-Shot Learning Natural Language Processing Tool for Data Abstraction from Unstructured Healthcare Data. (arXiv:2308.00107v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00107
&lt;/p&gt;
&lt;p&gt;
&#39564;&#35777;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#23398;&#20064;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#30340;&#25968;&#25454;&#25277;&#35937;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#35813;&#24037;&#20855;&#22312;&#25552;&#21462;&#20449;&#24687;&#30340;&#36895;&#24230;&#19978;&#20248;&#20110;&#21307;&#24072;&#20154;&#24037;&#25277;&#35937;&#32773;&#65292;&#24182;&#22312;&#20934;&#30830;&#24615;&#19978;&#38750;&#21155;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#25551;&#36848;&#19968;&#31181;&#29992;&#20110;&#20174;PDF&#25991;&#26723;&#20013;&#25552;&#21462;&#25968;&#25454;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24037;&#20855;&#30340;&#24320;&#21457;&#21644;&#39564;&#35777;&#36807;&#31243;&#65292;&#20363;&#22914;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#25991;&#26412;&#12290;&#26448;&#26009;&#21644;&#26041;&#27861;&#65306;&#22522;&#20110;OpenAI&#30340;GPT-3.5&#27169;&#22411;&#24320;&#21457;&#20102;&#19968;&#31181;&#25968;&#25454;&#25277;&#35937;&#24037;&#20855;&#65292;&#24182;&#19982;&#19977;&#20301;&#21307;&#24072;&#20154;&#24037;&#25277;&#35937;&#32773;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#35780;&#20272;&#22312;&#25552;&#21462;199&#20221;&#21435;&#26631;&#35782;&#21270;&#30340;&#28608;&#32032;&#20999;&#38500;&#26415;&#30149;&#29702;&#25253;&#21578;&#20013;&#30340;14&#20010;&#29420;&#29305;&#21464;&#37327;&#30340;&#25968;&#25454;&#26102;&#65292;&#23436;&#25104;&#20219;&#21153;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#20934;&#30830;&#24615;&#12290;&#36825;&#20123;&#25253;&#21578;&#20197;&#21521;&#37327;&#21270;&#21644;&#25195;&#25551;&#26684;&#24335;&#36827;&#34892;&#20102;&#22788;&#29702;&#65292;&#20197;&#30830;&#23450;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#23545;&#25968;&#25454;&#25277;&#35937;&#30340;&#24433;&#21709;&#12290;&#35813;&#24037;&#20855;&#22312;&#25968;&#25454;&#25277;&#35937;&#36895;&#24230;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#35777;&#26126;&#20102;&#20934;&#30830;&#24615;&#30340;&#38750;&#21155;&#24615;&#12290;&#32467;&#26524;&#65306;&#20154;&#24037;&#25277;&#35937;&#32773;&#27599;&#20221;&#25253;&#21578;&#38656;&#35201;&#24179;&#22343;101&#31186;&#36827;&#34892;&#25968;&#25454;&#25277;&#35937;&#65292;&#26102;&#38388;&#33539;&#22260;&#20026;15&#31186;&#33267;284&#31186;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#36719;&#20214;&#24037;&#20855;&#38656;&#27714;&#30340;&#24179;&#22343;&#26102;&#38388;&#20026;...
&lt;/p&gt;
&lt;p&gt;
Objectives: To describe the development and validation of a zero-shot learning natural language processing (NLP) tool for abstracting data from unstructured text contained within PDF documents, such as those found within electronic health records. Materials and Methods: A data abstraction tool based on the GPT-3.5 model from OpenAI was developed and compared to three physician human abstractors in terms of time to task completion and accuracy for abstracting data on 14 unique variables from a set of 199 de-identified radical prostatectomy pathology reports. The reports were processed by the software tool in vectorized and scanned formats to establish the impact of optical character recognition on data abstraction. The tool was assessed for superiority for data abstraction speed and non-inferiority for accuracy. Results: The human abstractors required a mean of 101s per report for data abstraction, with times varying from 15 to 284 s. In comparison, the software tool required a mean of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24120;&#35782;&#30340;&#22240;&#26524;&#35299;&#37322;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#26679;&#21270;&#30340;&#20849;&#24773;&#22238;&#24212;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#32508;&#21512;&#32771;&#34385;&#20102;&#29992;&#25143;&#30340;&#35282;&#24230;&#21644;&#31995;&#32479;&#30340;&#35282;&#24230;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;&#24120;&#35782;&#30693;&#35782;&#25552;&#21319;&#20102;ChatGPT&#22312;&#31995;&#32479;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#39033;&#35780;&#20272;&#25351;&#26631;&#19978;&#36229;&#36807;&#20102;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.00085</link><description>&lt;p&gt;
&#20808;&#24605;&#32771;&#20877;&#22238;&#24212;&#65306;&#20026;&#20849;&#24773;&#22238;&#24212;&#29983;&#25104;&#38598;&#25104;&#24120;&#35782;&#30340;&#22240;&#26524;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Reasoning before Responding: Integrating Commonsense-based Causality Explanation for Empathetic Response Generation. (arXiv:2308.00085v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24120;&#35782;&#30340;&#22240;&#26524;&#35299;&#37322;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#26679;&#21270;&#30340;&#20849;&#24773;&#22238;&#24212;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#32508;&#21512;&#32771;&#34385;&#20102;&#29992;&#25143;&#30340;&#35282;&#24230;&#21644;&#31995;&#32479;&#30340;&#35282;&#24230;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;&#24120;&#35782;&#30693;&#35782;&#25552;&#21319;&#20102;ChatGPT&#22312;&#31995;&#32479;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#39033;&#35780;&#20272;&#25351;&#26631;&#19978;&#36229;&#36807;&#20102;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#20849;&#24773;&#22238;&#24212;&#29983;&#25104;&#26041;&#27861;&#35797;&#22270;&#25972;&#21512;&#24120;&#35782;&#30693;&#35782;&#25110;&#23545;&#24773;&#32490;&#21407;&#22240;&#30340;&#25512;&#29702;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#29992;&#25143;&#30340;&#32463;&#21382;&#21644;&#24863;&#21463;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#20174;&#29992;&#25143;&#30340;&#35282;&#24230;&#29702;&#35299;&#19978;&#19979;&#25991;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#24573;&#30053;&#20102;&#31995;&#32479;&#30340;&#35282;&#24230;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24120;&#35782;&#30340;&#22240;&#26524;&#35299;&#37322;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#26679;&#21270;&#30340;&#20849;&#24773;&#22238;&#24212;&#29983;&#25104;&#65292;&#21516;&#26102;&#32771;&#34385;&#29992;&#25143;&#30340;&#35282;&#24230;&#65288;&#29992;&#25143;&#30340;&#27442;&#26395;&#21644;&#21453;&#24212;&#65289;&#21644;&#31995;&#32479;&#30340;&#35282;&#24230;&#65288;&#31995;&#32479;&#30340;&#24847;&#22270;&#21644;&#21453;&#24212;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#19982;&#24120;&#35782;&#30693;&#35782;&#30456;&#32467;&#21512;&#65292;&#22686;&#24378;&#20102;ChatGPT&#22312;&#31995;&#32479;&#30340;&#35282;&#24230;&#19978;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#22522;&#20110;&#24120;&#35782;&#30340;&#22240;&#26524;&#35299;&#37322;&#19982;ChatGPT&#21644;&#22522;&#20110;T5&#27169;&#22411;&#30340;&#26041;&#27861;&#36827;&#34892;&#25972;&#21512;&#12290;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#33258;&#21160;&#35780;&#20272;&#21644;&#20154;&#24037;&#35780;&#20272;&#19978;&#20248;&#20110;&#20854;&#20182;&#21487;&#27604;&#36739;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent approaches to empathetic response generation try to incorporate commonsense knowledge or reasoning about the causes of emotions to better understand the user's experiences and feelings. However, these approaches mainly focus on understanding the causalities of context from the user's perspective, ignoring the system's perspective. In this paper, we propose a commonsense-based causality explanation approach for diverse empathetic response generation that considers both the user's perspective (user's desires and reactions) and the system's perspective (system's intentions and reactions). We enhance ChatGPT's ability to reason for the system's perspective by integrating in-context learning with commonsense knowledge. Then, we integrate the commonsense-based causality explanation with both ChatGPT and a T5-based model. Experimental evaluations demonstrate that our method outperforms other comparable methods on both automatic and human evaluations.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#31639;&#27861;&#20197;&#21450;&#21033;&#29992;&#23884;&#20837;&#27169;&#22411;&#25429;&#25417;&#30693;&#35782;&#22270;&#35889;&#20013;&#35821;&#20041;&#30340;&#19981;&#21516;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#30693;&#35782;&#22270;&#35889;&#21644;&#35821;&#35328;&#27169;&#22411;&#30456;&#20114;&#21463;&#30410;&#30340;&#35266;&#28857;&#12290;</title><link>http://arxiv.org/abs/2308.00081</link><description>&lt;p&gt;
&#20026;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26500;&#24314;&#35821;&#20041;&#20016;&#23500;&#30340;&#23884;&#20837;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards Semantically Enriched Embeddings for Knowledge Graph Completion. (arXiv:2308.00081v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#31639;&#27861;&#20197;&#21450;&#21033;&#29992;&#23884;&#20837;&#27169;&#22411;&#25429;&#25417;&#30693;&#35782;&#22270;&#35889;&#20013;&#35821;&#20041;&#30340;&#19981;&#21516;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#30693;&#35782;&#22270;&#35889;&#21644;&#35821;&#35328;&#27169;&#22411;&#30456;&#20114;&#21463;&#30410;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23884;&#20837;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#30446;&#21069;&#30340;&#22823;&#22810;&#25968;&#31639;&#27861;&#23558;&#30693;&#35782;&#22270;&#35889;&#35270;&#20026;&#19968;&#20010;&#22810;&#21521;&#26631;&#35760;&#22270;&#65292;&#32570;&#20047;&#25429;&#25417;&#24213;&#23618;&#35821;&#20041;&#30340;&#33021;&#21147;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#25429;&#33719;&#20102;&#22823;&#37327;&#20449;&#24687;&#65292;&#36825;&#19968;&#25429;&#33719;&#23545;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20135;&#29983;&#20102;&#38761;&#21629;&#24615;&#24433;&#21709;&#12290;&#30693;&#35782;&#22270;&#35889;&#21487;&#20197;&#20174;LLMs&#20013;&#21463;&#30410;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#22522;&#20110;&#19981;&#21516;&#29983;&#25104;&#23884;&#20837;&#27169;&#22411;&#21464;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#31639;&#27861;&#12290;&#39318;&#20808;&#35752;&#35770;&#20102;&#21508;&#31181;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#31639;&#27861;&#65292;&#22914;&#36716;&#23548;&#21644;&#24402;&#32435;&#38142;&#25509;&#39044;&#27979;&#20197;&#21450;&#23454;&#20307;&#31867;&#22411;&#39044;&#27979;&#31639;&#27861;&#12290;&#28982;&#21518;&#65292;&#20171;&#32461;&#20102;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#31867;&#22411;&#20449;&#24687;&#12289;LLMs&#20197;&#21450;&#25429;&#25417;&#19981;&#21516;&#25551;&#36848;&#36923;&#36753;&#20844;&#29702;&#20013;&#30340;&#35821;&#20041;&#30340;&#31639;&#27861;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#23545;&#29616;&#26377;&#31639;&#27861;&#30340;&#20851;&#38190;&#21453;&#24605;&#23545;&#35770;&#25991;&#36827;&#34892;&#24635;&#32467;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embedding based Knowledge Graph (KG) Completion has gained much attention over the past few years. Most of the current algorithms consider a KG as a multidirectional labeled graph and lack the ability to capture the semantics underlying the schematic information. In a separate development, a vast amount of information has been captured within the Large Language Models (LLMs) which has revolutionized the field of Artificial Intelligence. KGs could benefit from these LLMs and vice versa. This vision paper discusses the existing algorithms for KG completion based on the variations for generating KG embeddings. It starts with discussing various KG completion algorithms such as transductive and inductive link prediction and entity type prediction algorithms. It then moves on to the algorithms utilizing type information within the KGs, LLMs, and finally to algorithms capturing the semantics represented in different description logic axioms. We conclude the paper with a critical reflection on
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20799;&#31461;&#25925;&#20107;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#22312;&#36136;&#37327;&#21644;&#32454;&#33410;&#26041;&#38754;&#20173;&#28982;&#26377;&#22256;&#38590;&#65292;&#26080;&#27861;&#36798;&#21040;&#23454;&#38469;&#25925;&#20107;&#30340;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2308.00073</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20799;&#31461;&#25925;&#20107;&#30340;&#21487;&#20449;&#24230;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Trustworthiness of Children Stories Generated by Large Language Models. (arXiv:2308.00073v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00073
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20799;&#31461;&#25925;&#20107;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#22312;&#36136;&#37327;&#21644;&#32454;&#33410;&#26041;&#38754;&#20173;&#28982;&#26377;&#22256;&#38590;&#65292;&#26080;&#27861;&#36798;&#21040;&#23454;&#38469;&#25925;&#20107;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#29983;&#25104;&#25991;&#23398;&#25991;&#26412;&#30340;&#24040;&#22823;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#29983;&#25104;&#20799;&#31461;&#25925;&#20107;&#26041;&#38754;&#30340;&#25928;&#26524;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30340;&#26816;&#39564;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#25351;&#26631;&#35780;&#20272;&#20102;LLM&#29983;&#25104;&#30340;&#20799;&#31461;&#25925;&#20107;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#32467;&#26524;&#19982;&#26087;&#26377;&#21644;&#26032;&#30340;&#20799;&#31461;&#25925;&#20107;&#36827;&#34892;&#27604;&#36739;&#21644;&#23545;&#27604;&#65292;&#20197;&#26356;&#22909;&#22320;&#35780;&#20272;&#23427;&#20204;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLM&#22312;&#29983;&#25104;&#20799;&#31461;&#25925;&#20107;&#30340;&#36136;&#37327;&#21644;&#32454;&#24494;&#24046;&#21035;&#26041;&#38754;&#20173;&#23384;&#22312;&#22256;&#38590;&#65292;&#26080;&#27861;&#36798;&#21040;&#23454;&#38469;&#25925;&#20107;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown a tremendous capacity for generating literary text. However, their effectiveness in generating children's stories has yet to be thoroughly examined. In this study, we evaluate the trustworthiness of children's stories generated by LLMs using various measures, and we compare and contrast our results with both old and new children's stories to better assess their significance. Our findings suggest that LLMs still struggle to generate children's stories at the level of quality and nuance found in actual stories
&lt;/p&gt;</description></item><item><title>&#22312;&#20197;&#33394;&#21015;-&#24052;&#21202;&#26031;&#22374;&#21644;&#22303;&#32819;&#20854;-&#24211;&#23572;&#24503;&#20914;&#31361;&#30340;&#32972;&#26223;&#19979;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#29992;&#25143;&#35821;&#35328;&#23545;ChatGPT&#20013;&#20914;&#31361;&#27515;&#20129;&#20272;&#35745;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20351;&#29992;&#25915;&#20987;&#32773;&#30340;&#35821;&#35328;&#36827;&#34892;&#26597;&#35810;&#26102;&#65292;GPT-3.5&#25552;&#20379;&#30340;&#20272;&#35745;&#36739;&#20351;&#29992;&#34987;&#25915;&#20987;&#32676;&#20307;&#30340;&#35821;&#35328;&#26597;&#35810;&#26102;&#20302;27&#177;11&#65285;&#12290;&#27492;&#22806;&#65292;&#21542;&#35748;&#23384;&#22312;&#27492;&#31867;&#34989;&#20987;&#30340;&#22238;&#31572;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#36825;&#31181;&#24046;&#24322;&#65292;&#24418;&#25104;&#20102;&#19968;&#31181;&#26032;&#30340;&#20559;&#35265;&#26426;&#21046;&#65292;&#21487;&#33021;&#21152;&#22823;&#29616;&#26377;&#30340;&#23186;&#20307;&#20559;&#35265;&#24182;&#21152;&#21095;&#20449;&#24687;&#23396;&#31435;&#12290;</title><link>http://arxiv.org/abs/2308.00072</link><description>&lt;p&gt;
&#29992;&#25143;&#35821;&#35328;&#23545;ChatGPT&#20013;&#20914;&#31361;&#27515;&#20129;&#20272;&#35745;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
How User Language Affects Conflict Fatality Estimates in ChatGPT. (arXiv:2308.00072v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00072
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20197;&#33394;&#21015;-&#24052;&#21202;&#26031;&#22374;&#21644;&#22303;&#32819;&#20854;-&#24211;&#23572;&#24503;&#20914;&#31361;&#30340;&#32972;&#26223;&#19979;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#29992;&#25143;&#35821;&#35328;&#23545;ChatGPT&#20013;&#20914;&#31361;&#27515;&#20129;&#20272;&#35745;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20351;&#29992;&#25915;&#20987;&#32773;&#30340;&#35821;&#35328;&#36827;&#34892;&#26597;&#35810;&#26102;&#65292;GPT-3.5&#25552;&#20379;&#30340;&#20272;&#35745;&#36739;&#20351;&#29992;&#34987;&#25915;&#20987;&#32676;&#20307;&#30340;&#35821;&#35328;&#26597;&#35810;&#26102;&#20302;27&#177;11&#65285;&#12290;&#27492;&#22806;&#65292;&#21542;&#35748;&#23384;&#22312;&#27492;&#31867;&#34989;&#20987;&#30340;&#22238;&#31572;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#36825;&#31181;&#24046;&#24322;&#65292;&#24418;&#25104;&#20102;&#19968;&#31181;&#26032;&#30340;&#20559;&#35265;&#26426;&#21046;&#65292;&#21487;&#33021;&#21152;&#22823;&#29616;&#26377;&#30340;&#23186;&#20307;&#20559;&#35265;&#24182;&#21152;&#21095;&#20449;&#24687;&#23396;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
OpenAI&#30340;ChatGPT&#35821;&#35328;&#27169;&#22411;&#22240;&#20854;&#24378;&#22823;&#30340;&#22797;&#26434;&#38382;&#39064;&#35299;&#20915;&#21644;&#20449;&#24687;&#26816;&#32034;&#33021;&#21147;&#32780;&#22791;&#21463;&#38738;&#30544;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#35813;&#27169;&#22411;&#26159;&#21542;&#20250;&#22797;&#21046;&#35821;&#35328;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#20559;&#35265;&#30340;&#25285;&#24551;&#20063;&#19981;&#26029;&#20986;&#29616;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22312;&#20197;&#33394;&#21015;-&#24052;&#21202;&#26031;&#22374;&#21644;&#22303;&#32819;&#20854;-&#24211;&#23572;&#24503;&#20914;&#31361;&#30340;&#32972;&#26223;&#19979;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#20351;&#29992;GPT-3.5&#65292;&#25105;&#20204;&#37319;&#29992;&#33258;&#21160;&#26597;&#35810;&#36807;&#31243;&#65292;&#20197;&#24076;&#20271;&#26469;&#35821;&#21644;&#38463;&#25289;&#20271;&#35821;&#26597;&#35810;&#20851;&#20110;&#29305;&#23450;&#31354;&#34989;&#20013;&#30340;&#20260;&#20129;&#20154;&#25968;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21457;&#29616;&#65292;&#24403;&#20351;&#29992;&#25915;&#20987;&#32773;&#30340;&#35821;&#35328;&#36827;&#34892;&#26597;&#35810;&#26102;&#65292;GPT-3.5&#25552;&#20379;&#30340;&#27515;&#20129;&#20272;&#35745;&#36739;&#20351;&#29992;&#34987;&#25915;&#20987;&#32676;&#20307;&#30340;&#35821;&#35328;&#26597;&#35810;&#26102;&#20302;27&#177;11&#65285;&#12290;&#21542;&#35748;&#23384;&#22312;&#27492;&#31867;&#34989;&#20987;&#30340;&#22238;&#31572;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#36825;&#31181;&#24046;&#24322;&#65292;&#21019;&#36896;&#20102;&#19968;&#31181;&#22312;&#24120;&#35268;&#25628;&#32034;&#24341;&#25806;&#20013;&#19981;&#23384;&#22312;&#30340;&#26032;&#30340;&#20559;&#35265;&#26426;&#21046;&#12290;&#36825;&#31181;&#35821;&#35328;&#20559;&#35265;&#26377;&#21487;&#33021;&#25918;&#22823;&#29616;&#26377;&#30340;&#23186;&#20307;&#20559;&#35265;&#65292;&#24182;&#21152;&#21095;&#20449;&#24687;&#23396;&#31435;&#65292;&#26368;&#32456;&#21152;&#37325;&#20914;&#31361;&#12290;
&lt;/p&gt;
&lt;p&gt;
OpenAI's ChatGPT language model has gained popularity as a powerful tool for complex problem-solving and information retrieval. However, concerns arise about the reproduction of biases present in the language-specific training data. In this study, we address this issue in the context of the Israeli-Palestinian and Turkish-Kurdish conflicts. Using GPT-3.5, we employed an automated query procedure to inquire about casualties in specific airstrikes, in both Hebrew and Arabic for the former conflict and Turkish and Kurdish for the latter. Our analysis reveals that GPT-3.5 provides 27$\pm$11 percent lower fatality estimates when queried in the language of the attacker than in the language of the targeted group. Evasive answers denying the existence of such attacks further increase the discrepancy, creating a novel bias mechanism not present in regular search engines. This language bias has the potential to amplify existing media biases and contribute to information bubbles, ultimately reinf
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#25512;&#29702;&#26041;&#27861;&#65292;&#22312;&#38646;&#23556;&#20987;&#21051;&#26495;&#21360;&#35937;&#35782;&#21035;&#20013;&#21462;&#24471;&#20102;&#37325;&#35201;&#30340;&#36827;&#23637;&#65292;&#24182;&#21457;&#29616;&#25512;&#29702;&#30340;&#24615;&#33021;&#22686;&#30410;&#36828;&#36828;&#36229;&#36807;&#27169;&#22411;&#35268;&#27169;&#25193;&#23637;&#30340;&#22686;&#30410;&#12290;&#25512;&#29702;&#19981;&#20165;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#36824;&#25552;&#39640;&#20102;&#20915;&#31574;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.00071</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#25512;&#29702;&#26041;&#27861;&#29992;&#20110;&#21051;&#26495;&#21360;&#35937;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Interpretable Stereotype Identification through Reasoning. (arXiv:2308.00071v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#25512;&#29702;&#26041;&#27861;&#65292;&#22312;&#38646;&#23556;&#20987;&#21051;&#26495;&#21360;&#35937;&#35782;&#21035;&#20013;&#21462;&#24471;&#20102;&#37325;&#35201;&#30340;&#36827;&#23637;&#65292;&#24182;&#21457;&#29616;&#25512;&#29702;&#30340;&#24615;&#33021;&#22686;&#30410;&#36828;&#36828;&#36229;&#36807;&#27169;&#22411;&#35268;&#27169;&#25193;&#23637;&#30340;&#22686;&#30410;&#12290;&#25512;&#29702;&#19981;&#20165;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#36824;&#25552;&#39640;&#20102;&#20915;&#31574;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20351;&#29992;&#20102;&#21253;&#21547;&#22266;&#26377;&#20559;&#35265;&#30340;&#22823;&#37327;&#25968;&#25454;&#38598;&#65292;&#21487;&#33021;&#20250;&#19981;&#32463;&#24847;&#22320;&#25345;&#32493;&#31995;&#32479;&#24615;&#27495;&#35270;&#65292;&#22240;&#27492;&#65292;&#23457;&#26597;&#21644;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#23558;&#20844;&#24179;&#24615;&#25972;&#21512;&#21040;&#23427;&#20204;&#30340;&#21457;&#23637;&#20013;&#65292;&#20197;&#30830;&#20445;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#20844;&#27491;&#21644;&#26080;&#20559;&#30340;&#29305;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;Vicuna-13B-v1.3&#30340;&#38646;&#23556;&#20987;&#21051;&#26495;&#21360;&#35937;&#35782;&#21035;&#20013;&#25512;&#29702;&#30340;&#37325;&#35201;&#24615;&#12290;&#23613;&#31649;&#25105;&#20204;&#35266;&#23519;&#21040;&#20174;13B&#21040;33B&#30340;&#35268;&#27169;&#25193;&#23637;&#20250;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#25512;&#29702;&#30340;&#24615;&#33021;&#22686;&#30410;&#36828;&#36828;&#36229;&#36807;&#35268;&#27169;&#25193;&#23637;&#30340;&#22686;&#30410;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25512;&#29702;&#21487;&#33021;&#26159;&#20351;LLMs&#22312;&#21051;&#26495;&#21360;&#35937;&#31561;&#39046;&#22495;&#20219;&#21153;&#19978;&#36229;&#36234;&#35268;&#27169;&#23450;&#24459;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23545;&#36873;&#23450;&#30340;&#25512;&#29702;&#36861;&#36394;&#36827;&#34892;&#23450;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#31361;&#20986;&#26174;&#31034;&#20102;&#25512;&#29702;&#19981;&#20165;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#36824;&#25552;&#39640;&#20102;&#20915;&#31574;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given that language models are trained on vast datasets that may contain inherent biases, there is a potential danger of inadvertently perpetuating systemic discrimination. Consequently, it becomes essential to examine and address biases in language models, integrating fairness into their development to ensure these models are equitable and free from bias. In this work, we demonstrate the importance of reasoning in zero-shot stereotype identification based on Vicuna-13B-v1.3. While we do observe improved accuracy by scaling from 13B to 33B, we show that the performance gain from reasoning significantly exceeds the gain from scaling up. Our findings suggest that reasoning could be a key factor that enables LLMs to trescend the scaling law on out-of-domain tasks such as stereotype identification. Additionally, through a qualitative analysis of select reasoning traces, we highlight how reasoning enhances not just accuracy but also the interpretability of the decision.
&lt;/p&gt;</description></item><item><title>FinPT&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#36164;&#37329;&#39118;&#38505;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22823;&#22411;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#19978;&#36827;&#34892;&#20010;&#20154;&#36164;&#26009;&#35843;&#25972;&#65292;&#22635;&#20805;&#37329;&#34701;&#34920;&#26684;&#25968;&#25454;&#24182;&#33719;&#24471;&#33258;&#28982;&#35821;&#35328;&#23458;&#25143;&#36164;&#26009;&#65292;&#20174;&#32780;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.00065</link><description>&lt;p&gt;
FinPT:&#20351;&#29992;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#36164;&#37329;&#39118;&#38505;&#39044;&#27979;&#20013;&#20010;&#20154;&#36164;&#26009;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
FinPT: Financial Risk Prediction with Profile Tuning on Pretrained Foundation Models. (arXiv:2308.00065v1 [q-fin.RM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00065
&lt;/p&gt;
&lt;p&gt;
FinPT&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#36164;&#37329;&#39118;&#38505;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22823;&#22411;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#19978;&#36827;&#34892;&#20010;&#20154;&#36164;&#26009;&#35843;&#25972;&#65292;&#22635;&#20805;&#37329;&#34701;&#34920;&#26684;&#25968;&#25454;&#24182;&#33719;&#24471;&#33258;&#28982;&#35821;&#35328;&#23458;&#25143;&#36164;&#26009;&#65292;&#20174;&#32780;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36164;&#37329;&#39118;&#38505;&#39044;&#27979;&#22312;&#37329;&#34701;&#39046;&#22495;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#33258;&#21160;&#26816;&#27979;&#28508;&#22312;&#39118;&#38505;&#65292;&#20174;&#32780;&#33410;&#30465;&#21171;&#21160;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#36817;&#24180;&#26469;&#36825;&#19968;&#39046;&#22495;&#30340;&#21457;&#23637;&#28382;&#21518;&#20110;&#20197;&#19979;&#20004;&#20010;&#20107;&#23454;&#65306;1&#65289;&#25152;&#20351;&#29992;&#30340;&#31639;&#27861;&#26377;&#20123;&#36807;&#26102;&#65292;&#29305;&#21035;&#26159;&#22312;&#29983;&#25104;AI&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24555;&#36895;&#21457;&#23637;&#30340;&#32972;&#26223;&#19979;&#65307;2&#65289;&#32570;&#20047;&#32479;&#19968;&#19988;&#24320;&#28304;&#30340;&#37329;&#34701;&#22522;&#20934;&#24050;&#32463;&#38459;&#30861;&#20102;&#30456;&#20851;&#30740;&#31350;&#22810;&#24180;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FinPT&#21644;FinBench&#65306;&#21069;&#32773;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#36164;&#37329;&#39118;&#38505;&#39044;&#27979;&#26041;&#27861;&#65292;&#23545;&#22823;&#22411;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#20010;&#20154;&#36164;&#26009;&#35843;&#25972;&#65307;&#21518;&#32773;&#26159;&#19968;&#22871;&#20851;&#20110;&#36164;&#37329;&#39118;&#38505;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#65292;&#22914;&#36829;&#32422;&#12289;&#27450;&#35784;&#21644;&#27969;&#22833;&#12290;&#22312;FinPT&#20013;&#65292;&#25105;&#20204;&#23558;&#37329;&#34701;&#34920;&#26684;&#25968;&#25454;&#22635;&#20805;&#21040;&#39044;&#23450;&#20041;&#30340;&#25351;&#20196;&#27169;&#26495;&#20013;&#65292;&#36890;&#36807;&#25552;&#31034;LLMs&#33719;&#24471;&#33258;&#28982;&#35821;&#35328;&#23458;&#25143;&#36164;&#26009;&#65292;&#24182;&#36827;&#34892;&#31934;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
Financial risk prediction plays a crucial role in the financial sector. Machine learning methods have been widely applied for automatically detecting potential risks and thus saving the cost of labor. However, the development in this field is lagging behind in recent years by the following two facts: 1) the algorithms used are somewhat outdated, especially in the context of the fast advance of generative AI and large language models (LLMs); 2) the lack of a unified and open-sourced financial benchmark has impeded the related research for years. To tackle these issues, we propose FinPT and FinBench: the former is a novel approach for financial risk prediction that conduct Profile Tuning on large pretrained foundation models, and the latter is a set of high-quality datasets on financial risks such as default, fraud, and churn. In FinPT, we fill the financial tabular data into the pre-defined instruction template, obtain natural-language customer profiles by prompting LLMs, and fine-tune 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#20154;&#26426;&#20132;&#20114;&#30340;&#26032;&#22411; alpha &#25366;&#25496;&#33539;&#24335;&#65292;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#25552;&#31034;&#24037;&#31243;&#31639;&#27861;&#26694;&#26550;&#65292;&#24320;&#21457;&#20102; Alpha-GPT&#12290;&#36890;&#36807;&#22810;&#20010;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102; Alpha-GPT &#22312;&#37327;&#21270;&#25237;&#36164;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.00016</link><description>&lt;p&gt;
Alpha-GPT&#65306;&#20154;&#26426;&#20132;&#20114;&#24335; Alpha &#25366;&#25496;&#22312;&#37327;&#21270;&#25237;&#36164;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Alpha-GPT: Human-AI Interactive Alpha Mining for Quantitative Investment. (arXiv:2308.00016v1 [q-fin.CP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#20154;&#26426;&#20132;&#20114;&#30340;&#26032;&#22411; alpha &#25366;&#25496;&#33539;&#24335;&#65292;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#25552;&#31034;&#24037;&#31243;&#31639;&#27861;&#26694;&#26550;&#65292;&#24320;&#21457;&#20102; Alpha-GPT&#12290;&#36890;&#36807;&#22810;&#20010;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102; Alpha-GPT &#22312;&#37327;&#21270;&#25237;&#36164;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37327;&#21270;&#25237;&#36164;&#30740;&#31350;&#20013;&#65292;&#25366;&#25496;&#26032;&#30340; alpha&#65288;&#26377;&#25928;&#30340;&#20132;&#26131;&#20449;&#21495;&#25110;&#22240;&#23376;&#65289;&#26159;&#20854;&#20013;&#26368;&#37325;&#35201;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#20256;&#32479;&#30340; alpha &#25366;&#25496;&#26041;&#27861;&#65292;&#26080;&#35770;&#26159;&#25163;&#24037;&#21512;&#25104;&#22240;&#23376;&#36824;&#26159;&#31639;&#27861;&#25366;&#25496;&#22240;&#23376;&#65288;&#22914;&#36951;&#20256;&#32534;&#31243;&#25628;&#32034;&#65289;&#65292;&#37117;&#23384;&#22312;&#22266;&#26377;&#30340;&#23616;&#38480;&#24615;&#65292;&#23588;&#20854;&#22312;&#23454;&#26045;&#37327;&#21270;&#20998;&#26512;&#24072;&#30340;&#24819;&#27861;&#26041;&#38754;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340; alpha &#25366;&#25496;&#33539;&#24335;&#65292;&#24341;&#20837;&#20102;&#20154;&#26426;&#20132;&#20114;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25552;&#31034;&#24037;&#31243;&#31639;&#27861;&#26694;&#26550;&#26469;&#23454;&#29616;&#36825;&#20010;&#33539;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102; Alpha-GPT&#65292;&#19968;&#31181;&#26032;&#30340;&#20132;&#20114;&#24335; alpha &#25366;&#25496;&#31995;&#32479;&#26694;&#26550;&#65292;&#20197;&#19968;&#31181;&#21551;&#21457;&#24335;&#30340;&#26041;&#24335;&#8220;&#29702;&#35299;&#8221;&#37327;&#21270;&#30740;&#31350;&#20154;&#21592;&#30340;&#24819;&#27861;&#65292;&#24182;&#36755;&#20986;&#20855;&#26377;&#21019;&#36896;&#24615;&#12289;&#28145;&#20837;&#27934;&#23519;&#21147;&#21644;&#26377;&#25928;&#24615;&#30340; alpha&#12290;&#36890;&#36807;&#22810;&#20010; alpha &#25366;&#25496;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102; Alpha-GPT &#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the most important tasks in quantitative investment research is mining new alphas (effective trading signals or factors). Traditional alpha mining methods, either hand-crafted factor synthesizing or algorithmic factor mining (e.g., search with genetic programming), have inherent limitations, especially in implementing the ideas of quants. In this work, we propose a new alpha mining paradigm by introducing human-AI interaction, and a novel prompt engineering algorithmic framework to implement this paradigm by leveraging the power of large language models. Moreover, we develop Alpha-GPT, a new interactive alpha mining system framework that provides a heuristic way to ``understand'' the ideas of quant researchers and outputs creative, insightful, and effective alphas. We demonstrate the effectiveness and advantage of Alpha-GPT via a number of alpha mining experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#25991;&#26412;&#25366;&#25496;&#21644;&#32593;&#32476;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#37096;&#38376;&#20043;&#38388;&#30340;&#25216;&#26415;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#25216;&#26415;&#21019;&#26032;&#20013;&#65292;&#38388;&#25509;&#32852;&#31995;&#21644;&#30452;&#25509;&#32852;&#31995;&#21516;&#31561;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2308.00014</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#30456;&#20114;&#20381;&#36182;&#30340;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
A new mapping of technological interdependence. (arXiv:2308.00014v1 [econ.EM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#25991;&#26412;&#25366;&#25496;&#21644;&#32593;&#32476;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#37096;&#38376;&#20043;&#38388;&#30340;&#25216;&#26415;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#25216;&#26415;&#21019;&#26032;&#20013;&#65292;&#38388;&#25509;&#32852;&#31995;&#21644;&#30452;&#25509;&#32852;&#31995;&#21516;&#31561;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21738;&#20123;&#25216;&#26415;&#32852;&#31995;&#24433;&#21709;&#20102;&#37096;&#38376;&#30340;&#21019;&#26032;&#33021;&#21147;&#65311;&#36825;&#20123;&#25928;&#24212;&#22914;&#20309;&#36890;&#36807;&#25216;&#26415;&#31354;&#38388;&#20256;&#36882;&#65311;&#26412;&#25991;&#20351;&#29992;&#26032;&#39062;&#30340;&#25991;&#26412;&#25366;&#25496;&#21644;&#32593;&#32476;&#20998;&#26512;&#26041;&#27861;&#22238;&#31572;&#20102;&#36825;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#32654;&#22269;&#19987;&#21033;&#21830;&#26631;&#23616;&#65288;USPTO&#65289;&#25480;&#20104;&#30340;650&#19975;&#39033;&#19987;&#21033;&#30340;&#25991;&#26412;&#65292;&#24182;&#24212;&#29992;&#32593;&#32476;&#20998;&#26512;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#21322;&#20010;&#19990;&#32426;&#65288;&#20174;1976&#24180;&#21040;2021&#24180;&#65289;&#26399;&#38388;&#19981;&#21516;&#37096;&#38376;&#20043;&#38388;&#30340;&#25216;&#26415;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#23384;&#22312;&#20110;&#25216;&#26415;&#39046;&#22495;&#20043;&#38388;&#30340;&#20840;&#35889;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#35777;&#26126;&#19987;&#21033;&#25991;&#26412;&#21253;&#21547;&#20102;&#24448;&#24448;&#26080;&#27861;&#36890;&#36807;&#20256;&#32479;&#30340;&#21019;&#26032;&#25351;&#26631;&#65288;&#20363;&#22914;&#19987;&#21033;&#24341;&#29992;&#65289;&#25429;&#25417;&#21040;&#30340;&#20016;&#23500;&#20449;&#24687;&#12290;&#36890;&#36807;&#20351;&#29992;&#32593;&#32476;&#20998;&#26512;&#65292;&#25105;&#20204;&#35760;&#24405;&#20102;&#38388;&#25509;&#32852;&#31995;&#21644;&#30452;&#25509;&#32852;&#31995;&#21516;&#31561;&#37325;&#35201;&#65292;&#24182;&#19988;&#21069;&#32773;&#22823;&#37096;&#20998;&#20351;&#29992;&#20256;&#32479;&#30340;&#38388;&#25509;&#32852;&#31995;&#24230;&#37327;&#26041;&#27861;&#65288;&#22914;Leontief&#36870;&#30697;&#38453;&#65289;&#24448;&#24448;&#20250;&#34987;&#38544;&#34255;&#12290;&#26368;&#21518;&#65292;&#22522;&#20110;&#20914;&#20987;&#21709;&#24212;&#20998;&#26512;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Which technological linkages affect the sector's ability to innovate? How do these effects transmit through the technology space? This paper answers these two key questions using novel methods of text mining and network analysis. We examine technological interdependence across sectors over a period of half a century (from 1976 to 2021) by analyzing the text of 6.5 million patents granted by the United States Patent and Trademark Office (USPTO), and applying network analysis to uncover the full spectrum of linkages existing across technology areas. We demonstrate that patent text contains a wealth of information often not captured by traditional innovation metrics, such as patent citations. By using network analysis, we document that indirect linkages are as important as direct connections and that the former would remain mostly hidden using more traditional measures of indirect linkages, such as the Leontief inverse matrix. Finally, based on an impulse-response analysis, we illustrate 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26102;&#38388;&#24120;&#35782;&#25512;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;&#36890;&#36807;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#26469;&#25552;&#39640;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22686;&#24378;&#27169;&#22411;&#20173;&#28982;&#38590;&#20197;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.00002</link><description>&lt;p&gt;
&#26102;&#38388;&#24120;&#35782;&#25512;&#29702;&#19982;&#33719;&#21462;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
An Overview Of Temporal Commonsense Reasoning and Acquisition. (arXiv:2308.00002v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26102;&#38388;&#24120;&#35782;&#25512;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;&#36890;&#36807;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#26469;&#25552;&#39640;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22686;&#24378;&#27169;&#22411;&#20173;&#28982;&#38590;&#20197;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24120;&#35782;&#25512;&#29702;&#26159;&#25351;&#29702;&#35299;&#30701;&#35821;&#12289;&#21160;&#20316;&#21644;&#20107;&#20214;&#30340;&#20856;&#22411;&#26102;&#38388;&#32972;&#26223;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#38656;&#35201;&#36825;&#31181;&#30693;&#35782;&#30340;&#38382;&#39064;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#36825;&#31181;&#33021;&#21147;&#22312;&#26102;&#38388;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#21487;&#33021;&#24212;&#29992;&#20110;&#26102;&#38388;&#32447;&#25688;&#35201;&#12289;&#26102;&#38388;&#38382;&#31572;&#21644;&#26102;&#38388;&#33258;&#28982;&#35821;&#35328;&#25512;&#26029;&#31561;&#26041;&#38754;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34429;&#28982;&#21892;&#20110;&#29983;&#25104;&#35821;&#27861;&#27491;&#30830;&#30340;&#21477;&#23376;&#21644;&#35299;&#20915;&#20998;&#31867;&#20219;&#21153;&#65292;&#20294;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#24448;&#24448;&#20250;&#37319;&#21462;&#25463;&#24452;&#65292;&#24182;&#38519;&#20837;&#31616;&#21333;&#30340;&#35821;&#35328;&#38519;&#38449;&#12290;&#26412;&#25991;&#31456;&#27010;&#36848;&#20102;&#22312;&#26102;&#38388;&#24120;&#35782;&#25512;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#29305;&#21035;&#20851;&#27880;&#36890;&#36807;&#21508;&#31181;&#22686;&#24378;&#26041;&#24335;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#20197;&#21450;&#23545;&#36234;&#26469;&#36234;&#22810;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22686;&#24378;&#27169;&#22411;&#22312;&#25512;&#29702;&#20219;&#21153;&#19978;&#20173;&#28982;&#38590;&#20197;&#36798;&#21040;&#20154;&#31867;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal commonsense reasoning refers to the ability to understand the typical temporal context of phrases, actions, and events, and use it to reason over problems requiring such knowledge. This trait is essential in temporal natural language processing tasks, with possible applications such as timeline summarization, temporal question answering, and temporal natural language inference. Recent research on the performance of large language models suggests that, although they are adept at generating syntactically correct sentences and solving classification tasks, they often take shortcuts in their reasoning and fall prey to simple linguistic traps. This article provides an overview of research in the domain of temporal commonsense reasoning, particularly focusing on enhancing language model performance through a variety of augmentations and their evaluation across a growing number of datasets. However, these augmented models still struggle to approach human performance on reasoning task
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#22522;&#20110;UNESCO&#25688;&#35201;&#35760;&#24405;&#30340;&#33258;&#21160;&#20998;&#26512;&#24037;&#20855;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#20027;&#39064;&#24314;&#27169;&#21644;&#32039;&#24352;&#26816;&#27979;&#26041;&#27861;&#65292;&#25552;&#20379;&#23545;&#32852;&#21512;&#22269;&#25945;&#31185;&#25991;&#32452;&#32455;&#19990;&#30028;&#36951;&#20135;&#21517;&#24405;&#21644;&#20195;&#34920;&#24615;&#38750;&#29289;&#36136;&#25991;&#21270;&#36951;&#20135;&#20154;&#31867;&#20195;&#34920;&#20316;&#21517;&#24405;&#20915;&#31574;&#36807;&#31243;&#30340;&#26377;&#20215;&#20540;&#27934;&#23519;&#12290;&#24212;&#29992;&#31243;&#24207;&#21487;&#20026;&#22806;&#20132;&#23448;&#12289;&#24459;&#24072;&#12289;&#25919;&#27835;&#31185;&#23398;&#23478;&#21644;&#22269;&#38469;&#20851;&#31995;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#39640;&#25928;&#25628;&#32034;&#25152;&#36873;&#20027;&#39064;&#30456;&#20851;&#25991;&#26723;&#27573;&#33853;&#21644;&#29305;&#23450;&#28436;&#35762;&#32773;&#38472;&#36848;&#30340;&#20415;&#21033;&#12290;</title><link>http://arxiv.org/abs/2307.16573</link><description>&lt;p&gt;
&#28145;&#20837;&#29702;&#35299;&#22269;&#38469;&#20851;&#31995;&#35821;&#35328;&#65306;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#23545;UNESCO&#25688;&#35201;&#35760;&#24405;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Deep Dive into the Language of International Relations: NLP-based Analysis of UNESCO's Summary Records. (arXiv:2307.16573v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16573
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#22522;&#20110;UNESCO&#25688;&#35201;&#35760;&#24405;&#30340;&#33258;&#21160;&#20998;&#26512;&#24037;&#20855;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#20027;&#39064;&#24314;&#27169;&#21644;&#32039;&#24352;&#26816;&#27979;&#26041;&#27861;&#65292;&#25552;&#20379;&#23545;&#32852;&#21512;&#22269;&#25945;&#31185;&#25991;&#32452;&#32455;&#19990;&#30028;&#36951;&#20135;&#21517;&#24405;&#21644;&#20195;&#34920;&#24615;&#38750;&#29289;&#36136;&#25991;&#21270;&#36951;&#20135;&#20154;&#31867;&#20195;&#34920;&#20316;&#21517;&#24405;&#20915;&#31574;&#36807;&#31243;&#30340;&#26377;&#20215;&#20540;&#27934;&#23519;&#12290;&#24212;&#29992;&#31243;&#24207;&#21487;&#20026;&#22806;&#20132;&#23448;&#12289;&#24459;&#24072;&#12289;&#25919;&#27835;&#31185;&#23398;&#23478;&#21644;&#22269;&#38469;&#20851;&#31995;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#39640;&#25928;&#25628;&#32034;&#25152;&#36873;&#20027;&#39064;&#30456;&#20851;&#25991;&#26723;&#27573;&#33853;&#21644;&#29305;&#23450;&#28436;&#35762;&#32773;&#38472;&#36848;&#30340;&#20415;&#21033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#21270;&#36951;&#20135;&#26159;&#25152;&#26377;&#22269;&#23478;&#37117;&#24863;&#20852;&#36259;&#30340;&#22269;&#38469;&#20851;&#31995;&#39046;&#22495;&#12290;&#23558;&#25991;&#21270;&#36951;&#20135;&#21015;&#20837;&#32852;&#21512;&#22269;&#25945;&#31185;&#25991;&#32452;&#32455;&#19990;&#30028;&#36951;&#20135;&#21517;&#24405;&#21644;&#12298;&#20195;&#34920;&#24615;&#38750;&#29289;&#36136;&#25991;&#21270;&#36951;&#20135;&#20154;&#31867;&#20195;&#34920;&#20316;&#21517;&#24405;&#12299;&#30340;&#36807;&#31243;&#32463;&#24120;&#24341;&#21457;&#22269;&#23478;&#38388;&#30340;&#32039;&#24352;&#19982;&#20914;&#31361;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;&#33258;&#21160;&#24037;&#20855;&#65292;&#23545;&#19978;&#36848;&#20004;&#20010;&#21517;&#24405;&#30340;&#20915;&#31574;&#36807;&#31243;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#27934;&#23519;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;UNESCO&#25688;&#35201;&#35760;&#24405;&#30340;&#21019;&#26032;&#20027;&#39064;&#24314;&#27169;&#21644;&#32039;&#24352;&#26816;&#27979;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#22312;&#32039;&#24352;&#26816;&#27979;&#19978;&#36798;&#21040;&#20102;&#20196;&#20154;&#31216;&#36190;&#30340;72%&#20934;&#30830;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#19987;&#20026;&#22806;&#20132;&#23448;&#12289;&#24459;&#24072;&#12289;&#25919;&#27835;&#31185;&#23398;&#23478;&#21644;&#22269;&#38469;&#20851;&#31995;&#30740;&#31350;&#20154;&#21592;&#37327;&#36523;&#23450;&#21046;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#29992;&#20110;&#39640;&#25928;&#25628;&#32034;&#36873;&#23450;&#25991;&#26723;&#27573;&#33853;&#21644;&#29305;&#23450;&#28436;&#35762;&#32773;&#20851;&#20110;&#36873;&#23450;&#20027;&#39064;&#30340;&#38472;&#36848;&#12290;&#35813;&#24212;&#29992;&#31243;&#24207;&#26159;&#25552;&#21319;&#25991;&#21270;&#36951;&#20135;&#31649;&#29702;&#21644;&#20915;&#31574;&#21046;&#23450;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cultural heritage is an arena of international relations that interests all states worldwide. The inscription process on the UNESCO World Heritage List and the UNESCO Representative List of the Intangible Cultural Heritage of Humanity often leads to tensions and conflicts among states. This research addresses these challenges by developing automatic tools that provide valuable insights into the decision-making processes regarding inscriptions to the two lists mentioned above. We propose innovative topic modelling and tension detection methods based on UNESCO's summary records. Our analysis achieved a commendable accuracy rate of 72% in identifying tensions. Furthermore, we have developed an application tailored for diplomats, lawyers, political scientists, and international relations researchers that facilitates the efficient search of paragraphs from selected documents and statements from specific speakers about chosen topics. This application is a valuable resource for enhancing the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#30417;&#30563;&#23398;&#20064;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#34892;&#20026;&#19978;&#21463;&#21040;&#37329;&#26631;&#31614;&#30340;&#26174;&#33879;&#24433;&#21709;&#65292;&#20294;&#23545;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#35828;&#65292;&#26631;&#31614;&#19981;&#24179;&#34913;&#24433;&#21709;&#36739;&#23567;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#19978;&#19979;&#25991;&#23398;&#20064;&#23545;&#26631;&#31614;&#25200;&#21160;&#30340;&#25935;&#24863;&#24615;&#36739;&#20302;&#12290;</title><link>http://arxiv.org/abs/2307.15411</link><description>&lt;p&gt;
&#30740;&#31350;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#23398;&#20064;&#34892;&#20026;&#65306;&#19982;&#30417;&#30563;&#23398;&#20064;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Investigating the Learning Behaviour of In-context Learning: A Comparison with Supervised Learning. (arXiv:2307.15411v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15411
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#30417;&#30563;&#23398;&#20064;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#34892;&#20026;&#19978;&#21463;&#21040;&#37329;&#26631;&#31614;&#30340;&#26174;&#33879;&#24433;&#21709;&#65292;&#20294;&#23545;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#35828;&#65292;&#26631;&#31614;&#19981;&#24179;&#34913;&#24433;&#21709;&#36739;&#23567;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#19978;&#19979;&#25991;&#23398;&#20064;&#23545;&#26631;&#31614;&#25200;&#21160;&#30340;&#25935;&#24863;&#24615;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#20196;&#20154;&#27880;&#30446;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#65292;&#22312;&#27809;&#26377;&#26126;&#30830;&#39044;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#20165;&#36890;&#36807;&#23569;&#37327;&#35757;&#32451;&#26679;&#20363;&#23601;&#21487;&#20197;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;LLM&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#23545;&#20110;ICL&#22914;&#20309;&#20174;&#32473;&#23450;&#30340;&#25552;&#31034;&#20013;&#23398;&#20064;&#30693;&#35782;&#30340;&#20102;&#35299;&#36824;&#24456;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;ICL&#30340;&#23398;&#20064;&#34892;&#20026;&#65292;&#25105;&#20204;&#20351;&#29992;&#30456;&#21516;&#30340;&#28436;&#31034;&#26679;&#20363;&#36890;&#36807;ICL&#21644;&#30417;&#30563;&#23398;&#20064;&#65288;SL&#65289;&#20998;&#21035;&#35757;&#32451;&#30456;&#21516;&#30340;LLM&#65292;&#24182;&#30740;&#31350;&#23427;&#20204;&#22312;&#19968;&#31995;&#21015;&#20998;&#31867;&#20219;&#21153;&#19978;&#22312;&#26631;&#31614;&#25200;&#21160;&#65288;&#22122;&#22768;&#26631;&#31614;&#21644;&#26631;&#31614;&#19981;&#24179;&#34913;&#65289;&#19979;&#30340;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#37329;&#26631;&#31614;&#23545;&#20110;&#19979;&#28216;&#30340;&#19978;&#19979;&#25991;&#24615;&#33021;&#26377;&#37325;&#22823;&#24433;&#21709;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65307;&#28982;&#32780;&#65292;&#23545;&#20110;ICL&#26469;&#35828;&#65292;&#26631;&#31614;&#19981;&#24179;&#34913;&#23545;&#25152;&#26377;&#27169;&#22411;&#22823;&#23567;&#37117;&#19981;&#22826;&#37325;&#35201;&#12290;&#20854;&#27425;&#65292;&#22312;&#19982;SL&#36827;&#34892;&#27604;&#36739;&#26102;&#65292;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#34920;&#26126;ICL&#23545;&#26631;&#31614;&#25200;&#21160;&#30340;&#25935;&#24863;&#24615;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown remarkable capacity for in-context learning (ICL), where learning a new task from just a few training examples is done without being explicitly pre-trained. However, despite the success of LLMs, there has been little understanding of how ICL learns the knowledge from the given prompts. In this paper, to make progress toward understanding the learning behaviour of ICL, we train the same LLMs with the same demonstration examples via ICL and supervised learning (SL), respectively, and investigate their performance under label perturbations (i.e., noisy labels and label imbalance) on a range of classification tasks. First, via extensive experiments, we find that gold labels have significant impacts on the downstream in-context performance, especially for large language models; however, imbalanced labels matter little to ICL across all model sizes. Second, when comparing with SL, we show empirically that ICL is less sensitive to label perturbations th
&lt;/p&gt;</description></item><item><title>Gzip&#19982;KNN&#30456;&#27604;&#36739;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#31616;&#21333;&#30340;&#35789;&#34955;&#21305;&#37197;&#21487;&#20197;&#33719;&#24471;&#31867;&#20284;&#25110;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#26356;&#21152;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2307.15002</link><description>&lt;p&gt;
Gzip&#19982;KNN&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#23545;&#27604;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Gzip versus bag-of-words for text classification with KNN. (arXiv:2307.15002v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15002
&lt;/p&gt;
&lt;p&gt;
Gzip&#19982;KNN&#30456;&#27604;&#36739;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#31616;&#21333;&#30340;&#35789;&#34955;&#21305;&#37197;&#21487;&#20197;&#33719;&#24471;&#31867;&#20284;&#25110;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;KNN&#30340;&#25991;&#26412;&#20998;&#31867;&#20013;&#21387;&#32553;&#36317;&#31163;&#65288;gzip&#65289;&#30340;&#26377;&#25928;&#24615;&#24341;&#36215;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#26356;&#31616;&#21333;&#30340;&#26041;&#27861;&#21487;&#20197;&#36798;&#21040;&#31867;&#20284;&#25110;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#21487;&#33021;&#19981;&#38656;&#35201;&#25991;&#26412;&#21387;&#32553;&#12290;&#23454;&#38469;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#31616;&#21333;&#30340;&#8220;&#35789;&#34955;&#8221;&#21305;&#37197;&#21487;&#20197;&#36798;&#21040;&#31867;&#20284;&#25110;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
The effectiveness of compression distance in KNN-based text classification ('gzip') has recently garnered lots of attention. In this note, we show that similar or better effectiveness can be achieved with simpler means, and text compression may not be necessary. Indeed, we find that a simple 'bag-of-words' matching can achieve similar or better accuracy, and is more efficient.
&lt;/p&gt;</description></item><item><title>CliniDigest&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#20020;&#24202;&#35797;&#39564;&#25688;&#35201;&#24037;&#20855;&#65292;&#33021;&#22815;&#23454;&#26102;&#12289;&#30495;&#23454;&#21644;&#20840;&#38754;&#22320;&#23558;&#38271;&#31687;&#35797;&#39564;&#25551;&#36848;&#21387;&#32553;&#25104;&#31616;&#27905;&#30340;&#25688;&#35201;&#12290;</title><link>http://arxiv.org/abs/2307.14522</link><description>&lt;p&gt;
CliniDigest: &#22522;&#20110;&#22823;&#35268;&#27169;&#20020;&#24202;&#35797;&#39564;&#25551;&#36848;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
CliniDigest: A Case Study in Large Language Model Based Large-Scale Summarization of Clinical Trial Descriptions. (arXiv:2307.14522v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14522
&lt;/p&gt;
&lt;p&gt;
CliniDigest&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#20020;&#24202;&#35797;&#39564;&#25688;&#35201;&#24037;&#20855;&#65292;&#33021;&#22815;&#23454;&#26102;&#12289;&#30495;&#23454;&#21644;&#20840;&#38754;&#22320;&#23558;&#38271;&#31687;&#35797;&#39564;&#25551;&#36848;&#21387;&#32553;&#25104;&#31616;&#27905;&#30340;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#35797;&#39564;&#26159;&#35780;&#20272;&#26032;&#30340;&#29983;&#29289;&#21307;&#23398;&#24178;&#39044;&#25514;&#26045;&#30340;&#30740;&#31350;&#12290;&#20026;&#20102;&#35774;&#35745;&#26032;&#30340;&#35797;&#39564;&#65292;&#30740;&#31350;&#20154;&#21592;&#20174;&#24403;&#21069;&#21644;&#24050;&#23436;&#25104;&#30340;&#35797;&#39564;&#20013;&#27762;&#21462;&#28789;&#24863;&#12290;2022&#24180;&#65292;&#27599;&#22825;&#24179;&#22343;&#26377;100&#22810;&#20010;&#20020;&#24202;&#35797;&#39564;&#25552;&#20132;&#21040;ClinicalTrials.gov&#65292;&#27599;&#20010;&#35797;&#39564;&#24179;&#22343;&#26377;&#32422;1500&#20010;&#21333;&#35789;&#12290;&#36825;&#20960;&#20046;&#19981;&#21487;&#33021;&#21450;&#26102;&#36319;&#19978;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;GPT-3.5&#21019;&#24314;&#20102;&#19968;&#20010;&#25209;&#37327;&#20020;&#24202;&#35797;&#39564;&#25688;&#35201;&#24037;&#20855;CliniDigest&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;CliniDigest&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#25552;&#20379;&#23454;&#26102;&#12289;&#30495;&#23454;&#21644;&#20840;&#38754;&#30340;&#20020;&#24202;&#35797;&#39564;&#25688;&#35201;&#30340;&#24037;&#20855;&#12290;CliniDigest&#21487;&#20197;&#23558;&#22810;&#36798;85&#20010;&#20020;&#24202;&#35797;&#39564;&#25551;&#36848;&#65288;&#32422;10500&#20010;&#21333;&#35789;&#65289;&#32553;&#20943;&#20026;&#19968;&#20010;&#31616;&#27905;&#30340;200&#20010;&#35789;&#30340;&#25688;&#35201;&#65292;&#24102;&#26377;&#21442;&#32771;&#25991;&#29486;&#21644;&#26377;&#38480;&#30340;&#34394;&#26500;&#20869;&#23481;&#12290;&#25105;&#20204;&#24050;&#32463;&#27979;&#35797;&#20102;CliniDigest&#22312;27&#20010;&#21307;&#23398;&#23376;&#39046;&#22495;&#20013;&#28041;&#21450;&#30340;457&#20010;&#35797;&#39564;&#30340;&#25688;&#35201;&#33021;&#21147;&#12290;&#23545;&#20110;&#27599;&#20010;&#39046;&#22495;&#65292;CliniDigest&#29983;&#25104;&#30340;&#25688;&#35201;&#24179;&#22343;&#20026;153&#20010;&#21333;&#35789;&#65292;&#26631;&#20934;&#24046;&#20026;69&#20010;&#21333;&#35789;&#65292;&#20854;&#20013;&#27599;&#20010;&#25688;&#35201;&#24179;&#22343;&#20351;&#29992;54&#20010;&#21333;&#35789;&#12290;
&lt;/p&gt;
&lt;p&gt;
A clinical trial is a study that evaluates new biomedical interventions. To design new trials, researchers draw inspiration from those current and completed. In 2022, there were on average more than 100 clinical trials submitted to ClinicalTrials.gov every day, with each trial having a mean of approximately 1500 words [1]. This makes it nearly impossible to keep up to date. To mitigate this issue, we have created a batch clinical trial summarizer called CliniDigest using GPT-3.5. CliniDigest is, to our knowledge, the first tool able to provide real-time, truthful, and comprehensive summaries of clinical trials. CliniDigest can reduce up to 85 clinical trial descriptions (approximately 10,500 words) into a concise 200-word summary with references and limited hallucinations. We have tested CliniDigest on its ability to summarize 457 trials divided across 27 medical subdomains. For each field, CliniDigest generates summaries of $\mu=153,\ \sigma=69 $ words, each of which utilizes $\mu=54\
&lt;/p&gt;</description></item><item><title>EmotionPrompt&#26159;&#19968;&#20010;&#22522;&#20110;&#24515;&#29702;&#23398;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24773;&#24863;&#21050;&#28608;&#34701;&#20837;&#21040;&#25552;&#31034;&#20013;&#65292;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#39033;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21516;&#26102;&#25913;&#21892;&#20102;&#20854;&#30495;&#23454;&#24615;&#21644;&#20449;&#24687;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.11760</link><description>&lt;p&gt;
EmotionPrompt: &#36890;&#36807;&#24773;&#24863;&#21050;&#28608;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#24515;&#29702;&#23398;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
EmotionPrompt: Leveraging Psychology for Large Language Models Enhancement via Emotional Stimulus. (arXiv:2307.11760v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11760
&lt;/p&gt;
&lt;p&gt;
EmotionPrompt&#26159;&#19968;&#20010;&#22522;&#20110;&#24515;&#29702;&#23398;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24773;&#24863;&#21050;&#28608;&#34701;&#20837;&#21040;&#25552;&#31034;&#20013;&#65292;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#39033;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21516;&#26102;&#25913;&#21892;&#20102;&#20854;&#30495;&#23454;&#24615;&#21644;&#20449;&#24687;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25512;&#29702;&#12289;&#35821;&#35328;&#29702;&#35299;&#21644;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#31561;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#24182;&#34987;&#35270;&#20026;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;LLMs&#23545;&#25552;&#31034;&#30340;&#25935;&#24863;&#24615;&#20173;&#28982;&#26159;&#20854;&#26085;&#24120;&#24212;&#29992;&#30340;&#20027;&#35201;&#29942;&#39048;&#12290;&#26412;&#25991;&#20174;&#24515;&#29702;&#23398;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#25552;&#20986;&#20102;EmotionPrompt&#26469;&#25506;&#32034;&#24773;&#24863;&#26234;&#33021;&#20197;&#25552;&#21319;LLMs&#30340;&#24615;&#33021;&#12290;EmotionPrompt&#22522;&#20110;&#19968;&#20010;&#38750;&#24120;&#31616;&#21333;&#26126;&#20102;&#30340;&#21407;&#21017;&#65306;&#23558;&#24773;&#24863;&#21050;&#28608;&#34701;&#20837;&#21040;&#25552;&#31034;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30456;&#21516;&#30340;&#21333;&#19968;&#25552;&#31034;&#27169;&#26495;&#19978;&#65292;&#19982;&#21407;&#22987;&#30340;&#38646;&#26679;&#26412;&#25552;&#31034;&#21644;Zero-shot-CoT&#30456;&#27604;&#65292;&#22312;8&#20010;&#20219;&#21153;&#19978;&#37117;&#26174;&#33879;&#20248;&#20110;&#22810;&#31181;&#27169;&#22411;&#65306;ChatGPT&#12289;Vicuna-13b&#12289;Bloom&#21644;T5&#12290;&#27492;&#22806;&#65292;&#35266;&#23519;&#21040;EmotionPrompt&#33021;&#22815;&#25552;&#39640;&#30495;&#23454;&#24615;&#21644;&#20449;&#24687;&#37327;&#12290;&#25105;&#20204;&#30456;&#20449;EmotionPrompt&#20026;&#25506;&#32034;&#36328;&#23398;&#31185;&#30693;&#35782;&#24320;&#36767;&#20102;&#19968;&#26465;&#26032;&#30340;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved significant performance in many fields such as reasoning, language understanding, and math problem-solving, and are regarded as a crucial step to artificial general intelligence (AGI). However, the sensitivity of LLMs to prompts remains a major bottleneck for their daily adoption. In this paper, we take inspiration from psychology and propose EmotionPrompt to explore emotional intelligence to enhance the performance of LLMs. EmotionPrompt operates on a remarkably straightforward principle: the incorporation of emotional stimulus into prompts. Experimental results demonstrate that our \method, using the same single prompt templates, significantly outperforms original zero-shot prompt and Zero-shot-CoT on 8 tasks with diverse models: ChatGPT, Vicuna-13b, Bloom, and T5. Further, EmotionPrompt was observed to improve both truthfulness and informativeness. We believe that EmotionPrompt heralds a novel avenue for exploring interdisciplinary knowledg
&lt;/p&gt;</description></item><item><title>Jina Embeddings&#26159;&#19968;&#32452;&#39640;&#24615;&#33021;&#30340;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#25991;&#26412;&#30340;&#35821;&#20041;&#26412;&#36136;&#12290;&#35813;&#35770;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;Jina Embeddings&#30340;&#24320;&#21457;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#24615;&#33021;&#35780;&#20272;&#39564;&#35777;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.11224</link><description>&lt;p&gt;
Jina Embeddings:&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#24615;&#33021;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models. (arXiv:2307.11224v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11224
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings&#26159;&#19968;&#32452;&#39640;&#24615;&#33021;&#30340;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#25991;&#26412;&#30340;&#35821;&#20041;&#26412;&#36136;&#12290;&#35813;&#35770;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;Jina Embeddings&#30340;&#24320;&#21457;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#24615;&#33021;&#35780;&#20272;&#39564;&#35777;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Jina Embeddings&#30001;&#19968;&#32452;&#39640;&#24615;&#33021;&#30340;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#32452;&#25104;&#65292;&#33021;&#22815;&#23558;&#21508;&#31181;&#25991;&#26412;&#36755;&#20837;&#36716;&#21270;&#20026;&#25968;&#20540;&#34920;&#31034;&#65292;&#20174;&#32780;&#25429;&#25417;&#25991;&#26412;&#30340;&#35821;&#20041;&#26412;&#36136;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#24182;&#38750;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#65292;&#20294;&#22312;&#23494;&#38598;&#26816;&#32034;&#21644;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#31561;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;Jina Embeddings&#30340;&#24320;&#21457;&#36807;&#31243;&#65292;&#20174;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#25104;&#23545;&#21644;&#19977;&#20803;&#25968;&#25454;&#38598;&#24320;&#22987;&#12290;&#23427;&#24378;&#35843;&#20102;&#25968;&#25454;&#28165;&#29702;&#22312;&#25968;&#25454;&#38598;&#20934;&#22791;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#23545;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#36827;&#34892;&#20102;&#28145;&#20837;&#25506;&#35752;&#65292;&#26368;&#21518;&#21033;&#29992;Massive Textual Embedding Benchmark&#65288;MTEB&#65289;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#24615;&#33021;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings constitutes a set of high-performance sentence embedding models adept at translating various textual inputs into numerical representations, thereby capturing the semantic essence of the text. While these models are not exclusively designed for text generation, they excel in applications such as dense retrieval and semantic textual similarity. This paper details the development of Jina Embeddings, starting with the creation of a high-quality pairwise and triplet dataset. It underlines the crucial role of data cleaning in dataset preparation, gives in-depth insights into the model training process, and concludes with a comprehensive performance evaluation using the Massive Textual Embedding Benchmark (MTEB).
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#22312;&#19981;&#21516;&#26102;&#38388;&#28857;&#19978;&#30340;&#24615;&#33021;&#21644;&#34892;&#20026;&#21464;&#21270;&#65292;&#21457;&#29616;&#23427;&#20204;&#30340;&#34920;&#29616;&#21487;&#20197;&#26377;&#24456;&#22823;&#30340;&#24046;&#24322;&#65292;&#21253;&#25324;&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#12289;&#22238;&#31572;&#25935;&#24863;&#38382;&#39064;&#12289;&#29983;&#25104;&#20195;&#30721;&#21644;&#35270;&#35273;&#25512;&#29702;&#31561;&#20219;&#21153;&#19978;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#30456;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;&#30340;&#34892;&#20026;&#22312;&#30456;&#23545;&#30701;&#30340;&#26102;&#38388;&#20869;&#21487;&#20197;&#21457;&#29983;&#26174;&#33879;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.09009</link><description>&lt;p&gt;
ChatGPT&#30340;&#34892;&#20026;&#38543;&#26102;&#38388;&#21464;&#21270;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
How is ChatGPT's behavior changing over time?. (arXiv:2307.09009v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#22312;&#19981;&#21516;&#26102;&#38388;&#28857;&#19978;&#30340;&#24615;&#33021;&#21644;&#34892;&#20026;&#21464;&#21270;&#65292;&#21457;&#29616;&#23427;&#20204;&#30340;&#34920;&#29616;&#21487;&#20197;&#26377;&#24456;&#22823;&#30340;&#24046;&#24322;&#65292;&#21253;&#25324;&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#12289;&#22238;&#31572;&#25935;&#24863;&#38382;&#39064;&#12289;&#29983;&#25104;&#20195;&#30721;&#21644;&#35270;&#35273;&#25512;&#29702;&#31561;&#20219;&#21153;&#19978;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#30456;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;&#30340;&#34892;&#20026;&#22312;&#30456;&#23545;&#30701;&#30340;&#26102;&#38388;&#20869;&#21487;&#20197;&#21457;&#29983;&#26174;&#33879;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GPT-3.5&#21644;GPT-4&#26159;&#20004;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26381;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#36827;&#34892;&#26356;&#26032;&#26159;&#19981;&#36879;&#26126;&#30340;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23545;GPT-3.5&#21644;GPT-4&#30340;2023&#24180;3&#26376;&#21644;2023&#24180;6&#26376;&#29256;&#26412;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#28041;&#21450;&#22235;&#39033;&#19981;&#21516;&#30340;&#20219;&#21153;&#65306;1&#65289;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#65292;2&#65289;&#22238;&#31572;&#25935;&#24863;/&#21361;&#38505;&#38382;&#39064;&#65292;3&#65289;&#29983;&#25104;&#20195;&#30721;&#21644;4&#65289;&#35270;&#35273;&#25512;&#29702;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;GPT-3.5&#21644;GPT-4&#30340;&#24615;&#33021;&#21644;&#34892;&#20026;&#22312;&#26102;&#38388;&#19978;&#21487;&#20197;&#26377;&#24456;&#22823;&#30340;&#21464;&#21270;&#12290;&#20363;&#22914;&#65292;GPT-4&#65288;2023&#24180;3&#26376;&#65289;&#22312;&#35782;&#21035;&#36136;&#25968;&#26041;&#38754;&#34920;&#29616;&#38750;&#24120;&#20986;&#33394;&#65288;&#20934;&#30830;&#29575;&#20026;97.6%&#65289;&#65292;&#20294;GPT-4&#65288;2023&#24180;6&#26376;&#65289;&#22312;&#30456;&#21516;&#30340;&#38382;&#39064;&#19978;&#34920;&#29616;&#38750;&#24120;&#24046;&#65288;&#20934;&#30830;&#29575;&#20026;2.4%&#65289;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;GPT-3.5&#65288;2023&#24180;6&#26376;&#65289;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#27604;GPT-3.5&#65288;2023&#24180;3&#26376;&#65289;&#35201;&#22909;&#24471;&#22810;&#12290;GPT-4&#22312;6&#26376;&#20221;&#23545;&#22238;&#31572;&#25935;&#24863;&#38382;&#39064;&#30340;&#24847;&#24895;&#36739;3&#26376;&#20221;&#35201;&#20302;&#65292;&#32780;&#26080;&#35770;&#26159;GPT-4&#36824;&#26159;GPT-3.5&#22312;6&#26376;&#20221;&#30340;&#20195;&#30721;&#29983;&#25104;&#20013;&#37117;&#26377;&#26356;&#22810;&#30340;&#26684;&#24335;&#38169;&#35823;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#30456;&#21516;LLM&#26381;&#21153;&#30340;&#34892;&#20026;&#22312;&#30456;&#23545;&#36739;&#30701;&#30340;&#26102;&#38388;&#20869;&#21487;&#20197;&#21457;&#29983;&#37325;&#22823;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
GPT-3.5 and GPT-4 are the two most widely used large language model (LLM) services. However, when and how these models are updated over time is opaque. Here, we evaluate the March 2023 and June 2023 versions of GPT-3.5 and GPT-4 on four diverse tasks: 1) solving math problems, 2) answering sensitive/dangerous questions, 3) generating code and 4) visual reasoning. We find that the performance and behavior of both GPT-3.5 and GPT-4 can vary greatly over time. For example, GPT-4 (March 2023) was very good at identifying prime numbers (accuracy 97.6%) but GPT-4 (June 2023) was very poor on these same questions (accuracy 2.4%). Interestingly GPT-3.5 (June 2023) was much better than GPT-3.5 (March 2023) in this task. GPT-4 was less willing to answer sensitive questions in June than in March, and both GPT-4 and GPT-3.5 had more formatting mistakes in code generation in June than in March. Overall, our findings shows that the behavior of the same LLM service can change substantially in a relat
&lt;/p&gt;</description></item><item><title>SAS&#35270;&#39057;&#38382;&#31572;&#36890;&#36807;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#35299;&#20915;&#20102;&#35270;&#39057;&#38382;&#31572;&#20013;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;</title><link>http://arxiv.org/abs/2307.04192</link><description>&lt;p&gt;
SAS&#35270;&#39057;&#38382;&#31572;&#65306;&#33258;&#36866;&#24212;&#37319;&#26679;&#29992;&#20110;&#39640;&#25928;&#35270;&#39057;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
SAS Video-QA: Self-Adaptive Sampling for Efficient Video Question-Answering. (arXiv:2307.04192v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04192
&lt;/p&gt;
&lt;p&gt;
SAS&#35270;&#39057;&#38382;&#31572;&#36890;&#36807;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#35299;&#20915;&#20102;&#35270;&#39057;&#38382;&#31572;&#20013;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#38382;&#31572;&#26159;&#35270;&#39057;&#29702;&#35299;&#39046;&#22495;&#30340;&#19968;&#39033;&#22522;&#30784;&#20219;&#21153;&#12290;&#23613;&#31649;&#24403;&#21069;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(VLMs)&#37197;&#22791;&#20102;&#35270;&#39057;&#21464;&#25442;&#22120;(Video Transformers)&#65292;&#23454;&#29616;&#20102;&#26102;&#38388;&#24314;&#27169;&#24182;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#32467;&#26524;&#65292;&#20294;&#20195;&#20215;&#26159;&#24040;&#22823;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#22240;&#27492;&#22312;&#23454;&#26102;&#24212;&#29992;&#22330;&#26223;&#20013;&#36807;&#20110;&#26114;&#36149;&#12290;&#19968;&#31181;&#32463;&#27982;&#30340;&#35299;&#20915;&#26041;&#27861;&#26159;&#21482;&#23545;&#35270;&#39057;&#30340;&#19968;&#23567;&#37096;&#20998;&#24103;&#36827;&#34892;&#37319;&#26679;&#65292;&#26469;&#20195;&#34920;&#35270;&#39057;&#30340;&#20027;&#35201;&#20869;&#23481;&#65292;&#24182;&#22312;&#36825;&#20123;&#37319;&#26679;&#24103;&#19978;&#35843;&#25972;&#22270;&#20687;-&#25991;&#26412;&#27169;&#22411;&#12290;&#26368;&#36817;&#30340;&#35270;&#39057;&#29702;&#35299;&#27169;&#22411;&#36890;&#24120;&#38543;&#26426;&#37319;&#26679;&#19968;&#32452;&#24103;&#25110;&#29255;&#27573;&#65292;&#32780;&#19981;&#32771;&#34385;&#23427;&#20204;&#30340;&#20869;&#37096;&#20851;&#32852;&#24615;&#21644;&#19982;&#38382;&#39064;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#26080;&#30446;&#26631;&#30340;&#37319;&#26679;&#21487;&#33021;&#20250;&#36951;&#28431;&#21487;&#20197;&#25512;&#23548;&#20986;&#27491;&#30830;&#31572;&#26696;&#30340;&#20851;&#38190;&#24103;&#65292;&#22312;&#37319;&#26679;&#31232;&#30095;&#31243;&#24230;&#22686;&#21152;&#26102;&#65292;&#24773;&#20917;&#20250;&#21464;&#24471;&#26356;&#31967;&#65292;&#32780;&#38543;&#30528;&#35270;&#39057;&#38271;&#24230;&#30340;&#22686;&#21152;&#65292;&#37319;&#26679;&#31232;&#30095;&#31243;&#24230;&#20063;&#20250;&#22686;&#21152;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#24103;&#37319;&#26679;&#31574;&#30053;&#65292;&#20998;&#21035;&#26159;
&lt;/p&gt;
&lt;p&gt;
Video question--answering is a fundamental task in the field of video understanding. Although current vision--language models (VLMs) equipped with Video Transformers have enabled temporal modeling and yielded superior results, they are at the cost of huge computational power and thus too expensive to deploy in real-time application scenarios. An economical workaround only samples a small portion of frames to represent the main content of that video and tune an image--text model on these sampled frames. Recent video understanding models usually randomly sample a set of frames or clips, regardless of internal correlations between their visual contents, nor their relevance to the problem. We argue that such kinds of aimless sampling may omit the key frames from which the correct answer can be deduced, and the situation gets worse when the sampling sparsity increases, which always happens as the video lengths increase. To mitigate this issue, we propose two frame sampling strategies, namel
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21453;&#20107;&#23454;&#20219;&#21153;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#19968;&#23450;&#30340;&#25277;&#35937;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#20219;&#21153;&#27714;&#35299;&#36807;&#31243;&#20013;&#24448;&#24448;&#20063;&#20381;&#36182;&#20110;&#29421;&#31364;&#12289;&#38590;&#20197;&#36716;&#31227;&#30340;&#36807;&#31243;&#65292;&#36825;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#35299;&#37322;&#21644;&#29702;&#35299;&#26377;&#30528;&#37325;&#35201;&#30340;&#21551;&#31034;&#12290;</title><link>http://arxiv.org/abs/2307.02477</link><description>&lt;p&gt;
&#25512;&#29702;&#36824;&#26159;&#32972;&#35829;&#65311;&#36890;&#36807;&#21453;&#20107;&#23454;&#20219;&#21153;&#25506;&#32034;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#21644;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks. (arXiv:2307.02477v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02477
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21453;&#20107;&#23454;&#20219;&#21153;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#19968;&#23450;&#30340;&#25277;&#35937;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#20219;&#21153;&#27714;&#35299;&#36807;&#31243;&#20013;&#24448;&#24448;&#20063;&#20381;&#36182;&#20110;&#29421;&#31364;&#12289;&#38590;&#20197;&#36716;&#31227;&#30340;&#36807;&#31243;&#65292;&#36825;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#35299;&#37322;&#21644;&#29702;&#35299;&#26377;&#30528;&#37325;&#35201;&#30340;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#20986;&#33394;&#34920;&#29616;&#34920;&#26126;&#23427;&#20204;&#20855;&#22791;&#19968;&#23450;&#31243;&#24230;&#30340;&#25277;&#35937;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#20123;&#33021;&#21147;&#26159;&#36890;&#29992;&#19988;&#21487;&#36716;&#31227;&#30340;&#65292;&#36824;&#26159;&#19987;&#38376;&#38024;&#23545;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#29305;&#23450;&#20219;&#21153;&#65311;&#20026;&#20102;&#20998;&#24320;&#36825;&#20123;&#25928;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#22522;&#20110;&#8220;&#21453;&#20107;&#23454;&#8221;&#20219;&#21153;&#21464;&#31181;&#65292;&#36825;&#20123;&#21464;&#31181;&#19982;&#25903;&#25745;&#26631;&#20934;&#20219;&#21153;&#30340;&#40664;&#35748;&#20551;&#35774;&#26377;&#25152;&#20559;&#31163;&#12290;&#22312;&#19968;&#22871;&#21253;&#21547;11&#20010;&#20219;&#21153;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#21453;&#20107;&#23454;&#21464;&#31181;&#30340;&#38750;&#24179;&#20961;&#24615;&#33021;&#65292;&#20294;&#19982;&#40664;&#35748;&#26465;&#20214;&#30456;&#27604;&#65292;&#24615;&#33021;&#26174;&#33879;&#32780;&#25345;&#32493;&#22320;&#19979;&#38477;&#12290;&#36825;&#34920;&#26126;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20855;&#22791;&#25277;&#35937;&#20219;&#21153;&#27714;&#35299;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#20063;&#20381;&#36182;&#20110;&#29421;&#31364;&#12289;&#38590;&#20197;&#36716;&#31227;&#30340;&#20219;&#21153;&#27714;&#35299;&#36807;&#31243;&#12290;&#36825;&#20123;&#32467;&#26524;&#20419;&#20351;&#25105;&#20204;&#23545;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#36827;&#34892;&#26356;&#21152;&#35880;&#24910;&#30340;&#35299;&#37322;&#65292;&#20197;&#21306;&#20998;&#36825;&#20123;&#34892;&#20026;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
The impressive performance of recent language models across a wide range of tasks suggests that they possess a degree of abstract reasoning skills. Are these skills general and transferable, or specialized to specific tasks seen during pretraining? To disentangle these effects, we propose an evaluation framework based on "counterfactual" task variants that deviate from the default assumptions underlying standard tasks. Across a suite of 11 tasks, we observe nontrivial performance on the counterfactual variants, but nevertheless find that performance substantially and consistently degrades compared to the default conditions. This suggests that while current LMs may possess abstract task-solving skills to a degree, they often also rely on narrow, non-transferable procedures for task-solving. These results motivate a more careful interpretation of language model performance that teases apart these aspects of behavior.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#31995;&#32479;&#35780;&#20272;&#20102;ChatGPT&#12289;GPT-4&#21644;&#20154;&#31867;&#23548;&#24072;&#22312;&#19981;&#21516;&#30340;&#32534;&#31243;&#25945;&#32946;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;GPT-4&#20248;&#20110;ChatGPT&#65292;&#25509;&#36817;&#20110;&#20154;&#31867;&#23548;&#24072;&#12290;</title><link>http://arxiv.org/abs/2306.17156</link><description>&lt;p&gt;
&#32534;&#31243;&#25945;&#32946;&#30340;&#29983;&#25104;AI&#65306;&#27604;&#36739;ChatGPT&#12289;GPT-4&#21644;&#20154;&#31867;&#23548;&#24072;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Generative AI for Programming Education: Benchmarking ChatGPT, GPT-4, and Human Tutors. (arXiv:2306.17156v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17156
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#31995;&#32479;&#35780;&#20272;&#20102;ChatGPT&#12289;GPT-4&#21644;&#20154;&#31867;&#23548;&#24072;&#22312;&#19981;&#21516;&#30340;&#32534;&#31243;&#25945;&#32946;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;GPT-4&#20248;&#20110;ChatGPT&#65292;&#25509;&#36817;&#20110;&#20154;&#31867;&#23548;&#24072;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;AI&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#39640;&#35745;&#31639;&#26426;&#25945;&#32946;&#26041;&#38754;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#20026;&#21021;&#32423;&#32534;&#31243;&#25552;&#20379;&#19979;&#19968;&#20195;&#25945;&#32946;&#25216;&#26415;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#30740;&#31350;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#19982;&#32534;&#31243;&#25945;&#32946;&#30456;&#20851;&#30340;&#19981;&#21516;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65307;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#30001;&#20110;&#22810;&#31181;&#21407;&#22240;&#32780;&#21463;&#38480;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#32771;&#34385;&#30340;&#26159;&#24050;&#32463;&#36807;&#26102;&#30340;&#27169;&#22411;&#25110;&#20165;&#20165;&#29305;&#23450;&#30340;&#22330;&#26223;&#12290;&#22240;&#27492;&#65292;&#32570;&#20047;&#19968;&#20010;&#31995;&#32479;&#30340;&#30740;&#31350;&#26469;&#23545;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#30340;&#32534;&#31243;&#25945;&#32946;&#22330;&#26223;&#22522;&#20934;&#27979;&#35797;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#20004;&#20010;&#27169;&#22411;&#65292;ChatGPT&#65288;&#22522;&#20110;GPT-3.5&#65289;&#21644;GPT-4&#65292;&#24182;&#23558;&#20854;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#19982;&#20154;&#31867;&#23548;&#24072;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#20351;&#29992;&#20116;&#20010;&#21021;&#32423;Python&#32534;&#31243;&#38382;&#39064;&#21644;&#26469;&#33258;&#22312;&#32447;&#24179;&#21488;&#30340;&#30495;&#23454;&#38169;&#35823;&#31243;&#24207;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#20351;&#29992;&#19987;&#23478;&#35780;&#27880;&#26469;&#35780;&#20272;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-4&#26126;&#26174;&#20248;&#20110;ChatGPT&#65288;&#22522;&#20110;GPT-3.5&#65289;&#65292;&#24182;&#19988;&#25509;&#36817;&#20110;&#20154;&#31867;&#23548;&#24072;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI and large language models hold great promise in enhancing computing education by powering next-generation educational technologies for introductory programming. Recent works have studied these models for different scenarios relevant to programming education; however, these works are limited for several reasons, as they typically consider already outdated models or only specific scenario(s). Consequently, there is a lack of a systematic study that benchmarks state-of-the-art models for a comprehensive set of programming education scenarios. In our work, we systematically evaluate two models, ChatGPT (based on GPT-3.5) and GPT-4, and compare their performance with human tutors for a variety of scenarios. We evaluate using five introductory Python programming problems and real-world buggy programs from an online platform, and assess performance using expert-based annotations. Our results show that GPT-4 drastically outperforms ChatGPT (based on GPT-3.5) and comes close to hu
&lt;/p&gt;</description></item><item><title>CrunchGPT&#26159;&#19968;&#20010;&#22522;&#20110;ChatGPT&#30340;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#26694;&#26550;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#29992;&#25143;&#25552;&#31034;&#26469;&#21327;&#35843;&#25972;&#20010;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#23454;&#29616;&#26080;&#32541;&#38598;&#25104;&#25968;&#25454;&#21644;&#29289;&#29702;&#30693;&#35782;&#65292;&#35299;&#20915;&#20102;SciML&#22312;&#39044;&#22788;&#29702;&#12289;&#38382;&#39064;&#24314;&#27169;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#21518;&#22788;&#29702;&#21644;&#20998;&#26512;&#31561;&#26041;&#38754;&#30340;&#32791;&#26102;&#38382;&#39064;&#65292;&#25299;&#23637;&#20102;&#20854;&#24037;&#19994;&#24212;&#29992;&#21644;&#25968;&#23383;&#23402;&#29983;&#26694;&#26550;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15551</link><description>&lt;p&gt;
CrunchGPT&#65306;&#22522;&#20110;ChatGPT&#30340;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CrunchGPT: A chatGPT assisted framework for scientific machine learning. (arXiv:2306.15551v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15551
&lt;/p&gt;
&lt;p&gt;
CrunchGPT&#26159;&#19968;&#20010;&#22522;&#20110;ChatGPT&#30340;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#26694;&#26550;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#29992;&#25143;&#25552;&#31034;&#26469;&#21327;&#35843;&#25972;&#20010;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#23454;&#29616;&#26080;&#32541;&#38598;&#25104;&#25968;&#25454;&#21644;&#29289;&#29702;&#30693;&#35782;&#65292;&#35299;&#20915;&#20102;SciML&#22312;&#39044;&#22788;&#29702;&#12289;&#38382;&#39064;&#24314;&#27169;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#21518;&#22788;&#29702;&#21644;&#20998;&#26512;&#31561;&#26041;&#38754;&#30340;&#32791;&#26102;&#38382;&#39064;&#65292;&#25299;&#23637;&#20102;&#20854;&#24037;&#19994;&#24212;&#29992;&#21644;&#25968;&#23383;&#23402;&#29983;&#26694;&#26550;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#65288;SciML&#65289;&#36817;&#24180;&#26469;&#22312;&#35745;&#31639;&#31185;&#23398;&#21644;&#24037;&#31243;&#30340;&#35768;&#22810;&#19981;&#21516;&#39046;&#22495;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;&#20854;&#30446;&#26631;&#26159;&#22312;&#19981;&#38656;&#35201;&#22797;&#26434;&#21644;&#35745;&#31639;&#23494;&#38598;&#30340;&#25968;&#25454;&#21516;&#21270;&#26041;&#26696;&#30340;&#24773;&#20917;&#19979;&#65292;&#26080;&#32541;&#22320;&#23558;&#25968;&#25454;&#21644;&#29289;&#29702;&#30693;&#35782;&#38598;&#25104;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;&#39044;&#22788;&#29702;&#12289;&#38382;&#39064;&#24314;&#27169;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#21518;&#22788;&#29702;&#21644;&#20998;&#26512;&#20173;&#28982;&#26159;&#32791;&#26102;&#30340;&#65292;&#24182;&#19988;&#21487;&#33021;&#38480;&#21046;SciML&#22312;&#24037;&#19994;&#24212;&#29992;&#21644;&#25968;&#23383;&#23402;&#29983;&#26694;&#26550;&#20013;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23558;SciML&#30340;&#21508;&#20010;&#38454;&#27573;&#25972;&#21512;&#21040;ChatGPT&#30340;&#20254;&#19979;&#65292;&#24418;&#25104;CrunchGPT&#65292;&#23427;&#36890;&#36807;&#29992;&#25143;&#31616;&#21333;&#30340;&#25552;&#31034;&#26469;&#21327;&#35843;&#25972;&#20010;SciML&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#20010;&#31034;&#20363;&#65292;&#28436;&#31034;&#20102;CrunchGPT&#22312;&#27668;&#21160;&#23398;&#20013;&#20248;&#21270;&#26426;&#32764;&#21644;&#22312;&#21508;&#31181;&#20960;&#20309;&#24418;&#29366;&#20013;&#33719;&#24471;&#27969;&#22330;&#30340;&#28508;&#22312;&#29992;&#36884;&#65292;&#24182;&#24378;&#35843;&#20102;&#39564;&#35777;&#38454;&#27573;&#12290;&#20026;&#20102;&#28436;&#31034;CrunchGPT&#30340;&#27969;&#31243;&#21644;
&lt;/p&gt;
&lt;p&gt;
Scientific Machine Learning (SciML) has advanced recently across many different areas in computational science and engineering. The objective is to integrate data and physics seamlessly without the need of employing elaborate and computationally taxing data assimilation schemes. However, preprocessing, problem formulation, code generation, postprocessing and analysis are still time consuming and may prevent SciML from wide applicability in industrial applications and in digital twin frameworks. Here, we integrate the various stages of SciML under the umbrella of ChatGPT, to formulate CrunchGPT, which plays the role of a conductor orchestrating the entire workflow of SciML based on simple prompts by the user. Specifically, we present two examples that demonstrate the potential use of CrunchGPT in optimizing airfoils in aerodynamics, and in obtaining flow fields in various geometries in interactive mode, with emphasis on the validation stage. To demonstrate the flow of the CrunchGPT, and
&lt;/p&gt;</description></item><item><title>W-procer&#26159;&#19968;&#31181;&#22522;&#20110;&#21152;&#26435;&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#30340;&#21307;&#23398;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;&#65292;&#22312;&#26500;&#24314;&#22522;&#20110;&#21407;&#22411;&#30340;&#23545;&#27604;&#25439;&#22833;&#21644;&#21152;&#26435;&#32593;&#32476;&#26041;&#38754;&#20855;&#26377;&#21019;&#26032;&#24615;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.18624</link><description>&lt;p&gt;
W-procer: &#22522;&#20110;&#21152;&#26435;&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#30340;&#21307;&#23398;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
W-procer: Weighted Prototypical Contrastive Learning for Medical Few-Shot Named Entity Recognition. (arXiv:2305.18624v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18624
&lt;/p&gt;
&lt;p&gt;
W-procer&#26159;&#19968;&#31181;&#22522;&#20110;&#21152;&#26435;&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#30340;&#21307;&#23398;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;&#65292;&#22312;&#26500;&#24314;&#22522;&#20110;&#21407;&#22411;&#30340;&#23545;&#27604;&#25439;&#22833;&#21644;&#21152;&#26435;&#32593;&#32476;&#26041;&#38754;&#20855;&#26377;&#21019;&#26032;&#24615;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#24050;&#25104;&#20026;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#30340;&#19968;&#31181;&#21463;&#27426;&#36814;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20256;&#32479;&#37197;&#32622;&#21147;&#27714;&#20943;&#23569;&#20855;&#26377;&#30456;&#21516;&#26631;&#31614;&#30340;&#26631;&#35760;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#24182;&#22686;&#21152;&#20855;&#26377;&#19981;&#21516;&#26631;&#31614;&#30340;&#26631;&#35760;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#23384;&#22312;&#22823;&#37327;&#34987;&#27880;&#37322;&#20026;&#8220;O&#8221;&#65288;&#21363;&#8220;OUTSIDE&#8221;&#65289;&#30340;&#23454;&#20307;&#65292;&#24182;&#19988;&#23427;&#20204;&#19981;&#24076;&#26395;&#34987;&#25512;&#31163;&#21040;&#24403;&#21069;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26631;&#35760;&#20026;&#8220;O&#8221;&#20197;&#22806;&#30340;&#20854;&#20182;&#23454;&#20307;&#65292;&#36825;&#31181;&#35774;&#23450;&#25928;&#26524;&#19981;&#20339;&#65292;&#21487;&#33021;&#20250;&#24471;&#20986;&#21547;&#26377;&#22122;&#22768;&#21407;&#22411;&#26631;&#31614;&#30340;&#35821;&#20041;&#34920;&#31034;&#65292;&#23613;&#31649;&#23384;&#22312;&#35768;&#22810;&#8220;O&#8221;&#26631;&#31614;&#23454;&#20307;&#19982;&#26377;&#26631;&#31614;&#23454;&#20307;&#30456;&#20851;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21307;&#23398;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#22522;&#20110;&#21152;&#26435;&#21407;&#22411;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65288;W-PROCER&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20027;&#35201;&#22260;&#32469;&#26500;&#24314;&#22522;&#20110;&#21407;&#22411;&#30340;&#23545;&#27604;&#25439;&#22833;&#21644;&#21152;&#26435;&#32593;&#32476;&#23637;&#24320;&#12290;&#36825;&#20123;&#32452;&#20214;&#22312;&#21327;&#21161;&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#38754;&#21457;&#25381;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23558;W-PROCER&#24212;&#29992;&#20110;&#19968;&#20010;&#20844;&#20849;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#20248;&#24322;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning has become a popular solution for few-shot Name Entity Recognization (NER). The conventional configuration strives to reduce the distance between tokens with the same labels and increase the distance between tokens with different labels. The effect of this setup may, however, in the medical domain, there are a lot of entities annotated as OUTSIDE (O), and they are undesirably pushed apart to other entities that are not labeled as OUTSIDE (O) by the current contrastive learning method end up with a noisy prototype for the semantic representation of the label, though there are many OUTSIDE (O) labeled entities are relevant to the labeled entities. To address this challenge, we propose a novel method named Weighted Prototypical Contrastive Learning for Medical Few Shot Named Entity Recognization (W-PROCER). Our approach primarily revolves around constructing the prototype-based contractive loss and weighting network. These components play a crucial role in assisting t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#21457;&#29616;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#22312;&#20219;&#21153;&#29305;&#23450;&#23376;&#31354;&#38388;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#21442;&#25968;&#21270;&#21644;&#24494;&#35843;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;&#35813;&#23376;&#31354;&#38388;&#20013;&#65292;&#21482;&#38656;&#23569;&#37327;&#33258;&#30001;&#21442;&#25968;&#21363;&#21487;&#26377;&#25928;&#24494;&#35843;&#27169;&#22411;&#65292;&#24182;&#19988;&#26576;&#20123;&#32500;&#24230;&#23545;&#20110;&#24341;&#20837;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2305.17446</link><description>&lt;p&gt;
&#24494;&#23567;&#23376;&#31354;&#38388;&#20013;&#21457;&#29983;&#24494;&#35843;: &#25506;&#32034;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#22312;&#20219;&#21153;&#29305;&#23450;&#23376;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning Happens in Tiny Subspaces: Exploring Intrinsic Task-specific Subspaces of Pre-trained Language Models. (arXiv:2305.17446v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17446
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#21457;&#29616;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#22312;&#20219;&#21153;&#29305;&#23450;&#23376;&#31354;&#38388;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#21442;&#25968;&#21270;&#21644;&#24494;&#35843;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;&#35813;&#23376;&#31354;&#38388;&#20013;&#65292;&#21482;&#38656;&#23569;&#37327;&#33258;&#30001;&#21442;&#25968;&#21363;&#21487;&#26377;&#25928;&#24494;&#35843;&#27169;&#22411;&#65292;&#24182;&#19988;&#26576;&#20123;&#32500;&#24230;&#23545;&#20110;&#24341;&#20837;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#30693;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#36807;&#24230;&#21442;&#25968;&#21270;&#24182;&#20855;&#26377;&#26174;&#33879;&#30340;&#20887;&#20313;&#65292;&#34920;&#26126;PLMs&#30340;&#33258;&#30001;&#24230;&#36739;&#23567;&#12290;&#26412;&#25991;&#20174;&#26032;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#37325;&#26032;&#21442;&#25968;&#21270;&#21644;&#24494;&#35843;PLMs&#30340;&#38382;&#39064;&#65306;&#21457;&#29616;&#20869;&#22312;&#30340;&#20219;&#21153;&#29305;&#23450;&#23376;&#31354;&#38388;&#12290;&#20855;&#20307;&#22320;&#65292;&#36890;&#36807;&#21033;&#29992;&#32473;&#23450;&#20219;&#21153;&#30340;&#24494;&#35843;&#36807;&#31243;&#30340;&#21160;&#24577;&#65292;&#23398;&#20064;&#20102;&#21442;&#25968;&#20248;&#21270;&#36712;&#36857;&#20197;&#25581;&#31034;&#20854;&#20869;&#22312;&#30340;&#20219;&#21153;&#29305;&#23450;&#23376;&#31354;&#38388;&#12290;&#19968;&#20010;&#20851;&#38190;&#21457;&#29616;&#26159;&#65292;&#22312;&#23376;&#31354;&#38388;&#20013;&#65292;PLMs&#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#30340;&#33258;&#30001;&#21442;&#25968;&#36827;&#34892;&#26377;&#25928;&#30340;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#23376;&#31354;&#38388;&#30340;&#24494;&#35843;&#36807;&#31243;&#20013;&#20986;&#29616;&#20102;&#19968;&#20123;&#24322;&#24120;&#32500;&#24230;&#12290;&#31105;&#29992;&#36825;&#20123;&#32500;&#24230;&#20250;&#20005;&#37325;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#12290;&#36825;&#34920;&#26126;&#36825;&#20123;&#32500;&#24230;&#23545;&#20110;&#24341;&#20837;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#21040;&#19979;&#28216;&#20219;&#21153;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models (PLMs) are known to be overly parameterized and have significant redundancy, indicating a small degree of freedom of the PLMs. Motivated by the observation, in this paper, we study the problem of re-parameterizing and fine-tuning PLMs from a new perspective: Discovery of intrinsic task-specific subspace. Specifically, by exploiting the dynamics of the fine-tuning process for a given task, the parameter optimization trajectory is learned to uncover its intrinsic task-specific subspace. A key finding is that PLMs can be effectively fine-tuned in the subspace with a small number of free parameters. Beyond, we observe some outlier dimensions emerging during fine-tuning in the subspace. Disabling these dimensions degrades the model performance significantly. This suggests that these dimensions are crucial to induce task-specific knowledge to downstream tasks.
&lt;/p&gt;</description></item><item><title>Diable&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#31995;&#32479;&#65292;&#23427;&#36890;&#36807;&#22312;&#34920;&#26684;&#19978;&#36827;&#34892;&#25805;&#20316;&#26469;&#26356;&#26032;&#23545;&#35805;&#29366;&#24577;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#26102;&#38388;&#25928;&#29575;&#25552;&#39640;&#20102;2.4&#20493;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#31454;&#20105;&#24615;&#30340;&#30446;&#26631;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.17020</link><description>&lt;p&gt;
Diable: &#22312;&#34920;&#26684;&#19978;&#36827;&#34892;&#30340;&#39640;&#25928;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
Diable: Efficient Dialogue State Tracking as Operations on Tables. (arXiv:2305.17020v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17020
&lt;/p&gt;
&lt;p&gt;
Diable&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#31995;&#32479;&#65292;&#23427;&#36890;&#36807;&#22312;&#34920;&#26684;&#19978;&#36827;&#34892;&#25805;&#20316;&#26469;&#26356;&#26032;&#23545;&#35805;&#29366;&#24577;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#26102;&#38388;&#25928;&#29575;&#25552;&#39640;&#20102;2.4&#20493;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#31454;&#20105;&#24615;&#30340;&#30446;&#26631;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#31995;&#32479;&#23558;&#23436;&#25972;&#30340;&#23545;&#35805;&#21382;&#21490;&#20316;&#20026;&#36755;&#20837;&#65292;&#23558;&#24403;&#21069;&#29366;&#24577;&#34920;&#31034;&#20026;&#21253;&#21547;&#25152;&#26377;&#27133;&#30340;&#21015;&#34920;&#65292;&#24182;&#22312;&#27599;&#20010;&#23545;&#35805;&#22238;&#21512;&#20013;&#20174;&#22836;&#24320;&#22987;&#29983;&#25104;&#25972;&#20010;&#29366;&#24577;&#12290;&#36825;&#31181;&#26041;&#27861;&#25928;&#29575;&#20302;&#19979;&#65292;&#29305;&#21035;&#26159;&#24403;&#27133;&#30340;&#25968;&#37327;&#24456;&#22810;&#19988;&#23545;&#35805;&#24456;&#38271;&#26102;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Diable&#65292;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#24418;&#24335;&#21270;&#26041;&#27861;&#65292;&#31616;&#21270;&#20102;&#39640;&#25928;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#31995;&#32479;&#30340;&#35774;&#35745;&#21644;&#23454;&#29616;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#22320;&#23884;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#23545;&#35805;&#29366;&#24577;&#34920;&#31034;&#20026;&#34920;&#26684;&#65292;&#24182;&#23558;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#24418;&#24335;&#21270;&#20026;&#34920;&#26684;&#25805;&#20316;&#20219;&#21153;&#12290;&#22312;&#27599;&#20010;&#22238;&#21512;&#20013;&#65292;&#31995;&#32479;&#36890;&#36807;&#22522;&#20110;&#23545;&#35805;&#19978;&#19979;&#25991;&#29983;&#25104;&#34920;&#26684;&#25805;&#20316;&#26469;&#26356;&#26032;&#20808;&#21069;&#30340;&#29366;&#24577;&#12290;&#22312;MultiWoz&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;Diable (i) &#20248;&#20110;&#24378;&#22823;&#30340;&#39640;&#25928;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#22522;&#20934;&#65292;(ii) &#26102;&#38388;&#25928;&#29575;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;2.4&#20493;&#65292;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#24615;&#30340;&#32852;&#21512;&#30446;&#26631;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;(iii) &#23545;&#26080;&#22122;&#22768;&#30340;&#36755;&#20837;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequence-to-sequence state-of-the-art systems for dialogue state tracking (DST) use the full dialogue history as input, represent the current state as a list with all the slots, and generate the entire state from scratch at each dialogue turn. This approach is inefficient, especially when the number of slots is large and the conversation is long. We propose Diable, a new task formalisation that simplifies the design and implementation of efficient DST systems and allows one to easily plug and play large language models. We represent the dialogue state as a table and formalise DST as a table manipulation task. At each turn, the system updates the previous state by generating table operations based on the dialogue context. Extensive experimentation on the MultiWoz datasets demonstrates that Diable (i) outperforms strong efficient DST baselines, (ii) is 2.4x more time efficient than current state-of-the-art methods while retaining competitive Joint Goal Accuracy, and (iii) is robust to no
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlpacaFarm&#30340;&#20302;&#25104;&#26412;&#27169;&#25311;&#22120;&#65292;&#35813;&#27169;&#25311;&#22120;&#20026;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#35774;&#35745;LLM&#25552;&#31034;&#26469;&#27169;&#25311;&#20154;&#31867;&#21453;&#39304;&#65292;&#25552;&#20986;&#33258;&#21160;&#35780;&#20272;&#24182;&#25552;&#20379;&#21442;&#32771;&#23454;&#29616;&#65292;&#20811;&#26381;&#20102;&#25968;&#25454;&#25910;&#38598;&#30340;&#39640;&#26114;&#25104;&#26412;&#12289;&#32570;&#20047;&#21487;&#20449;&#30340;&#35780;&#20272;&#21644;&#32570;&#20047;&#21442;&#32771;&#26041;&#27861;&#23454;&#29616;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.14387</link><description>&lt;p&gt;
AlpacaFarm: &#19968;&#31181;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#27169;&#25311;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback. (arXiv:2305.14387v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14387
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlpacaFarm&#30340;&#20302;&#25104;&#26412;&#27169;&#25311;&#22120;&#65292;&#35813;&#27169;&#25311;&#22120;&#20026;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#35774;&#35745;LLM&#25552;&#31034;&#26469;&#27169;&#25311;&#20154;&#31867;&#21453;&#39304;&#65292;&#25552;&#20986;&#33258;&#21160;&#35780;&#20272;&#24182;&#25552;&#20379;&#21442;&#32771;&#23454;&#29616;&#65292;&#20811;&#26381;&#20102;&#25968;&#25454;&#25910;&#38598;&#30340;&#39640;&#26114;&#25104;&#26412;&#12289;&#32570;&#20047;&#21487;&#20449;&#30340;&#35780;&#20272;&#21644;&#32570;&#20047;&#21442;&#32771;&#26041;&#27861;&#23454;&#29616;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#22240;&#20854;&#33391;&#22909;&#30340;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#32780;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#24320;&#21457;&#36825;&#20123;LLMs&#38656;&#35201;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#35757;&#32451;&#30340;&#22797;&#26434;&#19988;&#23578;&#19981;&#26126;&#30830;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;&#23558;&#27492;&#25351;&#20196;&#36319;&#38543;&#36807;&#31243;&#22797;&#21046;&#21644;&#29702;&#35299;&#38754;&#20020;&#19977;&#22823;&#25361;&#25112;&#65306; &#25968;&#25454;&#25910;&#38598;&#30340;&#39640;&#26114;&#25104;&#26412;&#65292;&#32570;&#20047;&#21487;&#20449;&#30340;&#35780;&#20272;&#21644;&#32570;&#20047;&#21442;&#32771;&#26041;&#27861;&#23454;&#29616;&#12290;&#25105;&#20204;&#36890;&#36807;AlpacaFarm&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#36825;&#26159;&#19968;&#20010;&#20302;&#25104;&#26412;&#30340;&#27169;&#25311;&#22120;&#65292;&#21487;&#29992;&#20110;&#20174;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#12290;&#31532;&#19968;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;LLM&#25552;&#31034;&#26469;&#27169;&#25311;&#20154;&#31867;&#21453;&#39304;&#65292;&#20854;&#25104;&#26412;&#27604;&#20247;&#21253;&#24037;&#20316;&#32773;&#20415;&#23452;45&#20493;&#65292;&#24182;&#19988;&#19982;&#20154;&#31867;&#21453;&#39304;&#20855;&#26377;&#39640;&#24230;&#19968;&#33268;&#24615;&#12290;&#31532;&#20108;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#30495;&#23454;&#19990;&#30028;&#20132;&#20114;&#20013;&#33719;&#24471;&#30340;&#20154;&#31867;&#25351;&#20196;&#36827;&#34892;&#39564;&#35777;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#20026;&#20960;&#31181;&#20174;&#37197;&#23545;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#65288;PPO&#65292;best-of-n&#65292;expert iteration&#31561;&#65289;&#25552;&#20379;&#20102;&#21442;&#32771;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) such as ChatGPT have seen widespread adoption due to their ability to follow user instructions well. Developing these LLMs involves a complex yet poorly understood workflow requiring training with human feedback. Replicating and understanding this instruction-following process faces three major challenges: the high cost of data collection, the lack of trustworthy evaluation, and the absence of reference method implementations. We address these challenges with AlpacaFarm, a simulator that enables research and development for learning from feedback at a low cost. First, we design LLM prompts to simulate human feedback that are 45x cheaper than crowdworkers and display high agreement with humans. Second, we propose an automatic evaluation and validate it against human instructions obtained on real-world interactions. Third, we contribute reference implementations for several methods (PPO, best-of-n, expert iteration, and more) that learn from pairwise feedback
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#36830;&#32493;&#23398;&#20064;&#22330;&#26223;&#19979;&#30340;&#21475;&#35821;&#35821;&#35328;&#29702;&#35299;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22686;&#37327;&#31867;&#21035;&#22330;&#26223;&#21644;&#19977;&#31181;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#24182;&#26174;&#31034;&#24207;&#21015;&#32423;&#30693;&#35782;&#33976;&#39311;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#32489;&#25928;&#12290;</title><link>http://arxiv.org/abs/2305.13899</link><description>&lt;p&gt;
&#38024;&#23545;&#22686;&#37327;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#35821;&#38899;&#29702;&#35299;&#24207;&#21015;&#32423;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Sequence-Level Knowledge Distillation for Class-Incremental End-to-End Spoken Language Understanding. (arXiv:2305.13899v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13899
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#36830;&#32493;&#23398;&#20064;&#22330;&#26223;&#19979;&#30340;&#21475;&#35821;&#35821;&#35328;&#29702;&#35299;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22686;&#37327;&#31867;&#21035;&#22330;&#26223;&#21644;&#19977;&#31181;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#24182;&#26174;&#31034;&#24207;&#21015;&#32423;&#30693;&#35782;&#33976;&#39311;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#32489;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#22312;&#36880;&#27493;&#23398;&#20064;&#26032;&#27010;&#24565;&#26041;&#38754;&#30340;&#33021;&#21147;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#24369;&#28857;&#65292;&#36825;&#22952;&#30861;&#20102;&#23427;&#20204;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#30340;&#20351;&#29992;&#12290;&#23427;&#20204;&#20542;&#21521;&#20110;&#23558;&#24403;&#21069;&#25968;&#25454;&#20998;&#24067;&#25311;&#21512;&#24471;&#36234;&#26469;&#36234;&#22909;&#65292;&#32780;&#24573;&#30053;&#20102;&#36807;&#21435;&#25152;&#33719;&#21462;&#30340;&#30693;&#35782;&#65292;&#23548;&#33268;&#20102;&#28798;&#38590;&#24615;&#30340;&#36951;&#24536;&#38382;&#39064;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#24212;&#29992;&#20110;&#36830;&#32493;&#23398;&#20064;&#24773;&#22659;&#30340;&#21475;&#35821;&#35821;&#35328;&#29702;&#35299;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#20026;SLURP&#25968;&#25454;&#38598;&#23450;&#20041;&#20102;&#19968;&#20010;&#22686;&#37327;&#31867;&#21035;&#22330;&#26223;&#65292;&#24182;&#38024;&#23545;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;Transformer&#27169;&#22411;&#25552;&#20986;&#20102;&#19977;&#31181;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#26041;&#27861;&#20197;&#20943;&#36731;&#36951;&#24536;&#65306;&#31532;&#19968;&#31181;KD&#26041;&#27861;&#24212;&#29992;&#20110;&#32534;&#30721;&#22120;&#36755;&#20986;&#65288;audio-KD&#65289;&#65292;&#20854;&#20313;&#20004;&#31181;&#26041;&#27861;&#21017;&#20998;&#21035;&#22312;&#35299;&#30721;&#22120;&#36755;&#20986;&#30340;&#26631;&#35760;&#32423;&#65288;tok-KD&#65289;&#25110;&#24207;&#21015;&#32423;&#65288;seq-KD&#65289;&#20998;&#24067;&#19978;&#36827;&#34892;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;seq-KD&#26174;&#33879;&#22320;&#25913;&#21892;&#20102;&#25152;&#26377;&#32489;&#25928;&#25351;&#26631;&#65292;&#23558;&#23427;&#19982;audio-KD&#30456;&#32467;&#21512;&#36827;&#19968;&#27493;&#38477;&#20302;&#20102;&#24179;&#22343;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#24182;&#25552;&#39640;&#20102;&#23454;&#20307;&#39044;&#27979;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to learn new concepts sequentially is a major weakness for modern neural networks, which hinders their use in non-stationary environments. Their propensity to fit the current data distribution to the detriment of the past acquired knowledge leads to the catastrophic forgetting issue. In this work we tackle the problem of Spoken Language Understanding applied to a continual learning setting. We first define a class-incremental scenario for the SLURP dataset. Then, we propose three knowledge distillation (KD) approaches to mitigate forgetting for a sequence-to-sequence transformer model: the first KD method is applied to the encoder output (audio-KD), and the other two work on the decoder output, either directly on the token-level (tok-KD) or on the sequence-level (seq-KD) distributions. We show that the seq-KD substantially improves all the performance metrics, and its combination with the audio-KD further decreases the average WER and enhances the entity prediction metric.
&lt;/p&gt;</description></item><item><title>&#36830;&#32493;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#38754;&#20020;&#30528;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#35299;&#20915;&#26032;&#22686;&#23454;&#20307;&#21644;&#20851;&#31995;&#20197;&#21450;&#22810;&#27169;&#24577;&#28304;&#25968;&#25454;&#21464;&#21270;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.08698</link><description>&lt;p&gt;
&#36830;&#32493;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Continual Multimodal Knowledge Graph Construction. (arXiv:2305.08698v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08698
&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#38754;&#20020;&#30528;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#35299;&#20915;&#26032;&#22686;&#23454;&#20307;&#21644;&#20851;&#31995;&#20197;&#21450;&#22810;&#27169;&#24577;&#28304;&#25968;&#25454;&#21464;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#65288;MKGC&#65289;&#28041;&#21450;&#20351;&#29992;&#22810;&#31181;&#24418;&#24335;&#30340;&#25968;&#25454;&#65292;&#22914;&#25991;&#26412;&#21644;&#22270;&#20687;&#65292;&#21019;&#24314;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#32467;&#26500;&#21270;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MKGC&#27169;&#22411;&#22312;&#22788;&#29702;&#21160;&#24577;&#29616;&#23454;&#22330;&#26223;&#20013;&#26032;&#22686;&#23454;&#20307;&#21644;&#20851;&#31995;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#30446;&#21069;&#30340;&#36830;&#32493;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#35774;&#32622;&#20027;&#35201;&#20851;&#27880;&#20174;&#25991;&#26412;&#25968;&#25454;&#20013;&#25552;&#21462;&#23454;&#20307;&#21644;&#20851;&#31995;&#65292;&#24573;&#35270;&#20102;&#20854;&#20182;&#22810;&#27169;&#24577;&#28304;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#25506;&#32034;&#36830;&#32493;MKGC&#30340;&#25361;&#25112;&#65292;&#20197;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#65292;&#24182;&#30830;&#20445;&#20445;&#30041;&#20174;&#19981;&#21516;&#24418;&#24335;&#30340;&#25968;&#25454;&#20013;&#25552;&#21462;&#30340;&#36807;&#21435;&#30693;&#35782;&#12290;&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#36890;&#36807;&#24320;&#21457;&#32456;&#36523;MKGC&#22522;&#20934;&#25968;&#25454;&#38598;&#26469;&#30740;&#31350;&#36825;&#20010;&#22797;&#26434;&#30340;&#20027;&#39064;&#12290;&#22522;&#20110;&#32463;&#39564;&#21457;&#29616;&#65292;&#24403;&#22810;&#23186;&#20307;&#25968;&#25454;&#35757;&#32451;&#26102;&#65292;&#19968;&#20123;&#20856;&#22411;&#30340;MKGC&#27169;&#22411;&#21487;&#33021;&#22312;&#36830;&#32493;&#35774;&#32622;&#20013;&#24847;&#22806;&#34920;&#29616;&#19981;&#20339;&#65292;&#19982;&#37027;&#20123;&#20165;&#21033;&#29992;&#25991;&#26412;&#36164;&#28304;&#30340;&#27169;&#22411;&#30456;&#27604;&#12290;&#25105;&#20204;&#20197;&#23454;&#39564;&#35777;&#25454;&#20026;&#22522;&#30784;&#65292;&#24635;&#32467;&#20986;&#20197;&#19979;&#35770;&#28857;&#65306;&#36830;&#32493;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#38754;&#20020;&#30528;&#25968;&#25454;&#28304;&#21464;&#21270;&#23548;&#33268;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Knowledge Graph Construction (MKGC) involves creating structured representations of entities and relations using multiple modalities, such as text and images. However, existing MKGC models face challenges in handling the addition of new entities and relations in dynamic real-world scenarios. The current continual setting for knowledge graph construction mainly focuses on entity and relation extraction from text data, overlooking other multimodal sources. Therefore, there arises the need to explore the challenge of continual MKGC to address the phenomenon of catastrophic forgetting and ensure the retention of past knowledge extracted from different forms of data. This research focuses on investigating this complex topic by developing lifelong MKGC benchmark datasets. Based on the empirical findings that several typical MKGC models, when trained on multimedia data, might unexpectedly underperform compared to those solely utilizing textual resources in a continual setting, we p
&lt;/p&gt;</description></item><item><title>&#25688;&#35201;&#29983;&#25104;&#39046;&#22495;&#30446;&#21069;&#30340;&#30740;&#31350;&#20851;&#27880;&#28857;&#22312;&#20110;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#21644;&#22823;&#35268;&#27169;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#30340;&#36716;&#21464;&#65292;&#20197;&#21450;&#35780;&#20272;&#25688;&#35201;&#29983;&#25104;&#31995;&#32479;&#30340;&#25361;&#25112;&#21644;&#25351;&#20196;&#35843;&#25972;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#25688;&#35201;&#29983;&#25104;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.04853</link><description>&lt;p&gt;
&#25688;&#35201;&#29983;&#25104;&#30340;&#24403;&#21069;&#29366;&#24577;
&lt;/p&gt;
&lt;p&gt;
The Current State of Summarization. (arXiv:2305.04853v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04853
&lt;/p&gt;
&lt;p&gt;
&#25688;&#35201;&#29983;&#25104;&#39046;&#22495;&#30446;&#21069;&#30340;&#30740;&#31350;&#20851;&#27880;&#28857;&#22312;&#20110;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#21644;&#22823;&#35268;&#27169;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#30340;&#36716;&#21464;&#65292;&#20197;&#21450;&#35780;&#20272;&#25688;&#35201;&#29983;&#25104;&#31995;&#32479;&#30340;&#25361;&#25112;&#21644;&#25351;&#20196;&#35843;&#25972;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#25688;&#35201;&#29983;&#25104;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25991;&#26412;&#20449;&#24687;&#30340;&#29190;&#28856;&#24615;&#22686;&#38271;&#65292;&#25688;&#35201;&#29983;&#25104;&#31995;&#32479;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#26088;&#22312;&#31616;&#26126;&#25212;&#35201;&#22320;&#20171;&#32461;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#29983;&#25104;&#30340;&#24403;&#21069;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;&#20316;&#20026;&#20854;&#20013;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#21521;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#21644;&#22823;&#35268;&#27169;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#36716;&#21464;&#30340;&#29616;&#26377;&#33539;&#20363;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#35780;&#20272;&#25688;&#35201;&#29983;&#25104;&#31995;&#32479;&#30340;&#25361;&#25112;&#20197;&#21450;&#22522;&#20110;&#25351;&#20196;&#35843;&#25972;&#30340;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#25688;&#35201;&#29983;&#25104;&#20013;&#30340;&#28508;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#31616;&#35201;&#27010;&#36848;&#20102;&#30446;&#21069;&#23558;&#25688;&#35201;&#29983;&#25104;&#31995;&#32479;&#25972;&#21512;&#21040;&#21830;&#19994;&#24212;&#29992;&#20013;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the explosive growth of textual information, summarization systems have become increasingly important. This work aims to concisely indicate the current state of the art in abstractive text summarization. As part of this, we outline the current paradigm shifts towards pre-trained encoder-decoder models and large autoregressive language models. Additionally, we delve further into the challenges of evaluating summarization systems and the potential of instruction-tuned models for zero-shot summarization. Finally, we provide a brief overview of how summarization systems are currently being integrated into commercial applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#29702;&#35770;&#26041;&#27861;&#8212;&#8212;&#34892;&#20026;&#26399;&#26395;&#36793;&#30028;&#65288;BEB&#65289;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#40784;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#24182;&#35777;&#26126;&#20219;&#20309;&#23545;&#40784;&#36807;&#31243;&#37117;&#26080;&#27861;&#26681;&#38500;&#19981;&#24076;&#26395;&#30340;&#34892;&#20026;&#65292;&#36825;&#23545;&#20110;&#38450;&#27490;&#24694;&#24847;&#25915;&#20987;&#26159;&#19981;&#23433;&#20840;&#30340;&#12290;</title><link>http://arxiv.org/abs/2304.11082</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#30340;&#22522;&#26412;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Fundamental Limitations of Alignment in Large Language Models. (arXiv:2304.11082v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#29702;&#35770;&#26041;&#27861;&#8212;&#8212;&#34892;&#20026;&#26399;&#26395;&#36793;&#30028;&#65288;BEB&#65289;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#40784;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#24182;&#35777;&#26126;&#20219;&#20309;&#23545;&#40784;&#36807;&#31243;&#37117;&#26080;&#27861;&#26681;&#38500;&#19981;&#24076;&#26395;&#30340;&#34892;&#20026;&#65292;&#36825;&#23545;&#20110;&#38450;&#27490;&#24694;&#24847;&#25915;&#20987;&#26159;&#19981;&#23433;&#20840;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#19982;&#20154;&#20132;&#20114;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#26041;&#38754;&#26159;&#23545;&#40784;&#20854;&#34892;&#20026;&#65292;&#20351;&#20854;&#23545;&#20854;&#20154;&#31867;&#29992;&#25143;&#26377;&#29992;&#19988;&#26080;&#23475;&#12290;&#36825;&#36890;&#24120;&#36890;&#36807;&#35843;&#25972;&#27169;&#22411;&#30340;&#26041;&#24335;&#26469;&#23454;&#29616;&#65292;&#20197;&#22686;&#24378;&#25152;&#38656;&#30340;&#34892;&#20026;&#24182;&#25233;&#21046;&#19981;&#24076;&#26395;&#30340;&#34892;&#20026;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#34892;&#20026;&#26399;&#26395;&#36793;&#30028;(BEB)&#30340;&#29702;&#35770;&#26041;&#27861;&#65292;&#23427;&#20801;&#35768;&#25105;&#20204;&#27491;&#24335;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20960;&#20010;&#20869;&#22312;&#29305;&#24449;&#21644;&#23545;&#40784;&#30340;&#38480;&#21046;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#23545;&#20110;&#20219;&#20309;&#20855;&#26377;&#34987;&#35813;&#27169;&#22411;&#34920;&#29616;&#20986;&#30340;&#26377;&#38480;&#27010;&#29575;&#30340;&#34892;&#20026;&#65292;&#37117;&#23384;&#22312;&#21487;&#20197;&#35302;&#21457;&#27169;&#22411;&#36755;&#20986;&#27492;&#34892;&#20026;&#30340;&#25552;&#31034;&#65292;&#20854;&#27010;&#29575;&#38543;&#25552;&#31034;&#30340;&#38271;&#24230;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;&#36825;&#24847;&#21619;&#30528;&#20219;&#20309;&#20943;&#24369;&#19981;&#24076;&#26395;&#30340;&#34892;&#20026;&#20294;&#26410;&#23558;&#20854;&#23436;&#20840;&#28040;&#38500;&#30340;&#23545;&#40784;&#36807;&#31243;&#37117;&#26080;&#27861;&#25269;&#24481;&#38024;&#23545;&#24615;&#25915;&#20987;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#31034;&#20102;&#39046;&#20808;&#30340;
&lt;/p&gt;
&lt;p&gt;
An important aspect in developing language models that interact with humans is aligning their behavior to be useful and unharmful for their human users. This is usually achieved by tuning the model in a way that enhances desired behaviors and inhibits undesired ones, a process referred to as alignment. In this paper, we propose a theoretical approach called Behavior Expectation Bounds (BEB) which allows us to formally investigate several inherent characteristics and limitations of alignment in large language models. Importantly, we prove that for any behavior that has a finite probability of being exhibited by the model, there exist prompts that can trigger the model into outputting this behavior, with probability that increases with the length of the prompt. This implies that any alignment process that attenuates undesired behavior but does not remove it altogether, is not safe against adversarial prompting attacks. Furthermore, our framework hints at the mechanism by which leading al
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;mCPT&#27169;&#22411;&#29992;&#20110;&#22810;&#35821;&#35328;&#30340;&#12289;&#22810;&#26631;&#31614;&#30340;&#38646;&#26679;&#26412;&#25110;&#23569;&#26679;&#26412;&#30340;&#26694;&#26550;&#26816;&#27979;&#20219;&#21153;&#65292;&#24182;&#22312;&#35199;&#29677;&#29273;&#35821;&#21644;&#20854;&#20182;8&#31181;&#35821;&#35328;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25104;&#32489;&#12290;&#35813;&#26041;&#26696;&#37319;&#29992;&#20102;&#22522;&#20110;&#22810;&#35821;&#35328;&#21464;&#21387;&#22120;&#30340;&#39044;&#35757;&#32451;&#31243;&#24207;&#65292;&#20351;&#29992;&#26631;&#31614;&#24863;&#30693;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.09901</link><description>&lt;p&gt;
SemEval-2023&#20219;&#21153;3&#19978;&#30340;mCPT&#65306;&#29992;&#20110;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#26694;&#26550;&#26816;&#27979;&#30340;&#22810;&#35821;&#35328;&#26631;&#31614;&#24863;&#30693;&#23545;&#27604;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
mCPT at SemEval-2023 Task 3: Multilingual Label-Aware Contrastive Pre-Training of Transformers for Few- and Zero-shot Framing Detection. (arXiv:2303.09901v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;mCPT&#27169;&#22411;&#29992;&#20110;&#22810;&#35821;&#35328;&#30340;&#12289;&#22810;&#26631;&#31614;&#30340;&#38646;&#26679;&#26412;&#25110;&#23569;&#26679;&#26412;&#30340;&#26694;&#26550;&#26816;&#27979;&#20219;&#21153;&#65292;&#24182;&#22312;&#35199;&#29677;&#29273;&#35821;&#21644;&#20854;&#20182;8&#31181;&#35821;&#35328;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25104;&#32489;&#12290;&#35813;&#26041;&#26696;&#37319;&#29992;&#20102;&#22522;&#20110;&#22810;&#35821;&#35328;&#21464;&#21387;&#22120;&#30340;&#39044;&#35757;&#32451;&#31243;&#24207;&#65292;&#20351;&#29992;&#26631;&#31614;&#24863;&#30693;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#38646;&#26679;&#26412;&#30340;&#35199;&#29677;&#29273;&#35821;&#26694;&#26550;&#26816;&#27979;&#20219;&#21153;&#30340;&#33719;&#32988;&#31995;&#32479;&#65292;&#24182;&#22312;&#21478;&#22806;&#20843;&#31181;&#35821;&#35328;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25104;&#32489;&#12290;&#26694;&#26550;&#26816;&#27979;&#20219;&#21153;&#30340;&#25361;&#25112;&#22312;&#20110;&#22312;&#21482;&#26377;&#23569;&#37327;&#25110;&#38646;&#20010;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#35782;&#21035;&#19968;&#32452;14&#20010;&#26694;&#26550;&#65292;&#21363;&#22810;&#35821;&#35328;&#22810;&#26631;&#31614;&#30340;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#35774;&#32622;&#12290;&#25105;&#20204;&#24320;&#21457;&#30340;&#35299;&#20915;&#26041;&#26696;&#37319;&#29992;&#20102;&#22522;&#20110;&#22810;&#35821;&#35328;&#21464;&#21387;&#22120;&#30340;&#39044;&#35757;&#32451;&#31243;&#24207;&#65292;&#20351;&#29992;&#26631;&#31614;&#24863;&#30693;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#12290;&#38500;&#20102;&#25551;&#36848;&#31995;&#32479;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#23884;&#20837;&#31354;&#38388;&#20998;&#26512;&#21644;&#28040;&#34701;&#30740;&#31350;&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#31243;&#24207;&#22914;&#20309;&#25903;&#25345;&#26694;&#26550;&#26816;&#27979;&#20197;&#25512;&#36827;&#35745;&#31639;&#26694;&#26550;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the winning system for the zero-shot Spanish framing detection task, which also achieves competitive places in eight additional languages. The challenge of the framing detection task lies in identifying a set of 14 frames when only a few or zero samples are available, i.e., a multilingual multi-label few- or zero-shot setting. Our developed solution employs a pre-training procedure based on multilingual Transformers using a label-aware contrastive loss function. In addition to describing the system, we perform an embedding space analysis and ablation study to demonstrate how our pre-training procedure supports framing detection to advance computational framing analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#25968;&#23383;&#30165;&#36857;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#26816;&#27979;&#25233;&#37057;&#30151;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#30693;&#35782;&#24863;&#30693;&#30340;&#25233;&#37057;&#30151;&#26816;&#27979;&#26694;&#26550;&#65292;&#24182;&#25581;&#31034;&#20102;&#20851;&#38190;&#22240;&#32032;&#12290;&#36890;&#36807;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#30340;&#23454;&#35777;&#30740;&#31350;&#35777;&#26126;&#20102;&#20854;&#20934;&#30830;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.05389</link><description>&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#20351;&#29992;&#25968;&#23383;&#30165;&#36857;&#36827;&#34892;&#25233;&#37057;&#30151;&#26816;&#27979;&#65306;&#19968;&#31181;&#30693;&#35782;&#24863;&#30693;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Depression Detection Using Digital Traces on Social Media: A Knowledge-aware Deep Learning Approach. (arXiv:2303.05389v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#25968;&#23383;&#30165;&#36857;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#26816;&#27979;&#25233;&#37057;&#30151;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#30693;&#35782;&#24863;&#30693;&#30340;&#25233;&#37057;&#30151;&#26816;&#27979;&#26694;&#26550;&#65292;&#24182;&#25581;&#31034;&#20102;&#20851;&#38190;&#22240;&#32032;&#12290;&#36890;&#36807;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#30340;&#23454;&#35777;&#30740;&#31350;&#35777;&#26126;&#20102;&#20854;&#20934;&#30830;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25233;&#37057;&#30151;&#26159;&#19968;&#31181;&#20840;&#29699;&#24120;&#35265;&#30340;&#30142;&#30149;&#12290;&#23427;&#24456;&#38590;&#35786;&#26029;&#65292;&#24182;&#19988;&#25345;&#32493;&#34987;&#20302;&#20272;&#12290;&#30001;&#20110;&#25233;&#37057;&#30151;&#24739;&#32773;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#19981;&#26029;&#20998;&#20139;&#20182;&#20204;&#30340;&#30151;&#29366;&#12289;&#37325;&#22823;&#29983;&#27963;&#20107;&#20214;&#21644;&#27835;&#30103;&#26041;&#27861;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#19978;&#29992;&#25143;&#29983;&#25104;&#30340;&#25968;&#23383;&#30165;&#36857;&#36827;&#34892;&#25233;&#37057;&#30151;&#26816;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#25239;&#20987;&#25233;&#37057;&#30151;&#26041;&#38754;&#20855;&#26377;&#29420;&#29305;&#30340;&#20248;&#21183;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#20419;&#36827;&#21019;&#26032;&#30340;&#26041;&#27861;&#26469;&#23545;&#25239;&#25233;&#37057;&#30151;&#24182;&#20943;&#36731;&#20854;&#31038;&#20250;&#21644;&#32463;&#27982;&#36127;&#25285;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#32570;&#20047;&#26377;&#25928;&#30340;&#25163;&#27573;&#23558;&#24050;&#24314;&#31435;&#30340;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#32435;&#20837;&#25233;&#37057;&#30151;&#26816;&#27979;&#20013;&#65292;&#25110;&#32773;&#38754;&#20020;&#29305;&#24449;&#25552;&#21462;&#22256;&#38590;&#32780;&#24433;&#21709;&#24615;&#33021;&#12290;&#22312;&#35774;&#35745;&#31185;&#23398;&#30740;&#31350;&#33539;&#24335;&#30340;&#25351;&#23548;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#30693;&#35782;&#24863;&#30693;&#30340;&#25233;&#37057;&#30151;&#26816;&#27979; (DKDD) &#26694;&#26550;&#65292;&#20197;&#20934;&#30830;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#30340;&#25233;&#37057;&#39118;&#38505;&#65292;&#24182;&#35299;&#37322;&#23545;&#36825;&#31181;&#26816;&#27979;&#36215;&#20851;&#38190;&#20316;&#29992;&#30340;&#22240;&#32032;&#12290;&#36890;&#36807;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Depression is a common disease worldwide. It is difficult to diagnose and continues to be underdiagnosed. Because depressed patients constantly share their symptoms, major life events, and treatments on social media, researchers are turning to user-generated digital traces on social media for depression detection. Such methods have distinct advantages in combating depression because they can facilitate innovative approaches to fight depression and alleviate its social and economic burden. However, most existing studies lack effective means to incorporate established medical domain knowledge in depression detection or suffer from feature extraction difficulties that impede greater performance. Following the design science research paradigm, we propose a Deep Knowledge-aware Depression Detection (DKDD) framework to accurately detect social media users at risk of depression and explain the critical factors that contribute to such detection. Extensive empirical studies with real-world data
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;HL&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#25193;&#23637;&#20102;COCO&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;14997&#20010;&#22270;&#20687;&#21644;134,973&#20010;&#20154;&#24037;&#27880;&#37322;&#30340;&#39640;&#32423;&#21035;&#25551;&#36848;&#65292;&#28041;&#21450;&#22330;&#26223;&#12289;&#21160;&#20316;&#21644;&#29702;&#30001;&#65292;&#21487;&#20197;&#29992;&#20110;&#23545;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26356;&#39640;&#32423;&#21035;&#30340;&#27979;&#35797;&#21644;&#24494;&#35843;&#12290;</title><link>http://arxiv.org/abs/2302.12189</link><description>&lt;p&gt;
HL&#25968;&#25454;&#38598;: &#22522;&#20110;&#35270;&#35273;&#30340;&#22330;&#26223;&#12289;&#21160;&#20316;&#21644;&#29702;&#30001;&#30340;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;
HL Dataset: Visually-grounded Description of Scenes, Actions and Rationales. (arXiv:2302.12189v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12189
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;HL&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#25193;&#23637;&#20102;COCO&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;14997&#20010;&#22270;&#20687;&#21644;134,973&#20010;&#20154;&#24037;&#27880;&#37322;&#30340;&#39640;&#32423;&#21035;&#25551;&#36848;&#65292;&#28041;&#21450;&#22330;&#26223;&#12289;&#21160;&#20316;&#21644;&#29702;&#30001;&#65292;&#21487;&#20197;&#29992;&#20110;&#23545;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26356;&#39640;&#32423;&#21035;&#30340;&#27979;&#35797;&#21644;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#25551;&#36848;&#25968;&#25454;&#38598;&#20391;&#37325;&#20110;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#25551;&#36848;&#65292;&#25551;&#36848;&#22270;&#20687;&#20013;&#21487;&#35265;&#30340;&#29289;&#20307;&#65292;&#20363;&#22914;&#8220;&#20154;&#20204;&#22312;&#20844;&#22253;&#37324;&#21507;&#19996;&#35199;&#8221;&#12290;&#34429;&#28982;&#36825;&#20123;&#25968;&#25454;&#38598;&#23545;&#20110;&#35780;&#20272;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#35782;&#21035;&#21644;&#25551;&#36848;&#35270;&#35273;&#20869;&#23481;&#30340;&#33021;&#21147;&#24456;&#26377;&#29992;&#65292;&#20294;&#23427;&#20204;&#19981;&#25903;&#25345;&#28041;&#21450;&#27169;&#22411;&#27979;&#35797;&#25110;&#24494;&#35843;&#30340;&#21463;&#25511;&#23454;&#39564;&#65292;&#20351;&#29992;&#26356;&#39640;&#32423;&#30340;&#25551;&#36848;&#65292;&#20154;&#20204;&#21457;&#29616;&#24456;&#23481;&#26131;&#21644;&#33258;&#28982;&#22320;&#20135;&#29983;&#12290;&#20363;&#22914;&#65292;&#20154;&#20204;&#36890;&#24120;&#26681;&#25454;&#22270;&#20687;&#25152;&#25551;&#32472;&#30340;&#22330;&#26223;&#31867;&#22411;&#65288;&#8220;&#20154;&#20204;&#22312;&#24230;&#20551;&#32988;&#22320;&#8221;&#65289;&#21644;&#20182;&#20204;&#36827;&#34892;&#30340;&#21160;&#20316;&#65288;&#8220;&#20154;&#20204;&#27491;&#22312;&#37326;&#39184;&#8221;&#65289;&#26469;&#25551;&#36848;&#22270;&#20687;&#12290;&#36825;&#20123;&#25551;&#36848;&#22522;&#20110;&#20010;&#20154;&#32463;&#39564;&#21644;&#24120;&#35782;&#24615;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#39640;&#32423;&#21035;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#25193;&#23637;&#20102;&#26469;&#33258;COCO&#25968;&#25454;&#38598;&#30340;14997&#20010;&#22270;&#20687;&#65292;&#24182;&#19982;&#19968;&#32452;&#26032;&#30340;134,973&#20010;&#20154;&#24037;&#27880;&#37322;&#65288;&#39640;&#32423;&#21035;&#65289;&#25551;&#36848;&#23545;&#40784;&#65292;&#36825;&#20123;&#25551;&#36848;&#20174;&#22330;&#26223;&#12289;&#21160;&#20316;&#21644;&#29702;&#30001;&#19977;&#20010;&#26041;&#38754;&#36827;&#34892;&#20102;&#25910;&#38598;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20351;&#29992;&#20174;&#29420;&#31435;&#38405;&#35835;&#32773;&#32452;&#25910;&#38598;&#30340;&#20449;&#24515;&#24471;&#20998;&#25193;&#23637;&#20102;&#35813;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current captioning datasets focus on object-centric captions, describing the visible objects in the image, e.g. "people eating food in a park". Although these datasets are useful to evaluate the ability of Vision &amp; Language models to recognize and describe visual content, they do not support controlled experiments involving model testing or fine-tuning, with more high-level captions, which humans find easy and natural to produce. For example, people often describe images based on the type of scene they depict ('people at a holiday resort') and the actions they perform ('people having a picnic'). Such descriptions draw on personal experience and commonsense assumptions. We present the High-Level Dataset a dataset extending 14997 images from the COCO dataset, aligned with a new set of 134,973 human-annotated (high-level) captions collected along three axes: scenes, actions, and rationales. We further extend this dataset with confidence scores collected from an independent set of readers,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;In-Context RALM&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#30456;&#20851;&#25991;&#20214;&#20316;&#20026;&#36755;&#20837;&#30340;&#19968;&#37096;&#20998;&#65292;&#26080;&#38656;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#35757;&#32451;&#21363;&#21487;&#26174;&#33879;&#25552;&#39640;&#35821;&#35328;&#24314;&#27169;&#24615;&#33021;&#21644;&#28304;&#24402;&#22240;&#33021;&#21147;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;RALM&#26041;&#27861;&#65292;&#23427;&#20855;&#26377;&#26356;&#31616;&#21333;&#30340;&#37096;&#32626;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2302.00083</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
In-Context Retrieval-Augmented Language Models. (arXiv:2302.00083v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00083
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;In-Context RALM&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#30456;&#20851;&#25991;&#20214;&#20316;&#20026;&#36755;&#20837;&#30340;&#19968;&#37096;&#20998;&#65292;&#26080;&#38656;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#35757;&#32451;&#21363;&#21487;&#26174;&#33879;&#25552;&#39640;&#35821;&#35328;&#24314;&#27169;&#24615;&#33021;&#21644;&#28304;&#24402;&#22240;&#33021;&#21147;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;RALM&#26041;&#27861;&#65292;&#23427;&#20855;&#26377;&#26356;&#31616;&#21333;&#30340;&#37096;&#32626;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;(RALM)&#26041;&#27861;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#23558;&#30456;&#20851;&#25991;&#20214;&#20174;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#20986;&#26469;&#19982;&#35821;&#35328;&#27169;&#22411;(LM)&#36827;&#34892;&#21327;&#21516;&#65292;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#35821;&#35328;&#24314;&#27169;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#36824;&#21487;&#20197;&#32531;&#35299;&#20107;&#23454;&#19981;&#20934;&#30830;&#30340;&#25991;&#26412;&#29983;&#25104;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#33258;&#28982;&#30340;&#28304;&#24402;&#22240;&#26426;&#21046;&#12290;&#29616;&#26377;&#30340;RALM&#26041;&#27861;&#30528;&#37325;&#20110;&#20462;&#25913;LM&#26550;&#26500;&#20197;&#20415;&#20110;&#25972;&#21512;&#22806;&#37096;&#20449;&#24687;&#65292;&#20174;&#32780;&#22823;&#22823;&#22686;&#21152;&#20102;&#37096;&#32626;&#30340;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#31216;&#20026;&#19978;&#19979;&#25991;RALM&#65306;&#20445;&#25345;LM&#26550;&#26500;&#19981;&#21464;&#65292;&#24182;&#22312;&#36755;&#20837;&#20013;&#28155;&#21152;&#26816;&#32034;&#21040;&#30340;&#25991;&#20214;&#65292;&#26080;&#38656;&#23545;LM&#36827;&#34892;&#20219;&#20309;&#36827;&#19968;&#27493;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;&#29616;&#25104;&#30340;&#36890;&#29992;&#26816;&#32034;&#22120;&#30340;&#19978;&#19979;&#25991;RALM&#22312;&#27169;&#22411;&#22823;&#23567;&#21644;&#19981;&#21516;&#35821;&#26009;&#24211;&#20013;&#33021;&#22815;&#25552;&#20379;&#20986;&#20154;&#24847;&#26009;&#30340;&#22823;&#24133;&#24230;&#30340;LM&#22686;&#30410;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#65292;&#25991;&#20214;&#26816;&#32034;&#21644;&#25490;&#21517;&#26426;&#21046;&#21487;&#20197;&#38024;&#23545;RALM&#35774;&#32622;&#36827;&#34892;&#19987;&#38376;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Augmented Language Modeling (RALM) methods, which condition a language model (LM) on relevant documents from a grounding corpus during generation, were shown to significantly improve language modeling performance. In addition, they can mitigate the problem of factually inaccurate text generation and provide natural source attribution mechanism. Existing RALM approaches focus on modifying the LM architecture in order to facilitate the incorporation of external information, significantly complicating deployment. This paper considers a simple alternative, which we dub In-Context RALM: leaving the LM architecture unchanged and prepending grounding documents to the input, without any further training of the LM. We show that In-Context RALM that builds on off-the-shelf general purpose retrievers provides surprisingly large LM gains across model sizes and diverse corpora. We also demonstrate that the document retrieval and ranking mechanism can be specialized to the RALM setting to 
&lt;/p&gt;</description></item><item><title>PCW&#26041;&#27861;&#21487;&#20197;&#32531;&#35299;&#29616;&#25104;LLM&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#38480;&#21046;&#65292;&#23558;&#38271;&#19978;&#19979;&#25991;&#21010;&#20998;&#20026;&#22359;&#24182;&#22312;&#27599;&#20010;&#31383;&#21475;&#20869;&#37325;&#29992;&#20301;&#32622;&#23884;&#20837;&#65292;&#25552;&#39640;&#20102;&#22788;&#29702;&#38271;&#25991;&#26412;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2212.10947</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24182;&#34892;&#19978;&#19979;&#25991;&#31383;&#21475;
&lt;/p&gt;
&lt;p&gt;
Parallel Context Windows for Large Language Models. (arXiv:2212.10947v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10947
&lt;/p&gt;
&lt;p&gt;
PCW&#26041;&#27861;&#21487;&#20197;&#32531;&#35299;&#29616;&#25104;LLM&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#38480;&#21046;&#65292;&#23558;&#38271;&#19978;&#19979;&#25991;&#21010;&#20998;&#20026;&#22359;&#24182;&#22312;&#27599;&#20010;&#31383;&#21475;&#20869;&#37325;&#29992;&#20301;&#32622;&#23884;&#20837;&#65292;&#25552;&#39640;&#20102;&#22788;&#29702;&#38271;&#25991;&#26412;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#38271;&#25991;&#26412;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21463;&#21040;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#38480;&#21046;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#21253;&#25324;&#35757;&#32451;&#19987;&#38376;&#30340;&#26550;&#26500;&#65292;&#20294;&#19981;&#33021;&#36731;&#26494;&#22320;&#24212;&#29992;&#20110;&#29616;&#25104;&#30340;LLM&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24182;&#34892;&#19978;&#19979;&#25991;&#31383;&#21475;&#65288;PCW&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#32531;&#35299;&#20219;&#20309;&#29616;&#25104;LLM&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#38480;&#21046;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#36827;&#19968;&#27493;&#35757;&#32451;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#20851;&#38190;&#22312;&#20110;&#23558;&#38271;&#19978;&#19979;&#25991;&#21010;&#20998;&#20026;&#22359;&#65288;&#8220;&#31383;&#21475;&#8221;&#65289;&#65292;&#38480;&#21046;&#27880;&#24847;&#26426;&#21046;&#20165;&#22312;&#27599;&#20010;&#31383;&#21475;&#20869;&#24212;&#29992;&#65292;&#24182;&#36328;&#31383;&#21475;&#37325;&#29992;&#20301;&#32622;&#23884;&#20837;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#22312;750&#19975;&#21040;1780&#20159;&#20010;&#21442;&#25968;&#33539;&#22260;&#20869;&#30340;&#27169;&#22411;&#19978;&#27979;&#35797;PCW&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20854;&#22312;&#36755;&#20837;&#21644;&#36755;&#20986;&#31354;&#38388;&#19981;&#21516;&#30340;&#20219;&#21153;&#20013;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#22312;&#20854;&#20182;&#38656;&#35201;&#38271;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#22810;&#36339;&#38382;&#39064;&#21644;&#20351;&#29992;&#22810;&#20010;&#26816;&#32034;&#30340;&#26816;&#32034;&#22686;&#24378;&#22411;&#38382;&#31572;&#20013;&#23637;&#31034;&#20102;&#39069;&#22806;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
When applied for processing long text, Large Language Models (LLMs) are limited by their context window. Existing efforts to address this limitation involve training specialized architectures, and cannot be easily applied to off-the-shelf LLMs. We present Parallel Context Windows (PCW), a method that alleviates the context window restriction for any off-the-shelf LLM without further training. The key to the approach is to carve a long context into chunks (``windows''), restrict the attention mechanism to apply only within each window, and re-use the positional embeddings across the windows. Our main results test the PCW approach on in-context learning with models that range in size between 750 million and 178 billion parameters, and show substantial improvements for tasks with diverse input and output spaces. We show additional benefits in other settings where long context windows may be beneficial: multi-hop questions and retrieval-augmented question answering with multiple retrieved 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23450;&#20041;&#20102;&#25968;&#25454;&#23637;&#24320;&#24230;&#30340;&#27010;&#24565;&#65292;&#24182;&#21457;&#29616;&#24120;&#29992;&#30340;&#24230;&#37327;&#26041;&#27861;&#19981;&#21487;&#38752;&#65292;&#25552;&#20986;&#20102;&#20843;&#31181;&#22791;&#36873;&#30340;&#25968;&#25454;&#23637;&#24320;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#25512;&#33616;&#20351;&#29992;&#19968;&#31181;&#22522;&#20110;&#20027;&#25104;&#20998;&#21644;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#21487;&#38752;&#22320;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#30340;&#23637;&#24320;&#24230;&#12290;</title><link>http://arxiv.org/abs/2212.08172</link><description>&lt;p&gt;
&#39640;&#32500;&#28508;&#31354;&#38388;&#20013;&#21487;&#38752;&#30340;&#23637;&#24320;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Reliable Measures of Spread in High Dimensional Latent Spaces. (arXiv:2212.08172v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23450;&#20041;&#20102;&#25968;&#25454;&#23637;&#24320;&#24230;&#30340;&#27010;&#24565;&#65292;&#24182;&#21457;&#29616;&#24120;&#29992;&#30340;&#24230;&#37327;&#26041;&#27861;&#19981;&#21487;&#38752;&#65292;&#25552;&#20986;&#20102;&#20843;&#31181;&#22791;&#36873;&#30340;&#25968;&#25454;&#23637;&#24320;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#25512;&#33616;&#20351;&#29992;&#19968;&#31181;&#22522;&#20110;&#20027;&#25104;&#20998;&#21644;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#21487;&#38752;&#22320;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#30340;&#23637;&#24320;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#28508;&#31354;&#38388;&#30340;&#20960;&#20309;&#29305;&#24615;&#21487;&#20197;&#36890;&#36807;&#25805;&#20316;&#36825;&#20123;&#29305;&#24615;&#26469;&#25913;&#21892;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#20854;&#20013;&#19968;&#20010;&#29305;&#24615;&#26159;&#27169;&#22411;&#28508;&#31354;&#38388;&#20013;&#30340;&#25968;&#25454;&#23637;&#24320;&#24230;&#65292;&#21363;&#21487;&#29992;&#28508;&#31354;&#38388;&#30340;&#20805;&#20998;&#21033;&#29992;&#31243;&#24230;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#25968;&#25454;&#23637;&#24320;&#24230;&#65292;&#24182;&#35777;&#26126;&#20102;&#24120;&#29992;&#30340;&#25968;&#25454;&#23637;&#24320;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#24179;&#22343;&#20313;&#24358;&#30456;&#20284;&#24230;&#21644;&#20998;&#21306;&#20989;&#25968;&#30340;&#26368;&#23567;/&#26368;&#22823;&#27604;&#20363;I&#65288;V&#65289;&#65292;&#19981;&#33021;&#25552;&#20379;&#21487;&#38752;&#30340;&#23545;&#27604;&#19981;&#21516;&#27169;&#22411;&#28508;&#31354;&#38388;&#20351;&#29992;&#24773;&#20917;&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#20843;&#31181;&#22791;&#36873;&#30340;&#25968;&#25454;&#23637;&#24320;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#20854;&#20013;&#38500;&#19968;&#31181;&#22806;&#65292;&#25152;&#26377;&#26041;&#27861;&#22312;&#24212;&#29992;&#20110;&#19971;&#31181;&#21512;&#25104;&#25968;&#25454;&#20998;&#24067;&#26102;&#37117;&#20248;&#20110;&#24403;&#21069;&#30340;&#24230;&#37327;&#26041;&#27861;&#12290;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#24230;&#37327;&#26041;&#27861;&#20013;&#65292;&#25512;&#33616;&#20351;&#29992;&#19968;&#31181;&#22522;&#20110;&#20027;&#25104;&#20998;&#30340;&#24230;&#37327;&#26041;&#27861;&#21644;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#23427;&#20204;&#21487;&#20197;&#25552;&#20379;&#21487;&#38752;&#30340;&#12289;&#30456;&#23545;&#30340;&#23637;&#24320;&#24230;&#37327;&#65292;&#24182;&#21487;&#29992;&#20110;&#27604;&#36739;&#19981;&#21516;&#22823;&#23567;&#21644;&#32500;&#24230;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding geometric properties of natural language processing models' latent spaces allows the manipulation of these properties for improved performance on downstream tasks. One such property is the amount of data spread in a model's latent space, or how fully the available latent space is being used. In this work, we define data spread and demonstrate that the commonly used measures of data spread, Average Cosine Similarity and a partition function min/max ratio I(V), do not provide reliable metrics to compare the use of latent space across models. We propose and examine eight alternative measures of data spread, all but one of which improve over these current metrics when applied to seven synthetic data distributions. Of our proposed measures, we recommend one principal component-based measure and one entropy-based measure that provide reliable, relative measures of spread and can be used to compare models of different sizes and dimensionalities.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#27169;&#24577;&#27010;&#29575;&#34701;&#21512;&#25552;&#31034;&#65288;MultiPoint&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#27169;&#24577;&#30340;&#19981;&#21516;&#32447;&#32034;&#36827;&#34892;&#23569;&#26679;&#26412;&#24773;&#24863;&#20998;&#26512;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#30740;&#31350;&#20013;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#30417;&#30563;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#25552;&#31034;&#26469;&#20943;&#23569;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2211.06607</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#27169;&#24577;&#27010;&#29575;&#34701;&#21512;&#25552;&#31034;&#30340;&#23569;&#26679;&#26412;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Few-shot Multimodal Sentiment Analysis based on Multimodal Probabilistic Fusion Prompts. (arXiv:2211.06607v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06607
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#27169;&#24577;&#27010;&#29575;&#34701;&#21512;&#25552;&#31034;&#65288;MultiPoint&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#27169;&#24577;&#30340;&#19981;&#21516;&#32447;&#32034;&#36827;&#34892;&#23569;&#26679;&#26412;&#24773;&#24863;&#20998;&#26512;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#30740;&#31350;&#20013;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#30417;&#30563;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#25552;&#31034;&#26469;&#20943;&#23569;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#19978;&#22810;&#27169;&#24577;&#20869;&#23481;&#30340;&#26222;&#21450;&#65292;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#37325;&#35270;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#22312;&#36825;&#19968;&#39046;&#22495;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#30417;&#30563;&#25968;&#25454;&#65292;&#32780;&#36825;&#31181;&#25968;&#25454;&#30340;&#37319;&#38598;&#38750;&#24120;&#32791;&#26102;&#21644;&#21171;&#21160;&#23494;&#38598;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#35299;&#20915;&#23569;&#26679;&#26412;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;&#22810;&#27169;&#24577;&#27010;&#29575;&#34701;&#21512;&#25552;&#31034;&#65288;MultiPoint&#65289;&#65292;&#23427;&#21033;&#29992;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#22810;&#26679;&#32447;&#32034;&#36827;&#34892;&#23569;&#26679;&#26412;&#24773;&#24863;&#26816;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#31181;&#19968;&#33268;&#20998;&#24067;&#37319;&#26679;&#26041;&#27861;&#65288;CDS&#65289;&#65292;&#20197;&#30830;&#20445;&#23569;&#26679;&#26412;&#25968;&#25454;&#38598;&#20855;&#26377;&#19982;&#25972;&#20010;&#25968;&#25454;&#38598;&#30456;&#21516;&#30340;&#31867;&#21035;&#20998;&#24067;&#12290;&#19982;&#20043;&#21069;&#20027;&#35201;&#20351;&#29992;&#22522;&#20110;&#25991;&#26412;&#27169;&#24577;&#30340;&#25552;&#31034;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#25552;&#31034;&#65292;&#20197;&#20943;&#23569;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#24046;&#24322;&#24182;&#21160;&#24577;&#22320;&#23558;&#22810;&#27169;&#24577;&#28436;&#31034;&#32435;&#20837;&#20854;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal sentiment analysis has gained significant attention due to the proliferation of multimodal content on social media. However, existing studies in this area rely heavily on large-scale supervised data, which is time-consuming and labor-intensive to collect. Thus, there is a need to address the challenge of few-shot multimodal sentiment analysis. To tackle this problem, we propose a novel method called Multimodal Probabilistic Fusion Prompts (MultiPoint) that leverages diverse cues from different modalities for multimodal sentiment detection in the few-shot scenario. Specifically, we start by introducing a Consistently Distributed Sampling approach called CDS, which ensures that the few-shot dataset has the same category distribution as the full dataset. Unlike previous approaches primarily using prompts based on the text modality, we design unified multimodal prompts to reduce discrepancies between different modalities and dynamically incorporate multimodal demonstrations into
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#24418;&#24335;&#35821;&#35328;&#30340;&#26041;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#21457;&#29616;&#22823;&#22411;NLP&#31070;&#32463;&#27169;&#22411;&#35201;&#20040;&#23436;&#20840;&#23398;&#20064;&#20851;&#31995;&#65292;&#35201;&#20040;&#23436;&#20840;&#19981;&#23398;&#20064;&#12290;&#36716;&#25442;&#35206;&#30422;&#29575;&#26159;&#20851;&#38190;&#65292;&#23558;&#36719;&#21487;&#23398;&#20064;&#30028;&#38480;&#35774;&#32622;&#20026;&#27599;&#20010;&#36716;&#25442;400&#20010;&#31034;&#20363;&#12290;</title><link>http://arxiv.org/abs/2208.08195</link><description>&lt;p&gt;
&#20351;&#29992;&#24418;&#24335;&#35821;&#35328;&#23545;&#32452;&#21512;&#24615;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Compositionality with Formal Languages. (arXiv:2208.08195v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.08195
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#24418;&#24335;&#35821;&#35328;&#30340;&#26041;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#21457;&#29616;&#22823;&#22411;NLP&#31070;&#32463;&#27169;&#22411;&#35201;&#20040;&#23436;&#20840;&#23398;&#20064;&#20851;&#31995;&#65292;&#35201;&#20040;&#23436;&#20840;&#19981;&#23398;&#20064;&#12290;&#36716;&#25442;&#35206;&#30422;&#29575;&#26159;&#20851;&#38190;&#65292;&#23558;&#36719;&#21487;&#23398;&#20064;&#30028;&#38480;&#35774;&#32622;&#20026;&#27599;&#20010;&#36716;&#25442;400&#20010;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#24050;&#30693;&#30340;&#21407;&#22987;&#27010;&#24565;&#37325;&#26032;&#32452;&#21512;&#25104;&#26356;&#22823;&#30340;&#26032;&#32452;&#21512;&#26159;&#19968;&#31181;&#20856;&#22411;&#30340;&#20154;&#31867;&#35748;&#30693;&#33021;&#21147;&#12290;&#22312;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#36807;&#31243;&#20013;&#65292;&#22823;&#22411;NLP&#31070;&#32463;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#33719;&#24471;&#36825;&#31181;&#33021;&#21147;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#20174;&#24418;&#24335;&#35821;&#35328;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#30830;&#23450;&#24615;&#26377;&#38480;&#29366;&#24577;&#36716;&#25442;&#22120;&#29983;&#25104;&#20855;&#26377;&#21487;&#25511;&#32452;&#21512;&#24615;&#23646;&#24615;&#30340;&#26080;&#30028;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#23545;&#35768;&#22810;&#36716;&#25442;&#22120;&#36827;&#34892;&#38543;&#26426;&#25277;&#26679;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#23427;&#20204;&#30340;&#21738;&#20123;&#23646;&#24615;&#33021;&#22815;&#23545;&#31070;&#32463;&#32593;&#32476;&#23545;&#32452;&#21512;&#24615;&#20851;&#31995;&#30340;&#21487;&#23398;&#20064;&#24615;&#20570;&#20986;&#36129;&#29486;&#12290;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#35201;&#20040;&#23436;&#20840;&#23398;&#20064;&#20851;&#31995;&#65292;&#35201;&#20040;&#23436;&#20840;&#19981;&#23398;&#20064;&#12290;&#20851;&#38190;&#22312;&#20110;&#36716;&#25442;&#35206;&#30422;&#29575;&#65292;&#23558;&#36719;&#21487;&#23398;&#20064;&#30028;&#38480;&#35774;&#32622;&#20026;&#27599;&#20010;&#36716;&#25442;400&#20010;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recombining known primitive concepts into larger novel combinations is a quintessentially human cognitive capability. Whether large neural models in NLP can acquire this ability while learning from data is an open question. In this paper, we investigate this problem from the perspective of formal languages. We use deterministic finite-state transducers to make an unbounded number of datasets with controllable properties governing compositionality. By randomly sampling over many transducers, we explore which of their properties contribute to learnability of a compositional relation by a neural network. We find that the models either learn the relations completely or not at all. The key is transition coverage, setting a soft learnability limit at 400 examples per transition.
&lt;/p&gt;</description></item></channel></rss>