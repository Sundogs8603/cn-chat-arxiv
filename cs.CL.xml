<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#28304;Python&#24211;Frame Semantic Transformer&#65292;&#35813;&#24211;&#29992;&#20110;&#26694;&#26550;&#35821;&#20041;&#35299;&#26512;&#65292;&#24182;&#22312;&#26131;&#29992;&#24615;&#21644;&#24615;&#33021;&#26041;&#38754;&#23454;&#29616;&#25509;&#36817;&#26368;&#26032;&#27700;&#24179;&#65292;&#21487;&#36890;&#36807;&#22312;&#25512;&#29702;&#26102;&#20351;&#29992;FrameNet&#35789;&#27719;&#21333;&#20803;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#22312;&#35757;&#32451;&#26399;&#38388;&#20351;&#29992;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#26469;&#25552;&#39640;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12788</link><description>&lt;p&gt;
&#24320;&#28304;&#26694;&#26550;&#35821;&#20041;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
Open-source Frame Semantic Parsing. (arXiv:2303.12788v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#28304;Python&#24211;Frame Semantic Transformer&#65292;&#35813;&#24211;&#29992;&#20110;&#26694;&#26550;&#35821;&#20041;&#35299;&#26512;&#65292;&#24182;&#22312;&#26131;&#29992;&#24615;&#21644;&#24615;&#33021;&#26041;&#38754;&#23454;&#29616;&#25509;&#36817;&#26368;&#26032;&#27700;&#24179;&#65292;&#21487;&#36890;&#36807;&#22312;&#25512;&#29702;&#26102;&#20351;&#29992;FrameNet&#35789;&#27719;&#21333;&#20803;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#22312;&#35757;&#32451;&#26399;&#38388;&#20351;&#29992;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#26469;&#25552;&#39640;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26368;&#36817;&#20960;&#24180;&#26469;&#26694;&#26550;&#35821;&#20041;&#35299;&#26512;&#30340;&#26368;&#26032;&#25216;&#26415;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#32456;&#31471;&#29992;&#25143;&#26469;&#35828;&#65292;&#23558;&#26368;&#26032;&#27169;&#22411;&#24212;&#29992;&#20110;&#23454;&#36341;&#20173;&#28982;&#24456;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Frame Semantic Transformer&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;Python&#24211;&#65292;&#21487;&#20197;&#22312;&#20851;&#27880;&#26131;&#29992;&#24615;&#30340;&#21516;&#26102;&#65292;&#22312;FrameNet 1.7&#19978;&#23454;&#29616;&#25509;&#36817;&#26368;&#26032;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#22312;Propbank&#21644;FrameNet&#31034;&#20363;&#19978;&#24494;&#35843;&#30340;T5&#27169;&#22411;&#20316;&#20026;&#22522;&#30784;&#65292;&#24182;&#36890;&#36807;&#22312;&#25512;&#29702;&#26102;&#20351;&#29992;FrameNet&#35789;&#27719;&#21333;&#20803;&#20026;T5&#25552;&#20379;&#25552;&#31034;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#35757;&#32451;&#26399;&#38388;&#20351;&#29992;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#26469;&#25552;&#39640;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the state-of-the-art for frame semantic parsing has progressed dramatically in recent years, it is still difficult for end-users to apply state-of-the-art models in practice. To address this, we present Frame Semantic Transformer, an open-source Python library which achieves near state-of-the-art performance on FrameNet 1.7, while focusing on ease-of-use. We use a T5 model fine-tuned on Propbank and FrameNet exemplars as a base, and improve performance by using FrameNet lexical units to provide hints to T5 at inference time. We enhance robustness to real-world data by using textual data augmentations during training.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BERT&#30340;&#20919;&#22066;&#28909;&#35773;&#26816;&#27979;&#31995;&#32479;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26816;&#27979;&#23391;&#21152;&#25289;&#35821;&#20013;&#30340;&#20919;&#22066;&#28909;&#35773;&#65292;&#23545;&#31038;&#20132;&#23186;&#20307;&#20998;&#26512;&#31561;&#20219;&#21153;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2303.12772</link><description>&lt;p&gt;
&#20351;&#29992;BERT&#21644;&#21487;&#35299;&#37322;AI&#30340;&#21487;&#35299;&#37322;&#23391;&#21152;&#25289;&#20919;&#22066;&#28909;&#35773;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Interpretable Bangla Sarcasm Detection using BERT and Explainable AI. (arXiv:2303.12772v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12772
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BERT&#30340;&#20919;&#22066;&#28909;&#35773;&#26816;&#27979;&#31995;&#32479;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26816;&#27979;&#23391;&#21152;&#25289;&#35821;&#20013;&#30340;&#20919;&#22066;&#28909;&#35773;&#65292;&#23545;&#31038;&#20132;&#23186;&#20307;&#20998;&#26512;&#31561;&#20219;&#21153;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#38754;&#30340;&#35805;&#25110;&#20276;&#38543;&#36127;&#21521;&#21160;&#26426;&#30340;&#35821;&#21477;&#36890;&#24120;&#34987;&#23450;&#20041;&#20026;&#20919;&#22066;&#28909;&#35773;&#65292;&#32780;&#22312;&#24403;&#20170;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#22914;Facebook&#12289;Twitter&#12289;Reddit&#31561;&#19978;&#24191;&#27867;&#20351;&#29992;&#12290;&#26368;&#36817;&#65292;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#30340;&#27963;&#36291;&#29992;&#25143;&#25968;&#37327;&#21576;&#29616;&#29190;&#28856;&#24615;&#22686;&#38271;&#65292;&#36825;&#22686;&#24378;&#20102;&#38656;&#35201;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#26469;&#23436;&#25104;&#22810;&#39033;&#20219;&#21153;&#65292;&#22914;&#30830;&#23450;&#24066;&#22330;&#38656;&#27714;&#12289;&#24773;&#24863;&#20998;&#26512;&#12289;&#23041;&#32961;&#26816;&#27979;&#31561;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20919;&#22066;&#28909;&#35773;&#36890;&#24120;&#24847;&#21619;&#30528;&#30456;&#21453;&#30340;&#24847;&#24605;&#65292;&#20854;&#26816;&#27979;&#32463;&#24120;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22240;&#27492;&#36890;&#36807;NLP&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#24847;&#20041;&#25552;&#21462;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#12290;&#22240;&#27492;&#65292;&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#38024;&#23545;&#33521;&#35821;&#20013;&#30340;&#20919;&#22066;&#28909;&#35773;&#26816;&#27979;&#24050;&#32463;&#26377;&#22823;&#37327;&#30740;&#31350;&#65292;&#24182;&#19988;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#27493;&#65292;&#20294;&#23391;&#21152;&#25289;&#35821;&#20013;&#30340;&#20919;&#22066;&#28909;&#35773;&#26816;&#27979;&#22987;&#32456;&#27809;&#26377;&#25913;&#21892;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#31995;&#32479;&#65292;&#22312;&#20351;&#29992;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26102;&#21482;&#33021;&#36798;&#21040;99.60\%&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23454;&#29616;&#20919;&#22066;&#28909;&#35773;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
A positive phrase or a sentence with an underlying negative motive is usually defined as sarcasm that is widely used in today's social media platforms such as Facebook, Twitter, Reddit, etc. In recent times active users in social media platforms are increasing dramatically which raises the need for an automated NLP-based system that can be utilized in various tasks such as determining market demand, sentiment analysis, threat detection, etc. However, since sarcasm usually implies the opposite meaning and its detection is frequently a challenging issue, data meaning extraction through an NLP-based model becomes more complicated. As a result, there has been a lot of study on sarcasm detection in English over the past several years, and there's been a noticeable improvement and yet sarcasm detection in the Bangla language's state remains the same. In this article, we present a BERT-based system that can achieve 99.60\% while the utilized traditional machine learning algorithms are only ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;ChatGPT&#35780;&#20272;&#20013;&#38754;&#20020;&#30340;&#25968;&#25454;&#27745;&#26579;&#25361;&#25112;&#65292;&#36890;&#36807;&#20542;&#21521;&#24615;&#26816;&#27979;&#20219;&#21153;&#38416;&#36848;&#20102;&#36825;&#19968;&#38382;&#39064;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#22312;&#38381;&#21512;&#19988;&#25345;&#32493;&#35757;&#32451;&#27169;&#22411;&#30340;&#26102;&#20195;&#30830;&#20445;&#27169;&#22411;&#35780;&#20272;&#30340;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12767</link><description>&lt;p&gt;
&#25105;&#20204;&#33021;&#30456;&#20449;ChatGPT&#30340;&#35780;&#20272;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can we trust the evaluation on ChatGPT?. (arXiv:2303.12767v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;ChatGPT&#35780;&#20272;&#20013;&#38754;&#20020;&#30340;&#25968;&#25454;&#27745;&#26579;&#25361;&#25112;&#65292;&#36890;&#36807;&#20542;&#21521;&#24615;&#26816;&#27979;&#20219;&#21153;&#38416;&#36848;&#20102;&#36825;&#19968;&#38382;&#39064;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#22312;&#38381;&#21512;&#19988;&#25345;&#32493;&#35757;&#32451;&#27169;&#22411;&#30340;&#26102;&#20195;&#30830;&#20445;&#27169;&#22411;&#35780;&#20272;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#31532;&#19968;&#20010;&#34987;&#24191;&#27867;&#37319;&#32435;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23637;&#31034;&#20986;&#22312;&#22810;&#39033;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#21331;&#36234;&#30340;&#34920;&#29616;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#27169;&#22411;&#30340;&#38381;&#21512;&#24615;&#20197;&#21450;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21644;&#20154;&#31867;&#21453;&#39304;&#19981;&#26029;&#26356;&#26032;&#65292;&#35780;&#20272;ChatGPT&#22312;&#19981;&#21516;&#38382;&#39064;&#39046;&#22495;&#30340;&#34920;&#29616;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#37325;&#28857;&#35752;&#35770;&#20102;&#22312;ChatGPT&#30340;&#35780;&#20272;&#20013;&#23384;&#22312;&#30340;&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#20542;&#21521;&#24615;&#26816;&#27979;&#20219;&#21153;&#20316;&#20026;&#26696;&#20363;&#36827;&#34892;&#20102;&#35828;&#26126;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#22914;&#20309;&#22312;&#38381;&#21512;&#21644;&#25345;&#32493;&#35757;&#32451;&#27169;&#22411;&#30340;&#26102;&#20195;&#65292;&#36991;&#20813;&#25968;&#25454;&#27745;&#26579;&#21644;&#30830;&#20445;&#20844;&#24179;&#30340;&#27169;&#22411;&#35780;&#20272;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT, the first large language model (LLM) with mass adoption, has demonstrated remarkable performance in numerous natural language tasks. Despite its evident usefulness, evaluating ChatGPT's performance in diverse problem domains remains challenging due to the closed nature of the model and its continuous updates via Reinforcement Learning from Human Feedback (RLHF). We highlight the issue of data contamination in ChatGPT evaluations, with a case study of the task of stance detection. We discuss the challenge of preventing data contamination and ensuring fair model evaluation in the age of closed and continuously trained models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27979;&#35797;&#20102;&#20351;&#29992;2D&#22270;&#20687;&#21644;3D&#36712;&#36857;&#23545;&#21160;&#35789;&#35821;&#20041;&#34920;&#31034;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;2D&#35270;&#35273;&#27169;&#24577;&#30340;&#34920;&#29616;&#19982;3D&#36712;&#36857;&#31867;&#20284;&#65292;&#25361;&#25112;&#20102;&#20256;&#32479;&#30340;&#26234;&#24935;&#12290;</title><link>http://arxiv.org/abs/2303.12737</link><description>&lt;p&gt;
&#27604;&#36739;&#36712;&#36857;&#21644;&#35270;&#35273;&#27169;&#24577;&#23545;&#21160;&#35789;&#34920;&#31034;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Comparing Trajectory and Vision Modalities for Verb Representation. (arXiv:2303.12737v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27979;&#35797;&#20102;&#20351;&#29992;2D&#22270;&#20687;&#21644;3D&#36712;&#36857;&#23545;&#21160;&#35789;&#35821;&#20041;&#34920;&#31034;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;2D&#35270;&#35273;&#27169;&#24577;&#30340;&#34920;&#29616;&#19982;3D&#36712;&#36857;&#31867;&#20284;&#65292;&#25361;&#25112;&#20102;&#20256;&#32479;&#30340;&#26234;&#24935;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19977;&#32500;&#36712;&#36857;&#65292;&#21363;&#29289;&#20307;&#38543;&#26102;&#38388;&#30340;3D&#20301;&#32622;&#21644;&#26059;&#36716;&#65292;&#34987;&#35777;&#26126;&#21487;&#20197;&#32534;&#30721;&#21160;&#35789;&#35821;&#20041;&#30340;&#20851;&#38190;&#26041;&#38754;&#65288;&#20363;&#22914;&#65292;roll&#21644;slide&#30340;&#21547;&#20041;&#65289;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;NLP&#20013;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#20351;&#29992;2D&#22270;&#20687;&#20316;&#20026;&#19990;&#30028;&#30340;&#34920;&#31034;&#12290;&#32771;&#34385;&#21040;3D&#31354;&#38388;&#22312;&#21160;&#35789;&#35821;&#20041;&#30340;&#24418;&#24335;&#27169;&#22411;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#39044;&#26399;&#36825;&#20123;2D&#22270;&#20687;&#20250;&#23548;&#33268;&#36139;&#30240;&#30340;&#34920;&#31034;&#65292;&#26080;&#27861;&#25429;&#25417;&#21040;&#24494;&#22937;&#30340;&#24046;&#24322;&#12290;&#26412;&#25991;&#22312;&#21463;&#25511;&#23454;&#39564;&#20013;&#30452;&#25509;&#27979;&#35797;&#20102;&#36825;&#20010;&#20551;&#35774;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#33258;&#30417;&#30563;&#30340;&#22270;&#20687;&#21644;&#36712;&#36857;&#32534;&#30721;&#22120;&#65292;&#28982;&#21518;&#35780;&#20272;&#23427;&#20204;&#23398;&#20064;&#21306;&#20998;&#21160;&#35789;&#27010;&#24565;&#30340;&#31243;&#24230;&#12290;&#19982;&#25105;&#20204;&#26368;&#21021;&#30340;&#39044;&#26399;&#30456;&#21453;&#65292;&#25105;&#20204;&#21457;&#29616;2D&#35270;&#35273;&#27169;&#24577;&#30340;&#34920;&#29616;&#19982;3D&#36712;&#36857;&#31867;&#20284;&#12290;&#34429;&#28982;&#36824;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#25105;&#20204;&#30340;&#21021;&#27493;&#21457;&#29616;&#25361;&#25112;&#20102;&#20256;&#32479;&#30340;&#26234;&#24935;&#65306;&#26356;&#20016;&#23500;&#30340;&#29615;&#22659;&#34920;&#31034;&#24517;&#28982;&#20250;&#23548;&#33268;&#26356;&#22909;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Three-dimensional trajectories, or the 3D position and rotation of objects over time, have been shown to encode key aspects of verb semantics (e.g., the meanings of roll vs. slide). However, most multimodal models in NLP use 2D images as representations of the world. Given the importance of 3D space in formal models of verb semantics, we expect that these 2D images would result in impoverished representations that fail to capture nuanced differences in meaning. This paper tests this hypothesis directly in controlled experiments. We train self-supervised image and trajectory encoders, and then evaluate them on the extent to which each learns to differentiate verb concepts. Contrary to our initial expectations, we find that 2D visual modalities perform similarly well to 3D trajectories. While further work should be conducted on this question, our initial findings challenge the conventional wisdom that richer environment representations necessarily translate into better representation lea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35270;&#35273;&#21644;&#25991;&#26412;&#20559;&#32622;&#22522;&#20934;MMBias&#65292;&#28085;&#30422;14&#20010;&#20154;&#21475;&#23376;&#32676;&#65292;&#24182;&#21033;&#29992;&#35813;&#22522;&#20934;&#35780;&#20272;&#20102;&#22810;&#20010;&#33258;&#25105;&#30417;&#30563;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;&#21253;&#25324;CLIP&#12289;ALBEF&#21644;ViLT&#65289;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#34920;&#29616;&#20986;&#20559;&#21521;&#26576;&#20123;&#32676;&#20307;&#30340;&#26377;&#24847;&#20041;&#30340;&#20559;&#35265;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#35268;&#27169;&#39044;&#20808;&#35757;&#32451;&#27169;&#22411;&#35774;&#35745;&#30340;&#21435;&#20559;&#32622;&#26041;&#27861;&#20316;&#20026;&#21518;&#22788;&#29702;&#27493;&#39588;&#65292;&#21487;&#20197;&#20943;&#36731;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#20445;&#35777;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.12734</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#20559;&#24046;&#65306;&#24341;&#20837;&#19968;&#31181;&#26694;&#26550;&#20197;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21051;&#26495;&#21360;&#35937;&#20559;&#24046;&#65292;&#36229;&#36234;&#24615;&#21035;&#21644;&#31181;&#26063;&#12290;
&lt;/p&gt;
&lt;p&gt;
MultiModal Bias: Introducing a Framework for Stereotypical Bias Assessment beyond Gender and Race in Vision Language Models. (arXiv:2303.12734v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12734
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35270;&#35273;&#21644;&#25991;&#26412;&#20559;&#32622;&#22522;&#20934;MMBias&#65292;&#28085;&#30422;14&#20010;&#20154;&#21475;&#23376;&#32676;&#65292;&#24182;&#21033;&#29992;&#35813;&#22522;&#20934;&#35780;&#20272;&#20102;&#22810;&#20010;&#33258;&#25105;&#30417;&#30563;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;&#21253;&#25324;CLIP&#12289;ALBEF&#21644;ViLT&#65289;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#34920;&#29616;&#20986;&#20559;&#21521;&#26576;&#20123;&#32676;&#20307;&#30340;&#26377;&#24847;&#20041;&#30340;&#20559;&#35265;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#35268;&#27169;&#39044;&#20808;&#35757;&#32451;&#27169;&#22411;&#35774;&#35745;&#30340;&#21435;&#20559;&#32622;&#26041;&#27861;&#20316;&#20026;&#21518;&#22788;&#29702;&#27493;&#39588;&#65292;&#21487;&#20197;&#20943;&#36731;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#20445;&#35777;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#33258;&#20027;&#35757;&#32451;&#30340;&#31361;&#30772;&#20026;&#19968;&#31867;&#39044;&#20808;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#24102;&#26469;&#20102;&#26032;&#30340;&#26426;&#36935;&#12290;&#34429;&#28982;&#23545;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#36827;&#34892;&#20102;&#19968;&#20123;&#35843;&#26597;&#65292;&#20294;&#20027;&#35201;&#38598;&#20013;&#22312;&#24615;&#21035;&#21644;&#31181;&#26063;&#20559;&#35265;&#19978;&#65292;&#23545;&#20110;&#20854;&#20182;&#30456;&#20851;&#32676;&#20307;&#65292;&#22914;&#23447;&#25945;&#12289;&#22269;&#31821;&#12289;&#24615;&#21462;&#21521;&#25110;&#27531;&#30142;&#20154;&#32676;&#65292;&#32473;&#20104;&#30340;&#20851;&#27880;&#36739;&#23569;&#65292;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#32570;&#20047;&#36866;&#24403;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#31216;&#20026;MMBias&#30340;&#35270;&#35273;&#21644;&#25991;&#26412;&#20559;&#24046;&#22522;&#20934;&#26469;&#35299;&#20915;&#36825;&#19968;&#24046;&#36317;&#65292;&#21253;&#25324;&#32422;3800&#20010;&#22270;&#20687;&#21644;&#30701;&#35821;&#65292;&#28085;&#30422;14&#20010;&#20154;&#21475;&#23376;&#32676;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#20960;&#20010;&#33879;&#21517;&#30340;&#33258;&#25105;&#30417;&#30563;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#21253;&#25324;CLIP&#12289;ALBEF&#21644;ViLT&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#34920;&#29616;&#20986;&#26377;&#24847;&#20041;&#30340;&#20559;&#24046;&#65292;&#20559;&#21521;&#26576;&#20123;&#32676;&#20307;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#38024;&#23545;&#36825;&#31181;&#22823;&#35268;&#27169;&#39044;&#20808;&#35757;&#32451;&#27169;&#22411;&#35774;&#35745;&#30340;&#21435;&#20559;&#32622;&#26041;&#27861;&#65292;&#21487;&#20197;&#20316;&#20026;&#21518;&#22788;&#29702;&#27493;&#39588;&#24212;&#29992;&#20110;&#20943;&#36731;&#20559;&#24046;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent breakthroughs in self supervised training have led to a new class of pretrained vision language models. While there have been investigations of bias in multimodal models, they have mostly focused on gender and racial bias, giving much less attention to other relevant groups, such as minorities with regard to religion, nationality, sexual orientation, or disabilities. This is mainly due to lack of suitable benchmarks for such groups. We seek to address this gap by providing a visual and textual bias benchmark called MMBias, consisting of around 3,800 images and phrases covering 14 population subgroups. We utilize this dataset to assess bias in several prominent self supervised multimodal models, including CLIP, ALBEF, and ViLT. Our results show that these models demonstrate meaningful bias favoring certain groups. Finally, we introduce a debiasing method designed specifically for such large pre-trained models that can be applied as a post-processing step to mitigate bias, while p
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#38024;&#23545;VLSP2022-EVJVQA&#20849;&#20139;&#20219;&#21153;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#24207;&#21015;&#21040;&#24207;&#21015;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#22312;&#22810;&#35821;&#35328;&#35270;&#35273;&#38382;&#31572;&#20013;&#23558;&#39044;&#35757;&#32451;&#30340;VQA&#27169;&#22411;&#21644;&#22270;&#20687;&#29305;&#24449;&#38598;&#25104;&#65292;&#33719;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.12671</link><description>&lt;p&gt;
&#22522;&#20110;&#21367;&#31215;&#24207;&#21015;&#21040;&#24207;&#21015;&#32593;&#32476;&#23558;&#22270;&#20687;&#29305;&#24449;&#38598;&#25104;&#21040;&#22810;&#35821;&#35328;&#35270;&#35273;&#38382;&#31572;&#20013;
&lt;/p&gt;
&lt;p&gt;
Integrating Image Features with Convolutional Sequence-to-sequence Network for Multilingual Visual Question Answering. (arXiv:2303.12671v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12671
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#38024;&#23545;VLSP2022-EVJVQA&#20849;&#20139;&#20219;&#21153;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#24207;&#21015;&#21040;&#24207;&#21015;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#22312;&#22810;&#35821;&#35328;&#35270;&#35273;&#38382;&#31572;&#20013;&#23558;&#39044;&#35757;&#32451;&#30340;VQA&#27169;&#22411;&#21644;&#22270;&#20687;&#29305;&#24449;&#38598;&#25104;&#65292;&#33719;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#38382;&#31572;(VQA)&#26159;&#19968;&#39033;&#35201;&#27714;&#35745;&#31639;&#26426;&#22522;&#20110;&#22270;&#20687;&#22238;&#31572;&#36755;&#20837;&#38382;&#39064;&#30340;&#20219;&#21153;&#12290;&#36825;&#20010;&#20219;&#21153;&#23545;&#20154;&#31867;&#26469;&#35828;&#24456;&#23481;&#26131;&#65292;&#20294;&#23545;&#35745;&#31639;&#26426;&#26469;&#35828;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;VLSP2022-EVJVQA&#20849;&#20139;&#20219;&#21153;&#22312;&#19968;&#20010;&#26032;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;UIT-EVJVQA&#20013;&#36827;&#34892;&#20102;&#22810;&#35821;&#35328;&#39046;&#22495;&#30340;VQA&#20219;&#21153;&#65292;&#20854;&#20013;&#38382;&#39064;&#21644;&#31572;&#26696;&#29992;&#19977;&#31181;&#19981;&#21516;&#35821;&#35328;&#32534;&#20889;&#65306;&#33521;&#35821;&#12289;&#36234;&#21335;&#35821;&#21644;&#26085;&#35821;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#25361;&#25112;&#20316;&#20026;&#19968;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#23398;&#20064;&#20219;&#21153;&#65292;&#36890;&#36807;&#23558;&#26469;&#33258;&#39044;&#35757;&#32451;&#30340;&#26368;&#20808;&#36827;&#30340;VQA&#27169;&#22411;&#21644;&#22270;&#20687;&#29305;&#24449;&#30340;&#25552;&#31034;&#19982;&#21367;&#31215;&#24207;&#21015;&#21040;&#24207;&#21015;&#32593;&#32476;&#38598;&#25104;&#22312;&#19968;&#36215;&#26469;&#29983;&#25104;&#25152;&#38656;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#22312;&#20844;&#20849;&#27979;&#35797;&#38598;&#19978;&#33719;&#24471;&#20102;F1&#24471;&#20998;&#36798;&#21040;0.3442&#65292;&#22312;&#31169;&#26377;&#27979;&#35797;&#38598;&#19978;&#33719;&#24471;&#20102;0.4210&#30340;&#22909;&#25104;&#32489;&#65292;&#24182;&#22312;&#27604;&#36187;&#20013;&#21462;&#24471;&#20102;&#31532;&#19977;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual Question Answering (VQA) is a task that requires computers to give correct answers for the input questions based on the images. This task can be solved by humans with ease but is a challenge for computers. The VLSP2022-EVJVQA shared task carries the Visual Question Answering task in the multilingual domain on a newly released dataset: UIT-EVJVQA, in which the questions and answers are written in three different languages: English, Vietnamese and Japanese. We approached the challenge as a sequence-to-sequence learning task, in which we integrated hints from pre-trained state-of-the-art VQA models and image features with Convolutional Sequence-to-Sequence network to generate the desired answers. Our results obtained up to 0.3442 by F1 score on the public test set, 0.4210 on the private test set, and placed 3rd in the competition.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#35780;&#20272;&#20102;&#30446;&#26631;&#35770;&#25454;&#22312;&#35875;&#35328;&#31435;&#22330;&#20998;&#31867;&#20013;&#30340;&#20316;&#29992;&#65292;&#35777;&#26126;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36807;&#20110;&#20381;&#36182;&#34920;&#38754;&#20449;&#21495;&#65292;&#30446;&#26631;&#19981;&#29420;&#31435;&#30340;&#20107;&#20214;&#33258;&#28982;&#39640;&#21457;&#29983;&#29575;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#36807;&#24230;&#25311;&#21512;&#12290;</title><link>http://arxiv.org/abs/2303.12665</link><description>&lt;p&gt;
&#35780;&#20272;&#30446;&#26631;&#35770;&#25454;&#22312;&#35875;&#35328;&#31435;&#22330;&#20998;&#31867;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Role of Target Arguments in Rumour Stance Classification. (arXiv:2303.12665v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12665
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#35780;&#20272;&#20102;&#30446;&#26631;&#35770;&#25454;&#22312;&#35875;&#35328;&#31435;&#22330;&#20998;&#31867;&#20013;&#30340;&#20316;&#29992;&#65292;&#35777;&#26126;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36807;&#20110;&#20381;&#36182;&#34920;&#38754;&#20449;&#21495;&#65292;&#30446;&#26631;&#19981;&#29420;&#31435;&#30340;&#20107;&#20214;&#33258;&#28982;&#39640;&#21457;&#29983;&#29575;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#36807;&#24230;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#19968;&#32452;&#23545;&#35805;&#65292;&#31435;&#22330;&#20998;&#31867;&#26088;&#22312;&#30830;&#23450;&#31572;&#22797;&#23545;&#32473;&#23450;&#30446;&#26631;&#30340;&#24847;&#35265;&#65288;&#20363;&#22914;&#21516;&#24847;&#25110;&#19981;&#21516;&#24847;&#65289;&#12290;&#22312;&#36825;&#39033;&#20219;&#21153;&#20013;&#65292;&#31435;&#22330;&#30340;&#30446;&#26631;&#39044;&#35745;&#26159;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#26159;&#20351;&#20854;&#19982;&#24773;&#24863;&#20998;&#26512;&#19981;&#21516;&#30340;&#20027;&#35201;&#22240;&#32032;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#19968;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;&#19968;&#20010;&#24573;&#30053;&#30446;&#26631;&#30340;&#27169;&#22411;&#20248;&#20110;&#30446;&#26631;&#24863;&#30693;&#27169;&#22411;&#65292;&#34920;&#26126;&#22312;&#39044;&#27979;&#31435;&#22330;&#26102;&#30446;&#26631;&#24182;&#19981;&#26377;&#29992;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#36825;&#19968;&#29616;&#35937;&#65292;&#38024;&#23545;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#35875;&#35328;&#31435;&#22330;&#20998;&#31867;&#65288;RSC&#65289;&#65292;&#20854;&#20013;&#30446;&#26631;&#26159;&#28304;&#25512;&#25991;&#20013;&#38544;&#21547;&#30340;&#35875;&#35328;&#25925;&#20107;&#12290;&#25105;&#20204;&#22312;&#27979;&#35797;&#25968;&#25454;&#20013;&#25552;&#20986;&#20102;&#23545;&#25239;&#25915;&#20987;&#65292;&#26088;&#22312;&#35780;&#20272;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#24182;&#35780;&#20272;&#25968;&#25454;&#22312;&#27169;&#22411;&#24615;&#33021;&#20013;&#30340;&#20316;&#29992;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#21253;&#25324;&#20351;&#29992;&#25972;&#20010;&#23545;&#35805;&#32447;&#31243;&#30340;&#26041;&#27861;&#22312;&#20869;&#30340;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36807;&#20110;&#20381;&#36182;&#34920;&#38754;&#20449;&#21495;&#12290;&#25105;&#20204;&#30340;&#20551;&#35774;&#26159;&#65292;&#30446;&#26631;&#19981;&#29420;&#31435;&#30340;&#20107;&#20214;&#33258;&#28982;&#39640;&#21457;&#29983;&#29575;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#36807;&#24230;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Considering a conversation thread, stance classification aims to identify the opinion (e.g. agree or disagree) of replies towards a given target. The target of the stance is expected to be an essential component in this task, being one of the main factors that make it different from sentiment analysis. However, a recent study shows that a target-oblivious model outperforms target-aware models, suggesting that targets are not useful when predicting stance. This paper re-examines this phenomenon for rumour stance classification (RSC) on social media, where a target is a rumour story implied by the source tweet in the conversation. We propose adversarial attacks in the test data, aiming to assess the models robustness and evaluate the role of the data in the models performance. Results show that state-of-the-art models, including approaches that use the entire conversation thread, overly relying on superficial signals. Our hypothesis is that the naturally high occurrence of target-indepen
&lt;/p&gt;</description></item><item><title>AfroDigits&#26159;&#31532;&#19968;&#20010;&#21457;&#24067;&#30340;&#38754;&#21521;&#38750;&#27954;&#35821;&#35328;&#30340;&#38899;&#39057;&#25968;&#23383;&#25968;&#25454;&#38598;&#65292;&#20026;&#38750;&#27954;&#35821;&#35328;&#20013;&#30340;&#35821;&#38899;&#24212;&#29992;&#31243;&#24207;&#24320;&#36767;&#20102;&#36947;&#36335;&#12290;</title><link>http://arxiv.org/abs/2303.12582</link><description>&lt;p&gt;
AfroDigits&#65306;&#38754;&#21521;&#38750;&#27954;&#35821;&#35328;&#30340;&#22522;&#20110;&#31038;&#21306;&#39537;&#21160;&#30340;&#21475;&#35821;&#25968;&#23383;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
AfroDigits: A Community-Driven Spoken Digit Dataset for African Languages. (arXiv:2303.12582v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12582
&lt;/p&gt;
&lt;p&gt;
AfroDigits&#26159;&#31532;&#19968;&#20010;&#21457;&#24067;&#30340;&#38754;&#21521;&#38750;&#27954;&#35821;&#35328;&#30340;&#38899;&#39057;&#25968;&#23383;&#25968;&#25454;&#38598;&#65292;&#20026;&#38750;&#27954;&#35821;&#35328;&#20013;&#30340;&#35821;&#38899;&#24212;&#29992;&#31243;&#24207;&#24320;&#36767;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35821;&#38899;&#25216;&#26415;&#30340;&#36827;&#27493;&#26159;&#26174;&#33879;&#30340;&#65292;&#20294;&#30001;&#20110;&#38750;&#27954;&#35821;&#26009;&#24211;&#30340;&#21294;&#20047;&#65292;&#20854;&#22312;&#38750;&#27954;&#35821;&#35328;&#20013;&#30340;&#38598;&#25104;&#20173;&#28982;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AfroDigits&#65292;&#36825;&#26159;&#19968;&#20010;&#26497;&#31616;&#30340;&#12289;&#30001;&#31038;&#21306;&#39537;&#21160;&#30340;&#38750;&#27954;&#35821;&#35328;&#21475;&#35821;&#25968;&#23383;&#25968;&#25454;&#38598;&#65292;&#30446;&#21069;&#35206;&#30422;&#20102;38&#31181;&#38750;&#27954;&#35821;&#35328;&#12290;&#20316;&#20026;AfroDigits&#23454;&#38469;&#24212;&#29992;&#30340;&#28436;&#31034;&#65292;&#25105;&#20204;&#20351;&#29992;Wav2Vec2.0-Large&#21644;XLS-R&#27169;&#22411;&#65292;&#22312;&#20845;&#31181;&#38750;&#27954;&#35821;&#35328;[Igbo(ibo), Yoruba(yor), Rundi(run), Oshiwambo(kua), Shona(sna)&#21644;Oromo(gax)]&#19978;&#36827;&#34892;&#20102;&#38899;&#39057;&#25968;&#23383;&#20998;&#31867;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#28151;&#21512;&#38750;&#27954;&#35821;&#26009;&#24211;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#25928;&#26524;&#12290;AfroDigits&#26159;&#38750;&#27954;&#35821;&#35328;&#20013;&#31532;&#19968;&#20010;&#21457;&#24067;&#30340;&#38899;&#39057;&#25968;&#23383;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30456;&#20449;&#23427;&#23558;&#20026;&#38754;&#21521;&#38750;&#27954;&#30340;&#35821;&#38899;&#24212;&#29992;&#31243;&#24207;&#24320;&#36767;&#36947;&#36335;&#65292;&#22914;&#30005;&#35805;&#21495;&#30721;&#21644;&#34903;&#36947;&#22320;&#22336;&#30340;&#35782;&#21035;&#12290;&#25105;&#20204;&#22312;https:/ / afrodigits&#20844;&#24320;&#21457;&#24067;&#25968;&#25454;&#38598;&#21644;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advancement of speech technologies has been remarkable, yet its integration with African languages remains limited due to the scarcity of African speech corpora. To address this issue, we present AfroDigits, a minimalist, community-driven dataset of spoken digits for African languages, currently covering 38 African languages. As a demonstration of the practical applications of AfroDigits, we conduct audio digit classification experiments on six African languages [Igbo (ibo), Yoruba (yor), Rundi (run), Oshiwambo (kua), Shona (sna), and Oromo (gax)] using the Wav2Vec2.0-Large and XLS-R models. Our experiments reveal a useful insight on the effect of mixing African speech corpora during finetuning. AfroDigits is the first published audio digit dataset for African languages and we believe it will, among other things, pave the way for Afro-centric speech applications such as the recognition of telephone numbers, and street numbers. We release the dataset and platform publicly at https:/
&lt;/p&gt;</description></item><item><title>RepoCoder&#26159;&#19968;&#20010;&#31616;&#21333;&#12289;&#36890;&#29992;&#21644;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#19968;&#20010;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#26816;&#32034;&#22120;&#21644;&#39044;&#35757;&#32451;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#24211;&#32423;&#21035;&#20195;&#30721;&#23436;&#25104;&#27969;&#31243;&#20013;&#23454;&#29616;&#20102;&#26377;&#25928;&#21033;&#29992;&#24211;&#32423;&#21035;&#20449;&#24687;&#36827;&#34892;&#20195;&#30721;&#23436;&#25104;&#21644;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2303.12570</link><description>&lt;p&gt;
RepoCoder&#65306;&#36890;&#36807;&#36845;&#20195;&#26816;&#32034;&#21644;&#29983;&#25104;&#23454;&#29616;&#30340;&#20195;&#30721;&#23384;&#20648;&#24211;&#32423;&#21035;&#23436;&#25104;
&lt;/p&gt;
&lt;p&gt;
RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation. (arXiv:2303.12570v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12570
&lt;/p&gt;
&lt;p&gt;
RepoCoder&#26159;&#19968;&#20010;&#31616;&#21333;&#12289;&#36890;&#29992;&#21644;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#19968;&#20010;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#26816;&#32034;&#22120;&#21644;&#39044;&#35757;&#32451;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#24211;&#32423;&#21035;&#20195;&#30721;&#23436;&#25104;&#27969;&#31243;&#20013;&#23454;&#29616;&#20102;&#26377;&#25928;&#21033;&#29992;&#24211;&#32423;&#21035;&#20449;&#24687;&#36827;&#34892;&#20195;&#30721;&#23436;&#25104;&#21644;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24211;&#32423;&#21035;&#20195;&#30721;&#23436;&#25104;&#20219;&#21153;&#26159;&#22522;&#20110;&#20195;&#30721;&#24211;&#26356;&#24191;&#38420;&#19978;&#19979;&#25991;&#20013;&#32487;&#32493;&#32534;&#20889;&#26410;&#23436;&#25104;&#20195;&#30721;&#30340;&#36807;&#31243;&#12290;&#20294;&#26159;&#23545;&#20110;&#33258;&#21160;&#23436;&#25104;&#24037;&#20855;&#32780;&#35328;&#65292;&#24456;&#38590;&#21033;&#29992;&#25955;&#24067;&#22312;&#19981;&#21516;&#25991;&#20214;&#20013;&#30340;&#26377;&#29992;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;RepoCoder&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#12289;&#36890;&#29992;&#21644;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#12290;&#23427;&#36890;&#36807;&#25972;&#21512;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#26816;&#32034;&#22120;&#21644;&#39044;&#35757;&#32451;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#31616;&#21270;&#20102;&#24211;&#32423;&#21035;&#20195;&#30721;&#23436;&#25104;&#27969;&#31243;&#65292;&#20174;&#32780;&#20801;&#35768;&#26377;&#25928;&#21033;&#29992;&#24211;&#32423;&#21035;&#20449;&#24687;&#36827;&#34892;&#20195;&#30721;&#23436;&#25104;&#65292;&#24182;&#20855;&#26377;&#19981;&#21516;&#31890;&#24230;&#23618;&#38754;&#30340;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;RepoCoder &#36824;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#26816;&#32034;-&#29983;&#25104;&#27169;&#22411;&#65292;&#24357;&#21512;&#20102;&#26816;&#32034;&#19978;&#19979;&#25991;&#21644;&#39044;&#26399;&#23436;&#25104;&#30446;&#26631;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;RepoEval&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#26368;&#26032;&#21644;&#39640;&#36136;&#37327;&#30495;&#23454;&#19990;&#30028;&#30340;&#20195;&#30721;&#24211;&#65292;&#28085;&#30422;&#20102;&#34892;&#12289;API &#35843;&#29992;&#21644;&#20989;&#25968;&#20307;&#23436;&#25104;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of repository-level code completion is to continue writing the unfinished code based on a broader context of the repository. While for automated code completion tools, it is difficult to utilize the useful information scattered in different files. We propose RepoCoder, a simple, generic, and effective framework to address the challenge. It streamlines the repository-level code completion process by incorporating a similarity-based retriever and a pre-trained code language model, which allows for the effective utilization of repository-level information for code completion and grants the ability to generate code at various levels of granularity. Furthermore, RepoCoder utilizes a novel iterative retrieval-generation paradigm that bridges the gap between retrieval context and the intended completion target. We also propose a new benchmark RepoEval, which consists of the latest and high-quality real-world repositories covering line, API invocation, and function body completion sce
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23545; 33 &#31181;&#35821;&#35328;&#20013; 8 &#20010;&#19981;&#21516;&#20219;&#21153;&#30340;&#29983;&#25104; AI &#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#27604;&#36739;&#20102;&#29983;&#25104; LLMs &#21644;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#34920;&#29616;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2303.12528</link><description>&lt;p&gt;
MEGA: &#22810;&#35821;&#35328;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#32508;&#21512;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
MEGA: Multilingual Evaluation of Generative AI. (arXiv:2303.12528v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12528
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23545; 33 &#31181;&#35821;&#35328;&#20013; 8 &#20010;&#19981;&#21516;&#20219;&#21153;&#30340;&#29983;&#25104; AI &#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#27604;&#36739;&#20102;&#29983;&#25104; LLMs &#21644;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#34920;&#29616;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;AI&#27169;&#22411;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65288;&#22914;&#35821;&#35328;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#35821;&#35328;&#29983;&#25104;&#65289;&#19978;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#24403;&#20170;AI&#31038;&#21306;&#26368;&#37325;&#35201;&#30340;&#38382;&#39064;&#20043;&#19968;&#26159;&#20851;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#33021;&#21147;&#21644;&#38480;&#21046;&#65292;&#35780;&#20272;&#29983;&#25104;AI&#26174;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22823;&#22810;&#25968;&#20851;&#20110;&#29983;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30740;&#31350;&#37117;&#38480;&#20110;&#33521;&#35821;&#65292;&#19981;&#28165;&#26970;&#36825;&#20123;&#27169;&#22411;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#20854;&#20182;&#35821;&#35328;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#39318;&#20010;&#20840;&#38754;&#35780;&#20272; 8 &#39033;&#19981;&#21516;&#20219;&#21153;&#21644; 33 &#31181;&#35821;&#35328;&#30340;&#29983;&#25104;LLMs MEGA &#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#36824;&#23558;&#29983;&#25104;LLMs&#30340;&#24615;&#33021;&#19982;&#36825;&#20123;&#20219;&#21153;&#19978;&#26368;&#20808;&#36827;&#30340;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#30830;&#23450;&#29983;&#25104;&#27169;&#22411;&#30340;&#34920;&#29616;&#22914;&#20309;&#19982;&#19978;&#19968;&#20195;LLMs&#30456;&#27604;&#12290;&#25105;&#20204;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#24443;&#24213;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI models have impressive performance on many Natural Language Processing tasks such as language understanding, reasoning and language generation. One of the most important questions that is being asked by the AI community today is about the capabilities and limits of these models, and it is clear that evaluating generative AI is very challenging. Most studies on generative Large Language Models (LLMs) are restricted to English and it is unclear how capable these models are at understanding and generating other languages. We present the first comprehensive benchmarking of generative LLMs MEGA, which evaluates models on standard NLP benchmarks, covering 8 diverse tasks and 33 typologically diverse languages. We also compare the performance of generative LLMs to State of the Art (SOTA) non-autoregressive models on these tasks to determine how well generative models perform compared to the previous generation of LLMs. We present a thorough analysis of the performance of model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#23545;&#20165;&#25991;&#26412;&#20219;&#21153;&#30340;&#34920;&#29616;&#26159;&#21542;&#26377;&#25552;&#39640;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#22871;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#65292;&#35777;&#26126;&#20102;&#22810;&#27169;&#24577;&#35757;&#32451;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#22312;&#35270;&#35273;&#25512;&#29702;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12513</link><description>&lt;p&gt;
BERT&#26159;&#21542;&#30450;&#30446;&#65311;&#25506;&#32034;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#23545;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Is BERT Blind? Exploring the Effect of Vision-and-Language Pretraining on Visual Language Understanding. (arXiv:2303.12513v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#23545;&#20165;&#25991;&#26412;&#20219;&#21153;&#30340;&#34920;&#29616;&#26159;&#21542;&#26377;&#25552;&#39640;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#22871;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#65292;&#35777;&#26126;&#20102;&#22810;&#27169;&#24577;&#35757;&#32451;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#22312;&#35270;&#35273;&#25512;&#29702;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#20154;&#20351;&#29992;&#35270;&#35273;&#24819;&#35937;&#26469;&#29702;&#35299;&#21644;&#25512;&#29702;&#35821;&#35328;&#65292;&#20294;&#26159;&#20687;BERT&#36825;&#26679;&#30340;&#27169;&#22411;&#20351;&#29992;&#22312;&#20165;&#21253;&#25324;&#25991;&#26412;&#30340;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#33719;&#21462;&#30340;&#30693;&#35782;&#26469;&#25512;&#29702;&#35821;&#35328;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#26159;&#21542;&#21487;&#20197;&#25552;&#39640;&#22312;&#28041;&#21450;&#38544;&#21547;&#35270;&#35273;&#25512;&#29702;&#30340;&#20165;&#25991;&#26412;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#37325;&#28857;&#26159;&#38646;&#26679;&#26412;&#25506;&#27979;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#29992;&#20110;&#25506;&#27979;&#25991;&#26412;&#32534;&#30721;&#22120;&#27169;&#22411;&#35270;&#35273;&#25512;&#29702;&#33021;&#21147;&#30340;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;&#65288;VLU&#65289;&#20219;&#21153;&#65292;&#20197;&#21450;&#21508;&#31181;&#38750;&#35270;&#35273;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#20219;&#21153;&#29992;&#20110;&#27604;&#36739;&#12290;&#25105;&#20204;&#36824;&#36129;&#29486;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#38646;&#26679;&#26412;&#30693;&#35782;&#25506;&#27979;&#26041;&#27861;&#65292;Stroop probing&#65292;&#29992;&#20110;&#23558;&#20687;CLIP&#36825;&#26679;&#30340;&#27169;&#22411;&#24212;&#29992;&#20110;&#20165;&#25991;&#26412;&#20219;&#21153;&#65292;&#32780;&#19981;&#38656;&#35201;&#20687;BERT&#27169;&#22411;&#30340;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#22836;&#37027;&#26679;&#30340;&#39044;&#27979;&#22836;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;SOTA&#22810;&#27169;&#24577;&#35757;&#32451;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#22312;VLU&#20219;&#21153;&#19978;&#20248;&#20110;&#21333;&#27169;&#24577;&#35757;&#32451;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#20294;&#22312;NLU&#20219;&#21153;&#19978;&#19981;&#21450;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most humans use visual imagination to understand and reason about language, but models such as BERT reason about language using knowledge acquired during text-only pretraining. In this work, we investigate whether vision-and-language pretraining can improve performance on text-only tasks that involve implicit visual reasoning, focusing primarily on zero-shot probing methods. We propose a suite of visual language understanding (VLU) tasks for probing the visual reasoning abilities of text encoder models, as well as various non-visual natural language understanding (NLU) tasks for comparison. We also contribute a novel zero-shot knowledge probing method, Stroop probing, for applying models such as CLIP to text-only tasks without needing a prediction head such as the masked language modelling head of models like BERT. We show that SOTA multimodally trained text encoders outperform unimodally trained text encoders on the VLU tasks while being underperformed by them on the NLU tasks, lendin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#38454;&#27573;&#24494;&#35843;&#26694;&#26550;&#65292;&#38024;&#23545;&#23569;&#26679;&#26412;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#22810;&#35821;&#35328;&#23398;&#20064;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#20248;&#21183;&#65292;&#23427;&#37319;&#29992;&#19968;&#31181;&#36890;&#29992;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#39592;&#26550;&#65292;&#24182;&#20351;&#29992;&#27880;&#24847;&#26426;&#21046;&#22788;&#29702;&#22810;&#27169;&#24577;&#20449;&#24687;&#21644;&#27599;&#20010;&#20219;&#21153;&#30340;&#35821;&#35328;&#29305;&#23450;&#30340;&#24494;&#35843;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2303.12489</link><description>&lt;p&gt;
&#23569;&#26679;&#26412;&#12289;&#22810;&#27169;&#24577;&#12289;&#22810;&#20219;&#21153;&#12289;&#22810;&#35821;&#35328;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Few-shot Multimodal Multitask Multilingual Learning. (arXiv:2303.12489v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12489
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#38454;&#27573;&#24494;&#35843;&#26694;&#26550;&#65292;&#38024;&#23545;&#23569;&#26679;&#26412;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#22810;&#35821;&#35328;&#23398;&#20064;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#20248;&#21183;&#65292;&#23427;&#37319;&#29992;&#19968;&#31181;&#36890;&#29992;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#39592;&#26550;&#65292;&#24182;&#20351;&#29992;&#27880;&#24847;&#26426;&#21046;&#22788;&#29702;&#22810;&#27169;&#24577;&#20449;&#24687;&#21644;&#27599;&#20010;&#20219;&#21153;&#30340;&#35821;&#35328;&#29305;&#23450;&#30340;&#24494;&#35843;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#36801;&#31227;&#23398;&#20064;&#33539;&#24335;&#65292;&#22312;&#25968;&#25454;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36807;&#21435;&#20027;&#35201;&#26159;&#22312;&#26500;&#24314;&#21333;&#27169;&#24577;&#21333;&#35821;&#35328;&#27169;&#22411;&#30340;&#32972;&#26223;&#19979;&#25506;&#35752;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#29616;&#26377;&#30340;&#23569;&#26679;&#26412;&#22810;&#20219;&#21153;&#23398;&#20064;&#39046;&#22495;&#30340;&#22823;&#37096;&#20998;&#25991;&#29486;&#37117;&#26159;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#30340;&#65292;&#38656;&#35201;&#25163;&#21160;&#29983;&#25104;&#25552;&#31034;&#20316;&#20026;&#36755;&#20837;&#65292;&#36825;&#23548;&#33268;&#20102;&#32467;&#26524;&#30340;&#24046;&#24322;&#65292;&#21462;&#20915;&#20110;&#25163;&#21160;&#25552;&#31034;&#24037;&#31243;&#30340;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#20250;&#24102;&#26469;&#30456;&#24403;&#22823;&#30340;&#35745;&#31639;&#12289;&#20869;&#23384;&#12289;&#23384;&#20648;&#25104;&#26412;&#65292;&#26368;&#32456;&#23548;&#33268;&#39640;&#25512;&#29702;&#24310;&#36831;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#27599;&#27425;&#36827;&#34892;&#39044;&#27979;&#26102;&#37117;&#35201;&#36890;&#36807;&#27169;&#22411;&#36816;&#34892;&#25152;&#26377;&#25552;&#31034;&#30340;&#31034;&#20363;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#36890;&#36807;&#24494;&#35843;&#33539;&#24335;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#36991;&#20813;&#20102;&#19978;&#36848;&#38382;&#39064;&#65292;&#22312;&#20219;&#21153;&#30340;&#22522;&#30784;&#19978;&#20197;&#19968;&#27425;&#24615;&#30340;&#20195;&#20215;&#24494;&#35843;&#26435;&#37325;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#32570;&#20047;&#23569;&#26679;&#26412;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#32463;&#39564;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#38454;&#27573;&#24494;&#35843;&#26694;&#26550;&#65292;&#38024;&#23545;&#23569;&#26679;&#26412;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#22810;&#35821;&#35328;&#23398;&#20064;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#19968;&#31181;&#36890;&#29992;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#39592;&#26550;&#65292;&#24182;&#20351;&#29992;&#27880;&#24847;&#26426;&#21046;&#22788;&#29702;&#22810;&#27169;&#24577;&#20449;&#24687;&#21644;&#27599;&#20010;&#20219;&#21153;&#30340;&#35821;&#35328;&#29305;&#23450;&#30340;&#24494;&#35843;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#34920;&#29616;&#20986;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#65292;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
While few-shot learning as a transfer learning paradigm has gained significant traction for scenarios with limited data, it has primarily been explored in the context of building unimodal and unilingual models. Furthermore, a significant part of the existing literature in the domain of few-shot multitask learning perform in-context learning which requires manually generated prompts as the input, yielding varying outcomes depending on the level of manual prompt-engineering. In addition, in-context learning suffers from substantial computational, memory, and storage costs which eventually leads to high inference latency because it involves running all of the prompt's examples through the model every time a prediction is made. In contrast, methods based on the transfer learning via the fine-tuning paradigm avoid the aforementioned issues at a one-time cost of fine-tuning weights on a per-task basis. However, such methods lack exposure to few-shot multimodal multitask learning. In this pap
&lt;/p&gt;</description></item><item><title>GrapeQA&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#8220;&#37325;&#35201;&#23454;&#20307;&#22270;&#24418;&#22686;&#24378;&#8221;&#21644;&#8220;&#19978;&#19979;&#25991;&#24863;&#30693;&#33410;&#28857;&#21098;&#26525;&#8221;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#38382;&#31572;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.12320</link><description>&lt;p&gt;
GrapeQA&#65306;&#22686;&#24378;&#38382;&#31572;&#21151;&#33021;&#30340;&#22270;&#24418;&#22686;&#24378;&#21644;&#21098;&#26525;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GrapeQA: GRaph Augmentation and Pruning to Enhance Question-Answering. (arXiv:2303.12320v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12320
&lt;/p&gt;
&lt;p&gt;
GrapeQA&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#8220;&#37325;&#35201;&#23454;&#20307;&#22270;&#24418;&#22686;&#24378;&#8221;&#21644;&#8220;&#19978;&#19979;&#25991;&#24863;&#30693;&#33410;&#28857;&#21098;&#26525;&#8221;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#38382;&#31572;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24120;&#35782;&#38382;&#31572;&#26041;&#27861;&#32467;&#21512;&#20102;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#30340;&#33021;&#21147;&#21644;&#30693;&#35782;&#22270;&#65288;KG&#65289;&#25552;&#20379;&#30340;&#25512;&#29702;&#12290; &#20856;&#22411;&#26041;&#27861;&#20174;KG&#20013;&#25910;&#38598;&#19982;QA&#21305;&#37197;&#30340;&#33410;&#28857;&#20197;&#24418;&#25104;&#24037;&#20316;&#22270;&#65288;WG&#65289;&#65292;&#28982;&#21518;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#36827;&#34892;&#25512;&#29702;&#12290;&#36825;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#65288;i&#65289;&#24456;&#38590;&#20174;WG&#20013;&#25429;&#33719;QA&#20013;&#30340;&#25152;&#26377;&#20449;&#24687;&#65292;&#65288;ii&#65289;WG&#21253;&#21547;&#19968;&#20123;&#26469;&#33258;KG&#30340;&#19981;&#30456;&#20851;&#33410;&#28857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GrapeQA&#30340;&#31639;&#27861;&#20197;&#23545;WG&#36827;&#34892;&#20004;&#20010;&#31616;&#21333;&#30340;&#25913;&#36827;&#65306;&#65288;i&#65289;&#29992;&#20110;&#22270;&#24418;&#22686;&#24378;&#30340;&#37325;&#35201;&#23454;&#20307;&#65288;Prominent Entities&#65289;&#35782;&#21035;QA&#23545;&#24403;&#20013;&#30456;&#20851;&#25991;&#26412;&#22359;&#65292;&#24182;&#20351;&#29992;&#30456;&#24212;&#30340;&#28508;&#22312;&#34920;&#31034;&#20174;LM&#36827;&#34892;&#22686;&#24378;&#65307;&#65288;ii&#65289;&#23558;&#19981;&#30456;&#20851;&#30340;&#33410;&#28857;&#21098;&#26525;&#12290;&#25105;&#20204;&#22312;OpenBookQA&#65292;CommonsenseQA&#21644;MedQA-USMLE&#19978;&#35780;&#20272;&#20102;&#32467;&#26524;&#65292;&#24182;&#21457;&#29616;GrapeQA&#26174;&#31034;&#20986;&#25345;&#32493;&#30340;&#25913;&#36827;&#65292;&#36229;&#36807;&#20102;&#20854;LM + KG&#21069;&#36523;&#65288;&#29305;&#21035;&#26159;QA-GNN&#65289;&#24182;&#33719;&#24471;&#20102;&#24040;&#22823;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Commonsense question-answering (QA) methods combine the power of pre-trained Language Models (LM) with the reasoning provided by Knowledge Graphs (KG). A typical approach collects nodes relevant to the QA pair from a KG to form a Working Graph (WG) followed by reasoning using Graph Neural Networks(GNNs). This faces two major challenges: (i) it is difficult to capture all the information from the QA in the WG, and (ii) the WG contains some irrelevant nodes from the KG. To address these, we propose GrapeQA with two simple improvements on the WG: (i) Prominent Entities for Graph Augmentation identifies relevant text chunks from the QA pair and augments the WG with corresponding latent representations from the LM, and (ii) Context-Aware Node Pruning removes nodes that are less relevant to the QA pair. We evaluate our results on OpenBookQA, CommonsenseQA and MedQA-USMLE and see that GrapeQA shows consistent improvements over its LM + KG predecessor (QA-GNN in particular) and large improveme
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#20803;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;SUPMER&#65292;&#21253;&#25324;&#20803;&#26799;&#24230;&#27491;&#21017;&#21270;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#27867;&#21270;&#65292;&#36890;&#36807;&#38170;&#23450;&#30340;&#20803;&#35757;&#32451;&#20219;&#21153;&#21644;&#22522;&#20110;&#35838;&#31243;&#30340;&#20219;&#21153;&#22686;&#24378;&#20016;&#23500;&#20102;&#20219;&#21153;&#20998;&#24067;&#65292;&#35299;&#20915;&#20102;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#33391;&#22909;&#21021;&#22987;&#21270;&#36719;&#25552;&#31034;&#21644;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.12314</link><description>&lt;p&gt;
&#20855;&#26377;&#20803;&#26799;&#24230;&#27491;&#21017;&#21270;&#30340;&#33258;&#30417;&#30563;&#20803;&#25552;&#31034;&#23398;&#20064;&#29992;&#20110;&#23569;&#26679;&#26412;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Meta-Prompt Learning with Meta-Gradient Regularization for Few-shot Generalization. (arXiv:2303.12314v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12314
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#20803;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;SUPMER&#65292;&#21253;&#25324;&#20803;&#26799;&#24230;&#27491;&#21017;&#21270;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#27867;&#21270;&#65292;&#36890;&#36807;&#38170;&#23450;&#30340;&#20803;&#35757;&#32451;&#20219;&#21153;&#21644;&#22522;&#20110;&#35838;&#31243;&#30340;&#20219;&#21153;&#22686;&#24378;&#20016;&#23500;&#20102;&#20219;&#21153;&#20998;&#24067;&#65292;&#35299;&#20915;&#20102;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#33391;&#22909;&#21021;&#22987;&#21270;&#36719;&#25552;&#31034;&#21644;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#35843;&#25972;&#26159;&#19968;&#31181;&#21442;&#25968;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23427;&#23398;&#20064;&#36719;&#25552;&#31034;&#24182;&#20351;&#20923;&#32467;&#30340;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#29305;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#23613;&#31649;&#26377;&#25928;&#65292;&#20294;&#25552;&#31034;&#35843;&#25972;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#19968;&#26041;&#38754;&#20005;&#37325;&#20381;&#36182;&#20110;&#33391;&#22909;&#30340;&#36719;&#25552;&#31034;&#21021;&#22987;&#21270;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23427;&#24456;&#23481;&#26131;&#23548;&#33268;&#36807;&#24230;&#25311;&#21512;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#25110;&#30417;&#30563;&#20803;&#23398;&#20064;&#26469;&#21021;&#22987;&#21270;&#36719;&#25552;&#31034;&#65292;&#20294;&#23427;&#20204;&#19981;&#33021;&#23545;&#26410;&#35265;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#25968;&#25454;&#26377;&#25928;&#30340;&#27867;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#20197;&#19978;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#25105;&#30417;&#30563;&#20803;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#20803;&#26799;&#24230;&#27491;&#21017;&#21270;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#27867;&#21270;&#65288;SUPMER&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#32452;&#33258;&#30417;&#30563;&#38170;&#23450;&#30340;&#20803;&#35757;&#32451;&#20219;&#21153;&#65292;&#20855;&#26377;&#19981;&#21516;&#30340;&#20219;&#21153;&#26684;&#24335;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#35838;&#31243;&#30340;&#20219;&#21153;&#22686;&#24378;&#36827;&#19968;&#27493;&#20016;&#23500;&#20102;&#20219;&#21153;&#20998;&#24067;&#12290;&#28982;&#21518;&#23558;&#19968;&#31181;&#26032;&#30340;&#20803;&#26799;&#24230;&#27491;&#21017;&#21270;&#26041;&#27861;&#38598;&#25104;&#21040;&#20803;&#25552;&#31034;&#23398;&#20064;&#20013;&#12290;&#23427;&#20803;&#23398;&#20064;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#22914;&#20309;&#36716;&#25442;&#21407;&#22987;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt tuning is a parameter-efficient method, which learns soft prompts and conditions frozen language models to perform specific downstream tasks. Though effective, prompt tuning under few-shot settings on the one hand heavily relies on a good initialization of soft prompts. On the other hand, it can easily result in overfitting. Existing works leverage pre-training or supervised meta-learning to initialize soft prompts but they cannot data-efficiently generalize to unseen downstream tasks. To address the above problems, this paper proposes a novel Self-sUpervised meta-Prompt learning framework with meta-gradient Regularization for few-shot generalization (SUPMER). We first design a set of self-supervised anchor meta-training tasks with different task formats and further enrich the task distribution with curriculum-based task augmentation. Then a novel meta-gradient regularization method is integrated into meta-prompt learning. It meta-learns to transform the raw gradients during few
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;CTC/Attention&#26550;&#26500;&#21644;&#22810;&#29305;&#24449;&#34701;&#21512;&#32593;&#32476;&#30340;&#22303;&#32819;&#20854;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21644;&#29305;&#24449;&#25552;&#21462;&#22120;LSPC&#30340;&#20248;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#22303;&#32819;&#20854;&#35821;&#38899;&#35821;&#26009;&#24211;&#19978;&#21462;&#24471;&#33391;&#22909;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.12300</link><description>&lt;p&gt;
&#22522;&#20110;&#28151;&#21512;CTC/Attention&#26550;&#26500;&#21644;&#22810;&#29305;&#24449;&#34701;&#21512;&#32593;&#32476;&#30340;&#22303;&#32819;&#20854;&#35821;&#38899;&#35782;&#21035;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploring Turkish Speech Recognition via Hybrid CTC/Attention Architecture and Multi-feature Fusion Network. (arXiv:2303.12300v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;CTC/Attention&#26550;&#26500;&#21644;&#22810;&#29305;&#24449;&#34701;&#21512;&#32593;&#32476;&#30340;&#22303;&#32819;&#20854;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21644;&#29305;&#24449;&#25552;&#21462;&#22120;LSPC&#30340;&#20248;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#22303;&#32819;&#20854;&#35821;&#38899;&#35821;&#26009;&#24211;&#19978;&#21462;&#24471;&#33391;&#22909;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#25216;&#26415;&#36805;&#36895;&#21457;&#23637;&#12290;&#30001;&#20110;&#32570;&#20047;&#22303;&#32819;&#20854;&#35821;&#38899;&#25968;&#25454;&#65292;&#22303;&#32819;&#20854;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#24615;&#33021;&#36739;&#24046;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31995;&#21015;&#35821;&#38899;&#35782;&#21035;&#35843;&#20248;&#25216;&#26415;&#12290;&#39318;&#20808;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#37319;&#29992;&#23558;&#36895;&#24230;&#25200;&#21160;&#21644;&#22122;&#22768;&#28155;&#21152;&#30456;&#32467;&#21512;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#24182;&#23558;&#27874;&#26463;&#25628;&#32034;&#23485;&#24230;&#35774;&#32622;&#20026;16&#26102;&#65292;&#27169;&#22411;&#30340;&#24615;&#33021;&#26368;&#20339;&#12290;&#20854;&#27425;&#65292;&#20026;&#20805;&#20998;&#21033;&#29992;&#26377;&#25928;&#29305;&#24449;&#20449;&#24687;&#12289;&#25552;&#39640;&#29305;&#24449;&#25552;&#21462;&#20934;&#30830;&#24615;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;LSPC&#12290;&#24182;&#36890;&#36807;&#23558;LSPC&#21644;LiGRU&#32593;&#32476;&#30456;&#32467;&#21512;&#26500;&#25104;&#20849;&#20139;&#32534;&#30721;&#22120;&#32467;&#26500;&#23454;&#29616;&#27169;&#22411;&#21387;&#32553;&#12290;&#26368;&#21518;&#65292;&#22312;&#20197;&#19978;&#20004;&#28857;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#29305;&#24449;&#34701;&#21512;&#32593;&#32476;&#12290;&#35813;&#32593;&#32476;&#25552;&#20986;&#20102;&#28151;&#21512;CTC/Attention&#26550;&#26500;&#65292;&#20351;&#27169;&#22411;&#20860;&#20855;&#20004;&#32773;&#30340;&#20248;&#28857;&#65292;&#24182;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#19981;&#21516;&#30340;&#22768;&#23398;&#29305;&#24449;&#65292;&#27604;&#20351;&#29992;&#21333;&#19968;&#29305;&#24449;&#26356;&#20855;&#23454;&#39564;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#20316;&#32773;&#24320;&#21457;&#30340;&#22303;&#32819;&#20854;&#35821;&#38899;&#35821;&#26009;&#24211;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, End-to-End speech recognition technology based on deep learning has developed rapidly. Due to the lack of Turkish speech data, the performance of Turkish speech recognition system is poor. Firstly, this paper studies a series of speech recognition tuning technologies. The results show that the performance of the model is the best when the data enhancement technology combining speed perturbation with noise addition is adopted and the beam search width is set to 16. Secondly, to maximize the use of effective feature information and improve the accuracy of feature extraction, this paper proposes a new feature extractor LSPC. LSPC and LiGRU network are combined to form a shared encoder structure, and model compression is realized. The results show that the performance of LSPC is better than MSPC and VGGnet when only using Fbank features, and the WER is improved by 1.01% and 2.53% respectively. Finally, based on the above two points, a new multi-feature fusion network is pr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19968;&#32452; transformer &#27169;&#22411;&#65292;&#22312;&#26410;&#30693;&#30340;&#20013;&#25991;&#23383;&#31526;&#21629;&#21517;&#20219;&#21153;&#20013;&#65292;&#36825;&#20123;&#27169;&#22411;&#34920;&#29616;&#24471;&#19982;&#20154;&#31867;&#24456;&#30456;&#20284;&#65292;&#33021;&#22815;&#24456;&#22909;&#22320;&#25429;&#25417;&#20154;&#31867;&#23383;&#31526;&#21629;&#21517;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2303.12294</link><description>&lt;p&gt;
&#35780;&#20272;&#21464;&#25442;&#22120;&#27169;&#22411;&#21644;&#20154;&#31867;&#34892;&#20026;&#22312;&#20013;&#25991;&#23383;&#31526;&#21629;&#21517;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating Transformer Models and Human Behaviors on Chinese Character Naming. (arXiv:2303.12294v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19968;&#32452; transformer &#27169;&#22411;&#65292;&#22312;&#26410;&#30693;&#30340;&#20013;&#25991;&#23383;&#31526;&#21629;&#21517;&#20219;&#21153;&#20013;&#65292;&#36825;&#20123;&#27169;&#22411;&#34920;&#29616;&#24471;&#19982;&#20154;&#31867;&#24456;&#30456;&#20284;&#65292;&#33021;&#22815;&#24456;&#22909;&#22320;&#25429;&#25417;&#20154;&#31867;&#23383;&#31526;&#21629;&#21517;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35768;&#22810;&#23383;&#27597;&#35821;&#35328;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26469;&#35299;&#37322;&#20154;&#31867;&#30340;&#23383;&#32032;-&#38899;&#32032;&#26144;&#23556;&#36807;&#31243;&#12290;&#36825;&#20123;&#27169;&#22411;&#19981;&#20165;&#25104;&#21151;&#22320;&#23398;&#20064;&#20102;&#23383;&#27597;&#23383;&#31526;&#20018;&#21450;&#20854;&#21457;&#38899;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#32780;&#19988;&#36824;&#25429;&#25417;&#20102;&#20154;&#31867;&#22312;&#30701;&#26242;&#21333;&#35789;&#21629;&#21517;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19968;&#32452;&#21464;&#25442;&#22120;&#27169;&#22411;&#65292;&#24182;&#23558;&#23427;&#20204;&#30340;&#34920;&#29616;&#19982;&#20154;&#31867;&#22312;&#26410;&#30693;&#20013;&#25991;&#23383;&#31526;&#21629;&#21517;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#27169;&#22411;&#21644;&#20154;&#31867;&#30340;&#34892;&#20026;&#38750;&#24120;&#30456;&#20284;&#65292;&#23427;&#20204;&#22312;&#27599;&#20010;&#23383;&#31526;&#30340;&#20934;&#30830;&#24230;&#20998;&#24067;&#26041;&#38754;&#20855;&#26377;&#31867;&#20284;&#30340;&#20934;&#30830;&#24230;&#65292;&#24182;&#19988;&#22312;&#31572;&#26696;&#19978;&#26377;&#24456;&#22823;&#30340;&#37325;&#21472;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#30340;&#31572;&#26696;&#19982;&#20154;&#31867;&#30340;&#31572;&#26696;&#39640;&#24230;&#30456;&#20851;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#21464;&#25442;&#22120;&#27169;&#22411;&#33021;&#22815;&#24456;&#22909;&#22320;&#25429;&#25417;&#20154;&#31867;&#30340;&#23383;&#31526;&#21629;&#21517;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network models have been proposed to explain the grapheme-phoneme mapping process in humans for many alphabet languages. These models not only successfully learned the correspondence of the letter strings and their pronunciation, but also captured human behavior in nonce word naming tasks. How would the neural models perform for a non-alphabet language (e.g., Chinese) unknown character task? How well would the model capture human behavior? In this study, we evaluate a set of transformer models and compare their performances with human behaviors on an unknown Chinese character naming task. We found that the models and humans behaved very similarly, that they had similar accuracy distribution for each character, and had a substantial overlap in answers. In addition, the models' answers are highly correlated with humans' answers. These results suggested that the transformer models can well capture human's character naming behavior.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#32452;&#36890;&#29992;&#30340;&#35821;&#20041;&#35282;&#33394;&#26631;&#31614;&#65292;&#38024;&#23545;Universal Dependencies&#65292;&#24182;&#24212;&#29992;&#20110;&#22235;&#31181;&#35821;&#35328;&#30340;&#25968;&#25454;&#20013;&#12290;</title><link>http://arxiv.org/abs/2303.12220</link><description>&lt;p&gt;
&#28145;&#23618;&#21477;&#27861;&#20851;&#31995;&#30340;&#32479;&#19968;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
A Unified Taxonomy of Deep Syntactic Relations. (arXiv:2303.12220v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#32452;&#36890;&#29992;&#30340;&#35821;&#20041;&#35282;&#33394;&#26631;&#31614;&#65292;&#38024;&#23545;Universal Dependencies&#65292;&#24182;&#24212;&#29992;&#20110;&#22235;&#31181;&#35821;&#35328;&#30340;&#25968;&#25454;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#22810;&#20010;&#28145;&#23618;&#21477;&#27861;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#20986;&#19968;&#32452;&#26222;&#36866;&#30340;&#35821;&#20041;&#35282;&#33394;&#26631;&#31614;&#12290;&#25552;&#26696;&#32771;&#34385;&#20102;&#21508;&#31181;&#29702;&#35770;&#35821;&#35328;&#23398;&#35270;&#35282;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#20102;Meaning-Text Theory&#21644;Functional Generative Description&#26694;&#26550;&#12290;&#20026;&#20102;&#30740;&#31350;&#30340;&#30446;&#30340;&#65292;&#20351;&#29992;&#20102;&#26469;&#33258;&#22235;&#31181;&#35821;&#35328;&#30340;&#25968;&#25454;&#8212;&#8212;&#35199;&#29677;&#29273;&#35821;&#21644;&#21152;&#27888;&#32599;&#23612;&#20122;&#35821;&#65288;Taule&#31561;&#65292;2011&#65289;&#65292;&#25463;&#20811;&#35821;&#65288;Hajic&#31561;&#65292;2017&#65289;&#21644;&#33521;&#35821;&#65288;Hajic&#31561;&#65292;2012&#65289;&#12290;&#35813;&#25552;&#26696;&#38024;&#23545;Universal Dependencies&#65288;de Marneffe&#31561;&#65292;2021&#65289;&#65292;&#24182;&#36827;&#19968;&#27493;&#24847;&#22270;&#23558;&#36890;&#29992;&#30340;&#35821;&#20041;&#35282;&#33394;&#26631;&#31614;&#24212;&#29992;&#20110;UD&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper analyzes multiple deep-syntactic frameworks with the goal of creating a proposal for a set of universal semantic role labels. The proposal examines various theoretic linguistic perspectives and focuses on Meaning-Text Theory and Functional Generative Description frameworks.  For the purpose of this research, data from four languages is used -- Spanish and Catalan (Taule et al., 2011), Czech (Hajic et al., 2017), and English (Hajic et al., 2012). This proposal is oriented towards Universal Dependencies (de Marneffe et al., 2021) with a further intention of applying the universal semantic role labels to the UD data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986; MGVLT &#27169;&#22411;&#29992;&#20110;&#29983;&#25104;&#22270;&#20687;&#21644;&#25991;&#26412;&#24207;&#21015;&#65292;&#36890;&#36807;&#38750;&#33258;&#22238;&#24402;&#25513;&#30721;&#39044;&#27979;&#23454;&#29616;&#20102;&#21452;&#21521;&#19978;&#19979;&#25991;&#32534;&#30721;&#21644;&#24555;&#36895;&#35299;&#30721;&#31561;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2303.12208</link><description>&lt;p&gt;
MAGVLT: &#24102;&#25513;&#30721;&#30340;&#29983;&#25104;&#35270;&#35273;&#35821;&#35328;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
MAGVLT: Masked Generative Vision-and-Language Transformer. (arXiv:2303.12208v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986; MGVLT &#27169;&#22411;&#29992;&#20110;&#29983;&#25104;&#22270;&#20687;&#21644;&#25991;&#26412;&#24207;&#21015;&#65292;&#36890;&#36807;&#38750;&#33258;&#22238;&#24402;&#25513;&#30721;&#39044;&#27979;&#23454;&#29616;&#20102;&#21452;&#21521;&#19978;&#19979;&#25991;&#32534;&#30721;&#21644;&#24555;&#36895;&#35299;&#30721;&#31561;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22312;&#22810;&#27169;&#24577;&#22270;&#20687;&#25991;&#26412;&#25968;&#25454;&#19978;&#30340;&#29983;&#25104;&#24314;&#27169;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#21457;&#23637;&#65292;&#20294;&#20173;&#26377;&#35768;&#22810;&#38480;&#21046;&#65292;&#20363;&#22914;&#20165;&#29983;&#25104;&#19968;&#31181;&#27169;&#24577;&#30340;&#22266;&#23450;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#21487;&#20197;&#29983;&#25104;&#22270;&#20687;&#21644;&#25991;&#26412;&#24207;&#21015;&#30340;&#32479;&#19968;&#29983;&#25104;&#24335;&#35270;&#35273;&#35821;&#35328;&#65288;VL&#65289;&#27169;&#22411;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#33258;&#22238;&#24402;&#25513;&#30721;&#39044;&#27979;&#30340;&#29983;&#25104;VL&#21464;&#21387;&#22120;&#65292;&#21517;&#20026;MAGVLT&#65292;&#24182;&#23558;&#20854;&#19982;&#33258;&#22238;&#24402;&#29983;&#25104;VL&#21464;&#21387;&#22120;&#65288;ARGVLT&#65289;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#19982;ARGVLT&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;MAGVLT&#23454;&#29616;&#20102;&#21452;&#21521;&#19978;&#19979;&#25991;&#32534;&#30721;&#65292;&#36890;&#36807;&#36845;&#20195;&#32454;&#21270;&#30340;&#24182;&#34892;&#26631;&#35760;&#39044;&#27979;&#23454;&#29616;&#20102;&#24555;&#36895;&#35299;&#30721;&#65292;&#20855;&#26377;&#22270;&#20687;&#21644;&#25991;&#26412;&#22635;&#20805;&#31561;&#25193;&#23637;&#32534;&#36753;&#21151;&#33021;&#12290;&#20026;&#20102;&#20174;&#22836;&#24320;&#22987;&#20005;&#26684;&#35757;&#32451;&#25105;&#20204;&#30340;MAGVLT&#27169;&#22411;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#20174;&#22270;&#20687;&#21040;&#25991;&#26412;&#12289;&#20174;&#25991;&#26412;&#21040;&#22270;&#20687;&#12289;&#20197;&#21450;&#32852;&#21512;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#25513;&#30721;&#39044;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
While generative modeling on multimodal image-text data has been actively developed with large-scale paired datasets, there have been limited attempts to generate both image and text data by a single model rather than a generation of one fixed modality conditioned on the other modality. In this paper, we explore a unified generative vision-and-language (VL) model that can produce both images and text sequences. Especially, we propose a generative VL transformer based on the non-autoregressive mask prediction, named MAGVLT, and compare it with an autoregressive generative VL transformer (ARGVLT). In comparison to ARGVLT, the proposed MAGVLT enables bidirectional context encoding, fast decoding by parallel token predictions in an iterative refinement, and extended editing capabilities such as image and text infilling. For rigorous training of our MAGVLT with image-text pairs from scratch, we combine the image-to-text, text-to-image, and joint image-and-text mask prediction tasks. Moreove
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545; SemEval-2023 &#20219;&#21153; 6 &#24320;&#21457;&#30340; Legal-BERT-HSLN &#27169;&#22411;&#21644; Legal-LUKE &#27169;&#22411;&#65292;&#20854;&#20013; Legal-BERT-HSLN &#27169;&#22411;&#36890;&#36807;&#32771;&#34385;&#21477;&#20869;&#21644;&#21477;&#38388;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#20197;&#39044;&#27979;&#20462;&#36766;&#35282;&#33394;&#65292;Legal-LUKE &#27169;&#22411;&#26159;&#20855;&#26377;&#27861;&#24459;&#19978;&#19979;&#25991;&#21644;&#23454;&#20307;&#30693;&#35782;&#30340;&#27169;&#22411;&#65292;&#20197;&#35782;&#21035;&#27861;&#24459;&#23454;&#20307;&#12290;&#27169;&#22411;&#30456;&#27604;&#22522;&#32447;&#27169;&#22411;&#26356;&#20934;&#30830;&#65292;&#33021;&#22815;&#35299;&#20915;&#22312;&#20154;&#21475;&#20247;&#22810;&#30340;&#22269;&#23478;&#22788;&#29702;&#27861;&#24459;&#25991;&#20214;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.12135</link><description>&lt;p&gt;
&#21033;&#29992;&#19978;&#19979;&#25991;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#27861;&#24459;&#25991;&#20214;
&lt;/p&gt;
&lt;p&gt;
Understand Legal Documents with Contextualized Large Language Models. (arXiv:2303.12135v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545; SemEval-2023 &#20219;&#21153; 6 &#24320;&#21457;&#30340; Legal-BERT-HSLN &#27169;&#22411;&#21644; Legal-LUKE &#27169;&#22411;&#65292;&#20854;&#20013; Legal-BERT-HSLN &#27169;&#22411;&#36890;&#36807;&#32771;&#34385;&#21477;&#20869;&#21644;&#21477;&#38388;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#20197;&#39044;&#27979;&#20462;&#36766;&#35282;&#33394;&#65292;Legal-LUKE &#27169;&#22411;&#26159;&#20855;&#26377;&#27861;&#24459;&#19978;&#19979;&#25991;&#21644;&#23454;&#20307;&#30693;&#35782;&#30340;&#27169;&#22411;&#65292;&#20197;&#35782;&#21035;&#27861;&#24459;&#23454;&#20307;&#12290;&#27169;&#22411;&#30456;&#27604;&#22522;&#32447;&#27169;&#22411;&#26356;&#20934;&#30830;&#65292;&#33021;&#22815;&#35299;&#20915;&#22312;&#20154;&#21475;&#20247;&#22810;&#30340;&#22269;&#23478;&#22788;&#29702;&#27861;&#24459;&#25991;&#20214;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#21475;&#20247;&#22810;&#30340;&#22269;&#23478;&#65292;&#22914;&#21360;&#24230;&#65292;&#24453;&#22788;&#29702;&#30340;&#27861;&#24459;&#26696;&#20214;&#25968;&#37327;&#19981;&#26029;&#22686;&#21152;&#65292;&#36825;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#22823;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#26377;&#25928;&#30340;&#25216;&#26415;&#26469;&#22788;&#29702;&#21644;&#29702;&#35299;&#27861;&#24459;&#25991;&#20214;&#23558;&#38750;&#24120;&#26377;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#25105;&#20204;&#38024;&#23545; SemEval-2023 &#20219;&#21153; 6&#65288;Modi &#31561;&#20154;&#65292;2023&#65289;&#25152;&#24320;&#21457;&#30340;&#29702;&#35299;&#27861;&#24459;&#25991;&#26412;&#31995;&#32479;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#24320;&#21457;&#20102; Legal-BERT-HSLN &#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#21477;&#20869;&#21644;&#21477;&#38388;&#30340;&#32508;&#21512;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20197;&#39044;&#27979;&#20462;&#36766;&#35282;&#33394;&#65288;&#23376;&#20219;&#21153; A&#65289;&#65292;&#28982;&#21518;&#35757;&#32451;&#20986; Legal-LUKE &#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#27861;&#24459;&#19978;&#19979;&#25991;&#21270;&#21644;&#23454;&#20307;&#24863;&#30693;&#33021;&#21147;&#65292;&#20197;&#35782;&#21035;&#27861;&#24459;&#23454;&#20307;&#65288;&#23376;&#20219;&#21153; B&#65289;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#35774;&#35745;&#30340;&#27169;&#22411;&#27604;&#22522;&#32447;&#27169;&#22411;&#26356;&#20934;&#30830;&#65292;&#22914;&#22312;&#23376;&#20219;&#21153; B &#20013; F1 &#20540;&#25552;&#39640;&#20102;&#36798; 15.0%&#12290;&#25105;&#20204;&#22312;&#20219;&#21153;&#25490;&#34892;&#27036;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#34920;&#29616;&#65292;&#22914; 0.834 &#24494;&#24179;&#22343; F1 &#20540;&#65292;&#24182;&#22312;&#23376;&#20219;&#21153; A &#20013;&#25490;&#21517;&#31532; 5&#12290;
&lt;/p&gt;
&lt;p&gt;
The growth of pending legal cases in populous countries, such as India, has become a major issue. Developing effective techniques to process and understand legal documents is extremely useful in resolving this problem. In this paper, we present our systems for SemEval-2023 Task 6: understanding legal texts (Modi et al., 2023). Specifically, we first develop the Legal-BERT-HSLN model that considers the comprehensive context information in both intra- and inter-sentence levels to predict rhetorical roles (subtask A) and then train a Legal-LUKE model, which is legal-contextualized and entity-aware, to recognize legal entities (subtask B). Our evaluations demonstrate that our designed models are more accurate than baselines, e.g., with an up to 15.0% better F1 score in subtask B. We achieved notable performance in the task leaderboard, e.g., 0.834 micro F1 score, and ranked No.5 out of 27 teams in subtask A.
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#25913;&#36827;&#24341;&#36215;&#20102;&#20844;&#20247;&#24191;&#27867;&#20851;&#27880;&#12290;&#23427;&#20204;&#24191;&#27867;&#24212;&#29992;&#21450;&#20854;&#30495;&#23454;&#33021;&#21147;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#20294;&#21516;&#26102;&#20063;&#24102;&#26469;&#20102;&#23545;&#20854;&#21487;&#33021;&#30340;&#24694;&#24847;&#29992;&#36884;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#35201;&#27010;&#36848;&#20197;&#21450;&#22312;&#32593;&#32476;&#38450;&#24481;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.12132</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#19982;&#22312;&#32593;&#32476;&#38450;&#24481;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Fundamentals of Generative Large Language Models and Perspectives in Cyber-Defense. (arXiv:2303.12132v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12132
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#25913;&#36827;&#24341;&#36215;&#20102;&#20844;&#20247;&#24191;&#27867;&#20851;&#27880;&#12290;&#23427;&#20204;&#24191;&#27867;&#24212;&#29992;&#21450;&#20854;&#30495;&#23454;&#33021;&#21147;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#20294;&#21516;&#26102;&#20063;&#24102;&#26469;&#20102;&#23545;&#20854;&#21487;&#33021;&#30340;&#24694;&#24847;&#29992;&#36884;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#35201;&#27010;&#36848;&#20197;&#21450;&#22312;&#32593;&#32476;&#38450;&#24481;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#22312;2022&#24180;&#24213;&#21644;2023&#24180;&#21021;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#23588;&#20854;&#26159;&#24341;&#20837;&#20102;&#19982;&#29992;&#25143;&#26399;&#26395;&#30340;AI&#20132;&#20114;&#19968;&#33268;&#30340;&#27169;&#22411;&#65288;&#23545;&#35805;&#27169;&#22411;&#65289;&#12290;&#20154;&#20204;&#20851;&#27880;&#30340;&#28966;&#28857;&#21487;&#20197;&#35828;&#26159;GPT3&#27169;&#22411;&#30340;&#36825;&#31181;&#25913;&#36827;&#8212;&#8212;ChatGPT&#21450;&#20854;&#38543;&#21518;&#19982;&#36741;&#21161;&#21151;&#33021;&#38598;&#25104;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#20316;&#20026;Microsoft Bing&#30340;&#25628;&#32034;&#37096;&#20998;&#12290;&#23613;&#31649;&#27492;&#21069;&#24050;&#32463;&#25237;&#20837;&#20102;&#22823;&#37327;&#30740;&#31350;&#36827;&#34892;&#24320;&#21457;&#65292;&#20294;&#23427;&#20204;&#22312;&#21508;&#31181;&#26085;&#24120;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#21644;&#36866;&#29992;&#24615;&#20173;&#19981;&#28165;&#26970;&#19988;&#29421;&#31364;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#24182;&#19981;&#38656;&#35201;&#25216;&#26415;&#19987;&#19994;&#30693;&#35782;&#65292;&#36825;&#22312;&#30456;&#24403;&#22823;&#30340;&#31243;&#24230;&#19978;&#26159;&#36890;&#36807;&#23545;&#35805;&#24494;&#35843;&#23454;&#29616;&#30340;&#65292;&#25581;&#31034;&#20102;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#23427;&#20204;&#30495;&#23454;&#33021;&#21147;&#30340;&#33539;&#22260;&#12290;&#36825;&#24341;&#36215;&#20102;&#20844;&#20247;&#23545;&#20854;&#28508;&#22312;&#24212;&#29992;&#30340;&#20852;&#22859;&#21644;&#23545;&#20854;&#33021;&#21147;&#21450;&#21487;&#33021;&#30340;&#24694;&#24847;&#29992;&#36884;&#30340;&#25285;&#24551;&#12290;&#26412;&#32508;&#36848;&#26088;&#22312;&#25552;&#20379;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#35201;&#27010;&#36848;&#20197;&#21450;&#22312;&#32593;&#32476;&#38450;&#24481;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Language Models gained significant attention in late 2022 / early 2023, notably with the introduction of models refined to act consistently with users' expectations of interactions with AI (conversational models). Arguably the focal point of public attention has been such a refinement of the GPT3 model -- the ChatGPT and its subsequent integration with auxiliary capabilities, including search as part of Microsoft Bing. Despite extensive prior research invested in their development, their performance and applicability to a range of daily tasks remained unclear and niche. However, their wider utilization without a requirement for technical expertise, made in large part possible through conversational fine-tuning, revealed the extent of their true capabilities in a real-world environment. This has garnered both public excitement for their potential applications and concerns about their capabilities and potential malicious uses. This review aims to provide a brief overview of th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#26631;&#39064;&#35780;&#20272;&#25351;&#26631;PAC-S&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#26631;&#39064;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#25351;&#26631;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#65307;&#28304;&#20195;&#30721;&#21644;&#35757;&#32451;&#27169;&#22411;&#24050;&#32463;&#20844;&#24320;&#12290;</title><link>http://arxiv.org/abs/2303.12112</link><description>&lt;p&gt;
&#22522;&#20110;&#27491;&#26679;&#26412;&#22686;&#24378;&#23545;&#27604;&#23398;&#20064;&#30340;&#22270;&#20687;&#35270;&#39057;&#26631;&#39064;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Positive-Augmented Constrastive Learning for Image and Video Captioning Evaluation. (arXiv:2303.12112v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#26631;&#39064;&#35780;&#20272;&#25351;&#26631;PAC-S&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#26631;&#39064;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#25351;&#26631;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#65307;&#28304;&#20195;&#30721;&#21644;&#35757;&#32451;&#27169;&#22411;&#24050;&#32463;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;CLIP&#27169;&#22411;&#22312;&#24456;&#22810;&#36328;&#27169;&#24577;&#20219;&#21153;&#19978;&#37117;&#38750;&#24120;&#26377;&#25928;&#65292;&#21253;&#25324;&#20174;&#35270;&#35273;&#21644;&#35821;&#35328;&#32467;&#26500;&#20013;&#29983;&#25104;&#30340;&#26631;&#39064;&#35780;&#20272;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23545;&#27604;&#24230;&#30340;&#22270;&#20687;&#26631;&#39064;&#35780;&#20272;&#25351;&#26631;&#37197;&#26041;&#65292;&#21363;&#27491;&#26679;&#26412;&#22686;&#24378;&#30340;&#23545;&#27604;&#24230;&#23398;&#20064;&#20998;&#25968;&#65288;PAC-S&#65289;&#65292;&#20197;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#24335;&#32479;&#19968;&#20102;&#23545;&#27604;&#24230;&#35270;&#35273;-&#35821;&#20041;&#31354;&#38388;&#30340;&#23398;&#20064;&#21644;&#31574;&#23637;&#25968;&#25454;&#19978;&#29983;&#25104;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#28155;&#21152;&#12290;&#36328;&#36234;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26032;&#25351;&#26631;&#22312;&#22270;&#20687;&#21644;&#35270;&#39057;&#19978;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#26368;&#39640;&#65292;&#20248;&#20110;&#29616;&#26377;&#21442;&#32771;&#25351;&#26631;&#65288;&#22914;CIDEr&#21644;SPICE&#65289;&#21644;&#26080;&#21442;&#32771;&#25351;&#26631;&#65288;&#22914;CLIP-Score&#65289;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#27969;&#34892;&#30340;&#22270;&#20687;&#26631;&#39064;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#37319;&#29992;&#19981;&#21516;&#36328;&#27169;&#24577;&#29305;&#24449;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#28304;&#20195;&#30721;&#21644;&#35757;&#32451;&#27169;&#22411;&#26159;&#20844;&#24320;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The CLIP model has been recently proven to be very effective for a variety of cross-modal tasks, including the evaluation of captions generated from vision-and-language architectures. In this paper, we propose a new recipe for a contrastive-based evaluation metric for image captioning, namely Positive-Augmented Contrastive learning Score (PAC-S), that in a novel way unifies the learning of a contrastive visual-semantic space with the addition of generated images and text on curated data. Experiments spanning several datasets demonstrate that our new metric achieves the highest correlation with human judgments on both images and videos, outperforming existing reference-based metrics like CIDEr and SPICE and reference-free metrics like CLIP-Score. Finally, we test the system-level correlation of the proposed metric when considering popular image captioning approaches, and assess the impact of employing different cross-modal features. Our source code and trained models are publicly availa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#22312;&#38646;-shot&#23398;&#20064;&#29615;&#22659;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#35780;&#20272;&#25919;&#27835;&#23478;&#30340;&#24847;&#35782;&#24418;&#24577;&#65292;&#20026;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#25919;&#27835;&#21151;&#33021;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2303.12057</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#38646;-shot&#23398;&#20064;&#29615;&#22659;&#19979;&#29992;&#20110;&#35780;&#20272;&#25919;&#27835;&#23478;&#30340;&#24847;&#35782;&#24418;&#24577;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Can Be Used to Estimate the Ideologies of Politicians in a Zero-Shot Learning Setting. (arXiv:2303.12057v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12057
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22312;&#38646;-shot&#23398;&#20064;&#29615;&#22659;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#35780;&#20272;&#25919;&#27835;&#23478;&#30340;&#24847;&#35782;&#24418;&#24577;&#65292;&#20026;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#25919;&#27835;&#21151;&#33021;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#34164;&#21547;&#30340;&#22823;&#37327;&#30693;&#35782;&#21487;&#20197;&#20026;&#31038;&#20250;&#31185;&#23398;&#20013;&#30340;&#21487;&#35266;&#27979;&#24615;&#21644;&#27979;&#37327;&#38382;&#39064;&#25552;&#20379;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20854;&#20013;&#19968;&#31181;&#27169;&#22411;&#22312;&#34913;&#37327;&#31435;&#27861;&#32773;&#30340;&#28508;&#22312;&#24847;&#35782;&#24418;&#24577;&#26041;&#38754;&#30340;&#25928;&#29992;&#65292;&#36825;&#26377;&#21161;&#20110;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#22609;&#36896;&#25919;&#31574;&#30340;&#25919;&#27835;&#21151;&#33021;&#65292;&#20197;&#21450;&#25919;&#27835;&#34892;&#20026;&#32773;&#20195;&#34920;&#20854;&#36873;&#27665;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#31034;ChatGPT&#22312;&#20004;&#20004;&#27604;&#36739;&#20013;&#36873;&#25321;&#26356;&#33258;&#30001;&#27966;&#65288;&#25110;&#20445;&#23432;&#27966;&#65289;&#30340;&#21442;&#35758;&#21592;&#65292;&#23558;&#31532;116&#23626;&#32654;&#22269;&#22269;&#20250;&#30340;&#21442;&#35758;&#21592;&#25353;&#29031;&#33258;&#30001;&#27966;-&#20445;&#23432;&#27966;&#30340;&#20809;&#35889;&#36827;&#34892;&#32553;&#25918;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LLM&#22312;&#37325;&#22797;&#36845;&#20195;&#20013;&#20135;&#29983;&#20102;&#31283;&#23450;&#30340;&#31572;&#26696;&#65292;&#27809;&#26377;&#20135;&#29983;&#24187;&#35273;&#65292;&#24182;&#19988;&#19981;&#20165;&#20165;&#26159;&#20174;&#21333;&#19968;&#26469;&#28304;&#20013;&#22797;&#21046;&#20449;&#24687;&#12290;&#36825;&#20010;&#26032;&#23610;&#24230;&#19982;&#29616;&#26377;&#30340;&#33258;&#30001;&#27966;-&#20445;&#23432;&#27966;&#23610;&#24230;&#65288;&#22914;NOMINATE&#65289;&#24378;&#30456;&#20851;&#65292;&#20294;&#20063;&#22312;&#20960;&#20010;&#37325;&#35201;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#65292;&#27604;&#22914;&#27491;&#30830;&#23450;&#20301;&#19968;&#20123;&#36335;&#24452;&#20381;&#36182;&#21644;&#33258;&#30001;&#27966;&#27966;&#21035;&#30340;&#35758;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;
The mass aggregation of knowledge embedded in large language models (LLMs) holds the promise of new solutions to problems of observability and measurement in the social sciences. We examine the utility of one such model for a particularly difficult measurement task: measuring the latent ideology of lawmakers, which allows us to better understand functions that are core to democracy, such as how politics shape policy and how political actors represent their constituents. We scale the senators of the 116th United States Congress along the liberal-conservative spectrum by prompting ChatGPT to select the more liberal (or conservative) senator in pairwise comparisons. We show that the LLM produced stable answers across repeated iterations, did not hallucinate, and was not simply regurgitating information from a single source. This new scale strongly correlates with pre-existing liberal-conservative scales such as NOMINATE, but also differs in several important ways, such as correctly placin
&lt;/p&gt;</description></item><item><title>EVA-02&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#19979;&#19968;&#20195;&#35270;&#35273;&#34920;&#24449;&#65292;&#20855;&#26377;&#37325;&#24314;&#24378;&#22823;&#19988;&#31283;&#20581;&#29305;&#24449;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#21508;&#31181;&#20195;&#34920;&#24615;&#35270;&#35273;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#21516;&#26102;&#20351;&#29992;&#30340;&#21442;&#25968;&#21644;&#35745;&#31639;&#39044;&#31639;&#26174;&#33879;&#36739;&#23569;&#12290;</title><link>http://arxiv.org/abs/2303.11331</link><description>&lt;p&gt;
EVA-02&#65306;&#26032;&#19990;&#32426;&#31119;&#38899;&#25112;&#22763;&#30340;&#35270;&#35273;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
EVA-02: A Visual Representation for Neon Genesis. (arXiv:2303.11331v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11331
&lt;/p&gt;
&lt;p&gt;
EVA-02&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#19979;&#19968;&#20195;&#35270;&#35273;&#34920;&#24449;&#65292;&#20855;&#26377;&#37325;&#24314;&#24378;&#22823;&#19988;&#31283;&#20581;&#29305;&#24449;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#21508;&#31181;&#20195;&#34920;&#24615;&#35270;&#35273;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#21516;&#26102;&#20351;&#29992;&#30340;&#21442;&#25968;&#21644;&#35745;&#31639;&#39044;&#31639;&#26174;&#33879;&#36739;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21457;&#24067;EVA-02&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#19979;&#19968;&#20195;&#35270;&#35273;&#34920;&#24449;&#65292;&#32463;&#36807;&#39044;&#35757;&#32451;&#65292;&#36890;&#36807;&#25513;&#34109;&#22270;&#20687;&#24314;&#27169;&#37325;&#24314;&#24378;&#22823;&#19988;&#31283;&#20581;&#30340;&#29305;&#24449;&#65292;&#23454;&#29616;&#35821;&#35328;&#21644;&#35270;&#35273;&#30340;&#23545;&#40784;&#12290;&#20351;&#29992;&#26356;&#26032;&#30340;&#26222;&#36890;Transformer&#26550;&#26500;&#20197;&#21450;&#26469;&#33258;&#24320;&#25918;&#19988;&#26131;&#20110;&#35775;&#38382;&#30340;&#24040;&#22411;CLIP&#35270;&#35273;&#32534;&#30721;&#22120;&#30340;&#24191;&#27867;&#39044;&#35757;&#32451;&#65292;EVA-02&#22312;&#21508;&#31181;&#20195;&#34920;&#24615;&#35270;&#35273;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#21516;&#26102;&#20351;&#29992;&#30340;&#21442;&#25968;&#21644;&#35745;&#31639;&#39044;&#31639;&#26174;&#33879;&#36739;&#23569;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20165;&#20351;&#29992;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20855;&#26377;304M&#21442;&#25968;&#30340;EVA-02&#22312;ImageNet-1K val&#38598;&#19978;&#23454;&#29616;&#20102;&#24778;&#20154;&#30340;90.0&#24494;&#35843;top-1&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;EVA-02-CLIP&#22312;ImageNet-1K&#19978;&#30340;&#38646;&#26679;&#26412;top-1&#21487;&#36798;80.4&#65292;&#32988;&#36807;&#20102;&#20197;&#21069;&#26368;&#22823;&#19988;&#26368;&#22909;&#30340;&#24320;&#28304;CLIP&#65292;&#20165;&#20351;&#29992;&#20102;&#32422;1/6&#30340;&#21442;&#25968;&#21644;&#22270;&#20687;&#25991;&#26412;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#22235;&#31181;EVA-02&#21464;&#20307;&#65292;&#20854;&#27169;&#22411;&#22823;&#23567;&#21508;&#19981;&#30456;&#21516;&#65292;&#33539;&#22260;&#20174;6M&#21040;304M&#21442;&#25968;&#65292;&#22343;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We launch EVA-02, a next-generation Transformer-based visual representation pre-trained to reconstruct strong and robust language-aligned vision features via masked image modeling. With an updated plain Transformer architecture as well as extensive pre-training from an open &amp; accessible giant CLIP vision encoder, EVA-02 demonstrates superior performance compared to prior state-of-the-art approaches across various representative vision tasks, while utilizing significantly fewer parameters and compute budgets. Notably, using exclusively publicly accessible training data, EVA-02 with only 304M parameters achieves a phenomenal 90.0 fine-tuning top-1 accuracy on ImageNet-1K val set. Additionally, our EVA-02-CLIP can reach up to 80.4 zero-shot top-1 on ImageNet-1K, outperforming the previous largest &amp; best open-sourced CLIP with only ~1/6 parameters and ~1/6 image-text training data. We offer four EVA-02 variants in various model sizes, ranging from 6M to 304M parameters, all with impressive
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20381;&#36182;&#24615;&#24314;&#27169;&#26041;&#27861;&#65292;&#30001;&#24773;&#24863;&#24815;&#24615;&#21644;&#24863;&#26579;&#39537;&#21160;&#65288;EmotionIC&#65289;&#65292;&#29992;&#20110;&#22312;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#31867;&#32423;&#21035;&#19978;&#36827;&#34892;&#20250;&#35805;&#24773;&#24863;&#35782;&#21035;&#12290;&#35774;&#35745;&#20102;&#22810;&#39033;&#20855;&#20307;&#26041;&#27861;&#65292;&#21253;&#25324;&#36523;&#20221;&#25513;&#30721;&#22810;&#22836;&#27880;&#24847;&#65288;IM-MHA&#65289;&#21644;&#22522;&#20110;&#23545;&#35805;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;(DialogGRU)&#65292;&#20197;&#25235;&#21462;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.11117</link><description>&lt;p&gt;
EmotionIC&#65306;&#22522;&#20110;&#24773;&#24863;&#24815;&#24615;&#21644;&#24863;&#26579;&#30340;&#20381;&#36182;&#24314;&#27169;&#21487;&#29992;&#20110;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
EmotionIC: Emotional Inertia and Contagion-driven Dependency Modelling for Emotion Recognition in Conversation. (arXiv:2303.11117v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11117
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20381;&#36182;&#24615;&#24314;&#27169;&#26041;&#27861;&#65292;&#30001;&#24773;&#24863;&#24815;&#24615;&#21644;&#24863;&#26579;&#39537;&#21160;&#65288;EmotionIC&#65289;&#65292;&#29992;&#20110;&#22312;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#31867;&#32423;&#21035;&#19978;&#36827;&#34892;&#20250;&#35805;&#24773;&#24863;&#35782;&#21035;&#12290;&#35774;&#35745;&#20102;&#22810;&#39033;&#20855;&#20307;&#26041;&#27861;&#65292;&#21253;&#25324;&#36523;&#20221;&#25513;&#30721;&#22810;&#22836;&#27880;&#24847;&#65288;IM-MHA&#65289;&#21644;&#22522;&#20110;&#23545;&#35805;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;(DialogGRU)&#65292;&#20197;&#25235;&#21462;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38543;&#30528;&#20154;&#26426;&#30028;&#38754;&#25216;&#26415;&#30340;&#36827;&#27493;&#21644;&#23454;&#26045;&#65292;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#35782;&#21035;&#65288;ERC&#65289;&#21560;&#24341;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#24314;&#27169;&#26041;&#27861;&#22312;&#20840;&#23616;&#21644;&#23616;&#37096;&#19978;&#19979;&#25991;&#20381;&#36182;&#26041;&#38754;&#20002;&#22833;&#20102;&#20381;&#36182;&#20449;&#24687;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#19988;&#22312;&#20998;&#31867;&#32423;&#21035;&#19981;&#32771;&#34385;&#19978;&#19979;&#25991;&#20381;&#36182;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20381;&#36182;&#24615;&#24314;&#27169;&#26041;&#27861;&#65292;&#30001;&#24773;&#24863;&#24815;&#24615;&#21644;&#24863;&#26579;&#39537;&#21160;&#65288;EmotionIC&#65289;&#65292;&#29992;&#20110;&#22312;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#31867;&#32423;&#21035;&#19978;&#36827;&#34892;&#20250;&#35805;&#24773;&#24863;&#35782;&#21035;&#12290;&#22312;&#29305;&#24449;&#25552;&#21462;&#32423;&#21035;&#65292;&#25105;&#20204;&#35774;&#35745;&#30340;&#36523;&#20221;&#25513;&#30721;&#22810;&#22836;&#27880;&#24847;&#65288;IM-MHA&#65289;&#25429;&#25417;&#23545;&#35805;&#20013;&#22522;&#20110;&#36523;&#20221;&#30340;&#38271;&#36317;&#31163;&#19978;&#19979;&#25991;&#65292;&#20197;&#21253;&#21547;&#19981;&#21516;&#21442;&#19982;&#32773;&#30340;&#19981;&#21516;&#24433;&#21709;&#26500;&#24314;&#20840;&#23616;&#24773;&#24863;&#27675;&#22260;&#65292;&#32780;&#35774;&#35745;&#30340;&#22522;&#20110;&#23545;&#35805;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;(DialogGRU)&#21017;&#32858;&#21512;&#20102;&#20108;&#20803;&#23545;&#35805;&#30340;&#24773;&#24863;&#20542;&#21521;&#65292;&#24182;&#24212;&#29992;&#20110;&#20998;&#31867;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion Recognition in Conversation (ERC) has attracted growing attention in recent years as a result of the advancement and implementation of human-computer interface technologies. However, previous approaches to modeling global and local context dependencies lost the diversity of dependency information and do not take the context dependency into account at the classification level. In this paper, we propose a novel approach to dependency modeling driven by Emotional Inertia and Contagion (EmotionIC) for conversational emotion recognition at the feature extraction and classification levels. At the feature extraction level, our designed Identity Masked Multi-head Attention (IM-MHA) captures the identity-based long-distant context in the dialogue to contain the diverse influence of different participants and construct the global emotional atmosphere, while the devised Dialogue-based Gate Recurrent Unit (DialogGRU) that aggregates the emotional tendencies of dyadic dialogue is applied to
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#31890;&#24230;&#20013;&#25991;BERT&#65288;MigBERT&#65289;&#65292;&#21516;&#26102;&#32771;&#34385;&#23383;&#31526;&#21644;&#21333;&#35789;&#65292;&#20197;&#26356;&#22909;&#22320;&#34920;&#29616;&#21333;&#35789;&#30340;&#35821;&#20041;&#12290;&#23454;&#39564;&#35777;&#26126;MigBERT&#22312;&#21508;&#31181;&#20013;&#25991;NLP&#20219;&#21153;&#19978;&#22343;&#21487;&#23454;&#29616;&#26032;&#30340;SOTA&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.10893</link><description>&lt;p&gt;
&#23383;&#31526;&#65292;&#35789;&#36824;&#26159;&#20004;&#32773;&#20860;&#22791;&#65311;&#8212;&#8212;&#37325;&#35775;&#20013;&#25991;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#35789;&#31890;&#24230;
&lt;/p&gt;
&lt;p&gt;
Character, Word, or Both? Revisiting the Segmentation Granularity for Chinese Pre-trained Language Models. (arXiv:2303.10893v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10893
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#31890;&#24230;&#20013;&#25991;BERT&#65288;MigBERT&#65289;&#65292;&#21516;&#26102;&#32771;&#34385;&#23383;&#31526;&#21644;&#21333;&#35789;&#65292;&#20197;&#26356;&#22909;&#22320;&#34920;&#29616;&#21333;&#35789;&#30340;&#35821;&#20041;&#12290;&#23454;&#39564;&#35777;&#26126;MigBERT&#22312;&#21508;&#31181;&#20013;&#25991;NLP&#20219;&#21153;&#19978;&#22343;&#21487;&#23454;&#29616;&#26032;&#30340;SOTA&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#25913;&#36827;&#12290;&#22823;&#22810;&#25968;&#20013;&#25991;PLMs&#20165;&#23558;&#36755;&#20837;&#25991;&#26412;&#35270;&#20026;&#19968;&#31995;&#21015;&#23383;&#31526;&#65292;&#24182;&#23436;&#20840;&#24573;&#30053;&#21333;&#35789;&#20449;&#24687;&#12290;&#23613;&#31649;&#25972;&#35789;&#36974;&#30422;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#21333;&#35789;&#20013;&#30340;&#35821;&#20041;&#20173;&#28982;&#26080;&#27861;&#33391;&#22909;&#22320;&#34920;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#20013;&#25991;PLMs&#30340;&#20998;&#35789;&#31890;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#31890;&#24230;&#30340;&#20013;&#25991;BERT&#65288;MigBERT&#65289;&#65292;&#21516;&#26102;&#32771;&#34385;&#23383;&#31526;&#21644;&#21333;&#35789;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#23398;&#20064;&#23383;&#31526;&#21644;&#21333;&#35789;&#32423;&#34920;&#31034;&#30340;&#30446;&#26631;&#20989;&#25968;&#12290;&#25105;&#20204;&#23545;&#21508;&#31181;&#20013;&#25991;NLP&#20219;&#21153;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#20197;&#35780;&#20272;&#29616;&#26377;&#30340;PLMs&#21644;&#25152;&#25552;&#20986;&#30340;MigBERT&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MigBERT&#22312;&#25152;&#26377;&#36825;&#20123;&#20219;&#21153;&#20013;&#22343;&#23454;&#29616;&#20102;&#26032;&#30340;SOTA&#24615;&#33021;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#21333;&#35789;&#22312;&#35821;&#20041;&#19978;&#27604;&#23383;&#31526;&#26356;&#20016;&#23500;&#12290;&#26356;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;MigBERT&#20063;&#21487;&#20197;&#22312;&#26085;&#35821;&#20013;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models (PLMs) have shown marvelous improvements across various NLP tasks. Most Chinese PLMs simply treat an input text as a sequence of characters, and completely ignore word information. Although Whole Word Masking can alleviate this, the semantics in words is still not well represented. In this paper, we revisit the segmentation granularity of Chinese PLMs. We propose a mixed-granularity Chinese BERT (MigBERT) by considering both characters and words. To achieve this, we design objective functions for learning both character and word-level representations. We conduct extensive experiments on various Chinese NLP tasks to evaluate existing PLMs as well as the proposed MigBERT. Experimental results show that MigBERT achieves new SOTA performance on all these tasks. Further analysis demonstrates that words are semantically richer than characters. More interestingly, we show that MigBERT also works with Japanese. Our code and model have been released here~\footnote{htt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#36830;&#32493;&#25552;&#31034;&#21644;&#25277;&#21462;&#24335;&#38382;&#31572;&#30340;&#26041;&#27861;&#20174;Twitter&#20013;&#25552;&#21462;COVID-19&#20107;&#20214;&#65292;&#22312;COVID-19&#20107;&#20214;&#25554;&#27133;&#20013;&#36798;&#21040;&#20102;5%&#20197;&#19978;&#30340;F1&#24471;&#20998;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2303.10659</link><description>&lt;p&gt;
&#36890;&#36807;&#36830;&#32493;&#25552;&#31034;&#30340;&#25277;&#21462;&#24335;&#38382;&#31572;&#20174;Twitter&#20013;&#25552;&#21462;COVID-19&#20107;&#20214;
&lt;/p&gt;
&lt;p&gt;
COVID-19 event extraction from Twitter via extractive question answering with continuous prompts. (arXiv:2303.10659v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#36830;&#32493;&#25552;&#31034;&#21644;&#25277;&#21462;&#24335;&#38382;&#31572;&#30340;&#26041;&#27861;&#20174;Twitter&#20013;&#25552;&#21462;COVID-19&#20107;&#20214;&#65292;&#22312;COVID-19&#20107;&#20214;&#25554;&#27133;&#20013;&#36798;&#21040;&#20102;5%&#20197;&#19978;&#30340;F1&#24471;&#20998;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;COVID-19&#24109;&#21367;&#20840;&#29699;&#65292;&#31038;&#20132;&#23186;&#20307;&#20998;&#26512;&#21487;&#20197;&#22686;&#24378;&#20256;&#32479;&#35843;&#26597;&#65292;&#35780;&#20272;&#35813;&#22823;&#27969;&#34892;&#30340;&#28436;&#21464;&#24773;&#20917;&#65292;&#24182;&#25429;&#25417;&#26377;&#21161;&#20110;&#21355;&#29983;&#26426;&#26500;&#24212;&#23545;&#30340;&#28040;&#36153;&#32773;&#35758;&#35770;&#65292;&#19968;&#33324;&#28041;&#21450;&#25366;&#25496;&#25552;&#21040;&#26816;&#27979;&#21576;&#38451;&#24615;&#25110;&#20851;&#20110;&#39044;&#38450;&#25110;&#27835;&#30103;&#36873;&#39033;&#30340;&#35752;&#35770;&#30340;&#25259;&#38706;&#20107;&#20214;&#12290;&#35813;&#35770;&#25991;&#23558;&#20107;&#20214;&#25552;&#21462;&#38382;&#39064;&#36716;&#21270;&#20026;&#25277;&#21462;&#24335;&#38382;&#31572;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36830;&#32493;&#25552;&#31034;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#22312;&#20849;&#20139;&#20219;&#21153;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;COVID-19&#20107;&#20214;&#27133;&#30340;&#25152;&#26377;&#32454;&#33410;&#37117;&#33021;&#36798;&#21040;5%&#20197;&#19978;&#30340;&#24494;&#24179;&#22343;F1&#24471;&#20998;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;&#65292;&#36830;&#32493;&#25552;&#31034;&#23545;&#25805;&#20316;&#32467;&#26524;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
As COVID-19 ravages the world, social media analytics could augment traditional surveys in assessing how the pandemic evolves and capturing consumer chatter that could help healthcare agencies in addressing it. This typically involves mining disclosure events that mention testing positive for the disease or discussions surrounding perceptions and beliefs in preventative or treatment options. The 2020 shared task on COVID-19 event extraction (conducted as part of the W-NUT workshop during the EMNLP conference) introduced a new Twitter dataset for benchmarking event extraction from COVID-19 tweets. In this paper, we cast the problem of event extraction as extractive question answering using recent advances in continuous prompting in language models. On the shared task test dataset, our approach leads to over 5% absolute micro-averaged F1-score improvement over prior best results, across all COVID-19 event slots. Our ablation study shows that continuous prompts have a major impact on the 
&lt;/p&gt;</description></item><item><title>ChatGPT&#22312;&#31639;&#27861;&#21644;&#25968;&#25454;&#32467;&#26500;&#32771;&#35797;&#20013;&#21462;&#24471;20.5&#20998;&#30340;&#25104;&#32489;&#65292;&#34920;&#29616;&#20986;&#33021;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22823;&#23398;&#32771;&#35797;&#20013;&#25104;&#21151;&#65292;&#20294;&#19981;&#33021;&#35828;&#26126;&#20854;&#23545;&#35745;&#31639;&#26426;&#31185;&#23398;&#26377;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2303.09461</link><description>&lt;p&gt;
ChatGPT&#21442;&#21152;&#35745;&#31639;&#26426;&#31185;&#23398;&#32771;&#35797;
&lt;/p&gt;
&lt;p&gt;
ChatGPT Participates in a Computer Science Exam. (arXiv:2303.09461v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09461
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#22312;&#31639;&#27861;&#21644;&#25968;&#25454;&#32467;&#26500;&#32771;&#35797;&#20013;&#21462;&#24471;20.5&#20998;&#30340;&#25104;&#32489;&#65292;&#34920;&#29616;&#20986;&#33021;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22823;&#23398;&#32771;&#35797;&#20013;&#25104;&#21151;&#65292;&#20294;&#19981;&#33021;&#35828;&#26126;&#20854;&#23545;&#35745;&#31639;&#26426;&#31185;&#23398;&#26377;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35201;&#27714;ChatGPT&#21442;&#21152;&#8220;&#31639;&#27861;&#21644;&#25968;&#25454;&#32467;&#26500;&#8221;&#30340;&#26412;&#31185;&#35745;&#31639;&#26426;&#31185;&#23398;&#32771;&#35797;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#31243;&#24207;&#22312;&#25972;&#20010;&#32771;&#35797;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#23558;&#20854;&#31572;&#26696;&#25163;&#21160;&#22797;&#21046;&#21040;&#32771;&#35797;&#31572;&#39064;&#32440;&#19978;&#65292;&#19982;&#20854;&#20182;200&#21517;&#23398;&#29983;&#19968;&#36215;&#36827;&#34892;&#21311;&#21517;&#35780;&#20998;&#12290;&#25105;&#20204;&#21457;&#29616;ChatGPT&#21193;&#24378;&#36890;&#36807;&#20102;&#32771;&#35797;&#65292;&#33719;&#24471;&#20102;40&#20998;&#20013;&#30340;20.5&#20998;&#12290;&#36825;&#20010;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#30830;&#23454;&#21487;&#20197;&#22312;&#20687;&#22823;&#23398;&#32771;&#35797;&#36825;&#26679;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#20013;&#25104;&#21151;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#32771;&#35797;&#20013;&#30340;&#20219;&#21153;&#22312;&#32467;&#26500;&#19978;&#19982;&#20854;&#20182;&#22312;&#32447;&#21487;&#25214;&#21040;&#30340;&#32771;&#35797;&#21367;&#12289;&#23436;&#25104;&#30340;&#20316;&#19994;&#38382;&#39064;&#21644;&#25945;&#23398;&#26448;&#26009;&#30456;&#20284;&#12290;&#22240;&#27492;&#65292;&#20174;&#36825;&#20010;&#23454;&#39564;&#20013;&#24471;&#20986;ChatGPT&#26377;&#20219;&#20309;&#35745;&#31639;&#26426;&#31185;&#23398;&#29702;&#35299;&#30340;&#32467;&#35770;&#26159;&#20026;&#26102;&#36807;&#26089;&#30340;&#12290;&#25105;&#20204;&#19982;ChatGPT&#30340;&#35848;&#35805;&#35760;&#24405;&#21487;&#20197;&#22312;\url{https://github.com/tml-tuebingen/chatgpt-algorithm-exam}&#19978;&#25214;&#21040;&#65292;&#25972;&#20010;&#35780;&#20998;&#32771;&#35797;&#22312;&#26412;&#25991;&#30340;&#38468;&#24405;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
We asked ChatGPT to participate in an undergraduate computer science exam on ''Algorithms and Data Structures''. We evaluated the program on the entire exam as posed to the students. We hand-copied its answers onto an exam sheet, which was subsequently graded in a blind setup alongside those of 200 participating students. We find that ChatGPT narrowly passed the exam, obtaining 20.5 out of 40 points. This impressive performance indicates that ChatGPT can indeed succeed in challenging tasks like university exams. At the same time, the tasks in our exam are structurally similar to those on other exams, solved homework problems, and teaching materials that can be found online. Therefore, it would be premature to conclude from this experiment that ChatGPT has any understanding of computer science. The transcript of our conversation with ChatGPT is available at \url{https://github.com/tml-tuebingen/chatgpt-algorithm-exam}, and the entire graded exam is in the appendix of this paper.
&lt;/p&gt;</description></item><item><title>UPRISE&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#26816;&#32034;&#22120;&#65292;&#21487;&#33258;&#21160;&#20026;&#32473;&#23450;&#30340;&#38646;&#26679;&#26412;&#20219;&#21153;&#36755;&#20837;&#26816;&#32034;&#25552;&#31034;&#65292;&#20174;&#32780;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#35780;&#20272;&#12290;&#23427;&#36890;&#36807;&#36328;&#20219;&#21153;&#21644;&#36328;&#27169;&#22411;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;&#20854;&#36890;&#29992;&#24615;&#21644;&#28508;&#21147;&#65292;&#21516;&#26102;&#34920;&#26126;&#20855;&#26377;&#20943;&#36731;&#24187;&#35273;&#38382;&#39064;&#21644;&#25552;&#39640;LLM&#24615;&#33021;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.08518</link><description>&lt;p&gt;
UPRISE: &#36890;&#29992;&#25552;&#31034;&#26816;&#32034;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation. (arXiv:2303.08518v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08518
&lt;/p&gt;
&lt;p&gt;
UPRISE&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#26816;&#32034;&#22120;&#65292;&#21487;&#33258;&#21160;&#20026;&#32473;&#23450;&#30340;&#38646;&#26679;&#26412;&#20219;&#21153;&#36755;&#20837;&#26816;&#32034;&#25552;&#31034;&#65292;&#20174;&#32780;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#35780;&#20272;&#12290;&#23427;&#36890;&#36807;&#36328;&#20219;&#21153;&#21644;&#36328;&#27169;&#22411;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;&#20854;&#36890;&#29992;&#24615;&#21644;&#28508;&#21147;&#65292;&#21516;&#26102;&#34920;&#26126;&#20855;&#26377;&#20943;&#36731;&#24187;&#35273;&#38382;&#39064;&#21644;&#25552;&#39640;LLM&#24615;&#33021;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22240;&#20854;&#20986;&#33394;&#30340;&#33021;&#21147;&#32780;&#21463;&#27426;&#36814;&#65292;&#20294;&#38656;&#35201;&#29305;&#23450;&#27169;&#22411;&#30340;&#24494;&#35843;&#25110;&#20219;&#21153;&#29305;&#23450;&#25552;&#31034;&#24037;&#31243;&#21487;&#33021;&#20250;&#38459;&#30861;&#20854;&#19968;&#33324;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;UPRISE&#65288;&#36890;&#29992;&#25552;&#31034;&#26816;&#32034;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#35780;&#20272;&#65289;&#65292;&#35813;&#26041;&#27861;&#35843;&#25972;&#20102;&#36731;&#37327;&#32423;&#21644;&#22810;&#21151;&#33021;&#30340;&#26816;&#32034;&#22120;&#65292;&#20197;&#33258;&#21160;&#26816;&#32034;&#32473;&#23450;&#38646;&#26679;&#26412;&#20219;&#21153;&#36755;&#20837;&#30340;&#25552;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#36328;&#20219;&#21153;&#21644;&#36328;&#27169;&#22411;&#26041;&#26696;&#20013;&#23637;&#31034;&#20102;&#36890;&#29992;&#24615;&#65306;&#26816;&#32034;&#22120;&#38024;&#23545;&#21508;&#31181;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#65292;&#20294;&#22312;&#30475;&#19981;&#35265;&#30340;&#20219;&#21153;&#31867;&#22411;&#19978;&#36827;&#34892;&#27979;&#35797;&#65307;&#25105;&#20204;&#22312;&#19968;&#20010;&#23567;&#22411;&#20923;&#32467;LLM&#8212;&#8212;GPT-Neo-2.7B&#19978;&#35843;&#25972;&#26816;&#32034;&#22120;&#65292;&#20294;&#22312;&#19981;&#21516;&#35268;&#27169;&#30340;LLM&#19978;&#27979;&#35797;&#26816;&#32034;&#22120;&#65292;&#20363;&#22914;BLOOM-7.1B&#12289;OPT-66B&#21644;GPT3-175B&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;UPRISE&#22312;&#25105;&#20204;&#19982;ChatGPT&#30340;&#23454;&#39564;&#20013;&#20943;&#36731;&#20102;&#24187;&#35273;&#38382;&#39064;&#65292;&#34920;&#26126;&#23427;&#26377;&#28508;&#21147;&#25913;&#36827;&#29978;&#33267;&#26159;&#26368;&#24378;&#30340;LLM&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are popular for their impressive abilities, but the need for model-specific fine-tuning or task-specific prompt engineering can hinder their generalization. We propose UPRISE (Universal Prompt Retrieval for Improving zero-Shot Evaluation), which tunes a lightweight and versatile retriever that automatically retrieves prompts for a given zero-shot task input. Specifically, we demonstrate universality in a cross-task and cross-model scenario: the retriever is tuned on a diverse set of tasks, but tested on unseen task types; we use a small frozen LLM, GPT-Neo-2.7B, for tuning the retriever, but test the retriever on different LLMs of much larger scales, such as BLOOM-7.1B, OPT-66B and GPT3-175B. Additionally, we show that UPRISE mitigates the hallucination problem in our experiments with ChatGPT, suggesting its potential to improve even the strongest LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#25991;&#26412;&#24341;&#23548;&#30340;&#35270;&#39057;&#34917;&#20840;&#65292;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#25513;&#30721;&#35270;&#39057;&#29983;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#22788;&#29702;&#19977;&#31181;&#24773;&#20917;&#30340;&#35270;&#39057;&#34917;&#20840;&#12290;&#35813;&#26041;&#27861;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.12824</link><description>&lt;p&gt;
&#35762;&#36848;&#25925;&#20107;&#65306;&#36890;&#36807;&#22810;&#27169;&#24577;&#25513;&#30721;&#35270;&#39057;&#29983;&#25104;&#32479;&#19968;&#25991;&#26412;&#24341;&#23548;&#30340;&#35270;&#39057;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Tell Me What Happened: Unifying Text-guided Video Completion via Multimodal Masked Video Generation. (arXiv:2211.12824v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#25991;&#26412;&#24341;&#23548;&#30340;&#35270;&#39057;&#34917;&#20840;&#65292;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#25513;&#30721;&#35270;&#39057;&#29983;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#22788;&#29702;&#19977;&#31181;&#24773;&#20917;&#30340;&#35270;&#39057;&#34917;&#20840;&#12290;&#35813;&#26041;&#27861;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32473;&#23450;&#21069;&#20960;&#20010;&#38745;&#24577;&#24103;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#19968;&#20010;&#35270;&#39057;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#20197;&#26102;&#38388;&#19978;&#30340;&#36830;&#36143;&#24615;&#26469;&#39044;&#27979;&#21512;&#29702;&#30340;&#26410;&#26469;&#24103;&#12290;&#38500;&#20102;&#35270;&#39057;&#39044;&#27979;&#20043;&#22806;&#65292;&#20174;&#26368;&#21518;&#19968;&#24103;&#20498;&#22238;&#25110;&#32773;&#22312;&#22836;&#23614;&#20043;&#38388;&#36827;&#34892;&#25554;&#20540;&#20063;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20294;&#26159;&#23427;&#20204;&#24456;&#23569;&#34987;&#29992;&#20110;&#35270;&#39057;&#34917;&#20840;&#12290;&#30001;&#20110;&#20165;&#20973;&#20960;&#20010;&#24103;&#30340;&#25552;&#31034;&#21487;&#33021;&#20250;&#26377;&#19981;&#21516;&#30340;&#32467;&#26524;&#65292;&#22240;&#27492;&#33021;&#22815;&#36981;&#24490;&#33258;&#28982;&#35821;&#35328;&#26469;&#25191;&#34892;&#35270;&#39057;&#34917;&#20840;&#30340;&#31995;&#32479;&#21487;&#33021;&#20250;&#26174;&#33879;&#25552;&#39640;&#21487;&#25511;&#24615;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#25991;&#26412;&#24341;&#23548;&#30340;&#35270;&#39057;&#34917;&#20840;(TVC)&#65292;&#35201;&#27714;&#27169;&#22411;&#22312;&#25351;&#20196;&#30340;&#25351;&#23548;&#19979;&#20174;&#37096;&#20998;&#24103;&#20013;&#29983;&#25104;&#35270;&#39057;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#25513;&#30721;&#35270;&#39057;&#29983;&#25104;(MMVG)&#26469;&#35299;&#20915;&#36825;&#20010;TVC&#20219;&#21153;&#12290;&#22312;&#35757;&#32451;&#26399;&#38388;&#65292;MMVG&#23558;&#35270;&#39057;&#24103;&#31163;&#25955;&#20026;&#35270;&#35273;&#20196;&#29260;&#65292;&#24182;&#36974;&#30422;&#20102;&#22823;&#37096;&#20998;&#20196;&#29260;&#20197;&#36827;&#34892;&#20219;&#24847;&#26102;&#38388;&#28857;&#30340;&#35270;&#39057;&#34917;&#20840;&#12290;&#22312;&#25512;&#29702;&#26102;&#65292;&#19968;&#20010;&#21333;&#29420;&#30340;MMVG&#27169;&#22411;&#21487;&#20197;&#22788;&#29702;TVC&#30340;&#25152;&#26377;&#19977;&#31181;&#24773;&#20917;&#65292;&#21253;&#25324;&#20174;&#21069;&#20960;&#20010;&#24103;&#39044;&#27979;&#30340;&#35270;&#39057;&#65292;&#20174;&#26368;&#21518;&#19968;&#24103;&#20498;&#22238;&#30340;&#35270;&#39057;&#20197;&#21450;&#22312;&#22836;&#37096;&#21644;&#23614;&#37096;&#20043;&#38388;&#36827;&#34892;&#25554;&#20540;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#24615;&#33021;&#65292;&#19982;&#20808;&#21069;&#30340;&#35270;&#39057;&#34917;&#20840;&#26041;&#27861;&#30456;&#27604;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;MMVG&#22312;&#25991;&#26412;&#24341;&#23548;&#30340;&#35270;&#39057;&#34917;&#20840;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating a video given the first several static frames is challenging as it anticipates reasonable future frames with temporal coherence. Besides video prediction, the ability to rewind from the last frame or infilling between the head and tail is also crucial, but they have rarely been explored for video completion. Since there could be different outcomes from the hints of just a few frames, a system that can follow natural language to perform video completion may significantly improve controllability. Inspired by this, we introduce a novel task, text-guided video completion (TVC), which requests the model to generate a video from partial frames guided by an instruction. We then propose Multimodal Masked Video Generation (MMVG) to address this TVC task. During training, MMVG discretizes the video frames into visual tokens and masks most of them to perform video completion from any time point. At inference time, a single MMVG model can address all 3 cases of TVC, including video pred
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25991;&#26412;&#35270;&#39057;&#36328;&#27169;&#24577;&#26816;&#32034;&#30340;&#21327;&#20316;&#25552;&#31034;&#35843;&#25972;&#65288;VoP&#65289;&#26694;&#26550;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;VoP&#20855;&#26377;&#26356;&#23569;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#21644;&#26356;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#30456;&#20284;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.12764</link><description>&lt;p&gt;
VoP&#65306;&#29992;&#20110;&#25991;&#26412;&#35270;&#39057;&#36328;&#27169;&#24577;&#26816;&#32034;&#30340;&#21327;&#20316;&#25552;&#31034;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
VoP: Text-Video Co-operative Prompt Tuning for Cross-Modal Retrieval. (arXiv:2211.12764v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25991;&#26412;&#35270;&#39057;&#36328;&#27169;&#24577;&#26816;&#32034;&#30340;&#21327;&#20316;&#25552;&#31034;&#35843;&#25972;&#65288;VoP&#65289;&#26694;&#26550;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;VoP&#20855;&#26377;&#26356;&#23569;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#21644;&#26356;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#30456;&#20284;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26368;&#36817;&#30340;&#30740;&#31350;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;CLIP&#26469;&#36890;&#36807;&#20351;&#29992;&#39069;&#22806;&#30340;&#37325;&#27169;&#22359;&#26469;&#35843;&#25972;backbone&#20174;&#32780;&#23454;&#29616;&#25991;&#26412;&#35270;&#39057;&#36328;&#27169;&#24577;&#26816;&#32034;&#65292;&#36825;&#19981;&#20165;&#24102;&#26469;&#20102;&#22823;&#37327;&#30340;&#35745;&#31639;&#36127;&#25285;&#21644;&#26356;&#22810;&#30340;&#21442;&#25968;&#65292;&#20063;&#23548;&#33268;&#20102;&#19978;&#28216;&#27169;&#22411;&#30340;&#30693;&#35782;&#36951;&#24536;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;VoP&#65306;&#25991;&#26412;&#35270;&#39057;&#21327;&#20316;&#25552;&#31034;&#35843;&#25972;&#65292;&#20197;&#23454;&#29616;&#23545;&#25991;&#26412;&#35270;&#39057;&#26816;&#32034;&#20219;&#21153;&#30340;&#39640;&#25928;&#35843;&#25972;&#12290;&#25152;&#25552;&#20986;&#30340;VoP&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#20855;&#26377;&#24341;&#20837;&#35270;&#39057;&#21644;&#25991;&#26412;&#25552;&#31034;&#30340;&#33021;&#21147;&#65292;&#21487;&#35270;&#20026;&#20855;&#26377;&#20165;0.1&#65285;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#24378;&#22823;&#22522;&#32447;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#35270;&#39057;&#30340;&#26102;&#31354;&#29305;&#24449;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19977;&#31181;&#26032;&#22411;&#35270;&#39057;&#25552;&#31034;&#26426;&#21046;&#65292;&#20197;&#25552;&#39640;&#19981;&#21516;&#21487;&#35757;&#32451;&#21442;&#25968;&#35268;&#27169;&#30340;&#24615;&#33021;&#12290;VoP&#22686;&#24378;&#30340;&#22522;&#26412;&#24605;&#24819;&#26159;&#20998;&#21035;&#21033;&#29992;&#29305;&#23450;&#30340;&#21487;&#35757;&#32451;&#25552;&#31034;&#26469;&#27169;&#25311;&#24103;&#20301;&#32622;&#65292;&#24103;&#19978;&#19979;&#25991;&#21644;&#23618;&#20989;&#25968;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#23436;&#20840;&#24494;&#35843;&#30456;&#27604;&#65292;&#22686;&#24378;&#30340;VoP&#23454;&#29616;&#20102;&#30456;&#20284;&#29978;&#33267;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#23569;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#21644;&#26356;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many recent studies leverage the pre-trained CLIP for text-video cross-modal retrieval by tuning the backbone with additional heavy modules, which not only brings huge computational burdens with much more parameters, but also leads to the knowledge forgetting from upstream models. In this work, we propose the VoP: Text-Video Co-operative Prompt Tuning for efficient tuning on the text-video retrieval task. The proposed VoP is an end-to-end framework with both video &amp; text prompts introducing, which can be regarded as a powerful baseline with only 0.1% trainable parameters. Further, based on the spatio-temporal characteristics of videos, we develop three novel video prompt mechanisms to improve the performance with different scales of trainable parameters. The basic idea of the VoP enhancement is to model the frame position, frame context, and layer function with specific trainable prompts, respectively. Extensive experiments show that compared to full fine-tuning, the enhanced VoP achie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#27969;Transformer&#30340;&#36890;&#29992;&#20107;&#20214;&#36793;&#30028;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;&#65292;&#32467;&#21512;&#22810;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#36793;&#30028;&#31867;&#22411;&#25552;&#31034;&#65292;&#20197;&#21450;&#21333;&#35789;&#32423;&#21035;&#30340;&#38598;&#25104;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#29983;&#25104;&#26356;&#20154;&#24615;&#21270;&#30340;&#23383;&#24149;&#65292;&#24182;&#22312;GEBC&#27979;&#35797;&#38598;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2207.03038</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#27969;Transformer&#30340;&#36890;&#29992;&#20107;&#20214;&#36793;&#30028;&#23383;&#24149;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Dual-Stream Transformer for Generic Event Boundary Captioning. (arXiv:2207.03038v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.03038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#27969;Transformer&#30340;&#36890;&#29992;&#20107;&#20214;&#36793;&#30028;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;&#65292;&#32467;&#21512;&#22810;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#36793;&#30028;&#31867;&#22411;&#25552;&#31034;&#65292;&#20197;&#21450;&#21333;&#35789;&#32423;&#21035;&#30340;&#38598;&#25104;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#29983;&#25104;&#26356;&#20154;&#24615;&#21270;&#30340;&#23383;&#24149;&#65292;&#24182;&#22312;GEBC&#27979;&#35797;&#38598;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#21442;&#21152;CVPR2022&#36890;&#29992;&#20107;&#20214;&#36793;&#30028;&#23383;&#24149;&#29983;&#25104;&#27604;&#36187;&#30340;&#20248;&#32988;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#20219;&#21153;&#35201;&#27714;&#23383;&#24149;&#29983;&#25104;&#27169;&#22411;&#22312;&#32473;&#23450;&#35270;&#39057;&#36793;&#30028;&#21608;&#22260;&#33021;&#22815;&#29702;&#35299;&#30636;&#26102;&#29366;&#24577;&#21464;&#21270;&#65292;&#20351;&#20854;&#27604;&#20256;&#32479;&#35270;&#39057;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#27969;Transformer&#65292;&#25913;&#36827;&#20102;&#35270;&#39057;&#20869;&#23481;&#32534;&#30721;&#21644;&#23383;&#24149;&#29983;&#25104;&#20004;&#20010;&#26041;&#38754;&#65306;(1)&#25105;&#20204;&#21033;&#29992;&#19977;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#20174;&#19981;&#21516;&#31890;&#24230;&#25552;&#21462;&#35270;&#39057;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#36793;&#30028;&#31867;&#22411;&#20316;&#20026;&#25552;&#31034;&#65292;&#24110;&#21161;&#27169;&#22411;&#29983;&#25104;&#23383;&#24149;&#12290;(2)&#25105;&#20204;&#29305;&#21035;&#35774;&#35745;&#19968;&#20010;&#31216;&#20026;&#21452;&#27969;Transformer&#30340;&#27169;&#22411;&#65292;&#20197;&#23398;&#20064;&#21306;&#20998;&#24615;&#36793;&#30028;&#23383;&#24149;&#34920;&#31034;&#12290;(3)&#20026;&#20102;&#29983;&#25104;&#19982;&#20869;&#23481;&#30456;&#20851;&#19988;&#26356;&#21152;&#20154;&#24615;&#21270;&#30340;&#23383;&#24149;&#65292;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#21333;&#35789;&#32423;&#21035;&#30340;&#38598;&#25104;&#31574;&#30053;&#26469;&#25913;&#21892;&#25551;&#36848;&#36136;&#37327;&#12290;&#22312;GEBC&#27979;&#35797;&#38598;&#19978;&#21069;&#26223;&#19981;&#20439;&#30340;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes our champion solution for the CVPR2022 Generic Event Boundary Captioning (GEBC) competition. GEBC requires the captioning model to have a comprehension of instantaneous status changes around the given video boundary, which makes it much more challenging than conventional video captioning task. In this paper, a Dual-Stream Transformer with improvements on both video content encoding and captions generation is proposed: (1) We utilize three pre-trained models to extract the video features from different granularities. Moreover, we exploit the types of boundary as hints to help the model generate captions. (2) We particularly design an model, termed as Dual-Stream Transformer, to learn discriminative representations for boundary captioning. (3) Towards generating content-relevant and human-like captions, we improve the description quality by designing a word-level ensemble strategy. The promising results on the GEBC test split demonstrate the efficacy of our proposed 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;Gogioso&#21644;Pinzani&#22312;QPL 2021&#20013;&#25552;&#20986;&#30340;&#26463;&#29702;&#35770;&#27169;&#22411;&#65292;&#20026;&#35821;&#20041;&#27495;&#20041;&#30340;&#20004;&#20010;&#29305;&#24449;&#65288;&#19981;&#21516;&#21487;&#33021;&#35299;&#37322;&#30340;&#32852;&#21512;&#21487;&#20449;&#24230;&#21644;&#26576;&#20123;&#35789;&#22312;&#36807;&#31243;&#20013;&#25198;&#28436;&#26356;&#37325;&#35201;&#35282;&#33394;&#30340;&#22240;&#26524;&#32467;&#26500;&#65289;&#36827;&#34892;&#24314;&#27169;&#12290;&#36890;&#36807;&#23545;&#24515;&#29702;&#35821;&#35328;&#23398;&#25991;&#29486;&#20013;&#30340;&#27495;&#20041;&#30701;&#35821;&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#26512;&#65292;&#30740;&#31350;&#20154;&#21592;&#23545;&#20154;&#31867;&#23545;&#20110;&#36825;&#20123;&#27495;&#20041;&#30340;&#21028;&#26029;&#36827;&#34892;&#20102;&#23454;&#35777;&#27979;&#37327;&#12290;</title><link>http://arxiv.org/abs/2206.06807</link><description>&lt;p&gt;
&#35821;&#20041;&#27495;&#20041;&#30340;&#22240;&#26524;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
The Causal Structure of Semantic Ambiguities. (arXiv:2206.06807v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.06807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;Gogioso&#21644;Pinzani&#22312;QPL 2021&#20013;&#25552;&#20986;&#30340;&#26463;&#29702;&#35770;&#27169;&#22411;&#65292;&#20026;&#35821;&#20041;&#27495;&#20041;&#30340;&#20004;&#20010;&#29305;&#24449;&#65288;&#19981;&#21516;&#21487;&#33021;&#35299;&#37322;&#30340;&#32852;&#21512;&#21487;&#20449;&#24230;&#21644;&#26576;&#20123;&#35789;&#22312;&#36807;&#31243;&#20013;&#25198;&#28436;&#26356;&#37325;&#35201;&#35282;&#33394;&#30340;&#22240;&#26524;&#32467;&#26500;&#65289;&#36827;&#34892;&#24314;&#27169;&#12290;&#36890;&#36807;&#23545;&#24515;&#29702;&#35821;&#35328;&#23398;&#25991;&#29486;&#20013;&#30340;&#27495;&#20041;&#30701;&#35821;&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#26512;&#65292;&#30740;&#31350;&#20154;&#21592;&#23545;&#20154;&#31867;&#23545;&#20110;&#36825;&#20123;&#27495;&#20041;&#30340;&#21028;&#26029;&#36827;&#34892;&#20102;&#23454;&#35777;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27495;&#20041;&#26159;&#33258;&#28982;&#35821;&#35328;&#29616;&#35937;&#65292;&#22312;&#19981;&#21516;&#30340;&#35821;&#27861;&#12289;&#35821;&#20041;&#21644;&#35821;&#29992;&#23618;&#38754;&#19978;&#21457;&#29983;&#12290;&#23427;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65307;&#20363;&#22914;&#65292;&#22312;&#24515;&#29702;&#35821;&#35328;&#23398;&#39046;&#22495;&#65292;&#25105;&#20204;&#26377;&#22810;&#31181;&#31454;&#20105;&#24615;&#30340;&#30740;&#31350;&#20154;&#31867;&#28040;&#27495;&#36807;&#31243;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#30740;&#31350;&#26159;&#32463;&#39564;&#24615;&#30340;&#65292;&#22522;&#20110;&#30524;&#21160;&#36319;&#36394;&#31561;&#27979;&#37327;&#26041;&#27861;&#12290;&#26412;&#25991;&#39318;&#27425;&#23581;&#35797;&#20026;&#35821;&#20041;&#27495;&#20041;&#24418;&#24335;&#21270;&#36825;&#20123;&#36827;&#31243;&#65292;&#20854;&#20013;&#25105;&#20204;&#30830;&#23450;&#20102;&#20004;&#20010;&#29305;&#24449;&#65306;(1)&#19981;&#21516;&#21487;&#33021;&#35299;&#37322;&#20043;&#38388;&#30340;&#32852;&#21512;&#21487;&#20449;&#24230;&#65292;(2)&#26681;&#25454;&#26576;&#20123;&#35789;&#22312;&#36807;&#31243;&#20013;&#25198;&#28436;&#26356;&#37325;&#35201;&#35282;&#33394;&#30340;&#22240;&#26524;&#32467;&#26500;&#12290;Gogioso&#21644;Pinzani&#22312;QPL 2021&#20013;&#25552;&#20986;&#30340;&#26032;&#22411;&#26463;&#29702;&#35770;&#30830;&#23450;&#22240;&#26524;&#24615;&#27169;&#22411;&#24182;&#23545;&#36825;&#20123;&#29305;&#24449;&#36827;&#34892;&#25512;&#29702;&#25552;&#20379;&#20102;&#24037;&#20855;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#29702;&#35770;&#24212;&#29992;&#20110;&#20174;&#24515;&#29702;&#35821;&#35328;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#30340;&#27495;&#20041;&#30701;&#35821;&#25968;&#25454;&#38598;&#21644;&#25105;&#20204;&#20351;&#29992;Amazon&#30340;&#26426;&#26800;&#22303;&#32819;&#20854;&#24341;&#25806;&#25910;&#38598;&#30340;&#20154;&#31867;&#21487;&#20449;&#24230;&#21028;&#26029;&#20013;&#12290;&#25105;&#20204;&#27979;&#37327;&#20102;&#20854;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#12289;&#27495;&#20041;&#27700;&#24179;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ambiguity is a natural language phenomenon occurring at different levels of syntax, semantics, and pragmatics. It is widely studied; in Psycholinguistics, for instance, we have a variety of competing studies for the human disambiguation processes. These studies are empirical and based on eyetracking measurements. Here we take first steps towards formalizing these processes for semantic ambiguities where we identified the presence of two features: (1) joint plausibility degrees of different possible interpretations, (2) causal structures according to which certain words play a more substantial role in the processes. The novel sheaf-theoretic model of definite causality developed by Gogioso and Pinzani in QPL 2021 offers tools to model and reason about these features. We applied this theory to a dataset of ambiguous phrases extracted from Psycholinguistics literature and their human plausibility judgements collected by us using the Amazon Mechanical Turk engine. We measured the causal fr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Visual Spatial Reasoning&#65288;VSR&#65289;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;10k&#20010;&#33258;&#28982;&#25991;&#26412;-&#22270;&#20687;&#37197;&#23545;&#65292;&#29992;&#20110;&#25512;&#29702;&#21253;&#25324;66&#31181;&#31354;&#38388;&#20851;&#31995;&#65292;&#30740;&#31350;&#21457;&#29616;&#30446;&#21069;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#38590;&#20197;&#25429;&#25417;&#20851;&#31995;&#20449;&#24687;&#21644;&#36739;&#23569;&#20851;&#27880;&#29289;&#20307;&#30340;&#26041;&#21521;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2205.00363</link><description>&lt;p&gt;
&#35270;&#35273;&#31354;&#38388;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Visual Spatial Reasoning. (arXiv:2205.00363v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.00363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Visual Spatial Reasoning&#65288;VSR&#65289;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;10k&#20010;&#33258;&#28982;&#25991;&#26412;-&#22270;&#20687;&#37197;&#23545;&#65292;&#29992;&#20110;&#25512;&#29702;&#21253;&#25324;66&#31181;&#31354;&#38388;&#20851;&#31995;&#65292;&#30740;&#31350;&#21457;&#29616;&#30446;&#21069;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#38590;&#20197;&#25429;&#25417;&#20851;&#31995;&#20449;&#24687;&#21644;&#36739;&#23569;&#20851;&#27880;&#29289;&#20307;&#30340;&#26041;&#21521;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;&#20851;&#31995;&#26159;&#20154;&#31867;&#35748;&#30693;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20197;&#21508;&#31181;&#26041;&#24335;&#29992;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30446;&#21069;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#38590;&#20197;&#25429;&#25417;&#20851;&#31995;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Visual Spatial Reasoning&#65288;VSR&#65289;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;10k&#20010;&#33258;&#28982;&#25991;&#26412;-&#22270;&#20687;&#37197;&#23545;&#65292;&#21253;&#25324;66&#31181;&#33521;&#35821;&#30340;&#31354;&#38388;&#20851;&#31995;&#65288;&#22914;&#65306;&#22312;&#19979;&#38754;&#65292;&#22312;&#21069;&#38754;&#21644;&#38754;&#23545;&#65289;&#12290;&#34429;&#28982;&#20351;&#29992;&#20102;&#30475;&#20284;&#31616;&#21333;&#30340;&#27880;&#37322;&#26684;&#24335;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25968;&#25454;&#38598;&#21253;&#25324;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35821;&#35328;&#29616;&#35937;&#65292;&#20363;&#22914;&#19981;&#21516;&#30340;&#21442;&#32771;&#26694;&#26550;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20154;&#31867;&#21644;&#27169;&#22411;&#34920;&#29616;&#20043;&#38388;&#30340;&#24040;&#22823;&#24046;&#36317;&#65306;&#20154;&#31867;&#20934;&#30830;&#29575;&#39640;&#36798;95%&#20197;&#19978;&#65292;&#32780;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20165;&#33021;&#36798;&#21040;70%&#24038;&#21491;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;VLM&#25353;&#20851;&#31995;&#34920;&#29616;&#30340;&#33021;&#21147;&#19982;&#35757;&#32451;&#31034;&#20363;&#25968;&#37327;&#20960;&#20046;&#27809;&#26377;&#30456;&#20851;&#24615;&#65292;&#24182;&#19988;&#27979;&#35797;&#30340;&#27169;&#22411;&#36890;&#24120;&#26080;&#27861;&#35782;&#21035;&#28041;&#21450;&#23545;&#35937;&#26041;&#21521;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatial relations are a basic part of human cognition. However, they are expressed in natural language in a variety of ways, and previous work has suggested that current vision-and-language models (VLMs) struggle to capture relational information. In this paper, we present Visual Spatial Reasoning (VSR), a dataset containing more than 10k natural text-image pairs with 66 types of spatial relations in English (such as: under, in front of, and facing). While using a seemingly simple annotation format, we show how the dataset includes challenging linguistic phenomena, such as varying reference frames. We demonstrate a large gap between human and model performance: the human ceiling is above 95%, while state-of-the-art models only achieve around 70%. We observe that VLMs' by-relation performances have little correlation with the number of training examples and the tested models are in general incapable of recognising relations concerning the orientations of objects.
&lt;/p&gt;</description></item></channel></rss>